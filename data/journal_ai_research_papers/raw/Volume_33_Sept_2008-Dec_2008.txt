Journal of Artificial Intelligence Research 33 (2008) 521-549

Submitted 6/08; published 12/08

A Multiagent Reinforcement Learning Algorithm with
Non-linear Dynamics
Sherief Abdallah

SHERIEF. ABDALLAH @ BUID . AC . AE

Faculty of Informatics
The British University in Dubai
United Arab Emirates
(Fellow) School of Informatics
University of Edinburgh
United Kingdom

Victor Lesser

LESSER @ CS . UMASS . EDU

Department of Computer Science
University of Massachusetts Amherst
United States

Abstract
Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize
agents’ decisions. Due to the complexity of the problem, the majority of the previously developed
MARL algorithms assumed agents either had some knowledge of the underlying game (such as
Nash equilibria) and/or observed other agents actions and the rewards they received.
We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows
agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum
knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does
not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know
the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show
that our algorithm converges in benchmark two-player-two-action games. We also show that our
algorithm converges in the challenging Shapley’s game where previous MARL algorithms failed
to converge without knowing the underlying game or the NE. Furthermore, we show that WPL
outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and
learning concurrently.
An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents
interact with one another. Such an analysis not only verifies whether agents using a given MARL
algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to
convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPL’s convergence is difficult, because of the non-linear nature of WPL’s dynamics,
unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we
numerically solve WPL’s dynamics differential equations and compare the solution to the dynamics
of previous MARL algorithms.

1. Introduction
The decision problem of an agent can be viewed as selecting a particular action at a given state.
A well-known simple example of single-agent decision problem is the multi-armed bandit (MAB)
problem: an agent needs to choose a lever among a set of available levers. The reward of executing each action is drawn randomly according to a fixed distribution. The agent’s goal is to choose
c
2008
AI Access Foundation. All rights reserved.

A BDALLAH AND L ESSER

the lever (the action) with the highest expected reward. In order to do so, the agent samples the
underlying reward distribution of each action (by trying different actions and observing the resulting rewards). The goal of reinforcement learning algorithms in general is to eventually stabilize
(converge) to a strategy that maximizes the agent’s payoff. Traditional reinforcement learning algorithms (such as Q-learning) guarantee convergence to the optimal policy in a stationary environment
(Sutton & Barto, 1999), which simply means that the reward distribution associated with each action
is fixed and does not change over time.
In a multiagent system, the reward each agent receives for executing a particular action depends
on other agents’ actions as well. For example, consider extending the MAB problem to the multiagent case. The reward agent A gets for choosing lever 1 depends on which lever agent B has
chosen. If both agents A and B are learning and adapting their strategies, the stationary assumption
of the single-agent case is violated (the reward distribution is changing) and therefore single-agent
reinforcement learning techniques are not guaranteed to converge. Furthermore, in a multi-agent
context the optimality criterion is not as clear as in the single agent case. Ideally, we want all
agents to reach the equilibrium that maximizes their individual payoffs. However, when agents do
not communicate and/or agents are not cooperative, reaching a globally optimal equilibrium is not
always attainable (Claus & Boutilier, 1998). An alternative goal that we pursue here is converging to the Nash Equilibrium (NE) (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng,
2007), which is by definition a local maximum across agents (no agent can do better by deviating
unilaterally from the NE).
An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how policies of multiple learning agents evolve over time while interacting with one another. Such an analysis not only reveals whether agents using a particular MARL
algorithm will eventually converge, but also points out features of the MARL algorithm that are
exhibited during the convergence period. Analyzing the dynamics of a MARL algorithm in even
the simplest domains (particularly, two-player-two-action games) is a challenging task and therefore was performed on only few, relatively simple, MARL algorithms (Singh, Kearns, & Mansour,
2000; Bowling & Veloso, 2002) and for a restrictive subset of games. These analyzed dynamics
were either linear or piece-wise linear.
Recently, several multi-agent reinforcement learning (MARL) algorithms have been proposed
and studied (Claus & Boutilier, 1998; Singh et al., 2000; Peshkin, Kim, Meuleau, & Kaelbling,
2000; Littman, 2001; Bowling & Veloso, 2002; Hu & Wellman, 2003; Bowling, 2005; Abdallah &
Lesser, 2006; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007). Most of the MARL algorithms
assumed an agent knew the underlying game structure, or the Nash Equilibrium (NE) (Bowling &
Veloso, 2002; Banerjee & Peng, 2007). Some even required knowing what actions other agents
executed and what rewards they received (Hu & Wellman, 2003; Conitzer & Sandholm, 2007).
These assumptions are restrictive in open domains with limited communication (such as ebay or
Peer-to-Peer file-sharing) where an agent rarely knows of other agents’ existence let alone observing
their actions and knowing how their actions affect the agent’s local reward.
On the other hand, if agents are unaware of the underlying game (particularly the NE) and they
are not observing each other, then even a simple game with two players and two actions can be
challenging. For example, suppose Player 1 observes that its getting reward 10 from executing its
first action and reward 8 for executing action 2. As time passes, and Player 2 changes its policy,
Player 1 observes a switch in the reward associated with each action: action 1 now has a reward of
7 and action 2 has a reward of 9. Note that in both cases Player 1 is unaware of its own NE strategy
522

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

and is oblivious to the current policy of Player 2. The only feedback Player 1 is getting is the change
in its reward function, which in turn depends on Player 2’s policy. The same situation applies in
reverse to Player 2.
In this paper we propose a new MARL algorithm that enables agents to converge to a Nash
Equilibrium, in benchmark games, assuming each agent is oblivious to other agents and receives
only one type of feedback: the reward associated with choosing a given action. The new algorithm
is called the Weighted Policy Learner or WPL for reasons that will become clear shortly. We experimentally show that WPL converges in well-known benchmark two-player-two-action games.
Furthermore, we show that WPL converges in the challenging Shapley’s game, where state-of-theart MARL algorithms failed to converge (Bowling & Veloso, 2002; Bowling, 2005),1 unlike WPL.
We also show that WPL outperforms state-of-the-art MARL algorithms (shorter time to converge,
better performance during convergence, and better average reward) in a more realistic domain of
100 agents interacting and learning with one another. We also analyze WPL’s dynamics and show
that they are non-linear. This non-linearity made our attempt to solve, symbolically, the differential
equations representing WPL’s dynamics unsuccessful. Instead, we solve the equations numerically
and verify that our theoretical analysis and our experimental results are consistent. We compare the
dynamics of WPL with earlier MARL algorithms and discuss interesting differences and similarities.
The paper is organized as follows. Section 2 lays out the necessary background, including game
theory and closely related MARL algorithms. Section 3 describes our proposed algorithm. Section 4
analyzes WPL’s dynamics for this restricted class of games and compares it to previous algorithms’
dynamics. Section 5 discusses our experimental results. We conclude in Section 6.

2. Background
In this section we introduce the necessary background for our contribution. First, a brief review of
relevant game theory definitions is given. Then a review of relevant MARL algorithms is provided,
with particular focus on gradient-ascent MARL (GA-MARL) algorithms that are closely related to
our algorithm.
2.1 Game Theory
Game theory provides a framework for modeling agents’ interaction and was used by previous
researchers in order to analyze the convergence properties of MARL algorithms (Claus & Boutilier,
1998; Singh et al., 2000; Bowling & Veloso, 2002; Wang & Sandholm, 2003; Bowling, 2005;
Conitzer & Sandholm, 2007; Abdallah & Lesser, 2006). A game specifies, in a compact and simple
manner, how the payoff of an agent depends on other agents’ actions. A (normal form) game is
defined by the tuple hn, A1 , ..., An , R1 , ..., Rn i, where n is the number of players2 in the game, Ai
is the set of actions available to agent i, and Ri : A1 × ... × An → < is the reward (payoff) agent
i will receive as a function of the joint action executed by all agents. If the game has only two
players, then it is convenient to define their reward functions as a payoff matrix as shown in Table
1. Each cell (i, j) in the matrix represents the payoff received by the row player (Player 1) and
the column player (Player 2), respectively, if the row player plays action i and the column player
1. Except for MARL algorithms that assumed agents know information about the underlying game (Hu & Wellman,
2003; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007).
2. We use the terms agent and player interchangeably.

523

A BDALLAH AND L ESSER

plays action j. Table 1 and Table 2 provide example benchmark games that were used in evaluating
previous MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Conitzer & Sandholm, 2007;
Banerjee & Peng, 2007).
Table 1: Benchmark 2-action games. The coordination game has two pure NE(s): [(0, 1)r , (0, 1)c ]
and [(1, 0)r , (1, 0)c ]. Both matching-pennies and tricky games have one mixed NE where
all actions are played with equal probability, NE=[( 21 , 12 )r , ( 21 , 12 )c ]
(a) coordination

a1
a2

a1
2,1
0,0

(b) matching pennies

a2
0,0
1,2

H
T

H
1,-1
-1,1

T
-1,1
1,-1

(c) tricky

a1
a2

a1
0,3
1,0

a2
3,2
2,1

(d) general
game

a1
a2

2-player-2-action

a1
r11 , c11
r21 , c21

a2
r12 , c12
r22 , c22

Table 2: Benchmark 3-action games. Both games have one mixed NE where all actions are played
with equal probability, NE= [( 13 , 13 , 13 )r , ( 31 , 13 , 13 )c ].
(a) rock paper scissors

r1
r2
r3

c1
0,0
1,-1
-1,1

c2
-1,1
0,0
1,-1

(b) Shapley’s

c3
1, -1
-1,1
0,0

r1
r2
r3

c1
0,0
0,1
1,0

c2
1,0
0,0
0,1

c3
0,1
1,0
0,0

A policy (or a strategy) of an agent i is denoted by πi ∈ P D(Ai ), where P D(Ai ) is the set
of probability distributions over actions Ai . The probability of choosing an action ak according to
policy πi is πi (ak ). A policy is deterministic or pure if the probability of playing one action is 1
while the probability of playing other actions is 0, (i.e. ∃k : πi (ak ) = 1 AND ∀l 6= k : πi (al ) = 0),
otherwise the policy is stochastic or mixed.
A joint policy π is the collection of individual agents’ policies, i.e. π = hπ1 , π2 , ..., πn i, where
n is the number of agents. For convenience, the joint policy is usually expressed as π = hπi , π−i i,
where π−i is the collection of all policies of agents other than agent i.
V
Let variable A−i = {ha1 , ..., an i : aj ∈ Aj iP
6= j}. The
P expected reward agent i would get,
if agents follow a joint policy π, is Vi (hπi , π−i i) = ai ∈Ai a−i ∈A−i πi (ai )π−i (a−i ).Ri (ai , a−i ),
i.e. the reward averaged over the joint policy. A joint policy is a Nash Equilibrium, or NE, iff no
∗ i
agent can get higher expected reward by changing its policy unilaterally. More formally, hπi∗ , π−i
∗ i) ≥ V (hπ , π ∗ i). An NE is pure if all its constituting policies are
is an NE iff ∀i : Vi (hπi∗ , π−i
i
i −i
pure. Otherwise the NE is called mixed or stochastic. Any game has at least one Nash equilibrium,
but may not have any pure (deterministic) equilibrium.
Consider again the benchmark games in Table 1. The coordination game (Table 1(a)) is an
example of games that have at least one pure NE. The matching pennies game (Table 1(b)) is an
example of games that do not have any pure NE and only have a mixed NE (where each player
plays a1 and a2 with equal probability). Convergence of GA-MARL algorithms in games with pure
524

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

NE is easier than games where the only NE is mixed (Singh et al., 2000; Bowling & Veloso, 2002;
Zinkevich, 2003). The tricky game is similar to matching pennies game in that it only has one
mixed NE (no pure NE), yet some GA-MARL algorithms succeeded in converging in the matching
pennies games, while failing in the tricky game (Bowling & Veloso, 2002).
Table 2 shows well-known 2-player-3-action benchmark games. Shapley’s game, in particular,
has received considerable attention from MARL community (Bowling, 2005; Conitzer & Sandholm, 2007; Banerjee & Peng, 2007) as it remains challenging for state-of-the-art MARL algorithms despite its apparent simplicity (and similarity to the rock-paper-scissors game which is not
as challenging). Our proposed algorithm is the first MARL algorithm to converge in Shapley’s game
without observing other agents or knowing the underlying NE strategy.
Before introducing MARL algorithms in the following section, two issues are worth noting. The
first issue is the general assumption in the MARL context: agents play the same game repeatedly
for a large number of times. This is a necessary assumption in order for agents to gain experience
and learn. The second issue is that the games’ rewards, described in this section, are deterministic
given the joint action. However, from each agent’s perspective, the rewards are stochastic because
of the randomness caused by the other agents’ actions in the system.
2.2 Multiagent Reinforcement Learning, MARL
Early MARL algorithms were based on the Q-learning algorithm (Sutton & Barto, 1999), and therefore could only learn a deterministic policy (Claus & Boutilier, 1998), significantly limiting their
applicability in competitive and partially observable domains. The Joint Action Learner (Claus &
Boutilier, 1998) is an example of this family of algorithms that also required the knowledge of other
agents’ chosen actions.
Another class of MARL algorithms is the class of Equilibrium learners such as Nash-Q (Hu &
Wellman, 2003), OAL (Wang & Sandholm, 2003), and AWESOME (Conitzer & Sandholm, 2007).
Most of these algorithms assumed that each agent observed other agents’ actions in addition to
knowing the underlying game.3 Each agent then computed the Nash Equilibria. The purpose of
learning is to allow agents to converge to a particular Nash Equilibrium (in case all agents execute
the same MARL algorithm).
Observing other agents or knowing the underlying game structure are not applicable in open
and large domains, which motivated the development of gradient ascent learners. Gradient ascent
MARL algorithms (GA-MARL) learn a stochastic policy by directly following the expected reward
gradient. The ability to learn a stochastic policy is particularly important when the world is not fully
observable or has a competitive nature. Consider for example a blind robot in a maze (i.e. the robot
cannot distinguish between maze locations). Any deterministic policy (i.e. one that always chooses
a particular action everywhere) may never escape the maze, while a stochastic policy that chooses
each action with non-zero probability will eventually escape the maze.4 Similarly, in a competitive
domain a stochastic policy may be the only stable policy (e.g. the Nash Equilibrium in a competitive
game). The remainder of this section focuses on this family of algorithms as it is closely related to
our proposed algorithm.
3. Nash-Q did not require knowing the underlying game but required observing other agents rewards in addition to their
chosen actions.
4. Note that a uniformly random policy may not be optimal if the maze is biased in a specific direction.

525

A BDALLAH AND L ESSER

The Infinitesimal Gradient Ascent algorithm (IGA) (Singh et al., 2000) and its generalization
(Generalized IGA or GIGA) (Zinkevich, 2003) were proved to converge in games with pure NE.
However, both algorithms failed to converge in games with mixed NE, and therefore may not be
suitable for applications that require a mixed policy.
Several modifications to IGA and GIGA were proposed to avoid divergence in games with
mixed NE, including: IGA/PHC-WoLF (Bowling & Veloso, 2002), PHC-PDWoLF (Banerjee &
Peng, 2003), and GIGA-WoLF (Bowling, 2005). They all used some form of the Win or Learn Fast
heuristics (Bowling & Veloso, 2002), whose purpose is to speedup learning if the agent is doing
worse than its NE policy (losing) and to slow down learning if the agent is doing better than the NE
policy. The main problem with this heuristic is that an agent cannot know whether it is doing better
or worse than its NE policy without knowing the underlying game a prior. Therefore, a practical
implementation of the WoLF heuristic needed to use approximation methods for predicting the
performance of the agent’s NE policy. Our algorithm, called WPL, uses a different heuristic that
does not require knowing the NE policy and consequently, as we show, converges in both the tricky
game and Shapley’s game, where algorithms relying on the WoLF heuristic failed. Furthermore,
we show that in a large-scale partially-observable domain (the distributed task allocation problem,
DTAP) WPL outperforms state-of-the-art GA-MARL algorithm GIGA-WoLF.
The following section reviews in further detail the well known GA-MARL algorithms IGA,
IGA-WoLF, and GIGA-WoLF which we will use to compare our algorithm against.
2.3 Gradient-Ascent MARL Algorithms
The first GA-MARL algorithm whose dynamics were analyzed is the Infinitesimal Gradient Ascent
(IGA) (Singh et al., 2000). IGA is a simple gradient ascent algorithm where each agent i updates
its policy πi to follow the gradient of expected payoffs (or the value function) Vi , as illustrated by
the following equations.
∂Vi (π t )
∂πi
t
← projection(πi + ∆πit+1 )
∆πit+1 ← η

πit+1

Parameter η is called the policy-learning-rate and approaches zero in the limit (η → 0), hence
the word Infinitesimal in IGA. Function projection projects the updated policy to the space of
valid policies. The original IGA paper (Singh et al., 2000) defined the projection function (and
therefore IGA) in case each agent has only two actions to choose from. A general definition of the
projection function was later developed and the resulting algorithm was called Generalized IGA or
GIGA (Zinkevich, 2003). The generalized function is projection(x) = argminx0 :valid(x0 ) |x − x0 |,
where |x − x0 | is the Euclidean distance between x and x0 . APvalid P
policy π (over a set of actions
A) must satisfy two constraints: ∀a ∈ A : 1 ≥ π(a) ≥ 0 and π = a∈A π(a) = 1.
In other words, the space of valid policies is a simplex, which is a line segment (0,1),(1,0) in
case of two actions, a triangular surface (1,0,0), (0,1,0), (0,0,1) in case of three actions, and so
on. A joint policy, π, is a point in this simplex.
It is quite possible (especially when trying to
P
follow an approximate policy gradient) that π deviates from 1 and goes beyond the simplex. The
generalized projection function projects an invalid policy to the closest valid policy within the
simplex (Zinkevich, 2003).
526

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

The IGA algorithm did not converge in all two-player-two-action games (Singh et al., 2000).
Algorithm IGA-WoLF (WoLF stands for Win or Learn Fast) was proposed (Bowling & Veloso,
2002) in order to improve convergence properties of IGA by using two different learning rates as
follows. If a player is getting an average reward lower than the reward it would get for executing
its NE strategy, then the learning rate should be large. Otherwise (the player current policy is better
than its NE policy), the learning rate should be small. More formally,
∂Vi (π)
∆πi (a) ←
(a) ·
∂πi



ηlose if Vi (πi , π−i ) < Vi (πi∗ , π−i )
ηwin otherwise

πi ← projection(πi + ∆πi )
Where πi∗ is the NE policy for agent i and ηlose > ηwin are the learning rates. The dynamics of
IGA-WoLF have been analyzed and proved to converge in all 2-player-2-action games (Bowling &
Veloso, 2002), as we briefly review Section 2.4. IGA-WoLF has limited practical use, because it required each agent to know its equilibrium policy (which means knowing the underlying game). An
approximation to IGA-WoLF was proposed, PHC-WoLF, where an agent’s NE strategy is approximated by averaging the agent’s own policy over time (Bowling & Veloso, 2002). The approximate
algorithm, however, failed to converge in the tricky game shown in Table 1(c).
The GIGA-WoLF algorithm extended the GIGA algorithm with the WoLF heuristic. GIGAWoLF kept track of two policies π and z. Policy π was used to select actions for execution. Policy z
was used to approximate the NE policy. The update equations of GIGA-WoLF are (Bowling, 2005,
2004):
π̂ t+1 = projection(π t + δrt )
z

t+1

η t+1

= projection(π + δr /3)


||z t+1 − z t ||
= min 1, t+1
z
− π̂ t
t

t

π t+1 = π̂ t+1 + η t+1 (z t+1 − π̂ t+1 )

(1)
(2)
(3)
(4)

The main idea is a variant of the WoLF heuristics (Bowling & Veloso, 2002). An agent i changes
its policy πi faster if the agent is performing worse than policy z, i.e. V πi < V z . Because z moves
slower than π, GIGA-WoLF uses z to realize that it needs to change the current gradient direction.
This approximation allows GIGA-WoLF to converge in the tricky game, but not in Shapley’s game
(Bowling, 2005).
The following section briefly reviews the analysis of IGA’s and IGA-WoLF’s dynamics, showing
how a joint policy of two agents evolves over time. We will then build on this analysis in Section 4
when we analyze WPL’s dynamics.
2.4 Dynamics of GA-MARL Algorithms
Differential equations were used to model the dynamics of IGA (Singh et al., 2000). To simplify
analysis, the authors only considered two-player-two-action games, as we also do here. We will
refer to the joint policy of the two players at time t by the probabilities of choosing the first action
(pt , q t ), where π1 = (pt , 1 − pt ) is the policy of player 1 and π2 = (q t , 1 − q t ) is the policy of
527

A BDALLAH AND L ESSER

player 2. The t notation will be omitted when it does not affect clarity (for example, when we are
considering only one point in time).
IGA’s update equations can be simplified to be (note that rij and cij are the payoffs for row and
column players respectively):
pt+1 = pt + η

∂Vr (pt , q t )
= pt + η(Vr (1, q t ) − Vr (0, q t ))
∂p

where


Vr (1, q t ) − Vr (0, q t ) = r11 q t + r12 (1 − q t ) − r21 q t + r22 (1 − q t )
= q t (r11 − r12 − r21 + r22 ) + (r12 − r22 )
= u1 q t + u2

(5)

and similarly,
q t+1 = q t + η · (u3 pt + u4 )
where u1 , u2 , u3 , and u4 are game-dependent constants having the following values.
u1 = r11 − r12 − r21 + r22
u2 = r12 − r22
u3 = c11 − c12 − c21 + c22
u4 = c21 − c22
In analyzing IGA, the original paper distinguished between three types of games depending on
the u1 and u3 parameters: u1 u3 > 0, u1 u3 = 0, and u1 u3 < 0. Figure 1(a), Figure 1(b), and Figure
1(c) are taken from (Bowling & Veloso, 2002) and illustrate the dynamics of IGA for each of the
three cases. Each figure displays the space of joint policies, where the horizontal axis represent the
policy of the row player, p, and the vertical axis represents the policy of the column player q.5
The joint policy of the two agents evolves over time by following the directed lines in the
figures. For example, Figure 1(b) illustrates that starting from any joint policy, the two players will
eventually evolve to one of two equilibriums, either the bottom-left corner, or the upper-right corner.
It should be noted that, for simplicity, the dynamics shown in Figure 1 are unconstrained: the effect
of the projection function (the simplex described in Section 2.3) is not shown. In IGA’s analysis
(Singh et al., 2000), the effect of the projection function was taken into account by considering all
possible locations of the simplex.
In the first and the second cases, IGA converged. In the third case IGA oscillated around the
equilibrium strategy, without ever converging to it. This happens because at any point of time, unless
both agents are at the NE, one agent is better off changing its policy to be closer to its NE strategy,
while the other agent is better off changing its policy to be further away from its NE strategy. These
roles switch as one of the agents crosses its NE strategy as shown in Figure 1(c).
As described in Section 2.3, IGA-WoLF was proposed (Bowling & Veloso, 2002) as an extension to IGA that ensures convergence in the third case. The idea of IGA-WoLF, as described earlier,
5. Note that Figure 1(b) and Figure 1(c) are divided into four quadrants A, B, C, and D for clarification. The gradient
direction in each quadrant is illustrated by arrows in Figure 1(c).

528

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

Figure 1: An illustration of IGA’s dynamics (Bowling & Veloso, 2002).
is to have two learning rates by which an agent is moving toward or away from the NE. The following equations capture the dynamics of IGA-WoLF, where hp∗ , q ∗ i is the NE. The equations use a
factored form of ηlose and ηwin .
p =p
t

q =q
t

t−1

+ η(u1 q

t−1

t−1

+ η(u3 p

t−1

+ u2 ) ·



llose if Vr (pt−1 , q t−1 ) < Vr (p∗ , q t−1 )
lwin otherwise

+ u4 ) ·



llose if Vc (q t−1 , pt−1 ) < Vc (q ∗ , pt−1 )
lwin otherwise

The dynamics of IGA-WoLF is best illustrated visually by Figure 2, which is taken from (Bowling & Veloso, 2002). The switching between learning rates causes switching between ellipses of
smaller diameters, eventually leading to convergence.
The main limitation of IGA-WoLF is that it assumed each agent knows the NE policy (needed
to switch between the two modes of the learning rate). The following section presents WPL, which
does not make this assumption, at the expense of having more complex dynamics as we show in
Section 4.

3. Weighted Policy Learner (WPL)
We propose in this paper the Weighted Policy Learner (WPL) algorithm, which has the following
update equations:
529

A BDALLAH AND L ESSER

Figure 2: An illustration of IGA-WoLF’s dynamics for the case of mixed NE (Bowling & Veloso, 2002).

∂Vi (π)
∆πi (a) ←
·η·
∂πi (a)

(

i (π)
πi (a)
if ∂V
∂πi (a) < 0
1 − πi (a) otherwise

πi ← projection(πi + ∆πi )
The projection function is adopted from GIGA (Zinkevich, 2003) with minor modification:∀a :
1 ≥ π(a) ≥  (the modification ensures a minimum amount of exploration ). The algorithm works
as follows. If the gradient for a particular action is negative then the gradient is weighted by πi (a),
otherwise (gradient is positive) the gradient is weighted by (1−πi (a)). So effectively, the probability
of choosing a good action increases by a rate that decreases as the probability approaches 1 (or the
boundary of the simplex). Similarly, the probability of choosing a bad action decreases at a rate that
also decreases as the probability approaches zero.
So unless the gradient direction changes, a WPL agent decreases its learning rate as the agent
gets closer to simplex boundary. This means, for a 2-player-2-action game, a WPL agent will move
towards its NE strategy (away from the simplex boundary) faster than moving away from its NE
strategy (towards the simplex boundary), because the NE strategy is always inside the simplex.
WPL is biased against pure (deterministic) policies and will reach a pure policy only in the
limit (because the rate of updating the policy approaches zero). This theoretical limitation is of
little practical concern for two reasons. The first reason is exploration: 100% pure strategies are
bad because they prevent agents from exploring other actions (in an open dynamic environment the
reward of an action may change over time).
The second reason is that if an action dominates another action (the case when a game has a
∂Vi (π)
i (π)
pure NE), then ∂V
∂πi (a) can be large enough that ∂πi (a) · η > 1. In this case WPL will jump to a
pure policy in one step.6 Note that in Section 4, in order to simplify theoretical analysis, we assume
η → 0. From a practical perspective, however, η is set to value close to 0, but never 0. This holds
6. In fact, WPL can even go beyond the simplex of valid policies, that is when the projection function comes into play

530

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

for all gradient-ascent MARL algorithms (Bowling & Veloso, 2002; Bowling, 2005; Banerjee &
Peng, 2007).
There are key differences between IGA-WoLF and WPL’s update rules despite the apparent
similarity. Both algorithms have two modes of learning rates (corresponding to the two conditions
in the policy update rule). However, IGA-WoLF needs to know the equilibrium strategy in order to
distinguish between the two modes, unlike WPL that needs to know only the value of each action.
Another difference is that IGA-WoLF used two fixed learning rates (ηl > ηw ) for the two modes,
while WPL uses a continuous spectrum of learning rates, depending on the current policy. This can
be understood from the definition of ∆πi in WPL’s update equations, which includes an additional
(continuous) scaling factor: πit . This particular feature causes WPL’s dynamics to be non-linear, as
we discuss in the following section.

4. Analyzing WPL’s Dynamics
Claim 1 WPL has non-linear dynamics.
Proof.
The policies of two agents following WPL can be expressed as follows
t

t−1

+ η(u3 p

t

t−1

+ η(u1 q

q ←q

t−1

+ u4 )



1 − q t−1 if u3 pt−1 + u4 > 0
q t−1
otherwise

+ u2 )



1 − pt−1 if u1 q t−1 + u2 > 0
pt−1
otherwise

and
p ←p

t−1

Note that u1 q t−1 + u2 = Vr (1, q t−1 ) − Vr (0, q t−1 ) from Equation 5. It then follows that
pt − pt−1
=
η (t − (t − 1))
(u1 q

t−1

+ u2 )



1 − pt−1 if Vr (1, q t−1 ) > Vr (0, q t−1 )
pt−1
otherwise



1 − q t−1 if Vc (1, pt−1 ) > Vc (0, pt−1 )
q t−1
otherwise

and analogously for the column player
q t − q t−1
=
η (t − (t − 1))
(u3 p

t−1

+ u4 )

As η → 0, the equations above become differential:
q 0 (t) =
(u3 p

t−1

+ u4 )



1 − q t−1 if Vc (1, pt−1 ) > Vc (0, pt−1 )
q t−1
otherwise
531

(6)

A BDALLAH AND L ESSER

p0 (t) =
(u1 q

t−1

+ u2 )



1 − pt−1 if Vr (1, q t−1 ) > Vr (0, q t−1 )
pt−1
otherwise

(7)

Notice here that the NE strategy does not appear in WPL’s equations, unlike IGA-WoLF. Furthermore, while IGA and IGA-WoLF needed to take the projection function into account, we can
safely ignore the projection function while analyzing the dynamics of WPL for two-player-twoaction games. This is due to the way WPL scales the learning rate using the current policy. By
the definition of p0 (t), a positive p0 (t) approaches zero as p approaches one and a negative p0 (t)
approaches zero as p approaches zero. In other words, as p (or q) approaches 0 or 1, the learning
rate approaches zero, and therefore p (or q) will never go beyond the simplex of valid policies.7
This observation will become apparent in Section 4.2 when we compare the dynamics of WPL to
the dynamics of both IGA and IGA-WoLF.
Following IGA-WoLF’s analysis (Bowling & Veloso, 2002), as illustrated in Figure 3, we will
focus on the challenging case when there is no deterministic NE (the NE is inside the joint policy
space) and analyze how p and q evolve over time. This is the case where the original IGA oscillated
as shown in Figure 1. It is important to note, however, that all gradient ascent MARL algorithms
(including WPL) converge in 2x2 games cases where there is (at least) one pure NE, because the
dynamics do not have any loops and eventually lead to one of the pure equilibriums (Singh et al.,
2000; Bowling & Veloso, 2002).
We will solve WPL’s differential equations for the period 0 → T 4 in Figure 3, assuming that
Player 2 starts at its NE policy q ∗ at time 0 and returns to q ∗ at time T 4. If we can prove that, over
the period 0 → T 4, Player 2’s policy p(t) gets closer to the NE policy p∗ , i.e. pmin2 − pmin1 > 0
in Figure 3, then by induction the next time period p will get closer to the equilibrium and so on.
For readability, p and q will be used instead of p(t) and q(t) for the remainder of this section.
Without loss of generality, we can assume that u1 q + u2 > 0 iff q > q ∗ and u3 p + u4 > 0 iff p < p∗ .
The overall period 0 → T 4 is divided into four intervals defined by times 0, T 1, T 2, T 3, and T 4.
Each period corresponds to one combination of p and q as follows. For the first period 0 → T 1,
p < p∗ and q > q ∗ , therefore agent p is better off moving toward the NE, while agent q is better off
moving away from the NE. The differential equations can be solved by dividing p0 and q 0
dp
(1 − p)(u1 q + u2 )
=
dq
(1 − q)(u3 p + u4 )
Then by separation we have
Z

p∗

pmin1

u3 p + u4
dp =
1−p

Z

qmax

q∗

u1 q + u2
dq
1−q

1 − pmin1
=
1 − p∗
1 − q∗
−u1 (qmax − q ∗ ) + (u1 + u2 )ln
1 − qmax

−u3 (p∗ − pmin1 ) + (u3 + u4 )ln

7. In practice WPL still needs the projection function because η must be set to a small value strictly larger than 0.

532

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

Policy
q
qmax

(p*,qmax)

q*
qmin

(pmin2,q*)
(pmin1,q*)

pmax

(pmax,q*)

p

p*
pmin2

pmin1

T1

T2

T3

time

T4

Figure 3: An illustration of WPL’s dynamics. The figure to the left shows policies evolution over time,
while the figure to the right shows the joint policy space.

Similarly, for period T 1 → T 2, where p > p∗ and q > q ∗
1 − p∗
=
1 − pmax
q∗
u1 (q ∗ − qmax ) + u2 ln
qmax

−u3 (pmax − p∗ ) + (u3 + u4 )ln

and for period T 2 → T 3, where p > p∗ and q < q ∗
u3 (p∗ − pmax ) + u4 ln

p∗
pmax

= u1 (qmin − q ∗ ) + u2 ln

qmin
q∗

and finally for period T 3 → T 4, where p < p∗ and q < q ∗
pmin2
=
p∗
1 − qmin
−u1 (q ∗ − qmin ) + (u1 + u2 )ln
1 − q∗
u3 (pmin2 − p∗ ) + u4 ln

These are 4 non-linear equations (note the existence of both x and ln(x) in all equations) in
5 unknowns (pmin , pmin2 , pmax , qmin1 , qmax ), along with the inequalities governing the constants
u1 , u2 , u3 , and u4 .
Because WPL’s dynamics are non-linear, we could not obtain a closed-form solution and therefore we could not formally prove WPL’s convergence.8 Instead, we solve the equations numerically
in the following section. Although a numerical solution is still not a formal proof, it provides useful
insights into understanding WPL’s dynamics.
8. If the equations were linear, we could have substituted for all unknowns in terms of pmin1 and p2min and easily
determined whether pmin2 − pmin1 > 0, similar to IGA-WoLF.

533

A BDALLAH AND L ESSER

4.1 Numerical Solution
We have used Matlab to solve the equations numerically. Figure 4 shows the theoretical behavior
predicted by our model for the matching-pennies game. There is a clear resemblance to the actual
(experimental) behavior for the same game (Figure 5). Note that the time-scale on the horizontal
axes of both figures are effectively the same, because what is displayed on the horizontal axis in
Figure 5 is decision steps in the simulation. When multiplied by the actual policy-learning-rate η
(the time step) used in the experiments, 0.001, both axes become identical.
1
0.9
0.8
0.7

policy

0.6
0.5
0.4
0.3
0.2
0.1
0

0

2

4

6

8

10

time

Figure 4: Convergence of WPL as predicted by the theoretical model for the matching pennies game.

1

player1
player2

0.8

policy

0.6
0.4
0.2
0
0

2000

4000

6000

8000

10000

time

Figure 5: Convergence of WPL through experiments, using η = 0.001 .
Figure 6 plots p(t) versus q(t), for a game with NE= (0.9, 0.9) (u1 = 0.5, u2 = −0.45, u3 =
−0.5, , u4 = 0.45) and starting from 157 different initial joint policies (40 initial policies over each
side of the joint policy space). Figure 7 plots p(t) and q(t) against time, showing convergence from
each of the 157 initial joint policies.
We repeated the above numerical solution for 100 different NE(s) that make a 10x10 grid in the
p-q space (and starting from the 157 boundary initial joint policies for each individual NE). The
WPL algorithm converges to the NE in a spiral fashion similar to the specific case in Figure 6 in
all the 100 NE(s). Instead of drawing 100 figures (one for each NE), Figure 8 plots the merge of
the 100 figures in a compact way: plotting agents’ joint policy from time 700 to time 800 (which is
enough for convergence as Figure 7 shows). The two agents converge in all the 100 NE cases, as
indicated by the centric points. Note that if the algorithm was not converging, then the joint policies
trajectories after time 700 should occupy more space, because the 157 initial joint policies are on
the policy space boundary.
534

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

q

p
Figure 6: An illustration of WPL convergence to the (0.9,0.9) NE in the p-q space: p on the horizontal axis
and q on the vertical axis.

policy

time

Figure 7: An illustration of WPL convergence to the (0.9,0.9) NE: p(t) (gray) and q(t) (black) are plotted on
the vertical axis against time (horizontal axis).

Figure 8: An illustration of WPL’s convergence for 10x10 NE(s).
535

A BDALLAH AND L ESSER

4.2 Comparing Dynamics of IGA, IGA-WoLF, and WPL
With differential equations modeling each of the three algorithms, we now compare their dynamics
and point out the main distinguishing characteristics of WPL. Matlab was again used to solve the
differential equations (of the three algorithms) numerically. Figure 9 shows the dynamics for a
game with u1u3 < 0 and the NE=(0.5,0.5). The joint strategy moves in clockwise direction. The
dynamics of WPL are very close to IGA-WoLF, with IGA-WoLF converging a bit faster (after one
complete round around the NE, IGA-WoLF is closer to the NE than WPL). It is still impressive that
WPL has comparable performance to IGA-WoLF, since WPL does not require agents to know their
NE strategy or the underlying game a priori, unlike IGA-WoLF.
1
IGA
IGA−WoLF
WPL

0.9
0.8
0.7

player 2

0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.2

0.4

0.6

0.8

1

player 1

Figure 9: Dynamics of IGA, IGA-WoLF, and WPL in a game with NE=(0.5,0.5).
Figure 10 shows the dynamics in a game with u1u3 < 0 but the NE=(0.5,0.1). Three interesting
regions in the figure are designated with A,B, and C. Region A shows that both IGA and IGAWoLF dynamics are discontinuous due to the effect of the projection function. Because WPL uses
a smooth policy weighting scheme, the dynamics remain continuous. This is also true in region B.
In region C, WPL initially deviates from the NE more than IGA, but eventually converges as well.
The reason is that because the NE, in this case, is closer to the boundary, policy weighting makes
the vertical player move at a much slower pace when moving downward (the right half) than the
horizontal player.
Figure 11 shows the dynamics for the coordination game (Table 1(a)), starting from initial joint
policy (0.1,0.6). The coordination game has two NEs: (0,0) and (1,1). All algorithms converge to
the closer NE, (0,0), but again we see that both IGA and IGA-WoLF have discontinuity in their
dynamics, unlike WPL which smoothly converge to the NE. Notice that WPL converges to a pure
NE in the limit, but the graph shows the joint policy space, so there is no depiction of time.
The continuity of WPL’s dynamics comes into play when the target policy is mixed. While IGA,
GIGA, and GIGA-WoLF algorithms go through extreme deterministic policies during the transient
period prior to convergence, which can cause ripple effect in realistic settings where large number of
agents are interacting asynchronously (e.g. through a network). WPL never reaches such extremes
and the experimental results in Section 5.4 show that GIGA-WoLF takes significantly more time to
converge compared to WPL in a domain of 100 agents.
536

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

1
IGA
IGA−WoLF
WPL

0.8

player 2

A
0.6

0.4

0.2

0

C

B
0

0.1

0.2

0.3

0.4

0.5
player 1

0.6

0.7

0.8

0.9

1

Figure 10: Dynamics of IGA, IGA-WoLF, and WPL in a game with NE=(0.5,0.1). Three regions are of
particular interest and highlighted in the figure: A, B, and C.

1
IGA
IGA−WoLF
WPL

0.9
0.8
0.7

player 2

0.6
0.5
0.4
0.3
0.2
0.1
0

0

0.1

0.2

0.3

0.4

0.5
0.6
player 1

0.7

0.8

0.9

Figure 11: Dynamics of IGA, IGA-WoLF, and WPL in the coordination game with two NEs=(0,0) and
(1,1). Note that IGA’s dynamics is exactly as IGA-WoLF’s dynamics in this case.

The following section presents our experimental results for both benchmark 2-player-2-action
games and larger games involving more actions and more agents.

5. Experimental Results
This section is divided into three parts. Section 5.1 discusses main learning parameters that need
to be set in practice. Section 5.2 presents the experimental results for the 2x2 benchmark games.
Section 5.3 presents the experimental results for domains with larger number of actions and/or
agents.
537

A BDALLAH AND L ESSER

5.1 Learning Parameters
Conducting experiments for WPL involves setting two main parameters: the policy-learning-rate,
η, and the value-learning-rate, α. In theory (as discussed in Section 4) the policy-learning-rate,
η, should approach zero. In practice, however, this is not possible and we have tried setting η to
different small values between 0.01 and 0.00001. The smaller η is, the longer it will take WPL to
converge, and the smaller the oscillations around the NE will become (and vice versa), similar to
previous GA-MARL algorithms. A reasonable value that we have used in most of our experiments
is η = 0.002.
The value-learning-rate α is used to compute the expected reward of an action a at time t, or
rt (a), which is not known a priori. The common approach, which was used in previous GA-MARL
algorithms and we also use here, is using the equation rt+1 (a) ← αRt + (1 − α)rt (a), where Rt
is the sample reward received at time t and 0 ≤ α ≤ 1 (Sutton & Barto, 1999). We have tried three
values for α: 0.01,0.1, and 1.
5.2 Benchmark 2x2 Games
We have implemented a set of previous MARL algorithms for comparison: PHC-WoLF (the realistic
implementation of IGA-WoLF), GIGA, and GIGA-WoLF. In our experiments we have not used any
decaying rates. The reason is that in an open system where dynamics can continuously change, one
would want learning to be continuous as well. We have fixed the exploration rate  to 0.1 (which
comes into play in the modified projection function in Section 3), the policy-learning-rate η to
0.002, and the value-learning-rate α to 0.1 (unless otherwise specified).
The first set of experiments show the results of applying our algorithm on the three benchmark
games described in Table 1 over 10 simulation runs. Figure 12 plots π(r1) and π(c1) (i.e. the probability of choosing the first action for the row player and the column player respectively) against time.
For the matching pennies and the tricky games, the initial joint policy is ([0.1, 0.9]r , [0.9, 0.1]c ) (we
also tested 0.5,0.5 as initial joint policy with similar results) and the plot is the average across the
10 runs, with standard deviation shown as vertical bar. In the coordination game we plotted a single
run because there are two probable pure NEs and averaging over runs will not capture that. WPL
converges to one of the NE strategies in all the runs.
Figure 13 shows the convergence of GIGA, PHC-WoLF, and GIGA-WoLF algorithms for the
coordination game. As expected, GIGA, PHC-WoLF, and GIGA-WoLF converges to one of the NE
strategies faster, because WPL is slightly biased against pure strategies.
Figure 14 shows the convergence of previous GA-MARL algorithms for the matching pennies
game. GIGA hopelessly oscillates around the NE, as expected. PHC-WoLF is better, but with relatively high standard deviation across runs (when compared to WPL). GIGA-WoLF has comparable
performance to WPL, but GIGA-WoLF takes longer to converge (the width of oscillations around
the NE is higher than WPL).
The tricky game deserves more attention as it is one of the challenging two-player-two-action
games (Bowling & Veloso, 2002). Figure 15 shows the convergence of PHC-WoLF, GIGA, and
GIGA-WoLF. Only GIGA-WoLF converges but with slower rate than our approach (Figure 12(c)).
The performance of GIGA, PHC-WoLF, and GIGA-WoLF conforms to the results reported previously by their authors (Bowling & Veloso, 2002; Bowling, 2005). The remainder of our experiments (Section 5.3) focuses on GIGA-WoLF because it has performed the best among previous
GA-MARL algorithms.
538

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) WPL: coordination game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) WPL: matching pennies game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) WPL: tricky game

Figure 12: Convergence of WPL in benchmark two-player-two-action games. The figures plot the probability of playing the first action for each player (vertical axis) against time (horizontal axis).

5.3 Games Larger than 2x2
Figures 16 and 17 plot the policy of the row player over time for the games in Table 2 for both
WPL and GIGA-WoLF when the initial joint strategy is ([0.1, 0.8, 0.1]r , [0.8, 0.1, 0.1]c ) (we have
also tried ([ 13 , 13 , 31 ]r , [ 31 , 13 , 13 ]c ), which produced similar results), η = 0.001, and for two values of
α: 0.1 and 1. For the rock-paper-scissors game (Figure 16) both GIGA-WoLF and WPL converge
when α = 0.1, while only WPL converges when α = 1. In Shapley’s game (Figure 17) GIGAWoLF keeps oscillating for both α = 1 and α = 0.1 (GIGA-WoLF’s performance gets worse as α
539

A BDALLAH AND L ESSER

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) PHC-WoLF: coordination game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) GIGA: coordination game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) GIGA-WoLF: coordination game

Figure 13: Convergence of the previous GA-MARL algorithms (GIGA, PHC-WoLF, and GIGA-WoLF) in
the coordination game. The figures plot the probability of playing the first action for each player
(vertical axis) against time (horizontal axis).

increases). WPL on the other hand performs better as α increases and converges in Shapley’s game
when α = 1.
We believe the reason is that small α leads to an out-of-date reward estimate which in turn
leads agents to continuously chase the NE without successfully converging. Smaller α means more
samples contribute to the computed expected reward. Using more samples to estimate the reward
makes the estimate more accurate. However, the time required to get more samples may in fact
degrade the accuracy of reward estimate, because the expected reward changes over time.
540

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) PHC-WoLF: matching game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) GIGA: matching game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) GIGA-WoLF: matching game

Figure 14: Convergence of the GA-MARL algorithms (GIGA, PHC-WoLF, and GIGA-WoLF) in the matching pennies game. The figures plot the probability of playing the first action for each player
(vertical axis) against time (horizontal axis).

Setting the value of α always to 1 can result in sub-optimal performance, as illustrated by the
following experiment. Consider the biased game shown in Table 3. The NE policy of the biased
game is mixed with probabilities not uniform across actions, unlike previous benchmark games that
had a mixed NE with uniform probability across actions. When α was set to 0.01, WPL converges
to a policy close to the NE policy as shown in Figure 18. When α = 1, WPL converged to a policy
far from the NE policy as shown in Figure 19.
541

A BDALLAH AND L ESSER

1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(a) PHC-WoLF: tricky game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(b) GIGA: tricky game
1

player1
player2

0.8

0.6

policy
0.4

0.2

0
0

50000

100000

150000

200000

time
(c) GIGA-WoLF: tricky game

Figure 15: Convergence of the GA-MARL algorithms (GIGA, PHC-WoLF, and GIGA-WoLF) in the tricky
game. The figures plot the probability of playing the first action for each player (vertical axis)
against time (horizontal axis).

Table 3: Biased game: NE=(0.15,0.85) & (0.85,0.15)
a1
a2
a1 1.0,1.85 1.85,1.0
a2 1.15,1.0 1.00,1.15

542

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

0.6

policy

policy

0.4

0.4

0.2

0.2

0

a2

0
0

50000

100000

150000

200000

0

50000

time

100000

150000

200000

time
(b) WPL, α = 0.1

(a) GIGA-WoLF, α = 0.1
1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

a2

0.6

policy

policy

0.4

0.4

0.2

0.2

0

0
0

100000

200000

300000

400000

500000

600000

0

time

50000

100000

150000

200000

time

(c) GIGA-WolF, α = 1

(d) WPL, α = 1

Figure 16: Convergence of GIGA-WoLF and WPL in the rock-paper-scissors game. The figures plot the
probability of playing each action of the first player (vertical axis) against time (horizontal axis).

To understand the effect of having α = 1 in case of WPL, consider the following scenario in the
biased game. Suppose the policy of the column player is fixed at 0.7,0.3. If the value of α is small,
then the action value function approximates well the expected value of each action. The value of
the first row action (from the row player perspective) = 0.7×1 + 0.3×1.85= 1.255. Similarly, the
value of the second row action = 0.7 ×1.15 + 0.3×1 = 1.105. Unless the column player changes
its policy, the row player gradient clearly points toward increasing the probability of choosing the
first action (up to 1). Now consider the case when α = 1. In that case the action value reflects the
latest (sample) reward, not the average. In this case, the probability of choosing the first action, p,
increases on average by
∆=

0.7 × [0.7 × (1 − 1.15) × p + 0.35 × (1 − 1)] + 0.3×
[0.7 × (1.85 − 1.15) × (1 − p) + 0.3 × (1.85 − 1) × (1 − p)]

=

−0.297p + 0.2235

which means that the row player’s policy will effectively stabilize when ∆ = 0 or p = 0.75. In
other words, it is possible for players to stabilize at an equilibrium that is not an NE. Note that this
problem occurs mainly in biased games. In common benchmark games where the NE is uniform
across actions or pure, WPL will still converge to an NE even if α = 1, as shown in Figure 16 and
Figure 17.
Tuning α is not an easy task when the expected reward itself is dynamically changing (because
other agents are changing their policies concurrently). We are currently investigating an extension to
543

A BDALLAH AND L ESSER

1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

a2

0.6

policy

policy

0.4

0.4

0.2

0.2

0

0
0

50000

100000

150000

200000

0

50000

time
(a) GIGA-WolF, α = 0.1

100000

150000

200000

time
(b) WPL, α = 0.1

1

1

a0

a0

a1
0.8

a1
0.8

a2

0.6

0.6

policy

policy

0.4

0.4

0.2

0.2

0

a2

0
0

50000

100000

150000

200000

0

50000

100000

time

150000

200000

time

(c) GIGA-WolF, α = 1

(d) WPL, α = 1

Figure 17: Convergence of GIGA-WoLF and WPL in Shapley’s game. The figures plot the probability of
playing each action of the first player (vertical axis) against time (horizontal axis).

1

player1
player2

0.8
0.6

policy
0.4
0.2
0
0

200000

400000

600000

800000

1e+006

time

Figure 18: Convergence of WPL with fixed value-learning-rate of 0.01 for the biased game. Horizontal axis
is time. Vertical axis is the probability of the first action for each player.

WPL that automatically recognizes oscillations and adjusts the value of α accordingly. Preliminary
results are promising.
The following section illustrates the applicability of our algorithm by applying WPL in a more
realistic setting involving 100 agents.
544

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

1

player1
player2

0.8

policy

0.6
0.4
0.2
0
0

500000

1e+006

1.5e+006

2e+006

2.5e+006

3e+006

time

Figure 19: Convergence of WPL with fixed value-learning-rate of 1.0 for the biased game. Horizontal axis
is time. Vertical axis is the probability of the first action for each player.

5.4 Distributed Task Allocation Problem (DTAP)
We use a simplified version of the distributed task allocation domain (DTAP) (Abdallah & Lesser,
2007), where the goal of the multiagent system is to assign tasks to agents such that the service
time of each task is minimized. For illustration, consider the example scenario depicted in Figure
20. Agent A0 receives task T1, which can be executed by any of the agents A0, A1, A2, A3, and
A4. All agents other than agent A4 are overloaded, and therefore the best option for agent A0 is
to forward task T1 to agent A2 which in turn forwards the task to its left neighbor (A5) until task
T1 reaches agent A4. Although agent A0 does not know that A4 is under-loaded (because agent
A0 interacts only with its immediate neighbors), agent A0 will eventually learn (through experience
and interaction with its neighbors) that sending task T1 to agent A2 is the best action without even
knowing that agent A4 exists.

A1

A0
A5

A2
A3

T1

A4

Figure 20: Task allocation using a network of agents.
Q-learning is not appropriate in such setting due to communication delay (which results in
partial observability). For example, even if two neighbors have practically the same load, Q-learning
will assign all incoming requests to one of the neighbors until a feedback is received later indicating
change in the load. It should be noted that Q-learning was successfully used in the packet routing
domain (Boyan & Littman, 1994; Dutta, Jennings, & Moreau, 2005), where load balancing is not
545

A BDALLAH AND L ESSER

the main concern (the main objective is routing a packet from a particular source to a particular
destination).
Now let us define the different aspects of the DTAP domain more formally. Each time unit,
agents make decisions regarding all task requests received during this time unit. For each task, the
agent can either execute the task locally or send the task to a neighboring agent. If an agent decides
to execute the task locally, the agent adds the task to its local queue, where tasks are executed on a
first come first serve basis, with unlimited queue length.
Each agent has a physical location. Communication delay between two agents is proportional
to the Euclidean distance between them, one time unit per distance unit. Agents interact via two
types of messages. A REQUEST message hi, j, T i indicates a request sent from agent i to agent
j requesting task T . An UPDATE message hi, j, T, Ri indicates a feedback (reward signal) from
agent i to agent j that task T took R time steps to complete (the time steps are computed since agent
i received T ’s request).
P The main goal of DTAP is to reduce the total service time, averaged over tasks, AT ST =
T ∈T τ

T ST (T )

, where T τ is the set of task requests received during a time period τ and TST is the
total time a task spends in the system. TST consists of the time for routing a task request through
the network, the time a task request spends in the local queue, and the time of actually executing the
task.
It should be noted that although the underlying simulator have different underlying states, we
deliberately made agent oblivious to these states. The only feedback an agent gets (consistent with
our initial claim) is its own reward. The agents learn a joint policy that makes a good compromise
over the different unobserved states (because the agents can not distinguish between these states).
We have evaluated WPL’s performance using the following setting.9 100 agents are organized
in a 10x10 grid. Communication delay between two adjacent agents is two time units. Tasks arrive
at the 4x4 sub-grid at the center at rate 0.5 tasks/time unit. All agents can execute a task with a rate
of 0.1 task/time unit (both task arrival and service durations follow an exponential distribution).
Figure 21 shows the results of applying GIGA, GIGA-WoLF and WPL using value-learning-rate
of 1 and policy-learning-rate of 0.0001. GIGA fail to converge, while WPL and GIGA-WoLF do
converge. WPL converges faster and to a better ATST: GIGA-WoLF’s ATST converges to around
100 time units, while WPL’s ATST converges to around 70 time units.
We believe GIGA-WoLF’s slow convergence is due to the way GIGA-WoLF works. GIGAWoLF relies on learning a slowly moving policy in addition to the actual policy π in order to approximate the NE policy. This requires more time than the WPL algorithm. Furthermore, GIGA-WoLF’s
dynamics can be discontinuous prior to convergence and reach extreme deterministic policies even
if the NE policy is mixed. In a large system, this can have ripple effect and slow system-wide
convergence. WPL, on the other hand, has continuous dynamics, allowing faster collective convergence.
|T τ |

6. Conclusion and Future Work
This work presents WPL, a gradient ascent multiagent reinforcement learning algorithm (GAMARL) that assumes an agent neither knows the underlying game nor observes other agents. We
experimentally show that WPL converges in benchmark 2-player-2-action games. We also show
9. The simulator is available online at http://www.cs.umass.edu/˜shario/dtap.html.

546

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

3000

WPL
GIGA

2500

GIGA−WoLF

2000

ATST

1500
1000
500
0
0

20000

40000

60000

80000

100000

time

Figure 21: ATST of a 10x10 grid for different MARL algorithms. Horizontal axis is time while vertical axis
is ATST.

that WPL converges in Shapley’s game where none of the previous GA-MARL algorithms successfully converged. We verify the practicality of our algorithm in the distributed task allocation domain
with a network of 100 agents. WPL outperforms the state-of-the-art GA-MARL algorithms in both
the speed of convergence and the expected reward. We analyze the dynamics of WPL and show
that it has continuous non-linear dynamics, while previous GA-MARL algorithms had discontinuous dynamics. We show that our predicted theoretical behavior is consistent with our experimental
results.
In this work we briefly illustrated the importance of value-learning-rate and how it affects convergence, particularly for our proposed algorithm WPL. Finding the right balance and theoretically
analyzing how it affects convergence are interesting research questions. We are currently investigating an extension to WPL that automatically finds a good value-learning-rate. Preliminary experimental results are promising.
Another future direction we are considering is extending our theoretical analysis to games with
more actions and more players in order to verify our experimental findings in Shapley’s game and
the distributed task allocation domain. We are currently investigating alternative methodologies
for analyzing dynamics, including the evolutionary dynamics (Tuyls, ’t Hoen, & Vanschoenwinkel,
2006) and Lyapunov stability theory (Khalil, 2002).

7. Acknowledgments
This work is based on our previous conference publications (Abdallah & Lesser, 2006, 2008).

References
Abdallah, S., & Lesser, V. (2006). Learning the task allocation game. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, pp. 850–857.
Abdallah, S., & Lesser, V. (2007). Multiagent reinforcement learning and self-organization in a
network of agents. In Proceedings of the International Joint Conference on Autonomous
Agents and Multiagent Systems.
547

A BDALLAH AND L ESSER

Abdallah, S., & Lesser, V. (2008). Non-linear dynamics in multiagent reinforcement learning algorithms. In Proceedings of the International Joint Conference on Autonomous Agents and
Multiagent Systems, pp. 1321–1324.
Banerjee, B., & Peng, J. (2003). Adaptive policy gradient in multiagent learning. In Proceedings
of the International Joint Conference on Autonomous Agents and Multi Agent Systems, pp.
686–692.
Banerjee, B., & Peng, J. (2007). Generalized multiagent learning with performance bound. Autonomous Agents and Multiagent Systems, 15(3), 281–312.
Bowling, M. (2004). Convergence and no-regret in multiagent learning. Tech. rep., University of
Alberta.
Bowling, M. (2005). Convergence and no-regret in multiagent learning. In Proceedings of the
Annual Conference on Advances in Neural Information Processing Systems, pp. 209–216.
Bowling, M., & Veloso, M. (2002). Multiagent learning using a variable learning rate. Artificial
Intelligence, 136(2), 215–250.
Boyan, J. A., & Littman, M. L. (1994). Packet routing in dynamically changing networks: A
reinforcement learning approach. In Proceedings of the Annual Conference on Advances in
Neural Information Processing Systems, pp. 671–678.
Claus, C., & Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative multiagent
systems.. In Proceedings of the National Conference on Artificial intelligence/Innovative
Applications of Artificial Intelligence, pp. 746–752.
Conitzer, V., & Sandholm, T. (2007). AWESOME: A general multiagent learning algorithm that
converges in self-play and learns a best response against stationary opponents. Machine
Learning, 67(1-2), 23–43.
Dutta, P. S., Jennings, N. R., & Moreau, L. (2005). Cooperative information sharing to improve
distributed learning in multi-agent systems. Journal of Artificial Intelligence Research, 24,
407–463.
Hu, J., & Wellman, M. P. (2003). Nash Q-learning for general-sum stochastic games. Journal of
Machine Learning Research, 4, 1039–1069.
Khalil, H. K. (2002). Nonlinear Systems. Prentice-Hall, Upper Saddle River, NJ, USA.
Littman, M. (2001). Value-function reinforcement learning in Markov games. Cognitive Systems
Research, 2(12), 55–66.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning to cooperate via policy
search. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pp. 307–
314.
Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence of gradient dynamics in generalsum games. In Proceedings of the Conference on Uncertainty in Artificial Intelligence, pp.
541–548.
548

M ULTIAGENT L EARNING WITH N ON - LINEAR DYNAMICS

Sutton, R., & Barto, A. (1999). Reinforcement Learning: An Introduction. MIT Press.
Tuyls, K., ’t Hoen, P. J., & Vanschoenwinkel, B. (2006). An evolutionary dynamical analysis of
multi-agent learning in iterated games. Autonomous Agents and Multi-Agent Systems, 12(1),
115–153.
Wang, X., & Sandholm, T. (2003). Reinforcement learning to play an optimal Nash equilibrium
in team Markov games. In Proceedings of the Annual Conference on Advances in Neural
Information Processing Systems, pp. 1571–1578.
Zinkevich, M. (2003). Online convex programming and generalized infinitesimal gradient ascent..
In Proceedings of the International Conference on Machine Learning, pp. 928–936.

549

Journal of Artificial Intelligence Research 33 (2008) 615-655

Submitted 09/08; published 12/08

The Latent Relation Mapping Engine:
Algorithm and Experiments
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

Institute for Information Technology
National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Abstract
Many AI researchers and cognitive scientists have argued that analogy is the core
of cognition. The most influential work on computational modeling of analogy-making is
Structure Mapping Theory (SMT) and its implementation in the Structure Mapping Engine
(SME). A limitation of SME is the requirement for complex hand-coded representations.
We introduce the Latent Relation Mapping Engine (LRME), which combines ideas from
SME and Latent Relational Analysis (LRA) in order to remove the requirement for handcoded representations. LRME builds analogical mappings between lists of words, using a
large corpus of raw text to automatically discover the semantic relations among the words.
We evaluate LRME on a set of twenty analogical mapping problems, ten based on scientific
analogies and ten based on common metaphors. LRME achieves human-level performance
on the twenty problems. We compare LRME with a variety of alternative approaches and
find that they are not able to reach the same level of performance.

1. Introduction
When we are faced with a problem, we try to recall similar problems that we have faced
in the past, so that we can transfer our knowledge from past experience to the current
problem. We make an analogy between the past situation and the current situation, and we
use the analogy to transfer knowledge (Gentner, 1983; Minsky, 1986; Holyoak & Thagard,
1995; Hofstadter, 2001; Hawkins & Blakeslee, 2004).
In his survey of the computational modeling of analogy-making, French (2002) cites
Structure Mapping Theory (SMT) (Gentner, 1983) and its implementation in the Structure
Mapping Engine (SME) (Falkenhainer, Forbus, & Gentner, 1989) as the most influential
work on modeling of analogy-making. In SME, an analogical mapping M : A → B is from
a source A to a target B. The source is more familiar, more known, or more concrete,
whereas the target is relatively unfamiliar, unknown, or abstract. The analogical mapping
is used to transfer knowledge from the source to the target.
Gentner (1983) argues that there are two kinds of similarity, attributional similarity
and relational similarity. The distinction between attributes and relations may be understood in terms of predicate logic. An attribute is a predicate with one argument, such as
large(X), meaning X is large. A relation is a predicate with two or more arguments, such
as collides with(X, Y ), meaning X collides with Y .
The Structure Mapping Engine prefers mappings based on relational similarity over
mappings based on attributional similarity (Falkenhainer et al., 1989). For example, SME
is able to build a mapping from a representation of the solar system (the source) to a
c
°2008
National Research Council Canada. Reprinted with permission.

Turney

representation of the Rutherford-Bohr model of the atom (the target). The sun is mapped
to the nucleus, planets are mapped to electrons, and mass is mapped to charge. Note that
this mapping emphasizes relational similarity. The sun and the nucleus are very different
in terms of their attributes: the sun is very large and the nucleus is very small. Likewise,
planets and electrons have little attributional similarity. On the other hand, planets revolve
around the sun like electrons revolve around the nucleus. The mass of the sun attracts the
mass of the planets like the charge of the nucleus attracts the charge of the electrons.
Gentner (1991) provides evidence that children rely primarily on attributional similarity
for mapping, gradually switching over to relational similarity as they mature. She uses the
terms mere appearance to refer to mapping based mostly on attributional similarity, analogy
to refer to mapping based mostly on relational similarity, and literal similarity to refer to a
mixture of attributional and relational similarity. Since we use analogical mappings to solve
problems and make predictions, we should focus on structure, especially causal relations,
and look beyond the surface attributes of things (Gentner, 1983). The analogy between
the solar system and the Rutherford-Bohr model of the atom illustrates the importance of
going beyond mere appearance, to the underlying structures.
Figures 1 and 2 show the LISP representations used by SME as input for the analogy
between the solar system and the atom (Falkenhainer et al., 1989). Chalmers, French,
and Hofstadter (1992) criticize SME’s requirement for complex hand-coded representations.
They argue that most of the hard work is done by the human who creates these high-level
hand-coded representations, rather than by SME.
(defEntity sun :type inanimate)
(defEntity planet :type inanimate)
(defDescription solar-system
entities (sun planet)
expressions (((mass sun) :name mass-sun)
((mass planet) :name mass-planet)
((greater mass-sun mass-planet) :name >mass)
((attracts sun planet) :name attracts-form)
((revolve-around planet sun) :name revolve)
((and >mass attracts-form) :name and1)
((cause and1 revolve) :name cause-revolve)
((temperature sun) :name temp-sun)
((temperature planet) :name temp-planet)
((greater temp-sun temp-planet) :name >temp)
((gravity mass-sun mass-planet) :name force-gravity)
((cause force-gravity attracts-form) :name why-attracts)))

Figure 1: The representation of the solar system in SME (Falkenhainer et al., 1989).
Gentner, Forbus, and their colleagues have attempted to avoid hand-coding in their
recent work with SME.1 The CogSketch system can generate LISP representations from
simple sketches (Forbus, Usher, Lovett, Lockwood, & Wetzel, 2008). The Gizmo system
can generate LISP representations from qualitative physics models (Yan & Forbus, 2005).
The Learning Reader system can generate LISP representations from natural language text
(Forbus et al., 2007). These systems do not require LISP input.
1. Dedre Gentner, personal communication, October 29, 2008.

616

The Latent Relation Mapping Engine

(defEntity nucleus :type inanimate)
(defEntity electron :type inanimate)
(defDescription rutherford-atom
entities (nucleus electron)
expressions (((mass nucleus) :name mass-n)
((mass electron) :name mass-e)
((greater mass-n mass-e) :name >mass)
((attracts nucleus electron) :name attracts-form)
((revolve-around electron nucleus) :name revolve)
((charge electron) :name q-electron)
((charge nucleus) :name q-nucleus)
((opposite-sign q-nucleus q-electron) :name >charge)
((cause >charge attracts-form) :name why-attracts)))

Figure 2: The Rutherford-Bohr model of the atom in SME (Falkenhainer et al., 1989).

However, the CogSketch user interface requires the person who draws the sketch to identify the basic components in the sketch and hand-label them with terms from a knowledge
base derived from OpenCyc. Forbus et al. (2008) note that OpenCyc contains more than
58,000 hand-coded concepts, and they have added further hand-coded concepts to OpenCyc,
in order to support CogSketch. The Gizmo system requires the user to hand-code a physical
model, using the methods of qualitative physics (Yan & Forbus, 2005). Learning Reader
uses more than 28,000 phrasal patterns, which were derived from ResearchCyc (Forbus
et al., 2007). It is evident that SME still requires substantial hand-coded knowledge.
The work we present in this paper is an effort to avoid complex hand-coded representations. Our approach is to combine ideas from SME (Falkenhainer et al., 1989) and Latent
Relational Analysis (LRA) (Turney, 2006). We call the resulting algorithm the Latent Relation Mapping Engine (LRME). We represent the semantic relation between two terms
using a vector, in which the elements are derived from pattern frequencies in a large corpus
of raw text. Because the semantic relations are automatically derived from a corpus, LRME
does not require hand-coded representations of relations. It only needs a list of terms from
the source and a list of terms from the target. Given these two lists, LRME uses the corpus
to build representations of the relations among the terms, and then it constructs a mapping
between the two lists.
Tables 1 and 2 show the input and output of LRME for the analogy between the solar
system and the Rutherford-Bohr model of the atom. Although some human effort is involved
in constructing the input lists, it is considerably less effort than SME requires for its input
(contrast Figures 1 and 2 with Table 1).
Scientific analogies, such as the analogy between the solar system and the RutherfordBohr model of the atom, may seem esoteric, but we believe analogy-making is ubiquitous
in our daily lives. A potential practical application for this work is the task of identifying
semantic roles (Gildea & Jurafsky, 2002). Since roles are relations, not attributes, it is
appropriate to treat semantic role labeling as an analogical mapping problem.
For example, the Judgement semantic frame contains semantic roles such as judge,
evaluee, and reason, and the Statement frame contains roles such as speaker, addressee, message, topic, and medium (Gildea & Jurafsky, 2002). The task of identifying
617

Turney

Source A
planet
attracts
revolves
sun
gravity
solar system
mass

Target B
revolves
atom
attracts
electromagnetism
nucleus
charge
electron

Table 1: The representation of the input in LRME.
Source A
solar system
sun
planet
mass
attracts
revolves
gravity

Mapping M
→
→
→
→
→
→
→

Target B
atom
nucleus
electron
charge
attracts
revolves
electromagnetism

Table 2: The representation of the output in LRME.
semantic roles is to automatically label sentences with their roles, as in the following examples (Gildea & Jurafsky, 2002):
• [Judge She] blames [Evaluee the Government] [Reason for failing to do enough to
help].
• [Speaker We] talked [Topic about the proposal] [Medium over the phone].
If we have a training set of labeled sentences and a testing set of unlabeled sentences, then
we may view the task of labeling the testing sentences as a problem of creating analogical
mappings between the training sentences (sources) and the testing sentences (targets). Table 3 shows how “She blames the Government for failing to do enough to help.” might be
mapped to “They blame the company for polluting the environment.” Once a mapping has
been found, we can transfer knowledge, in the form of semantic role labels, from the source
to the target.
Source A
she
blames
government
failing
help

Mapping M
→
→
→
→
→

Target B
they
blame
company
polluting
environment

Table 3: Semantic role labeling as analogical mapping.
In Section 2, we briefly discuss the hypotheses behind the design of LRME. We then
precisely define the task that is performed by LRME, a specific form of analogical mapping,
618

The Latent Relation Mapping Engine

in Section 3. LRME builds on Latent Relational Analysis (LRA), hence we summarize LRA
in Section 4. We discuss potential applications of LRME in Section 5.
To evaluate LRME, we created twenty analogical mapping problems, ten science analogy problems (Holyoak & Thagard, 1995) and ten common metaphor problems (Lakoff &
Johnson, 1980). Table 1 is one of the science analogy problems. Our intended solution is
given in Table 2. To validate our intended solutions, we gave our colleagues the lists of
terms (as in Table 1) and asked them to generate mappings between the lists. Section 6
presents the results of this experiment. Across the twenty problems, the average agreement
with our intended solutions (as in Table 2) was 87.6%.
The LRME algorithm is outlined in Section 7, along with its evaluation on the twenty
mapping problems. LRME achieves an accuracy of 91.5%. The difference between this
performance and the human average of 87.6% is not statistically significant.
Section 8 examines a variety of alternative approaches to the analogy mapping task. The
best approach achieves an accuracy of 76.8%, but this approach requires hand-coded partof-speech tags. This performance is significantly below LRME and human performance.
In Section 9, we discuss some questions that are raised by the results in the preceding
sections. Related work is described in Section 10, future work and limitations are considered
in Section 11, and we conclude in Section 12.

2. Guiding Hypotheses
In this section, we list some of the assumptions that have guided the design of LRME. The
results we present in this paper do not necessarily require these assumptions, but it might
be helpful to the reader, to understand the reasoning behind our approach.
1. Analogies and semantic relations: Analogies are based on semantic relations
(Gentner, 1983). For example, the analogy between the solar system and the Rutherford-Bohr model of the atom is based on the similarity of the semantic relations
among the concepts involved in our understanding of the solar system to the semantic
relations among the concepts involved in the Rutherford-Bohr model of the atom.
2. Co-occurrences and semantic relations: Two terms have an interesting, significant semantic relation if and only if they they tend to co-occur within a relatively
small window (e.g., five words) in a relatively large corpus (e.g., 1010 words). Having
an interesting semantic relation causes co-occurrence and co-occurrence is a reliable
indicator of an interesting semantic relation (Firth, 1957).
3. Meanings and semantic relations: Meaning has more to do with relations among
words than individual words. Individual words tend to be ambiguous and polysemous.
By putting two words into a pair, we constrain their possible meanings. By putting
words into a sentence, with multiple relations among the words in the sentence, we
constrain the possible meanings further. If we focus on word pairs (or tuples), instead
of individual words, word sense disambiguation is less problematic. Perhaps a word
has no sense apart from its relations with other words (Kilgarriff, 1997).
4. Pattern distributions and semantic relations: There is a many-to-many mapping between semantic relations and the patterns in which two terms co-occur. For
example, the relation CauseEffect(X, Y ) may be expressed as “X causes Y ”, “Y
619

Turney

from X”, “Y due to X”, “Y because of X”, and so on. Likewise, the pattern
“Y from X” may be an expression of CauseEffect(X, Y ) (“sick from bacteria”) or
OriginEntity(X, Y ) (“oranges from Spain”). However, for a given X and Y , the statistical distribution of patterns in which X and Y co-occur is a reliable signature of
the semantic relations between X and Y (Turney, 2006).
To the extent that LRME works, we believe its success lends some support to these hypotheses.

3. The Task
In this paper, we examine algorithms that generate analogical mappings. For simplicity, we
restrict the task to generating bijective mappings; that is, mappings that are both injective
(one-to-one; there is no instance in which two terms in the source map to the same term
in the target) and surjective (onto; the source terms cover all of the target terms; there is
no target term that is left out of the mapping). We assume that the entities that are to be
mapped are given as input. Formally, the input I for the algorithms is two sets of terms, A
and B.
I = {hA, Bi}

(1)

Since the mappings are bijective, A and B must contain the same number of terms, m.
A = {a1 , a2 , . . . , am }

(2)

B = {b1 , b2 , . . . , bm }

(3)

A term, ai or bj , may consist of a single word (planet) or a compound of two or more words
(solar system). The words may be any part of speech (nouns, verbs, adjectives, or adverbs).
The output O is a bijective mapping M from A to B.
O = {M : A → B}

(4)

M (ai ) ∈ B

(5)

M (A) = {M (a1 ), M (a2 ), . . . , M (am )} = B

(6)

The algorithms that we consider here can accept a batch of multiple independent mapping
problems as input and generate a mapping for each one as output.
I = {hA1 , B1 i , hA2 , B2 i , . . . , hAn , Bn i}

(7)

O = {M1 : A1 → B1 , M2 : A2 → B2 , . . . , Mn : An → Bn }

(8)

Suppose the terms in A are in some arbitrary order a.
a = ha1 , a2 , . . . , am i
The mapping function M : A → B, given a, determines a unique ordering b of B.
620

(9)

The Latent Relation Mapping Engine

b = hM (a1 ), M (a2 ), . . . , M (am )i

(10)

Likewise, an ordering b of B, given a, defines a unique mapping function M . Since there
are m! possible orderings of B, there are also m! possible mappings from A to B. The task
is to search through the m! mappings and find the best one. (Section 6 shows that there is
a relatively high degree of consensus about which mappings are best.)
Let P (A, B) be the set of all m! bijective mappings from A to B. (P stands for permutation, since each mapping corresponds to a permutation.)
P (A, B) = {M1 , M2 , . . . , Mm! }

(11)

m = |A| = |B|

(12)

m! = |P (A, B)|

(13)

In the following experiments, m is 7 on average and 9 at most, so m! is usually around
7! = 5, 040 and at most 9! = 362, 880. It is feasible for us to exhaustively search P (A, B).
We explore two basic kinds of algorithms for generating analogical mappings, algorithms
based on attributional similarity and algorithms based on relational similarity (Turney,
2006). The attributional similarity between two words, sima (a, b) ∈ <, depends on the
degree of correspondence between the properties of a and b. The more correspondence
there is, the greater their attributional similarity. The relational similarity between two
pairs of words, simr (a : b, c : d) ∈ <, depends on the degree of correspondence between the
relations of a : b and c : d. The more correspondence there is, the greater their relational
similarity. For example, dog and wolf have a relatively high degree of attributional similarity,
whereas dog : bark and cat : meow have a relatively high degree of relational similarity.
Attributional mapping algorithms seek the mapping (or mappings) Ma that maximizes
the sum of the attributional similarities between the terms in A and the corresponding
terms in B. (When there are multiple mappings that maximize the sum, we break the tie
by randomly choosing one of them.)
Ma = arg max

m
X

sima (ai , M (ai ))

(14)

M ∈P (A,B) i=1

Relational mapping algorithms seek the mapping (or mappings) Mr that maximizes the
sum of the relational similarities.
Mr = arg max

m X
m
X

simr (ai : aj , M (ai ) : M (aj ))

(15)

M ∈P (A,B) i=1 j=i+1

In (15), we assume that simr is symmetrical. For example, the degree of relational similarity
between dog : bark and cat : meow is the same as the degree of relational similarity between
bark : dog and meow : cat.
simr (a : b, c : d) = simr (b : a, d : c)

(16)

We also assume that simr (a : a, b : b) is not interesting; for example, it may be some constant
value for all a and b. Therefore (15) is designed so that i is always less than j.
621

Turney

Let scorer (M ) and scorea (M ) be defined as follows.

scorer (M ) =
scorea (M ) =

m X
m
X

simr (ai : aj , M (ai ) : M (aj ))

i=1 j=i+1
m
X

sima (ai , M (ai ))

(17)

(18)

i=1

Now Mr and Ma may be defined in terms of scorer (M ) and scorea (M ).
Mr = arg max scorer (M )

(19)

M ∈P (A,B)

Ma = arg max scorea (M )

(20)

M ∈P (A,B)

Mr is the best mapping according to simr and Ma is the best mapping according to sima .
Recall Gentner’s (1991) terms, discussed in Section 1, mere appearance (mostly attributional similarity), analogy (mostly relational similarity), and literal similarity (a mixture of
attributional and relational similarity). We take it that Mr is an abstract model of mapping based on analogy and Ma is a model of mere appearance. For literal similarity, we can
combine Mr and Ma , but we should take care to normalize scorer (M ) and scorea (M ) before
we combine them. (We experiment with combining them in Section 9.2.)

4. Latent Relational Analysis
LRME uses a simplified form of Latent Relational Analysis (LRA) (Turney, 2005, 2006)
to calculate the relational similarity between pairs of words. We will briefly describe past
work with LRA before we present LRME.
LRA takes as input I a set of word pairs and generates as output O the relational
similarity simr (ai : bi , aj : bj ) between any two pairs in the input.
I = {a1 : b1 , a2 : b2 , . . . , an : bn }

(21)

O = {simr : I × I → <}

(22)

LRA was designed to evaluate proportional analogies. Proportional analogies have the form
a : b :: c : d, which means “a is to b as c is to d”. For example, mason : stone :: carpenter : wood
means “mason is to stone as carpenter is to wood”. A mason is an artisan who works with
stone and a carpenter is an artisan who works with wood.
We consider proportional analogies to be a special case of bijective analogical mapping,
as defined in Section 3, in which |A| = |B| = m = 2. For example, a1 : a2 :: b1 : b2 is equivalent
to M0 in (23).
A = {a1 , a2 } , B = {b1 , b2 } , M0 (a1 ) = b1 , M0 (a2 ) = b2 .
From the definition of scorer (M ) in (17), we have the following result for M0 .
622

(23)

The Latent Relation Mapping Engine

scorer (M0 ) = simr (a1 : a2 , M0 (a1 ) : M0 (a2 )) = simr (a1 : a2 , b1 : b2 )

(24)

That is, the quality of the proportional analogy mason : stone :: carpenter : wood is given by
simr (mason : stone, carpenter : wood).
Proportional analogies may also be evaluated using attributional similarity. From the
definition of scorea (M ) in (18), we have the following result for M0 .
scorea (M0 ) = sima (a1 , M0 (a1 )) + sima (a2 , M0 (a2 )) = sima (a1 , b1 ) + sima (a2 , b2 )

(25)

For attributional similarity, the quality of the proportional analogy mason : stone :: carpenter :
wood is given by sima (mason, carpenter) + sima (stone, wood).
LRA only handles proportional analogies. The main contribution of LRME is to extend
LRA beyond proportional analogies to bijective analogies for which m > 2.
Turney (2006) describes ten potential applications of LRA: recognizing proportional
analogies, structure mapping theory, modeling metaphor, classifying semantic relations,
word sense disambiguation, information extraction, question answering, automatic thesaurus generation, information retrieval, and identifying semantic roles. Two of these
applications (evaluating proportional analogies and classifying semantic relations) are experimentally evaluated, with state-of-the-art results.
Turney (2006) compares the performance of relational similarity (24) and attributional
similarity (25) on the task of solving 374 multiple-choice proportional analogy questions from
the SAT college entrance test. LRA is used to measure relational similarity and a variety
of lexicon-based and corpus-based algorithms are used to measure attributional similarity.
LRA achieves an accuracy of 56% on the 374 SAT questions, which is not significantly
different from the average human score of 57%. On the other hand, the best performance
by attributional similarity is 35%. The results show that attributional similarity is better
than random guessing, but not as good as relational similarity. This result is consistent
with Gentner’s (1991) theory of the maturation of human similarity judgments.
Turney (2006) also applies LRA to the task of classifying semantic relations in nounmodifier expressions. A noun-modifier expression is a phrase, such as laser printer, in which
the head noun (printer) is preceded by a modifier (laser). The task is to identify the semantic
relation between the noun and the modifier. In this case, the relation is instrument; the
laser is an instrument used by the printer. On a set of 600 hand-labeled noun-modifier pairs
with five different classes of semantic relations, LRA attains 58% accuracy.
Turney (2008) employs a variation of LRA for solving four different language tests,
achieving 52% accuracy on SAT analogy questions, 76% accuracy on TOEFL synonym
questions, 75% accuracy on the task of distinguishing synonyms from antonyms, and 77%
accuracy on the task of distinguishing words that are similar, words that are associated,
and words that are both similar and associated. The same core algorithm is used for all
four tests, with no tuning of the parameters to the particular test.

5. Applications for LRME
Since LRME is an extension of LRA, every potential application of LRA is also a potential
application of LRME. The advantage of LRME over LRA is the ability to handle bijective
623

Turney

analogies when m > 2 (where m = |A| = |B|). In this section, we consider the kinds of
applications that might benefit from this ability.
In Section 7.2, we evaluate LRME on science analogies and common metaphors, which
supports the claim that these two applications benefit from the ability to handle larger sets
of terms. In Section 1, we saw that identifying semantic roles (Gildea & Jurafsky, 2002)
also involves more than two terms, and we believe that LRME will be superior to LRA for
semantic role labeling.
Semantic relation classification usually assumes that the relations are binary; that is,
a semantic relation is a connection between two terms (Rosario & Hearst, 2001; Nastase
& Szpakowicz, 2003; Turney, 2006; Girju et al., 2007). Yuret observed that binary relations may be linked by underlying n-ary relations.2 For example, Nastase and Szpakowicz
(2003) defined a taxonomy of 30 binary semantic relations. Table 4 shows how six binary relations from Nastase and Szpakowicz (2003) can be covered by one 5-ary relation,
Agent:Tool:Action:Affected:Theme. An Agent uses a Tool to perform an Action. Somebody
or something is Affected by the Action. The whole event can be summarized by its Theme.
Nastase and Szpakowicz (2003)
Relation
Example
agent
student protest
purpose
concert hall
beneficiary
student discount
instrument
laser printer
object
metal separator
object property sunken ship

Agent:Tool:Action:Affected:Theme
Agent:Action
Theme:Tool
Affected:Action
Tool:Agent
Affected:Tool
Action:Affected

Table 4: How six binary semantic relations from Nastase and Szpakowicz (2003) can be
viewed as different fragments of one 5-ary semantic relation.

In SemEval Task 4, we found it easier to manually tag the datasets when we expanded
binary relations to their underlying n-ary relations (Girju et al., 2007). We believe that this
expansion would also facilitate automatic classification of semantic relations. The results
in Section 9.3 suggest that all of the applications for LRA that we discussed in Section 4
might benefit from being able to handle bijective analogies when m > 2.

6. The Mapping Problems
To evaluate our algorithms for analogical mapping, we created twenty mapping problems,
given in Appendix A. The twenty problems consist of ten science analogy problems, based
on examples of analogy in science from Chapter 8 of Holyoak and Thagard (1995), and ten
common metaphor problems, derived from Lakoff and Johnson (1980).
The tables in Appendix A show our intended mappings for each of the twenty problems. To validate these mappings, we invited our colleagues in the Institute for Information
Technology to participate in an experiment. The experiment was hosted on a web server
2. Deniz Yuret, personal communication, February 13, 2007. This observation was in the context of our
work on building the datasets for SemEval 2007 Task 4 (Girju et al., 2007).

624

The Latent Relation Mapping Engine

(only accessible inside our institute) and people participated anonymously, using their web
browsers in their offices. There were 39 volunteers who began the experiment and 22 who
went all the way to the end. In our analysis, we use only the data from the 22 participants
who completed all of the mapping problems.
The instructions for the participants are in Appendix A. The sequence of the problems
and the order of the terms within a problem were randomized separately for each participant,
to remove any effects due to order. Table 5 shows the agreement between our intended
mapping and the mappings generated by the participants. Across the twenty problems,
the average agreement was 87.6%, which is higher than the agreement figures for many
linguistic annotation tasks. This agreement is impressive, given that the participants had
minimal instructions and no training.
Type

science
analogies

common
metaphors

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10

Source → Target
solar system → atom
water flow → heat transfer
waves → sounds
combustion → respiration
sound → light
projectile → planet
artificial selection → natural selection
billiard balls → gas molecules
computer → mind
slot machine → bacterial mutation
war → argument
buying an item → accepting a belief
grounds for a building → reasons for a theory
impediments to travel → difficulties
money → time
seeds → ideas
machine → mind
object → idea
following → understanding
seeing → understanding

Average

Agreement
90.9
86.9
81.8
79.0
79.2
97.4
74.7
88.1
84.3
83.6
93.5
96.1
87.9
100.0
77.3
89.0
98.7
89.1
96.6
78.8
87.6

m
7
8
8
8
7
7
7
8
9
5
7
7
6
7
6
7
7
5
8
6
7.0

Table 5: The average agreement between our intended mappings and the mappings of the
22 participants. See Appendix A for the details.

The column labeled m gives the number of terms in the set of source terms for each
mapping problem (which is equal to the number of terms in the set of target terms). For the
average problem, m = 7. The third column in Table 5 gives a mnemonic that summarizes
the mapping (e.g., solar system → atom). Note that the mnemonic is not used as input for
any of the algorithms, nor was the mnemonic shown to the participants in the experiment.
The agreement figures in Table 5 for each individual mapping problem are averages over
the m mappings for each problem. Appendix A gives a more detailed view, showing the
agreement for each individual mapping in the m mappings. The twenty problems contain
a total of 140 individual mappings (20 × 7). Appendix A shows that every one of these 140
625

Turney

mappings has an agreement of 50% or higher. That is, in every case, the majority of the
participants agreed with our intended mapping. (There are two cases where the agreement
is exactly 50%. See problems A5 in Table 14 and M5 in Table 16 in Appendix A.)
If we select the mapping that is chosen by the majority of the 22 participants, then we
will get a perfect score on all twenty problems. More precisely, if we try all m! mappings for
each problem, and select the mapping that maximizes the sum of the number of participants
who agree with each individual mapping in the m mappings, then we will have a score of
100% on all twenty problems. This is strong support for the intended mappings that are
given in Appendix A.
In Section 3, we applied Genter’s (1991) categories – mere appearance (mostly attributional similarity), analogy (mostly relational similarity), and literal similarity (a mixture
of attributional and relational similarity) – to the mappings Mr and Ma , where Mr is the
best mapping according to simr and Ma is the best mapping according to sima . The twenty
mapping problems were chosen as analogy problems; that is, the intended mappings in
Appendix A are meant to be relational mappings, Mr ; mappings that maximize relational
similarity, simr . We have tried to avoid mere appearance and literal similarity.
In Section 7 we use the twenty mapping problems to evaluate a relational mapping
algorithm (LRME), and in Section 8 we use them to evaluate several different attributional
mapping algorithms. Our hypothesis is that LRME will perform significantly better than
any of the attributional mapping algorithms on the twenty mapping problems, because they
are analogy problems (not mere appearance problems and not literal similarity problems).
We expect relational and attributional mapping algorithms would perform approximately
equally well on literal similarity problems, and we expect that mere appearance problems
would favour attributional algorithms over relational algorithms, but we do not test these
latter two hypotheses, because our primary interest in this paper is analogy-making.
Our goal is to test the hypothesis that there is a real, practical, effective, measurable
difference between the output of LRME and the output of the various attributional mapping algorithms. A skeptic might claim that relational similarity simr (a : b, c : d) can be
reduced to attributional similarity sima (a, c) + sima (b, d); therefore our relational mapping
algorithm is a complicated solution to an illusory problem. A slightly less skeptical claim
is that relational similarity versus attributional similarity is a valid distinction in cognitive
psychology, but our relational mapping algorithm does not capture this distinction. To test
our hypothesis and refute these skeptical claims, we have created twenty analogical mapping
problems, and we will show that LRME handles these problems significantly better than
the various attributional mapping algorithms.

7. The Latent Relation Mapping Engine
The Latent Relation Mapping Engine (LRME) seeks the mapping Mr that maximizes the
sum of the relational similarities.
Mr = arg max

m X
m
X

simr (ai : aj , M (ai ) : M (aj ))

(26)

M ∈P (A,B) i=1 j=i+1

We search for Mr by exhaustively evaluating all of the possibilities. Ties are broken randomly. We use a simplified form of LRA (Turney, 2006) to calculate simr .
626

The Latent Relation Mapping Engine

7.1 Algorithm
Briefly, the idea of LRME is to build a pair-pattern matrix X, in which the rows correspond
to pairs of terms and the columns correspond to patterns. For example, the row xi: might
correspond to the pair of terms sun : solar system and the column x:j might correspond to
the pattern “∗ X centered Y ∗”. In these patterns, “∗” is a wild card, which can match
any single word. The value of an element xij in X is based on the frequency of the pattern
for x:j , when X and Y are instantiated by the terms in the pair for xi: . For example, if we
take the pattern “∗ X centered Y ∗” and instantiate X : Y with the pair sun : solar system,
then we have the pattern “∗ sun centered solar system ∗”, and thus the value of the element
xij is based on the frequency of “∗ sun centered solar system ∗” in the corpus. The matrix
X is smoothed with a truncated singular value decomposition (SVD) (Golub & Van Loan,
1996) and the relational similarity simr between two pairs of terms is given by the cosine of
the angle between the two corresponding row vectors in X.
In more detail, LRME takes as input I a set of mapping problems and generates as
output O a corresponding set of mappings.

I = {hA1 , B1 i , hA2 , B2 i , . . . , hAn , Bn i}

(27)

O = {M1 : A1 → B1 , M2 : A2 → B2 , . . . , Mn : An → Bn }

(28)

In the following experiments, all twenty mapping problems (Appendix A) are processed in
one batch (n = 20).
The first step is to make a list R that contains all pairs of terms in the input I. For
each mapping problem hA, Bi in I, we add to R all pairs ai : aj , such that ai and aj are
members of A, i 6= j, and all pairs bi : bj , such that bi and bj are members of B, i 6= j.
If |A| = |B| = m, then there are m(m − 1) pairs from A and m(m − 1) pairs from B.3 A
typical pair in R would be sun : solar system. We do not allow duplicates in R; R is a list
of pair types, not pair tokens. For our twenty mapping problems, R is a list of 1,694 pairs.
For each pair r in R, we make a list S(r) of the phrases in the corpus that contain the
pair r. Let ai : aj be the terms in the pair r. We search in the corpus for all phrases of the
following form:
“[0 to 1 words] ai [0 to 3 words] aj [0 to 1 words]”

(29)

If ai : aj is in R, then aj : ai is also in R, so we find phrases with the members of the pairs
in both orders, S(ai : aj ) and S(aj : ai ). The search template (29) is the same as used by
Turney (2008).
In the following experiments, we search in a corpus of 5 × 1010 English words (about 280
GB of plain text), consisting of web pages gathered by a web crawler.4 To retrieve phrases
3. We have m(m − 1) here, not m(m − 1)/2, because we need the pairs in both orders. We only want
to calculate simr for one order of the pairs, because i is always less than j in (26); however, to ensure
that simr is symmetrical, as in (16), we need to make the matrix X symmetrical, by having rows in the
matrix for both orders of every pair.
4. The corpus was collected by Charles Clarke at the University of Waterloo. We can provide copies of the
corpus on request.

627

Turney

from the corpus, we use Wumpus (Büttcher & Clarke, 2005), an efficient search engine for
passage retrieval from large corpora.5
With the 1,694 pairs in R, we find a total of 1,996,464 phrases in the corpus, an average
of about 1,180 phrases per pair. For the pair r = sun : solar system, a typical phrase s in
S(r) would be “a sun centered solar system illustrates”.
Next we make a list C of patterns, based on the phrases we have found. For each pair
r in R, where r = ai : aj , if we found a phrase s in S(r), then we replace ai in s with X
and we replace aj with Y . The remaining words may be either left as they are or replaced
with a wild card symbol “∗”. We then replace ai in s with Y and aj with X, and replace
the remaining words with wild cards or leave them as they are. If there are n remaining
words in s, after ai and aj are replaced, then we generate 2n+1 patterns from s, and we add
these patterns to C. We only add new patterns to C; that is, C is a list of pattern types,
not pattern tokens; there are no duplicates in C.
For example, for the pair sun : solar system, we found the phrase “a sun centered solar
system illustrates”. When we replace ai : aj with X : Y , we have “a X centered Y
illustrates”. There are three remaining words, so we can generate eight patterns, such as
“a X ∗ Y illustrates”, “a X centered Y ∗”, “∗ X ∗ Y illustrates”, and so on. Each of these
patterns is added to C. Then we replace ai : aj with Y : X, yielding “a Y centered X
illustrates”. This gives us another eight patterns, such as “a Y centered X ∗”. Thus the
phrase “a sun centered solar system illustrates” generates a total of sixteen patterns, which
we add to C.
Now we revise R, to make a list of pairs that will correspond to rows in the frequency
matrix F. We remove any pairs from R for which no phrases were found in the corpus,
when the terms were in either order. Let ai : aj be the terms in the pair r. We remove
r from R if both S(ai : aj ) and S(aj : ai ) are empty. We remove such rows because they
would correspond to zero vectors in the matrix F. This reduces R from 1,694 pairs to 1,662
pairs. Let nr be the number of pairs in R.
Next we revise C, to make a list of patterns that will correspond to columns in the
frequency matrix F. In the following experiments, at this stage, C contains millions of
patterns, too many for efficient processing with a standard desktop computer. We need to
reduce C to a more manageable size. We select the patterns that are shared by the most
pairs. Let c be a pattern in C. Let r be a pair in R. If there is a phrase s in S(r), such
that there is a pattern generated from s that is identical to c, then we say that r is one of
the pairs that generated c. We sort the patterns in C in descending order of the number
of pairs in R that generated each pattern, and we select the top tnr patterns from this
sorted list. Following Turney (2008), we set the parameter t to 20; hence C is reduced to
the top 33,240 patterns (tnr = 20 × 1,662 = 33,240). Let nc be the number of patterns in
C (nc = tnr ).
Now that the rows R and columns C are defined, we can build the frequency matrix
F. Let ri be the i-th pair of terms in R (e.g., let ri be sun : solar system) and let cj be
the j-th pattern in C (e.g., let cj be “∗ X centered Y ∗”). We instantiate X and Y in the
pattern cj with the terms in ri (“∗ sun centered solar system ∗”). The element fij in F is
the frequency of this instantiated pattern in the corpus.
5. Wumpus was developed by Stefan Büttcher and it is available at http://www.wumpus-search.org/.

628

The Latent Relation Mapping Engine

Note that we do not need to search again in the corpus for the instantiated pattern for
fij , in order to find its frequency. In the process of creating each pattern, we can keep track
of how many phrases generated the pattern, for each pair. We can get the frequency for fij
by checking our record of the patterns that were generated by ri .
The next step is to transform the matrix F of raw frequencies into a form X that
enhances the similarity measurement. Turney (2006) used the log entropy transformation,
as suggested by Landauer and Dumais (1997). This is a kind of tf-idf (term frequency
times inverse document frequency) transformation, which gives more weight to elements in
the matrix that are statistically surprising. However, Bullinaria and Levy (2007) recently
achieved good results with a new transformation, called PPMIC (Positive Pointwise Mutual
Information with Cosine); therefore LRME uses PPMIC. The raw frequencies in F are used
to calculate probabilities, from which we can calculate the pointwise mutual information
(PMI) of each element in the matrix. Any element with a negative PMI is then set to zero.

fij
pij = Pnr Pnc

j=1 fij

i=1

(30)

Pnc

j=1 fij
pi∗ = Pnr Pnc

(31)

Pnr
f
Pncij
= Pnr i=1

(32)

i=1

p∗j

i=1

µ

j=1 fij
j=1 fij

pij
pi∗ p∗j

¶

pmiij = log
½
pmiij if pmiij > 0
xij =
0 otherwise

(33)
(34)

Let ri be the i-th pair of terms in R (e.g., let ri be sun : solar system) and let cj be the
j-th pattern in C (e.g., let cj be “∗ X centered Y ∗”). In (33), pij is the estimated probability
of the of the pattern cj instantiated with the pair ri (“∗ sun centered solar system ∗”), pi∗
is the estimated probability of ri , and p∗j is the estimated probability of cj . If ri and cj are
statistically independent, then pi∗ p∗j = pij (by the definition of independence), and thus
pmiij is zero (since log(1) = 0). If there is an interesting semantic relation between the
terms in ri , and the pattern cj captures an aspect of that semantic relation, then we should
expect pij to be larger than it would be if ri and cj were indepedent; hence we should find
that pij > pi∗ p∗j , and thus pmiij is positive. (See Hypothesis 2 in Section 2.) On the other
hand, terms from completely different domains may avoid each other, in which case we
should find that pmiij is negative. PPMIC is designed to give a high value to xij when the
pattern cj captures an aspect of the semantic relation between the terms in ri ; otherwise,
xij should have a value of zero, indicating that the pattern cj tells us nothing about the
semantic relation between the terms in ri .
In our experiments, F has a density of 4.6% (the percentage of nonzero elements) and
X has a density of 3.8%. The lower density of X is due to elements with a negative PMI,
which are transformed to zero by PPMIC.
629

Turney

Now we smooth X by applying a truncated singular value decomposition (SVD) (Golub
& Van Loan, 1996). We use SVDLIBC to calculate the SVD of X.6 SVDLIBC is designed
for sparse (low density) matrices. SVD decomposes X into the product of three matrices
UΣVT , where U and V are in column orthonormal form (i.e., the columns are orthogonal
and have unit length, UT U = VT V = I) and Σ is a diagonal matrix of singular values
(Golub & Van Loan, 1996). If X is of rank r, then Σ is also of rank r. Let Σk , where
k < r, be the diagonal matrix formed from the top k singular values, and let Uk and Vk be
the matrices produced by selecting the corresponding columns from U and V. The matrix
Uk Σk VkT is the matrix of rank k that best approximates the original matrix X, in the sense
that it minimizes the approximation errors. That is, X̂ = Uk Σk VkT minimizes kX̂ − XkF
over all matrices X̂ of rank k, where k . . . kF denotes the Frobenius norm (Golub & Van
Loan, 1996). We may think of this matrix Uk Σk VkT as a smoothed or compressed version
of the original matrix X. Following Turney (2006), we set the parameter k to 300.
The relational similarity simr between two pairs in R is the inner product of the two
corresponding rows in Uk Σk VkT , after the rows have been normalized to unit length. We can
simplify calculations by dropping Vk (Deerwester, Dumais, Landauer, Furnas, & Harshman,
1990). We take the matrix Uk Σk and normalize each row to unit length. Let W be the
resulting matrix. Now let Z be WWT , a square matrix of size nr ×nr . This matrix contains
the cosines of all combinations of two pairs in R.
For a mapping problem hA, Bi in I, let a : a0 be a pair of terms from A and let b : b0 be
a pair of terms from B. Suppose that ri = a : a0 and rj = b : b0 , where ri and rj are the
i-th and j-th pairs in R. Then simr (a : a0 , b : b0 ) = zij , where zij is the element in the i-th
row and j-th column of Z. If either a : a0 or b : b0 is not in R, because S(a : a0 ), S(a0 : a),
S(b : b0 ), or S(b0 : b) is empty, then we set the similarity to zero. Finally, for each mapping
problem in I, we output the map Mr that maximizes the sum of the relational similarities.

Mr = arg max

m X
m
X

simr (ai : aj , M (ai ) : M (aj ))

(35)

M ∈P (A,B) i=1 j=i+1

The simplified form of LRA used here to calculate simr differs from LRA used by Turney
(2006) in several ways. In LRME, there is no use of synonyms to generate alternate forms of
the pairs of terms. In LRME, there is no morphological processing of the terms. LRME uses
PPMIC (Bullinaria & Levy, 2007) to process the raw frequencies, instead of log entropy.
Following Turney (2008), LRME uses a slightly different search template (29) and LRME
sets the number of columns nc to tnr , instead of using a constant. In Section 7.2, we
evaluate the impact of two of these changes (PPMIC and nc ), but we have not tested
the other changes, which were mainly motivated by a desire for increased efficiency and
simplicity.
7.2 Experiments
We implemented LRME in Perl, making external calls to Wumpus for searching the corpus
and to SVDLIBC for calculating SVD. We used the Perl Net::Telnet package for interprocess
6. SVDLIBC is the work of Doug Rohde and it is available at http://tedlab.mit.edu/∼dr/svdlibc/.

630

The Latent Relation Mapping Engine

communication with Wumpus, the PDL (Perl Data Language) package for matrix manipulations (e.g., calculating cosines), and the List::Permutor package to generate permutations
(i.e., to loop through P (A, B)).
We ran the following experiments on a dual core AMD Opteron 64 computer, running
64 bit Linux. Most of the running time is spent searching the corpus for phrases. It took
16 hours and 27 minutes for Wumpus to fetch the 1,996,464 phrases. The remaining steps
took 52 minutes, of which SVD took 10 minutes. The running time could be cut in half by
using RAID 0 to speed up disk access.
Table 6 shows the performance of LRME in its baseline configuration. For comparison,
the agreement of the 22 volunteers with our intended mapping has been copied from Table 5.
The difference between the performance of LRME (91.5%) and the human participants
(87.6%) is not statistically significant (paired t-test, 95% confidence level).

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
Average

Source → Target
solar system → atom
water flow → heat transfer
waves → sounds
combustion → respiration
sound → light
projectile → planet
artificial selection → natural selection
billiard balls → gas molecules
computer → mind
slot machine → bacterial mutation
war → argument
buying an item → accepting a belief
grounds for a building → reasons for a theory
impediments to travel → difficulties
money → time
seeds → ideas
machine → mind
object → idea
following → understanding
seeing → understanding

Accuracy
LRME Humans
100.0
90.9
100.0
86.9
100.0
81.8
100.0
79.0
71.4
79.2
100.0
97.4
71.4
74.7
100.0
88.1
55.6
84.3
100.0
83.6
71.4
93.5
100.0
96.1
100.0
87.9
100.0
100.0
100.0
77.3
100.0
89.0
100.0
98.7
60.0
89.1
100.0
96.6
100.0
78.8
91.5
87.6

Table 6: LRME in its baseline configuration, compared with human performance.
In Table 6, the column labeled Humans is the average of 22 people, whereas the LRME
column is only one algorithm (it is not an average). Comparing an average of several scores
to an individual score (whether the individual is a human or an algorithm) may give a
misleading impression. In the results for any individual person, there are typically several
100% scores and a few scores in the 55-75% range. The average mapping problem has seven
terms. It is not possible to have exactly one term mapped incorrectly; if there are any
incorrect mappings, then there must be two or more incorrect mappings. This follows from
the nature of bijections. Therefore a score of 5/7 = 71.4% is not uncommon.
631

Turney

Table 7 looks at the results from another perspective. The column labeled LRME wrong
gives the number of incorrect mappings made by LRME for each of the twenty problems.
The five columns labeled Number of people with N wrong show, for various values of N ,
how may of the 22 people made N incorrect mappings. For the average mapping problem,
15 out of 22 participants had a perfect score (N = 0); of the remaining 7 participants, 5
made only two mistakes (N = 2). Table 7 shows more clearly than Table 6 that LRME’s
performance is not significantly different from (individual) human performance. (For yet
another perspective, see Section 9.1).

Mapping
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
M1
M2
M3
M4
M5
M6
M7
M8
M9
M10
Average

LRME
wrong
0
0
0
0
2
0
2
0
4
0
2
0
0
0
0
0
0
2
0
0
1

Number of people with N wrong
N =0 N =1 N =2 N =3 N ≥4
16
0
4
2
0
14
0
5
0
3
9
0
9
2
2
9
0
9
0
4
10
0
7
2
3
20
0
2
0
0
8
0
6
6
2
13
0
8
0
1
11
0
7
2
2
13
0
9
0
0
17
0
5
0
0
19
0
3
0
0
14
0
8
0
0
22
0
0
0
0
9
0
11
0
2
15
0
4
3
0
21
0
1
0
0
18
0
2
1
1
19
0
3
0
0
13
0
3
3
3
15
0
5
1
1

m
7
8
8
8
7
7
7
8
9
5
7
7
6
7
6
7
7
5
8
6
7

Table 7: Another way of viewing LRME versus human performance.
In Table 8, we examine the sensitivity of LRME to the parameter settings. The first row
shows the accuracy of the baseline configuration, as in Table 6. The next eight rows show
the impact of varying k, the dimensionality of the truncated singular value decomposition,
from 50 to 400. The eight rows after that show the effect of varying t, the column factor,
from 5 to 40. The number of columns in the matrix (nc ) is given by the number of rows (nr
= 1,662) multiplied by t. The second last row shows the effect of eliminating the singular
value decomposition from LRME. This is equivalent to setting k to 1,662, the number
of rows in the matrix. The final row gives the result when PPMIC (Bullinaria & Levy,
2007) is replaced with log entropy (Turney, 2006). LRME is not sensitive to any of these
manipulations: None of the variations in Table 8 perform significantly differently from the
baseline configuration (paired t-test, 95% confidence level). (This does not necessarily mean
that the manipulations have no effect; rather, it suggests that a larger sample of problems
would be needed to show a significant effect.)
632

The Latent Relation Mapping Engine

Experiment
baseline configuration

varying k

varying t

dropping SVD
log entropy

k
300
50
100
150
200
250
300
350
400
300
300
300
300
300
300
300
300
1662
300

t
20
20
20
20
20
20
20
20
20
5
10
15
20
25
30
35
40
20
20

nc
33,240
33,240
33,240
33,240
33,240
33,240
33,240
33,240
33,240
8,310
16,620
24,930
33,240
41,550
49,860
58,170
66,480
33,240
33,240

Accuracy
91.5
89.3
92.8
91.3
92.6
90.6
91.5
90.6
90.6
86.9
94.0
94.0
91.5
90.1
90.6
89.5
91.7
89.7
83.9

Table 8: Exploring the sensitivity of LRME to various parameter settings and modifications.

8. Attribute Mapping Approaches
In this section, we explore a variety of attribute mapping approaches for the twenty mapping
problems. All of these approaches seek the mapping Ma that maximizes the sum of the
attributional similarities.
Ma = arg max

m
X

sima (ai , M (ai ))

(36)

M ∈P (A,B) i=1

We search for Ma by exhaustively evaluating all of the possibilities. Ties are broken randomly. We use a variety of different algorithms to calculate sima .
8.1 Algorithms
In the following experiments, we test five lexicon-based attributional similarity measures
that use WordNet:7 HSO (Hirst & St-Onge, 1998), JC (Jiang & Conrath, 1997), LC (Leacock & Chodrow, 1998), LIN (Lin, 1998), and RES (Resnik, 1995). All five are implemented
in the Perl package WordNet::Similarity,8 which builds on the WordNet::QueryData9 package. The core idea behind them is to treat WordNet as a graph and measure the semantic
distance between two terms by the length of the shortest path between them in the graph.
Similarity increases as distance decreases.
7. WordNet was developed by a team at Princeton and it is available at http://wordnet.princeton.edu/.
8. Ted Pedersen’s WordNet::Similarity package is at http://www.d.umn.edu/∼tpederse/similarity.html.
9. Jason Rennie’s WordNet::QueryData package is at http://people.csail.mit.edu/jrennie/WordNet/.

633

Turney

HSO works with nouns, verbs, adjectives, and adverbs, but JC, LC, LIN, and RES only
work with nouns and verbs. We used WordNet::Similarity to try all possible parts of speech
and all possible senses for each input word. Many adjectives, such as true and valuable,
also have noun and verb senses in WordNet, so JC, LC, LIN, and RES are still able to
calculate similarity for them. When the raw form of a word is not found in WordNet,
WordNet::Similarity searches for morphological variations of the word. When there are
multiple similarity scores, for multiple parts of speech and multiple senses, we select the
highest similarity score. When there is no similarity score, because a word is not in WordNet,
or because JC, LC, LIN, or RES could not find an alternative noun or verb form for an
adjective or adverb, we set the score to zero.
We also evaluate two corpus-based attributional similarity measures: PMI-IR (Turney,
2001) and LSA (Landauer & Dumais, 1997). The core idea behind them is that “a word
is characterized by the company it keeps” (Firth, 1957). The similarity of two terms is
measured by the similarity of their statistical distributions in a corpus. We used the corpus
of Section 7 along with Wumpus to implement PMI-IR (Pointwise Mutual Information
with Information Retrieval). For LSA (Latent Semantic Analysis), we used the online
demonstration.10 We selected the Matrix Comparison option with the General Reading up
to 1st year college (300 factors) topic space and the term-to-term comparison type. PMI-IR
and LSA work with all parts of speech.
Our eighth similarity measure is based on the observation that our intended mappings
map terms that have the same part of speech (see Appendix A). Let POS(a) be the partof-speech tag assigned to the term a. We use part-of-speech tags to define a measure of
attributional similarity, simPOS (a, b), as follows.

 100 if a = b
10 if POS(a) = POS(b)
(37)
simPOS (a, b) =

0 otherwise
We hand-labeled the terms in the mapping problems with part-of-speech tags (Santorini,
1990). Automatic taggers assume that the words that are to be tagged are embedded in
a sentence, but the terms in our mapping problems are not in sentences, so their tags are
ambiguous. We used our knowledge of the intended mappings to manually disambiguate
the part-of-speech tags for the terms, thus guaranteeing that corresponding terms in the
intended mapping always have the same tags.
For each of the first seven attributional similarity measures above, we created seven more
similarity measures by combining them with simPOS (a, b). For example, let simHSO (a, b) be
the Hirst and St-Onge (1998) similarity measure. We combine simPOS (a, b) and simHSO (a, b)
by simply adding them.
simHSO+POS (a, b) = simHSO (a, b) + simPOS (a, b)

(38)

The values returned by simPOS (a, b) range from 0 to 100, whereas the values returned by
simHSO (a, b) are much smaller. We chose large values in (37) so that getting POS tags to
match up has more weight than any of the other similarity measures. The manual POS tags
10. The online demonstration of LSA is the work of a team at the University of Colorado at Boulder. It is
available at http://lsa.colorado.edu/.

634

The Latent Relation Mapping Engine

and the high weight of simPOS (a, b) give an unfair advantage to the attributional mapping
approach, but the relational mapping approach can afford to be generous.
8.2 Experiments
Table 9 presents the accuracy of the various measures of attributional similarity. The
best result without POS labels is 55.9% (HSO). The best result with POS labels is 76.8%
(LIN+POS). The 91.5% accuracy of LRME (see Table 6) is significantly higher than the
76.8% accuracy of LIN+POS (and thus, of course, significantly higher than everything else
in Table 9; paired t-test, 95% confidence level). The average human performance of 87.6%
(see Table 5) is also significantly higher than the 76.8% accuracy of LIN+POS (paired t-test,
95% confidence level). In summary, humans and LRME perform significantly better than
all of the variations of attributional mapping approaches that were tested.
Algorithm
HSO
JC
LC
LIN
RES
PMI-IR
LSA
POS (hand-labeled)
HSO+POS
JC+POS
LC+POS
LIN+POS
RES+POS
PMI-IR+POS
LSA+POS

Reference
Hirst and St-Onge (1998)
Jiang and Conrath (1997)
Leacock and Chodrow (1998)
Lin (1998)
Resnik (1995)
Turney (2001)
Landauer and Dumais (1997)
Santorini (1990)
Hirst and St-Onge (1998)
Jiang and Conrath (1997)
Leacock and Chodrow (1998)
Lin (1998)
Resnik (1995)
Turney (2001)
Landauer and Dumais (1997)

Accuracy
55.9
54.7
48.5
48.2
43.8
54.4
39.6
44.8
71.1
73.6
69.5
76.8
71.6
72.8
65.8

Table 9: The accuracy of attribute mapping approaches for a wide variety of measures of
attributional similarity.

9. Discussion
In this section, we examine three questions that are suggested by the preceding results.
Is there a difference between the science analogy problems and the common metaphor
problems? Is there an advantage to combining the relational and attributional mapping approaches? What is the advantage of the relational mapping approach over the attributional
mapping approach?
9.1 Science Analogies versus Common Metaphors
Table 5 suggests that science analogies may be more difficult than common metaphors. This
is supported by Table 10, which shows how the agreement of the 22 participants with our
intended mapping (see Section 6) varies between the science problems and the metaphor
635

Turney

problems. The science problems have a lower average performance and greater variation in
performance. The difference between the science problems and the metaphor problems is
statistically significant (paired t-test, 95% confidence level).
Participant
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
Average
Standard deviation

All 20
72.6
88.2
90.0
71.8
95.7
83.4
79.6
91.9
89.7
80.7
94.5
90.6
93.2
97.1
86.6
80.5
93.3
86.5
92.9
90.4
82.7
96.2
87.6
7.2

Average Accuracy
10 Science 10 Metaphor
59.9
85.4
85.9
90.5
86.3
93.8
56.4
87.1
94.2
97.1
83.9
82.9
73.6
85.7
95.0
88.8
90.0
89.3
81.4
80.0
95.7
93.3
87.4
93.8
89.6
96.7
94.3
100.0
88.5
84.8
80.2
80.7
89.9
96.7
78.9
94.2
96.0
89.8
84.1
96.7
74.9
90.5
94.9
97.5
84.6
90.7
10.8
5.8

Table 10: A comparison of the difficulty of the science problems versus the metaphor problems for the 22 participants. The numbers in bold font are the scores that are
above the scores of LRME.
The average science problem has more terms (7.4) than the average metaphor problem
(6.6), which might contribute to the difficulty of the science problems. However, Table 11
shows that there is no clear relation between the number of terms in a problem (m in
Table 5) and the level of agreement. We believe that people find the metaphor problems
easier than the science problems because these common metaphors are entrenched in our
language, whereas the science analogies are more peripheral.
Table 12 shows that the 16 algorithms studied here perform slightly worse on the science
problems than on the metaphor problems, but the difference is not statistically significant
(paired t-test, 95% confidence level). We hypothesize that the attributional mapping approaches are not performing well enough to be sensitive to subtle differences between science
analogies and common metaphors.
Incidentally, these tables give us another view of the performance of LRME in comparison to human performance. The first row in Table 12 shows the performance of LRME on
636

The Latent Relation Mapping Engine

Num terms
5
6
7
8
9

Agreement
86.4
81.3
91.1
86.5
84.3

Table 11: The average agreement among the 22 participants as a function of the number of
terms in the problems.

Algorithm
LRME
HSO
JC
LC
LIN
RES
PMI-IR
LSA
POS
HSO+POS
JC+POS
LC+POS
LIN+POS
RES+POS
PMI-IR+POS
LSA+POS
Average
Standard deviation

All 20
91.5
55.9
54.7
48.5
48.2
43.8
54.4
39.6
44.8
71.1
73.6
69.5
76.8
71.6
72.8
65.8
61.4
14.7

Average Accuracy
10 Science 10 Metaphor
89.8
93.1
57.4
54.3
57.4
52.1
49.6
47.5
46.7
49.7
39.0
48.6
49.5
59.2
37.3
41.9
42.1
47.4
66.9
75.2
78.1
69.2
70.8
68.2
68.8
84.8
70.3
72.9
65.7
79.9
69.1
62.4
59.9
62.9
15.0
15.3

Table 12: A comparison of the difficulty of the science problems versus the metaphor problems for the 16 algorithms.

the science and metaphor problems. In Table 10, we have marked in bold font the cases
where human scores are greater than LRME’s scores. For all 20 problems, there are 8
such cases; for the 10 science problems, there are 8 such cases; for the 10 metaphor problems, there are 10 such cases. This is further evidence that LRME’s performance is not
significantly different from human performance. LRME is near the middle of the range of
performance of the 22 human participants.
9.2 Hybrid Relational-Attributional Approaches
Recall the definitions of scorer (M ) and scorea (M ) given in Section 3.
637

Turney

scorer (M ) =
scorea (M ) =

m X
m
X

simr (ai : aj , M (ai ) : M (aj ))

i=1 j=i+1
m
X

sima (ai , M (ai ))

(39)

(40)

i=1

We can combine the scores by simply adding them or multiplying them, but scorer (M ) and
scorea (M ) may be quite different in the scales and distributions of their values; therefore
we first normalize them to probabilities.
scorer (M )
Mi ∈P (A,B) scorer (Mi )

(41)

scorea (M )
Mi ∈P (A,B) scorea (Mi )

(42)

probr (M ) = P
proba (M ) = P

For these probability estimates, we assume that scorer (M ) ≥ 0 and scorea (M ) ≥ 0. If
necessary, a constant value may be added to the scores, to ensure that they are not negative.
Now we can combine the scores by adding or multiplying the probabilities.
¡
¢
Mr+a = arg max probr (M ) + proba (M )

(43)

M ∈P (A,B)

¡
¢
Mr×a = arg max probr (M ) × proba (M )

(44)

M ∈P (A,B)

Table 13 shows the accuracy when LRME is combined with LIN+POS (the best attributional mapping algorithm in Table 9, with an accuracy of 76.8%) or with HSO (the best
attributional mapping algorithm that does not use the manual POS tags, with an accuracy
of 55.9%). We try both adding and multiplying probabilities. On its own, LRME has an
accuracy of 91.5%. Combining LRME with LIN+POS increases the accuracy to 94.0%, but
this improvement is not statistically significant (paired t-test, 95% confidence level). Combining LRME with HSO results in a decrease in accuracy. The decrease is not significant
when the probabilities are multiplied (85.4%), but it is significant when the probabilities
are added (78.5%).
In summary, the experiments show no significant advantage to combining LRME with
attributional mapping. However, it is possible that a larger sample of problems would
show a significant advantage. Also, the combination methods we explored (addition and
multiplication of probabilities) are elementary. A more sophisticated approach, such as a
weighted combination, may perform better.
9.3 Coherent Relations
We hypothesize that LRME benefits from a kind of coherence among the relations. On the
other hand, attributional mapping approaches do not involve this kind of coherence.
638

The Latent Relation Mapping Engine

Components
Relational Attributional
LRME
LIN+POS
LRME
LIN+POS
LRME
HSO
LRME
HSO

Combination
add probabilities
multiply probabilities
add probabilities
multiply probabilities

Accuracy
94.0
94.0
78.5
85.4

Table 13: The performance of four different hybrids of relational and attributional mapping
approaches.

Suppose we swap two of the terms in a mapping. Let M be the original mapping and
let M 0 be the new mapping, where M 0 (a1 ) = M (a2 ), M 0 (a2 ) = M (a1 ), and M 0 (ai ) = M (ai )
for i > 2. With attributional similarity, the impact of this swap on the score of the mapping
is limited. Part of the score is not affected.

scorea (M ) = sima (a1 , M (a1 )) + sima (a2 , M (a2 )) +

m
X

sima (ai , M (ai ))

(45)

sima (ai , M (ai ))

(46)

i=3

scorea (M 0 ) = sima (a1 , M (a2 )) + sima (a2 , M (a1 )) +

m
X
i=3

On the other hand, with relational similarity, the impact of a swap is not limited in this
way. A change to any part of the mapping affects the whole score. There is a kind of global
coherence to relational similarity that is lacking in attributional similarity.
Testing the hypothesis that LRME benefits from coherence is somewhat complicated,
because we need to design the experiment so that the coherence effect is isolated from any
other effects. To do this, we move some of the terms outside of the accuracy calculation.
Let M∗ : A → B be one of our twenty mapping problems, where M∗ is our intended
mapping and m = |A| = |B|. Let A0 be a randomly selected subset of A of size m0 . Let B 0
be M∗ (A0 ), the subset of B to which M∗ maps A0 .

A0 ⊂ A

(47)

0

B ⊂B
0

(48)
0

B = M∗ (A )
¯ ¯ ¯ ¯
m0 = ¯A0 ¯ = ¯B 0 ¯
0

m <m

(49)
(50)
(51)

There are two ways that we might use LRME to generate a mapping M 0 : A0 → B 0 for this
new reduced mapping problem, internal coherence and total coherence.
1. Internal coherence: We can select M 0 based on hA0 , B 0 i alone.
639

Turney

A0 = {a1 , ..., am0 }

(52)

0

B = {b1 , ..., bm0 }

(53)
m0

M 0 = arg max

m0

X X

M ∈P (A0 ,B 0 ) i=1 j=i+1

simr (ai : aj , M (ai ) : M (aj ))

(54)

In this case, M 0 is chosen based only on the relations that are internal to hA0 , B 0 i.
2. Total coherence: We can select M 0 based on hA, Bi and the knowledge that M 0
must satisfy the constraint that M 0 (A0 ) = B 0 . (This knowledge is also embedded in
internal coherence.)

A = {a1 , ..., am }

(55)

B = {b1 , ..., bm }
©
ª
P (A, B) = M | M ∈ P (A, B) and M (A0 ) = B 0
m X
m
X
0
M = arg max
simr (ai : aj , M (ai ) : M (aj ))
0

(56)
(57)
(58)

M ∈P 0 (A,B) i=1 j=i+1

In this case, M 0 is chosen using both the relations that are internal to hA0 , B 0 i and
other relations in hA, Bi that are external to hA0 , B 0 i.

Suppose that we calculate the accuracy of these two methods based only on the subproblem hA0 , B 0 i. At first it might seem that there is no advantage to total coherence,
because it must explore a larger space of possible mappings than internal coherence (since
|P 0 (A, B)| is larger than |P (A0 , B 0 )|), but the additional terms that it explores are not
involved in calculating the accuracy. However, we hypothesize that total coherence will
have a higher accuracy than internal coherence, because the additional external relations
help to select the correct mapping.
To test this hypothesis, we set m0 to 3 and we randomly generated ten new reduced
mapping problems for each of the twenty problems (i.e., a total of 200 new problems of size
3). The average accuracy of internal coherence was 93.3%, whereas the average accuracy
of total coherence was 97.3%. The difference is statistically significant (paired t-test, 95%
confidence level).
On the other hand, the attributional mapping approaches cannot benefit from total
coherence, because there is no connection between the attributes that are in hA0 , B 0 i and
the attributes that are outside. We can decompose scorea (M ) into two independent parts.
640

The Latent Relation Mapping Engine

A00 = A \ A0
0

(59)
00

A=A ∪A
©
ª
P (A, B) = M | M ∈ P (A, B) and M (A0 ) = B 0
X
M 0 = arg max
sima (ai , M (ai ))

(60)

0

(61)
(62)

M ∈P 0 (A,B) a ∈A
i


= arg max 
M ∈P 0 (A,B)


X

ai

sima (ai , M (ai )) +

∈A0

X
ai

sima (ai , M (ai ))

(63)

∈A00

These two parts can be optimized independently. Thus the terms that are external to
hA0 , B 0 i have no influence on the part of M 0 that covers hA0 , B 0 i.
Relational mapping cannot be decomposed into independent parts in this way, because
the relations connect the parts. This gives relational mapping approaches an inherent
advantage over attributional mapping approaches.
To confirm this analysis, we compared internal and total coherence using LIN+POS
on the same 200 new problems of size 3. The average accuracy of internal coherence was
88.0%, whereas the average accuracy of total coherence was 87.0%. The difference is not
statistically significant (paired t-test, 95% confidence level). (The only reason that there is
any difference is that, when two mappings have the same score, we break the ties randomly.
This causes random variation in the accuracy.)
The benefit from coherence suggests that we can make analogy mapping problems easier
for LRME by adding more terms. The difficulty is that the new terms cannot be randomly
chosen; they must fit with the logic of the analogy and not overlap with the existing terms.
Of course, this is not the only important difference between the relational and attributional mapping approaches. We believe that the most important difference is that relations
are more reliable and more general than attributes, when using past experiences to make
predictions about the future (Hofstadter, 2001; Gentner, 2003). Unfortunately, this hypothesis is more difficult to evaluate experimentally than our hypothesis about coherence.

10. Related Work
French (2002) gives a good survey of computational approaches to analogy-making, from the
perspective of cognitive science (where the emphasis is on how well computational systems
model human performance, rather than how well the systems perform). We will sample a
few systems from his survey and add a few more that were not mentioned.
French (2002) categorizes analogy-making systems as symbolic, connectionist, or symbolicconnectionist hybrids. Gärdenfors (2004) proposes another category of representational
systems for AI and cognitive science, which he calls conceptual spaces. These spatial or geometric systems are common in information retrieval and machine learning (Widdows, 2004;
van Rijsbergen, 2004). An influential example is Latent Semantic Analysis (Landauer &
Dumais, 1997). The first spatial approaches to analogy-making began to appear around the
same time as French’s (2002) survey. LRME takes a spatial approach to analogy-making.
641

Turney

10.1 Symbolic Approaches
Computational approaches to analogy-making date back to Analogy (Evans, 1964) and
Argus (Reitman, 1965). Both of these systems were designed to solve proportional analogies
(analogies in which |A| = |B| = 2; see Section 4). Analogy could solve proportional
analogies with simple geometric figures and Argus could solve simple word analogies. These
systems used hand-coded rules and were only able to solve the limited range of problems
that their designers had anticipated and coded in the rules.
French (2002) cites Structure Mapping Theory (SMT) (Gentner, 1983) and the Structure
Mapping Engine (SME) (Falkenhainer et al., 1989) as the prime examples of symbolic
approaches:
SMT is unquestionably the most influential work to date on the modeling of
analogy-making and has been applied in a wide range of contexts ranging from
child development to folk physics. SMT explicitly shifts the emphasis in analogymaking to the structural similarity between the source and target domains. Two
major principles underlie SMT:
• the relation-matching principle: good analogies are determined by mappings of relations and not attributes (originally only identical predicates
were mapped) and
• the systematicity principle: mappings of coherent systems of relations are
preferred over mappings of individual relations.
This structural approach was intended to produce a domain-independent mapping process.
LRME follows both of these principles. LRME uses only relational similarity; no attributional similarity is involved (see Section 7.1). Coherent systems of relations are preferred
over mappings of individual relations (see Section 9.3). However, the spatial (statistical,
corpus-based) approach of LRME is quite different from the symbolic (logical, hand-coded)
approach of SME.
Martin (1992) uses a symbolic approach to handle conventional metaphors. Gentner,
Bowdle, Wolff, and Boronat (2001) argue that novel metaphors are processed as analogies,
but conventional metaphors are recalled from memory without special processing. However,
the line between conventional and novel metaphor can be unclear.
Dolan (1995) describes an algorithm that can extract conventional metaphors from a
dictionary. A semantic parser is used to extract semantic relations from the Longman
Dictionary of Contemporary English (LDOCE). A symbolic algorithm finds metaphorical
relations between words, using the extracted relations.
Veale (2003, 2004) has developed a symbolic approach to analogy-making, using WordNet as a lexical resource. Using a spreading activation algorithm, he achieved a score of
43.0% on a set of 374 multiple-choice lexical proportional analogy questions from the SAT
college entrance test (Veale, 2004).
Lepage (1998) has demonstrated that a symbolic approach to proportional analogies can
be used for morphology processing. Lepage and Denoual (2005) apply a similar approach
to machine translation.
642

The Latent Relation Mapping Engine

10.2 Connectionist Approaches
Connectionist approaches to analogy-making include ACME (Holyoak & Thagard, 1989)
and LISA (Hummel & Holyoak, 1997). Like symbolic approaches, these systems use handcoded knowledge representations, but the search for mappings takes a connectionist approach, in which there are nodes with weights that are incrementally updated over time,
until the system reaches a stable state.
10.3 Symbolic-Connectionist Hybrid Approaches
The third family examined by French (2002) is hybrid approaches, containing elements
of both the symbolic and connectionist approaches. Examples include Copycat (Mitchell,
1993) and Tabletop (French, 1995). Much of the work in the Fluid Analogies Research
Group (FARG) concerns symbolic-connectionist hybrids (Hofstadter & FARG, 1995).
10.4 Spatial Approaches
Marx, Dagan, Buhmann, and Shamir (2002) present the coupled clustering algorithm, which
uses a feature vector representation to find analogies in collections of text. For example,
given documents on Buddhism and Christianity, it finds related terms, such as {school,
Mahayana, Zen} for Buddhism and {tradition, Catholic, Protestant} for Christianity.
Mason (2004) describes the CorMet system for extracting conventional metaphors from
text. CorMet is based on clustering feature vectors that represent the selectional preferences
of verbs. Given keywords for the source domain laboratory and the target domain finance,
it is able to discover mappings such as liquid → income and container → institution.
Turney, Littman, Bigham, and Shnayder (2003) present a system for solving lexical
proportional analogy questions from the SAT college entrance test, which combines thirteen
different modules. Twelve of the modules use either attributional similarity or a symbolic
approach to relational similarity, but one module uses a spatial (feature vector) approach
to measuring relational similarity. This module worked much better than any of the other
modules; therefore, it was studied in more detail by Turney and Littman (2005). The
relation between a pair of words is represented by a vector, in which the elements are pattern
frequencies. This is similar to LRME, but one important difference is that Turney and
Littman (2005) used a fixed, hand-coded set of 128 patterns, whereas LRME automatically
generates a variable number of patterns from the given corpus (33,240 patterns in our
experiments here).
Turney (2005) introduced Latent Relational Analysis (LRA), which was examined more
thoroughly by Turney (2006). LRA achieves human-level performance on a set of 374
multiple-choice proportional analogy questions from the SAT college entrance exam. LRME
uses a simplified form of LRA. A similar simplification of LRA is used by Turney (2008), in
a system for processing analogies, synonyms, antonyms, and associations. The contribution
of LRME is to go beyond proportional analogies, to larger systems of analogical mappings.
10.5 General Theories of Analogy and Metaphor
Many theories of analogy-making and metaphor either do not involve computation or they
suggest general principles and concepts that are not specific to any particular computational
643

Turney

approach. The design of LRME has been influenced by several theories of this type (Gentner,
1983; Hofstadter & FARG, 1995; Holyoak & Thagard, 1995; Hofstadter, 2001; Gentner,
2003).
Lakoff and Johnson (1980) provide extensive evidence that metaphor is ubiquitous in
language and thought. We believe that a system for analogy-making should be able to
handle metaphorical language, which is why ten of our analogy problems are derived from
Lakoff and Johnson (1980). We agree with their claim that a metaphor does not merely
involve a superficial relation between a couple of words; rather, it involves a systematic set
of mappings between two domains. Thus our analogy problems involve larger sets of words,
beyond proportional analogies.
Holyoak and Thagard (1995) argue that analogy-making is central in our daily thought,
and especially in finding creative solutions to new problems. Our ten scientific analogies
were derived from their examples of analogy-making in scientific creativity.

11. Limitations and Future Work
In Section 4, we mentioned ten applications for LRA, and in Section 5 we claimed that the
results of the experiments in Section 9.3 suggest that LRME may perform better than LRA
on all ten of these applications, due to its ability to handle bijective analogies when m > 2.
Our focus in future work will be testing this hypothesis. In particular, the task of semantic
role labeling, discussed in Section 1, seems to be a good candidate application for LRME.
The input to LRME is simpler than the input to SME (compare Figures 1 and 2 in
Section 1 with Table 1), but there is still some human effort involved in creating the input.
LRME is not immune to the criticism of Chalmers, French, and Hofstadter (1992), that
the human who generates the input is doing more work than the computer that makes the
mappings, although it is not a trivial matter to find the right mapping out of 5,040 (7!)
choices.
In future work, we would like to relax the requirement that hA, Bi must be a bijection
(see Section 3), by adding irrelevant words (distractors) and synonyms. The mapping
algorithm will be forced to decide what terms to include in the mapping and what terms
to leave out.
We would also like to develop an algorithm that can take a proportional analogy (m = 2)
as input (e.g., sun:planet::nucleus:electron) and automatically expand it to a larger analogy
(m > 2, e.g., Table 2). That is, it would automatically search the corpus for new terms to
add to the analogy.
The next step would be to give the computer only the topic of the source domain (e.g.,
solar system) and the topic of the target domain (e.g., atomic structure), and let it work
out the rest on its own. This might be possible by combining ideas from LRME with ideas
from coupled clustering (Marx et al., 2002) and CorMet (Mason, 2004).
It seems that analogy-making is triggered in people when we encounter a problem
(Holyoak & Thagard, 1995). The problem defines the target for us, and we immediately
start searching for a source. Analogical mapping enables us to transfer our knowledge of
the source to the target, hopefully leading to a solution to the problem. This suggests that
the input to the ideal analogical mapping algorithm would be simply a statement that there
644

The Latent Relation Mapping Engine

is a problem (e.g., What is the structure of the atom?). Ultimately, the computer might
find the problems on its own as well. The only input would be a large corpus.
The algorithms we have considered here all perform exhaustive search of the set of
possible mappings P (A, B). This is acceptable when the sets are small, as they are here,
but it will be problematic for larger problems. In future work, it will be necessary to use
heuristic search algorithms instead of exhaustive search.
It takes almost 18 hours for LRME to process the twenty mapping problems (Section 7).
With better hardware and some changes to the software, this time could be significantly
reduced. For even greater speed, the algorithm could run continuously, building a large
database of vector representations of term pairs, so that it is ready to create mappings as
soon as a user requests them. This is similar to the vision of Banko and Etzioni (2007).
LRME, like LRA and LSA (Landauer & Dumais, 1997), uses a truncated singular value
decomposition (SVD) to smooth the matrix. Many other algorithms have been proposed
for smoothing matrices. In our past work with LRA (Turney, 2006), we experimented with
Nonnegative Matrix Factorization (NMF) (Lee & Seung, 1999), Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999), Iterative Scaling (IS) (Ando, 2000), and Kernel
Principal Components Analysis (KPCA) (Scholkopf, Smola, & Muller, 1997). We had some
interesting results with small matrices (around 1000 × 2000), but none of the algorithms
seemed substantially better than truncated SVD, and none of them scaled up to the matrix
sizes that we have here (1,662 × 33,240). However, we believe that SVD is not unique, and
future work is likely to discover a smoothing algorithm that is more efficient and effective
than SVD. The results in Section 7.2 do not show a significant benefit from SVD. Table 8
hints that PPMIC (Bullinaria & Levy, 2007) is more important than SVD.
LRME extracts knowledge from many fragments of text. In Section 7.1, we noted
that we found an average of 1,180 phrases per pair. The information from these 1,180
phrases is combined in a vector, to represent the semantic relation for a pair. This is
quite different from relation extraction in (for example) the Automatic Content Extraction
(ACE) Evaluation.11 The task in ACE is to identify and label a semantic relation in a single
sentence. Semantic role labeling also involves labeling a single sentence (Gildea & Jurafsky,
2002).
The contrast between LRME and ACE is analogous to the distinction in cognitive
psychology between semantic and episodic memory. Episodic memory is memory of a
specific event in one’s personal past, whereas semantic memory is memory of basic facts and
concepts, unrelated to any specific event in the past. LRME extracts relational information
that is independent of any specific sentence, like semantic memory. ACE is concerned with
extracting the relation in a specific sentence, like episodic memory. In cognition, episodic
memory and semantic memory work together synergistically. When we experience an event,
we use our semantic memory to interpret the event and form a new episodic memory,
but semantic memory is itself constructed from our past experiences, our accumulated
episodic memories. This suggests that there should be a synergy from combining LRME-like
semantic information extraction algorithms with ACE-like episodic information extraction
algorithms.
11. ACE is an annual event that began in 1999. Relation Detection and Characterization (RDC) was
introduced to ACE in 2001. For more information, see http://www.nist.gov/speech/tests/ace/.

645

Turney

12. Conclusion
Analogy is the core of cognition. We understand the present by analogy to the past. We
predict the future by analogy to the past and the present. We solve problems by searching
for analogous situations (Holyoak & Thagard, 1995). Our daily language is saturated with
metaphor (Lakoff & Johnson, 1980), and metaphor is based on analogy (Gentner et al.,
2001). To understand human language, to solve human problems, to work with humans,
computers must be able to make analogical mappings.
Our best theory of analogy-making is Structure Mapping Theory (Gentner, 1983), but
the Structure Mapping Engine (Falkenhainer et al., 1989) puts too much of the burden
of analogy-making on its human users (Chalmers et al., 1992). LRME is an attempt to
shift some of that burden onto the computer, while remaining consistent with the general
principles of SMT.
We have shown that LRME is able to solve bijective analogical mapping problems with
human-level performance. Attributional mapping algorithms (at least, those we have tried
so far) are not able to reach this level. This supports SMT, which claims that relations are
more important than attributes when making analogical mappings.
There is still much research to be done. LRME takes some of the load off the human
user, but formulating the input to LRME is not easy. This paper is an incremental step
towards a future in which computers can make surprising and useful analogies with minimal
human assistance.

Acknowledgments
Thanks to my colleagues at the Institute for Information Technology for participating in
the experiment in Section 6. Thanks to Charles Clarke and Egidio Terra for their corpus.
Thanks to Stefan Büttcher for making Wumpus available and giving me advice on its use.
Thanks to Doug Rohde for making SVDLIBC available. Thanks to the WordNet team at
Princeton University for WordNet, Ted Pedersen for the WordNet::Similarity Perl package,
and Jason Rennie for the WordNet::QueryData Perl package. Thanks to the LSA team
at the University of Colorado at Boulder for the use of their online demonstration of LSA.
Thanks to Deniz Yuret, André Vellino, Dedre Gentner, Vivi Nastase, Yves Lepage, Diarmuid
Ó Séaghdha, Roxana Girju, Chris Drummond, Howard Johnson, Stan Szpakowicz, and the
anonymous reviewers of JAIR for their helpful comments and suggestions.

Appendix A. Details of the Mapping Problems
In this appendix, we provide detailed information about the twenty mapping problems.
Figure 3 shows the instructions that were given to the participants in the experiment in
Section 6. These instructions were displayed in their web browsers. Tables 14, 15, 16,
and 17 show the twenty mapping problems. The first column gives the problem number
(e.g., A1) and a mnemonic that summarizes the mapping (e.g., solar system → atom). The
second column gives the source terms and the third column gives the target terms.
The mappings shown in these tables are our intended mappings. The fourth column
shows the percentage of participants who agreed with our intended mappings. For example,
646

The Latent Relation Mapping Engine

Systematic Analogies and Metaphors
Instructions
You will be presented with twenty analogical mapping problems, ten based on scientific
analogies and ten based on common metaphors. A typical problem will look like this:
horse
legs
hay
brain
dung

→
→
→
→
→

∇
∇
∇
∇
∇

?
?
?
?
?

You may click on the drop-down menus above, to see what options are available.
Your task is to construct an analogical mapping; that is, a one-to-one mapping between the
items on the left and the items on the right. For example:
horse
legs
hay

→ car
→ wheels
→ gasoline

∇
∇
∇

brain
dung

→ driver
→ exhaust

∇
∇

This mapping expresses an analogy between a horse and a car. The horse’s legs are like the
car’s wheels. The horse eats hay and the car consumes gasoline. The horse’s brain controls
the movement of the horse like the car’s driver controls the movement of the car. The horse
generates dung as a waste product like the car generates exhaust as a waste product.
You should have no duplicate items in your answers on the right-hand side. If there are
any duplicates or missing items (question marks), you will get an error message when you
submit your answer.
You are welcome to use a dictionary as you work on the problems, if you would find it
helpful.
If you find the above instructions unclear, then please do not continue with this exercise.
Your answers to the twenty problems will be used as a standard for evaluating the output
of a computer algorithm; therefore, you should only proceed if you are confident that you
understand this task.
Figure 3: The instructions for the participants in the experiment in Section 6.

647

Turney

Mapping
A1
solar system
→ atom

A2
water flow
→ heat transfer

A3
waves
→ sounds

A4
combustion
→ respiration

A5
sound
→ light

Source
solar system
sun
planet
mass
attracts
revolves
gravity
Average agreement:
water
flows
pressure
water tower
bucket
filling
emptying
hydrodynamics
Average agreement:
waves
shore
reflects
water
breakwater
rough
calm
crashing
Average agreement:
combustion
fire
fuel
burning
hot
intense
oxygen
carbon dioxide
Average agreement:
sound
low
high
echoes
loud
quiet
horn
Average agreement:

→
→
→
→
→
→
→
→

Target
atom
nucleus
electron
charge
attracts
revolves
electromagnetism

→
→
→
→
→
→
→
→

heat
transfers
temperature
burner
kettle
heating
cooling
thermodynamics

→
→
→
→
→
→
→
→

sounds
wall
echoes
air
insulation
loud
quiet
vibrating

→
→
→
→
→
→
→
→

respiration
animal
food
breathing
living
vigorous
oxygen
carbon dioxide

→
→
→
→
→
→
→

light
red
violet
reflects
bright
dim
lens

Agreement
86.4
100.0
95.5
86.4
90.9
95.5
81.8
90.9
86.4
95.5
86.4
72.7
72.7
95.5
95.5
90.9
86.9
86.4
77.3
95.5
95.5
81.8
63.6
100.0
54.5
81.8
72.7
95.5
90.9
72.7
59.1
77.3
77.3
86.4
79.0
86.4
50.0
54.5
100.0
90.9
77.3
95.5
79.2

POS
NN
NN
NN
NN
VBZ
VBZ
NN
NN
VBZ
NN
NN
NN
VBG
VBG
NN
NNS
NN
VBZ
NN
NN
JJ
JJ
VBG
NN
NN
NN
VBG
JJ
JJ
NN
NN
NN
JJ
JJ
VBZ
JJ
JJ
NN

Table 14: Science analogy problems A1 to A5, derived from Chapter 8 of Holyoak and
Thagard (1995).

648

The Latent Relation Mapping Engine

Mapping
A6
projectile
→ planet

A7
artificial selection
→ natural selection

A8
billiard balls
→ gas molecules

A9
computer
→ mind

A10
slot machine
→ bacterial mutation

Source
projectile
trajectory
earth
parabolic
air
gravity
attracts
Average agreement:
breeds
selection
conformance
artificial
popularity
breeding
domesticated
Average agreement:
balls
billiards
speed
table
bouncing
moving
slow
fast
Average agreement:
computer
processing
erasing
write
read
memory
outputs
inputs
bug
Average agreement:
slot machines
reels
spinning
winning
losing
Average agreement:

→
→
→
→
→
→
→
→

Target
planet
orbit
sun
elliptical
space
gravity
attracts

→
→
→
→
→
→
→

species
competition
adaptation
natural
fitness
mating
wild

→
→
→
→
→
→
→
→

molecules
gas
temperature
container
pressing
moving
cold
hot

→
→
→
→
→
→
→
→
→

mind
thinking
forgetting
memorize
remember
memory
muscles
senses
mistake

→
→
→
→
→

bacteria
genes
mutating
reproducing
dying

Agreement
100.0
100.0
100.0
100.0
100.0
90.9
90.9
97.4
100.0
59.1
59.1
77.3
54.5
95.5
77.3
74.7
90.9
72.7
81.8
95.5
77.3
86.4
100.0
100.0
88.1
90.9
95.5
100.0
72.7
54.5
81.8
72.7
90.9
100.0
84.3
68.2
72.7
86.4
90.9
100.0
83.6

POS
NN
NN
NN
JJ
NN
NN
VBZ
NNS
NN
NN
JJ
NN
VBG
JJ
NNS
NN
NN
NN
VBG
VBG
JJ
JJ
NN
VBG
VBG
VB
VB
NN
NNS
NNS
NN
NNS
NNS
VBG
VBG
VBG

Table 15: Science analogy problems A6 to A10, derived from Chapter 8 of Holyoak and
Thagard (1995).

649

Turney

Mapping
M1
war
→ argument

M2
buying an item
→ accepting a belief

M3
grounds for a building
→ reasons for a theory

M4
impediments to travel
→ difficulties

M5
money
→ time

Source
war
soldier
destroy
fighting
defeat
attacks
weapon
Average agreement:
buyer
merchandise
buying
selling
returning
valuable
worthless
Average agreement:
foundations
buildings
supporting
solid
weak
crack
Average agreement:
obstructions
destination
route
traveller
travelling
companion
arriving
Average agreement:
money
allocate
budget
effective
cheap
expensive
Average agreement:

→
→
→
→
→
→
→
→

Target
argument
debater
refute
arguing
acceptance
criticizes
logic

→
→
→
→
→
→
→

believer
belief
accepting
advocating
rejecting
true
false

→
→
→
→
→
→

reasons
theories
confirming
rational
dubious
flaw

→
→
→
→
→
→
→

difficulties
goal
plan
person
problem solving
partner
succeeding

→
→
→
→
→
→

time
invest
schedule
efficient
quick
slow

Agreement
90.9
100.0
90.9
95.5
90.9
95.5
90.9
93.5
100.0
90.9
95.5
100.0
95.5
95.5
95.5
96.1
72.7
77.3
95.5
90.9
95.5
95.5
87.9
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
95.5
86.4
86.4
86.4
50.0
59.1
77.3

POS
NN
NN
VB
VBG
NN
VBZ
NN
NN
NN
VBG
VBG
VBG
JJ
JJ
NNS
NNS
VBG
JJ
JJ
NN
NNS
NN
NN
NN
VBG
NN
VBG
NN
VB
NN
JJ
JJ
JJ

Table 16: Common metaphor problems M1 to M5, derived from Lakoff and Johnson (1980).

650

The Latent Relation Mapping Engine

Mapping
M6
seeds
→ ideas

M7
machine
→ mind

M8
object
→ idea

M9
following
→ understanding

M10
seeing
→ understanding

Source
seeds
planted
fruitful
fruit
grow
wither
blossom
Average agreement:
machine
working
turned on
turned off
broken
power
repair
Average agreement:
object
hold
weigh
heavy
light
Average agreement:
follow
leader
path
follower
lost
wanders
twisted
straight
Average agreement:
seeing
light
illuminating
darkness
view
hidden
Average agreement:

→
→
→
→
→
→
→
→

Target
ideas
inspired
productive
product
develop
fail
succeed

→
→
→
→
→
→
→

mind
thinking
awake
asleep
confused
intelligence
therapy

→
→
→
→
→

idea
understand
analyze
important
trivial

→
→
→
→
→
→
→
→

understand
speaker
argument
listener
misunderstood
digresses
complicated
simple

→
→
→
→
→
→

understanding
knowledge
explaining
confusion
interpretation
secret

Agreement
90.9
95.5
81.8
95.5
81.8
100.0
77.3
89.0
95.5
100.0
100.0
100.0
100.0
95.5
100.0
98.7
90.9
81.8
81.8
95.5
95.5
89.1
100.0
100.0
100.0
100.0
86.4
90.9
95.5
100.0
96.6
68.2
77.3
86.4
86.4
68.2
86.4
78.8

POS
NNS
VBD
JJ
NN
VB
VB
VB
NN
VBG
JJ
JJ
JJ
NN
NN
NN
VB
VB
JJ
JJ
VB
NN
NN
NN
JJ
VBZ
JJ
JJ
VBG
NN
VBG
NN
NN
JJ

Table 17: Common metaphor problems M6 to M10, derived from Lakoff and Johnson
(1980).

651

Turney

in problem A1, 81.8% of the participants (18 out of 22) mapped gravity to electromagnetism.
The final column gives the part-of-speech (POS) tags for the source and target terms. We
used the Penn Treebank tags (Santorini, 1990). We assigned these tags manually. Our
intended mappings and our tags were chosen so that mapped terms have the same tags.
For example, in A1, sun maps to nucleus, and both sun and nucleus are tagged NN. The
POS tags are used in the experiments in Section 8. The POS tags are not used by LRME
and they were not shown to the participants in the experiment in Section 6.

References
Ando, R. K. (2000). Latent semantic space: Iterative scaling improves precision of interdocument similarity measurement. In Proceedings of the 23rd Annual ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR-2000), pp.
216–223.
Banko, M., & Etzioni, O. (2007). Strategies for lifelong knowledge extraction from the web.
In Proceedings of the 4th International Conference on Knowledge Capture (K-CAP
2007), pp. 95–102.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations from word cooccurrence statistics: A computational study. Behavior Research Methods, 39 (3),
510–526.
Büttcher, S., & Clarke, C. (2005). Efficiency vs. effectiveness in terabyte-scale information retrieval. In Proceedings of the 14th Text REtrieval Conference (TREC 2005),
Gaithersburg, MD.
Chalmers, D. J., French, R. M., & Hofstadter, D. R. (1992). High-level perception, representation, and analogy: A critique of artificial intelligence methodology. Journal of
Experimental & Theoretical Artificial Intelligence, 4 (3), 185–211.
Deerwester, S. C., Dumais, S. T., Landauer, T. K., Furnas, G. W., & Harshman, R. A.
(1990). Indexing by latent semantic analysis. Journal of the American Society for
Information Science (JASIS), 41 (6), 391–407.
Dolan, W. B. (1995). Metaphor as an emergent property of machine-readable dictionaries. In Proceedings of the AAAI 1995 Spring Symposium Series: Representation and
Acquisition of Lexical Knowledge: Polysemy, Ambiguity and Generativity, pp. 27–32.
Evans, T. (1964). A heuristic program to solve geometric-analogy problems. In Proceedings
of the Spring Joint Computer Conference, pp. 327–338.
Falkenhainer, B., Forbus, K. D., & Gentner, D. (1989). The structure-mapping engine:
Algorithm and examples. Artificial Intelligence, 41 (1), 1–63.
Firth, J. R. (1957). A synopsis of linguistic theory 1930–1955. In Studies in Linguistic
Analysis, pp. 1–32. Blackwell, Oxford.
Forbus, K., Usher, J., Lovett, A., Lockwood, K., & Wetzel, J. (2008). Cogsketch: Opendomain sketch understanding for cognitive science research and for education. In
Proceedings of the Fifth Eurographics Workshop on Sketch-Based Interfaces and Modeling, Annecy, France.
652

The Latent Relation Mapping Engine

Forbus, K. D., Riesbeck, C., Birnbaum, L., Livingston, K., Sharma, A., & Ureel, L. (2007). A
prototype system that learns by reading simplified texts. In AAAI Spring Symposium
on Machine Reading, Stanford University, California.
French, R. (1995). The Subtlety of Sameness: A Theory and Computer Model of AnalogyMaking. MIT Press, Cambridge, MA.
French, R. M. (2002). The computational modeling of analogy-making. Trends in Cognitive
Sciences, 6 (5), 200–205.
Gärdenfors, P. (2004). Conceptual Spaces: The Geometry of Thought. MIT Press.
Gentner, D. (1983). Structure-mapping: A theoretical framework for analogy. Cognitive
Science, 7 (2), 155–170.
Gentner, D. (1991). Language and the career of similarity. In Gelman, S., & Byrnes, J.
(Eds.), Perspectives on Thought and Language: Interrelations in Development, pp.
225–277. Cambridge University Press.
Gentner, D. (2003). Why we’re so smart. In Gentner, D., & Goldin-Meadow, S. (Eds.),
Language in Mind: Advances in the Study of Language and Thought, pp. 195–235.
MIT Press.
Gentner, D., Bowdle, B. F., Wolff, P., & Boronat, C. (2001). Metaphor is like analogy. In
Gentner, D., Holyoak, K. J., & Kokinov, B. N. (Eds.), The analogical mind: Perspectives from Cognitive Science, pp. 199–253. MIT Press, Cambridge, MA.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational
Linguistics, 28 (3), 245–288.
Girju, R., Nakov, P., Nastase, V., Szpakowicz, S., Turney, P., & Yuret, D. (2007). Semeval2007 task 04: Classification of semantic relations between nominals. In Proceedings
of the Fourth International Workshop on Semantic Evaluations (SemEval 2007), pp.
13–18, Prague, Czech Republic.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
Hawkins, J., & Blakeslee, S. (2004). On Intelligence. Henry Holt.
Hirst, G., & St-Onge, D. (1998). Lexical chains as representations of context for the detection
and correction of malapropisms. In Fellbaum, C. (Ed.), WordNet: An Electronic
Lexical Database, pp. 305–332. MIT Press.
Hofmann, T. (1999). Probabilistic Latent Semantic Indexing. In Proceedings of the 22nd
Annual ACM Conference on Research and Development in Information Retrieval (SIGIR ’99), pp. 50–57, Berkeley, California.
Hofstadter, D. (2001). Epilogue: Analogy as the core of cognition. In Gentner, D., Holyoak,
K. J., & Kokinov, B. N. (Eds.), The Analogical Mind: Perspectives from Cognitive
Science, pp. 499–538. MIT Press.
Hofstadter, D., & FARG (1995). Fluid Concepts and Creative Analogies: Computer Models
of the Fundamental Mechanisms of Thought. Basic Books, New York, NY.
653

Turney

Holyoak, K., & Thagard, P. (1989). Analogical mapping by constraint satisfaction. Cognitive
Science, 13, 295–355.
Holyoak, K., & Thagard, P. (1995). Mental Leaps. MIT Press.
Hummel, J., & Holyoak, K. (1997). Distributed representations of structure: A theory of
analogical access and mapping. Psychological Review, 104, 427–466.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based on corpus statistics
and lexical taxonomy. In Proceedings of the International Conference on Research in
Computational Linguistics (ROCLING X), pp. 19–33, Tapei, Taiwan.
Kilgarriff, A. (1997). I don’t believe in word senses. Computers and the Humanities, 31,
91–113.
Lakoff, G., & Johnson, M. (1980). Metaphors We Live By. University Of Chicago Press.
Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge.
Psychological Review, 104 (2), 211–240.
Leacock, C., & Chodrow, M. (1998). Combining local context and WordNet similarity for
word sense identification. In Fellbaum, C. (Ed.), WordNet: An Electronic Lexical
Database. MIT Press.
Lee, D. D., & Seung, H. S. (1999). Learning the parts of objects by nonnegative matrix
factorization. Nature, 401, 788–791.
Lepage, Y. (1998). Solving analogies on words: An algorithm. In Proceedings of the 36th
Annual Conference of the Association for Computational Linguistics, pp. 728–735.
Lepage, Y., & Denoual, E. (2005). Purest ever example-based machine translation: Detailed
presentation and assessment. Machine Translation, 19 (3), 251–282.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th
International Conference on Machine Learning (ICML-98).
Martin, J. H. (1992). Computer understanding of conventional metaphoric language. Cognitive Science, 16 (2), 233–270.
Marx, Z., Dagan, I., Buhmann, J., & Shamir, E. (2002). Coupled clustering: A method
for detecting structural correspondence. Journal of Machine Learning Research, 3,
747–780.
Mason, Z. (2004). CorMet: A computational, corpus-based conventional metaphor extraction system. Computational Linguistics, 30 (1), 23–44.
Minsky, M. (1986). The Society of Mind. Simon & Schuster, New York, NY.
Mitchell, M. (1993). Analogy-Making as Perception: A Computer Model. MIT Press, Cambridge, MA.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations. In
Fifth International Workshop on Computational Semantics (IWCS-5), pp. 285–301,
Tilburg, The Netherlands.
Reitman, W. R. (1965). Cognition and Thought: An Information Processing Approach. John
Wiley and Sons, New York, NY.
654

The Latent Relation Mapping Engine

Resnik, P. (1995). Using information content to evaluate semantic similarity in a taxonomy.
In Proceedings of the 14th International Joint Conference on Artificial Intelligence
(IJCAI-95), pp. 448–453, San Mateo, CA. Morgan Kaufmann.
Rosario, B., & Hearst, M. (2001). Classifying the semantic relations in noun-compounds
via a domain-specific lexical hierarchy. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing (EMNLP-01), pp. 82–90.
Santorini, B. (1990). Part-of-speech tagging guidelines for the Penn Treebank Project. Tech.
rep., Department of Computer and Information Science, University of Pennsylvania.
(3rd revision, 2nd printing).
Scholkopf, B., Smola, A. J., & Muller, K.-R. (1997). Kernel principal component analysis. In
Proceedings of the International Conference on Artificial Neural Networks (ICANN1997), pp. 583–588, Berlin.
Turney, P. D. (2001). Mining the Web for synonyms: PMI-IR versus LSA on TOEFL. In
Proceedings of the Twelfth European Conference on Machine Learning (ECML-01),
pp. 491–502, Freiburg, Germany.
Turney, P. D. (2005). Measuring semantic similarity by latent relational analysis. In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence
(IJCAI-05), pp. 1136–1141, Edinburgh, Scotland.
Turney, P. D. (2006). Similarity of semantic relations. Computational Linguistics, 32 (3),
379–416.
Turney, P. D. (2008). A uniform approach to analogies, synonyms, antonyms, and associations. In Proceedings of the 22nd International Conference on Computational
Linguistics (Coling 2008), pp. 905–912, Manchester, UK.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning of analogies and semantic
relations. Machine Learning, 60 (1–3), 251–278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules to solve multiple-choice synonym and analogy problems. In Proceedings of
the International Conference on Recent Advances in Natural Language Processing
(RANLP-03), pp. 482–489, Borovets, Bulgaria.
van Rijsbergen, C. J. (2004). The Geometry of Information Retrieval. Cambridge University
Press, Cambridge, UK.
Veale, T. (2003). The analogical thesaurus. In Proceedings of the 15th Innovative Applications of Artificial Intelligence Conference (IAAI 2003), pp. 137–142, Acapulco,
Mexico.
Veale, T. (2004). WordNet sits the SAT: A knowledge-based approach to lexical analogy. In
Proceedings of the 16th European Conference on Artificial Intelligence (ECAI 2004),
pp. 606–612, Valencia, Spain.
Widdows, D. (2004). Geometry and Meaning. Center for the Study of Language and
Information, Stanford, CA.
Yan, J., & Forbus, K. D. (2005). Similarity-based qualitative simulation. In Proceedings of
the 27th Annual Meeting of the Cognitive Science Society, Stresa, Italy.

655

Journal of Artificial Intelligence Research 33 (2008) 149–178

Submitted 03/08; published 09/08

Complexity of Strategic Behavior in Multi-Winner Elections
Reshef Meir
Ariel D. Procaccia
Jeffrey S. Rosenschein
Aviv Zohar

reshef24@cs.huji.ac.il
arielpro@cs.huji.ac.il
jeff@cs.huji.ac.il
avivz@cs.huji.ac.il

School of Engineering and Computer Science
The Hebrew University of Jerusalem

Abstract
Although recent years have seen a surge of interest in the computational aspects of
social choice, no specific attention has previously been devoted to elections with multiple
winners, e.g., elections of an assembly or committee. In this paper, we characterize the
worst-case complexity of manipulation and control in the context of four prominent multiwinner voting systems, under different formulations of the strategic agent’s goal.

1. Introduction
Computational aspects of voting have been the focus of much interest, in a variety of fields.
In multiagent systems, the attention has been motivated by applications of well-studied
voting systems1 as a method of preference aggregation. For instance, Ghosh, Mundhe,
Hernandez, and Sen (1999) designed an automated movie recommendation system, in which
the conflicting preferences a user may have about movies were represented as agents, and
movies to be suggested were selected according to a voting scheme (in this example there are
multiple winners, as several movies are recommended to the user). In general, the candidates
in a virtual election can be entities such as beliefs, joint plans (Ephrati & Rosenschein, 1997),
or schedules (Haynes, Sen, Arora, & Nadella, 1997).
Different aspects of voting rules have been explored by computer scientists. An issue
that has been particularly well-studied is manipulation. In many settings, a voter may be
better off revealing its preferences untruthfully. For instance, in real-life elections where
each voter awards a single point to its favorite candidate, it may be judged pointless to
vote for a candidate that appears, from polls, to be a sure loser, even if that candidate is a
voter’s truthful first choice.
The celebrated Gibbard-Satterthwaite Theorem (Gibbard, 1973; Satterthwaite, 1975)
implies that under any non-dictatorial voting scheme (i.e., there is no single voter that
always dictates the outcome of the election), there always exist elections in which a voter
can improve its utility by lying about its true preferences.2 Nevertheless, it has been
suggested that bounded-rational agents may find it hard to determine exactly which lie to
use, and thus may give up on manipulations altogether. In other words, computational
complexity may be an obstacle that prevents strategic behavior. The first to address this
1. We use the terms “voting schemes”, “voting rules”, “voting systems”, and “voting protocols” interchangeably.
2. This theorem has also been generalized to the multiple winner setting (Duggan & Schwartz, 2000).
c
2008
AI Access Foundation. All rights reserved.

Meir, Procaccia, Rosenschein, & Zohar

point were Bartholdi, Tovey and Trick (1989); Bartholdi and Orlin (1991) later showed that
manipulating the important Single Transferable Vote (STV) voting rule is an N P-complete
problem.
More recently, it has been shown that voting protocols can be tweaked by adding an
elimination preround, in a way that makes manipulation hard (Conitzer & Sandholm, 2003).
Conitzer, Sandholm, and Lang (2007) studied a setting where there is an entire coalition of
manipulators. In this setting, the problem of manipulation by the coalition is N P-complete
in a variety of protocols, even when the number of candidates is constant.
Another related issue that has received significant attention is the computational difficulty of controlling an election. Here, the authority that conducts the elections attempts to
achieve strategic results by adding or removing registered voters or candidates. The same
rationale is at work here as well: if it is computationally hard to determine how to improve
the outcome of the election by control, the chairman might give up on cheating altogether.
Bartholdi, Tovey and Trick (1992) first analyzed the computational complexity of different
methods of controlling an election in the Plurality and Condorcet protocols.
In this paper, we augment the classical problems of manipulation and control by introducing multiple winners. Specifically, we assume the manipulator has a utility function
over the candidates, and that the manipulator’s goal is to achieve a set of winners with
total utility above some threshold. We study the abovementioned problems with respect
to four simple but important multi-winner voting schemes: SNTV, Bloc voting, Approval,
and Cumulative voting.
The paper proceeds as follows. In Section 2, we describe the voting rules in question. In
Section 3, we deal with manipulation problems. In Section 4, we deal with control problems.
We discuss related work in Section 5, and conclude in Section 6.

2. Multi-Winner Voting Schemes
In this section we present several multi-winner voting systems of significance. Although the
discussion is self-contained, interested readers can find more details in an article by Brams
and Fishburn (2002).
Let the set of voters be V = {v1 , v2 , . . . vn }; let the set of candidates be C = {c1 , . . . cm }.
Furthermore, assume that k ∈ N candidates are to be elected.
Multi-winner voting rules differ from single-winner ones in the properties that they are
expected to satisfy. A major concern in multi-winner elections is proportional representation: a faction that consists of a fraction X of the population should be represented by
approximately a fraction X of the seats in the assembly. This property is not satisfied
by (generalizations of) many of the rules usually considered with respect to single-winner
elections.
Thus, we here examine four of the prevalent multi-winner voting rules. In all four, the
candidates are given points by the voters, and the k candidates with the most points win
the election. The schemes differ in the way points are awarded to candidates.
• Single Non-Transferable Vote (SNTV): each voter gives one point to a favorite candidate.3
3. SNTV in single winner elections is also known as Plurality.

150

Complexity of Strategic Behavior in Multi-Winner Elections

• Bloc voting: each voter gives one point to each of k candidates.4
• Approval voting: each voter approves or disapproves any candidate; an approved
candidate is awarded one point, and there is no limit to the number of candidates a
voter can approve.
• Cumulative voting: allows voters to express intensities of preference, by asking them
to distribute a fixed number of points among the candidates. Cumulative voting
is especially interesting, since it encourages minority representation and maximizes
social welfare (Brams & Fishburn, 2002).
Scoring rules are a prominent family of voting rules. A voting rule in this family is
defined by a vector of integers α
~ = hα1 , . . . , αm i, where αl ≥ αl+1 for l = 1, . . . , m − 1.
Each voter reports a ranking of the candidates, thus awarding α1 points to the top-ranked
candidate, α2 points to the second candidate, and in general αl points to the candidate
ranked in place l. Notice that SNTV is a family of scoring rules defined for each |C| by
the vector h1, 0, . . . , 0i, and Bloc is the family of scoring rules defined for each |C|, k by the
vector h1, . . . , 1, 0, . . . , 0i, where the number of 1’s is k.

3. Manipulation
A voter is considered to be a manipulator, or is said to vote strategically, if the voter
reveals false preferences in an attempt to improve its outcome in the election. Settings
where manipulation is possible are to be avoided, since it may lead to a socially undesirable
outcome emerging as the winner of the election. Therefore, computational resistance to
manipulation is considered an advantage.
In the classical formalization of the manipulation problem (Bartholdi et al., 1989), we
are given a set C of candidates, a set V of voters, and a distinguished candidate p ∈ C. We
also have full knowledge of the voters’ votes. We are asked whether it is possible to cast an
additional vote, the manipulator’s ballot, in a way that makes p win the election.
When generalizing this problem for the k-winner case, several formulations are possible.
A very general formulation is given by the following definition.
Definition 3.1. In the Manipulation problem, we are given a set C of candidates, a
set V of voters that have already cast their vote, the number of winners k ∈ N, a utility
function u : C → Z, and an integer t ∈ N. We
P are asked whether the manipulator can cast
its vote such that in the resulting election, c∈W u(c) ≥ t, where W is the set of winners,
|W | = k.
Notice that the number of winners k is a parameter of the problem.
Remark 3.2. The manipulator’s utility function is implicitly assumed to be additive. One
can consider more elaborate utility functions, such as the ones investigated in the context
of combinatorial auctions, but that is beyond the scope of this paper.
4. Bloc voting is also known as k-Approval.

151

Meir, Procaccia, Rosenschein, & Zohar

Remark 3.3. We make the standard assumption that tie-breaking is adversarial to the
manipulator (Conitzer et al., 2007), i.e., if there are several candidates that perform equally
well in the election, the ones with the lower utility for the manipulator will be elected.
When the number of winners is k = 1, this assumption is equivalent to formulating the
manipulation problems in their unique winner version, where the ballot must be cast in a
way that some designated candidate is strictly better than the rest (e.g., has higher score).
One might argue that the general formulation of the problem given above makes manipulation harder. Indeed, the manipulator might have the following, more specific, goals
in mind.
1. The manipulator has a specific candidate whom he is interested in seeing among the
winners (constructive manipulation).
2. The manipulator has a specific candidate whom he is interested in excluding from the
set of winners (destructive manipulation) (Conitzer et al., 2007).
3. The manipulator has some (additive) boolean-valued utility function over the candidates u : C → {0, 1}.
The first and second settings are naturally special cases of the third, while the third is
a special case of Definition 3. We intend to explore all the foregoing formulations of the
Manipulation problem. Notice that one can also consider other goals, for example when
the manipulator has a favorite set of candidates and he is interested in seeing all of them,
or as many as possible of them, among the winners. However, we will only investigate these
goals insofar as they are special cases of a boolean-valued utility function.
Remark 3.4. Unless explicitly mentioned otherwise, we usually assume a general (additive)
utility function, as in Definition 3.1.
We will find it convenient to represent SNTV and Bloc voting using a common framework. We consider l-Bloc voting rules—voting rules where every voter gives one point to
each of exactly l candidates, where l ≤ k. Notice that in SNTV l = 1, while in Bloc voting
l = k. We remind the reader that the number of winners k is not constant, but is rather a
parameter of the Manipulation problem.
Proposition 3.5. Let l ∈ {1, . . . , k}. Then Manipulation in l-Bloc voting is in P.
Proof. The manipulator is faced with the score awarded to the candidates by the voters in
V ; let s[c] be the total score of candidate c. Order the candidates by their score, and let
s0 be the score of the k’th highest candidate. For example, if k = 3, |C| = m = 4, and the
initial scores are 8, 5, 5, 3, then s0 = 5. In addition, let
A = {c ∈ C : s[c] > s0 }.
Notice that |A| ≤ k − 1. Let
B = {c ∈ C : s[c] = s0 }.
B may be large, and in particular if all the candidates have the same score, then B = C.
152

Complexity of Strategic Behavior in Multi-Winner Elections

Now, candidates in A with at least s0 + 2 initial points will be elected regardless of
the actions of the manipulator, as the manipulator can award at most one point to any
candidate. Candidates in A with exactly s0 +1 points will be elected, unless other candidates
with lower utility ultimately receive s0 + 1 points due to the manipulator’s vote—as ties
are broken adversarially to the manipulator. Let us now examine the candidates with less
than s0 points. Notice that candidates with s[c] ≤ s0 − 2 will lose in any case. Moreover,
since ties are broken adversarially, voting for candidates with s0 − 1 points cannot benefit
the manipulator. To conclude the point, the manipulator can do no better than to make
sure the candidates in B (with exactly s0 points) that are eventually elected have as high
utility as possible.
Therefore, to put it simply, the manipulator’s optimal strategy is to vote for the top
(in terms of utility) k − |A| (top l if l < k − |A|) candidates in B, thus guaranteeing
that these candidates be among the winners, and cast its remaining votes in favor of some
candidates in A (which will win anyway). This discussion leads to the conclusion that
Algorithm 1 decides the Manipulation problem. Clearly the computational complexity of
Algorithm 1 Decides the Manipulation problem in l-Bloc voting
1: procedure Manipulate-Bloc(V, C, k, u, t, l)
2:
s[c] ← |{v ∈ V : v votes for candidate c}|
3:
s0 ← score of k’th highest candidate
4:
A ← {c ∈ C : s[c] > s0 }
⊲ |A| ≤ k − 1
5:
B ← {c ∈ C : s[c] = s0 }, ordered by u(c) in decreasing order
6:
if l ≤ k − |A| then
7:
manipulator votes for top l candidates in B
8:
else
9:
manipulator votes for top k − |A| candidates in B and l + |A| − k candidates in
A
10:
end if
11:
if utility of winners ≥ t then
12:
return true
13:
else
14:
return false
15:
end if
16: end procedure
the algorithm is polynomial in the input size.
We have the following immediate corollary:
Corollary 3.6. Manipulation in SNTV and Bloc voting is in P.
The situation in Approval voting is not dissimilar. Indeed, the question is: can a voter
gain by approving more than k candidates? A priori, the answer is yes. However, given
the votes of all other voters, clearly the manipulator cannot gain by approving candidates
other than the k eventual winners of the election. On the other hand, the manipulator also
does not benefit from approving less than k voters. Say the manipulator approved l < k
153

Meir, Procaccia, Rosenschein, & Zohar

voters, and the candidates in W , where |W | = k, eventually won the election. Let C ∗ be
the candidates that the manipulator approved, and W ∗ = W \ C ∗ be the winners that the
manipulator did not approve; W ∗ ≥ k − l. If the manipulator approves the l candidates
in C ∗ as well as some k − l candidates in W ∗ , the set of winners is clearly still going to
be W . Therefore, the manipulator can do no better than approve exactly k candidates; we
have already demonstrated in Proposition 3.5 that this can be accomplished optimally and
efficiently. Therefore:
Corollary 3.7. Manipulation in Approval is in P.
Remark 3.8. Formally, manipulation in Approval is a subtle issue, since the issue may be
ill-defined when the voters are assumed to have linear preferences over the candidates. In
this case, there are multiple sincere ballots (where all approved candidates are preferred to
all disapproved candidates). In some specific settings, a voter cannot gain by casting an
insincere ballot (Endriss, 2007), but this is not always true. In any case, the manipulation
problem according to our definition is well-defined and nontrivial in Approval.
In contrast to the abovementioned three voting rules, Cumulative voting turns out to
be computationally hard to manipulate under a general utility function.
Proposition 3.9. Manipulation in Cumulative voting is N P-complete.
The implicit assumption made in the proposition is that the number of points to be
distributed is not a constant, but rather a parameter of the Manipulation problem under
Cumulative voting.
The proof of Proposition 3.9 relies on a reduction from one of the most well-known
N P-complete problems, the Knapsack problem.
Definition 3.10. In the Knapsack problem, we are given a set of items A = {a1 , . . . , an },
for each a ∈ A a weight w(a) ∈ N and a value υ(a),
P t ∈ N. We are
P a capacity b ∈ N, and
′
asked whether there is a subset A ⊆ A such that a∈A′ υ(a) ≥ t while a∈A′ w(a) ≤ b.

Proof of Proposition 3.9. The problem is clearly in N P.
To see that Manipulation in Cumulative voting is N P-hard, we prove that Knapsack
reduces to this problem. We are given an input hA, w, υ, b, ti of Knapsack, and construct
an instance of Manipulation in Cumulative voting as follows.
Let n = |A|. There are 2n voters, V = {v1 , . . . , v2n }, 3n candidates, C = {c1 , . . . , c3n },
and n winners. In addition, each voter may distribute b points among the candidates. We
want the voters in V to cast their votes in a way that the following three conditions are
satisfied:
1. For j = 1, . . . , n, cj has b − w(aj ) + 1 points.
2. For j = n + 1, . . . , 2n, cj has at most b points.
3. For j = 2n + 1, . . . , 3n, cj has exactly b points.
This can easily be done. Indeed, for i = 1, . . . , n, voter vi awards b − w(ai ) + 1 points
to candidate ci , and awards its remaining w(ai ) − 1 points to candidate cn+i . Now, for
i = 1, . . . , n, voter n + i awards all its b points to candidate c2n+i .
154

Complexity of Strategic Behavior in Multi-Winner Elections

We define the utility u of candidates as follows:
(
υ(aj ) j = 1, . . . , n
u(cj ) =
0
j = n + 1, . . . , 3n
The transformation is clearly polynomial time computable, so it only remains to verify
that it is a reduction. Assume that there is a subset A′ ⊆ A with total weight at most b
and total value at least t. Let C ′ = {cj : aj ∈ A′ }. The manipulator awards w(aj ) points
to each candidate c ∈ C ′ , raising the total score of these candidates to b + 1. Since initially
all candidates have at most b points, all candidatesPc ∈ C ′ are among
P the n winners of the
u(c)
=
election. The total utility of these candidates is:
′
a∈A′ υ(a) ≥ t (since for
c∈C
all j = 1, . . . , n, u(cj ) = υ(aj )).
In the other direction, assume that the manipulator is able to distribute b points in a way
that the winners of the election have total utility at least t. Recall that there are initially at
least n candidates with b points and utility 0, and that ties are broken adversarially to the
manipulator. Therefore, there must be a subset C ′ ⊆ C of candidates that ultimately have
a score of at least b+1, such that their total utility is at least t. Let A′ be the corresponding
items in the Knapsack instance, i.e., aj ∈ A′ if and only if cj ∈ C ′ . The total weight of
items in A′ is at most b, as only b points were distributed among the candidates in C ′ by
the manipulator, and each cj ∈ C ′ initially has b − w(aj ) + 1 points. It also holds that the
total utility of the items in A′ is exactly the total utility of the candidates in C ′ , namely at
least t.
The next proposition gives a negative answer to the question of whether Manipulation
in Cumulative voting is still hard under more restricted formulations of the manipulator’s
goal, as discussed at the beginning of the section. Indeed, we put forward an algorithm that
decides the problem under any boolean-valued utility function.
Proposition 3.11. Manipulation in Cumulative voting with any boolean-valued utility
function u : C → {0, 1} is in P.
Remark 3.12. The result holds even if the number of points to be distributed is exponential
in the number of voters and candidates.
Proof of Proposition 3.11. Let s[c] be the score of candidate c ∈ C before the manipulator
has cast his vote, and s∗ [c] be c’s score when the manipulator’s vote is taken into account.
Assume without loss of generality that s[c1 ] ≥ s[c2 ] ≥ . . . ≥ s[cm ]. Let D = {d1 , d2 , . . .} be
the set of desirable candidates d ∈ C with u(d) = 1, and again assume these are sorted by
nonincreasing scores.
Informally, we are going to find a threshold thresh such that pushing t candidates above
the threshold guarantees their victory. Then we will check whether it is possible to distribute
L points such that at least t candidates pass this threshold, where L is the number of points
available to each voter.
Formally, consider Algorithm 2 (w.l.o.g. k ≥ t and |D| ≥ t, otherwise manipulation is
impossible). The algorithm clearly halts in polynomial time. It only remains to prove the
correctness of the algorithm.
155

Meir, Procaccia, Rosenschein, & Zohar

Algorithm 2 Decides Manipulation in Cumulative voting with boolean-valued utility
1: j ∗ ← max{j : |{c1 , c2 , . . . , cj−1 } ∩ D| + k + 1 − j ≥ t and cj ∈
/ D}⊲ j ∗ exists, since the
condition holds for the first candidate not in D
2: thresh ← s[cj ∗ ]
Pt
3: S ←
j=1 max{0, thresh + 1 − s[dj ]}
4: if S ≤ L then
5:
return true
6: else
7:
return false
8: end if
Lemma 3.13. Algorithm 2 correctly decides Manipulation in Cumulative voting with
any boolean-valued utility function.
Proof. Denote by Ŵ = {c1 , . . . , ck } the k candidates with highest score (sorted) before
the manipulator’s vote, and by W the final set of k winners. The threshold candidate cj ∗
partitions Ŵ into two disjoint subsets: Ŵu = {c1 , . . . , cj ∗ −1 }, Ŵd = {cj ∗ , . . . , ck }. By the
maximality of j ∗ , it holds that:
|Ŵu ∩ D| + |Ŵd | = |Ŵu ∩ D| + (k + 1 − j ∗ ) = t.

(1)

Note that S is the exact number of votes required to push t desirable candidates above
the threshold. Now, we must show that the manipulator can cast his vote in a way such
that the winner set W satisfies |W ∩ D| ≥ t if, and only if, L ≥ S.
Suppose first that S ≤ L. Then it is clearly possible to push t desirable candidates
above thresh. Ŵu ∩ D were above the threshold already; it follows that Ŵd was replaced
entirely by desirable candidates.
Let W = {w1 , . . . , wk } be the set of new winners. In particular, we can write W =
Ŵu ∪{wj ∗ , . . . , wk }. Ŵu contains |Ŵu ∩D| desirable candidates, while {wj ∗ , . . . , wk } consists
purely of desirable candidates. By Equation (1):
|W ∩ D| = |Ŵu ∩ D| + |{wj ∗ , . . . , wk }|
= |Ŵu ∩ D| + |Ŵd |
=t
Conversely, suppose S > L. We must show that the manipulator cannot distribute L
points in a way such that t candidates from D are among the winners.
Clearly there is no possibility to push t desirable candidates above thresh. Consider
some ballot cast by the manipulator, and assume w.l.o.g. that the manipulator distributed
points only among the candidates in D. Denote the new set of winners by W = Wu ⊎ Wd ,
where
Wu = {c ∈ C : s∗ [c] > thresh}
Wd = {c ∈ C : s∗ [c] ≤ thresh}.
We claim that
|Wu ∩ D| = k − t,
156

(2)

Complexity of Strategic Behavior in Multi-Winner Elections

where D = C \ D. Indeed, by Equation (1)
|Ŵu ∩ D| = t − k − 1 + j ∗ ,
and therefore
|Wu ∩ D| = |Ŵu ∩ D| = |Ŵu | − |Ŵu ∩ D|
= (j ∗ − 1) − (t − k − 1 + j ∗ )
=k−t
The first equality follows from the fact that no points were distributed to the candidates in
D.
Denote by F the set of candidates that were pushed above the threshold. Formally:
F = {c ∈ D : s∗ [c] > thresh and s[c] ≤ thresh}
Thus:
Wu = Ŵu ⊎ F.
Let w∗ be the new position of candidate cj ∗ when the candidates are sorted by nonincreasing s∗ [c]. It holds that
w∗ = j ∗ + |F |.
We now claim that
|Wd ∩ D| ≥ 1.

(3)

Indeed,
|Wu ∩ D| < t

⇒

|Ŵu ∩ D| + |F | = |Wu ∩ D| < t = |Ŵu ∩ D| + k + 1 −
|F | < k + 1 −
w∗

=

j∗

+ |F | <

j∗

j∗

+k+1−

w∗

≤k

=k+1

|Wd ∩ D| ≥ 1
By combining Equations (2) and (3), we finally obtain:
|W ∩ D| = k − |W ∩ D|
= k − (|Wu ∩ D| + |Wd ∩ D|)
=t−1
<t

The proof of Proposition 3.11 is completed.
157

⇒
⇒

j∗

cj ∗ ∈ W d

≤ k − (k − t + 1)

j∗

⇒
⇒
⇒

Meir, Procaccia, Rosenschein, & Zohar

Remark 3.14. The proof shows that the manipulation of Cumulative voting by a coalition
of (even weighted) voters, as in the work of Conitzer et al. (2007), is tractable under a
boolean-valued utility function. This follows by simply joining the (weighted) score pools
of all the voters in the coalition.
Remark 3.15. It is possible to show that if the number of points to be distributed is
polynomially bounded, manipulation of Cumulative voting is in P even under general utility
functions.

4. Control
In the control setting, we assume that the authority controlling the election (hereinafter,
the chairman) has the power to tweak the election’s electorate or slate of candidates in a
way that might change the outcome. This is also a form of undesirable strategic behavior,
but on the part of a behind-the-scenes player who is not supposed to take an active part in
the election.
In one setting, the chairman might add or remove voters that support his candidate,
but the number of voters he can add/remove without alerting attention to his actions is
limited. The problems are formally defined as follows:
Definition 4.1. In the problem of Control by Adding Voters, we are given a set
C of candidates, a set V of registered voters, a set V ′ of unregistered voters, the number
of winners k ∈ N, a utility function u : C → Z, and integers r, t ∈ N. We are asked
whether it is possible to register at most r voters from V ′ such that in the resulting election,
P
c∈W u(c) ≥ t, where W is the set of winners, |W | = k.

Definition 4.2. In the problem of Control by Removing Voters, we are given a set C
of candidates, a set V of registered voters, the number of winners k ∈ N, a utility function
u : C → Z, and integers r, t ∈ N. We are asked whether
it is possible to remove at most
P
r voters from V such that in the resulting election, c∈W u(c) ≥ t, where W is the set of
winners, |W | = k.
Another possible misuse of the chairman’s authority is tampering with the slate of
candidates. Removing candidates is obviously helpful, but even adding candidates can
sometimes tip the scales in the direction of the chairman’s favorites.
Definition 4.3. In the problem of Control by Adding Candidates, we are given a
set C of registered candidates, a set C ′ of unregistered candidates, a set V of voters, the
number of winners k, a utility function u : C ∪C ′ → Z, and integers r, t ∈ N. All voters have
preferences over all candidates C ∪ C ′ . We are asked whether it is possibleP
to add at most
r candidates C ′′ from C ′ , such that in the resulting elections on C ∪ C ′′ , c∈W u(c) ≥ t,
where W is the set of winners, |W | = k.
Definition 4.4. In the problem of Control by Removing Candidates, we are given a
set C of candidates, a set V of voters, the number of winners k, a utility function u : C → Z,
and integers r, t ∈ N. We are asked whetherPit is possible to remove at most r candidates
from C ′ , such that in the resulting elections c∈W u(c) ≥ t, where W is the set of winners,
|W | = k.
158

Complexity of Strategic Behavior in Multi-Winner Elections

Some clarification is in order. In the context of scoring rules, the assumption in the above
two problems is that the voters have rankings of all the candidates in C ∪ C ′ . Therefore, if
some candidates are added or removed, the voters’ preferences over the new set of candidates
are still well-defined. The same goes for Approval: each voter approves or disapproves
every candidates in C ∪ C ′ . However, in the context of Cumulative voting, the problems
of control by adding/removing candidates are not well-defined. Indeed, one would require
a specification of how the voters distribute their points among every possible subset of
candidates, and this would require a representation of exponential size.5 Consequently, we
do not consider control by adding or removing candidates in Cumulative voting.
Remark 4.5. Some authors (e.g., Hemaspaandra et al., 2007b) have considered other types
of control, such as control by partitioning the set of voters. In this paper, we will restrict
our attention to the four types of control mentioned above.
Remark 4.6. Unless stated otherwise, we assume that ties are broken adversarially to the
chairman.
As before, unless explicitly mentioned otherwise, we are going to assume a general
additive utility function, as in the above definitions.
Remark 4.7. It is clear that all the above computational problems are in N P for all voting
rules in question. Therefore, in our N P-completeness proofs we will only show hardness.
4.1 Controlling the Set of Voters
Control by Adding Voters is tractable in SNTV, though the procedure is not trivial.
Bartholdi et al. (1992) showed that control by adding voters is easy in SNTV when there is
a single winner, so the former result can be seen as an extension of the latter (to the case
where there are multiple winners and a general [additive] utility function).
Proposition 4.8. Control by Adding Voters in SNTV is in P.
Proof. We describe an algorithm, Control-SNTV, that efficiently decides Control in
SNTV. Informally, the algorithm works as follows. It first calculates the number of points
awarded to candidates by voters in V . Then, at each stage, the algorithm analyzes an
election where the l top winners in the original election remain winners, and attempts to
select the other k − l winners in a way that maximizes utility. This is done by setting the
threshold to be one point above the score of the (l + 1)-highest candidate; the algorithm
pushes the scores of potential winners to this threshold (see Figure 1 for an illustration).
A formal description of Control-SNTV is given as Algorithm 3. The procedure Push
works as follows: its first parameter is the threshold thr, and its second parameter is the
number of candidates to be pushed, pushN um. The procedure also has implicit access to the
input of Control-SNTV, namely the parameters of the given Control instance. Push
returns a subset V ′′ ⊆ V ′ to be registered. We say that the procedure pushes a candidate
c to the threshold if exactly thr − s[c] voters v ∈ V ′ that vote for c are registered. In
other words, the procedure registers enough voters from V ′ in order to ensure that c’s score
5. It is possible to imagine compact representations, but that is beyond the scope of this paper.

159

Meir, Procaccia, Rosenschein, & Zohar

6

03

22

4
3

22

0

1

1

2

0

5

03

22

22

2

0

1

5

Figure 1: The left panel illustrates an input of the Control problem in SNTV. Each
candidate is represented by a circled number—the utility of the candidate. The
location of the circle determines the score of the candidate, based on the voters in
V . Let k = 5; the winners are blackened. Now, assume that there are 6 voters in
V ′ , 3 voting for each of the two bottom candidates, and that r = 3. The chairman
can award 3 points to the candidate with utility 5 and score 0, but that would
not change the result of the election. Alternatively, the chairman can award 3
points to candidate with utility 2 and score 1, thus improving the utility by 1, as
can be seen in the right panel. This election is considered by the algorithm when
l = 4, s[ci5 ] = 3, and the threshold is 4.

reaches theP
threshold. Push finds a subset C ′ of candidates of size at most pushN um that
maximizes c∈C ′ u(c), under the restriction that all candidates in C ′ can be simultaneously
pushed to the threshold by registering a subset V ′′ ⊆ V ′ s.t. |V ′′ | ≤ r. The procedure returns
this subset V ′′ .
Now, assume we have a procedure Push that is always correct (in maximizing the utility
of at most k − l candidates it is able to push to the threshold s[cl+1 ] + 1, while registering
no more than r voters) and runs in polynomial time. Clearly, Control-SNTV also runs
in polynomial time. Furthermore:
Lemma 4.9. Control-SNTV correctly decides the Control problem in SNTV.
Proof. Let W = {cj1 , . . . , cjk } be the k winners of the election that does not take into
account the votes of voters in V ′ (the original election), sorted by descending score, and
for candidates with identical score, by ascending utility. Let W ∗ = {c∗j1 , . . . , c∗jk } be the
candidates that won some controlled election with maximum utility, sorted by descending
score, then by ascending utility; let s∗ [c] be the final score of candidate c in the optimal
election. Let min be the smallest index such that cjmin ∈
/ W ∗ (w.l.o.g. min exists, otherwise
∗
W = W and we are done). It holds that for all candidates c ∈ W ∗ , s∗ [c] ≥ s[cjmin ]. Now,
160

Complexity of Strategic Behavior in Multi-Winner Elections

Algorithm 3 Decides the Control problem in SNTV.
1: procedure Control-SNTV(C, V, V ′ , k, u, r, t)
2:
s[c] ← |{v ∈ V : v votes for candidate c}|
3:
Sort candidates by descending score
⊲ Break ties by ascending utility
4:
Let the sorted candidates be {ci1 , . . . , cim }
5:
for l = 0, . . . , k do
⊲ Fix l top winners
6:
V ′′ ←Push(s[cl+1 ] + 1, k − l)
⊲ Select other winners; see details below
7:
ul ← utility from election where V ′′ are registered
8:
end for
9:
if maxl ul ≥ t then return true
10:
else
11:
return false
12:
end if
13: end procedure

we can assume w.l.o.g. that if c ∈ W ∗ and s∗ [c] = s[cjmin ] then c ∈ W (and consequently,
c = cjq for some q < min). Indeed, it must hold that u[c] ≤ u[cjmin ] (as tie-breaking is
adversarial to the chairman), and if indeed c ∈
/ W even though c ∈ W ∗ , then the chairman
must have registered voters that vote for c, although this can only lower the total utility or
keep it unchanged.
It is sufficient to show that one of the elections that is considered by the algorithm has a
set of winners with utility at least that of W ∗ . Indeed, let W ′ = {cj1 , . . . , cjmin−1 } ⊆ W ; all
other k − min + 1 candidates c ∈ W ∗ \ W ′ have s[c] ≥ s[cjmin ] + 1. The algorithm considers
the election where the first min − 1 winners, namely W ′ , remain fixed, and the threshold
is s[cjmin ] + 1. Surely, it is possible to push all the candidates in W ∗ \ W ′ to the threshold,
and in such an election, the winners would be W ∗ . Since Push maximizes the utility of
the k − min + 1 candidates it pushes to the threshold, the utility returned by Push for
l = min − 1 is at least as large as the total utility of the winners in W ∗ .

It remains to explain why the procedure Push can be implemented to run in polynomial
time. Recall the Knapsack problem; a more general formulation of the problem is when
there are two resource types. Each item has two weight measures, w1 (ai ) and w2 (ai ),
that specify how much resource it consumes from each type, and the knapsack has two
capacities: b1 and b2 . The requirement is that the total amount of resource of the first
type that is consumed does not exceed b1 , and the total use of resource of the second
type does not exceed b2 . This problem, which often has more than two dimensions, is
called Multidimensional Knapsack. Push essentially solves a special case of the twodimensional knapsack problem, where the capacities are b1 = r (the number of voters
the chairman is allowed to register), and b2 = pushN um (the number of candidates to
be pushed). If the threshold is thr, for each candidate cj that is supported by at least
thr − s[cj ] voters in V ′ , we set w1 (aj ) = thr − s[cj ], w2 (aj ) = 1, and υ(aj ) = u(cj ). The
Multidimensional Knapsack problem can be solved in time that is polynomial in the
number of items and the capacities of the knapsack (Kellerer, Pferschy, & Pisinger, 2004)
161

Meir, Procaccia, Rosenschein, & Zohar

(via dynamic programming, for example). Since in our case the capacities are bounded by
|V ′ | and m, Push can be designed to run in polynomial time.
The following lemma allows us to extend our results for Control by Adding Voters
to Control by Removing Voters, and vice versa. We shall momentarily apply it to
SNTV, but it will also prove useful later on.
Lemma 4.10. Let R = hV, C, r, t, k, ui be an instance of Control by Removing Voters
in some voting rule, and let A = hV ′ , V ′′ , C ∗ , r∗ , t∗ , k ∗ , u∗ i be an instance of Control by
Adding Voters in the same voting rule, such that:
C∗ = C
r∗ = r
t∗ = t −

X

u(c)

c∈C

k ∗ = |C| − k
u∗ (c) = −u(c)
V ′′ = V
Let U be the subset of voters selected by the chairman from V = V ′′ . Denote by sU [c], s∗U [c]
the final score of candidate c in the election obtained by removing or adding the voters in
U , respectively. If it holds that
∀c, c′ ∈ C, ∀U ⊆ V,

s∗U [c] ≥ s∗U [c′ ] ⇔ sU [c] ≤ sU [c′ ]

(4)

then R is in Control by Removing Voters if and only if A is in Control by Adding
Voters.
Proof. From the condition (4) on sU , s∗U it holds that the k ∗ = |C| − k winners of the
constructed instance are exactly the |C| − k losers of the original instance.6 That is, if W
are the winners in the given instance and W ∗ are the winners in the constructed instance,
we have that W ∗ = C \ W .
From this it follows that,
X
X
X
u(c) − (t∗ +
u(c))
u(c) − t =
c∈W

c∈C

c∈C\W ∗

=−

X

u∗ (c) +

c∈C\W ∗

=

X

u∗ (c) − t∗

X

u∗ (c) − t∗

c∈C

c∈W ∗

P
Thus, for any choice of subset U to be removed or added, c∈W u(c) ≥ t if, and only
P
if, c∈W ∗ u∗ (c) ≥ t∗ . We conclude that the given instance is a “yes” instance if and only if
the constructed instance is a “yes” instance.
6. This statement also takes into account the tie-breaking scheme, as candidates with lower utility in the
original instance are now candidates with higher utility.

162

Complexity of Strategic Behavior in Multi-Winner Elections

Applying the lemma, we easily obtain:
Proposition 4.11. Control by Removing Voters in SNTV is in P.
Proof. We will give a polynomial time reduction from Control by Removing Voters
in SNTV to Control by Adding Voters in SNTV, which was shown above (Proposition 4.8) to be in P. Given an instance hV, C, r, t, k, ui of Control by Removing Voters,
define an equivalent instance hV ′ , V ′′ , C ∗ , r∗ , t∗ , k ∗ , u∗ i of the latter problem. We want to
use Lemma 4.10, so we need to define V ′ correctly.
For each voter v ∈ V = V ′′ , let f (v) ∈ C be the candidate that voter v ranks first; f (v)
gives all the relevant information about voter v’s ballot. The ballots of the voters in V ′ are
defined by the following rule. For each candidate c ∈ C,
|{v ′ ∈ V ′ : f (v ′ ) = c}| = |V | − |{v ∈ V : f (v) = c}|
It holds that:
sU [c] = |{v ∈ V : f (v) = c}| − |{v ∈ U : f (v) = c}|
From the definition of V ′ ,
s∗U [c] = |{v ′ ∈ V ′ : f (v ′ ) = c}| + |{v ′ ∈ U : f (v ′ ) = c}|
= |V | − |{v ∈ V : f (v) = c}| + |{v ∈ U : f (v) = c}|
= |V | − (|{v ∈ V : f (v) = c}| − |{v ∈ U : f (v) = c}|)
= |V | − sU [c]
Hence for all c, c′ ∈ C, and for any U ⊆ V :
s∗U [c] ≥ s∗U [c′ ] ⇔ sU [c] ≤ sU [c′ ].
This implies that the conditions of Lemma 4.10 hold, thus there is a polynomial reduction
from Control by Removing Voters in SNTV to Control by Adding Voters in
SNTV. Since the latter problem is in P, so is the former.
In the rest of the section we prove that control by adding/removing voters is hard in the
other three voting rules under consideration, even if the chairman simply wants to include
or exclude a specific candidate. In fact, this statement includes 12 different results (3 voting
rules × adding/removing × include/exclude). Instead of proving each result separately, we
will use some generic reductions. Our proof scheme is as follows: we shall first establish
that Control by Removing Voters is hard in Approval, Bloc, and Cumulative voting,
even if the chairman wants to include a candidate. We will then use Lemma 4.10 to show
that adding voters is hard in the foregoing rules, even if the chairman wants to exclude
a candidate. Finally, Lemma 4.17 will give us the six remaining results: removing while
excluding, and adding while including. The overall scheme is illustrated in Figure 2.
The following result is known from the work of Hemaspaandra et al. (2007b).
Proposition 4.12. (Hemaspaandra et al., 2007b) Control by Removing voters in
Approval is N P-complete, even if the chairman is trying to make a distinguished candidate
win in single winner elections.
163

Meir, Procaccia, Rosenschein, & Zohar

Figure 2: Scheme of the hardness proofs of control by adding/removing voters when the
chairman wants to include/exclude a distinguished candidate, in the voting rules:
Bloc, Approval, Cumulative voting.

This result, as with some other results that appear later, is stated for single winner
elections, i.e., when the number of winners k satisfies k = 1. Clearly the hardness of the
problem when k = 1 implies the hardness of the more general formulation of the problem
when k is also a parameter.
Proposition 4.13. Control by Removing Voters in Cumulative voting is N P-complete,
even if the chairman is trying to make a distinguished candidate win in single winner elections.
Remark 4.14. In instances of control problems where the chairman simply wants to include/exclude a distinguished candidate, we replace the utility function u with the distinguished candidate p, i.e., u(p) = 1 (respectively, u(p) = −1) for include (resp., for exclude)
and u(c) = 0 for all other candidates c. The threshold is t = 1 (resp., t = 0).
Proof of Proposition 4.13. We will use a reduction from Control by Removing Voters
in Approval. Let hV, C, p, ri be an instance of this problem in Approval, where p ∈ C is the
distinguished candidate (and k = 1). Define an instance of the above problem in Cumulative
voting as follows: The pool of points satisfies L∗ = |C|; r∗ = r; p∗ = p; C ∗ = C ⊎ C ′ , where
C ′ contains |V | · L∗ candidates.
We now construct a set of voters. For each voter v ∈ V , we add to V ′ a voter that
awards 1 point to every candidate whom v approves, and then gives all other points (no
164

Complexity of Strategic Behavior in Multi-Winner Elections

more than L∗ ) to distinct candidates in C ′ . V ′′ contains two more voters; each of these
voters gives one point to each candidate in C. Finally, let V ∗ = V ′ ⊎ V ′′ .
Removing voters from V ′′ cannot promote p∗ , so the chairman may limit himself without
loss of generality to removing voters from V ′ . Now, every candidate in C ′ has at most one
point, so none of them beats any candidate from C (each of whom has at least two points).
Furthermore, for every selection of voters from V ′ and corresponding voters from V to
remove, each candidate in C gets the same score in the new instance as he got in the original
instance, plus 2. This directly implies that control is possible in the constructed instance
(of Cumulative voting) if and only if it is possible in the given instance (of Approval).
In order to complete our hardness proofs for removing voters while including a distinguished candidate, we give a similar result for Bloc. Notice, however, that here the reduction
constructs instances with multiple winners.
Proposition 4.15. Control by Removing Voters in Bloc is N P-complete, even when
the chairman simply wants to include a distinguished candidate among the winners.
Proof. We prove the proposition by a polynomial time reduction from Control by Removing Voters in Approval. Let hV, C, p, k, ri be an instance of the latter problem (with
k winners; it is possible to let k = 1). Denote, as usual, n = |V | and m = |C|. Define:
C ∗ = C ⊎ C ′ ⊎ C ′′ , where C ′ contains m new candidates, and C ′′ contains 10 · k · n new
candidates; p∗ = p; k ∗ = k + m; r∗ = r. We need to design the new instance such that all
the candidates in C ′ will be among winners, and all the candidates in C ′′ to be among the
losers. We define the a voter set accordingly.
1. V ′ contains one voter for each voter in V (n in total). For each v ∈ V , v ′ gives a point
to all candidates in C that v approved. His other points go to candidates in C ′ , and
then to C ′′ .
2. V ′′ contains r + 2 voters. Each voter gives a point to each candidate in C, and gives
his other k points to candidates in C ′ (arbitrarily).
3. V ′′′ contains 4n voters. Each voter awards points to all C ′ , and to k more candidates
from C ′′ (in a way that will be specified later).
Finally let V ∗ = V ′ ⊎ V ′′ ⊎ V ′′′ .
We claim that hV ∗ , C ∗ , p∗ , k ∗ , r∗ i ∈ Control by Removing Voters in Bloc if and
only if hV, C, p, k, ri ∈ Control by Removing Voters in Approval.
For each candidate c, denote s[c], s∗ [c] the total number of votes c obtains in original
and constructed instances, before removing any voters. Note that the whole set C ′′ gets at
most (4n)k + nk votes, which is less than its size (10kn), so it is possible to scatter the votes
of V ′′ such that each candidate in C ′′ has at most 1 point. In addition, each candidate in
C ′ has at least 4n points. It also holds that
∀c ∈ C, s∗ [c] = s[c] + r + 2,
(from the votes of V ′ , V ′′ ), and thus:
∀c ∈ C, 4n − r > r + n + 2 ≥ s∗ [c] > r + 1.
165

Meir, Procaccia, Rosenschein, & Zohar

We can now conclude that:
∀c ∈ C, c′ ∈ C ′ , s∗ [c] < s∗ [c′ ] − r,
and
∀c ∈ C, c′′ ∈ C ′′ , s∗ [c] > s∗ [c′′ ] + r.
Without loss of generality the chairman removes only voters from V ′ (= V ), since only
these votes may influence the result of the elections. Denote by sU [c], s∗U [c] the score of
candidate c in both instances, after removing the subset U ⊆ V = V ′ . For any U such that
|U | ≤ r it holds that:
∀c1 , c2 ∈ C, s∗U [c1 ] ≥ s∗U [c2 ] ⇐⇒ sU [c1 ] ≥ sU [c2 ].
Moreover,
∀c ∈ C, c′ ∈ C ′ , s∗U [c] < s∗U [c′ ],
and
∀c ∈ C, c′′ ∈ C ′′ , s∗U [c] > s∗U [c′′ ].
In other words, all the candidates in C ′ will be among winners, whereas all the candidates
in C ′′ will be among losers, and the ranking of the candidates in C will not change.
We conclude that the k ∗ winners of the constructed instance are exactly the m candidates
of C ′ together with the k winners of the given Approval instance. Thus control is possible
in the original instance of Approval (i.e., it is possible to make p win) if and only if it is
possible in the constructed instance of Bloc.
As promised, we now use Lemma 4.10 to transform the results of all three voting rules
from Control (Include winner) by Removing Voters to Control (Exclude winner) by Adding
Voters.
Proposition 4.16. Control by Adding Voters in Approval, Bloc, and Cumulative voting is N P-hard, even when the chairman simply wants to exclude a distinguished candidate
from the set of winners.
Proof. We prove the proposition using Lemma 4.10 and with similar notations. Consider
an instance of Control by Removing Voters when the chairman wants to include a
candidate. Then the utility function u is 1 on p and 0 otherwise, while t = 1. The utility
function u∗ constructed by Lemma 4.10 is -1 on p and 0 otherwise, and the threshold t∗
is t − 1 = 0; that is, the chairman wants to exclude p. Now we can obtain the desired
result directly by showing that the lemma’s condition holds. In other words, we must show
that for the three voting rules in question, given an instance of Control by Removing
Voters, it is possible to construct an instance of Control by Adding Voters that
satisfies the condition (4).
Approval and Bloc: The proof here is very similar to that of Proposition 4.11. For
each voter in the Control by Removing Voters instance, we add a voter to V ′ that gives
points to the complement subset of candidates. Formally, denote by Cv ⊆ C the candidates
to which v awards points. For each v ∈ V , add v ′ to V ′ , such that Cv′ = C \ Cv . Note that
166

Complexity of Strategic Behavior in Multi-Winner Elections

this construction creates a legal Bloc instance since |Cv′ | = |C \ Cv | = |C| − k = k ∗ , and
thus

(∀v ∈ V, |Cv | = k) ⇒ ∀v ′ ∈ V ′ , |Cv′ | = k ∗ .
Let U ⊆ V ; it holds for both voting rules that:

sU [c] = |{v ∈ V : c ∈ Cv }| − |{v ∈ U : c ∈ Cv }|
and,
s∗U [c] = |{v ′ ∈ V ′ : c ∈ Cv′ }| + |{v ∈ U : c ∈ Cv }|
= |{v ∈ V : c ∈
/ Cv }| + |{v ∈ U : c ∈ Cv }|
= |V | − |{v ∈ V : c ∈ Cv }| + |{v ∈ U : c ∈ Cv }|
= |V | − (|{v ∈ V : c ∈ Cv }| − |{v ∈ U : c ∈ Cv }|)
= |V | − sU [c],
and thus:
∀c, c′ ∈ C, ∀U ⊆ V,

s∗U [c] ≥ s∗U [c′ ] ⇔ sU [c] ≤ sU [c′ ].

Cumulative voting: For each original voter, we will add a new voter to V ′ that
distributes his pool in a way that complements the original
distribution. Formally, let svc be
P
v
the score that voter v gives to candidate c; ∀v ∈ V,
c∈C sc = L. The point distribution
′
of the new voter v ′ ∈ V ′ is svc = L − svc , for all c ∈ C. Note that this is a legal Cumulative
voting instance, with L∗ = L(m − 1):
X
X ′ X
L − svc = mL −
svc = mL − L = L(m − 1) = L∗ .
∀v ′ ∈ V ′ ,
svc =
c∈C

c∈C

c∈C

Once again we will look at the final score after adding/removing the voters of U ⊆ V :
X
X
X
sU [c] =
svc =
svc −
svc .
v∈V

v∈V \U

v∈U

Further,
s∗U [c] =

X

svc

v∈V ′ ⊎U

=

X

′

svc +

v ′ ∈V ′

=

X

X

v∈U

(L −

svc )

+

v∈V

= L|V | −

svc
X

v∈U

X

svc

+

v∈V

= L|V | −

svc

X

v∈V

= L|V | − sU [c]
167

X

svc

v∈U

svc

−

X

v∈U

svc

!

Meir, Procaccia, Rosenschein, & Zohar

Therefore, we have:
∀c, c′ ∈ C, ∀U ⊆ V,

s∗U [c] ≥ s∗U [c′ ] ⇔ sU [c] ≤ sU [c′ ].

Our final generic transformation lemma shows that the constructive and destructive
settings, i.e., including/excluding a distinguished candidate, are basically equivalent under
the three voting rules in question. A subtle point, that we shall deal with after we formulate
and prove the lemma, is that the lemma reverses the tie-breaking scheme: adversarial tiebreaking (as we have assumed so far) becomes friendly tie-breaking, i.e., ties are broken in
favor of candidates with higher utility; and vice versa.

Lemma 4.17. For every instance of Control by Adding (resp., Removing) Voters
with adversarial tie-breaking where W ⊆ C can be made to win by adding (resp., removing)
r voters, there is an instance of Control by Adding (resp., Removing) Voters with
friendly tie-breaking with the same candidate set C where C\W can be made to win by adding
(resp., removing) r voters. Similarly, friendly tie-breaking is transformed to adversarial tiebreaking. This statement holds in Approval, Bloc, and Cumulative Voting.

Proof. For clarity, we will prove the lemma for adding voters; the proof for removing voters
is practically identical. We will also prove the result for Cumulative voting (with a pool of
points of size L), but it is straightforward to extend it to Approval and Bloc, generally by
replacing L with 1.
Given an instance hV, V ′ , C, p, k, ri, let U ⊆ V ′ be a subset of r voters such that adding
these voters induces the set of winners W . Construct an instance of Control by Adding
Voters as follows: set the number of winners to be k ∗ = |C| − k, and set the pool of points
to be L∗ = L(|C| − 1). For each voter v ∈ V, V ′ we have a voter v ∗ who gives L − svc to
candidate c, where svc is the score given to c by the voter v. Define the set V ∗ (resp., V ′∗ )
as the set containing all such voters corresponding to voters in V (resp., V ′ ). Note that
∀v ∗ ∈ V ∗ ⊎ V ′∗ ,

X

c∈C

∗

svc =

X

L − svc = L|C| −

c∈C

X

svc = L|C| − L = L(|C| − 1) = L∗ .

c∈C

Denote by V̂ the final set of voters that results from adding the voters of U , and by V̂ ∗
the corresponding set of voters in the constructed instance (i.e., v ∗ ∈ V̂ ∗ ⇔ v ∈ V̂ ). Also
denote by s[c], ŝ[c] the score of each candidate c ∈ C in the original instance and in the new
instance, after adding the voters.
168

Complexity of Strategic Behavior in Multi-Winner Elections

As in previous proofs, we get that the candidates’ order has reversed. Formally, when
comparing the score of two candidates:
X ∗
X ∗
ŝ[c1 ] − ŝ[c2 ] =
ŝvc1 −
ŝvc2
v ∗ ∈V̂ ∗

=

X

v ∗ ∈V̂ ∗

(L − svc1 ) −

v∈V̂

X

v∈V̂

= |V̂ |L −

X

(svc1 ) − |V̂ |L +

v∈V̂

=

X

v∈V̂

(L − svc2 )

svc2

−

X

X

(svc2 )

v∈V̂

svc1

v∈V̂

= s[c2 ] − s[c1 ]
Therefore, since we assumed the tie-breaking scheme is reversed, the k candidates with
highest score in the original instance are the k candidates with the lowest score in the
constructed instance, and, moreover, C \ W form the k ∗ = m − k winners of the new
instance.7
Lemma 4.17 allows us to derive straightforward reductions between versions of Control
where the chairman wants to include/exclude a distinguished candidate: it is possible to
include a candidate in a given instance if and only if it is possible to exclude a candidate in
the constructed instance, and vice versa. The only point we have to address is the change
in the tie-breaking scheme. Fortunately, it is easy to obtain versions of Propositions 4.12,
4.13, and 4.15 under friendly tie-breaking, by slight modifications to the proofs. Hence, the
lemma, when applied to these three propositions, yields:
Proposition 4.18. Control by Removing Voters in Bloc, Approval, and Cumulative
voting is N P-complete, even when the chairman simply wants to exclude a distinguished
candidate.
Recall that Lemma 4.10 preserves the tie-breaking scheme. Thus, we can obtain a
“friendly” version of Proposition 4.16. Then, Lemma 4.17 applied to the “friendly” Proposition 4.16 gives us the final result of this section. Note that the constructive version (i.e.,
including a distinguished candidate) of Control by Adding Voters in Approval is already known to be N P-hard even for single winner elections (Hemaspaandra et al., 2007b).
Proposition 4.19. Control by Adding Voters in Approval (Hemaspaandra et al.,
2007b), Bloc, and Cumulative voting is N P-complete, even when the chairman simply wants
to include a distinguished candidate.
Here emerges an issue worth clarifying. As noted above, the constructive version (i.e.,
including a distinguished candidate) of Control by Adding/Removing Voters in Approval is already known to be N P-hard even for single winner elections (Hemaspaandra
et al., 2007b). On the other hand, Hemaspaandra et al. (2007b) show that the destructive
7. To be precise, candidates with utilities identical to those of C \ W are the winners; the names of the
winners may change, but this is irrelevant for our purposes.

169

Meir, Procaccia, Rosenschein, & Zohar

version of the same problems (i.e., adding/removing voters while excluding a distinguished
candidate) is in P when k = 1. Our surprising results imply that this is no longer the case
when the number of winners k is also a parameter.
At first it may appear that it follows from Lemma 4.17 that the constructive and destructive versions of this problem are equivalent, which would seem contradictory. A closer
examination reveals that there is no such conflict: if we apply the lemma to the constructive
result with k = 1, we obtain a reduction to the destructive version, but with k ∗ = |C| − 1.
4.2 Controlling the Set of Candidates
We now turn to the problem of controlling the set of candidates. As noted at the beginning
of this section, this problem is ill-defined when it comes to Cumulative voting, so in the
following we restrict ourselves to SNTV, Bloc voting, and Approval voting.
Another subtle point is that in the problem of destructive control by removing candidates
in single winner elections, it is assumed that the chairman cannot remove the distinguished
candidate p. There does not seem to be an elegant way to generalize this assumption to
multi-winner elections. Hence, we do not discuss control by removing candidates when the
goal is to exclude a distinguished candidate.
Now, we recall that Bartholdi et al. (1992) show that control by adding/removing candidates in SNTV is N P-complete, even in single winner elections (in particular, even when
the chairman wants to include a single candidate among the winners). Hemaspaandra et
al. (2007b) extended this result to destructive control. Crucially, since these results hold
for single winner elections, they also apply to Bloc voting, as Bloc voting and SNTV are
identical when k = 1. So, in fact, Bartholdi et al. and Hemaspaandra et al. together
imply that control by adding candidates in SNTV and Bloc is N P-complete, even when the
chairman just wants to include or exclude a distinguished candidate, and control by removing candidates in SNTV and Bloc is N P-complete, even when the chairman just wants to
include a distinguished candidate (again, we do not discuss the exclusion of a candidate).
Although Approval voting seems more complicated than SNTV (or even Bloc), surprisingly it is much easier to control by tampering with the set of candidates.
Proposition 4.20. Control by Adding Candidates in Approval is in P, under any
utility function.
Interestingly, in the single winner setting, Approval is immune to control by adding
candidates (Hemaspaandra et al., 2007b), i.e., it is not possible to get a candidate elected
by adding other candidates. However, in our multiple winner model it is clearly possible to
gain utility by adding candidates.
Proof of Proposition 4.20. We will actually solve the following problem: can the chairman
add exactly r candidates, in a way that all the added candidates become winners, and the
utility is at least t? Solving this problem entails that the chairman can also solve the original
problem, as he can simply run the algorithm for every r′ ≤ r.
First note that each candidate in c ∈ C, C ′ has a fixed number of points s[c], regardless
of the participation of any other candidate. The chairman selects a subset D ⊆ C ′ , |D| = r,
and the winners are the k candidates in C ⊎ D with the highest score. Therefore, it is only
effective to add candidates that will actually be winners.
170

Complexity of Strategic Behavior in Multi-Winner Elections

Say indeed that D ⊆ W , where W is the set of winners. Regardless of the identity of
the candidates in D, the winners from C are exactly the |C| − r candidates with the highest
score in C. Since r is fixed and (without loss of generality) r ≤ k, the utility of the winners
from C is also fixed, and we only need to optimize D, i.e., select the best r candidates from
C ′ that can be made to be winners.
Lemma 4.21. Let C = {c1 , . . . , cm } be sorted by nonincreasing score, then by nondecreasing
utility, i.e., s[cj ] ≥ s[cj+1 ] for all j = 1, . . . , m − 1, and if s[cj ] = s[cj+1 ] then u(cj ) ≤
u(cj+1 ). Let W be the set of winners after the candidates D ⊆ C ′ , |D| = r, have been
added. Then D ⊆ W if and only if for all d ∈ D, one of the following holds:
1. s[d] > s[ck−r+1 ].
2. s[d] = s[ck−r+1 ] and u(d) < u(ck−r+1 ).
Proof. Assume first that D ⊆ W . Among the k candidates with highest score, at least r
are not from C. Thus, at least r candidates among the highest-scoring k candidates in C
are excluded from W ; the candidates {ck−r+1 , . . . , ck } are certainly excluded from the set
of winners. Since candidates with lower score are excluded first, and equality is broken in
favor of candidates with lower utility, we have that either ∀c ∈ W , s[ck−r+1 ] < s[c], or an
equality holds and the utility of c is lower; in particular, this is true for any d ∈ D ⊆ W .
Conversely, suppose that for all d ∈ D, either condition 1 or condition 2 holds. Then
exactly k candidates are preferred (by the voting rule and the tie-breaking mechanism) to
ck−r+1 : {c1 , . . . , ck−r }, as well as the candidates in D. Thus these are the k winners.
Denote thresh(r) = s[ck−r+1 ]. By Lemma 4.21, it is sufficient to consider the candidates
{c ∈ C ′ : s[c] > thresh(r) or s[c] = thresh(r) ∧ u(c) < u(ck−r+1 )}.
Clearly, it is possible to achieve a utility of t if and only if this is accomplished by adding
the r candidates with highest utility in this set.
Proposition 4.22. Control by Removing Candidates in Approval is in P, under any
utility function.
Proof. This is actually an easier problem than the problem of control by adding candidates,
and we give a simpler algorithm: First, sort the candidates by decreasing score, sorting
candidates with tied scores by increasing utilities. Next, remove the r candidates with
lowest utility out of the first k + r candidates. Finally, check if the total utility of the
winner set reaches t.
All candidates ranked below position k + r cannot be elected and are therefore irrelevant
from the chairman’s perspective. From the additivity of the utility function, it is clear that
leaving the k candidates with highest utility maximizes the total utility of the chairman.
Thus, our simple algorithm is optimal, and if it fails then we can deduce that control is
impossible.
171

Meir, Procaccia, Rosenschein, & Zohar

5. Related Work
Academic interest in the complexity of manipulation in voting was initiated by Bartholdi,
Tovey and Trick’s (1989) seminal paper. The authors suggested that computational complexity may prevent manipulation in practice, and presented a voting rule, namely SecondOrder Copeland, which is hard to manipulate. The paper examined single-winner elections,
where the goal of the manipulator is to cast its vote in a way that makes a given candidate
win the election.
Bartholdi and Orlin (1991) later proved that the important Single Transferable Vote
(STV) rule is N P-hard to manipulate. STV is one of the prominent voting rules in the
literature on voting. It proceeds in rounds; in the first round, each voter votes for the
candidate it ranks first. In every subsequent round, the candidate with the least number of
votes is eliminated, and the votes of voters who voted for that candidate are transfered to
the next surviving candidate in their ranking.
Conitzer and Sandholm (2003) examined voting rules that are usually easy to manipulate
in single-winner elections, but induced hardness by introducing the notion of a preround.
In the preround, the candidates are paired; the candidates in each pair compete against
each other. The introduction of a preround can make an election N P-hard, #P-hard, or
PSPACE-hard, depending on whether the preround precedes, comes after, or is interleaved
with the voting rule, respectively. Elkind and Lipmaa (2005a) generalized this approach
using Hybrid voting rules, which are composed of several base voting rules.
Some authors also considered a setting where there is an entire coalition of manipulators.
In this setting, the standard formulation of the manipulation problem is as follows: we are
given a set of votes that have been cast, and a set of manipulators. In addition, all votes
are weighted, e.g., a voter with weight k counts as k voters voting identically. We are asked
whether the manipulators can cast their vote in a way that makes a specific candidate win
the election.
Conitzer, Sandholm, and Lang (2007) have shown that this problem is N P-hard in
a variety of voting rules. Indeed, in this setting the manipulators must coordinate their
strategies, on top of taking the weights into account, so manipulation is made much more
complicated. In fact, the problem is so complicated that the hardness results hold even
when the number of candidates is constant. Hemaspaandra and Hemaspaandra (2007)
generalized some of these last results by exactly characterizing the scoring rules in which
manipulation is N P-hard. Elkind and Lipmaa (2005b) have shown how to use one-way
functions to make coalitional manipulation hard.
However, some recent papers have argued that worst-case computational hardness may
not be sufficient to prevent manipulation. Indeed, although such hardness implies that
the problem has an infinite number of hard instances, it may still be the case that under reasonable real-world distributions over instances, the problem is easy to solve. This
theme was discussed by Procaccia and Rosenschein (2007b, 2007a, 2008), by Conitzer and
Sandholm (2006), and by Erdélyi et al. (2007).
The complexity of control by a chairman has received somewhat less attention, but
nevertheless much is already known. Bartholdi, Tovey and Trick (1992) have studied the
complexity of seven different types of control in two voting rules (and a single-winner setting): adding voters/candidates, deleting voters/candidates, partitioning the voters, and
172

Complexity of Strategic Behavior in Multi-Winner Elections

Manipulation
Rule
SNTV
Bloc
Approval
Cumulative

In/Ex candidate

P
P
P
P

Boolean utility

General utility

P
P
P
P

P
P
P
N P-c

⇐
⇐
⇐
⇐

⇐
⇐
⇐

Table 1: The complexity of manipulation in multi-winner elections
two types of candidate partitioning. The authors have reached the conclusion that different
voting rules differ significantly in terms of their resistance to control. Hemaspaandra and
Hemaspaandra (2007b) extended these results to destructive control, where the chairman
wants a candidate not to be elected.
Hemaspaandra, Hemaspaandra and Rothe (2007a) have contributed to this investigation by examining twenty different types of control. They showed that in the unique winner
setting, there is a voting rule which is resistant to all twenty types. However, it is unclear
whether one of the “natural” single-winner voting rules has this property of total resistance to manipulation (though some voting rules come close (Faliszewski, Hemaspaandra,
Hemaspaandra, & Rothe, 2007)).
Finally, our structured setting, where voters essentially have preferences over subsets of
candidates, is related to recent work on combinatorial voting (see, e.g., Lang (2007) and
the references listed there).
Note that this paper subsumes parts of Procaccia, Rosenschein and Zohar (2007) and
Meir, Procaccia and Rosenschein (2008).

6. Discussion
The analysis in this paper has focused on the complexity of manipulating and controlling four simple voting schemes which are often considered in the context of multi-winner
elections: SNTV, Bloc, Approval, and Cumulative voting. In our formulation of the computational problems, we have assumed the manipulator (or chairman) has some additive
utility function over the candidates. We distinguished three major cases: the manipulator wants one candidate to be included in the set of winners, or excluded from the set of
winners; the manipulator has a binary utility function; and the manipulator has a general
utility function.
At this point we wish to direct the reader’s attention to Table 1, which summarizes our
results regarding manipulation. A left (respectively right) implication arrow in the table
means that the result in the cell is implied by the result in the left (respectively right)
cell. For a general utility function, manipulation is hard in Cumulative voting, and easy
in the other three. The results about Cumulative voting, however, do not carry over when
the manipulator has a binary utility function. Another interesting result, which does not
appear above, is that when we restrict ourselves to boolean utility functions, any scoring
rule can be easily manipulated. This result is formally stated and proven in Appendix A.
173

Meir, Procaccia, Rosenschein, & Zohar

Control by Adding/Removing Voters
Rule
In/Ex candidate Boolean utility General utility
SNTV
P [2] ⇐
P
⇐
P
Bloc
N P-c
⇒ N P-c
⇒ N P-c
Approval
N P-c*
⇒ N P-c
⇒ N P-c
Cumulative
N P-c
⇒ N P-c
⇒ N P-c
Control by Adding/Removing Candidates
Rule
In/Ex candidate** Boolean utility General utility
SNTV
N P-c [1,2]
⇒ N P-c
⇒ N P-c
Bloc
N P-c [1,2]
⇒ N P-c
⇒ N P-c
Approval
P
⇐
P
⇐
P
Cumulative
Irrelevant
Irrelevant
Irrelevant
Table 2: The complexity of control by adding/removing voters/candidates in multi-winner
elections. * This result was known for including a candidate (Hemaspaandra et al.,
2007b), even in single-winner elections. However, when k = 1, destructive control
by adding/removing voters in Approval is tractable, while this is not the case
when k is a parameter. ** In the context of control by removing candidates,
we do not discuss the case of excluding a candidate. [1] Bartholdi et al. (1992).
[2] Hemaspaandra et al. (2007b).

It remains an open question which scoring rules (if any) are N P-hard to manipulate under
general utility functions.
Table 2 summarizes our results regarding control. Notice that with respect to control, all
results are true for the three types of utility functions that appear in the table. Our results
imply that control by adding or removing voters is easy in SNTV but hard in the other three
rules. Surprisingly, the situation has turned on its head when it comes to control by adding
or removing candidates: here it is Approval voting which is easy to control, while other
rules are hard. In short, there is no clear hierarchy of resistance to control. We conclude
that one has to adopt a voting rule ad hoc, depending on whether control by tampering
with the set of voters, or with the set of candidates, is the major concern.
Note that some types of control, such as partitioning the set of voters or the set of
candidates (Bartholdi et al., 1992), have not been investigated in this paper.
Finally, we wish to connect this work with the ongoing discussion of worst-case versus
average-case complexity of manipulation and control in elections. As mentioned in Section 5, a strand of recent research argues that worst-case hardness is not a strong enough
guarantee of resistance to strategic behavior, by showing that manipulation problems that
are known to be N P-hard are tractable according to different average-case notions (Conitzer
& Sandholm, 2006; Procaccia & Rosenschein, 2007a; Zuckerman et al., 2008; Erdélyi et al.,
2007). However, that research is still highly inconclusive. Therefore, worst-case complexity of manipulation and control remains an important benchmark for comparing different
174

Complexity of Strategic Behavior in Multi-Winner Elections

voting rules, and still inspires a considerable and steadily growing body of work (Conitzer
et al., 2007; Hemaspaandra et al., 2007a, 2007b; Faliszewski et al., 2007).

Acknowledgments
The authors wish to thank the anonymous JAIR referees for very helpful comments. This
work was partially supported by grant #898/05 from the Israel Science Foundation. Ariel
Procaccia is supported by the Adams Fellowship Program of the Israel Academy of Sciences
and Humanities.

Appendix A. Manipulating Scoring Rules
In Section 3, we have shown that SNTV and Bloc voting, which are both scoring rules, are
easy to manipulate under a general utility function. The next proposition establishes that
this is true for any scoring rule, under a boolean-valued utility function.
Proposition A.1. Let P be a scoring rule defined by the parameters α
~ = hα1 , . . . , αm i.
Manipulation in P with any boolean-valued utility function u : C → {0, 1} is in P.
Proof. Let α
~ = hα1 , . . . , αm i be the parameters of the scoring rule in question. Denote the
score of each candidate c ∈ C, before the manipulator has cast his vote, by s[c]. Let J be
the manipulator’s preference profile, given by:
J = c j 1 ≻ c j 2 ≻ . . . ≻ cj m
Suppose some candidate c ∈ C was ranked in place l by the manipulator, c = cjl . Denote
the final score of candidate c, according to the manipulator’s profile J , by:
sJ [c] = s[c] + αl
Finally, denote the winner set that results from the manipulator’s ballot J by WJ .
Lemma A.2. Given C ′ ⊆ C, |C| = k, it is possible to determine in polynomial time if
there exists J s.t. C ′ = WJ .
Proof. Denote C ′ = {c′1 . . . c′k },
C ′′ = C \ C ′ = {c′′1 , . . . c′′m−k },
where both C ′ , C ′′ are sorted by nondecreasing score s[c]. Let
J ∗ = c′1 ≻ c′2 ≻ . . . ≻ c′k ≻ c′′1 ≻ . . . ≻ c′′m−k
This preference profile ranks the players in C ′ first, while giving more points to candidates
with lower initial score. Candidates from C ′′ are ranked next, and the same rule applies.
The intuition is that we would like the candidates in C ′ to have a high-as-possible, more or
less balanced, score. Likewise, we would like the candidates in C ′′ to have a low-as-possible
balanced score. This strategy generalizes the algorithm of Bartholdi et al. (1989).
175

Meir, Procaccia, Rosenschein, & Zohar

We claim that there exists J s.t. C ′ = WJ if and only if C ′ = WJ ∗ . If C ′ = WJ ∗ then
obviously there exists J s.t. C ′ = WJ . Conversely, suppose there exists some J # such
that C ′ = WJ # . Without loss of generality, this holds (by the adversarial tie breaking
assumption)8 if and only if
∀c′ ∈ C ′ , c′′ ∈ C ′′ , sJ # [c′′ ] < sJ # [c′ ].

(5)

We argue that it is possible to obtain J ∗ from J # by iteratively transposing pairs of
candidates, without changing the winner set. Indeed, we distinguish between three cases:
1. ∃j1 , j2 ∈ {1, 2, . . . , k} such that s[c′j1 ] > s[c′j2 ] , but in J # it holds that c′j1 ≻ c′j2 .
Now, transpose the rankings of c′j1 and c′j2 in J # , i.e., consider the preference profile
which is identical to J # except that the places of c′j1 and c′j2 are switched. Denote
by W the new set of winners.
The score of c′j2 increased, so he is certainly still in W . Moreover, the new final
(possibly lower) score of c′j1 is:
s[c′j1 ] + αj2 ≥ s[c′j2 ] + αj2 = sJ # [c′j2 ]
By (5) we have that:
∀c′′ ∈ C ′′ , sJ # [c′′ ] < sJ # [c′j2 ]
Therefore, c′j1 ∈ W even after the transposition. We conclude that it still holds that
C′ = W .
2. ∃j1 , j2 ∈ {1, 2, . . . , m − k} such that s[c′′j1 ] > s[c′′j2 ], but in J # it holds that c′′j1 ≻ c′′j2 .
A similar argument holds in this case.
3. ∃c′ ∈ C ′ , c′′ ∈ C ′′ such that in J # it holds that c′′ ≻ c′ . Clearly the desirable candidate c′ can only rank higher if we transpose the two candidates.
Using the three types of transpositions, we can replace a couple of candidates at each
step until we obtain J ∗ from J # . In each such step it remains true that C ′ = W , thus
C ′ = WJ ∗ .
Lemma A.3. Given C ′ ⊆ C, C ′ ≤ k, it is possible to determine in polynomial time if there
exists J s.t. C ′ ⊆ WJ .
Proof. Let C ′ ⊆ C, |C ′ | = k ′ < k. We add to C ′ the k − k ′ candidates from C ′′ with the
highest score (according to s[c]), and denote this new set of size k by C ∗ . According to
Lemma A.2, we can determine efficiently if there exists J such that C ∗ = WJ .
We argue that it is enough to check C ∗ . Indeed, assume that there ∃J such that
′
C ⊆ WJ . Let c ∈ C \ WJ such that there exists c′ ∈ WJ with s[c′ ] < s[c]. Now, if we
transpose, in the ranking J , the candidates c and c′ , clearly c becomes a winner while c′
becomes a loser. Therefore, it is possible to make C ∗ the set of winners.
8. Tie breaking works against candidates with utility 1 (which are the ones we ultimately care about), but
in favor of candidates in C ′ with utility 0. However, for ease of exposition, we do not deal with such
borderline cases here.

176

Complexity of Strategic Behavior in Multi-Winner Elections

To complete the proof of the proposition, we denote by D the set of candidates whose
utility is 1. The total utility is at least t if and only if there is a subset of D of size t that can
be included in W . Let C ′ be the t candidates with the highest score s[c] in D. By similar
arguments as before, if this subset cannot win then no other subset of D of size t can. By
Lemma A.3 we can efficiently find out whether it is possible to include C ′ in W .

References
Bartholdi, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice and Welfare, 8, 341–354.
Bartholdi, J., Tovey, C. A., & Trick, M. A. (1989). The computational difficulty of manipulating an election. Social Choice and Welfare, 6, 227–241.
Bartholdi, J., Tovey, C. A., & Trick, M. A. (1992). How hard is it to control an election.
Mathematical and Computer Modelling, 16, 27–40.
Brams, S. J., & Fishburn, P. C. (2002). Voting procedures. In Arrow, K. J., Sen, A. K., &
Suzumura, K. (Eds.), Handbook of Social Choice and Welfare, chap. 4. North-Holland.
Conitzer, V., & Sandholm, T. (2003). Universal voting protocol tweaks to make manipulation hard. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 781–788.
Conitzer, V., & Sandholm, T. (2006). Nonexistence of voting rules that are usually hard to
manipulate. In Proceedings of the 21st National Conference on Artificial Intelligence
(AAAI), pp. 627–634.
Conitzer, V., Sandholm, T., & Lang, J. (2007). When are elections with few candidates
hard to manipulate?. Journal of the ACM, 54, 1–33.
Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness or shared
beliefs: Gibbard-Satterthwaite generalized. Social Choice and Welfare, 17 (1), 85–93.
Elkind, E., & Lipmaa, H. (2005a). Hybrid voting protocols and hardness of manipulation.
In 16th Annual International Symposium on Algorithms and Computation, Lecture
Notes in Computer Science, pp. 206–215. Springer-Verlag.
Elkind, E., & Lipmaa, H. (2005b). Small coalitions cannot manipulate voting. In International Conference on Financial Cryptography, Lecture Notes in Computer Science.
Springer-Verlag.
Endriss, U. (2007). Vote manipulation in the presence of multiple sincere ballots. In Proceedings of the 11th Conference on Theoretical Aspects of Rationality and Knowledge
(TARK), pp. 125–134.
Ephrati, E., & Rosenschein, J. S. (1997). A heuristic technique for multiagent planning.
Annals of Mathematics and Artificial Intelligence, 20, 13–67.
Erdélyi, G., Hemaspaandra, L. A., Rothe, J., & Spakowski, H. (2007). On approximating optimal weighted lobbying, and frequency of correctness versus average-case polynomial
time. Tech. rep., arXiv:cs/0703097v1.
177

Meir, Procaccia, Rosenschein, & Zohar

Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Llull and
Copeland voting broadly resist bribery and control. In Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI), pp. 724–730.
Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting for movies: the anatomy of
a recommender system. In Proceedings of the 3rd Annual Conference on Autonomous
Agents (AGENTS), pp. 434–435.
Gibbard, A. (1973). Manipulation of voting schemes. Econometrica, 41, 587–602.
Haynes, T., Sen, S., Arora, N., & Nadella, R. (1997). An automated meeting scheduling system that utilizes user preferences. In Proceedings of the 1st International Conference
on Autonomous Agents (AGENTS), pp. 308–315.
Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy for voting systems. Journal of
Computer and System Sciences, 73 (1), 73–83.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007a). Hybrid elections broaden
complexity-theoretic resistance to control. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI), pp. 1308–1314.
Hemaspaandra, E., Hemaspaandra, L. A., & Rothe, J. (2007b). Anyone but him: The
complexity of of precluding an alternative. Artificial Intelligence, 171 (5–6), 255–285.
Kellerer, H., Pferschy, U., & Pisinger, D. (2004). Knapsack Problems. Springer.
Lang, J. (2007). Vote and aggregation in combinatorial domains with structured preferences.
In Proceedings of the 20th International Joint Conference on Artificial Intelligence
(AAAI), pp. 1366–1371.
Meir, R., Procaccia, A. D., & Rosenschein, J. S. (2008). A broader picture of the complexity
of strategic behavior in multi-winner elections. In Proceedings of the 7th International
Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 991–
998.
Procaccia, A. D., Rosenschein, J. S., & Zohar, A. (2007). Multi-winner elections: Complexity of manipulation, control and winner-determination. In Proceedings of the 20th
International Joint Conference on Artificial Intelligence (IJCAI), pp. 1476–1481.
Procaccia, A. D., & Rosenschein, J. S. (2007a). Average-case tractability of manipulation in
elections via the fraction of manipulators. In Proceedings of the 6th International Joint
Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 718–720,
Honolulu, Hawaii.
Procaccia, A. D., & Rosenschein, J. S. (2007b). Junta distributions and the average-case
complexity of manipulating elections. Journal of Artificial Intelligence Research, 28,
157–181.
Satterthwaite, M. (1975). Strategy-proofness and Arrow’s conditions: Existence and correspondence theorems for voting procedures and social welfare functions. Journal of
Economic Theory, 10, 187–217.
Zuckerman, M., Procaccia, A. D., & Rosenschein, J. S. (2008). Algorithms for the coalitional
manipulation problem. In Proceedings of the 19th ACM-SIAM Symposium on Discrete
Algorithms (SODA), pp. 277–286, San Francisco, California.

178

Journal of Artificial Intelligence Research 33 (2008) 403-432

Submitted 07/08; published 11/08

The Computational Complexity of Dominance and
Consistency in CP-Nets
Judy Goldsmith

GOLDSMIT @ CS . UKY. EDU

Department of Computer Science
University of Kentucky
Lexington, KY 40506-0046, USA

Jérôme Lang

LANG @ IRIT. FR

IRIT
Université de Toulouse, UPS
31062 Toulouse Cedex, France

Miroslaw Truszczyński

MIREK @ CS . UKY. EDU

Department of Computer Science
University of Kentucky
Lexington, KY 40506-0046, USA

Nic Wilson

N . WILSON @4 C . UCC . IE

Cork Constraint Computation Centre
University College
Cork, Ireland

Abstract
We investigate the computational complexity of testing dominance and consistency in CP-nets.
Previously, the complexity of dominance has been determined for restricted classes in which the
dependency graph of the CP-net is acyclic. However, there are preferences of interest that define
cyclic dependency graphs; these are modeled with general CP-nets. In our main results, we show
here that both dominance and consistency for general CP-nets are PSPACE-complete. We then
consider the concept of strong dominance, dominance equivalence and dominance incomparability,
and several notions of optimality, and identify the complexity of the corresponding decision problems. The reductions used in the proofs are from STRIPS planning, and thus reinforce the earlier
established connections between both areas.

1. Introduction
The problems of eliciting, representing and computing with preferences over a multi-attribute domain arise in many fields such as planning, design, and group decision making. However, in a
multi-attribute preference domain, such computations may be nontrivial, as we show here for the
CP-net representation. Natural questions that arise in a preference domain are, “Is this item preferred to that one?”, and “Is this set of preferences consistent?” More formally, a set of preferences
is consistent if and only if no item is preferred to itself. We assume that preferences are transitive,
i.e., if α is preferred to β, and β is preferred to γ, then α is preferred to γ.
An explicit representation of a preference ordering of elements, also called outcomes, of such
multi-variable domains is exponentially large in the number of attributes. Therefore, AI researchers
have developed languages for representing preference orderings in a succinct way. The formalism
of CP-nets (Boutilier, Brafman, Hoos, & Poole, 1999) is among the most popular ones. A CP-net
c
2008
AI Access Foundation. All rights reserved.

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

provides a succinct representation of preference ordering on outcomes in terms of local preference
statements of the form p : xi > x j , where xi , x j are values of a variable X and p is a logical condition.
Informally, a preference statement p : xi > x j means that given p, xi is strictly preferred to x j ceteris
paribus, that is, all other things being equal. The meaning of a CP-net is given by a certain ordering relation, called dominance, on the set of outcomes, derived from such reading of preference
statements. If one outcome dominates another, we say that the dominant one is preferred.
Reasoning about the preference ordering (dominance relation) expressed by a CP-net is far from
easy. The key problems include dominance testing and consistency testing. In the first problem,
given a CP-net and two outcomes α and β, we want to decide whether β dominates α. The second
problem asks whether there is a dominance cycle in the dominance ordering defined by an input
CP-net, that is, whether there is an outcome that dominates (is preferred to) itself.
We study the computational complexity of these two problems. The results obtained prior to this
work concerned only restricted classes of CP-nets, all requiring that the graph of variable dependencies implied by preference statements in the CP-net be acyclic. Under certain assumptions, the
dominance-testing problem is in NP and, under some additional assumptions, even in P (Domshlak
& Brafman, 2002; Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004a). We show that the complexity in the general case is PSPACE-complete, and this holds even for the propositional case, by
exhibiting in Section 4 a PSPACE-hardness proof for dominance testing.
We then turn to consistency testing. While acyclic CP-nets are guaranteed to be consistent, this
is not the case with general CP-nets (Domshlak & Brafman, 2002; Brafman & Dimopoulos, 2004).
In Section 5, we show that consistency testing is as hard as dominance testing.
In the following two sections we study decision problems related to dominance and optimality
in CP-nets. First, we consider the complexity of deciding strict dominance, dominance equivalence
and dominance incomparability of outcomes in a CP-net. Then, we study the complexity of deciding
the optimality of outcomes, and the existence of optimal outcomes, for several notions of optimality.
To prove the hardness part of the results, we first establish the PSPACE-hardness of some problems related to propositional STRIPS planning. We then show that these problems can be reduced
to CP-net dominance and consistency testing by exploiting connections between actions in STRIPS
planning and preference statements in CP-nets.
The complexity results in this paper address CP-nets whose dominance relation may contain
cycles. Most earlier work has concentrated on the acyclic model. However, as argued earlier, for
instance by Domshlak and Brafman (2002), acyclic CP-nets are not sufficiently expressive to capture human preferences on even some simple domains.1 Consider, for instance, a diner who has
to choose either red or white wine, and either fish or meat. Given red wine, they prefer meat, and
conversely, given meat they prefer red wine. On the other hand, given white wine, they prefer fish,
and conversely, given fish they prefer white wine. This gives a consistent cyclic CP-net, and there is
no acyclic CP-net giving rise to the same preferences on outcomes. So, such cyclicity of preference
variables does not necessarily lead to a cyclic order on outcomes.

1. We do not mean to say that cyclic CP-nets are sufficient to capture all possible human preferences on simple domains
– this is obviously not true. However, we note that every preference relation extends the preference relation induced
by some CP-net with possibly cyclic dependencies. Not only is this property no longer true when cyclic dependencies
are precluded but, in the case of binary variables, the number of linear orders that extends some acyclic CP-net is
exponentially smaller than the number of all linear orders (Xia, Conitzer, & Lang, 2008).

404

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

We assume some familiarity with the complexity class PSPACE. We refer to Papadimitriou
(1994) for details. In particular, we later use the identities NPSPACE = PSPACE = coPSPACE.
In several places, we will consider versions of decision problem, in which input instances are
assumed to have some additional property. Such problems are usually formulated in the following
way: “Q, given R”2 . We first note that “Q, given R” is not the same problem as “Q and R”. Let
us recall the definition of a decision problem as presented by Ausiello et al. (1999). A decision
problem is a pair P = hIP ,YP i where IP is a set of strings (formally, a subset of Σ∗ , where Σ is a
finite alphabet), The decision problem P = hIP ,YP i reads as follows: given a string x ∈ IP , decide
whether x ∈ YP . A problem hIP ,YP i is in a complexity class C if the language YP ⊆ Σ∗ is in C (this
does not depend on IP ). A problem hIQ ,YQ i is reducible to hIP ,YP i if there is a polynomial-time
function F such that (1) for every x ∈ IQ , F(x) ∈ IP , and (2) for every x ∈ IQ , x ∈ YQ if and only
if F(x) ∈ YP . Thus, if P is the decision problem “Q, given R”, then IP is the set of all strings
satisfying R, while YP is the set of all strings satisfying R ∩ Q. For all such problems, it is granted
that the input belongs to R; to solve them we do not have to check that the input string is indeed
an element of R. Such problems “Q, given R” are widespread in the literature. However, in most
cases, R is a very simple property, that can be checked in polynomial (and often linear) time, such
as “decide whether a graph possesses a Hamiltonian cycle, given that every vertex has a degree at
most 3”. Here, however, we will consider several problems “Q, given R” where R itself is not in the
class P (unless the polynomial hierarchy collapses). However, as we said above, the complexity of
recognizing whether a given string is in R does not matter. In other words, the complexity of “Q,
given R” is the same, whether R can be recognized in unit time or is PSPACE-complete. We will
come back to this when the first such problem appears in the paper (cf. the proof of Proposition 5).
In no case that we consider is the complexity of R greater than the complexity of Q.
A part of this paper (up to Section 5) is an extended version of our earlier conference publication
(Goldsmith, Lang, Truszczyński, & Wilson, 2005). Sections 6 and 7 are entirely new.

2. Generalized Propositional CP-Nets
Let V = {x1 , . . . , xn } be a finite set of variables. For each variable x ∈ V , we assume a finite domain
Dx of values. An outcome is an n-tuple (d1 , . . . , dn ) of Dx1 × · · · × Dxn .
In this paper, we focus on propositional variables: variables with binary domains. Let V be a
finite set of propositional variables. For every x ∈ V , we set Dx = {x, ¬x} (thus, we overload the
notation and write x both for the variable and for one of its values). We refer to x and ¬x as literals.
Given a literal l we write ¬l to denote the dual literal to l. The focus on binary variables makes the
presentation clearer and has no impact on our complexity results.
We also note that in the case of binary domains, we often identify an outcome with the set of
its values (literals). In fact, we also often identify such sets with the conjunctions of their elements.
Sets (conjunctions) of literals corresponding to outcomes are consistent and complete.
A conditional preference rule (sometimes, a preference rule or just a rule) over V is an expression p : l > ¬l, where l is a literal of some atom x ∈ V and p is a propositional formula over V that
does not involve variable x.
2. In the literature one often finds the following formulation: “Q, even if R”, which does not have exactly the same
meaning as “Q, given R”. Specifically, when saying “Q is NP-complete, even if R”, one means “Q is NP-complete,
and Q, given R is NP-complete as well”.

405

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

In the rest of the paper, we need to refer to two different languages: a conditional preference
language where for every (binary) variable x, the conditional preference table for x needs to specify
a preferred value of x for every possible assignment of its parent variables, and a more general
language where the tables may be incomplete (for some values of its parents, the preferred value of
x may not be specified) and/or locally inconsistent (for some values of its parents, the table may both
contain the information that x is preferred and the information that ¬x is preferred). We call these
languages respectively CP-nets and GCP-nets (for “generalized CP-nets”). Note that GCP-nets are
not new, as similar structures have been discussed before (Domshlak, Rossi, Venable, & Walsh,
2003). The reason why we use this terminology (“CP-nets” and “GCP-nets”) is twofold. First, even
if the assumptions of completeness and local consistency for CP-nets are sometimes relaxed, most
papers on CP-nets do make them. Second, we could have used “CP-nets” and “locally consistent,
complete CP-nets” instead of “GCP-nets” and “CP-nets”, but we felt our notation is simpler and
more transparent.
Definition 1 (Generalized CP-net) A generalized CP-net C (for short, a GCP-net) over V is a
set of conditional preference rules. For x ∈ V we define pC+ (x) and pC− (x), usually written just:
p+ (x) and p− (x), as follows: pC+ (x) is equal to the disjunction of all p such that there exists a rule
p : x > ¬x in C; pC− (x) is the disjunction of all p such that there exists a rule p : ¬x > x in C. We
define the associated directed graph GC (the dependency graph) over V to consist of all pairs (y, x)
of variables such that y appears in either p+ (x) or p− (x).
In our complexity results we will also need the following representation of GCP-nets: a GCPnet C is said to be in conjunctive form if C only contains rules p : l > ¬l such that p is a (possibly
empty) conjunction of literals. In this case all formulas p− (x), p+ (x) are in disjunctive normal form,
that is, a disjunction of conjunctions of literals (including ⊤ – the empty conjunction of literals).
GCP-nets determine a transitive relation on outcomes, interpreted in terms of preference. A
preference rule p : l > ¬l represents the statement “given that p holds, l is preferred to ¬l ceteris
paribus”. Its intended meaning is as follows. If outcome β satisfies p and l, then β is preferred to
the outcome α which differs from β only in that it assigns ¬l to variable x. In this situation we say
that there is an improving flip from α to β sanctioned by the rule p : l > ¬l.
Definition 2 If α0 , . . . , αm is a sequence of outcomes with m ≥ 1 and each next outcome in the
sequence is obtained from the previous one by an improving flip, then we say that α0 , . . . , αm is an
improving sequence from α0 to αm for the GCP-net, and that αm dominates α0 , written α0 ≺ αm .
Finally, a GCP-net is consistent if there is no outcome α which is strictly preferred to itself, that
is, such that α ≺ α.
The main objective of the paper is to establish the complexity of the following two problems
concerning the notion of dominance associated with GCP-nets (sometimes under restrictions on the
class of input GCP-nets).
Definition 3
GCP - DOMINANCE : given a GCP-net C and two outcomes α and β, decide whether α ≺ β in C, that
is, whether β dominates α in C.
GCP - CONSISTENCY : given a GCP-net C, decide whether C is consistent.
406

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

GCP-nets extend the notion of CP-nets (Boutilier et al., 1999). There are two properties of
GCP-nets that are essential in linking the two notions.
Definition 4
A GCP-net C over V is locally consistent if for every x ∈ V , the formula pC− (x) ∧ pC+ (x) is unsatisfiable. It is locally complete if for every x ∈ V , the formula pC− (x) ∨ pC+ (x) is a tautology.
Informally, local consistency means that there is no outcome in which both x is preferred over
¬x and ¬x is preferred over x. Local completeness means that, for every variable x, in every outcome
either x is preferred over ¬x or ¬x is preferred over x.
Definition 5 (Propositional CP-net) A CP-net over the set V of (propositional) variables is a locally consistent and locally complete GCP-net over V .
It is not easy to decide whether a GCP-net is actually a CP-net. In fact, the task is coNPcomplete.
Proposition 1 The problem of deciding, given a GCP-net C, whether C is a CP-net is coNPcomplete.
Proof: Deciding whether a GCP-net C is a CP-net consists of checking local consistency and local
completeness. Each of these tasks amounts to n validity tests (one for each variable). It follows that
deciding whether a GCP-net is a CP-net is the intersection of 2n problems from coNP. Hence, it is
in coNP, itself. Hardness comes from the following reduction from UNSAT. To any propositional
formula ϕ we assign the CP-net C(ϕ), defined by its set of variables Var(ϕ)∪{z}, where z 6∈ Var(ϕ),
and the following tables:
−
+
• for any variable x 6= z: pC(ϕ)
(x) = ⊤; pC(ϕ)
(x) = ⊥;
−
+
(z) = ⊥.
(z) = ¬ϕ; pC(ϕ)
• pC(ϕ)
−
−
+
+
(z) = ⊥. There(z) ∧ pC(ϕ)
(x) = ⊥; moreover, pC(ϕ)
(x) ∧ pC(ϕ)
For any variable x 6= z, we have pC(ϕ)
−
+
fore, C(ϕ) is locally consistent. Now, for any variable x 6= z, we have pC(ϕ) (x) ∨ pC(ϕ)
(x) = ⊤.
−
+
Moreover, pC(ϕ) (z) ∨ pC(ϕ) (z) = ¬ϕ. Thus, C(ϕ) is locally complete if and only if ϕ is unsatisfiable.
It follows that C(ϕ) is a CP-net if and only if ϕ is unsatisfiable.


Many works on CP-nets make use of explicit conditional preference tables that list every combination of values of parent variables (variables on which x depends) exactly once, each such combination designating either x or ¬x as preferred.3 Clearly, CP-nets in this restricted sense can be
regarded as CP-nets in our sense that, for every variable x, satisfy the following condition:
if y1 , . . . , yk are all the atoms appearing in p+ (x) and p− (x) then every complete and
consistent conjunction of literals over {y1 , . . . , yn } appears as a disjunct in exactly one
of p+ (x) and p− (x).
3. There are exceptions. Some are discussed for instance by Boutilier et al. (2004a) in Section 6 of their paper.

407

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Under this embedding, the concepts of dominance and consistency we introduced here for GCP-nets
generalize the ones considered for CP-nets as defined by Boutilier et al. (2004a).
Problems CP - DOMINANCE and CP - CONSISTENCY are defined analogously to Definition 3. In
the paper we are interested in the complexity of dominance and consistency problems for both GCPnets and CP-nets. Therefore, the matter of the way in which these nets (especially CP-nets, as for
GCP-nets there are no alternative proposals) are represented is important. Our representation of
CP-nets is often more compact than the one proposed by Boutilier et al. (2004a), as the formulas
p+ (x) and p− (x) implied by the conditional preference tables can often be given equivalent, but
exponentially smaller, disjunctive normal form representations. Thus, when defining a decision
problem, it is critical to specify the way to represent its input instances, as the representation may
affect the complexity of the problem. Unless stated otherwise, we assume that GCP-nets (and thus,
CP-nets) are represented as a set of preference rules, as described in Definition 1. Therefore, the
size of a GCP-net is given by the total size of the formulas p− (x), p+ (x), x ∈ V .
We now note a key property of consistent GCP-nets, which we will use several times later in the
paper.
Proposition 2 If a GCP-net C is consistent then it is locally consistent.
Proof: If C is not locally consistent then there exists a variable x and an outcome α satisfying
pC− (x) ∧ pC+ (x). Then α ≺ α can be shown by flipping x from its current value in α to the dual value
and then flipping it back: since α satisfies pC− (x) ∧ pC+ (x), and since pC− (x) ∧ pC+ (x) does not involve
any occurrences of x, both flips are allowed.

Finally, we conclude this section with an example illustrating the notions discussed above.
Example 1 Consider a GCP-net C on variables V = {x, y} with four rules, defined as follows:
x : y > ¬y; ¬x : ¬y > y; y : ¬x > x; ¬y : x > ¬x. We have p+ (y) = x, p− (y) = ¬x, p+ (x) = ¬y and
p− (x) = y. Therefore C is locally consistent and locally complete, and so is a CP-net.
There is a cycle of dominance between outcomes: x ∧ y ≺ ¬x ∧ y ≺ ¬x ∧ ¬y ≺ x ∧ ¬y ≺ x ∧ y,
and so C is inconsistent. This shows that consistency is a strictly stronger property than local
consistency.

3. Propositional STRIPS Planning
In this section we derive some technical results on propositional STRIPS planning which form the
basis of our complexity results in Sections 4 and 5. We establish the complexity of plan existence
problems for propositional STRIPS planning under restrictions on input instances that make the
problem of use in the studies of dominance and consistency in GCP-nets.
Let V be a finite set of variables. A state over V is a complete and consistent set of literals over
V , which we often view as the conjunction of its members. A state is therefore equivalent to an
outcome, defined in a CP-nets context.
Definition 6 (Propositional STRIPS planning) By a propositional STRIPS instance we mean a
tuple hV, α0 , γ, ACTi, where
1. V is a finite set of propositional variables;
408

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

2. α0 is a state over V , called the initial state;
3. γ is a state called the goal;4
4. ACT is a finite set of actions, where each action a ∈ ACT is described by a consistent conjunction of literals pre(a) (a precondition) and a consistent conjunction of literals post(a) (a
postcondition, or effect).5
An action a is executable in a state α if α |= pre(a). The effect of a in state α, denoted by eff (a, α),
is the state α′ containing the same literals as α for all variables not mentioned in post(a), and
the literals of post(a). We assume that an action can be applied to any state, but that it does not
change the state if its preconditions do not hold: if α 6|= pre(a) (given that states are complete,
this is equivalent to α |= ¬pre(a)) then eff (a, α) = α. This assumption has no influence as far as
complexity results are concerned.
The PROPOSITIONAL STRIPS PLAN EXISTENCE problem, or STRIPS PLAN for short, is to decide whether for a given propositional STRIPS instance hV, α0 , γ, ACTi there is a finite sequence
of actions leading from the initial state α0 to the final state γ. Each such sequence is a plan for
hV, α0 , γ, ACTi. A plan is irreducible if every one of its actions changes the state.
We assume, without loss of generality, that for any action a, no literal in post(a) appears also
in pre(a); otherwise we can omit the literal from post(a) without changing the effect of the action;
if post(a) then becomes an empty conjunction, the action a can be omitted from ACT as it has no
effect.
We have the following result due to Bylander (1994).
Proposition 3 (Bylander, 1994)

STRIPS PLAN

is PSPACE-complete.

Typically, propositional STRIPS instances do not require that goals be states. Instead, goals are
defined as consistent conjunctions of literals that do not need to be complete. In such a setting, a
plan is a sequence of actions that leads from the start state to a state in which the goal holds. We
restrict consideration to complete goals. This restriction has no effect on the complexity of the plan
existence problem: it remains PSPACE-complete under the goal-completeness restriction (Lang,
2004).
3.1 Acyclic STRIPS
Definition 7 (Acyclic sets of actions) A set of actions ACT (we use the same notation as in Definition 6) is acyclic if there is no state α such that hV, α, α, ACTi has a non-empty irreducible plan,
that is to say, if there are no non-trivial directed cycles in the graph on states induced by ACT.
We will now establish the complexity of the following problem:
ACTION - SET ACYCLICITY :

given a set ACT of actions, decide whether ACT is acyclic.

Proposition 4
ACTION - SET ACYCLICITY is PSPACE-complete.
4. Note that in standard STRIPS the goal can be a partial state. This point is discussed just after Proposition 3.
5. We emphasize that we allow negative literals in preconditions and goals. Some definitions of STRIPS do not allow
this. This particular variant of STRIPS is sometimes called PSN (propositional STRIPS with negation) in the literature.

409

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Proof: The argument for the membership in PSPACE is standard; we nevertheless give some details.
We will omit such details for further proofs of membership in PSPACE. The following nondeterministic algorithm decides that ACT has a cycle:
guess α0 ;
α := α0 ;
repeat
guess an action a ∈ ACT ;
α′ := eff (a, α);
α := α′
until α = α0 .
This algorithm works in nondeterministic polynomial space (because we only need to store α0 ,
α and α′ ), which shows that ACTION - SET ACYCLICITY is in NPSPACE, and therefore in PSPACE,
since NPSPACE = PSPACE. Thus, ACTION - SET ACYCLICITY is in coPSPACE, hence in PSPACE,
since coPSPACE = PSPACE.
We will now show that the complement of the ACTION - SET ACYCLICITY problem is PSPACEhard by reducing the ACYCLIC STRIPS PLAN problem to it.
Let PE = hV, α0 , γ, ACTi be an instance of the ACYCLIC STRIPS PLAN problem. In particular,
we have that ACT is acyclic. Let a be a new action defined by pre(a) = γ and post(a) = α0 . It is easy
to see that ACT ∪ {a} is not acyclic if and only if there exists a plan for PE. Thus, the PSPACEhardness of the complement of the ACTION - SET ACYCLICITY problem follows from Proposition
5. Consequently, the ACTION - SET ACYCLICITY problem is coPSPACE-hard. Since PSPACE =
coPSPACE, the ACTION - SET ACYCLICITY problem is PSPACE-hard, as well.

Next, we consider the STRIPS planning problem restricted to instances that have acyclic sets of
actions. Formally, we consider the following problem:
ACYCLIC STRIPS PLAN : Given a propositional STRIPS instance hV, α0 , γ, ACTi such
that ACT is acyclic and α0 6= γ, decide whether there is a plan for hV, α0 , γ, ACTi

This is the first of our problems of the form “Q, given R” that we encounter and it illustrates
well the concerns we discussed at the end of the introduction. Here, R is the set of all propositional
STRIPS instances hV, α0 , γ, ACTi such that ACT is acyclic, and Q is the set of all such instances for
which there is a plan for hV, α0 , γ, ACTi. Checking whether a given propositional STRIPS instance
is actually acyclic is itself PSPACE-complete (this is what Proposition 4 states), but this does not
matter when it comes to solving ACYCLIC STRIPS PLAN: when considering an instance of ACYCLIC
STRIPS PLAN , we already know that it is acyclic (and this is reflected in the reduction below).
Proposition 5
ACYCLIC STRIPS PLAN

is PSPACE-complete.

Proof: The argument for the membership in PSPACE is standard (cf. the proof of Proposition 4). To
prove PSPACE-hardness, we first exhibit a polynomial-time reduction F from STRIPS PLAN. Let
PE = hV, α0 , γ, ACTi be an instance of STRIPS PLAN. The idea behind the reduction is to introduce
a counter, so that each time an action is executed, the counter is incremented. The counter may
count up to 2n , where n = |V |, making use of n additional variables. The counter is initialized to
410

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

0. Once it reaches 2n − 1 it can no longer be incremented and no action can be executed. Hence,
the set of actions in the resulting instance of STRIPS PLAN is acyclic: we are guaranteed to produce
an instance of R. To describe the reduction, we write V as {x1 , . . . , xn }. We define F(PE) = PE′ =
hV ′ , α′0 , γ′ , ACT ′ i as follows:
• V ′ = {x1 , . . . , xn , z1 , . . . , zn }, where zi are new variables we will use to implement the counter;
• α′0 = α0 ∧ ¬z1 ∧ · · · ∧ ¬zn ;
• γ′ = γ ∧ z1 ∧ · · · ∧ zn ;
• for each action a ∈ ACT, we include in ACT ′ n actions ai , 1 ≤ i ≤ n, such that:

pre(ai ) = pre(a) ∧ ¬zi ∧ zi+1 ∧ · · · ∧ zn
– for i ≤ n − 1 :
post(ai ) = post(a) ∧ zi ∧ ¬zi+1 ∧ · · · ∧ ¬zn , and

pre(an ) = pre(a) ∧ ¬zn
– for i = n :
post(an ) = post(a) ∧ zn .
• Furthermore, we include in ACT ′ n actions bi , 1 ≤ i ≤ n, such that:

pre(bi ) = ¬zi ∧ zi+1 ∧ · · · ∧ zn
– for i ≤ n − 1 :
post(bi ) = zi ∧ ¬zi+1 ∧ · · · ∧ ¬zn , and

pre(bn ) = ¬zn
– for i = n :
post(bn ) = zn .
We will denote states over V ′ by pairs (α, k), where α is a state over V and k is an integer, 0 ≤
k ≤ 2n − 1. We view k as a compact representation of a state over variables z1 , . . . , zn : assuming that
the binary representation of k is d1 . . . dn (with dn being the least significant digit), k represents the
state which contains zi if di = 1 and ¬zi , otherwise. For instance, let V = {x1 , x2 , x3 }. Then we have
V ′ = {x1 , x2 , x3 , z1 , z2 , z3 }, and the state ¬x1 ∧ x2 ∧ x3 ∧ z1 ∧ ¬z2 ∧ z3 is denoted by (¬x1 ∧ x2 ∧ x3 , 5).
We note that the effect of ai or bi on state (α, k) is either void, or increments the counter:
eff (ai , (α, k)) =



(eff (a, α), k + 1) if ai is executable in (α, k)
(α, k)
otherwise

eff (bi , (α, k)) =



(α, k + 1) if bi is executable in (α, k)
(α, k)
otherwise

Next, we remark that at most one ai and at most one bi are executable in a given state (α, k).
More precisely,
• if k < 2n − 1, then exactly one bi is executable in (α, k); denote by i(k) the index such that bi(k)
is executable in (α, k) (this index depends only on k). We also have that ai(k) is executable in
(α, k), provided that a is executable in α.
• if k = 2n − 1, then no ai and no bi is executable in (α, k).
411

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Now we show that PE′ is acyclic. Assume π is an irreducible plan for hV ′ , α′ , α′ , ACT ′ i. Let
= (α, k). If k < 2n − 1, then π is empty, since any action in ACT ′ in any state either is nonexecutable or increments the counter, and an irreducible plan contains only actions whose effect is
non-void. If k = 2n − 1, then no action of ACT ′ is executable in α′ and again π is empty. Thus, there
exists no non-empty irreducible plan for hV ′ , α′ , α′ , ACT ′ i, and this holds for all α′ . Therefore PE′
is acyclic.
We now claim that there is a plan for PE if and only if there is a plan for PE′ . First, assume that
there is a plan in PE. Let π be a shortest plan in PE and let m be its length (the number of actions
used). We have m ≤ 2n − 1, since no state along π repeats (otherwise, shorter plans than π for PE
would exist). Let α0 , α1 , . . . , αm = γ be the sequence of states obtained by executing π. Let a be the
action used in the transition from αk to αk+1 . Since k < 2n − 1 (because m ≤ 2n − 1 and k ≤ m − 1),
there is exactly one i, 1 ≤ i ≤ n, such that the action ai applies at the state (α, k) over V ′ . Replacing
a with ai in π yields a plan that when started at (α0 , 0) leads to (αm , m) = (γ, m). Appending that
plan with appropriate actions bi to increment the counter to 2n − 1 yields a plan for PE′ . Conversely,
if τ is a plan for PE′ , the plan obtained from τ by removing all actions of the form b j and replacing
each action ai with a is a plan for PE, since ai has the same effect on V as a does. Thus, the claim
follows.

α′

We emphasize that this reduction F from STRIPS PLAN to ACYCLIC STRIPS PLAN (or, equivalently, to STRIPS PLAN given ACTION - SET ACYCLICITY) works because it satisfies the following
two conditions:
1. for every instance PE of STRIPS PLAN, F(PE) is an instance of ACYCLIC STRIPS PLAN (this
holds because for every PE, F(PE) is acyclic);
2. for every PE of STRIPS PLAN, F(PE) is a positive instance of ACYCLIC
only if PE is a positive instance of STRIPS PLAN.

STRIPS PLAN

if and

3.2 Mapping STRIPS Plans to Single-Effect STRIPS Plans
Versions of the STRIPS PLAN and ACYCLIC STRIPS PLAN problems that are important for us allow only actions with exactly one literal in their postconditions in their input propositional STRIPS
instances. We call such actions single-effect actions.6 We refer to the restricted problems as SE
STRIPS PLAN and ACYCLIC SE STRIPS PLAN , respectively.
To prove PSPACE-hardness of both problems, we describe a mapping from STRIPS instances to
single-effect STRIPS instances.7
Consider an instance PE = hV, α0 , γ, ACTi of the STRIPS PLAN problem, where ACT is not necessarily acyclic. For each action a ∈ ACT we introduce a new variable xa , whose intuitive meaning
is that action a is currently being executed.
V
We set X = a∈ACT ¬xa . That is, X is the conjunction of negative literals of all the additional
V
variables. In addition, for each a ∈ ACT we set Xa = xa ∧ b∈ACT−{a} ¬xb . We now define an
instance PE′ = hV ′ , α′0 , γ′ , S(ACT)i of the SE STRIPS PLAN problem as follows:
6. Such actions are also called “unary” actions in the planning literature. We stick to the terminology “single-effect”
although it is less commonly used, simply because it is more explicit.
7. PSPACE-completeness of propositional STRIPS planning with single-effect actions was proved already by Bylander
(1994). However, to deal with acyclicity we need to give a different reduction than the one used in that paper.

412

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

• Set of variables: V ′ = V ∪ {xa : a ∈ ACT};
• initial state: α′0 = α0 ∧ X;
• goal state: γ′ = γ ∧ X;
• set of actions: S(ACT) = {ai : a ∈ ACT, i = 1, . . . , 2|post(a)| + 1}.
Let a be an action in ACT such that post(a) = l1 ∧ · · · ∧ lq , where l1 , . . . , lq are literals.
– For i = 1, . . . , q, we define an action ai by setting:
pre(ai ) = pre(a) ∧ X ∧ ¬li ; post(ai ) = xa .
The role of ai is to enforce that Xa holds after ai is successfully applied, and in this
way to enable “starting the execution of a”, provided that no action is currently being
executed, that the ith effect of a is not already true, and that the precondition of a is true.
– For i = q + 1, . . . , 2q, we define action ai by setting:
pre(ai ) = Xa ; post(ai ) = li .
The role of ai is to make the ith effect of a true.
– Finally, we define a2q+1 by setting:
pre(a2q+1 ) = Xa ∧ l1 ∧ · · · ∧ lq ; post(a2q+1 ) = ¬xa .
Thus, a2q+1 is designed so that X holds after a2q+1 is successfully applied; that is, a2q+1
“closes” the execution of a, thus allowing for the next action to be executed.
Let π be a sequence of actions in ACT. We define S(π) to be the sequence of actions in S(ACT)
obtained by replacing each action a in π by a1 , . . . , a2q+1 , where q = |post(a)|. Now consider a
sequence τ of actions from S(ACT). Remove from τ every action ai such that i 6= 2|post(a)| + 1,
and replace actions of the form a2|post(a)|+1 by a. We denote the resulting sequence of actions from
ACT by S′ (τ). We note that S′ (S(π)) = π. The following properties then hold.
Lemma 1 With the above definitions,
(i) if π is a plan for PE then S(π) is a plan for PE′ ;
(ii) if τ is an irreducible plan for PE′ then S′ (τ) is an irreducible plan for PE;
(iii) ACT is acyclic if and only if S(ACT) is acyclic.
Proof: (i) Let a ∈ ACT be an action, let α be a state and let β be the state obtained from α by
applying a. Let θ be the V ′ -state obtained by applying the sequence of actions ha1 , . . . , a2q+1 i
(where q = |post(a)|) to the state α ∧ X of PE′ . We will show that θ = β ∧ X.
We note that if for each i = 1, . . . , q, state α ∧ X does not satisfy pre(ai ) then the sequence of
actions ha1 , . . . , a2q+1 i has no effect, so the state is still α ∧ X. For this to happen, either α doesn’t
satisfy pre(a), or all of l1 , . . . , lq already hold in α so post(a) holds in α. In either case, α = β, and
so θ = β ∧ X.
413

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Suppose now that for some i ∈ {1, . . . , q}, α does satisfy pre(ai ). Then the first such action
causes xa and hence Xa to hold. After applying actions aq+1 , . . . , a2q , l1 ∧ · · · ∧ lq holds, and so
post(a) holds. After applying a2q+1 both post(a) and X hold. No other variable in V has changed,
so θ = β ∧ X, as required.
Applying this result iteratively implies that if π is a plan for PE then S(π) is a plan for PE′ .

ai

(ii) Let τ be an irreducible plan for PE′ , so that every action in τ changes the state, which implies
that every action in τ is performed in a state where its precondition is true. We will show that S′ (τ)
/ When τ = 0,
/ S′ (τ) = 0,
/ too, and the assertion follows.
is a plan for PE. We will assume that τ 6= 0.
j
′
Write the first action in τ as a , where a ∈ ACT, and let τ be the maximal initial subsequence of
τ consisting of all actions of the form ai . We must have j ≤ |post(a)|, since X holds in α′0 (by our
assumption above, action a j does apply) and X is inconsistent with the precondition of ai for each
i > |post(a)|. Also, pre(a j ) and ¬l j hold in α′0 and so, in α0 as well. Thus, α0 satisfies pre(a), and
applying a changes the state, since ¬l j holds in α0 and post(a) |= l j . Let us denote by β the state
resulting from applying a to α0 . As we noted, β 6= α0 ,
Let β′ be the state resulting after applying τ′ to α′0 . If β′ is the goal state γ′ then X holds in β′ . If
β′ is not the goal state then τ 6= τ′ . Let bi be the action in τ directly following the last action in τ′ .
By the definition of τ′ , a 6= b. After applying a j , Xa holds, so in β′ either Xa holds or X holds. Thus,
Xb does not hold, as a 6= b. Since bi changes the state, i must be in {1, . . . , |post(b)|}, so X holds in
β′ in this case, too.
Hence the last action in τ′ is a2q+1 , where q = |post(a)|. Since the only variables in V which can
be affected by actions ai are those that appear in the literals in post(a) and since the action a2q+1
can be executed (otherwise it would not belong to τ), it follows that β′ = β ∧ X.
Applying this reasoning repeatedly, we show that applying S′ (τ) to α0 yields γ, and that each
action in S′ (τ) changes the state, so S′ (τ) is an irreducible plan for PE, which is non-empty if and
only if τ is non-empty.
(iii) Suppose ACT is not acyclic, so that there exists state α and a non-empty irreducible plan π for
PEα = hV, α, α, ACTi. Then, by (i), S(π) is a plan for PE′α = hV ′ , α ∧ X, α ∧ X, S(ACT )i. Because
π is non-empty and irreducible, it changes some state, so S(π) also changes some state, and hence
can be reduced to a non-empty irreducible plan for PE′α . Therefore S(ACT) is not acyclic.
Conversely, suppose that S(ACT) is not acyclic. Then there exists a state α′ and a non-empty
irreducible plan τ for hV ′ , α′ , α′ , S(ACT)i. We will first prove that X holds at some state obtained
during the execution of this plan.
/ By
Suppose that X holds at no such state, and let a j be the first action in τ. We note that τ 6= 0.
our assumption, X does not hold either before or after applying a j . Therefore q + 1 ≤ j ≤ 2q, where
q = |post(a)|. Since τ is irreducible, a j changes the state. Thus, ¬l j holds in α′ and l j holds in the
state resulting from α′ after applying a j .
By our assumption, Xa holds before and after applying a j . Thus, the next action, if there is one,
must also be of the form ai for q + 1 ≤ i ≤ 2q. Repeating this argument implies that all actions in
τ are of the form ai where q + 1 ≤ i ≤ 2q. Since the set of literals in post(a) is consistent, l j is
never reset back to ¬l j . Thus, the state resulting from α′ after applying τ is different from α′ , a
contradiction.
Thus, X holds at some state reached during the execution of τ. Let us consider one such state.
It can be written as β ∧ X, for some state β over V . We can cyclically permute τ to generate a
non-empty irreducible plan τ′ for hV ′ , β ∧ X, β ∧ X, S(ACT)i. By part (ii), S′ (τ′ ) is a non-empty
414

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

irreducible plan for hV, β, β, ACTi. Therefore ACT is not acyclic.



Proposition 6
SE STRIPS PLAN

and ACYCLIC SE STRIPS PLAN are PSPACE-complete.

Proof: Again, the argument for the membership in PSPACE is standard. PSPACE-hardness of
ACYCLIC SE STRIPS PLAN is shown by reduction from ACYCLIC STRIPS PLAN . The same construction shows that STRIPS PLAN is reducible to SE STRIPS PLAN, and thus SE STRIPS PLAN is
PSPACE-complete.
Let us consider an instance PE = hV, α0 , γ, ACTi of ACYCLIC STRIPS PLAN. We define PE′ =
′
hV , α′0 , γ′ , S(ACT)i, which by Lemma 1(iii) is an instance of the ACYCLIC SE STRIPS PLAN problem. By Lemma 1(i) and (ii) there exists a plan for PE if and only if there exists a plan for PE′ . This
implies that ACYCLIC SE STRIPS PLAN is PSPACE-hard.


4. Dominance
The goal of this section is to prove that the GCP - DOMINANCE problem is PSPACE-complete, and
that the complexity does not go down even when we restrict the class of inputs to CP-nets. We
use the results on propositional STRIPS planning from Section 3 to prove that the general GCP DOMINANCE problem is PSPACE-complete. We then show that the complexity does not change if
we require the input GCP-net to be locally consistent and locally complete.
The similarities between dominance testing in CP-nets and propositional STRIPS planning were
first noted by Boutilier et al. (1999). They presented a reduction, discussed later in more detail by
Boutilier et al. (2004a), from the dominance problem to the plan existence problem for a class
of propositional STRIPS planning specifications consisting of unary actions (actions with single
effects). We prove our results for the GCP - DOMINANCE and GCP - CONSISTENCY problems by constructing a reduction in the other direction.
This reduction is much more complex than the one used by Boutilier et al. (1999), due to the
fact that CP-nets impose more restrictions than STRIPS planning. Firstly, STRIPS planning allows
multiple effects, but GCP-nets only allow flips x > ¬x or ¬x > x that change the value of one
variable; this is why we constructed the reduction from STRIPS planning to single-effect STRIPS
planning in the last section. Secondly, CP-nets impose two more restrictions, local consistency and
local completeness, which do not have natural counterparts in the context of STRIPS planning.
For all dominance and consistency problems we consider, the membership in PSPACE can be
demonstrated similarly to the membership proof of Proposition 4, namely by considering nondeterministic polynomial space algorithms consisting of repeatedly guessing appropriate improving flips
and making use of the fact that PSPACE = NPSPACE = coPSPACE. Therefore, from now on we
only provide arguments for the PSPACE-hardness of problems we consider.
4.1 Dominance for Generalized CP-Nets
We will prove that the GCP - DOMINANCE problem is PSPACE-complete by a reduction from the
problem SE STRIPS PLAN, which we now know to be PSPACE-complete.
415

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

4.1.1 M APPING S INGLE -E FFECT STRIPS P ROBLEMS
P ROBLEMS

TO

GCP-N ETS D OMINANCE

Let hV, α0 , γ, ACTi be an instance of the SE STRIPS PLAN problem. For every action a ∈ ACT
we denote by la the unique literal in the postcondition of a, that is, post(a) = la . We denote by
pre′ (a) the conjunction of all literals in pre(a) different from ¬la (we recall that by a convention we
adopted earlier, pre′ (a) does not contain la ). We then define ca to be the conditional preference rule
pre′ (a) : la > ¬la and define M(ACT) to be the GCP-net C = {ca : a ∈ ACT}, which is in conjunctive
form.
A sequence of states in a plan corresponds to an improving sequence from α0 to γ, which leads
to the following result.
Lemma 2 With the above notation,
(i) there is a non-empty irreducible plan for hV, α0 , γ, ACTi if and only if γ dominates α0 in
M(ACT);
(ii) ACT is acyclic if and only if M(ACT) is consistent.
Proof: We first note the following equivalence. Let a be an action in ACT, and let α and β be
different outcomes (or, in the STRIPS setting, states). The action a applied to α yields β if and only
if the rule ca sanctions an improving flip from α to β. This is because a applied to α yields β if and
only if α satisfies pre(a) and α and β differ only on literal la , with β satisfying la and α satisfying
¬la . This is if and only if α satisfies pre′ (a) and α and β differ only on literal la , with β satisfying
la , and α satisfying ¬la . This, in turn, is equivalent to say that rule ca sanctions an improving flip
from α to β.
Proof of (i): Suppose first that there exists a non-empty irreducible plan a1 , . . . , am for hV, α0 , γ, ACTi.
Let α0 , α1 , . . . , αm = γ be the corresponding sequence of outcomes, and, for each i = 1, . . . , m, action ai , when applied in state αi−1 , yields different state αi . By the above equivalence, for each
i = 1, . . . , m, cai sanctions an improving flip from αi−1 to αi , which implies that α0 , α1 , . . . , αm is an
improving flipping sequence in M(ACT), and therefore γ dominates α0 in M(ACT).
Conversely, suppose that γ dominates α0 in M(ACT), so that there exists an improving flipping
sequence α0 , α1 , . . . , αm with αm = γ, and m ≥ 1. For each i = 1, . . . , m, let cai be an element of
M(ACT) which sanctions the improving flip from αi−1 to αi . Then, by the above equivalence,
action ai , when applied to state αi−1 yields αi (which is different from αi−1 ), and so a1 , . . . , am is a
non-empty irreducible plan for hV, α0 , γ, ACTi.
Proof of (ii): ACT is not acyclic if and only if there exists a state α and a non-empty irreducible
plan for hV, α, α, ACTi. By (i) this is if and only if there exists an outcome α which dominates itself
in M(ACT), which is if and only if M(ACT) is not consistent.


Theorem 1 The GCP - DOMINANCE problem is PSPACE-complete. Moreover, this remains so under
the restrictions that the GCP-net is consistent and is in conjunctive form.
Proof: PSPACE-hardness is shown by reduction from ACYCLIC SE STRIPS PLAN (Proposition 6).
Let hV, α0 , γ, ACTi be an instance of the ACYCLIC SE STRIPS PLAN problem. By Lemma 2(ii),
M(ACT) is a consistent GCP-net in conjunctive form. Since α0 6= γ (imposed in the definition of
416

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

the problem ACYCLIC SE STRIPS PLAN), there is a plan for hV, α0 , γ, ACTi if and only if there is a
non-empty irreducible plan for hV, α0 , γ, ACTi, which, by Lemma 2(i), is if and only if γ dominates
α0 in C.

Theorem 1 implies the PSPACE-completeness of dominance in the more general conditional
preference language introduced by Wilson (2004b), where the conditional preference rules are written in conjunctive form.
4.2 Dominance in CP-Nets
In this section we show that GCP - DOMINANCE remains PSPACE-complete under the restriction to
locally consistent and locally complete GCP-nets, that is, CP-nets. We refer to this restriction of
GCP - DOMINANCE as CP - DOMINANCE .
Consistency of a GCP-net implies local consistency (Proposition 2). Therefore, the reduction in the proof of Theorem 1 (from ACYCLIC SE STRIPS PLAN to GCP - DOMINANCE restricted
to consistent GCP-nets) is also a reduction to GCP - DOMINANCE restricted to locally consistent
GCP-nets. PSPACE-hardness of ACYCLIC SE STRIPS PLAN (Proposition 6) then implies that GCP DOMINANCE restricted to locally consistent GCP-nets is PSPACE-hard, and, in fact, PSPACEcomplete since membership in PSPACE is easily obtained with the usual line of argumentation.
We will show PSPACE-hardness for CP - DOMINANCE by a reduction from GCP - DOMINANCE
for consistent GCP-nets.
4.2.1 M APPING L OCALLY C ONSISTENT GCP-N ETS

TO

CP-N ETS

Let C be a locally consistent GCP-net. Let V = {x1 , . . . , xn } be the set of variables of C. We define
/ We define a GCP-net C′ over V ′ , which we
V ′ = V ∪ {y1 , . . . , yn }, where {y1 , . . . , yn } ∩ V = 0.
will show is a CP-net. To this end, for every z ∈ V ′ we will define conditional preference rules
q+ (z) : z > ¬z and q− (z) : ¬z > z to be included in C′ by specifying formulas q+ (z) and q− (z).
First, for each variable xi ∈ V , we set
q+ (xi ) = yi and q− (xi ) = ¬yi .
Thus, xi depends only on yi . We also note that the formulas q+ (xi ) and q− (xi ) satisfy local consistency and local completeness requirements.
Next, for each variable yi , 1 ≤ i ≤ n, we define
ei = (x1 ↔ y1 ) ∧ · · · ∧ (xi−1 ↔ yi−1 ) ∧ (xi+1 ↔ yi+1 ) ∧ · · · ∧ (xn ↔ yn ),
fi+ = ei ∧ p+ (xi ) and fi− = ei ∧ p− (xi ).
Finally, we define
q+ (yi ) = fi+ ∨ (¬ fi− ∧ xi )
and
q− (yi ) = fi− ∨ (¬ fi+ ∧ ¬xi ).
Thus, yi depends on every variable in V ′ but itself.
We note that by the local consistency of C, formulas fi+ ∧ fi− , 1 ≤ i ≤ n, are unsatisfiable.
Consequently, formulas q+ (yi ) ∧ q− (yi ), 1 ≤ i ≤ n, are unsatisfiable. Thus, C′ is locally consistent.
417

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Finally, q+ (yi ) ∨ q− (yi ) is equivalent to fi+ ∨ ¬xi ∨ fi− ∨ xi , so is a tautology. Thus, C′ is locally
complete and hence a CP-net over V ′ .
Let α and β be outcomes over {x1 , . . . , xn } and {y1 , . . . , yn }, respectively. By αβ we denote the
outcome over V ′ obtained by concatenating n-tuples α and β. Conversely, every outcome for C′ can
be written in this way.
Let α be an outcome over V . We define α to be the outcome over {y1 , . . . , yn } obtained by
replacing in α every component of the form xi with yi and every component ¬xi with ¬yi . Then for
every i, 1 ≤ i ≤ n, αα |= ei .
Let s be a sequence α0 , . . . , αm of outcomes over V . Define L(s) to be the sequence of V ′ outcomes: α0 α0 , α0 α1 , α1 α1 , α1 α2 , . . . , αm αm . Further, let t be a sequence ε0 , ε1 , . . . , εm of V ′ outcomes with ε0 = αα and εm = ββ. Define L′ (t) to be the sequence obtained from t by projecting
each element in t to V and iteratively removing elements in the sequence which are the same as their
predecessor (until any two consecutive outcomes are different).
Lemma 3 With the above definitions,
(i) if s is an improving sequence for C from α to β then L(s) is an improving sequence for C′ from
αα to ββ;
(ii) if t is an improving sequence from αα to ββ then L′ (t) is an improving sequence from α to β;
(iii) C is consistent if and only if C′ is consistent.
Proof: Let e = ni=1 (xi ↔ yi ). The definitions have been arranged so that the GCP-net C and the
CP-net C′ have the following properties:
(a) If e does not hold in an outcome γ over V ′ , then every improving flip applicable to γ changes the
value of some variable xi or yi so that xi ↔ yi holds after the flip.
Indeed, let us assume that there is an improving flip from γ to some outcome γ′ over V ′ . If the
flip concerns a variable xi , then xi ↔ ¬yi holds in γ. Consequently, xi ↔ yi holds in γ′ .
Thus, let us assume that the flip concerns a variable yi . If ei holds in γ then, since e does not,
xi ↔ ¬yi holds in γ. Thus, xi ↔ yi holds in γ′ . If ei does not hold in γ then neither fi+ nor fi− does.
Thus, if xi (¬xi , respectively) holds in γ, yi (¬yi , respectively) holds in γ′ . Since the flip concerns yi ,
it follows that xi ↔ yi holds in γ′ .
(b) No improving flip from αα changes any variable xi .
Indeed, for any variable xi , since e holds in αα, xi ↔ yi holds in αα, too. Thus, no improving
flip changes xi .
(c) There is an improving flip in C′ that changes variable yi in an outcome αα if and only if there is
an improving flip for the GCP-net C from outcome α that changes variable xi . After applying the
improving flip (changing variable yi ) to αα, there is exactly one improving flip possible. It changes
xi and results in an outcome ββ, where β is the outcome over V resulting from applying to α the
improving flip changing the variable xi .
To prove (c), let us first assume that ¬yi holds in αα and observe that in such case ¬xi holds in
αα, too. It follows that q+ (yi ) holds in αα if and only if p+ (xi ) holds in α. Consequently, changing
yi in αα is an improving flip in C′ if and only if changing xi in α is an improving flip in C. The
argument in the case when yi holds in αα is analogous (but involves q− (yi ) and p− (xi )). Thus, the
first part of (c) follows.
V

418

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Let β be the outcome obtained by applying an improving flip to xi in α. It follows that the
improving flip changing the value of yi in αα results in the outcome αβ. In this outcome, by (a),
an improving flip must concern x j or y j such that x j ↔ y j holds after the flip. Since for every j 6= i,
x j ↔ y j holds in αβ, the only improving flips in αβ concern either xi or yi . By the local consistency
of C′ , yi cannot be flipped right back. Clearly, changing xi is an improving flip that can be applied
to αβ. By our discussion, it is the only improving flip applicable in αβ and it results in the outcome
ββ. This proves the second part of (c).
Proof of (i): The assertion follows by iterative application of (c).
Proof of (ii): Suppose that t is an improving sequence ε0 , ε1 , . . . , εm of V ′ -outcomes with ε0 = αα
and εm = ββ. Since e holds in ε0 , (b) implies that the first flip changes some variable yi , and (c)
implies that the second flip changes variable xi to make xi ↔ yi hold again. Hence ε2 can be written
as δδ. By (c) there is an improving flip in C from outcome α changing variable xi , that is, leading
from α to δ. Iterating this process shows that L′ (t) is an improving sequence from α to β.
Proof of (iii): Suppose that C is inconsistent. Then there exists some outcome α and an improving
sequence s in C from α to α. By (i), L(s) is an improving sequence from αα to αα, proving that C′
is inconsistent.
Conversely, suppose that C′ is inconsistent, so there exists an improving sequence t for C′ from
some outcome to itself. By (a), any improving flip applied to an outcome in which e does not hold
increases (by one) the number of i such that xi ↔ yi holds. This implies that e must hold in some
outcome in t, because t is not acyclic. Write this outcome as αα. We can cyclically permute t to
form an improving sequence t2 from αα to itself. Part (ii) then implies that L′ (t2 ) is an improving
flipping sequence for C from α to itself, showing that C is inconsistent.


Theorem 2 CP - DOMINANCE is PSPACE-complete. This holds even if we restrict the CP-nets to
being consistent.
Proof: We use a reduction from PSPACE-hardness of the GCP - DOMINANCE problem when the
GCP-nets are restricted to being consistent (Theorem 1). Let C be a consistent, and hence locally
consistent, GCP-net over V , and let α and β be outcomes over V . Consider the CP-net C′ over
variables V ′ constructed above. Lemma 3(i) and (ii) imply that β dominates α in C if and only if ββ
dominates αα in C′ . Moreover, C′ is consistent by Lemma 3(iii). Consequently, the hardness part
of the assertion follows.

Note that PSPACE-hardness obviously remains if we require input outcomes to be different,
because the reduction for Theorem 1 uses a pair of different outcomes.
Notice the huge complexity gap with the problem of deciding whether there exists a nondominated outcome, which is “only” NP-complete (Domshlak et al., 2003, 2006).

5. Consistency of GCP-Nets
In this section we show that the
from Sections 3 and 4.

GCP - CONSISTENCY

419

problem is PSPACE-complete, using results

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Theorem 3
GCP - CONSISTENCY is PSPACE-complete. This holds even under the restriction to GCP-nets in
conjunctive form.
Proof: PSPACE-hardness is shown by reduction from ACTION - SET ACYCLICITY. We apply function S from Section 3.2 followed by M from Section 4.1. This maps instances of ACTION - SET
ACYCLICITY to instances of GCP - CONSISTENCY in conjunctive form. By Lemma 1(iii) and Lemma
2 (ii), an instance of ACTION - SET ACYCLICITY is acyclic if and only if the corresponding instance
of GCP - CONSISTENCY is consistent, proving the result.

We now show that consistency testing remains PSPACE-complete for CP-nets (GCP-nets that
are both locally consistent and locally complete).
Theorem 4

CP - CONSISTENCY

is PSPACE-complete.

Proof: We use a reduction from GCP - CONSISTENCY under the restriction that the GCP-net is in
conjunctive form. Let C be a GCP-net in conjunctive form. We define a CP-net C′ as follows. Because C is in conjunctive form, local consistency can be decided in polynomial time, as it amounts
to checking the consistency of a conjunction of conjunctions of literals. If C is not locally consistent
we set C′ to be a predetermined inconsistent but locally consistent CP-net, such as in the example
in Section 2. Otherwise, C is locally consistent and for C′ we take the CP-net we constructed in
Section 4.2. The mapping from locally consistent GCP-nets to CP-nets, described in Section 4.2,
preserves consistency (Lemma 3 (iii)). Since local inconsistency implies inconsistency (Proposition 2), we have that the GCP-net C is consistent if and only if the CP-net C′ is consistent. Thus,
PSPACE-hardness of the CP - CONSISTENCY problem follows from Theorem 3.


6. Additional Problems Related to Dominance in GCP-Nets
Having proved our main results on consistency of and dominance in GCP-nets, we move on to
additional questions concerning the dominance relation. Before we state them, we introduce more
terminology.
Let α and β be outcomes in a GCP-net C. We say that α and β are dominance-equivalent in C,
written α ≈C β, if α = β, or α ≺C β and β ≺C α. Next, α and β are dominance-incomparable in C
if α 6= β, α⊀C β and β⊀C α. Finally, α strictly dominates β if β ≺C α and α6≺C β.
Definition 8
We define the following decision problems:
SELF - DOMINANCE : given a GCP-net C and an outcome α, decide whether α ≺C α, that is, whether
α dominates itself in C.
STRICT DOMINANCE : given a GCP-net C and outcomes α and β, decide whether α strictly dominates β in C.
DOMINANCE EQUIVALENCE : given a GCP-net C and outcomes α and β, decide whether α and β
are dominance-equivalent in C.
DOMINANCE INCOMPARABILITY : given a GCP-net C and outcomes α and β, decide whether α
and β are dominance-incomparable in C.
420

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

When establishing the complexity of these problems, we will use polynomial-time reductions
from the problem GCP - DOMINANCE. Let H be a GCP-net with the set of variables V = {x1 , . . . , xn },
and let β be an outcome. We define a GCP-net G = Θ1 (H, β) with the set of variables W = V ∪ {y}
by setting the conditions for flips on variables xi , i = 1, . . . , n, and y as follows:
1. if xi ∈ β:
+
p+
G (xi ) = pH (xi ) ∨ ¬y
−
p−
G (xi ) = pH (xi ) ∧ y
2. if ¬xi ∈ β:
+
p+
G (xi ) = pH (xi ) ∧ y
−
p−
G (xi ) = pH (xi ) ∨ ¬y
3. p+
G (y) = β
4. p−
G (y) = ¬β.
The mapping Θ1 can be computed in polynomial time. Moreover, one can check that if H is a
locally consistent GCP-net, Θ1 (H, β) is also locally consistent. Finally, if H is a CP-net, Θ1 (H, β)
is a CP-net, as well.
For every V -outcome γ, we let γ+ = γ ∧ y and γ− = γ ∧ ¬y. We note that every W -outcome is of
the form γ+ or γ− . To explain the structure of the GCP-net G, we point out that there is an improving
flip in G from γ+ into δ+ if and only if there is an improving flip in H from γ to δ (thus, G restricted
to outcomes of the form γ+ forms a copy of the GCP-net H). Moreover, there is an improving flip
in G from γ− into δ− if and only if δ agrees with β on exactly one more variable xi than γ does.
Finally, an improving flip moves between outcomes of different type if and only if it transforms β−
to β+ , or γ+ to γ− for some γ 6= β.
We now formalize some useful properties of the GCP-net G = Θ1 (H, β). We use the notation
introduced above.
Lemma 4 For every V -outcome γ, γ− ≺G β+ and, if γ 6= β, γ+ ≺G β+ (in other words, β+ dominates
every other W -outcome).
Proof: Consider any V -outcome γ 6= β. Then γ ∧ ¬y ≺C β ∧ ¬y since, given ¬y, changing a literal
to the form it has in β is an improving flip. By the definition, we also have β ∧ ¬y ≺C β ∧ y and
γ ∧ y ≺G γ ∧ ¬y (as γ 6= β). It follows that β− ≺G β+ and γ+ ≺G γ− ≺G β+ . Thus, the assertion
follows.


Lemma 5 For arbitrary V -outcome α different from β, the following statements are equivalent:
1. β ≺H α;
2. β+ ≺G α+ ;
3. β+ ≈G α+ .
421

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Proof: By Lemma 4, α+ ≺G β+ . Thus, the conditions (2) and (3) are equivalent.
[(1)⇒(2)] Clearly (recall our discussion about the structure of G), if there is an improving flip from
γ to δ in H, then there is an improving flip from γ+ to δ+ in G. Thus, if there is an improving
sequence in H from β to α, there is an improving sequence in G from β+ to α+ .
[(2)⇒(1)] Let us assume β+ ≺G α+ , and let us consider an improving sequence of minimum length
from β+ to α+ . By the minimality, no internal element in such a sequence is β+ . Thus, no internal
element equals β− either (as the only improving flip from β− leads to β+ ). Since an improving flip
from γ− to γ+ requires that γ = β, all outcomes in the sequence are of the form γ+ . By dropping
y from each outcome in this sequence, we get an improving flipping sequence from α to β in H.
Thus, β ≺H α.

Lemma 6 Let H be consistent and let α and β be different V -outcomes. Then, α+ ≺G α+ if and
only if β ≺H α.
Proof: Suppose there exists an improving sequence from α+ to itself. There must be an outcome
in the sequence of the form γ ∧ ¬y (otherwise, dropping y in every outcome yields an improving
sequence from α to α in H, contradicting the consistency of H). To perform an improving flip from
¬y to y we need β to hold, which implies that β+ appears in the sequence. Thus, β+ ≺G α+ . By
Lemma 5, β ≺H α.
Conversely, let us assume that β ≺H α. Again by Lemma 5, β+ ≺G α+ . By Lemma 4, α+ ≺G β+ .
Thus, α+ ≺G α+ .

The next construction is similar. Let H be a GCP-net on variables V = {x1 , . . . , xn }, and let α
be an outcome. We define a GCP-net F = Θ2 (H, α) as follows. As before, we set W = V ∪ {y} to
be the set of variables of F. We define the conditions for flips on variables xi , i = 1, . . . , n, and y as
follows:
+
1. p+
G (xi ) = pH (xi ) ∧ y
−
2. p−
G (xi ) = pH (xi ) ∧ y

3. p+
G (y) = ¬α
4. p−
G (y) = α.
Informally, outcomes of the form γ+ form in F a copy of H. There are no improving flips between
outcomes of the form γ− . There is an improving flip from α+ to α− and, for every γ 6= α, from γ− to
γ+ . In particular, if F is consistent then Θ2 (H, α) is consistent, The mapping Θ2 can be computed
in polynomial time and we also have the following property.
Lemma 7 Let β be a V -outcome different from α. Then the following conditions are equivalent:
1. β ≺H α
2. α− strictly dominates β− in F
3. α− and β− are not dominance-incomparable in F.
422

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Proof: If there exists an improving sequence from β− to α− then the first improving flip in the sequence changes β− to β+ . Moreover, there is an improving flip from γ+ to γ− if and only if γ = α.
Thus, β− ≺F α− if and only if β ≺H α. Since α− ⊀F β− all three conditions are equivalent.


Proposition 7 The following problems are PSPACE-complete: SELF - DOMINANCE, STRICT
INANCE , DOMINANCE EQUIVALENCE , and DOMINANCE INCOMPARABILITY .

DOM -

Proof: For all four problems, membership is proven easily as for the problems in earlier sections.
For the PSPACE-hardness proofs, we use the problem CP - DOMINANCE in a version when we
required that the input CP-net be consistent and the two input outcomes different. The problem is
PSPACE-hard by Theorem 2.
Let H be a consistent CP-net on a set V of variables, and let α and β be two different V -outcomes.
By Lemma 5, β ≺H α can be decided by deciding the problem DOMINANCE EQUIVALENCE for α+
and β+ in the GCP-net Θ1 (H, β). Thus, the PSPACE-hardness of DOMINANCE EQUIVALENCE
follows.
Next, the equivalence of Lemma 6, α+ ≺G α+ ⇔ β ≺H α, which holds due to consistency of H,
shows that the problem SELF - DOMINANCE is PSPACE-hard.
Finally, by Lemma 7, β ≺H α can be decided either by deciding the problem STRICT DOMI NANCE for outcomes α− and β− in Θ2 (H, α), or by deciding the complement of the problem DOM INANCE INCOMPARABILITY for α− and β− in the GCP-net Θ2 (H, α). It follows that STRICT DOM INANCE and DOMINANCE INCOMPARABILITY (the latter by the fact that coPSPACE=PSPACE) are
PSPACE-complete.8


Corollary 1 The problems SELF - DOMINANCE and DOMINANCE EQUIVALENCE are PSPACE-complete under the restriction to CP-nets. The problems STRICT DOMINANCE and DOMINANCE IN COMPARABILITY remain PSPACE-complete under the restriction to consistent CP-nets.
Proof: Since in the proof of Proposition 7 we have that H is a CP-net, the claim for the first two
problems follows by our remarks that the mapping Θ1 preserves the property of being a CP-net.
For the last two problems, we observe that since H in the proof of Proposition 7 is assumed to
be consistent, F = Θ2 (H, α) is consistent, too. Thus, it is also locally consistent and the mapping
F to F ′ we used for the proof of Theorem 2 applies. In particular, F ′ is a consistent CP-net and has
the following properties (implied by Lemma 3):
1. α strictly dominates β in F if and only if αα strictly dominates ββ in F ′
2. α and β are dominance-incomparable in F if and only if αα and ββ are dominance-incomparable in F ′ .
Since F ′ is a consistent CP-net, the claim for the last two problems follows, too.



8. For STRICT DOMINANCE, the result could have been also obtained as a simple corollary of Theorem 2, since in
consistent GCP-nets dominance is equivalent to strict dominance.

423

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

7. Problems Concerning Optimality in GCP-Nets
The dominance relation ≺C of a GCP-net C determines a certain order relation, which gives rise to
several notions of optimality. We will introduce them and study the complexity of corresponding
decision problems.
We first observe that the dominance equivalence relation is indeed an equivalence relation (reflexive, symmetric and transitive). Thus, it partitions the set of all outcomes into non-empty equivalence classes, which we call dominance classes. We denote the dominance class of an outcome α
in a GCP-net C by [α]C .
The relation ≺C induces on the set of dominance classes a strict order relation (a relation that is
irreflexive and transitive). Namely, we define [α]C ≺Cdc [β]C if [α]C 6= [β]C (equivalently, α 6≈C β) and
α ≺C β. One can check that the definition of the relation ≺Cdc on dominance classes is independent
of the choice of representatives of the classes.
Definition 9 (Non-dominated class, optimality in GCP-nets) Let C be a GCP-net. A dominance
class [α]C is non-dominated if it is maximal in the strict order ≺Cdc (there is no dominance class
[β]C such that [α]C ≺Cdc [β]C ). A dominance class is dominating if for every dominance class [β]C ,
[α]C = [β]C or [β]C ≺Cdc [α]C .
An outcome α is weakly non-dominated if it belongs to a non-dominated class. If α is weakly
non-dominated and is the only element in its dominance class, then α is non-dominated.
An outcome α is dominating if it belongs to a dominating class. An outcome α is strongly
dominating if it is dominating and non-dominated.
Outcomes that are weakly non-dominated, non-dominated, dominating and strongly dominating
capture some notions of optimality. In the context of CP-nets, weakly non-dominated and nondominated outcomes were proposed and studied before (Brafman & Dimopoulos, 2004). They were
referred to as weakly and strongly optimal there. Similar notions of optimality were also studied
earlier for the problem of defining winners in partial tournaments (Brandt, Fischer, & Harrenstein,
2007). We will study here the complexity of problems to decide whether a given outcome is optimal
and whether optimal outcomes exist.
First, we note the following general properties (simple consequences of properties of finite strict
orders).
Lemma 8 Let C be a GCP-net.
1. There exist non-dominated classes and so, weakly non-dominated outcomes.
2. Dominating outcomes and nondominated outcomes are weakly non-dominated.
3. A strongly dominating outcome is dominating and non-dominated.
4. The following conditions are equivalent:
(a) C has a unique non-dominated class;
(b) C has a dominating outcome;
(c) weakly non-dominated and dominating outcomes in C coincide.
For consistent GCP-nets only two different notions of optimality remain.
424

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Lemma 9 Let C be a consistent GCP-net. Then:
1. Each dominance class is a singleton, ≺C is a strict order, and ≺C and ≺Cdc coincide (modulo
the one-to-one and onto correspondence α 7→ [α]C )
2. If α is a weakly non-dominated outcome, α is non-dominated (weakly non-dominated and
non-dominated outcomes coincide)
3. If α is a dominating outcome, α is strongly dominating (strongly dominating and dominating
outcomes coincide).
4. Finally, α is a unique (weakly) non-dominated outcome if and only if α is strongly dominating.
Next, we observe that all concepts of optimality we introduced are different. To this end, we will
show GCP-nets with a single non-dominated class that is a singleton, with multiple non-dominated
classes, each being a singleton, with a single non-dominated class that is not a singleton, and with
multiple non-dominated classes, each containing more than one element. We will also show a GCPnet with two non-dominated classes, one of them a singleton and the other one consisting of several
outcomes.
Example 2 Consider the following GCP-net C with two binary variables a and b
: a > ā
: b > b̄
This GCP-net determines a strict preorder on the dominance classes, in which {ab} is the only
maximal class (in fact, all dominance classes are singletons). Thus, ab is both non-dominated and
dominating and so, it is strongly dominating.
Example 3 Consider the following GCP-net C with two binary variables a and b
b : a > ā
b̄ : ā > a
a : b > b̄
ā : b̄ > b
This GCP-net determines a strict preorder, in which {ab} and {āb̄} are two different non-dominated
classes. Thus, ab and āb̄ are non-dominated and there is no dominating outcome.
Example 4 Consider a GCP-net with variables a, b and c, defined as follows:
a : b > b̄
ā : b̄ > b
b̄ : a > ā
b : ā > a
ab : c > c̄

425

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

There are two dominance classes: Sc = {abc, ab̄c, ābc, āb̄c} and Sc̄ = {abc̄, ab̄c̄, ābc̄, āb̄c̄}. Every
outcome in Sc strictly dominates every outcome in Sc̄ , therefore, Sc is the unique non-dominated
class and every outcome in Sc is dominating. Because Sc is not a singleton, there are no nondominated outcomes (and so, no strongly dominating outcome, either).
Example 5 Let us remove from the GCP-net of Example 4 the preference statement ab : c > c̄. Then
Sc and Sc̄ are still the two dominance classes, but now every outcome is Sc is incomparable with
any outcome in Sc̄ . Thus, Sc and Sc̄ are both non-dominated. Since there are two non-dominated
classes, there is no dominating outcome. Since each class has more than one element, there are no
non-dominated outcomes. All outcomes are weakly non-dominated, though.
Example 6 Let us modify the GCP-net of Example 4 by changing the preference statement b̄ : a > ā
into b̄c : a > ā. The dominance relation ≺ of this GCP-net satisfies the following properties: (i)
the four outcomes in Sc dominate each other; (ii) āb̄c̄ ≻ ābc̄ ≻ abc̄ ≻ ab̄c̄; (iii) any outcome in Sc
dominates abc̄ (and, a fortiori, ab̄c̄). One can check that there are five dominance classes: Sc , {abc̄},
{ābc̄}, {ab̄c̄} and {āb̄c̄}. Two of them are non-dominated: Sc and {āb̄c̄}. Since there are two nondominated classes, there is no dominating outcome. On the other hand, {āb̄c̄} is a non-dominated
outcome (a unique one).
We will consider the following decision problems corresponding to the notions of optimality we
introduced.
Definition 10
For a given GCP-net C:
WEAKLY NON - DOMINATED OUTCOME : given an outcome α, decide whether α is weakly nondominated in C
NON - DOMINATED OUTCOME : given an outcome α, decide whether α is non-dominated in C
DOMINATING OUTCOME : given an outcome α, decide whether α is dominating in C
STRONGLY DOMINATING OUTCOME : given an outcome α, decide whether α is strongly dominating in C
EXISTENCE OF A NON - DOMINATED OUTCOME : decide whether C has a non-dominated outcome
EXISTENCE OF A DOMINATING OUTCOME : decide whether C has a dominating outcome
EXISTENCE OF A STRONGLY DOMINATING OUTCOME : decide whether C has a strongly dominating outcome.
In some of the hardness proofs, we will again use the reductions Θ1 and Θ2 , described in the
previous section. We note the following additional useful properties of the GCP-net G = Θ1 (H, β).
Lemma 10 For arbitrary V -outcome α different from β, the following statements are equivalent:
1. β+ ≺G α+
2. α+ is weakly non-dominated in G
3. α+ is a dominating outcome in G.
426

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Proof: Since β+ is dominating in G (Lemma 4), weakly non-dominated outcomes and dominating
outcomes coincide (Lemma 8). It follows that the conditions (1)-(3) are equivalent to each other. 

Proposition 8 The following problems are PSPACE-complete: WEAKLY NON - DOMINATED OUTCOME and DOMINATING OUTCOME . The result holds also for the problems restricted to CP-nets.
Proof: The membership is easy to prove by techniques similar to those we used earlier.
For the PSPACE-hardness proofs, we use reductions from CP - DOMINANCE for consistent CPnets (in the version where the two input outcomes are different). Let H be a CP-net, and α and
β two different V -outcomes. By Lemmas 5 and 10, β ≺H α can be decided by deciding either of
the problems WEAKLY NON - DOMINATED OUTCOME and DOMINATING OUTCOME for the GCPnet G = Θ1 (H, β) and the outcome α+ . We observed earlier, that if H is a CP-net, then so is
G = Θ1 (H, β). Thus, the second part of the assertion follows.

Next, we will consider the problem STRONGLY DOMINATING OUTCOME. We will exploit the
reduction F = Θ2 (H, α), which we discussed in the previous section. We observe the following
property of F.
Lemma 11 Let H be a GCP-net and F = Θ2 (H, α). Then α− is strongly dominating in F if and
only if α is dominating in H.
Proof: Let us assume that α is dominating in H. From the definition of F, it follows that for every
V -outcome γ 6= α, γ+ ≺F α+ and γ− ≺F γ+ . Since α+ ≺F α− , α− is dominating in F. Since there
is no improving flip leading out of α− , α− is strongly dominating.
Conversely, let us assume that α− is strongly dominating in F and let γ be a V -outcome different from α. Let us consider an improving sequence from γ+ to α− . All outcomes in the sequence
other than the last one, α− , are of the form δ+ . Moreover, the outcome directly preceding α− is
α+ . Dropping y from every outcome in the segment of the sequence between γ+ and α+ yields an
improving sequence from γ to α in H.

We now have the following consequence of this result.
Proposition 9 The problem STRONGLY
stricted to CP-nets.

DOMINATING OUTCOME

is PSPACE-complete, even if re-

Proof: Let H be a CP-net (over the set V of variables) and α an outcome. By Lemma 11, the problem DOMINATING OUTCOME can be decided by deciding the problem STRONGLY DOMINATING
OUTCOME for F = Θ2 (H, α) and α− . Thus, the PSPACE-hardness of STRONGLY DOMINATING
OUTCOME follows by Proposition 8. The membership in PSPACE is, as in other cases, standard and
is omitted.
Since H is a CP-net, it is locally consistent and so, F is locally consistent, too. As in the proof
of Corollary 1 we use the mapping from GCP-net F to CP-net F ′ defined in Section 4.2. By Lemma
3, α is a strongly dominating outcome in F if and only if αα dominates every outcome of the form
γγ, which is if and only if αα is a strongly dominating outcome in F ′ , since any F ′ -outcome is
dominated by an outcome of the form γγ (using the rules q+ (xi ) = yi and q− (xi ) = ¬yi ). Therefore
427

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

for F and α can be decided by deciding
for F ′ and αα. Thus, the second part of the claim follows.

STRONGLY DOMINATING OUTCOME
NATING OUTCOME

STRONGLY DOMI 

The problem NON - DOMINATED OUTCOME is easier. It is known to be in P for CP-nets (Brafman
& Dimopoulos, 2004). The result extends to GCP-nets. Indeed, if H is a GCP-net and α an outcome,
α is non-dominated if and only if there is no improving flip that applies to α. The latter holds if and
only if for every variable x in H, if x (respectively, ¬x) holds in α, then p− (x) (respectively, p+ (x))
does not hold in α. Since the conditions can be checked in polynomial the claim holds and we have
the following result.
Proposition 10 The problem NON - DOMINATED

OUTCOME

for GCP-nets is in P.

Next, we will consider the problems concerning the existence of optimal outcomes. Let H be a
GCP-net on the set of variables V = {x1 , . . . , xn }, and let α and β be two different V -outcomes. For
every i = 1, 2, . . . , n, we define formulas αi as follows. If xi ∈ α, then αi is the conjunction of all
literals in α, except that instead of xi we take ¬xi . Similarly, if ¬xi ∈ α, then αi is the conjunction of
all literals in α, except that instead of ¬xi we take xi . Thus, αi is the outcome that results in α when
the literal in corresponding to xi is flipped into its dual.
We now define a GCP-net E = Θ3 (H, α, β) by taking W = V ∪ {y} as the set of variables of E
and by defining the flipping conditions as follows:
+
1. p+
E (xi ) = (pH (xi ) ∧ y) ∨ (¬y ∧ ¬α ∧ ¬αi )
−
−
pE (xi ) = pH (xi ) ∧ y

2. p+
E (y) = β
3. p−
E (y) = ¬β.
The GCP-net Θ3 (H, α, β) has the following properties. The outcomes of the form γ+ (= γ ∧ y)
form a copy of H. There is no improving flip for the outcome α− (= α ∧ ¬y). Next, there is no
improving flip into α− from an outcome of the form γ− . To see this, let us assume that such a flip
exists and concerns a variable, say, xi . It follows that γ = αi . By the definition of flipping conditions,
an improving flip for γ− that involves xi is impossible, a contradiction. Thus, the only improving
flip that leads to α− originates in α+ .
We also have that for every outcome γ other than α and β, γ− ≺E β− . It follows from the fact
that for every outcome γ other than α and β, γ− has an improving flip. Indeed, for each such γ there
is a variable xi such that (i) xi is false in γ, and (ii) flipping the literal of xi to its dual does not lead to
α (that is, γ is not αi ). (For even if γ = αi for some i, then, because γ, α 6= β, there exists i′ 6= i such
that γ and β differ on xi′ , so that xi′ satisfies (i) and (ii).) Thus, a flip on that variable is improving.
As all improving flips between outcomes containing ¬y result in one more variable xi assigned to
true, thus having the same status as it has in β, γ− ≺E β− follows.
Finally, we have β− ≺E β+ and, for every outcome γ other than β, γ+ ≺E γ− . This leads to the
following property of E = Θ3 (H, α, β).
Lemma 12 Let H be a GCP-net and let α and β be two different outcomes. Then β ≺H α if and
only if Θ3 (H, α, β) has a (strongly) dominating outcome.
428

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Proof: (Only if) Based on our earlier remarks, α+ ≺E α− . Moreover, since β ≺H α, we have
β+ ≺E α+ . In addition, for every γ different from α and β, γ+ ≺E γ− ≺E β− ≺E β+ . Thus, α− is
both dominating and strongly dominating (the latter follows from the fact that no improving flips
lead out of α− ).
(If) Let us assume that α− is dominating (and so, the argument applies also when α− is strongly
dominating). Then there is an improving sequence from β+ to α− . Let us consider a shortest such
sequence. Clearly, α+ is the outcome just before α− in that sequence (as we pointed out, no improving flip from an outcome of the form γ− to α− is possible). Moreover, by the definition of
Θ3 (H, α, β) and the fact that we are considering a shortest sequence from β+ to α− , every outcome
in the sequence between β+ and α+ is of the form γ+ . By dropping y from each of these outcomes,
we get an improving sequence from β to α.


Proposition 11 The problem EXISTENCE OF DOMINATING OUTCOME and the problem EXISTENCE
OF STRONGLY DOMINATING OUTCOME are PSPACE-complete, even if restricted to CP-nets.
Proof: We show the hardness part only, as the membership part is straightforward. To prove hardness we notice that by Lemma 12, given a consistent CP-net H and two outcomes α and β, β ≺H α
can be decided by deciding either of the problems EXISTENCE OF DOMINATING OUTCOME and
EXISTENCE OF STRONGLY DOMINATING OUTCOME for Θ3 (H, α, β). To prove the second part of
the assertion, we note that if H is consistent, E = Θ3 (H, α, β) is consistent, too and so, the mapping
from locally consistent GCP nets to CP-nets applies. Let us denote the result of applying the mapping to E by E ′ . Then, using the same argument as in the proof of Proposition 9, E has a (strongly)
dominating outcome if and only if E ′ has a strongly dominating outcome. Thus, one can decide
whether β ≺H α in a consistent CP-net H by deciding either of the problems EXISTENCE OF DOM INATING OUTCOME and EXISTENCE OF STRONGLY DOMINATING OUTCOME for E ′ .

We also note that the problem EXISTENCE
standard complexity theory assumptions).

OF NON - DOMINATED OUTCOME

Proposition 12 The problem EXISTENCE OF NON - DOMINATED

OUTCOME

is easier (under

is NP-complete.

Proof: We note that in the case of GCP-nets in conjunctive form the problem is known to be NP-hard
(Domshlak et al., 2003, 2006). Thus, the problem is NP-hard for GCP-nets. The membership in the
class NP follows from Proposition 10.

If we restrict to consistent GCP-nets, the situation simplifies. First, we recall (Lemma 9) that if
a GCP-net is consistent then weakly non-dominated and non-dominated outcomes coincide, and the
same is true for dominating and strongly dominating outcomes. Moreover, for consistent GCP-nets,
non-dominated outcomes exist (and so, the corresponding decision problem is trivially in P). Thus,
for consistent GCP-nets we will only consider problems DOMINATING OUTCOME and EXISTENCE
OF DOMINATING OUTCOME .
Proposition 13 The problems DOMINATING OUTCOME and
COME restricted to consistent GCP-nets are in coNP.
429

EXISTENCE OF DOMINATING OUT-

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Proof: Using Lemmas 8 and 9, α is not a dominating outcome if and only if there exists an outcome
β 6= α which is non-dominated. Similarly, there is no dominating outcome in a consistent GCP-net
if and only if there are at least two non-dominated outcomes. Thus, guessing non-deterministically
an outcome β 6= α, and verifying that β is non-dominated, is a non-deterministic polynomial-time
algorithm deciding the complement of the problem DOMINATING OUTCOME. The argument for the
other problem is similar.

We do not know if the bounds in Proposition 13 are tight, that is, whether these two problems
are coNP-complete. We conjecture they are.

8. Concluding Remarks
We have shown that dominance and consistency testing in CP-nets are both PSPACE-complete. Also
several related problems related to dominance and optimality in CP-nets are PSPACE-complete, too.
The repeated use of reductions from planning problems confirms the importance of the structural similarity between STRIPS planning and reasoning with CP-nets. This suggests that the welldeveloped field of planning algorithms for STRIPS representations, especially for unary operators
(Brafman & Domshlak, 2003), could be useful for implementing algorithms for dominance and
consistency in CP-nets.
Our theorems extend to CP-nets with non-binary domains, and to extensions and variations of
CP-nets, such as TCP-nets (Brafman & Domshlak, 2002; Brafman, Domshlak, & Shimony, 2006)
that allow for explicit priority of some variables over others, and the more general language for
conditional preferences (Wilson, 2004a, 2004b), where the conditional preference rules are written
in conjunctive form.
The complexity result for dominance is also relevant for the following constrained optimisation
problem: given a CP-net and a constraint satisfaction problem (CSP), find an optimal solution (a
solution of the CSP which is not dominated by any other solution of the CSP). This is computationally complex, intuitively because a complete algorithm involves many dominance checks when
the definition of dominance under constraints allows for dominance paths to go through outcomes
violating the constraints (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004b).9 The problem of
checking whether a given solution of a CSP is non-dominated can be seen to be PSPACE-complete
by a reduction from CP-dominance that uses a CSP that has exactly two solutions.
Our results reinforce the need for work on finding special classes of problems where dominance
and consistency can be tested efficiently (Domshlak & Brafman, 2002; Boutilier et al., 2004a),
and for incomplete methods for checking consistency and constrained optimisation (Wilson, 2004a,
2006).
Several open problems remain. We do not know the complexity of deciding whether the preference relation induced by a CP-net is complete. We do not know whether dominance and consistency
testing remain PSPACE-complete when the number of parents in the dependency graph is bounded
by a constant. We also do not know whether these two problems remain PSPACE-complete for
CP-nets in conjunctive form (the reduction used to prove Theorems 2 and 4 yields CP-nets that are
not in conjunctive form). Two additional open problems are listed at the end of Section 7.
9. With another possible definition, where going through outcomes violating the constraints is not allowed (Prestwich,
Rossi, Venable, & Walsh, 2005), dominance testing is not needed to check whether a given solution is non-dominated.

430

T HE C OMPUTATIONAL C OMPLEXITY OF D OMINANCE AND C ONSISTENCY IN CP-N ETS

Acknowledgments
Jérôme Lang’s new address is: LAMSADE, Université Paris-Dauphine, 75775 Paris Cedex 16,
France. The authors are grateful to the reviewers for their excellent comments, and to Pierre Marquis
for helpful discussions. This work was supported in part by the NSF under Grants ITR-0325063,
IIS-0097278 and KSEF-1036-RDE-008, by the ANR Project ANR–05–BLAN–0384 “Preference
Handling and Aggregation in Combinatorial Domains”, by Science Foundation Ireland under Grants
No. 00/PI.1/C075 and 05/IN/I886, and by Enterprise Ireland Ulysses travel grant FR/2006/36.

References
Ausiello, G., Crescenzi, P., Gambosi, G., Kann, V., Marchetti-Spaccamela, A., & Protasi, M. (1999).
Complexity and Approximation. Combinatorial optimization problems and their approximability properties. Springer Verlag.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004a). CP-nets: a tool for
representing and reasoning with conditional ceteris paribus statements. Journal of Artificial
Intelligence Research, 21, 135–191.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004b). Preference-based constrained optimization with CP-nets. Computational Intelligence, 20(2), 137–157.
Boutilier, C., Brafman, R., Hoos, H., & Poole, D. (1999). Reasoning with conditional ceteris paribus
preference statements. In Proceedings of UAI-99, pp. 71–80.
Brafman, R., Domshlak, C., & Shimony, E. (2006). On graphical modeling of preference and
importance. Journal of Artificial Intelligence Research, 25, 389–424.
Brafman, R., & Dimopoulos, Y. (2004). Extended semantics and optimization algorithms for CPnetworks. Computational Intelligence, 20(2), 218–245.
Brafman, R., & Domshlak, C. (2002). Introducing variable importance trade-offs into CP-nets. In
Proceedings of UAI-02, pp. 69–76.
Brafman, R., & Domshlak, C. (2003). Structure and complexity of planning with unary operators.
Journal of Artificial Intelligence Research, 18, 315–439.
Brandt, F., Fischer, F., & Harrenstein, P. (2007). The computational complexity of choice sets. In
Proceedings of TARK-07, pp. 82–91.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning. Artificial
Intelligence, 69(1–2), 165–204.
Domshlak, C., & Brafman, R. (2002). CP-nets—reasoning and consistency testing. In Proceedings
of KR-02, pp. 121–132.
Domshlak, C., Prestwich, S., Rossi, F., Venable, K., & Walsh, T. (2006). Hard and soft constraints
for reasoning about qualitative conditional preferences. Journal of Heuristics, 12(4/5), 263–
285.
Domshlak, C., Rossi, F., Venable, K., & Walsh, T. (2003). Reasoning about soft constraints and
conditional preferences: complexity results and approximation techniques. In Proceedings of
IJCAI-03, pp. 215–220.
431

G OLDSMITH , L ANG , T RUSZCZY ŃSKI & W ILSON

Goldsmith, J., Lang, J., Truszczyński, M., & Wilson, N. (2005). The computational complexity of
dominance and consistency in CP-nets. In Proceedings of IJCAI-05, pp. 144–149.
Lang, J. (2004). Logical preference representation and combinatorial vote. Annals of Mathematics
and Artificial Intelligence, 42(1), 37–71.
Papadimitriou, C. (1994). Computational complexity. Addison-Wesley.
Prestwich, S., Rossi, F., Venable, B., & Walsh, T. (2005). Constraint-based preferential optimization.
In Proceedings of AAAI-05, pp. 461–466.
Wilson, N. (2004a). Consistency and constrained optimisation for conditional preferences. In
Proceedings of ECAI-04, pp. 888–892.
Wilson, N. (2004b). Extending CP-nets with stronger conditional preference statements. In Proceedings of AAAI-04, pp. 735–741.
Wilson, N. (2006). An efficient upper approximation for conditional preference. In Proceedings of
ECAI-06, pp. 472–476.
Xia, L., Conitzer, V., & Lang, J. (2008). Voting on multiattribute domains with cyclic preferential
dependencies. In Proceedings of AAAI-08, pp. 202–207.

432

Journal of Artificial Intelligence Research 33 (2008) 551–574

Submitted 09/08; published 12/08

Learning to Reach Agreement in a Continuous Ultimatum Game
Steven de Jong
Simon Uyttendaele

STEVEN . DEJONG @ MICC . UNIMAAS . NL

MICC, Maastricht University
P.O. Box 616, 6200 MD Maastricht, The Netherlands

Karl Tuyls

K . P. TUYLS @ TUE . NL

Eindhoven University of Technology
P.O. Box 513, 5600 MB Eindhoven, The Netherlands

Abstract
It is well-known that acting in an individually rational manner, according to the principles of classical game theory, may lead to sub-optimal solutions in a class of problems named social dilemmas.
In contrast, humans generally do not have much difficulty with social dilemmas, as they are able
to balance personal benefit and group benefit. As agents in multi-agent systems are regularly confronted with social dilemmas, for instance in tasks such as resource allocation, these agents may
benefit from the inclusion of mechanisms thought to facilitate human fairness. Although many of
such mechanisms have already been implemented in a multi-agent systems context, their application is usually limited to rather abstract social dilemmas with a discrete set of available strategies
(usually two). Given that many real-world examples of social dilemmas are actually continuous in
nature, we extend this previous work to more general dilemmas, in which agents operate in a continuous strategy space. The social dilemma under study here is the well-known Ultimatum Game, in
which an optimal solution is achieved if agents agree on a common strategy. We investigate whether
a scale-free interaction network facilitates agents to reach agreement, especially in the presence of
fixed-strategy agents that represent a desired (e.g. human) outcome. Moreover, we study the influence of rewiring in the interaction network. The agents are equipped with continuous-action
learning automata and play a large number of random pairwise games in order to establish a common strategy. From our experiments, we may conclude that results obtained in discrete-strategy
games can be generalized to continuous-strategy games to a certain extent: a scale-free interaction network structure allows agents to achieve agreement on a common strategy, and rewiring in
the interaction network greatly enhances the agents’ ability to reach agreement. However, it also
becomes clear that some alternative mechanisms, such as reputation and volunteering, have many
subtleties involved and do not have convincing beneficial effects in the continuous case.

1. Introduction
Sharing limited resources with others is a common challenge for individuals in human societies as
well as for agents in multi-agent systems (Chevaleyre et al., 2006). Often, there is a conflict of
interest between personal benefit and group (or social) benefit. This conflict is most prominently
present in a class of problems named social dilemmas, in which individuals need to consider not
only their personal benefit, but also the effects of their choices on others, as failure to do so may
lead to sub-optimal solutions. In such dilemmas, classical game theory, which assumes players or
agents to be completely individually rational in strategic circumstances, seems to be of limited value
(Gintis, 2001; Maynard-Smith, 1982), as individually rational players are not socially conditioned.
Humans on the other hand generally show a remarkable ability to address social dilemmas, due
c
2008
AI Access Foundation. All rights reserved.

D E J ONG , U YTTENDAELE & T UYLS

to their tendency to consider concepts such as fairness in addition to personal benefit (see, e.g.
Dannenberg et al., 2007; Fehr & Schmidt, 1999; Gintis, 2001; Oosterbeek et al., 2004).
A prime example of a social dilemma is modeled in the well-known Ultimatum Game (Gueth
et al., 1982).1 In this game, two agents bargain about the division of an amount R, obtained from
an outsider. The first agent proposes an offer r2 to the second agent (e.g. ‘you receive $4 of the
$10’). If the second agent accepts, each agent gets his share (i.e. the first agent receives R − r2 , and
the second receives r2 ); however, if the second agent rejects, both agents are left with nothing. An
individually rational first agent would offer the smallest amount possible, knowing that the second
agent can then choose between obtaining this amount by accepting, or nothing by rejecting. Thus,
accepting the smallest amount possible is the individually rational response. In contrast, human
players of the game hardly (if ever) offer less than about 20%, and if such an offer occurs, it is not
likely to be accepted (Bearden, 2001; Oosterbeek et al., 2004). Thus, an individually rational player
that plays as a proposer against a human player will probably not gain any money.
Researchers have proposed various mechanisms that may be responsible for the emergence of
fair strategies in human populations playing social dilemmas, as well as the resistance of these
strategies against invasion by individually rational strategies (see, e.g. Fehr & Schmidt, 1999; Gintis, 2001; Nowak et al., 2000; Santos et al., 2006a). Often, these mechanisms have been implemented in multi-agent systems for validation purposes, i.e. if the agents can be shown to prefer
fair strategies over individually rational ones, this makes it more plausible that humans are actually
affected by the underlying mechanisms. However, we argue that multi-agent systems driven by fairness mechanisms may not only be used to validate these mechanisms, but also to allow agents to act
in a fair way in real-world applications. Given that agents often face tasks such as resource sharing
and allocation (if not explicitly, then implicitly, e.g. sharing limited computational resources), and
that these tasks regularly contain elements of social dilemmas, it is important to enable agents to
act not only based on individual rationality, but also based on fairness (Chevaleyre et al., 2006).
Unfortunately, existing work usually introduces a number of abstractions that do not allow the resulting multi-agent systems to be applied to realistic problems such as resource allocation. Most
prominently, most work has focused on social dilemmas with discrete strategy sets (usually limited
to two).2 This abstraction simplifies the dilemmas at hand and does not reflect their potential realworld nature, since in many dilemmas, especially those related to real-world resource allocation,
there is a continuum of strategies (i.e. a continuous strategy space) rather than a discrete set of
pure strategies. Moreover, in social dilemmas with continuous strategy spaces, qualifications such
as ‘cooperation’ and ‘defection’, which are often used in discrete social dilemmas, are actually relative: a certain strategy may be seen as cooperative (i.e. desirable) in a certain context, whereas
it may be either defective or simply naive in another one. It is clear that the dilemma may be far
more complicated in continuous strategy spaces, and that agents may need to use a different way of
determining whether their behavior is desirable.
1. The analogy between the Ultimatum Game and other social dilemmas, such as the Public Goods Game, can be shown
with full mathematical rigor (Sigmund et al., 2001). De Jong & Tuyls (2008) report preliminary results of applying
the methodology described in this paper to the Public Goods Game.
2. Theoretical work in the field of evolutionary game theory occasionally is not limited to discrete strategy sets. Worth
mentioning here is the work of Peters (2000), which introduces a theoretical extension of the evolutionary stable
strategy concept (ESS) for continuous strategy spaces, i.e. the extended stability calculus. This extension provides a
theoretical solution concept that can clarify egalitarian outcomes. However, the concept does not shed light on how
learning agents possibly achieve these fair outcomes in social dilemmas. The work is therefore complementary to
this work, which aims at mechanisms enabling agents to find fair outcomes.

552

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

In this paper, we generalize existing work on achieving agreement, cooperation and fairness
in social dilemmas to continuous strategy spaces, with the aim of presenting a methodology that
allows agents to reach satisfactory outcomes in these dilemmas, as well as in real-world problems
containing elements of these dilemmas. We apply our proposed methodology to the Ultimatum
Game, an example of a social dilemma. In this game, a population of agents needs to reach agreement (i.e. converge to a common strategy) in order to obtain a satisfactory payoff. This agreement
then specifies the population’s cooperative, desirable strategy. The precise nature of the agreement
may vary.3 In our population of agents, any strategy that is agreed upon by a sufficient number of
agents will be successful, and will dictate the culture, or the desirable strategy, of the population.
If we wish our agents to learn a ‘human’ strategy, we may introduce a number of simulated human
agents, i.e. agents that play according to our own desirable strategy. The learning agents should
then be able to imitate this strategy, even if they already reached agreement on a different, possibly
relatively defective, strategy.
The remainder of this paper is structured as follows. First, in §2, we give a brief overview of the
related work that this paper aims to continue. Next, in §3, we discuss our methodology, aimed at
establishing agreement in large populations of learning agents. In §4, we present experiments and
results. In §5, we outline a number of alternative approaches that were proposed for dilemmas with
discrete strategy sets, but fail to impress in a dilemma with a continuous strategy space. We discuss
why this is the case. Finally, we conclude this paper in §6.

2. Related Work
This work basically builds upon two tracks of existing work. We will give an overview of these
tracks here and indicate how they are related to our current work. For a more extensive discussion,
we refer to previous work (De Jong et al., 2008b).
2.1 Learning Fairness in Bargaining
De Jong et al. (2008a) investigated the behavior of agents playing the Ultimatum Game and the
Nash Bargaining Game with continuous action learning automata. In both games, all agents were
interacting at the same time. For the Ultimatum Game, this required an extension to more than two
players in which agents, one after the other, demanded a portion of the reward at hand. The last
player received what was left. The Homo Egualis utility function, as developed by Fehr & Schmidt
(1999) and Gintis (2001), was used to represent a desired outcome, i.e. a required minimal amount
every agent wished to obtain. Up to 100 agents were able to successfully find and maintain agreements in both games. In addition, it was observed that the solutions agreed upon corresponded to
solutions agreed upon by humans, as reported in literature. In this work, we similarly use continuous action learning automata to learn agreement in the Ultimatum Game. However, our multi-agent
system, organized in a network structure, can efficiently be populated with a much larger number of
agents (e.g. thousands). In contrast to our previous work, agents play pairwise games. Moreover,
3. Note the analogy with humans, where cultural background is one of the primary influences on what constitutes a fair,
cooperative, or desirable strategy. Although there is a general tendency to deviate from pure individual rationality in
favor of more socially-aware strategies, the exact implications vary greatly (Henrich et al., 2004; Oosterbeek et al.,
2004; Roth et al., 1991). In the Ultimatum Game for instance, the actual amounts offered and minimally accepted
vary between 10% and as much as 70%, depending on various factors, such as the amount to bargain about (Cameron,
1999; De Jong et al., 2008c; Slonim & Roth, 1998; Sonnegard, 1996), and culture (Henrich et al., 2004). Cultural
differences may persist between groups of agents, but also within these groups (Axelrod, 1997).

553

D E J ONG , U YTTENDAELE & T UYLS

we do not use the Homo Egualis utility function. Instead, the desired, human-inspired outcome
offered by the Homo Egualis utility function is replaced here by (potentially) including agents that
always play according to a certain fixed strategy (i.e. simulated human players).
2.2 Network Topology
Santos et al. (2006b) investigated the impact of scale-free networks on resulting strategies in social
dilemmas. A scale-free network was used in order to randomly determine the two agents (neighbors) that would play together in various social dilemmas, such as the Prisoner’s Dilemma and the
Snowdrift Game. The agents were limited to two strategies, i.e., cooperate and defect, which were
initially equally probable. It was observed that, due to the scale-free network, defectors could not
spread over the entire network in both games, as they do in other network structures. The authors
identified that the topology of the network contributed to the observed maintained cooperation. In
subsequent research, Santos et al. (2006a) introduced rewiring in the network and played many different social dilemmas, once again with two strategies per agent. They concluded that the ‘ease’
(measure of individuals’ inertia to readjust their ties) of rewiring was increasing the rate at which
cooperators efficiently wipe out defectors. In contrast to the work of Santos et al. (2006a,b), which
used a discrete strategy set, this work uses a continuous strategy space. This requires another view
on fairness, cooperation and agreement, departing from the traditional view that fairness is achieved
by driving all agents to (manually labeled) cooperative strategies. In social dilemmas such as the
Ultimatum Game, any strategy that agents may agree on leads to satisfactory outcomes.

3. Methodology
Before discussing our methodology in detail, we first outline our basic setting. We continue by
explaining continuous-action learning automata, as they are central to our methodology. Next, we
discuss the structure and topology of the networks of interaction we use. After this we discuss the
agent types and initial strategies of agents. Then we elaborate on how we provide the additional
possibility of rewiring connections between agents. Finally, we explain our experimental setup.
3.1 The Basic Setting
We study a large group of adaptive agents, driven by continuous action learning automata, playing
the Ultimatum Game in pairwise interactions. Pairs are chosen according to a (scale-free) network
of interaction. Every agent is randomly assigned the role of proposer or responder in the Ultimatum
Game. Agents start with different strategies. For good performance, most of them need to converge
to agreement by playing many pairwise games, i.e. they need to learn a common strategy. Some
agents may be fixed in their strategies; these agents represent an external strategy that the adaptive
agents need to converge to, for instance a preference dictated by humans. As an addition to this
basic setting, we study the influence of adding the option for agents to rewire in their network of
interaction as a response to an agent that behaved in a defecting manner.
3.2 Continuous Action Learning Automata
Continuous Action Learning Automata (CALA; Thathachar & Sastry, 2004) are learning automata
developed for problems with continuous action spaces. CALA are essentially function optimizers:
for every action a from their continuous, one-dimensional action space A, they receive a feedback
554

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

β (x) – the goal is to optimize this feedback. CALA have a proven convergence to (local) optima,
given that the feedback function β (x) is sufficiently smooth. The advantage of CALA over many
other reinforcement learning techniques (see, e.g. Sutton & Barto, 1998), is that it is not necessary
to discretize continuous action spaces, because actions are simply real numbers.
3.2.1 H OW CALA W ORK
Essentially, CALA maintain a Gaussian distribution from which actions are pulled. In contrast to
standard learning automata, CALA require feedback on two actions, being the action corresponding
to the mean µ of the Gaussian distribution, and the action corresponding to a sample x, taken from
this distribution. These actions lead to a feedback β (µ) and β (x), respectively, and in turn, this
feedback is used to update the probability distribution’s µ and σ. More precisely, the update formula
for CALA can be written as:
x−µ
µ = µ + λ β(x)−β(µ)
Φ(σ)
Φ(σ)


2
β(x)−β(µ)
x−µ
σ = σ + λ Φ(σ)
− 1 − λK (σ − σL )
Φ(σ)

(1)

In this equation, λ represents the learning rate; K represents a large constant driving down σ. The
variance σ is kept above a threshold σL to keep calculations tractable even in case of convergence.4
This is implemented using the function:
Φ (σ) = max (σ, σL )

(2)

The intuition behind the update formula is quite straightforward (De Jong et al., 2008a). Using this
update formula, CALA rather quickly converge to a (local) optimum. With multiple (e.g. n) learning
automata, every automaton i receives feedback with respect to the joint actions, respectively βi (µ̄)
and βi (x̄), with µ̄ = µ1 , . . . , µn and x̄ = x1 , . . . , xn . In this case, there still is convergence to a
(local) optimum (Thathachar & Sastry, 2004).
3.2.2 M ODIFICATIONS TO CALA FOR OUR P URPOSES
As has been outlined above, we use CALA to enable agents to learn a sensible proposer and responder strategy in the Ultimatum Game. When playing the Ultimatum Game, two agents may agree
on only one of their two joint actions (i.e. they obtain one high and one very low feedback), or
may even disagree on both of them (i.e. they obtain two very low feedbacks). Both situations need
additional attention, as their occurrence prevents the CALA from converging to correct solutions.
To address these situations, we propose two domain-specific modifications to the update formula of
the CALA (De Jong et al., 2008a).5
First, in case both joint actions yield a feedback of 0, the CALA are unable to draw effective
conclusions, even though they may have tried a very ineffective strategy and thus should actually
4. We used the following settings after some initial experiments: λ = 0.02, K = 0.001 and σL = 10−7 . The
precise settings for λ and σL are not a decisive influence on the outcomes, although other values may lead to slower
convergence. If K is chosen to be large, as (rather vaguely) implied by Thathachar & Sastry (2004), then σ decreases
too fast, i.e. usually the CALA stop exploring before a sufficient solution has been found.
5. Note that such modifications are not uncommon in the literature; see, e.g. the work of Selten & Stoecker (1986) on
learning direction theory. Grosskopf (2003) successfully applied directional learning to the setting of the Ultimatum
Game, focusing on responder competition (which is not further addressed in this paper).

555

D E J ONG , U YTTENDAELE & T UYLS

learn. In order to counter this problem, we introduce a ‘driving force’, which allows agents to update
their strategy even if the feedback received is 0. This driving force is defined as:

For proposers: β (µ) = µ − x
iff β (µ) = β (x) = 0
(3)
For responders: β (µ) = x − µ
The effect of this modification, which we call zero-feedback avoidance (ZFA), is that an agent
playing as the proposer will learn to offer more, and an agent playing as the responder will accept
to lower his expectation. In both roles, this will lead to a more probable agreement.
Second, if one joint action yields agreement, but the other a feedback of 0, the CALA may adapt
their strategies too drastically in favor of the first joint action – in fact, shifts of µ to values greater
than 109 were observed (De Jong & Tuyls, 2008; De Jong et al., 2008a). To tackle this problem,
we restrict the difference that is possible between the two feedbacks the CALA receive in every
iteration. More precisely, we empirically set:


 β (µ) − β (x) 

≤1
(4)


Φ (σ)
Thus, if there is a large difference in feedback between the µ-action and the x-action, we preserve
the direction indicated by this feedback, but prevent the automaton to jump too far in that direction.
We call this modification strategy update limitation (SUL).
3.3 The Network of Interaction
A scale-free network (Barabasi & Albert, 1999) is a network in which the degree distribution follows
a power law. More precisely, the fraction P (k) of nodes in the network having k connections to
other nodes goes for large values of k as P (k) ∼ k −γ . The value of the constant γ is typically in the
range 2 < γ < 3. Scale-free networks are noteworthy because many empirically observed networks
appear to be scale-free, including the world wide web, protein networks, citation networks, and also
social networks. The mechanism of preferential attachment has been proposed to explain power law
degree distributions in some networks. Preferential attachment implies that nodes prefer attaching
themselves to nodes that already have a large number of neighbors, over nodes that have a small
number of neighbors.
Previous research has indicated that scale-free networks contribute to the emergence of cooperation (Santos et al., 2006b). We wish to determine whether this phenomenon still occurs in
continuous strategy spaces and therefore use a scale-free topology for our interaction network, using the Barabasi-Albert model. More precisely, the probability pi that a newly introduced node is
connected to an existing node i with degree ki is equal to:
ki
pi = P
(5)
j kj
When we construct the network, the two first nodes are linked to each other, after which the other
nodes are introduced sequentially and connected to one or more existing nodes, using pi . In this
way, the newly introduced node will more probably connect to a heavily linked hub than to one
having only a few connections. In our simulations, we connect every new node to one, two or three
existing ones (uniform probabilities). This yields networks of interaction that are more realistic than
the acyclic ones obtained by always connecting new nodes to only one existing node. For example,
if the network is modeling a friendship network, avoiding cycles means assuming that all friends of
a certain person are never friends of each other.
556

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

3.4 Agent Types and Strategies
In order to study how agreement concerning a common strategy emerges, we need to make our
agents learn to reach such a common strategy, starting from a situation in which it is absent (i.e.
agents have different strategies). Moreover, we need to study whether a common strategy can
be established from ‘example’ agents, and whether it is robust against agents that use a different,
potentially relatively defective strategy.
3.4.1 T WO T YPES OF AGENTS
We introduce two types of agents, i.e. dynamic strategy (DS) agents and fixed strategy (FS) agents.
DS agents are the learning agents. They start with a certain predefined strategy and are allowed to
adapt their strategy constantly, according to the learning mechanism of their learning automaton.
Basically, these agents are similar to those used in earlier work (De Jong et al., 2008a). FS agents
are (optional) ‘good examples’: they model an example strategy that needs to be learned by the
(other) agents in our system, and therefore refuse to adapt this strategy.
3.4.2 O NE OR T WO CALA PER AGENT ?
As has been outlined above, each agent needs to be able to perform two different roles in the Ultimatum Game, i.e. playing as the proposer as well as playing as the responder. In other words, an
agent is in one of two distinct states, and each state requires it to learn a different strategy. As CALA
are stateless learners, each agent therefore would require two CALA. Nonetheless, in the remainder
of this paper, we equip every DS agent with only one CALA, representing both the agent’s proposer
strategy as well as its responder strategy.
Our choice for one CALA is motivated by two observations, i.e. (1) human behavior, and (2)
some initial experiments. First, human strategies are often consistent, implying that they generally
accept their own offers, but reject offers that are lower (Oosterbeek et al., 2004), even with high
amounts at stake (De Jong et al., 2008c; Sonnegard, 1996). Second, in a set of initial experiments,
we observed that agents using two CALA will generally converge to one single strategy anyway. As
an illustration, three learning curves obtained in a fully connected network of three agents playing
the Ultimatum Game are displayed in Figure 1. It is clearly visible that agents’ proposer strategies
(bold lines) are strongly attracted to other agents’ responder strategies (thin lines), and especially to
the lowest of these responder strategies. In the presence of a FS agent that offers 4.5 and accepts at
least 1, the first strategy is immediately ignored in favor of the (lower) second one. With only DS
agents, once again all strategies are attracted to the lowest responder strategy present.6
In future work, we will study this observation from the perspective of evolutionary game theory
and replicator equations (Gintis, 2001; Maynard-Smith & Price, 1973). For the current paper, we
use the observation to justify an abstraction, i.e. we limit the complexity of our agents by equipping
them with only one CALA. This CALA then represents the agent’s proposer strategy as well as
it’s responder strategy. It is updated when the agent plays as a proposer as well as when it plays
as a responder, according to the CALA update formula presented in §3.2.1 and the modifications
presented in §3.2.2. Thus, the agents’ single CALA receive twice as much feedback as two separate
CALA would. This abstraction therefore increases the efficiency of the learning process.
6. The agents more quickly adapt their strategies downward than upward (Figure 1). Therefore, when multiple (e.g. 10)
DS agents are learning (i.e. without any FS agents), their strategy usually converges to 0. This is due to an artifact of
the learning process; two CALA trying to learn each others’ current strategy tend to be driven downward.

557

5

5

4.5

4.5

4

4

3.5

3.5

3

Strategy

Strategy

D E J ONG , U YTTENDAELE & T UYLS

2.5
2
1.5

2
1.5

1

1

0.5

0.5

0

0
500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

10

(top-left) Two DS agents, one starting at offering and accepting 4.5, and one starting at offering and accepting 0.01, learn
to play optimally against each other and a FS agent offering
4.5 and accepting 1. The DS agents rather quickly learn to
offer and accept 1.

9
8
7
Strategy

3
2.5

6

(top-right) Two DS agents, both starting at offering 4.5 and
accepting 1, learn to play optimally against each other and
a FS agent also offering 4.5 and accepting 1. Again the DS
agents learn to offer and accept 1.

5
4
3

(bottom-left) Three DS agents, starting at different initial
strategies (i.e. offering 9, 4.5, and 1, and accepting 3, 2 and
1, respectively), quickly learn a single, similar strategy.

2
1
0
500 1000 1500 2000 2500 3000 3500 4000 4500
Iteration

Figure 1: Evolving strategies in a fully connected network of three agents. Proposal strategies are
indicated with a bold line, response strategies are indicated with a thin line. Agents
converge to a situation in which their two initial strategies become similar.

3.4.3 AGENTS ’ S TRATEGIES
In our simulations, we use two types of DS agents and one type of FS agents. More precisely,
DSr agents are learning agents that start at a rational solution of offering X ∼ N (0.01, 1) (and
also accepting their own amount or more). DSh agents start with a more human, fair solution, i.e.
of offering X ∼ N (4.5, 1) (and also accepting their own amount or more). Since FS agents are
examples of a desired solution, we equip them with a fair, human-inspired solution to see whether
the other agents are able to adapt to this solution. The FS agents always offer 4.5, and accept
any offer of 4.5 or more. All agents are limited to strategies taken from a continuous interval
c = [0, 10], where 10 is chosen as the upper bound (instead of the more common 1) because it is a
common amount of money that needs to be shared in the Ultimatum Game. If any agent’s strategy
falls outside the interval c, we round off the strategy to the nearest value within the interval.
3.5 Rewiring
Agents play together based on their connections in the interaction network. Thus, in order to avoid
playing with a certain undesirable neighbor j, agent i may decide to break the connection between
558

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

him and j and create a new link to a random neighbor of j (Santos et al., 2006a).7 For rewiring,
we use a heuristic proposed by Santos et al.: agents want to disconnect themselves from (relative)
defectors, as they prefer to play with relative cooperators. Thus, the probability that agent i unwires
from agent j, is calculated as:
si − sj
pr =
(6)
C
Here, si and sj are the agents’ current strategies (more precisely, agent i’s responder strategy and
agent j’s proposer strategy), and C is the amount at stake in the Ultimatum Game, i.e. 10. Even if
agents determine that they want to unwire because of this probability, they may still not be allowed
to, if this breaks the last link for one of them. If unwiring takes place, agent i creates a new wire to
a random neighbor of agent j.
3.6 Experimental Setup
Using the aforementioned types of agents, we need to determine whether our proposed methodology
possesses the traits that we would like to see. Our population can be said to have established a
successful agreement if it manages to reach a common strategy that incorporates the preferences of
the good examples, while at the same time discouraging those agents that try to exploit the dominant
strategy. Thus, in a population consisting of only DS agents, any strategy that is shared by most (or
all) agents leads to good performance, since all agents agree in all games, yielding an average payoff
of 5 per game per agent – our architecture should be able to find such a common strategy. When
using DS as well as FS agents, the FS agents dictate the strategy that the DS agents should converge
to, regardless of whether they start as DSh or as DSr agents.
In order to measure whether the agents achieved a satisfactory outcome, we study four quantities
related to the learning process and the final outcome, viz. (1) the point of convergence, (2) the
learned strategy, (3) the performance and (4) the resulting network structure. We will briefly explain
these four quantities below. In general, we remark that every simulation lasts for 3, 000 iterations
per agent, i.e. 3, 000n iterations for n agents. We repeat every simulation 50 times to obtain reliable
estimates of the quantities of interest.
3.6.1 P OINT OF C ONVERGENCE
The most important quantity concerning the agents’ learning process is the point of convergence,
which, if present, tells us how many games the agents needed to play in order to establish an agreement. To determine the point of convergence, we calculate and save the average population strategy
avg(t) after each pairwise game (i.e. each iteration of the learning process). After T iterations, we
obtain an ordered set of T averages, i.e. {avg(1), . . . , avg(T )}. Initially, the average population
strategy changes over time, as the agents are learning. At a certain point in time t, the agents stop
learning, and as a result, the average population strategy avg(t) does not change much anymore.
To estimate this point t, i.e. the point of convergence, we find the lowest t for which the standard
deviation on the subset {avg(t), . . . , avg(T )} is at most 10−3 . Subsequently, we report the number
of games per agent played at iteration t, i.e. nt . In our experiments, every simulation is repeated
7. Note that we may also choose to allow an agent i to create a new connection to specific other agents instead of only
random neighbors of their neighbor j. However, especially in combination with reputation (see §5.1), this allows
(relative) defectors to quickly identify (relative) cooperators, with which they may then connect themselves in an
attempt to exploit. Preliminary experiments have shown that this behavior leads to the interaction network losing its
scale-freeness, which may seriously impair the emergence of agreement.

559

D E J ONG , U YTTENDAELE & T UYLS

Avg
Std
Conv

0

0

00
45

0
00

00
40

35

00

0

0
30

0

00
25

0
00

00
20

0
15

00

00
50

Iteration

10

Strategy

10
9
8
7
6
5
4
3
2
1
0
0

0
00

0

Avg
Std
Conv

Population strategy

45

0
00

00
40

35

00

0

0
30

0

00
25

0
00

00
20

00

00

0
15

10

50

0

Strategy

Population strategy
10
9
8
7
6
5
4
3
2
1
0

Iteration

Figure 2: Two examples of the convergence point of a single run. In both graphs, we display the
average strategy of the population (bold line) as well as the standard deviation on this
average (thin line). The dotted vertical line denotes the convergence point, as found by
the analysis detailed in the text.

50 times, resulting in 50 convergence points. We will use a box plot to visualize the distribution of
these 50 convergence points.8
As an example, in Figure 2 (left), we see how 17 FS agents, 17 DSh agents and 16 DSr agents
converge to agreement, using rewiring. Only the first 50, 000 games are shown. In addition to a
bold line denoting the average population strategy, we also plot a thinner line, denoting the standard
deviation on this average. Using the method outlined above, the point of convergence is determined
to be around 27, 500 games, i.e. approximately 550 games per agent were necessary. In Figure
2 (right), we show similar results for 10 FS agents and 40 DSr agents, once again using rewiring.
Here, the point of convergence is around 34, 000 games, i.e. approximately 680 games per agent
were necessary, which means that learning to reach agreement was more difficult.
3.6.2 L EARNED S TRATEGY
Once we established at which iteration t the agents have converged, we can state that the average
learned strategy is precisely avg(t). We repeat every simulation 50 times to obtain a reliable estimate of this average. Once again, in our results, we use a box plot to visualize the distribution of
the average learned strategy.
3.6.3 P ERFORMANCE
To measure performance, we first allow our agents to learn from playing 3, 000 Ultimatum Games
each. Then, we fix the strategies of all DS agents. We let every agent play as a proposer against all
its neighbors (one by one), and count the number of games that were successful.9 We divide this
8. In our box plots, we report the average instead of the median, as the average is a more informative quantity, e.g. when
comparing our results with existing work. This may result in the box plots’ mid point being located outside the box.
9. Note that the CALA update formula prevents agents from converging to an exact strategy, as the standard deviation
of the CALA’s Gaussian is kept artificially strictly positive. Therefore, there is some noise on the strategies agents
have converged to. To counter this noise while measuring performance, we set responders’ strategies to 99% of their
actual strategies. Thus, an agent having a strategy of 4 will propose 4 and accept any offer of 3.96 or more.

560

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

number through the total number of games played (i.e. twice the number of edges in the interaction
network). The resulting number denotes the performance, which lies between 0 (for utterly catastrophic) and 1 (for complete agreement). Human players of the Ultimatum Game typically achieve
a performance of 0.8–0.9 (Fehr & Schmidt, 1999; Oosterbeek et al., 2004). Once again, the 50
repetitions lead to 50 measures of performance, which are displayed in a box plot in our results.
3.6.4 R ESULTING N ETWORK S TRUCTURE
Since the network of interaction may be rewired by agents that are not satisfied about their neighbors, we are interested in the network structure resulting from the agents’ learning processes. We
examine the network structure by looking at the degree distribution of the nodes in the network (i.e.
the number of neighbors of the agents). With 50 repeated simulations, we may draw a single box
plot expressing the average degree distribution.

4. Experiments and Results
We present our experiments and results in two subsections. First, we study a setup without rewiring
and a setup with rewiring, varying the number of agents, while keeping the proportion of DSh, DSr
and FS agents constant and equal (i.e. 33% for each type of agent). Second, we study the same
two setups with various population sizes, this time varying the proportion of FS agents, where the
remainder of the population is half DSh and half DSr. In general, we remark that every experiment
reports results that are averaged over 50 simulations. In every simulation, we allow the agents to
play 3, 000n random games, where n denotes the number of agents (i.e. the population size).
4.1 Varying the Population Size
In many multi-agent systems, increasing the number of agents (i.e. the population size) causes
difficulties. Many mechanisms that work with a relatively low number of agents stop working
well with a high number of agents, for instance due to computational complexity or undesired
emergent properties. According to previous research, this issue of scalability also applies to the
task of learning social dilemmas. Indeed, previous research using evolutionary algorithms in games
with discrete strategy sets mentions that the number of games needed to converge to an agreement
(i.e. on cooperation) may be “prohibitively large” (Santos et al., 2006a).10
Since our agents are learning in continuous strategy spaces, we may expect a scalability issue
as well. To determine whether our proposed methodology has such an issue, we vary the population
size between 10 and 10, 000 (with some steps in between), while keeping the proportion of FS,
DSh and DSr agents constant at one-third each. We study a setup without rewiring as well as a
setup with rewiring, and determine (1) the point of convergence, i.e. the number of games per agent
needed to reach convergence; (2) the average learned strategy the agents converged to; (3) the final
performance of the system; and finally (4) the resulting network structure. Especially the first and
third of these quantities give an indication of the scalability of our methodology.
10. In order to limit the time taken for learning, Santos et al. (2006a) terminate the learning process after 108 iterations,
while using at most 103 agents, leading to an average of (more than) 105 games per agent being available. Still, with
this high number of games per agent, they report that the agents occasionally do not converge.

561

D E J ONG , U YTTENDAELE & T UYLS

W ITH REWIRING
3000

2500

2500
Games per agent

Games per agent

N O REWIRING
3000

2000
1500
1000

2000
1500
1000
500

500

0

0
10

50

100 200 500
Number of agents

10

1000 10000

50

100 200 500 1000 10000
Number of agents

Figure 3: Games per agent until convergence; without rewiring (left) and with rewiring (right).

4.1.1 P OINT OF C ONVERGENCE
A setup without rewiring (Figure 3, left) tends to require more games per agent as the total number
of agents increases. At a certain point, i.e. around a population size of 200 agents, this tendency
stops, mainly because the average number of games per agent approaches the maximum, i.e. 3, 000
games per agent. A setup with rewiring (same figure, right) convincingly outperforms one without
rewiring, as increasing the population size hardly affects the number of games per agent required
to reach convergence. Independent from the population size, the setup requires approximately 500
games per agent to converge. Note the difference with previous research (i.e. Santos et al., 2006a),
which reports requiring 105 games per agent (or more).
4.1.2 L EARNED S TRATEGY
A setup without rewiring (Figure 4, left) on average converges to a strategy of offering as well as
accepting around 3, where 4.5 would be required, as the 33% FS agents present in the population
all play with this strategy (i.e. the 66% DS agents on average have a strategy of 2). With increasing
population size, this average strategy is not affected; however, it becomes more and more with
certainty established. Once again, a setup with rewiring (same figure, right) shows convincingly
better results. Independent from the population size, the learning agents all converge to the desired
strategy, i.e. 4.5.
4.1.3 P ERFORMANCE
With a setup without rewiring (Figure 5, left), we already saw that the average learned strategy of
the DS agents is not very good. Performance is seriously affected; at around 60%, it indicates that
few DS agents ever agree with FS agents. However, average performance is not influenced by the
population size. As with the learned strategy, the performance of around 60% only becomes more
certainly established. As expected, a setup with rewiring (same figure, right) shows much more
satisfying results, i.e. generally above 80% agreement. These results are actually positively affected
by the population size, as the average performance increases with an increasing population.
562

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

N O REWIRING

W ITH REWIRING
5
Average converged strategy

Average converged strategy

5
4
3
2
1

4
3
2
1
0

0
10

50

100
200
500
Number of agents

1000

10

10000

50

100 200 500
Number of agents

1000 10000

Figure 4: Average learned strategy; without rewiring (left) and with rewiring (right).

W ITH REWIRING

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Performance at convergence

Performance at convergence

N O REWIRING

10

50

100
200
500
Number of agents

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10

1000 10000

50

100 200 500
Number of agents

1000 10000

Figure 5: Final performance; without rewiring (left) and with rewiring (right).

4.1.4 R ESULTING N ETWORK S TRUCTURE
We look at the network structure resulting from learning to reach agreement, and determine whether
this structure is influenced by the population size. Obviously, a setup without rewiring (Figure 6,
left) does not display any influence here, as the network is static. A setup with rewiring (same
figure, right) shows an interesting tendency. The average degree of the resulting network stays low,
while the maximum degree increases with an increasing population size. Clearly, as the population
size increases, the hubs in the scale-free network receive more and more preferential attachment,
and correspondingly, less densely connected nodes become even less densely connected. When we
examine the number of times agents actually rewire, we find that this number generally lies below
1, 000, i.e. a very low percentage of the total number of games played actually made the agents
rewire to a random neighbor of an undesired proposer.
563

D E J ONG , U YTTENDAELE & T UYLS

W ITH REWIRING
150

120

120

Degree distribution

Degree distribution

N O REWIRING
150

90
60
30

90
60
30
0

0
10

50

100 200 500
Number of agents

10

1000 10000

50

100 200 500
Number of agents

1000 10000

Figure 6: Resulting network structure; without rewiring (left) and with rewiring (right).
4.1.5 I N C ONCLUSION
In conclusion to this subsection, we may state that the proposed methodology is not suffering from
severe scalability issues. A setup that does not include rewiring is clearly outperformed by one
that does include rewiring, but neither a setup without rewiring, nor a setup with rewiring, suffer
severely from increasing the number of agents.
4.2 Varying the Proportion of Good Examples (FS Agents)
In this section, we investigate the behavior of the proposed methodology when the proportion of
good examples in the population (i.e. FS agents with a strategy of 4.5) is varied. The remainder
of the population consists of DSh and DSr agents in equal proportions. We experimented with a
number of population sizes, ranging from 50 to 500.
Since the results for each population size are rather similar, we restrict ourselves to graphically
reporting and analyzing the results of our experiments with 100 agents in the remainder of this
section. A selection of the remaining results is given in Table 1. Specifically, for a setup without rewiring and a setup with rewiring, we report on the population size (Pop), the percentage FS
agents used (%FS), the average number of games per agent needed to converge (Games), the average learned strategy (Strat), the average performance (Perf), and finally, the maximum number of
connections that a single agent has with other agents in the network (Netw). As we will discuss
below, the results reported in Table 1 for population sizes other than 100 are highly similar to those
for a population size of 100 agents.
4.2.1 P OINT OF C ONVERGENCE
A setup without rewiring (Figure 7, left) requires more and more games per agent to converge, until
the proportion of FS agents reaches around 30%. Then, the required number of games decreases
again, although there is a great deal of uncertainty. Introducing rewiring (same figure, right) yields
much better results. The number of games required per agent hardly exceeds 700, and this number
decreases steadily with an increasing proportion of the population being an FS agent.
564

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

N O REWIRING
Pop

% FS

50

W ITH REWIRING

Games

Strat

Perf

Netw

Pop

% FS

0
30
50
80

663.80
2,588.50
1,800.02
259.86

0.01
2.87
4.12
4.47

0.63
0.59
0.70
0.87

15
15
16
15

50

0
30
50
80

200

0
30
50
80

671.30
2,796.85
1,354.80
288.35

0.01
2.64
4.17
4.47

0.63
0.57
0.70
0.88

18
17
18
18

200

500

0
30
50
80

662.50
2,793.55
1,237.75
264.60

0.01
2.85
4.18
4.47

0.64
0.59
0.69
0.89

20
20
21
21

500

Games

Strat

Perf

Netw

639.38
528.52
485.60
356.34

0.01
4.45
4.47
4.49

0.63
0.81
0.89
0.96

22
38
29
23

0
30
50
80

743.00
540.40
493.40
382.20

0.01
4.45
4.47
4.49

0.62
0.87
0.91
0.97

20
52
28
24

0
30
50
80

650.20
549.95
498.00
380.91

0.01
4.45
4.47
4.49

0.65
0.87
0.92
0.97

60
100
55
35

Table 1: Summary of the results of experiments in which the proportion of FS agents is varied. For
details and additional results, see the main text.
W ITH REWIRING

3000

3000

2500

2500
Games per agent

Games per agent

N O REWIRING

2000
1500
1000

2000
1500
1000

500

500

0

0
0

0 10 20 30 40 50 60 70 80 90 100
Percentage FS agents

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 7: Games per agent until convergence; without rewiring (left) and with rewiring (right).

4.2.2 L EARNED S TRATEGY
Interestingly, a population consisting of only DS agents tends to converge to offering and accepting
the lowest amount possible, both in a setup that does not use rewiring (Figure 8, left), as well as in
a setup that does (same figure, right). As has been explained in §3, DS agents tend to adapt their
strategies downward more easily than upward. Thus, two DS agents that are having approximately
the same strategy, may slowly pull each others’ strategy downward. With many DS agents, the
probability that this happens increases. Adding FS agents to the population results in different
behavior for the two setups. A setup without rewiring has difficulties moving away from the lowest
amount possible; only with a sufficient number of FS agents (i.e. 30% of the population) does the
average learned strategy reflect that the DS agents move towards the strategy dictated by the FS
agents. With rewiring, results are convincingly better; even with only 10% FS agents, the DS agents
on average converge towards offering and accepting the amount dictated by these agents, i.e. 4.5.
565

D E J ONG , U YTTENDAELE & T UYLS

N O REWIRING

W ITH REWIRING
5
Average converged strategy

Average converged strategy

5
4
3
2
1

4
3
2
1
0

0
0

10

20

30 40 50 60 70
Percentage FS agents

80

0

90 100

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 8: Average learned strategy; without rewiring (left) and with rewiring (right).
W ITH REWIRING

Performance at convergence

Performance at convergence

N O REWIRING
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 9: Final performance; without rewiring (left) and with rewiring (right).
4.2.3 P ERFORMANCE
The observations concerning the learned strategy, as reported above, are reflected in the performance
of the collective of agents. In a setup without rewiring (Figure 9, left), performance decreases
initially with an increasing proportion of FS agents, as the DS agents refuse to adapt to the dictated
strategy. When the proportion of FS agents becomes large enough, the DS agents start picking up
this strategy, resulting in increasing performance. A setup with rewiring (same figure, right) does
better, as performance increases with an increasing number of FS agents. Even though the average
learned strategy is close to 4.5 for every proportion of FS agents, low proportions of FS agents
still display less performance than higher proportions. This may require additional explanation.
Note that the box plot of Figure 8 shows the distribution of the average strategy over 50 repeated
simulations; i.e. it does not show the strategy distribution within a single simulation.
Thus, even though the average strategy in a single simulation is always very close to 4.5, there is
still variance. With a low number of FS agents, this variance is most prominently caused by inertia,
566

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

W ITH REWIRING

Degree distribution

Degree distribution

N O REWIRING
100
90
80
70
60
50
40
30
20
10
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

100
90
80
70
60
50
40
30
20
10
0
0

10 20 30 40 50 60 70 80 90 100
Percentage FS agents

Figure 10: Resulting network structure; without rewiring (left) and with rewiring (right).
i.e. not all DS agents are directly connected to an FS agent, which implies that they need to learn
their desired strategy from neighboring agents who are also learning. Especially with rewiring, this
may result in two agents playing together that are compatible with most of their neighbors, but not
(yet) with each other.
4.2.4 R ESULTING N ETWORK S TRUCTURE
Clearly, the network structure of a setup without rewiring (Figure 10, left) is not influenced by
varying the proportion of FS agents. When rewiring is used (same figure, right), we observe an
interesting phenomenon, closely related to our observations in §4.1. Once again, the number of
times agents actually rewire generally lies below 1, 000. Even though this is a low number, it does
affect the network structure in a useful way. With a low proportion of FS agents, there is a large
tendency for increased preferential attachment. With 10% FS agents for instance, there is a single
agent that connects to 70 out of 100 other agents. With an increasing proportion of FS agents, the
maximum degree of the network decreases, until finally, it closely resembles the original network.
Clearly, in the presence of only few examples of the desired strategy, DS agents attempt to connect
to other agents that provide such examples. This is interesting and useful emergent behavior.
4.2.5 I N C ONCLUSION
When we compare the results obtained in a population of 100 agents with the results for other
population sizes, as reported in Table 1, we see that these are highly similar. In conclusion to this
subsection, we may state that a setup that is not using rewiring has severe difficulties converging to
a desired example if the proportion of FS agents providing this example is low. Only for, e.g. half
of the population consisting of examples, does the other half learn the desired behavior. A setup
that is using rewiring has absolutely no problems converging to the desired strategy, even with a
low proportion of FS agents. In both cases, completely omitting the examples leads to the agents
converging to the individually rational solution. This is caused by an artifact of the learning method
used, i.e. as mentioned before, two CALA trying to learn each others’ strategy tend to be driven
downward to the lowest value allowed.
567

D E J ONG , U YTTENDAELE & T UYLS

5. Discussion
The results presented in the previous section suggest that mechanisms that lead to cooperative solutions in social dilemmas with only a discrete set of strategies (e.g. scale-free networks and rewiring),
also lead to agreement in social dilemmas with a continuous strategy space. In this section, however, we show that this is not a trivial issue. More precisely, we discuss a number of mechanisms
that enhance agents’ abilities to reach cooperation in social dilemmas with discrete strategy sets,
but do not directly enhance agents’ abilities to reach agreement in continuous strategy spaces. We
empirically analyze why this is the case.
5.1 Reputation
Reputation is one of the main concepts used in behavioral economics to explain how fairness
emerges (e.g. Bowles et al., 1997; Fehr, 2004). Basically, it is assumed that interactions between
people lead to expectations concerning future interactions. These expectations may be positive or
negative and may be kept to oneself, or actually shared with peers.
In work closely related to our work, Nowak et al. (2000) show that reputation deters agents
from accepting low offers in the Ultimatum Game, as this information will spread, leading to the
agents also receiving low offers in return. Then, if all agents refuse to accept low offers, they
should provide high offers. Thus, Nowak et al. argue that the population goes toward providing
and accepting high offers. However, we note that any shared strategy (i.e. any agreement) in the
Ultimatum Game yields an expected payoff of 50% of the amount at stake for both agents. Thus,
reputation may indeed help agents to decide which strategy to play against others, but a preference
for playing cooperatively (i.e. providing high offers) does not directly result from reputation.
5.1.1 S PREADING R EPUTATION
We study the effects of reputation by optionally adding a second network to our system. As with
the interaction network, we consider the reputation network to be scale-free. In contrast to the
interaction network however, the reputation network is assumed to be static, as agents are truthful
concerning reputation, making it unnecessary for agents to consider rewiring. Note that two agents
sharing reputation information may or may not be connected as well in the interaction network,
and as a consequence, two agents playing an Ultimatum Game may or may not share reputation
information with each other. In effect, after every Ultimatum Game, the responding agent may
broadcast reputation information to its neighbors in the reputation network. The information is sent
by the responder and concerns the offer just done by the proposer; this is the only information that
is guaranteed to be correct. Agents receive information with a probability:
pij = 1 −

d
H

(7)

Here, d is the distance between the sender and the (potential) receiver j in the reputation network.
Thus, reputation information may travel for at most H hops, with a decreasing probability per hop.
In our simulations, we set H = 5. We note that in relatively small networks, this implies that
reputation information is essentially public.
Note that reputation information may be helpful only if we allow agents to do something with
this information. In the work of Nowak et al. (2000), for instance, the reputation of others is used
by agents to determine what to offer to these others. Given (1) the observation that reputation, used
568

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

in this way, should not necessarily promote cooperative strategies (see above), and (2) the fact that
we already use CALA to determine what agents offer to each other, we want the reputation to affect
something else than agents’ strategies. We will discuss a number of ways in which agents may use
reputation, as taken from literature, i.e. interacting with a preferred neighbor (below) and using
reputation to facilitate voluntary participation (§5.3).
5.1.2 U SING R EPUTATION
Without reputation, agents play against a random neighbor in the interaction network. Reputation
may be used to make agents prefer interacting with specific neighbors – Chiang (2008) discusses that
strategies of fairness could evolve to be dominant if agents are allowed to choose preferred partners
to play against. Chiang allows agents to select partners that have helped the agent previously.
To determine who is a preferred partner, we use the heuristic proposed by Santos et al. (2006a),
i.e. an agent prefers playing with (relative) cooperators, as these help it in obtaining a high payoff
if it is the responder. Thus, the probability that agent i plays with agent j ∈ Ni , where Ni is the set
of agent i’s neighbors, is:
s̃j − si
pij = P
(8)
k∈Ni s̃k
Here, si , s̃j and s̃k are the agents’ current strategies (for agents other than i, these are estimates
based on reputation and previous experience).
There are two problems with this approach. First, the number of times that an agent i receives
information about an agent j ∈ Ni may be rather low, especially with many agents. Even with only
50 agents, we observe that only around 25% of the reputation information received by agents actually concerned one of their neighbors. This problem may be addressed by making the reputation
network identical to the interaction network (as neighbor relations in both networks are then identical). However, this may be seen as a considerable abstraction. Second, the probability that agent i
has information concerning all of his neighbors is low, so we need to specify default values for s0j .
Clearly, any default value is more often wrong than right, unless we use a centralized mechanism
to estimate it by, for instance, using the current average population strategy, which is what we do in
our simulations.
With this mechanism in place, we perform the same experiments as in §4, i.e. we vary the
population size between 10 and 10, 000 agents, and the proportion of FS agents in steps of 10%. A
statistical analysis reveals no significant difference between a setup that uses reputation and a setup
that does not. When we further analyze the results, we see that, as expected, agents almost always
need to resort to default values for their neighbors’ strategies. Thus, on average, the reputation
system does not often change the probabilities that certain neighbors are selected.
5.2 Reputation and Rewiring
As we have seen in §4, rewiring works very well without reputation (i.e. purely based on an agent’s
own experience). Adding reputation may be beneficial to agents, as they no longer need to interact
with each other to be allowed to unwire. Thus, agents may once again increase their preference
for certain others. Reputation information (i.e. the amount offered by a certain agent) propagates
through the (static) reputation network, allowing agents receiving such information to potentially
unwire from one of their neighbors if they consider this neighbor’s behavior to be undesirable. The
same rewiring mechanism is used here as detailed in §3 (i.e. Equation 6). We allow the responder
569

D E J ONG , U YTTENDAELE & T UYLS

in the Ultimatum Game to broadcast reputation information through the reputation network, with a
maximum of H = 5 hops.
Once again, we perform the same experiments as in §4, and once again, there is no significant
difference in the main results. We further analyze the number of times agents actually rewired, and
find that this number on average increases by a factor 2 with respect to a setup in which reputation is
not used (i.e. as reported in §4.3). However, this increase does not increase performance. On average, agents have only few neighbors; thus, they generally receive reputation information concerning
a neighbor that, in the absence of reputation, they would play against soon anyway.
5.3 Volunteering
According to existing research on human fairness (e.g. Boyd & Mathew, 2007; Hauert et al., 2007;
Sigmund et al., 2001) the mechanism of volunteering may contribute to reaching cooperation in
games with only two strategies. The mechanism of volunteering consists in allowing players not to
participate in certain games, enabling them to fall back on a safe ‘side income’ that does not depend
on other players’ strategies. Such risk-averse optional participation can prevent exploiters from
gaining the upper hand, as they are left empty-handed by more cooperative players preferring not to
participate. Clearly, the ‘side income’ must be carefully selected, such that agents are encouraged to
participate if the population is sufficiently cooperative. Experiments show that volunteering indeed
allows a collective of players to spend “most of its time in a happy state” (Boyd & Mathew, 2007)
in which most players are cooperative.
The biggest problem when applying volunteering is that we basically introduce yet another
social dilemma. An agent may refrain from participating to make a statement against the other
agent, which may convince this other agent to become more social in the future, but to make this
statement, the agent must refuse an expected positive payoff: in the Ultimatum Game with randomly
assigned roles, the expected payoff is always positive. Nonetheless, we study whether volunteering
promotes agreement in games with continuous strategy spaces. We once again use the heuristic
proposed by Santos et al. (2006a), which has already been applied in various mechanisms in this
paper: if agent i thinks that agent j is a (relative) cooperator, then he agrees on playing. If both
agents agree, then a game is played. To prevent agents from not playing any game (after all, both
agents see each other as a relative cooperator only if they already are playing the same strategy), we
introduce a 10% probability that games are played anyway, even if one or both agents does not want
to. Note that reputation may be used here, as it may allow agents to estimate whether one of their
neighbors is a relative cooperator or not, without having to play with this neighbor.
Unfortunately, experimental results point out that agents using volunteering (with and without
reputation) have severe difficulties establishing a common strategy (Uyttendaele, 2008). As a result, when measuring performance, we see that only around 50% of the games is played. Of the
games played, the performance is similar to a setup with rewiring (e.g. above 80%), which may
be expected, as two agents usually only agree to play if their strategies are similar. The reason
why agents do not converge properly is quite simple: they avoid playing with other agents that are
different from them. Therefore, they do not learn to behave in a way more similar to these others.
5.4 General Discussion
In general, we may state that with the mechanism of rewiring, we clearly find a good balance
between allowing agents to play with more preferred neighbors on the one hand, and forcing agents
570

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

to learn from those different from them on the other hand. The additions discussed above allow
agents to be too selective, i.e. they have too much influence on who they play against. While this
may be in the interest of individual agents, it generally leads to agents not playing against others that
are different from them, instead of learning from these others, as is required to obtain convergence
to agreement.

6. Conclusion
In this paper, we argue that mechanisms thought to allow humans to find fair, satisfactory solutions
to social dilemmas, may be useful for multi-agent systems, as many multi-agent systems need to
address tasks that contain elements of social dilemmas, e.g. resource allocation (Chevaleyre et al.,
2006). Existing work concerning (human-inspired) fairness in multi-agent systems is generally restricted to discrete strategy sets, usually with only two strategies, one of which is deemed to be
‘cooperative’ (i.e. desirable). However, many real-world applications of multi-agent systems pose
social dilemmas which require a strategy taken from a continuous strategy space, rather than a discrete strategy set. We observed that the traditional concept of cooperation is not trivially applicable
to continuous strategy spaces, especially since it is no longer feasible to manually label a certain
strategy as ‘cooperative’ in an absolute manner. A certain strategy may be cooperative in a certain
culture, whereas it may be defective or naive in another. Thus, cooperation is a relative rather than
absolute concept in continuous strategy spaces.
We propose that the concept of agreement (as introduced in statistical physics; Dall’Asta et al.,
2006) may be used as an alternative to cooperation. We discuss the emergence of agreement in
continuous strategy spaces, using learning agents that play pairwise Ultimatum Games, based on
a scale-free interaction network and the possibility to rewire in this network. In the Ultimatum
Game, two agents agree if the offering agent offers at least the minimal amount that satisfies the
responding agent (in this case, the two agents cooperate). Thus, for our population of agents to
agree on many random pairwise games, the agents should converge to the same strategy. Without
any external influences, any shared strategy is sufficient. With external influences, e.g. a preference
dictated by humans, agents should adapt to the dictated strategy, even if they are already agreeing
on a completely different strategy. We propose a methodology, based on continuous-action learning
automata, interactions in scale-free networks, and rewiring in these networks, aimed at allowing
agents to reach agreement. A set of experiments investigates the usability of this methodology.
In conclusion, we give four statements. (1) Our proposed methodology is able to establish agreement on a common strategy, especially when agents are given the option to rewire in their network
of interaction. Humans playing the Ultimatum Game reach an agreement of approximately 80-90%
(Oosterbeek et al., 2004). Without rewiring, our agents do worse (generally, 65% of the games are
successful); with rewiring, they do as well as humans. Thus, as in games with a discrete strategy set,
rewiring greatly enhances the agents’ abilities to reach agreement, without compromising the scalefree network structure. This indicates that interactions in scale-free networks, as well as rewiring in
these networks, are plausible mechanisms for making agents reach agreement. (2) In comparison to
methodologies reported on in related work (e.g. Santos et al., 2006b), our methodology facilitates
convergence with only a low number of games per agent needed (e.g. 500 instead of 10,000). This
indicates that continuous-action learning automata are a satisfactory approach when we are aiming
at allowing agents to learn from a relatively low number of examples. (3) The performance of the
collective is not seriously influenced by its size. This is clearly influenced by the characteristics
571

D E J ONG , U YTTENDAELE & T UYLS

of a scale-free, self-similar network. (4) Concepts such as reputation or volunteering, which have
been reported to facilitate cooperative outcomes in discrete-strategy games, do not seem to have
(additional) benefits in continuous strategy spaces.
Although the Ultimatum Game is only one example of a social dilemma, its core difficulty is
present in all social dilemmas: selecting an individually rational action, which should optimize one’s
payoff, actually may hurt this payoff. In the Ultimatum Game, this problem is caused by the fact
that we may play (as a proposer) against someone who would rather go home empty-handed than
accept a deal that is perceived as unfair. Similar fairness-related problems may arise in various other
interactions, e.g. in bargaining about the division of a reward, or in resource allocation (Chevaleyre
et al., 2006; Endriss, 2008). Many multi-agent systems need to allocate resources, if not explicitly
in their assigned task, then implicitly, for instance because multiple agents share a certain, limited
amount of computation time. Thus, fair division is an important area of research, which recently is
receiving increasing attention from the multi-agent systems community (Endriss, 2008). As humans
often display adequate and immediate ability to come up with a fair division that is accepted by
most of them, it will definitely pay off if we allow agents to learn to imitate human strategies. In
this paper, we examined how such a task may be executed.

Acknowledgments
The authors wish to thank the anonymous referees for their valuable contributions. Steven de Jong
is funded by the ‘Breedtestrategie’ programme of Maastricht University.

References
Axelrod, R. (1997). The Dissemination of Culture: A Model with Local Convergence and Global
Polarization. Journal of Conflict Resolution, 41:203–226.
Barabasi, A.-L. and Albert, R. (1999). Emergence of scaling in random networks. Science, 286:509–
512.
Bearden, J. N. (2001). Ultimatum Bargaining Experiments: The State of the Art. SSRN eLibrary.
Bowles, S., Boyd, R., Fehr, E., and Gintis, H. (1997). Homo reciprocans: A Research Initiative on
the Origins, Dimensions, and Policy Implications of Reciprocal Fairness. Advances in Complex
Systems, 4:1–30.
Boyd, R. and Mathew, S. (2007). A Narrow Road to Cooperation. Science, 316:1858–1859.
Cameron, L. (1999). Raising the stakes in the ultimatum game: Evidence from Indonesia. Journal
of Economic Inquiry, 37:47–59.
Chevaleyre, Y., Dunne, P., Endriss, U., Lang, J., Lemaître, M., Maudet, N., Padget, J., Phelps, S.,
Rodriguez-Aguilar, J., and Sousa, P. (2006). Issues in Multiagent Resource Allocation. Informatica, 30:3–31.
Chiang, Y.-S. (2008). A Path Toward Fairness: Preferential Association and the Evolution of Strategies in the Ultimatum Game. Rationality and Society, 20(2):173–201.
572

L EARNING TO R EACH AGREEMENT IN A C ONTINUOUS U LTIMATUM G AME

Dall’Asta, L., Baronchelli, A., Barrat, A., and Loreto, V. (2006). Agreement dynamics on smallworld networks. Europhysics Letters, 73(6):pp. 969–975.
Dannenberg, A., Riechmann, T., Sturm, B., and Vogt, C. (2007). Inequity Aversion and Individual
Behavior in Public Good Games: An Experimental Investigation. SSRN eLibrary.
de Jong, S. and Tuyls, K. (2008). Learning to cooperate in public-goods interactions. Presented at
the EUMAS’08 Workshop, Bath, UK, December 18-19.
de Jong, S., Tuyls, K., and Verbeeck, K. (2008a). Artificial Agents Learning Human Fairness.
In Proceedings of the international joint conference on Autonomous Agents and Multi-Agent
Systems (AAMAS’08), pages 863–870.
de Jong, S., Tuyls, K., and Verbeeck, K. (2008b). Fairness in multi-agent systems. Knowledge
Engineering Review, 23(2):153–180.
de Jong, S., Tuyls, K., Verbeeck, K., and Roos, N. (2008c). Priority Awareness: Towards a Computational Model of Human Fairness for Multi-agent Systems. Adaptive Agents and Multi-Agent
Systems III. Adaptation and Multi-Agent Learning, 4865:117–128.
Endriss, U. (2008). Fair Division. Tutorial at the International Conference on Autonomous Agents
and Multi-Agent Systems (AAMAS).
Fehr, E. (2004). Don’t lose your reputation. Nature, 432:499–500.
Fehr, E. and Schmidt, K. (1999). A Theory of Fairness, Competition and Cooperation. Quarterly
Journal of Economics, 114:817–868.
Gintis, H. (2001). Game Theory Evolving: A Problem-Centered Introduction to Modeling Strategic
Interaction. Princeton University Press, Princeton, USA.
Grosskopf, B. (2003). Reinforcement and Directional Learning in the Ultimatum Game with Responder Competition. Experimental Economics, 6(2):141–158.
Gueth, W., Schmittberger, R., and Schwarze, B. (1982). An Experimental Analysis of Ultimatum
Bargaining. Journal of Economic Behavior and Organization, 3 (4):367–388.
Hauert, C., Traulsen, A., Brandt, H., Nowak, M. A., and Sigmund, K. (2007). Via freedom to
coercion: the emergence of costly punishment. Science, 316:1905–1907.
Henrich, J., Boyd, R., Bowles, S., Camerer, C., Fehr, E., and Gintis, H. (2004). Foundations of
Human Sociality: Economic Experiments and Ethnographic Evidence from Fifteen Small-Scale
Societies. Oxford University Press, Oxford, UK.
Maynard-Smith, J. (1982). Evolution and the Theory of Games. Cambridge University Press.
Maynard-Smith, J. and Price, G. R. (1973). The logic of animal conflict. Nature, 246:15–18.
Nowak, M. A., Page, K. M., and Sigmund, K. (2000). Fairness versus reason in the Ultimatum
Game. Science, 289:1773–1775.
573

D E J ONG , U YTTENDAELE & T UYLS

Oosterbeek, H., Sloof, R., and van de Kuilen, G. (2004). Cultural Differences in Ultimatum Game
Experiments: Evidence from a Meta-Analysis. Experimental Economics, 7:171–188.
Peters, R. (2000). Evolutionary Stability in the Ultimatum Game. Group Decision and Negotiation,
9:315–324.
Roth, A. E., Prasnikar, V., Okuno-Fujiwara, M., and Zamir, S. (1991). Bargaining and Market
Behavior in Jerusalem, Ljubljana, Pittsburgh, and Tokyo: An Experimental Study. American
Economic Review, 81(5):1068–95.
Santos, F. C., Pacheco, J. M., and Lenaerts, T. (2006a). Cooperation Prevails When Individuals
Adjust Their Social Ties. PLoS Comput. Biol., 2(10):1284–1291.
Santos, F. C., Pacheco, J. M., and Lenaerts, T. (2006b). Evolutionary Dynamics of Social Dilemmas
in Structured Heterogeneous Populations. Proc. Natl. Acad. Sci. USA, 103:3490–3494.
Selten, R. and Stoecker, R. (1986). End behavior in sequences of finite Prisoner’s Dilemma supergames : A learning theory approach. Journal of Economic Behavior & Organization, 7(1):47–
70.
Sigmund, K., Hauert, C., and Nowak, M. A. (2001). Reward and punishment. Proceedings of the
National Academy of Sciences, 98(19):10757–10762.
Slonim, R. and Roth, A. (1998). Learning in high stakes ulitmatum games: An experiment in the
Slovak republic. Econometrica, 66:569–596.
Sonnegard, J. (1996). Determination of first movers in sequential bargaining games: An experimental study. Journal of Economic Psychology, 17:359–386.
Sutton, R. S. and Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge, MA. A Bradford Book.
Thathachar, M. A. L. and Sastry, P. S. (2004). Networks of Learning Automata: Techniques for
Online Stochastic Optimization. Kluwer Academic Publishers, Dordrecht, the Netherlands.
Uyttendaele, S. (2008). Fairness and agreement in complex networks. Master’s thesis, MICC,
Maastricht University.

574

Journal of Artificial Intelligence Research 33 (2008) 349-402

Submitted 05/08; published 11/08

Learning Partially Observable Deterministic Action Models
EYAL @ ILLINOIS . EDU

Eyal Amir
Computer Science Department
University of Illinois, Urbana-Champaign
Urbana, IL 61801, USA

ALLENC 256@ YAHOO . COM

Allen Chang
2020 Latham st., apartment 25
Mountainview, CA 94040, USA

Abstract
We present exact algorithms for identifying deterministic-actions’ effects and preconditions in
dynamic partially observable domains. They apply when one does not know the action model (the
way actions affect the world) of a domain and must learn it from partial observations over time.
Such scenarios are common in real world applications. They are challenging for AI tasks because
traditional domain structures that underly tractability (e.g., conditional independence) fail there
(e.g., world features become correlated). Our work departs from traditional assumptions about
partial observations and action models. In particular, it focuses on problems in which actions are
deterministic of simple logical structure and observation models have all features observed with
some frequency. We yield tractable algorithms for the modified problem for such domains.
Our algorithms take sequences of partial observations over time as input, and output deterministic action models that could have lead to those observations. The algorithms output all or one of
those models (depending on our choice), and are exact in that no model is misclassified given the
observations. Our algorithms take polynomial time in the number of time steps and state features
for some traditional action classes examined in the AI-planning literature, e.g., STRIPS actions. In
contrast, traditional approaches for HMMs and Reinforcement Learning are inexact and exponentially intractable for such domains. Our experiments verify the theoretical tractability guarantees,
and show that we identify action models exactly. Several applications in planning, autonomous
exploration, and adventure-game playing already use these results. They are also promising for
probabilistic settings, partially observable reinforcement learning, and diagnosis.

1. Introduction
Partially observable domains are common in the real world. They involve situations in which one
cannot observe the entire state of the world. Many examples of such situations are available from
all walks of life, e.g., the physical worlds (we do not observe the position of items in other rooms),
the Internet (we do not observe more than a few web pages at a time), and inter-personal communications (we do not observe the state of mind of our partners).
Autonomous agents’ actions involve a special kind of partial observability in such domains.
When agents explore a new domain (e.g., one goes into a building or meets a new person), they
have limited knowledge about their action models (actions’ preconditions and effects). These action models do not change with time, but they may depend on state features. Such agents can act
intelligently, if they learn how their actions affect the world and use this knowledge to respond to
their goals.
c
°2008
AI Access Foundation. All rights reserved.

A MIR & C HANG

Learning action models is important when goals change. When an agent acted for a while, it can
use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning
action models differs from Reinforcement Learning. It enables reasoning about actions instead of
expensive trials in the world.
Learning actions’ effects and preconditions is difficult in partially observable domains. The
difficulty stems from the absence of useful conditional independence structures in such domains.
Most fully observable domains include such structures, e.g., the Markov property (independence of
the state at time t + 1 from the state at time t − 1, given the (observed) state at time t). These are
fundamental to tractable solutions of learning and decision making.
In partially observable domains those structures fail (e.g., the state of the world at time t + 1
depends on the state at time t − 1 because we do not observe the state at time t), and complex
approximate approaches are the only feasible path. For these reasons, much work so far has been
limited to fully observable domains (e.g., Wang, 1995; Pasula, Zettlemoyer, & Kaelbling, 2004),
hill-climbing (EM) approaches that have unbounded error in deterministic domains (e.g., Ghahramani, 2001; Boyen, Friedman, & Koller, 1999), and approximate action models (Dawsey, Minsker,
& Amir, 2007; Hill, Minsker, & Amir, 2007; Kuffner. & LaValle, 2000; Thrun, 2003).
This paper examines the application of an old-new structure to learning in partially observable
domains, namely, determinism and logical formulation. It focuses on some such deterministic domains in which tractable learning is feasible, and shows that a traditional assumption about the form
of determinism (the STRIPS assumption, generalized to ADL, Pednault, 1989) leads to tractable
learning and state estimation. Learning in such domains has immediate applications (e.g., exploration by planning, Shahaf, Chang, & Amir, 2006; Chang & Amir, 2006) and it can also serve as
the basis for learning in stochastic domains. Thus, a fundamental advance in the application of
such a structure is important for opening the field to new approaches of broader applicability. The
following details the technical aspects of our advance.
The main contribution of this paper is an approach called SLAF (Simultaneous Learning and
Filtering) for exact learning of actions models in partially observable deterministic domains. This
approach determines the set of possible transition relations, given an execution sequence of actions
and partial observations. For example, the input could come from watching another agent act or
from watching the results of our own actions’ execution. The approach is online, and updates a
propositional logical formula called Transition Belief Formula. This formula represents the possible
transition relations and world states at every time step. In this way, it is similar in spirit to Bayesian
learning of HMMs (e.g., Ghahramani, 2001) and Logical Filtering (Amir & Russell, 2003).
The algorithms that we present differ in their range of applicability and their computational
complexity. First, we present a deduction-based algorithm that is applicable to any nondeterministic
learning problem, but that takes time that is worst-case exponential in the number of domain fluents.
Then, we present algorithms that update a logical encoding of all consistent transition relations in
polynomial time per step, but that are limited in applicability to special classes of deterministic
actions.
One set of polynomial-time algorithms that we present applies to action-learning scenarios in
which actions are ADL (Pednault, 1989) (with no conditional effects) and one of the following
holds: (a) the action model already has preconditions known, and we observe action failures (e.g.,
when we perform actions in the domain), or (b) actions execution always succeeds (e.g., when an
expert or tutor performs actions).
350

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Our algorithms output a transition belief formula that represents the possible transition relations
and states after partial observations of the state and actions. They do so by updating each component
of the formula separately in linear time. Thus, updating the transition belief formula with every
action execution and observation takes linear time in the size of the input formula.
Processing a sequence of T action executions and observations takes time O(T 2 · n) for case
(b). The main reason for this is a linear growth in the representation size of the transition belief
formula: at time t, the iterative process that updates this formula would process a formula that has
size linear in t.
For case (a) processing a sequence of length T takes polynomial time O(T · n k )), only if we
observe every feature in the domain every ≤ k steps in expectation, for some fixed k. The reason for
this is that the transition belief formula can be kept in k-CNF (k Conjunctive Normal
V Form),
W thus
of size O(nk ). (Recall that a propositional formula is in k-CNF, if it is of the form i≤m j≤k li,j ,
with every li,j a propositional variable or its negation.) Case (b) takes time O(T · n) under the same
assumption.
Another set of polynomial-time algorithms that we present takes linear time in the representation
size. In this case actions are known to be injective, i.e., map states 1:1. There, we can bound
the computation time for T steps with O(T · nk ), if we approximate the transition-belief formula
representation with a k-CNF formula.
In contrast, work on learning in Dynamic Bayesian Networks (e.g., Boyen et al., 1999), reinforcement learning in POMDPs (e.g., Littman, 1996), and Inductive Logic Programming (ILP)
(e.g., Wang, 1995) either approximate the solution with unbounded error for deterministic domains,
n
or take time Ω(22 ), and are inapplicable in domains larger than 10 features. Our algorithms are
better in this respect, and scale polynomially and practically to domains of 100’s of features and
more. Section 8 provides a comparison with these and other works.
We conduct a set of experiments that verify these theoretical results. These experiments show
that our algorithms are faster and better qualitatively than related approaches. For example, we can
learn some ADL actions’ effects in domains of > 100 features exactly and efficiently.
An important distinction must be made between learning action models and traditional creation
of AI-Planning operators. From the perspective of AI Planning, action models are the result of
explicit modeling, taking into account modeling decisions. In contrast, learning action models
is deducing all possible transition relations that are compatible with a set of partially observed
execution trajectories.
In particular, action preconditions are typically used by the knowledge engineer to control the
granularity of the action model so as to leave aside from specification unwanted cases. For example,
if driving a truck with insufficient fuel from one site to another might generate unexpected situations
that the modeller does not want to consider, then a simple precondition can be used to avoid considering that case. The intention in this paper is not to mimic this modeling perspective, but instead
find action models that generate sound states when starting from a sound state. Sound state is any
state in which the system can be in practice, namely, ones that our observations of real executions
can reflect.
Our technical advance for deterministic domains is important for many applications such as
automatic software interfaces, internet agents, virtual worlds, and games. Other applications, such
as robotics, human-computer interfaces, and program and machine diagnosis can use deterministic
action models as approximations. Finally, understanding the deterministic case better can help us
351

A MIR & C HANG

develop better results for stochastic domains, e.g., using approaches such as those by Boutilier,
Reiter, and Price (2001), Hajishirzi and Amir (2007).
In the following, Section 2 defines SLAF precisely, Section 3 provides a deduction-based exact
SLAF algorithm, Section 4 presents tractable action-model-update algorithms, Section 5 gives sufficient conditions and algorithms for keeping the action-model representation compact (thus, overall
polynomial time), and Section 7 presents experimental results.

2. Simultaneous Learning and Filtering (SLAF)
Simultaneous Learning and Filtering (SLAF) is the problem of tracking a dynamic system from
a sequence of time steps and partial observations, when we do not have the system’s complete
dynamics initially. A solution for SLAF is a representation of all combinations of action models
that could possibly have given rise to the observations in the input, and a representation of all the
corresponding states in which the system may now be (after the sequence of time steps that were
given in the input occurs).
Computing (the solution for) SLAF can be done in a recursive fashion by dynamic programming
in which we determine SLAF for time step t+1 from our solution of SLAF for time t. In this section
we define SLAF formally in such a recursive fashion.
Ignoring stochastic information or assumptions, SLAF involves determining the set of possible
ways in which actions can change the world (the possible transition models, defined formally below)
and the set of states the system might be in. Any transition model determines a set of possible states,
so a solution to SLAF is a transition model and its associated possible states.
We define SLAF with the following formal tools, borrowing intuitions from work on Bayesian
learning of Hidden Markov Models (HMMs) (Ghahramani, 2001) and Logical Filtering (Amir &
Russell, 2003).
Definition 2.1 A transition system is a tuple hP, S, A, Ri, where
• P is a finite set of propositional fluents;
• S ⊆ P ow(P) is the set of world states.
• A is a finite set of actions;
• R ⊆ S × A × S is the transition relation (transition model).
Thus, a world state, s ∈ S, is a subset of P that contains propositions true in this state (omitted
propositions are false in that state), and R(s, a, s0 ) means that state s0 is a possible result of action a
in state s. Our goal in this paper is to find R, given known P, S, A, and a sequence of actions and
partial observations (logical sentences on any subset of P).
Another, equivalent, representation for S that we will also use in this paper is the following.
A literal is a proposition, p ∈ P, or its negation, ¬p. A complete term over P is a conjunction of
literals from P such that every fluent appears exactly once. Every state corresponds to a complete
term of P and vice versa. For that reason, we sometime identify a state s ∈ S with this term. E.g.,
for states s1 , s2 , s1 ∨s2 is the disjunction of the complete terms corresponding to s 1 , s2 , respectively.
A transition belief state is a set of tuples hs, Ri where s is a state and R a transition relation.
Let R = P ow(S × A × S) be the set of all possible transition relations on S, A. Let S = S × R.
When we hold a transition belief state ρ ⊆ S we consider every tuple hs, Ri ∈ ρ possible.

352

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Figure 1: Locked door with unknown key domain.
Example 2.2 Consider a domain where an agent is in a room with a locked door (see Figure 1).
In its possession are three different keys, and suppose the agent cannot tell from observation only
which key opens the door. The goal of the agent is to unlock the door.
This domain can be represented as follows: let the set of variables defining the state space
be P = {locked} where locked is true if and only if the door is locked. Let the set of states be
S = {s1 , s2 } where s1 = {locked} (the state in which the door is locked) and s2 = {} (here the
door is unlocked). Let A = {unlock1 , unlock2 , unlock3 } be the three actions wherein the agent
tries unlocking the door using each of the three keys.
Let R1 = {hs1 , unlock1 , s2 i, hs1 , unlock2 , s1 i, hs1 , unlock3 , s1 i} represent a transition relation in which key 1 unlocks the door and the other keys do not. Define R 2 and R3 in a similar
fashion (e.g., with R2 key 2 unlocks the door and keys 1 and 3 do not). A transition belief state
represents the set of possibilities that we consider consistent with our observations so far. Consider
a transition belief state given by ρ = {hs1 , R1 i, hs1 , R2 i, hs1 , R3 i}, i.e., the state of the world is
fully known but the action model is only partially known.
We would like the agent to be able to open the door despite not knowing which key opens it. To
do this, the agent will learn the actual action model (i.e., which key opens the door). In general, not
only will learning an action model be useful in achieving an immediate goal, but such knowledge
will be useful as the agent attempts to perform other tasks in the same domain.2
Definition 2.3 (SLAF Semantics) Let ρ ⊆ S be a transition belief state. The SLAF of ρ with
actions and observations haj , oj i1≤j≤t is defined by
1. SLAF [a](ρ) =
{hs0 , Ri | hs, a, s0 i ∈ R, hs, Ri ∈ ρ};
2. SLAF [o](ρ) = {hs, Ri ∈ ρ | o is true in s};
3. SLAF [haj , oj ii≤j≤t ](ρ) =
SLAF [haj , oj ii+1≤j≤t ](SLAF [oi ](SLAF [ai ](ρ))).
Step 1 is progression with a, and Step 2 filtering with o.
Example 2.4 Consider the domain from Example 2.2. The progression of ρ on the action unlock 1
is given by SLAF [unlock1 ](ρ) = {hs2 , R1 i, hs1 , R2 i, hs1 , R3 i}. Likewise, the filtering of ρ on the
observation ¬locked (the door became unlocked) is given by SLAF [¬locked](ρ) = {hs 2 , R1 i}.2
353

A MIR & C HANG

Example 2.5 A slightly more involved example is the following situation presented in Figure 2.
There, we have two rooms, a light bulb, a switch, an action of flipping the switch, and an observation, E (we are in the east room). The real states of the world before and after the action, s2, s2 0 ,
respectively (shown in the top part), are not known to us.
West

East
off

PSfrag replacements

West
off

s2 = ¬sw ∧ ¬lit ∧ E

on

=⇒
sw-on

<s1,R1>

ρ1

East

s20 = sw ∧ lit ∧ E
<s1’,R1>

<s3,R3>

<s1’’,R1>
<s3’,R3>

<s2,R2>

on

<s2’,R2>

ρ2

Figure 2: Top: Two rooms and flipping the light switch. Bottom: SLAF semantics; progressing
an action (the arrows map state-transition pairs) and then filtering with an observation
(crossing out some pairs).
The bottom of Figure 2 demonstrates how knowledge evolves after performing the action sw-on.
There, ρ1 = {hs1 , R1 i, hs2 , R2 i, hs3 , R3 i} for some s1 , R1 , s3 , R3 , s2 = {E}, and R2 that includes hs2 , sw-on, s02 i (the identity and full details of R1 , R2 , R3 are irrelevant here, so we omit
them). ρ2 is the resulting transition belief state after action sw-on and observation E: ρ 2 =
SLAF [sw-on, E](ρ1 ). 2
We assume that observations (and an observation model relating observations to state fluents)
are given to us as logical sentences over fluents after performing an action. They are denoted with
o.
This approach to transition belief states generalizes Version Spaces of action models (e.g.,
Wang, 1995) as follows: If the current state, s, is known, then the version space’s lattice contains
the set of transition relations ρs = {R | hs, Ri ∈ ρ}. Thus, from the perspective of version spaces,
SLAF semantics is equivalent to a set of version spaces, one for each state in which we might be.
This semantics also generalizes belief states: If the transition relation, R, is known, then the
belief state (set of possible states) is ρR = {s | hs, Ri ∈ ρ} (read ρ restricted to R), and Logical
Filtering (Amir & Russell, 2003) of belief state σ and action a is equal to (thus, we can define it as)
F ilter[a](σ) = (SLAF [a]({hs, Ri | s ∈ σ}))R .
Thus, SLAF semantics is equivalent to holding a set of belief states, each conditioned on a transition
relation, similar to saying “if the transition relation is R, then the belief state (set of states) is σ R ”.

3. Learning by Logical Inference
Learning transition models using Definition 2.3 directly is intractable – it requires space Ω(2 2 )
in many cases. The reason for that is the explicit representation of the very large set of possible
transition-state pairs. Instead, in this section and the rest of this paper we represent transition belief states more compactly using propositional logic. In many scenarios there is some amount of
structure that can be exploited to make a propositional representation compact.
|P|

354

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

A combinatorial argument implies that no encoding is compact for all sets. Nonetheless, we are
motivated by the success of propositional (logical) approaches for logical filtering (Amir & Russell,
2003; Shahaf & Amir, 2007) and logical-database regression (Reiter, 1991, 2001), and observe that
propositional logic represents compactly some natural sets of exponential size.
In this section we re-define SLAF as an operation on propositional logical formulas with a
propositional formula as output. SLAF’s input is a propositional formula that represents a transition
belief state, and SLAF computes a new transition belief formula from that input and a sequence of
actions and observations.
We want to find algorithms for SLAF that manipulate an input formula and produce a correct
output. We use general-purpose logical inference for this task in this section. In later sections we
sidestep expensive general-purpose inference, and make assumptions that lead to tractable algorithms. For the rest of this paper we focus on deterministic transition relations, namely, transition
relations that are partial functions (every action has at most one outcome state for every state).
3.1 Representing Transition Relations in Logic
Our initial algorithm for solving SLAF (to be presented momentarily) does so with a compact
representation of transition belief states. We present this logical encoding of transition belief states
first, and define a deduction-based algorithm in the next section.
We use the following general terminology for propositional logical languages (all the terminological conventions apply with or without subscripts and superscripts). L denotes a vocabulary, i.e.,
a set of propositional variables that we use in the present context. L denotes a language, i.e., a set
of propositional sentences. ϕ, ψ, and other script Greek letters stand for propositional formulas in
the language of the present context. F, G also stand for such formulas, but in a restricted context
(see below). L(ϕ) denotes the vocabulary of ϕ. L(L) denotes the language built from propositions
in L using the standard propositional connectives (¬, ∨, ∧,...). L(ϕ) is a shorthand for L(L(ϕ)).
We represent deterministic transition relations with a propositional vocabulary, L A , whose
propositions are of the form aFG , for a ∈ A, F a literal over P, and G a logical formula. F is
the effect of aFG , and G is the precondition of aFG . When proposition aFG takes the truth value TRUE,
this has the intended meaning that “If G holds in the present state, then F holds in the state that
results from executing a”.
We let F ⊆ P ∪ {¬p | p ∈ P} be the set of all effects, F , that we consider. We let G be the
set of all preconditions, G, that we consider. In the rest of this section and Section 4 we assume
that G represents a single state in S. Recall that we identify a state with a complete term which is
the conjunction of literals that hold in that state. We use this representation of states and write a Fs
instead of aFG . Later we build on this definition and consider G’s that are more general formulas.
From our assumption (G ≡ S for now, as stated above) we conclude that L A has O(2|P| · 2|P| ·
|A|) propositional variables. We prove fundamental results for this language and a set of axioms,
disregarding the size of the language for a moment. Section 5 focuses on decreasing the language
size for computational efficiency.
Our semantics for vocabulary LA lets every interpretation (truth assignment), M , for LA correspond with a transition relation, RM . Every transition relation has at least one (possibly more)
interpretation that corresponds to it, so this correspondence is surjective (onto) but not injective
(1-to-1). Every propositional sentence ϕ ∈ L(LA ) specifies a set of transition models as follows:
355

A MIR & C HANG

The set of models1 (satisfying interpretations) of ϕ, I[ϕ] = {M interpretation to LA | M |= ϕ},
specifies the corresponding set of transition relations, {RM | M ∈ I[ϕ]}.
Informally, assume that propositions aFs 1 , ...aFs k ∈ LA take the value TRUE in M , and that all
other propositions with precondition
V s take the value FALSE. Then, R M (with action0 a) takes the
0
state s to a state s that satisfies i≤k Fi , and is identical to s otherwise. If no such s exists (e.g.,
Fi = ¬Fj , for some i, j ≤ k), then RM takes s to no s0 (thus, a is not executable in s according to
RM ).
The following paragraphs show how interpretations over L A correspond to transition relations.
They culminate with a precise definition of the correspondence between formulas in L(L A ∪ P) and
transition belief states ρ ⊆ S.
E VERY I NTERPRETATION OF LA C ORRESPONDS TO A U NIQUE T RANSITION R ELATION
Every interpretations of LA defines a unique transition relation RM as follows. Let M be an interpretation of LA . For every state s ∈ S and an action a ∈ A we either define a unique state s 0 such
that hs, a, s0 i ∈ RM or decide that there is no s0 for which hs, a, s0 i ∈ RM .
M gives an interpretation for every proposition aFs , for F a fluent or its negation. If for any
fluent p ∈ P, M [aps ] = M [a¬p
s ] = TRUE (M [ϕ] is the truth value of ϕ according to interpretation
M ), we decide that there is no s0 such that hs, a, s0 i ∈ RM . Otherwise, define
s0 = {p ∈ P | M |= aps } ∪ {p ∈ s | M |= ¬a¬p
s }
In the left-hand side of ∪ we consider the cases of p ∈ P for which M [a ps ] 6= M [a¬p
s ], and on the
right-hand side of ∪ we treat the cases of p ∈ P for which M [aps ] = M [a¬p
s ] = F ALSE (this is
called inertia because p keeps its previous value for lack of other specifications). Put another way,
0
s0 [p] = M [aps ] ∨ (s[p] ∧ ¬M [a¬p
s ]), if we view s as an interpretation of P. RM is well defined, i.e.,
there is only one RM for every M .
E VERY T RANSITION R ELATION
LA

HAS AT

L EAST O NE C ORRESPONDING I NTERPRETATION

OF

It is possible that RM = RM 0 for M 6= M 0 . This occurs in two circumstances: (a) cases in which
there is no hs, a, s0 i ∈ RM for some s, a and (b) when M [aps ] = M [a¬p
s ] = F ALSE (inertia) and
p
¬p
0
0
M [as ] = s[p], M [as ] = s[¬p] (not inertia).
For an example of the first circumstance, let p be a fluent, let M be an interpretation such that
0
M [aps ] = M [a¬p
s ] for some G. Define M an interpretation that is identical to M on all propositions
p ¬p
p
0
besides as , as as follows. Define M [as ] to have the opposite truth assignment to M [aps ] (FALSE
¬p
instead of TRUE, and TRUE instead of FALSE). Define M 0 [a¬p
s ] = M [as ].
Then, RM = RM 0 because they map all pairs s, a in the same way. In particular, for state s that
corresponds to G, there is no hs, a, s0 i ∈ RM and similarly there is no hs, a, s0 i ∈ RM 0 .
Finally, every transition relation R has at least one interpretation M such that R = R M . To see
this, define MR for every hs, a, s0 i ∈ R the interpretation to aps (p any fluent) MR [aps ] = T RU E iff
/ s0 . Finally, for all s, a for
p ∈ s0 . Also, for the same hs, a, s0 i define MR [a¬p
s ] = F ALSE iff p ∈
p
¬p
which there is no such s0 , define MR [as ] = MR [as ] = T RU E. Then, R = RMR .
1. We overload the word model for multiple related meanings. model refers to a satisfying interpretation of a logical
formula. Transition model is defined in Definition 2.1 to be a transition relation in a transition system. Action model
is define in the Introduction section to be any well-defined specification of actions’ preconditions and effects.

356

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

E VERY T RANSITION R ELATION D EFINES A F ORMULA OVER LA
Every deterministic transition relation R defines a logical formula whose set of models all map
to R. There are many such possible formulas, and we define the most general one (up to logical
equivalence) that does not make use of inertia.
Define T h(R) as follows.
F
0
0
T h0 (R) = {aFs , ¬a¬F
s | as ∈ LA , hs, a, s i ∈ R, s |= F }
p
¬p
T h1 (R) = {as ∨ as | p ∈ P, s ∈ S}

W
0
0
T h2 (R) = { p∈P (aps ∧ a¬p
s ) | ¬∃s , hs, a, s i ∈ R}
T h(R) = T h0 (R) ∪ T h1 (R) ∪ T h2

T h0 addresses fluent changes, T h1 addresses fluent innertia (effectively disallowing innertia in
our definition), and T h2 addresses conditions in which actions are not executable. Thus, T h(R)
includes as a model M every interpretation that satisfies RM = R and that requires no inertia for
its definition of RM . It represents R in that each of its models M satisfies RM = R.
It is illuminating to see how our modeling decisions (above and throughout this section) lead
to the last definition. On the one hand, we choose to have every interpretation of L A correspond
to a transition relation (we do this to simplify later arguments about logical entailment). Consequently, we associate interpretations M with M [aFs ] = M [a¬F
s ] = F ALSE with transition relations R(s, a, s0 ) that keep the value of F fixed between s, s0 (this is inertia for F in a, s). On the
other hand, when we define T h(R) above, we choose axioms that exclude such models (thus, we
avoid models that include inertia) because it simplifies our later discussion of learning algorithms.
In summary, we consider every interpretation of LA as representing exactly one transition relation, and we consider the set of axioms defining R as those that define it directly, i.e., without inertia
(without M [aFs ] = M [a¬F
s ] = F ALSE).
T RANSITION B ELIEF S TATES C ORRESPOND TO F ORMULAS OVER LA ∪ P
Thus, for every
W transition belief state ρ we can define a formula in L(L A ∪ P) that corresponds to
it: T h(ρ) = hs,Ri∈ρ (s ∧ T h(R)). Other formulas exist that would characterize ρ in a similar way,
and they are not all equivalent. This is so because there are stronger formulas ϕ ∈ L(L A ) such that
ϕ |= T h(R) and T h(R) 6|= ϕ and every model, M , of ϕ satisfies RM = R.
Similarly, for every formula ϕ ∈ L(LA ∪ P) we define a transition belief state ρ(ϕ) = {hM ¹P
, RM i | M |= ϕ, }, i.e., all the state-transition pairs that satisfy ϕ (M ¹P is M restricted to
P, viewed as a complete term over P). We say that formula ϕ is a transition belief formula, if
T h(ρ(ϕ)) ≡ ϕ (note: ρ(T h(ρ)) = ρ always holds).
3.2 Transition-Formula Filtering
In this section, we show that computing the transition belief formula for SLAF [a](ϕ) for successful
action a and transition belief formula ϕ is equivalent to a logical consequence finding operation.
This characterization of SLAF as consequence-finding permits using consequence finding as an
algorithm for SLAF, and is important later in this paper for proving the correctness of our more
tractable, specialized algorithms.
Let CnL (ϕ) denote the set of logical consequences of ϕ restricted to vocabulary L. That is,
L
Cn (ϕ) contains the set of prime implicates of ϕ that contain only propositions from the set L.
357

A MIR & C HANG

Recall that an implicate α of a formula ϕ is a clause entailed by ϕ (ϕ |= α). Recall that a prime implicate α of a formula ϕ is an implicate of ϕ that is not subsumed (entailed) by any other implicates
of ϕ.
Consequence finding is any process that computes CnL (ϕ) for an input, ϕ. For example, propositional resolution (Davis & Putnam, 1960; Chang & Lee, 1973) is an efficient consequence finder
when used properly (Lee, 1967; del Val, 1999) (Marquis, 2000, surveys results about prime implicates and consequence finding algorithms). Thus, Cn L (ϕ) ≡ {ψ ∈ L(L)| ϕ |= ψ}.
For a set of propositions P, let P 0 represent the same set of propositions but with every proposition primed (i.e., each proposition f is annotated to become f 0 ). Typically, we will use a primed
fluent to denote the value of the unprimed fluent one step into the future after taking an action. Let
ϕ[P 0 /P] denote the same formula as ϕ, but with all primed fluents replaced by their unprimed counterparts. For example, the formula (a ∨ b0 )[P 0 /P] is equal to a ∨ b when b ∈ P. (See Section 8 for a
discussion and comparison with relevant formal verification techniques.)
The following lemma shows the logical equivalence of existential quantification of quantified
boolean formulas and consequence finding restricted to a vocabulary. Recall that quantified boolean
formulas (QBF) are propositional formulas with the addition of existential and universal quantifiers
over propositions. Informally, the QBF ∃x.ϕ is true for a given interpretation if and only if there
exists some true/false valuation of x that makes ϕ true under the assignment. The lemma will prove
useful for showing the equivalence between SLAF and consequence-finding.
Lemma 3.1 ∃x.ϕ ≡ CnL(ϕ)\{x} (ϕ), for any propositional logic formula ϕ and propositional variable x.
P ROOF
See Section B.1. 2
The lemma extends easily to the case of multiple variables:
Corollary 3.2 For any formula ϕ and set of propositional variables X, ∃X.ϕ ≡ Cn L(ϕ)\X (ϕ).
We present an algorithm for updating transition belief formulas whose output is equivalent to
that of SLAF (when SLAF is applied to the equivalent transition belief state). Our algorithm applies
consequence finding to the input transition belief formula together with a set of axioms that define
transitions between time steps. We present this set of axioms first.
For a deterministic (possibly conditional) action, a, the action model of a (for time t) is axiomatized as
V
Teff (a) = l∈F ,G∈G ((alG ∧ G) ⇒ l0 ) ∧
W
V
(1)
l
0
l∈F (l ⇒ ( G∈G (aG ∧ G)))

The first part of (1) says that assuming a executes at time t, and it causes l when G holds, and G
holds at time t, then l holds at time t + 1. The second part says that if l holds after a’s execution,
then it must be that alG holds and also G holds in the current state. These two parts are very similar
to (in fact, somewhat generalize) effect axioms and explanation closure axioms used in the Situation
Calculus (see McCarthy & Hayes, 1969; Reiter, 2001).
Now, we are ready to describe our zeroth-level algorithm (SLAF 0 ) for SLAF of a transition
belief formula. Let L0 = P 0 ∪LA be the vocabulary that includes only fluents of time t+1 and effect
propositions from LA . Recall (Definition 2.3) that SLAF has two operations: progression (with an
action) and filtering (with an observation). At time t we apply progression for the given action a t
and current transition belief formula, ϕt , and then apply filtering with the current observations:
358

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

0

ϕt+1 = SLAF0 [at , ot ](ϕt ) = (CnL (ϕt ∧ Teff (at )))[P 0 /P] ∧ ot

(2)

This is identical to Definition 2.3 (SLAF semantics), with the above replacing 1 and 2.
0
As stated above, for SLAF0 we can implement CnL (ϕ) using consequence finding algorithms
such as resolution and some of its variants (e.g., Simon & del Val, 2001; McIlraith & Amir, 2001;
Lee, 1967; Iwanuma & Inoue, 2002). The following theorem shows that this formula-SLAF algorithm is correct and exact.
Theorem 3.3 (Representation) For ϕ transition belief formula, a action,
SLAF [a]({hs, Ri ∈ S | hs, Ri satisfies ϕ}) =
{hs, Ri ∈ S | hs, Ri satisfies SLAF0 [a](ϕ)}
P ROOF
See Section B.2. 2
This theorem allows us to identify SLAF0 with SLAF , and we do so throughout the rest of the
paper. In particular, we show that polynomial-time algorithms for SLAF in special cases are correct
by showing that their output is logically equivalent to that of SLAF 0 .
U SING

THE

O UTPUT

OF

SLAF0

The output of any algorithm for SLAF of a transition belief formula is a logical formula. The way
to use this formula for answering questions about SLAF depends on the query and the form of the
output formula. When we wish to find if a transition model and state are possible, we wish to see if
M |= ϕ, for M an interpretation of L = P ∪ LA and ϕ the output of SLAF0 .
The answer can be found by a simple model-checking algorithm 2 . For example, to check that
an interpretation satisfies a logical formula we assign the truth values of the variables in the interpretation into the formula; computing the truth value of the formula can be done in linear time.
Thus, this type of query from SLAF takes linear time in the size of the output formula from
SLAF because the final query is about a propositional interpretation and a propositional formula.
When we wish to find if a transition model is possible or if a state is possible, we can do so with
propositional satisfiability (SAT) solver algorithms (e.g., Moskewicz, Madigan, Zhao, Zhang, &
Malik, 2001). Similarly, when we wish to answer whether all the possible models satisfy a property
we can use a SAT solver.
Example 3.4 Recall Example 2.4 in which we discuss a locked door and three combinations. Let
ϕ0 = locked, and let ϕ1 = SLAF0 [unlock2 , locked](ϕ0 ). We wish to find if ϕ1 implies that trying
to unlock the door with key 2 fails to open it. This is equivalent to asking if all models consistent
with ϕ1 give the value TRUE to unlock2locked
locked .
We can answer such a query by taking the SLAF0 output formula, ϕ1 , and checking if ϕ1 ∧
¬unlock2locked
locked is SAT (has a model). (Follows from the Deduction Theorem for propositional logic:
ϕ |= ψ iff ϕ ∧ ¬ψ is not SAT.)
One example application of this approach is the goal achievement algorithm of Chang and Amir
(2006). It relies on SAT algorithms to find potential plans given partial knowledge encoded as a
transition belief formula.
2. This is not model checking in the sense used in the Formal Verification literature. There, the model is a transition
model, and checking is done by updating a formula in OBDD with some transformations

359

A MIR & C HANG

Our zeroth-level algorithm may enable more compact representation, but it does not guarantee
it, nor does it guarantee tractable computation. In fact, no algorithm can maintain compact representation or tractable computation in general. Deciding if a clause is true as a result of SLAF is
coNP-hard because the similar decision problem for Logical Filtering is coNP-hard (Eiter & Gottlob, 1992; Amir & Russell, 2003) even for deterministic actions. (The input representation for
both problems includes an initial belief state formula in CNF. The input representation for Filtering
includes further a propositional encoding in CNF of the (known) transition relation.)
Also, any representation of transition belief states that uses poly(|P|) propositions grows exponentially (in the number of time steps and |P|) for some starting transition belief states and action
sequences, when actions are allowed to be nondeterministic 3 . The question of whether such exponential growth must happen with deterministic actions and flat formula representations (e.g., CNF,
DNF, etc.; see Darwiche & Marquis, 2002) is open (logical circuits are known to give a solution
for deterministic actions, when a representation is given in terms of fluents at time 0, Shahaf et al.,
2006).

4. Factored Formula Update
Update of any representation is hard when it must consider the set of interactions between all parts
of the representation. Operations like those used in SLAF 0 consider such interactions, manipulate
them, and add many more interactions as a result. When processing can be broken into independent
pieces, computation scales up linearly with the number of pieces (i.e., computation time is the
total of times it takes for each piece separately). So, it is important to find decompositions that
enable such independent pieces and computation. Hereforth we examine one type of decomposition,
namely, one that follows logical connectives.
Learning world models is easier when SLAF distributes over logical connectives. A function,
f , distributes over a logical connective, · ∈ {∨, ∧, ...}, if f (ϕ · ψ) ≡ f (ϕ) · f (ψ). Computation of
SLAF becomes tractable, when it distributes over ∧, ∨. The bottleneck of computation in that case
becomes computing SLAF for each part separately.
In this section we examine conditions that guarantee such distribution, and present a linear-time
algorithm that gives an exact solution in those cases. We will also show that the same algorithm
gives a weaker transition belief formula when such distribution is not possible.
Distribution properties that always hold for SLAF follow from set theoretical considerations and
Theorem 3.3:
Theorem 4.1 For ϕ, ψ transition belief formulas, a action,
SLAF [a](ϕ ∨ ψ) ≡ SLAF [a](ϕ) ∨ SLAF [a](ψ)
|= SLAF [a](ϕ ∧ ψ) ⇒ SLAF [a](ϕ) ∧ SLAF [a](ψ)
P ROOF
See Appendix B.3. 2
Stronger distribution properties hold for SLAF whenever they hold for Logical Filtering.
Theorem 4.2 Let ρ1 , ρ2 be transition belief states.
SLAF [a](ρ1 ∩ ρ2 ) = SLAF [a](ρ1 ) ∩ SLAF [a](ρ2 )
3. This follows from a theorem about filtering by Amir and Russell (2003), even if we provide a proper axiomatization
(note that our axiomatization above is for deterministic actions only).

360

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

iff for every R
R
R
R
F ilter[a](ρR
1 ∩ ρ2 ) = F ilter[a](ρ1 ) ∩ F ilter[a](ρ2 ).

We conclude the following corollary from Theorems 3.3, 4.2 and theorems by Amir and Russell
(2003).
Corollary 4.3 For ϕ, ψ transition belief formulas, a action, SLAF [a](ϕ ∧ ψ) ≡ SLAF [a](ϕ) ∧
SLAF [a](ψ) if for every relation R in ρϕ ,ρψ one of the following holds:
1. a in R maps states 1:1
2. a in R has no conditional effects, ϕ ∧ ψ includes all its prime implicates, and we observe if a
fails
3. The state is known for R: for at most one s, hs, Ri ∈ ρϕ ∪ ρψ .
Condition 2 combines semantics and syntax. It is particularly useful in the correct computation
of SLAF in later sections. It states when ϕ ∧ ψ has a particular syntactic form (namely, together
they include their joint prime implicates), and a is simple enough (but not necessarily 1:1), then
computation of SLAF can be broken into separate SLAF of ϕ and ψ.
Figure 3 presents Procedure Factored-SLAF, which computes SLAF exactly when the conditions of Corollary 4.3 hold. Consequently, Factored-SLAF returns an exact solution whenever our
actions are known to be 1:1. If our actions have no conditional effects and their success/failure is
observed, then a modified Factored-SLAF can solve this problem exactly too (see Section 5).
PROCEDURE Factored-SLAF(hai , oi i0<i≤t ,ϕ)
∀i, ai action, oi observation, ϕ transition belief formula.
1. For i from 1 to t do,
(a) Set ϕ ← Step-SLAF(oi ,ai ,ϕ).
(b) Eliminate subsumed clauses in ϕ.
2. Return ϕ.
PROCEDURE Step-SLAF(o,a,ϕ)
o an observation formula in L(P), a an action, ϕ a transition belief formula.
1. If ϕ is a literal, then return o∧Literal-SLAF(a,ϕ).
2. If ϕ = ϕ1 ∧ ϕ2 , return Step-SLAF(o,a,ϕ1 )∧Step-SLAF(o,a,ϕ2 ).
3. If ϕ = ϕ1 ∨ ϕ2 , return Step-SLAF(o,a,ϕ1 )∨Step-SLAF(o,a,ϕ2 ).
PROCEDURE Literal-SLAF(a,ϕ)
a an action, ϕ a proposition in Lt or its negation.
0
1. Return CnL (ϕ ∧ Teff (a))[P 0 /P ] .

Figure 3: SLAF using distribution over ∧, ∨
If we pre-compute (and cache) the 2n possible responses of Literal-SLAF, then every time step
t in this procedure requires linear time in the representation size of ϕ, our transition belief formula
at that time step. This is a significant improvement over the (super exponential) time taken by
a straightforward algorithm, and over the (potentially exponential) time taken by general-purpose
consequence finding used in our zeroth-level SLAF procedure above.
Theorem 4.4 Step-SLAF(a, o, ϕ) returns a formula ϕ0 such that SLAF [a, o](ϕ) |= ϕ0 . If every run
of Literal-SLAF takes time c, then Step-SLAF takes time O(|ϕ|c). (recall that |ϕ| is the syntactic,
representation size of ϕ.) Finally, if we assume one of the assumptions of Corollary 4.3, then
ϕ0 ≡ SLAF [a, o](ϕ).
361

A MIR & C HANG

A belief-state formula is a transition belief formula that has no effect propositions, i.e., it includes only fluent variables and no propositions of the form a FG . This is identical to the traditional
use of the term belief-state formula, e.g., (Amir & Russell, 2003). We can give a closed-form solution for the SLAF of a belief-state formula (procedure Literal-SLAF in Figure 3). This makes
procedure Literal-SLAF tractable, avoiding general-purpose inference in filtering a single literal,
and also allows us to examine the structure of belief state formulas in more detail.
V
Theorem 4.5 For belief-state formula ϕ ∈ L(P), action a, Ca = G∈G,l∈F (alG ∨ a¬l
G ), and
G1 , ..., Gm ∈ G all the terms in G such that Gi |= ϕ,
SLAF [a](ϕ) ≡

^

m
_

¬li
(li ∨ aG
) ∧ Ca
i

l1 ,...,lm ∈F i=1

V

Here, l1 ,...,lm ∈F means a conjunction over all possible (combinations of) selections of m literals
from F.
P ROOF
See Appendix B.4. 2
This theorem is significant in that it says that we can can write down the result of SLAF in a
prescribed form. While this form is still potentially of exponential size, it boils down to simple
computations. Its proof follows by a straightforward (though a little long) derivation of the possible
prime implicates of SLAF [a](ϕt ).
A consequence of Theorem 4.5 is that we can implement Procedure Literal-SLAF using the
equivalence
¾
½
Theorem 4.5
L(l) ⊆ P
SLAF [a](l) ≡
l ∧ SLAF [a](TRUE) otherwise

Notice that the computation in Theorem 4.5 for ϕ = l a literal is simple because G 1 , ..., Gm are
all the complete terms in L(P) that include l. This computation does not require a general-purpose
consequence finder, and instead we need to answer 2n+1 queries at the initialization phase, namely,
storing in a table the values of SLAF [a](l) for all l = p and l = ¬p for p ∈ P and also for
l = TRUE.
In general, m could be as high as 2|P| , the number of complete terms in G, and finding G1 , ..., Gm
may take time exponential in |P|. Still, the simplicity of this computation and formula provide the
basic ingredients needed for efficient computations in the following sections for restricted cases. It
should also give guidelines on future developments of SLAF algorithms.

5. Compact Model Representation
Previous sections presented algorithms that are potentially intractable for long sequences of actions
and observations. For example, in Theorem 4.5, m could be as high as 2 |P| , the number of complete
terms in G. Consequently, clauses may have exponential length (in n = |P|) and there may be a
super-exponential number of clauses in this result.
In this section we focus on learning action models efficiently in the presence of action preconditions and failures. This is important for agents that have only partial domain knowledge and are
therefore likely to attempt some inexecutable actions.
We restrict our attention to deterministic actions with no conditional effects, and provide an
overall polynomial bound on the growth of our representation, its size after many steps, and the
362

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

time taken to compute the resulting model. This class of actions generalizes STRIPS (Fikes, Hart,
& Nilsson, 1972), so our results apply to a large part of the AI-planning literature.
We give an efficient algorithm for learning both non-conditional deterministic action effects and
preconditions, as well as an efficient algorithm for learning such actions’ effects in the presence of
action failures.
5.1 Actions of Limited Effect
In many domains we can assume that every action a affects at most k fluents, for some small k > 0.
It is also common to assume that our actions are STRIPS, and that they may fail without us knowing,
leaving the world unchanged. Those assumptions together allow us to progress SLAF with a limited
(polynomial factor) growth in the formula size.
We use a language that is similar to the one in Section 3, but which uses only action propositions
alG with G being
a fluent term of size k (instead of a fluent term of size n in G). Semantically,
V
l
al1 ∧...∧lk ≡ lk+1 ,...,ln all1 ∧...∧ln .
Theorem 5.1 Let ϕ ∈ L(P) be a belief-state formula, and a a STRIPS action with ≤ k fluents
affected or in the precondition term. Let G k be the set of all terms of k fluents in L(P) that are
consistent with ϕ. Then,
SLAF [a](ϕ) ≡

^

k
_

¬li
(li ∨ aG
) ∧ Ca
i

i=1
G1 , ..., Gk ∈ G k
G1 ∧ ... ∧ Gk |= ϕ
l1 , ..., lk ∈ F

V
Here, ... refers to a conjunction over all possible (combinations of) selections of m literals from
F and G1 , ..., Gk from G k such that G1 ∧ ... ∧ Gk |= ϕ.
P ROOF
See Section B.5. 2
The main practical difference between this theorem and Theorem 4.5 is the smaller number of
terms that need to be checked for practical computation. The limited language enables and entails
a limited number of terms that are at play here. Specifically, when a has at most k literals in its
preconditions, we need to check all combinations of k terms G 1 , ..., Gk ∈ G k , computation that is
bounded by O(exp(k)) iterations.
The proof uses two insights. First, if a has only one case in which change occurs, then every
clause in Theorem 4.5 is subsumed by a clause that is entailed by SLAF [a](ϕ t ), has at most one
alGi i per literal li (i.e., li 6= lj for i 6= j) and Gi is a fluent term (has no disjunctions). Second, every
alG with G term is equivalent to a formula on alGi with Gi terms of length k, if a affects only k
fluents.
Thus, we can encode all of the clauses in the conjunction using a subset of the (extended) action
effect propositions, alG , with G being a term of size k. There are O(nk ) such terms, and O(nk+1 )
such propositions. Every clause is of length 2k, with the identity of the clause determined by the
2
first half (the set of action effect propositions). Consequently, SLAF [a](ϕ t ) takes O(nk +k · k 2 )
2
space to represent using O(nk +k ) clauses of length ≤ 2k.
363

A MIR & C HANG

5.2 Actions of No Conditional Effects: Revised Language
In thisSsection we reformulate the representation that we presented above in Section 3.1. Let
L0f = a∈A {af , af ◦ , a¬f , a[f ] , a[¬f ] } for every f ∈ P. Let the vocabulary for the formulas representing transition belief states be defined as L = P ∪ L0f . The intuition behind propositions in this
vocabulary is as follows:
V
• al – “a causes l” for literal l. Formally, al ≡ s∈S als .
V
• af ◦ – “a keeps f ”. Formally, af ◦ ≡ s∈S ((s ⇒ f ) ⇒ afs ) ∧ ((s ⇒ ¬f ) ⇒ a¬f
s ).
V
).
(Thus, l is a
• a[l] – “a causes FALSE if ¬l”. Formally, a[l] ≡ s∈S (s ⇒ ¬l) ⇒ (als ∧ a¬l
s
precondition for executing a, and it must hold when a executes.)
For each model of a transition belief formula over L, the valuation of the fluents from P defines a
state. The valuation of the propositions from all L0f defines an unconditional deterministic transition
relation as follows: action proposition af (a¬f ) is true if and only if action a in the transition
relation causes f (¬f ) to hold after a is executed. Action proposition a f ◦ is true if and only if
action a does not affect fluent f . Action proposition a[f ] (a[¬f ] ) is true if and only if f (¬f ) is in
the precondition of a. We assume the existence of logical axioms that disallow “inconsistent” or
“impossible” models. These axioms are:
1. af ∨ a¬f ∨ af ◦
2. ¬(af ∧ a¬f ) ∧ ¬(a¬f ∧ af ◦ ) ∧ ¬(af ∧ af ◦ )
¢
¡
3. ¬ a[f ] ∧ a[¬f ]

for all possible a ∈ A and f ∈ P. The first two axioms state that in every action model, exactly one
of af , a¬f , or af ◦ must hold (thus, a causes f , its negation, or keeps f unchanged). The last axiom
disallows interpretations where both a[f ] and a[¬f ] hold. We state these axioms so that we do not
need to represent these constraints explicitly in the transition belief formula itself.
We will use the set theoretic and propositional logic notations for transition belief states interchangeably. Note that the vocabulary we have defined is sufficient for describing any unconditional
STRIPS action model, but not any deterministic action model in general.
Example 5.2 Consider the domain from Example 2.2. The transition belief state ϕ can be represented by the transition belief formula:
locked ∧
((unlock1¬locked ∧ unlock2locked◦ ∧ unlock3locked◦ ) ∨
(unlock1locked◦ ∧ unlock2¬locked ∧ unlock3locked◦ ) ∨
(unlock1locked◦ ∧ unlock2locked◦ ∧ unlock3¬locked )).
2
We provide an axiomatization that is equivalent to SLAF and is a special case of T eff (1) with
our notation above. We do this over P and P 0 . Recall that we intend primed fluents to represent the
value of the fluent immediately after action a is taken.
364

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

τeff (a) ≡

^

Prea,f ∧ Effa,f

f ∈P

Prea,f

≡

^

(a[l] ⇒ l)

l∈{f,¬f }

Effa,f

≡

^

((al ∨ (af ◦ ∧ l)) ⇒ l0 ) ∧ (l0 ⇒ (al ∨ (af ◦ ∧ l))).

l∈{f,¬f }

Here Prea,f describes the precondition of action a. It states that if literal l occurs in the precondition
of a then literal l must have held in the state before taking a. The formula Eff a,f describes the effects
of action a. It states that the fluents before and after taking the action must be consistent according
to the action model defined by the propositions af , a¬f , and af ◦ .
Now we can show that the revised axiomatization of action models, τ eff , leads to an equivalent
definition to SLAF within our restricted action models.
0

Theorem 5.3 For any successful action a, SLAF [a](ϕ) ≡ Cn L∪P (ϕ ∧ τeff (a))[P 0 /P] .
P ROOF

See Appendix B.6. 2

5.3 Always-Successful Non-Conditional Actions
We are now ready to present an algorithm that learns effects for actions that have no conditional
effects. The algorithm allows actions that have preconditions that are not fully known. Still, it
assumes that the filtered actions all executed successfully (without failures), so it cannot effectively
learn those preconditions (e.g., it would know only some more than it knew originally about those
preconditions after seeing a sequence of events). Such a sequence of actions might, for example, be
generated by an expert agent whose execution traces can be observed.
The algorithm maintains transition belief formulas in a special fluent-factored form, defined below. By maintaining formulas in this special form, we will show that certain logical consequence
finding operations can be performed very efficiently. A formula is fluent-factored, if it is the conjunction of formulas ϕf such that each ϕf concerns only one fluent, f , and action propositions.
Also, for every fluent, f , ϕf is conjunction of a positive element, a negative element, and a
neutral one ϕf ≡ (¬f ∨ explf ) ∧ (f ∨ expl¬f ) ∧ Af , with explf , expl¬f , Af formulae over action
propositions af , a¬f , a[f ] , a[¬f ] , and af ◦ (possibly multiple different actions). The intuition here
is that explf and expl¬f are all the possible explanations for f being true and false, respectively.
Also, Af holds knowledge about actions’ effects and preconditions on f , knowledge that does
not depend on f ’s current value. Note that any formula in L(L f ) can be represented as a fluentfactored formula. Nonetheless, a translation sometimes leads to a space blowup, so we maintain our
representation in this form by construction.
The new learning algorithm, AS-STRIPS-SLAF4 , is shown in Figure 4. To simplify exposition,
it is described for the case of a single action-observation pair, though it should be obvious how to
apply the algorithm to sequences of actions and observations. Whenever an action is taken, first
the subformulas Af , explf , and expl¬f for each ϕf are updated according to steps 1.(a)-(c). Then,
4. AS-STRIPS-SLAF extends AE-STRIPS-SLAF (Amir, 2005) by allowing preconditions for actions

365

A MIR & C HANG

Algorithm AS-STRIPS-SLAF[ha, oi](ϕ)
Inputs:
V Successful action a, observation term o, and fluent-factored transition belief formula ϕ =
f ∈P ϕf .

Returns: Fluent-factored transition belief formula for SLAF [ha, oi](ϕ)
1. For every f ∈ P
(a) Set Af ← (¬a[f ] ∨ explf ) ∧ (¬a[¬f ] ∨ expl¬f ) ∧ Af
(b) Set explf ← (af ∨ (af ◦ ∧ ¬a[¬f ] ∧ explf ))
(c) Set expl¬f ← (a¬f ∨ (af ◦ ∧ ¬a[f ] ∧ expl¬f ))

(d) If o |= f (f is observed) then seta , ϕf ← (¬f ∨ >) ∧ (f ∨ ⊥) ∧ Af ∧ explf
(e) If o |= ¬f then set ϕf ← (¬f ∨ ⊥) ∧ (f ∨ >) ∧ Af ∧ expl¬f [Note: If o 6|= f and
o 6|= ¬f , we do nothing beyond the earlier steps.]
2. Simplify ϕ (e.g., eliminate subsumed clauses in ϕ).
3. Return ϕ
a. The term (¬f ∨ >) is the new explf , and (f ∨ ⊥) is the new expl¬f . They appear here without simplification
to conform to Step 1a of the algorithm. This also emphasizes the syntactic nature of the procedure, and that no
implicit logical simplification is assumed.

Figure 4: SLAF algorithm for always successful STRIPS actions.
when an observation is received, each ϕf is updated according to the observation according to steps
1.(d)-(e). Step 2 merely indicates that in most implementations, it is likely that some simplification
procedure will be used on the formula such as subsumption elimination. However, the use of any
such simplification procedure is not strictly necessary in order for the theoretical guarantees of our
algorithm to hold.
For example, if we know nothing about actions that affect f (e.g., when we start our exploration),
then ϕf = (¬f ∨ T RU E) ∧ (f ∨ T RU E) ∧ T RU E. With this representation, SLAF [a](ϕf ) is the
conjunction (¬f ∨explf )∧(f ∨expl¬f )∧Af as computed by step 1 of Procedure AS-STRIPS-SLAF.
A similar formula holds for observations.
The following theorem shows the correctness of the algorithm. It shows that the steps taken
by the algorithm produce a result equivalent to the logical consequence-finding characterization of
SLAF of Theorem 5.3.
Theorem 5.4 SLAF[ha, oi](ϕ) ≡ AS-STRIPS-SLAF[ha, oi](ϕ) for any fluent-factored formula ϕ,
successfully executed action a, and observation term o.
P ROOF
See Appendix B.7 2
The time and space complexity of procedure AS-STRIPS-SLAF are given in the following
theorem. As a time guarantee, it is shown that the procedure takes linear time in the size of the input
formula. Under the condition that the algorithm receives observations often enough—specifically
366

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

that every fluent is observed at least once every k calls to the procedure—it is possible to show
that the transition belief formula remains in k-CNF indefinitely (recall that ϕ is in k-CNF for some
fixed k, if ϕ is a conjunction of clauses each of size ≤ k). Thus, regardless of the length of actionobservation input sequence, the output of AS-STRIPS-SLAF and the value of ϕ throughout its
computation is in k-CNF. This amounts to a space guarantee on the size of the formula.
Theorem 5.5 The following are true of AS-STRIPS-SLAF:
1. The procedure takes linear time in the size of the input formula for a single action, observation
pair input.
2. If for every fluent and every k steps there is an observation of that fluent in one of those steps,
and the input formula is in k-CNF, them the resulting formula (after an arbitrary number of
steps) is in k-CNF.
3. If the input of AS-STRIPS-SLAF is fluent-factored, then so is its output.
P ROOF
See Appendix B.8 2
The following corollary follows immediately from the above.
Corollary 5.6 In order to process T steps of actions and observations, AS-STRIPS-SLAF requires
O (T · |P|) time. Additionally, if ³
every fluent´is observed every at most k steps, then the resulting
formula always has size that is O |P| · |A|k .
This Corollary holds because Theorem 5.5(2) guarantees a bound on the size of our belief-state
formula at any point in the algorithm.

5.4 Learning Actions Which May Fail
In many partially observable domains, a decision-making agent cannot know beforehand whether
each action it decides to take will fail or succeed. In this section we consider possible action failure,
and assume that the agent knows whether each action it attempts fails or succeeds after trying the
action.
More precisely, we assume that there is an additional fluent OK observed by the agent such
that OK is true if and only if the action succeeded. A failed action, in this case, may be viewed as
an extra “observation” by the agent that the preconditions for the action were not met. That is, an
action failure is equivalent to the observation
^
¬
(a[f ] ⇒ f ) ∧ (a[¬f ] ⇒ ¬f ).
(3)
f ∈P

Action failures make performing the SLAF operation considerably more difficult. In particular,
observations of the form (3) cause interactions between fluents where the value of a particular fluent
might no longer depend on only the action propositions for that fluent, but on the action propositions
for other fluents as well. Transition belief states can no longer be represented by convenient fluentfactored formulas in such cases, and it becomes more difficult to devise algorithms which give
useful time and space performance guarantees.
367

A MIR & C HANG

Algorithm PRE-STRIPS-SLAF[a, o](ϕ)
Inputs: Action a and observation
term o. The transition belief formula ϕ has the following facV W
tored form: ϕ = i j ϕi,j , where each ϕi,j is a fluent-factored formula.
Returns: Filtered transition belief formula ϕ
1. If o |= ¬OK:
W
(a) Set ϕ ← ϕ ∧ i F (¬li ) where li are the literals appearing in a’s precondition, and
F (l) is the V
fluent-factored formula equivalent to l (i.e., F (l) = ((l ⇒ >) ∧ (¬l ⇒
⊥) ∧ >) ∧ f ∈P ((f ⇒ >) ∧ (¬f ⇒ >) ∧ >))

(b) Set ϕi,j ← AS-STRIPS-SLAF[o](ϕi,j ) for all ϕi,j
2. Else (o |= OK):
(a) For all ϕi,j

i. Set ϕi,j ← AS-STRIPS-SLAF[P ](ϕi,j ), where P is the precondition of a
ii. Set ϕi,j ← AS-STRIPS-SLAF[ha, oi](ϕi,j )
3. Each ϕi,j is factored into Ai,j ∧ Bi,j where Bi,j contains all (and only) clauses containing
a fluent from P. For
W any i such that there exists B such that for all j, B i,j ≡ B, replace
W
ϕ
with
B
∧
j Ai,j
j i,j
4. Simplify each ϕi,j (e.g. remove subsumed clauses)
5. Return ϕ
Figure 5: Algorithm for handling action failures when preconditions are known.
As we shall demonstrate, action failures can be dealt with tractably if we assume that the action
preconditions are known by the agent. That is, the agent must learn the effects of the actions it can
take, but does not need to learn the preconditions of these actions. In particular, this means that for
each action a, the algorithm is given access to a formula (more precisely, logical term) P a describing
the precondition of action a. Clearly, because the algorithm does not need to learn the preconditions
of its actions, we can restrict the action proposition vocabulary used to describe belief states to the
ones of the forms af , a¬f , and af ◦ , as we no longer need action propositions of the forms a[f ] or
a[¬f ] .
We present procedure PRE-STRIPS-SLAF5 (Figure 5) that performs SLAF on transition belief
formulas in the presence of action failures for actions of non-conditional effects. It maintains transition belief
as conjunctions of disjunctions of fluent-factored formulas (formulas of the
V formulas
W
form ϕ = i j ϕi,j where each ϕi,j is fluent factored). Naturally, such formulas are a superset of
all fluent-factored formulas.
5. PRE-STRIPS-SLAF is essentially identical to CNF-SLAF (Shahaf et al., 2006)

368

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

The algorithm operates as follows: When an action executes successfully (and an ensuing observation is received), each of the component fluent-factored formulas ϕ i,j is filtered separately
according to the AS-STRIPS-SLAF procedure on the action-observation pair (Step 2). On the other
hand, when an action fails, a disjunction of fluent-factored formulas is appended to the transition
belief formula (Step 1). Each component of the disjunction corresponds to one of the possible reasons the action failed (i.e., to one of the literals occurring in the action’s precondition). Finally, as
observations are accumulated by the learning algorithm, it collapses disjunctions of fluent-factored
formulas occurring in the belief formula together (Step 3) or simplifies them generally (Step 4),
decreasing the total size of the formula. As with the case of AS-STRIPS-SLAF, these simplification
steps are not necessary in order for our time and space guarantees to hold.
The proof of correctness of Algorithm PRE-STRIPS-SLAF relies on our distribution results
from Section 4, Theorem 4.1 and Corollary 4.3.
We proceed to show the correctness of PRE-STRIPS-SLAF. The following theorem shows that
the procedure always returns a filtered transition belief formula that is logically weaker than the
exact result, so it always produces a safe approximation. Additionally, the theorem shows that
under the conditions of Corollary 4.3, the filtered transition belief formula is an exact result.
Theorem 5.7 The following are true:
1. SLAF[a, o](ϕ) |= PRE-STRIPS-SLAF[a, o](ϕ)
2. PRE-STRIPS-SLAF[a, o](ϕ) ≡ SLAF[a, o](ϕ) if Corollary 4.3 holds.
P ROOF
See Appendix B.9. 2
Now we consider the time and space complexity of the algorithm. The following theorem shows
that (1) the procedure is time efficient, and (2) given frequent enough observations (as in Theorem
5.5), the algorithm is space efficient because the transition belief formula stays indefinitely compact.
Theorem 5.8 The following are true of PRE-STRIPS-SLAF:
1. The procedure takes time linear in the size of the formula for a single action, observation pair
input.
2. If every fluent is observed every at most k steps and the input formula is in m · k-CNF, then
the filtered formula is in m · k-CNF, where m is the maximum number of literals in any action
precondition.
P ROOF
See Appendix B.10 2
Therefore, we get the following corollary:
Corollary 5.9 In order to process T steps of actions and observations, PRE-STRIPS-SLAF requires
O (T · |P|) time. If every fluent is ³
observed at least
´ as frequently as every k steps, then the resulting
mk
formula always has size that is O |P| · |A|
.

6. Building on Our Results
In this section we describe briefly how one might extend our approach to include an elaborate
observation model, bias, and parametrized actions.
369

A MIR & C HANG

6.1 Expressive Observation Model
The observation model that we use throughout this paper is very simple: at every state, if a fluent is
observed to have value v, then this is its value in the current state. We can consider an observation
model that is more general.
An observation model, O, is a set of logical sentences that relates propositions in a set Obs with
fluents in P. Obs includes propositions that do not appear in P, and which are independent of the
previous and following state (times t − 1 and t + 1) given the fluents at time t.
SLAF with o is the result of conjoining ϕt with CnLt (o∧O), i.e., finding the prime implicates of
o ∧ O and conjoining those with ϕt . We can embed this extension into our SLAF algorithms above,
if we can maintain the same structures that those algorithms use. If O is in k-CNF and at every step
we observe all but (at most) 1 variable, then finding the prime implicates is easy. Embedding this
into the transition-belief formula is done by conjunction of these prime implicates with the formula,
and removal of subsumed clauses. The resulting formula is still fluent factored, if the input was
fluent factored. Then, the algorithms above remain applicable with the same time complexity, by
replacing ot with the prime implicates of ot ∧ Ot .
Using the Model The algorithms we described above provide an exact solution to SLAF, and
all the tuples hs, Ri that are in this solution are consistent with our observations. They compute a
solution to SLAF that is represented as a logical formula. We can use a SAT solver (e.g., Moskewicz
et al., 2001) to answer queries over this formula, such as checking if it entails a f , for action a and
fluent f . This would show if in all consistent models action a makes f have the value TRUE.
The number of variables in the result formula is always independent of T , and is linear in |P|
for some of our algorithms. Therefore, we can use current SAT solvers to treat domains of 1000
features and more.
Preference and Probabilistic Bias Many times we have information that leads us to prefer some
possible action models over others. For example, sometimes we can assume that actions change
only few fluents, or we suspect that an action (e.g., open-door) does not affect some features (e.g.,
position) normally. We can represent such bias using a preference model (e.g., McCarthy, 1986;
Ginsberg, 1987) or a probabilistic prior over transition relations (e.g., Robert, Celeux, & Diebolt,
1993).
We can add this bias at the end of our SLAF computation, and get an exact solution if we can
compute the effect of this bias together with a logical formula efficiently. Preferential biases were
studied before and fit easily with the result of our algorithms (e.g., we can use implementations of
Doherty, Lukaszewicz, & Szalas, 1997, for inference with such bias).
Also, algorithms for inference with probabilistic bias and logical sentences are now emerging
and can be used here too (Hajishirzi & Amir, 2007). There, the challenge is to not enumerate
tentative models explicitly, a challenge that is overcome with some success in the work of Hajishirzi
and Amir (2007) for the similar task of filtering. We can use such algorithms to apply probabilistic
bias to the resulting logical formula.
For example, given a probabilistic graphical model (e.g., Bayesian Networks) and a set of propositional logical sentences, we can consider the logical sentences as observations. With this approach,
−
−
a logical sentence ϕ gives rise to a characteristic function δϕ (→
x ) which is 1 when →
x satisfies ϕ and
0 otherwise. For a conjunction of clauses we get a set of such functions (one per clause). Thus,
inference in the combined probabilistic-logical system is a probabilistic inference. For example,
370

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

one can consider variable elimination (e.g., Dechter, 1999) in which there are additional potential
functions.
Parametrized Actions In many systems and situations it is natural to use parametrized actions.
These are action schemas whose effect depend on their parameters, but their definition applies
identically to all instantiations.
For example, move(b, x, y) can be an action which moves b from position x to position y, with
b, x, y as parameters of the action. These are very common in planning systems (e.g., STRIPS,
PDDL, Situation Calculus). A complete treatment of parameterized actions is outside the scope of
this paper, but we give guidelines for a generalization of our current approach for such actions.
Consider a domain with a set of fluent predicates and a universe of named objects. The propositional fluents that are defined in this domain are all ground instantiations of predicate fluents. SLAF
can work on this set of propositional fluents and instantiated actions in the same manner as for the
→
−
rest of this paper. We have action propositions a( X )lG instantiated for every vector of object names
→
−
X.
−
−
−
−
y )lG .
The different treatment comes in additional axioms that say that ∀ →
x,→
y .a(→
x )lG ⇐⇒ a(→
Inference over a transition belief state with these axioms will be able to join information collected
about different instantiations of those actions. We expect that a more thorough treatment will be
able to provide more efficient algorithms whose time complexity depend on the number of action
schemas instead of the number of instantiated actions.
Several approaches already start to address this problem, including the work of Nance, Vogel,
and Amir (2006) for filtering and the work of Shahaf and Amir (2006) for SLAF.

7. Experimental Evaluation
Previous sections discussed the problem settings that we consider and algorithms for their solutions. They showed that modifying traditional settings for learning in dynamic partially observable
domains is important. Determinism alone does not lead to tractability, but our additional assumptions of simple, logical action structure and bounded from 0 frequency of observations for all fluents
do. Specifically, so far we showed that the time and space for computing SLAF of a length-T time
sequence over n fluents are polynomial in T and n.
This section considers the practical considerations involved in using our SLAF procedures. In
particular, it examines the following questions:
• How much time and space do SLAF computations take in practice?
• How much time is required to extract a model from the logical formula in the result of our
SLAF procedures?
• What is the quality of the learned model (taking an arbitrary consistent model)? How far is it
from the true (generating) model?
• Do the conditions for the algorithms’ correctness hold in practice?
• Can the learned model be used for successful planning and execution? How do the learning
procedures fit with planning and execution?
We implemented our algorithms and ran experiments with AS-STRIPS-SLAF over the following domains taken from the 3rd International Planning Competition (IPC): Drivelog, Zenotravel,
Blocksworld, and Depots (details of the domains and the learning results appear in Appendix C).
371

A MIR & C HANG

Each such experiment involves running a chosen algorithm over a sequence of randomly generated
action-observation sequences of 5000 steps. Information was recorded every 200 steps.
The random-sequence generator receives the correct description of the domain, specified in
PDDL (Ghallab, Howe, Knoblock, McDermott, Ram, Veloso, Weld, & Wilkins, 1998; Fox & Long,
2002) (a plannig-domain description language), the size of the domain, and a starting state. (The
size of the domain is the number of propositional fluents in it. It is set by a specification of the
number of objects in the domain and the number and arity of predicates in the domain.) It generates
a valid sequence of actions and observations for this domain and starting state, i.e., a sequence that
is consistent with the input PDDL to that generator but in which actions may fail (action failure is
consistent with the PDDL if the action is attempted in a state in which it canot execute).
For our experiments, we chose to have observations as follows: In every time step we select 10
fluents uniformly at random to observe. We applied no additional restrictions (such as making sure
each fluent was observed every fixed k steps).
Our SLAF algorithm receives only such a sequences of actions and observations, and no domain
information otherwise (e.g., it does not receive the size of the domain, the fluents, the starting state,
or the PDDL). The starting knowledge for the algorithm is the empty knowledge, TRUE.
For each domain we ran the algorithm over different numbers of propositional fluents (19 to 250
fluents). We collected the time and space taken for each SLAF computation and plotted them as a
function of the input-sequence length (dividing by t the total computation time for t steps). The
time and space results are shown in Figures 6, 7, 8, and 9. The graphs are broken into the different
domains and compare the time and space taken for different domain sizes. The time is SLAF-time
without CNF simplification (e.g. we do not remove subsumed clauses)
How much time and space do SLAF computations take in practice? We can answer the first
question now. We can observe in these figures that the time per step remains relatively constant
throughout execution. Consequently, the time taken to perform SLAF of different domains grows
linearly with the number of time steps. Also, we see that the time for SLAF grows with the domain
size, but scales easily for moderate domain sizes (1ms per step of SLAF for domains of 200 fluents).
How much time is required to extract a model from the logical formula in the result of our
SLAF procedures? Our SLAF procedures return a logical formula for each sequence of actions
and observations. We need to apply further work to extract a candidate (consistent) model from that
formula. This computation is done with a SAT solver for CNF formulas.
What is the quality of the learned model (taking an arbitrary consistent model)? How far is it
from the true (generating) model? There are sometimes many possible models, and with little
further bias we must consider each of them possible and as likely. We decided to introduce one
such bias, namely, that actions are instances of actions schemas. Thus, the actions are assumed to
have the same effect on their parameters (or objects), given properties of those parameters. Thus,
actions’ effects are assumed independent of the identity of those parameter.
So, with the vanilla implementation, there are propositions which look like
((STACK
((STACK
((STACK
((STACK
etc.

E
E
A
A

G)
G)
B)
A)

CAUSES
CAUSES
CAUSES
CAUSES

(ON
(ON
(ON
(ON

E
G
A
A

G))
E))
B))
A))

372

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

SLAF Time/Step: Blocksworld Domain
1.6

1.4

1.2

Time (ms)

1

19 fluents
41 fluents
71 fluents
131 fluents
209 fluents

0.8

0.6

0.4

0.2

0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Time/Step: Depots Domain
1.6

1.4

1.2

Time (ms)

1

58 fluents
94 fluents
138 fluents
190 fluents
250 fluents

0.8

0.6

0.4

0.2

0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 6: SLAF-time without CNF simplification for domains Blocksworld and Depots

Instead, we replace ground propositions like those above with schematized propositions:
((STACK ?X ?Y) CAUSES (ON ?X ?Y))
((STACK ?X ?Y) CAUSES (ON ?Y ?X))
((STACK ?X ?Y) CAUSES (ON ?X ?Y))
etc.
Thus, the belief-state formula looks something like:
373

A MIR & C HANG

SLAF Time/Step: Driverlog Domain
1.8
1.6
1.4

Time (ms)

1.2
31 fluents
76 fluents
122 fluents
186 fluents
231 fluents

1
0.8
0.6
0.4
0.2
0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Time/Step: Zeno-Travel Domain
0.8

0.7

0.6

Time (ms)

0.5
58 fluents
91 fluents
134 fluents

0.4

0.3

0.2

0.1

0
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 7: SLAF-time without CNF simplification for domains Driverlog and Zeno-Travel

(AND
(AND
(OR (ON E G)
(OR ((STACK ?X ?Y) CAUSES (NOT (ON ?X ?Y)))
(AND ((STACK ?X ?Y) KEEPS (ON ?X ?Y))
(NOT ((STACK ?X ?Y) NEEDS (ON ?X ?Y))))))
...
374

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

SLAF Space: Blocksworld Domain
300K

Space (#lisp symbols)

250K

200K
19 fluents
41 fluents
71 fluents
131 fluents
209 fluents

150K

100K

50K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Space: Depots Domain
160K

140K

Space (#lisp symbols)

120K

100K

58 fluents
94 fluents
138 fluents
190 fluents
250 fluents

80K

60K

40K

20K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 8: SLAF space for domains Blocksworld and Depots

An example fragment of a model (the complete output is given in Appendix C) that is consistent
with our training data is
Blocksworld domain:
* 209 fluents
* 1000 randomly selected actions
* 10 fluents observed per step
* "schematized" learning
375

converting to CNF
clause count: 235492
variable count: 187
adding clauses
calling zchaff

A MIR & C HANG

SLAF Space: Driverlog Domain
250K

Space (#lisp symbols)

200K

150K

31 fluents
76 fluents
122 fluents
186 fluents
231 fluents

100K

50K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

SLAF Space: Zeno-Travel Domain
60K

Space (#lisp symbols)

50K

40K
58 fluents
91 fluents
134 fluents

30K

20K

10K

K
200

1000

1800

2600

3400

4200

5000

Input Sequence Length

Figure 9: SLAF space for domains Driverlog and Zeno-Travel

* 1:1 precondition heuristics

parsing result
SLAF time: 2.203
Inference time: 42.312
Learned model:

(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB)))
(UNSTACK NEEDS (CLEAR ?OB))
(UNSTACK NEEDS (ARM-EMPTY))
376

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
(UNSTACK
...

NEEDS (NOT (HOLDING ?OB)))
NEEDS (ON ?OB ?UNDEROB))
CAUSES (CLEAR ?UNDEROB))
CAUSES (NOT (CLEAR ?OB)))
CAUSES (NOT (ARM-EMPTY)))
CAUSES (HOLDING ?OB))
CAUSES (NOT (ON ?OB ?UNDEROB)))
KEEPS (ON-TABLE ?UNDEROB))
KEEPS (ON-TABLE ?OB))
KEEPS (HOLDING ?UNDEROB))
KEEPS (ON ?UNDEROB ?UNDEROB))
KEEPS (ON ?OB ?OB))
KEEPS (ON ?UNDEROB ?OB))

Sometimes, there are multiple possible schematized propositions that correspond to a ground
action proposition, in which case we disjoin the propositions together when doing the replacement
(i.e., a single ground propositional symbol gets replaced by a disjunction of schema propositions).
This replacement is a simple procedure, but one that is effective in both deriving more information for fewer steps and also in speeding up model finding from the SLAF formula. We implemented
it to run while SLAF runs. One could do this during the SAT-solving portion of the algorithm, with
an equivalent result.
Regarding the latter, we ran into scaling issues with the SAT solver (ZChaff , Moskewicz et al.,
2001; Tang, Yinlei Yu, & Malik, 2004) and the common lisp compiler in large experiments. We got
around these issues by applying the replacement scheme above, thus reducing greatly the number
of variables that the SAT solver handles.
Another issue that we ran into is that the SAT solver tended to choose models with blank preconditions (the sequences that we used in these experiments include no action failure, so ’no preconditions’ is never eliminated by our algorithm). To add some ”bias” to the extracted action model,
we added axioms of the following form:
(or (not (,a causes ,f)) (,a needs (not ,f)))
(or (not (,a causes (not ,f))) (,a needs ,f))
These axioms state that if an action a causes a fluent f to hold, then a requires f to not hold in
the precondition (similarly, there is an analagous axiom for ¬f ). Intuitively, these axioms cause
the sat solver to favor 1:1 action models. We got the idea for this heuristic from the work of Wu,
Yang, and Jiang (2007), which uses a somewhat similar set of axioms to bias their results in terms
of learning preconditions. Clearly, these axioms don’t always hold, and in the results, one can see
that the learned preconditions are often inaccurate.
Some other inaccuracies in the learned action models are reasonable. For example, if a fluent
never changes over the course of an action sequence, the algorithm may infer that an arbitrary action
causes that fluent to hold.
Do the conditions for the algorithms’ correctness hold in practice? In all of those scenarios
that we report here, the conditions that guarantee correctness for our algorithms hold. In our experiments we assumed that the main conditions of the algorithms hold, namely, that actions are
377

A MIR & C HANG

deterministic and of few preconditions. We did not enforce having observations for every fluent
every (fixed) k steps. The latter condition is not necessery for correctness of the algorithm, but is
necessary to guarantee polynomial-time computation. Our experiments verify that this is not necessary in practice, and it indeed be the case that the algorithms can have a polynomial-time guarantee
for our modified observation
An earlier work of Hlubocky and Amir (2004) has included a modified version of AS-STRIPSSLAF in their architecture and tested it on a suite of adventure-game-like virtual environments that
are generated at random. These include arbitrary numbers of places, objects of various kinds, and
configurations and settings of those. There, an agent’s task is to exit a house, starting with no
knowledge about the state space, available actions and their effects, or characteristics of objects.
Their experiments show that the agent learns the effects of its actions very efficiently. This agent
makes decisions using the learned knowledge, and inference with the resulting representation is fast
(a fraction of a second per SAT problem in domains including more than 30 object, modes, and
locations).
Can the learned model be used for successful planning and execution? How do the learning
procedures fit with planning and execution? The learned model can be used for planning by
translating it to PDDL. However, that model is not always the correct one for the domain, so the
plan may not be feasible or may not lead to the required goal. In those cases, we can interleave
planning, execution, and learning, as was described in the work of Chang and Amir (2006). There,
one finds a short plan with a consistent action model, executes that plan, collects observations, and
applies SLAF to those. When plan failure can be detected (e.g., that the goal was not achieved), the
results of Chang and Amir (2006) guarantee that the joint planning-execution-learning procedure
would reach the goal in a bounded amount of time. That bounded time is in fact linear in the length
of the longest plan needed for reaching the goal, and is exponential in the complexity of the action
model that we need to learn.

8. Comparison with Related Work
HMMs (Boyen & Koller, 1999; Boyen et al., 1999; Murphy, 2002; Ghahramani, 2001) can be used
to estimate a stochastic transition model from observations. Initially, we expected to compare our
work with the HMM implementation of Murphy (2002), which uses EM (a hill-climbing approach).
Unfortunately, HMMs require an explicit representation of the state space, and our smallest domain (31 features) requires a transition matrix of (231 )2 entries. This prevents initializing HMMs
procedures on any current computer.
Structure learning approaches in Dynamic Bayes Nets (DBNs) (e.g., Ghahramani & Jordan,
1997; Friedman, Murphy, & Russell, 1998; Boyen et al., 1999) use EM and additional approximations (e.g., using factoring, variation, or sampling), and are more tractable. However, they are still
limited to small domains (e.g., 10 features , Ghahramani & Jordan, 1997; Boyen et al., 1999), and
also have unbounded errors in discrete deterministic domains, so are not usable in our settings.
A simple approach for learning transition models was devised in the work of Holmes and
Charles Lee Isbell (2006) for deterministic POMDPs. There, the transition and observation models
are deterministic. This approach is close to ours in that it represents the hidden state and possible models using a finite structure, a Looping Prediction Suffix Tree. This structure can be seen
as related to our representation in that both grow models that relate action histories to possible
transition models. In our work here such interactions are realized in the recursive structure of the
378

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

transition-belief formula built by AS-STRIPS-SLAF, e.g.,
(af ∨ (af ◦ ∧ ¬a[¬f ] ∧ explf ))
where explf refers to a similar formula to this that was created in the previous time step.
The main difference that we should draw between our work and that of Holmes and Charles
Lee Isbell (2006) is that the latter refers to states explicitly, whereas our work refers to features.
Consequently, our representation is (provably) more compact and our procedures scale to larger
domains both theoretically and in practice. Furthermore, our procedure provably maintains a reference to all the possible models when data is insufficient to determine a single model, whereas the
work of Holmes and Charles Lee Isbell (2006) focuses only on the limit case of enough information for determining a single consistent model. On the down-side, our procedure does not consider
stochasticity in the belief state, and this remains an area for further development.
A similar relationship holds between our work and that of Littman, Sutton, and Singh (2002).
In that work, a model representation is given with a size linear in the number of states. This model,
predictive state representation (PSR), is based on action/observation histories and predicts behavior based on those histories. That work prefers a low-dimensional vector basis instead a featurebased representation of states (one of the traditional hallmarks of the Knowledge Representation
approach). There is no necessary correspondence between the basis vectors and some intuitive features in the real world necessarily. This enables a representation of the world that is more closely
based on behavior.
Learning PSRs (James & Singh, 2004) is nontrivial because one needs to find a good lowdimensional vector basis (the stage called discovery of tests). That stage of learning PSRs requires
matrices of size Ω(n2 ), for states spaces of size n.
Our work advances over that line of work in providing correct results in time that is polylogarithmic in the number of states. Specifically, our work learns (deterministic) transition models
in polynomial time in the state features, thus taking time O(poly(log n)).
Reinforcement Learning (RL) approaches (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996)
compute a mapping between world states and preferred actions. They are highly intractable in
partially observable domains (Kaelbling, Littman, & Cassandra, 1998), and approximation (e.g.,
Kearns, Mansour, & Ng, 2000; Meuleau, Peshkin, Kim, & Kaelbling, 1999; Even-Dar, Kakade, &
Mansour, 2005; McCallum, 1995) is practical only for small domains (e.g., 10-20 features) with
small horizon time T .
In contrast to HMMs, DBNs, and RL, our algorithms are exact and tractable in large domains (>
300 features). We take advantages of properties common to discrete domains, such as determinism,
limited effects of actions, and observed failure.
Previous work on learning deterministic action models in the AI-Planning literature assumes
fully observable deterministic domains. These learn parametrized STRIPS actions using, e.g., version spaces (Gil, 1994; Wang, 1995), general classifiers (Oates & Cohen, 1996), or hill-climbing
ILP (Benson, 1995). Recently, the work of Pasula et al. (2004) gave an algorithm that learns stochastic actions with no conditional effects. The work of Schmill, Oates, and Cohen (2000) approximates
partial observability by assuming that the world is fully observable. We can apply these to partially
observable learning problems (sometimes) by using the space of belief states instead of world states,
but this increases the problem size to exponentially, so this is not practical for our problem.
Finally, recent research on learning action models in partially observable domains includes the
works of Yang, Wu, and Jiang (2005) and of Shahaf and Amir (2006). In the works of Yang et al.
379

A MIR & C HANG

(2005), example plan traces are encoded as a weighted maximum SAT problem, from which a
candidate STRIPS action model is extracted. In general, there may be many possible action models
for any given set of example traces, and therefore the approach is by nature approximate (in contrast
to ours, which always identifies the exact set of possible action models). The work also introduces
additional approximations in the form of heuristic rules meant to rule out unlikely action models.
The work of Shahaf and Amir (2006) presents an approach for solving SLAF using logicalcircuit encodings of transition belief states. This approach performs tractable SLAF for more general deterministic models than those that we present in Section 5, but it also requires SAT solvers
for logical circuits. Such SAT solvers are not optimized nowadays in comparison with CNF SAT
solvers, so their overall performance for answering questions from SLAF is lower than ours. Most
importantly, the representation given by Shahaf and Amir (2006) grows (linearly) with the number
of time steps, and this can still hinder long sequences of actions and observations. In comparison,
our transition belief formula has bounded size that is independent of the number of time steps that
we track.
Our encoding language, LA is typical in methods for software and hardware verification and
testing. Relevant books and methods (e.g., Clarke, Grumberg, & Peled, 1999) are closely related
to this representation, and results that we achieve here are applicable there and vice versa. The
main distinction we should draw between our work and that done in Formal Methods (e.g., Model
Checking and Bounded Model Checking) is that we are able to conclude size bounds for logical formulas involved in the computation. While OBDDs are used with some success in Model Checking,
and CNF representations are used with some success in Bounded Model Checking, they have little
bounds for the sizes of formulas in theory or in practice. The conditions available in AI applications
are used in the current manuscript to deliver such bounds and yield tractability and scalability results
that are both theoretical and of practical significance.
Interestingly, methods that use Linear Temporal Logics (LTL) cannot distinguish between what
can happen and what actually happens (Calvanese, Giacomo, & Vardi, 2002). Thus, they cannot
consider what causes an occurence. Our method is similar in that it does not consider alternate
futures for a state explicitly. However, we use an extended language, namely L A , that makes those
alternatives explicit. This allows us to forego the limitations of LTL and produce the needed result.

9. Conclusions
We presented a framework for learning the effects and preconditions of deterministic actions in
partially observable domains. This approach differs from earlier methods in that it focuses on determining the exact set of consistent action models (earlier methods do not). We showed that in several
common situations this can be done exactly in time that is polynomial (sometime linear) in the number of time steps and features. We can add bias and compute an exact solution for large domains
(hundreds of features), in many cases. Furthermore, we show that the number of action-observation
traces that must be seen before convergence is polynomial in the number of features of the domain.
These positive results contrast with the difficulty encountered by many approaches to learning of
dynamic models and reinforcement learning in partially observable domains.
The results that we presented are promising for many applications, including reinforcement
learning, agents in virtual domains, and HMMs. Already, this work is being applied to autonomous
agents in adventure games, and exploration is guided by the transition belief state that we compute
380

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

and information gain criteria. In the future we plan to extend these results to stochastic domains,
and domains with continuous features.

Acknowledgments
We wish to thank Megan Nance for providing the code and samples for the sequence generator,
and also wish to thank Dafna Shahaf for an encouraging collaboration that enhanced our development and understanding of these results. The first author also wishes to acknowledge stimulating
discussion with Brian Hlubocky on related topics. We wish to acknowledge support from DAF
Air Force Research Laboratory Award FA8750-04-2-0222 (DARPA REAL program). The second
author wishes to acknowledge support from a University of Illinois at Urbana-Champaign, College
of Engineering fellowship. Finally, the first author acknowledges support from a joint Fellowship
(2007-2008) with the Center for Advanced Studies and the Beckman Institute at the University of
Illinois Urbana-Champaign.
Earlier versions of the results in this manuscript appeared in conference proceedings (Amir,
2005; Shahaf et al., 2006).

Appendix A. Our Representation and Domain Descriptions
The transition relation RM for interpretation M in LA is defined in Section 3.1 in a way that is
similar to the interpretation of Domain Descriptions. Domain Descriptions are a common method
for specifying structured deterministic domains (Fagin, Ullman, & Vardi, 1983; Lifschitz, 1990;
Pednault, 1989; Gelfond & Lifschitz, 1998). Other methods that are equivalent in our contexts
include Successor-State Axioms (Reiter, 2001) and the Fluent Calculus (Thielscher, 1998). Their
relevance and influence on our work merit a separate exposition and relationship to our work.
A domain description D is a finite set of effect rules of the form “a causes F if G” that describe the effects of actions, for F and G being state formulas (propositional combinations of fluent
names). We say that F is the head or effect and G is the precondition for each such rule. We write
“a causes F ”, for “a causes F if TRUE”. We denote by ED (a) the set of effect rules in D for action
a ∈ A. For an effect rule e, let Ge be its precondition and Fe its effect. Rule e is active in state s, if
s |= Ge (s taken here as a interpretation in P).
Every domain description D defines a unique transition relation R D (s, a, s0 ) as follows.
V
• Let F (a, s) be the conjunction of effects of rules that are active in s for a, i.e., {Fe | e ∈
ED (a), s |= Ge }. We set F (a, s) = T RU E when no rule of a is active in s.
• Let I(a, s) be the set of fluents that are not affected by a in s, i.e., I(a, s) = {f ∈ P D | ∀e ∈
ED (a) (s |= Ge ) ⇒ (f ∈
/ L(Fe ))}.
• Define (recalling that world states are sets of fluents)
¯ 0
½
¾
¯
0 ¯ (s ∩ I(a, s)) = (s ∩ I(a, s))
RD = hs, a, s i ¯
(4)
and s0 |= F (a, s)

Thus, if action a has an effect of FALSE in s, then it cannot execute in s.
This definition applies inertia (a fluent keeps its value) to all fluents that appear in no active rule.
In some contexts it is useful to specify this inertia explicitly by extra effect rules of the form “a
keeps f if G”, for a fluent f ∈ P. It is a shorthand for writing the two rules “a causes f if f ∧ G”
and “a causes ¬f if ¬f ∧ G”. When D includes all its inertia (keeps) statements, we say that D is
a complete domain description.
381

A MIR & C HANG

Example A.1 Consider the scenario of Figure 2 and assume that actions and observations occur as
in Figure 10. Actions are assumed deterministic, with no conditional effects, and their preconditions
must holds before they execute successfully. Then, every action affects every fluent either negatively,
positively, or not at all. Thus, every transition relation has a complete domain description that
includes only rules of the form “a causes l” or “a keeps l”, where l is a fluent literal (a fluent or
its negation).
Time step
Action
Location
Bulb
Switch

1

go-W

E
?
¬sw

2
¬E
¬lit
?

go-E

3
E
?
¬sw

sw-on

4
E
?
sw

go-W

5
¬E
lit
?

go-E

6
E
?
sw

Figure 10: An action-observation sequence (table entries are observations). Legend: E: east; ¬E:
west; lit: light is on; ¬lit: light is off; sw: switch is on; ¬sw: switch is off.

Consequently, every transition relation R is completely defined by some domain description D
such that (viewing a tuple as a set of its elements)

 
 

 a causes E,   a causes sw,   a causes lit, 
Y
a causes ¬E × a causes ¬sw × a causes ¬lit
R∈

 
 
9  a keeps E
8
a keeps lit
a keeps sw
> go-W >
a∈

>
<

>
=

go-E >
>
>
;
: sw-on >

Say that initially we know the effects of go-E, go-W, but do not know what sw-on does. Then,
transition filtering starts with the product set of R (of 27 possible relations) and all possible 2 3
states. Also, at time step 4 we know that the world state is exactly {E, ¬lit, ¬sw}. We try sw-on
and get that F ilter[sw-on](ρ4 ) includes the same set of transition relations but with each of those
transition relations projecting the state {E, ¬lit, ¬sw} to an appropriate choice from S. When we
receive the observations o5 = ¬E ∧ ¬sw of time step 5, ρ5 = F ilter[o5 ](F ilter[sw-on](ρ4 ))
removes from the transition belief state all the relations that gave rise to ¬E or to ¬sw. We are left
with transition relations satisfying one of the tuples in


½
¾
 sw-on causes lit 
©
ª
sw-on causes E,
sw-on causes ¬lit
× sw-on causes sw ×
sw-on keeps E


sw-on keeps lit

Finally, when we perform action go-W, again we update the set of states associated with every
transition relation in the set of pairs ρ5 . When we receive the observations of time step 6, we
conclude ρ6 = F ilter[o6 ](F ilter[go-W](ρ5 )) =
 




sw-on keeps E, 
sw-on causes E, 

+
+ *¬E  
*¬E  






 
  

   
sw-on causes sw,
sw-on causes sw,
lit
lit
(5)
,
,
,
sw-on causes lit, 
sw-on causes lit, 

  
  







sw
sw
 




go-E...
go-E...
2
382

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Appendix B. Proofs
B.1 Proof of Lemma 3.1: Consequence Finding and Existential Quantification
P ROOF
Consider some CNF form of ϕ. Suppose the clauses containing the literal x are x ∨
α1 , . . . , x ∨ αa where α1 , . . . , αa are clauses. Suppose the clauses containing the literal ¬x are
¬x ∨ β1 , . . . , ¬x ∨ βb . V
Suppose the clauses
not containing x or ¬x are γ1 , . . . , γc . Then note
V
L(ϕ)\{x}
that Cn
(ϕ) ≡ ( 1≤i≤c γi ) ∧ ( 1≤i≤a,1≤j≤b αi ∨ βj ) (the formula produced by adding
all resolvents over variable x and then removing clauses not belonging to L(L(ϕ)\{x})), since
resolution is complete for consequence finding.
Necessity. (∃x.ϕ |= CnL(ϕ)\{x} (ϕ)) Consider any model m of ∃x.ϕ. By definition, m can be
extended to L(ϕ) (i.e., by assigning some value to m(x)) such that m(ϕ) = 1. Extend m such that
this is the case. Now suppose for contradiction m is not a model of Cn L(ϕ)\{x} (ϕ). It cannot be the
case that m(γk ) = 0 for any k, because then m(ϕ) = 0, contradiction. Then, it must be the case
that m(αi ∨ βj ) = 0 for some 1 ≤ i ≤ a and 1 ≤ j ≤ b. Therefore, m(αi ) = 0 and m(βj ) = 0.
But m is a model of ϕ, so both m(x ∨ αi ) = 1 or m(¬x ∨ βj ) = 1. Thus either m(αi ) = 1 or
m(βj ) = 1, contradiction.
Sufficiency. (CnL(ϕ)\{x} (ϕ) |= ∃x.ϕ) Consider any model m of CnL(ϕ)\{x} (ϕ). Suppose for
contradiction m(∃x.ϕ) = 0. That is, if m is extended to L(ϕ), then m(ϕ) = 0. Now, extend m
to L(ϕ) such that m(x) = 0. It cannot be the case that m(γk ) = 0 for some k since m models
CnL(ϕ)\{x} (ϕ). Because m(¬x) = 1, it cannot be the case that m(¬x ∨ β j ) = 0 for any j.
Therefore m(x ∨ αi ) = 0 for some 1 ≤ i ≤ a. Therefore, m(αi ) = 0. Because m(αi ∨ βj ) = 1 for
all 1 ≤ j ≤ n, we must have that m(βj ) = 1 for all j. But now if we alter m such that m(x) = 1,
then m satisfies ϕ, contradiction. 2
B.2 Proof of Theorem 3.3
P ROOF
Both sides of the equality relation are sets of state-transition-relation pairs. We show
that the two sets have the same elements. We show first that the left-hand side of the equality is
contained in the right-hand side.
Take hs0 , Ri ∈ SLAF [a]({hs, Ri ∈ S | hs, Ri satisfies ϕ}). From Definition 2.3 there is s ∈ S
such that s ∈ {s ∈ S | hs, Ri satisfies ϕ} such that hs, a, s0 i ∈ R. In other words, there is s ∈ S
such that hs, Ri satisfies ϕ and hs, a, s0 i ∈ R.
To prove that hs0 , Ri satisfies SLAF0 [a](ϕ) we need to show that ϕ ∧ Teff (at ) has a model M
such that RM = R and s0 interprets P 0 . Let M be such that s interprets P, s0 interprets P 0 , and MR
interpreting LA as in the previous
section. This interpretation
does not
Wsatisfy this formula only if
V
V
one of the conjuncts of ϕ ∧ l∈F ,G∈G ((alG ∧ G) ⇒ l0 ) ∧ l∈F (l0 ⇒ ( G∈G (alG ∧ G))) is falsified.
This cannot be the case for ϕ by our choice of s.
Assume by contradiction that (alG ∧ G) ⇒ l0 ) fails for some l. Then, (alG ∧ G) hold in M and l0
is FALSE. The portion of M that interprets LA is built according to MR , for our R. Since hs, R, s0 i
we know that s0 satisfies l, by the construction of MR . This is a contradicts l 0 being FALSE in M
(M interprets P 0 according to s0 ), and therefore we conclude that (alG ∧ G) ⇒ l0 ) for every l0 .
W
Similarly, assume by contradiction that (l 0 ⇒ ( G∈G (alG ∧ G))) fails for some l. Then, l 0 holds
in s0 , and als fails. Again, from the way we constructed MR it must be that als in M takes the value
that corresponds to the l 0 ’s truth value in s0 . Thus, it must be that als takes the value TRUE in M ,
and we are done with the first direction.
383

A MIR & C HANG

For the opposite direction (showing the right-hand side is contained in the left-hand side), take
hs0 , Ri ∈ S that satisfies SLAF [a](ϕ). We show that
hs0 , Ri ∈ SLAF [a]({hs, Ri ∈ S | hs, Ri satisfies ϕ}).
hs0 , Ri |= SLAF [a](ϕ) implies (Corollary 3.2) that there is s ∈ S such that hs 0 , R, si |=
ϕ ∧ Teff (a) (s0 , R, s interpreting P 0 , LA , P, respectively). A similar argument to the one we give for
the first part shows that hs, a, s0 i ∈ R, and hs0 , Ri ∈ SLAF [a]({hs, Ri ∈ S | hs, Ri satisfies ϕ}).
2
B.3 Proof of Theorem 4.1: Distribution of SLAF Over Connectives
For the first part, we will show that the sets of models of SLAF [a](ϕ ∨ ψ) and SLAF [a](ϕ) ∨
SLAF [a](ψ) are identical.
Take a model M of SLAF [a](ϕ ∨ ψ). Let M 0 be a model of ϕ ∨ ψ for which SLAF [a](M 0 ) =
M . Then, M 0 is a model of ϕ or M is a model of ψ. Without loss of generalization, assume M
is a model of ϕ. Thus, M |= SLAF [a](ϕ), and it follows that M is a model of SLAF [a](ϕ) ∨
SLAF [a](ψ).
For the other direction, take M a model of SLAF [a](ϕ) ∨ SLAF [a](ψ). Then, M a model of
SLAF [a](ϕ) or M is a model of SLAF [a](ψ). Without loss of generalization assume that M is a
model of SLAF [a](ϕ). Take M 0 a model of ϕ such that M = SLAF [a](M 0 ). So, M 0 |= ϕ ∨ ψ. It
follows that M |= SLAF [a](ϕ ∨ ψ).
A similar argument follows for the ∧ case. Take a model M of SLAF [a](ϕ ∧ ψ). Let M 0 be
a model of ϕ ∧ ψ for which SLAF [a](M 0 ) = M . Then, M 0 is a model of ϕ and ψ. Thus, M |=
SLAF [a](ϕ) and M |= SLAF [a](ψ). It follows that M is a model of SLAF [a](ϕ)∧SLAF [a](ψ).
2
B.4 Proof of Theorem 4.5: Closed form for SLAF of a belief state formula
P ROOF SKETCH
We follow the characterization offered by Theorem 3.3 and Formula (1). We
take ϕt ∧at ∧Teff (a, t) and resolve out the literals of time t. This resolution is guaranteed to generate
a set of consequences that is equivalent to CnLt+1 (ϕt ∧ at ∧ Teff (a, t)).
V
Assuming ϕt ∧at , Teff (a, t) is logically equivalent to Teff (a, t)|ϕ = l∈F ,G∈G,G|=ϕ ((alG ∧Gt ) ⇒
V
lt+1 ) ∧ l∈F ,G∈G,G|=ϕ (lt+1 ⇒ (Gt ⇒ alG )). This follows from two observations. First, notice that
ϕt implies that for any G ∈ G such that G 6|= ϕ we get Gt ⇒ alG and (alG ∧ Gt ) ⇒ lt+1 (the
antecedent does notWhold, so the formula is true). Second, notice that, in
V the second part of the
original Teff (a, t), ( G∈G,G|=ϕ (alG ∧ Gt )) is equivalent (assuming ϕ) to ( G∈G,G|=ϕ (Gt ⇒ alG )).
Now, resolving out the literals of time t from ϕt ∧ at ∧ Teff (a, t)|ϕ should consider all the
resolutions of clauses (Gt is a term) of the form alG ∧ Gt ⇒ lt+1 and all the clauses of the form
lt+1 ⇒ (Gt ⇒ alG ) with each other. This yields the equivalent to
^

m
_

[ lit+1
i=1
G1 , ...,
WGm ∈ G
ϕ |= i≤m Gi
l1 , ..., lm ∈ F

m
_
¬li
∧ ¬alGi i )]
∨ (aG
i
i=1

because to eliminate all W
the literals of time t we have to resolve together sets of clauses with matching Gi ’s such that ϕ |= i≤n Gi . The formula above encodes all the resulting clauses for a chosen
384

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

li
i
set of G1 , ..., Gm and a chosen set of literals l1 , ..., lm . The reason for including (a¬l
Gi ∧ ¬aGi ) is
that we can always choose a clause with Gi , li of a specific type (either one that includes ¬alG or
one that produces a¬l
G.
Finally, we get the formula in our theorem because aFG ≡ ¬a¬F
G (G characterizes exactly one
state in S), and the fact that there is one set of G1 , ..., Gm that is stronger than all the rest (it entails
all the rest) because G1 , ..., Gm are complete terms. This set is the complete fluent assignments G i
that satisfy ϕ. 2

B.5 Proof of Theorem 5.1: Closed form when k affected fluents
P ROOF SKETCH
For literal l and clause C in the conjunction in Theorem 4.5, we aggregate all
the action propositions to a single action proposition alGl , with G the disjunction of all the complete
preconditions for l in C (notice that there cannot be l’s negation because then the C is a tautology).
Lemma B.2 shows that Gl is equivalent to a fluent term. First we prove the more restricted Lemma
B.1.
W
¬li
t+1
) be a clause of the formula in Theorem 4.5, and let Gl =
Lemma B.1 Let C = m
∨ aG
i=1 (li
i
W
{Gi | li = l, Gi |= ¬li }, for any literal l6 . Assume that a’s effect is deterministic with only
one case with a term precondition (if that case does not hold, then nothing changes). Then, G l is
equivalent to a term.
P ROOF
Gl is a disjunction of complete state terms Gi , so it represents the set of states that
corresponds to those Gi ’s. The intuition that we apply is that
W
lit+1 ∨ ¬alGi i ≡
C≡ m
W
Vm i=1
t+1
( i=1 alGi i ) ⇒ ( m
i=1 li ) ≡
0
l
a Gl ⇒ l ∨ C
for C 0 the part of C that does not affect l. The reason is that for complete terms G i we know
li
l
i
that ¬a¬l
Gi ≡ aGi . Thus, our choice of G is such that it includes all the conditions under which l
changes, if we assume the precondition alGl .
We now compute the action model of a for l by updating a copy of G l . Let li be a fluent literal,
and set Glt = Gl .
1. If there is no Gl2 ∈ Glt such that Gl2 |= li , then all of the terms in Glt include ¬li . Thus,
Glt ≡ Glt ∧ ¬li , and alGl ≡ alGl ∧¬li . Thus, add ¬li as a conjunct to Glt .
2. Otherwise, if there is no Gl2 ∈ Glt such that Gl2 |= ¬li , then all of the terms in Glt include li .
Thus, Glt ≡ Glt ∧ li , and alGl ≡ alGl ∧li . Thus, add li as a conjunct to Glt .
3. Otherwise, we know that under both li and ¬li the value of the fluent of l changes into l =
TRUE. Since we assume that the value of l changes under a single case of its preconditions
(i.e., a has no conditional effects - it either succeeds with the change, or fails with the change),
then li cannot be part of those preconditions, i.e., for every term G i in Glt we can replace li
with ¬li and vice versa, and the precondition would still entail l after the action. Thus,
alGl ≡ alGl \{l } , for Glt \ {li } the result of replacing both li and ¬li by TRUE.
t

t

i

6. Not related to the literal l in the proof above.

385

A MIR & C HANG

The result of these replacements is a term Glt that is consistent with ϕt (it is consistent with the
original Gl ) and satisfies “a has 1 case” |= alGl ⇐⇒ alGl . 2
t

W
¬li
t+1
cl =
Lemma B.2 Let C = m
∨ aG
be a clause of the formula in Theorem 4.5, and let G
i=1 li
i
W
{Gi | li = l}, for any literal l. Assume that a’s effect is deterministic with only one case with a
cl is equivalent
term precondition (if that case does not hold, then nothing changes). Then, either G
cl
to a term, or C is subsumed by another clause that is entailed from that formula such that there G
is equivalent to a term.
Gl .

cl and not in
P ROOF
Consider Gl from Lemma B.1, and let Gl1 a complete fluent term in G
l
l
l
Thus, G1 |= l. Let Gt the term that is equivalent to G according to Lemma B.1.
Clause C is equivalent to alGl ⇒ a¬l
∨ .... However, alGl already asserts change from ¬l to l
Gl
t

t

1

in the result of action a, and a¬l
asserts a change under a different condition from l to ¬l. Thus,
Gl
1

. We get a subsuming clause C 0 = C \ a¬l
. In this way we can
“a has 1 case” |= alGl ⇒ ¬a¬l
Gl
Gl
t

1

1

remove from C all the literals alGi with Gi not in Gl .

cl . However, clause C is now
After such a process we are left with a clause C that has Gl ≡ G
not of the form of Theorem 5.1 because some of the Gi ’s are missing. How do we represent that in
the theorem? We must allow some of the Gi ’s to be missing.
2
Proof of theorem continues Thus, the representation of C 0 (the new clause) takes space O(n2 )
(each propositional symbol is encoded with O(n) space, assuming constant encoding for every
fluent p ∈ P).
However, the number of propositional symbols is still large (there are O(2 n ) fluent terms, so this
encoding still requires O(2n n) (all preconditions to all effects) new propositional symbols. Now
we notice that we can limit our attention to clauses C that have at most k literals l whose action
proposition alGl satisfies |= Gl ⇒ l. This is because if we have more than k such propositions in C,
V
W
l
say {alGi l }i≤k0 , then C ≡ ( ki=1 ¬alGi l ) ⇒ aGk+1
∨ ... i≤m lit+1 , which is always subsumed by
l
i
i
k+1
V
l
.
The
latter
is
a
sentence
that is always true, if we assume that a can change
( ki=1 ¬alGi l ) ⇒ aGk+1
l
i

k+1

at most k fluents (aGk+1
asserts that lk+1 remains the same, and ¬alGi l asserts that li changes in
lk+1
i
one of the conditions satisfied by Gli ).
V
l
(which state that we have at most k effects)
Using clauses of the form ( ki=1 ¬alGi l ) ⇒ aGk+1
l
l

i

k+1

l

we can resolve away aGj l for j > k in every clause C of the original conjunction. Thus, we are
j
W
V
left with clauses of the form C ≡ ( ki=1 ¬alGi l ) ⇒ i≤m lit+1 . Now, since our choice of literals
i
other than l1 , ..., lk is independent of the rest of the clause, and we can do so for every polarity of
those fluents, we get that all of these clauses resolve (on these combinations of literals) into (and are
subsumed by)
k
_
^
lit+1
(6)
C ≡ ( ¬alGi l ) ⇒
i=1

i

386

i≤k

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Thus, we get a conjunction of clauses of the form (6), with G li (i ≤ k) being a fluent term. So,
the conjunction of clauses in Theorem 4.5 is equivalent to a conjunction of clauses such that each
clause has at most 2k literals. Thus, the space required to represent each clause is 2kn.
Finally, we use the fact that every action is dependent on at most k other fluents. Every proposition that asserts no-change in li is equivalent to a conjunction of literals stating that none of the
possible k preconditions that are consistent with it affect li . For example alli1 ∧...∧lk ∧lu implies

that alli1 ∧...∧lk ∧ alli1 ∧...∧lk−1 ∧lu ∧ .... Similarly, each one the elements in the conjunction implies

alli1 ∧...∧lk ∧lu . 2

B.6 Proof of Theorem 5.3: Equivalent Restricted Definition of Action Axioms
P ROOF
Let ϕ0 = ∃P.(ϕ∧τeff (a)). Now we claim that for any successful actionS
a, SLAF [a](ϕ) ≡
0
ϕ[P 0 /P] . To see this, consider any model of ϕ0 . The valuation to the fluents in f ∈P L0f define a
transition relation R and the valuation to the fluents in P 0 define a state s0 ∈ S such that hs0 , Ri ∈ ϕ0 .
By the definition of ϕ0 , hs0 , Ri ∈ ϕ0 if and only if there exists hs, Ri ∈ ϕ such that τeff (a) is satisfied. Finally note that τeff (a) is satisfied if and only if the preconditions of action a are met and
s0 is consistent with the effects of action a applied to s. That is, τeff (a) is satisfied if and only if
hs, a, s0 i ∈ R. Together, these observations and Corollary 3.2 yield the theorem. 2
B.7 Proof of Theorem 5.4: AS-STRIPS-SLAF is Correct
P ROOF
Let the shorthand notation C(ψ) denote C(ψ) ≡ CnL (ψ)[P 0 /P] .
By definition, SLAF [ha, oi](ϕ) ≡ SLAF [o](SLAF [a](ϕ)). From Theorem 5.3, we have
SLAF [a](ϕ) ≡ C(ϕ ∧ τeff (a)). A formula equivalent to C(ϕ ∧ τeff (a)) may be generated by
resolving
V out all fluents from P (by following the procedure from the proof of Lemma 3.1). Suppose
ϕ = f ∈P ϕf is in fluent-factored form. Then we may rewrite C(ϕ ∧ τeff (a)) as:
0



SLAF [a](ϕ) ≡ 










^

f ∈P

^

f ∈P

^

f ∈P

^

f ∈P



C(Prea,f ) ∧ C(Effa,f ) ∧ C(ϕf ) ∧

(7)



C(Prea,f ∧ Effa,f ) ∧


C(ϕf ∧ Prea,f ) ∧


C(ϕf ∧ Effa,f )

This equivalence holds because all resolvents generated by resolving out literals from P in C(ϕ ∧
τeff (a)) will still be generated in the above formula. That is, each pair of clauses that can be possibly resolved together (where a fluent from P is “resolved out”) in ϕ ∧ τ eff (a) to generate a new
consequence in C (ϕ ∧ τeff (a)) will appear together in one of the C (·) components of (7). Because
387

A MIR & C HANG

every clause in ϕ ∧ τeff (a) contains at most one literal from P, we see that all possible consequences
will be generated.
Now we note that Effa,f may be rewritten as follows:
Effa,f

≡

^

((al ∨ (af ◦ ∧ l)) ⇒ l0 ) ∧ (l0 ⇒ (al ∨ (af ◦ ∧ l)))

(8)

l∈{f,¬f }

≡

^

(l ⇒ (l0 ∨ a¬l )) ∧ (l0 ⇒ (al ∨ af ◦ ))

l∈{f,¬f }

It is straightforward to verify the equivalence of the rewritten formula to the original formula. Note
that in performing this rewriting, we may discard clauses of the form a f ∨ a¬f ∨ af ◦ , as they must
be true in every consistent model (given the axioms described in Section 5.2).
Now consider the consequences that are generated by each component of (7). We may compute
these consequences by performing resolution. We have that C(Pre a,f ) ≡ ¬a[f ] ∨ ¬a[¬f ] , but we
may discard these clauses because only inconsistent action models will violate this clause. By
the definition of fluent-factored formulas, C(ϕf ) ≡ Af . Next, the remaining components can be
computed straightforwardly:
^
C(Effa,f ) ≡
(¬l0 ∨ al ∨ af ◦ )
l∈{f,¬f }

C(ϕf ∧ Prea,f ) ≡ C(ϕf ) ∧ C(Prea,f ) ∧

^

(¬a[l] ∨ expll )

l∈{f,¬f }

C(Prea,f ∧ Effa,f ) ≡ C(Prea,f ) ∧ C(Effa,f ) ∧

^

(¬l0 ∨ al ∨ ¬a[¬l] )

l∈{f,¬f }

C(ϕf ∧ Effa,f ) ≡ C(ϕf ) ∧ C(Effa,f ) ∧

^

(¬l0 ∨ al ∨ expll )

l∈{f,¬f }

Finally, it is not difficult to see that in steps (a)-(c) the procedure sets ϕ to the following formula (in
fluent-factored form):
^
^
SLAF [a](ϕ) ≡
Af ∧
(¬a[l] ∨ expll ) ∧ (¬l0 ∨ al ∨ (af ◦ ∧ ¬a[¬l] ∧ expll )
f ∈P

l∈{f,¬f }

Now, note that SLAF [o](SLAF [a](ϕ)) ≡ SLAF [a](ϕ) ∧ o. Note that because o is a term,
then SLAF [a](ϕ) ∧ o can be made fluent-factored by performing unit resolution. This is exactly
what steps 1.(d)-(e) do. 2
B.8 Proof of Theorem 5.5: AS-STRIPS-SLAF Complexity
P ROOF
Consider the size of the formula returned by AS-STRIPS-SLAF. Overview: We note
that if a formula is in i-CNF, for any integer i > 0, then the filtered formula after one step is in
(i + 1)-CNF. Then, we note that every observation of fluent f resets the f -part of the belief state
formula to 1-CNF (thus, to i = 1).
Further Details: For the first part, this is because each call to the procedure appends at most
one literal to any existing clauses of the formula, and no new clauses of length more than k + 1 are
388

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

generated. Additionally, if every fluent is observed every at most k steps, then the transition belief
formula stays in k-CNF (i.e., indefinitely compact). This is because existing clauses may only grow
in length (1 literal per timestep) when augmented in steps 1.(a)-(c), but when the appropriate fluent
is observed in steps 1.(d)-(e), the clauses stop growing. Finally, it is easy to see that each of the
steps 1.(a)-(e) can be performed in polynomial time. 2
B.9 Proof of Theorem 5.7: PRE-STRIPS-SLAF is Correct
P ROOF
Consider the semantics of SLAF when filtering on a STRIPS action with a known precondition. In the case of an action failure, a world is in the filtered transition belief state if and only
if the world did not meet the action precondition (and satisfies the observation). Clearly, step 1 of
the algorithm performs this filtering by conjoining the belief formula with the negation of the action
precondition (converted into a logically equivalent disjunction of fluent-factored formulas).
In the case of an action success, filtering can be performed by first removing worlds which
do not satisfy the action precondition (so in all remaining worlds, the action is executable) and
then filtering the remaining worlds using algorithm AS-STRIPS-SLAF. Moreover, by Theorem 4.1
and Corollary 4.3 it follows that filtering the formula ϕ can be performed by filtering each of the
subformulas ϕi,j separately. Furthermore, SLAF[ha, oi](ϕ) |= PRE-STRIPS-SLAF[ha, oi](ϕ), and
PRE-STRIPS-SLAF[ha, oi](ϕ) ≡ SLAF[ha, oi](ϕ) if any of the conditions of Corollary 4.3. The
filtering of each subformula is performed by steps 2 and 3 of the algorithm.
Finally, note that steps 3 and 4 serve only to simplify the belief formula and produce a logically
equivalent formula. 2
B.10 Proof of Theorem 5.8: PRE-STRIPS-SLAF Complexity
P ROOF
Note that each call to AS-STRIPS-SLAF on a subformula takes time linear in the size
of the subformula, and the steps not involving AS-STRIPS-SLAF can be performed in linear time.
Thus the the total time complexity is linear. Additionally, note that if every fluent is observed every
at most k steps, then every fluent-factored subformula ϕi,j of the belief formula is in k-CNF, by a
theorem of Amir
W (2005). If action preconditions contain at most m literals, then each disjunction
of the form j ϕi,j contains at most m disjuncts. Therefore, the entire belief formula stays in
m · k-CNF, indefinitely. 2

Appendix C. Experiments And Their Outputs
Our experiments (Section 7) examine properties of our algorithms for learning action models. They
show that learning is tractable and exact. In this Appendix section we bring the generating models
and the learned models for more detailed comparison by the reader. Recall that our algorithms
output a representation for the set of models that are possible given the input. Below we bring only
one satisfying model from the learned formula.
Our experiments include the following domains from the International Planning Competition
(IPC): Drivelog, Zenotravel, Blocksworld, and Depots.
C.1 Driverlog Domain
The Driverlog domain has the following generating PDDL:
(define (domain driverlog)

389

A MIR & C HANG

(:requirements :typing)
(:types
location locatable - object
driver truck obj - locatable )
(:predicates
(at ?obj - locatable ?loc - location)
(in ?obj1 - obj ?obj - truck)
(driving ?d - driver ?v - truck)
(path ?x ?y - location)
(empty ?v - truck) )
(:action LOAD-TRUCK
:parameters
(?obj - obj
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (at ?obj ?loc))
:effect
(and (not (at ?obj ?loc)) (in ?obj ?truck)))
(:action UNLOAD-TRUCK
:parameters
(?obj - obj
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (in ?obj ?truck))
:effect
(and (not (in ?obj ?truck)) (at ?obj ?loc)))
(:action BOARD-TRUCK
:parameters
(?driver - driver
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (at ?driver ?loc) (empty ?truck))
:effect
(and (not (at ?driver ?loc)) (driving ?driver ?truck)
(not (empty ?truck))))
(:action DISEMBARK-TRUCK
:parameters
(?driver - driver
?truck - truck
?loc - location)
:precondition
(and (at ?truck ?loc) (driving ?driver ?truck))
:effect
(and (not (driving ?driver ?truck)) (at ?driver ?loc)
(empty ?truck)))
(:action DRIVE-TRUCK
:parameters
(?truck - truck
?loc-from - location
?loc-to location
?driver - driver)
:precondition
(and (at ?truck ?loc-from)
(driving ?driver ?truck)
(path ?loc-from ?loc-to))
:effect
(and (not (at ?truck ?loc-from)) (at ?truck ?loc-to)))
(:action WALK
:parameters
(?driver - driver
?loc-from - location
?loc-to - location)
:precondition
(and (at ?driver ?loc-from) (path ?loc-from ?loc-to))
:effect
(and (not (at ?driver ?loc-from)) (at ?driver ?loc-to))) )

One learned model (one possible satisfying model of our formula) from our random-sequence
input in this Driverlog domain is the following (brought together with the experimental parameters).
Driverlog domain:
* IPC3 problem 99
* 231 fluents
* 1000 randomly selected actions
* 10 fluents observed per step

390

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

* "schematized" learning
* 1:1 precondition heuristics
* Action distribution:
((BOARD-TRUCK . 52) (DRIVE-TRUCK . 86) (DISEMBARK-TRUCK . 52)
(WALK . 529) (UNLOAD-TRUCK . 139) (LOAD-TRUCK . 142))
converting to CNF
clause count: 82338
variable count: 210
adding clauses
calling zchaff
parsing result
SLAF time: 2.469
Inference time: 8.406
Learned model:
(WALK NEEDS (AT ?DRIVER ?LOC-FROM))
(WALK NEEDS (NOT (AT ?DRIVER ?LOC-TO)))
(WALK CAUSES (AT ?DRIVER ?LOC-TO))
(WALK CAUSES (NOT (AT ?DRIVER ?LOC-FROM)))
(WALK KEEPS (PATH ?LOC-FROM ?LOC-FROM))
(WALK KEEPS (PATH ?LOC-TO ?LOC-TO))
(WALK KEEPS (PATH ?LOC-TO ?LOC-FROM))
(WALK KEEPS (PATH ?LOC-FROM ?LOC-TO))
(DRIVE-TRUCK NEEDS (NOT (AT ?TRUCK ?LOC-TO)))
(DRIVE-TRUCK NEEDS (AT ?TRUCK ?LOC-FROM))
(DRIVE-TRUCK CAUSES (AT ?TRUCK ?LOC-TO))
(DRIVE-TRUCK CAUSES (NOT (AT ?TRUCK ?LOC-FROM)))
(DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-TO))
(DRIVE-TRUCK KEEPS (AT ?DRIVER ?LOC-FROM))
(DRIVE-TRUCK KEEPS (DRIVING ?DRIVER ?TRUCK))
(DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-TO))
(DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-FROM))
(DRIVE-TRUCK KEEPS (PATH ?LOC-FROM ?LOC-TO))
(DRIVE-TRUCK KEEPS (PATH ?LOC-TO ?LOC-FROM))
(DRIVE-TRUCK KEEPS (EMPTY ?TRUCK))
(DISEMBARK-TRUCK NEEDS (NOT (AT ?DRIVER ?LOC)))
(DISEMBARK-TRUCK NEEDS (DRIVING ?DRIVER ?TRUCK))
(DISEMBARK-TRUCK NEEDS (NOT (EMPTY ?TRUCK)))
(DISEMBARK-TRUCK CAUSES (AT ?DRIVER ?LOC))
(DISEMBARK-TRUCK CAUSES (NOT (DRIVING ?DRIVER ?TRUCK)))
(DISEMBARK-TRUCK CAUSES (EMPTY ?TRUCK))
(DISEMBARK-TRUCK KEEPS (AT ?TRUCK ?LOC))
(DISEMBARK-TRUCK KEEPS (PATH ?LOC ?LOC))
(BOARD-TRUCK NEEDS (AT ?DRIVER ?LOC))
(BOARD-TRUCK NEEDS (NOT (DRIVING ?DRIVER ?TRUCK)))
(BOARD-TRUCK NEEDS (EMPTY ?TRUCK))
(BOARD-TRUCK CAUSES (NOT (AT ?DRIVER ?LOC)))
(BOARD-TRUCK CAUSES (DRIVING ?DRIVER ?TRUCK))
(BOARD-TRUCK CAUSES (NOT (EMPTY ?TRUCK)))
(BOARD-TRUCK KEEPS (AT ?TRUCK ?LOC))
(BOARD-TRUCK KEEPS (PATH ?LOC ?LOC))
(UNLOAD-TRUCK NEEDS (NOT (AT ?OBJ ?LOC)))
(UNLOAD-TRUCK NEEDS (IN ?OBJ ?TRUCK))
(UNLOAD-TRUCK CAUSES (AT ?OBJ ?LOC))

391

A MIR & C HANG

(UNLOAD-TRUCK CAUSES (NOT (IN ?OBJ ?TRUCK)))
(UNLOAD-TRUCK KEEPS (AT ?TRUCK ?LOC))
(UNLOAD-TRUCK KEEPS (PATH ?LOC ?LOC))
(UNLOAD-TRUCK KEEPS (EMPTY ?TRUCK))
(LOAD-TRUCK NEEDS (AT ?OBJ ?LOC))
(LOAD-TRUCK NEEDS (NOT (IN ?OBJ ?TRUCK)))
(LOAD-TRUCK CAUSES (NOT (AT ?OBJ ?LOC)))
(LOAD-TRUCK CAUSES (IN ?OBJ ?TRUCK))
(LOAD-TRUCK KEEPS (AT ?TRUCK ?LOC))
(LOAD-TRUCK KEEPS (PATH ?LOC ?LOC))
(LOAD-TRUCK KEEPS (EMPTY ?TRUCK))

C.2 Zeno-Travel Domain
The Zeno-Travel domain has the following generating PDDL:
(define (domain zeno-travel)
(:requirements :typing)
(:types aircraft person city flevel - object)
(:predicates (at ?x - (either person aircraft) ?c - city)
(in ?p - person ?a - aircraft)
(fuel-level ?a - aircraft ?l - flevel)
(next ?l1 ?l2 - flevel))
(:action board
:parameters (?p - person ?a - aircraft ?c - city)
:precondition (and (at ?p ?c) (at ?a ?c))
:effect (and (not (at ?p ?c)) (in ?p ?a)))
(:action debark
:parameters (?p - person ?a - aircraft ?c - city)
:precondition (and (in ?p ?a) (at ?a ?c))
:effect (and (not (in ?p ?a)) (at ?p ?c)))
(:action fly
:parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 - flevel)
:precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1))
:effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))
(fuel-level ?a ?l2)))
(:action zoom
:parameters (?a - aircraft ?c1 ?c2 - city ?l1 ?l2 ?l3 - flevel)
:precondition (and (at ?a ?c1) (fuel-level ?a ?l1) (next ?l2 ?l1)
(next ?l3 ?l2) )
:effect (and (not (at ?a ?c1)) (at ?a ?c2) (not (fuel-level ?a ?l1))
(fuel-level ?a ?l3) ))
(:action refuel
:parameters (?a - aircraft ?c - city ?l - flevel ?l1 - flevel)
:precondition (and (fuel-level ?a ?l) (next ?l ?l1) (at ?a ?c))
:effect (and (fuel-level ?a ?l1) (not (fuel-level ?a ?l)))))

One learned model (one possible satisfying model of our formula) from our random-sequence
input in this Zeno-Travel domain is the following (brought together with the experimental parameters).
Zenotravel domain:

392

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

*
*
*
*
*
*
*

IPC3 problem 9
91 fluents, 21000 possible unique actions
1000 actions in learned action sequence
5 observed fluents per step
"schematized" learning
1:1 precondition heuristics
Action distribution: ((ZOOM . 27) (FLY . 216) (REFUEL . 264)
(BOARD . 249) (DEBARK . 244))

converting to CNF
clause count: 71119
variable count: 138
adding clauses
calling zchaff
parsing result
SLAF time: 1.109
Inference time: 11.015
Learned model:
(REFUEL NEEDS (FUEL-LEVEL ?A ?L))
(REFUEL NEEDS (NOT (FUEL-LEVEL ?A ?L1)))
(REFUEL CAUSES (NOT (FUEL-LEVEL ?A ?L)))
(REFUEL CAUSES (FUEL-LEVEL ?A ?L1))
(REFUEL KEEPS (NEXT ?L ?L))
(REFUEL KEEPS (NEXT ?L ?L1))
(REFUEL KEEPS (NEXT ?L1 ?L))
(REFUEL KEEPS (NEXT ?L1 ?L1))
(ZOOM NEEDS (NOT (FUEL-LEVEL ?A ?L3)))
(ZOOM NEEDS (FUEL-LEVEL ?A ?L1))
(ZOOM CAUSES (FUEL-LEVEL ?A ?L3))
(ZOOM CAUSES (NOT (FUEL-LEVEL ?A ?L1)))
(ZOOM KEEPS (FUEL-LEVEL ?A ?L2))
(ZOOM KEEPS (NEXT ?L3 ?L3))
(ZOOM KEEPS (NEXT ?L3 ?L2))
(ZOOM KEEPS (NEXT ?L3 ?L1))
(ZOOM KEEPS (NEXT ?L2 ?L3))
(ZOOM KEEPS (NEXT ?L2 ?L2))
(ZOOM KEEPS (NEXT ?L2 ?L1))
(ZOOM KEEPS (NEXT ?L1 ?L3))
(ZOOM KEEPS (NEXT ?L1 ?L2))
(ZOOM KEEPS (NEXT ?L1 ?L1))
(FLY NEEDS (NOT (FUEL-LEVEL ?A ?L2)))
(FLY NEEDS (FUEL-LEVEL ?A ?L1))
(FLY CAUSES (FUEL-LEVEL ?A ?L2))
(FLY CAUSES (NOT (FUEL-LEVEL ?A ?L1)))
(FLY KEEPS (NEXT ?L2 ?L2))
(FLY KEEPS (NEXT ?L2 ?L1))
(FLY KEEPS (NEXT ?L1 ?L2))
(FLY KEEPS (NEXT ?L1 ?L1))
(DEBARK NEEDS (IN ?P ?A))
(DEBARK CAUSES (NOT (IN ?P ?A)))
(BOARD NEEDS (NOT (IN ?P ?A)))
(BOARD CAUSES (IN ?P ?A))

393

A MIR & C HANG

C.3 Blocks-World Domain
The Blocksworld domain has the following generating PDDL:
(define (domain blocksworld)
(:requirements :strips)
(:predicates (clear ?x - object)
(on-table ?x - object)
(arm-empty)
(holding ?x - object)
(on ?x ?y - object))
(:action pickup
:parameters (?ob - object)
:precondition (and (clear ?ob) (on-table ?ob) (arm-empty))
:effect (and (holding ?ob) (not (clear ?ob)) (not (on-table ?ob))
(not (arm-empty))))
(:action putdown
:parameters (?ob - object)
:precondition (holding ?ob)
:effect (and (clear ?ob) (arm-empty) (on-table ?ob)
(not (holding ?ob))))
(:action stack
:parameters (?ob - object
?underob - object)
:precondition (and (clear ?underob) (holding ?ob))
:effect (and (arm-empty) (clear ?ob) (on ?ob ?underob)
(not (clear ?underob)) (not (holding ?ob))))
(:action unstack
:parameters (?ob - object
?underob - object)
:precondition (and (on ?ob ?underob) (clear ?ob) (arm-empty))
:effect (and (holding ?ob) (clear ?underob) (not (on ?ob ?underob))
(not (clear ?ob)) (not (arm-empty)))))

One learned model (one possible satisfying model of our formula) from our random-sequence
input in this Blocksworld domain is the following (brought together with the experimental parameters).
Blocksworld domain:
* 209 fluents
* 1000 randomly selected actions
* 10 fluents observed per step
* "schematized" learning
* 1:1 precondition heuristics
converting to CNF
clause count: 235492
variable count: 187
adding clauses
calling zchaff
parsing result
SLAF time: 2.203
Inference time: 42.312

394

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Learned model:
(UNSTACK NEEDS (NOT (CLEAR ?UNDEROB)))
(UNSTACK NEEDS (CLEAR ?OB))
(UNSTACK NEEDS (ARM-EMPTY))
(UNSTACK NEEDS (NOT (HOLDING ?OB)))
(UNSTACK NEEDS (ON ?OB ?UNDEROB))
(UNSTACK CAUSES (CLEAR ?UNDEROB))
(UNSTACK CAUSES (NOT (CLEAR ?OB)))
(UNSTACK CAUSES (NOT (ARM-EMPTY)))
(UNSTACK CAUSES (HOLDING ?OB))
(UNSTACK CAUSES (NOT (ON ?OB ?UNDEROB)))
(UNSTACK KEEPS (ON-TABLE ?UNDEROB))
(UNSTACK KEEPS (ON-TABLE ?OB))
(UNSTACK KEEPS (HOLDING ?UNDEROB))
(UNSTACK KEEPS (ON ?UNDEROB ?UNDEROB))
(UNSTACK KEEPS (ON ?OB ?OB))
(UNSTACK KEEPS (ON ?UNDEROB ?OB))
(STACK NEEDS (CLEAR ?UNDEROB))
(STACK NEEDS (NOT (CLEAR ?OB)))
(STACK NEEDS (NOT (ARM-EMPTY)))
(STACK NEEDS (HOLDING ?OB))
(STACK NEEDS (NOT (ON ?OB ?UNDEROB)))
(STACK CAUSES (NOT (CLEAR ?UNDEROB)))
(STACK CAUSES (CLEAR ?OB))
(STACK CAUSES (ARM-EMPTY))
(STACK CAUSES (NOT (HOLDING ?OB)))
(STACK CAUSES (ON ?OB ?UNDEROB))
(STACK KEEPS (ON-TABLE ?UNDEROB))
(STACK KEEPS (ON-TABLE ?OB))
(STACK KEEPS (HOLDING ?UNDEROB))
(STACK KEEPS (ON ?UNDEROB ?UNDEROB))
(STACK KEEPS (ON ?OB ?OB))
(STACK KEEPS (ON ?UNDEROB ?OB))
(PUTDOWN NEEDS (NOT (CLEAR ?OB)))
(PUTDOWN NEEDS (NOT (ON-TABLE ?OB)))
(PUTDOWN NEEDS (NOT (ARM-EMPTY)))
(PUTDOWN NEEDS (HOLDING ?OB))
(PUTDOWN CAUSES (CLEAR ?OB))
(PUTDOWN CAUSES (ON-TABLE ?OB))
(PUTDOWN CAUSES (ARM-EMPTY))
(PUTDOWN CAUSES (NOT (HOLDING ?OB)))
(PUTDOWN KEEPS (ON ?OB ?OB))
(PICKUP NEEDS (CLEAR ?OB))
(PICKUP NEEDS (ON-TABLE ?OB))
(PICKUP NEEDS (ARM-EMPTY))
(PICKUP NEEDS (NOT (HOLDING ?OB)))
(PICKUP CAUSES (NOT (CLEAR ?OB)))
(PICKUP CAUSES (NOT (ON-TABLE ?OB)))
(PICKUP CAUSES (NOT (ARM-EMPTY)))
(PICKUP CAUSES (HOLDING ?OB))
(PICKUP KEEPS (ON ?OB ?OB))

395

A MIR & C HANG

C.4 Depot Domain
The Depot domain has the following generating PDDL:
(define (domain Depot)
(:requirements :typing)
(:types place locatable - object
depot distributor - place
truck hoist surface - locatable
pallet crate - surface)
(:predicates (at ?x - locatable ?y - place)
(on ?x - crate ?y - surface)
(in ?x - crate ?y - truck)
(lifting ?x - hoist ?y - crate)
(available ?x - hoist)
(clear ?x - surface))
(:action Drive
:parameters (?x - truck ?y - place ?z - place)
:precondition (and (at ?x ?y))
:effect (and (not (at ?x ?y)) (at ?x ?z)))
(:action Lift
:parameters (?x - hoist ?y - crate ?z - surface ?p - place)
:precondition (and (at ?x ?p) (available ?x) (at ?y ?p) (on ?y ?z)
(clear ?y))
:effect (and (not (at ?y ?p)) (lifting ?x ?y) (not (clear ?y))
(not (available ?x)) (clear ?z) (not (on ?y ?z))))
(:action Drop
:parameters (?x - hoist ?y - crate ?z - surface ?p - place)
:precondition (and (at ?x ?p) (at ?z ?p) (clear ?z) (lifting ?x ?y))
:effect (and (available ?x) (not (lifting ?x ?y)) (at ?y ?p)
(not (clear ?z)) (clear ?y) (on ?y ?z)))
(:action Load
:parameters (?x - hoist ?y - crate ?z - truck ?p - place)
:precondition (and (at ?x ?p) (at ?z ?p) (lifting ?x ?y))
:effect (and (not (lifting ?x ?y)) (in ?y ?z) (available ?x)))
(:action Unload
:parameters (?x - hoist ?y - crate ?z - truck ?p - place)
:precondition (and (at ?x ?p) (at ?z ?p) (available ?x) (in ?y ?z))
:effect (and (not (in ?y ?z)) (not (available ?x)) (lifting ?x ?y))) )

One learned model (one possible satisfying model of our formula) from our random-sequence
input in this Depot domain is the following (brought together with the experimental parameters).
Depots domain:
* IPC3 problem 5
* 250 fluents
* 1000 randomly selected actions
* 10 fluents observed per step
* "schematized" learning
* 1:1 precondition heuristics
converting to CNF

396

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

clause count: 85359
variable count: 236
adding clauses
calling zchaff
parsing result
SLAF time: 2.797
Inference time: 8.062
Learned model:
(UNLOAD NEEDS (IN ?Y ?Z))
(UNLOAD NEEDS (NOT (LIFTING ?X ?Y)))
(UNLOAD NEEDS (AVAILABLE ?X))
(UNLOAD CAUSES (NOT (IN ?Y ?Z)))
(UNLOAD CAUSES (LIFTING ?X ?Y))
(UNLOAD CAUSES (NOT (AVAILABLE ?X)))
(UNLOAD KEEPS (AT ?Z ?P))
(UNLOAD KEEPS (AT ?Y ?P))
(UNLOAD KEEPS (AT ?X ?P))
(UNLOAD KEEPS (ON ?Y ?Y))
(UNLOAD KEEPS (CLEAR ?Y))
(LOAD NEEDS (NOT (IN ?Y ?Z)))
(LOAD NEEDS (LIFTING ?X ?Y))
(LOAD NEEDS (NOT (AVAILABLE ?X)))
(LOAD CAUSES (IN ?Y ?Z))
(LOAD CAUSES (NOT (LIFTING ?X ?Y)))
(LOAD CAUSES (AVAILABLE ?X))
(LOAD KEEPS (AT ?Z ?P))
(LOAD KEEPS (AT ?Y ?P))
(LOAD KEEPS (AT ?X ?P))
(LOAD KEEPS (ON ?Y ?Y))
(LOAD KEEPS (CLEAR ?Y))
(DROP NEEDS (NOT (AT ?Y ?P)))
(DROP NEEDS (NOT (ON ?Y ?Z)))
(DROP NEEDS (LIFTING ?X ?Y))
(DROP NEEDS (NOT (AVAILABLE ?X)))
(DROP NEEDS (CLEAR ?Z))
(DROP NEEDS (NOT (CLEAR ?Y)))
(DROP CAUSES (AT ?Y ?P))
(DROP CAUSES (ON ?Z ?Z))
(DROP CAUSES (NOT (ON ?Z ?Z)))
(DROP CAUSES (ON ?Z ?Y))
(DROP CAUSES (NOT (ON ?Z ?Y)))
(DROP CAUSES (ON ?Y ?Z))
(DROP CAUSES (NOT (LIFTING ?X ?Y)))
(DROP CAUSES (LIFTING ?X ?Z))
(DROP CAUSES (NOT (LIFTING ?X ?Z)))
(DROP CAUSES (AVAILABLE ?X))
(DROP CAUSES (NOT (CLEAR ?Z)))
(DROP CAUSES (CLEAR ?Y))
(DROP KEEPS (AT ?Z ?P))
(DROP KEEPS (AT ?X ?P))
(DROP KEEPS (ON ?Z ?Z))
(DROP KEEPS (ON ?Z ?Y))
(DROP KEEPS (ON ?Y ?Y))
(DROP KEEPS (LIFTING ?X ?Z))

397

A MIR & C HANG

(LIFT NEEDS (AT ?Y ?P))
(LIFT NEEDS (ON ?Y ?Z))
(LIFT NEEDS (NOT (LIFTING ?X ?Y)))
(LIFT NEEDS (AVAILABLE ?X))
(LIFT NEEDS (NOT (CLEAR ?Z)))
(LIFT NEEDS (CLEAR ?Y))
(LIFT CAUSES (NOT (AT ?Y ?P)))
(LIFT CAUSES (NOT (ON ?Y ?Z)))
(LIFT CAUSES (ON ?Z ?Z))
(LIFT CAUSES (NOT (ON ?Z ?Z)))
(LIFT CAUSES (ON ?Z ?Y))
(LIFT CAUSES (NOT (ON ?Z ?Y)))
(LIFT CAUSES (LIFTING ?X ?Y))
(LIFT CAUSES (LIFTING ?X ?Z))
(LIFT CAUSES (NOT (LIFTING ?X ?Z)))
(LIFT CAUSES (NOT (AVAILABLE ?X)))
(LIFT CAUSES (CLEAR ?Z))
(LIFT CAUSES (NOT (CLEAR ?Y)))
(LIFT KEEPS (AT ?Z ?P))
(LIFT KEEPS (AT ?X ?P))
(LIFT KEEPS (ON ?Y ?Y))
(LIFT KEEPS (ON ?Z ?Z))
(LIFT KEEPS (ON ?Z ?Y))
(LIFT KEEPS (LIFTING ?X ?Z))
(DRIVE NEEDS (AT ?X ?Y))
(DRIVE NEEDS (NOT (AT ?X ?Z)))
(DRIVE CAUSES (NOT (AT ?X ?Y)))
(DRIVE CAUSES (AT ?X ?Z))

References
Amir, E. (2005). Learning partially observable deterministic action models. In Proc. Nineteenth
International Joint Conference on Artificial Intelligence (IJCAI ’05), pp. 1433–1439. International Joint Conferences on Artificial Intelligence.
Amir, E., & Russell, S. (2003). Logical filtering. In Proc. Eighteenth International Joint Conference
on Artificial Intelligence (IJCAI ’03), pp. 75–82. Morgan Kaufmann.
Benson, S. (1995). Inductive learning of reactive action models. In Proceedings of the 12th International Conference on Machine Learning (ICML-95).
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming for first-order MDPs.
In Proc. Seventeenth International Joint Conference on Artificial Intelligence (IJCAI ’01), pp.
690–697. Morgan Kaufmann.
Boyen, X., Friedman, N., & Koller, D. (1999). Discovering the hidden structure of complex dynamic
systems. In Proceedings of the 15th Conference on Uncertainty in Artificial Intelligence—UAI
1999, pp. 91–100. Morgan Kaufmann. Available at http://www.cs.stanford.edu/ xb/uai99/.
Boyen, X., & Koller, D. (1999). Approximate learning of dynamic models. In Kearns, M. S., Solla,
S. A., & Kohn, D. A. (Eds.), Advances in Neural Information Processing Systems 11: Proceedings of the 1998 Conference—NIPS 1998, pp. 396–402. Cambridge: MIT Press. Available at http://www.cs.stanford.edu/ xb/nips98/.
398

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Calvanese, D., Giacomo, G. D., & Vardi, M. Y. (2002). Reasoning about actions and planning in
LTL action theories. In Principles of Knowledge Representation and Reasoning: Proc. Eighth
Int’l Conference (KR ’2002), pp. 593–602. Morgan Kaufmann.
Chang, A., & Amir, E. (2006). Goal achievement in partially known, partially observable domains.
In Proceedings of the 16th Int’l Conf. on Automated Planning and Scheduling (ICAPS’06).
AAAI Press.
Chang, C.-L., & Lee, R. C.-T. (1973). Symbolic Logic and Mechanical Theorem Proving. Academic
Press.
Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial Intelligence Research, 17, 229–264.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal of the
ACM, 7, 201–215.
Dawsey, W., Minsker, B., & Amir, E. (2007). Real-time assessment of drinking water systems using
Bayesian networks. In World Environmental and Water Resources Congress.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial Intelligence,
113(1–2), 41–85.
del Val, A. (1999). A new method for consequence finding and compilation in restricted language. In Proc. National Conference on Artificial Intelligence (AAAI ’99), pp. 259–264.
AAAI Press/MIT Press.
Doherty, P., Lukaszewicz, W., & Szalas, A. (1997). Computing circumscription revisited: A reduction algorithm. Journal of Automated Reasoning, 18(3), 297–336.
Eiter, T., & Gottlob, G. (1992). On the complexity of propositional knowledge base revision, updates, and counterfactuals. Artificial Intelligence, 57(2-3), 227–270.
Even-Dar, E., Kakade, S. M., & Mansour, Y. (2005). Reinforcement learning in POMDPs. In
Proc. Nineteenth International Joint Conference on Artificial Intelligence (IJCAI ’05), pp.
660–665. International Joint Conferences on Artificial Intelligence.
Fagin, R., Ullman, J. D., & Vardi, M. Y. (1983). On the semantics of updates in databases. In
Proceedings of the Second ACM SIGACT-SIGMOD Symposium on Principles of Database
Systems, pp. 352–365, Atlanta, Georgia.
Fikes, R., Hart, P., & Nilsson, N. (1972). Learning and executing generalized robot plans. Artificial
Intelligence, 3, 251–288.
Fox, M., & Long, D. (2002). PDDL2.1: An extension to PDDL for expressing temporal planning
domains. http://www.dur.ac.uk/d.p.long/IPC/pddl.html. Used in AIPS’02 competition.
Friedman, N., Murphy, K., & Russell, S. (1998). Learning the structure of dynamic probabilistic
networks. In Proc. Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI ’98).
Morgan Kaufmann.
Gelfond, M., & Lifschitz, V. (1998). Action languages. Electronic Transactions on Artificial Intelligence (http://www.etaij.org), 3, nr 16.
399

A MIR & C HANG

Ghahramani, Z. (2001). An introduction to Hidden Markov Models and Bayesian networks. International Journal of Pattern Recognition and Artificial Intelligence, 15(1), 9–42.
Ghahramani, Z., & Jordan, M. I. (1997). Factorial hidden markov models. Machine Learning, 29,
245–275.
Ghallab, M., Howe, A., Knoblock, C., McDermott, D., Ram, A., Veloso, M., Weld, D., & Wilkins,
D. (1998). PDDL – The Planning Domain Definition Language, version 1.2. Tech. rep. CVC
TR-98-003/DCS TR-1165, Yale center for computational vision and control.
Gil, Y. (1994). Learning by experimentation: Incremental refinement of incomplete planning domains. In Proceedings of the 11th International Conference on Machine Learning (ICML-94),
pp. 10–13.
Ginsberg, M. L. (1987). Readings in Nonmonotonic Reasoning, chap. 1, pp. 1–23. Morgan Kaufmann, Los Altos, CA.
Hajishirzi, H., & Amir, E. (2007). Stochastic filtering in probabilistic action models. In Proc. National Conference on Artificial Intelligence (AAAI ’07).
Hill, D. J., Minsker, B., & Amir, E. (2007). Real-time Bayesian anomaly detection for environmental
sensor data. In 32nd Congress of the International Association of Hydraulic Engineering and
Research (IAHR 2007).
Hlubocky, B., & Amir, E. (2004). Knowledge-gathering agents in adventure games. In AAAI-04
Workshop on Challenges in Game AI. AAAI Press.
Holmes, M. P., & Charles Lee Isbell, J. (2006). Looping suffix tree-based inference of partially
observable hidden state. In Proceedings of the 23rd International Conference on Machine
Learning (ICML-06), pp. 409–416. ACM Press.
Iwanuma, K., & Inoue, K. (2002). Minimal answer computation and sol. In Logics in Artificial
Intelligence: Proceedings of the Eighth European Conference, Vol. 2424 of LNAI, pp. 245–
257. Springer-Verlag.
James, M., & Singh, S. (2004). Learning and discovery of predictive state representations in dynamical systems with reset. In Proceedings of the 21st International Conference on Machine
Learning (ICML-04), pp. 417–424. ACM Press.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101, 99–134.
Kearns, M., Mansour, Y., & Ng, A. Y. (2000). Approximate planning in large POMDPs via reusable
trajectories. In Proceedings of the 12th Conference on Neural Information Processing Systems
(NIPS’98), published 1999, pp. 1001–1007. MIT Press.
Kuffner., J. J., & LaValle, S. M. (2000). Rrt-connect: An efficient approach to single-query path
planning.. In IEEE International Conference on Robotics and Automation (ICRA), pp. 995–
1001.
Lee, R. C.-T. (1967). A Completeness Theorem and a Computer Program for Finding Theorems
Derivable from Given Axioms. Ph.D. thesis, University of California, Berkeley.
Lifschitz, V. (1990). On the semantics of STRIPS. In Allen, J. F., Hendler, J., & Tate, A. (Eds.),
Readings in Planning, pp. 523–530. Morgan Kaufmann, San Mateo, California.
400

L EARNING PARTIALLY O BSERVABLE D ETERMINISTIC ACTION M ODELS

Littman, M. L. (1996). Algorithms for sequential decision making. Ph.D. thesis, Department of
Computer Science, Brown University. Technical report CS-96-09.
Littman, M. L., Sutton, R., & Singh, S. (2002). Predictive representations of state. In Proceedings of
the 15th Conference on Neural Information Processing Systems (NIPS’01), published 2002.
MIT Press.
Marquis, P. (2000). Consequence finding algorithms. In Gabbay, D., & Smets, P. (Eds.), Handbook of Defeasible Reasoning and Uncertainty Management Systems, Vol. 5: Algorithms for
defeasible and uncertain reasoning. Kluwer.
McCallum, R. A. (1995). Instance-based utile distinctions for reinforcement learning with hidden
state. In Proceedings of the 12th International Conference on Machine Learning (ICML-95).
Morgan Kaufmann.
McCarthy, J. (1986). Applications of Circumscription to Formalizing Common Sense Knowledge.
Artificial Intelligence, 28, 89–116.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint of artificial intelligence. In Meltzer, B., & Michie, D. (Eds.), Machine Intelligence 4, pp. 463–502.
Edinburgh University Press.
McIlraith, S., & Amir, E. (2001). Theorem proving with structured theories. In Proc. Seventeenth
International Joint Conference on Artificial Intelligence (IJCAI ’01), pp. 624–631. Morgan
Kaufmann.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers for
partially observable environments. In Proc. Fifteenth Conference on Uncertainty in Artificial
Intelligence (UAI ’99). Morgan Kaufmann.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering an
Efficient SAT Solver. In Proceedings of the 38th Design Automation Conference (DAC’01).
Murphy, K. (2002). Dynamic Bayesian Networks: Representation, Inference and Learning. Ph.D.
thesis, University of California at Berkeley.
Nance, M., Vogel, A., & Amir, E. (2006). Reasoning about partially observed actions. In Proc. National Conference on Artificial Intelligence (AAAI ’06). AAAI Press.
Oates, T., & Cohen, P. R. (1996). Searching for planning operators with context-dependent and
probabilistic effects. In Proc. National Conference on Artificial Intelligence (AAAI ’96), pp.
863–868. AAAI Press.
Pasula, H. M., Zettlemoyer, L. S., & Kaelbling, L. P. (2004). Learning probabilistic relational
planning rules. In Proceedings of the 14th Int’l Conf. on Automated Planning and Scheduling
(ICAPS’04). AAAI Press.
Pednault, E. P. D. (1989). ADL: exploring the middle ground between STRIPS and the situation
calculus. In Proc. First International Conference on Principles of Knowledge Representation
and Reasoning (KR ’89), pp. 324–332.
Reiter, R. (2001). Knowledge In Action: Logical Foundations for Describing and Implementing
Dynamical Systems. MIT Press.
401

A MIR & C HANG

Reiter, R. (1991). The frame problem in the situation calculus: A simple solution (sometimes) and
a completeness result for goal regression. In Lifschitz, V. (Ed.), Artificial Intelligence and
Mathematical Theory of Computation (Papers in Honor of John McCarthy), pp. 359–380.
Academic Press.
Robert, C. P., Celeux, G., & Diebolt, J. (1993). Bayesian estimation of hidden Markov chains: a
stochastic implementation. Statist. Prob. Letters, 16, 77–83.
Schmill, M. D., Oates, T., & Cohen, P. R. (2000). Learning planning operators in real-world, partially observable environments. In Proceedings of the 5th Int’l Conf. on AI Planning and
Scheduling (AIPS’00), pp. 246–253. AAAI Press.
Shahaf, D., & Amir, E. (2006). Learning partially observable action schemas. In Proc. National
Conference on Artificial Intelligence (AAAI ’06). AAAI Press.
Shahaf, D., & Amir, E. (2007). Logical circuit filtering. In Proc. Twentieth International Joint Conference on Artificial Intelligence (IJCAI ’07), pp. 2611–2618. International Joint Conferences
on Artificial Intelligence.
Shahaf, D., Chang, A., & Amir, E. (2006). Learning partially observable action models: Efficient
algorithms. In Proc. National Conference on Artificial Intelligence (AAAI ’06). AAAI Press.
Simon, L., & del Val, A. (2001). Efficient consequence-finding. In Proc. Seventeenth International
Joint Conference on Artificial Intelligence (IJCAI ’01), pp. 359–365. Morgan Kaufmann.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: an introduction. MIT Press.
Tang, D., Yinlei Yu, D. R., & Malik, S. (2004). Analysis of search based algorithms for satisfiability of quantified boolean formulas arising from circuit state space diameter problems. In
Proceedings of the Seventh International Conference on Theory and Applications of Satisfiability Testing (SAT2004).
Thielscher, M. (1998). Introduction to the fluent calculus. Electronic Transactions on Artificial
Intelligence (http://www.etaij.org), 3, nr 14.
Thrun, S. (2003). Robotic mapping: a survey. In Exploring artificial intelligence in the new millennium, pp. 1–35. Morgan Kaufmann.
Wang, X. (1995). Learning by observation and practice: an incremental approach for planning operator acquisition. In Proceedings of the 12th International Conference on Machine Learning
(ICML-95), pp. 549–557. Morgan Kaufmann.
Wu, K., Yang, Q., & Jiang, Y. (2007). Arms: an automatic knowledge engineering tool for learning
action models for ai planning. The Knowledge Engineering Review, 22(2), 135–152.
Yang, Q., Wu, K., & Jiang, Y. (2005). Learning actions models from plan examples with incomplete
knowledge.. In Biundo, S., Myers, K. L., & Rajan, K. (Eds.), ICAPS, pp. 241–250. AAAI.

402

Journal of Artificial Intelligence Research 33 (2008) 223-258

Submitted 5/08; published 10/08

Completeness and Performance of the APO Algorithm
Tal Grinshpoun
Amnon Meisels

grinshpo@cs.bgu.ac.il
am@cs.bgu.ac.il

Ben-Gurion University of the Negev
Department of Computer Science
Beer-Sheva, Israel

Abstract
Asynchronous Partial Overlay (APO) is a search algorithm that uses cooperative mediation to solve Distributed Constraint Satisfaction Problems (DisCSPs). The algorithm
partitions the search into different subproblems of the DisCSP. The original proof of completeness of the APO algorithm is based on the growth of the size of the subproblems. The
present paper demonstrates that this expected growth of subproblems does not occur in
some situations, leading to a termination problem of the algorithm. The problematic parts
in the APO algorithm that interfere with its completeness are identified and necessary
modifications to the algorithm that fix these problematic parts are given. The resulting
version of the algorithm, Complete Asynchronous Partial Overlay (CompAPO), ensures
its completeness. Formal proofs for the soundness and completeness of CompAPO are
given. A detailed performance evaluation of CompAPO comparing it to other DisCSP
algorithms is presented, along with an extensive experimental evaluation of the algorithm’s
unique behavior. Additionally, an optimization version of the algorithm, CompOptAPO,
is presented, discussed, and evaluated.

1. Introduction
Algorithms that solve Distributed Constraint Satisfaction Problems (DisCSPs) attempt to
achieve concurrency during problem solving in order to utilize the distributed nature of these
problems. Distributed backtracking, which forms the majority of DisCSP algorithms, can
take many forms. Asynchronous Backtracking (ABT) (Yokoo, Durfee, Ishida, & Kuwabara,
1998; Yokoo & Hirayama, 2000), Asynchronous Forward-Checking (AFC) (Meisels & Zivan, 2007), and Concurrent Dynamic Backtracking (ConcDB) (Zivan & Meisels, 2006a) are
representative examples of the family of distributed backtracking algorithms. All of these
algorithms maintain one or more partial solutions of the DisCSP and attempt to extend the
partial solution into a complete one. The ABT algorithm attempts to achieve concurrency
by asynchronously assigning values to the variables. The AFC algorithm performs value
assignments synchronously, but achieves its concurrency by performing asynchronous computation in the form of forward checking. The ConcDB algorithm concurrently attempts
to extend multiple partial solutions, scanning different parts of the search space.
A completely different approach to achieve concurrency can be by the merging of partial
solutions into a complete one. The inherent concurrency of merging partial solutions makes
it a fascinating paradigm for solving DisCSPs. However, such an approach is prone to many
errors – deadlocks could prevent termination, and failures could occur in the attempt to
merge all of the partial solutions. Consequently, it is hard to develop such an algorithm
c
°2008
AI Access Foundation. All rights reserved.

Grinshpoun & Meisels

that is both correct and well performing. A recently published algorithm, Asynchronous
Partial Overlay (APO) (Mailler, 2004; Mailler & Lesser, 2006), attempts to solve DisCSPs
by merging partial solutions. It uses the concept of mediation to centralize the search
procedure in different parts of the DisCSP. Due to its unique approach, several researchers
have already proposed changes and modifications to the APO algorithm (Benisch & Sadeh,
2006; Semnani & Zamanifar, 2007). Unfortunately, none of these studies has examined the
completeness of APO. Additionally, the distinctive behavior of the APO algorithm calls for
a thorough experimental evaluation. The present paper presents an in-depth investigation
of the completeness and termination of the APO algorithm, constructs a correct version of
the algorithm – CompAPO – and goes on to present extensive experimental evaluation of
the complete APO algorithm.
The APO algorithm partitions the agents into groups that attempt to find consistent
partial solutions. The partition mechanism is dynamic during search and enables a dynamic
change of groups. The key factor in the termination (and consequently the completeness) of
the APO algorithm as presented in the original correctness proof (Mailler & Lesser, 2006)
is the monotonic growth of initially partitioned groups during search. This growth arises
because the subproblems overlap, allowing agents to increase the size of the subproblems
they solve. We have discovered that this expected growth of groups does not occur in
some situations, leading to a termination problem of the APO algorithm. Nevertheless, the
unique way in which APO attempts to solve DisCSPs has encouraged us to try and fix it.
The termination problem of the APO algorithm is shown in section 4 by constructing a scenario that leads to an infinite loop of the algorithm’s run (Grinshpoun, Zazon,
Binshtok, & Meisels, 2007). Such a running example is essential to the understanding of
APO’s completeness problem, since the algorithm is very complex. To help understand
the problem, a full pseudo-code of APO that follows closely the original presentation of
the algorithm (Mailler & Lesser, 2006) is given. The erroneous part in the proof of APO’s
completeness as presented by Mailler and Lesser (2006) is shown and the problematic parts
in the algorithm that interfere with its completeness are identified. Necessary modifications
to the algorithm are proposed, in order to fix these problematic parts. The resulting version
of the algorithm ensures its completeness, and is termed Complete Asynchronous Partial
Overlay (CompAPO) (Grinshpoun & Meisels, 2007). Formal proofs for the soundness and
completeness of CompAPO are presented.
The modifications of CompAPO may potentially affect the performance of the algorithm.
Also, in the evaluation of the original APO algorithm (Mailler & Lesser, 2006), it was compared to the AWC algorithm (Yokoo, 1995), which is not an efficient DisCSP solver (Zivan,
Zazone, & Meisels, 2007). Moreover, the tests in the work of Mailler and Lesser (2006)
were only performed on relatively sparse problems, and the comparison with AWC was
made by the use of some problematic measures. An extensive experimental evaluation of
CompAPO compares its performance with other DisCSP search algorithms on randomly
generated DisCSPs. Our experiments show that CompAPO performs significantly different than other DisCSP algorithms, which is not surprising considering its singular way of
problem solving.
Asynchronous Partial Overlay is actually a family of algorithms. The completeness and
termination problems that are presented and corrected in the present study apply to all the
members of the family. The OptAPO algorithm (Mailler & Lesser, 2004; Mailler, 2004) is
224

Completeness and Performance of the APO Algorithm

an optimization version of APO that solves Distributed Constraint Optimization Problems
(DisCOPs). The present paper proposes similar modifications to those of APO in order
to achieve completeness for OptAPO. The resulting CompOptAPO algorithm is evaluated
extensively on randomly generated DisCOPs.
The plan of the paper is as follows. DisCSPs are presented briefly in section 2. Section 3, gives a short description of the APO algorithm along with its pseudo-code. An
infinite loop scenario for APO is described in detail in section 4 and the problems that
lead to the infinite looping are analyzed in section 5. Section 6 presents a detailed solution
to the problem that forms the CompAPO version of the algorithm, followed by proofs for
the soundness and completeness of CompAPO (section 7). An optimization version of the
algorithm, CompOptAPO, is presented and discussed in section 8. An extensive performance evaluation of CompAPO and CompOptAPO is in section 9. Our conclusions are
summarized in section 10.

2. Distributed Constraint Satisfaction
A distributed constraints satisfaction problem – DisCSP, is composed of a set of k agents
A1 , A2 , ..., Ak . Each agent Ai contains a set of constrained variables xi1 , xi2 , ..., xini . Constraints or relations R are subsets of the Cartesian product of the domains of the constrained variables. For a set of constrained variables xik , xjl , ..., xmn , with domains of values
for each variable Dik , Djl , ..., Dmn , the constraint is defined as R ⊆ Dik × Djl × ... × Dmn .
A binary constraint Rij between any two variables xj and xi is a subset of the Cartesian
product of their domains – Rij ⊆ Dj × Di . In a distributed constraint satisfaction problem
(DisCSP), the agents are connected by constraints between variables that belong to different agents (Yokoo et al., 1998). In addition, each agent has a set of constrained variables,
i.e. a local constraint network.
An assignment (or a label) is a pair < var, val >, where var is a variable of some agent
and val is a value from var ‘s domain that is assigned to it. A compound label is a set of
assignments of values to a set of variables. A solution s to a DisCSP is a compound label
that includes all variables of all agents, which satisfies all the constraints. Agents check
assignments of values against non-local constraints by communicating with other agents
through sending and receiving messages.
Current studies of DisCSPs follow the assumption that all agents hold exactly one variable (Yokoo & Hirayama, 2000; Bessiere, Maestre, Brito, & Meseguer, 2005). Accordingly,
the present study often uses the variable’s name xi to represent the agent it belongs to (Ai ).
In addition, the following common assumptions are used in the present study:
• The amount of time that passes between the sending and the receiving of a message
is finite.
• Messages sent by agent Ai to agent Aj are received by Aj in the order they were sent.

3. Asynchronous Partial Overlay
Asynchronous Partial Overlay (APO) is an algorithm for solving DisCSPs that applies
cooperative mediation. The pseudo-code in Algorithms 1, 2, and 3 follows closely the
presentation of APO in the work of Mailler and Lesser (2006).
225

Grinshpoun & Meisels

Algorithm 1 APO procedures for initialization and local resolution.
procedure initialize
1: di ← random d ∈ Di ;
2: pi ← sizeof (neighbors) + 1;
3: mi ← true;
4: mediate ← false;
5: add xi to the good list;
6: send (init, (xi , pi , di , mi , Di , Ci )) to neighbors;
7: init list ← neighbors;
when received (init, (xj , pj , dj , mj , Dj , Cj )) do
1: add (xj , pj , dj , mj , Dj , Cj ) to agent view ;
2: if xj is a neighbor of some xk ∈ good list do
3:
add xj to the good list;
4:
add all xl ∈ agent view ∧ xl ∈
/ good list that can now be connected to the good list;
5:
pi ← sizeof (good list);
6: if xj ∈
/ init list do
7:
send (init, (xi , pi , di , mi , Di , Ci )) to xj ;
8: else
9:
remove xj from init list;
10: check agent view;
when received (ok?, (xj , pj , dj , mj )) do
1: update agent view with (xj , pj , dj , mj );
2: check agent view;
procedure check agent view
1: if init list 6= ∅ or mediate 6= false do
2:
return;
3: m′i ← hasConf lict(xi );
4: if m′i and ¬∃j(pj > pi ∧ mj == true) do
5:
if ∃(d′i ∈ Di )(d′i ∪agent view does not conflict) and di conflicts exclusively with lower
priority neighbors do
6:
di ← d′i ;
7:
send (ok?, (xi , pi , di , mi )) to all xj ∈ agent view;
8:
else
9:
mediate;
10: else if mi 6= m′i do
11:
mi ← m′i ;
12:
send (ok?, (xi , pi , di , mi )) to all xj ∈ agent view;

226

Completeness and Performance of the APO Algorithm

Algorithm 2 Procedures for mediating an APO session and for choosing a solution during
an APO mediation.
procedure mediate
1: pref erences ← ∅;
2: counter ← 0;
3: for each xj ∈ good list do
4:
send (evaluate?, (xi , pi )) to xj ;
5:
counter ← counter + 1;
6: mediate ← true;
when received (wait!, (xj , pj )) do
1: update agent view with (xj , pj );
2: counter ← counter − 1;
3: if counter == 0 do choose solution;
when received (evaluate!, (xj , pj , labeled Dj )) do
1: record (xj , labeled Dj ) in preferences;
2: update agent view with (xj , pj );
3: counter ← counter − 1;
4: if counter == 0 do choose solution;
procedure choose solution
1: select a solution s using a Branch and Bound search that:
2:
1. satisfies the constraints between agents in the good list
3:
2. minimizes the violations for agents outside of the session
4: if ¬∃s that satisfies the constraints do
5:
broadcast no solution;
6: for each xj ∈ agent view do
7:
if xj ∈ pref erences do
/ agent view do
8:
if d′j ∈ s violates an xk and xk ∈
9:
send (init, (xi , pi , di , mi , Di , Ci )) to xk ;
10:
add xk to init list;
11:
send (accept!, (d′j , xi , pi , di , mi )) to xj ;
12:
update agent view for xj ;
13:
else
14:
send (ok?, (xi , pi , di , mi )) to xj ;
15: mediate ← false;
16: check agent view;

227

Grinshpoun & Meisels

Algorithm 3 Procedures for receiving an APO session.
when received (evaluate?, (xj , pj )) do
1: mj ← true;
2: if mediate == true or ∃k(pk > pj ∧ mk == true) do
3:
send (wait!, (xi , pi )) to xj ;
4: else
5:
mediate ← true;
6:
label each d ∈ Di with the names of the agents that would be violated by setting
di ← d;
7:
send (evaluate!, (xi , pi , labeled Di )) to xj ;
when received (accept!, (d, xj , pj , dj , mj )) do
1: di ← d;
2: mediate ← false;
3: send (ok?, (xi , pi , di , mi )) to all xj ∈ agent view;
4: update agent view with (xj , pj , dj , mj );
5: check agent view;

At the beginning of its problem solving, the APO algorithm performs an initialization
phase, in which neighboring agents exchange data through init messages (procedure initialize in Algorithm 1). Following that, agents check their agent view to identify conflicts
between themselves and their neighbors (procedure check agent view in Algorithm 1). If
during this check, an agent finds a conflict with one of its neighbors, it expresses desire to
act as a mediator. In case the agent does not have any neighbors that wish to mediate and
have a wider view of the constraint graph than itself, the agent successfully assumes the
role of mediator.
Using mediation (Algorithms 2, and 3), agents can solve subproblems of the DisCSP
by conducting an internal Branch and Bound search (procedure choose solution in Algorithm 2). For a complete solution of the DisCSP, the solutions of the subproblems must be
compatible. When solutions of overlapping subproblems have conflicts, the solving agents
increase the size of the subproblems that they work on. The original paper (Mailler &
Lesser, 2006) uses the term preferences to describe potential conflicts between solutions
of overlapping subproblems. In the present paper we use the term external constraints to
describe such conflicts. A detailed description of the APO algorithm can be found in the
work of Mailler and Lesser (2006).

4. An Infinite Loop Scenario
Consider the 3-coloring problem presented in Figure 1 by the solid lines. Each agent can
assign one of the three available colors Red, Green, or Blue. To the standard inequality
constraints that the solid lines represent, four weaker constraints (diagonal dashed lines)
are added. The dashed lines represent constraints that do not allow only the combinations
(Green,Green) and (Blue,Blue) to be assigned by the agents. Ties in the priorities of agents
are broken using an anti-lexicographic ordering of their names.
228

Completeness and Performance of the APO Algorithm

A1
Red

A2
Red

A8
Green

A3
Blue

A7
Blue

A4
Green

A6
Red

A5
Red

Figure 1: The constraints graph with the initial assignments.
Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
R
R
B
G
R
R
B
G

mi
m1
m2
m3
m4
m5
m6
m7
m8

=t
=t
=f
=f
=t
=t
=f
=f

dj
d2
d1
d1
d3
d3
d5
d1
d1

values
=R, d3
=R, d3
=R, d2
=B, d5
=B, d4
=R, d7
=R, d5
=R, d7

=B, d7 =B, d8 =G
=B
=R, d4 =G, d5 =R
=R
=G, d6 =R, d7 =B
=B
=R, d6 =R, d8 =G
=B

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= t, m3 = f , m7 = f , m8 = f
= t, m3 = f
= t, m2 = t, m4 = f , m5 = t
= f , m5 = t
= f , m4 = f , m6 = t, m7 = f
= t, m7 = f
= t, m5 = t, m6 = t, m8 = f
= t, m7 = f

Table 1: Configuration 1.
The initial selection of values by all agents is depicted in Figure 1. In the initial state,
two constraints are violated – (A1 , A2 ) and (A5 , A6 ). Assume that agents A3 , A4 , A7 , and
A8 are the first to complete their initialization phase by exchanging init messages with all
their neighbors (procedure initialize in Algorithm 1). These agents do not have conflicts,
therefore they set mi ←false and send ok? messages to their neighbors when each of them
runs the check agent view procedure (Algorithm 1). Only after the arrival of the ok?
messages from agents A3 , A4 , A7 , and A8 , do agents A1 , A2 , A5 , and A6 accept the last init
messages from their other neighbors and complete the initialization phase. Agents A2 and
A6 have conflicts, but they complete the check agent view procedure without mediating
or changing their state. This is true, because in the agent views of A2 and A6 , m1 = true
and m5 = true, respectively. These neighbors have higher priority than agents A2 and A6
respectively. We denote by configuration 1 the states of all the agents at this point of the
processing and present the configuration in Table 1.
After all agents complete their initializations, agents A1 and A5 detect that they have
conflicts, and that they have no neighbor with a higher priority that wants to mediate.
Consequently, agents A1 and A5 start mediation sessions, since they cannot change their
own color to a consistent state with their neighbors.
229

Grinshpoun & Meisels

Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
G
B
R
G
R
R
B
R

mi
m1
m2
m3
m4
m5
m6
m7
m8

=f
=f
=f
=f
=t
=t
=f
=f

dj values
d2 =B, d3 =R, d7 =B, d8 =R
d1 =G, d3 =R
d1 =G, d2 =B, d4 =G, d5 =R
d 3 =B, d5 =R
d 3 =B, d4 =G, d6 =R, d7 =B
d5 =R, d7 =B
d1 =G, d5 =R, d6 =R, d 8 =G
d1 =G, d7 =B

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= f , m3 = f , m7 = f , m8 = f
= f , m3 = f
= f , m2 = f , m4 = f , m5 = t
= f , m5 = t
= f , m4 = f , m6 = t, m7 = f
= t, m7 = f
= f , m5 = t, m6 = t, m8 = f
= f , m7 = f

Table 2: Configuration 2 – obsolete data in agent views is in bold face.
Let us first observe A1 ’s mediation session. A1 sends evaluate? messages to its neighbors A2 , A3 , A7 , and A8 (procedure mediate in Algorithm 2). All these agents reply with
evaluate! messages (Algorithm 3). A1 conducts a Branch and Bound search to find a
solution that satisfies all the constraints between A1 , A2 , A3 , A7 , and A8 , and also minimizes external constraints (procedure choose solution in Algorithm 2). In our example,
A1 finds the solution (A1 ←Green, A2 ←Blue, A3 ←Red, A7 ←Blue, A8 ←Red), which
satisfies the internal constraints, and minimizes to zero the external constraints. A1 sends
accept! messages to its neighbors, informing them of its solution. A2 , A3 , A7 , and A8
receive the accept! messages and send ok? messages with their new states to their neighbors (Algorithm 3). However, the ok? messages from A8 to A7 and from A3 to A4 and to
A5 are delayed. Observe that the algorithm is asynchronous and naturally deals with such
scenarios.
Concurrently with the above mediation session of A1 , agent A5 starts its own mediation
session. A5 sends evaluate? messages to its neighbors A3 , A4 , A6 , and A7 . Let us assume
that the message to A7 is delayed. A4 and A6 receive the evaluate? messages and reply
with evaluate!, since they do not know any agents of higher priority than A5 that want
to mediate. A3 , is in A1 ’s mediation session, so it replies with wait!. We denote by
configuration 2 the states of all the agents at this point of the processing (see Table 2).
Only after A1 ’s mediation session is over, A7 receives the delayed evaluate? message
from A5 . Since A7 is no longer in a mediation session, nor does it expect a mediation session
from a node of higher priority than A5 (see A7 ’s view in Table 2), agent A7 replies with
evaluate!. Notice that A7 ’s view of d8 is obsolete (the ok? message from A8 to A7 is still
delayed). When agent A5 receives the evaluate! message from A7 , it can continue the
mediation session involving agents A4 , A5 , A6 , and A7 . Since the ok? messages from A3
to A4 and A5 are also delayed, agent A5 starts its mediation session with knowledge about
agents A3 and A8 that is not updated (see bold-faced data in Table 2).
Agent A5 conducts a Branch and Bound search to find a solution that satisfies all the
constraints between A4 , A5 , A6 , and A7 , that also minimizes external constraints. In our
example, A5 finds the solution (A4 ←Red, A5 ←Green, A6 ←Blue, A7 ←Red), which
satisfies the internal constraints, and minimizes to zero the external constraints (remember
that A5 has wrong data about the assignments of A3 and A8 ). A5 sends accept! messages
to A4 , A6 , and A7 , informing them of its solution. The agents receive these messages and
send ok? messages with their new states to their neighbors. By now, all the delayed
230

Completeness and Performance of the APO Algorithm

A1
Green

A2
Blue

A8
Red

A3
Red

A7
Red

A4
Red

A6
Blue

A5
Green

Figure 2: The graph in configuration 3.
Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
G
B
R
R
G
B
R
R

mi
m1
m2
m3
m4
m5
m6
m7
m8

=f
=f
=t
=t
=f
=f
=t
=t

dj
d2
d1
d1
d3
d3
d5
d1
d1

values
=B, d3 =R, d7 =R, d8 =R
=G, d3 =R
=G, d2 =B, d4 =R, d5 =G
=R, d5 =G
=R, d4 =R, d6 =B, d7 =R
=G, d7 =R
=G, d5 =G, d6 =B, d8 =R
=G, d7 =R

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= f , m3 = t, m7 = t, m8 = t
= f , m3 = t
= f , m2 = f , m4 = t, m5 = f
= t, m5 = f
= t, m4 = t, m6 = f , m7 = t
= f , m7 = t
= f , m5 = f , m6 = f , m8 = t
= f , m7 = t

Table 3: Configuration 3.
messages get to their destinations, and two constraints are violated – (A3 ,A4 ) and (A7 ,A8 ).
Consequently, agents A3 , A4 , A7 , and A8 want to mediate, whereas agents A1 , A2 , A5 , and
A6 do not wish to mediate, since they do not have any conflicts. We denote by configuration
3 the states of all the agents after A5 ’s solution has been assigned and all delayed messages
arrived at their destinations (see Figure 2 and Table 3).
Until now, we have shown a series of steps that led from configuration 1 to configuration
3. A careful look at Figures 1 and 2 reveals that these configurations are actually isomorphic.
Consequently, we will next show a very similar series of steps that will lead us right back
to configuration 1.
Agents A3 and A7 detect that they have conflicts and that they have no neighbor with
a higher priority that wants to mediate. Consequently, agents A3 and A7 start mediation
sessions, since they cannot change their own color to a consistent state with their neighbors.
We will first observe A3 ’s mediation session. A3 sends evaluate? messages to its
neighbors A1 , A2 , A4 , and A5 . All these agents reply with evaluate! messages. A3
conducts a Branch and Bound search to find a solution that satisfies all the constraints
between A1 , A2 , A3 , A4 , and A5 , and also minimizes external constraints. Agent A3 finds
the solution (A1 ←Green, A2 ←Red, A3 ←Blue, A4 ←Green, A5 ←Red), which satisfies
231

Grinshpoun & Meisels

Agent
A1
A2
A3
A4
A5
A6
A7
A8

Color
G
R
B
G
R
B
B
R

mi
m1
m2
m3
m4
m5
m6
m7
m8

=f
=f
=f
=f
=f
=f
=t
=t

dj values
d 2 =B, d3 =B, d7 =R, d8 =R
d1 =G, d3 =B
d1 =G, d2 =R, d4 =G, d5 =R
d3 =B, d5 =R
d3 =B, d4 =G, d6 =B, d7 =R
d 5 =G, d7 =R
d1 =G, d 5 =G, d6 =B, d8 =R
d1 =G, d7 =R

mj
m2
m1
m1
m3
m3
m5
m1
m1

values
= f , m3
= f , m3
= f , m2
= f , m5
= f , m4
= f , m7
= f , m5
= f , m7

= f,
=f
= f,
=f
= f,
=t
= f,
=t

m7 = t, m8 = t
m4 = f , m5 = f
m6 = f , m7 = t
m6 = f , m8 = t

Table 4: Configuration 4 – obsolete data in agent views is in bold face.

the internal constraints, and minimizes to zero the external constraints. A3 sends accept!
messages to its neighbors, informing them of its solution. A1 , A2 , A4 , and A5 receive
the accept! messages and send ok? messages with their new states to their neighbors.
However, the ok? messages from A2 to A1 and from A5 to A6 and to A7 are delayed.
Concurrently with the above mediation session of A3 , agent A7 starts its own mediation
session. A7 sends evaluate? messages to its neighbors A1 , A5 , A6 , and A8 . Let us assume
that the message to A1 is delayed. A6 and A8 receive the evaluate? messages and reply
with evaluate!, since they do not know any agents of higher priority than A7 that want
to mediate. A5 , is in A3 ’s mediation session, so it replies with wait!. We denote by
configuration 4 the states of all the agents at this point of the processing (see Table 4).
Only after A3 ’s mediation session is over, A1 receives the delayed evaluate? message
from A7 . Since A1 is no longer in a mediation session, nor does it expect a mediation session
from a node of higher priority than A7 (see A1 ’s view in Table 4), agent A1 replies with
evaluate!. Notice that A1 ’s view of d2 is obsolete (the ok? message from A2 to A1 is still
delayed). When agent A7 receives the evaluate! message from A1 , it can continue the
mediation session involving agents A1 , A6 , A7 , and A8 . Since the ok? messages from A5
to A6 and A7 are also delayed, agent A7 starts its mediation session with knowledge about
agents A2 and A5 that is not updated (see bold-faced data in Table 4).
Agent A7 conducts a Branch and Bound search to find a solution that satisfies all the
constraints between A1 , A6 , A7 , and A8 , that also minimizes external constraints. In our
example, A7 finds the solution (A1 ←Red, A6 ←Red, A7 ←Blue, A8 ←Green), which
satisfies the internal constraints, and minimizes to zero the external constraints (remember
that A7 has wrong data about A2 and A5 ). A7 sends accept! messages to A1 , A6 , and A8 ,
informing them of its solution. The agents receive these messages and send ok? messages
with their new states to their neighbors. By now, all the delayed messages get to their
destination, and two constraints are violated – (A1 ,A2 ) and (A5 ,A6 ). Consequently, agents
A1 , A2 , A5 , and A6 want to mediate, whereas agents A3 , A4 , A7 , and A8 do not wish to
mediate, since they do not have any conflicts. Notice that all the agents have returned to
the exact states they were in configuration 1 (see Figure 1 and Table 1).
The cycle that we have just shown between configuration 1 and configuration 3 can
continue indefinitely. This example contradicts the termination and completeness of the
APO algorithm.
232

Completeness and Performance of the APO Algorithm

It should be noted that we did not mention all the messages passed in the running of our
example. We mentioned only those messages that are important for the understanding of the
example, since the example is complicated enough. For instance, after agent A1 completes
its mediation session (before configuration 2 ), there is some straightforward exchange of
messages between agents, before the mj values of all the agents become correct (as presented
in Table 2).

5. Analyzing the Problems
In the previous section a termination problem of the APO algorithm was described by
constructing a scenario that leads to an infinite loop of the algorithm’s run. To better
understand the completeness problem of APO, one must refer to the completeness proof
of the APO algorithm as given by Mailler and Lesser (2006). The proof is based on the
incorrect assertion that when a mediation session terminates it has three possible outcomes:
1. A solution with no external conflicts.
2. No solution exists.
3. A solution with at least one external violated constraint.
In the first case, the mediator presumably finds a solution to the subproblem. In the
second case, the mediator discovers that the overall problem is unsolvable. In the third
case, the mediator adds the agent (or agents) with whom the external conflicts were found
to its good list, which is used to define future mediations. In this way, either a solution
or no solution is found (first two cases), or the good list grows, consequently bringing the
problem solving closer to a centralized solution (third case).
However, the infinite loop scenario in section 4 shows that the assertion claiming that
these three cases cover all the possible outcomes of a mediation session is incorrect. There
are two possible reasons for this incorrectness. The first reason is the possibility that a
mediator initiates a partial mediation session without obtaining a lock on all the agents in
its good list. The second reason is incorrect information about external constraints when
neighboring mediation sessions are performed concurrently. Both reasons relate to the
concurrency of mediation sessions.
5.1 Partial Mediation Sessions
The first reason for the incorrectness of the ”always growth” assertion is the possibility
that a mediator initiates a partial mediation session without obtaining a lock on all the
agents in its good list. This possibility can occur because of earlier engagements of some of
its good list’s members with other mediation sessions. In APO’s code, these agents send a
wait! message.
Let us consider some partial mediation session. Assume that the mediator finds a
solution to the subproblem, but such that has external conflicts with agents outside the
mediation session. Assume also that all these conflicts are with agents that are already in
the mediator’s good list. Notice that this is possible, since these agents can be engaged in
other mediation sessions and have earlier sent wait! messages to the mediator. The present
mediation session falls into case 3 of the original proof. However, it is apparent that no new
agents will be added to the good list – contradicting the assertion.
233

Grinshpoun & Meisels

Another possible outcome of partial mediation sessions is a situation in which an agent
or several agents that have the entire graph in their good list try to mediate, but fail to
get a lock on all the agents in their good list. Consequently, the situation in which a single
agent holds the entire constraint graph, does not necessarily lead to a solution, due to an
oscillation.
5.2 Neighboring Mediation Sessions
The second reason for the incorrectness of the assertion in the original proof (Mailler &
Lesser, 2006) is the potential existence of obsolete information of external constraints. This
reason involves a scenario in which two neighboring mediation sessions are performed concurrently. Both the mediation sessions in the scenario end with finding a solution that
presumably has no external conflicts, but the combination of both solutions causes new
conflicts. This was the case with the mediation sessions of agents A1 and A5 in the example
of section 4. Such a scenario seemingly fits the first case in the assertion, in which no
external conflicts are found by each of the mediation sessions. Consequently, no externalconflict-free partial solution is found – contradicting the assertion. Furthermore, none of
the mediators increase their good list. This enables the occurrence of an infinite loop, as
displayed in section 4.

6. Complete Asynchronous Partial Overlay
A two-part solution that solves the completeness problem of APO is presented. The first
part of the solution insures that the first reason for the incorrectness of the assertion (see
section 5.1) could not occur. This is achieved by preventing partial mediation sessions that
go on without the participation of the entire mediator’s good list. The second part of the
solution addresses the scenario in which two neighboring mediation sessions are performed
concurrently. In such a scenario, the results of the mediation sessions can create new
conflicts. In order to ensure that good lists grow and rule out an infinite loop, the second
part of the solution makes sure that at least one of the good lists grows. Combined with
the first part that insures that mediation sessions will involve the entire good lists of the
mediators, the completeness of APO is secured.
6.1 Preventing Partial Mediation Sessions
Our proposed algorithm disables the initiation of partial mediation sessions by making the
mediator wait until it obtains a lock on all the agents in its good list. Algorithm 4 presents
the changes and additions to APO that are needed for preventing partial mediation sessions.
When the mediator receives a wait! message from at least one of the agents in its
good list, it simply cancels the mediation session (wait!, line 2) and sets the counter to a
special value of -1 (wait!, line 3). To notify the other participants of the canceled mediation
session, the mediator sends a cancel! message to each of the participants (wait!, line 4).
Upon receiving a cancel! message, the receiving agent updates its agent view (cancel!,
line 1) and frees itself from the mediator’s lock (cancel!, line 2). However, the agent is still
234

Completeness and Performance of the APO Algorithm

Algorithm 4 Preventing partial mediation sessions.
when received (wait!, (xj , pj )) do
1: update agent view with (xj , pj );
2: mediate ← false;
3: counter ← −1;
4: send (cancel!, (xi , pi )) to all xj ∈ good list;
5: check agent view;
when received (evaluate!, (xj , pj , labeledDj )) do
1: update agent view with (xj , pj );
2: if counter 6= −1 do
3:
record (xj , labeledDj ) in preferences;
4:
counter ← counter − 1;
5:
if counter = 0 do choose solution;
when received (cancel!, (xj , pj )) do
1: update agent view with (xj , pj );
2: mediate ← false;
3: check agent view;

aware of the mediator’s willingness to mediate. Consequently, it will not join a mediation
session of a lower priority agent. The special value of counter is used by the mediator to
disregard evaluate! messages that arrive after a wait! message (that causes a cancellation)
due to asynchronous message passing (evaluate!, line 2).
The cancellation of a mediation session upon receiving a single wait! message introduces
a need for a unique identification for mediation sessions. Consider a wait! message that a
mediator receives. Upon receiving the message, the mediator cancels the mediation session
and calls check agent view. It may decide to initiate a new mediation session. However,
it might receive a wait! message from another agent corresponding to the previous, already
cancelled, mediation session. Consequently, the new mediation session would be mistakenly
cancelled too. To prevent the occurrence of such a problem, a unique id has to be added to
each mediation session. This way, a mediator could disregard obsolete wait! and evaluate!
messages. The unique identification of mediation sessions is removed from the pseudo-code
in order to keep it as simple as possible.
This approach may imply some kind of a live-lock, where repeatedly no agent succeeds at
initiating a mediation sessions. However, such a live-lock cannot occur due to the priorities
of the agents. Consider agent xp that has the highest priority among all the agents that
wish to mediate. In case agent xp obtains a lock on all the agents in its good list, it can
initiate a mediation session and there is no live-lock. The interesting situation is when agent
xp fails to get a lock on all the agents in its good list (receives at least one wait! message).
Even in this case agent xp will eventually succeed at initiating a mediation session, since
all the agents in its good list are aware of its willingness to mediate. The agents that are
at the moment locked by other mediators (both initiated mediation sessions and mediation
235

Grinshpoun & Meisels

sessions that are to be canceled) will eventually be freed by either cancel! or accept!
messages. Since these agents are all aware of agent xp ’s willingness to mediate, they will
not join any mediation session other than agent xp ’s (unless xp informs them that it no
longer wishes to mediate). Consequently, agent xp will eventually obtain a lock on all the
agents in its good list – contradicting the implied live-lock.
6.2 Neighboring Mediation Sessions
Sequential and concurrent neighboring mediation sessions may result in new conflicts being
created without any of the good lists growing. Such mediation sessions may lead to an
infinite loop as depicted in section 4. A7 in configuration 2 is an example of an agent that
participates in sequential neighboring mediation sessions (of the mediators A1 and A5 ). On
the other hand, A3 in configuration 2 is an example of an agent whose neighbors have an
incorrect view of, due to concurrent mediation sessions.
A solution to the problem of subsequent neighboring mediation sessions could be obtained if an agent (for example, A7 in configuration 2 ) would agree to participate in a new
mediation session only when its agent view is updated with all the changes of the previous
mediation session. This is achieved by the mediator sending its entire solution s in the
accept! messages, instead of just specific d′j ’s. Therefore, the sending of accept! messages
(choose solution, line 11) in Algorithm 2 is changed to the following:
send (accept!, (s, xi , pi , di , mi )) to xj ;
Upon receiving the revised accept! message (Algorithm 5), agent i now updates all the
dk ’s in the received solution s accept for dk ’s that are not in i’s agent view (accept!, lines
1-3). Notice that agent i still has to send ok? messages to its neighbors (accept!, line 7),
since not all of its neighbors were necessarily involved in the mediation session.
A solution to the problem of concurrent neighboring mediation sessions could be obtained if the mediator is informed post factum that a new conflict has been created due to
concurrent mediation sessions. In this manner, the mediator can add the new conflicting
agent to its good list. Algorithm 5 presents the changes and additions to APO that are
needed for handling concurrent neighboring mediation sessions.
When an agent xi participating in a mediation session receives the accept! message
from its mediator, it keeps a list of all its neighbors (in the constraint graph) that are not
included in the accept! message (not part of the mediation session), each associated with
the mediator (accept!, lines 4-5). The list is named conc list, since it contains agents that
are potentially involved in concurrent mediation sessions.
Upon receiving an ok? message from an agent xj belonging to the conc list (ok?, line
2), agent xi checks if the data from the received ok? message generates a new conflict
with xj (ok?, line 3). If no new conflict was generated, agent xj is just removed from the
conc list (ok?, line 6). However, in case a conflict was generated (ok?, lines 3-5), agent xi
perceives that agent xj and itself have been involved in concurrent mediation sessions that
created new conflicts. In this case, agent xi ’s mediator should add agent xj to its agent view
and good list. Hence, agent xi sends a new add! message to the mediator (associated with
agent xj in the conc list). When the mediator receives the add! message it adds agent xj
to its agent view and its good list (add!, lines 1-2).
236

Completeness and Performance of the APO Algorithm

Algorithm 5 Handling neighboring mediation sessions.
when received (accept!, (s, xj , pj , dj , mj )) do
1: for each xk ∈ agent view (starting with xi ) do
2:
if xk ∈ s do
3:
update agent view with (xk , dk );
4:
else if di does not generate a conflict with the existing dk do
5:
add (xk , xj ) to conc list;
6: mediate ← false;
7: send (ok?, (xi , pi , di , mi )) to all xj ∈ agent view;
8: update agent view with (xj , pj , dj , mj );
9: check agent view;
when received (ok?, (xj , pj , dj , mj )) do
1: update agent view with (xj , pj , dj , mj );
2: if xj ∈ conc list do
3:
if dj generates a conflict with di do
4:
for each tuple (xj , xk ) in conc list do
5:
send (add!, (xj )) to xk ;
6:
remove all tuples (xj , xk ) from conc list;
7: check agent view;
when received (add!, (xj )) do
1: send (init, (xi , pi , di , mi , Di , Ci )) to xj ;
2: add xj to init list;

There is a slight problem with this solution, since it may push the problem solving
process to become centralized. This may happen because an ok? message from agent xj
that generates a new conflict may actually have been the result of a later mediation session
that agent xj was involved in. In such a case, xj ’s mediator already added agent xi to
its good list. Adding agent xj to the good list of xi ’s mediator is not necessary for the
completeness of the algorithm. It does lead to a faster convergence of the problem into a
centralized one. Nevertheless, experiments show that the effect of such growth of good lists
is negligible (see section 9.3).
6.3 Preventing Busy-Waiting
To insure that partial mediation sessions do not occur, a wait! message received by a mediator (Algorithm 4) causes it to cancel the mediation session (section 6.1). The cancellation
of the session is immediately followed by a call to check agent view (wait!, line 5). Such
a call will most likely result in an additional attempt by the agent to start a mediation
session, due to the high probability that the agent’s view did not change since its previous
mediation attempt. The reasons that failed the previous mediation attempt may very well
cause the new mediation session not to succeed also. Such subsequent mediation attempts
may occur several times before the mediation session succeeds or the mediator decides to
237

Grinshpoun & Meisels

stop its attempts. As a matter of fact, the mediator remains in a busy-waiting mode, until
either its view changes, or the reasons for the mediation session’s failure are no longer valid.
The latter case enables the mediation session to take place.
Such a state of busy-waiting adds unnecessary overhead to the computation load of the
problem solving. In particular, it increases the number of sent messages. To prevent this
overhead, the mediating agent xm has to work in an interrupt-based manner rather than a
busy-waiting manner. In an interrupt-based approach the mediator is notified (interrupted)
when the reason for the previous mediation session’s failure is no longer valid. This is done
by an ok? message that is sent to the mediator by the agent xw that sent the preceding
wait! message, which caused the mediation session to fail. The agent xw will send such
an ok? message only when the reason that caused it to send the wait! message becomes
obsolete. Namely, when one of the following occurs:
• The mediation session that xw was involved in is over.
• An agent with a higher priority than xm no longer wants to mediate.
• The init list of xw has been emptied out.
In order to remember which agents have to be notified (interrupted) when one of the
above instances occurs, an agent maintains a list of pending mediators called wait list. Each
time an agent sends a wait! message to a mediator, it adds that mediator to its wait list.
Whenever an agent sends ok? messages, it clears its wait list.
A few changes to the pseudo-code must be applied in order to use the interrupt-based
method. To maintain the wait list, the following line has to be added to Algorithm 3 after
line 3 in evaluate? (inside the if statement):
add xj to wait list;
Also, after sending ok? messages to the entire agent view, as done for example in procedure
check agent view line 7 (Algorithm 1), the following line should be added:
empty wait list;
Finally, there is need to interrupt pending mediators whenever the reason for their mediation
session’s failure may be no longer valid. For example, when an agent is removed from the
init list (init, line 9) in Algorithm 1, the following lines need to be added (inside the else
statement):
if init list == ∅ do
send (ok?, (xi , pi , di , mi )) to all xw ∈ wait list;
empty wait list;
These lines handle the case when the init list has been emptied out. Similar additions must
be applied to deal with the other mentioned cases. Applying this interrupt-based method
rules out the need for busy-waiting. Thus, the call for check agent view (wait!, line 5)
can be discarded.
238

Completeness and Performance of the APO Algorithm

7. Soundness and Completeness
In this section we will show that CompAPO is both sound and complete. Our proofs
follow the basic structure and assumptions of the original APO proofs (Mailler & Lesser,
2006). The original completeness proof was incorrect because of the incompleteness of
the original algorithm. Consequently, we will not use the assertion that was discussed
in detail in section 3 and that played a key role in the original (and incorrect) proof of
completeness (Mailler & Lesser, 2006). The following lemmas are needed for the proofs of
soundness and completeness.
Lemma 1 Links are bidirectional. i.e. if xi has xj in its agent view then eventually xj will
have xi in its agent view.
Proof (as appears in the work of Mailler and Lesser, 2006):
Assume that xi has xj in its agent view and that xi is not in the agent view of xj . In
order for xi to have xj in its agent view, xi must have received an init message at some
point from xj . There are two cases.
Case 1: xj is in the init list of xi . In this case, xi must have sent xj an init message
first, meaning that xj received an init message and therefore has xi in its agent view – a
contradiction.
Case 2: xj is not in the init list of xi . In this case, when xi receives the init message
from xj , it responds with an init message. That means that if the reliable communication
assumption holds, eventually xj will receive xi ’s init message and add xi to its agent view
– also a contradiction.
Definition 1 An agent is considered to be in a stable state if it is waiting for messages,
but no message will ever reach it.
Definition 2 A deadlock is a state in which an agent that has conflicts and desires to
mediate enters a stable state.
Lemma 2 A deadlock cannot occur in the CompAPO algorithm.
Proof:
Assume that agent xi enters a deadlock. This means that agent xi desires to mediate,
but is in a stable state. The consequence of this is that agent xi would not be able to get
a lock on all the agents in its good list.
One possibility is that xi already invited the members in its good list to join its mediation session by sending evaluate? messages. After a finite time it will receive either
evaluate! or wait! messages from all the agents in its good list. Depending on the replies,
xi either initiates a mediation session or cancels it. Either way, xi is not in a stable state –
contradicting the assumption.
The other possibility is that xi did not reach the stage in which it invites other agents
to join its mediation session. This can only happen, if there exists at least one agent xj that
in xi ’s point of view both desires to mediate (m′j = true) and has a higher priority than xi
(p′j > pi ). There are two cases in which xj would not mediate a session that included xi ,
when xi was expecting it to:
239

Grinshpoun & Meisels

Case 1: xi has m′j = true in its agent view when the actual value should be false.
Assume that xi has m′j = true in its agent view when actually mj = false. This would
mean that at some point xj changed the value of mj to false without informing xi about
it. There is only one place in which xj changes the value of mj – the check agent view
procedure. Note that in this procedure, whenever the flag changes from true to false,
the agent sends ok? messages to all the agents in its agent view. Since by Lemma 1 we
know that xi is in the agent view of xj , agent xi must have eventually received the message
informing it that mj = false, contradicting the assumption.
Case 2: xj believes that xi should be mediating when xi does not believe it should be.
In xj ’s point of view, m′i = true and p′i > pj . By the previous case, we know that if xj
believes that mi is true (m′i = true) then this must be the case. We only need to show
that the condition p′i > pj is impossible. Assume that xj believes that p′i > pj when in fact
pi < pj . This means that at some point xi sent a message to xj informing it that its current
priority was p′i . Since we know that priorities only increase over time (all the good lists can
only get larger), we know that p′i ≤ pi (xj always has the correct value or an underestimate
of pi ). Since pi < pj and p′i ≤ pi then p′i < pj – a contradiction to the assumption.
Definition 3 The algorithm is considered to be in a stable state when all the agents are in
a stable state.
Theorem 1 The CompAPO algorithm is sound. i.e., it reaches a stable state only if it has
either found an answer or no solution exists.
Proof:
We assume that all the agents reach a stable state, and consider all the cases in which
this can happen.
Case 1: No agent has conflicts. In this case, all the agents are in a stable state and
with no conflicts. This means that the current value that each agent has for its variable
satisfies all its constraints. Consequently, the current values are a valid solution to the
overall problem, and the CompAPO algorithm has found an answer.
Case 2: A no solution message has been broadcast. In this case, at least one agent
found out that some subproblem has no solution, and informed all the agents about it by
broadcasting a no solution message. Consequently, each agent that receives this message
(all the agents) stops its run and reports that no solution exists.
Case 3: Some agents have conflicts. Let us consider some agent xi that has a conflict.
Since it has a conflict, xi desires to mediate. If it is able to perform a mediation session then
it is not in a stable state in contradiction to the assumption. Therefore, the only condition
in which xi can remain in a stable state is if it is expecting a mediation request from a
higher priority agent xj that does not send it – in other words, when it is deadlocked. By
Lemma 2 this cannot happen.
Since only cases 1 and 2 can occur, the CompAPO algorithm reaches a stable state
only if it has either found an answer or no solution exists. Consequently, the CompAPO
algorithm is sound. ¤

240

Completeness and Performance of the APO Algorithm

Lemma 3 If there exist agents that hold the entire graph in their good list and desire to
mediate, then one of these agents will perform a mediation session.
Proof:
We shall consider two cases – when there is only one such agent that holds the entire
graph in its good list and desires to mediate, and when there are several such agents.
Case 1: Consider agent xi to be the only agent that holds the entire graph in its
good list and desires to mediate. Since xi has the entire graph in its good list it has the
highest possible priority. Moreover, all of the agents are aware of xi ’s priority (pi ) and desire
to mediate (mi ) due to ok? messages they received from xi containing this information
(xi sent ok? messages to all the agents in its agent view, which holds the entire graph).
Consequently, no agent will engage from this point on, in any mediation session other than
xi ’s. Since all mediation sessions are finite and no new mediation sessions will occur, agent
xi will eventually get a lock on all the agents and will perform a mediation session.
Case 2: If several such agents exist, then the tie in the priorities is broken by the
agents’ index. Consider xi to be the one with the highest index out of these agents, and
apply the same proof of case 1.
Lemma 4 If an agent holding the entire graph in its good list performs a mediation session,
the algorithm reaches a stable state.
Proof:
Consider the mediator to be agent xi . Following the first part of CompAPO’s solution
(section 6.1), an agent can perform a mediation session only if it received evaluate! messages from all the agents in its good list. Since xi holds the entire graph in its good list, it
means that all the agents in the graph have sent evaluate! messages to xi and set their
mediate flags to be true. This means that until xi completes its search and returns accept! messages with its solution, no agent can change its assignment. Assuming that the
centralized internal solver that xi uses is sound and complete, it will find a solution to the
entire problem if such a solution exists, or alternatively conclude that no solution exists.
If no solution exists, then xi informs all the agents about this and the problem solving
terminates. Otherwise, each agent receives the accept! message from xi that contains the
solution to the entire problem. Consequently, no agent has any conflicts and the algorithm
reaches a stable state.
Lemma 5 Infinite value changes without any mediation sessions cannot occur.
Proof:
The proof will focus on line 6 of the check agent view procedure, the only place in
the code in which a value is changed without a mediation session. As a reminder, notice
that all the agents in the graph are ordered by their priority (ties are broken by the IDs of
the agents).
Consider the agent with the lowest priority (xp1 ). Agent xp1 cannot change its own
value, since line 5 in the check agent view procedure states that in order to reach the
value change in line 6, the current value must conflict exclusively with lower priority agents.
Clearly this is impossible for agent xp1 , which has the lowest priority in the graph.
241

Grinshpoun & Meisels

Now, consider the next agent in the ordering, xp2 . Agent xp2 can change its current
value when it is in conflict exclusively with lower priority agents. The only lower priority
agent in this case is xp1 . If xp1 and xp2 are not neighbors, then agent xp2 cannot change
its own value for the same reason as agent xp1 . Otherwise, agent xp2 will know the upto-date value of agent xp1 in finite time (any previously sent updates regarding xp1 ’s value
will eventually reach agent xp2 ), since we proved that the value of xp1 cannot be changed
without a mediation session. After xp2 has the up-to-date value of all its lower priority
neighbors (only xp1 ), it can change its own value at most once without a mediation session.
Eventually, all the neighbors of xp2 will be updated with the final change of its value.
In general, any agent xpi (including the highest priority agent) will in finite time have
the up-to-date values of all its lower priority agents. When this happens, it can change its
own value at most once without a mediation session. Eventually, all the neighbors of xpi
will be updated with the final change of its value. Thus, infinite value changes without any
mediation sessions cannot occur.
This proof implicitly relies on the fact that the ordering of agents does not change.
However, the priority of an agent may change in time. Nevertheless, the priorities are
bounded by the size of the graph, so the number of priority changes is finite. This proves
that value changes cannot indefinitely occur without any mediation sessions.
Lemma 6 If from this point on no agent will mediate or desire to mediate, the algorithm
will reach a stable state.
Proof:
We shall consider two cases – when there are no messages that have not yet arrived to
their destinations, and when there are such messages.
Case 1: Consider the case when there are no messages that have not yet arrived to their
destination. If no agent desires to mediate, then all the mi ’s are false, meaning that no
agent in the graph has conflicts. Consequently, the current state of the graph is a solution
that satisfies all the constraints, and the algorithm reaches a stable state.
Case 2: Consider the case when there are some messages that have not yet arrived to
their destinations. Eventually these messages will arrive. According to the assumption of
the lemma, the arrival of these messages will not make any of the agents desire to mediate.
Next, we consider the arrival of each type of message and show that it cannot lead to infinite
exchange of messages:
• evaluate?, evaluate!, wait!, cancel!: These messages must belong to an obsolete
mediation session, or otherwise contradict the assumption of the lemma. Accordingly,
they may result in some limited exchange of messages (e.g., sending wait! in line 3
of evaluate?). Some of these messages may lead to a call to the check agent view
procedure.
• accept!: This message cannot be received without contradicting the assumption,
since the receiving agent has to be in an active mediation session when receiving an
accept! message.
• init: This message is part of a handshake between two agents. Consequently, at
most a single additional init message will be sent. This leads to a call to the
check agent view procedure by each of the involved agents.
242

Completeness and Performance of the APO Algorithm

• add!: This message results in the sending of a single init message.
• ok?: This message may result in the sending of a finite number of add! messages. It
also leads to a call to the check agent view procedure.
By examining all the types of messages, we conclude that each message can at most lead
to a finite exchange of messages, and to a finite number of calls to the check agent view
procedure. We only need to show that a call to check agent view cannot lead to infinite
exchange of messages.
The check agent view procedure has 4 possible outcomes. It may simply return (line
2), change the value of the variable (lines 6-7), mediate (line 9), or update its desire to mediate (lines 11-12). According to the assumption of the lemma, it cannot mediate. An update
of its desire to mediate, means that the value of mi was true, or will be updated to true.
Either way, this is again in contradiction to the assumption of the lemma. Consequently,
the only possibilities are a simple return, or a change in the value of its own variable. According to Lemma 5, such value changes cannot indefinitely occur without any mediation
sessions. Consequently, the final messages will eventually arrive to their destinations, and
the first case of the proof will hold.
Definition 4 One says that the algorithm advances if at least one of the good lists grows.
Lemma 7 After every n mediation sessions, the algorithm either advances or reaches a
stable state.
Proof:
Consider a mediation session of agent xi . The mediation session has three possible
outcomes – no solution satisfying the constraints within the good list, a solution satisfying
the constraints within the good list but with violations of external constraints, and a solution
satisfying all the constraints within the good list and all the external constraints.
Case 1: No solution that satisfies the constraints within xi ’s good list exists, therefore
the entire problem is unsatisfiable. In this case, xi informs all the agents about this and
the problem solving terminates.
Case 2: xi finds a solution that satisfies the constraints within its good list but violates
external constraints. In this case, xi adds the agents with whom there are external conflicts
to its good list. These agents were not already in xi ’s good list, since the mediation session
included the entire good list of xi (according to section 6.1). Consequently, xi ’s good list
grows and the algorithm advances.
Case 3: xi finds a solution that satisfies the constraints within its good list and all the
external constraints. Following the second part of CompAPO’s solution (section 6.2), agents
from xi ’s good list maintain a conc list, and would notify xi to add agents to its good list
in case they experience new conflicts due to concurrent mediation sessions. In such a case,
xi would be notified, its good list would grow and the algorithm would advance.
The only situation in which the algorithm does not advance or reach a stable state, is
when all the mediation sessions experience case 3, and no concurrent mediation sessions
create new conflicts. In that case, after at most n mediation sessions (equal to the overall
number of agents), all the agents would have no desire to mediate. According to Lemma 6,
the algorithm reaches a stable state.
243

Grinshpoun & Meisels

Lemma 8 If there exists a group of agents that desire to mediate, a mediation session will
eventually occur.
Proof:
No agent will manage to get a lock on all the agents in its good list (essential for a
mediation session to occur) only if all the agents in the group that sent evaluate? messages
got at least one wait! message each. If this is the case, consider xi to be the highest priority
agent among this group.
Each wait! message that agent xi received is either from an agent that is a member
of the group or from an agent outside the group, currently involved in another mediation
session. In case this agent (xj ) belongs to the group, xj also got some wait! message
(clearly, this wait! message arrived after xj sent wait! to xi ). xj will therefore cancel its
mediation session, and will wait for xi ’s next evaluate? message (since xj is now aware of
xi ’s desire to mediate and pi is the highest priority among the agents that currently desire to
mediate). In case xj does not belong to the group, the mediation session that xj is involved
in will eventually terminate, and xi will get the lock, unless xj has a higher priority than
xi (pj > pi ) and also xj desires to mediate when the session terminates. If this is the case,
xj will eventually get the lock for the same reasons.
Theorem 2 The CompAPO algorithm is complete. i.e., if a solution exists, the algorithm
will find it, and if a solution does not exist, the algorithm will report that fact.
Proof:
Since we have shown in Theorem 1 that whenever the algorithm reaches a stable state,
the problem is solved and that when it finds a subset of variables that is unsatisfiable it
terminates, we only need to show that it always reaches one of these two states in a finite
time.
According to Lemma 6, if from some point in time no agent will mediate or desire
to mediate, the algorithm will reach a stable state. According to Lemma 8 if there exist
agents that desire to mediate, eventually a mediation session will occur. From Lemmas 6
and 8 we conclude that the only possibility for the algorithm not to reach a stable state
is by continuous occurrences of mediation sessions. According to Lemma 7, after every n
mediation sessions, the algorithm either advances or reaches a stable state. Consequently,
the algorithm either reaches a stable state or continuously advances.
In case the algorithm continuously advances, the good lists continuously grow. At some
point, some agents (eventually all the agents) will hold the entire graph in their good list.
One of these agents will eventually desire to mediate (if not, then according to Lemma 6, the
algorithm reaches a stable state). According to Lemma 3, one of these agents will perform
a mediation session. According to Lemma 4, the algorithm reaches a stable state. ¤

8. OptAPO – an Optimizing APO
Distributed Constraint Optimization Problems (DisCOPs) are a version of distributed constraint problems, in which the goal is to find an optimal solution to the problem, rather
than a satisfying one. In an optimization problem, an agent associates a cost with violated
constraints and maintains bounds on these costs in order to reach an optimal solution that
minimizes the number of violated constraints.
244

Completeness and Performance of the APO Algorithm

A number of algorithms were proposed in the last few years for solving DisCOPs. The
simplest algorithm of these is Synchronous Branch and Bound (SyncBB) (Hirayama &
Yokoo, 1997), which is a distributed version of the well-known centralized Branch and
Bound algorithm. Another algorithm which uses a Branch and Bound scheme is Asynchronous Forward Bounding (AFB) (Gershman, Meisels, & Zivan, 2006), in which agents
perform sequential assignments which are propagated for bounds checking and early detection of a need to backtrack. A number of algorithms use a pseudo-tree which is derived
from the structure of the DisCOP in order to improve the process of acquiring a solution
for the optimization problem. ADOPT (Modi, Shen, Tambe, & Yokoo, 2005) is such an
asynchronous algorithm in which assignments are passed down the pseudo-tree. Agents
compute upper and lower bounds for possible assignments and send costs up to their parents in the pseudo-tree. These costs are eventually accumulated by the root agent. Another
algorithm which exploits a pseudo tree is DPOP (Petcu & Faltings, 2005). In DPOP, each
agent receives from the agents which are its sons in the pseudo-tree, all the combinations of
partial solutions in their sub-tree and their corresponding costs. The agent calculates and
generates all the possible partial solutions which include the partial solutions it received
from its sons and its own assignments and sends the resulting combinations up the pseudotree. Once the root agent receives all the information from its sons, it produces the optimal
solution and propagates it down the pseudo-tree to the rest of the agents.
Another very different approach was implemented in the Optimal Asynchronous Partial Mediation (OptAPO) (Mailler & Lesser, 2004; Mailler, 2004) algorithm, which is an
optimization version of the APO algorithm. Differently to APO, the OptAPO algorithm
introduces a second type of mediation sessions called passive mediation sessions. The goal
of the passive sessions is to update the bounds on the costs without changing the values of
variables. These sessions add parallelism to the algorithm and accelerate the distribution of
information. This might solve many problems that result from incorrect information, which
is discussed in section 5.2. However, active mediation sessions also occur in OptAPO. The
active sessions may consist of parts of the good list (partial mediation sessions), and as a
result lead to the problems described in section 5.1. Moreover, a satisfiable problem should
also be solved by OptAPO, returning a zero optimal cost. Therefore, the infinite loop scenario described in section 4 will also occur in OptAPO, which behaves like APO when the
problem is satisfiable.
The OptAPO algorithm must be corrected in order for the aforementioned problems to
be solved. In section 6 several modifications to the APO algorithm are proposed. These
changes turn APO into a complete search algorithm – CompAPO. Equivalent modifications
must also be applied to the OptAPO algorithm in order to ensure its correctness. Interestingly, these modifications to APO are to procedures that are similar in APO and OptAPO.
The main differences between APO and OptAPO are in the addition of passive mediation sessions (procedure check agent view) to OptAPO, and in the internal search that
mediators perform (procedure choose solution). However, neither of these procedures is
effected by the modifications of CompAPO. Thus, the pseudo-code of the changes that must
be applied to OptAPO is very similar to the modifications of CompAPO, and is therefore
omitted from this paper. The performance of the resulting algorithm – CompOptAPO –
is evaluated in section 9.5. The full pseudo-code of the original OptAPO algorithm can be
found in the work of Mailler and Lesser (2004).
245

Grinshpoun & Meisels

9. Experimental Evaluation
The original (and incomplete) version of the APO algorithm was evaluated by Mailler and
Lesser (2006). It was compared to the AWC algorithm (Yokoo, 1995), which is not an
efficient DisCSP solver (Zivan et al., 2007). The experiments were performed on 3-coloring
problems, which are a subclass of uniform random constraints problems. These problems are
characterized by a small domain size, low constraints density, and fixed constraints tightness
(for the characterization of random CSPs see the works of Prosser, 1996 and Smith, 1996).
The comparison between APO and AWC (Mailler & Lesser, 2006) was made with respect to
three measures – the number of sent messages, the number of cycles, and the serial runtime.
While the number of sent messages is a very important and widely accepted measure, the
other measures are problematic. During a cycle, incoming messages are delivered, the
agent is allowed to process the information, and any messages that were created during
the processing are added to the outgoing queue to be delivered at the beginning of the
next cycle. The meaning of such a cycle in APO is that a mediation session that possibly
involves the entire graph takes just a single cycle. Such a measure is clearly problematic,
since every centralized algorithm solves a problem in just one cycle. Measuring the serial
runtime is also not adequate for distributed CSPs, since it does not take into account any
concurrent computations during the problem solving. In order to measure the concurrent
runtime of DisCSP algorithms in an implementation independent way, one needs to count
non-concurrent constraint checks (NCCCs) (Meisels, Razgon, Kaplansky, & Zivan, 2002).
This measure has gained global agreement in the DisCSP and DisCOP community (Bessiere
et al., 2005; Zivan & Meisels, 2006b) and will be used in the present evaluation.
The modifications of CompAPO, and especially the prevention of partial mediation
sessions (section 6.1) add synchronization to the algorithm, which may tax heavily the
performance of the algorithm. Thus, it is important to evaluate the effect of these changes by
comparing CompAPO to other (incomplete) versions of the APO algorithm. Additionally,
we evaluate the effectiveness of the interrupt-based method compared to busy-waiting.
9.1 Experimental Setup
In all our experiments we use a simulator in which agents are simulated by threads, which do
not hold any shared memory and communicate only through message passing. The network
of constraints, in each of our experiments, is generated randomly by selecting the probability
p1 of a constraint among any pair of variables and the probability p2 , for the occurrence
of a violation among two assignments of values to a constrained pair of variables. Such
uniform random constraints networks of n variables, k values in each domain, a constraints
density of p1 and tightness p2 are commonly used in experimental evaluations of CSP
algorithms (Prosser, 1996; Smith, 1996).
Experiments were conducted for several density values. Our setup included problems
generated with 15 agents (n = 15) and 10 values (k = 10). We drew 100 different instances
for each combination of p1 and p2 . Through all our experiments each agent holds a single
variable.
246

Completeness and Performance of the APO Algorithm

Figure 3: Mean NCCCs in sparse problems (p1 = 0.1).

Figure 4: Mean NCCCs in medium density problems (p1 = 0.4).
9.2 Comparison to Other Algorithms
The performance of CompAPO is compared to three asynchronous search algorithms – the
well known Asynchronous Backtracking (ABT) (Yokoo et al., 1998; Yokoo & Hirayama,
2000), the extremely efficient Asynchronous Forward-Checking with Backjumping (AFCCBJ) (Meisels & Zivan, 2007), and to Asynchronous Weak Commitment (AWC) (Yokoo,
1995), which was used in the original APO evaluation (Mailler & Lesser, 2006).
Results are presented for three sets of tests with different values of problem density –
sparse (p1 = 0.1), medium (p1 = 0.4), and dense (p1 = 0.7). In all the sets the value of p2
varies between 0.1 and 0.9, to cover all ranges of problem difficulty.
247

Grinshpoun & Meisels

Figure 5: Mean NCCCs in dense problems (p1 = 0.7).

Figure 6: Mean number of messages in medium density problems (p1 = 0.4).

In order to evaluate the performance of the algorithms, two independent measures of
performance are used – search effort in the form of NCCCs and communication load in
the form of the total number of messages sent. Figures 3, 4, and 5 present the number of
NCCCs performed by CompAPO while solving problems with different densities. Figure 6
shows the total number of messages sent during the problem solving process. All figures
exhibit the phase-transition phenomenon – for increasing values of the tightness, p2 , problem
difficulty increases, reaches a maximum, and then drops back to a low value. This is
termed the easy-hard-easy transition of hard problems (Prosser, 1996), and was observed
for DisCSPs (Meisels & Zivan, 2007; Bessiere et al., 2005).
248

Completeness and Performance of the APO Algorithm

The performance of CompAPO in NCCCs turns out to be very poor in the phase
transition region compared to other asynchronous search algorithms. The worst results are
when the problems are relatively sparse (Figures 3 and 4). However, even for dense problems
both ABT and AFC-CBJ clearly outperform CompAPO (Figure 5). When comparing
CompAPO to AWC, the results are significantly different. AWC is known to perform best
in sparse problems. Thus, like ABT and AFC-CBJ it clearly outperforms CompAPO for
such problems (Figure 3). For medium density problems, AWC still performs better than
CompAPO but the difference between the performances of these algorithms is much smaller
(Figure 4). For dense problems, AWC performs extremely bad with about ten times more
NCCCs than CompAPO. The results of AWC are omitted from Figure 5, since it did not
finish running this set of tests in a reasonable time and we had to stop its run after 40
hours.
Notice that the scale in Figure 4 is different than in Figures 3 and 5. This is due
to especially poor performance of APO around the phase transition of medium density
problems. Such behavior is untypical, since most DisCSP algorithm suffer from their worst
performance around the phase transition of high density problems (Figure 5). The fact
that the performance of CompAPO is better on high density problems than on medium
density ones can be explained by the faster convergence to a centralized solution in dense
problems. In problems around the phase transition, the CompAPO algorithm frequently
reaches full centralization anyway. Thus, the faster convergence to a centralized solution
actually improves the performance of the algorithm.
While the search effort performed by the agents running CompAPO is extremely high,
the communication load on the system remains particularly low. This can be seen in
Figure 6, for medium density problems. Similar results were achieved for sparse and dense
problems. This is not surprising, since the major part of the search effort is carried out
by agents performing mediation sessions without the need for an extensive exchange of
messages.
9.3 Comparison to Other Versions of APO
Several versions of the APO algorithm were proposed by Benisch and Sadeh (2006). One of
these versions (APO-BT) uses simple backtracking as its mediation procedure, instead of
the Branch and Bound that was originally proposed with APO (APO-BB). The performance
of CompAPO is compared to these two incomplete versions of the algorithm.
The modifications of CompAPO, and especially the prevention of partial mediation
sessions (section 6.1) add synchronization to the algorithm. A potential partial mediation
session must wait for other sessions to end until the mediator is able to get a lock on its entire
good list. Such synchronization may tax the performance of the algorithm. Nevertheless,
our experiments show that CompAPO actually performs slightly better than APO-BB as
measured by NCCCs (Figures 7 and 8). The improved performance can be explained by
the better distribution of data when the entire solution is sent with the accept! message
(section 6.2). Figure 9 shows that the effect of CompAPO’s modifications is even greater
on the communication load. This substantial advantage of CompAPO may be explained by
the use of the interrupt-based approach (section 6.3) that helps performance by eliminating
the unnecessary overhead of busy-waiting.
249

Grinshpoun & Meisels

Figure 7: Mean NCCCs in medium density problems (p1 = 0.4).

Figure 8: Mean NCCCs in dense problems (p1 = 0.7).
Figure 10 presents the mean size of the largest mediation session occurring during search,
for medium density problems (p1 = 0.4) with 15 variables. The average size of the largest
mediation session is around 12 (out of a maximum of 15). It occurs for problems in the
phase transition region when p2 is 0.5 and 0.6. Although this number is not very far from
the maximum of 15, it does suggest that a considerable portion of the hard problems are
solved without reaching a full centralization.
The part of the code of CompAPO that solves the neighboring mediation sessions problem (section 6.2) implies a potential additional growth to good lists (ok?, line 5), which
may result in a faster centralization during problem solving. Nevertheless, Figure 10 clearly
shows that CompAPO does not centralize faster than the original version of APO (APOBB), except for very tight, unsolvable problems.
250

Completeness and Performance of the APO Algorithm

Figure 9: Mean number of messages in medium density problems (p1 = 0.4).

Figure 10: Mean size of the largest mediation session (p1 = 0.4 and n = 15).
Our experiments show that in medium density problems, the APO-BT version performs
poorly with respect to both NCCCs and the number of sent messages in comparison to
APO-BB and CompAPO (Figures 7 and 9). The reason for ABO-BT’s poor performance
can be easily explained by its frequent convergence to full centralization as shown in Figure 10. Nevertheless, APO-BT has a lower communication load than APO-BB in the phase
transition. The reason for this is actually the same reason that leads to APO-BT’s extensive search effort. A prompt convergence to full centralization yields a high search effort
(NCCCs), but at the same time may reduce the communication load.
Figure 8 shows that on dense problems APO-BT performs better than APO-BB and
almost the same as CompAPO. This supports the results reported by Benisch and Sadeh
(2006) for dense random DisCSPs. The same paper also presents the results for structured
251

Grinshpoun & Meisels

Figure 11: Interrupt-based vs. busy-waiting (mean NCCCs with p1 = 0.4).
3-coloring problems, in which APO-BT is outperformed by APO-BB. Similar behavior is
observed in the experiments that we conducted on sparser problems (Figure 7), which
suggests that the variance in APO-BT’s performance has more to do with the density of
the problem than with its structure.
Benisch and Sadeh propose an additional version to the APO algorithm, in which the
mediation session selection rule is the inverse of the original selection rule (Benisch & Sadeh,
2006). The version called IAPO instructs agents to choose the smallest mediation session
rather than the largest one. It is not clear that IAPO can be turned into a correct algorithm,
since the correctness proofs presented in section 7 rely on the fact that the largest mediation
sessions are chosen. Consequently, the evaluation of IAPO is omitted from this paper.
9.4 Interrupt-Based Versus Busy-Waiting
Figures 11 and 12 present two measures of performance comparing different methods for
synchronization that is needed in order to avoid conflicts between concurrent mediation
sessions – interrupt-based and busy-waiting (section 6.3). The interrupt-based method
clearly outperforms busy-waiting for harder problem instances. Predictably, the difference
in performance is more pronounced when measuring the number of messages (Figure 12).
9.5 Evaluation of CompOptAPO
The original (and incomplete) version of the OptAPO algorithm was evaluated by Mailler
and Lesser (2004). It was compared to the ADOPT algorithm (Modi et al., 2005), which
is not the best DisCOP solver. Similarly to the original results of APO (Mailler & Lesser,
2006), the comparison between OptAPO and ADOPT (Mailler & Lesser, 2004) was made
with respect to three measures – the number of sent messages, the number of cycles, and
the serial runtime. For the same reasons as in DisCSP algorithms, cycles and serial runtime
are also problematic for measuring the performance of DisCOP algorithms. As was the case
252

Completeness and Performance of the APO Algorithm

Figure 12: Interrupt-based vs. busy-waiting (mean number of messages with p1 = 0.4).
with CompAPO, the CompOptAPO algorithm will also be evaluated by counting NCCCs
and the number of sent messages.
The Distributed Optimization problems used in the following experiments are random
Max-DisCSPs. Max-DisCSP is a subclass of DisCOP in which all constraint costs (weights)
are equal to one (Modi et al., 2005). This feature simplifies the task of generating random problems, since by using Max-DisCSPs one does not have to decide on the costs of
the constraints. Max-CSPs are commonly used in experimental evaluations of constraint
optimization problems (COPs) (Larrosa & Schiex, 2004). Other experimental evaluations
of DisCOPs include graph coloring problems (Modi et al., 2005; Zhang, Xing, Wang, &
Wittenburg, 2005), which are a subclass of Max-DisCSP. The advantage of using random
Max-DisCSP problems is the fact that they create an evaluation framework that is known
to exhibit the phase-transition phenomenon in centralized COPs. This is important when
evaluating algorithms for solving DisCOPs, enabling a known analogy with behavior of centralized algorithms as the problem difficulty changes. The problems solved in this section
are randomly generated Max-DisCSP with 10 agents (n = 10) and 10 values (k = 10), constraint density of either p1 = 0.4 or p1 = 0.7, and varying constraint tightness 0.4 ≤ p2 < 1.
The performance of CompOptAPO is compared to three search algorithms – Synchronous Branch and Bound (SyncBB) (Hirayama & Yokoo, 1997), AFB (Gershman et al.,
2006), and ADOPT (Modi et al., 2005). ADOPT was used in the original OptAPO evaluation (Mailler & Lesser, 2004).
It must be noted that in our experiments with the original OptAPO algorithm, we
have experienced several runs in which the algorithm failed to advance and did not reach a
solution. This shows that the termination problem of OptAPO occurs in practice and not
just in theory, for a scenario that involves particular message delays as the one presented
in section 4 for the APO algorithm. Additionally, we discovered that OptAPO may not
always be able to report the optimal cost (i.e., the number of broken constraints in our MaxDisCSP experiments). To understand how this can happen consider an ”almost” disjoint
253

Grinshpoun & Meisels

Figure 13: An example 3-coloring problem.

Figure 14: Mean NCCCs in sparse optimization problems (p1 = 0.4).

graph, such as the one depicted in Figure 13. In this example we assume that agents A1
and A2 do not have any conflicts. Consequently, the knowledge regarding the cost F is
not exchanged between the two groups, and no agent holds the correct overall cost of the
problem (F = 3). Nevertheless, when the OptAPO algorithm terminates, it does so with
the optimal solution. Thus, the optimal value can be derived upon termination by summing
the number of broken constraints of all of the agents. The result must be divided by two
to account for broken constraints being counted by each of the involved agents.
The performance of CompOptAPO in NCCCs is comparable with other DisCOP algorithms when the problems are relatively loose (low p2 value), with only the ADOPT
algorithm performing slightly better. This is the case for both sparse and dense problems
(Figures 14 and 15, respectively). As the problems become tighter, CompOptAPO clearly
outperforms both ADOPT and SyncBB. In fact, the ADOPT algorithm failed to terminate
in reasonable time for tight problems (p2 > 0.8 in Figure 14 and p2 > 0.6 in Figure 15). However, on tight problems the AFB algorithm is much faster than CompOptAPO. Actually,
AFB was the only algorithm in our experiments that managed to terminate in reasonable
time for problems that are both dense (p1 = 0.7) and tight (p2 = 0.9).
254

Completeness and Performance of the APO Algorithm

Figure 15: Mean NCCCs in dense optimization problems (p1 = 0.7).

Figure 16: Mean number of messages in dense optimization problems (p1 = 0.7).

Similarly to CompAPO, the communication load on the system remains particularly
low when running the CompOptAPO algorithm. This can be seen in Figure 16 for dense
problems. Similar results are observed for sparse problems. This is not surprising, since
the major part of the search effort is carried out by agents performing mediation sessions
without the need for an extensive exchange of messages.

255

Grinshpoun & Meisels

10. Conclusions
The APO search algorithm is an asynchronous distributed algorithm for DisCSPs. The
algorithm partitions the search into different subproblems. Each subproblem is solved by
a selected agent – the mediator. When conflicts arise between a solution to a subproblem
and its neighboring agents, the conflicting agents are added to the subproblem. Ideally,
the algorithm either leads to compatible solutions of constraining subproblems, or to the
growth of subproblems whose solution is incompatible with neighboring agents. This twooption situation was used in the original APO paper (Mailler & Lesser, 2006) to prove the
termination and completeness of the algorithm.
The proof of completeness of the APO algorithm as presented by Mailler and Lesser
(2006) is based on the growth of the size of the subproblems. It turns out that this expected
growth of groups does not occur in some situations, leading to a termination problem of
the algorithm. The present paper demonstrates this problem by following an example
that does not terminate. Furthermore, the paper identifies the problematic parts in the
original algorithm that interfere with its completeness and applies modifications that solve
the problematic parts. The resulting CompAPO algorithm ensures the completeness of the
search. Formal proofs for the soundness and completeness of CompAPO are presented.
The CompAPO algorithm forms a class by itself of DisCSP search algorithms. In contrast to backtracking or concurrent search processes, it achieves concurrency by solving
subproblems concurrently. It is therefore both interesting and important to evaluate the
performance of CompAPO and to compare it to other DisCSP search algorithms.
Asynchronous Partial Overlay is actually a family of algorithms. The completeness and
termination problems that are presented and corrected in the present study apply to all the
members of the family. The OptAPO algorithm (Mailler & Lesser, 2004; Mailler, 2004) is
an optimization version of APO that solves Distributed Constraint Optimization Problems
(DisCOPs). The present paper shows that similar modification to the ones made to the
APO algorithm must also be applied to OptAPO in order to ensure its correctness. These
changes call for performance evaluation of the resulting CompOptAPO algorithm.
The experimental evaluation that was presented in section 9 demonstrates that the
performance of CompAPO is poor compared to other asynchronous search algorithms. On
randomly generated DisCSPs the runtime of APO, as measured by NCCCs, is longer by
up to two orders of magnitude than that of ABT (Yokoo et al., 1998; Yokoo & Hirayama,
2000) and AFC-CBJ (Meisels & Zivan, 2007).
The total number of messages sent by CompAPO is considerably smaller than the corresponding number for ABT or AFC-CBJ. This is a clear result of the fact that hard problem
instances tend to be solved by a small number of mediators in a semi-centralized manner.
The runtime performance of CompOptAPO is better than that of ADOPT (Modi et al.,
2005) and SyncBB (Hirayama & Yokoo, 1997) for hard instances of randomly generated
DisCOPs. Similarly to the DisCSP case, the total number of messages sent by CompOptAPO is considerably smaller than the corresponding number for other DisCOP algorithms.
However, in the phase-transition region of randomly generated DisCOPs, the runtime of
CompOptAPO is longer by more than an order of magnitude than that of AFB (Gershman
et al., 2006).
256

Completeness and Performance of the APO Algorithm

References
Benisch, M., & Sadeh, N. (2006). Examining DCSP coordination tradeoffs. In Proceedings
of the Fifth International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS’06), pp. 1405–1412. ACM.
Bessiere, C., Maestre, A., Brito, I., & Meseguer, P. (2005). Asynchronous backtracking
without adding links: a new member in the ABT family. Artificial Intelligence, 161:12, 7–24.
Gershman, A., Meisels, A., & Zivan, R. (2006). Asynchronous forward-bounding for distributed constraints optimization. In Proc. ECAI-06, pp. 103–107.
Grinshpoun, T., & Meisels, A. (2007). CompAPO: A complete version of the APO algorithm. In Proceedings of the 2007 IEEE/WIC/ACM International Conference on
Intelligent Agent Technology (IAT 2007), pp. 370–376.
Grinshpoun, T., Zazon, M., Binshtok, M., & Meisels, A. (2007). Termination problem of the
APO algorithm. In Proceedings of the Eighth International Workshop on Distributed
Constraint Reasoning (DCR’07), pp. 113–124.
Hirayama, K., & Yokoo, M. (1997). Distributed partial constraint satisfaction problem.
In Proceedings of the Third International Conference on Principles and Practice of
Constraint Programming (CP-97), pp. 222–236.
Larrosa, J., & Schiex, T. (2004). Solving weighted csp by maintaining arc consistency.
Artificial Intelligence, 159, 1–26.
Mailler, R. (2004). A mediation-based approach to cooperative, distributed problem solving.
Ph.D. thesis, University of Massachusetts.
Mailler, R., & Lesser, V. (2004). Solving distributed constraint optimization problems using
cooperative mediation. In Proceedings of the Third International Joint Conference on
Autonomous Agents and MultiAgent Systems (AAMAS’04), pp. 438–445. ACM.
Mailler, R., & Lesser, V. (2006). Asynchronous partial overlay: A new algorithm for solving
distributed constraint satisfaction problems. Journal of Artificial Intelligence Research
(JAIR), 25, 529–576.
Meisels, A., Razgon, I., Kaplansky, E., & Zivan, R. (2002). Comparing performance of
distributed constraints processing algorithms. In Proc. AAMAS-2002 Workshop on
Distributed Constraint Reasoning DCR, pp. 86–93.
Meisels, A., & Zivan, R. (2007). Asynchronous forward-checking for DisCSPs. Constraints,
12 (1), 131–150.
Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). Adopt: asynchronous distributed
constraints optimization with quality guarantees. Artificial Intelligence, 161:1-2, 149–
180.
Petcu, A., & Faltings, B. (2005). A scalable method for multiagent constraint optimization.
In Proceedings of the International Joint Conference on Artificial Intelligence, pp.
266–271.
257

Grinshpoun & Meisels

Prosser, P. (1996). An empirical study of phase transitions in binary constraint satisfaction
problems. Artificial Intelligence, 81, 81–109.
Semnani, S. H., & Zamanifar, K. (2007). MaxCAPO: A new expansion of APO to solve
distributed constraint satisfaction problems. In Proceedings of the International Conference on Artificial Intelligence and Soft Computing (ASC 2007).
Smith, B. M. (1996). Locating the phase transition in binary constraint satisfaction problems. Artificial Intelligence, 81, 155 – 181.
Yokoo, M. (1995). Asynchronous weak-commitment search for solving distributed constraint satisfaction problems. In Proceedings of the First International Conference on
Principles and Practice of Constraint Programming (CP-95), pp. 88 – 102.
Yokoo, M., Durfee, E. H., Ishida, T., & Kuwabara, K. (1998). Distributed constraint satisfaction problem: Formalization and algorithms.. IEEE Trans. on Data and Kn. Eng.,
10, 673–685.
Yokoo, M., & Hirayama, K. (2000). Algorithms for distributed constraint satisfaction: A
review. Autonomous Agents and Multi-Agent Systems, 3 (2), 185–207.
Zhang, W., Xing, Z., Wang, G., & Wittenburg, L. (2005). Distributed stochastic search
and distributed breakout: properties, comparison and applications to constraints optimization problems in sensor networks. Artificial Intelligence, 161:1-2, 55–88.
Zivan, R., & Meisels, A. (2006a). Concurrent search for distributed CSPs. Artificial Intelligence, 170 (4), 440–461.
Zivan, R., & Meisels, A. (2006b). Message delay and DisCSP search algorithms. Annals of
Mathematics and Artificial Intelligence(AMAI), 46(4), 415–439.
Zivan, R., Zazone, M., & Meisels, A. (2007). Min-domain ordering for asynchronous backtracking. In Proceedings of the 13th International Conference on Principles and Practice of Constraint Programming (CP-2007), pp. 758–772.

258

Journal of Artificial Intelligence Research 33 (2008) 465-519

Submitted 05/08; published 12/08

AND/OR Multi-Valued Decision Diagrams (AOMDDs)
for Graphical Models
Robert Mateescu

MATEESCU @ PARADISE . CALTECH . EDU

Electrical Engineering Department
California Institute of Technology
Pasadena, CA 91125, USA

Rina Dechter

DECHTER @ ICS . UCI . EDU

Donald Bren School of Information and Computer Science
University of California Irvine
Irvine, CA 92697, USA

Radu Marinescu

R . MARINESCU @4 C . UCC . IE

Cork Constraint Computation Centre
University College Cork, Ireland

Abstract
Inspired by the recently introduced framework of AND/OR search spaces for graphical models, we propose to augment Multi-Valued Decision Diagrams (MDD) with AND nodes, in order
to capture function decomposition structure and to extend these compiled data structures to general weighted graphical models (e.g., probabilistic models). We present the AND/OR Multi-Valued
Decision Diagram (AOMDD) which compiles a graphical model into a canonical form that supports polynomial (e.g., solution counting, belief updating) or constant time (e.g. equivalence of
graphical models) queries. We provide two algorithms for compiling the AOMDD of a graphical
model. The first is search-based, and works by applying reduction rules to the trace of the memory
intensive AND/OR search algorithm. The second is inference-based and uses a Bucket Elimination
schedule to combine the AOMDDs of the input functions via the the APPLY operator. For both
algorithms, the compilation time and the size of the AOMDD are, in the worst case, exponential in
the treewidth of the graphical model, rather than pathwidth as is known for ordered binary decision
diagrams (OBDDs). We introduce the concept of semantic treewidth, which helps explain why
the size of a decision diagram is often much smaller than the worst case bound. We provide an
experimental evaluation that demonstrates the potential of AOMDDs.

1. Introduction
The paper extends decision diagrams into AND/OR multi-valued decision diagrams (AOMDDs)
and shows how graphical models can be compiled into these data-structures. The work presented in
this paper is based on two existing frameworks: (1) AND/OR search spaces for graphical models
and (2) decision diagrams.
1.1 AND/OR Search Spaces
AND/OR search spaces (Dechter & Mateescu, 2004a, 2004b, 2007) have proven to be a unifying
framework for various classes of search algorithms for graphical models. The main characteristic is
the exploitation of independencies between variables during search, which can provide exponential
speedups over traditional search methods that can be viewed as traversing an OR structure. The
c
2008
AI Access Foundation. All rights reserved.

M ATEESCU , D ECHTER & M ARINESCU

AND nodes capture problem decomposition into independent subproblems, and the OR nodes represent branching according to variable values. AND/OR spaces can accommodate dynamic variable
ordering, however most of the current work focuses on static decomposition. Examples of AND/OR
search trees and graphs will appear later, for example in Figures 6 and 7.
The AND/OR search space idea was originally developed for heuristic search (Nilsson, 1980).
In the context of graphical models, AND/OR search (Dechter & Mateescu, 2007) was also inspired
by search advances introduced sporadically in the past three decades for constraint satisfaction and
more recently for probabilistic inference and for optimization tasks. Specifically, it resembles the
pseudo tree rearrangement (Freuder & Quinn, 1985, 1987), that was adapted subsequently for distributed constraint satisfaction by Collin, Dechter, and Katz (1991, 1999) and more recently by
Modi, Shen, Tambe, and Yokoo (2005), and was also shown to be related to graph-based backjumping (Dechter, 1992). This work was extended by Bayardo and Miranker (1996) and Bayardo and
Schrag (1997) and more recently applied to optimization tasks by Larrosa, Meseguer, and Sanchez
(2002). Another version that can be viewed as exploring the AND/OR graphs was presented recently for constraint satisfaction (Terrioux & Jégou, 2003b) and for optimization (Terrioux & Jégou,
2003a). Similar principles were introduced recently for probabilistic inference, in algorithm Recursive Conditioning (Darwiche, 2001) as well as in Value Elimination (Bacchus, Dalmao, & Pitassi,
2003b, 2003a), and are currently at the core of the most advanced SAT solvers (Sang, Bacchus,
Beame, Kautz, & Pitassi, 2004).
1.2 Decision Diagrams
Decision diagrams are widely used in many areas of research, especially in software and hardware
verification (Clarke, Grumberg, & Peled, 1999; McMillan, 1993). A BDD represents a Boolean
function by a directed acyclic graph with two terminal nodes (labeled 0 and 1), and every internal
node is labeled with a variable and has exactly two children: low for 0 and high for 1. If isomorphic
nodes were not merged, we would have the full search tree, also called Shannon tree, which is the
usual full tree explored by a backtracking algorithm. The tree is ordered if variables are encountered
in the same order along every branch. It can then be compressed by merging isomorphic nodes
(i.e., with the same label and identical children), and by eliminating redundant nodes (i.e., whose
low and high children are identical). The result is the celebrated reduced ordered binary decision
diagram, or OBDD for short, introduced by Bryant (1986). However, the underlying structure is
OR, because the initial Shannon tree is an OR tree. If AND/OR search trees are reduced by node
merging and redundant nodes elimination we get a compact search graph that can be viewed as a
BDD representation augmented with AND nodes.
1.3 Knowledge Compilation for Graphical Models
In this paper we combine the two ideas, creating a decision diagram that has an AND/OR structure, thus exploiting problem decomposition. As a detail, the number of values is also increased
from two to any constant. In the context of constraint networks, decision diagrams can be used to
represent the whole set of solutions, facilitating solutions count, solution enumeration and queries
on equivalence of constraint networks. The benefit of moving from OR structure to AND/OR is in
a lower complexity of the algorithms and size of the compiled structure. It typically moves from
being bounded exponentially in pathwidth pw∗ , which is characteristic to chain decompositions or
linear structures, to being exponentially bounded in treewidth w∗ , which is characteristic of tree
466

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

structures (Bodlaender & Gilbert, 1991) (it always holds that w∗ ≤ pw∗ and pw∗ ≤ w∗ · log n,
where n is the number of variables of the model). In both cases, the compactness result achieved in
practice is often far smaller than what the bounds suggest.
A decision diagram offers a compilation of a propositional knowledge-base. An extension of
the OBDDs was provided by Algebraic Decision Diagrams (ADD) (Bahar, Frohm, Gaona, Hachtel,
Macii, Pardo, & Somenzi, 1993), where the terminal nodes are not just 0 or 1, but take values from
an arbitrary finite domain. The knowledge compilation approach has become an important research
direction in automated reasoning in the past decade (Selman & Kautz, 1996; Darwiche & Marquis,
2002; Cadoli & Donini, 1997). Typically, a knowledge representation language is compiled into a
compact data structure that allows fast responses to various queries. Accordingly, the computational
effort can be divided between an offline and an online phase where most of the work is pushed
offline. Compilation can also be used to generate compact building blocks to be used by online
algorithms multiple times. Macro-operators compiled during or prior to search can be viewed in
this light (Korf & Felner, 2002), while in graphical models the building blocks are the functions
whose compact compiled representations can be used effectively across many tasks.
As one example, consider product configuration tasks and imagine a user that chooses sequential options to configure a product. In a naive system, the user would be allowed to choose any valid
option at the current level based only on the initial constraints, until either the product is configured,
or else, when a dead-end is encountered, the system would backtrack to some previous state and
continue from there. This would in fact be a search through the space of possible partial configurations. Needless to say, it would be very unpractical, and would offer the user no guarantee of
finishing in a limited time. A system based on compilation would actually build the backtrack-free
search space in the offline phase, and represent it in a compact manner. In the online phase, only
valid partial configurations (i.e., that can be extended to a full valid configuration) are allowed, and
depending on the query type, response time guarantees can be offered in terms of the size of the
compiled structure.
Numerous other examples, such as diagnosis and planning problems, can be formulated as
graphical models and could benefit from compilation (Palacios, Bonet, Darwiche, & Geffner, 2005;
Huang & Darwiche, 2005a). In diagnosis, compilation can facilitate fast detection of possible faults
or explanations for some unusual behavior. Planning problems can also be formulated as graphical
models, and a compilation would allow swift adjustments according to changes in the environment.
Probabilistic models are one of the most used types of graphical models, and the basic query is to
compute conditional probabilities of some variables given the evidence. A compact compilation of a
probabilistic model would allow fast response to queries that incorporate evidence acquired in time.
For example, two of the most important tasks for Bayesian networks are computing the probability
of the evidence, and computing the maximum probable explanation (MPE). If some of the model
variables become assigned (evidence), these tasks can be performed in time linear in the compilation size, which in practice is in many cases smaller than the upper-bound based on the treewidth or
pathwidth of the graph. Formal verification is another example where compilation is heavily used
to compare equivalence of circuit design, or to check the behavior of a circuit. Binary Decision
Diagram (BDD) (Bryant, 1986) is arguably the most widely known and used compiled structure.
The contributions made in this paper to knowledge compilation in general and to decision diagrams in particular are the following:
1. We formally describe the AND/OR Multi-Valued Decision Diagram (AOMDD) and prove it
to be a canonical representation for constraint networks, given a pseudo tree.
467

M ATEESCU , D ECHTER & M ARINESCU

2. We extend the AOMDD to general weighted graphical models.
3. We give a compilation algorithm based on AND/OR search, that saves the trace of a memory
intensive search and then reduces it in one bottom up pass.
4. We present the APPLY operator that combines two AOMDDs and show that its complexity is
at most quadratic in the input, but never worse than exponential in the treewidth.
5. We give a scheduling order for building the AOMDD of a graphical model starting with
the AOMDDs of its functions which is based on a Variable Elimination algorithm. This
guarantees that the complexity is at most exponential in the induced width (treewidth) along
the ordering.
6. We show how AOMDDs relate to various earlier and recent compilation frameworks, providing a unifying perspective for all these methods.
7. We introduce the semantic treewidth, which helps explain why compiled decision diagrams
are often much smaller than the worst case bound.
8. We provide an experimental evaluation of the new data structure.
The structure of the paper is as follows. Section 2 provides preliminary definitions, a description
of binary decision diagrams and the Bucket Elimination algorithm. Section 3 gives an overview of
AND/OR search spaces. Section 4 introduces the AOMDD and discusses its properties. Section
5 describes a search-based algorithm for compiling the AOMDD. Section 6 presents a compilation
algorithm based on a Bucket Elimination schedule and the APPLY operation. Section 7 proves that
the AOMDD is a canonical representation for constraint networks given a pseudo tree, and Section
8 extends the AOMDD to weighted graphical models and proves their canonicity. Section 9 ties
the canonicity to the new concept of semantic treewidth. Section 10 provides an experimental
evaluation. Section 11 presents related work and Section 12 concludes the paper. All the proofs
appear in an appendix.

2. Preliminaries
Notations A reasoning problem is defined in terms of a set of variables taking values from finite
domains and a set of functions defined over these variables. We denote variables or subsets of
variables by uppercase letters (e.g., X, Y, . . .) and values of variables by lower case letters (e.g.,
x, y, . . .). Sets are usually denoted by bold letters, for example X = {X1 , . . . , Xn } is a set of
variables. An assignment (X1 = x1 , . . . , Xn = xn ) can be abbreviated as x = (hX1 , x1 i, . . . ,
hXn , xn i) or x = (x1 , . . . , xn ). For a subset of variables Y, DY denotes the Cartesian product of
the domains of variables in Y. The projection of an assignment x = (x1 , . . . , xn ) over a subset Y
is denoted by xY or x[Y]. We will also denote by Y = y (or y for short) the assignment of values
to variables in Y from their respective domains. We denote functions by letters f , g, h etc., and the
scope (set of arguments) of the function f by scope(f ).
2.1 Graphical Models
D EFINITION 1 (graphical model) A graphical model M is a 4-tuple, M = hX, D, F, ⊗i, where:
468

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

1. X = {X1 , . . . , Xn } is a finite set of variables;
2. D = {D1 , . . . , Dn } is the set of their respective finite domains of values;
3. F = {f1 , . . . , fr } is a set of positive real-valued discrete functions (i.e., their domains can
be listed), each defined over a subset of variables Si ⊆ X, called its scope, and denoted by
scope(fi ).
Q P
4. ⊗ is a combination operator1 (e.g., ⊗ ∈ { , , 1} – product, sum, join), that can take as
input two (or more) real-valued discrete functions, and produce another real-valued discrete
function.
The graphical model represents the combination of all its functions: ⊗ri=1 fi .
Several examples of graphical models appear later, for example: Figure 1 shows a constraint
network and Figure 2 shows a belief network.
In order to define the equivalence of graphical models, it is useful to introduce the notion of
universal graphical model that is defined by a single function.
D EFINITION 2 (universal equivalent graphical model) Given a graphical model M
=
hX, D, F1 , ⊗i the universal equivalent model of M is u(M) = hX, D, F2 = {⊗fi ∈F1 fi }, ⊗i.
Two graphical models are equivalent if they represent the same function. Namely, if they have
the same universal model.
D EFINITION 3 (weight of a full and a partial assignment) Given a graphical model M =
hX, D, Fi, the weight of a full assignment x = (x1 , . . . , xn ) is defined by w(x) =
⊗f ∈F f (x[scope(f )]). Given a subset of variables Y ⊆ X, the weight of a partial assignment
y is the combination of all the functions whose scopes are included in Y (denoted by FY ) evaluated
at the assigned values. Namely, w(y) = ⊗f ∈FY f (y[scope(f )]).
Consistency For most graphical models, the range of the functions has a special zero value “0”
that is absorbing relative to the combination operator (e.g., multiplication). Combining anything
with “0” yields a “0”. The “0” value expresses the notion of inconsistent assignments. It is a primary
concept in constraint networks but can also be defined relative to other graphical models that have a
“0” element.
D EFINITION 4 (consistent partial assignment, solution) Given a graphical model having a “0”
element, a partial assignment is consistent if its cost is non-zero. A solution is a consistent assignment to all the variables.
D EFINITION 5 (primal graph) The primal graph of a graphical model is an undirected graph that
has variables as its vertices and an edge connects any two variables that appear in the scope of the
same function.
The primal graph captures the structure of the knowledge expressed by the graphical model. In
particular, graph separation indicates independency of sets of variables given some assignments to
other variables. All of the advanced algorithms for graphical models exploit the graphical structure,
by using a heuristically good elimination order, a tree decomposition or some similar method. We
will use the concept of pseudo tree, which resembles the tree rearrangements introduced by Freuder
and Quinn (1985):
1. The combination operator can also be defined axiomatically (Shenoy, 1992).

469

M ATEESCU , D ECHTER & M ARINESCU

E

A

A

D

E

B

D
F

B

G

F

C

G
C

(a) Graph coloring problem

(b) Constraint graph

Figure 1: Constraint network
D EFINITION 6 (pseudo tree) A pseudo tree of a graph G = (X, E) is a rooted tree T having the
same set of nodes X, such that every arc in E is a backarc in T (A path in a rooted tree starts at the
root and ends at one leaf. Two nodes can be connected by a backarc only if there exists a path that
contains both).
We use the common concepts and parameters from graph theory, that characterize the connectivity of the graph, and how close it is to a tree or to a chain. The induced width of a graphical model
governs the complexity of solving it by Bucket Elimination (Dechter, 1999), and was also shown to
bound the AND/OR search graph when memory is used to cache solved subproblems (Dechter &
Mateescu, 2007).
D EFINITION 7 (induced graph, induced width, treewidth, pathwidth) An ordered graph is a
pair (G, d), where G = ({X1 , . . . , Xn }, E) is an undirected graph, and d = (X1 , . . . , Xn ) is an
ordering of the nodes. The width of a node in an ordered graph is the number of neighbors that
precede it in the ordering. The width of an ordering d, denoted w(d), is the maximum width over
all nodes. The induced width of an ordered graph, w∗ (d), is the width of the induced ordered graph
obtained as follows: for each node, from last to first in d, its preceding neighbors are connected
in a clique. The induced width of a graph, w∗ , is the minimal induced width over all orderings.
The induced width is also equal to the treewidth of a graph. The pathwidth pw∗ of a graph is the
treewidth over the restricted class of orderings that correspond to chain decompositions.
Various reasoning tasks, or queries can be defined over graphical models. Those can be defined formally using marginalization operators such as projection, summation and minimization.
However, since our goal is to present a compilation of a graphical model which is independent of
the queries that can be posed on it, we will discuss tasks in an informal manner only. For more
information see the work of Kask, Dechter, Larrosa, and Dechter (2005).
Throughout the paper, we will use two examples of graphical models: constraint networks
and belief networks. In the case of constraint networks, the functions can be understood as relations. In other words, the functions (also called constraints) can take only two values, {0, 1}, or
{f alse, true}. A 0 value indicates that the corresponding assignment to the variables is inconsistent (not allowed), and a 1 value indicates consistency. Belief networks are an example of the more
general case of graphical models (also called weighted graphical models). The functions in this case
are conditional probability tables, so the values of a function are real numbers in the interval [0, 1].

470

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Example 1 Figure 1(a) shows a graph coloring problem that can be modeled by a constraint network. Given a map of regions, the problem is to color each region by one of the given colors {red,
green, blue}, such that neighboring regions have different colors. The variables of the problems
are the regions, and each one has the domain {red, green, blue}. The constraints are the relation
“different” between neighboring regions. Figure 1(b) shows the constraint graph, and a solution
(A=red, B=blue, C=green, D=green, E=blue, F=blue, G=red) is given in Figure 1(a). A more
detailed example will be given later in Example 8.
Propositional Satisfiability A special case of a CSP is propositional satisfiability (SAT). A formula ϕ in conjunctive normal form (CNF) is a conjunction of clauses α1 , . . . , αt , where a clause
is a disjunction of literals (propositions or their negations). For example, α = (P ∨ ¬Q ∨ ¬R) is
a clause, where P , Q and R are propositions, and P , ¬Q and ¬R are literals. The SAT problem
is to decide whether a given CNF theory has a model, i.e., a truth-assignment to its propositions
that does not violate any clause. Propositional satisfiability (SAT) can be defined as a CSP, where
propositions correspond to variables, domains are {0, 1}, and constraints are represented by clauses,
for example the clause (¬A ∨ B) is a relation over its propositional variables that allows all tuples
over (A, B) except (A = 1, B = 0).
Cost Networks An immediate extension of constraint networks are cost networks where the set
of functions are real-valued cost functions, and the primary task is optimization. Also, GAI-nets
(generalized additive independence, Fishburn, 1970) can be used to represent utility functions. An
example of cost functions will appear in Figure 19.
D EFINITION
P8 (cost network, combinatorial optimization) A cost network is a 4-tuple,
hX, D, C, i, where X is a set of variables X = {X1 , . . . , Xn }, associated with a set of
discrete-valued domains, D = {D1 , . . . , Dn }, and a set of cost functions C = {C1 , . . . , Cr }. Each
Ci is a real-valued function defined on a subset of variables Si ⊆ X. The combination operator, is
P
. The reasoning problem is to find a minimum cost solution.

Belief Networks (Pearl, 1988) provide a formalism for reasoning about partial beliefs under conditions of uncertainty. They are defined by a directed acyclic graph over vertices representing random
variables of interest (e.g., the temperature of a device, the gender of a patient, a feature of an object, the occurrence of an event). The arcs signify the existence of direct causal influences between
linked variables quantified by conditional probabilities that are attached to each cluster of parentschild vertices in the network.
Q
D EFINITION 9 (belief networks) A belief network (BN) is a graphical model P = hX, D, PG , i,
where X = {X1 , . . . , Xn } is a set of variables over domains D = {D1 , . . . , Dn }. Given a directed acyclic graph G over X as nodes, PG = {P1 , . . . , Pn }, where Pi = {P (Xi | pa (Xi ) ) }
are conditional probability tables (CPTs for short) associated with each Xi , where pa(Xi ) are the
parents of Xi in the
Qacyclic graph G. A belief network represents a probability distribution over X,
P (x1 , . . . , xn ) = ni=1 P (xi |xpa(Xi ) ). An evidence set e is an instantiated subset of variables.
When formulated as a graphical model, functions in F denote conditional probability tables
and the scopes of these functions are determined by the directed acyclic graph G: each function
Q
fi ranges over variable Xi and its parents in G. The combination operator is product, ⊗ = .
The primal graph of a belief network (viewed as an undirected model) is called a moral graph. It
connects any two variables appearing in the same CPT.
471

M ATEESCU , D ECHTER & M ARINESCU

A Season

Sprinkler B

Watering D

A

C Rain

B

F Wetness

D

G Slippery

C

F

G

(a) Directed acyclic graph

(b) Moral graph

Figure 2: Belief network
Example 2 Figure 2(a) gives an example of a belief network over 6 variables, and Figure 2(b)
shows its moral graph . The example expresses the causal relationship between variables “Season”
(A), “The configuration of an automatic sprinkler system” (B), “The amount of rain expected”
(C), “The amount of manual watering necessary” (D), “The wetness of the pavement” (F ) and
“Whether or not the pavement is slippery” (G). The belief network expresses the probability distribution P (A, B, C, D, F, G) = P (A) · P (B|A) · P (C|A) · P (D|B, A) · P (F |C, B) · P (G|F ).
Another example of a belief network and CPTs appears in Figure 9.
The two most popular tasks for belief networks are defined below:
D EFINITION 10 (belief updating, most probable explanation (MPE)) Given a belief network
and evidence e, the belief updating task is to compute the posterior marginal probability of variable
Xi , conditioned on the evidence. Namely,
X

Bel(Xi = xi ) = P (Xi = xi | e) = α

n
Y

P (xk , e|xpak ),

{(x1 ,...,xi−1 ,xi+1 ,...,xn )|E=e,Xi =xi } k=1

where α is a normalization constant. The most probable explanation (MPE) task is to find a
complete assignment which agrees with the evidence, and which has the highest probability among
all such assignments. Namely, to find an assignment (xo1 , . . . , xon ) such that
P (xo1 , . . . , xon ) = maxx1 ,...,xn

n
Y

P (xk , e|xpak ).

k=1

2.2 Binary Decision Diagrams Review
Decision diagrams are widely used in many areas of research to represent decision processes. In
particular, they can be used to represent functions. Due to the fundamental importance of Boolean
functions, a lot of effort has been dedicated to the study of Binary Decision Diagrams (BDDs),
which are extensively used in software and hardware verification (Clarke et al., 1999; McMillan,
1993). The earliest work on BDDs is due to Lee (1959), who introduced the binary-decision program, that can be understood as a linear representation of a BDD (e.g., a depth first search ordering
of the nodes), where each node is a branching instruction indicating the address of the next instruction for both the 0 and the 1 value of the test variable. Akers (1978) presented the actual graphical
472

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C
0
1
0
1
0
1
0
1

f(ABC)
0
0
0
1
0
1
0
1

A

A

B

C

C

0

C

0

(a) Table

0

B

B

1

0

B

0

1

B

C

1

0

(b) Unordered tree

C

0

0

C

1

0

C

1

0

1

(c) Ordered tree

Figure 3: Boolean function representations
representation and further developed the BDD idea. However, it was Bryant (1986) that introduced
what is now called the Ordered Binary Decision Diagram (OBDD). He restricted the order of variables along any path of the diagram, and presented algorithms (most importantly the apply procedure, that combines two OBDDs by an operation) that have time complexity at most quadratic in the
sizes of the input diagrams. OBDDs are fundamental for applications with large binary functions,
especially because in many practical cases they provide very compact representations.
A BDD is a representation of a Boolean function. Given B = {0, 1}, a Boolean function
f : Bn → B, has n arguments, X1 , · · · , Xn , which are Boolean variables, and takes Boolean
values.
Example 3 Figure 3(a) shows a table representation of a Boolean function of three variables. This
explicit representation is the most straightforward, but also the most costly due to its exponential
requirements. The same function can also be represented by a binary tree, shown in Figure 3(b),
that has the same exponential size in the number of variables. The internal round nodes represent
the variables, the solid edges are the 1 (or high) value, and the dotted edges are the 0 (or low) value.
The leaf square nodes show the value of the function for each assignment along a path. The tree
shown in 3(b) is unordered, because variables do not appear in the same order along each path.
In building an OBDD, the first condition is to have variables appear in the same order (A,B,C)
along every path from root to leaves. Figure 3(c) shows an ordered binary tree for our function.
Once an order is imposed, there are two reduction rules that transform a decision diagram into an
equivalent one:
(1) isomorphism: merge nodes that have the same label and the same children.
(2) redundancy: eliminate nodes whose low and high edges point to the same node, and connect
parent of removed node directly to child of removed node.
Applying the two reduction rules exhaustively yields a reduced OBDD, sometimes denoted
rOBDD. We will just use OBDD and assume that it is completely reduced.
Example 4 Figure 4(a) shows the binary tree from Figure 3(c) after the isomorphic terminal nodes
(leaves) have been merged. The highlighted nodes, labeled with C, are also isomorphic, and Figure
4(b) shows the result after they are merged. Now, the highlighted nodes labeled with C and B are
redundant, and removing them gives the OBDD in Figure 4(c).
2.3 Bucket Elimination Review
Bucket Elimination (BE) (Dechter, 1999) is a well known variable elimination algorithm for inference in graphical models. We will describe it using the terminology for constraint networks, but BE
473

M ATEESCU , D ECHTER & M ARINESCU

A

A

B

B

C

C

C

0

1

A

B

C

B

C

C

0

(a) Isomorphic nodes

B

C

1

0

(b) Redundant nodes

1

(c) OBDD

Figure 4: Reduction rules
A:

A
C1(AC)
C2(AB)
C3(ABE)

B

C

C4(BCD)

h4(A)

B:

C2(AB)

E:

C3(ABE)

A

h3(AB)

h2(AB)

AB bucket-B
AB

ABE

C:
E

D

(a) Constraint network

D:

C1(AC)

h1(BC)

bucket-A

A

bucket-E

AB

ABC bucket-C
BC

BCD bucket-D

C4 (BCD)

(b) BE execution

(c) Bucket tree

Figure 5: Bucket Elimination
can also be applied to any graphical model. Consider a constraint network R = hX, D, Ci and an
ordering d = (X1 , X2 , . . . , Xn ). The ordering d dictates an elimination order for BE, from last to
first. Each variable is associated with a bucket. Each constraint from C is placed in the bucket of its
latest variable in d. Buckets are processed from Xn to X1 by eliminating the bucket variable (the
constraints residing in the bucket are joined together, and the bucket variable is projected out) and
placing the resulting constraint (also called message) in the bucket of its latest variable in d. After
its execution, BE renders the network backtrack free, and a solution can be produced by assigning
variables along d. BE can also produce the solutions count if marginalization is done by summation
(rather than projection) over the functional representation of the constraints, and join is substituted
by multiplication.
BE also constructs a bucket tree, by linking the bucket of each Xi to the destination bucket of
its message (called the parent bucket). A node in the bucket tree typically has a bucket variable, a
collection of constraints, and a scope (the union of the scopes of its constraints). If the nodes of the
bucket tree are replaced by their respective bucket variables, it is easy to see that we obtain a pseudo
tree.
Example 5 Figure 5(a) shows a network with four constraints. Figure5(b) shows the execution of
Bucket Elimination along d = (A, B, E, C, D). The buckets are processed from D to A.2 Figure
5(c) shows the bucket tree. The pseudo tree corresponding to the order d is given in Fig. 6(a).
2. The representation in Figure 5 reverses the top down bucket processing described in earlier papers (Dechter, 1999).

474

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Procedure GeneratePseudoTree(G, d)
1
2
3
4
5

input : graph G = (X, E); order d = (X1 , . . . , Xn )
output : Pseudo tree T
Make X1 the root of T
Condition on X1 (eliminate X1 and its incident edges from G). Let G1 , . . . , Gp be the resulting connected
components of G
for i = 1 to p do
Ti = GeneratePseudoTree (Gi , d|Gi )
Make root of Ti a child of X1

6 return T

2.4 Orderings and Pseudo Trees
Given an ordering d, the structural information captured in the primal graph through the scopes
of the functions F = {f1 , . . . , fr } can be used to create the unique pseudo tree that corresponds
to d (Mateescu & Dechter, 2005). This is precisely the bucket tree (or elimination tree), that is
created by BE (when variables are processed in reverse d). The same pseudo tree can be created by
conditioning on the primal graph, and processing variables in the order d, as described in Procedure
GeneratePseudoTree. In the following, d|Gi is the restriction of the order d to the nodes of
the graph Gi .

3. Overview of AND/OR Search Space for Graphical Models
The AND/OR search space is a recently introduced (Dechter & Mateescu, 2004a, 2004b, 2007)
unifying framework for advanced algorithmic schemes for graphical models. Its main virtue consists in exploiting independencies between variables during search, which can provide exponential
speedups over traditional search methods oblivious to problem structure. Since AND/OR MDDs
are based on AND/OR search spaces we need to provide a comprehensive overview for the sake of
completeness.
3.1 AND/OR Search Trees
The AND/OR search tree is guided by a pseudo tree of the primal graph. The idea is to exploit
the problem decomposition into independent subproblems during search. Assigning a value to a
variable (also known as conditioning), is equivalent in graph terms to removing that variable (and its
incident edges) from the primal graph. A partial assignment can therefore lead to the decomposition
of the residual primal graph into independent components, each of which can be searched (or solved)
separately. The pseudo tree captures precisely all these decompositions given an order of variable
instantiation.
D EFINITION 11 (AND/OR search tree of a graphical model) Given a graphical model M =
hX, D, Fi, its primal graph G and a pseudo tree T of G, the associated AND/OR search tree
has alternating levels of OR and AND nodes. The OR nodes are labeled Xi and correspond to
variables. The AND nodes are labeled hXi , xi i (or simply xi ) and correspond to value assignments.
The structure of the AND/OR search tree is based on T . The root is an OR node labeled with the
root of T . The children of an OR node Xi are AND nodes labeled with assignments hXi , xi i that

475

M ATEESCU , D ECHTER & M ARINESCU

A

A

B

1

B

B

0
E

E

0

C

D

1
C

0 1

E

0

1

D
0 1

(a) Pseudo tree

0 1

0
C

E

0

1

D

D

0 1

0 1

0 1

1
C

E

0

1

D

D

0 1

0 1

0 1

C
0

1

D

D

D

0 1

0 1

0 1

(b) Search tree

Figure 6: AND/OR search tree
are consistent with the assignments along the path from the root. The children of an AND node
hXi , xi i are OR nodes labeled with the children of variable Xi in the pseudo tree T .
Example 6 Figure 6 shows an example of an AND/OR search tree for the graphical model given in
Figure 5(a), assuming all tuples are consistent, and variables are binary valued. When some tuples
are inconsistent, some of the paths in the tree do not exist. Figure 6(a) gives the pseudo tree that
guides the search, from top to bottom, as indicated by the arrows. The dotted arcs are backarcs
from the primal graph. Figure 6(b) shows the AND/OR search tree, with the alternating levels of
OR (circle) and AND (square) nodes, and having the structure indicated by the pseudo tree.
The AND/OR search tree can be traversed by a depth first search algorithm, thus using linear
space. It was already shown (Freuder & Quinn, 1985; Bayardo & Miranker, 1996; Darwiche, 2001;
Dechter & Mateescu, 2004a, 2007) that:
T HEOREM 1 Given a graphical model M over n variables, and a pseudo tree T of depth m, the
size of the AND/OR search tree based on T is O(n k m ), where k bounds the domains of variables.
A graphical model of treewidth w∗ has a pseudo tree of depth at most w∗ log n, therefore it has an
∗
AND/OR search tree of size O(n k w log n ).
The AND/OR search tree expresses the set of all possible assignments to the problem variables
(all solutions). The difference from the traditional OR search space is that a solution is no longer a
path from root to a leaf, but rather a tree, defined as follows:
D EFINITION 12 (solution tree) A solution tree of an AND/OR search tree contains the root node.
For every OR node, it contains one of its child nodes and for each of its AND nodes it contains all
its child nodes, and all its leaf nodes are consistent.
3.2 AND/OR Search Graph
The AND/OR search tree may contain nodes that root identical subproblems. These nodes are said
to be unifiable. When unifiable nodes are merged, the search space becomes a graph. Its size
becomes smaller at the expense of using additional memory by the search algorithm. The depth first
search algorithm can therefore be modified to cache previously computed results, and retrieve them
when the same nodes are encountered again. The notion of unifiable nodes is defined formally next.

476

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

D EFINITION 13 (minimal AND/OR graph, isomorphism) Two AND/OR search graphs G and G0
are isomorphic if there exists a one to one mapping σ from the vertices of G to the vertices of G0
such that for any vertex v, if σ(v) = v 0 , then v and v 0 root identical subgraphs relative to σ. An
AND/OR graph is called minimal if all its isomorphic subgraphs are merged. Isomorphic nodes
(that root isomorphic subgraphs) are also said to be unifiable.
It was shown by Dechter and Mateescu (2007) that:
T HEOREM 2 A graphical model M has a unique minimal AND/OR search graph relative to a
pseudo-tree T .
The minimal AND/OR graph of a graphical model G relative to a pseudo tree T is denoted by
MT (G). Note that the definition of minimality used in the work of Dechter and Mateescu (2007)
is based only on isomorphism reduction. We will extend it here by also including the elimination
of redundant nodes. The previous theorem only shows that given an AND/OR graph, the merge
operator has a fixed point, which is the minimal AND/OR graph. We will show in this paper that
the AOMDD is a canonical representation, namely that any two equivalent graphical models can
be represented by the same unique AOMDD given that they accept the same pseudo tree, and the
AOMDD is minimal in terms of number of nodes.
Some unifiable nodes can be identified based on their contexts. We can define graph based
contexts for both OR nodes and AND nodes, just by expressing the set of ancestor variables in T
that completely determine a conditioned subproblem. However, it can be shown that using caching
based on OR contexts makes caching based on AND contexts redundant and vice versa, so we will
only use OR caching. Any value assignment to the context of X separates the subproblem below X
from the rest of the network.
D EFINITION 14 (OR context) Given a pseudo tree T of an AND/OR search space,
context(X) = [X1 . . . Xp ] is the set of ancestors of X in T , ordered descendingly, that are connected in the primal graph to X or to descendants of X.
D EFINITION 15 (context unifiable OR nodes) Given an AND/OR search graph, two OR nodes n1
and n2 are context unifiable if they have the same variable label X and the assignments of their
contexts is identical. Namely, if π1 is the partial assignment of variables along the path to n1 , and
π2 is the partial assignment of variables along the path to n2 , then their restriction to the context of
X is the same: π1 |context(X) = π2 |context(X) .
The depth first search algorithm that traverses the AND/OR search tree, can be modified to
traverse a graph, if enough memory is available. We could allocate a cache table for each variable X,
the scope of the table being context(X). The size of the cache table for X is therefore the product
of the domains of variables in its context. For each variable X, and for each possible assignment
to its context, the corresponding conditioned subproblem is solved only once and the computed
value is saved in the cache table, and whenever the same context assignment is encountered again,
the value of the subproblem is retrieved from the cache table. Such an algorithm traverses what is
called the context minimal AND/OR graph.
D EFINITION 16 (context minimal AND/OR graph) The context minimal AND/OR graph is obtained from the AND/OR search tree by merging all the context unifiable OR nodes.
477

M ATEESCU , D ECHTER & M ARINESCU

R

F

G

B

[]

C

A
J

K

[C]

H

[C]

L

[CK]

A

[CH]

N

[CKL]

B

[CHA]

O

[CKLN]

P

[CKO]

H
E
C

D

L

E

[CHAB]

R

[HAB]

J

[CHAE]

F

[AR]

D

[CEJ]

G

[AF]

M

[CD]

K
M
N
P
O

(a) Primal graph

(b) Pseudo tree
C

0

0

K

H

K

0

1

0

1

L

L

L

L

H

0

1

A

0

A

1

A

A

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

N

N

N

N

N

N

N

N

B

B

B

B

B

B

B

B

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

O

O

O

O

O

O

O

O

O

O

O

O

O

O

O

O

0

E

0

1

E

E

E

0

1

E

E

E

0

1

E

E

E

E

0

1

E

E

E

E

1

E

0

0

1

R

R

R

0

1

R

R

R

R

1

R

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 1 0 1 0 1 0 1 0 1
P

P

P

P

P

P

P

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

P
J

J

J

J

J

J

J

J

J

J

J

J

J

J

J

J

0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1
0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

D

D

D

D

0 1 0 1 0 1 0 1

D

D

D

F

F

F

F

0 1 0 1 0 1 0 1

D

0 1 0 1 0 1 0 1

G

G

G

G

0 1 0 1 0 1 0 1
M

M

M

M

0 1 0 1 0 1 0 1

(c) Context minimal graph

Figure 7: AND/OR search graph
It was already shown (Bayardo & Miranker, 1996; Dechter & Mateescu, 2004a, 2007) that:
T HEOREM 3 Given a graphical model M, its primal graph G and a pseudo tree T , the size of the
context minimal AND/OR search graph based on T , and therefore the size of its minimal AND/OR
∗
search graph, is O(n k wT (G) ), where wT∗ (G) is the induced width of G over the depth first traversal
of T , and k bounds the domain size.

Example 7 Let’s look at the impact of caching on the size of the search space by examining a larger
example. Figure 7(a) shows a graphical model with binary variables and Figure 7(b) a pseudo tree
that drives the AND/OR search. The context of each node is given in square brackets. The context
minimal graph is given in Figure 7(c). Note that it is far smaller than the AND/OR search tree,
which has 28 = 256 AND nodes at the level of M alone (because M is at depth 8 in the pseudo tree).
The shaded rectangles show the size of each cache table, equal to the number of OR nodes that
appear in each one. A cache entry is useful whenever there are more than one incoming edges into
the OR node. Incidentally, the caches that are not useful (namely OR nodes with only one incoming
arc), are called dead caches (Darwiche, 2001), and can be determined based only on the pseudo
478

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

tree inspection, therefore a cache table need not be allocated for them. The context minimal graph
can also explain the execution of BE along the same pseudo tree (or, equivalently, along its depth
first traversal order). The buckets are the shaded rectangles, and the processing is done bottom up.
The number of possible assignments to each bucket equals the number of AND nodes that appear
in it. The message scope is identical to the context of the bucket variable, and the message itself is
identical to the corresponding cache table. For more details on the relationship between AND/OR
search and BE see the work of Mateescu and Dechter (2005).
3.3 Weighted AND/OR Graphs
In the previous subsections we described the structure of the AND/OR trees and graphs. In order
to use them to solve a reasoning task, we need to define a way of using the input function values
during the traversal of an AND/OR graph. This is realized by placing weights (or costs) on the
OR-to-AND arcs, dictated by the function values. Only the functions that are relevant contribute to
an OR-to-AND arc weight, and this is captured by the buckets relative to the pseudo tree:
D EFINITION 17 (buckets relative to a pseudo tree) Given a graphical model M = hX, D, F, ⊗i
and a pseudo tree T , the bucket of Xi relative to T , denoted BT (Xi ), is the set of functions whose
scopes contain Xi and are included in pathT (Xi ), which is the set of variables from the root to Xi
in T . Namely,
BT (Xi ) = {f ∈ F|Xi ∈ scope(f ), scope(f ) ⊆ pathT (Xi )}.

A function belongs to the bucket of a variable Xi iff its scope has just been fully instantiated
when Xi was assigned. Combining the values of all functions in the bucket, for the current assignment, gives the weight of the OR-to-AND arc:
D EFINITION 18 (OR-to-AND weights) Given an AND/OR graph of a graphical model M, the
weight w(n,m) (Xi , xi ) of arc (n, m) where Xi labels n and xi labels m, is the combination of
all the functions in BT (Xi ) assigned by values along the current path to the AND node m, πm .
Formally, w(n,m) (Xi , xi ) = ⊗f ∈BT (Xi ) f (asgn(πm )[scope(f )]).
D EFINITION 19 (weight of a solution tree) Given a weighted AND/OR graph of a graphical model
M, and given a solution tree t having the OR-to-AND set of arcs arcs(t), the weight of t is defined
by w(t) = ⊗e∈arcs(t) w(e).
Example 8 We start with the more straightforward case of constraint networks. Since functions
only take values 0 or 1, and the combination is by product (join of relations), it follows that any ORto-AND arc can only have a weight of 0 or 1. An example is given in Figure 8. Figure 8(a) shows
a constraint graph, 8(b) a pseudo tree for it, and 8(c) the four relations that define the constraint
problem. Figure 8(d) shows the AND/OR tree that can be traversed by a depth first search algorithm
that only checks the consistency of the input functions (i.e., no constraint propagation is used).
Similar to the OBDD representation, the OR-to-AND arcs with a weight of 0 are denoted by dotted
lines, and the tree is not unfolded below them, since it will not contain any solution. The arcs with
a weight of 1 are drawn with solid lines.
479

M ATEESCU , D ECHTER & M ARINESCU

A
B
C

A

D

F

B

E

(a) Constraint graph
A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

B
0
0
0
0
1
1
1
1

C RABC
0
1
1
1
0
0
1
1
0
1
1
1
0
1
1
0

C
0
0
1
1
0
0
1
1

C

E

D

F

(b) Pseudo tree

D RBCD
0
1
1
1
0
1
1
0
0
1
1
0
0
1
1
1

A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

E RABE
0
1
1
0
0
1
1
1
0
0
1
1
0
1
1
0

A
0
0
0
0
1
1
1
1

E
0
0
1
1
0
0
1
1

F RAEF
0
0
1
1
0
1
1
1
0
1
1
1
0
1
1
0

(c) Relations
A

1

1

0

1

B

B

1

1

1
1

0
C

C

E

1
1

0
C

E

C

E

E

1

1

1

0

0

1

1

1

1

1

0

1

1

0

1

0

0

1

0

1

0

1

0

1

0

1

0

1

0

1

0

1

D

D

D

F

1

1 1

0 0

1

0

1

1

1

0

0

1
0

F

D

F

1 0

1 1

1

1

1

1

1

0

0

0

D

D

F

F

1 1

0

1

0

1

0

1

1

0

1

0

1

0

1
0

1
1

(d) AND/OR tree

Figure 8: AND/OR search tree for constraint networks
Example 9 Figure 9 shows a weighted AND/OR tree for a belief network. Figure 9(a) shows the
directed acyclic graph, and the dotted arc BC added by moralization. Figure 9(b) shows the pseudo
tree, and 9(c) shows the conditional probability tables. Figure 9(d) shows the weighted AND/OR
tree.
As we did for constraint networks, we can move from weighted AND/OR search trees to
weighted AND/OR search graphs by merging unifiable nodes. In this case the arc labels should be
also considered when determining unifiable subgraphs. This can yield context-minimal weighted
AND/OR search graphs and minimal weighted AND/OR search graphs.

4. AND/OR Multi-Valued Decision Diagrams (AOMDDs)
In this section we begin describing the contributions of this paper. The context minimal AND/OR
graph (Definition 16) offers an effective way of identifying some unifiable nodes during the execution of the search algorithm. Namely, context unifiable nodes are discovered based only on their
paths from the root, without actually solving their corresponding subproblems. However, merging based on context is not complete, which means that there may still exist unifiable nodes in
the search graph that do not have identical contexts. Moreover, some of the nodes in the context
480

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

P(A)

A

A
0
1

A

P(B | A)

P(A)
.6
.4

A
0
1

P(C | A)
A
0
1

B=1
.6
.9

B=0
.4
.1

P(D | B,C)

B
B

B
0
0
1
1

C

E
E

C
D

D

(a) Belief network

C
0
1
0
1

P(E | A,B)
D=1
.8
.9
.7
.5

D=0
.2
.1
.3
.5

C=1
.8
.3

C=0
.2
.7

(b) Pseudo tree

A
0
0
1
1

B
0
1
0
1

E=0
.4
.5
.7
.2

E=1
.6
.5
.3
.8

(c) CPTs

A

.6

.4

0

1

B

B

.4
0

0

C

.6
1

E

.2

.8

.5

0

1

0

D

.2
0

1

D

.8
1

.1
0

.2

.8

.7

0

1

0

D

.9
1

.3
0

C

0

E

.3

.7

.3

1

0

1

D

D

.7 .5
1

1

E

C

.5

.9

0

1

E

.4

.1

.6

.2

.5

0

1

.2
0

C

.8

.7

.3

1

0

1

D

.8
1

D

.1
0

.9
1

.3
0

D

.7 .5
1

0

.5
1

(d) Weighted AND/OR tree

Figure 9: Weighted AND/OR search tree for belief networks
minimal AND/OR graph may be redundant, for example when the set of solutions rooted at variable Xi is not dependant on the specific value assigned to Xi (this situation is not detectable based
on context). This is sometimes termed as “interchangeable values” or “symmetrical values”. As
overviewed earlier, Dechter and Mateescu (2007, 2004a) defined the complete minimal AND/OR
graph which is an AND/OR graph whose unifiable nodes are all merged, and Dechter and Mateescu
(2007) also proved the canonicity for non-weighted graphical models.
In this paper we propose to augment the minimal AND/OR search graph with removing redundant variables as is common in OBDD representation as well as adopt notational conventions
common in this community. This yields a data structure that we call AND/OR BDD, that exploits
decomposition by using AND nodes. We present the extension over multi-valued variables yielding
AND/OR MDD or AOMDD and define them for general weighted graphical models. Subsequently
we present two algorithms for compiling the canonical AOMDD of a graphical model: the first is
search-based, and uses the memory intensive AND/OR graph search to generate the context minimal
AND/OR graph, and then reduces it bottom up by applying reduction rules; the second is inferencebased, and uses a Bucket Elimination schedule to combine the AOMDDs of initial functions by
APPLY operations (similar to the apply for OBDDs). As we will show, both approaches have the
same worst case complexity as the AND/OR graph search with context based caching, and also the
same complexity as Bucket Elimination, namely time and space exponential in the treewidth of the
∗
problem, O(n k w ). The benefit of each of these generation schemes will be discussed.

481

M ATEESCU , D ECHTER & M ARINESCU

A

A

1

(a) OBDD

2

…

k

(b) MDD

Figure 10: Decision diagram nodes (OR)
A

A

1

…

…

…

(a) AOBDD

2

k

…

…

…

(b) AOMDD

Figure 11: Decision diagram nodes (AND/OR)
4.1 From AND/OR Search Graphs to Decision Diagrams
An AND/OR search graph G of a graphical model M = hX, D, F, ⊗i represents the set of all
possible assignments to the problem variables (all solutions and their costs). In this sense, G can
be viewed as representing the function f = ⊗fi ∈F fi that defines the universal equivalent graphical
model u(M) (Definition 2). For each full assignment x = (x1 , . . . , xn ), if x is a solution expressed
by the tree tx , then f (x) = w(tx ) = ⊗e∈arcs(tx ) w(e) (Definition 19); otherwise f (x) = 0 (the
assignment is inconsistent). The solution tree tx of a consistent assignment x can be read from G
in linear time by following the assignments from the root. If x is inconsistent, then a dead-end is
encountered in G when attempting to read the solution tree tx , and f (x) = 0. Therefore, G can be
viewed as a decision diagram that determines the values of f for every complete assignment x.
We will now see how we can process an AND/OR search graph by reduction rules similar to
the case of OBDDs, in order to obtain a representation of minimal size. In the case of OBDDs,
a node is labeled with a variable name, for example A, and the low (dotted line) and high (solid
line) outgoing arcs capture the restriction of the function to the assignments A = 0 or A = 1. To
determine the value of the function, one needs to follow either one or the other (but not both) of the
outgoing arcs from A (see Figure 10(a)). The straightforward extension of OBDDs to multi-valued
variables (multi-valued decision diagrams, or MDDs) was presented by Srinivasan, Kam, Malik,
and Brayton (1990), and the node structure that they use is given in Figure 10(b). Each outgoing arc
is associated with one of the k values of variable A.
In this paper we generalize the OBDD and MDD representations demonstrated in Figures 10(a)
and 10(b) by allowing each outgoing arc to be an AND arc. An AND arc connects a node to a set of
nodes, and captures the decomposition of the problem into independent components. The number of
AND arcs emanating from a node is two in the case of AOBDDs (Figure 11(a)), or the domain size
of the variable in the general case (Figure 11(b)). For a given node A, each of its k AND arcs can
connect it to possibly different number of nodes, depending on how the problem decomposes based
on each particular assignment of A. The AND arcs are depicted by a shaded sector that connects
the outgoing lines corresponding to the independent components.

482

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
1

…

2

…

…k

…

…

(a) Nonterminal meta-node

0

1

(b) Terminal meta-node 0

(c) Terminal meta-node 1

Figure 12: Meta-nodes
We define the AND/OR Decision Diagram representation based on AND/OR search graphs. We
find that it is useful to maintain the semantics of Figure 11 especially when we need to express the
redundancy of nodes, and therefore we introduce the meta-node data structure, which defines small
portions of any AND/OR graph, based on an OR node and its AND children:
D EFINITION 20 (meta-node) A meta-node u in an AND/OR search graph can be either: (1) a
terminal node labeled with 0 or 1, or (2) a nonterminal node, that consists of an OR node labeled
X (therefore var(u) = X) and its k AND children labeled x1 , . . . , xk that correspond to the value
assignments of X. Each AND node labeled xi stores a list of pointers to child meta-nodes, denoted
by u.childreni . In the case of weighted graphical models, the AND node xi also stores the OR-toAND arc weight w(X, xi ).
The rectangle in Figure 12(a) is a meta-node for variable A, that has a domain of size k. Note
that this is very similar to Figure 11, with the small difference that the information about the value of
A that corresponds to each outgoing AND arc is now stored in the AND nodes of the meta-node. We
are not showing the weights in that figure. A larger example of an AND/OR graph with meta-nodes
appears later in Figure 16.
The terminal meta-nodes play the role of the terminal nodes in OBDDs. The terminal metanode 0, shown in Figure 12(b), indicates inconsistent assignments, while the terminal meta-node 1,
shown in figure 12(c) indicates consistent ones.
Any AND/OR search graph can now be viewed as a diagram of meta-nodes, simply by grouping
OR nodes with their AND children, and adding the terminal meta-nodes appropriately.
Once we have defined the meta-nodes, it is easier to see when a variable is redundant with respect to the outcome of the function based on the current partial assignment. A variable is redundant
if any of its assignments leads to the same set of solutions.
D EFINITION 21 (redundant meta-node) Given a weighted AND/OR search graph G represented
with meta-nodes, a meta-node u with var(u) = X and |D(X)| = k is redundant iff:
(a) u.children1 = . . . = u.childrenk and
(b) w(X, x1 ) = . . . = w(X, xk ).
An AND/OR graph G, that contains a redundant meta-node u, can be transformed into an equivalent graph G 0 by replacing any incoming arc into u with its common list of children u.children1 ,
absorbing the common weight w(X, x1 ) by combination into the weight of the parent meta-node
corresponding to the incoming arc, and then removing u and its outgoing arcs from G. The
value X = x1 is picked here arbitrarily, because they are all isomorphic. If u is the root of the
483

M ATEESCU , D ECHTER & M ARINESCU

Procedure RedundancyReduction
: AND/OR graph G; redundant meta-node u, with var(u) = X; List of meta-node parents of u,
denoted by P arents(u).
output : Reduced AND/OR graph G after the elimination of u.
1 if P arents(u) is empty then
2
return independent AND/OR graphs rooted by meta-nodes in u.children1 , and constant w(X, x1 )
input

3 forall v ∈ P arents(u) (assume var(v) == Y ) do
4
forall i ∈ {1, . . . , |D(Y )|} do
5
if u ∈ v.childreni then
6
v.childreni ← v.childreni \ {u}
7
v.childreni ← v.childreni ∪ u.children1
8
w(Y, yi ) ← w(Y, yi ) ⊗ w(X, x1 )
9 remove u
10 return reduced AND/OR graph G

Procedure IsomorphismReduction
: AND/OR graph G; isomorphic meta-nodes u and v; List of meta-node parents of u, denoted by
P arents(u).
output : Reduced AND/OR graph G after the merging of u and v.
forall p ∈ P arents(u) do
if u ∈ p.childreni then
p.childreni ← p.childreni \ {u}
p.childreni ← p.childreni ∪ {v}

input

1
2
3
4

5 remove u
6 return reduced AND/OR graph G

graph, then the common weight w(X, x1 ) has to be stored separately as a constant. Procedure
RedundancyReduction formalizes the redundancy elimination.
D EFINITION 22 (isomorphic meta-nodes) Given a weighted AND/OR search graph G represented
with meta-nodes, two meta-nodes u and v having var(u) = var(v) = X and |D(X)| = k are
isomorphic iff:
(a) u.childreni = v.childreni ∀i ∈ {1, . . . , k} and
(b) wu (X, xi ) = wv (X, xi ) ∀i ∈ {1, . . . , k}, (where wu , wv are the weights of u and v).
Procedure IsomorphismReduction formalizes the process of merging isomorphic metanodes. Naturally, the AND/OR graph obtained by merging isomorphic meta-nodes is equivalent to
the original one. We can now define the AND/OR Multi-Valued Decision Diagram:
D EFINITION 23 (AOMDD) An AND/OR Multi-Valued Decision Diagram (AOMDD) is a weighted
AND/OR search graph that is completely reduced by isomorphic merging and redundancy removal,
namely:
(1) it contains no isomorphic meta-nodes; and
(2) it contains no redundant meta-nodes.

484

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A

A

2…k

1

B

c

2

d

2

…k

z

…
1

1

z

k

…

y

c

…

d

y

(b) After eliminating the B
meta-node

(a) Fragment of an AOMDD

Figure 13: Redundancy reduction
A
1

A

2…k

1

2

…k

B
1

C

2

d

2

1

C

…
1

B

…k

1

e

2

…

…k

C

…
k

2

…
k

1

y

d

(a) Fragment of an AOMDD

2

k

e

…

y

(b) After merging the isomorphic C meta-nodes

Figure 14: Isomorphism reduction
Example 10 Figure 13 shows an example of applying the redundancy reduction rule to a portion
of an AOMDD. On the left side, in Figure 13(a), the meta-node of variable B is redundant (we
don’t show the weights of the OR-to-AND arcs, to avoid cluttering the figure). Any of the values
{1, . . . , k} of B will lead to the same set of meta-nodes {c, d, . . . , y}, which are coupled in an AND
arc. Therefore, the meta-node of B can be eliminated. The result is shown in Figure 13(b), where
the meta-nodes {c, d, . . . , y} and z are coupled in an AND arc outgoing from A = 1.
In Figure 14 we show an example of applying the isomorphism reduction rule. In this case, the
meta-nodes labeled with C in Figure 14(a) are isomorphic (again, we omit the weights). The result
of merging them is shown in Figure 14(b).
Examples of AOMDDs appear in Figures 16, 17 and 18. Note that if the weight on an OR-toAND arc is zero, then the descendant is the terminal meta-node 0. Namely, the current path is a
dead-end, cannot be extended to a solution, and is therefore linked directly to 0.

5. Using AND/OR Search to Generate AOMDDs
In Section 4.1 we described how we can transform an AND/OR graph into an AOMDD by applying
reduction rules. In Section 5.1 we describe the explicit algorithm that takes as input a graphi485

M ATEESCU , D ECHTER & M ARINESCU

cal model, performs AND/OR search with context-based caching to obtain the context minimal
AND/OR graph, and in Section 5.2 we give the procedure that applies the reduction rules bottom
up to obtain the AOMDD.
5.1 Algorithm AND/OR-S EARCH -AOMDD
Algorithm 1, called AND/OR-S EARCH -AOMDD, compiles a graphical model into an AOMDD.
A memory intensive (with context-based caching) AND/OR search is used to create the context minimal AND/OR graph (see Definition 16). The input to AND/OR-S EARCH -AOMDD is a graphical
model M and a pseudo tree T , that also defines the OR-context of each variable.
Each variable Xi has an associated cache table, whose scope is the context of Xi in T . This
ensures that the trace of the search is the context minimal AND/OR graph. A list denoted by LXi
(see line 35), is used for each variable Xi to save pointers to meta-nodes labeled with Xi . These
lists are used by the procedure that performs the bottom up reduction, per layers of the AND/OR
graph (one layer contains all the nodes labeled with one given variable). The fringe of the search
is maintained on a stack called OPEN. The current node (either OR or AND node) is denoted by
n, its parent by p, and the current path by πn . The children of the current node are denoted by
successors(n). For each node n, the Boolean attribute consistent(n) indicates if the current path
can be extended to a solution. This information is useful for pruning the search space.
The algorithm is based on two mutually recursive steps: Forward (beginning at line 5) and
Backtrack (beginning at line 29), which call each other (or themselves) until the search terminates.
In the forward phase, the AND/OR graph is expanded top down. The two types of nodes, AND and
OR, are treated differently according to their semantics.
Before an OR node is expanded, the cache table of its variable is checked (line 8). If the entry
is not null, a link is created to the already existing OR node that roots the graph equivalent to the
current subproblem. Otherwise, the OR node is expanded by generating its AND descendants. The
OR-to-AND weight (see Definition 18) is computed in line 13. Each value xi of Xi is checked for
consistency (line 14). The least expensive check is to verify that the OR-to-AND weight is non-zero.
However, the deterministic (inconsistent) assignments in M can be extracted to form a constraint
network. Any level of constraint propagation can be performed in this step (e.g., look ahead, arc
consistency, path consistency, i-consistency etc.). The computational overhead can increase, in the
hope of pruning the search space more aggressively. We should note that constraint propagation is
not crucial for the algorithm, and the complexity guarantees are maintained even if only the simple
weight check is performed. The consistent AND nodes are added to the list of successors of n (line
16), while the inconsistent ones are linked to the terminal 0 meta-node (line 19).
An AND node n labeled with hXi , xi i is expanded (line 20) based on the structure of the pseudo
tree. If Xi is a leaf in T , then n is linked to the terminal 1 meta-node (line 22). Otherwise, an OR
node is created for each child of Xi in T (line 24).
The forward step continues as long as the current node is not a dead-end and still has unevaluated
successors. The backtrack phase is triggered when a node has an empty set of successors (line 29).
Note that, as each successor is processed, it is removed from the set of successors in line 42. When
the backtrack reaches the root (line 32), the search is complete, the context minimal AND/OR graph
is generated, and the Procedure B OTTOM U P R EDUCTION is called.
When the backtrack step processes an OR node (line 31), it saves a pointer to it in cache, and
also adds a pointer to the corresponding meta-node to the list LXi . The consistent attribute of

486

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Algorithm 1: AND/OR S EARCH - AOMDD
input : M = hX, D, Fi; pseudo tree T rooted at X1 ; parents pai (OR-context) for every variable Xi .
output : AOMDD of M.
1 forall Xi ∈ X do
Initialize context-based cache table CacheXi (pai ) with null entries
2
3 Create new OR node t, labeled with Xi ; consistent(t) ← true; push t on top of OPEN
4 while OPEN 6= φ do
5
n ← top(OPEN); remove n from OPEN
// Forward
6
successors(n) ← φ
7
if n is an OR node labeled with Xi then
// OR-expand
if CacheXi (asgn(πn )[pai ]) 6= null then
8
Connect parent of n to CacheXi (asgn(πn )[pai ])
9
// Use the cached pointer
10
11
12
13
14
15
16
17
18
19

else
forall xi ∈ Di do
Create new AN D node t, labeled with hXi , xi i
w(X, xi ) ←
⊗
f (asgn(πn )[pai ])
f ∈BT (Xi )

if hXi , xi i is consistent with πn then
consistent(t) ← true
add t to successors(n)
else
consistent(t) ← f alse
make terminal 0 the only child of t

20
21
22
23
24
25
26
27

if n is an AND node labeled with hXi , xi i then
if childrenT (Xi ) == φ then
make terminal 1 the only child of n
else
forall Y ∈ childrenT (Xi ) do
Create new OR node t, labeled with Y
consistent(t) ← f alse
add t to successors(n)

28
29
30
31
32
33

Add successors(n) to top of OPEN
while successors(n) == φ do
let p be the parent of n
if n is an OR node labeled with Xi then
if Xi == X1 then
Call BottomUpReduction procedure

34
35
36
37
38
39

// Constraint Propagation

// AND-expand

// Backtrack

// Search is complete
// begin reduction to AOMDD

Cache(asgn(πn )[pai ]) ← n
Add meta-node of n to the list LXi
consistent(p) ← consistent(p) ∧ consistent(n)
if consistent(p) == f alse then
remove successors(p) from OPEN
successors(p) ← φ

40
41

if n is an AND node labeled with hXi , xi i then
consistent(p) ← consistent(p) ∨ consistent(n);

42
43

remove n from successors(p)
n←p

487

// Save in cache

// Check if p is dead-end

M ATEESCU , D ECHTER & M ARINESCU

Procedure BottomUpReduction
: A graphical model M = hX, D, Fi; a pseudo tree T of the primal graph, rooted at X1 ; Context
minimal AND/OR graph, and lists LXi of meta-nodes for each level Xi .
output : AOMDD of M.
Let d = {X1 , . . . , Xn } be the depth first traversal ordering of T
for i ← n down to 1 do
Let H be a hash table, initially empty
forall meta-nodes n in LXi do
if H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) returns a meta-node
p then
merge n with p in the AND/OR graph
input

1
2
3
4
5
6
7
8
9
10
11
12

else if n is redundant then
eliminate n from the AND/OR graph
combine its weight with that of the parent
else
hash n into the table H:
H(Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )) ← n

13 return reduced AND/OR graph

the AND parent p is updated by conjunction with consistent(n). If the AND parent p becomes
inconsistent, it is not necessary to check its remaining OR successors (line 38). When the backtrack
step processes an AND node (line 40), the consistent attribute of the OR parent p is updated by
disjunction with consistent(n).
The AND/OR search algorithm usually maintains a value for each node, corresponding to a task
that is solved. We did not include values in our description because an AOMDD is just an equivalent
representation of the original graphical model M. Any task over M can be solved by a traversal
of the AOMDD. It is however up to the user to include more information in the meta-nodes (e.g.,
number of solutions for a subproblem).
5.2 Reducing the Context Minimal AND/OR Graph to an AOMDD
Procedure BottomUpReduction processes the variables bottom up relative to the pseudo tree T .
We use the depth first traversal ordering of T (line 1), but any other bottom up ordering is as good.
The outer for loop (starting at line 2) goes through each level of the context minimal AND/OR graph
(where a level contains all the OR and AND nodes labeled with the same variable, in other words it
contains all the meta-nodes of that variable). For efficiency, and to ensure the complexity guarantees
that we will prove, a hash table, initially empty, is used for each level. The inner for loop (starting at
line 4) goes through all the metanodes of a level, that are also saved (or pointers to them are saved)
in the list LXi . For each new meta-node n in the list LXi , in line 5 the hash table H is checked to
verify if a node isomorphic with n already exists. If the hash table H already contains a node p corresponding to the hash key (Xi , n.children1 , . . . , n.childrenki , wn (Xi , x1 ), . . . , wn (Xki , xki )),
then p and n are isomorphic and should be merged. Otherwise, if the new meta-node n is redundant,
then it is eliminated from the AND/OR graph. If none of the previous two conditions is met, then
the new meta-node n is hashed into table H.

488

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
D

G

C

B

F

E

A

H

B
C
D

(a)

F
E

G

H

(b)

Figure 15: (a) Constraint graph for C = {C1 , . . . , C9 }, where C1 = F ∨ H, C2 = A ∨ ¬H,
C3 = A ⊕ B ⊕ G, C4 = F ∨ G, C5 = B ∨ F , C6 = A ∨ E, C7 = C ∨ E, C8 = C ⊕ D,
C9 = B ∨ C; (b) Pseudo tree (bucket tree) for ordering d = (A, B, C, D, E, F, G, H)

Proposition 1 The output of Procedure BottomUpReduction is the AOMDD of M along the
pseudo tree T , namely the resulting AND/OR graph is completely reduced.
Note that we explicated Procedure BottomUpReduction separately only for clarity. In practice, it can actually be included in Algorithm AND/OR-S EARCH -AOMDD, and the reduction rules
can be applied whenever the search backtracks. We can maintain a hash table for each variable, during the AND/OR search, to store pointers to meta-nodes. When the search backtracks out of an
OR node, it can already check the redundancy of that meta-node, and also look up in the hash table
to check for isomorphism. Therefore, the reduction of the AND/OR graph can be done during the
AND/OR search, and the output will be the AOMDD of M.
From Theorem 3 and Proposition 1 we can conclude:
T HEOREM 4 Given a graphical model M and a pseudo tree T of its primal graph G, the AOMDD
∗
of M corresponding to T has size bounded by O(n k wT (G) ) and it can be computed by Algorithm
∗
AND/OR-S EARCH -AOMDD in time O(n k wT (G) ), where wT∗ (G) is the induced width of G over
the depth first traversal of T , and k bounds the domain size.

6. Using Bucket Elimination to Generate AOMDDs
In this section we propose to use a Bucket Elimination (BE) type algorithm to guide the compilation
of a graphical model into an AOMDD. The idea is to express the graphical model functions as
AOMDDs, and then combine them with APPLY operations based on a BE schedule. The APPLY is
very similar to that from OBDDs (Bryant, 1986), but it is adapted to AND/OR search graphs. It
takes as input two functions represented as AOMDDs based on the same pseudo tree, and outputs
the combination of initial functions, also represented as an AOMDD based on the same pseudo tree.
We will describe it in detail in Section 6.2.
We will start with an example based on constraint networks. This is easier to understand because
the weights on the arcs are all 1 or 0, and therefore are depicted in the figures by solid and dashed
lines, respectively.
Example 11 Consider the network defined by X = {A, B, . . . , H}, DA = . . . = DH = {0, 1} and
the constraints (where ⊕ denotes XOR): C1 = F ∨H, C2 = A∨¬H, C3 = A⊕B⊕G, C4 = F ∨G,
489

M ATEESCU , D ECHTER & M ARINESCU

m7

A

m7
A

A
0

0

1

B
0

1

0

C
0

C
1

0

1

0

0

1

C
1

0

A

1

B

B

0

0

F

C
1

B
1

0

1

F
1

0

1

F
1

B

F

0

1

0

1

B

C
D
0

D

G

E
1

0

0

0

1

0

1

m6

G
1

0

H
1

0

0

1

D

E

1

m3

F

H
1

G

m3

m6

A
0

0

B

C

0

C
0

A

1

0

1

D
1

0

0

C

C
1

0

0

F

B

0

1

0

A

1

0

0

G

1

0

0

1

0

0

H
0

1

B

F
1

G
1

1

F

F

1

F
0

1

A

B
1

B
1

E

D
1

1

B

A

0

H

1

H
1

0

F
1

C

0 1

0

1

0

m5

C9

C

0

E

0

C

D
1

0

0

0

E

D

1

C
1

0

1

0

0

1

G

1

0

1

0

C

A

0 1

C6

C7

1

0

1

0

1

0

1

0

B

H

0 1

1

A
1

0

H

1

G

E

C3

0

1

A

1

G
0

G

F

F
0

B

G

H

m1

m1

1

B

0 1

C8

m2

A

A

1

E

E

1

0

C5

m2

A
1

D

D

D

m4

m5
0

0 1

1
m4

C4

0

H
1

0

F

1

F

0 1

0 1

G

C1

C2

H

Figure 16: Execution of BE with AOMDDs
A
0

A

1

B
0

B
1

0

B

1

B

C
C
0

C
1

0

C
1

0

C
1

0

F
1

0

F
1

0

F
1

0

C

1

0

D

1

D

E

D
0

D
1

0

E
1

0

G
1

0

G
1

0

H
1

0

C

C

F

0

D

D

E

F

H
1

D

E

F

F

F

1

G

G

G

G

H

0

D

1

H

1

(a)

G

0

(b)

Figure 17: (a) The final AOMDD; (b) The OBDD corresponding to d
C5 = B ∨ F , C6 = A ∨ E, C7 = C ∨ E, C8 = C ⊕ D, C9 = B ∨ C. The constraint graph is
shown in Figure 15(a). Consider the ordering d = (A, B, C, D, E, F, G, H). The pseudo tree (or
bucket tree) induced by d is given in Fig. 15(b). Figure 16 shows the execution of BE with AOMDDs
along ordering d. Initially, the constraints C1 through C9 are represented as AOMDDs and placed
in the bucket of their latest variable in d. The scope of any original constraint always appears on a

490

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Algorithm 2: BE-AOMDD
: Graphical model M = hX, D, Fi, where X = {X1 , . . . , Xn }, F = {f1 , . . . , fr } ; order
d = (X1 , . . . , Xn )
output : AOMDD representing ⊗i∈F fi
1 T = GeneratePseudoTree(G, d);
2 for i ← 1 to r do
// place functions in buckets
3
place Gfaomdd
in
the
bucket
of
its
latest
variable
in
d
i
input

4 for i ← n down to 1 do
message(Xi ) ← G1aomdd
5
6
while bucket(Xi ) 6= φ do
7
pick Gfaomdd from bucket(Xi );
8
9
10

// process buckets
// initialize with AOMDD of 1 ;
// combine AOMDDs in bucket of Xi

bucket(Xi ) ← bucket(Xi ) \ {Gfaomdd };
message(Xi ) ← APPLY(message(Xi ), Gfaomdd )
add message(Xi ) to the bucket of the parent of Xi in T

11 return message(X1 )

path from root to a leaf in the pseudo tree. Therefore, each original constraint is represented by an
AOMDD based on a chain (i.e., there is no branching into independent components at any point).
The chain is just the scope of the constraint, ordered according to d. For bi-valued variables, the
original constraints are represented by OBDDs, for multiple-valued variables they are MDDs. Note
that we depict meta-nodes: one OR node and its two AND children, that appear inside each gray
node. The dotted edge corresponds to the 0 value (the low edge in OBDDs), the solid edge to the
1 value (the high edge). We have some redundancy in our notation, keeping both AND value nodes
and arc-types (dotted arcs from “0” and solid arcs from “1”).
The BE scheduling is used to process the buckets in reverse order of d. A bucket is processed
by joining all the AOMDDs inside it, using the APPLY operator. However, the step of elimination
of the bucket variable is omitted because we want to generate the full AOMDD. In our example,
the messages m1 = C1 ./ C2 and m2 = C3 ./ C4 are still based on chains, therefore they are
OBDDs. Note that they contain the variables H and G, which have not been eliminated. However,
the message m3 = C5 ./ m1 ./ m2 is not an OBDD anymore. We can see that it follows the
structure of the pseudo tree, where F has two children, G and H. Some of the nodes corresponding
to F have two outgoing edges for value 1.
The processing continues in the same manner. The final output of the algorithm, which coincides
with m7 , is shown in Figure 17(a). The OBDD based on the same ordering d is shown in Fig.
17(b). Notice that the AOMDD has 18 nonterminal nodes and 47 edges, while the OBDD has 27
nonterminal nodes and 54 edges.
6.1 Algorithm BE-AOMDD
Algorithm 2, called BE-AOMDD, creates the AOMDD of a graphical model by using a BE schedule for APPLY operations. Given an order d of the variables, first a pseudo tree is created based on
,
the primal graph. Each initial function fi is then represented as an AOMDD, denoted by Gfaomdd
i
and placed in its bucket. To obtain the AOMDD of a function, the scope of the function is ordered
according to d, a search tree (based on a chain) that represents fi is generated, and then reduced
by Procedure BottomUpReduction. The algorithm proceeds exactly like BE, with the only difference that the combination of functions is realized by the APPLY algorithm, and variables are not
491

M ATEESCU , D ECHTER & M ARINESCU

eliminated but carried over to the destination bucket. The messages between buckets are initialized
with the dummy AOMDD of 1, denoted by G1aomdd , which is neutral for combination.
In order to create the compilation of a graphical model based on AND/OR graphs, it is necessary
to traverse the AND/OR graph top down and bottom up. This is similar to the inward and outward
message passing in a tree decomposition. Note that BE-AOMDD describes the bottom up traversal
explicitly, while the top down phase is actually performed by the APPLY operation. When two
AOMDDs are combined, after the top chain portion of their pseudo tree is processed, the remaining
independent branches are attached only if they participate in the newly restricted set of solutions.
This amounts to an exchange of information between the independent branches, which is equivalent
to the top down phase.
6.2 The AOMDD APPLY Operation
We will now describe how to combine two AOMDDs. The APPLY operator takes as input two
AOMDDs representing functions f1 and f2 and returns an AOMDD representing f1 ⊗ f2 .
In OBDDs the apply operator combines two input diagrams based on the same variable ordering.
Likewise, in order to combine two AOMDDs we assume that their pseudo trees are identical. This
condition is satisfied by any two AOMDDs in the same bucket of BE-AOMDD. However, we
present here a version of APPLY that is more general, by relaxing the previous condition from
identical to compatible pseudo trees. Namely, there should be a pseudo tree in which both can be
embedded. In general, a pseudo tree induces a strict partial order between the variables where a
parent node always precedes its child nodes.
D EFINITION 24 (compatible pseudo trees) A strict partial order d1 = (X, <1 ) over a set X is
consistent with a strict partial order d2 = (Y, <2 ) over a set Y, if for all x1 , x2 ∈ X ∩ Y, if
x1 <2 x2 then x1 <1 x2 . Two partial orders d1 and d2 are compatible iff there exists a partial
order d that is consistent with both. Two pseudo trees are compatible iff the partial orders induced
via the parent-child relationship, are compatible.
For simplicity, we focus on a more restricted notion of compatibility, which is sufficient when
using a BE like schedule for the APPLY operator to combine the input AOMDDs (as described in
Section 6). The APPLY algorithm that we will present can be extended to the more general notion
of compatibility.
D EFINITION 25 (strictly compatible pseudo trees) A pseudo tree T1 having the set of nodes X1
can be embedded in a pseudo tree T having the set of nodes X if X1 ⊆ X and T1 can be obtained
from T by deleting each node in X \ X1 and connecting its parent to each of its descendents. Two
pseudo trees T1 and T2 are strictly compatible if there exists T such that both T1 and T2 can be
embedded in T .
Algorithm APPLY (algorithm 3) takes as input one node from Gfaomdd and a list of nodes from
Ggaomdd . Initially, the node from Gfaomdd is its root node, and the list of nodes from Ggaomdd is in fact
also made of just one node, which is its root. We will sometimes identify an AOMDD by its root
node. The pseudo trees Tf and Tg are strictly compatible, having a target pseudo tree T .
The list of nodes from Ggaomdd always has a special property: there is no node in it that can be the
ancestor in T of another (we refer to the variable of the meta-node). Therefore, the list z1 , . . . , zm
492

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

APPLY (v1 ; z1 , . . . , zm )
input : AOMDDs Gfaomdd with nodes vi and Ggaomdd with nodes zj , based on strictly compatible pseudo
trees Tf , Tg that can be embedded in T .
var(v1 ) is an ancestor of all var(z1 ), . . . , var(zm ) in T .
var(zi ) and var(zj ) are not in ancestor-descendant relation in T , ∀i 6= j.
output : v1 ⊗ (z1 ∧ . . . ∧ zm ), based on T .
if H1 (v1 , z1 , . . . , zm ) 6= null then return H1 (v1 , z1 , . . . , zm );
// is in cache
if (any of v1 , z1 , . . . , zm is 0) then return 0
if (v1 = 1) then return 1
if (m = 0) then return v1
// nothing to combine
create new nonterminal meta-node u
var(u) ← var(v1 ) (call it Xi , with domain Di = {x1 , . . . , xki } )
for j ← 1 to ki do
u.childrenj ← φ
// children of the j-th AND node of u
// assign weight from v1
wu (Xi , xj ) ← wv1 (Xi , xj )
if ( (m = 1) and (var(v1 ) = var(z1 ) = Xi ) ) then
temp Children ← z1 .childrenj
// combine input weights
wu (Xi , xj ) ← wv1 (Xi , xj ) ⊗ wz1 (Xi , xj )

Algorithm 3:

1
2
3
4
5
6
7
8
9
10
11
12
13
14

else

15
16
17
18
19

group nodes from v1 .childrenj ∪ temp Children in several {v 1 ; z 1 , . . . , z r }
for each {v 1 ; z 1 , . . . , z r } do
y ← APPLY(v 1 ; z 1 , . . . , z r )
if (y = 0) then
u.childrenj ← 0; break

20
21

temp Children ← {z1 , . . . , zm }

else
u.childrenj ← u.childrenj ∪ {y}

22
23
24

if (u.children1 = . . . = u.childrenki ) and (wu (Xi , x1 ) = . . . = wu (Xi , xki )) then
promote wu (Xi , x1 ) to parent
return u.children1
// redundancy

25
26

if (H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki )) 6= null) then
return H2 (Xi , u.children1 , . . . , u.childrenki , wu (Xi , x1 ), . . . , wu (Xki , xki ))
// isomorphism

27 Let H1 (v1 , z1 , . . . , zm ) = u
28 Let H2 (Xi , u.children1 , . . . , u.childrenki , w u (Xi , x1 ), . . . , w u (Xki , xki )) = u
29 return u

// add u to H1
// add u to H2

from g expresses a decomposition with respect to T , so all those nodes appear on different branches.
We will employ the usual techniques from OBDDs to make the operation efficient. First, if one of
the arguments is 0, then we can safely return 0. Second, a hash table H1 is used to store the nodes
that have already been processed, based on the nodes (v1 , z1 , . . . , zr ). Therefore, we never need
to make multiple recursive calls on the same arguments. Third, a hash table H2 is used to detect
isomorphic nodes. This is typically split in separate tables for each variable. If at the end of the
recursion, before returning a value, we discover that a meta-node with the same variable, the same
children and the same weights has already been created, then we don’t need to store it and we simply
return the existing node. And fourth, if at the end of the recursion we discover that we created a
redundant node (all the children are the same and all the weights are the same), then we don’t store
it, and return instead one of its identical lists of children, and promote the common weight.
493

M ATEESCU , D ECHTER & M ARINESCU

A
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C
0
1
0
1
0
1
0
1

f(ABC)
0
0
0
1
0
1
0
1

A
0
0
0
0
1
1
1
1

A
B
C

A

A1
0

0

C

A3

*

0

A4

B2

A5

B3

0

B

B

D

D

1

D
1

D

C

A
0

0

0

A

A1B1
B

1

B4

A

1

B
0

1

1

g(ABC)
0
0
0
1
0
1
1
0

A
0

1

0

D
0
1
0
1
0
1
0
1

B1
1

B

A2

B
0
0
1
1
0
0
1
1

0

B6

=

B

A2B2

B7

0

1

C
0

B5

0

A4B6

1

A4

1

1

D

B4
1

0

B
0

1

D

B7
1

0

1

1

Figure 18: Example of APPLY operation
Note that v1 is always an ancestor of all z1 , . . . , zm in T . We consider a variable in T to be an
ancestor of itself. A few self explaining checks are performed in lines 1-4. Line 2 is specific for
multiplication, and needs to be changed for other combination operations. The algorithm creates a
new meta-node u, whose variable is var(v1 ) = Xi – recall that var(v1 ) is highest (closest to root)
in T among v1 , z1 , . . . , zm . Then, for each possible value of Xi , line 7, it starts building its list of
children.
One of the important steps happens in line 15. There are two lists of meta-nodes, one from
each original AOMDD f and g, and we will refer only to their variables, as they appear in T . Each
of these lists has the important property mentioned above, that its nodes are not ancestors of each
other. The union of the two lists is grouped into maximal sets of nodes, such that the highest node
in each set is an ancestor of all the others. It follows that the root node in each set belongs to one of
the original AOMDD, say v 1 is from f , and the others, say z 1 , . . . , z r are from g. As an example,
suppose T is the pseudo tree from Fig. 15(b), and the two lists are {C, G, H} from f and {E, F }
from g. The grouping from line 15 will create {C; E} and {F ; G, H}. Sometimes, it may be the
case that a newly created group contains only one node. This means there is nothing more to join
in recursive calls, so the algorithm will return, via line 4, the single node. From there on, only one
of the input AOMDDs is traversed, and this is important for the complexity of APPLY, discussed
below.
Example 12 Figure 18 shows the result of combining two Boolean functions by an AND operation
(or product). The input functions f and g are represented by AOMDDs based on chain pseudo
trees, while the results is based on the pseudo tree that expresses the decomposition after variables
A and B are instantiated. The APPLY operator performs a depth first traversal of the two input
AOMDDs, and generates the resulting AOMDD based on the output pseudo tree. Similar to the
case of OBDDs, a function or an AOMDD can be identified by its root meta-node. In this example
the input meta-nodes have labels (A1 , A2 , B1 , B2 , etc.). The output meta-node labeled by A2 B2 is
494

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

the root of a diagram that represents the function obtained by combining the functions rooted by A2
and B2 .
6.3 Complexity of APPLY and BE-AOMDD
We now provide a characterization of the complexity of APPLY, based on different criteria. The
following propositions are inspired by the results that govern OBDD apply complexity, but are
adapted for pseudo tree orderings.
An AOMDD along a pseudo tree can be regarded as a union of regular MDDs, each restricted
to a full path from root to a leaf in the pseudo tree. Let πT be such a path in T . Based on the
definition of strictly compatible pseudo trees, πT has corresponding paths πTf in Tf and πTg in Tg .
The MDDs from f and g corresponding to πTf and πTg can be combined using the regular MDD
apply. This process can be repeated for every path πT . The resulting MDDs, one for each path in T
need to be synchronized on their common parts (on the intersection of the paths). The algorithm we
proposed does all this processing at once, in a depth first search traversal over the inputs. Based on
our construction, we can give a first characterization of the complexity of AOMDD APPLY as being
governed by the complexity of MDD apply.
Proposition 2 Let π1 , . . . , πl be the set of paths in T enumerated from left to right and let Gfi and
Ggi be the MDDs restricted to path πi , then the size of the output of AOMDD apply
by
P
P is bounded
i | · |G i | ≤ n · max |G i | · |G i |. The time complexity is also bounded by
i | · |G i | ≤
|G
|G
i
g
g
g
i f
i f
f
n · maxi |Gfi | · |Ggi |.
A second characterization of the complexity can be given, similar to the MDD case, in terms of
total number of nodes of the inputs:
Proposition 3 Given two AOMDDs Gfaomdd and Ggaomdd based on strictly compatible pseudo trees,
the size of the output of APPLY is at most O(| Gfaomdd | · | Ggaomdd |).
We can further detail the previous proposition as follows. Given AOMDDs Gfaomdd and Ggaomdd ,
based on compatible pseudo trees Tf and Tg and the common pseudo tree T , we define the intersection pseudo tree Tf ∩g as being obtained from T by the following two steps: (1) mark all the
subtrees whose nodes belong to either Tf or Tg but not to both (the leaves of each subtree should be
leaves in T ); (2) remove the subtrees marked in step (1) from T . Steps (1) and (2) are applied just
once (that is, not recursively). The part of AOMDD Gfaomdd corresponding to the variables in Tf ∩g
is denoted by Gff ∩g , and similarly for Ggaomdd it is denoted by Ggf ∩g .
Proposition 4 The time complexity of
|Gfaomdd | + |Ggaomdd |).

APPLY

and the size of the output are O(|Gff ∩g | · |Ggf ∩g | +

We now turn to the complexity of the BE-AOMDD algorithm. Each bucket has an associated
bucket pseudo tree. The top chain of the bucket pseudo tree for variable Xi contains all and only
the variables in context(Xi ). For any other variables that appear in the bucket pseudo tree, their
associated buckets have already been processed. The original functions that belong to the bucket
of Xi have their scope included in context(Xi ), and therefore their associated AOMDDs are based
495

M ATEESCU , D ECHTER & M ARINESCU

on chains. Any other functions that appear in bucket of Xi are messages received from independent branches below. Therefore, any two functions in the bucket of Xi only share variables in the
context(Xi ), which forms the top chain of the bucket pseudo tree. We can therefore characterize
the complexity of APPLY in terms of treewidth, or context size of a bucket variable.
Proposition 5 Given two AOMDDs in the same bucket of BE-AOMDD, the time and space complexity of the APPLY between them is at most exponential in the context size of the bucket variable
(namely the number of the variables in the top chain of the bucket pseudo tree).
We can now bound the complexity of BE-AOMDD and the output size:
T HEOREM 5 The space complexity of BE-AOMDD and the size of the output AOMDD are
∗
O(n k w ), where n is the number of variables, k is the maximum domain size and w∗ is the treewidth
∗
of the bucket tree. The time complexity is bounded by O(r k w ), where r is the number of initial
functions.

7. AOMDDs Are Canonical Representations
It is well known that OBDDs are canonical representations of Boolean functions given an ordering
of the variables (Bryant, 1986), namely a strict ordering of any CNF specification of the same
Boolean function will yield an identical OBDD, and this property extends to MDDs (Srinivasan
et al., 1990). The linear ordering of the variables defines a chain pseudo tree that captures the
structure of the OBDD or MDD. In the case of AOBDDs and AOMDDs, the canonicity is with
respect to a pseudo tree, transitioning from total orders (that correspond to a linear ordering) to
partial orders (that correspond to a pseudo tree ordering). On the one hand we gain the ability to have
a more compact compiled structure, but on the other hand canonicity is no longer with respect to
all equivalent graphical models, but only relative to those graphical models that are consistent with
the pseudo tree that is used. Specifically, if we start from a strict ordering we can generate a chain
AOMDD that will be canonical relative to all equivalent graphical models. If however we want to
exploit additional decomposition we can use a partial ordering captured by a pseudo-tree and create
a more compact AOMDD. This AOMDD however is canonical relative to those equivalent graphical
models that can accept the same pseudo tree that guided the AOMDD. In general, AOMDD can be
viewed as a more flexible framework for compilation that allows both partial and total orderings.
Canonicity is restricted to a subset of graphical models whose primal graph agrees with the partial
order but it is relevant to a larger set of orderings which are consistent with the pseudo-tree.
In the following subsection we discuss the canonicity of AOMDD for constraint networks. The
case of general weighted graphical models is discussed in Section 8.
7.1 AOMDDs for Constraint Networks Are Canonical Representations
The case of constraint networks is more straightforward, because the weights on the OR-to-AND
arcs can only be 0 or 1. We will show that any equivalent constraint networks, that admit the same
pseudo tree T , have the same AOMDD based on T . We start with a proposition that will help prove
the main theorem.
Proposition 6 Let f be a function, not always zero, defined by a constraint network over X. Given
a partition {X1 , . . . , Xm } of the set of variables X (namely, Xi ∩ Xj = φ, ∀ i 6= j, and X =
496

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

i
i
∪m
i=1 X ), if f = f1 ⊗ . . . ⊗ fm and f = g1 ⊗ . . . ⊗ gm , such that scope(fi ) = scope(gi ) = X for
all i ∈ {1, . . . , m}, then fi = gi for all i ∈ {1, . . . , m}. Namely, if f can be decomposed over the
given partition, then the decomposition is unique.

We are now ready to show that AOMDDs for constraint networks are canonical representations
given a pseudo tree.
T HEOREM 6 (AOMDDs are canonical for a given pseudo tree) Given a constraint network, and
a pseudo tree T of its constraint graph, there is a unique (up to isomorphism) AOMDD that represents it, and it has the minimal number of meta-nodes.
A constraint network is defined by its relations (or functions). There exist equivalent constraint
networks that are defined by different sets of functions, even having different scope signatures.
However, equivalent constraint networks define the same function, and we can ask if the AOMDD
of different equivalent constraint networks is the same. The following corollary can be derived
immediately from Theorem 6.
Corollary 1 Two equivalent constraint networks that admit the same pseudo tree T have the same
AOMDD based on T .

8. Canonical AOMDDs for Weighted Graphical Models
Theorem 6 ensures that the AOMDD is canonical for constraint networks, namely for functions that
can only take the values 0 or 1. The proof relied on the fact that the OR-to-AND weights can only
be 0 or 1, and on Proposition 6 that ensured the unique decomposition of a function defined by a
constraint network.
In this section we turn to general weighted graphical models. We can first observe that Proposition 6 is no longer valid for general functions. This is because the valid solutions (having strictly
positive weight) can have their weight decomposed in more than one way into a product of positive
weights.
Therefore we raise the issue of recognizing nodes that root AND/OR graphs that represent the
same universal function, even though the graphical representation is different. We will see that the
AOMDD for a weighted graphical model is not unique under the current definitions, but we can
slightly modify them to obtain canonicity again. We have to note that canonicity of AOMDDs for
weighted graphical models (e.g., belief networks) is far less crucial than in the case of OBDDs that
are used in formal verification. Even more than that, sometimes it may be useful not to eliminate
the redundant nodes, in order to maintain a simpler semantics of the AND/OR graph that represents
the model.
The loss of canonicity of AOMDD for weighted graphical models can happen because of the
weights on the OR-to-AND arcs, and we suggest a possible way of re-enforcing it if a more compact
and canonical representation is needed.
Example 13 Figure 19 shows a weighted graphical model, defined by two (cost) functions,
f (M, A, B) and g(M, B, C). Assuming the order (M,A,B,C), Figure 20 shows the AND/OR search
tree on the left. The arcs are labeled with function values, and the leaves show the value of the
corresponding full assignment (which is the product of numbers on the arcs of the path). We can
497

M ATEESCU , D ECHTER & M ARINESCU

A

M
0
0
0
0
1
1
1
1

M
A

M

B
B
C

C

A
0
0
1
1
0
0
1
1

B f(M,A,B)
0
12
1
5
0
18
1
2
0
4
1
10
0
6
1
4

M
0
0
0
0
1
1
1
1

B
0
0
1
1
0
0
1
1

C g(M,B,C)
0
3
1
5
0
14
1
12
0
9
1
15
0
7
1
6

Figure 19: Weighted graphical model
M
0

1

0

1

A

A

A

A

0

1

B

0

B

5

18

2

0

1

0

1

C

C

5

0

1

36 60

14
0

C

3

5

1

0

1

70 60

54 90

14
0

0

B

4

10

0

C

12

1

B

12

3

M

C

B

6

1

0

C

C

9

15

7

6

9

15

7

1

0

1

0

1

0

1

0

36 60

70 60

54 90

0

B

12

5

18

2

1

0

1

0

1

C

6
1

28 24

5

0

1

14
0

B

4
0

C

3

1

B

4

C

12

28 24

1

10

6

4

1

0

1

C

C

12

9

15

7

6

1

0

1

0

1

36 60

Figure 20: AND/OR search tree and context minimal graph

see that either value of M (0 or 1) gives rise to the same function (because the leaves in the two
subtrees have the same values). However, the two subtrees can not be identified as representing the
same function by the usual reduction rules. The right part of the figure shows the context minimal
graph, which has a compact representation of each subtree, but does not share any of their parts.
What we would like in this case is to have a method of recognizing that the left and right subtrees
corresponding to M = 0 and M = 1 represent the same function. We can do this by normalizing
the values in each level, and processing bottom up. In Figure 21 left, the values on the OR-to-AND
arcs have been normalized, for each OR variable, and the normalization constant was promoted
up to the OR value. In Figure 21 right, the normalization constants are promoted upwards again
by multiplication. This process does not change the value of each full assignment, and therefore
produces equivalent graphs.
We can see already that some of the nodes labeled by C can now be merged, producing the graph
in Figure 22 on the left. Continuing the same process we obtain the AOMDD for the weighted graph,
shown in Figure 22 on the right.
We can define the AOMDD of a weighted graphical model as follows:
D EFINITION 26 (AOMDD of weighted graphical model) The AOMDD of a weighted graphical
model is an AND/OR graph, with meta-nodes, such that: (1) for each meta-node, its weights sum to
1; (2) the root meta-node has a constant associated with it; (3) it is completely reduced, namely it
has no isomorphic meta-nodes, and no redundant meta-nodes.

498

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

M
M

0

1

A

A

0

1

B

0

B

B

12

5

18

2

0

1

0

1

8
3/8
0

5/8

14/26

1

0

12/26

3/8

6

4

1

0

1

1

0

14/26

1

0

1

26*5

0

1

3/8

26*2

0

1

1

0

7/13

1

0

B

24*4

13*10

0

24*6

1

C

5/8

1

B

8*18

C

12/26

0

B

8*12

13 C
5/8

A

B

10

24 C

1

A
0

B

4
0

26 C

C

1

0

13*4

0

1

C

6/13

C

3/8

1

5/8

0

7/13

1

0

6/13
1

Figure 21: Normalizing values bottom up
844

M

1/2
0

M

1/2

1
0

A

1

A
A

0

1

B

96
0

0

B

130

144

1

0

1

B

52
1

96
0

B

130

144

1

0

C

3/8
0

1

7/13
0

196/422

0

1

B

52

B

96/226

1

130/226

0

1

C

5/8

226/422

144/196

52/196

0

1

C

6/13

3/8

1

0

C

5/8
1

7/13
0

6/13
1

Figure 22: AOMDD for the weighted graph
The procedure of transforming a weighted AND/OR graph into an AOMDD is very similar to
Procedure B OTTOM U P R EDUCTION from Section 5. The only difference is that when a new layer
is processed, first the meta-node weights are normalized and promoted to the parent, and then the
procedure continues as usual with the reduction rules.
T HEOREM 7 Given two equivalent weighted graphical models that accept a common pseudo tree
T , normalizing arc values together with exhaustive application of reduction rules yields the same
AND/OR graph, which is the AOMDD based on T .
Finite Precision Arithmetic The implementation of the algorithm described in this section may
prove to be challenging on machines that used finite precision arithmetic. Since the weights are
real-valued, the repeated normalization may lead to precision errors. One possible approach, which
we also used in our experiments, is to define some ε-tolerance, for some user defined sufficiently
small ε, and consider the weights to be equal if they are within ε of each other.

9. Semantic Treewidth
A graphical model M represents a universal function F = ⊗fi . The function F may be represented
by different graphical models. Given a particular pseudo tree T , that captures some of the structural
information of F , we are interested in all the graphical models that accept T as a pseudo tree, namely
their primal graphs only contain edges that are backarcs in T . Since the size of the AOMDD for F
based on T is bounded in the worst case by the induced width of the graphical model along T , we
define the semantic treewidth to be:
499

M ATEESCU , D ECHTER & M ARINESCU

1
A

2

3

B
C

C

o
B

o

1

2

A

3

4

o
o

C
D

A

o

D

B

4

o

A B
1 3
1 4
2 4
3 1
4 1
4 2

o
o

(a) The two solutions

A C
1 2
1 4
2 1
2 3
3 2
3 4
4 1
4 3

A D
1 2
1 3
2 1
2 3
2 4
3 1
3 2
3 4
4 2
4 3

D
B C
1 3
1 4
2 4
3 1
4 1
4 2

B D
1 2
1 4
2 1
2 3
3 2
3 4
4 1
4 3

C D
1 3
1 4
2 4
3 1
4 1
4 2

(b) First model

A

A B
2 4
3 1

B

C

B C
1 4
4 1

D

C D
1 3
4 2

(c) Second model

Figure 23: The 4-queen problem
D EFINITION 27 (semantic treewidth) The semantic treewidth of a graphical model M relative
to a pseudo tree T denoted by swT (M), is the smallest treewidth taken over all models R that
are equivalent to M, and accept the pseudo tree T . Formally, it is defined by swT (M) =
minR,u(R)=u(M) wT (R), where u(M) is the universal function of M, and wT (R) is the induced
width of R along T . The semantic treewidth of a graphical model, M, is the minimal semantic
treewidth over all the pseudo trees that can express its universal function.
Computing the semantic treewidth can be shown to be NP-hard.3
T HEOREM 8 Computing the semantic treewidth of a graphical model M is NP-hard.
Theorem 8 shows that computing the semantic treewidth is hard, and it is likely that the actual
complexity is even higher. However, the semantic treewidth can explain why sometimes the minimal
AND/OR graph or OBDD are much smaller than the exponential in treewidth or pathwidth upper
bounds. In many cases, there could be a huge disparity between the treewidth of M and the semantic
treewidth along T .
Example 14 Figure 23(a) shows the two solutions of the 4-queen problem. The problem is expressed by a complete graph of treewidth 3, given in Figure 23(b). Figure 23(c) shows an equivalent
problem (i.e., that has the same set of solutions), which has treewidth 1. The semantic treewidth of
the 4-queen problem is 1.
Based on the fact that an AOMDD is a canonical representation of the universal function of a
graphical model, we can conclude that the size of the AOMDD is bounded exponentially by the
semantic treewidth along the pseudo tree, rather than the treewidth of the given graphical model
representation.
Proposition 7 The size of the AOMDD of a graphical model M is bounded by O(n k swT (M )),
where n is the number of variables, k is the maximum domain size and swT (M) is the semantic
treewidth of M along the pseudo tree T .
3. We thank David Eppstein for the proof.

500

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

A
A

B

B

C

C

B

M

D

D

C

N

D

P

0

1

(a) OBDD representation

(b) Primal graph with hidden
variables M, N and P .

Figure 24: The parity function
Example 15 Consider a constraint network on n variables such that every two variables are constrained by equality (X = Y ). One graph representation is a complete graph, another is a chain
and another is a tree. If the problem is specified as a complete graph, and if we use a linear order,
the OBDD will have a linear size because there exists a representation having a pathwidth of 1
(rather than n).
While the semantic treewidth can yield a much better upper bound on the AOMDD, it can also
be a very bad bound. It is well known that the parity function on n variables has a very compact,
chain-like OBDD representation. Yet, the only constraint network representation of a parity function
is the function itself (namely a complete graph on all the variables), whose treewidth and semantic
treewidth is its number of variables, n. The OBDD representation of the parity function suggests
that the addition of hidden variables can simplify its presentation. We show an example in Figure
24. On the left side, in Figure 24(a) we have the OBDD representation of the parity function for
four binary variables. A graphical model would represent this function by a complete graph on the
four variables. However, we could add the extra variables M, N and P in Figure 24(b), sometimes
called “hidden” variables, that can help decompose the model. In this case M can form a constraint
together with A and B such that M represents the parity of A and B, namely M = 1 if A ⊕ B = 1,
where ⊕ is the parity (XOR) operator. Similarly, N would capture the parity of M and C, and P
would capture the parity of N and D, and would also give the parity of the initial four variables.
The two structures are surprisingly similar. It would be interesting to study further the connection
between hidden variables and compact AOBDDs, but we leave this for future work.

10. Experimental Evaluation
Our experimental evaluation is in preliminary stages, but the results we have are already encouraging. We ran the search-based compile algorithm, by recording the trace of the AND/OR search, and
then reducing the resulting AND/OR graph bottom up. In these results we only applied the reduction by isomorphism and still kept the redundant meta-nodes. We implemented our algorithms in
C++ and ran all experiments on a 2.2GHz Intel Core 2 Duo with 2GB of RAM, running Windows.

501

M ATEESCU , D ECHTER & M ARINESCU

10.1 Benchmarks
We tested the performance of the search-based compilation algorithm on random Bayesian networks, instances from the Bayesian Network Repository and a subset of networks from the UAI’06
Inference Evaluation Dataset.
Random Bayesian Networks The random Bayesian networks were generated using parameters
(n, k, c, p), where n is the number of variables, k is the domain size, c is the number of conditional
probability tables (CPTs) and p is the number of parents in each CPT. The structure of the network
was created by randomly picking c variables out of n and, for each, randomly picking p parents from
their preceding variables, relative to some ordering. The remaining n − c variables are called root
nodes. The entries of each probability table were generated randomly using a uniform distribution,
and the table was then normalized. It is also possible to control the amount of determinism in the
network by forcing a percentage det of the CPTs to have only 0 and 1 entries.
Bayesian Network Repository The Bayesian Network Repository4 contains a collection of belief
networks extracted from various real-life domains which are often used for benchmarking probabilistic inference algorithms.
UAI’06 Inference Evaluation Dataset The UAI 2006 Inference Evaluation Dataset5 contains a
collection of random as well as real-world belief networks that were used during the first UAI 2006
Inference Evaluation contest. For our purpose we selected a subset of networks which were derived
from the ISCAS’89 digital circuits benchmark.6 ISCAS’89 circuits are a common benchmark used
in formal verification and diagnosis. Each of these circuits was converted into a Bayesian network
by removing flip-flops and buffers in a standard way, creating a deterministic conditional probability
table for each gate, and putting uniform distributions on the input signals.
10.2 Algorithms
We consider two search-based compilation algorithms, denoted by AOMDD-BCP and AOMDDSAT, respectively, that reduce the context minimal AND/OR graph explored via isomorphism, while
exploiting the determinism (if any) present in the network. The approach we take for handling the
determinism is based on unit resolution over a CNF encoding (i.e., propositional clauses) of the zero
probability tuples of the CPTs. The idea of using unit resolution during search for Bayesian networks was first explored by Allen and Darwiche (2003). AOMDD-BCP is conservative and applies
only unit resolution at each node in the search graph, whereas AOMDD-SAT is more aggressive and
detects inconsistency by running a full SAT solver. We used the zChaff SAT solver (Moskewicz,
Madigan, Zhao, Zhang, & Malik, 2001) for both unit resolution as well as full satisfiability. For
comparison, we also ran an OR version of AOMDD-BCP, called MDD-BCP.
For reference we also report results obtained with the ACE7 compiler. ACE compiles a Bayesian
network into an Arithmetic Circuit (AC) and then uses the AC to answer multiple queries with respect to the network. An arithmetic circuit is a representation that is equivalent to AND/OR graphs
(Mateescu & Dechter, 2007). Each time ACE compiler is invoked, it uses one of two algorithms
as the basis for compilation. First, if an elimination order can be generated for the network having
4.
5.
6.
7.

http://www.cs.huji.ac.il/compbio/Repository/
http://ssli.ee.washington.edu/bilmes/uai06InferenceEvaluation
Available at: http://www.fm.vslib.cz/kes/asic/iscas/
Available at: http://reasoning.cs.ucla.edu/ace

502

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Network

(w*, h)

(n, k)

ACE
#nodes time

MDD w/ BCP
AOMDD w/ BCP
AOMDD w/ SAT
#meta #cm(OR) time
#meta #cm(OR)
time #meta #cm(OR)
time
Bayesian Network Repository
alarm
(4, 13) (37, 4)
1,511 0.01 208,837 682,195 73.35
320
459
0.05
320
459
0.22
cpcs54
(14, 23) (54, 2)
196,933 0.06
- 65,158 66,405
6.97 65,158 66,405
6.97
cpcs179
(8, 14) (179, 4)
67,919 0.05
9,990 32,185 46.56 9,990 32,185 46.56
cpcs360b (20, 27) (360, 2) 5,258,826 1.72
diabetes
(4, 77) (413, 21) 7,615,989 1.81
hailfinder (4, 16) (56, 11)
8,815 0.01
2,068
2,202
0.34 1,893
2,202
1.48
mildew
(4, 13) (35, 100) 823,913 0.39
- 73,666 110,284 1367.81 62,903 65,599 3776.82
mm
(20, 57) (1220, 2)
47,171 1.49
- 38,414 58,144
4.54 30,274 52,523 99.55
munin2
(9, 32) (1003, 21) 2,128,147 1.91
munin3
(9, 32) (1041, 21) 1,226,635 1.27
munin4
(9, 32) (1044, 21) 2,423,009 4.44
pathfinder (6, 11) (109, 63)
18,250 0.05 610,854 1,303,682 352.18
6,984 16,267 30.71 2,265 15,963 50.36
pigs
(11, 26) (441, 3) 636,684 0.19
- 261,920 294,101 174.29 198,284 294,101 1277.72
water
(10, 15) (32, 4)
59,642 0.52 707,283 1,138,096 95.14 18,744 20,926
2.02 18,503 19,225
7.45
UAI’06 Evaluation Dataset
BN 42
(21, 62) (851, 2)
4,860 1.35
- 107,025 341,428 53.50 42,445 43,280 57.36
BN 43
(26, 65) (851, 2)
10,373 1.62
- 1,343,923 1,679,013 1807.63 313,388 314,669 434.38
BN 44
(25, 56) (851, 2)
4,235 1.31
- 155,588 187,589 20.90 47,222 48,540 66.09
BN 45
(22, 54) (851, 2)
12,319 1.50
- 390,795 487,593 68.81 126,182 126,929 177.50
BN 46
(20, 46) (851, 2)
5,912 2.90 1,125,658 1,228,332 94.93 16,711 17,532
1.31 7,337
7,513
5.54
BN 47
(39, 57) (632, 2)
1,448 1.17 42,419 47,128 2.87
1,873
2,663
0.24 1,303
2,614
2.36
BN 49
(40, 60) (632, 2)
1,408 1.16 18,344 19,251 1.32
1,205
1,539
0.19
952
1,515
1.34
BN 51
(41, 68) (632, 2)
1,467 1.15 63,851 68,005 4.22
4,442
5,267
0.50 3,653
5,195
4.58
BN 53
(47, 87) (532, 2)
1,357 0.91 14,210 19,162 1.49
4,819
9,561
0.74 1,365
1,719
1.36
BN 55
(49, 92) (532, 2)
1,288 0.93
5,168
6,088 0.57
1,972
2,816
0.26
790
904
0.75
BN 57
(49, 85) (532, 2)
1,276 0.90 48,436 51,611 3.52
4,036
5,089
0.37
962
1,277
1.01
BN 59
(52, 87) (511, 2)
1,749 0.93 332,030 353,720 25.61 22,963 29,146
2.14 10,655 18,752 14.17
BN 61
(41, 64) (638, 2)
1,411 1.10 20,459 20,806 1.45
1,244
1,589
0.17 1,016
1,528
1.37
BN 63
(53, 95) (511, 2)
1,324 0.90 11,461 17,087 1.28
7,182 14,048
1.07 1,419
2,177
1.69
BN 65
(56, 86) (411, 2)
1,184 0.75
- 20,764 23,102
1.52 12,569 19,778 12.90
BN 67
(54, 88) (411, 2)
1,031 0.74
- 179,067 511,031 154.91
716
1,169
0.78
Positive Random Bayesian Networks (n=75, k=2, p=2, c=65)
r75-1
(12, 22) (75, 2)
67,737 0.31
- 21,619 21,619
2.59 21,619 21,619
2.59
r75-2
(12, 23) (75, 2)
46,703 0.29
- 18,083 18,083
1.88 18,083 18,083
1.88
r75-3
(11, 26) (75, 2)
53,245 0.30
- 18,419 18,419
1.86 18,419 18,419
1.86
r75-4
(11, 19) (75, 2)
28,507 0.29
8,363
8,363
1.16 8,363
8,363
1.16
r75-5
(13, 24) (75, 2)
149,707 0.36
- 42,459 42,459
4.61 42,459 42,459
4.61
r75-6
(14, 24) (75, 2)
132,107 1.19
- 62,621 62,621
6.95 62,621 62,621
6.95
r75-7
(12, 24) (75, 2)
89,913 0.36
- 21,583 21,583
2.42 21,583 21,583
2.42
r75-8
(14, 24) (75, 2)
86,183 0.36
- 49,001 49,001
6.23 49,001 49,001
6.23
r75-9
(11, 19) (75, 2)
29,025 0.30
7,681
7,681
0.81 7,681
7,681
0.81
r75-10
(10, 24) (75, 2)
20,291 0.28
5,905
5,905
0.63 5,905
5,905
0.63
Deterministic Random Bayesian Networks (n=100, k=2, p=2, c=90) and det = 25% of the CPTs containing only 0 and 1 entries
r100d25-1 (13, 31) (100, 2)
68,398 0.38
- 34,035 34,075
2.94 34,035 34,075 12.77
r100d25-2 (16, 28) (100, 2) 150,134 0.46
- 70,241 70,931
7.72 70,241 70,931 27.17
r100d25-3 (16, 29) (100, 2) 705,200 0.96
- 134,079 135,203 13.80 134,079 135,203 50.51
r100d25-4 (16, 31) (100, 2) 161,902 0.54
- 79,366 79,488
7.26 79,366 79,488 28.06
r100d25-5 (16, 29) (100, 2) 185,348 0.53
- 140,627 140,636 14.57 140,627 140,636 49.42
r100d25-6 (18, 28) (100, 2) 148,835 0.66
- 204,232 210,066 17.56 197,134 210,066 92.24
r100d25-7 (16, 29) (100, 2) 264,629 0.60
- 134,344 135,008 14.26 133,850 135,008 55.60
r100d25-8 (17, 27) (100, 2)
65,186 0.46
- 36,857 36,887
2.95 36,857 36,887 11.97
r100d25-9 (14, 27) (100, 2) 140,014 0.40
- 58,421 59,791
6.88 58,172 59,791 23.21
r100d25-10 (16, 27) (100, 2) 173,808 0.58
- 69,110 69,136
7.50 69,110 69,136 26.50

Table 1: Results for experiments with 50 Bayesian networks from 3 problem classes; w∗ =
treewidth, h = depth of pseudo tree, n = number of variables, k = domain size, time
given in seconds; bold types highlight the best results across rows.

503

M ATEESCU , D ECHTER & M ARINESCU

sufficiently small induced width, then tabular variable elimination will be used as the basis. This
algorithm is similar to the one discussed by Chavira and Darwiche (2007), but uses tables to represent factors rather than ADDs. If the induced width is large, then logical model counting will be
used as the basis. Tabular variable elimination is typically efficient when width is small but cannot
handle networks when the width is larger. Logical model counting, on the other hand, incurs more
overhead than tabular variable elimination, but can handle many networks having larger treewidth.
Both tabular variable elimination and logical model counting produce ACs that exploit local structure, leading to efficient online inference. When logical model counting is invoked, it proceeds
by encoding the Bayesian network into a CNF (Chavira & Darwiche, 2005; Chavira, Darwiche, &
Jaeger, 2006), simplifying the CNF, compiling the CNF into a d-DNNF, and then extracting the AC
from the compiled d-DNNF. A dtree over the CNF clauses drives the compilation step.
In all our experiments we report the compilation time in seconds (time), the number of OR
nodes in the context minimal graph explored (#cm), the number of meta-nodes of the resulting
AOMDD (#meta), as well as the size of the AC compiled by ACE (#nodes). For each network we
specify the number of variables (n), domain size (k), induced width (w∗ ) and pseudo tree depth (h).
A ’-’ stands for exceeding the 2GB memory limit by the respective algorithm. The best performance
points are highlighted.
10.3 Evaluation on Bayesian Networks
Table 1 reports the results obtained for experiments with 50 Bayesian networks. The AOMDD
compilers as well as ACE used the min-fill heuristic (Kjæaerulff, 1990) to construct the guiding
pseudo tree and dtree, respectively.
10.3.1 BAYESIAN N ETWORKS R EPOSITORY
We see that ACE is overall the fastest compiler on this domain, outperforming both AOMDD-BCP
and AOMDD-SAT with up to several orders of magnitude (e.g., mildew, pigs). However, the
diagrams compiled by ACE and AOMDD-BCP (resp. AOMDD-SAT) are comparable in size. In
some cases, AOMDD-BCP and AOMDD-SAT were able to compile much smaller diagrams than
ACE. For example, the diagram produced by AOMDD-BCP for the mildew network is 13 times
smaller than the one compiled by ACE. In principle the output produced by ACE and AOMDD
should be similar if both are guided by the same pseudo tree/dtree. Our scheme should be viewed
as a compilation alternative which (1) extends decision diagrams and (2) mimics traces of search
properties that may make this representation accessible. The OR compiler MDD-BCP was able
to compile only 3 out of the 14 test instances, but their sizes were far larger than those produced
by AOMDD-BCP. For instance, on the pathfinder network, AOMDD-BCP outputs a decision
diagram almost 2 orders of magnitude smaller than MDD-BCP.
10.3.2 UAI’06 DATASET
For each of the UAI’06 Dataset instances we picked randomly 30 variables and instantiated as
evidence. We see that ACE is the best performing compiler on this dataset. AOMDD-BCP is
competitive with ACE in terms of compile time only on 9 out the 16 test instances. AOMDD-SAT
is able to compile the smallest diagrams for 6 networks only (e.g., BN 47, BN 49, BN 55, BN 57,
BN 61, BN 67). As before, the difference in size between the compiled data-structures produces
by MDD-BCP and AOMDD-BCP is up to 2 orders of magnitude in favor of the latter.
504

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

10.3.3 R ANDOM N ETWORKS
The problem instances denoted by r75-1 through r75-10 were generated from a class of random
belief networks with parameters (n = 75, k = 2, p = 2, c = 65). Similarly, the instances denoted
by r100d25-1 through r100d25-10 belong to a class with parameters (n = 100, k = 2, p = 2, c =
90). In the latter case, det = 25% of the CPTs are deterministic, namely they contain only 0 and
1 probability tuples. These test instances were compiled without any evidence. We see that on this
domain AOMDD-BCP/AOMDD-SAT were able to compile the smallest diagrams, which were on
average about 2 times smaller than those produced by ACE. However, ACE was again the fastest
compiler. Notice that the OR compiler MDD-BCP ran out of memory in all test cases.
10.4 The Impact of Variable Ordering
As theory dictates, the AOMDD size is influenced by the quality of the guiding pseudo tree. In
addition to the min-fill heuristic we also considered the hypergraph heuristic which constructs the
pseudo tree by recursively decomposing the dual hypergraph associated with the graphical model.
This idea was also explored by Darwiche (2001) for constructing dtrees that guide ACE.
Since both the min-fill and hypergraph partitioning heuristics are randomized (namely ties are
broken randomly), the size of the AOMDD guided by the resulting pseudo tree may vary significantly from one run to the next. Figure 25 displays the AOMDD size using hypergraph and min-fill
based pseudo trees for 6 networks selected from Table 1, over 20 independent runs. We also record
the average induced width and depth obtained for the pseudo trees (see the header of each plot in
Figure 25). We see that the two heuristics do not dominate each other, namely the variance in output
size is quite significant in both cases.
10.5 Memory Usage
Table 2 shows the memory usage (in MBytes) of ACE, AOMDD-BCP and AOMDD-SAT, respectively, on the Bayesian networks from Table 1. We see that in some cases the AOMDD based compilers require far less memory than ACE. For example, on the “mildew” network, both AOMDDBCP and AOMDD-SAT use about 22 MB of memory to compile the AND/OR decision diagram,
while ACE requires as much as 218 MB of memory. Moreover, the compiled AOMDD has in this
case about one order of magnitude fewer nodes than that constructed by ACE. When comparing the
two AND/OR search-based compilers, we observe that on networks with a significant amount of
determinism, such as those from the UAI’06 Evaluation dataset, AOMDD-SAT uses on average two
times less memory than AOMDD-BCP. The most dramatic savings in memory usage due to the aggressive constraint propagation employed by AOMDD-SAT compared with AOMDD-BCP can be
seen on the “BN 67” network. In this case, the difference in memory usage between AOMDD-SAT
and AOMDD-BCP is about 2 orders of magnitude in favor of the former.

11. Related Work
The related work can be viewed along two directions: (1) the work related to the AND/OR search
idea for graphical models and (2) the work related to compilation for graphical models that exploits
problem structure.
An extensive discussion for (1) was provided in the previous work of Dechter and Mateescu
(2007). Since this is not the focus of the paper, we just mention that the AND/OR idea was origi505

M ATEESCU , D ECHTER & M ARINESCU

Figure 25: Effect of variable ordering.

506

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Network

ACE
AOMDD w/ BCP
AOMDD w/ SAT
#nodes memory (MB) #nodes memory (MB) #nodes memory (MB)
Bayesian Network Repository
alarm
1,511
0.00
320
0.0206
320
0.0206
cpcs54
196,933
4.00 65,158
3.4415 65,158
3.4415
cpcs179
67,919
5.00
9,990
1.9263 9,990
1.9263
cpcs360b 5,258,826
204.00
diabetes
7,615,989
449.00
hailfinder
8,815
0.00
2,068
0.1576 1,893
0.1740
mildew
823,913
218.00 73,666
22.5781 62,903
22.1467
mm
47,171
369.00 38,414
1.5719 30,274
1.3711
munin2
2,128,147
202.00
munin3
1,226,635
150.00
munin4
2,423,009
n/a
pathfinder
18,250
10.00
6,984
0.6009 2,265
0.3515
pigs
636,684
31.00 261,920
23.3761 198,284
17.7096
water
59,642
161.00 18,744
1.09578 18,503
1.3258
UAI’06 Evaluation Dataset
BN 42
4,860
n/a 107,025
4.5622 42,445
1.9323
BN 43
10,373
n/a 1,343,923
57.8422 313,388
14.2828
BN 44
4,235
n/a 155,588
6.5613 47,222
2.1628
BN 45
12,319
n/a 390,795
17.9325 126,182
5.7958
BN 46
5,912
n/a
16,711
0.6929 7,337
0.3401
BN 47
1,448
n/a
1,873
0.0720 1,303
0.0583
BN 49
1,408
n/a
1,205
0.0449
952
0.0409
BN 51
1,467
n/a
4,442
0.1689 3,653
0.1633
BN 53
1,357
n/a
4,819
0.1814 1,365
0.0587
BN 55
1,288
n/a
1,972
0.0723
790
0.0336
BN 57
1,276
n/a
4,036
0.1495
962
0.0411
BN 59
1,749
n/a
22,963
0.8501 10,655
0.4587
BN 61
1,411
n/a
1,244
0.0463 1,016
0.0445
BN 63
1,324
n/a
7,182
0.2728 1,419
0.0607
BN 65
1,184
n/a
20,764
0.7539 12,569
0.5384
BN 67
1,031
n/a 179,067
6.9603
716
0.0304
Positive Random Bayesian Networks with parameters (n=75, k=2, p=2, c=65)
r75-1
67,737
1.00 21,619
1.2503 21,619
1.2503
r75-2
46,703
1.00 18,083
0.9957 18,083
0.9957
r75-3
53,245
1.00 18,419
0.9955 18,419
0.9955
r75-4
28,507
1.00
8,363
0.5171 8,363
0.5171
r75-5
149,707
3.00 42,459
2.3299 42,459
2.3299
r75-6
132,107
3.00 62,621
3.4330 62,621
3.4330
r75-7
89,913
2.00 21,583
1.1942 21,583
1.1942
r75-8
86,183
2.00 49,001
2.8130 49,001
2.8130
r75-9
29,025
1.00
7,681
0.4124 7,681
0.4124
r75-10
20,291
1.00
5,905
0.3261 5,905
0.3261
Deterministic Random Bayesian Networks with parameters (n=100, k=2, p=2, c=90)
r100d25-1
68,398
5.00 34,035
1.6290 34,035
1.7149
r100d25-2 150,134
10.00 70,241
3.6129 70,241
3.7810
r100d25-3 705,200
40.00 134,079
6.6372 134,079
6.9873
r100d25-4 161,902
22.00 79,366
3.8113 79,366
4.0079
r100d25-5 185,348
15.00 140,627
7.0839 140,627
7.4660
r100d25-6 148,835
37.00 204,232
9.1757 197,134
9.6542
r100d25-7 264,629
19.00 134,344
6.9619 133,850
6.9961
r100d25-8
65,186
21.00 36,857
1.6872 36,857
1.8278
r100d25-9 140,014
6.00 58,421
3.1058 58,172
3.2055
r100d25-10 173,808
27.00 69,110
3.5578 69,110
3.6636

Table 2: Memory usage in MBytes of ACE, AOMDD-BCP and AOMDD-SAT on the 50 Bayesian
networks from Table 1. Bold types highlight the best performance across rows. The “n/a”
indicates that the respective memory usage statistic was not available from ACE’s output.

507

M ATEESCU , D ECHTER & M ARINESCU

nally developed for heuristic search (Nilsson, 1980). As mentioned in the introduction, the AND/OR
search for graphical models is based on a pseudo tree that spans the graph of the model, similar to
the tree rearrangement of Freuder and Quinn (1985, 1987). The idea was adapted for distributed
constraint satisfaction by Collin et al. (1991, 1999) and more recently by Modi et al. (2005), and was
also shown to be related to graph-based backjumping (Dechter, 1992). This work was extended by
Bayardo and Miranker (1996), Bayardo and Schrag (1997) and more recently applied to optimization tasks by Larrosa et al. (2002). Another version that can be viewed as exploring the AND/OR
graphs was presented recently for constraint satisfaction (Terrioux & Jégou, 2003b) and for optimization (Terrioux & Jégou, 2003a). Similar principles were introduced recently for probabilistic
inference, in algorithm Recursive Conditioning (Darwiche, 2001) as well as in Value Elimination
(Bacchus et al., 2003b, 2003a), and are currently at the core of the most advanced SAT solvers (Sang
et al., 2004).
For direction (2), there are various lines of related research. The formal verification literature,
beginning with the work of Bryant (1986) contains a very large number of papers dedicated to the
study of BDDs. However, BDDs are in fact OR structures (the underlying pseudo tree is a chain)
and do not take advantage of the problem decomposition in an explicit way. The complexity bounds
for OBDDs are based on pathwidth rather than treewidth.
As noted earlier, the work of Bertacco and Damiani (1997) on Disjoint Support Decomposition
(DSD) is related to AND/OR BDDs in various ways. The main common aspect is that both approaches show how structure decomposition can be exploited in a BDD-like representation. DSD
is focused on Boolean functions and can exploit more refined structural information that is inherent to Boolean functions. In contrast, AND/OR BDDs assume only the structure conveyed in the
constraint graph, and are therefore more broadly applicable to any constraint expression and also
to graphical models in general. They allow a simpler and higher level exposition that yields graphbased bounds on the overall size of the generated AOMDD. The full relationship between these two
formalisms should be studied further.
McMillan (1994) introduced the BDD trees, along with the operations for combining them. For
2w
circuits of bounded tree width, BDD trees have a linear space upper bound of O(|g|2w2 ), where
|g| is the size of the circuit g (typically linear in the number of variables) and w is the treewidth. This
bound hides some very large constants to claim the linear dependence on |g| when w is bounded.
However, McMillan maintains that when the input function is a CNF expression BDD-trees have
the same bounds as AND/OR BDDs, namely they are exponential in the treewidth only.
To sketch just a short comparison between McMillan’s BDD trees and AOMMDs, consider an
example where we have a simple pseudo tree with root α, left child β and right child γ. Each of
these nodes may stand for a set of variables. In BDD trees, the assignments to β are grouped into
equivalence classes according to the cofactors generated by them on the remaining α and γ. For
example assignments β1 and β2 are equivalent if they generate the same function on α and γ. The
node β can be represented by a BDD whose leaves are the cofactors. The same is done for γ. The
node α is then represented by a matrix of BDDs, where each column corresponds to a cofactor of β
and each line to a cofactor of γ. By contrast, an AOMDD represents the node α as a BDD whose
leaves are the cofactors (the number of distinct functions on β and γ) and then each cofactor is the
root of a decomposition (an AND node) between β and γ. Moreover, the representations of β (as
descendants of different cofactor of α) are shared as much as possible and the same goes for γ. This
is only a high level description, that becomes slightly more complicated when redundant nodes are
eliminated, but the idea remains the same.
508

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

The AND/OR structure restricted to propositional theories is very similar to deterministic decomposable negation normal form (d-DNNF) (Darwiche & Marquis, 2002; Darwiche, 2002). More
recently, Huang and Darwiche (2005b) used the trace of the DPLL algorithm to generate an OBDD,
and compared with the typical formal verification approach of combining the OBDDs of the input
function according to some schedule. The structures that were investigated in that case are still OR.
This idea is extended in our present work by the AND/OR search compilation algorithm.
McAllester, Collins, and Pereira (2004) introduced the case factor diagrams (CFD), which subsume Markov random fields of bounded tree width and probabilistic context free grammars (PCFG).
CFDs are very much related to the AND/OR graphs. The CFDs target the minimal representation,
by exploiting decomposition (similar to AND nodes) but also by exploiting context sensitive information and allowing dynamic ordering of variables based on context. CFDs do not eliminate the
redundant nodes, and part of the cause is that they use zero suppression. There is no claim about
CFDs being canonical forms, and also no description of how to combine two CFDs.
There are numerous variants of decision diagrams that are designed to represent integer-valued
or real-valued functions. For a comprehensive view we refer the reader to the survey of Drechsler
and Sieling (2001). Algebraic decision diagrams (ADDs) (Bahar et al., 1993) provide a compilation for general real-valued rather than Boolean functions. Their main drawback is that their
size increases very fast if the number of terminals becomes large. There are several approaches
that try to alleviate this problem. However the structure that they capture is still OR, and they
do not exploit decomposition. Some alternatives introduce edge values (or weights) that enable
more subgraph sharing. Edge-valued binary decision diagrams (EVBDDs) (Lai & Sastry, 1992)
use additive weights, and when multiplicative weights are also allowed they are called factored
EVBDDs (FEVBDDs) (Tafertshofer & Pedram, 1997). Another type of BDDs called K*BMDs
(Drechsler, Becker, & Ruppertz, 1996) also use integer weights, both additive and multiplicative
in parallel. ADDs have also been extended to affine ADDs (Sanner & McAllester, 2005), through
affine transformations that can achieve more compression. The result was shown to be beneficial
for probabilistic inference algorithms, such as tree clustering, but they still do not exploit the AND
structure.
More recently, independently and in parallel to our work on AND/OR graphs (Dechter & Mateescu, 2004a, 2004b), Fargier and Vilarem (2004) and Fargier and Marquis (2006, 2007) proposed the compilation of CSPs into tree-driven automata, which have many similarities to our work.
Their main focus is the transition from linear automata to tree automata (similar to that from OR
to AND/OR), and the possible savings for tree-structured networks and hyper-trees of constraints
due to decomposition. Their compilation approach is guided by a tree-decomposition while ours is
guided by a variable-elimination based algorithms. And it is well known that Bucket Elimination
and cluster-tree decomposition are in principle the same (Dechter & Pearl, 1989).
Wilson (2005) extended OBDDs to semi-ring BDDs. The semi-ring treatment is restricted to
the OR search spaces, but allows dynamic variable ordering. It is otherwise very similar in aim and
scope to our AOMDD. When restricting the AOMDD to OR graphs only, the two are closely related,
except that we express BDDs using the Shenoy-Shafer axiomatization that is centered on the two
operation of combination and marginalization rather then on the semi-ring formulation. Minimality
in the formulation of Wilson (2005) is more general allowing merging nodes having different values
and therefore it can capture symmetries (called interchangeability).
Another framework very similar to AOMDDs, that we became aware of only recently, is Probabilistic Decision Graphs (PDG) of Jaeger (2004). This work preceded most of the relevant work
509

M ATEESCU , D ECHTER & M ARINESCU

we discussed above (Fargier & Vilarem, 2004; Wilson, 2005) and went somewhat unnoticed, perhaps due to notational and cultural differences. It is however similar in motivation, framework and
proposed algorithms. We believe our AND/OR framework is more accessible. We define the framework over multi-valued domains, provide greater details in algorithms and complexity analysis,
make an explicit connection with search frameworks, fully address the issues of canonicity as well
as provide an empirical demonstration. In particular, the claim of canonicity for PDGs is similar to
the one we make for AOMDDs of weighted models, in that it is relative to the trees (or forests) that
can represent the given probability distribution.
There is another line of research by Drechsler and his group (e.g. Zuzek, Drechsler, & Thornton,
2000), who use AND/OR graphs for Boolean function representation, that may seem similar to our
approach. However, the semantics and purpose of their AND/OR graphs are different. They are
constructed based on the technique of recursive learning and are used to perform Boolean reasoning,
i.e. to explore the logic consequences of a given assumption based on the structure of the circuit,
especially to derive sets of implicants. The meaning of AND and OR in their case is related to
the meaning of the gates/functions, while in our case the meaning is not related to the semantic of
the functions. The AND/OR enumeration tree that results from a circuit according to Zuzek et al.
(2000) is not related to the AND/OR decomposition that we discuss.

12. Conclusion
We propose the AND/OR multi-valued decision diagram (AOMDD), which emerges from the study
of AND/OR search spaces for graphical models (Dechter & Mateescu, 2004a, 2004b; Mateescu &
Dechter, 2005; Dechter & Mateescu, 2007) and ordered binary decision diagrams (OBDDs) (Bryant,
1986). This data-structure can be used to compile any graphical model.
Graphical models algorithms that are search-based and compiled data-structures such as BDDs
differ primarily by their choices of time vs. memory. When we move from regular OR search
space to an AND/OR search space the spectrum of algorithms available is improved for all time
vs. memory decisions. We believe that the AND/OR search space clarifies the available choices
and helps guide the user into making an informed selection of the algorithm that would fit best the
particular query asked, the specific input function and the available computational resources.
The contribution of our work is: (1) We formally describe the AOMDD and prove that it is a
canonical representation of a constraint network. (2) We extend the AOMDD to general weighted
graphical models. (3) We give a compilation algorithm based on AND/OR search, that saves the
trace of a memory intensive search (the context minimal AND/OR graph), and then reduces it
in one bottom up pass. (4) We describe the APPLY operator that combines two AOMDDs by an
operation and show that its complexity is quadratic in the input, but never worse than exponential
in the treewidth. (5) We give a scheduling order for building the AOMDD of a graphical model
starting with the AOMDDs of its functions which is based on a Variable Elimination algorithm.
This guarantees that the complexity is at most exponential in the induced width (treewidth) along the
ordering. (6) We show how AOMDDs relate to various earlier and recent compilation frameworks,
providing a unifying perspective for all these methods. (7) We introduce the semantic treewidth,
which helps explain why compiled decision diagrams are often much smaller than the worst case
bound. Finally, (8) we provide a preliminary empirical demonstration of the power of the current
scheme.

510

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Acknowledgments
This work was done while Robert Mateescu and Radu Marinescu were at the University of California, Irvine. The authors would like to thank the anonymous reviewers for their constructive
suggestions to improve the paper, David Eppstein for a useful discussion of complexity issues, and
Lars Otten and Natasha Flerova for comments on the final version of the manuscript. This work was
supported by the NSF grants IIS-0412854 and IIS-0713118, and the initial part by the Radcliffe fellowship 2005-2006 (through the partner program), with Harvard undergraduate student John Cobb.

Appendix
Proof of Proposition 1
Consider the level of variable Xi , and the meta-nodes in the list LXi . After one pass through the
meta-nodes in LXi (the inner for loop), there can be no two meta-nodes at the level of Xi in the
AND/OR graph that are isomorphic, because they would have been merged in line 6. Also, during
the same pass through the meta-nodes in LXi all the redundant meta-nodes in LXi are eliminated
in line 8. Processing the meta-nodes in the level of Xi will not create new redundant or isomorphic
meta-nodes in the levels that have been processed before. It follows that the resulting AND/OR
graph is completely reduced. 2
Proof of Theorem 4
The bound on the size follows directly from Theorem 3. The AOMDD size can only be smaller than
∗
the size of the context minimal AND/OR graph, which is bounded by O(n k wT (G) ). To prove the
time bound, we have to rely on the use of the hash table, and the assumption that an efficient implementation allows an access time that is constant. The time bound of AND/OR-S EARCH -AOMDD
∗
is O(n k wT (G) ), from Theorem 3, because it takes time linear in the output (we assume here that
no constraint propagation is performed during search). Procedure B OTTOM U P R EDUCTION (procedure 1) takes time linear in the size of the context minimal AND/OR graph. Therefore, the AOMDD
∗
can be computed in time O(n k wT (G) ), and the result is the same for the algorithm that performs
the reduction during the search. 2
Proof of Proposition 2
The complexity of OBDD (and MDD) apply is known to be quadratic in the input. Namely, the
number of nodes in the output is at most the product of number of nodes in the input. Therefore, the
number of nodes that can appear along one path in the output AOMDD can be at most the product
of the number of nodes in each input, along the same path, |Gfi | · |Ggi |. Summing over all the paths
in T gives the result. 2
Proof of Proposition 3
The argument is identical to the case of MDDs. The recursive calls in APPLY lead to combinations
of one node from Gfaomdd and one node from Ggaomdd (rather than a list of nodes). The number of
total possible such combinations is O(| Gfaomdd | · | Ggaomdd |). 2
Proof of Proposition 4
The recursive calls of APPLY can generate one meta-node in the output for each combination of
511

M ATEESCU , D ECHTER & M ARINESCU

nodes from Gff ∩g and Ggf ∩g . Let’s look at combinations of nodes from Gff ∩g and Ggaomdd \ Ggf ∩g .
The meta-nodes from Ggaomdd \ Ggf ∩g that can participate in such combinations (let’s call this set A)
are only those from levels (of variables) right below Tf ∩g . This is because of the mechanics of the
recursive calls in APPLY. Whenever a node from f that belongs to Gff ∩g is combined with a node
from g that belongs to A, line 15 of APPLY expands the node from f , and the node (or nodes) from
A remain the same. This will happen until there are no more nodes from f that can be combined
with the node (or nodes) from A, and at that point APPLY will simply copy the remaining portion of
its output from Ggaomdd . The size of A is therefore proportional to | Ggf ∩g | (because it is the layer
of metanodes immediately below Ggf ∩g ). A similar argument is valid for the symmetrical case. And
there are no combinations between nodes in Ggaomdd \ Ggf ∩g and Ggaomdd \ Ggf ∩g . The bound follows
from all these arguments. 2
Proof of Proposition 5
The APPLY operation works by constructing the output AOMDD from root to leaves. It first creates a
meta-node for the root variable, and then recursively creates its children metanodes by using APPLY
on the corresponding children of the input. The worst case that can happen is when the output is
not reduced at all, and a recursive call is made for each possible descendant. This corresponds to an
unfolding of the full AND/OR search tree based on the context variables, which is exponential in
the context size. When the APPLY finishes the context variables, and arrives at the first branching in
the bucket pseudo tree, the remaining branches are independent. Similar to the case of OBDDs,
where one function occupies a single place in memory, the APPLY can simply create a link to
the corresponding branches of the inputs (this is what happens in line 4 in the APPLY algorithm).
Therefore, the time and space complexity is at most exponential in the context size. 2
Proof of Theorem 5
The space complexity is governed by that of BE. Since an AOMDD never requires more space than
∗
that of a full exponential table (or a tree), it follows that BE-AOMDD only needs space O(n k w ).
The size of the output AOMDD is also bounded, per layers, by the number of assignments to the
context of that layer (namely, by the size of the context minimal AND/OR graph). Therefore,
∗
because context size is bounded by treewidth, it follows that the output has size O(n k w ). The
time complexity follows from Proposition 5, and from the fact that the number of functions in a
bucket cannot exceed r, the original number of functions.
2
Proof of Proposition 6
It suffices to prove the proposition for m = 2. The general result can then be obtained by induction.
It is essential that the function is defined by a constraint network (i.e., the values are only 0 or 1),
and that the function takes value 1 at least for one assignment. The value 1 denotes consistent assignments (solutions), while 0 denotes inconsistent assignments. Suppose f = f1 ⊗f2 . Let’s denote
by x a full assignment to X, and by x1 and x2 the projection of x over X1 and X2 , respectively. We
can write x = x1 x2 (concatenation of partial assignments). It follows that f (x) = f1 (x1 ) ∗ f2 (x2 ).
Therefore, if f (x) = 1, it must be that f1 (x1 ) = 1 and f2 (x2 ) = 1. We claim that for any x1 ,
f1 (x1 ) = 1 only if there exists some x2 such that f (x1 x2 ) = 1. Suppose by contradiction that there
exist some x1 such that f1 (x1 ) = 1 and f (x1 x2 ) = 0 for any other x2 . Since f is not always zero,

512

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

it follows that f2 is not always zero, and therefore there must be some x2 for which f2 (x2 ) = 1.
This leads to a contradiction, and therefore the functions f1 and f2 are uniquely defined by f . 2
Proof of Theorem 6
The proof is by structural induction over the depth of the pseudo tree T . It follows the canonicity
proofs for OBDDs (Bryant, 1986) and MDDs (Srinivasan et al., 1990), but extends them from linear
orderings to tree orderings that capture function decomposition according to the pseudo tree T . The
depth of T , along each of its paths from root to a leaf, is actually the size of the dependency set, or
the set of variables on which the value of the function depends. Remember that the AOMDD is an
AND/OR graph that is completely reduced. We will use the word function, denoted by f , to refer
to the universal relation, or its characteristic function, defined by the constraint network.
Assume the depth of T is 0. This means that the function does not depend on any variable,
and must be one of the constants 0 or 1. Suppose the function is the constant 0. Then, it must be
that the AOMDD does not contain the terminal meta-node 1, since all the nodes must be reachable
along some path, and it would mean that the function can also evaluate to 1. Suppose the AOMDD
contains a nonterminal meta-node, say labeled with X, where X can take k different values. It must
be that all the k children meta-nodes of X are the terminal meta-node 0. If there are more than one
terminal 0, then the AOMDD is not completely reduced. If there is only one 0, it follows that the
meta-node labeled with X is redundant. Therefore, from all the above, it follows that the AOMDD
representing the constant 0 is made of only the terminal 0. This is unique, and contains the smallest
number of nodes. A similar argument applies for the constant 1.
Now, suppose that the statement of the theorem holds for any constraint network that admits a
pseudo tree of depth strictly smaller than p, and that we have a constraint network with a pseudo
tree of depth equal to p, where p > 0. Let X be the root of T , having domain {x1 , . . . , xk }. We
denote by fi , where i ∈ {1, . . . , k}, the functions defined by the restricted constraint network for
X = xi , namely fi = f |X=xi . Let Y1 , . . . , Ym be the children of X in T . Suppose we have two
AOMDDs of f , denoted by G and G 0 . We will show that these two AND/OR graphs are isomorphic.
The functions fi can be decomposed according to the pseudo tree T when the root X is removed.
This can in fact be a forest of independent pseudo trees (they do not share any variables), rooted by
Y1 , . . . , Ym . Based on Proposition 6, there is a unique decomposition fi = fiY1 ∗ . . . ∗ fiYm , for all
Y
i ∈ {1, . . . , k}. Based on the induction hypothesis, each of the function fi j has a unique AOMDD.
In the AND/OR graphs G and G 0 , if we look at the subgraphs descending from X = xi , they both are
completely reduced and define the same function, fi , therefore there exists an isomorphic mapping
σi between them. Let v be the root metanode of G and v 0 the root of G 0 . We claim that G and G 0 are
isomorphic according to the following mapping:
 0
v,
if u = v;
σ(u) =
σi (u), if u is in the subgraph rooted by hX, xi i.
To prove this, we have to show that σ is well defined, and that it is an isomorphic mapping.
If a meta-node u in G is contained in both subgraphs rooted by hX, xi i and hX, xj i, Then the
AND/OR graphs rooted by σi (u) and σj (u) are isomorphic to the one rooted at u, and therefore to
each other. Since G 0 is completely reduced, it does not contain isomorphic subgraphs, and therefore
σi (u) = σj (u). Therefore σ is well defined.
We can now show that σ is a bijection. To show that it is one-to-one, assume two distinct metanodes u1 and u2 in G, with σ(u1 ) = σ(u2 ). Then, the subgraphs rooted by u1 and u2 are isomorphic
513

M ATEESCU , D ECHTER & M ARINESCU

to the subgraph rooted by σ(u1 ), and therefore to each other. Since G is completely reduced, it must
be that u1 = u2 . The fact that σ is onto and is an isomorphic mapping follows from its definition and
from the fact that each σi is onto and the only new node is the root meta-node. Since the AOMDDs
only contain one root meta-node (more than one root would lead to the conclusion that the root
meta-nodes are isomorphic and should be merged), we conclude that G and G 0 are isomorphic.
Finally, we can show that among all the AND/OR graphs representing f , the AOMDD has
minimal number of meta-nodes. Suppose G is an AND/OR graph that represents f , with minimal
number of meta-nodes, but without being an AOMDD. Namely, it is not completely reduced. Any
reduction rule would transform G into an AND/OR graph with smaller number of meta-nodes,
leading to a contradiction. Therefore, G must be the unique AOMDD that represents f . 2
Proof of Corollary 1
The proof of Theorem 6 did not rely on the scopes that define the constraint network. As long as the
network admits the decomposition induced by the pseudo tree T , the universal function defined by
the constraint network will always have the same AOMDD, and therefore any constraint network
equivalent to it that admits T will also have the same AOMDD. 2
Proof of Theorem 7
The constant that is associated with the root is actually the sum of the weights of all solutions. This
can be derived from the definition of the weighted AOMDD. The weights of each meta-node are
normalized (they sum to 1), therefore the values computed for each OR node by AND/OR search
is always 1 (when the task is computing
P the sum of all solution weights). Therefore, the constant
of the weighted AOMDD is always x w(x) regardless of the graphical model. We will prove that
weighted AOMDDs are canonical for functions that are normalized.
Assume we have two different weighted AOMDDs, denoted by G 1 and G 2 , for the same normalized function f . Let the root variable be A, with the domain {a1 , . . . , ak }. Let x denote a full
assignment to all the variables. Similar to the above argument for the root constant,
P because all
the meta-nodes have normalized weights, it follows that w1 (A, a1 ) = w2 (A, a1 ) = x|A=a1 f (x).
The superscript of w1 and w2 indicates the AOMDD, and the summation is over all possible assignments restricted to A = a1 . It follows that the root meta-nodes are identical. For each value of the
root variable, the restricted functions represented in G 1 and G 2 are identical, and we will recursively
apply the same argument as above.
However, for the proof to be complete, we have to discuss the case when a restricted function
is decomposed into independent functions, according to the pseudo tree. Suppose there are two
independent components, rooted by B and C. If one of them is the 0 function, it follows that the
entire function is 0. We will prove that the meta-nodes of B in G 1 and G 2 are identical. If B has only
one value b1 extendable to a solution, its weight must be 1 in both meta-nodes, so the meta-nodes
are identical. If B has more than one value, suppose without loss of generality that the weights are
different for the first value b1 , and
w1 (B, b1 ) > w2 (B, b1 ).

(1)

Since f 6= 0, there must be a value C = c1 such that B = b1 , C = c1 can be extended to a full
solution. The sum of the weights of all these possible extensions is
X
f (x) = w1 (B, b1 ) ∗ w1 (C, c1 ) = w2 (B, b1 ) ∗ w2 (C, c1 ).
(2)
x|B=b1 ,C=c1

514

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

From Equations 1 and 2 and the fact that the weight are non-zero, it follows that
w1 (C, c1 ) < w2 (C, c1 ).

(3)

From Equation 1, the fact that B has more than one value and the fact that the weights of B are
normalized, it follows that there should be a value b2 such that
w1 (B, b2 ) < w2 (B, b2 ).

(4)

From Equations 3 and 4, it follows that
w1 (B, b2 ) ∗ w1 (C, c1 ) < w2 (B, b2 ) ∗ w2 (C, c1 ).

(5)

However, both sides of
P the Equation 5 represent the sum of weights of all solutions when B =
b2 , C = c1 , namely x|B=b2 ,C=c1 f (x), leading to a contradiction. Therefore, it must be that
Equation 1 is false. Continuing the same argument for all values of B, it follows that the metanodes of B are identical, and similarly the meta-nodes of C are identical.
If the decomposition has more than two components, the same argument applies, when B is the
first component and C is a meta-variable that combines all the other components.
2
Proof of Theorem 8
Consider the well known NP-complete problem of 3- COLORING: Given a graph G, is there a
3-coloring of G? Namely, can we color its vertices using only three colors, such that any two
adjacent vertices have different colors? We will reduce 3- COLORING to the problem of computing
the semantic treewidth of a graphical model. Let H be a graph that is 3-colorable, and has a nontrivial semantic treewidth. It is easy to build examples for H. If G is 3-colorable, then G ∪ H is
also 3-colorable and will have a non-trivial semantic treewidth, because adding G will not simplify
the task of describing the colorings of H. However, if G is not 3-colorable, then G ∪ H is also not
3-colorable, and will have a semantic treewidth of zero. 2
Proof of Proposition 7
Since AOMDDs are canonical representations of graphical models, it follows that the graphical
model for which the actual semantic treewidth is realized will have the same AOMDD as M, and
therefore the AOMDD is bounded exponentially in the semantic treewidth. 2

References
Akers, S. (1978). Binary decision diagrams. IEEE Transactions on Computers, C-27(6), 509–516.
Allen, D., & Darwiche, A. (2003). New advances in inference by recursive conditioning. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence (UAI’03), pp.
2–10.
Bacchus, F., Dalmao, S., & Pitassi, T. (2003a). Algorithms and complexity results for #SAT and
Bayesian inference. In Proceedings of the 44th Annual IEEE Symposium on Foundations of
Computer Science (FOCS’03), pp. 340–351.
515

M ATEESCU , D ECHTER & M ARINESCU

Bacchus, F., Dalmao, S., & Pitassi, T. (2003b). Value elimination: Bayesian inference via backtracking search. In Proceedings of the Nineteenth Conference on Uncertainty in Artificial
Intelligence (UAI’03), pp. 20–28.
Bahar, R., Frohm, E., Gaona, C., Hachtel, G., Macii, E., Pardo, A., & Somenzi, F. (1993). Algebraic decision diagrams and their applications. In IEEE/ACM International Conference on
Computer-Aided Design (ICCAD’93), pp. 188–191.
Bayardo, R., & Miranker, D. (1996). A complexity analysis of space-bound learning algorithms for
the constraint satisfaction problem. In Proceedings of the Thirteenth National Conference on
Artificial Intelligence (AAAI’96), pp. 298–304.
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques to solve real world SAT
instances. In Proceedings of the Fourteenth National Conference on Artificial Intelligence
(AAAI’97), pp. 203–208.
Bertacco, V., & Damiani, M. (1997). The disjunctive decomposition of logic functions. In
IEEE/ACM International Conference on Computer-Aided Design (ICCAD’97), pp. 78–82.
Bodlaender, H. L., & Gilbert, J. R. (1991). Approximating treewidth, pathwidth and minimum
elimination tree height. Tech. rep., Utrecht University.
Bryant, R. E. (1986). Graph-based algorithms for Boolean function manipulation. IEEE Transactions on Computers, 35, 677–691.
Cadoli, M., & Donini, F. M. (1997). A survey on knowledge compilation. AI Communications,
10(3-4), 137–150.
Chavira, M., & Darwiche, A. (2005). Compiling Bayesian networks with local structure. In
Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI’05), pp. 1306–1312.
Chavira, M., & Darwiche, A. (2007). Compiling Bayesian networks using variable elimination.
In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence (IJCAI’07), pp. 2443–2449.
Chavira, M., Darwiche, A., & Jaeger, M. (2006). Compiling relational Bayesian networks for exact
inference. International Journal of Approximate Reasoning, 42(1-2), 4–20.
Clarke, E., Grumberg, O., & Peled, D. (1999). Model Checking. MIT Press.
Collin, Z., Dechter, R., & Katz, S. (1991). On the feasibility of distributed constraint satisfaction.
In Proceedings of the Twelfth International Conference of Artificial Intelligence (IJCAI’91),
pp. 318–324.
Collin, Z., Dechter, R., & Katz, S. (1999). Self-stabilizing distributed constraint satisfaction. The
Chicago Journal of Theoretical Computer Science, 115, special issue on self-stabilization.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 125(1-2), 5–41.

516

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Darwiche, A. (2002). A logical approach to factoring belief networks. In Proceedings of the
Eighth International Conference on Principles of Knowledge Representation and Reasoning
(KR’02), pp. 409–420.
Darwiche, A., & Marquis, P. (2002). A knowledge compilation map. Journal of Artificial Intelligence Research (JAIR), 17, 229–264.
Dechter, R. (1992). Constraint networks. Encyclopedia of Artificial Intelligence, 276–285.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial Intelligence,
113, 41–85.
Dechter, R., & Mateescu, R. (2007). AND/OR search spaces for graphical models. Artificial Intelligence, 171(2-3), 73–106.
Dechter, R., & Mateescu, R. (2004a). Mixtures of deterministic-probabilistic networks and their
AND/OR search space. In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI’04), pp. 120–129.
Dechter, R., & Mateescu, R. (2004b). The impact of AND/OR search spaces on constraint satisfaction and counting. In Proceedings of the Tenth International Conference on Principles and
Practice of Constraint Programming (CP’04), pp. 731–736.
Dechter, R., & Pearl, J. (1989). Tree clustering for constraint networks. Artificial Intelligence, 38,
353–366.
Drechsler, R., Becker, B., & Ruppertz, S. (1996). K*BMDs: A new data structure for verification.
In Proceedings of the 1996 European conference on Design and Test (ED&TC’96), pp. 2–8.
Drechsler, R., & Sieling, D. (2001). Binary decision diagrams in theory and practice. International
Journal on Software Tools for Technology Transfer (STTT), 3(2), 112–136.
Fargier, H., & Marquis, P. (2006). On the use of partially ordered decision graphs in knowledge
compilation and quantified boolean formulae. In Proceedings of The Twenty-First National
Conference on Artificial Intelligence (AAAI’06), pp. 42–47.
Fargier, H., & Marquis, P. (2007). On valued negation normal form formulas. In Proceedings of the
Twentieth International Joint Conference on Artificial Intelligence (IJCAI’07), pp. 360–365.
Fargier, H., & Vilarem, M. (2004). Compiling CSPs into tree-driven automata for interactive solving. Constraints, 9(4), 263–287.
Fishburn, P. C. (1970). Utility Theory for Decision Making. Wiley, NewYork.
Freuder, E. C., & Quinn, M. J. (1985). Taking advantage of stable sets of variables in constraint
satisfaction problems. In Proceedings of the Ninth International Joint Conference on Artificial
Intelligence (IJCAI’85), pp. 1076–1078.
Freuder, E. C., & Quinn, M. J. (1987). The use of lineal spanning trees to represent constraint
satisfaction problems. Tech. rep. 87-41, University of New Hampshire, Durham.
517

M ATEESCU , D ECHTER & M ARINESCU

Huang, J., & Darwiche, A. (2005a). On compiling system models for faster and more scalable diagnosis. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI’05),
pp. 300–306.
Huang, J., & Darwiche, A. (2005b). DPLL with a trace: From SAT to knowledge compilation.
In Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence
(IJCAI’05), pp. 156–162.
Jaeger, M. (2004). Probabilistic decision graphs - combining verification and AI techniques for
probabilistic inference. International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems, 12, 19–42.
Kask, K., Dechter, R., Larrosa, J., & Dechter, A. (2005). Unifying cluster-tree decompositions for
reasoning in graphical models. Artificial Intelligence, 166 (1-2), 165–193.
Kjæaerulff, U. (1990). Triangulation of graph-based algorithms giving small total state space. Tech.
rep., University of Aalborg, Denmark.
Korf, R., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence, 134(1-2),
9–22.
Lai, Y.-T., & Sastry, S. (1992). Edge-valued binary decision for multi-level hierarchical verification.
In Proceedings of the Twenty Nineth Design Automation Conference, pp. 608–613.
Larrosa, J., Meseguer, P., & Sanchez, M. (2002). Pseudo-tree search with soft constraints. In
Proceedings of the European Conference on Artificial Intelligence (ECAI’02), pp. 131–135.
Lee, C. (1959). Representation of switching circuits by binary-decision programs. Bell System
Technical Journal, 38, 985–999.
Mateescu, R., & Dechter, R. (2005). The relationship between AND/OR search and variable elimination. In Proceedings of the Twenty First Conference on Uncertainty in Artificial Intelligence
(UAI’05), pp. 380–387.
Mateescu, R., & Dechter, R. (2007). AND/OR multi-valued decision diagrams (AOMDDs) for
weighted graphical models. In Proceedings of the Twenty Third Conference on Uncertainty
in Artificial Intelligence (UAI’07), pp. 276–284.
McAllester, D., Collins, M., & Pereira, F. (2004). Case-factor diagrams for structured probabilistic
modeling. In Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence (UAI’04), pp. 382–391.
McMillan, K. L. (1993). Symbolic Model Checking. Kluwer Academic.
McMillan, K. L. (1994). Hierarchical representation of discrete functions with application to model
checking. In Computer Aided Verification, pp. 41–54.
Modi, P. J., Shen, W., Tambe, M., & Yokoo, M. (2005). ADOPT: asynchronous distributed constraint optimization with quality guarantees. Artificial Intelligence, 161, 149–180.

518

AND/OR M ULTI -VALUED D ECISION D IAGRAMS (AOMDD S ) FOR G RAPHICAL M ODELS

Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering an
efficient SAT solver. In Proceedings of the Thirty Eighth Design Automation Conference, pp.
530–535.
Nilsson, N. J. (1980). Principles of Artificial Intelligence. Tioga, Palo Alto, CA.
Palacios, H., Bonet, B., Darwiche, A., & Geffner, H. (2005). Pruning conformant plans by counting models on compiled d-DNNF representations. In Proceedings of the 15th International
Conference on Planning and Scheduling (ICAPS’05), pp. 141–150.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Sang, T., Bacchus, F., Beame, P., Kautz, H., & Pitassi, T. (2004). Combining component caching
and clause learning for effective model counting. In Proceedings of the Seventh International
Conference on Theory and Applications of Satisfiability Testing (SAT’04).
Sanner, S., & McAllester, D. (2005). Affine algebraic decision diagrams (AADDs) and their application to structured probabilistic inference. In Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence (IJCAI’05), pp. 1384–1390.
Selman, B., & Kautz, H. (1996). Knowledge compilation and theory approximation. Journal of the
ACM, 43(2), 193–224.
Shenoy, P. (1992). Valuation-based systems for Bayesian decision analysis. Operations Research,
40, 463–484.
Srinivasan, A., Kam, T., Malik, S., & Brayton, R. K. (1990). Algorithms for discrete function
manipulation. In International conference on CAD, pp. 92–95.
Tafertshofer, P., & Pedram, M. (1997). Factored edge-valued binary decision diagrams. Formal
Methods in System Design, 10(2-3), 243–270.
Terrioux, C., & Jégou, P. (2003a). Bounded backtracking for the valued constraint satisfaction
problems. In Proceedings of the Ninth International Conference on Principles and Practice
of Constraint Programming (CP’03), pp. 709–723.
Terrioux, C., & Jégou, P. (2003b). Hybrid backtracking bounded by tree-decomposition of constraint networks. Artificial Intelligence, 146, 43–75.
Wilson, N. (2005). Decision diagrams for the computation of semiring valuations. In Proceedings
of the Nineteenth International Joint Conference on Artificial Intelligence (IJCAI’05), pp.
331–336.
Zuzek, A., Drechsler, R., & Thornton, M. (2000). Boolean function representation and spectral
characterization using AND/OR graphs. Integration, The VLSI journal, 29, 101–116.

519

Journal of Artificial Intelligence Research 33 (2008) 1-31

Submitted 05/08; published 09/08

Anytime Induction of Low-cost, Low-error Classifiers:
a Sampling-based Approach
Saher Esmeir
Shaul Markovitch

esaher@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
Technion–Israel Institute of Technology
Haifa 32000, Israel

Abstract
Machine learning techniques are gaining prevalence in the production of a wide range of
classifiers for complex real-world applications with nonuniform testing and misclassification
costs. The increasing complexity of these applications poses a real challenge to resource
management during learning and classification. In this work we introduce ACT (anytime
cost-sensitive tree learner), a novel framework for operating in such complex environments.
ACT is an anytime algorithm that allows learning time to be increased in return for lower
classification costs. It builds a tree top-down and exploits additional time resources to
obtain better estimations for the utility of the different candidate splits. Using sampling
techniques, ACT approximates the cost of the subtree under each candidate split and favors
the one with a minimal cost. As a stochastic algorithm, ACT is expected to be able to
escape local minima, into which greedy methods may be trapped. Experiments with a
variety of datasets were conducted to compare ACT to the state-of-the-art cost-sensitive
tree learners. The results show that for the majority of domains ACT produces significantly
less costly trees. ACT also exhibits good anytime behavior with diminishing returns.

1. Introduction
Traditionally, machine learning algorithms have focused on the induction of models with
low expected error. In many real-word applications, however, several additional constraints
should be considered. Assume, for example, that a medical center has decided to use machine learning techniques to build a diagnostic tool for heart disease. The comprehensibility
of decision tree models (Hastie, Tibshirani, & Friedman, 2001, chap. 9) makes them the preferred choice on which to base this tool. Figure 1 shows three possible trees. The first tree
(upper-left) makes decisions using only the results of cardiac catheterization (heart cath).
This tree is expected to be highly accurate. Nevertheless, the high costs and risks associated with the heart cath procedure make this decision tree impractical. The second tree
(lower-left) dispenses with the need for cardiac catheterization and reaches a decision based
on a single, simple, inexpensive test: whether or not the patient complains of chest pain.
Such a tree would be highly accurate: most people do not experience chest pain and are
indeed healthy. The tree, however, does not distinguish between the costs of different types
of errors. While a false positive prediction might result in extra treatments, a false negative
prediction might put a person’s life at risk. Therefore, a third tree (right) is preferred, one
that attempts to minimize test costs and misclassification costs simultaneously.
c
°2008
AI Access Foundation. All rights reserved.

Esmeir & Markovitch

heart cath
no

normal

chest pain
no

yes

yes

blood
pressure

alerting

no

blood
pressure
no

yes

cardiac
stress

normal

normal

no

normal

normal

heart cath

yes

no

no

yes

yes

alerting

yes

normal

alerting

normal

heart cath

chest pain
no

yes

alerting

Figure 1: Three possible decision trees for diagnosis of heart diseases. The upper-left tree
bases its decision solely on heart cath and is therefore accurate but prohibitively
expensive. The lower-left tree dispenses with the need for heart cath and reaches
a decision using a single, simple, and inexpensive test: whether or not the patient
complains of chest pain. Such a tree would be highly accurate but does not
distinguish between the costs of the different error types. The third (right-hand)
tree is preferable: it attempts to minimize test costs and misclassification costs
simultaneously.

cost(a1-10) = $$

a9
a10
0

1

a7

a10
1

a6

a9

0

cost(a1-8) = $$
cost(a9,10) = $$$$$$

a1

0

a9
1

1

a4
0

0

a4
1

1

0

Figure 2: Left: an example of a difficulty greedy learners might face. Right: an example of
the importance of context-based feature evaluation.

Finding a tree with the lowest expected total cost is at least NP-complete.1 As in the
cost insensitive case, a greedy heuristic can be used to bias the search towards low-cost trees.
Decision Trees with Minimal Cost (DTMC), a greedy method that attempts to minimize
1. Finding the smallest consistent tree, which is an easier problem, is NP-complete (Hyafil & Rivest, 1976).

2

Anytime Induction of Low-cost, Low-error Classifiers

both types of costs simultaneously, has been recently introduced (Ling, Yang, Wang, &
Zhang, 2004; Sheng, Ling, Ni, & Zhang, 2006). A tree is built top-down, and a greedy split
criterion that takes into account both testing and misclassification costs is used. The basic
idea is to estimate the immediate reduction in total cost after each split, and to prefer the
split with the maximal reduction. If no split reduces the cost on the training data, the
induction process is stopped.
Although efficient, the DTMC approach can be trapped into a local minimum and
produce trees that are not globally optimal. For example, consider the concept and costs
described in Figure 2 (left). There are 10 attributes, of which only a9 and a10 are relevant.
The cost of a9 and a10 , however, is significantly higher than the others. Such high costs may
hide the usefulness of a9 and a10 , and mislead the learner into repeatedly splitting on a1−8 ,
which would result in a large, expensive tree. The problem would be intensified if a9 and
a10 were interdependent, with a low immediate information gain (e.g., a9 ⊕ a10 ). In that
case, even if the costs were uniform, a local measure might fail to recognize the relevance
of a9 and a10 .
DTMC is appealing when learning resources are very limited. However, it requires a
fixed runtime and cannot exploit additional resources to escape local minima. In many
real-life applications, we are willing to wait longer if a better tree can be induced (Esmeir &
Markovitch, 2006). For example, the importance of the model in saving patients’ lives may
convince the medical center to allocate 1 month to learn it. Algorithms that can exploit
additional time to produce better solutions are called anytime algorithms (Boddy & Dean,
1994).
The ICET algorithm (Turney, 1995) was a pioneer in non-greedy search for a tree that
minimizes test and misclassification costs. ICET uses genetic search to produce a new
set of costs that reflects both the original costs and the contribution of each attribute in
reducing misclassification costs. Then it builds a tree using the EG2 algorithm (Nunez,
1991) but with the evolved costs instead of the original ones. EG2 is a greedy cost-sensitive
algorithm that builds a tree top-down and evaluates candidate splits by considering both
the information gain they yield and their measurement costs. It does not, however, take
into account the misclassification cost of the problem.
ICET was shown to significantly outperform greedy tree learners, producing trees of
lower total cost. ICET can use additional time resources to produce more generations and
hence widen its search in the space of costs. Because the genetic operations are randomized,
ICET is more likely to escape local minima – into which EG2 with the original costs might
be trapped. Nevertheless, two shortcomings limit ICET’s ability to benefit from extra time.
First, after the search phase, it uses the greedy EG2 algorithm to build the final tree.
But because EG2 prefers attributes with high information gain (and low test cost), the
usefulness of highly relevant attributes may be underestimated by the greedy measure in the
case of hard-to-learn concepts where attribute interdependency is hidden. This will result
in more expensive trees. Second, even if ICET overcomes the above problem by randomly
reweighting the attributes, it searches the space of parameters globally, regardless of the
context in the tree. This imposes a problem if an attribute is important in one subtree but
useless in another. To better understand these shortcomings, consider the concept described
by the tree in Figure 2 (right). There are 10 attributes with similar costs. The value of a1
determines whether the target concept is a7 ⊕ a9 or a4 ⊕ a6 . The interdependencies result in
3

Esmeir & Markovitch

a low gain for all attributes. Because ICET assigns costs globally, the attributes will have
similar costs as well. Therefore, ICET will not be able to recognize which one is relevant in
which context. If the irrelevant attributes are cheaper, the problem is intensified and the
model might end up relying on irrelevant attributes.
Recently, we have introduced the cost-insensitive LSID3 algorithm, which can induce
more accurate trees when allocated more time (Esmeir & Markovitch, 2007a). The algorithm evaluates a candidate split by estimating the size of the smallest consistent tree under
it. The estimation is based on sampling the space of consistent trees, where the size of the
sample is determined in advance according to the allocated time. LSID3 is not designed,
however, to minimize test and misclassification costs. In this work we build on LSID3 and
propose ACT, an anytime cost-sensitive tree learner that can exploit additional time to
produce lower-cost trees. Applying the sampling mechanism in the cost-sensitive setup,
however, is not trivial and imposes three major challenges: (1) how to produce the sample,
(2) how to evaluate the sampled trees, and (3) how to prune the induced trees. In Section
3 we show how these obstacles may be overcome.
In Section 4 we report an extensive set of experiments that compares ACT to several
decision tree learners using a variety of datasets with costs assigned by human experts
or automatically. The results show that ACT is significantly better for the majority of
problems. In addition, ACT is shown to exhibit good anytime behavior with diminishing
returns.

2. Cost-Sensitive Classification
Offline concept learning consists of two stages: the learning stage, where a set of labeled
examples is used to induce a classifier; and the classification stage, where the induced
classifier is used to classify unlabeled instances. These two stages involve different types
of costs (Turney, 2000). Our primary goal in this work is to trade learning speed for a
reduction in test and misclassification costs. To make the problem well defined, we need to
specify: (1) how misclassification costs are represented, (2) how test costs are calculated,
and (3) how we should combine both types of cost.
To answer these questions, we adopt the model described by Turney (1995). In a problem
with |C| different classes, a misclassification cost matrix M is a |C| × |C| matrix whose Mi,j
entry defines the penalty of assigning the class ci to an instance that actually belongs to the
class cj . Typically, entries on the main diagonal of a classification cost matrix (no error)
are all zero.
When classifying an example e using a tree T , we propagate e down the tree along a
single path from the root of T to one of its leaves. Let Θ(T, e) be the set of tests along this
path. We denote by cost(θ) the cost of administering the test θ. The testing cost of e in T
P
is therefore tcost(T, e) = θ∈Θ cost(θ). Note that we use sets notation because tests that
appear several times are charged for only once. In addition, the model described by Turney
(1995) handles two special test types, namely grouped and delayed tests.
Grouped Tests. Some tests share a common cost, for which we would like to charge only
once. Typically, the test also has an extra (possibly different) cost. For example, consider a
tree path with tests like cholesterol level and glucose level. For both values to be measured,
a blood test is needed. Taking blood samples to measure the cholesterol level clearly lowers
4

Anytime Induction of Low-cost, Low-error Classifiers

the cost of measuring the glucose level. Formally, each test possibly belongs to a group.2 If
it’s the first test from the group to be administered, we charge for the full cost. If another
test from the same group has already been administered earlier in the decision path, we
charge only for the marginal cost.
Delayed Tests. Sometimes the outcome of a test cannot be obtained immediately, e.g.,
lab test results. Such tests, called delayed tests, force us to wait until the outcome is
available. Alternatively, Turney (1995) suggests taking into account all possible outcomes:
when a delayed test is encountered, all the tests in the subtree under it are administered
and charged for. Once the result of the delayed test is available, the prediction is at hand.
One problem with this setup is that it follows all paths in the subtree, regardless of the
outcome of non-delayed costs. Moreover, it is not possible to distinguish between the delays
different tests impose: for example, one result might be ready after several minutes while
another only after a few days. In this work we do not handle delayed tests, but we do
explain how ACT can be modified to take them into account.
After the test and misclassification costs have been measured, an important question
remains: How should we combine them? Following Turney (1995), we assume that both
cost types are given in the same scale. A more general model would require a utility function
that combines both types. Qin, Zhang, and Zhang (2004) presented a method to handle the
two kinds of cost scales by setting a maximal budget for one kind of cost and minimizing
the other one. Alternatively, patient preferences can be elicited and summarized as a utility
function (Lenert & Soetikno, 1997).
Note that the algorithm we introduce in this paper can be adapted to any cost model. An
important property of our cost-sensitive setup is that maximizing generalization accuracy,
which is the goal of most existing learners, can be viewed as a special case: when accuracy
is the only objective, test costs are ignored and misclassification cost is uniform.

3. The ACT Algorithm
ACT, our proposed anytime framework for induction of cost-sensitive decision trees, builds
on the recently introduced LSID3 algorithm. LSID3 adopts the general top-down scheme
for induction of decision trees (TDIDT): it starts from the entire set of training examples,
partitions it into subsets by testing the value of an attribute, and then recursively builds
subtrees. Unlike greedy inducers, LSID3 invests more time resources for making better split
decisions. For every candidate split, LSID3 attempts to estimate the size of the resulting
subtree were the split to take place. Following Occam’s razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987; Esmeir & Markovitch, 2007b), it favors the one with the smallest
expected size.
The estimation is based on a biased sample of the space of trees rooted at the evaluated
attribute. The sample is obtained using a stochastic version of ID3 (Quinlan, 1986), which
we call SID3. In SID3, rather than choosing an attribute that maximizes the information
gain ∆I (as in ID3), we choose the splitting attribute semi-randomly. The likelihood that
an attribute will be chosen is proportional to its information gain. Due to its randomization,
2. In this model each test may belong to a single group. However, it is easy to extend our work to allow
tests that belong to several groups.

5

Esmeir & Markovitch

Procedure LSID3-Choose-Attribute(E, A, r)
If r = 0
Return ID3-Choose-Attribute(E, A)
Foreach a ∈ A
Foreach vi ∈ domain(a)
Ei ← {e ∈ E | a(e) = vi }
mini ← ∞
Repeat r times
T ← SID3(Ei , A − {a})
mini ← min (mini , Size(T ))
P|domain(a)|
totala ← i=1
mini
Return a for which totala is minimal
Figure 3: Attribute selection in LSID3
repeated invocations of SID3 result in different trees. For each candidate attribute a, LSID3
invokes SID3 r times to form a sample of r trees rooted at a, and uses the size of the smallest
tree in the sample to evaluate a. Obviously, when r is larger, the resulting size estimations
are expected to be more accurate, improving the final tree. Consider, for example, a 3-XOR
concept with several additional irrelevant attributes. For LSID3 to prefer one of the relevant
attributes at the root, one of the trees in the samples of the relevant attributes must be the
smallest. The probability for this event increases with the increase in sample size.
LSID3 is a contract anytime algorithm parameterized by r, the sample size. Additional
time resources can be utilized by forming larger samples. Figure 3 lists the procedure for
attribute selection as applied by LSID3. Let m = |E| be the number of examples and
n = |A| be the number of attributes. The runtime complexity of LSID3 is O(rmn3 ). LSID3
was shown to exhibit good anytime behavior with diminishing returns. When applied to
hard concepts, it produced significantly better trees than ID3 and C4.5.
ACT takes the same sampling approach as LSID3. The three major components of
LSID3 that need to be replaced in order to adapt it for cost-sensitive problems are: (1)
sampling the space of trees, (2) evaluating a tree, and (3) pruning a tree.
3.1 Obtaining the Sample
LISD3 uses SID3 to bias the samples towards small trees. In ACT, however, we would like
to bias our sample towards low-cost trees. For this purpose, we designed a stochastic version
of the EG2 algorithm, which attempts to build low cost trees greedily. In EG2, a tree is
built top-down, and the test that maximizes ICF is chosen for splitting a node, where,
ICF (θ) =

2∆I(θ) − 1
.
(cost (θ) + 1)w

∆I is the information gain (as in ID3). The parameter w ∈ [0, 1] controls the bias
towards lower cost attributes. When w = 0, test costs are ignored and ICF relies solely
6

Anytime Induction of Low-cost, Low-error Classifiers

Procedure SEG2-Choose-Attribute(E, A)
Foreach a ∈ A
∆I (a) ← Information-Gain(E, a)
c (a) ← Cost(a)
2∆I(a) −1
p (a) ← (c(a)+1)
w
a∗ ← Choose attribute at random from A;
for each attribute a, the probability
of selecting it is proportional to p (a)
Return a∗
Figure 4: Attribute selection in SEG2
on the information gain. Larger values of w strengthen the effect of test costs on ICF. We
discuss setting the value of w in Section 3.5.
In stochastic EG2 (SEG2), we choose splitting attributes semi-randomly, proportionally
to their ICF. Because SEG2 is stochastic, we expect to be able to escape local minima for at
least some of the trees in the sample. Figure 4 formalizes the attribute selection component
in SEG2. To obtain a sample of size r, ACT uses EG2 once and SEG2 r − 1 times. EG2
and SEG2 are given direct access to context-based costs, i.e., if an attribute has already
been tested, its cost is zero and if another attribute that belongs to the same group has
been tested, a group discount is applied.
3.2 Evaluating a Subtree
LSID3 is a cost-insensitive learning algorithm. As such, its main goal is to maximize the
expected accuracy of the learned tree. Occam’s razor states that given two consistent
hypotheses, the smaller one is likely to be more accurate. Following Occam’s razor, LSID3
uses the tree size as a preference bias and favors splits that are expected to reduce its final
size.
In a cost-sensitive setup, however, our goal is to minimize the expected total cost of
classification. Therefore, rather than choosing an attribute that minimizes the size, we
would like to choose one that minimizes the total cost. Given a decision tree, we need to
come up with a procedure that estimates the expected cost of using the tree to classify a
future case. This cost has two components: the test cost and the misclassification cost.
3.2.1 Estimating Test Costs
Assuming that the distribution of future cases would be similar to that of the learning
examples, we can estimate the test costs using the training data. Given a tree, we calculate
the average test cost of the training examples and use it to estimate the test cost of new
cases. For a (sub)tree T built from E, a set of m training examples, we denote the average
cost of traversing T for an example from E by
tcost(T, E) =

1 X
tcost(T, e).
m e∈E
7

Esmeir & Markovitch

d
The estimated test cost for an unseen example e∗ is therefore tcost(T,
e∗ ) = tcost(T, E).
Observe that costs are calculated in the relevant context. If an attribute a has already
been tested in upper nodes, we will not charge for testing it again. Similarly, if an attribute
from a group g has already been tested, we will apply a group discount to the other attributes
from g. If a delayed attribute is encountered, we sum the cost of the entire subtree.

3.2.2 Estimating Misclassification Costs
How to go about estimating the cost of misclassification is not obvious. The tree size can
no longer be used as a heuristic for predictive errors. Occam’s razor allows the comparison
of two consistent trees but provides no means for estimating accuracy. Moreover, tree size
is measured in a different currency than accuracy and hence cannot be easily incorporated
in the cost function.
Rather than using the tree size, we propose a different estimator: the expected error
(Quinlan, 1993). For a leaf with m training examples, of which s are misclassified, the
expected error is defined as the upper limit on the probability for error, i.e., EE(m, s, cf ) =
bin (m, s), where cf is the confidence level and U bin is the upper limit of the confidence
Ucf
interval for binomial distribution. The expected error of a tree is the sum of the expected
errors in its leaves.
Originally, the expected error was used by C4.5’s error-based pruning to predict whether
a subtree performs better than a leaf. Although lacking a theoretical basis, it was shown
experimentally to be a good heuristic. In ACT we use the expected error to approximate
the misclassification cost. Assume a problem with |C| classes and a misclassification cost
matrix M . Let c be the class label in a leaf l. Let ml be the total number of examples in
l and mil be the number of examples in l that belong to class i. When the penalties for
predictive errors are uniform (Mi,j = mc), the estimated misclassification cost in l is
d (l) = EE(ml , ml − mc , cf ) · mc.
mcost
l

In a problem with nonuniform misclassification costs, mc should be replaced by the cost
of the actual errors the leaf is expected to make. These errors are obviously unknown to the
learner. One solution is to estimate each error type separately using confidence intervals
for multinomial distribution and multiply it by the associated cost:
d (l) =
mcost

X

mul
Ucf
(ml , mil , |C|) · mc.

i6=c

Such approach, however, would result in an overly pessimistic approximation, mainly
when there are many classes. Alternatively, we compute the expected error as in the uniform
case and propose replacing mc by the weighted average of the penalty for classifying an
instance as c while it belongs to another class. The weights are derived from the proportions
mil
ml −mc using a generalization of Laplace’s law of succession (Good, 1965, chap. 4):
l

d (l) = EE(ml , ml − mc , cf ) ·
mcost
l

X
i6=c

Ã

!

mil + 1
· Mc,i .
ml − mcl + |C| − 1

Note that in a problem with C classes, the average is over C − 1 possible penalties
because Mc,c = 0. Hence, in a problem with two classes c1 , c2 if a leaf is marked as c1 , mc
8

Anytime Induction of Low-cost, Low-error Classifiers

Procedure ACT-Choose-Attribute(E, A, r)
If r = 0
Return EG2-Choose-Attribute(E, A)
Foreach a ∈ A
Foreach vi ∈ domain(a)
Ei ← {e ∈ E | a(e) = vi }
T ← EG2(a, Ei , A − {a})
mini ← Total-Cost(T, Ei )
Repeat r − 1 times
T ← SEG2(a, Ei , A − {a})
mini ← min (mini , Total-Cost(T, Ei ))
P|domain(a)|
mini
totala ← Cost(a) + i=1
Return a for which totala is minimal
Figure 5: Attribute selection in ACT
would be replaced by M1,2 . When classifying a new instance, the expected misclassification
cost of a tree T built from m examples is the sum of the expected misclassification costs in
the leaves divided by m:
1 X d
d
mcost(l),
mcost(T
)=
m l∈L
where L is the set of leaves in T . Hence, the expected total cost of T when classifying a
single instance is:
d
d
d
total(T,
E) = tcost(T,
E) + mcost(T
).

An alternative approach that we intend to explore in future work is to estimate the cost
of the sampled trees using the cost for a set-aside validation set. This approach is attractive
mainly when the training set is large and one can afford setting aside a significant part of
it.
3.3 Choosing a Split
Having decided about the sampler and the tree utility function, we are ready to formalize
the tree growing phase in ACT. A tree is built top-down. The procedure for selecting a
splitting test at each node is listed in Figure 5 and illustrated in Figure 6. We give a
detailed example of how ACT chooses splits and explain how the split selection procedure
is modified for numeric attributes.
3.3.1 Choosing a Split: Illustrative Examples
ACT’s evaluation is cost-senstive both in that it considers test and error costs simultaneously
and in that it can take into account different error penalties. To illustrate this let us consider
a two-class problem with mc = 100$ (uniform) and 6 attributes, a1 , . . . , a6 , whose costs are
10$. The training data contains 400 examples, out of which 200 are positive and 200 are
negative.
9

Esmeir & Markovitch

a

G2
SE
st( 4.9
=

co
)

cost(EG2)
=4.7
cost(SEG2)
=5.1

cost(EG2)
=8.9

Figure 6: Attribute evaluation in ACT. Assume that the cost of a in the current context is
0.4. The estimated cost of a subtree rooted at a is therefore 0.4 + min(4.7, 5.1) +
min(8.9, 4.9) = 10.

Test costs
a1 10
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 100
FN 100

r=1

T1

T2

a1
a3

5 +
95 EE = 7.3

-

a2
a4

a5

95 +
5
EE = 7.3

+

5 +
95 EE = 7.3

-

95 +
5
EE = 7.3

+

mcost (T1) = 7.3*4*100$ / 400 = 7.3$
tcost (T1) = 20$
total(T1) = 27.3$

0 +
50 EE = 1.4

-

a6

100 +
50 EE = 54.1

+

0 +
50 EE = 1.4

-

100 +
50 EE = 54.1

+

mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$
tcost (T2) = 20$
total (T2) = 47.7$

Figure 7: Evaluation of tree samples in ACT. The leftmost column defines the costs: 6
attributes with identical cost and uniform error penalties. T 1 was sampled for a1
and T 2 for a2 . EE stands for the expected error. Because the total cost of T1 is
lower, ACT would prefer to split on a1 .

Assume that we have to choose between a1 and a2 , and that r = 1. Let the trees in
Figure 7, denoted T 1 and T 2, be those sampled for a1 and a2 respectively. The expected
error costs of T1 and T2 are:3
d
mcost(T
1) =

d
mcost(T
2) =

=

1
4 · 7.3
(4 · EE (100, 5, 0.25)) · 100$ =
· 100$ = 7.3$
400
400
1
(2 · EE (50, 0, 0.25) · 100$ + 2 · EE (150, 50, 0.25) · 100$)
400
2 · 1.4 + 2 · 54.1
· 100$ = 27.7$
400

When both test and error costs are involved, ACT considers their sum. Since the test
cost of both trees is identical (20$), ACT would prefer to split on a1 . If, however, the cost
3. In this example we set cf to 0.25, as in C4.5. In Section 3.5 we discuss how to tune cf .

10

Anytime Induction of Low-cost, Low-error Classifiers

Test costs
a1 40
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 100
FN 100

r=1

T1

T2

a1
a3

-

a4

a5

95 +
5
EE = 7.3

5 +
95 EE = 7.3

95 +
5
EE = 7.3

5 +
95 EE = 7.3

a2

-

+

+

mcost (T1) = 7.3*4*100$ / 400 = 7.3$
tcost (T1) = 50$
total(T1) = 57.3$

a6

-

100 +
50 EE = 54.1

0 +
50 EE = 1.4

100 +
50 EE = 54.1

0 +
50 EE = 1.4

-

+

+

mcost (T2) = (1.4*2 + 54.1*2) * 100$ / 400 = 27.7$
tcost (T2) = 20$
total (T2) = 47.7$

Figure 8: Evaluation of tree samples in ACT. The leftmost column defines the costs: 6
attributes with identical cost (except for the expensive a1 ) and uniform error
penalties. T 1 was sampled for a1 and T 2 for a2 . Because the total cost of T2 is
lower, ACT would prefer to split on a2 .

Test costs
a1 10
a2 10
a3 10
a4 10
a5 10
a6 10
MC costs
FP 1
FN 199

r=1

T1

T2

a1
a3

5 +
95 EE = 7.3

-

a2
a4

a5

95 +
5
EE = 7.3

+

5 +
95 EE = 7.3

-

95 +
5
EE = 7.3

+

mcost (T1) = (7.3*2*1$ + 7.3*2*199$) / 400 = 7.3$
tcost (T1) = 20$
total (T1) = 27.3$

0 +
50 EE = 1.4

-

a6

100 +
50 EE = 54.1

+

0 +
50 EE = 1.4

-

100 +
50 EE = 54.1

+

mcost (T2) = (54.1*2*1$ + 1.4*2*199$) / 400 = 1.7$
tcost (T2) = 20$
total (T2) = 21.7$

Figure 9: Evaluation of tree samples in ACT. The leftmost column defines the costs: 6
attributes with identical cost and nonuniform error penalties. T 1 was sampled
for a1 and T 2 for a2 . Because the total cost of T2 is lower, ACT would prefer to
split on a2 .

of a1 were 40$, as in Figure 8, tcost(T 1) would become 50$ and the total cost of T 1 would
become 57.3$, while that of T 2 would remain 47.7$. Hence, in this case ACT would split
on a2 .
To illustrate how ACT handles nonuniform error penalties, let us assume that the cost
of all attributes is again 10$, while the cost of a false positive (FP ) is 1$ and the cost of a
false negative (F N ) is 199$. Let the trees in Figure 9, denoted T 1 and T 2, be those sampled
for a1 and a2 respectively. As in the first example, only misclassification costs play a role
because the test costs of both trees is the same. Although on average the misclassification
11

Esmeir & Markovitch

cost is also 100$, ACT now evaluates these trees differently:
d
mcost(T
1) =

=

d
mcost(T
2) =

=

1
(2 · EE (100, 5, 0.25) · 1$ + 2 · EE (100, 5, 0.25) · 199$)
400
2 · 7.3 · 1$ + 2 · 7.3.1 · 199$
= 7.3$
400
1
(2 · EE (50, 0, 0.25) · 199$ + 2 · EE (100, 50, 0.25) · 1$)
400
2 · 1.4 · 199$ + 2 · 54.1 · 1$
= 1.7$
400

Therefore, in the nonuniform setup, ACT would prefer a2 . This makes sense because in the
given setup we prefer trees that may result in more false positives but reduce the number
of expensive false negatives.
3.3.2 Choosing a Split when Attributes are Numeric
The selection procedure as formalized in Figure 5 must be modified slightly when an attribute is numeric: rather than iterating over the values the attribute can take, we first pick
r tests (split points) with the highest information gain and then invoke EG2 once for each
split point. This guarantees that numeric and nominal attributes get the same resources.
Chickering, Meek, and Rounthwaite (2001) introduced several techniques for generating a
small number of candidate split points dynamically with little overhead. In the future we
intend to apply these techniques to select r points, each of which will be evaluated with a
single invocation of EG2.
3.4 Cost-Sensitive Pruning
Pruning plays an important role in decision tree induction. In cost-insensitive environments,
the main goal of pruning is to simplify the tree in order to avoid overfitting the training
data. A subtree is pruned if the resulting tree is expected to yield a lower error.
When test costs are taken into account, pruning has another important role: reducing
test costs in a tree. Keeping a subtree is worthwhile only if its expected reduction in misclassification costs is larger than the cost of the tests in that subtree. If the misclassification
cost is zero, it makes no sense to keep any split in the tree. If, on the other hand, the misclassification cost is much larger than the test costs, we would expect similar behavior to
the cost-insensitive setup.
To handle this challenge, we propose a novel approach for cost-sensitive pruning. As in
error-based pruning (Quinlan, 1993), we scan the tree bottom-up. Then we compare the
expected total cost of each subtree to that of a leaf. If a leaf is expected to perform better,
the subtree is pruned.
The cost of a subtree is estimated as described in Section 3.2. Formally, let E be the
set of training examples that reach a subtree T , and let m be the size of E. Assume that
s examples in E do not belong to the default class.4 Let L be the set of leaves in T . We
4. If misclassification costs are uniform, the default class is the majority class. Otherwise, it is the class
that minimizes the misclassification cost in the node.

12

Anytime Induction of Low-cost, Low-error Classifiers

prune T into a leaf if:
X
1
d
d
mcost(l).
· EE(m, s, cf ) · mc ≤ tcost(T,
E) +
m
l∈L

The above assumes a uniform misclassification cost mc. In the case of nonuniform penalties,
we multiply the expected error by the average misclassification cost.
An alternative approach for post-pruning is early stopping of the growing phase. For
example, one could limit the depth of the tree, require a minimal number of examples in
each child (as in C4.5), or prevent splitting nodes when the splitting criterion fails to exceed
a predetermined threshold (as in DTMC). Obviously, any pre-pruning condition can also
be applied as part of the post-pruning procedure. The advantage of post-pruning, however,
is its ability to estimate the effect of a split on the entire subtree below it, and not only on
its immediate successors (horizon effect).
Consider for example the 2-XOR problem a ⊕ b. Splitting on neither a nor b would
have a positive gain and hence the growing would be stopped. If no pre-pruning is allowed,
the optimal tree would be found and would not be post-pruned because the utility of the
splits is correctly measured. Frank (2000) reports a comprehensive study about pruning of
decision trees, in which he compared pre- to post-pruning empirically in a cost-insensitive
setup. His findings show that the advantage of post-pruning on a variety of UCI datasets
is not significant. Because pre-pruning is computationally more efficient, Frank concluded
that, in practice, it might be a viable alternative to post-pruning. Despite these results, we
decided to use post-pruning in ACT, for the following reasons:
1. Several concepts not represented in the UCI repository may appear in real-world
problems. For example, parity functions naturally arise in real-world problems, such
as the Drosophila survival concept (Page & Ray, 2003).
2. When costs are involved, the horizon effect may appear more frequently because high
costs may hide good splits.
3. In our anytime setup the user is willing to wait longer in order to obtain a good tree.
Since post-pruning takes even less time than the induction of a single greedy tree, the
extra cost of post-pruning is minor.
In the future we plan to add a pre-pruning parameter which will allow early stopping
when resources are limited. Another interesting direction for future work would be to postprune the final tree but pre-prune the lookahead trees that form the samples. This would
reduce the runtime at the cost of less accurate estimations for the utility of each candidate
split.
3.5 Setting the Parameters of ACT
In addition to r, the sample size, ACT is parameterized by w, which controls the weight
of the test costs in EG2, and cf , the confidence factor used both for pruning and for error
estimation. ICET tunes w and cf using genetic search. In ACT we considered three different
alternatives: (1) keeping EG2’s and C4.5’s default values w = 1 and cf = 0.25, (2) tuning
13

Esmeir & Markovitch

the values using cross-validation, and (3) setting the values a priori, as a function of the
problem costs.
While the first solution is the simplest, it does not exploit the potential of adapting
the sampling mechanism to the specific problem costs. Although tuning the values using
grid search would achieve good results, it may be costly in terms of runtime. For example,
if we had 5 values for each parameter and used 5-fold cross-validation, we would need to
run ACT 125 times for the sake of tuning alone. In our anytime setup this time could be
invested to invoke ACT with a larger r and hence improve the results. Furthermore, the
algorithm would not be able to output any valid solution before the tuning stage is finished.
Alternatively, we could try to tune the parameters by invoking the much faster EG2, but
the results would not be as good because the optimal values for EG2 are not necessarily
good for ACT.
The third approach, which we chose for our experiments, is to set w and cf in advance,
according to the problem specific costs. w is set inverse proportionally to the misclassification cost: a high misclassification cost results in a smaller w, reducing the effect of attribute
costs on the split selection measure. The exact formula is:
w = 0.5 + e−x ,
where x is the average misclassification cost (over all non-diagnoal entries in M ) divided by
T C, the cost if we take all tests. Formally,
x=

P

Mi,j
.
(|C| − 1) · |C| · T C
i6=j

In C4.5 the default value of cf is 0.25. Larger cf values result in less pruning. Smaller
cf values lead to more aggressive pruning. Therefore, in ACT we set cf to a value in the
range [0.2, 0.3]; the exact value depends on the problem cost. When test costs are dominant,
we prefer aggressive pruning and hence a low value for cf . When test costs are negligible,
we prefer to prune less. The same value of cf is also used to estimate the expected error.
Again, when test costs are dominant, we can afford a pessimistic estimate of the error, but
when misclassification costs are dominant, we would prefer that the estimate be closer to
the error rate in the training data. The exact formula for setting cf is:
x−1
cf = 0.2 + 0.05(1 +
).
x+1

4. Empirical Evaluation
We conducted a variety of experiments to test the performance and behavior of ACT.
First we introduce a novel method for automatic adaption of existing datasets to the costsensitive setup. We then describe our experimental methodology and its motivation. Finally
we present and discuss our results.
4.1 Datasets
Typically, machine learning researchers use datasets from the UCI repository (Asuncion &
Newman, 2007). Only five UCI datasets, however, have assigned test costs.5 We include
5. Costs for these datasets have been assigned by human experts (Turney, 1995).

14

Anytime Induction of Low-cost, Low-error Classifiers

these datasets in our experiments. Nevertheless, to gain a wider perspective, we have
developed an automatic method that assigns costs to existing datasets. The method is
parameterized with:
1. cr, the cost range.
2. g, the number of desired groups as a percentage of the number of attributes. In a
problem with |A| attributes, there are g · |A| groups. The probability for an attribute
1
, as is the probability for it not to belong
to belong to each of these groups is g·|A|+1
to any of the groups.
3. d, the number of delayed tests as a percentage of the number of attributes.
4. ϕ, the group discount as a percentage of the minimal cost in the group (to ensure
positive costs).
5. ρ, a binary flag which determines whether costs are drawn randomly, uniformly (ρ = 0)
or semi-randomly (ρ = 1): the cost of a test is drawn proportionally to its information
gain, simulating a common case where valuable features tend to have higher costs. In
this case we assume that the cost comes from a truncated normal distribution, with
the mean being proportional to the gain.
Using this method, we assigned costs to 25 datasets: 20 arbitrarily chosen UCI datasets6
and 5 datasets that represent hard concepts and have been used in previous research. Appendix A gives detailed descriptions of these datasets.
Due to the randomization in the cost assignment process, the same set of parameters
defines an infinite space of possible costs. For each of the 25 datasets we sampled this space
4 times with
cr = [1, 100], g = 0.2, d = 0, ϕ = 0.8, ρ = 1.
These parameters were chosen in an attempt to assign costs in a manner similar to that in
which real costs are assigned. In total, we have 105 datasets: 5 assigned by human experts
and 100 with automatically generated costs.7
Cost-insensitive learning algorithms focus on accuracy and therefore are expected to perform well when testing costs are negligible relative to misclassification costs. However, when
testing costs are significant, ignoring them would result in expensive classifiers. Therefore,
evaluating cost-sensitive learners requires a wide spectrum of misclassification costs. For
each problem out of the 105, we created 5 instances, with uniform misclassification costs
mc = 100, 500, 1000, 5000, 10000. Later on, we also consider nonuniform misclassification
costs.
4.2 Methodology
We start our experimental evaluation by comparing ACT, given a fixed resource allocation, with several other cost-sensitive and cost-insensitive algorithms. Next we compare
the anytime behavior of ACT to that of ICET. Finally, we evaluate the algorithms with
6. The chosen UCI datasets vary in size, type of attributes, and dimension.
7. The additional 100 datasets are available at http://www.cs.technion.ac.il/∼esaher/publications/cost.

15

Esmeir & Markovitch

two modifications on the problem instances: random test cost assignment and nonuniform
misclassification costs.
4.2.1 Compared Algorithms
ACT is compared to the following algorithms:
• C4.5 : A cost-insensitive greedy decision tree learner. The algorithm has been reimplemented following the details in (Quinlan, 1993) and the default parameters have
been used.
• LSID3 : A cost-insensitive anytime decision tree learner. As such it uses extra time to
induce trees of higher accuracy. It is not able, however, to exploit additional allotted
time to reduce classification costs.
• IDX : A greedy top-down learner that prefers splits that maximize ∆I
c (Norton, 1989).
The algorithm does not take into account misclassification costs. IDX has been implemented on top of C4.5, by modifying the split selection criteria.
2

• CSID3 : A greedy top-down learner that prefers splits that maximize ∆Ic (Tan &
Schlimmer, 1989). The algorithm does not take into account misclassification costs.
CSID3 has been implemented on top of C4.5, by modifying the split selection criteria.
• EG2 : A greedy top-down learner that prefers splits that maximize

2∆I(θ) −1

w

(cost(θ)+1)
(Nunez, 1991). The algorithm does not take into account misclassification costs.
EG2 has been implemented on top of C4.5, by modifying the split selection criteria.

• DTMC : DTMC was implemented by following the original pseudo-code (Ling et al.,
2004; Sheng et al., 2006). However, the original pseudo-code does not support continuous attributes and multiple class problems. We added support to continuous
attributes, as in C4.5’s dynamic binary-cut discretization, with the cost reduction
replacing gain ratio for selecting cutting points. The extension to multiple class problems was straightforward. Note that DTMC does not post-prune the trees but only
pre-prunes them.
• ICET : ICET has been reimplemented following the detailed description given by
Turney (1995). To verify the results of the reimplementation, we compared them
with those reported in the literature. We followed the same experimental setup and
used the same 5 datasets. The results are indeed similar: the basic version of ICET
achieved an average cost of 50.8 in our reimplementation vs. 50 reported originally.
One possible reason for the slight difference may be that the initial population of
the genetic algorithm is randomized, as are the genetic operators and the process of
partitioning the data into training, validating, and testing sets. In his paper, Turney
introduced a seeded version of ICET, which includes the true costs in the initial
population, and reported it to perform better than the unseeded version. Therefore,
we use the seeded version for our comparison. The other parameters of ICET are the
default ones.
16

Anytime Induction of Low-cost, Low-error Classifiers

4.2.2 Normalized Cost
As Turney (1995) points out, using the average cost of classification for each dataset is
problematic because: (1) the cost differences of the algorithms become relatively small as
the misclassification cost increases, (2) it is difficult to combine the results for multiple
datasets in a fair manner (e.g., average), and (3) it is difficult to combine the average of the
different misclassification costs. To overcome these problems, Turney suggests normalizing
the average cost of classification by dividing it by the standard cost. Let T C be the cost if
we take all tests. Let fi be the frequency of class i in the data. The error if the response is
always class i is therefore (1 − fi ). The standard cost is defined as
T C + mini (1 − fi ) · maxi,j (Mi,j ) .
The standard cost is an approximation for the maximal cost in a given problem. It
consists of two components: the maximal test cost and the misclassification cost if the
classifier achieves only the baseline accuracy (e.g., a majority-based classifier when error
costs are uniform). Because some classifiers may perform even worse than the baseline
accuracy, the standard cost is not strictly an upper bound on real cost. In most of our
experiments, however, it has not been exceeded.
4.2.3 Statistical Significance
For each problem out of the 105, a single 10-fold cross-validation experiment was conducted.
The same partition to train-test sets was used for all compared algorithms. To determine
statistical significance of the performance differences between ACT, ICET, and DTMC we
used two tests:
• Paired t-test with α = 5% confidence. For each problem out of the 105 and for
each pair of algorithms, we have 10 pairs of results obtained from the 10-fold cross
validation runs. We used paired t-test to determine weather the difference between
the two algorithms on a given problem is significant (rejecting the null hypothesis that
the algorithms do not differ in their performance). Then, we count for each algorithm
how many times it was a significant winner.
• Wilcoxon test (Demsar, 2006), which compares classifiers over multiple datasets and
states whether one method is significantly better than the other (α = 5%).
4.3 Fixed-time Comparison
For each of the 105 × 5 problem instances, we ran the different algorithms, including ACT
with r = 5. We chose r = 5 so the average runtime of ACT would be shorter than ICET over
all problems. The other methods have much shorter runtime due to their greedy nature.
Table 1 summarizes the results.8 Each pair of numbers represents the average normalized
cost and its associated confidence interval (α = 5%). Figure 10 illustrates the average results
and plots the normalized costs for the different algorithms and misclassification costs.
Statistical significance test results for ACT, ICET, and DTMC are given in Table 2.
The algorithms are compared using both the t-test and the Wilcoxon test. The table lists
8. The full results are available at http://www.cs.technion.ac.il/∼esaher/publications/cost.

17

Esmeir & Markovitch

Table 1: Average cost of classification as a percentage of the standard cost of classification
for different mc values. The numbers represent the average over all 105 datasets
and the associated confidence intervals (α = 5%).
mc
100
500
1000
5000
10000

C4.5
50.6
49.9
50.4
53.3
54.5

LSID3

±4.2
±4.2
±4.6
±5.9
±6.4

45.3
43.0
42.4
43.6
44.5

IDX

±3.7
±3.9
±4.5
±6.1
±6.6

34.4
42.4
47.5
58.1
60.8

CSID3

±3.6
±3.6
±4.2
±5.9
±6.4

41.7
45.2
47.8
54.3
56.2

±3.8
±3.9
±4.4
±5.9
±6.4

EG2
35.1
42.5
47.3
57.3
59.9

DTMC

±3.6
±3.6
±4.3
±5.9
±6.4

14.6
37.7
47.1
57.6
59.5

±1.8
±3.1
±3.8
±5.2
±5.6

ICET
24.3
36.3
40.6
45.7
47.1

±3.1
±3.1
±3.9
±5.6
±6.0

ACT
15.2
34.5
39.1
41.5
41.4

±1.9
±3.2
±4.2
±5.7
±6.0

Table 2: DTMC vs. ACT and ICET vs. ACT using statistical tests. For each mc, the first
column lists the number of t-test significant wins while the second column gives
the winner, if any, as implied by a Wilcoxon test over all datasets with α = 5%.
t − test WINS
mc
100
500
1000
5000
10000

DTMC vs. ACT
14
9
7
7
6

3
29
45
50
56

W ilcoxon WINNER

ICET vs. ACT
4
5
12
15
7

54
23
24
21
24

DTMC vs. ACT

ICET vs. ACT

DTMC
ACT
ACT
ACT
ACT

ACT
ACT
ACT
ACT
-

the number of t-test wins for each algorithm out of the 105 datasets, as well as the winner,
if any, when the Wilcoxon test was applied.
When misclassification cost is relatively small (mc = 100), ACT clearly outperforms
ICET, with 54 significant wins as opposed to ICET’s 4 significant wins. No significant
difference was found in the remaining runs. In this setup ACT was able to produce very
small trees, sometimes consisting of one node; the accuracy of the learned model was ignored
in this setup. ICET, on the contrary, produced, for some of the datasets, larger and more
costly trees. DTMC achieved the best results, and outperformed ACT 14 times. The
Wilcoxon test also indicates that DTMC is better than ACT and that ACT is better than
ICET. Further investigation showed that for a few datasets ACT produced unnecessarily
larger trees. We believe that a better tuning of cf would improve ACT in this scenario by
making the pruning more aggressive.
At the other extreme, when misclassification costs dominate (mc = 10000), the performance of DTMC is worse than ACT and ICET. The t-test indicates that ACT was
significantly better than ICET 24 times and significantly worse only 7 times. According to
the Wilcoxon test with α = 5%, the difference between ACT and ICET is not significant.
Taking α > 5.05%, however, would turn the result in favor of ACT. Observe that DTMC,
the winner when mc = 100, becomes the worst algorithm when mc = 10000. One reason
18

Anytime Induction of Low-cost, Low-error Classifiers

Average % Standard Cost

60

50

40

30
C4.5
LSID3
EG2
DTMC
ICET
ACT

20

10
100

1000
Misclassification Cost

10000

ACT Cost

Figure 10: Average normalized cost as a function of misclassification cost

100

100

100

80

80

80

60

60

60

40

40

40

20

20

20

0

0
0

20

40
60
ICET Cost

80

100

0
0

20

40
60
ICET Cost

80

100

0

20

40
60
ICET Cost

80

100

Figure 11: Illustration of the differences in performance between ACT and ICET for mc =
100, 1000, 10000 (from left to right). Each point represents a dataset. The x-axis
represents the cost of ICET while the y-axis represents that of ACT. The dashed
line indicates equality. Points are below it if ACT performs better and above it
if ICET is better.

for this phenomenon is that DTMC, as introduced by Ling et al. (2004), does not perform
post-pruning, although doing so might improve accuracy in some domains.
The above two extremes are less interesting: for the first we could use an algorithm that
always outputs a tree of size 1 while for the second we could use cost-insensitive learners.
The middle range, where mc ∈ {500, 1000, 5000}, requires that the learner carefully balance
the two types of cost. In these cases ACT has the lowest average cost and the largest
number of t-test wins. Moreover, the Wilcoxon test indicates that it is superior. ICET is
the second best method. As reported by Turney (1995), ICET is clearly better than the
greedy methods EG2, IDX, and CSID3.
Note that EG2, IDX, and CSID3, which are insensitive to misclassification cost, produced the same trees for all values of mc. These trees, however, are judged differently with
the change in misclassification cost.
Figure 11 illustrates the differences between ICET and ACT for mc = 100, 1000, 10000.
Each point represents one of the 105 datasets. The x-axis represents the cost of ICET while
the y-axis represents that of ACT. The dashed line indicates equality. As we can see, the
19

Esmeir & Markovitch

100

Average Accuracy

90

80

70
C4.5
LSID3
EG2
DTMC
ICET
ACT

60

50
100

1000
Misclassification Cost

10000

Figure 12: Average accuracy as a function of misclassification cost

majority of points are below the equality line, indicating that ACT performs better. For
mc = 10000 we can see that there are points located close to the x-axis but with large x
value. These points represent the difficult domains, such as XOR, which ICET could not
learn but ACT could.
4.4 Comparing the Accuracy of the Learned Models
When misclassification costs are low, an optimal algorithm would produce a very shallow
tree. When misclassification costs are dominant, an optimal algorithm would produce a
highly accurate tree. As we can see, ACT’s normalized cost increases with the increase in
misclassification cost. While it is relatively easy to produce shallow trees, some concepts
are not easily learnable and even cost-insensitive algorithms fail to achieve perfect accuracy
on them. Hence, as the importance of accuracy increases, the normalized cost increases too
because the predictive errors affect it more dramatically.
To learn more about the effect of misclassification costs on accuracy, we compare the
accuracy of the built trees for different misclassification costs. Figure 12 shows the results.
An important property of DTMC, ICET, and ACT is their ability to compromise on accuracy when needed. They produce inaccurate trees when accuracy is insignificant and much
more accurate trees when the penalty for error is high. ACT’s flexibility, however, is more
noteworthy: from the second least accurate method it becomes the most accurate one.
Interestingly, when accuracy is extremely important, both ICET and ACT achieve even
better accuracy than C4.5. The reason is their non-greedy nature. ICET performs an
implicit lookahead by reweighting attributes according to their importance. ACT performs
lookahead by sampling the space of subtrees before every split. Of the two, the results
indicate that ACT’s lookahead is more efficient in terms of accuracy. DTMC is less accurate
than C4.5. The reason is the different split selection criterion and the different pruning
mechanism.
In comparison to our anytime cost insensitive algorithm LSID3, ACT produced less
accurate trees when mc was relatively low. When mc was set to 5000, however, ACT
achieved comparable accuracy to LSID3 and slightly outperformed it for mc = 10000.
Statistical tests found the differences between the accuracy of the two algorithms in this
20

Anytime Induction of Low-cost, Low-error Classifiers

10
EG2
DTMC
ICET
ACT

84
8
Average Cost

Average Cost

82
80
EG2
DTMC
ICET
ACT

78
76

6

4

2

74
72

0
0

0.5

1

1.5

2
2.5
Time [sec]

3

3.5

4

4.5

0

0.2

0.4

0.6

0.8
1
Time [sec]

1.2

1.4

1.6

1.8

80
100
70

90
80
Average Cost

Average Cost

60
50
40
EG2
DTMC
ICET
ACT

30
20

EG2
DTMC
ICET
ACT

70
60
50
40
30

10

20
0
0

1

2
3
Time [sec]

4

5

0

1

2

3
Time [sec]

4

5

6

Figure 13: Average normalized cost as a function of time for (from top-left to bottom-right)
Breast-cancer-20, Monks1, Multi-XOR, and XOR5

case to be insignificant. ACT’s small advantage on some of the datasets indicates that, for
some problems, expected error is a better heuristic than tree size for maximizing accuracy.
4.5 Comparison of Anytime Behavior
Both ICET and ACT, like other typical anytime algorithms, perform better with increased
resource allocation. ICET is expected to exploit the extra time by producing more generations and hence better tuning the parameters for the final invocation of EG2. ACT can
use the extra time to acquire larger samples and hence achieve better cost estimations.
To examine the anytime behavior of ICET and ACT, we ran them on 4 problems,
namely Breast-cancer-20, Monks-1, Multi-XOR, and XOR5, with exponentially increasing
time allocation. mc was set to 5000. ICET was run with 2, 4, 8, . . . generations and ACT
with a sample size of 1, 2, 4, . . .. As in the fixed-time comparison, we used 4 instances for
each problem. Figure 13 plots the results averaged over the 4 instances. We also included
the results for the greedy methods EG2 and DTMC.
The results show good anytime behavior of both ICET and ACT: generally it is worthwhile to allocate more time. ACT dominates ICET for the four domains and is able to
produce less costly trees in shorter time.
One advantage of ACT over ICET is that it is able to consider the context in which an
attribute is judged. ICET, on the contrary, reassigns the cost of the attributes globally: an
21

Esmeir & Markovitch

Average % Standard Cost

60

50

DTMC

ICET

ACT

40

100
500
1000
5000
10000

30

20

10
100

DTMC
ICET
ACT
1000
Misclassification Cost

12.3
31.5
40.4
54.0
57.4

±1.8
±3.2
±3.9
±5.2
±5.6

18.7
31.8
36.4
43.7
45.6

±2.7
±3.4
±3.9
±5.5
±5.9

13.0
30.2
33.9
38.5
39.6

±2.0
±3.3
±4.0
±5.6
±6.1

10000

Figure 14: Average cost when test costs are assigned randomly

attribute cannot be assigned a high cost in one subtree and a low cost in another. The MultiXOR dataset exemplifies a concept whose attributes are important only in one sub-concept.
The concept is composed of four sub-concepts, each of which relies on different attributes
(see Appendix A for further details). As we expected, ACT outperforms ICET significantly
because the latter cannot assign context-based costs. Allowing ICET to produce more and
more generations (up to 128) does not result in trees comparable to those obtained by ACT.
4.6 Random Costs
The costs of 100 out of the 105 datasets were assigned using a semi-random mechanism that
gives higher costs to informative attributes. To ensure that ACT’s success is not due to
this particular cost assignment scheme, we repeated the experiments with the costs drawn
randomly uniformly from the given cost range cr, i.e., ρ was set to 0. Figure 14 shows the
results. As we can see, ACT maintains its advantage over the other methods: it dominates
them along the scale of mc values.
4.7 Nonuniform Misclassification Costs
So far, we have only used uniform misclassification cost matrices, i.e., the cost of each
error type was identical. As explained in Section 3, the ACT algorithm can also handle
complex misclassification cost matrices where the penalty for one type of error might be
higher than the penalty for another type. Our next experiment examines ACT in the
nonuniform scenario. Let FP denote the penalty for a false positive and FN the penalty for
false negative. When there are more than 2 classes, we split the classes into 2 equal groups
according to their order (or randomly if no order exists). We then assign a penalty FP for
misclassifying an instance that belongs to the first group and FN for one that belongs to
the second group.
To obtain a wide view, we vary the ratio between FP and FN and also examine different
absolute values. Table 3 and Figure 15 give the average results. Table 4 lists the number
of t-test significant wins each algorithm achieved. It is easy to see that ACT consistently
outperforms the other methods.
22

Anytime Induction of Low-cost, Low-error Classifiers

γ
8γ

γ
4γ

γ
2γ

γ
γ

2γ
γ

4γ
γ

8γ
γ

γ = 500

C4.5
EG2
DTCM
ICET
ACT

29.2
30.1
12.4
23.3
11.9

34.2
33.0
20.3
27.0
18.5

41.3
37.2
29.8
31.5
27.2

49.9
42.5
37.7
36.3
34.5

43.6
39.3
32.5
34.2
29.1

39.0
37.5
22.9
31.8
20.4

36.3
36.8
15.8
29.2
13.8

γ = 5000

Table 3: Comparison of C4.5, EG2, DTMC, ACT, and ICET when misclassification costs
are nonuniform. FP denotes the penalty for a false positive and FN the penalty
for a false negative. γ denotes the basic mc unit.

C4.5
EG2
DTCM
ICET
ACT

27.0
30.9
13.8
21.4
12.9

31.3
35.2
23.6
25.6
19.1

39.2
43.1
38.0
32.7
28.8

53.3
57.3
57.6
45.7
41.5

44.0
47.7
42.5
37.4
31.1

39.0
42.4
29.3
32.8
22.5

36.3
39.7
20.1
29.8
14.6

FP
FN

Table 4: Comparing DTMC, ACT, and ICET when misclassification costs are nonuniform.
For each F P/F N ratio, the columns list the number of t-test significant wins with
α = 5%. FP denotes the penalty for a false positive and FN the penalty for a
false negative. γ denotes the basic mc unit.
γ = 500
F P/F N
0.125
0.25
0.5
1
2
4
8

DTMC vs. ACT
4
2
10
9
5
3
1

22
31
25
29
35
40
27

γ = 5000

ICET vs. ACT
11
7
7
5
1
0
4

52
49
42
23
47
72
72

DTMC vs. ACT
5
10
16
7
5
4
0

44
49
52
50
61
58
62

ICET vs. ACT
12
4
10
15
9
0
0

44
36
25
21
31
44
67

Interestingly, the graphs are slightly asymmetric. The reason could be that for some
datasets, for example medical ones, it is more difficult to reduce negative errors than positive
ones, or vice versa. A similar phenomenon is reported by Turney (1995).
The highest cost for all algorithms is observed when F P = F N because, when the
ratio between FP and FN is extremely large or extremely small, the learner can easily
build a small tree whose leaves are labeled with the class that minimizes costs. When
misclassification costs are more balanced, however, the learning process becomes much
more complicated.
23

50

60

45

55
Average % Standard Cost

Average % Standard Cost

Esmeir & Markovitch

40
35
30
25
C4.5
EG2
DTMC
ICET
ACT

20
15
10
γ/8γ

γ/4γ

γ/2γ
γ/γ
2γ/γ
Misclassification Cost FP/FN

50
45
40
35
30
25

C4.5
EG2
DTMC
ICET
ACT

20
15

4γ/γ

10
γ/8γ

8γ/γ

γ/4γ

γ/2γ
γ/γ
2γ/γ
Misclassification Cost FP/FN

4γ/γ

8γ/γ

Figure 15: Comparison of C4.5, EG2, DTMC, ACT, and ICET when misclassification costs
are nonuniform. The misclassification costs are represented as a pair (F P/F N ).
FP denotes the penalty for a false positive and FN the penalty for a false
negative. γ denotes the basic mc unit. The figures plot the average cost as
a function of the ratio between FP and FN, for γ = 500 (left) and γ = 5000
(right).

5. Related Work
In addition to the works referred to earlier in this paper, several related works warrant
discussion here.
Cost-sensitive trees have been the subject of many research efforts. Several works proposed learning algorithms that consider different misclassification costs (Breiman, Friedman, Olshen, & Stone, 1984; Pazzani, Merz, Murphy, Ali, Hume, & Brunk, 1994; Provost
& Buchanan, 1995; Bradford, Kunz, Kohavi, Brunk, & Brodley, 1998; Domingos, 1999;
Elkan, 2001; Zadrozny, Langford, & Abe, 2003; Lachiche & Flach, 2003; Abe, Zadrozny, &
Langford, 2004; Vadera, 2005; Margineantu, 2005; Zhu, Wu, Khoshgoftaar, & Yong, 2007;
Sheng & Ling, 2007). These methods, however, do not consider test costs and hence are
appropriate mainly for domains where test costs are not a constraint.
Davis, Ha, Rossbach, Ramadan, and Witchel (2006) presented a greedy cost-sensitive
decision tree algorithm for forensic classification: the problem of classifying irreproducible
events. In this setup, they assume that all tests that might be used for testing must be
acquired and hence charged for before classification.
One way to exploit additional time when searching for a less costly tree is to widen
the search space. Bayer-Zubek and Dietterich (2005) formulated the cost-sensitive learning
problem as a Markov decision process (MDP), and used a systematic search algorithm
based on the AO* heuristic search procedure to solve the MDP. To make AO* efficient,
the algorithm uses a two-step lookahead based heuristic. Such limited lookahead is more
informed than immediate heuristics but still insufficient for complex domains and might
cause the search to go astray (Esmeir & Markovitch, 2007a). The algorithm was shown to
output better diagnostic policies than several greedy methods using reasonable resources.
An optimal solution, however, could not always be found due to time and memory limits.
A nice property of the algorithm is that it can serve as an anytime algorithm by computing
24

Anytime Induction of Low-cost, Low-error Classifiers

the best complete policy found so far. Its anytime behavior, nevertheless, is problematic
because policies that are optimal with respect to the train data tend to overfit. As a result,
the performance will eventually start to degrade.
Arnt and Zilberstein (2005) tackled the problem of time and cost sensitive classification
(TCSC). In TCSC, the utility of labeling an instance depends not only on the correctness of
the labeling, but also the amount of time it takes. Therefore the total cost function has an
additional component, which reflects the time needed to measure an attribute. Typically,
is has a super-linear form: the cost of a quick result is small and fairly constant, but as
the waiting time increases, the time cost grows at an increasing rate. The problem is
further complicated when a sequence of time-sensitive classification instances is considered,
where time spent administering tests for one case can adversely affect the costs of future
instances. Arnt and Zilberstein suggest solving these problems by extending the decision
theoretic approach introduced by Bayer-Zubek and Dietterich (2005). In our work, we
assume that the time it takes to administer a test is incorporated into its cost. In the
future, we intend to extend our framework to support time-sensitive classification, both for
individual cases and for sequences.
Fan, Lee, Stolfo, and Miller (2000) studied the problem of cost-sensitive intrusion detection systems (IDS). The goal is to maximize security while minimizing costs. Each
prediction (action) has a cost. Features are categorized into three cost levels according to
amount of information needed to compute their values. To reduce the cost of an IDS, high
cost rules are considered only when the predictions of low cost rules are not sufficiently
accurate.
Costs are also involved in the learning phase, during example acquisition and during
model learning. The problem of budgeted learning has been studied by Lizotte, Madani,
and Greiner (2003). There is a cost associated with obtaining each attribute value of a
training example, and the task is to determine what attributes to test given a budget.
A related problem is active feature-value acquisition. In this setup one tries to reduce
the cost of improving accuracy by identifying highly informative instances. Melville, SaarTsechansky, Provost, and Mooney (2004) introduced an approach in which instances are
selected for acquisition based on the accuracy of the current model and its confidence in
the prediction.
Greiner, Grove, and Roth (2002) were pioneers in studying classifiers that actively decide
what tests to administer. They defined an active classifier as a classifier that given a
partially specified instance, returns either a class label or a strategy that specifies which
test should be performed next. Greiner et al. also analyzed the theoretical aspects of
learning optimal active classifiers using a variant of the probably-approximately-correct
(PAC) model. They showed that the task of learning optimal cost-sensitive active classifiers
is often intractable. However, this task is shown to be achievable when the active classifier
is allowed to perform only (at most) a constant number of tests, where the limit is provided
before learning. For this setup they proposed taking a dynamic programming approach to
build trees of at most depth d.
Our setup assumed that we are charged for acquiring each of the feature values of the test
cases. The term test strategy (Sheng, Ling, & Yang, 2005) describes the process of feature
values acquisition: which values to query for and in what order. Several test strategies have
been studied, including sequential, single batch and multiple batch (Sheng et al., 2006),
25

Esmeir & Markovitch

each of which corresponds to a different diagnosis policy. These strategies are orthogonal
to our work because they assume a given decision tree.
Bilgic and Getoor (2007) tackled the problem of feature subset selection when costs are
involved. The objective is to minimize the sum of the information acquisition cost and the
misclassification cost. Unlike greedy approaches that compute the value of features one at
a time, they used a novel data structure called the value of information lattice (VOILA),
which exploits dependencies between missing features and makes it possible to share information value computations between different feature subsets possible. VIOLA was shown
empirically to achieve dramatic cost improvements without the prohibitive computational
costs of comprehensive search.

6. Conclusions
Machine learning techniques are increasingly being used to produce a wide range of classifiers for complex real-world applications that involve nonuniform testing and misclassification costs. The increasing complexity of these applications poses a real challenge to
resource management during learning and classification. In this work we introduced a novel
framework for operating in such complex environments. Our framework has four major
advantages:
• It uses a non-greedy approach to build a decision tree and therefore is able to overcome
local minima problems.
• It evaluates entire trees during the search; thus, it can be adjusted to any cost scheme
that is defined over trees.
• It exhibits good anytime behavior and allows learning speed to be traded for classification costs. In many applications we are willing to allocate more time than we
would allocate to greedy methods. Our proposed framework can exploit such extra
resources.
• The sampling process can easily be parallelized and the method benefit from distributed computer power.
To evaluate ACT we have designed an extensive set of experiments with a wide range
of costs. Since there are only a few publicly available cost-oriented datasets, we designed a
parametric scheme that automatically assigns costs for existing datasets. The experimental
results show that ACT is superior to ICET and DTMC, existing cost-sensitive algorithms
that attempt to minimize test costs and misclassification costs simultaneously. Significance
tests found the differences to be statistically strong. ACT also exhibited good anytime
behavior: with the increase in time allocation, the cost of the learned models decreased.
ACT is a contract anytime algorithm that requires its sample size to be predetermined.
In the future we intend to convert ACT into an interruptible anytime algorithm by adopting
the IIDT general framework (Esmeir & Markovitch, 2007a). In addition, we plan to apply
monitoring techniques (Hansen & Zilberstein, 2001) for optimal scheduling of ACT and to
examine other strategies for evaluating subtrees.
26

Anytime Induction of Low-cost, Low-error Classifiers

Table 5: Characteristics of the datasets used

Dataset
Breast Cancer
Bupa
Car
Flare
Glass
Heart
Hepatitis
Iris
KRK
Monks-1
Monks-2
Monks-3
Multiplexer-20
Multi-XOR
Multi-AND-OR
Nursery
Pima
TAE
Tic-Tac-Toe
Titanic
Thyroid
Voting
Wine
XOR 3D
XOR-5

Instances
277
345
1728
323
214
296
154
150
28056
124+432
169+432
122+432
615
200
200
8703
768
151
958
2201
3772
232
178
200
200

Attributes
Nominal (binary) Numeric
9 (3)
0 (0)
6 (0)
10 (5)
0 (0)
8(4)
13(13)
0 (0)
6(0)
6 (2)
6 (2)
6 (2)
20 (20)
11 (11)
11 (11)
8(8)
0(0)
4(1)
9 (0)
3(2)
15(15)
16 (16)
0 (0)
0 (0)
10 (10)

0
5
0
0
9
5
6
4
0
0
0
0
0
0
0
0
8
1
0
0
5
0
13
6
0

Max attribute
domain

Classes

13
4
7
4
2
8
4
4
4
2
2
2
5
26
3
4
2
2
2

2
2
4
4
7
2
2
3
17
2
2
2
2
2
2
5
2
3
2
2
3
2
3
2
2

Acknowledgments
This work was partially supported by funding from the EC-sponsored MUSCLE Network
of Excellence (FP6-507752).

Appendix A. Datasets
Table 5 lists the characteristics of the 25 datasets we used. Below we give a more detailed
description of the non-UCI datasets used in our experiments:
1. Multiplexer: The multiplexer task was used by several researchers for evaluating classifiers (e.g., Quinlan, 1993). An instance is a series of bits of length a + 2a , where a is
a positive integer. The first a bits represent an index into the remaining bits and the
label of the instance is the value of the indexed bit. In our experiments we considered
the 20-Multiplexer (a = 4). The dataset contains 500 randomly drawn instances.
2. Boolean XOR: Parity-like functions are known to be problematic for many learning
algorithms. However, they naturally arise in real-world data, such as the Drosophila
survival concept (Page & Ray, 2003). We considered XOR of five variables with five
additional irrelevant attributes.
27

Esmeir & Markovitch

3. Numeric XOR: A XOR based numeric dataset that has been used to evaluate learning
algorithms (e.g., Baram, El-Yaniv, & Luz, 2003). Each example consists of values for
x and y coordinates. The example is labeled 1 if the product of x and y is positive, and
−1 otherwise. We generalized this domain for three dimensions and added irrelevant
variables to make the concept harder.
4. Multi-XOR / Multi-AND-OR: These concepts are defined over 11 binary attributes.
In both cases the target concept is composed of several subconcepts, where the first
two attributes determines which of them is considered. The other 10 attributes are
used to form the subconcepts. In the Multi-XOR dataset, each subconcept is an XOR,
and in the Multi-AND-OR dataset, each subconcept is either AND or OR.

References
Abe, N., Zadrozny, B., & Langford, J. (2004). An iterative method for multi-class costsensitive learning. In Proceedings of the 10th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-2004), Seattle, WA, USA.
Arnt, A., & Zilberstein, S. (2005). Learning policies for sequential time and cost sensitive
classification. In Proceedings of the 1st international workshop on Utility-based data
mining (UBDM’05) held with KDD’05, pp. 39–45, New York, NY, USA. ACM Press.
Asuncion, A., & Newman, D. (2007).
UCI machine learning repository.
University of California, Irvine, School of Information and Computer Sciences.
http://www.ics.uci.edu/∼mlearn/MLRepository.html.
Baram, Y., El-Yaniv, R., & Luz, K. (2003). Online choice of active learning algorithms. In
Proceedings of the 20 International Conference on Machine Learning (ICML-2003),
pp. 19–26, Washington, DC, USA.
Bayer-Zubek, V., & Dietterich (2005). Integrating learning from examples into the search
for diagnostic policies. Artificial Intelligence, 24, 263–303.
Bilgic, M., & Getoor, L. (2007). Voila: Efficient feature-value acquisition for classification. In
Proceedings of the 22nd National Conference on Artificial Intelligence (AAAI-2007).
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam’s Razor.
Information Processing Letters, 24 (6), 377–380.
Boddy, M., & Dean, T. L. (1994). Deliberation scheduling for problem solving in time
constrained environments. Artificial Intelligence, 67 (2), 245–285.
Bradford, J., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. (1998). Pruning decision
trees with misclassification costs. In Proceedings of the 9th European Conference on
Machine Learning (ECML-1998), pp. 131–136.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification and Regression
Trees. Wadsworth and Brooks, Monterey, CA.
Chickering, D. M., Meek, C., & Rounthwaite, R. (2001). Efficient determination of dynamic
split points in a decision tree. In Proceedings of the 1st IEEE International Conference
28

Anytime Induction of Low-cost, Low-error Classifiers

on Data Mining (ICDM-2001), pp. 91–98, Washington, DC, USA. IEEE Computer
Society.
Davis, J. V., Ha, J., Rossbach, C. J., Ramadan, H. E., & Witchel, E. (2006). Cost-sensitive
decision tree learning for forensic classification. In Proceedings of the 17th European
Conference on Machine Learning (ECML-2006), pp. 622–629, Berlin, Germany.
Demsar, J. (2006). Statistical comparisons of classifiers over multiple data sets. Journal of
Machine Learning Research, 7, 1–30.
Domingos, P. (1999). Metacost: A general method for making classifiers cost-sensitive. In
Proceedings of the 5th International Conference on Knowledge Discovery and Data
Mining (KDD’1999), pp. 155–164.
Elkan, C. (2001). The foundations of cost-sensitive learning. In Proceedings of the 17th
International Joint Conference on Artificial Intelligence (IJCAI-2001), pp. 973–978,
Seattle, Washington, USA.
Esmeir, S., & Markovitch, S. (2006). When a decision tree learner has plenty of time. In
Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-2006),
Boston, MA, USA.
Esmeir, S., & Markovitch, S. (2007a). Anytime learning of decision trees. Journal of Machine
Learning Research, 8.
Esmeir, S., & Markovitch, S. (2007b). Occam’s razor just got sharper. In Proceedings
of the 20th International Joint Conference in Artificial Intelligence (IJCAI-2007),
Hyderabad, India.
Fan, W., Lee, W., Stolfo, S. J., & Miller, M. (2000). A multiple model cost-sensitive approach
for intrusion detection. In Proceedings of the 11th European Conference on Machine
Learning (ECML-2000), pp. 142–153, Barcelona, Catalonia, Spain.
Frank, E. (2000). Pruning Decision Trees and Lists. Ph.D. thesis, Department of Computer
Science, University of Waikato.
Good, I. (1965). The Estimation of Probabilities: An Essay on Modern Bayesian Methods.
MIT Press, USA.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classifiers.
Artificial Intelligence, 139 (2), 137–174.
Hansen, E. A., & Zilberstein, S. (2001). Monitoring and control of anytime algorithms: a
dynamic programming approach. Artificial Intelligence, 126 (1-2), 139–157.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. New York: Springer-Verlag.
Hyafil, L., & Rivest, R. L. (1976). Constructing optimal binary decision trees is NPcomplete. Information Processing Letters, 5 (1), 15–17.
Lachiche, N., & Flach, P. (2003). Improving accuracy and cost of two-class and multiclass probabilistic classifiers using roc curves. In Proceedings of the 20th International
Conference on Machine Learning (ICML-2003).
29

Esmeir & Markovitch

Lenert, L., & Soetikno, R. (1997). Automated computer interviews to elicit utilities: Potential applications in the treatment of deep venous thrombosis. American Medical
Informatics Association, 4 (1), 49–56.
Ling, C. X., Yang, Q., Wang, J., & Zhang, S. (2004). Decision trees with minimal costs. In
Proceedings of the 21st International Conference on Machine Learning (ICML-2004).
Lizotte, D. J., Madani, O., & Greiner, R. (2003). Budgeted learning of naive bayes classifiers.
In Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence (UAI2003), Acapulco, Mexico.
Margineantu, D. (2005). Active cost-sensitive learning. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-2005), Edinburgh, Scotland.
Melville, P., Saar-Tsechansky, M., Provost, F., & Mooney, R. J. (2004). Active feature acquisition for classifier induction. In Proceedings of the 4th IEEE International Conference
on Data Mining (ICDM-2004), pp. 483–486, Brighton, UK.
Norton, S. W. (1989). Generating better decision trees. In Sridharan, N. S. (Ed.), Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, pp.
800–805, Detroit, Michigan, USA.
Nunez, M. (1991). The use of background knowledge in decision tree induction. Machine
Learning, 6, 231–250.
Page, D., & Ray, S. (2003). Skewing: An efficient alternative to lookahead for decision
tree induction. In Proceedings of the 18th International Joint Conference on Artificial
Intelligence (IJCAI-2003), Acapulco, Mexico.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing
misclassification costs: knowledge intensive approaches to learning from noisy data.
In Proceedings of the 11th International Conference on Machine Learning (ICML1994).
Provost, F., & Buchanan, B. (1995). Inductive policy: The pragmatics of bias selection.
Machine Learning, 20 (1-2), 35–61.
Qin, Z., Zhang, S., & Zhang, C. (2004). Cost-sensitive decision trees with multiple cost
scales. Lecture Notes in Computer Scienc, AI 2004: Advances in Artificial Intelligence,
Volume 3339/2004, 380–390.
Quinlan, J. R. (1986). Induction of decision trees. Machine Learning, 1, 81–106.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San
Mateo, CA.
Sheng, S., Ling, C. X., Ni, A., & Zhang, S. (2006). Cost-sensitive test strategies. In
Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-2006),
Boston, MA, USA.
Sheng, S., Ling, C. X., & Yang, Q. (2005). Simple test strategies for cost-sensitive decision
trees. In Proceedings of the 9th European Conference on Machine Learning (ECML2005), pp. 365–376, Porto, Portugal.
30

Anytime Induction of Low-cost, Low-error Classifiers

Sheng, V. S., & Ling, C. X. (2007). Roulette sampling for cost-sensitive learning. In
Proceedings of the 18th European Conference on Machine Learning (ECML-2007),
pp. 724–731, Warsaw, Poland.
Tan, M., & Schlimmer, J. C. (1989). Cost-sensitive concept learning of sensor use in approach and recognition. In Proceedings of the 6th International Workshop on Machine
Learning, pp. 392–395, Ithaca, NY.
Turney, P. (2000). Types of cost in inductive concept learning. In Proceedings of the
Workshop on Cost-Sensitive Learning held with the 17th International Conference on
Machine Learning (ICML-2000), Stanford, CA.
Turney, P. D. (1995). Cost-sensitive classification: Empirical evaluation of a hybrid genetic
decision tree induction algorithm. Journal of Artificial Intelligence Research, 2, 369–
409.
Vadera, S. (2005). Inducing cost-sensitive non-linear decision trees. Technical report 03-052005, School of Computing, Science and Engineering, University of Salford.
Zadrozny, B., Langford, J., & Abe, N. (2003). Cost-sensitive learning by cost-proportionate
example weighting. In Proceedings of the 3rd IEEE International Conference on Data
Mining (ICDM-2003), Melbourne, Florida, USA.
Zhu, X., Wu, X., Khoshgoftaar, T., & Yong, S. (2007). An empirical study of the noise
impact on cost-sensitive learning. In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI-2007), Hyderabad, India.

31

Journal of Artificial Intelligence Research 33 (2008) 575-613

Submitted 4/08; published 12/08

On the Value of Correlation
Itai Ashlagi

iashlagi@hbs.edu

Harvard Business School,
Harvard University,
Boston, MA, 02163,USA

Dov Monderer

dov@ie.technion.ac.il

Faculty of Industrial Engineering and Management,
Technion - Israel Institute of Technology,
Haifa 32000, Israel

Moshe Tennenholtz

moshet@microsoft.com

Microsoft Israel R&D Center,
13 Shenkar St., Herzeliya 46725, Israel, and
Faculty of Industrial Engineering and Management,
Technion - Israel Institute of Technology,
Haifa 32000, Israel

Abstract
Correlated equilibrium generalizes Nash equilibrium to allow correlation devices. Correlated equilibrium captures the idea that in many systems there exists a trusted administrator who can recommend behavior to a set of agents, but can not enforce such behavior.
This makes this solution concept most appropriate to the study of multi-agent systems in
AI. Aumann showed an example of a game, and of a correlated equilibrium in this game in
which the agents’ welfare (expected sum of players’ utilities) is greater than their welfare in
all mixed-strategy equilibria. Following the idea initiated by the price of anarchy literature
this suggests the study of two major measures for the value of correlation in a game with
nonnegative payoffs:
1. The ratio between the maximal welfare obtained in a correlated equilibrium to the maximal
welfare obtained in a mixed-strategy equilibrium. We refer to this ratio as the mediation
value.
2. The ratio between the maximal welfare to the maximal welfare obtained in a correlated equilibrium. We refer to this ratio as the enforcement value.
In this work we initiate the study of the mediation and enforcement values, providing
several general results on the value of correlation as captured by these concepts. We also
present a set of results for the more specialized case of congestion games , a class of games
that received a lot of attention in the recent literature.

1. Introduction
Much work in the area of multi-agent systems adopts game-theoretic reasoning. This is
due to the fact that many existing systems consist of self-motivated participants, each of
which attempts to optimize his own performance. As a result the Nash equilibrium, the
central solution concept in game theory, has become a major tool in the study and analysis of
multi-agent systems. Nash equilibrium captures multi-agent behavior which is stable against
c
°2008
AI Access Foundation. All rights reserved.

Ashlagi, Monderer, & Tennenholtz

unilateral deviations. Naturally, a system that is fully controlled by a designer can enforce
behaviors which lead to a higher welfare than the one obtained in a fully decentralized system
in which agents behave selfishly and follow some Nash equilibrium. The comparison between
these quantities is studied under the title of work on ”the price of anarchy” (Koutsoupias &
Papadimitriou, 1999; Roughgarden & Tardos, 2002; Christodoulou & Koutsoupias, 2005),
and is a subject of much interest in computer science. However, fully controlled systems
versus fully uncontrolled systems are two extreme points. As was acknowledged in various
works in AI (Shoham & Tennenholtz, 1995a, 1995b) one of the main practical approaches to
dealing with realistic systems is to consider systems with some limited centralized control.
Indeed, in most realistic systems there is a designer who can recommend behavior; this
should be distinguished from the strong requirement that the designer can dictate behavior.
Correlated equilibrium, introduced by Aumann (1974), is the most famous game-theoretic
solution concept referring to a designer who can recommend but not enforce behavior. In
a game in strategic form, a correlated strategy is a probability distribution over the set
of strategy profiles, where a strategy profile is a vector of strategies, one for each player.
A correlated strategy is utilized as follows: A strategy profile is selected according to the
distribution, and every player is informed about her strategy in the profile. This selected
strategy for the player is interpreted as a recommendation of play. Correlated strategies are
most natural, since they capture the idea of a system administrator/reliable party who can
recommend behavior but can not enforce it. Hence, correlated strategies make perfect sense
in the context of congestion control, load balancing, trading, etc. A correlated strategy is
called a correlated equilibrium if it is better off for every player to obey her recommended
strategy if she believes that all other players obey their recommended strategies1 . Correlated
equilibrium makes perfect sense in the context of work on multi-agent systems in AI in which
there exists a mediator who can recommend behavior to the agents.2 A major potential
benefit of a mediator who is using a correlated equilibrium is to attempt to improve the
welfare of selfish players. In this paper, the welfare obtained in a correlated strategy is
defined to be the expected sum of the utilities of the players, and it is referred to as the
welfare obtained in this correlated strategy.
A striking example introduced in Aumann’s seminal paper (1974) is of a two-player
two-strategy game, where the welfare obtained in a correlated equilibrium is higher than
the welfare obtained in every mixed-strategy equilibrium of the game. A modification of
Aumann’s example serves us as a motivating example.
Aumann’s Example:

1. Every correlated strategy defines a Bayesian game in which the private signal of every player is her
recommended strategy. It is a correlated equilibrium if obeying the recommended strategy by every
player is a pure-strategy equilibrium in this Bayesian game.
2. The use of mediators in obtaining desired behaviors, in addition to improving social welfare, has been
further studied, (e.g., Monderer & Tennenholtz, 2004, 2006; Rozenfeld & Tennenholtz, 2007; Ashlagi,
Monderer, & Tennenholtz, 2008). However, the mediators discussed in that work makes use of more
powerful capabilities than just making recommendation based on probabilistic coin flips.

576

On The Value of Correlation

b1

b2

a1 5

0
1

a2

4

0
1

4

5

As a result, Aumann’s example suggests that correlation may be a way to improve
welfare while still assuming that players are rational in the classical game-theoretic sense.3
In this game, there are three mixed-strategy equilibrium profiles. Two of them are
obtained with pure strategies, (a1 , b1 ), and (a2 , b2 ). The welfare in each of these purestrategy equilibrium profiles equals six. There is an additional mixed-strategy equilibrium
in which every player chooses each of her strategies with equal probabilities. The welfare
obtained in this profile equals 5 (= 14 (6 + 0 + 8 + 6)) because every entry in the matrix
is played with probability 41 . Hence, the maximal welfare in a mixed-strategy equilibrium
equals 6. Consider the following correlated strategy: a probability of 1/3 is assigned to
every pure strategy profile but (a1 , b2 ). This correlated strategy is a correlated equilibrium.
Indeed, when the row player is recommended to play a1 she knows that the other player
is recommended to play b1 , and therefore she strictly prefers to play a1 . When the row
player is recommended to play a2 the conditional probability of each of the columns is half,
and therefore she weakly prefers to play a2 . Similar argument applied to the column player
shows that the correlated strategy is indeed a correlated equilibrium. The welfare associated
1
with this correlated equilibrium equals 20
3 (= 3 (6 + 8 + 6)).
The above discussion suggests one may wish to consider the value of correlation in
games. In order to address the challenge of studying the value of correlation, we tackle two
fundamental issues:
• How much can the society/system gain by adding a correlation device, where we
assume that without such a device the agents play a mixed-strategy equilibrium.
• How much does the society/system loose from the fact that the correlation device can
only recommend (and can not enforce) a course of action?
Accordingly we introduce two measures, the mediation value and the enforcement value.
These measures make sense mainly for games with nonnegative utilities, which are the focus
of this paper.
The mediation value measures the ratio between the maximal welfare in a correlated
equilibrium to the maximal welfare in a mixed-strategy equilibrium. Notice that the higher
the mediation value is, the more correlation helps. Hence, the mediation value measures
the value of a reliable mediator who can just recommend a play in a model in which there
is an anarchy without the mediator, where anarchy is defined to be the situation in which
the players use the welfare-best mixed-strategy equilibrium, that is, anarchy is the best
outcome reached by rational and selfish agents.4
3. Other advantages are purely computational ones. As has been recently shown, correlated equilibrium
can be computed in polynomial time even for structured representations of games (Kakade, Kearns,
Langford, & Ortiz, 2003; Papadimitriou, 2005).
4. The phenomenon of multiple equilibria forces a modeling choice of the concept of anarchy, which could
have been defined also as the welfare-worst mixed-strategy equilibrium, or as a convex combination of

577

Ashlagi, Monderer, & Tennenholtz

In Aumman’s example it can be shown that the correlated equilibrium introduced above
is the best correlated equilibrium, i.e., it attains the maximal welfare among all correlated
equilibria in the game. Hence, the mediation value of Aumann’s example is 10
9 .
The enforcement value measures the ratio between the maximal welfare to the maximal
welfare in a correlated equilibrium. That is, it is the value of a center who can dictate a
course of play with respect to a mediator who can just use correlation devices in equilibrium.
As the maximal welfare in Aumann’s example is 8, the enforcement value in this game equals
6
5.
In this paper we establish general and basic results concerning the measures defined
above. We consider the mediation (enforcement) value of classes of games, which is defined
to be the least upper bound of the mediation (enforcement) values of the games in the class.
We first study general games. Then we consider the important class of congestion games
(Rosenthal, 1973; Monderer & Shapley, 1996). Indeed, this class of games is perhaps the
most applicable to the game theory and CS synergy. In particular, results regarding the
price of anarchy have been obtained for congestion games. We restrict our study to simple
congestion games.
Next we summarize our main results and discuss some related literature.
1.1 Main Results for General Games
Aumann’s example implies that the mediation value of the class of two-player two-strategy
(2×2) games is at least 10/9. In this paper it is proved that the mediation value of this class
equals 4/3. Next, more complex games are studied. In particular we consider two possible
minimal extensions of 2 × 2 games: Two-player games with three strategies for one of the
players and two strategies for the other, and three-player games with two strategies for each
player. It is shown that the mediation value of each of these classes is unbounded, i.e., it
equals ∞. Consequently, the mediation value is unbounded for classes of larger games.5
This should be interpreted as a positive result, showing the extreme power of correlation.
Considering the enforcement value, it is proved that it equals ∞ for the classes of 2 × 2
or larger games. The proof of this result uses games with weakly dominant strategies. We
show, however, that the enforcement value of the class of three-player two-strategy games
without weakly dominated strategies also equals ∞.
1.2 Main Results for Simple Congestion Games
In a simple congestion game there is a set of facilities. Every facility j is associated with a
nonnegative payoff function wj . Every player chooses a facility, say facility j, and receives
wj (k), where k is the number of players that chose facility j.
For completeness we first deal with the simple case, in which there exist only two players,
and show that the mediation value of the class of simple congestion games with two facilities
equals 4/3. In the more general case, in which there are m ≥ 2 facilities, it is proved that
the mediation value is bounded from above by 2. However, it is proved that the mediation
the best-welfare and worst-welfare mixed strategy equilibrium .This choice is a matter of taste, and we
chose the “best” option.
5. A game Γ̃ is larger than the game Γ if it is obtained from Γ by adding players, and/or by adding strategies
to the players in Γ.

578

On The Value of Correlation

value equals 1 for the class of simple congestion games with non-increasing facility payoff
functions.
For the case of more than two players, we show that the mediation value is unbounded
for the class of games with three players and two facilities with non-increasing payoffs. In
contrast,
in the linear case, it is proved that the mediation value is bounded from above
√
by 5+1
≈
1.618 for the class of games with any number of players and two facilities with
2
non-increasing linear payoff functions. We give an example for a game in this class whose
mediation value equals 9/8, leaving open a significant gap.
Additional special theorems are proved for simple congestion games with symmetric
(identical) facilities; It is proved that, for every n ≥ 4, the mediation value is higher than 1
for the class of two symmetric (identical) facilities with non-increasing payoffs and n players.
This further illustrates the power of correlation. Nevertheless, we show that every simple
congestion game with any number of players and any number of symmetric facilities, in
which the facility payoff functions satisfy a certain concavity requirement, the best mixedstrategy equilibrium obtains the maximal welfare, and therefore both the mediation value
and the enforcement value of such a game equal 1.
Finally, we study the enforcement value in the case, in which there exist n players and m
symmetric facilities with arbitrary facility payoff functions. We characterize the set of such
games for which the enforcement value equals 1, and as a result, determine the situations
where correlation allows obtaining maximal welfare.
1.3 Related Literature
We end this introduction with a discussion of some relevant issues in the price of anarchy
literature, and their potential relationships to the concepts of mediation value and enforcement value.6 In many situations it is natural to deal with nonnegative costs rather than
utilities; indeed, the literature on the price of anarchy focused on such models. When
translating the definition of price of anarchy7 to games with utilities and not with costs,
the price of anarchy with utilities is the ratio between the maximal welfare to the minimal
welfare obtained in a mixed-strategy equilibrium. The higher this number is, the value of
a center is higher, where a center can enforce a course of play. Hence, the price of anarchy
with utilities measures the value of a center with respect to anarchy, where a center can
dictate a play, and when anarchy is measured by the worst social outcome reached by rational and selfish agents. Recently, Anshelevich, Dasgupta, Kleinberg, Tardos, Wexler, and
Roughgarden (2004) defined price of stability in models with costs.8 Accordingly, the price
of stability with utilities, is the ratio between the maximal welfare and the maximal welfare
in a mixed-strategy equilibrium. A relevant concept using correlated equilibrium in models
with costs has been defined independently by Christodoulou and Koutsoupias (2005), and
is referred to as the price of stability of correlated equilibria.9 When translated to a model
6. The concept of the price of anarchy has received much attention in the recent computer science literature,
(e.g., Marvonicolas & Spirakis, 2001; Czumaj & Vocking, 2002; Roughgarden, 2002; Roughgarden &
Tardos, 2002).
7. The price of anarchy is defined for games with costs as the ratio between the maximal cost in a mixedstrategy equilibrium to the minimal cost.
8. It is the ratio between the minimal cost in a a mixed strategy equilibrium to the minimal cost.
9. It is the ratio between the minimal cost in a correlated equilibrium to the minimal cost.

579

Ashlagi, Monderer, & Tennenholtz

with utilities, the price of stability of correlated equilibria with utilities is the ratio between
the maximal welfare and the maximal welfare in a mixed-strategy equilibrium, that is it is
the enforcement value.10 However, results proved for one of the ratios in one of the models
cannot be translated to results on the analogous ratio in the other model. This is due the
fact that when moving from one model to the other does not only require multiplication by
a negative constant, e.g. -1, but the numbers also need to be shifted to remain nonnegative;
needless to say, the corresponding ratios can be significantly changed by such shifting.11
We return to this discussion in Section 4.1.

2. Basic Definitions
A finite game in strategic form is a tuple Γ = (N, (S i )i∈N , (ui )i∈N ); N is a nonempty finite
set of players. Unless otherwise specified we assume that N = {1, 2, ...., n}, n ≥ 1. For
each i ∈ N , S i is a finite set of strategies of player i. Let S = S 1 × S 2 × · · · × S n be the
set of strategy profiles (n-tuples). An element of S is s = (si )i∈N . For each i ∈ N and
s ∈ S let s−i = (s1 , ..., si−1 , si+1 , ...sn ) denote the strategies played by everyone but i. Thus
s = (s−i , si ). For each player i ∈ N , let ui : S → R be the utility function of player i. ui (s)
is the utility of player i when the profile of strategies s is played. Γ is called a nonnegative
game if all utilities to all players are nonnegative, i.e., ui : S → R+ for every player i.
A player can also randomize among her strategies by using a mixed strategy - a distribution over her set of strategies. For any finite set C, ∆(C) denotes the set of probability
distributions over C. Thus P i = ∆(S i ) is the set of mixed strategies of player i. Let pi ∈ P i
be a mixed strategy of i. For every si ∈ S i , pi (si ) is the probability that player i plays
strategy si in pi . Every pure strategy si ∈ S i is, with the natural identification, a mixed
strategy psi ∈ P i in which
½
1 ti = s i
i
psi (t ) =
0 ti 6= si .
psi is called a pure strategy, and si is interchangeably called a strategy and a pure strategy
(when it is identified with psi ). Let P = P 1 × P 2 × · · · × P n be the set of mixed strategy
profiles.
Let si , ti ∈ S i be pure strategies of player i. We say that si weakly dominates ti , and ti
is weakly dominated by si if for all s−i ∈ S−i
ui (si , s−i ) ≥ ui (ti , s−i ),
where at least one inequality is strict. We say that si strictly dominates ti , and ti is strictly
dominated by si if all of the above inequalities are strict. If ui (si , s−i ) = u(ti , s−i ) for all
s−i ∈ S−i , we will say that si and ti are equivalent strategies for player i.
Any µ ∈ ∆(S) is called a correlated strategy. Every mixed strategy profile p ∈ P
can be interpreted as the correlated strategy µp , where for every strategy profile s ∈ S,
10. For completeness, one can define the mediation value with costs as the ratio between the minimal cost
at a mixed-strategy equilibrium to the minimal cost at a correlated equilibrium.
11. Interestingly, it can be shown that there exist classes of cost games in which the price of anarchy is
bounded, while the price of anarchy with utilities in the analogous classes of utility games is unbounded.
The class of cost games analogous to the class of utility games given in Example 2 in Section 4.1.2,
constitutes one such example.

580

On The Value of Correlation

Q
µp (s) , ni=1 pi (si ). Whenever necessary we identify p with µp . With a slight abuse of
notation, for every µ ∈ ∆(S), we denote by ui (µ) the expected utility of player i when the
correlated strategy µ ∈ ∆(S) is played, that is:
X
ui (µ) =
ui (s)µ(s).
(1)
s∈S

Naturally, for every p ∈ P we denote ui (p) = ui (µp ). Hence ui (p) is the expected utility
of player i when the mixed strategy profile p is played.
We say that p ∈ P is a mixed-strategy equilibrium if ui (p−i , pi ) ≥ ui (p−i , q i ) for every
player i ∈ N and for every q i ∈ P i . Let p ∈ P be a mixed-strategy equilibrium. If for every
i, pi is a pure strategy, we will also call p a pure-strategy equilibrium.
Definition 1 (Aumann, 1974, 1987) A correlated strategy µ ∈ ∆(S) is a correlated equilibrium of Γ if and only if for all i ∈ N and all si , ti ∈ S i :
X
µ(s−i , si )[ui (s−i , si ) − ui (s−i , ti )] ≥ 0.
(2)
s−i ∈S−i

Consider a third party that picks randomly a pure-strategy profile s ∈ S with respect to
a well-known correlated strategy µ, and recommends privately every player i to play si .
The left hand side of (2) captures the difference in expected utility between playing si , i.e.,
following the recommendation of the third party, and playing some other pure-strategy ti
given that all other players follow their own recommendations. Hence, if this difference is
nonnegative, player i is better off playing si .
It is well-known and easily verified that every mixed-strategy
is a correlated
Pn equilibrium
i (µ). The value u(µ) is
equilibrium. For every correlated strategy µ, let u(µ) ,
u
i=1
called the welfare at µ. Let N (Γ) be the set of all mixed-strategy equilibria in Γ and let
C(Γ) be the set of all correlated equilibria in Γ. We define vC (Γ) and vN (Γ) as follows:
vC (Γ) , max{u(µ) : µ ∈ C(Γ)},
vN (Γ) , max{u(p) : p ∈ N (Γ)}.
Note that vN (Γ) and vC (Γ) are well defined due to the compactness of N (Γ) and C(Γ)
respectively, and the continuity of u. Define opt(Γ) (the maximal welfare) as follows:
opt(Γ) , max{u(µ) : µ ∈ ∆(S)} = max{u(s) : s ∈ S}.
The mediation value of a nonnegative game Γ is defined as follows:
M V (Γ) ,

vC (Γ)
.
vN (Γ)

If both vN (Γ) = 0 and vC (Γ) = 0, we define M V (Γ) to be 1. If vN (Γ) = 0 and vC (Γ) > 0,
M V (Γ) is defined to be ∞. Denote by EV (Γ) the enforcement value of a nonnegative game
Γ. That is,
opt(Γ)
EV (Γ) ,
.
vC (Γ)
581

Ashlagi, Monderer, & Tennenholtz

If both vC (Γ) = 0 and opt(Γ) = 0, we define EV (Γ) to be 1. If vC (Γ) = 0 and opt(Γ) > 0,
EV (Γ) is defined to be ∞. Finally, for a class of nonnegative games A, we define the
mediation value and the enforcement value of this class as follows:
M V (A) , sup M V (Γ);

and EV (A) , sup EV (Γ).

Γ∈A

Γ∈A

One of the tools we need in this paper is linear programming. For any game Γ in
strategic form, C(Γ) is exactly the set of feasible solutions for the following linear program
Pb. Moreover, µ ∈ C(Γ) is an optimal solution for Pb if and only if u(µ) = vC (Γ).

P

max s∈S µ(s)u(s)





s.t.
b
P : µ(s) ≥ 0
∀s ∈ S,

P



s∈S µ(s) = 1,


P
i i −i
i i −i
i i
i
i
i
i
s−i ∈S−i µ(s)[u (t , s ) − u (s , s )] ≤ 0 ∀i ∈ N, ∀(s , t ) ∈ S × S , s 6= t .
The dual problem of Pb has one decision variable for each constraint
in the primal. Let β
P
denote the dual variable associated with the primal constraint s∈S µ(s) = 1. Let αi (ti |si )
denote the dual variable associated with the primal constraint defined by (si , ti ), that is, by
the constraint
X
µ(s)[ui (ti , s−i ) − ui (si , s−i )] ≤ 0,
s−i ∈S−i

and let α = (αi )i∈N , where αi = (αi (ti |si ))(si ,ti )∈S i ×S i , si 6=ti . The dual problem can be
written as follows:


min β



s.t.
b:
D

αi (ti |si ) ≥ 0
∀i ∈ N, ∀(si , ti ) ∈ S i × S i , ti 6= si ,


P
P


i i i
i i −i
i i −i
∀s ∈ S.
i∈N
{ti ∈S i | ti 6=si } α (t |s )[u (t , s ) − u (s , s )] + β ≥ u(s)
b are feasible and bounded, and their objective
It is well known that problems Pb and D
values equal vC (Γ). The feasibility is a consequence of the existence of a mixed-strategy
equilibrium proved by Nash (1951), and from the fact that every mixed-strategy equilibrium
is also a correlated equilibrium.12
We will also make use of the following notation and definitions. Let G be the class of all
nonnegative games in strategic form. For m1 , m2 , ..., mn ≥ 1 denote by Gm1 ×m2 ×···×mn ⊆ G
the class of all games with n players in which |S i | = mi for every player i.

3. Results for General Games
The following two basic lemmas are used in some of the proofs in this paper. The proof
of Lemma 1 follows directly from Definition 1, and the proof of Lemma 2 is standard.
Therefore, these proofs are omitted.
12. An elementary proof of existence of correlated equilibrium, which does not use the existence of a mixedstrategy equilibrium is given by Hart and Schmeidler (1989).

582

On The Value of Correlation

Lemma 1 Let Γ be a game in strategic form. Let si ∈ S i be weakly dominated by some
ti ∈ S i , and let s−i ∈ S−i . Then µ(s) = 0 for every correlated equilibrium µ for which
ui (ti , s−i ) > ui (si , s−i ).
Consequently, if si is a strictly dominated strategy, µ(s) = 0 for every correlated equilibrium
µ.
Next we define extensions of a game by adding a dummy strategy to one of the players, or by
adding a dummy player. Let Γ ∈ Gm1 ×m2 ×···×mn . The game Γ̃ ∈ Gm1 ×···×mi−1 ×(mi +1)×mi+1 ···×mn
is an extension of Γ by adding a dummy strategy to player i if it is obtained from Γ by adding
a strategy to player i such that the utility of all n players equal zero, when player i uses
this new strategy.
The game Γ̃ ∈ Gm1 ×m2 ···×mn ×1 is an extension of Γ by adding a dummy player, player
n + 1, if it is obtained from Γ by adding player n + 1 with a single strategy such that the
utilities of this player are all zeros, and the utility of all n players remain as in Γ. That is,
ui (s, d) = ui (s) for every s ∈ S, where d denotes the unique added strategy of player n + 1.
A game Γ̃ is a trivial extension of Γ if there is a sequence of games,
Γ = Γ0 , Γ1 , · · · , Γt = Γ̃
such that Γk is obtained from Γk−1 by adding a dummy player or a dummy strategy.
Lemma 2 Let Γ̃ be a trivial extension of Γ. Then, M V (Γ̃) = M V (Γ) and EV (Γ̃) =
EV (Γ).
3.1 The Mediation Value
In this section we show the power of correlation in general games. We start with extending
Aumann’s result on the power of correlation in 2 × 2 games.
3.1.1 Two-person two-strategy games
Aumann’s example shows that a mediation value of
We show:

10
9

can be obtained in a 2 × 2 game.

Theorem 1 M V (G2×2 ) = 43 .
The following lemma is needed for the proof of Theorem 1:
Lemma 3 (Peeters & Potters, 1999) Let Γ ∈ G2×2 . If there exist a correlated equilibrium
in Γ, which is not induced by a mixed-strategy equilibrium, Γ has at least two pure-strategy
equilibria.
Before we give the proof of Theorem 1 we need the following technical remarks, which holds
for the rest of the paper:
Remark: In all the games introduced by figures, we denote by player 1,2,3 the row,
column, and matrix (if exist) players respectively. In each strategy profile the players’
utilities are given from left to right where player i’s utility is the ith left payoff.
Proof of Theorem 1: We begin by showing that M V (G2×2 ) ≤ 34 . Figure 1 below describes
an arbitrary game Γ ∈ G2×2 .
583

Ashlagi, Monderer, & Tennenholtz

1

a
a2

b1
a, b
m, n

b2
j, k
c, d

Figure 1
By Lemma 3, If Γ has less than two pure-strategy equilibrium profiles, M V (Γ) = 1.
Therefore, we can assume without loss of generality that Γ has two or more pure-strategy
equilibrium profiles. However, if there are four pure-strategy equilibrium profiles, the mediation value equals 1. Therefore we should discuss the cases in which Γ has either two or
three pure-strategy equilibrium profiles.
Suppose Γ possesses exactly three pure-strategy equilibrium profiles, and without loss of
generality let (a2 , b2 ) be the only strategy profile not in equilibrium. Since (a1 , b1 ), (a2 , b1 )
and (a1 , b2 ) are all pure-strategy equilibria, m = a and b = k. Because (a2 , b2 ) is not an
equilibrium, c < j or d < n. If c < j, by Lemma 1, every correlated equilibrium µ ∈ C(Γ)
satisfies µ(a2 , b2 ) = 0, and therefore M V (Γ) = 1. Similarly, if d < n, µ(a2 , b2 ) = 0 implies
M V (Γ) = 1.
Suppose there are exactly two pure-strategy equilibrium profiles in Γ. These two equilibrium profiles may be in the same row, in the same column, or on a diagonal. Obviously,
the proof for the case that the two pure-strategy equilibrium profiles are in the same row
or in the same column is covered by the following proof, which assumes that the two purestrategy equilibrium profiles are in the first row. That is, (a1 , b1 ) and (a1 , b2 ) are both
pure-strategy equilibria. This assumption implies that b = k. Observe that any strategy
profile in which player 1 plays strategy a1 with probability one, and player 2 plays any
mixed strategy p2 ∈ P 2 , is a mixed-strategy equilibrium. Since there are exactly two purestrategy equilibria, it must be that m < a or c < j. If m < a and c < j, by Lemma 1 every
correlated equilibrium µ ∈ C(Γ) satisfies both µ(a2 , b1 ) = 0 and µ(a2 , b2 ) = 0. Therefore
M V (Γ) = 1. Suppose m = a. Therefore c < j. Hence, by Lemma 1 every correlated
equilibrium µ ∈ C(Γ) satisfies µ(a2 , b2 ) = 0. Since (a2 , b1 ) is not a pure-strategy equilibrium, n < d. Since b = k and n < d, then again by Lemma 1, every correlated equilibrium
µ ∈ C(Γ) satisfies µ(a2 , b1 ) = 0. Therefore M V (Γ) = 1. We showed that M V (Γ) = 1 if
there are two pure-strategy equilibrium profiles, and they are on the same row or the same
column.
We proceed to the last case in which the two pure-strategy equilibrium profiles are on
a diagonal. Without loss of generality let (a1 , b1 ) and (a2 , b2 ) be pure-strategy equilibrium
profiles. It was shown by Peeters and Potters (1999) that if c = j or a = m or b = k or
d = n, C(Γ) is exactly the convex hull of N (Γ). Hence, in this case there is no extreme
point of C(Γ) that is not a mixed-strategy equilibrium, and therefore the mediation value
equals 1. Therefore we can assume:
c > j, a > m, b > k

and d > n.

(3)

If both, u(a1 , b2 ) and u(a2 , b1 ) are smaller than max{u(a1 , b1 ), u(a2 , b2 )}, M V (Γ) = 1 and
the proof is completed. Therefore, without loss of generality we make the following two
assumptions:
(A1) u(a1 , b1 ) ≤ u(a2 , b2 ). That is, a + b ≤ c + d.
584

On The Value of Correlation

(A2) u(a2 , b1 ) ≥ u(a2 , b2 ). That is, m + n ≥ c + d.
Hence, the set of mixed-strategy equilibria is:
N (Γ) = {((1, 0), (1, 0)), ((0, 1), (0, 1)), ((

1
β
1
α
,
), (
,
))},
1+β 1+β 1+α 1+α

b−k
where α = a−m
c−j and β = d−n . Note that because of (3), α and β are positive.
Before continuing with the proof we will need the following geometric characterization of
C(Γ). By Peeters and Potters (1999), C(Γ) is a polyhedron with the following five extreme
points µi , i = 1, ..., 5:
!
Ã
µ
¶
µ
¶
α
1
1, 0
0, 0
(1+α)(1+β) , (1+α)(1+β)
,
µ1 =
, µ2 =
, µ3 =
αβ
β
0, 0
0, 1
, (1+α)(1+β)
(1+α)(1+β)
Ã
!
Ã
!
1
α
1
0
(1+α+αβ) , (1+α+αβ)
(1+β+αβ) ,
µ4 =
, µ5 =
,
αβ
αβ
β
0,
(1+α+αβ
(1+β+αβ) , (1+β+αβ)

where µi (j, k) denotes the probability given to the strategy profile (aj , bk ) in the correlated equilibrium µi . That is, the (j, k)th entry of µi is µi (aj , bk ). By our agreement
to identify mixed-strategy profiles with correlated strategies we observe that the set of
mixed-strategy equilibrium is precisely
N (Γ) = {µ1 , µ2 , µ3 }.

(4)

We have to prove that u(µ) ≤ 43 vN (Γ) for every correlated equilibrium µ ∈ C(Γ). It is
sufficient to prove that the inequality holds for the extreme points of C(Γ). Since µ1 , µ2
and µ3 are mixed-strategy equilibria, u(µi ) ≤ vN (Γ) for i = 1, 2, 3. Therefore it suffices to
prove the inequality for µ4 and µ5 .
We first derive a couple of inequalities which will be useful for us. By (A1) and since
all utilities are nonnegative, a ≤ c + d and d ≤ c + d. Therefore, since m < a and n < d, we
obtain the inequality
m + n ≤ 2(c + d).
(5)
Since (a1 , b1 ) and (a2 , b2 ) are the only pure-strategy equilibrium profiles, m + n ≤ a + d and
j + k ≤ c + b. By (A2) and since m + n ≤ a + d, c ≤ a. Hence j + k ≤ a + b. Therefore, by
(A1) we obtain
j + k ≤ c + d.
(6)
Note that inequality (6) implies that u(µ4 ) ≤ vN (Γ) since (a2 , b2 ) is a pure-strategy
equilibrium.
It remains to show that u(µ5 ) ≤ vN (Γ). By (A1), u(µ1 ) ≤ u(µ2 ). We distinguish
between the following two cases:
Case 1: u(µ3 ) ≥ u(µ2 ).
By (A1), u(µ1 ) ≤ u(µ2 ). Therefore, by (4), vN (Γ) = u(µ3 ).
Hence,
a + b + (m + n)β + (c + d)αβ
1 + β + α + αβ
u(µ5 )
=
×
.
vN (Γ)
1 + β + αβ
a + b + (j + k)α + (m + n)β + (c + d)αβ
585

Ashlagi, Monderer, & Tennenholtz

Therefore, because (j + k)α ≥ 0,
u(µ5 )
1 + β + α + αβ
≤
.
vN (Γ)
1 + β + αβ
For z > 0, let f1 (z) =
Let K =
profiles,

m+n−c−d
c+d−j−k .

1+β+z+zβ
1+β+zβ .

Hence, it suffices to show that f1 (α) ≤ 43 .

Since (a1 , b1 ) and (a2 , b2 ) are both pure-strategy equilibrium

m + n − c − d ≤ a − c,

c + d − j − k ≥ d − b.

Therefore by (A1),
K ≤ 1.
Since u(µ3 ) ≥ c + d,
a + b + (j + k)α + (m + n)β ≥ (c + d)(1 + α + β).
Therefore
α≤

β(m + n − c − d) + a + b − c − d
.
c+d−j−k

(7)

By Equation (7) and (A1), α ≤ βK. Since, K ≤ 1, α ≤ β. Note that f1 (z) is
non-decreasing in z > 0. Therefore,
f1 (α) ≤ f1 (β) =
Since

1+2β+β 2
1+β+β 2

=

(β+1)2
(β+1)2 −β

1 + 2β + β 2
.
1 + β + β2

(8)

is maximized for β > 0 at β = 1,
4
f1 (α) ≤ .
3

Case 2: u(µ3 ) < u(µ2 ).
By (A1), u(µ1 ) ≤ u(µ2 ). Therefore, by (4), vN (Γ) = u(µ2 ). Therefore,
u(µ5 )
a + b + (m + n)β + (c + d)αβ
=
.
vN (Γ)
(1 + β + αβ)(c + d)
For z > 0 let f2 (z) =

a+b+(m+n)β+(c+d)zβ
.
(1+β+zβ)(c+d)

(9)

Hence, It suffices to show that f2 (α) ≤ 43 .

Not that in this case the inequality (7) is reversed. That is
α≥

β(m + n − c − d) + a + b − c − d
,
c+d−j−k

α≥

β(m + n − c − d) + a + b − c − d
.
c+d

and since j, k ≥ 0,

(10)

We distinguish between the following two cases, noticing that f2 (x) is non-increasing
for x > 0:
586

On The Value of Correlation

Case 2.1: a + b = c + d: In this case, because f2 is non-increasing and (10) holds,
f2 (α) ≤

c + d + (m + n)β + (m + n − c − d)β 2
(1 + β +

(m+n−c−d) 2
β )(c
c+d

+ d)

.

(11)

Set x = c + d. Therefore, by (A2), there exists 1 ≤ t ≤ 2 for which tx = m + n.
By (11)
f2 (α) ≤

1 + tβ + (t − 1)β 2
4
x + txβ + (t − 1)xβ 2
=
≤ ,
2
2
x + xβ + (t − 1)xβ
1 + β + (t − 1)β
3

(12)

where the last inequality follows from similar arguments to those following (8).
Case 2.2: a + b < c + d:
Set x = a + b. Therefore by (A1), (A2) and (5), for some t > 1 and for some
1 ≤ k ≤ 2,
c + d = tx and m + n = ktx.
(13)
Hence,
kt =

a+b+c+d
m+n
≤
= t + 1,
a+b
a+b

where the inequality follows since m + n ≤ a + b + c + d. Therefore
t≤

1
.
k−1

(14)

Therefore
f2 (α) ≤

a + b + (m + n)β + (c + d)[ β(m+n−c−d)+a+b−c−d
]β
c+d
(1 + β +

β(m+n−c−d)+a+b−c−d
β)(c
c+d

+ d)

=

1 + ktβ + t(k − 1)β 2 + β − tβ
1 + β + tβ(β + 1)(k − 1)
=
≤
2
t + tβ + t(k − 1)β + β − tβ
t + tβ 2 (k − 1) + β

(15)

1 + β 2 + 2β
(β + 1)2
4
≤
≤ ,
1 + β 2 (k − 1) + β
(β + 1)2 − β
3

(16)

where the right inequality in (15) follows from (14), the left inequality in (16) follows since 1 ≤ k ≤ 2, and the right inequality in (16) follows since the maximum
(β+1)2
value of (β+1)
2 −β is attained at β = 1.
We showed that the mediation value is bounded from above by 43 . It remains to show that
this bound is tight. We show a family of games for which the mediation value approaches the
above 43 bound. Consider the family of games Γx shown in Figure 2 (a variant of Aumann’s
example), where x > 1.

587

Ashlagi, Monderer, & Tennenholtz

1

a
a2

b1
x,1
x-1,x-1

b2
0,0
1,x

Figure 2
In this game the strategy profiles (a1 , b1 ) and (a2 , b2 ) are both pure-strategy equilibrium
profiles, and u(a1 , b1 ) = u(a2 , b2 ) = x+1. There exists one more mixed-strategy equilibrium,
in which each player assigns the probability 0.5 to each of her strategies, which yields a
welfare lower than x + 1. The correlated strategy µ ∈ ∆(S), where each of the strategy
profiles (a1 , b1 ) ,(a2 , b1 ) and (a2 , b2 ) is played with equal probability 1/3 is a correlated
4x
4
equilibrium and u(µ) = 4x
3 . Hence M V (Γx ) ≥ 3(x+1) . Therefore M V (Γx ) → 3 when
x → ∞. 2
3.1.2 General Games
Theorem 1 shows that the mediation value of the class of nonnegative 2 × 2 games is finite.
In the next theorem we show that the mediation value is unbounded as we move slightly
beyond 2 × 2 games. In particular, if one of the players in a 2-player game has at least
three strategies, while the other remains with two strategies, the mediation value is already
unbounded. Similarly, if there are three players each with two strategies, the mediation
value is again unbounded. 13
Theorem 2 M V (Gm1 ×m2 ) = ∞ for every m1 , m2 ≥ 2 for which max(m1 , m2 ) ≥ 3.
Proof: By Lemma 2 it suffices to prove that M V (G3×2 ) = ∞. Let Γx,² be the following
parametric G2×3 game In Figure 3,
1

a
a2

b1
x, 1 − ²
0, z − ²

b2
z, 1
z − 1, z − 1

b3
0, 0
1, z

Figure 3
where z > 2 is fixed, x > z and 0 < ² < 0.5.
We first show that
N (Γx,² ) = {((1, 0)(0, 1, 0)), ((0, 1), (0, 0, 1)), ((², 1 − ²), (

1
x
, 0,
))}.
1+x
1+x

It is standard to check that each profile in N (Γx,² ) is indeed a mixed-strategy equilibrium,
and that if player 1 plays a pure-strategy in equilibrium, the only mixed-strategy equilibrium
are ((1, 0)(0, 1, 0)) and ((0, 1), (0, 0, 1)). We have to show that if player 1 plays fully mixed
1
x
(that is, he assigns positive probabilities to both a1 and a2 .) , ((², 1 − ²), ( 1+x
, 0, 1+x
)) is the
only mixed-strategy equilibrium. Indeed, note that if player 2 plays a pure strategy, player
1 is never indifferent between a1 and a2 and therefore player 1 is always better off deviating
to some pure strategy. If player 2 assigns positive probability only to b1 and b2 , player 1
is better off playing a1 . If player 2 assigns positive probability only to b2 and b3 , the only
13. The results presented in this paper showing that the mediation value may be ∞, can also be established
when we assume that the utilities are uniformly bounded, e.g. when all utilities are in the interval [0, 1].

588

On The Value of Correlation

mixed-strategy player 1 can play in order that player 2 will be indifferent between b2 and b3
is (1/2, 1/2), but then player 2 is better off playing b1 . Similarly, it can be shown that there
does not exist an equilibrium in which player 1 fully mixes and player 2 assigns positive
probabilities to all of her pure strategies. Suppose player 2 assigns positive probabilities
1
only to b1 and b3 . If the probability that player 2 assigns to b1 is higher (lower) than 1+x
,
1
x
, 0, 1+x
) in equilibrium in
player 1 is better off playing a1 (a2 ). Hence, player 2 plays ( 1+x
which player 1 fully mixes, and therefore It is standard to prove that player 1 must play
(², 1 − ²).
Next we show that vN (Γx,² ) ≤ z + 2 for small enough ². Notice that the welfare in both
pure-strategy equilibria is z + 1. The welfare in the mixed-strategy equilibrium ((², 1 −
1
x
²), ( 1+x
, 0, 1+x
) is:
²(x + 1 − ²) + (1 − ²)(z − ²) + x(1 − ²)(z + 1)
=
x+1
z − z² +

x
.
x+1

x
Note that z − z² + x+1
→ z + 1 as x → ∞ and ² → 0.
We proceed to show a net of games, Γx,² such that vC (Γx,² ) → ∞ as ² → 0 and x → ∞.
Let µ be a correlated strategy. µ is a correlated equilibrium in Γx,² if and only if the following
9 inequalities are satisfied (in brackets we relate the inequality here to the corresponding
inequality in (2)):

1. µ(a1 , b1 )x + µ(a1 , b2 ) − µ(a1 , b3 ) ≥ 0.

(i = 1, si = a1 , ti = a2 )

2. µ(a2 , b3 ) − µ(a2 , b2 ) − xµ(a2 , b1 ) ≥ 0.

(i = 1, si = a2 , ti = a1 )

3. µ(a1 , b1 ) ≤

(1−²)µ(a2 ,b1 )
.
²

4. µ(a1 , b1 ) ≥

²µ(a2 ,b1 )
1−² .

5. µ(a1 , b2 ) ≥

(1−²)µ(a2 ,b2 )
.
²

(i = 2, si = b1 , ti = b3 )

6. µ(a1 , b2 ) ≥ µ(a2 , b2 ).
7. µ(a2 , b3 ) ≥

(i = 2, si = b1 , ti = b2 )

(1−²)µ(a1 ,b3 )
.
²

(i = 2, si = b2 , ti = b1 )
(i = 2, si = b2 , ti = b3 )
(i = 2, si = b3 , ti = b1 )

8. µ(a2 , b3 ) ≥ µ(a1 , b3 ). (i = 2, si = b3 , ti = b2 )
P2 P3
i j
9.
i=1
j=1 µ(a , b ) = 1.
Set µ(a1 , b1 ) = P
², µ(a2P
, b1 ) = 2²2 , µ(a1 , b2 ) = ²(1 − ²), µ(a1 , b3 ) = µ(a2 , b2 ) = ²2 , µ(a2 , b3 ) =
2
1
3
1 − µ(a , b ) − i=1 2j=1 µ(ai , bj ) and let x = 4²12 . Let ² → 0. All nine constraints are
satisfied for small enough ². Note that lim²→0 xµ(a1 , b1 ) = ∞. Since M V (Γx,² ) ≥ xµ(a1 , b1 )
we obtain M V (Γx,² ) → ∞ as ² → 0. 2
Theorem 3 M V (Gm1 ×···×mn ) = ∞ for every n ≥ 3 and for every m1 , m2 , . . . , mn ≥ 2.
589

Ashlagi, Monderer, & Tennenholtz

Proof: By Lemma 2 it suffices to prove that M V (G2×2×2 ) = ∞. Consider the following
three player game Γ (Figure 4):
a1
a2

b1
0, 0, 0
0, 0, 0

b2
2, 0, 0
1, 0, 1

a1
a2

c1

b1
4, 4, 0
5, 0, 0

b2
0, 0, 1
0, 3, 0
c2

Figure 4
We will show that vN (Γ) = 0, that is, the welfare in every mixed-strategy equilibrium
is zero. In addition we will construct a correlated equilibrium, which yields a strictly
positive welfare. We begin with proving that vN (Γ) = 0. Note that the only pure-strategy
equilibrium profiles in the game are (a1 , b1 , c1 ) and (a2 , b1 , c1 ). Obviously, every strategy
profile in which players 2 and 3 play b1 and c1 respectively, and player 1 plays any mixed
strategy, is a mixed-strategy equilibrium. We next show that there are no other mixedstrategy equilibria in the game. First we show that there are no other mixed-strategy
equilibria in which at least one of the players plays a pure strategy. We verify this for each
player:
1. Assume player 3 plays c2 with probability one. If 1 > p2 (b1 ) > 0, p1 (a2 ) = 1, but then
player 3 is better off playing c1 . If p2 (b1 ) = 1, p1 (a2 ) = 1, but then player 2 is better
off playing b2 . If p2 (b2 ) = 1, player 1 is indifferent. If p1 (a1 ) ≥ 0.5, player 2 is better
off playing b1 . If p1 (a1 ) < 0.5, player 3 is better off playing c1 .
2. Assume player 3 plays c1 with probability one. If p2 (b2 ) > 0, p1 (a1 ) = 1, but then
player 3 is better off playing c2 .
3. Assume player 2 plays b1 with probability one. Player 3 is indifferent between her
strategies. If p3 (c2 ) > 0, p1 (a2 ) = 1, but then player 2 is better off playing b2 . p3 (c1 ) =
1 has been dealt in the previous case. Assume player 2 plays b2 with probability one.
If p3 (c1 ) > 0, p1 (a1 ) = 1, but then player 3 is better off playing c2 .
4. Assume player 1 plays a1 with probability one. If p2 (b2 ) > 0, p3 (c2 ) = 1, but then
player 2 is better off playing b1 with probability one. Assume player 1 plays a2 with
probability one. If p3 (c2 ) > 0, p2 (b2 ) = 1, but then player 3 is better off playing c1
with probability one.
Next we prove that there does not exist a completely mixed-strategy equilibrium, (an equilibrium in which every player assigns a positive probability to all of her strategies). Suppose
in negation that this is not the case. Let ((p, 1 − p), (q, 1 − q), (h, 1 − h)) be a completely
mixed-strategy equilibrium, that is 1 > p, q, h > 0. By the equilibrium properties, player
2 is indifferent between b1 and b2 given that players 1 and 3 play (p, 1 − p) and (h, 1 − h)
respectively. Hence 4p(1 − h) = 3(1 − p)(1 − h), which implies that p = 37 . Similarly, player
3 is indifferent between c1 and c2 given that players 1 and 2 play (p, 1 − p) and (q, 1 − q)
respectively. Therefore (1 − p)(1 − q) = p(1 − q) yielding p = 0.5, which is a contradiction.
Therefore, there does not exist a completely mixed-strategy equilibrium.
We proved that vN (Γ) = 0. It remains to prove that there exists a correlated equilibrium
with a a strictly positive welfare; this will imply that the mediation value is infinity. Let
590

On The Value of Correlation

µ ∈ ∆(S) be the following correlated strategy. µ(a1 , b2 , c1 ) = µ(a2 , b2 , c1 ) = µ(a1 , b1 , c2 ) =
µ(a2 , b1 , c2 ) = 0.25 and for all other s ∈ S, µ(s) = 0. We proceed to prove that µ is
a correlated equilibrium. Indeed, observe that the following inequalities, which define a
correlated equilibrium are satisfied (as usual, in brackets we relate the inequality here to
the corresponding inequality in (2)):
1. µ(a1 , b2 , c1 )(2 − 1) + µ(a1 , b1 , c2 )(4 − 5) ≥ 0.

(i = 1, si = a1 , ti = a2 )

2. µ(a2 , b2 , c1 )(1 − 2) + µ(a2 , b1 , c2 )(5 − 4) ≥ 0.

(i = 1, si = a2 , ti = a1 )

3. µ(a1 , b2 , c1 )(0 − 0) + µ(a2 , b2 , c1 )(0 − 0) ≥ 0.

(i = 2, si = b2 , ti = b1 )

4. µ(a1 , b1 , c2 )(4 − 0) + µ(a2 , b1 , c2 )(0 − 3) ≥ 0.

(i = 2, si = b1 , ti = b2 )

5. µ(a1 , b2 , c1 )(0 − 1) + µ(a2 , b1 , c1 )(1 − 0) ≥ 0.

(i = 3, si = c1 , ti = c2 )

6. µ(a1 , b1 , c2 )(0 − 0) + µ(a2 , b1 , c2 )(0 − 0) ≥ 0. (i = 3, si = c2 , ti = c1 )
2
The proof of Theorem 3 was based on showing that there exists a three-player nonnegative game, in which not all utilities are zero, yet the welfare in every mixed-strategy
equilibrium is zero. The next lemma shows that this phenomenon can not happen in a two
player game.
Lemma 4 Let Γ ∈ Gn×m , n, m ≥ 1. vN (Γ) = 0 implies that all the utilities in Γ are zero.
That is, for i = 1, 2,
ui (t1 , t2 ) = 0 ∀t1 ∈ S 1 , ∀t2 ∈ S 2 .

Proof: The proof is by induction on the total number of pure strategies n + m in the game.
First note that the assertion holds for all games, Γ ∈ Gn×m for which n + m = 2 because in
this case each player has exactly one strategy. Let k ≥ 2, and assume the assertion holds
for every game, Γ ∈ Gn×m for which n + m ≤ k.
Let Γ ∈ Gn×m be a game in which n + m = k + 1 and for which vN (Γ) = 0. We should
prove that Γ is the zero game. As k + 1 ≥ 3, there exists at least one player that has more
than one strategy; without loss of generality, player 1 is such a player.
Let p = (p1 , p2 ) be a mixed-strategy equilibrium in Γ, that is, p ∈ N (Γ). Because
vN (Γ) = 0, the welfare at p equals 0. That is, u(p) = u1 (p) + u2 (p) = 0. Because utilities
are nonnegative,
u1 (p) = 0 = u2 (p).
(17)
Let s1 be an arbitrary fixed strategy for player 1 for which p1 (s1 ) > 0, and let s2 be an
arbitrary fixed strategy for player 2 for which p2 (s2 ) > 0.
Claim 1: (i) u1 (t1 , s2 ) = 0 ∀t1 ∈ S 1 . (ii) u2 (s1 , t2 ) = 0 ∀t2 ∈ S 2 .
Proof of Claim 1. Let t1 ∈ S 1 . Since (p1 , p2 ) is a mixed-strategy equilibrium
, u1 (t1 , p2 ) ≤
P
1
1
2
1
1
1
2
1
1
2
u (p , p ) = 0. Since u is nonnegative, u (t , p ) = 0. Since u (t , p ) = t2 ∈S 2 p2 (t2 )u1 (t1 , t2 ),
u1 is nonnegative, and p2 (s2 ) > 0, u1 (t1 , s2 ) = 0. This proves (i). (ii) is similarly proved. 2
Consider the game Γ∗ ∈ G(n−1)×m obtained by removing s1 from S 1 ; In this game the
strategy set of player 1 is T 1 = S 1 \{s1 } and the strategy set of player 2 remains S 2 . With a
591

Ashlagi, Monderer, & Tennenholtz

slight abuse of notations The utility functions in Γ∗ are also denoted by u1 and u2 . Claim
2: All utilities in Γ∗ are zero.
Proof of Claim 2 :
Assume in negation that not all utilities in Γ∗ are zero. By the induction hypothesis
there exists a mixed-strategy equilibrium (q 1 , q 2 ) ∈ N (Γ∗ ) such that
u(q 1 , q 2 ) > 0.

(18)

Extend q 1 to the mixed strategy r1 of player 1 in Γ by defining r1 (s1 ) = 0. Obviously,
ui (r1 , q 2 ) = ui (q 1 , q 2 ) for i = 1, 2 and therefore (18) implies
u(r1 , q 2 ) > 0.

(19)

Since vN (Γ) = 0, (19) implies that (r1 , q 2 ) is not a mixed-strategy equilibrium in Γ. However, since r1 coincide with q 1 on T 1 , r1 (s1 ) = 0, and (q 1 , q 2 ) is a mixed-strategy equilibrium
in Γ∗ , player 2 does not have a profitable deviation from q 2 in Γ when player 1 uses r1 .
Therefore, player 1 must have a profitable deviation from r1 in Γ when player 2 uses q 2 .
In particular, player 1 must have a pure-strategy profitable deviation. Since (q 1 , q 2 ) is a
mixed-strategy equilibrium in Γ∗ , the only potential pure-strategy profitable deviation of
player 1 in Γ is s1 . Therefore,
u1 (s1 , q 2 ) > u1 (r1 , q 2 ),

(20)

u1 (s1 , q 2 ) > 0.

(21)

which implies in particular that
Next we show that (s1 , q 2 ) ∈ N (Γ). Since (q 1 , q 2 ) is a mixed strategy equilibrium in Γ∗ ,
u1 (q 1 , q 2 ) ≥ u1 (t1 , q 2 ) for every t1 ∈ S 1 , t1 6= s1 . Since r1 (s1 ) = 0, u1 (r1 , q 2 ) ≥ u1 (t1 , q 2 ) for
every t1 ∈ S 1 , t1 6= s1 . Therefore, by (20), s1 is a best-response to q 2 in Γ. By (ii) in Claim
1, every pure-strategy of player 2 is a best-response to s1 in Γ and hence, every mixedstrategy of player 2 is a best response to s1 , and in particular so is q 2 . Therefore, (s1 , q 2 )
is indeed a mixed-strategy equilibrium in Γ. Since vN (Γ) = 0, u1 (s1 , r2 ) = 0, contradicting
(21). This complete the proof of Claim 2.2
By Claim 2 and by (ii) in Claim 1, (s1 , t2 ) is in N (Γ) for every t2 ∈ S 2 . Since vN (Γ) = 0,
1
u(s , t2 ) = 0 for every t2 ∈ S 2 . Hence, for i = 1, 2, ui (s1 , t2 ) = 0 for every t2 ∈ S 2 . As each
ui is identically zero in Γ∗ , it follows that for i = 1, 2, ui (t1 , t2 ) = 0 for every t1 ∈ S 1 and
for every t2 ∈ S 2 .2
3.2 The Enforcement Value
The next theorem shows that the enforcement value is unbounded even in classes of small
games.
Theorem 4 EV (Gm1 ×···×mn ) = ∞ for every n ≥ 2 and for every m1 , m2 ≥ 2.
Proof: By Lemma 2 it suffices to prove the theorem for the case n = 2, m1 = 2, and m2 = 2.
Consider the following parametric Prisoners’ Dilemma game Γx , x > 1, given in Figure 5:

592

On The Value of Correlation

1

a
a2

b1
1, 1
0, x + 1

b2
x + 1, 0
x, x

Figure 5
By Lemma 1, every such game has a unique correlated equilibrium (a1 , b1 ) whose welfare
is 2. However for every x ≥ 1, opt(Γx ) = 2x. Therefore EV (Γx ) → ∞ when x → ∞. 2
The proof of Theorem 4 is based on constructing Prisoner’s Dilemma games with a
parameter x. In particular every player has a weakly dominant strategy in each one of these
games. The next theorem shows that even when ruling out weakly dominant strategies, the
enforcement value is unbounded.
Theorem 5 EV ({Γ|Γ ∈ G2×2×2 , no player has a weakly dominant strategy} = ∞.
Proof: Consider the family of parametric games Γz,² (Figure 6), where z =
1

a
a2

a1
4 − ², 4 − ², 4 − ²
4 + ², 4, 4

a2
4, 4 + ², 4
0, 0, z

1

a
a2

a1

a1
4, 4, 4 + ²
0, z, 0

1
²2

a2
z, 0, 0
0, 0, 0
a2

Figure 6
First observe that opt(Γz,² ) = z for every 0 < ² ≤ 0.25. In order to prove the result
b By the weak duality theorem every feasible solution (α, β)
we use the dual program D.
b satisfies β ≥ vC (Γ). Let x1 , x2 , x3 denote α1 (a1 |a2 ), α2 (a1 |a2 ) and
for the dual problem D
α3 (a1 |a2 ) respectively. Let y1 , y2 , y3 denote α1 (a2 |a1 ), α2 (a2 |a1 ) and α3 (a2 |a1 ) respectively.
The dual constraints can be written in the following way (recall that z = ²12 ):
2²y1 + 2²y2 + 2²y3 + β ≥ 12 − 3²,
−4y1 − 4y3 − 2²x2 + β ≥ 12 + ²,
−4y2 − 4y3 − 2²x1 + β ≥ 12 + ²,
−4y1 − 4y2 − 2²x3 + β ≥ 12 + ²,
1
1
− 2 y1 + 4x2 + 4x3 + β ≥ 2 ,
²
²
1
1
− 2 y2 + 4x1 + 4x3 + β ≥ 2 ,
²
²
1
1
− 2 y3 + 4x1 + 4x2 + β ≥ 2 ,
²
²
1
1
1
x1 + 2 x2 + 2 x3 + β ≥ 0.
²2
²
²
Set y1 = y2 = y3 = x1 = 0, β = 1² , and x2 = x3 = 4²12 , and observe that it is a feasible
solution for every sufficiently small ² > 0. Note that EV (Γz,² ) ≥ βz . However βz = 1² → ∞
as ² → 0, which completes the proof. 2
593

Ashlagi, Monderer, & Tennenholtz

4. Simple Congestion Games
In this section we explore the mediation and enforcement values in simple congestion games.
We first need a few notations and definitions.
A simple congestion form F = (N, M, (wj )j∈M ) is defined as follows. N is a nonempty
set of players; we keep our assumption that, whenever is convenient, N = {1, 2, . . . , n},
where n = |N | is the number of players. M is a nonempty set of facilities; Unless otherwise
specified we assume M = {1, 2, ..., m}. For j ∈ M , let wj ∈ Rn be the facility payoff
function, where for every 1 ≤ k ≤ n, wj (k) denotes the payoff of each user of facility j,
if there are exactly k users. A congestion form is nonnegative if for every j ∈ M , wj is
nonnegative. Let Q be the class of all nonnegative simple congestion forms and denote
by Qn×m ⊆ Q the class of all nonnegative simple congestion forms with n players and m
facilities. Every simple congestion form F = (N, M, (wj )j∈M ) defines a simple congestion
game ΓF = (N, (S i )i∈N , (ui )i∈N ), where N is the set of players, S i = M for every player
i ∈ N , and the utility functions (ui )i∈N are defined as follows.
Let S = ×i∈N S i . For every A = (A1 , A2 , ..., An ) ∈ S and every j ∈ M , let σj (A) =
|{i ∈ N : Ai = j}| be the number of users of facility j. Define ui : S → R by
ui (A) = wAi (σAi (A)).

(22)

We say that a facility j is non-increasing if wj (k) is a non-increasing function of k.
Define QN n×m ⊆ Qn×m as follows:
QN n×m , {F ∈ Qn×m | all facilities in F are non-increasing}
We will call a facility j linear if there exist a constant dj such that wj (k + 1) − wj (k) = dj
for every k ≥ 1, that is wj (k) = dj k + δj for every k ≥ 1, where δj is a constant.
Let F be a simple congestion form with n players and m facilities. A congestion vector
π = π(n, m) isPan m-tuple π = (πj )j∈M , where π1 , π2 , ..., πm ∈ Z ∗ (the set of nonnegative
integers) and m
j=1 πj = n. π represents the situation in which πj players choose facility
j. Every strategy profile A P∈ S uniquely determines a congestion vector π A . Note that
¡ ¢¡
¢
¡
¢
m−2
1
j=1 πj
there are πn1 n−π
· · · n− πm−1
strategy profiles in the game ΓF that correspond to
π2
a congestion vector π, and denote by Bπ the set of all such strategy profiles. Thus Bπ =
{A ∈ S| π A = π}. Given a congestion vector π, all
P strategy profiles in Bπ have the same
welfare which we denote by u(π). That is u(π) = {j∈M | πj >0} πj wj (πj ).
We say that a congestion vector π is in equilibrium if every strategy profile in Bπ is a purestrategy equilibrium. A congestion form is called facility symmetric if wj = wk ∀j, k ∈ M .
Obviously, a facility symmetric congestion form induces a symmetric game in strategic form.
Let In×m ⊆ Qn×m be defined by
In×m = {F ∈ Qn×m | F is facility symmetric }.
Define IN n×m ⊆ In×m as follows
IN n×m = {F ∈ In×m | all facilities in F are non-increasing}.
594

On The Value of Correlation

4.1 The Mediation Value
Although congestion games are especially interesting when the number of players is large,
we begin with presenting results for the case in which we have only two players, extending
upon the results in the previous section. Following that, we consider the more general
n-player case.
4.1.1 The two-player case (n = 2)
Theorem 1 provides a 43 a tight upper bound for the mediation value in the class of games
G2×2 . Hence, 43 is obviously an upper bound for the mediation value of nonnegative simple
congestion games with two players and two facilities, i.e., games generated by congestion
forms in Q2×2 . It turns out that this bound is tight:
Theorem 6 M V ({ΓF |F ∈ Q2×2 }) = 43 .
Proof: Let Fx , x > 1, be the following simple congestion form: M = {a1 , a2 }, wa1 = (x, 0),
and wa2 = (1, x − 1) . The games ΓFx , x > 1 are obtained from the games, Γx defined in
the proof of Theorem 1 (See Figure 2) by renaming the strategies of player 2. Hence, by
what is proved in Theorem 1, M V (ΓFx ) → 34 as x → ∞. 2
Next we consider the more general case in which two players can choose among m
facilities:
Theorem 7 M V ({ΓF |F ∈ Q2×m }) ≤ 2 for every m ≥ 2.
Proof: Let F ∈ Q2×m . By Rosenthal (1973) there exists a pure-strategy equilibrium in ΓF .
Let A be a fixed pure-strategy equilibrium with the largest welfare, that is
u(A) ≥ u(D) for every pure-strategy equilibrium D.

(23)

Let j and k be the facilities that players 1 and 2 choose in A respectively. We prove by
separation to two cases, j = k and j 6= k.
Case 1: j = k. We prove the theorem in this case by separation to cases wj (1) > wj (2)
and wj (1) ≤ wj (2).
Case 1.1: wj (1) > wj (2). In this case we show that j is a strictly dominant strategy,
which implies by Lemma 1 that M V (ΓF ) = 1. Indeed, let h 6= j be a facility. In order
to prove that j strictly dominates h we have to show that the following three inequalities
hold: wh (1) < wj (2), wh (2) < wj (1), and wh (1) < wj (1). However, since wj (1) > wj (2),
only the first two inequalities should be proved. We first prove that wh (1) < wj (2): Since
A is a pure strategy equilibrium in which both players choose j, wh (1) ≤ wj (2). Suppose
in negation that wh (1) = wj (2). Since A is a pure-strategy equilibrium, wj (2) ≥ wl (1) for
every l 6= j, and therefore, wh (1) ≥ wl (1) for every l 6= j. If, in addition, wj (1) ≥ wh (2),
the strategy profile in which one player chooses h, and the other player chooses j is a purestrategy equilibrium and obtains a larger welfare than A contradicting (23). Therefore,
wh (2) > wj (1). This implies that the strategy profile in which both players choose h is
a pure-strategy equilibrium and obtains a larger welfare than A, again a contradiction to
(23). Therefore wh (1) < wj (2).
595

Ashlagi, Monderer, & Tennenholtz

We next show that wh (2) < wj (1). If wh (2) ≥ wj (1) the strategy profile in which both
players choose h is a pure-strategy equilibrium and 2wh (2) ≥ 2wj (1) > 2wj (2) contradicting
(23). This completes the proof in Case 1.1.
Case 1.2: wj (1) ≤ wj (2). Let B be a pure strategy profile in which the maximum
welfare is obtained. That is, u(B) = opt(ΓF ). In order to prove the theorem it suffices to
prove that
u(B) ≤ 2u(A).
(24)
Suppose in B the players choose distinct facilities (one of these facilities can be j).
Since A is a pure-strategy equilibrium, wl (1) ≤ wj (2) for every l 6= j. Therefore, since
also wj (1) ≤ wj (2), u(B) ≤ u(A) and in particular (24) holds. Suppose the two players
choose the same facility in B, and denote this facility by s. If s = j B = A and (24) holds.
Therefore we assume s 6= j. If ws (2) ≤ wj (2), u(B) ≤ u(A), which implies (24). Therefore
we can assume ws (2) > wj (2) and in particular ws (2) > wj (1). Since wj (2) ≥ wl (1) for
every facility l, l 6= j, B is a pure-strategy equilibrium and u(B) > u(A), contradicting
(23). Hence the proof of Case 1.2 is complete.
Case 2: j 6= k. Let B be an arbitrary pure strategy profile in which the maximum
welfare is obtained. That is, u(B) = opt(ΓF ). It suffices to prove that (24) holds.
Recall that A is a pure-strategy equilibrium. If in B each player chooses a different
facility, u(B) ≤ u(A) and in particular (24) holds. Therefore we assume that in B both
players choose the same facility l (l can be j or k). We claim that
wl (2) ≤ max{wj (1), wk (1)}.

(25)

Indeed, if (25) does not hold, B is a pure strategy equilibrium, because a player in B does
not want to deviate to either j or k, and she does not want to deviate to any other facility
because A is a pure strategy equilibrium. Since u(B) > u(A) this contradicts (23). Since
u(B) = 2wl (2) and max{wj (1), wk (1)} ≤ u(A), (25) implies (24). 2
Notice that Theorems 6 and 7 imply that correlation can help in congestion games with
only two players. the next theorem shows that correlation cannot help increasing social
welfare when the facilities are non-increasing:
Theorem 8 M V (QN 2×m ) = 1 for every m ≥ 2.
Proof: Let F ∈ QN 2×m . Let j ∈ M be such that wj (1) ≥ wl (1) for all l ∈ M . If
wj (2) > wl (1) for all l 6= j, j is a strictly dominant strategy, which implies by Lemma 1
that M V (ΓF ) = 1. Suppose there exist some facility k,k 6= j such that wj (2) ≤ wk (1).
Choose such k for which wk (1) is maximal, that is wl (1) ≤ wk (1) for every l, l 6= j.
Therefore, the strategy profile in which one player chooses j and the other player chooses
k is a pure strategy equilibrium which obtains the maximal welfare. The existence of such
a strategy profile implies that M V (ΓF ) = 1. 2.
4.1.2 Simple congestion games with n players
Theorems 2 and 3 in Section 3.1.2 imply that correlation has an unbounded value when
considering arbitrary games. The next theorem shows that correlation has similar effects
in the context of simple congestion games. In particular, we show that the mediation value
is unbounded with the presence of an additional player:
596

On The Value of Correlation

Theorem 9 M V ({ΓF |F ∈ QN 3×2 }) = ∞.
Proof: Consider the following family of forms F² , 0 < ² ≤ 0.5: M = {a1 , a2 }, wa1 =
(z, 4, 4 − ²), wa2 = (4 + ², 0, 0), where z = ²12 . Observe that the games, ΓF² are the games
given in the proof of Theorem 5 (Figure 6), and that the monotonicity condition is satisfied
for every 0 < ² ≤ 0.5.
We first show that for every sufficiently small ², the welfare in every mixed-strategy
equilibrium is lower than 13. After that we will construct, for every sufficiently small ², a
correlated equilibria in ΓF² such that the welfare in these correlated equilibria approaches
infinity as ² → 0.
Note that the only pure-strategy equilibria are the strategy profiles in which two players
play a1 and one player plays a2 . The welfare in each of these strategy profiles is 12 + ²,
which is less than 13.
We proceed to deal with mixed-strategy equilibria in which at least one player does not
play a pure strategy. In such an equilibrium, no player plays the strategy a2 with probability
one because if one of the players does it, the only mixed-strategy equilibria are those in
which each of the other two players plays a1 with probability one, i.e., the equilibrium is
pure.
Suppose some player plays a1 with probability one. Note that the utility matrix of the
other two players is as given in Figure 7.
1

a
a2

a1
4 − ², 4 − ²
4 + ², 4

a2
4, 4 + ²
0, 0

Figure 7
Therefore it is standard to check that there exists a unique mixed-strategy equilibrium
in which one of the players play a1 with probability 1, and that this equilibrium, when this
player is player 1 is:
2
²
2
²
((1, 0), (
,
), (
,
)).
2+² 2+² 2+² 2+²
Since the game is symmetric, the only mixed-strategy equilibria are the three permutations
of this vector. The welfare in each of these mixed-strategy equilibria is
3(4 − ²)(

2²
2 2
² 2
) + 2(12 + ²) ·
) =
+z·(
2
2+²
(2 + ²)
2+²
48 + 36² + (z + 4)²2
(2 + ²)2

→ 12.25,

as ² → 0.
Consider a completely mixed-strategy equilibrium. That is, every player assigns positive
probabilities to both facilities. Denote this equilibrium by ((p, 1 − p), (q, 1 − q), (h, 1 −
h)) 0 < p, q, h < 1. Because player 1 is indifferent between a1 and a2 given that players 2
and 3 play (q, 1 − q) (h, 1 − h) respectively,
(4 − ²)qh + 4(1 − q)h + 4(1 − h)q + z(1 − h)(1 − q) = (4 + ²)qh.
597

(26)

Ashlagi, Monderer, & Tennenholtz

Note that similar equalities hold for players 2 and 3, when q and h are exchanged with p
respectively. For every fixed h there exists a unique q that solves equation (26). Therefore,
by permuting the names of the players, p = q = h. This enables us to reduce Equation (26)
to
(z − 8 − 2²)p2 + (8 − 2z)p + z = 0,
(27)
which yields

√
2z − 8 − 64 + 8z²
p=
2z − 16 − 4²

(28)

because the other solution of (28) does not satisfy 0 < p < 1. Therefore, the welfare in this
completely mixed-strategy equilibrium is
3p(1 − p)2 z + 3p2 (1 − p)(12 + ²) + p3 (12 − 3²) =
(3z − 24 − 5²)p3 − (6z − 36 + 3²)p2 + 3zp.

(29)

Let A = (36 − 3²)p2 − (24 + 5²)p3 and B = 3zp3 − 6zp2 + 3zp. Hence (29) = A + B. We
show that A → 12 and B → 0 as ² → 0. Observe that p → 1 as ² → 0. This implies that
√
A → 12 as ² → 0. Observe that z² = z. Thus, from (28) and for small enough ² we have
that p ≈ p̂ =

√ 1
2z− 8z 4
2z

=1−

√
8 − 43
.
2 z

For simplicity set c =

3

√
8
2 .

Therefore

3

3

B ≈ 3z(1 − cz − 4 )3 − 6z(1 − cz − 4 )2 + 3z(1 − cz − 4 ) =
3

6

9

3

6

3

3z[1 − 3cz − 4 + 3c2 z − 4 − c3 z − 4 ] − 6z[1 − 2cz − 4 + c2 z − 4 ] + 3z(1 − cz − 4 ) =
2

5

3c2 z − 4 − 3c3 z − 4 =
10

3c2 ² − 3c3 ² 4 → 0
as ² → 0.
This completes the proof that vN (ΓF² ) ≤ 13 for every sufficiently small enough ².
To complete the proof of the theorem we construct, for every sufficiently small ², a
correlated equilibria in ΓF² such that the welfare in these correlated equilibria approaches
infinity as ² → 0.
In order that a correlated strategy µ will be a correlated equilibrium in ΓF² , the following
inequalities should be satisfied (in brackets we relate the inequality to the corresponding
inequality in (2)):14
1. −2²µ(1, 1, 1) + 4µ(1, 2, 1) + 4µ(1, 1, 2) + zµ(1, 2, 2) ≥ 0.
2. 2²µ(2, 1, 1) − 4µ(2, 2, 1) − 4µ(2, 1, 2) − zµ(2, 2, 2) ≥ 0.
3. −2² + 4µ(2, 1, 1) + 4µ(1, 1, 2) + zµ(2, 1, 2) ≥ 0.

(i = 1, si = a2 , ti = a1 )

(i = 2, si = a1 , ti = a2 )

4. 2²µ(1, 2, 1) − 4µ(2, 2, 1) − 4µ(1, 2, 2) − zµ(2, 2, 2) ≥ 0.
5. −2²µ(1, 1, 1) + 4µ(1, 2, 1) + 4µ(2, 1, 1) + zµ(2, 2, 1) ≥ 0.
14. Here, we slightly abuse notation by letting µ(i, j, k) = µ(ai , aj , ak ).

598

(i = 1, si = a1 , ti = a2 )

(i = 2, si = a2 , ti = a1 )
(i = 3, si = a1 , ti = a2 )

On The Value of Correlation

6. 2²µ(1, 1, 2) − 4µ(1, 2, 2) − 4µ(2, 1, 2) − zµ(2, 2, 2) ≥ 0.
P8
7.
i=1 µi = 1.

(i = 3, si = a2 , ti = a1 )

For sufficiently small ², the above inequalities are satisfied by µ² , where µ² (1, 1, 1) =
1
3
µ² (2, 1, 2) = µ² (2, 2, 2) = 0, µ² (2, 1, 1) = µ² (1, 1, 2) = ² 4 , and µ² (2, 2, 1) = µ² (1, 2, 2) = ² 2 .
Note that zµ(2, 2, 1) = √1² → ∞ as ² → 0, and that u(µ) ≥ zµ(2, 2, 1). Therefore
u(µ) → ∞ as ² → 0. 2
We conjecture that Theorem 9 holds for more players and/or more facilities. That is,
M V ({ΓF |F ∈ QN n×m }) = ∞ for every n ≥ 3 and for every m ≥ 2. One way to prove this
conjecture is to modify Lemma 2, which is used to prove analogous extensions for general
games (see e.g., Theorems 2 and 3).
The next theorem deals with linear facilities:
Theorem 10
M V ({ΓF |F ∈ QN n×2 , all facilities of F are linear}) ≤ φ
for every n ≥ 2, where

√
φ = ( 5 + 1)/2.

(30)

The following lemma (Schrijver, 1986, page 61) is used in the proof of Theorem 10.
Lemma 5 (Farkas Lemma) Let s, t be positive integers. Given a matrix A of dimensions
s × t and a vector b ∈ Rs , one and only one of the following systems has a solution:
(i)

Ax ≥ b,

x ∈ Rt ;

(ii)

yT A = 0,

yT b > 0,

y ∈ Rs+ ,

where Rs+ denotes the set of all nonnegative vectors in Rs .
Proof of Theorem 10: Consider the nonnegative, non-increasing and linear congestion
form in which M = {f, g}, and wf and wg are the facility payoff functions of f and g
respectively, where wf (k) = df k + δf and wg (k) = dg k + δg . Obviously, df , dg ≤ 0. Denote
by Γ the induced congestion game. Assume w.l.o.g. that
wf (1) ≥ wg (1).

(31)

Denote by πk = (n−k, k) the congestion vector in which n−k players choose f and k players
choose g. Let τ , 0 ≤ τ ≤ n, be the largest integer such that πτ is in equilibrium. Since πτ
is in equilibrium, wg (τ ) ≥ wg (n − τ + 1), and because of the monotonicity condition,
wg (j) ≥ wg (n − j + 1) ∀j ≤ τ.
Recall that the welfare in πτ is denoted by u(πτ ).
Claim 1: M V (Γ)) = 1, whenever τ ∈ {n, 0}.
599

(32)

Ashlagi, Monderer, & Tennenholtz

Proof: Suppose τ = n. Since (0, n) is in equilibrium, wg (n) ≥ wf (1). Therefore, by (31),
wg (n) ≥ wg (1). By the monotonicity condition, for every k, u(πk ) = (n − k)wf (n − k) +
kwg (k) ≤ (n − k)wf (1) + kwg (1). Therefore, u(πk ) ≤ nwg (n) = u(πn ). Hence, the maximal
welfare, opt(Γ) is attained at the equilibrium πn , which implies that the mediation value
equals 1.
Suppose τ = 0. Since (n, 0) is an equilibrium, wf (n) ≥ wg (1). We claim that
wf (n) > wg (1).

(33)

Indeed, assume in negation that wf (n) = wg (1). Since (n − 1, 1) is not in equilibrium, either
wf (n − 1) < wg (2) or wg (1) < wf (n). Therefore, wf (n − 1) < wg (2), and because of the
monotonicity condition, wf (n) < wg (1), which yields a contradiction. Therefore (33) holds,
which implies that choosing f strictly dominates choosing g in Γ. Therefore, by Lemma 1,
every correlated equilibrium in Γ is a pure strategy profile, which generates the congestion
vector π0 . Hence, the mediation value equals 1.2
By Claim 1, at the rest of this proof we assume w.l.og. that
1 ≤ τ ≤ n − 1.

(34)

Note that if wg (1) < wf (n), choosing f strictly dominates choosing g, which implies that
the mediation value equals 1. Therefore we assume w.l.o.g. at the rest of the proof that
wg (1) ≥ wf (n).

(35)

The following is a key claim for our proof.
Claim 2: u(πj ) ≤ u(πτ ) for every j ≤ τ .
Proof: Let j < τ . Since the facilities are linear, wg (j) = wg (τ ) + (τ − j)dg and wf (n − j) =
wf (n − τ ) − (τ − j)df . Hence,
u(πτ ) − u(πj ) =
swg (τ ) + (n − τ )wf (n − τ ) − j(wg (τ ) + (τ − j)dg ) − (n − j)(wf (n − τ ) − (τ − j)df ) =
wf (n − τ )(j − τ ) + wg (τ )(τ − j) − j(τ − j)dg + (n − j)(τ − j)df =
(τ − j)(wg (τ ) − wf (n − τ ) + (n − j)df − jdg ) ≥
(τ − j)(wf (n − τ + 1) − wf (n − τ ) + (n − j)df − jdg ),

(36)

where the last inequality follows since πτ is in equilibrium. Since the facilities are linear,
RHS(36) = (τ − j)((n − j − 1)df − jdg ) =
(τ − j)(wf (1) − wf (n − j) − wg (1) + wg (j + 1)).

(37)

Since j < τ it suffices to show that (wf (1) − wf (n − j) − wg (1) + wg (j + 1)) ≥ 0. This
inequality follows from (31) and (32). 2
Next we define an auxiliary strategy profile. Denote by q̃ the mixed-strategy profile
in which players 1, 2, . . . , n − τ − 1 choose f with probability 1, and each of the other
w (1)−w (n)
τ + 1 players choose g with probability p̃ = gτ (df +dfg ) , i.e., each of them plays the mixed
600

On The Value of Correlation

strategy (1− p̃, p̃). By (35), p̃ ≥ 0. To see that p̃ ≤ 1, note that because πτ is in equilibrium,
wg (1) − wf (n) ≤ wg (1) − wg (τ + 1) + wf (n − τ ) − wf (n) = sdg + sdf .
Claim 3: (i) q̃ is a mixed strategy equilibrium; (ii) u(q̃) = nwf (n) + p̃df ((n − τ − 1)(τ +
1) + (τ + 1)τ ).
Proof: (i) We first show that ui (q̃−i , f ) ≥ ui (q̃−i , g) for every player i that chooses f with
probability 1. Let k = τ + 1. We have, ui (q̃−i , f ) − ui (q̃−i , g) =
k µ ¶
X
k j
p̃ (1 − p̃)k−j (wf (n − j) − wg (j + 1)) =
j
j=0

k µ ¶
X
k j
p̃ (1 − p̃)k−j (wf (1) − (n − j − 1)df − wg (1) + jdg ) =
j
j=0

wf (1) − wg (1) − (n − 1)df + k p̃df + k p̃dg .

(38)

Since wf (n) = wf (1) − (n − 1)df and k > k − 1,
RHS(38) = wf (n) − wg (1) + (k − 1)p̃df + (k − 1)p̃dg ≥ 0,
w (1)−w (n)

g
f
where the last inequality follows since p̃ ≤ 1, where p̃ = (k−1)(d
.
f +dg )
We next show that every player i that plays the mixed strategy (1 − p̃, p̃) is indifferent
between f and g. Observe that ui (q̃−i , f ) − ui (q̃−i , g) =

¶
n µ
X
k−1 j
p̃ (1 − p̃)k−1−j (wf (n − j) − wg (j + 1)) =
j
j=0

¶
n µ
X
k−1 j
p̃ (1 − p̃)k−1−j (wf (1) − (n − j − 1)df − wg (1) + jdg ) =
j
j=0

wf (1) − wg (1) − (n − 1)df + (k − 1)p̃df + (k − 1)p̃dg =
wf (n) − wg (1) + (k − 1)p̃df + (k − 1)p̃dg = 0.
(ii) Similar calculations as in part (i) yield that the expected payoff for each of the
n − τ − 1 players that choose f with probability one equals wf (n) + (τ + 1)p̃df , and that
the expected payoff for each of the other τ + 1 players equals wf (n) + τ p̃df . Therefore
u(q̃) = nwf (n) + p̃df ((n − τ − 1)(τ + 1) + (τ + 1)τ ). 2
We proceed with the main proof. Define
Z = φmax{u(πτ ), u(q̃)}.

(39)

If u(πk ) ≤ Z for every 0 ≤ k ≤ n, opt(Γ) ≤ Z, and therefore, vc (Γ) ≤ opt(Γ) ≤
φmax{u(πτ ), u(q̃)} implying that M V (Γ) ≤ φ. Therefore we can assume for the rest of the
proof that there exist an integer k̂, 0 ≤ k̂ ≤ n such that
u(πk̂ ) > Z..
601

(40)

Ashlagi, Monderer, & Tennenholtz

b In our case, the dual problem denoted by
We proceed by utilizing the dual program D.
b Γ is as follows:
D


min β





s.t.
b
DΓ : αi (f |g) ≥ 0
∀i ∈ N,


i

α (g|f ) ≥ 0
∀i ∈ N,



P
i
i
i
i
i
−i
i
i
−i
i∈N α (t |s )[u (t , s ) − u (s , s )] + β ≥ u(s) ∀s ∈ S,
where for every i, ti = f if si = g, and ti = g if si = f .
Recall that by the weak duality theorem, for every feasible solution for the dual problem,
(α, β), β ≥ vC (Γ). Therefore, in order to complete the proof, it suffices to show that there
b Γ in which β = Z, where Z is defined in
exists a feasible solution for the dual problem D
(39).
We begin with restricting the range of the variables in the dual problem (D̂)Γ . More
c1 ) is obtained from (D)
b by letting β = Z, αi (g|f ) = 0
specifically, the following system (D
i
and x = α (f |g) for every i ∈ N , i.e., x remains the only variable:


min β





s.t.
c1 : β = Z,
D



k(wf (n − k + 1) − wg (k))x ≥ u(πk ) − β




x ≥ 0.

k = 1, ..., n,

c1 defines a feasible solution, (α, β) of (D̂)Γ in
Obviously, every optimal solution, x of D
c1 is equivalent to the
which β = Z. Since β = Z, the existence of an optimal solution for D
c2 ):
existence of a feasible solution for the following system of constraints, (D
(
c2 : k(wf (n − k + 1) − wg (k))x ≥ u(πk ) − β
D
x ≥ 0.

k = 1, ..., n,

c2 is redundant, i.e., that if x satisfies the
We next show that the inequality x ≥ 0 in D
first n constraints, x ≥ 0. Recall that by (40) there exist an integer 0 ≤ k̂ ≤ n such that
u(πk̂ ) > Z. Since φ > 1, by Claim 2, k̂ > τ , and in particular, k̂ ≥ 1. Since k̂ > τ , πk̂ is not
in equilibrium. Since in πτ , a player in f does not wish to deviate to g, so is the case in πk̂
in which there are fewer players in f . Therefore, in πk̂ , a deviation of a player from g to f
is profitable, i.e., wf (n − k̂ + 1) − wg (k̂) > 0, and since k̂ > 0, k̂(wf (n − k̂ + 1) − wg (k̂)) > 0.
c2 and u(π ) > Z = β, x > 0. Hence D
b 2 is
Since x satisfies the constraint for k̂ in D
k̂
equivalent to
n
c3 : k(wf (n − k + 1) − wg (k))x ≥ u(πk ) − β
D
602

k = 1, ..., n.

On The Value of Correlation

c3 has a solution x if and only if the following system does
By Farkas Lemma (Lemma 5), D
not have a solution, y = (y1 , y2 , . . . , yn ):
Pn

Pk=1 yk k(wf (n − k + 1) − wg (k)) = 0,
n
c
P1 :
k=1 yk (u(πk ) − β) > 0,


yk ≥ 0

k = 1, ..., n.

c1 does not have a solution. Note that if y is a solution
Therefore, it suffices to show that P
c
for P1 , at least one variable yk is positive. Therefore, it suffices toPprove that there does
c1 , which is also a probability vector, i.e., n yk = 1. Let y be
not exist a solution for P
k=1
c1 . We have to prove that y does
a probability vector that satisfies the first constraint in P
not satisfy the second constraint, that is, we have to show that
n
X

yk u(πk ) ≤ β.

(41)

k=1

Let Y be the random variable in whichPyk = P (Y = k), k = 1, ..., n, and recall that
the expected value of Y satisfies E[Y ] = nk=1 kyk . We first derive the following useful
inequalities given in Claim 4:
Claim 4:
(i)
E[Y ] ≤

wg (1) − wf (n) + df + dg
.
(df + dg )

(42)

(ii)
n
X

yk u(πk ) = E[Y ](wf (1) − wf (n)) + nwf (n).

k=1

c1 is satisfied,
Proof: (i) Since the first constraint in P
0=

n
X

yk k(wf (n − k + 1) − wg (k)) =

k=1
n
X

yk k(wf (n) + (k − 1)df − wg (1) + (k − 1)dg ) =

k=1

E[Y ](wf (n) − wg (1) − df − dg ) + E(Y 2 )(df + dg ).
Since E[Y 2 ] ≥ E[Y ]2 ,
E[Y ](wf (n) − wg (1) − df − dg ) + E[Y ]2 (df + dg ) ≤ 0.
Since E[Y ] > 0, by dividing both sides by E[Y ] we obtain (42).
603

(43)

Ashlagi, Monderer, & Tennenholtz

(ii) We have
n
X

yk u(πk ) =

k=1

n
X

yk (kwg (k) + (n − k)wf (n − k)) =

k=1
n
X

yk (kwf (n − k + 1) + (n − k)wf (n − k)),

k=1

c1 is satisfied by y. Therefore,
where the last equality holds because the first constraint in P
n
X

yk u(πk ) =

k=1

wf (1)E[Y ] +

n
X

yk k(wf (1) − (n − k)df ) +

k=1
n
X

n
X

yk (n − k)wf (n − k) =

k=1

yk (n − k)(wf (n − k) − kdf ) = wf (1)E[Y ] + wf (n)

k=1

n
X

yk (n − k)

k=1

= E[Y ](wf (1) − wf (n)) + nwf (n),
which proves that (43) holds. 2
We proceed to prove that (41) holds. By plugging (43) in (41), we can equivalently
prove that
E[Y ](wf (1) − wf (n)) + nwf (n) ≤ β,
(44)
and we distinguish between the following two cases:
Case 1: p̃ < 1/φ. First we show that
E[Y ] ≤ τ φ.
Because p̃ < 1/φ, (wg (1) − wf (n))/(df + dg ) ≤ τ /φ. Hence, by (42), E[Y ] ≤
Since φ = φ1 + 1, τ φ = φτ + τ . Since τ ≥ 1, (45) holds.

(45)
τ
φ

+ 1.

Suppose in negation that (44) does not hold, i.e., E[Y ](wf (1) − wf (n)) + nwf (n) > β.
Note that β = Z ≥ φu(πτ ). Therefore, E[Y ](wf (1) − wf (n)) + nwf (n) > φu(πτ ). By
the definition of u(πτ ) and by the linearity of the facilities,
E[Y ] >

φτ wg (τ ) + φ(n − τ )wf (n − τ ) − nwf (n)
=
(n − 1)df

φτ wg (τ ) + φ(n − τ )(wf (n) + sdf ) − nwf (n)
.
(n − 1)df

(46)

Since πτ is in equilibrium,
RHS(46) ≥

φτ wf (n − τ + 1) + (n(φ − 1) − φτ )wf (n) + φ(n − τ )τ df
.
(n − 1)df

Since (n(φ − 1) − φτ )wf (n) ≥ 0,
RHS(47) ≥

φτ (wf (n − τ + 1) − wf (n)) + φ(n − τ )τ df
=
(n − 1)df
604

(47)

On The Value of Correlation

φτ (τ − 1)df + φ(n − τ )τ df
φτ (n − 1)
=
= φτ.
(n − 1)df
n−1
Therefore, if (44) does not hold,
E[Y ] > φτ,

(48)

E[Y ] ≤ p̃τ + 1.

(49)

contradicting (45).
Case 2: p̃ ≥ 1/φ.
By (42) and since

wg (1)−wf (n)
τ (df +dg )

= p̃,

Suppose in negation that the second constraint is satisfied, i.e., RHS(43) = E[Y ](wf (1)−
wf (n)) + nwf (n) > β. Note that β ≥ φu(q̃). Therefore
E[Y ] >

φu(q̃)
.
(wf (1) − wf (n)) + nwf (n)

Hence, by part(ii) of Claim 3 and by linearity of the facilities
E[Y ] >

φnwf (n) + φp̃df ((n − τ − 1)(τ + 1) + (τ + 1)τ ) − nwf (n)
=
(n − 1)df
n(φ − 1)wf (n) + φp̃df (τ + 1)(n − 1)
.
(n − 1)df

(50)

Since n(φ − 1)wf (n) ≥ 0 and p̃ ≤ 1/φ
RHS(50) ≥ τ + 1.
Hence, E[Y ] > τ + 1 contradicting (49) since p̃ ≤ 1.
This completes the proof of the theorem. 2
In Theorem 10 we derived, for every n ≥ 2, the upper bound φ for the mediation value
of the class of games, ({ΓF |F ∈ QN n×2 , all facilities of F are linear}). When n = 2, by
Theorem 8, we know that the mediation value equals 1. Unfortunately, we do not know
what is the mediation value for these classes of games when n ≥ 3. However, the example
below shows that, when n = 3, the mediation value for this class is at least 89 ; Hence, the
mediation value is between 1.125 and φ ≈ 1.618.
Example 1 Let n = 3, M = {f, g}, wf = (24, 12, 0), and wg = (8, 8, 8). It can be easily
verified that vN (Γ) = 32, which is obtained, e.g., at the pure-strategy equilibrium in which
two players choose f and the other player chooses g. Consider the correlated strategy µ,
which assigns the probability 61 to every strategy profile in which not all players choose the
same facility. It is easily verified that µ is a correlated equilibrium and that the welfare at
µ is 36. Hence, the mediation value is at least 36
32 .
605

Ashlagi, Monderer, & Tennenholtz

For the next discussion it is useful to recall that the price of stability of a cost game is the
ratio between the minimal cost at a mixed-strategy equilibrium and the minimal cost, and
that the price of stability with utilities of a utility game is the ratio between the maximal
welfare and the maximal welfare at a mixed-strategy equilibrium.
Let n, m ≥ 2. Another open question for us is estimating the mediation value, of simple
congestion games with n players, m facilities, and with nonnegative, non-increasing, and
linear facilities. One can think that an upper bound for this class can be derived from
the results about the price of stability for the analogous class of congestion games with
costs. Indeed, Christodoulou and Koutsoupias (2005) proved that the price of stability of
the class of congestion games with cost with n players, m facilities, and with nonnegative,
linear, and non-decreasing cost functions is bounded above by 1.6. Had this result proven
for the price of stability with utilities for our class of games it would have implied, in
particular, that the mediation value of this class is bounded above by 1.6, because the
mediation value cannot exceed the price of stability with utilities.15 However, as discussed
in the introduction, results about the price of stability in cost models cannot be transformed
to results about the price of stability with utilities in utility models. To illustrate this, we
show in the next example that the price of stability with utilities of the class {ΓF |F ∈
QN 2×2 , all facilities of F are linear} equals ∞.16
Example 2 Let x, ² > 0 be fixed. Let N = {1, 2}, M = {f, g} and for every d > ²,
let wfd = (x + d + ², x + ²), let wgd = (x, x), and Γd be the associated congestion game.
Since f strictly dominates g, the strategy profile in which both players choose f is the only
mixed-equilibrium in the game Γd . The welfare obtained in this equilibrium is 2x + 2². the
strategy profile attaining the maximal welfare is the one in which each player chooses a
different facility yielding a welfare of 2x + ² + d. Since 2x+²+d
2x+2² → ∞ when d → ∞, the price
of stability of {ΓF |F ∈ QN 2×2 , all facilities of F are linear} equals ∞.
Theorems 9, and Example 1 show that correlation can be helpful in the context of (even
non-increasing) congestion games. The next theorem shows that correlation can be helpful
even in the narrow class of facility symmetric forms with nonnegative and non-increasing
facilities:
Theorem 11 M V ({ΓF |F ∈ IN n×2 }) > 1 for every n ≥ 4.
Proof: Let n ≥ 4. It suffices to prove that there exists F ∈ IN n×2 such that M V (ΓF ) > 1.
Let 0 < ² < 1 be fixed and sufficiently small, and consider the following form, F : w1 =
w2 = (10n, 1, . . . , 1, 1 − ², 0). Note that the maximal welfare is obtained by any strategy
profile in which exactly n − 1 players choose the same facility, i.e., when the congestion
vector is π1 = (1, n − 1) or π2 = (n − 1, 1). Let L be the set of all strategy profiles with a
congestion vector π1 or π2 . Note that there exist exactly 2n strategy profiles in L. Let µ be
1
the correlated strategy in which every strategy profile in L is played with probability 2n
.
Since µ is a convex combination of welfare maximizers, the maximal welfare is attained at
15. Moreover, since 1.6 ≤ φ, such a result would have saved us the tedious proof of Theorem 10.
16. The proof of Christodoulou and Koutsoupias (2005) elegantly uses the fact that every congestion game
has an exact potential. It is not known to us wether the potential approach could simplify our proof
of Theorem 10. However, it turns out that their technique can not be applied directly to our setting
because essentially they bound the total cost of the players on every facility separately in a given profile.

606

On The Value of Correlation

µ. We claim that µ is a correlated equilibrium. Indeed, by (2), it is easily verified that in
ΓF there exists a unique constraint that should be satisfied in order to guarantee that µ is a
correlated equilibrium. This constraint is: 10n+((1−²)−1)(n−1)
≥ 0, which is indeed satisfied
2n
because we chose ² to be sufficiently small. Hence µ is a correlated equilibrium.
In order to prove that M V (ΓF ) > 1, it suffices to prove that no mixed-strategy equilibrium obtains the maximal welfare. First, note that only strategy profiles in L obtain
the maximal welfare. Hence, if a pure-strategy equilibrium obtains the maximal welfare, it
must belong to L. However, we claim that every strategy profile in L is not in equilibrium;
Indeed any player that chooses the facility chosen by n − 1 players is better off deviating to
the other facility since her utility will increase by ².
Therefore, it remains to show that in every mixed-strategy equilibrium, p = (p1 , . . . , pn ),
in which at least one player assigns a positive probability to each facility, there exists at
least one pure strategy profile, s, not in L, which is played with positive probability, that
is µp (s) > 0, where µp is the correlated strategy associated with p. Indeed, let p be a
mixed-strategy equilibrium, and assume w.l.o.g. that player i assigns a positive probability
to both facilities, that is, pi (1), pi (2) > 0. Assume in negation that µp (t) > 0 implies t ∈ L,
and let s be a strategy profile for which µp (s) > 0. Since pi (1), pi (2) > 0, µp (1, s−i ) > 0 and
µp (2, s−i ) > 0. Therefore, (1, s−i ) ∈ L and (2, s−i ) ∈ L, which is impossible since n ≥ 4. 2
If we further restrict the assumptions in Theorem 11 by requiring some concavity condition correlation cannot help anymore. We first define concavity:
Let n ≥ 2. A function v : {1, 2, ..., n} → R+ is concave if for every integer k, 2 ≤ k < n,
v(k + 1) − v(k) ≤ v(k) − v(k − 1).
Theorem 12 Let n ≥ m ≥ 2, and let F ∈ IN n×m . Define v(k) = kw(k) for every
1 ≤ k ≤ n, where w is the common facility payoff function, that is, w = wj for every
j ∈ M . If v is concave, there exists a pure-strategy equilibrium in ΓF which obtains the
maximal welfare. Consequently, M V (ΓF ) = 1.
Proof: For convenient we also define v(0) = 0. We first define an operator on congestion
vectors. Let π = (π1 , π2 , ..., πm ) be a congestion vector and let j, l ∈ M be any pair of
distinct facilities. Let π ∗ [j, l] be the congestion vector obtained from π by replacing πj and
π +π
π +π
πl with πj∗ = d j 2 l e and πl∗ = b j 2 l c, respectively, where bxc (dxe) denotes
Pm the largest
(least) integer that is not higher (lower) than x. Observe that u(π) =
i=1 v(πi ), and
therefore, by the concavity of v,
u(π ∗ [j, l]) ≥ u(π).

(51)

Next we describe a finite sequence of congestion vectors, such that each one of them
obtains the maximal welfare and the last one is also in equilibrium. Before that, set k1 =
n
n
bm
c, k2 = d m
e, and note that every congestion vector in which each coordinate is either k1
or k2 is in equilibrium.
Pick any congestion vector π that obtains the maximal welfare. If π is in equilibrium,
we are done. Otherwise, not all of its coordinates are k1 or k2 . In particular, there exist
two distinct coordinates ĵ and ˆl such that πĵ ≤ k1 , πl̂ ≥ k2 , and at least one inequality
is strict. Construct π ∗ = π ∗ [ĵ, ˆl]. By (51), u(π ∗ ) ≥ u(π), and therefore, π ∗ obtains the
maximal welfare. If π ∗ is in equilibrium we are done. Otherwise let π = π ∗ and repeat the
607

Ashlagi, Monderer, & Tennenholtz

process. If the sequence of π ∗ ’s does not terminate, it eventually reaches a π ∗ in which every
coordinates equals k1 or k2 , i.e., it reaches an equilibrium –a contradiction. Therefore, the
sequence terminates after finite number of stages, and hence the last congestion vector in
the sequence is in equilibrium and attains the maximal welfare. 2
4.2 The Enforcement Value
We already know from the construction in the proof of Theorem 4 that the enforcement
value of the class D = {Γx | x > 1} of Prisoner’s Dilemma games described in Figre 5
is unbounded. It is easily verified that every game in this class is a potential game, and
therefore, by (Monderer & Shapley, 1996), it is a congestion game. Moreover, it can be easily
verified that each such game can be derived from a simple and nonnegative congestion form
in Q2×2 . Therefore, EV ({ΓF |F ∈ Q2×2 ) = ∞. However, as noticed by Monderer (2007),
not every congestion game with nonnegative utilities can be represented by a congestion
form with nonnegative and non-increasing facilities. In particular, it can be shown that
the Prisoner’s Dilemma games in D cannot be represented by simple congestion forms
with nonnegative and non-increasing facilities. The next theorem shows that even if the
facility payoff functions are restricted to be non-increasing, the enforcement value remains
unbounded for two player games.
Theorem 13 EV ({ΓF |F ∈ QN 2×m }) = ∞ for every m ≥ 2.
Proof: Consider the games Γd ,d > 0, given in Example 2. Since using facility f strictly
dominates using g, by Lemma 1, the strategy profile in which both players choose f is the
d)
unique correlated equilibrium in Γd . As is proved in Example 2, opt(Γ
→ ∞, when d → ∞.
vC (Γd )
This proves the theorem for m = 2. For m > 2 the proof is obtained, as above, by naturally
modifying the games in Example 2. 2
Since Theorem 13 deals with two players, we deduce that the enforcement value is
unbounded also for the class of games generated by linear facilities in QN 2×m because for
two players, every non-increasing facility is linear.
Note that the proof of Theorem 13 utilizes games, which posses strictly, and in particular,
weakly dominant strategies. The next theorem deals with games without weakly dominant
strategies.
Theorem 14
(i) EV ({ΓF |F ∈ QN 2×2 , there are no weakly dominant strategies}) = 1.
(ii) EV ({ΓF |F ∈ QN 3×2 , there are no weakly dominant strategies}) = ∞.
Proof: Assertion (i) follows from Theorem 8. As for Assertion (ii), the proof follows by
observing that the game in the proof of Theorem 5 is a simple congestion game with nonincreasing facilities. 2
The next theorem shows that the enforcement value tends to ∞ when the number of
players tends to ∞ even when restricting the facilities to be symmetric and non-increasing.
Theorem 15 limn→∞ EV ({ΓF |F ∈ IN n×2 }) = ∞.
608

On The Value of Correlation

Proof: Consider the following family of forms Fn ∈ IN n×2 , n ≥ 3: M = {f, g} and let
√
wf = wg = ( n, 1, 0, 0, .., 0). Observe that the congestion vector, πn = (1, n − 1), obtains
√
the maximal welfare, which equals n. Therefore, in order to prove the theorem, It suffices
to show that
(52)
vC (ΓFn ) ≤ 3 for every n ≥ 3.
b By the
Let then n ≥ 3 be fixed. In order to prove (52), we use the dual program (D).
weak duality theorem, every feasible solution of the dual problem satisfies β ≥ vC (ΓFn ).
Therefore, it suffices to prove that the dual problem has a feasible solution with β = 3.
b Γ described right after (40).
However, in our case, the dual problem is just the problem, D
b Γ , in which all dual variables except β have an identical
We will find a feasible solution to D
i
i
value x, i.e., x = α (f |g) = α (g|f ) for each player i. Under this restriction, the dual
b Γ reduces to:
program, D


min β





s.t.



x ≥ 0,
√
√

(− n + n − 1)x + β ≥ n,





−2x + β ≥ 1,



n√n + β ≥ 0,
which has the feasible solution, (x, β) = (1, 3). Therefore (52) holds. 2
Although the enforcement value may be unbounded when we have facility symmetric
congestion forms, it is interesting to characterize those congestion games for which correlation enables to get the maximal welfare. This is done in the next theorem, but first we need
the following notations. Let F be a congestion form, and Let π be a congestion vector in
F . Let τ : M → M be a one to one function, i.e., τ is a permutation of the set of facilities.
We define the congestion vector τ π = (τ π)j∈M as follows: (τ π)j = πτ (j) for every facility j.
Recall that Bπ is theSset of all strategy profiles that induce the congestion vector π. We
further define, Lπ = τ Bτ π to be the set of all strategy profiles that induce a permutation
of the congestion vector π.
Let F ∈ In×m , that is, all facilities in F are symmetric. Therefore, u(π) = u(τ π)
for every permutation τ , and, in addition, for every pair of strategy profiles A, B ∈ Lπ ,
u(A) = u(B).
Theorem 16 Let n, m ≥ 2, and let F ∈ In×m . Then, vC (ΓF ) = opt(ΓF ) if and only if
there exist a congestion vector π = (π1 , ..., πm ) and a correlated equilibrium µ ∈ C(ΓF ) such
that the following two conditions hold:
1. u(π) = opt(ΓF ).
2. µ is distributed uniformly over all strategy profiles in Lπ ; that is, µ(d) = µ(d̄) for
every d, d̄ ∈ Lπ , and µ(d) = 0 for every d 6∈ Lπ .
The following lemma (Schrijver, 1986, page 61) is used in our proof:
609

Ashlagi, Monderer, & Tennenholtz

Lemma 6 (Variant of the Farkas Lemma) Let s, t be positive integers. Given a matrix
A of dimensions s × t and a vector b ∈ Rs , one and only one of the following systems has
a solution:
(i)

Ax = b,

(ii)

yT A ≥ 0,

x ≥ 0,

x ∈ Rt ;

yT b < 0,

y ∈ Rs .

Proof of Theorem 16:17
Clearly, if there exist a congestion vector π and a correlated equilibrium µ ∈ C(ΓF ),
which satisfy both conditions, vC (ΓF ) = opt(ΓF ). Before we prove the other direction we
need Claim 1 below.
Let w be the common facility payoff function, that is, w = wj for every j ∈ M . For
every congestion vector π we define Z(π) as follows:
Pm
P
j=1 πj
k6=j (w(πj ) − w(πk + 1))
Z(π) =
.
(53)
m!n
Claim 1: Suppose vC (ΓF ) = opt(ΓF ). Let π = (π1 , ..., πm ) be a congestion vector in which
u(π) = opt(ΓF ), and let µ be a correlated strategy distributed uniformly over all elements
in Lπ . µ is a correlated equilibrium if and only if Z(π) ≥ 0.
Proof: Let Dπ,i,j be the set of all strategy profiles in which player i chooses facility j and
the congestion vector is π. That is Dπ,i,j = {d : d ∈ Bπ , di = j}.
¡
¢ Qm ¡n−πj −Pl−1 πk 1k6=j ¢
k=1
Let sπ (j) = πn−1
, and note that |Dπ,i,j | = sπ (j) for every
l=1
−1
πl 1l6=j
j
i ∈ N.
Since µ is distributed uniformly over all elements in Lπ , µ is a correlated equilibrium if
and only if
P
Pm
k6=j (w(πj ) − w(πk + 1))
j=1 sπ (j)
≥ 0.
(54)
|Lπ |
Hence, it suffices to show that LHS(54) = Z(π). This follows since |Lπ | = |Bπ |m! =
n
¡ n ¢¡n−π1 ¢
¡n−Pm−2 πj ¢
(k−1
)
j=1
= nk . 2
·
·
·
m!
and
π1
π2
πm−1
(nk)
We proceed with proving the remaining direction. Suppose that vC (ΓF ) = opt(ΓF ), and
assume for a contradiction that there do not exist a congestion vector π and a correlated
equilibrium µ ∈ C(ΓF ) such that µ is distributed uniformly over all elements in Lπ and
u(π) = opt(ΓF ). Recall that for every strategy profile d ∈ S, π d is the congestion vector
induced
by d and that πkd is the number of players that choose facility k in d. Let D =
S
{d:u(π d )=opt(ΓF )} Lπ d . Because of our negation assumption, Claim 1 implies that
Z(π d ) < 0 for every d ∈ D.

(55)

We are about to utilize Lemma 6, and for that matter set J = |D|. Define the matrix
A of size J × n(m2 − m) as follows:
A(d, ijk) =

[w(πjd ) − w(πkd + 1)]1ij (d)
,
m!n

17. The proof technique was inspired by Nau and McCardle (1990).

610

(56)

On The Value of Correlation

where each row corresponds to a strategy profile d ∈ D and column ijk corresponds to
player i who chooses jth strategy (facility) and deviates to the kth strategy (j 6= k), and:
½
1 di = j
1ij (d) =
0 otherwise.
Every row d in the matrix A corresponds to strategy profile d ∈ D, which in turn,
d ). Set b(d) = Z(π d ) for every d ∈
corresponds to a congestion vector π d = (π1d , ..., πm
T
D. Note that the column vector x = (1, 1, ..., 1) ∈ RJ satisfies (i) in Lemma 6, where
b = (b(d))d∈D . Therefore, by Lemma 6, system (ii) in Lemma 6 does not have a solution.
Because vC (ΓF ) = opt(ΓF ), there exists a correlated equilibrium, say µ̄, satisfying u(µ̄) =
vC (ΓF ) = opt(ΓF ). Obviously, µ̄ is supported in D, that is µ(d) = 0 for every d 6∈ D. Let
y(d) = µ̄(d) for every d ∈ D, and let y = (y(d))d∈D . Since µ̄ is a correlated equilibrium
concentrated on D, yT A ≥ 0, and since y does not satisfy (ii),
yT b ≥ 0.

(57)

On the other hand, since y is a probability distribution on D, and (55) holds,
X
X
µ̄(d)b(d) < 0,
y(d)b(d) =
yT b =
d∈D

d∈D

contradicting (57). Therefore, our negation assumption cannot hold, and the theorem is
proved. 2
Theorem 16 shows that for symmetric congestion games, under the conditions of the
theorem, correlation helps in obtaining the maximal welfare. The next example shows that
there exist such games in which mixed-strategy equilibrium is not as useful as correlated
equilibrium:
Example 3 Let F ∈ I6×2 . Let wj = (1.5, 1, 4, 4.5, 4.5, 3) for every j = 1, 2. It is easy
to verify that the maximal welfare is obtained in a strategy profile A if and only if A ∈
Lπ1 ∪ Lπ2 , where π1 = (3, 3) and π2 = (1, 5). Let µ be the correlated strategy, which is
distributed uniformly over Lπ2 . It can be checked that µ is a correlated equilibrium. Hence,
by Theorem 16, vC (ΓF ) = opt(ΓF ). On the other hand, note that both π1 and π2 are not
in equilibrium. Hence, in every mixed-strategy profile, a profile which does not obtain the
maximal welfare will be played with positive probability. Therefore the mediation value is
greater than 1, i.e. the best mixed-strategy equilibrium is less useful than the best correlated
equilibrium.

5. Conclusion
In this work we have introduced and studied two measures for the value of correlation in
strategic interactions: the mediation value and the enforcement value. These measures
complement existing measures appearing in the price of anarchy literature, which are comparing the maximal welfare (when agent behavior can be dictated) to the welfare obtained
in Nash equilibrium (when agents are selfish). Indeed, correlation captures many interesting situations, which are common to computing systems and e-commerce applications. In
611

Ashlagi, Monderer, & Tennenholtz

many systems a reliable party can advise the agents on how to behave but can not enforce
such behavior. The gain that may be obtained by this capability is the major subject of
the study presented in this work. We studied and showed the power of this approach, both
for general games and in the context of congestion games.

Acknowledgments
A preliminary version of this paper appears in the proceedings of the 20th conference on
Uncertainty in Artificial Intelligence (UAI-05). We thank the German-Israeli Foundation
(GIF) for a financial support

References
Anshelevich, E., Dasgupta, A., Kleinberg, J., Tardos, E., Wexler, T., & Roughgarden,
T. (2004). The Price of Stability for Network Design with Fair Cost Allocation.
In Proceedings of the 45th IEEE Symposium on Foundations of Computer Science,
(FOCS-04), pp. 59–73.
Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). Mediators in position auctions. To appear in Games and Economic Behavior. A shorter version appears in the Proceedings
of the 8th ACM conference on Electronic Eommerce.
Aumann, R. (1974). Subjectivity and Correlation in Randomized Strategies. Journal of
Mathematical Economics, 1, 67–96.
Aumann, R. (1987). Correlated Equilibrium as an Expression of Bayesian Rationality.
Econometrica, 55, 1–18.
Christodoulou, G., & Koutsoupias, E. (2005). On the Price of Anarchy and Stability of
Correlated Equilibria of Linear Congestion Games. In Proceedings of the 13th Annual
European Symposium, ESA 2005, pp. 59–70.
Czumaj, A., & Vocking, B. (2002). Tight Bounds For Worst Case Equilibria. In Proceedings
of the 13th Annual Symposium on Discrete Algorithms, pp. 413–420.
Hart, S., & Schmeidler, D. (1989). Existence of Correlated Equilibria. Math. Oper. Res.,
14, 18–25.
Kakade, S., Kearns, M., Langford, J., & Ortiz, L. (2003). Correlated equilibria in graphical
games. In Proceedings of the 4th ACM conference on Electronic commerce, pp. 42–47.
Koutsoupias, E., & Papadimitriou, C. (1999). Worst-Case Equilibria. In Proceedings of the
16th Annual Symposium on Theoretical Aspects of Computer Science, pp. 404–413.
Marvonicolas, M., & Spirakis, P. (2001). The Price of Selfish Routing. In Proceedings of
the 33rd Symposium on Theory of Computing, pp. 510–519.
Monderer, D. (2007). Multipotential Games. In Twentieth International joint conference
on Artificial Intelligence (IJCAI-07) .
612

On The Value of Correlation

Monderer, D., & Shapley, L. (1996). Potential Games. Games and Economic Behavior, 14,
124–143.
Monderer, D., & Tennenholtz, M. (2004). K-Implementation. Journal of Artificial Intelligence Research (JAIR), 21, 37–62.
Monderer, D., & Tennenholtz, M. (2006). Strong mediated equilibrium. In Proceedings of
the AAAI.
Nash, J. (1951). Noncooperative Games. Ann. Math., 54, 286–295.
Nau, R. F., & McCardle, K. F. (1990). Coherent Behavior in Noncooperative Games.
Journal of Economic Theory, 50, 424–444.
Papadimitriou, C. (2001). Algorithms, Games, and the Internet. In Proceedings of the 16th
Annual ACM Symposium on Theoretical Aspects of Computer Science, pp. 749–753.
Papadimitriou, C. (2005). Computing correlated equilibria in multi-player games. In Proceedings of the thirty-seventh annual ACM symposium on Theory of computing, pp.
49–56.
Peeters, R., & Potters, J. (1999). On the Structure of the Set of Correlated Equilibria
in Two-by-Two Bimatrix Games. Technical report, Tilburg - Center for Economic
Research.
Rosenthal, R. (1973). A Class of Games Possessing Pure-Strategy Nash Equilibria. International Journal of Game Theory, 2, 65–67.
Roughgarden, T. (2002). Selfish Routing. PhD Thesis, Cornell University.
Roughgarden, T., & Tardos, E. (2002). How Bad is Selfish Routing?. Journal of the ACM,
49(2), 236–259.
Rozenfeld, O., & Tennenholtz, M. (2007). Routing mediators. In Proceedings of the 23rd
International Joint Conferences on Artificial Intelligence(IJCAI-07), pp. 1488–1493.
Schrijver, A. (1986). Theory of Linear and Integer Programming. Wiley, New York.
Shoham, Y., & Tennenholtz, M. (1995a). Artificial Social Systems. Computers and Artificial
Intelligence, 14, 533–562.
Shoham, Y., & Tennenholtz, M. (1995b). On Social Laws for Artificial Agent Societies:
Off-Line Design. Artificial Intelligence, 73, 231–252.

613

Journal of Artificial Intelligence Research 33 (2008) 285-348

Submitted 4/08; published 11/08

Computational Logic Foundations of KGP Agents
Antonis Kakas

antonis@ucy.ac.cy

Department of Computer Science, University of Cyprus
75 Kallipoleos Str., P.O. Box 537, CY-1678 Nicosia, Cyprus

Paolo Mancarella

paolo.mancarella@unipi.it

Dipartimento di Informatica, Università di Pisa
Largo B. Pontecorvo, 3 - 56127 Pisa, Italy

Fariba Sadri

fs@doc.ic.ac.uk

Department of Computing, Imperial College London
South Kensington Campus, London SW72AZ, UK

Kostas Stathis

kostas@cs.rhul.ac.uk

Department of Computer Science, Royal Holloway
University of London, Egham, Surrey TW20 0EX, UK

Francesca Toni

ft@doc.ic.ac.uk

Department of Computing, Imperial College London
South Kensington Campus, London SW72AZ, UK

Abstract
This paper presents the computational logic foundations of a model of agency called
the KGP (Knowledge, Goals and Plan) model. This model allows the specification of
heterogeneous agents that can interact with each other, and can exhibit both proactive
and reactive behaviour allowing them to function in dynamic environments by adjusting
their goals and plans when changes happen in such environments. KGP provides a highly
modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent’s state in response to reasoning,
sensing and acting. Transitions are orchestrated by cycle theories that specify the order in
which transitions are executed while taking into account the dynamic context and agent
preferences, as well as selection operators for providing inputs to transitions.

1. Introduction
It is widely acknowledged that the concept of agency provides a convenient and powerful
abstraction to describe complex software entities acting with a certain degree of autonomy
to accomplish tasks, often on behalf of a user (Wooldridge, 2002). An agent in this context
is understood as a software component with capabilities such as reacting, planning and
(inter) acting to achieve its goals in the environment in which it is situated. In this paper,
we present a model of agency, called KGP (Knowledge, Goals and Plan). The model
is hierarchical and highly modular, allowing independent specifications of a collection of
reasoning and physical capabilities, used to equip an agent with intelligent decision making
and adaptive behaviour. The model is particularly suited to open, dynamic environments
where the agents have to adapt to changes in their environment and they have to function
in circumstances where information is incomplete.
c
°2008
AI Access Foundation. All rights reserved.

Kakas, Mancarella, Sadri, Stathis & Toni

The development of the KGP model was originally motivated by the existing gap between modal logic specifications (Rao & Georgeff, 1991) of BDI agents (Bratman, Israel, &
Pollack, 1988) and their implementation (for example see the issues raised by Rao, 1996).
Another motivation for the development of KGP comes from our participation in the SOCS
project (SOCS, 2007), where we had the need for an agent model that satisfies several requirements. More specifically, we aimed at an agent model that was rich enough to allow
intelligent, adaptive and heterogeneous behaviour, formal so that it could lent itself well to
formal analysis, and implementable in such a way that the implementation was sufficiently
close to the formal specification to allow verification. Although several models of agency
have been proposed, none satisfies all of the above requirements at once.
To bridge the gap between specification and implementation the KGP model is based
on computational logic (CL). The focus of the work is to extend and synthesise a number
of useful computational logic techniques to produce formal and executable specifications of
agents. For this purpose, the model integrates abductive logic programming (ALP) (Kakas,
Kowalski, & Toni, 1992), logic programming with priorities (Kakas, Mancarella, & Dung,
1994; Prakken & Sartor, 1997) and constraint logic programming (Jaffar & Maher, 1994).
Each of these techniques has been explored in its own right, but their modular integration
within the KGP model explores extensions of each, as well as providing the high level agent
reasoning capabilities.
The KGP model provides a hierarchical architecture for agents. It specifies a collection
of modular knowledge bases, each formalised in CL. These knowledge bases support a collection of reasoning capabilities, such as planning, reactivity, and goal decision, all of which are
given formal specifications. The model also includes a specification of physical capabilities,
comprising of sensing and actuating. The capabilities are utilised within transitions, that
model how the state of the agent changes as a result of its reasoning, sensing and acting.
Transitions use selection operators providing them with inputs. A control component, called
cycle theory, also formalised in CL, specifies in what order the transitions are executed, depending on the environment, the state of the agent, and the preferences of the agent. The
cycle theory takes the agent control beyond the one-size-fits-all approach used by most
agent models, and allows us to specify agents with different preferences and profiles of behaviour (Sadri & Toni, 2005). In particular, whereas the majority of existing agent models
rely upon an “observe-plan-act”, by means of our cycle theory we can model behaviours
such as “observe-revise goals-plan–act” or “observe-plan-sense action preconditions-act” or
“observe-plan-act-plan-act”. We provide one example of cycle theory, that we refer to as
normal, allowing all behaviours above depending on different circumstances (the environment in which the agent is situated and its preferences). Note also that, with respect to
other agent models, the KGP model allows agents to revise their goals during their life-time,
and observing the environment according to two modalities: active and passive observation.
An agent built with a KGP architecture dynamically determines its goals, plans (partially) how to achieve the goals, interleaves planning with action executions and with making
observations in the environment and receiving any messages from other agents, adapts its
goals and plans to any new information it receives, and any changes it observes, and generates appropriate reactions.
A number of publications have already described aspects of (an initial version of) the
KGP agents. A precursor of the overall model has been described by Kakas, Mancarella,
286

Computational Logic Foundations of KGP Agents

Sadri, Stathis, and Toni (2004b), its planning component has been presented by Mancarella,
Sadri, Terreni, and Toni (2004), its cycle theory has been developed by Kakas, Mancarella,
Sadri, Stathis, and Toni (2004a) and its implementation has been discussed by Stathis et al.
(2004), by Yip, Forth, Stathis, and Kakas (2005), and by Bracciali, Endriss, Demetriou,
Kakas, Lu, and Stathis (2006). In this paper, we provide the full formal specification
of all the components of the KGP model, thus offering the complete technical account
of KGP in one place. In providing this full formal specification, we have adjusted and
further developed the model. In particular, the notion of state and its definition is novel,
the reasoning capabilities have been simplified and some have been added, the physical
capabilities have been extended (to include actuating) and formally defined, the transitions
and the selection operators have been formally defined in full.
The rest of the paper is structured as follows. In Sections 2 and 3 we give an outline of
the model and then review the background information necessary for the full description. In
Sections 4, 5, 6 and 7, respectively, we describe the internal state of KGP agents, their reasoning and physical capabilities, and their transitions. In Section 8 we describe the selection
operators which are then used in the cycle theory which is described in Section 9. Following
the detailed description of KGP agents we illustrate the model by a series of examples in
Section 10, and then compare the model with others in the literature in Section 11. Finally,
we conclude the paper in Section 12.

2. KGP Model: Outline
In this Section we give an overview of the KGP agent model and its components, and
provide some informal examples of its functioning. This model relies upon
• an internal (or mental) state, holding the agent Knowledge base (beliefs), Goals (desires) and Plans (intentions),
• a set of reasoning capabilities,
• a set of physical capabilities,
• a set of transition rules, defining how the state of the agent changes, and defined in
terms of the above capabilities,
• a set of selection operators, to enable and provide appropriate inputs to the transitions,
• a cycle theory, providing the control for deciding which transitions should be applied
when.
The model is defined in a modular fashion, in that different activities are encapsulated
within different capabilities and transitions, and the control is a separate module. The
model also has a hierarchical structure, depicted in Figure 1.
2.1 Internal State
This is a tuple hKB0 , F, C, Σi, where:
287

Kakas, Mancarella, Sadri, Stathis & Toni

CYCLE
THEORY

TRANSITIONS
S
T
A

SELECTION
OPERATORS

T
E
REASONING

CAPABILITIES

PHYSICAL CAPABILITIES

Figure 1: A graphical overview of the KGP model
• KB0 holds the beliefs of the agent about the external world in which it is situated
(including past communications), as well as a record of the actions it has already
executed.
• F is a forest of trees whose nodes are goals, which may be executable or not. Each
tree in the forest gives a hierarchical presentation of goals, in that the tree represents
the construction of a plan for the root of the tree. The set of leaves of any tree in F
forms a currently chosen plan for achieving the root of the tree. Executable goals are
actions which may be physical, communicative, or sensing. For simplicity, we assume
that actions are atomic and do not have a duration. Non-executable goals may be
mental or sensing. Only non-executable mental goals may have children, forming
(partial) plans for them. Actions have no children in any tree in F. Each goal has an
associated time variable, which is implicitly existentially quantified within the overall
state and serves two purposes: (1) indicating the time the goal is to be achieved,
which is instantiated if the goal is achieved at an appropriate time, and (2) providing
a unique identifier for that goal. In the remainder of the paper, we will often use the
following terminology for goals in F, when we want to emphasise their role and/or
their nature: the roots of trees in F will be referred to as top-level goals, executable
goals will be referred to as actions, and non-executable goals which are not top-level
goals will be referred to as sub-goals. Top-level goals will be classified as reactive or
non-reactive, as will be explained later. 1 Note that some top-level (reactive) goals
may be actions.
1. Roughly speaking, reactive goals are generated in response to observations, e.g. communications received
from other agents and changes in the environment, for example to repair plans that have already been
generated. Non-reactive goals, on the other hand, are the chosen desires of the agent.

288

Computational Logic Foundations of KGP Agents

• C is the Temporal Constraint Store, namely a set of constraint atoms in some given
underlying constraint language. These constrain the time variables of the goals in F.
For example, they may specify a time window over which the time of an action can
be instantiated, at execution time.
• Σ is a set of equalities instantiating time variables with time constants. For example,
when the time variables of actions are instantiated at action execution time, records
of the instantiations are kept in Σ.
2.2 Reasoning Capabilities
KGP supports the following reasoning capabilities:
• Planning, which generates plans for mental goals given as input. These plans consist
of temporally constrained sub-goals and actions designed for achieving the input goals.
• Reactivity, which is used to provide new reactive top-level goals, as a reaction to
perceived changes in the environment and the current plans held by the agent.
• Goal Decision, which is used to revise the non-reactive top-level goals, adapting the
agent’s state to changes in the environment.
• Identification of Preconditions and Identification of Effects for actions, which are used
to determine appropriate sensing actions for checking whether actions may be safely
executed (if their preconditions are known to hold) and whether recently executed
actions have been successful (by checking that some of their known effects hold).
• Temporal Reasoning, which allows the agent to reason about the evolving environment,
and to make predictions about properties, including non-executable goals, holding in
the environment, based on the (partial) information the agent acquires over its lifetime.
• Constraint Solving, which allows the agent to reason about the satisfiability of the
temporal constraints in C and Σ.
In the concrete realisation of the KGP model we provide in this paper, we have chosen
to realise the above capabilities in various extensions of the logic programming paradigm.
In particular, we use (conventional) logic programming for Identification of Preconditions
and Effects, abductive logic programming with constraints (see Section 3.2) for Planning,
Reactivity and Temporal Reasoning, and logic programming with priorities (see Section 3.3)
for Goal Decision.
2.3 Physical Capabilities
In addition to the reasoning capabilities, a KGP agent is equipped with “physical” capabilities, linking the agent to its environment, consisting of
• A Sensing capability, allowing the agent to observe that properties hold or do not
hold, and that other agents have executed actions.
• An Actuating capability, for executing (physical and communicative) actions.
289

Kakas, Mancarella, Sadri, Stathis & Toni

2.4 Transitions
The state hKB0 , F, C, Σi of an agent evolves by applying transition rules, which employ the
capabilities as follows:
• Goal Introduction (GI), possibly changing the top-level goals in F, and using Goal
Decision.
• Plan Introduction (PI), possibly changing F and C and using Planning.
• Reactivity (RE), possibly changing the reactive top-level goals in F and C, and using
the Reactivity capability.
• Sensing Introduction (SI), possibly introducing new sensing actions in F for checking
the preconditions of actions already in F.
• Passive Observation Introduction (POI), updating KB0 by recording unsolicited information coming from the environment, and using Sensing.
• Active Observation Introduction (AOI), possibly updating Σ and KB0 , by recording
the outcome of (actively sought) sensing actions, and using Sensing.
• Action Execution (AE), executing all types of actions and as a consequence updating
KB0 and Σ, and using Actuating.
• State Revision (SR), possibly revising F, and using Temporal Reasoning and Constraint Solving.
2.5 Cycle and Selection Operators
The behaviour of an agent is given by the application of transitions in sequences, repeatedly
changing the state of the agent. These sequences are not determined by fixed cycles of behaviour, as in conventional agent architectures, but rather by reasoning with cycle theories.
Cycle theories define preference policies over the order of application of transitions, which
may depend on the environment and the internal state of an agent. They rely upon the
use of selection operators for detecting which transitions are enabled and what their inputs
should be, as follows:
• action selection for inputs to AE; this selection operator uses the Temporal Reasoning
and Constraint Solving capabilities;
• goal selection for inputs to PI; this selection operator uses the Temporal Reasoning
and Constraint Solving capabilities;
• effect selection for inputs to AOI; this selection operator uses the Identification of
Effect reasoning capability;
• precondition selection for inputs to SI; this selection operator uses the Identification
of Preconditions, Temporal Reasoning and Constraint Solving capabilities;
290

Computational Logic Foundations of KGP Agents

The provision of a declarative control for agents in the form of cycle theories is a highly
novel feature of the model, which could, in principle, be imported into other agent systems.
In the concrete realisation of the KGP model we provide in this paper, we have chosen
to realise cycle theories in the same framework of logic programming with priorities and
constraints (see Section 3.3) that we also use for Goal Decision.
Some of the relationships between the capabilities, transitions and the selection operators
are summarised in Tables 2.5 and 2 below. Table 2.5 indicates which capabilities (rows)
are used by which transitions and selection operators. Table 2 indicates which selection
operators are used to compute possible inputs for which transitions in the cycle theory.

sensing
actuating
|=plan
|=pre
|=GD
|=react
|=T R
|=cs
|=ef f

AE
x
x

Transitions
AOI GI P OI
x
x

PI

RE

SR

SI

Selection operators
fGS fAS fES fP S

x
x

x

x
x

x
x

x
x
x

x
x

x

x
x

x

Table 1: A tabular overview of use of capabilities by transitions and selection operators.
Here, |=plan , |=pre , |=GD , |=react , |=T R , |=cs and |=ef f , stand for, respectively, the
planning, identification of preconditions, goal decision, reactivity, temporal reasoning, constraint solving and identification of effects (reasoning) capabilities, and
fGS , fAS , fES , fP S stand for, respectively, the goal, action, effect and precondition
selection operators.

AE
fGS
fAS
fES
fP S

AOI

GI

P OI

PI
x

RE

SR

SI

x
x
x

Table 2: A tabular overview of the connections between selection operators and transitions,
as required by the cycle theory. Here, fGS , fAS , fES , fP S stand for, respectively,
the goal, action, effect and precondition selection operators.
Before we provide these components, though, we introduce below informally a scenario
and some examples that will be used to illustrate the technical details of the KGP agent
291

Kakas, Mancarella, Sadri, Stathis & Toni

model throughout the paper. A full, formal presentation of these as well as additional
examples will be given throughout the paper and in Section 10.
2.6 Examples
We draw all our examples from a ubiquitous computing scenario that we call the San
Vincenzo scenario, presented by de Bruijn and Stathis (2003) and summarised as follows.
A businessman travels for work purposes to Italy and, in order to make his trip easier,
carries a personal communicator, namely a device that is a hybrid between a mobile phone
and a PDA. This device is the businessman’s KGP agent. This agent can be considered
as a personal service agent (Mamdani, Pitt, & Stathis, 1999) (or psa for short) because
it provides proactive information management and flexible connectivity to smart services
available in the global environment within which the businessman travels within.
2.6.1 Setting 1
The businessman’s psa requests from a ‘San Vincenzo Station’ agent, svs, the arrival time
of the train tr01 from Rome. As svs does not have this information it answers with a
refusal. Then later, svs receives information of the arrival time of the tr01 train from a
‘Central Office’ agent, co. When the psa requests the arrival time of tr01 again, svs will
accept the request and provide the information.
This first example requires one to use the Reactivity capability to model rules of interaction and the RE transition (a) to achieve interaction amongst agents, and (b) to specify
dynamic adjustments of the agent’s behaviour to changes, allowing different reactions to
the same request, depending on the current situation of the agent. Here, the interaction is a
form of negotiation of resources amongst agents, where resources are items of information.
Thus, the current situation of the agents amounts to what resources/information the agents
currently own.
This example also requires the combination of transitions RE, POI, and AE to achieve
the expected agents’ behaviours, as follows:
1. psa makes the initial request by applying AE
2. svs becomes aware of this request by performing POI (and changing its KB0 accordingly)
3. svs decides to reply with a refusal by performing RE (and adding the corresponding
action to its plan in F)
4. svs utters the refusal by performing AE
5. svs becomes aware, by POI, of the arrival time (modifying its KB0 accordingly)
6. psa makes the second request by applying AE again
7. svs decides to reply with the requested information by performing RE (and adding
the corresponding action to its plan in F) and communicates the information by
performing AE.
292

Computational Logic Foundations of KGP Agents

This sequence of transitions is given by the so-called normal cycle theory that we will
see in Section 9.
2.6.2 Setting 2
In preparation of the businessman’s next trip, his psa aims at getting a plane ticket from
Madrid to Denver as well as obtaining a visa to the USA. One possible way to buy plane
tickets is over the internet. Buying tickets this way is usually possible, but not to all
destinations (depending on whether the airlines flying to the destinations sell tickets over
the internet or not) and not without an internet connection. The psa does not currently have
the connection, nor the information that Denver is indeed a destination for which tickets
can be bought online. It plans to buy the ticket over the internet nonetheless, conditionally,
but checks the conditions before executing the planned action. After successfully buying
the ticket, psa focuses on the second goal, of obtaining a visa. This can be achieved by
applying to the USA embassy in Madrid, but the application requires an address in the
USA. This address can be obtained by arranging for a hotel in Denver.
This example illustrates the form of “partial” planning adopted by the KGP model
(where non-executable sub-goals as well as actions may be part of plans) and shows how
the combination of transition PI with SI and AE allows the psa agent to deal with partial
information, to generate conditional plans and plans with several “layers”, as follows:
1. psa is initially equipped with the top-level goals to get a ticket to Denver and to
obtain a visa (through an earlier application of GI)
2. by PI for the first goal, psa adds a “partial” plan to its F, of buying a ticket online
subject to sub-goals that there is an internet connection available and that online
tickets can be bought to Denver; these sub-goals are sensing goals
3. by SI, sensing actions are added to F to evaluate the sensing sub-goals in the environment
4. these sensing actions are executed by AE (and KB0 is modified accordingly)
5. depending on the sensed values of the sensing sub-goals the buying action may or
may not be executed by AE; let us assume in the remainder of the example that this
action is executed
6. SR is applied to eliminate all actions (since they have already been executed), subgoals and top-level goal of getting a ticket to Denver (since they have been achieved)
7. by PI for the remaining top-level goal of obtaining a visa, psa adds a plan to fill in
an application form (action) and acquiring a residence address in Denver (sub-goal)
8. the action cannot be executed, as psa knows that the businessman is not resident in
the USA; further PI introduces a plan for the sub-goal of booking a hotel (action) for
the subgoal of acquiring a residence address in Denver
9. AE executes the booking action
293

Kakas, Mancarella, Sadri, Stathis & Toni

10. AE executes the action of applying for a visa
11. SR eliminates all actions (since they have already been executed), sub-goal and toplevel goal of getting a visa (since they have been achieved).

3. Background
In this section we give the necessary background for the reasoning capabilities and the cycle
theory of KGP agents, namely:
• Constraint Logic Programming, pervasive to the whole model,
• Abductive Logic Programming, at the heart of the Planning, Reactivity and Temporal
Reasoning capabilities, and
• Logic Programming with Priorities, at the heart of the Goal Decision capability and
Cycle Theories.
3.1 Constraint Logic Programming
Constraint Logic Programming (CLP) (Jaffar & Maher, 1994) extends logic programming
with constraint predicates which are not processed as ordinary logic programming predicates,
defined by rules, but are checked for satisfiability and simplified by means of a built-in,
“black-box” constraint solver. These predicates are typically used to constrain the values
that variables in the conclusion of a rule can take (together with unification which is also
treated via an equality constraint predicate). In the KGP model, constraints are used
to determine the value of time variables, in goals and actions, under a suitable temporal
constraint theory.
The CLP framework is defined over a structure < consisting of a domain D(<) and a set
of constraint predicates which includes equality, together with an assignment of relations
on D(<) for each such constraint predicate. In CLP, constraints are built as first-order
formulae in the usual way from primitive constraints of the form c(t1 , . . . , tn ) where c is a
constraint predicate symbol and t1 , . . . , tn are terms constructed over the domain, D(<),
of values. Then the rules of a constraint logic program, P , take the same form as rules in
conventional logic programming given by
H ← L1 , . . . , Ln
with H an (ordinary) atom, L1 , . . . , Ln literals, and n ≥ 0. Literals can be positive, namely
ordinary atoms, or negative, namely of the form not B, where B is an ordinary atom,
or constraint atoms over <. The negation symbol not indicates negation as failure (first
introduced by Clark, 1978). All variables in H and Li are implicitly universally quantified,
with scope the entire rule. H is called the head (or the conclusion) and L1 , . . . , Ln is called
the body (or the conditions) of a rule of the form above. If n = 0, the rule is called a fact.
A valuation, ϑ, of a set of variables is a mapping from these variables to the domain
D(<) and the natural extension which maps terms to D(<). A valuation ϑ, on the set of all
variables appearing in a set of constraints C, is called an <-solution of C iff Cϑ, obtained by
applying ϑ to C, is satisfied, i.e. Cϑ evaluates to true under the given interpretation of the
294

Computational Logic Foundations of KGP Agents

constraint predicates and terms. This is denoted by ϑ |=< C. A set C is called <-solvable
or <-satisfiable, denoted by |=< C, iff it has at least one <-solution, i.e. ϑ |=< C for some
valuation ϑ.
One way to give the meaning of a constraint logic program P is to consider the grounding of the program over its Herbrand base and all possible valuations, over D(<), of its
constraint variables. In each such rule, if the ground constraints C in the body are evaluated to true then the rule is kept with the constraints C dropped, otherwise the whole
rule is dropped. Let ground(P ) be the resulting ground program. The meaning of P is
then given by the meaning |=LP of ground(P ), for which there are many different possible
choices (Kakas, Kowalski, & Toni, 1998). The resulting overall semantics for the constraint
logic program P will be referred to as |=LP (<) . More precisely, given a constraint logic
program P and a conjunction N ∧ C (where N is a conjunction of non-constraint literals
and C is a conjunction of constraint atoms), in the remainder of the paper we will write
P |=LP (<) N ∧ C
to denote that there exists a ground substitution ϑ over the variables of N ∧ C such that:
• ϑ |=< C
• ground(P ) |=LP N ϑ.
3.2 Abductive Logic Programming with Constraints
An abductive logic program with constraints is a tuple h<, P, A, Ii where:
• < is a structure as in Section 3.1
• P is a constraint logic program, namely a set of rules of the form
H ← L1 , . . . , Ln
as in Section 3.1
• A is a set of abducible predicates in the language of P . These are predicates not
occurring in the head of any clause of P (without loss of generality, see (Kakas et al.,
1998)). Atoms whose predicate is abducible are referred to as abducible atoms or
simply as abducibles.
• I is a set of integrity constraints, that is, a set of sentences in the language of P . All
the integrity constraints in the KGP model have the implicative form
L1 , . . . , Ln ⇒ A1 ∨ . . . ∨ Am (n ≥ 0, m > 0)
where Li are literals (as in the case of rules) 2 , Aj are atoms (possibly the special
atom f alse). The disjunction A1 ∨ . . . ∨ Am is referred to as the head of the constraint
and the conjunction L1 , . . . , Ln is referred to as the body. All variables in an integrity
constraint are implicitly universally quantified from the outside, except for variables
occurring only in the head, which are implicitly existentially quantified with scope the
head itself.
2. If n = 0, then L1 , . . . , Ln represents the special atom true.

295

Kakas, Mancarella, Sadri, Stathis & Toni

Given an abductive logic program with constraints h<, P, A, Ii and a formula (query)
Q, which is an (implicitly existentially quantified) conjunction of literals in the language
of P , the purpose of abduction is to find a (possibly minimal) set of (ground) abducible
atoms Γ which, together with P , “entails” (an appropriate ground instantiation of) Q, with
respect to some notion of “entailment” that the language of P is equipped with, and such
that the extension of P by Γ “satisfies” I (see (Kakas et al., 1998) for possible notions
of integrity constraint “satisfaction”). Here, the notion of “entailment” is the combined
semantics |=LP (<) , as discussed in Section 3.1.
Formally, given a query Q, a set ∆ of (possibly non-ground) abducible atoms, and a
set C of (possibly non-ground) constraints, the pair (∆, C) is an abductive answer (with
constraints) for Q, with respect to an abductive logic program with constraints h<, P, A, Ii,
iff for all groundings σ for the variables in Q, ∆, C such that σ |=< C, it holds that
(i) P ∪ ∆σ |=LP (<) Qσ, and
(ii) P ∪∆σ |=LP (<) I, i.e. for each B ⇒ H ∈ I, if P ∪∆σ |=LP (<) B then P ∪∆σ |=LP (<) H.
Here, ∆σ plays the role of Γ in the earlier informal description of abductive answer. Note
also that, by (ii), integrity constraints are not classical implications.
Note also that, when representing knowledge as an abductive logic program, one needs
to decide what should go into the logic program, what in the integrity constraints and what
in the abducibles. Intuitively, integrity constraints are “normative” in that they need to be
enforced, by making sure that their head holds whenever their body does (by condition (ii)
above), whereas logic programming rules enable, with the help of abducibles, the derivation
of given goals (by condition (i) above). Finally, abducibles are chosen amongst the literals
that cannot be derived by means of logic programming rules. In this paper, we will represent reactive constraints (that are condition-action rules forcing the reactive behaviour of
agents) as integrity constraints, thus to some extent addressing this knowledge representation challenge posed by abductive logic programming by imposing a sort of “structure” on
the abductive logic programs we use.
The notion of abductive answer can be extended to take into account an initial set
of (possibly non-ground) abducible atoms ∆0 and an initial set of (possibly non-ground)
constraint atoms C0 . In this extension, an abductive answer for Q, with respect to
(h<, P, A, Ii, ∆0 , C0 )
is a pair (∆, C) such that
(i) ∆ ∩ ∆0 = {}
(ii) C ∩ C0 = {}, and
(iii) (∆ ∪ ∆0 , C ∪ C0 ) is an abductive answer for Q with respect to h<, P, A, Ii (in the
earlier sense).
It is worth noticing that an abductive answer (∆, C) for the query true with respect to
(h<, P, A, Ii, ∆0 , C0 )
296

Computational Logic Foundations of KGP Agents

should be read as the fact that the abducibles in ∆0 ∪ ∆, along with the constraints in
C0 ∪ C, guarantee the overall consistency with respect to the integrity constraints given in
I. This will be used for the specification of some capabilities of KGP agents.
In the remainder of the paper, for simplicity, we will omit < from abductive logic
programs, which will be written simply as triples hP, A, Ii. In addition, all abductive logic
programs that will present in KGP are variants of a core event calculus (Kowalski & Sergot,
1986), that we will define in Section 5.1.1.
3.3 Logic Programming with Priorities
For the purposes of this paper, a logic program with priorities over a constraint structure
<, referred to as T , consists of four parts:
(i) a low-level or basic part P , consisting of a logic program with constraints; each rule
in P is assigned a name, which is a term; e.g. one such rule could be
n(X, Y ) : p(X) ← q(X, Y ), r(Y )
with name n(X, Y ) naming each ground instance of the rule;
(ii) a high-level part H, specifying conditional, dynamic priorities amongst rules in P or
H; e.g. one such priority could be
h(X) : m(X) Â n(X) ← c(X)
to be read: if (some instance of) the condition c(X) holds, then (the corresponding instance of) the rule named by m(X) should be given higher priority than (the
corresponding instance of) the rule named by n(X). The rule itself is named h(X);
(iii) an auxiliary part A, which is a constraint logic program defining (auxiliary) predicates
occurring in the conditions of rules in P, H and not in the conclusions of any rule in
P or H;
(iv) a notion of incompatibility which, for our purposes, can be assumed to be given as a
set of rules defining the predicate incompatible/2, e.g.
incompatible(p(X), p0 (X))
to be read: any instance of the literal p(X) is incompatible with the corresponding
instance of the literal p0 (X). We assume that incompatibility is symmetric and always
includes that r Â s is incompatible with s Â r for any two rule names r, s. We refer
to the set of all incompatibility rules as I.
Any concrete LPP framework is equipped with a notion of entailment, which we denote by |=pr , that is defined on top of the underlying logic programming with constraints
semantics |=LP (<) . This is defined differently by different approaches to LPP but they all
share the following pattern. Given a logic program with priorities T = hP, H, A, Ii and a
conjunction α of ground (non-auxiliary) atoms, T |=pr α iff
(i) there exists a subset P 0 of the basic part P such that P 0 ∪ A |=LP (<) α, and
297

Kakas, Mancarella, Sadri, Stathis & Toni

(ii) P 0 is “preferred” wrt H ∪A over any other subset P 00 of P that derives (under |=LP (<) )
a conclusion that is incompatible, wrt I, with α.
Each framework has its own way of specifying what is meant for one sub-theory P 0 to be
“preferred” over another sub-theory P 00 . For example, in existing literature (Kakas et al.,
1994; Prakken & Sartor, 1996; Kowalski & Toni, 1996; Kakas & Moraitis, 2003), |=pr is
defined via argumentation. This is also the approach that we adopt, relying on the notion
of an admissible argument as a sub-theory that is (i) consistent (does not have incompatible
conclusions) and (ii) whose rules do not have lower priority, with respect to the high-level
part H of our theory, than those of any other sub-theory that has incompatible conclusions
with it. The precise definition of how sets of rules are to be compared again is a matter of
choice in each specific framework of LPP.
Given such a concrete definition of admissible sub-theories, the preference entailment,
T |=pr α, is then given by:
(i) there exists a (maximal) admissible sub-theory T 0 of T such that T 0 |=LP (<) α, and
(ii) for any α that is incompatible with α there does not exist an admissible sub-theory
T 00 of T such that T 00 |=LP (<) α.
When only the first condition of the above is satisfied we say that the theory T credulously prefers or possibly prefers α. When both conditions are satisfied we say that the
theory sceptically prefers α.

4. The State of KGP Agents
In this Section we define formally the concept of state for a KGP agent. We also introduce
all the notation that we will use in the rest of the paper in order to refer to state components.
Where necessary, we will also try to exemplify our discussion with simple examples.
4.1 Preliminaries
In the KGP model we assume (possibly infinite) vocabularies of:
• fluents, indicated with f, f 0 , . . .,
• action operators, indicated with a, a0 , . . .,
• time variables, indicated with τ, τ 0 , . . .,
• time constants, indicated with t, t0 , . . . , 1, 2, . . ., standing for natural numbers (we also
often use the constant now to indicate the current time)
• names of agents, indicated with c, c0 , . . . .
• constants, other than the ones mentioned above, normally indicated with lower case
letters, e.g. r, r1 , . . .
298

Computational Logic Foundations of KGP Agents

• a given constraint language, including constraint predicates <, ≤, >, ≤, =, 6=, with respect to some structure < (e.g. the natural numbers) and equipped with a notion of
constraint satisfaction |=< (see Section 3.1).
We assume that the set of fluents is partitioned into two disjoint sets:
• mental fluents, intuitively representing properties that the agent itself is able to plan
for so that they can be satisfied, but can also be observed, and
• sensing fluents, intuitively representing properties which are not under the control of
the agent and can only be observed by sensing the external environment.
For example, problem f ixed and have resource may represent mental fluents, namely
the properties that a (given) problem has been fixed and a (given) resource should be obtained, whereas request accepted and connection on may represent sensing fluents, namely
the properties that a request for some (given) resource is accepted and that some (given)
connection is active. Note that it is important to distinguish between mental and sensing
fluents as they are treated differently by the control of the agent: mental fluents need to
be planned for, whereas sensing fluents can only be observed. This will be clarified later in
the paper.
We also assume that the set of action operators is partitioned into three disjoint sets:
• physical action operators, representing actions that the agent performs in order to
achieve some specific effect, which typically causes some changes in the environment;
• communication action operators, representing actions which involve communications
with other agents;
• sensing action operators, representing actions that the agent performs to establish
whether some fluent (either a sensing fluent or an expected effect of some action)
holds in the environment, or whether some agent has performed some action.
For example, sense(connection on, τ ) is an action literal representing the act of sensing whether or not a network connection is on at time τ , do(clear table, τ ) is an action literal representing the physical action of removing every item on a given table, and
tell(c1 , c2 , request(r1 ), d, τ ) is an action literal representing a communication action which
expresses that agent c1 is requesting from agent c2 the resource r1 within a dialogue with
identifier d, at time τ 3 .
Each fluent and action operator has an associated arity: we assume that this arity is
greater than or equal to 1, in that one argument (the last one, by convention) is always
the time point at which a given fluent holds or a given action takes place. This time point
may be a time variable or a time constant. Given a fluent f of arity n > 0, we refer
to f (s1 , . . . , sn−1 , x) and ¬f (s1 , . . . , sn−1 , x), where each si is a constant and x is a time
variable or a time constant as (timed) fluent literals 4 . Given a fluent literal `, we denote by `
3. The role of the dialogue identifier will become clearer in Section 10. Intuitively, this is used to “link”
communication actions occurring within the same dialogue.
4. Note that ¬ represents classical negation. Negation as failure occurs in the model only within the
knowledge bases of agents, supporting the reasoning capabilities and the cycle theory. All other negations
in the state are to be understood as classical negations.

299

Kakas, Mancarella, Sadri, Stathis & Toni

its complement, namely ¬f (s1 , . . . , sn−1 , x) if ` is f (s1 , . . . , sn−1 , x), and f (s1 , . . . , sn−1 , x) if
` is ¬f (s1 , . . . , sn−1 , x). Examples of fluent literals are have resource(pen, τ ), representing
that a certain resource pen should be obtained at some time τ , as well as (the ground)
¬on(box, table, 10), representing that at time 10 (a certain) box should not be on (a certain)
table.
Note that we assume that fluent literals are ground except for the time parameter. This
will allow us to keep the notation simpler and to highlight the crucial role played by the
time parameter. Given this simplification, we will often denote timed fluent literals simply
by `[x].
Given an action operator a of arity n > 0, we refer to a(s1 , . . . , sn−1 , x), where each si
is a constant and x is a time variable or time constant, as a (timed) action literal. Similarly
to the case of fluent literals, for simplicity, we will assume that timed action literals are
ground except possibly for the time. Hence, we will often denote timed action literals by
a[x].
We will adopt a special syntax for sensing actions, that will always have the form (x is
either a time variable or a time constant):
• sense(f, x), where f is a fluent, or
• sense(c : a, x), where c is the name of an agent and a is an action operator.
In the first case, the sensing action allows the agent to inspect the external environment in
order to check whether or not the fluent f holds at the time x of sensing. In the second
case, the sensing action allows the agent to determine whether, at time x, another agent c
has performed some action a.
We will now define formally the concept of state hKB0 , F, C, Σi of an agent.
4.2 Forest: F
Each node in each tree in F is:
• either a non-executable goal, namely a (non-ground) timed fluent literal,
• or an executable goal, namely a (non-ground) timed action literal.
An example of a tree in F is given in Figure 2, where p2 is some given problem that
the agent (c1 ) needs to fix by getting two resources r1 and r2 , and where the agent has
already decided to get r1 from some other agent c2 and has already planned to ask c2 by
the communication action tell(c1 , c2 , request(r1 ), d, τ4 ). For example, in the San Vincenzo
scenario, p2 may be “transfer to airport needs to be arranged”, r1 may be a taxi, and c2
a taxi company, needed for transportation to some train station, and finally r2 may be a
train ticket.
Note that the time variable τ in non-executable goals `[τ ] and actions a[τ ] in (any tree
in) F is to be understood as a variable that is existentially quantified within the whole state
of the agent. Whenever a goal or action is introduced within a state, its time variable is to
be understood as a distinguished, fresh variable, also serving as its identifier.
300

Computational Logic Foundations of KGP Agents

problem f ixed(p2, τ1 )

³³

³³

³³PPP
³³
PP
³
³
P

PP
PP

have resource(r1 , τ2 )

have resource(r2 , τ3 )

tell(c1 , c2 , request(r1 ), d, τ4 )

Figure 2: An example tree in F
As indicated in Section 2, roots of trees are referred to as top-level goals, executable
goals are often called simply actions, non-executable goals may be top-level goals or subgoals. For example, in Figure 2, the node with identifier τ1 is a top-level goal, the nodes
with identifiers τ2 , τ3 are sub-goals and the node with identifier τ4 is an action.
Notation 4.1 Given a forest F and a tree T ∈ F:
• for any node n of T , parent(n, T ), children(n, T ), ancestors(n, T ), siblings(n, T ),
descendents(n, T ), will indicate the parent of node n in T , the children of n in T ,
etc. and leaf (n, T ) will have value true if n is a leaf in T , false otherwise.
• for any node n of F, parent(n, F), children(n, F), ancestors(n, F), siblings(n, F),
descendents(n, F), leaf (n, F) will indicate the parent(n, T ) for the tree T in F where
n occurs, etc. (T is unique, due to the uniqueness of the time variable identifying
nodes).
• nodes(T ) will represent the set of nodes in T , and nodes(F) will represent the set
S
nodes(F) = T ∈F nodes(T ).
Again, as indicated in Section 2, each top-level goal in each tree in F will be either
reactive or non-reactive. We will see, in Section 7, that reactive top-level goals are introduced into the state by the RE transition whereas non-reactive top-level goals are introduced by the GI transition. For example, F of agent c1 may consist of the tree in
Figure 2, with root a non-reactive goal, as well as a tree with root the reactive goal (action)
301

Kakas, Mancarella, Sadri, Stathis & Toni

tell(c1 , c2 , accept request(r3 ), d0 , τ5 ). This action may be the reply (planned by agent c1 )
to some request for resource r3 by agent c2 (for example, in the San Vincenzo scenario, r3
may be a meeting requested by some colleague).
Notation 4.2 Given a forest F
• Rootsr (F) (resp. Rootsnr (F)) will denote the set of all reactive (resp. non-reactive)
top-level goals in F
• nodesr (F) (resp. nodesnr (F)) will denote the subset of nodes(F) consisting of nodes
in all trees whose root is in Rootsr (F) (resp. Rootsnr (F))
• r(F) (resp. nr(F)) stands for the reactive (resp. non-reactive) part of F, namely the
set of all trees in F whose root is in Rootsr (F) (resp. Rootsnr (F)).
Trivially, r(F) and nr(F) are disjoint, and F= r(F) ∪ nr(F).
4.3 Temporal Constraint Store: C
This is a set of constraint atoms, referred to as temporal constraints, in the given underlying
constraint language. Temporal constraints refer to time constants as well as to time variables
associated with goals (currently or previously) in the state.
For example, given a forest with the tree in Figure 2, C may contain τ1 > 10, τ1 ≤ 20,
indicating that the top-level goal (of fixing problem p2) needs to be achieved within the time
interval (10, 20], τ2 < τ1 , τ3 < τ1 , indicating that resources r1 and r2 need to be acquired
before the top-level goal can be deemed to be achieved, and τ4 < τ2 , indicating that the
agent needs to ask agent c2 first. Note that we do not need to impose that τ2 and τ3 are
executed in some order, namely C may contain neither τ2 < τ3 , nor τ3 < τ2 .
4.4 Agents’ Dynamic Knowledge Base: KB0
KB0 is a set of logic programming facts in the state of an agent, recording the actions which
have been executed (by the agent or by others) and their time of execution, as well as the
properties (i.e. fluents and their negation) which have been observed and the time of the
observation. Formally, these facts are of the following forms:
• executed(a, t) where a[t] is a ground action literal, meaning that action a has been
executed by the agent at time t.
• observed(`, t) where `[t] is a ground fluent literal, meaning that ` has been observed
to hold at time t.
• observed(c, a[t0 ], t) where c is an agent’s name, different from the name of the agent
whose state we are defining, t and t0 are time constants, and a[t0 ] is a (ground) action
literal. This means that the given agent has observed at time t that agent c has
executed the action a at time t0 5 .
5. We will see that, by construction, it will always be the case that t0 ≤ t. Note that the time of executed
actions, t0 , and the time of their observation, t, will typically be different in any concrete implementation
of the KGP model, as they depend, for example, on the time of execution of transitions within the
operational trace of an agent.

302

Computational Logic Foundations of KGP Agents

Note that all facts in KB0 are variable-free, as no time variables occur in them. Facts
of the first kind record actions that have been executed by the agent itself. Facts of the
second kind record observations made by the agent in the environment, excluding actions
executed by other agents, which are represented instead as facts of the third kind.
For example, if the action labelled by τ4 in Figure 2 is executed (by the AE transition)
at time 7 then executed(tell(c1 , c2 , request(r1 ), d), 7) will be added to KB0 . Moreover, if,
at time 9, c1 observes (e.g. by transition POI) that it has resource r2 , then the observation
observed(have resource(r2 ), 9) will be added to KB0 . Finally, KB0 may contain
observed(c2 , tell(c2 , c1 , request(r3 ), d0 , 1), 6)
to represent that agent c1 has become aware, at time 6, that agent c2 has requested, at the
earlier time 1, resource r3 from c1 .
4.5 Instantiation of Time Variables: Σ
When a time variable τ occurring in some non-executable goal `[τ ] or some action a[τ ] in F
is instantiated to a time constant t (e.g. at action execution time), the actual instantiation
τ = t is recorded in the Σ component of the state of the agent. For example, if the action
labelled by τ4 in Figure 2 is executed at time 7, then τ4 = 7 will be added to Σ.
The use of Σ allows one to record the instantiation of time variables while at the same
time keeping different goals with the same fluent distinguished. Clearly, for each time
variable τ there exists at most one equality τ = t in Σ.
Notation 4.3 Given a time variable τ , we denote by Σ(τ ) the time constant t, if any, such
that τ = t ∈ Σ.
It is worth pointing out that the valuation of any temporal constraint c ∈ C will always
take the equalities in Σ into account. Namely, any ground valuation for the temporal
variables in c must agree with Σ on the temporal variables assigned to them in Σ. For
example, given Σ = {τ = 3} and C = {τ1 > τ }, then τ1 = 10 is a suitable valuation,
whereas τ1 = 1 is not.

5. Reasoning Capabilities
In this section, we give detailed specifications for the various reasoning capabilities, specified within the framework of ordinary logic programming (for Temporal Reasoning and
Identification of Preconditions and Effects), of Abductive Logic Programming with Constraints (Section 3.2, for Planning and Reactivity), of Logic Programming with Priorities
with Constraints (Section 3.3, for Goal Decision), of constraint programming (Section 3.1,
for Constraint Solving).
The reasoning capabilities are defined by means of a notion of “entailment” with respect
to an appropriate knowledge base (and a time point now, where appropriate), as follows:
303

Kakas, Mancarella, Sadri, Stathis & Toni

• |=T R and KBT R for Temporal Reasoning, where KBT R is a constraint logic program
and a variant of the framework of the Event Calculus (EC) for reasoning about actions,
events and changes (Kowalski & Sergot, 1986) 6 ;
• |=now
plan and KBplan for Planning, where KBplan is an abductive logic program with
constraints, extending KBT R ;
• |=now
react and KBreact for Reactivity, where KBreact is an extension of KBplan , incorporating additional integrity constraints representing reactive rules;
• |=pre and KBpre , where KBpre is a logic program contained in KBT R ;
• |=ef f and KBef f , where KBef f is a logic program contained in KBT R ;
• |=now
GD and KBGD , where KBGD is a logic program with priorities and constraints.
The constraint solving capability is defined in terms of an “entailment” |=cs which is
basically |=< as defined in Section 3.1.
5.1 Temporal Reasoning, Planning, Reactivity, Identification of Preconditions
and Effects: EC-based Capabilities
These reasoning capabilities are all specified within the framework of the event calculus
(EC) for reasoning about actions, events and changes (Kowalski & Sergot, 1986). Below,
we first give the core EC and then show how to use it to define the various capabilities in
this section.
5.1.1 Preliminaries: Core Event Calculus
In a nutshell, the EC allows one to write meta-logic programs which “talk” about objectlevel concepts of fluents, events (that we interpret as action operators) 7 , and time points.
The main meta-predicates of the formalism are:
• holds at(F, T ) - a fluent F holds at a time T ;
• clipped(T1 , F, T2 ) - a fluent F is clipped (from holding to not holding) between times
T1 and T2 ;
• declipped(T1 , F, T2 ) - a fluent F is declipped (from not holding to holding) between
times T1 and T2 ;
• initially(F ) - a fluent F holds at the initial time, say time 0;
• happens(O, T ) - an operation O happens at a time T ;
• initiates(O, T, F ) - a fluent F starts to hold after an operation O at time T ;
6. A more sophisticated, abductive logic programming version of |=T R and KBT R is given by Bracciali and
Kakas (2004).
7. In this section we use the original event calculus terminology of events instead of operators, as in the
rest of the paper.

304

Computational Logic Foundations of KGP Agents

• terminates(O, T, F ) - a fluent F ceases to hold after an operation O at time T .
Roughly speaking, the last two predicates represent the cause-effects links between operations and fluents in the modelled world. We will also use a meta-predicate
• precondition(O, F ) - the fluent F is one of the preconditions for the executability of
the operation O.
Fluent literals in an agent’s state are mapped onto the EC as follows. The EC-like representation of a fluent literal f [τ ] (resp. ¬f [τ ]) in an agent’s state is the atom holds at(f, τ )
(resp. holds at(¬f, τ )). Moreover, when arguments other than the time variable need to be
considered, the EC representation of a fluent literal f (x1 , . . . , xn , τ ) (resp. ¬f (x1 , . . . , xn , τ ))
is holds at(f (x1 , . . . , xn ), τ ) (resp. holds at(¬f (x1 , . . . , xn ), τ ). 8
Similarly, action literals in the state of an agent can be represented in the EC in a
straightforward way. Given an action literal a[τ ] its EC representation is happens(a, τ ).
When arguments other than time are considered, as e.g. in a(x1 , . . . , xn , τ ), the EC representation is given by happens(a(x1 , . . . xn ), τ ).
In the remainder of the paper, with an abuse of terminology, we will sometimes refer
to f (x1 , . . . , xn ) and ¬f (x1 , . . . , xn ) interchangeably as fluent literals or fluents (although
strictly speaking they are fluent literals), and to a(x1 , . . . xn ) interchangeably as action
literals or action operators (although strictly speaking they are action literals).
The EC allows one to represent a wide variety of phenomena, including operations with
indirect effects, non-deterministic operations, and concurrent operations (Shanahan, 1997).
The core EC we use in this paper consists of two parts: domain-independent rules and
domain-dependent rules. The basic domain-independent rules, directly borrowed from the
original EC, are:
holds at(F, T2 ) ←
holds at(¬F, T2 ) ←
holds at(F, T ) ←
holds at(¬F, T ) ←
clipped(T1 , F, T2 ) ←
declipped(T1 , F, T2 ) ←

happens(O, T1 ), initiates(O, T1 , F ),
T1 < T2 , not clipped(T1 , F, T2 )
happens(O, T1 ), terminates(O, T1 , F ),
T1 < T2 , not declipped(T1 , F, T2 )
initially(F ), 0 ≤ T, not clipped(0, F, T )
initially(¬F ), 0 ≤ T, not declipped(0, F, T )
happens(O, T ), terminates(O, T, F ), T1 ≤ T < T2
happens(O, T ), initiates(O, T, F ), T1 ≤ T < T2

The domain-dependent rules define initiates, terminates, and initially, e.g. in the case
of setting 2.6.1 in Section 2.6 we may have
initiates(tell(C, svs, inf orm(Q, I), D), T, have inf o(svs, Q, I)) ←
holds at(trustworthy(C), T )
initially(¬have inf o(svs, arrival(tr01), I)
8. Note that we write holds at(¬f (x1 , . . . , xn ), τ ) instead of not holds at(f (x1 , . . . , xn ), τ ), as done e.g. by
Shanahan, 1997, because we want to reason at the object-level about properties being true or false in the
environment. We use not within the meta-level axioms of the event calculus (see below) to implement
persistence.

305

Kakas, Mancarella, Sadri, Stathis & Toni

initially(trustworthy(co))
Namely, an action by agent C of providing information I concerning a query Q to the
agent svs (the “San Vincenzo station” agent) initiates the agent svs having the information
about Q, provided that C is trustworthy. Moreover, initially agent co (the “Central Office”
agent) is trustworthy, and agent svs has no information about the arrival time of tr01. The
conditions for the rule defining initiates can be seen as preconditions for the effects of the
operator tell to take place. Preconditions for the executability of operators are specified by
means of a set of rules (facts) defining the predicate precondition, e.g.
precondition(tell(svs, C, inf orm(Q, I), D), have inf o(svs, Q, I))
namely the precondition for agent svs to inform any agent C of I about Q is that svs indeed
has information I about Q.
Notice that the presence in the language of fluents and their negation, e.g. f and ¬f ,
poses the problem of “inconsistencies”, i.e. it may be the case that both holds at(f, t) and
holds at(¬f, t) can be derived from the above axioms and a set of events (i.e. a given set
of happens atoms). However, it can easily be shown that this is never the case, provided
that the domain-dependent part does not contain two conflicting statements of the form
initially(f ) and initially(¬f ) since inconsistencies cannot be caused except at the initial
time point (see e.g. Miller & Shanahan, 2002, p. 459).
In the remainder of the paper we will assume that the domain-dependent part is always
consistent for our agents.
To allow agents to draw conclusions from the contents of KB0 , which represents the
“narrative” part of the agent’s knowledge, we add to the domain-independent rules the
following bridge rules:
holds at(F, T2 ) ←
holds at(¬F, T2 ) ←
happens(O, T ) ←
happens(O, T ) ←

observed(F, T1 ), T1 ≤ T2 , not clipped(T1 , F, T2 )
observed(¬F, T1 ), T1 ≤ T2 , not declipped(T1 , F, T2 )
executed(O, T )
observed( , O[T ], )

Notice that these bridge rules make explicit the translation from the state representation
to the EC representation of fluents and actions we have mentioned earlier on in this section.
Note also that we assume that a fluent holds from the time it is observed to hold. This
choice is dictated by the rationale that observations can only be considered and reasoned
upon from the moment the agent makes them. On the other hand, actions by other agents
have effect from the time they have been executed 9 .
Having introduced the ability to reason with narratives of events and observations, we
need to face the problem of “inconsistency” due to conflicting observations, e.g. an agent
may observe that both a fluent and its negation hold at the same time. As we have done
9. If the time of the action is unknown at observation time, then the last rule above may be replaced by
happens(O, T ) ← observed( , O[ ], T )
namely the value of a fluent is changed according to observations from the moment the observations
are made.

306

Computational Logic Foundations of KGP Agents

above for the set of initially atoms, we will assume that the external world is consistent
too, i.e. it can never happen that both observed(f, t) and observed(¬f, t) belong to KB0 ,
for any fluent f and time point t.
However, we still need to cope with the frame consistency problem, which arises, e.g.
given observations observed(f, t) and observed(¬f, t0 ), with t 6= t0 . This issue is analogous
to the case when two different events happen at the same time point and they initiate and
terminate the same fluent. In the original EC suitable axioms for the predicates clipped
and declipped are added, as given above, to avoid both a fluent and its negation holding at
the same time after the happening of two such events at the same time. We adopt here a
similar solution to cope with observations, namely by adding the following two axioms to
the domain-independent part:
clipped(T1 , F, T2 ) ←
declipped(T1 , F, T2 ) ←

observed(¬F, T ), T1 ≤ T < T2
observed(F, T ), T1 ≤ T < T2

This solution may be naive in some circumstances and more sophisticated solutions may
be adopted, as e.g. the one proposed by Bracciali and Kakas (2004).
5.1.2 Temporal Reasoning
The temporal reasoning capability is invoked by other components of the KGP model
(namely the Goal Decision capability, the State Revision transition and some of the selection operators, see Section 7) to prove or disprove that a given (possibly temporally
constrained) fluent literal holds, with respect to a given theory KBT R . For the purposes
of this paper KBT R is an EC theory composed of the domain-independent and domaindependent parts as given in Section 5.1.1, and of the “narrative” part given by KB0 . Then,
given a state S, a fluent literal `[τ ] and a possibly empty set 10 of temporal constraints T C,
the temporal reasoning capability |=T R is defined as
S |=T R `[τ ] ∧ T C iff KBT R |=LP (<) holds at(`, τ ) ∧ T C.
For example, given the EC formulation in Section 5.1.1 for setting 2.6.1 in Section 2.6,
if the state S = hKB0 , F, C, Σi for agent svs contains
KB0 = {observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d, 15), 17)},
then S |=T R have inf o(svs, arrival(tr01), 18, τ ) ∧ τ > 20.
5.1.3 Planning
A number of abductive variants of the EC have been proposed in the literature to deal with
planning problems, e.g. the one proposed by Shanahan, 1989. Here, we propose a novel
variant, somewhat inspired by the E-language (Kakas & Miller, 1997), to allow situated
agents to generate partial plans in a dynamic environment.
We will refer to KBplan = hPplan , Aplan , Iplan i as the abductive logic program where:
10. Here and in the remainder of the paper sets are seen as conjunctions, where appropriate.

307

Kakas, Mancarella, Sadri, Stathis & Toni

• Aplan = {assume holds, assume happens}, namely we consider two abducible predicates, corresponding to assuming that a fluent holds or that an action occurs, respectively, at a certain time point;
• Pplan is obtained by adding to the core EC axioms and the “narrative” given by KB0
the following rules
happens(O, T ) ← assume happens(O, T )
holds at(F, T ) ← assume holds(F, T )
• Iplan contains the following set of integrity constraints
holds at(F, T ), holds at(¬F, T ) ⇒ f alse
assume happens(O, T ), precondition(O, P ) ⇒ holds at(P, T )
assume happens(O, T ), not executed(O, T ), time now(T 0 ) ⇒ T > T 0
These integrity constraints in Iplan prevent the generation of (partial) plans which are
unfeasible. The first integrity constraint makes sure that no plan is generated which entails
that a fluent and its negation hold at the same time. The second integrity constraint makes
sure that, if a plan requires an action to occur at a certain time point, the further goal of
enforcing the preconditions of that action to hold at that time point is taken into account
in the same plan. This means that, if those preconditions are not already known to hold,
the plan will need to accommodate actions to guarantee that they will hold at the time of
execution of the action. Finally, the last integrity constraint forces all assumed unexecuted
actions in a plan to be executable in the future, where the predicate time now( ) is meant
to return the current time.
It is worth recalling that, in concrete situations, Pplan and Iplan will also contain domaindependent rules and constraints. Domain-dependent rules may be needed not only to define
initiates, terminates, initially and precondition, but they may also contain additional
rules/integrity constraints expressing ramifications, e.g.
holds at(f, T ) ⇒ holds at(f1 , T ) ∨ . . . ∨ holds at(fn , T )
for some specific fluents in the domain. Moreover, integrity constraints may represent
specific properties of actions and fluents in the domain. As an example, a domain-dependent
constraint could express that two actions of some type cannot be executed at the same time,
e.g.
holds at(tell(c, X, accept request(R), D), T ),
holds at(tell(c, X, ref use request(R), D), T ) ⇒ f alse
Intuitively, constructing a (partial) plan for a goal (that is a given leaf node in the
current forest) amounts to identifying actions and further sub-goals allowing to achieve the
goal, while assuming that all other nodes in the forest, both executable and non-executable,
are feasible. Concretely, the abductive logic program KBplan supports partial planning as
follows. Whenever a plan for a given goal requires the agent to execute an action, a[τ ] say,
the corresponding atom assume happens(a, τ ) is assumed, which amounts to intending
to execute the action (at some concrete time instantiating τ ). On the other hand, if a
plan for a given goal requires to plan for a sub-goal, `[τ ] say, the corresponding atom
assume holds(`, τ ) may be assumed, which amounts to setting the requirement that further
planning will be needed for the sub-goal itself. Notice that if only total plans are taken into
account, no atoms of the form assume holds( , ) will ever be generated.
308

Computational Logic Foundations of KGP Agents

now be KB
Formally, let KBplan
plan ∪ {time now(now)}, where now is a time constant (intuitively, the time when the planning capability is invoked). Then, the planning capability
11 .
|=now
plan is specified as follows

Let S = hKB0 , F, C, Σi be a state, and G = `[τ ] be a mental goal labeling a leaf node
in a tree T of F. Let also
CA = {assume happens(a, τ 0 ) | a[τ 0 ] ∈ nodes(F)},
CG = {assume holds(`0 , τ 0 ) | `0 [τ 0 ] ∈ nodes(F) \ {`[τ ]}}
and
• ∆0 = CA ∪ CG
• C0 = C ∪ Σ.
Then,
S, G |=now
plan (Xs , T C)
iff
Xs = {a[τ 0 ] | assume happens(a, τ 0 ) ∈ ∆} ∪ {`0 [τ 0 ] | assume holds(`0 , τ 0 ) ∈ ∆}
now , ∆ , C ). If
for some (∆, T C) which is an abductive answer for holds at(`, τ ), wrt (KBplan
0
0
no such abductive answer exists, then S, G |=now
⊥,
where
⊥
is
used
here
to
indicate
failure
plan
(i.e. that no such abductive answer exists).

As an example, consider setting 2.6.2 in Section 2.6. The domain-dependent part of
KBplan for agent psa (looking after the businessman in our scenario) contains
initiates(buy ticket online(F rom, T o), T, have ticket(F rom, T o))
precondition(buy ticket online(F rom, T o), available connection)
precondition(buy ticket online(F rom, T o), available destination(T o))
The goal G is have ticket(madrid, denver, τ ). Assume F only consists of a single tree
consisting solely of the root G, thus CA = CG = {}. Then, S, G |=now
plan (Xs , T C) where
Xs = {buy ticket online(madrid, denver, τ 0 ),
available connection(τ 00 ), available destination(denver, τ 000 )}
and T C = {τ 0 < τ, τ 0 = τ 00 = τ 000 , τ 0 > now}.
5.1.4 Reactivity
This capability supports the reasoning of reacting to stimuli from the external environment
as well as to decisions taken while planning.
As knowledge base KBreact supporting reactivity we adopt an extension of the knowledge
base KBplan as follows. KBreact = hPreact , Areact , Ireact i where
• Preact = Pplan
11. For simplicity we present the case of planning for single goals only.

309

Kakas, Mancarella, Sadri, Stathis & Toni

• Areact = Aplan
• Ireact = Iplan ∪ RR
where RR is a set of reactive constraints, of the form
Body ⇒ Reaction, T C
where
• Reaction is either assume holds(`, T ), `[T ] being a timed fluent literal, or
assume happens(a, T ), a[T ] being a timed action literal, 12 and
• Body is a non-empty conjunction of items of the form (where `[X] is a timed fluent
literal and a[X] is a timed action literal, for any X):
(i) observed(`, T 0 ),
(ii) observed(c, a[T 0 ], T 00 ),
(iii) executed(a, T 0 ),
(iv) holds at(`, T 0 ),
(v) assume holds(`, T 0 ),
(vi) happens(a, T 0 ),
(vii) assume happens(a, T 0 ),
(viii) temporal constraints on (some of) T, T 0 , T 00
which contains at least one item from one of (i), (ii) or (iii).
• T C are temporal constraints on (some of) T, T 0 , T 00 .
As for integrity constraints in abductive logic programming, all variables in Body are
implicitly universally quantified over the whole reactive constraint, and all variables in
Reaction, T C not occurring in Body are implicitly existentially quantified on the righthand
side of the reactive constraint. 13
Notice that Body must contain at least a trigger, i.e. a condition to be evaluated
in KB0 . Intuitively, a reactive constraint Body ⇒ Reaction, T C is to be interpreted as
follows: if (some instantiation of) all the observations in Body hold in KB0 and (some
corresponding instantiation of) all the remaining conditions in Body hold, then (the appropriate instantiation of) Reaction, with associated (the appropriate instantiation of) the
12. Here and below, with an abuse of notation, we use the notions of timed fluent and action literals liberally
and allow them to be non-ground, even though we have defined timed fluent and action literals as ground
except possibly for the time parameter.
13. Strictly speaking, syntactically reactive constraints are not integrity constraints (due to the presence of a
conjunction, represented by “,”, rather than a disjunction in the head). However, any reactive constraint
Body ⇒ Reaction, T C can be transformed into an integrity constraint Body ⇒ N ew with a new clause
N ew ← Reaction, T C in Preact . Thus, with an abuse of notation, we treat reactive constraints as
integrity constraints.

310

Computational Logic Foundations of KGP Agents

temporal constraints T C, should be added to F and C, respectively. Notice that Reaction
is an abducible so that no planning is performed by the reactivity capability.
now be the theory KB
Formally, let KBreact
react ∪ {time now(now)}, where now is a time
constant (intuitively, the time when the capability is invoked). Then, the reactivity capability |=now
react is specified as follows. Let S = hKB0 , F, C, Σi be a state. Let

CA = {assume happens(a, τ ) | a[τ ] ∈ nodesnr (F)},
CG = {assume holds(`, τ ) | `[τ ] ∈ nodesnr (F)}
and
• ∆0 = CA ∪ CG
• C0 = C ∪ Σ.
Then,
S |=now
react (Xs , T C)
iff
Xs = {a[τ ] | assume happens(a, τ ) ∈ ∆} ∪ {`[τ ] | assume holds(`, τ ) ∈ ∆}
now , ∆ , C ). If
for some (∆, T C) which is an abductive answer for the query true wrt (KBreact
0
0
now
no such abductive answer exists, then S |=react ⊥, where ⊥ is used here to indicate failure
(i.e. that no such abductive answer exists).

As an example, consider setting 2.6.1 in Section 2.6, and KBplan as given in Sections 5.1.1
and 5.1.3. Let RR of agent svs consist of:
observed(C, tell(C, svs, request(Q), D, T 0), T ), holds at(have inf o(svs, Q, I), T )
⇒ assume happens(tell(svs, C, inf orm(Q, I), D), T 0 ), T 0 > T
observed(C, tell(C, svs, request(Q), D, T 0), T ), holds at(no inf o(svs, Q), T )
⇒ assume happens(tell(svs, C, ref use(Q), D), T 0 ), T 0 > T
Then, given now = 30 and S = hKB0 , F, C, Σi with
KB0 = {observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d1, 15), 17),
observed(psa, tell(psa, svs, request(arrival(tr01)), d2, 20), 22)}
we obtain
S |=now
react ({tell(svs, psa, inf orm(arrival(tr01), 18), d2, τ )}, τ > 30).
5.1.5 Identification Of Preconditions
This capability is used by KGP agents to determine the preconditions for the executability
of actions which are planned for. These preconditions are defined in the domain-dependent
part of the EC by means of a set of rules of the form precondition(O, F ), representing that
the fluent F is a precondition for the executability of an action with action operator O (see
5.1.1). Let KBpre be the subset of KBT R containing the rules defining precondition( , ).
311

Kakas, Mancarella, Sadri, Stathis & Toni

Then the identification of preconditions capability |=pre is specified as follows. Given a state
S = hKB0 , F, C, Σi and a timed action literal a[τ ]
S, a[τ ] |=pre Cs
iff
Cs =

V

{`[τ ] | KBpre |=LP precondition(a, `)}14 .

5.1.6 Identification Of Effects
This capability is used by KGP agents to determine the effects of actions that have already
been executed, in order to check whether these actions have been successful. Note that
actions may have been unsuccessful because they could not be executed, or were executed
but they did not have the expected effect. Both are possible in situations where the agent
does not have full knowledge about the environment in which it is situated.
These effects are defined in the domain-dependent part of the EC by means of the set of
rules defining the predicates initiates and terminates. Let KBef f be the theory consisting
of the domain-dependent and domain-independent parts of the EC, as well as the narrative
part KB0 . Then, the identification of effects |=ef f is specified as follows. Given a state
S = hKB0 , F, C, Σi and an action operator a[t],
S, a[t] |=ef f `
iff
• ` = f and KBef f |=LP initiates(a, t, f )
• ` = ¬f and KBef f |=LP terminates(a, t, f )
5.2 Constraint Solving
The Constraint Solving capability can be simply defined in terms of the structure < and
the |=< notion presented in Section 3.1. Namely, given a state S = hKB0 , F, C, Σi and a
set of constraints T C:
• S |=cs T C iff |=< C ∧ Σ ∧ T C;
• there exists a total valuation σ such that S, σ |=cs T C iff there exists a total valuation
σ such that σ |=< C ∧ Σ ∧ T C.
5.3 Goal Decision
The Goal Decision reasoning capability allows the agent to decide, at a given time point,
the (non-reactive) top-level goals to be pursued, for which it will then go on to generate
plans aiming at achieving them. The generated goals are the goals of current preferred
interest but this interest may change over time.
14. We assume that

V

{} = true.

312

Computational Logic Foundations of KGP Agents

The Goal Decision capability operates according to a theory, KBGD , in which the agent
represents its goal preference policy. KBGD includes KBT R and thus the dynamic, observed
knowledge, KB0 , in the current state of the agent. KBGD is expressed in a variant of LPP
described in Section 3.3, whereby the rules in the lower or basic part P of the LPP theory
T have the form (T being a possibly empty sequence of variables):
n(τ, T ) : G[τ, T ] ← B[T ], C[T ]
where
• τ is a time variable, existentially quantified with scope the head of the rule and not a
member of T ;
• all variables except for τ are universally quantified with scope the rule;
• the head G[τ, T ] of the rule consists of a fluent literal conjoined with a (possibly
empty) set of temporal constraints, represented as h`[τ ], T C[τ, T ]i;
• B(T ) is a non-empty conjunction of literals on a set of auxiliary predicates that can
include atoms of the form holds at(`, T 0 ), where `[T 0 ] is a timed fluent literal, and the
atom time now(T 00 ) for some variables T 0 , T 00 ;
• the conditions of the rule are constrained by the (possibly empty) temporal constraints
C[T ].
Any such rule again represents all of its ground instances under any total valuation of
the variables in T that satisfies the constraints C[T ]. Each ground instance is named by the
corresponding ground instance of n(τ, T ). Intuitively, when the conditions of one such rule
are satisfied at a time now that grounds the variable T 00 with the current time at which the
capability is applied, then the goal in the head of the rule is sanctioned as one of the goals
that the agent would possibly prefer to achieve at this time. The decision whether such a
goal is indeed preferred would then depend on the high-level or strategy part H of KBGD ,
containing priority rules, as described in Section 3.3, between the rules in the lower-part or
between other rules in H. These priority rules can also include temporal atoms of the form
holds at(`, T 0 ) and the atom time now(T 00 ) in their conditions.
To accommodate this form of rules we only need to extend our notion of incompatibility
I in T to be defined on conclusions h`(τ ), T C[τ, T ]i. To simplify the notation, in the
remainder we often write h`(τ ), T Ci instead of h`(τ ), T C[τ, T ]i.
The incompatibility I can be defined in different ways. For example, a (relatively) weak
notion of incompatibility is given as follows. Two pairs h`1 (τ1 ), T C1 i and h`2 (τ2 ), T C2 i are
incompatible iff for every valuation σ such that T C1 and T C2 are both satisfied, the ground
instances of `1 (τ1 )σ and `2 (τ2 )σ are incompatible. A stronger notion would require that
it is sufficient for only one such valuation σ to exist that makes the corresponding ground
literals incompatible.
now the theory KB
Let us denote by KBGD
GD ∪ {time now(now)}, where now is a time
constant. Then, the goal decision capability, |=now
GD , is defined directly in terms of the
preference entailment, |=pr , of LPP (see Section 3.3), as follows.
Given a state S = hKB0 , F, C, Σi,
S |=now
GD Gs
313

Kakas, Mancarella, Sadri, Stathis & Toni

where
Gs = {G1 , G2 , . . . , Gn }, n ≥ 0, Gi = h`i (τi ), T Ci i for all i = 1, . . . , n
iff Gs is a maximal set such that
now
KBGD
|=pr h`1 (τ1 ), T C1 i ∧ . . . ∧ h`n (τn ), T Cn i.

This means that a new set of goals Gs is generated that is currently (sceptically) preferred
under the goal preference policy represented in KBGD and the current information in KB0 .
Note that any two goals in Gs are necessarily compatible with each other. There are two
special cases where there are no sceptically preferred goals at the time now. The first one
concerns the case where there are no goals that are currently sanctioned by the (lower-part)
of KBGD . When this is so |=now
GD returns an empty set of goals (n = 0). The second special
case occurs when there are at least two goals which are each separately credulously preferred
but these goals are incompatible which each other. Then S |=now
GD ⊥, where ⊥ is used to
indicate failure in identifying new goals to be pursued.
As an example, consider the San Vincenzo scenario where the psa agent needs to decide whether to return home or to recharge its battery. The agent’s goals are categorised
and assigned priority according to their category and possibly other factors. The KBGD
expressing this is given as follows:
• The low-level part contains the rules:

n(rh, τ1 ) : hreturn home(τ1 ), {τ1 < T 0 }i ←
holds at(f inished work, T ),
holds at(¬at home, T ),
time now(T ),
T0 = T + 6
n(rb, τ2 ) : hrecharge battery(τ2 ), {τ2 < T 0 }i ←
holds at(low battery, T ),
time now(T ),
T0 = T + 2
• The auxiliary part contains, in addition to KBT R and KB0 , the following rules that
specify the category of each goal and the relative urgency between these categories:

typeof (return home, required)
typeof (recharge battery, operational)
more urgent wrt type(operational, required)

314

Computational Logic Foundations of KGP Agents

• The incompatibility part consists of
incompatible(return home(T ), recharge battery(T ))
Namely, the two goals are pairwise incompatible, i.e. the agent can only do one of
these goals at a time.
• The high-level part contains the following priority rule:

gd pref (X, Y ) : n(X, ) ≺ n(Y, ) ← typeof (X, XT ),
typeof (Y, Y T ),
more urgent wrt type(XT, Y T ).
Then, for now = 1 and current state S = hKB0 , F, C, Σi such that finished work and
away from home both hold (by temporal reasoning) at time now, we have that
S |=now
GD {hreturn home(τ1 ), {τ1 < 7}i}.
Suppose instead that KB0 contains observed(low battery, 1). Then, using the weak
notion of incompatibility, requiring that
for every σ such that σ |=cs {τ1 < 7, τ2 < 3}
it holds that incompatible(return home(τ1 )σ, recharge battery(τ2 )σ)
we have:
S |=now
GD {hreturn home(τ1 ), {τ1 < 7}i, hrecharge battery(τ2 ), {τ2 < 3}i}.
Indeed, for σ = {τ1 = 3, τ2 = 2}, incompatible(return home(3), recharge battery(2)) does
not hold. However, using the stronger notion of incompatibility, requiring that
there exists σ such that σ |=cs {τ1 < 7, τ2 < 3}
it holds that incompatible(return home(τ1 )σ, recharge battery(τ2 )σ)
we have:
S |=now
GD {hrecharge battery(τ2 ), {τ2 < 3}i}.
Suppose now that KBGD contains a second operational goal hreplace part(τ3 ), {τ3 < 5}i
that is also sanctioned by a rule in its lower part at time now = 1. Then under the stronger
form of incompatibility the goal decision capability at now = 1 will return ⊥ as both these
operational goals are credulously preferred but none of them is sceptically preferred.
315

Kakas, Mancarella, Sadri, Stathis & Toni

6. Physical Capabilities
In addition to the reasoning capabilities we have defined so far, an agent is equipped with
physical capabilities that allow it to experience the world in which it is situated; this world
consists of other agents and/or objects that provide an environment for the agents in which
to interact and communicate.
We identify two types of physical capabilities: sensing and actuating. In representing
these capabilities we abstract away from the sensors and the actuators that an agent would
typically rely upon to access and affect the environment. We will also assume that these
sensors and actuators are part of the agent’s body, which we classify as an implementation
issue (Stathis et al., 2004).
The physical sensing capability models the way an agent interacts with its external
environment in order to inspect it, e.g. to find out whether or not some fluent holds at
a given time. On the other hand, the physical actuating capability models the way an
agent interacts with its external environment in order to affect it, by physically executing
its actions.
We represent the sensing physical capability of an agent as a function of the form:
sensing(L, t) = L0
where:
• L is a (possibly empty) set of
– fluent literals f ,
– terms of the form c : a (meaning that agent c has performed action a),
all to be sensed at a concrete time t, and
• L0 is a (possibly empty) set of elements s0 such that
– s0 is a term f : v, f being a fluent and v ∈ {true, f alse}, meaning that fluent f
has been observed to have value v (namely to be true or to be f alse) at time t,
or
– s0 is a term of the form c : a[t0 ], c being an agent name and a being an action,
meaning that agent c has performed action a at time t0 .
Note that physical sensing requires the time-stamp t to specify the time at which it is
applied within transitions. Note also that, given a non-empty set L, sensing(L, t) may be
partial, e.g. for some fluent f ∈ L, it can be that neither f : true ∈ L0 , nor f : f alse ∈ L0 .
Similarly, we represent the physical actuating capability as a function
actuating(As, t) = As0
where:
• As is a set of action literals {a1 , · · · , an }, n > 0, that the agent instructs the body to
actuate at time t;
316

Computational Logic Foundations of KGP Agents

• As0 ⊆ As is the subset of actions that the body has actually managed to perform.
The meaning of an action a belonging to As and not belonging to As0 is that the physical
actuators of the agent’s body were not able to perform a in the current situation. It is worth
pointing out that if an action a belongs to As0 it does not necessarily mean that the effects
of a have successfully been reached. Indeed, some of the preconditions of the executed
action (i) may have been wrongly believed by the agent to be true at execution time (as
other agents may have interfered with them) or (ii) the agent may have been unaware of
these preconditions. For example, after having confirmed availability, the agent may have
booked a hotel by sending an e-mail, but (i) some other agent has booked the last available
room in the meanwhile, or (ii) the agent did not provide a credit card number to secure the
booking. In other words, the beliefs of the agent (as held in KB0 ) may be incorrect and/or
incomplete.
In Section 7 and Section 8 below, we will see that AOI (Active Observation Introduction)
can be used to check effects of actions (identified by the fES effect selection operator, in
turn using the |=ef f reasoning capability) after actions have been executed. Moreover, SI
(Sensing Introduction) can be used to check preconditions of actions (identified by the fP S
precondition selection operator, in turn using the |=pre reasoning capability) just before they
are executed, to make sure that the actions are indeed executable. Overall, the following
cases may occur:
• an action belongs to As0 because it was executed and
– its preconditions held at the time of execution and its effects hold in the environment after execution;
– its preconditions were wrongly believed to hold at the time of execution (because
the agent has partial knowledge of the environment or its KBplan is incorrect)
and as a consequence its effects do not hold after execution;
– its preconditions were known not to hold at the time of execution (e.g. because
the agent observed only after having planned that they did not hold, but had no
time to -replan) and as a consequence its effects do not hold after execution;
• an action belongs to As \ As0 because it was not executed (the body could not execute
it).
The actuating physical capability does not check preconditions/effects: this is left
to other capabilities called within transitions before and/or after the transition invoking
actuating, as we will show below. As before, the way the body will carry out the actions
is an implementation issue (Stathis et al., 2004).

7. Transitions
The KGP model relies upon the state transitions GI, PI, RE, SI, POI, AOI, AE, SR, defined
below using the following representation
(T)

hKB0 , F, C, Σi
X
now
0
0
0
0
hKB0 , F , C , Σ i
317

Kakas, Mancarella, Sadri, Stathis & Toni

where T is the name of the transition, hKB0 , F, C, Σi is the agent’s state before the transition is applied, X is the input for the transition, now is the time of application of the
transition, hKB00 , F 0 , C 0 , Σ0 i is the revised state, resulting from the application of the transition T with input X at time now in state hKB0 , F, C, Σi. Please note that most transitions
only modify some of the components of the state. Also, for some transitions (namely GI,
RE, POI, SR) the input X is always empty and will be omitted. For the other transitions
(namely PI, SI, AOI, AE) the input is always non-empty (see Section 9) and is selected by
an appropriate selection operator (see Section 8).
Below we define each transition formally, by defining hKB00 , F 0 , C 0 , Σ0 i. Note that we
assume that each transition takes care of possible renaming of time variables in the output
of capabilities (if a capability is used by the transition), in order to guarantee that each
goal/action in the forest is univocally identified by its time variable.
7.1 Goal Introduction
This transition takes empty input. It calls the Goal Decision capability to determine the
new (non-reactive) top-level goals of the agent. If this capability returns a set of goals,
this means that the circumstances have now possibly changed the preferred top-level goals
of the agent and the transition will reflect this by changing the forest in the new state to
consist of one tree for each new (non-reactive) goal. On the other hand, if the Goal Decision
capability does not return any (non-reactive) goals (namely it returns ⊥) the state is left
unchanged, as, although the goals in the current state are no longer sceptically preferred
they may still be credulously preferred and, since there are no others to replace them, the
agent will carry on with its current plans to achieve them.
(GI)

hKB0 , F, C, Σi
now
hKB0 , F 0 , C 0 , Σi

where, given that S = hKB0 , F, C, Σi
(i) If S |=now
GD ⊥, then
– F0 = F
– C0 = C
(ii) otherwise, if S |=now
GD Gs and Gs 6= ⊥, then
– F 0 is defined as follows:
∗ nr(F 0 ) = {Tg[τ ] | hg[τ ], i ∈ Gs} where Tg[τ ] is a tree consisting solely of the
root g[τ ]
∗ r(F 0 ) = {}
– C 0 = {T C | h , T Ci ∈ Gs}
This transition drops (top-level) goals that have become “semantically” irrelevant (due
to changed circumstances of the agent or changes in its environment), and replaces them
with new relevant goals. We will see, in Section 7.8, that goals can also be dropped because
318

Computational Logic Foundations of KGP Agents

of the book-keeping activities of the State Revision (SR) transition, but that transition can
never add to the set of goals.
Note that, as GI will replace the whole forest in the old state by a new forest, it is
possible that the agent looses valuable information that it has in achieving its goals, when
one of the new preferred goals of the agent is the same as (or equivalent to) a current goal.
This effect though can be minimized by calling (in the cycle theory) the GI transition only
at certain times, e.g. after the current goals have been achieved or timed-out. Alternatively,
the earlier formalisation of the GI transition could be modified so that, in case (ii), for all
goals in Gs that already occur (modulo their temporal variables and associated temporal
constraints) as roots of (non-reactive) trees in F, these trees are kept in F 0 . A simple way
to characterise (some of) these goals is as follows. Let

Xs = {hg[τ ], T C, τ = τ 0 i |

hg[τ ], T Ci ∈ Gs,
g[τ 0 ] ∈ Rootsnr (F) and
|=cs C iff |=cs (C ∪ T C ∪ {τ = τ 0 })}

Gs0 = {hg[τ ], T Ci |

hg[τ ], T C, τ = τ 0 i ∈ Xs}

The new constraints on goals in Gs0 are equivalent to the old constraints in C. For example,
Gs may contain
G = hhave ticket(madrid, denver, τ2 ), {τ2 < 12}i
with have ticket(madrid, denver, τ1 ) ∈ Rootsnr (F) and C = {τ1 < 12}.
Then, G definitely belongs to Gs0 . Let
newC =

[

T C ∪ {τ = τ 0 }.

h ,T C,τ =τ 0 i∈Xs

Case (ii) can be redefined as follows, using these definitions of Xs, Gs0 and newC:
(ii0 ) otherwise, if S |=now
GD Gs and Gs 6= ⊥, then, if it is not the case that |=cs C ∪ newC,
then F 0 and C 0 are defined as in the earlier case (ii), otherwise (if |=cs C ∪ newC):
– F 0 is defined as follows:
∗ nr(F 0 ) = {Tg[τ ] | hg[τ ], i ∈ Gs \ Gs0 } ∪ F(Xs)
where Tg[τ ] is a tree consisting solely of the root g[τ ] and
F(Xs) is the set of all trees in F with roots goals of the form g[τ 0 ] such that
hg[τ ], , τ = τ 0 i ∈ Xs
∗ r(F 0 ) = {}
– C 0 = C ∪ {T C | h , T Ci ∈ Gs \ Gs0 } ∪ newC.
Note that we keep all temporal constraints in the state, prior to the application of GI,
but we force all variables of new goals that remain in the state after GI to be rewritten
using the old identifiers of the goals.
319

Kakas, Mancarella, Sadri, Stathis & Toni

7.2 Reactivity
This transition takes empty input. It calls the Reactivity capability in order to determine the
new top-level reactive goals in the state (if any), leaving the non-reactive part unchanged.
If no new reactive goals exist, the reactive part of the new state will be empty.
hKB0 , F, C, Σi
now
hKB0 , F 0 , C 0 , Σi

(RE)
where, given that S = hKB0 , F, C, Σi:
(i) If S |=now
react ⊥, then
• F 0 is defined as follows:
– r(F 0 ) = {}
– nr(F 0 ) = nr(F)
• C0 = C

(ii) otherwise, if S |=now
react (X s, T C), then
• F 0 is defined as follows:
– nr(F 0 ) = nr(F)
– r(F 0 ) = {Tx[τ ] | x[τ ] ∈ X s}
where Tx[τ ] is a tree consisting solely of the root x[τ ]
• C0 = C ∪ T C
Note that there is an asymmetry between case (ii) of GI and case (ii) of RE, as GI
eliminates all reactive goals in this case, whereas RE leaves all non-reactive goals unchanged.
Indeed, reactive goals may be due to the choice of specific non-reactive goals, so when the
latter change the former need to be re-evaluated. Instead, non-reactive goals are not affected
by newly acquired reactive goals (that are the outcome of enforcing reactive rules).
Note also that in case (ii), similarly to GI, as RE replaces the whole (reactive) forest
in the old state by a new (reactive) forest, it is possible that the agent loses valuable
information that it has in achieving its reactive goals, when one of the new reactive goals
is the same as (or equivalent to) a current goal. A variant of case (ii) for RE, mirroring the
variant given earlier for GI and using |=cs as well, can be defined to avoid this problem.
7.3 Plan Introduction
This transition takes as input a non-executable goal in the state (that has been selected by
the goal selection operator, see Section 8) and produces a new state by calling the agent’s
Planning capability, if the selected goal is a mental goal, or by simply introducing a new
sensing action, if the goal is a sensing goal.
(PI)

hKB0 , F, C, Σi
G
now
0
0
hKB0 , F , C , Σi

where G is the input goal (selected for planning in some tree T in F, and thus a leaf, see
Section 8) and
320

Computational Logic Foundations of KGP Agents

F 0 = (F \ {T | G is a leaf in T }) ∪ N ew
C0 = C ∪ T C
where N ew and T C are obtained as follows, S being hKB0 , F, C, Σi.
(i) if G is a mental goal: let S, G |=now
plan P . Then,
– either P = ⊥ and
N ew = {T } and T C = {},
– or P = (X s, T C) and
N ew = {T 0 } where T 0 is obtained from T by adding each element of X s as a
child of G.
(ii) if G = `[τ ] is a sensing goal, and a child of a goal G0 in T :
N ew = {T 0 } where T 0 is T with (a node labelled by) sense(`, τ 0 ) as a new child of G0
(here τ 0 is a new time variable) and
T C = {τ 0 ≤ τ }.
(iii) if G = `[τ ] is a sensing goal, and the root of T :
N ew = {T , T 0 } where T 0 is a tree consisting solely of the root (labelled by) sense(`, τ 0 )
(here τ 0 is a new time variable) and
T C = {τ 0 ≤ τ }.
7.4 Sensing Introduction
This transition takes as input a set of fluent literals that are preconditions of some actions
in the state and produces a new state by adding sensing actions as leaves in (appropriate)
trees in its forest component. Note that, when SI is invoked, these input fluent literals are
selected by the precondition selection operator, and are chosen amongst preconditions of
actions that are not already known to be true (see Section 8).
(SI)

hKB0 , F, C, Σi
SP s
now
0
0
hKB0 , F , C , Σi

with SP s a non-empty set of preconditions of actions (in the form of pairs “precondition,
action”) in some trees in F, where, given that:
- N ew = {h`[τ ], A, sense(`, τ 0 )i | h`[τ ], Ai ∈ SP s and τ 0 is a fresh variable}
- addSibling(T , A, SA) denotes the tree obtained by adding all elements in SA as new
siblings of A to the tree T such that leaf (A, T )
then
F 0 = F \ {T | leaf (A, T ) and h`[τ ], Ai ∈ SP s}
∪ {addSibling(T , A, SA) | leaf (A, T ) and
SA = {sense(`, τ 0 )|h`[τ ], A, sense(`[τ 0 ])i ∈ N ew}}
C 0 = C ∪ {τ 0 < τ | h`[τ ], , sense(`[τ 0 ])i ∈ N ew}
321

Kakas, Mancarella, Sadri, Stathis & Toni

Basically, for each fluent literal selected by the precondition selection operator as a
precondition of an action A, a new sensing action is added as a sibling of A, and the
constraint expressing that this sensing action must be performed before A is added to the
current set of temporal constraints.
7.5 Passive Observation Introduction
This transition updates KB0 by adding new observed facts reflecting changes in the environment. These observations are not deliberately made by the agent, rather, they are
“forced” upon the agent by the environment. These observations may be properties in the
form of positive or negative fluents (for example that the battery is running out) or actions
performed by other agents (for example messages addressed to the agent).
hKB0 , F, C, Σi
now
hKB00 , F, C, Σi

(POI)
where, if sensing(∅, now) = L, then
KB00 = KB0 ∪

{observed(f, now) | f : true ∈ L} ∪
{observed(¬f, now) | f : f alse ∈ L} ∪
{observed(c, a[t], now) | c : a[t] ∈ L}.
7.6 Active Observation Introduction
This transition updates KB0 by adding new facts deliberately observed by the agent, which
seeks to establish whether or not some given fluents hold at a given time. These fluents are
selected by the effect selection operator (see Section 8) and given as input to the transition.
Whereas POI is not “decided” by the agent (the agent is “interrupted” and forced an
observation by the environment), AOI is deliberate. Moreover, POI may observe fluents
and actions, whereas AOI only considers fluents (that are effects of actions executed by the
agent, as we will see in Section 8 and in Section 9).
(AOI)

hKB0 , F, C, Σi
SF s
now
0
hKB0 , F, C, Σi

where SF s = {f1 , . . . , fn }, n > 0, is a set of fluents selected for being actively sensed (by
the effect selection operator), and, if sensing(SF s, now) = L, then
KB00 = KB0 ∪
{observed(f, now) | f : true ∈ L} ∪
{observed(¬f, now) | f : f alse ∈ L}.
7.7 Action Execution
This transition updates KB0 , recording the execution of actions by the agent. The actions
to be executed are selected by the action selection operator (see Section 8) prior to the
transition, and given as input to the transition.
322

Computational Logic Foundations of KGP Agents

(AE)

hKB0 , F, C, Σi
SAs
now
hKB00 , F, C, Σ0 i

where SAs is a non-empty set of actions selected for execution (by the action selection
operator), and
• let A be the subset of all non-sensing actions in SAs and S be the subset of all sensing
actions in SAs;
• let sensing(S 0 , now) = L0 , where S 0 = {f | sense(f, τ ) ∈ S}
• let sensing(S 00 , now) = L00 , where S 00 = {c : a | sense(c : a, τ ) ∈ S}
• let actuating(A0 , now) = A00 , where A0 = {a | a[τ ] ∈ A}.
Then:
KB00 = KB0 ∪
{executed(a, now) | a ∈ A00 } ∪
{observed(f, now) | f : true ∈ L0 } ∪
{observed(¬f, now) | f : f alse ∈ L0 }
{observed(c, a[t], now) | c : a[t] ∈ L00 and
∃σ such that σ |=cs C ∧ τ = t where sense(c : a, τ ) ∈ S}
and
Σ0 = Σ ∪ {τ = now | a[τ ] ∈ SAs ∧ a ∈ A00 } ∪
{τ = now | sense(f, τ ) ∈ SAs ∧ (f : ) ∈ L0 } ∪
{τ = t | c : a[t] ∈ L00 and ∃σ such that σ |=cs C ∧ τ = t where sense(c : a, τ ) ∈ S}.
7.8 State Revision
The SR transition revises a state by removing all timed-out goals and actions and all goals
and actions that have become obsolete because one of their ancestors is already believed to
have been achieved. We will make use of the following terminology.
Notation 7.1 Given a state S, a timed fluent literal `[τ ], a timed fluent literal or action
operator x[τ ], and a time-point now:
• achieved(S, `[τ ], now) stands for
there exists a total valuation σ such that S, σ |=cs τ ≤ now and S |=T R `[τ ]σ
• timed out(S, x[τ ], now) stands for
there exists no total valuation σ such that S, σ |=cs τ > now.
323

Kakas, Mancarella, Sadri, Stathis & Toni

Then, the specification of the transition is as follows.
(SR)

hKB0 , F, C, Σi
now
hKB0 , F 0 , C, Σi

where F 0 is the set of all trees in F pruned so that nodes(F 0 ) is the biggest subset of
nodes(F) consisting of all goals/actions x[τ ] in some tree T in F such that (here S =
hKB0 , F, C, Σi):
(i) ¬timed out(S, x[τ ], now), and
(ii) if x is an action operator, it is not the case that executed(x, t) ∈ KB0 and (τ = t) ∈ Σ,
and
(iii) if x is a fluent literal, ¬achieved(S, x[τ ], now), and
(iv) for every y[τ 0 ] ∈ siblings(x[τ ], F)
– either y[τ 0 ] ∈ siblings(x[τ ], F 0 ),
– or y[τ 0 ] 6∈ siblings(x[τ ], F 0 ) and
∗ if y is a fluent literal then achieved(S, y[τ 0 ], now),
∗ if y is an action literal then executed(y, t) ∈ KB0 and τ 0 = t ∈ Σ,
and
(v) if x is a sensing action operator, x[τ ] = sense(`, τ ), then
– either there exists a[τ 0 ] ∈ siblings(x[τ ], F 0 ) such that ` is a precondition of a (i.e.
S, a[τ 0 ] |=pre Cs and `[τ 0 ] ∈ Cs) and τ < τ 0 ∈ C,
– or there exists `[τ 0 ] ∈ siblings(x[τ ], F 0 ) such that ` is a sensing fluent and τ <
τ 0 ∈ C, and
(vi) x[τ ] is a top-level goal or parent(x[τ ], F) = P and P ∈ nodes(F 0 ).
All conditions above specify what SR keeps in the trees in the forest in the state. Intuitively, these conditions may be understood in terms of what they prevent from remaining
in such trees:
• condition (i) removes timed-out goals and actions,
• condition (ii) removes actions that have already been executed,
• condition (iii) removes goals that are already achieved,
• condition (iv) removes goals and actions whose siblings are already timed out and
thus deleted, by condition (i),
• condition (v) removes sensing actions for preconditions of actions that have been
deleted and for sensing goals that have been deleted,
• condition (vi) recursively removes actions and goals whose ancestors have been removed.
The following example illustrates how SR is used to provide adjustment of the agent’s
goals and plans in the light of newly acquired information.
324

Computational Logic Foundations of KGP Agents

7.9 Setting 3
The agent psa has the goal to have a museum ticket for some (state-run) museum that the
businessman wants to visit, and a plan to buy the ticket. But before executing the plan psa
observes that it is the European Heritage day (ehd for short), via an appropriate “message”
from another agent mus (representing the museum), stating that all state-run museums in
Europe give out free tickets to anybody walking in on that day. Then, the psa’s goal is
already achieved and both goal and plan are deleted from its state.
Let the agent’s initial state be hKB0 , F, C, Σi with:
Σ = { } = KB0
F

= {T }

C = {τ1 ≤ 10, τ2 = τ3 , τ3 < τ1 }
where T consists of a top-level goal g1 = have(ticket, τ1 ), with two children,
g2 = have money(τ2 ) and a1 = buy(ticket, τ3 ),

15

and further assuming that KBT R contains
initiates(ehd, T, have(ticket))
initiates(buy(O), T, have(O))
precondition(buy(O), have money).
The remaining knowledge bases do not play any useful role for the purposes of this
example, and can therefore be considered to be empty. The “message” from the museum
agent mus is added to KB0 via POI, e.g. at time 6, in the following form:
observed(mus, ehd(5), 6)
i.e. at time 6 it is observed that at time 5 mus has announced that all state-run museums
in Europe are free on that day. Then, via SR, at time 8 say, g1 , g2 and a1 are eliminated
from F, as g1 is already achieved.

8. Selection Operators
The KGP model relies upon selection operators:
• fGS (goal selection, used to provide input to the PI transition);
• fP S (precondition selection, used to provide input to the SI transition);
• fES (effect selection, used to provide input to the AOI transition);
• fAS (action selection, used to provide input to the AE transition).
15. g1 and a1 can be reactive or not, as this does not matter for this example.

325

Kakas, Mancarella, Sadri, Stathis & Toni

Selection operators are defined in terms of (some of the) capabilities (namely Temporal
Reasoning, Identification of Preconditions and Effects and Constraint Solving).
At a high-level of description, the selection operators can all be seen as returning the set
of all items from a given initial set that satisfy a certain number of conditions. For example,
given a state hKB0 , F, C, Σi, the goal selection operator returns the set of all non-executable
goals in trees in F that satisfy some conditions; the precondition selection operator returns
the set of all pairs, each consisting of (i) a timed fluent literal which is a precondition of
some action in some tree in F and (ii) that action, satisfying some conditions; the effect
selection operator returns the set of all fluent literals which are effects of actions already
executed (as recorded in KB0 ) that satisfy some conditions; the action selection operator
returns the set of all actions in trees in F that satisfy some conditions.
The selection operators are formally defined below.
8.1 Goal Selection
Informally, the set of conditions for the goal selection operator is as follows. Given a state
S = hKB0 , F, C, Σi and a time-point t, the set of goals selected by fGS is a singleton set
consisting of a non-executable goal G in some tree in F such that at time t:
1. G is not timed out,
2. no ancestor of G is timed out,
3. no child of any ancestor of G is timed out,
4. neither G, nor any ancestor of G in any tree in F is already achieved.
5. G is a leaf
Intuitively, condition 1 ensures that G is not already timed-out, conditions 2-3 impose
that G belongs to a “still feasible” plan for some top-level goal in F, and condition 4 makes
sure that considering G is not wasteful.
Note that, as already mentioned in Section 5.1.3, for simplicity we select a single goal.
Formally, given a state S = hKB0 , F, C, Σi and a time-point t, let G(S, t) be the set of all
non-executable goals `[τ ] ∈ nodes(F) such that:
1. ¬timed out(S, `[τ ], t)
2. ¬timed out(S, G, t) for each G ∈ ancestors(`[τ ], F),
3. ¬timed out(S, X, t) for each X ∈ nodes(F) such that X is the child of some P ∈
ancestors(`[τ ], F)
4. ¬achieved(S, G, t) for each G ∈ {`[τ ]} ∪ ancestors(`[τ ], F)
5. leaf (G, F)
Then, if G(S, t) 6= {}:
fGS (S, t) = {G} for some G ∈ G(S, t).
Otherwise, fGS (S, t) = {}.
326

Computational Logic Foundations of KGP Agents

8.2 Effect Selection
Informally, the set of conditions for the effect selection operator is as follows. Given a state
S = hKB0 , F, C, Σi and a time-point t, fES selects all fluents f such that f or ¬f is one of
the effects of some action a[τ ] that has “recently” been executed.
Note that such f (or ¬f ) may not occur in F but could be some other (observable)
effect of the executed action, which is not necessarily the same as the goal that the action
contributes to achieving. For example, in order to check whether an internet connection is
available, the agent may want to observe that it can access a skype network even though
it is really interested in opening a browser (as it needs a browser in order to perform a
booking online).
Formally, given a state S = hKB0 , F, C, Σi and a time-point now, the set of all (timed)
fluents selected by fES is the set of all (timed) fluents f [τ ] such that there is an action
operator a with
1. executed(a, t0 ) ∈ KB0 , t0 = τ ∈ Σ and now − ² < t0 < now, where ² is a sufficiently
small number (that is left as a parameter here), and
2. S, a[τ ] |=ef f `, where ` = f or ` = ¬f .
8.3 Action Selection
Informally, the set of conditions for the action selection operator is as follows. Given a state
S = hKB0 , F, C, Σi and a time-point t, the set of all actions selected by fAS is defined as
follows. Let X (S, t) be the set of all actions A in trees in F such that:
1. A can be executed,
2. no ancestor of A is timed out,
3. no child of any ancestor of A is timed out,
4. no ancestor of A is already satisfied,
5. no precondition of A is known to be false,
6. A has not already been executed.
Then fAS (S, t) ⊆ X (S, t) such that all actions in fAS (S, t) are executable concurrently
at t.
Intuitively, conditions 2-4 impose that A belongs to a “still feasible” plan for some toplevel goals in F. Note that condition 1 in the definition of X (S, t) is logically redundant,
as it is also re-imposed by definition of fAS (S, t). However, this condition serves as a first
filter and is thus useful in practice.
Formally, given a state S = hKB0 , F, C, Σi, and a time-point t, the set of all actions
selected by fAS is defined as follows. Let X (S, t) be the set of all actions a[τ ] occurring as
leaves of some trees in F such that:
327

Kakas, Mancarella, Sadri, Stathis & Toni

1. there exists a total valuation σ such that S, σ |=cs τ = t, and
2. ¬timed out(S, G, t) for each G ∈ ancestors(a[τ ], F), and
3. ¬timed out(S, X, t) for each X ∈ children(G, F) and G ∈ ancestors(a[τ ], F), and
4. ¬achieved(S, G, t) for each G ∈ ancestors(a[τ ], F), and
5. let S, a[τ ] |=pre Cs and Cs = `1 [τ ] ∧ . . . ∧ `n [τ ];
if n > 0, then for no i = 1, . . . , n there exists a total valuation σ such that S, σ |=cs
τ = t and S |=T R `i [τ ]σ, and
6. there exists no t0 such that τ = t0 ∈ Σ and executed(a, t0 ) ∈ KB0 .
The formalisation of condition 6 allows for other instances of action A to have been
executed. Then,
fAS (S, t) = {a1 [τ1 ], . . . , am [τm ]} ⊆ X (S, t)
(where m ≥ 0), such that there exists a total valuation σ for the variables in C such that
S, σ |=cs τ1 = t ∧ . . . ∧ τm = t.
Note that the definition of the action selection operator can be extended to take into
account a notion of urgency with respect to the temporal constraints. However, such an
extension is beyond the scope of this work.
8.4 Precondition Selection
Informally, the set of conditions for the precondition selection operator is as follows. Given
a state S = hKB0 , F, C, Σi and a time-point t, the set of preconditions (of actions in
F) selected by fP S is the set of all pairs hC, Ai of (timed) preconditions C and actions
A ∈ nodes(F) such that:
1. C is a precondition of A and
2. C is not known to be true in S at t, and
3. A is one of the actions that could be selected for execution if fAS would be called at
the current time.
The reason why this selection operator returns pairs, rather then simply preconditions,
is that the transition SI, which makes use of the outputs of this selection operator, needs to
know the actions associated with the preconditions. This is because SI introduces sensing
actions for each precondition returned and has to place these sensing actions as siblings of
the associated actions in F, as seen in Section 7.4.
Formally, given a state S = hKB0 , F, C, Σi and a time-point t, the set of all preconditions
of actions selected by fP S is the set of all pairs hC, Ai of (timed) preconditions C and actions
A ∈ nodes(F) such that:
1. A = a[τ ], and S, a[τ ] |=pre Cs and C is a conjunct in Cs, and
328

Computational Logic Foundations of KGP Agents

2. there exists no total valuation σ for the variables in C such that S, σ |=cs τ = t and
S |=T R Cσ, and
3. A ∈ X (S, t), where X (S, t) is as defined in Section 8.3.

9. Cycle Theory
The behaviour of KGP agents results from the application of transitions in sequences,
repeatedly changing the state of the agent. These sequences are not fixed a priori, as
in conventional agent architectures, but are determined dynamically by reasoning with
declarative cycle theories, giving a form of flexible control. Cycle theories are given in the
framework of Logic Programming with Priorities (LPP) as discussed in Section 3.
9.1 Formalisation of Cycle Theories.
Here we use the following new notations:
• T (S, X, S 0 , t) to represent the application of transition T at time t in state S given
input X and resulting in state S 0 , and
• ∗T (S, X) to represent that transition T can potentially be chosen as the next transition
in state S, with input X.
Recall that, for some of the transitions, X may be the empty set {}, as indicated in Section 7.
Formally, a cycle theory Tcycle consists of the following parts.
• An initial part Tinitial , that determines the possible transitions that the agent could
perform when it starts to operate. Concretely, Tinitial consists of rules of the form
∗T (S0 , X) ← C(S0 , X)
which we refer to via the name R0|T (S0 , X). These rules sanction that, if conditions
C hold in the initial state S0 then the initial transition could be T , applied to state
S0 and input X. For example, the rule
R0|GI (S0 , {}) : ∗GI(S0 , {}) ← empty f orest(S0 )
sanctions that the initial transition should be GI, if the forest in the initial state S0
is empty.
Note that C(S0 , X) may be empty, and, if non-empty, C(S0 , X) may refer to the
current time via a condition time now(t). For example, the rule
R0|P I (S0 , G) : ∗P I(S0 , G) ← Gs = fGS (S0 , t), Gs 6= {}, G ∈ Gs, time now(t)
sanctions that the initial transition should be PI, if the forest in the initial state S0
contains some goal that can be planned for at the current time (in that the goal
selection operator picks that goal).
• A basic part Tbasic that determines the possible transitions following given transitions,
and consists of rules of the form
∗T 0 (S 0 , X 0 ) ← T (S, X, S 0 , t), EC(S 0 , X 0 )
329

Kakas, Mancarella, Sadri, Stathis & Toni

which we refer to via the name RT |T 0 (S 0 , X 0 ). These rules sanction that, after transition T has been executed, starting at time t in the state S and resulting in state S 0 ,
and the conditions EC evaluated in S 0 are satisfied, then transition T 0 could be the
next transition to be applied in S 0 , with input X 0 .16 EC are enabling conditions as
they determine when T 0 can be applied after T . They also determine input X 0 for T 0 ,
via calls to selection operators. As for the initial part of Tcycle , EC may be empty
and, if not, may refer to the current time. For example, the rule
RAE|P I (S 0 , G) : ∗P I(S 0 , G) ← AE(S, As, S 0 , t),
Gs = fGS (S 0 , t0 ), Gs 6= {}, G ∈ Gs, time now(t0 )
sanctions that PI should follow AE if at the current time there is some goal in the
current state that is selected by the goal selection function.
• A behaviour part Tbehaviour that contains rules describing dynamic priorities amongst
rules in Tbasic and Tinitial . Rules in Tbehaviour are of the form
RT |T 0 (S, X 0 ) Â RT |T 00 (S, X 00 ) ← BC(S, X 0 , X 00 )
with T 0 6= T 00 , which we will refer to via the name PTT0 ÂT 00 . Recall that RT |T 0 (·) and
RT |T 00 (·) are (names of) rules in Tbasic ∪ Tinitial . Note that, with an abuse of notation,
T could be 0 in the case that one such rule is used to specify a priority over the first
transition to take place, in other words, when the priority is over rules in Tinitial .
These rules in Tbehaviour sanction that, after transition T , if the conditions BC hold,
then we prefer the next transition to be T 0 over T 00 . The conditions BC are behaviour
conditions as they give the behavioural profile of the agent. For example, the rule
T
PGIÂT
0 : RT |GI (S, {}) Â RT |T 0 (S, X) ← empty f orest(S)

sanctions that GI should be preferred to any other transition after any transition that
results into a state with an empty forest. As for the other components of Tcycle , the
conditions BC may refer to the current time.
• An auxiliary part including definitions for any predicates occurring in the enabling
and behaviour conditions.
• An incompatibility part, in effect expressing that only one (instance of a) transition
can be chosen at any one time.
Hence, Tcycle is an LPP-theory where: (i) P = Tinitial ∪ Tbasic , and (ii) H = Tbehaviour .
9.2 Operational Trace
The cycle theory Tcycle of an agent is responsible for its behaviour, in that it induces an
operational trace of the agent, namely a (typically infinite) sequence of transitions
T1 (S0 , X1 , S1 , t1 ), . . . , Ti (Si−1 , Xi , Si , ti ), Ti+1 (Si , Xi+1 , Si+1 , ti+1 ), . . .
such that
16. Note that in order to determine that T 0 is a possible transition after T , with a rule of the earlier form,
one only needs to know that T has been applied and resulted into the state S 0 . This is conveyed by the
choice of name: RT |T 0 (S 0 , X 0 ). In other words, by using a Prolog notation, we could have represented
the rule as ∗T 0 (S 0 , X 0 ) ← T ( , , S 0 , ), EC(S 0 , X 0 ). Thus, the rule is “Markovian”.

330

Computational Logic Foundations of KGP Agents

• S0 is the given initial state;
• for each i ≥ 1, ti is given by the clock of the system (ti < ti+i );
• (Tcycle − Tbasic ) ∪ {time now(t1 )} |=pr ∗T1 (S0 , X1 );
• for each i ≥ 1
(Tcycle − Tinitial ) ∪ {Ti (Si−1 , Xi , Si , ti ), time now(ti+1 )} |=pr ∗Ti+1 (Si , Xi+1 )
namely each (non-final) transition in a sequence is followed by the most preferred transition,
as specified by Tcycle . If, at some stage, the most preferred transition determined by |=pr is
not unique, we choose one arbitrarily.
9.3 Normal Cycle Theory
The normal cycle theory is a concrete example of cycle theory, specifying a pattern of
operation where the agent prefers to follow a sequence of transitions that allows it to achieve
its goals in a way that matches an expected “normal” behaviour. Other examples of possible
cycle theories can be found in the literature (Kakas, Mancarella, Sadri, Stathis, & Toni,
2005; Sadri & Toni, 2006).
Basically, the “normal” agent first introduces goals (if it has none to start with) via GI,
then reacts to them, via RE, and then repeats the process of planning for them, via PI,
executing (part of) the chosen plans, via AE, revising its state, via SR, until all goals are
dealt with (successfully or revised away). At this point the agent returns to introducing
new goals via GI and repeating the above process. Whenever in this process the agent
is interrupted via a passive observation, via POI, it chooses to introduce new goals via
GI, to take into account any changes in the environment. Whenever it has actions which
are “unreliable”, in the sense that their preconditions definitely need to be checked, the
agent senses them (via SI) before executing the action. Whenever it has actions which are
“unreliable”, in the sense that their effects definitely need to be checked, the agent actively
introduces actions that aim at sensing these effects, via AOI, after having executed the
original actions. If initially the agent is equipped with some goals, then it would plan for
them straightaway by PI.
The full definition of the normal cycle theory is given in the appendix. This is used to
provide the control in the examples of the next section. Here, note that, although the normal
cycle theory is based on the classic observe-plan-act cycle of agent control, it generalises
this in several ways giving more flexibility on the agent behaviour to adapt to a changing
environment. For example, the goals of the agent need not be fixed but can be dynamically
changed depending on newly acquired information. Let us illustrates this feature with a
brief example here. Suppose that the current state of our agent contains the top-level nonreactive goal hreturn home(τ1 ), {τ1 < 7}i and that a POI occurs which adds an observation
observed(low battery, 2) at time 2. A subsequent GI transition generated by the normal
cycle theory introduces a new goal hrecharge battery(τ2 ), {τ2 < 3}i which, depending on
the details of KBGD , either replaces the previous goal or adds this as an additional goal.
The normal cycle theory will next choose to do a PI transition for the new and more urgent
goal of recharging its battery.
331

Kakas, Mancarella, Sadri, Stathis & Toni

10. Examples
In this section we revisit the examples introduced in Section 2.6 and used throughout the
paper to illustrate the various components of the KGP model. Overall, the aim here is
to illustrate the interplay of the transitions, and how this interplay provides the variety of
behaviours afforded by the KGP model, including reaction to observations, generation and
execution of conditional plans, and dynamic adjustment of goals and plans.
Unless specified differently, we will assume that Tcycle will be the normal cycle theory
presented in Section 9.3. We will provide any domain-dependent definition in the auxiliary
part of Tcycle explicitly, where required.
10.1 Setting 1 Formalised
We formalise here the initial state, knowledge bases and behaviour of svs for Setting 1
described in Section 2.6.1.
10.1.1 Initial State
For simplicity, the observations, goals and the plan of svs can be assumed to be empty
initially. More concretely let the (initial) state of svs be
KB0 = { }
F

= {}

C = {}
Σ = {}

10.1.2 Knowledge Bases
Following Section 5.1.4, we formulate the reactivity knowledge base for agent svs in terms
of the utterances query ref, ref use, inf orm inspired by the FIPA specifications for communicative acts (FIPA, 2001a, 2001b). However, although we use the same names of
communicative acts as in the FIPA specification, we do not adopt here their “mentalistic”
svs is formulated
semantic interpretation in terms of pre- and post-conditions. Thus, KBreact
as:
observed(C, tell(C, svs, query ref (Q), D, T 0), T ), holds at(have inf o(Q, I), T )
⇒ assume happens(tell(svs, C, inf orm(Q, I), D), T 0 ), T 0 > T
observed(C, tell(C, svs, query ref (Q), D, T 0), T ), holds at(no inf o(Q), T )
⇒ assume happens(tell(svs, C, ref use(Q), D), T 0 ), T 0 > T
assume happens(tell(svs, C, inf orm(Q, I), D), T ),
assume happens(tell(svs, C, ref use(Q), D), T 0 )
⇒ f alse
assume happens(A, T ), not executable(A) ⇒ f alse
executable(tell(svs, C, S, D)) ← C 6= svs
332

Computational Logic Foundations of KGP Agents

initially(no inf o(arrival(tr01))
precondition(tell(svs, C, inf orm(Q, I), D), have inf o(Q, I))
initiates(tell(C, svs, inf orm(Q, I), D), T, have inf o(Q, I))
terminates(tell(C, svs, inf orm(Q, I), D), T, no inf o(Q))
10.1.3 Behaviour
To illustrate the behaviour of the psa we will assume that this agent requests from svs, at
time 3, say, the arrival time of tr01. svs receives a request from psa at time 5 for the arrival
time of tr01. Via POI at time 5 svs records in its KB0 :
observed(psa, tell(psa, svs, query ref (arrival(tr01)), d, 3), 5)
where d is the dialogue identifier. Then, via RE, at time 7, say, svs modifies its state by
adding to F a tree T rooted at an action a1 to answer to psa. This action a1 is a refusal
represented as:
a1 = tell(svs, psa, ref use(arrival(tr01)), d, τ ),
and the temporal constraint τ > 7 is added to C.
The refusal action is generated via the Reactivity capability because svs does not have
information about the requested arrival time. svs executes the planned action a1 at time
10, say, via the AE transition, instantiating its execution time, adding the following record
to KB0 :
executed(tell(svs, psa, ref use(arrival(tr01)), d), 10),
and updating Σ by adding τ = 10 to it.
Suppose then that svs makes two observations as follows. At time 17 svs receives
information of the arrival time (18) of the tr01 train from co. Via POI, svs records in its
KB0 17 :
observed(co, tell(co, svs, inf orm(arrival(tr01), 18), d0 , 15), 17).
Assume further that at time 25 svs receives another request from psa about the arrival
time of tr01 and, via POI, svs records in its KB0 :
observed(psa, tell(psa, svs, query ref (arrival(tr01)), d00 , 20), 25)
with a new dialogue identifier d00 . This leads to a different answer from svs to the query of
psa. svs adds an action to its state to answer psa with the arrival time. This is done again
via RE, say at time 28. A new tree is added in F rooted at the (reactive) action
tell(svs, psa, inf orm(arrival(tr01), 18), d00 , τ 0 ),
and the new temporal constraint τ 0 > 28 is added to C.
Via AE, svs executes the action, instantiating its execution time to 30, say, and adding
the following record
17. d0 is the identifier of the dialogue within which this utterance has been performed, and would typically
be different from the earlier d.

333

Kakas, Mancarella, Sadri, Stathis & Toni

executed(tell(svs, psa, inf orm(arrival(tr01), 18), d00 ), 30)
to KB0 , and adding τ 0 = 30 to Σ.
Eventually, SR will clear the planned (and executed) actions from the F component of
the state of svs.
10.2 Setting 2 Formalised
We formalise here the initial state, knowledge bases and behaviour of psa for Setting 2
described in Section 2.6.2.
10.2.1 Initial State
Let us assume that initially the state of psa is as follows:
KB0 = { }
F

= {T1 , T2 }

C = {τ1 < 15, τ2 < 15}
Σ = {}
where T1 and T2 consist of a goals (respectively):
g1 = have ticket(madrid, denver, τ1 ) and
g2 = have visa(usa, τ2 ).
10.2.2 Knowledge Bases
psa
To plan for goal g1 , the KBplan
contains:

initiates(buy ticket online(F rom, T o), T, have ticket(F rom, T o))
precondition(buy ticket online(F rom, T o), available connection)
precondition(buy ticket online(F rom, T o), available destination(T o)).
psa
To plan for goal g2 , the KBplan
contains:

initiates(apply visa(usa), T, have visa(usa))
precondition(apply visa(usa), have address(usa))
initiates(book hotel(L), T, have address(usa)) ← holds(in(L, usa), T ).
10.2.3 Behaviour
When PI is called on the above state, at time 2, say, it generates a partial plan for the goal,
changing the state as follows. The goal g1 acquires three children in T1 . These are:
g11 = available connection(τ11 ),
g12 = available destination(denver, τ12 ),
a13 = buy ticket online(madrid, denver, τ13 ).
Also, consequently, the set of temporal constraints is updated to:
C = {τ1 < 15, τ2 < 15, τ11 = τ13 , τ12 = τ13 , τ13 < τ1 , τ1 > 2}.
334

Computational Logic Foundations of KGP Agents

The action a13 is generated as an action that initiates goal g1 . Moreover, every plan that
is generated must satisfy the integrity constraints in KBplan . In particular, any precondition
of actions in the tree that do not already hold must be generated as sub-goals in the tree.
This is why g11 and g12 are generated in the tree as above.
Now via the transition SI, the following sensing actions are added to T1 as siblings of
action a13 18 :
a14 = sense(available connection, τ14 )
a15 = sense(available destination(denver), τ15 )
and the constraints
τ14 = τ15 , τ14 < τ13
are added to C.
Then, via AE, these two sensing actions are executed (before the original action a1 ),
and KB0 is updated with the result of the sensing as follows. Suppose these two actions are
executed at time 5. Consider the first action that senses the fluent available connection. If
this fluent is confirmed by the physical sensing capability, i.e. if available connection : true
is in X such that
sensing({available connection, available destination}, 5) = X,
then observed(available connection, 5) is added to KB0 . On the other hand, if
available connection : f alse
is in X as above, then observed(¬available connection, 5) is added to KB0 . In both cases
τ14 = 5 is added to Σ.
If neither of these cases occurs, i.e. if the sensing capability cannot confirm either of
available connection or ¬available connection, then no fact is added to KB0 . Similarly
for the other precondition, available destination. Let us assume that after this step of AE,
KB0 becomes
observed(available connection, 5)
observed(available destination(denver), 5)
AE can then execute the original action a13 . Note that the agent might decide to execute
the action even if one or both preconditions are not known to be satisfied after the sensing.
If g1 is achieved, SR will eliminate it and a13 , a14 , a15 , g11 , g12 from the state. In the
resulting state, F = {T2 }, and PI is called, say at time 6. This results in generating a
partial plan for g2 , and changing the state so that in T2 the root g2 has children
a21 = apply visa(usa, τ21 )
g22 = have address(usa, τ22 )
and τ21 < τ2 , τ22 = τ21 are added to C. Then, further PI, say at time 7, introduces
a23 = book hotel(denver, τ23 )
18. For this we assume that the auxiliary part of Tcycle contains the rule
unreliable pre(As) ← buy ticket online( , , ) ∈ As

335

Kakas, Mancarella, Sadri, Stathis & Toni

as a child of g22 in T2 , and adding τ23 < τ22 to C. Then, AE at time 8 executes a23 , adding it
to KB0 , and further AE at time 9 executes a22 , also updating KB0 . Finally, SR eliminates
all actions and goals in T2 and returns an empty F in the state.

11. Related Work
Many proposals exist for models and architectures of individual agents based on computational logic foundations (see e.g. the survey by Fisher, Bordini, Hirsch, & Torroni,
2007). Some of these proposals are based on logic programming, for example IMPACT (Arisha, Ozcan, Ross, Subrahmanian, Eiter, & Kraus, 1999; Subrahmanian, Bonatti, Dix,
Eiter, Kraus, Ozcan, & Ross, 2000), AAA (Balduccini & Gelfond, 2008; Baral & Gelfond,
2001), DALI (Costantini & Tocchio, 2004), MINERVA (Leite, Alferes, & Pereira, 2002),
GOLOG (Levesque, Reiter, Lesperance, Lin, & Scherl, 1997), and IndiGolog (De Giacomo,
Levesque, & Sardiña, 2001). Other proposals are based on modal logic or first-order logic
approaches, for example the BDI model (Bratman et al., 1988; Rao & Georgeff, 1997) and
its extensions to deal with normative reasoning (Broersen, Dastani, Hulstijn, Huang, &
van der Torre, 2001), Agent0 (Shoham, 1993), AgentSpeak (Rao, 1996) and its variants,
3APL (Hindriks, de Boer, van der Hoek, & Meyer, 1999) and its variants (Dastani, Hobo,
& Meyer, 2007).
At a high level of comparison there are similarities in the objectives of most existing
computational logic models of agency and KGP, in that they all aim at specifying knowledgerich agents with certain desirable behaviours. There are also some similarities in the finer
details of the KGP model and some of the above related work, as well as differences.
A feature of the KGP which, to the best of our knowledge, is novel is the declarative
and context-sensitive specification of an agent’s cycle. To avoid a static cycle of control
(Rao & Georgeff, 1991; Rao, 1996), KGP relies upon a cycle theory which determines, at
run time, given the circumstances and the individual profile of the agent, what the next
step should be. The cycle theory is sensitive to both solicited and unsolicited information
that the agent receives from its environment, and helps the agent to adapt its behaviour
to the changes it experiences. The approach closest to our work is that of 3APL (Hindriks
et al., 1999) as extended by Dastani, de Boer, Dignum, and Meyer (2003), which provides
meta-programming constructs for specifying the cycle of an agent such as goal selection,
plan expansion, execution, as well as if-then-else and while-loop statements. Unlike the
imperative constructs of 3APL, KGP uses a set of selection operators that can be extended
to model different behaviours and types of agents. A flexible ordering of transitions is then
obtained using preference reasoning about which transitions can be applied at a specific
point in time. These preferences may change according to external events or changes in the
knowledge of the agent.
Another central distinguishing feature of the KGP model, in comparison with existing
models, including those based on logic programming, is its modular integration within
a single framework of abductive logic programming, temporal reasoning, constraint logic
programming, and preference reasoning based on logic programming with priorities, in order
to support a diverse collection of capabilities. Each one of these is specified declaratively
and equipped with its own provably correct computational counterpart (see Bracciali,
336

Computational Logic Foundations of KGP Agents

Demetriou, Endriss, Kakas, Lu, Mancarella, Sadri, Stathis, Terreni, & Toni, 2004, for a
detailed discussion).
Compared with existing logic programming approaches KGP has two main similarities
with MINERVA (Leite et al., 2002), an architecture that exploits computational logic and
gives both declarative and operational semantics to its agents. Unlike KGP, a MINERVA
agent consists of several specialised, possibly concurrent, sub-agents performing various
tasks, and relies upon MDLP (Multidimensional Dynamic Logic Programming) (Leite et al.,
2002). MDLP is the basic knowledge representation mechanism of an agent in MINERVA,
which is based on an extension of answer-set programming and explicit rules for updating
the agent’s knowledge base. In KGP instead we integrate abductive logic programming and
logic programming with priorities combined with temporal reasoning.
Closely related to our work in KGP is the logic-based agent architecture for reasoning
agents of Baral and Gelfond (2001). This architecture assumes that the state of an agent’s
environment is described by a set of fluents that evolve over time in terms of transitions
labelled by actions. An agent is also assumed to be capable of correctly observing the state
of the environment, performing actions, and remembering the history of what happened in
it. The agent’s knowledge base consists of an action description part specifying the internal
agent transitions, which are domain specific and not generic as in KGP. The knowledge
base also contains what the agent observes in the environment including its own actions,
as in KGP’s KB0 . The temporal aspects of agent transitions are specified in the action
language AL implemented in A-Prolog, a language of logic programs under the answerset programming semantics. The answer sets of domain specific programs specified in AL
correspond to plans that in KGP are hypothetical narratives of the abductive event calculus.
The control of the agent is based on a static observe-think-act cycle, an instance of the KGP
cycle theories. A more recent and refined account of the overall approach has given rise to
the AAA Architecture, see (Balduccini & Gelfond, 2008) for an overview.
DALI (Costantini & Tocchio, 2004) is a logic programming language designed for executable specification of logical agents. Like KGP, DALI attempts to provide constructs to
represent reactivity and proactivity in an agent using extended logic programs. A DALI
agent contains reactive rules, events, and actions aimed at interacting with an external
environment. Behaviour (in terms of reactivity or proactivity) of a DALI agent is triggered
by different event types: external, internal, present, and past events. All the events and
actions are time stamped so as to record when they occur. External events are like the
observations in KGP, while past events are like past observations. However, KGP does not
support internal events but has instead the idea of transitions that are called by the cycle
theory to trigger reactive or proactive behaviour.
IndiGolog (De Giacomo et al., 2001) is a high-level programming language for robots
and intelligent agents that supports, like KGP, on-line planning, sensing and plan execution
in dynamic and incompletely known environments. It is a member of the Golog family of
languages (Levesque et al., 1997) that use a Situation Calculus theory of action to perform
the reasoning required in executing the program. Instead in the KGP model we rely on
abductive logic programming and logic programming with priorities combined with temporal reasoning. Instead of the Situation Calculus in KGP we use the Event Calculus for
temporal reasoning, but our use of the Event Calculus is not a prerequisite of the model as
in InterRaP (Müller, Fischer, & Pischel, 1998), but can be replaced with another temporal
337

Kakas, Mancarella, Sadri, Stathis & Toni

reasoning framework, if needed. Apart from the difference between the use of the Situation and Event Calculi, in IndiGolog goals cannot be decided dynamically, whereas in the
KGP model they change dynamically according to the specifications in the Goal Decision
capability.
There is an obvious similarity of the KGP model with the BDI model (Bratman et al.,
1988) given by the correspondence between KGP’s knowledge, goals and plan and BDI’s
beliefs, desires and intentions, respectively. Apart from the fact that the BDI model is
based on modal logic, in KGP the knowledge (beliefs in BDI) is partitioned in modules,
to support the various reasoning capabilities. KGP also tries to bridge the gap between
specification and the practical implementation of an agent. This gap has been criticized in
BDI by Rao (1996), when he developed the AgentSpeak(L) language. The computational
model of AgentSpeak(L) has been formally studied by d’Inverno and Luck (1998), while
recent implementations of the AgentSpeak interpreter have been incorporated in the Jason
platform (Bordini & Hübner, 2005). Like the KGP implementation in PROSOCS (Bracciali
et al., 2006), the Jason implementation too seeks to narrow the gap between specification
and executable BDI agent programs. Jason also extends BDI with new features like belief
revision (Alechina, Bordini, Hübner, Jago, & Logan, 2006).
A particular line of work in BDI is that of Padgham and Lambrix (2005), who investigate
how the notion of capability can be integrated in the BDI Logic of Rao and Georgeff (1991),
so that a BDI agent can reason about its own capabilities. A capability in this work is
informally understood as the ability to act rationally towards achieving a particular goal,
in the sense of having an abstract plan type that is believed to achieve the goal. Formally,
the BDI logic of Rao and Georgeff is extended to incorporate a modality for capabilities
that constrains agent goals and intentions to be compatible with what the agent believes
are its capabilities. A set of compatibility axioms are then presented detailing the semantic
conditions to capture the desired inter-relationships among an agent’s beliefs, capabilities,
goals, and intentions. The work also summarises how the extensions of the BDI model can
be implemented by adapting the BDI interpreter to include capabilities, further arguing the
benefits of the extension over the original BDI Interpreter of Rao and Georgeff (1992).
In KGP capabilities equate to the reasoning capabilities of an agent that allow the agent
to plan actions from a given state, react to incoming observations, or decide upon which
goals to adopt. However, in KGP, we do not use capabilities at the level of an agent’s
domain specific knowledge to guide the agent in determining whether or not it is rational
to adopt a particular goal.
The issue of the separation between specification and implementation exists between
the KGP model and Agent0 (Shoham, 1993), and its later refinement PLACA (Thomas,
1995). Two other differences between the KGP and Agent0 and PLACA are the explicit
links that exist in the KGP model amongst the goals (in the structuring of the forest in the
agent state) and the richer theories in the KGP that specify priorities amongst potential
goals which are not restricted to temporal orderings. These explicit links are exploited
when revising goals and state, via the Revision transition, in the light of new information
or because of the passage of time.
The BOID architecture (Broersen et al., 2001) extends the well known BDI model (Rao
& Georgeff, 1992) with obligations, thus giving rise to four main components in representing
an agent: beliefs, obligations, intentions and desires. The focus of BOID is to find ways of
338

Computational Logic Foundations of KGP Agents

resolving conflicts amongst these components. In order to do so they define agent types,
including some well known types in agent theories such as realistic, selfish, social and simple
minded agents. The agent types differ in that they give different priorities to the rules for
each of the four components. For instance, the simple minded agent gives higher priority to
intentions, compared to desires and obligations, whereas a social agent gives higher priority
to obligations than desires. They use priorities with propositional logic formulae to specify
the four components and the agent types.
The existing KGP model already resolves some of the conflicts that BOID tries to address. For example, if there is a conflict between a belief and a prior intention, which means
that an intended action can no longer be executed due to the changes in the environment,
the KGP agent will notice this and will give higher priority to the belief than the prior
intention, allowing the agent in effect to retract the intended action and, time permitting,
to replan for its goals. The KGP model also includes a notion of priority used in the Goal
Decision capability and the cycle theory that controls the behaviour of the agent. The
KGP model has also been extended to deal with normative concepts, the extended model is
known as N-KGP (Sadri, Stathis, & Toni, 2006). What N-KGP has in common with BOID
is that it seeks to extend KGP with the addition of obligations. The N-KGP model also
extends the notion of priorities by incorporating them amongst different types of goals and
actions. A detailed comparison of N-KGP with related work is presented by Sadri, Stathis,
and Toni (2006).
There are features that are included in some other approaches that are absent in the
KGP model. BDI and, more so, the IMPACT system (Arisha et al., 1999; Subrahmanian
et al., 2000) allow agents to have in their knowledge bases representations of the knowledge
of other agents. These systems allow the agents both some degree of introspection and the
ability to reason about other agents’ beliefs and reasoning. The KGP model to this date
does not include any such features. IMPACT also allows the incorporation of legacy systems, possibly using diverse languages, and has a richer knowledge base language including
deontic concepts and probabilities. Similarly, the 3APL, system is based on a combination
of imperative and logic programming languages, and includes an optimisation component
absent from the KGP. This component in 3APL includes rules that identify if in a given
situation the agent is pursuing a suboptimal plan, and help the agent find a better way
of achieving its goals. 3APL also includes additional functionalities such as learning (van
Otterlo, Wiering, Dastani, & Meyer, 2003), which our model does not currently support.
2APL (Dastani et al., 2007) is an extension of 3APL with goals and goal-plan rules as well
as external and internal events. 2APL has a customisable (via graphical interface) cycle
which is fixed once customised.

12. Conclusions
We have presented the computational logic foundations of the KGP model of agency. The
model allows the specification of heterogeneous agents that can interact with each other, and
can exhibit both proactive and reactive behaviour allowing them to function in dynamic
environments by adjusting their goals and plans when changes happen in such environments. KGP incorporates a highly modular agent architecture that integrates a collection
339

Kakas, Mancarella, Sadri, Stathis & Toni

of reasoning and sensing capabilities, synthesised within transitions, orchestrated by cycle
theories that take into account the dynamic context and agent preferences.
The formal specification of the KGP components within computational logic has the
major advantage of facilitating both a formal analysis of the model and a direct verifiable
implementation. This formal analysis has been started by Sadri and Toni (2006), where
we give a formal analysis of KGP agents by exploring their effectiveness in terms of goal
achievement, and reactive awareness, and the impact of their reasoning capabilities towards
progress in goal achievement. An implementation of a precursor of this model, described
by Kakas et al. (2004b), has already been developed within the PROSOCS platform of
Stathis et al. (2004) upon provably correct computational counterparts defined for each
component of the model as given by Kakas et al. (2004b). Concrete choices for these
computational counterparts have been described by Bracciali et al. (2004). The resulting
development framework allows the deployment and testing of the functionality of the earlier
variant of KGP agents. Deployment of these agents relies upon the agent template designed
by Stathis et al. (2002), which builds upon previous work with the head/body metaphor
described by Steiner et al. (1991) and Haugeneder et al. (1994), and the mind/body architecture introduced by Bell (1995) and recently used by Huang, Eliens, and de Bra (2001).
This development platform has been applied to a number of practical applications, and,
in particular, to ambient intelligence by Stathis and Toni (2004). Also, Sadri (2005) has
provided guidelines for specifying applications using KGP agents. Future work includes implementing and deploying the revised KGP model given in this paper: we envisage that this
will pose limited conceptual challenges, as we will be able to capitalise on our experience
in implementing and deploying the precursor of this model.
Sadri, Stathis, and Toni (2006) have explored how the precursor of the KGP agent
model can be augmented with normative features allowing agents to reason about and
choose between their social and personal goals, prohibitions and obligations. It would be
interesting to continue this work for the finalised KGP model given in this paper.
Sadri and Toni (2005) have developed a number of different profiles of behaviour,
defined in terms of specific cycle theories, and formally proved their advantages in given
circumstances. It would be interesting to explore this dimension further, to characterise
different agent personalities and provide guidance, through formal properties, as to the
type of personality needed for applications.
Future work also includes extending the model to incorporate (i) other reasoning capabilities, including knowledge revision (e.g. by Inductive Logic Programming), and more
sophisticated forms of temporal reasoning, including identifying explanations for unexpected
observations, (ii) introspective reasoning and reasoning about the beliefs of other agents,
(iii) further experimentation with the model via its implementation, and (iv) development
of a concurrent implementation.

Acknowledgments
This work was supported by the EU FET Global Computing Initiative, within the SOCS
project (IST-2001-32530). We wish to thank all our colleagues in SOCS for useful discussions
during the development of KGP. We are also grateful to Chitta Baral and the anonymous
referees for helpful comments on an earlier version of this paper.
340

Computational Logic Foundations of KGP Agents

Appendix A. Normal Cycle Theory
We give here the main parts of the normal Tcycle , but exclude others, for example the
definitions for incompatible and the auxiliary part, including definitions for predicates such
as empty f orest, unreliable pre etc. For more details see (Kakas et al., 2005).
Tinitial : This consists of the following rules:
R0|GI (S0 , {}) : ∗GI(S0 , {}) ← empty f orest(S0 )
R0|AE (S0 , As) : ∗AE(S0 , As) ← empty non executable goals(S0 ), As = fAS (S0 , t),
As 6= {}, time now(t)
R0|P I (S0 , G) : ∗P I(S0 , G) ← Gs = fGS (S0 , t), Gs 6= {}, G ∈ Gs, time now(t)
Tbasic : This consists of the following rules:
• The rules for deciding what might follow an AE transition are as follows:
RAE|P I (S 0 , G) : ∗P I(S 0 , G) ← AE(S, As, S 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {},
G ∈ Gs, time now(t0 )
0
0
RAE|AE (S , As ) : ∗AE(S 0 , As0 ) ← AE(S, As, S 0 , t), As0 = fAS (S 0 , t0 ),
As0 6= {}, time now(t0 )
RAE|AOI (S 0 , F s) : ∗AOI(S 0 , F s) ← AE(S, As, S 0 , t), F s = fES (S 0 , t0 ),
F s 6= {}, time now(t0 )
RAE|SR (S 0 ) : ∗SR(S 0 , {}) ← AE(S, As, S 0 , t)
RAE|GI (S 0 , {}) : ∗GI(S 0 , {}) ← AE(S, As, S 0 , t)
Namely, AE could be followed by another AE, or by a PI, or by an AOI, or by a SR, or by
a GI, or by a POI.
• The rules for deciding what might follow SR are as follows
RSR|P I (S 0 , G) : ∗P I(S 0 , G) ← SR(S, {}, S 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G ∈ Gs,
time now(t0 )
RSR|GI (S 0 , {}) : ∗GI(S 0 , {}) ← SR(S, {}, S 0 , t), Gs = fGS (S 0 , t0 ), Gs = {},
time now(t0 )
0
RSR|AE (S , As) : ∗AE(S 0 , As) ← SR(S, {}, S 0 , t), As = fGS (S 0 , t0 ), As 6= {},
time now(t0 )
Namely, SR can only be followed by PI or GI or AE, depending on whether or not there
are goals to plan for in the state.
• The rules for deciding what might follow PI are as follows
RP I|AE (S 0 , As) : ∗AE(S 0 , As) ← P I(S, G, S 0 , t), As = fAS (S 0 , t0 ), As 6= {},
time now(t0 )
0
RP I|SI (S , P s) : ∗SI(S 0 , P s) ← P I(S, G, S 0 , t), P s = fP S (S 0 , t0 ), P s 6= {}, time now(t0 )
The second rule is here to allow the possibility of sensing the preconditions of an action
before its execution.
• The rules for deciding what might follow GI are as follows
RGI|RE (S 0 , {}) : ∗RE(S 0 , {}) ← GI(S, {}, S 0 , t)
RGI|P I (S 0 , G) : ∗P I(S 0 , G) ← GI(S, {}, S 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G ∈ Gs,
time now(t0 )
Namely, GI can only be followed by RE or PI, if there are goals to plan for.
• The rules for deciding what might follow RE are as follows
341

Kakas, Mancarella, Sadri, Stathis & Toni

RRE|P I (S 0 , G) : ∗P I(S 0 , G) ← RE(S, {}, S 0 , t), Gs = fGS (S 0 , t0 ), Gs 6= {}, G ∈ Gs,
time now(t0 )
RRE|SI (S 0 , P s) : ∗SI(S 0 , P s) ← RE(S, {}, S 0 , t), P s = fP S (S 0 , t0 ), P s 6= {},
time now(t0 )
• The rules for deciding what might follow SI are as follows
RSI|AE (S 0 , As) : ∗AE(S 0 , As) ← SI(S, P s, S 0 , t), As = fAS (S 0 , t0 ), As 6= {},
time now(t0 )
0
RSI|SR (S , {}) : ∗SR(S 0 , {}) ← SI(S, P s, S 0 , t)
• The rules for deciding what might follow AOI are as follows
RAOI|AE (S 0 , As) : ∗AE(S 0 , As) ← AOI(S, F s, S 0 , t), As = fAS (S 0 , t0 ), As 6= {},
time now(t0 )
0
RAOI|SR (S , {}) : ∗SR(S 0 , {}) ← AOI(S, F s, S 0 , t)
RAOI|SI (S 0 , P s) : ∗SI(S 0 , P s) ← AOI(S, F s, S 0 , t), P s = fP S (S 0 , t0 ), P s 6= {},
time now(t0 )
• The rules for deciding what might follow POI are as follows
RP OI|GI (S 0 , {}) : ∗GI(S 0 , {}) ← P OI(S, {}, S 0 , t)
Tbehaviour : This consists of the following rules:
• GI should be given higher priority if there are no trees in the state:
T
PGIÂT
0 : RT |GI (S, {}) Â RT |T 0 (S, X) ← empty f orest(S)
for all transitions T, T 0 , T 0 6= GI, and with T possibly 0 (indicating that if there are no
trees in the initial state of an agent, then GI should be its first transition).
• GI is also given higher priority after a POI:
P OI : R
0
0
PGIÂT
P OI|GI (S , {}) Â RP OI|T (S, {}, S )
for all transitions T 6= GI.
• After GI, the transition RE should be given higher priority:
GI
PREÂT
: RGI|RE (S, {}) Â RGI|T (S, X)
for all transitions T 6= RE.
• After RE, the transition PI should be given higher priority:
PPRE
IÂT : RRE|P I (S, G) Â RRE|T (S, X)
for all transitions T 6= P I.
• After PI, the transition AE should be given higher priority, unless there are actions in
the actions selected for execution whose preconditions are “unreliable” and need checking,
in which case SI will be given higher priority:
PI
PAEÂT
: RP I|AE (S, As) Â RP I|T (S, X) ← not unreliable pre(As)
for all transitions T 6= AE.
PI
: RP I|SI (S, P s) Â RP I|AE (S, As) ← unreliable pre(As)
PSIÂAE
• After SI, the transition AE should be given higher priority
SI
: RSI|AE (S, As) Â RSI|T (S, X)
PAEÂT
for all transitions T 6= AE.
• After AE, the transition AE should be given higher priority until there are no more
actions to execute in the state, in which case either AOI or SR should be given higher
priority, depending on whether there are actions which are “unreliable”, in the sense that
their effects need checking, or not:
342

Computational Logic Foundations of KGP Agents

AE
PAEÂT
: RAE|AE (S, As) Â RAE|T (S, X)
for all transitions T 6= AE. Note that, by definition of Tbasic , the transition AE is applicable
only if there are still actions to be executed in the state.
AE
AE
PAOIÂT
: RAE|AOI (S, F s) Â RAE|T (S, X)) ← BCAOI|T
(S, F s, t), time now(t)
AE
for all transitions T 6= AOI, where the behaviour condition BCAOI|T (S, F s, t) is defined (in
the auxiliary part) by:
AE
BCAOI|T
(S, F S, t) ← empty executable goals(S, t), unreliable ef f ect(S, t)
Similarly, we have:
AE (S, t), time now(t)
AE
: RAE|SR (S, {}) Â RAE|T (S, X)) ← BCSR|T
PSRÂT
for all transitions T 6= SR where:
AE (S, t) ← empty executable goals(S, t), not unreliable ef f ect(S, t)
BCSR|T
Here, we assume that the auxiliary part of Tcycle specifies whether a given set of actions
contains any “unreliable” action, in the sense expressed by unreliable ef f ect, and defines
the predicate empty executable goals.

• After SR, the transition PI should have higher priority:
PPSR
IÂT : RSR|P I (S, G) Â RSR|T (S, X))
for all transitions T 6= P I.
Note that, by definition of Tbasic , the transition PI is applicable only if there are still goals
to plan for in the state. If there are no actions and goals left in the state, then rule RGI|T
would apply.
• In the initial state PI should be given higher priority:
PP0 IÂT : R0|P I (S, G) Â R0|T (S, X)
for all transitions T 6= P I. Note that, by definition of Tinitial below, the transition PI is
applicable initially only if there are goals to plan for in the initial state.

343

Kakas, Mancarella, Sadri, Stathis & Toni

References
Alechina, N., Bordini, R. H., Hübner, J. F., Jago, M., & Logan, B. (2006). Belief Revision
for AgentSpeak Agents. In Nakashima, H., Wellman, M. P., Weiss, G., & Stone, P.
(Eds.), 5th International Joint Conference on Autonomous Agents and Multiagent
Systems (AAMAS 2006), pp. 1288–1290, Hakodate, Japan. ACM.
Arisha, K. A., Ozcan, F., Ross, R., Subrahmanian, V. S., Eiter, T., & Kraus, S. (1999).
IMPACT: a Platform for Collaborating Agents. IEEE Intelligent Systems, 14 (2),
64–72.
Balduccini, M., & Gelfond, M. (2008). The AAA Architecture: An Overview. In In AAAI
Spring Symposium on Architectures for Intelligent Theory-Based Agents (AITA08).
Baral, C., & Gelfond, M. (2001). Reasoning agents in dynamic domains. In Logic-based
artificial intelligence, pp. 257–279. Kluwer Academic Publishers, Norwell, MA, USA.
Bell, J. (1995). A Planning Theory of Practical Rationality. In Proceedings of AAAI’95
Fall Symposium on Rational Agency, pp. 1–4. AAAI Press.
Bordini, R. H., & Hübner, J. F. (2005). BDI Agent Programming in AgentSpeak using Jason
(Tutorial Paper). In Toni, F., & Torroni, P. (Eds.), Computational Logic in MultiAgent Systems, 6th International Workshop, CLIMA VI, Lecture Notes in Computer
Science, pp. 143–164. Springer.
Bracciali, A., & Kakas, A. (2004). Frame consistency: Reasoning with explanations.
In Proceedings of the 10th International Workshop on “Non-Monotonic Reasoning”
(NMR2004), Whistler BC, Canada.
Bracciali, A., Demetriou, N., Endriss, U., Kakas, A. C., Lu, W., Mancarella, P., Sadri, F.,
Stathis, K., Terreni, G., & Toni, F. (2004). The KGP Model of Agency for Global
Computing: Computational Model and Prototype Implementation. In Priami, C., &
Quaglia, P. (Eds.), Global Computing, pp. 340–367, Rovereto, Italy. Springer.
Bracciali, A., Endriss, U., Demetriou, N., Kakas, A. C., Lu, W., & Stathis, K. (2006).
Crafting the Mind of PROSOCS Agents. Applied Artificial Intelligence, 20 (2-4), 105–
131.
Bratman, M., Israel, D., & Pollack, M. (1988). Plans and resource-bounded practical reasoning. Computational Intelligence, 4.
Broersen, J., Dastani, M., Hulstijn, J., Huang, Z., & van der Torre, L. (2001). The BOID
Architecture: conficts between Beliefs, Obligations, Intentions and Desires. In Proceedings of Fifth International Conference on Autonomous Agents (Agents 2001), pp.
9–16. ACM Press, Montreal, Canada.
Clark, K. L. (1978). Negation as Failure. In Gallaire, H., & Minker, J. (Eds.), Logic and
Data Bases, pp. 293–322. Plenum Press.
Costantini, S., & Tocchio, A. (2004). The DALI Logic Programming Agent-Oriented Language. In Alferes, J. J., & Leite, J. A. (Eds.), Proceedings of the 9th European Conference on Logics in Artificial Intelligence, (JELIA 2004), Vol. 3229 of Lecture Notes
in Computer Science, pp. 685–688. Springer.
344

Computational Logic Foundations of KGP Agents

Dastani, M., de Boer, F., Dignum, F., & Meyer, J.-J. (2003). Programming Agent Deliberation: An approach illustrated using the 3APL Language. In Autonomous Agents
and Mult-agent Systems (AAMAS’03), pp. 97–104, Australia.
Dastani, M., Hobo, D., & Meyer, J.-J. (2007). Practical Extensions in Agent Programming
Languages. In Proceedings of the Sixth International Joint Conference on Autonomous
Agents and Multiagent Systems (AAMAS’07). ACM Press.
de Bruijn, O., & Stathis, K. (2003). Socio-Cognitive Grids: The Net as a Universal Human
Resource. In Kameas, A., & Streitz, N. (Eds.), Proceedings of the Conference of “Tales
of the Disappearing Computer”, pp. 211–218, Santorini. CTI Press.
De Giacomo, G., Levesque, H. J., & Sardiña, S. (2001). Incremental execution of guarded
theories. ACM Transactions on Computational Logic, 2 (4), 495–525.
d’Inverno, M., & Luck, M. (1998). Engineering AgentSpeak(L): A Formal Computational
Model. J. Log. Comput., 8 (3), 233–260.
FIPA Communicative Act Library Specification (2001a). Experimental specification
XC00037H. Foundation for Intelligent Physical Agents, http://www.fipa.org.
FIPA Query Interaction Protocol (2001b). Experimental specification XC00027F. Foundation for Intelligent Physical Agents, http://www.fipa.org.
Fisher, M., Bordini, R., Hirsch, B., & Torroni, P. (2007). Computational Logics and Agents:
A Road Map of Current Technologies and Future Trends. Computational Intelligence,
23 (1), 61–91.
Haugeneder, H., Steiner, D., & McCabe, F. (1994). IMAGINE: A framework for building
multi-agent systems. In Deen, S. M. (Ed.), Proceedings of the 1994 International
Working Conference on Cooperating Knowledge Based Systems (CKBS-94), pp. 31–
64, DAKE Centre, University of Keele, UK.
Hindriks, K. V., de Boer, F. S., van der Hoek, W., & Meyer, J. C. (1999). Agent programming in 3APL. Autonomous Agents and Multi-Agent Systems, 2(4), 357–401.
Huang, Z., Eliens, A., & de Bra, P. (2001). An Architecture for Web Agents. In Proceedings
of EUROMEDIA’01. SCS.
Jaffar, J., & Maher, M. (1994). Constraint logic programming: a survey. Journal of Logic
Programming, 19-20, 503–582.
Kakas, A. C., Kowalski, R. A., & Toni, F. (1998). The role of abduction in logic programming. In Gabbay, D. M., Hogger, C. J., & Robinson, J. A. (Eds.), Handbook of Logic in
Artificial Intelligence and Logic Programming, Vol. 5, pp. 235–324. Oxford University
Press.
Kakas, A. C., Mancarella, P., & Dung, P. M. (1994). The acceptability semantics for logic
programs. In Proceedings of the eleventh international conference on Logic programming, pp. 504–519, Cambridge, MA, USA. MIT Press.
Kakas, A. C., & Miller, R. (1997). A simple declarative language for describing narratives
with actions. Logic Programming, 31.
345

Kakas, Mancarella, Sadri, Stathis & Toni

Kakas, A. C., & Moraitis, P. (2003). Argumentation based decision making for autonomous
agents. In Rosenschein, J. S., Sandholm, T., Wooldridge, M., & Yokoo, M. (Eds.),
Proceedings of the Second International Joint Conference on Autonomous Agents and
Multiagent Systems (AAMAS-2003), pp. 883–890, Melbourne, Victoria. ACM Press.
Kakas, A., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2005). Declarative Agent
Control. In Leite, J., & Torroni, P. (Eds.), CLIMA V: Computational Logic in MultiAgent Systems, Vol. 3487 of Lecture Notes in Artificial Intelligence (LNAI), pp. 96–
110. Springer Verlag.
Kakas, A. C., Kowalski, R. A., & Toni, F. (1992). Abductive Logic Programming. J. Log.
Comput., 2 (6), 719–770.
Kakas, A. C., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2004a). Declarative Agent
Control. In Leite, J. A., & Torroni, P. (Eds.), Computational Logic in Multi-Agent Systems, 5th International Workshop, CLIMA V, Vol. 3487 of Lecture Notes in Computer
Science, pp. 96–110. Springer.
Kakas, A. C., Mancarella, P., Sadri, F., Stathis, K., & Toni, F. (2004b). The KGP Model of
Agency. In de Mántaras, R. L., & Saitta, L. (Eds.), Proceedings of the 16th Eureopean
Conference on Artificial Intelligence (ECAI 2004), pp. 33–37. IOS Press.
Kowalski, R. A., & Sergot, M. (1986). A logic-based calculus of events. New Generation
Computing, 4 (1), 67–95.
Kowalski, R., & Toni, F. (1996). Abstract argumentation. Artificial Intelligence and Law
Journal, Special Issue on Logical Models of Argumentation, 4 (3-4), 275–296. Kluwer
Academic Publishers.
Leite, J. A., Alferes, J. J., & Pereira, L. M. (2002). MIN ERVA: A dynamic logic programming agent architecture. In Intelligent Agents VIII: 8th International Workshop,
ATAL 2001, Seattle, WA, USA, Revised Papers, Vol. 2333 of Lecture Notes in Artificial Intelligence, pp. 141–157.
Levesque, H. J., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. B. (1997). GOLOG: A
logic programming language for dynamic domains. Journal of Logic Programming,
31 (1-3), 59–83.
Mamdani, E. H., Pitt, J., & Stathis, K. (1999). Connected Communities from the Standpoint
of Multi-agent Systems. New Generation Computing, 17 (4), 381–393.
Mancarella, P., Sadri, F., Terreni, G., & Toni, F. (2004). Planning partially for situated
agents. In Leite, J. A., & Torroni, P. (Eds.), Computational Logic in Multi-Agent Systems, 5th International Workshop, CLIMA V, Vol. 3487 of Lecture Notes in Computer
Science, pp. 230–248. Springer.
Miller, R., & Shanahan, M. (2002). Some alternative formulations of the event calculus.
In Kakas, A. C., & Sadri, F. (Eds.), Computational Logic: Logic Programming and
Beyond - Essays in Honour of Robert A. Kowalski, Vol. 2408 of Lecture Notes in
Computer Science, pp. 452–490. Springer.
Müller, J., Fischer, K., & Pischel, M. (1998). A Pragmatic BDI Architecture. In Huhns,
M. N., & Singh, M. P. (Eds.), Readings in Agents, pp. 217–225. Morgan Kaufmann
Publishers.
346

Computational Logic Foundations of KGP Agents

Padgham, L., & Lambrix, P. (2005). Formalisations of capabilities for BDI-agents. Autonomous Agents and Multi-Agent Systems, 10 (3), 249–271.
Prakken, H., & Sartor, G. (1996). A system for defeasible argumentation, with defeasible
priorities. In International Conference on Formal and Applied Practical Reasoning,
Springer Lecture Notes in AI 1085, pp. 510–524.
Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming with defeasible priorities. Journal of Applied Non-Classical Logics, 7 (1), 25–75.
Rao, A. S. (1996). AgentSpeak(L): BDI agents speak out in a logical computable language.
In van Hoe, R. (Ed.), Agents Breaking Away, 7th European Workshop on Modelling
Autonomous Agents in a Multi-Agent World, MAAMAW’96, Eindhoven, The Netherlands, January 22-25, 1996, Proceedings, Vol. 1038 of Lecture Notes in Computer
Science, pp. 42–55. Springer-Verlag.
Rao, A. S., & Georgeff, M. P. (1991). Modeling Rational Agents within a BDI-architecture.
In Fikes, R., & Sandewall, E. (Eds.), Proceedings of Knowledge Representation and
Reasoning (KR&R-91), pp. 473–484. Morgan Kaufmann Publishers.
Rao, A. S., & Georgeff, M. P. (1997). Modeling rational agents within a BDI-architecture.
In Huhns, M. N., & Singh, M. P. (Eds.), Readings in Agents, pp. 317–328. Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Rao, A. S., & Georgeff, M. P. (1992). An abstract architecture for rational agents. In Nebel,
B., Rich, C., & R.Swartout, W. (Eds.), 3rd International Conference on Principles
of Knowledge Representation and Reasoning (KR’92), pp. 439–449, Cambridge, MA,
USA. Morgan Kaufmann.
Sadri, F. (2005). Using the KGP model of agency to design applications (Tutorial Paper).
In Toni, F., & Torroni, P. (Eds.), Computational Logic in Multi-Agent Systems, 6th
International Workshop, CLIMA VI, Vol. 3900 of Lecture Notes in Computer Science,
pp. 165–185. Springer.
Sadri, F., Stathis, K., & Toni, F. (2006). Normative KGP agents. Computational & Mathematical Organization Theory, 12 (2-3), 101–126.
Sadri, F., & Toni, F. (2005). Variety of behaviours through profiles in logic-based agents.
In Toni, F., & Torroni, P. (Eds.), Computational Logic in Multi-Agent Systems, 6th
International Workshop, CLIMA VI, Vol. 3900 of Lecture Notes in Computer Science,
pp. 206–225. Springer.
Sadri, F., & Toni, F. (2006). A Formal Analysis of KGP agents. In Fisher, M., van der Hoek,
W., Konev, B., & Lisitsa, A. (Eds.), Logics in Artificial Intelligence, 10th European
Conference, JELIA 2006, Vol. 4160 of Lecture Notes in Computer Science, pp. 413–
425. Springer.
Shanahan, M. (1997). Solving the Frame Problem. MIT Press.
Shanahan, M. (1989). Prediction is deduction but explanation is abduction. In Proceedings
of the 11th International Joint Conference on Artificial Intelligence, pp. 1055–1060.
Shoham, Y. (1993). Agent-oriented programming. Artificial Intelligence, 60 (1), 51–92.
347

Kakas, Mancarella, Sadri, Stathis & Toni

SOCS (2007). Societies Of ComputeeS: a computational logic model for the description,
analysis and verification of global and open societies of heterogeneous computees.
http://lia.deis.unibo.it/research/socs/.
Stathis, K., Child, C., Lu, W., & Lekeas, G. K. (2002). Agents and Environments.
Tech. rep. Technical Report IST32530/CITY/005/DN/I/a1, SOCS Consortium, 2002.
IST32530/CITY/005/DN/I/a1.
Stathis, K., Kakas, A., Lu, W., Demetriou, N., Endriss, U., & Bracciali, A. (2004).
PROSOCS: a platform for programming software agents in computational logic. In
Müller, J., & Petta, P. (Eds.), Proceedings of From Agent Theory to Agent Implementation (AT2AI-4 – EMCSR’2004 Session M), pp. 523–528, Vienna, Austria.
Stathis, K., & Toni, F. (2004). Ambient Intelligence using KGP Agents. In Markopoulos, P.,
Eggen, B., Aarts, E. H. L., & Crowley, J. L. (Eds.), Ambient Intelligence: Proceedings
of Second European Symposium, EUSAI 2004, Vol. 3295 of Lecture Notes in Computer
Science, pp. 351–362. Springer.
Steiner, D. E., Haugeneder, H., & Mahling, D. (1991). Collaboration of knowledge bases
via knowledge based collaboration. In Deen, S. M. (Ed.), CKBS-90 — Proceedings of
the International Working Conference on Cooperating Knowledge Based Systems, pp.
113–133. Springer Verlag.
Subrahmanian, V. S., Bonatti, P., Dix, J., Eiter, T., Kraus, S., Ozcan, F., & Ross, R. (2000).
Heterogeneous Agent Systems. MIT Press/AAAI Press, Cambridge, MA, USA.
Thomas, S. R. (1995). The PLACA agent programming language. In Wooldridge, M. J., &
Jennings, N. R. (Eds.), Intelligent Agents, pp. 355–370, Berlin. Springer-Verlag.
van Otterlo, M., Wiering, M., Dastani, M., & Meyer, J.-J. (2003). A Characterization
of Sapient Agents. In Hexmoor, H. (Ed.), International Conference Integration of
Knowledge Intensive Multi-Agent Systems (KIMAS-03), pp. 172–177, Boston, Massachusetts. IEEE.
Wooldridge, M. (2002). An Introduction to Multiagent Systems. John Wiley & Sons.
Yip, A., Forth, J., Stathis, K., & Kakas, A. C. (2005). Software Anatomy of a KGP Agent.
In Gleizes, M. P., Kaminka, G. A., Nowé, A., Ossowski, S., Tuyls, K., & Verbeeck, K.
(Eds.), EUMAS 2005 - Proceedings of the Third European Workshop on Multi-Agent
Systems, pp. 459–472. Koninklijke Vlaamse Academie van Belie voor Wetenschappen
en Kunsten.

348

Journal of Artificial Intelligence Research 33 (2008) 79-107

Submitted 07/07; published 09/08

On the Use of Automatically Acquired Examples
for All-Nouns Word Sense Disambiguation
David Martinez

davidm@csse.unimelb.edu.au

University of Melbourne
3010, Melbourne, Australia

Oier Lopez de Lacalle

oier.lopezdelacalle@ehu.es

University of the Basque Country
20018, Donostia, Basque Country

Eneko Agirre

e.agirre@ehu.es

University of the Basque Country
20018, Donostia, Basque Country

Abstract
This article focuses on Word Sense Disambiguation (WSD), which is a Natural Language Processing task that is thought to be important for many Language Technology
applications, such as Information Retrieval, Information Extraction, or Machine Translation. One of the main issues preventing the deployment of WSD technology is the lack of
training examples for Machine Learning systems, also known as the Knowledge Acquisition Bottleneck. A method which has been shown to work for small samples of words is
the automatic acquisition of examples. We have previously shown that one of the most
promising example acquisition methods scales up and produces a freely available database
of 150 million examples from Web snippets for all polysemous nouns in WordNet. This
paper focuses on the issues that arise when using those examples, all alone or in addition to
manually tagged examples, to train a supervised WSD system for all nouns. The extensive
evaluation on both lexical-sample and all-words Senseval benchmarks shows that we are
able to improve over commonly used baselines and to achieve top-rank performance. The
good use of the prior distributions from the senses proved to be a crucial factor.

1. Introduction
This paper is devoted to the Word Sense Disambiguation (WSD) task for Natural Language
Processing (NLP). The goal of this task is to determine the senses of the words as they
appear in context. For instance, given the sentence “He took all his money from the bank.”,
if we focus on the word bank, the goal would be to identify the intended sense, which in
this context would be some “financial” sense, instead of other possibilities like the “edge
of river” sense. The senses can be defined in a dictionary, knowledge-base or ontology.
This task is defined as an intermediate step towards natural language understanding. The
construction of efficient algorithms for WSD would benefit many NLP applications such
as Machine Translation (MT), or Information Retrieval (IR) systems (Resnik, 2006). For
instance, if an MT system was to translate the previous example into French, it would
need to choose among the possible translations of the word bank. This word should be
translated as “banque” when it is used in the financial sense (as in the example), but as
“rive” when it is used in the “edge of river” sense. See the work of Vickrey, Biewald,
c
2008
AI Access Foundation. All rights reserved.

Martinez, Lopez de Lacalle & Agirre

Teyssier, and Koller (2005) for a recent evaluation of cross-lingual WSD in MT. For IR
engines, it would also be useful to determine which is the sense of the word in the query in
order to retrieve relevant documents, specially when working with multilingual documents
in Cross-Language Information Retrieval (CLIR), or other IR scenarios where recall is a key
performance factor, such as retrieving images by their captions. Some evidence in favor of
using WSD in IR has been gathered lately (Kim, Seo, & Rim, 2004; Liu, Liu, Yu, & Meng,
2004; Stevenson & Clough, 2004; Vossen, Rigau, Alegrı́a, Agirre, Farwell, & Fuentes, 2006).
WSD techniques can also fill an important role in the context of the Semantic Web.
The Web has grown focusing on human communication, rather than automatic processing.
The Semantic Web has the vision of automatic agents working with the information in the
Web at the semantic level, achieving interoperability with the use of common terminologies
and ontologies (Daconta, Obrst, & Smith, 2005). Unfortunately most of the information in
the Web is in unstructured textual form. The task of linking the terms in the texts into
concepts in a reference ontology is paramount to the Semantic Web.
Narrower domains like Biomedicine are also calling for WSD techniques. The Unified
Medical Language System (UMLS) (Humphreys, Lindberg, Schoolman, & Barnett, 1998) is
one of the most extensive ontologies in the field, and studies on mapping terms in medical
documents to this resource have reported high levels of ambiguity, which calls for WSD
technology (Weeber, Mork, & Aronson, 2001).
WSD has received the attention of many groups of researchers, with general NLP books
dedicating separate chapters to WSD (Manning & Schütze, 1999; Jurafsky & Martin, 2000;
Dale, Moisl, & Somers, 2000), special issues on WSD in NLP journals (Ide & Veronis,
1998; Edmonds & Kilgarriff, 2002), and books devoted specifically to the issue (Ravin &
Leacock, 2001; Stevenson, 2003; Agirre & Edmonds, 2006). The interested reader can start
with the dedicated chapter by Manning and Schütze (1999) and the WSD book (Agirre
& Edmonds, 2006). The widespread interest motivated the Senseval initiative1 , which
has joined different research groups in a common WSD evaluation framework since 1998.
The goal is to follow the example of other successful competitive evaluations, like DUC
(Document Understanding Conference) or TREC (Text Retrieval Conference).
WSD systems can be classified according to the knowledge they use to build their models, which can be derived from different resources like corpora, dictionaries, or ontologies.
Another distinction is drawn on corpus-based systems, distinguishing between those that
rely on hand-tagged corpora (supervised systems), and those that do not require this resource (unsupervised systems). This distinction is important because the effort required
to hand-tag senses is high, and it would be costly to obtain tagged examples for all word
senses and all languages, as some estimations show (Mihalcea & Chklovski, 2003). In spite
of this drawback (referred to as “the knowledge acquisition bottleneck”), most of recent
efforts have been devoted to the improvement of supervised systems, which are the ones
that obtain the highest performance, even with the current low amounts of training data.
These systems rely on sophisticated Machine Learning (ML) algorithms that construct their
models based on the features extracted from the training examples.
Alternatively, Senseval defines two kinds of WSD tasks: lexical-sample and all-words.
In a lexical-sample task the systems need to disambiguate specific occurrences of a handful
1. http://www.senseval.org

80

On the Use of Automatically Acquired Examples for All-Nouns WSD

of words for which relatively large numbers of training examples are provided (more than
100 examples in all cases). In the all-words task, no training data is provided, and testing
is done for whole documents. Systems need to tag all content words occurring in the texts,
even if only small amounts of external training data are available.
The analysis of the results for the English lexical-sample exercise in the third edition of
Senseval (Mihalcea & Edmonds, 2004) suggested that a plateau in performance had been
reached for ML methods. For this task, where the systems had relatively large amounts of
training data, there were many systems on the top, performing very close to each other.
The systems were able to significantly improve the baselines and attained accuracies above
70% (Mihalcea, Chklovski, & Killgariff, 2004).
The case was different in the all-words task (Snyder & Palmer, 2004), where supervised
systems also performed best. They used training examples from Semcor (Miller, Leacock,
Tengi, & Bunker, 1993), which is the only sizable all-words sense-tagged corpus at the time
of writing this paper. The scarcity of examples and the use of test documents from corpora
unrelated to Semcor heavily affected the performance, and only a few systems scored above
the baseline method of assigning the most frequent sense in Semcor. In order to be useful
for NLP applications, WSD systems have to address the knowledge acquisition bottleneck
for all (or at least a significant part) of the word types, as evaluated by all-words tasks.
Lexical-sample tasks are useful for evaluating WSD systems under ideal conditions (i.e.
regarding availability of training data), but they do not show systems to be scalable to
all the words in the vocabulary. In this work we will use a lexical-sample task in order to
adjust some parameters of our system, but the main evaluation is on an all-words task. Our
experiments are designed accordingly: the lexical-sample tests show empirical evidence on
specific parameters, and the all-words evaluation compares our systems to the state of the
art.
In this article, we explore a method to alleviate the knowledge acquisition bottleneck at
a large scale. We use WordNet (Fellbaum, 1998) to automatically acquire examples from the
Web. The seminal work of Leacock, Chodorow, and Miller (1998) showed that the approach
was promising, with good results on a small sample of nouns. Other works in the field of
automatic acquisition of examples have focused on exploring different approaches to the
acquisition process (Agirre, Ansa, Martinez, & Hovy, 2000; Mihalcea, 2002; Cuadros, Padró,
& Rigau, 2006), with a straightforward application to WSD. Those explorations typically
required costly querying over the Web, and thus tried a limited number of variations for
a handful of words. Our approach is different in spirit: we want to go through the whole
process for all nouns, from the acquisition of examples itself to their use on WSD and the
thorough evaluation on the Senseval 2 lexical-sample and Senseval 3 all-words datasets.
This comes at the cost of not exploring all the different possibilities at each step, but has
the advantage of showing that the results are extensive, and not limited to a small set of
nouns.
For these reasons, and given the prior work on acquisition techniques, we use the most
efficient and effective example acquisition method according to independent experiments
performed by Agirre et al. (2000) and Cuadros et al. (2006). The focus of this paper is thus
on the issues that arise when using those examples as training data of a supervised ML
system. This paper will show that the automatically acquired examples can be effectively
81

Martinez, Lopez de Lacalle & Agirre

used with or without pre-existing data, and that deciding the amount of examples to use
for each sense (the prior distribution) is a key issue.
The objectives of this paper are to show that existing methods to acquire examples from
the Web scale-up to all nouns, and to study other issues that arise when these examples
are to be used as training data in an all-nouns WSD system. Our goal is to build a stateof-the-art WSD system for all nouns using automatically retrieved examples.
Given the cost of large-scale example acquisition, we decided to limit the scope of our
work only to nouns. We think that noun disambiguation on its own can be a useful tool in
many applications, specially in the IR tasks mentioned above. Our method can be easily
adapted to verbs and adjectives (Cuadros et al., 2006), and we plan to pursue this line in
the future.
The work reported here has been partially published in two previous conference papers.
The method for the automatic acquisition of examples was described by Agirre and Lopez
de Lacalle (2004). A first try on the application of those examples to Word Sense Disambiguation was presented in Agirre and Martinez (2004b). In this paper we present a global
view of the whole system, together with a more thorough evaluation, which shows that the
automatically acquired examples can be used to build state-of-the-art WSD systems in a
variety of settings.
The article is structured as follows. After this introduction, related work on the knowledge acquisition bottleneck in WSD is described in Section 2, with a focus on automatic
example acquisition. Section 3 introduces the method to automatically build SenseCorpus,
our automatically acquired examples for WordNet senses. Section 4 describes the experimental setting. Section 5 explores some factors on the use of SenseCorpus and evaluates
them on a lexical-sample task. The final systems are thoroughly evaluated on an all-nouns
task in Section 6. Finally, Section 7 provides some discussion, and the conclusions and
further work are outlined in Section 8.

2. Related Work
The construction of WSD systems applicable to all words has been the goal of many research initiatives. In this section we will describe related work that looks for ways to
alleviate the knowledge acquisition bottleneck using the following techniques: bootstrapping, active learning, parallel corpora, automatic acquisition of examples and acquisition of
topic signatures. Sections 5 and 6, which evaluate our proposed system in public datasets,
will review the best performing systems in the literature.
Bootstrapping techniques consist on algorithms that learn from a few instances of labeled
data (seeds) and a big set of unlabeled examples. Among these approaches, we can highlight
co-training (Blum & Mitchell, 1998) and their derivatives (Collins & Singer, 1999; Abney,
2002). These techniques are very appropriate for WSD and other NLP tasks because of
the wide availability of untagged data and the scarcity of tagged data. However, these
systems have not been shown to perform well for fine-grained WSD. In his well-known work,
Yarowsky (1995) applied an iterative bootstrapping process to induce a classifier based on
Decision Lists. With a minimum set of seed examples, disambiguation results comparable
to supervised methods were obtained in a limited set of binary sense distinctions, but this
success has not been extended to fine-grained senses.
82

On the Use of Automatically Acquired Examples for All-Nouns WSD

Recent work on bootstrapping applied to WSD is also reported by Mihalcea (2004)
and Pham, Ng, and Lee (2005). In the former, the use of unlabeled data significantly
increases the performance of a lexical-sample system. In the latter, Pham et al. apply their
WSD classifier to the all-words task in Senseval-2, but targeting words over a threshold
of frequency in the Semcor and WSJ corpora. They observe a slight increase in accuracy
relying on unlabeled data.
Active learning is used to choose informative examples for hand-tagging, in order to
reduce manual cost. In one of the few works directly applied to WSD, Fujii, Inui, Tokunaga, and Tanaka (1998) used selective sampling for the acquisition of examples for the
disambiguation of verb senses, in an iterative process with human taggers. The informative
examples were chosen following two criteria: maximum number of neighbors in unsupervised
data, and minimum similarity with the supervised example set. Another active learning
approach is the Open Mind Word Expert (Mihalcea & Chklovski, 2003), which is a project
to collect sense-tagged examples from Web users. The system selects the examples to be
tagged applying a selective sampling method based on two different classifiers, choosing
the unlabeled examples where there is disagreement. The collected data was used in the
Senseval-3 English lexical-sample task.
Parallel corpora is another alternative to avoid the need of hand-tagged data. Recently
Chan and Ng (2005) built a classifier from English-Chinese parallel corpora. They grouped
senses that share the same Chinese translation, and then the occurrences of the word on the
English side of the parallel corpora were considered to have been disambiguated and “sense
tagged” by the appropriate Chinese translations. The system was successfully evaluated
in the all-words task of Senseval-2. However, parallel corpora is an expensive resource to
obtain for all target words. A related approach is to use monolingual corpora in a second
language and use bilingual dictionaries to translate the training data (Wang & Carroll,
2005). Instead of using bilingual dictionaries, Wang and Martinez (2006) applied machine
translation to text snippets in foreign languages back into English and achieved good results
on English lexical-sample WSD.
In the automatic acquisition of training examples, an external lexical resource (WordNet,
for instance) or a sense-tagged corpus is used to obtain new examples from a very large
untagged corpus (e.g. the Web). Leacock et al. (1998) present a method to obtain sensetagged examples using monosemous relatives from WordNet. Our approach is based on this
early work (cf. Section 3). In their algorithm, Leacock et al. (1998) retrieve the same number
of examples per each sense, and they give preference to monosemous relatives that consist
on a multiword containing the target word. Their experiment is evaluated over 14 nouns
with coarse sense-granularity and few senses. The results showed that the monosemous
corpus provided precision close to that of hand-tagged data.
Another automatic acquisition approach (Mihalcea & Moldovan, 1999) used information
in WordNet (e.g. monosemous synonyms and glosses) to construct queries, which were later
fed into the Altavista2 search engine. Four procedures were used sequentially, in a decreasing
order of precision, but with increasing levels of coverage. Results were evaluated by hand,
showing that 91% of the examples were correctly retrieved among a set of 1,080 instances
of 120 word senses. However, the corpus resulting from the experiment was not used to
2. http://www.altavista.com

83

Martinez, Lopez de Lacalle & Agirre

train a real WSD system. Agirre and Martinez (2000), in an early precursor of the work
presented here, tried to apply this technique to train a WSD system with unsatisfactory
results. The authors concluded that the examples themselves were correct, but that they
somehow mislead the ML classifier, providing biased features.
In related work, Mihalcea (2002) generated a sense tagged corpus (GenCor) by using a
set of seeds consisting of sense-tagged examples from four sources: (i) Semcor, (ii) WordNet,
(iii) examples created using the method above, and (iv) hand-tagged examples from other
sources (e.g. the Senseval-2 corpus). By means of an iterative process, the system obtained
new seeds from the retrieved examples. In total, a corpus with about 160,000 examples was
gathered. However, the evaluation was carried out on the lexical-sample task, showing that
the method was useful for a subset of the Senseval-2 testing words (results for 5 words were
provided), and without analysing which were the sources of the performance gain. Even if
the work presented here uses other techniques, our work can be seen as an extension of this
limited study, in the sense that we evaluate on all-words tasks.
These previous works focused on the use of two different kinds of techniques for the
automatic acquisition of examples, namely, the use of monosemous relatives alone (Leacock
et al., 1998) and the use of a combination of monosemous relatives and glosses (Mihalcea
& Moldovan, 1999; Mihalcea, 2002). In all cases the examples are directly used to feed
a supervised ML WSD system, but with limited evaluation and no indication that the
methods can scale-up. Unfortunately, no direct comparison of the alternative methods and
parameters to automatically acquire examples for WSD exists, but we can see a preference
to use the Web, as existing corpora would contain very few occurrences of the monosemous
terms or gloss fragments.
A closely related area to that of automatic acquisition of examples for WSD is that
of enriching knowledge bases with topic signatures. For instance, Agirre et al. (2000) and
Agirre, Ansa, Martinez, and Hovy (2001) used the combined monosemous-relatives plus
glosses strategy to query Altavista, retrieve the original documents and build lists of related
words for each word sense (so called topic signatures). The topic signatures are difficult to
evaluate by hand, so they were applied as context vectors to WSD in a straightforward way.
Note that the authors did not train a ML algorithm, but rather combined all the examples
in one vector per sense. They showed that using the Web compared favorably to using a
fixed corpus, but was computationally more costly: the system first needs to query a search
engine and then retrieve the original document in order to get an example for the sense.
As an alternative, Agirre and Lopez de Lacalle (2004) showed that it is possible to scale up
and gather examples for all nouns in WordNet if the query is limited to using monosemous
relatives and if the snippets returned by Google are used instead of the whole document.
At this point, Cuadros et al. (2006) set up a systematic framework for the evaluation
of the different parameters that affect the construction of topic signatures, including the
methods to automatically acquire examples. The study explores a wide range of querying
strategies (monosemous synonyms, monosemous relatives at different distances, and glosses,
combined using either and or or operators) on both a particular corpus (the British National Corpus) and the Web. The best results were obtained using Infomap3 on the British
National Corpus and our monosemous relatives method on the Web (Agirre & Lopez de
3. http://infomap-nlp.sourceforge.net

84

On the Use of Automatically Acquired Examples for All-Nouns WSD

Lacalle, 2004). Contrary to our method, Infomap returns only lists of related words, and
thus can not be used to retrieve training examples. These results are confirmed in other
experiments reported by Cuadros and Rigau (2006).
All in all, the literature shows that using monosemous relatives and snippets from the
Web (Agirre & Lopez de Lacalle, 2004) provides a method to automatically acquire examples
which scales up to all nouns in WordNet, and provides topic signatures of better quality
than other alternative methods. We will now explain how these examples were acquired.

3. Building a Sense-Tagged Corpus for all Nouns Automatically
In order to build this corpus (which we will refer to as SenseCorpus) we acquired 1,000
Google snippets for each monosemous noun in WordNet 1.6 (including multiwords, e.g.
church building). Then, for each word sense of an ambiguous noun, we gathered the examples of its monosemous relatives (e.g. for sense #2 of church, we gather examples from its
relative church building). The way to collect the examples is simply by querying the corpus
with the word or string of words (e.g. “church building”). This method is inspired in the
work by Leacock et al. (1998) and, as already mentioned in Section 2, it has been shown to
be both efficient and effective in experiments on topic signature acquisition.
The basic assumption of this method is that for a given word sense of the target word,
if we had a monosemous synonym of the word sense, then the examples of the synonym
should be very similar to those of the target word sense, and could therefore be used to train
a classifier of the target word sense. The same idea , to a lesser extent, can be applied to
other monosemous relatives, such as direct hyponyms, direct hypernyms, siblings, indirect
hyponyms, etc. The expected reliability decreases with the distance in the hierarchy from
the monosemous relative to the target word sense.
The actual method to build SenseCorpus is the following. We collected examples from
the Web for each of the monosemous relatives. The relatives have an associated number
(type), which correlates roughly with the distance to the target word, and indicates their
relevance: the higher the type, the less reliable the relative. Synonyms have type 0, direct
hyponyms get type 1, and distant hyponyms receive a type number equal to the distance
to the target sense. Direct hypernyms get type 2, because they are more general than the
target sense, and can thus introduce more noise than direct hyponyms. We also decided to
include less reliable siblings, but with type 3. More sophisticated schemes could be tried,
such as using WordNet similarity to weight the distance from the target to the relative
word. However, we chose this approach to capture the notion of distance for its simplicity,
and to avoid testing too many parameters. A sample of monosemous relatives for different
senses of church, together with its sense inventory in WordNet 1.7 is shown in Figure 1.
In the following subsections we will describe step by step the method to construct the
corpus. First we will explain the acquisition of the highest possible amount of examples per
sense, and then we will explain different ways to limit the number of examples per sense for
better performance.
3.1 Collecting the Examples
The method to collect the examples has been previously published (Agirre & Lopez de
Lacalle, 2004), and comprises the following steps:
85

Martinez, Lopez de Lacalle & Agirre

• Sense inventory (church)
– Sense 1: A group of Christians; any group professing Christian doctrine or belief.
– Sense 2: A place for public (especially Christian) worship.
– Sense 3: A service conducted in a church.
• Monosemous relatives for different senses (of church)
– Synonyms (Type 0): church building (sense 2), church service (sense 3) ...
– Direct hyponyms (Type 1): Protestant Church (sense 1), Coptic Church (sense 1) ...
– Direct hypernyms (Type 2): house of prayer (sense 2), religious service (sense 3) ...
– Distant hyponyms (Type 2,3,4...):
(sense 1)...

Greek Church (sense 1), Western Church

– Siblings (Type 3): Hebraism (sense 2), synagogue (sense 2) ...

Figure 1: Sense inventory and a sample of monosemous relatives in WordNet 1.7 for church.

1: We query Google4 with the monosemous relatives for each sense, and extract the
snippets returned by the search engine. All snippets are used (up to 1,000), but some of
them are dropped out in the next step.
2: We try to detect full meaningful sentences in the snippets which contain the target
word. We first detect sentence boundaries in the snippet and extract the sentence that
encloses the target word. Some of the sentences are filtered out, according to the following
criteria: length shorter than 6 words, having more non-alphanumeric characters than words
divided by two, or having more words in uppercase than in lowercase.
3: The automatically acquired examples contain a monosemous relative of the target
word. In order to use these examples to train the classifiers, the monosemous relative (which
can be a multiword term) is substituted by the target word. In the case of the monosemous
relative being a multiword that contains the target word (e.g. Protestant Church for church)
we can choose not to substitute, because Protestant, for instance, can be a useful feature
for the first sense of church. We tried both alternatives, and Section 5 will show that we
obtain slightly better results if no substitution is applied for such multiwords.
4: For a given word sense, we collect the desired number of examples (see the following
section) in order of their type: we first retrieve all examples of type 0, then type 1, etc. up to
type 3 until the necessary examples are obtained. We did not collect examples from type 4
upwards. We did not make any distinctions between the relatives from each type. Contrary
to Leacock et al. (1998) we do not give preference to multiword relatives containing the
target word.
All in all, we have acquired around 150 million examples for the nouns in WordNet using
this technique, which are publicly available5 .
4. We use the off-line XML interface kindly provided by Google for research.
5. http://ixa.si.ehu.es/Ixa/resources/sensecorpus.

86

On the Use of Automatically Acquired Examples for All-Nouns WSD

3.2 Number of Examples per Sense (Prior)
Previous work (Agirre & Martinez, 2000) has reported that the distribution of the number
of examples per word sense (prior for short) has a strong influence in the quality of the
results. That is, the results degrade significantly whenever the training and testing samples
have different distributions of the senses. It has also been shown that a type-based approach
that predicts the majority sense of a word in the domain can provide good performance by
itself (McCarthy, Koeling, Weeds, & Carroll, 2004).
As we are extracting examples automatically, we have to decide how many examples we
will use for each sense. In order to test the impact of the prior, different settings have been
tried:
• No prior: we take an equal amount of examples for each sense.
• Web prior: we take all examples gathered from the Web.
• Automatic ranking: the number of examples is given by a ranking obtained following
the method by McCarthy et al. (2004).
• Sense-tagged prior: we take a number of examples proportional to the relative frequency of the word senses in some hand-tagged corpus.
The first method assumes uniform priors. The second assumes that the number of
monosemous relatives and their occurrences are correlated to sense importance, that is,
frequent senses would have more occurrences of their monosemous relatives. The fourth
method uses the information in some hand-tagged corpus, typically Semcor. Note that this
last kind of prior requires hand-tagged data, while the rest (including the third method
below) are completely unsupervised.
The third method is more sophisticated and deserves some further clarification. McCarthy et al. (2004) present a method to acquire sense priors automatically from a domain
corpus. This is a two-step process. The first step is a corpus-based method, which given
a target word builds a list of contextually similar words (Lin, 1998) with weights. In this
case, the co-occurrence data was gathered from the British National Corpus. For instance,
given a target word like authority, the list of the topmost contextually similar words include government, police, official and agency 6 . The second step ranks the senses of the
target word, depending on the scores of a WordNet-based similarity metric (Patwardhan
& Pedersen, 2003) relative to the list of contextually similar words. Following with the
example, the pairwise WordNet similarity between authority and government is greater for
sense 5 of authority, which is evidence that this sense has some prominence in the corpus.
The pairwise similarity scores are added, yielding a ranking for the 7 senses of authority.
Table 2 shows in the column named Auto.MR the normalized scores assigned to each of
the senses of authority according to this technique.
Table 1 shows the number of examples per type (0,1,...) that are acquired for church
following the Semcor prior. The last column gives the number of examples in Semcor. Note
that the number of examples is sometimes smaller than 1,000 (maximum number of snippets
returned by Google in one query). This can be due to rare monosemous relatives, but is
6. Actual list of words taken from the demo in http://www.cs.ualberta.ca/~lindek/demos/depsim.htm.

87

Martinez, Lopez de Lacalle & Agirre

Sense
church#1
church#2
church#3
Overall

0
0
306
147
453

1
476
100
0
576

2
524
561
20
1,105

3
0
0
0
0

Total
1,000
967
167
2,134

Semcor
60
58
10
128

Table 1: Examples per type (0,1,2,3) that are acquired from the Web for the three senses
of church following the Semcor prior, and total number of examples in Semcor.

Sense
authority#1
authority#2
authority#3
authority#4
authority#5
authority#6
authority#7
Overall

Semcor
#ex
18
5
3
2
1
1
0
30

%
60.0
16.7
10.0
6.7
3.3
3.3
0.0
100.0

Web PR
#ex
%
338
0.5
44932
66.4
10798
16.0
886
1.3
6526
9.6
72
0.1
4106
6.1
67657
100.0

Auto.
#ex
138
75
93
67
205
71
67
716

SenseCorpus
MR
Semcor PR
%
#ex
%
19.3
338
33.7
10.5
277
27.6
13.0
166
16.6
9.4
111
11.1
28.6
55
5.5
9.9
55
5.5
9.4
1
0.1
100.0
1003
100.0

Semcor
#ex
324
90
54
36
18
18
1
541

MR
%
59.9
16.6
10.0
6.7
3.3
3.3
0.2
100.0

Senseval
test
#ex
%
37
37.4
17
17.2
1
1.0
0
0.0
34
34.3
10
10.1
0
0.0
99
100.0

Table 2: Distribution of examples for the senses of authority in different corpora. PR
(proportional) and MR (minimum ratio) columns correspond to different ways to
apply Semcor prior.

usually caused by the sentence extraction and filtering process, which discards around 50%
of the snippets.
The way to apply the prior is not straightforward. For illustration, we will focus on the
Semcor prior. In our first approach for Semcor prior, we assigned 1,000 examples to the
major sense in Semcor, and gave the other senses their proportion of examples. We call this
method proportional (PR). But in some cases the number of examples extracted will be
less than expected by the distribution of senses in Semcor. As a result, the actual number
of examples available would not follow the desired distribution.
As an alternative, we computed, for each word, the minimum ratio (MR) of examples
that were available for a given target distribution and a given number of examples extracted
from the Web. We observed that this last approach would reflect better the original prior,
at the cost of having less examples.
Table 2 presents the different distributions of examples for authority. There we can see
the Senseval-testing and Semcor distributions, together with the total number of examples
in the Web (Web PR); the Semcor proportional distribution (Semcor PR) and minimum
ratio (Semcor MR); and the automatic distribution with minimum ratio (Auto MR).
Getting a maximum of one thousand examples per monosemous relative allows to get up to
44,932 examples for the second sense (Web PR column), but only 72 for the sixth sense.
88

On the Use of Automatically Acquired Examples for All-Nouns WSD

Semcor
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
Average
Total

Web
prior
15,387
67,657
50,925
17,244
24,625
31,582
47,619
8,704
21,977
84,448
2,650
4,210
11,049
6,237
9,601
24,137
699,086

Automatic
prior
2,610
716
5,329
4,745
2,111
10,015
791
6,355
5,095
3,660
511
843
1,196
5,477
945
3,455
100,215

Semcor
prior
10,656
541
16,627
2,555
8,512
3,235
3,504
5,376
3,588
9,690
1,510
1,367
8,578
3,438
1,160
4,719
136,874

Semcor
Word
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew

Web
prior
20,874
6,682
16,714
12,161
100,109
648
608
32,553
34,968
33,055
10,315
5,361
10,356
10,767

Automatic
prior
277
2,730
1,846
884
6,385
464
608
9,813
8,005
2,877
2,176
2,657
3,081
8,013

Semcor
prior
2,209
1,531
1,248
2,959
7,855
287
594
24,746
4,264
2,152
2,059
2,458
2,175
2,000

Table 3: Number of examples following different sense distributions for the Senseval-2
nouns. Minimum ratio is applied both for the Semcor and automatic priors.

The sixth sense has a single monosemous relative, which is a rare word with few hits in
Google, while the second sense has many and frequent monosemous relatives.
Regarding the use of minimum ratio, the table illustrates how MR allows to better
approximate the distribution of senses in Semcor: the first sense7 has 60% in Semcor,
but only gets 33.7% in SenseCorpus with the proportional Semcor prior because there
are only 338 examples in SenseCorpus for the first sense. In contrast SenseCorpus with
minimum ratio using Semcor does assign 59.9% of the examples to the first sense. This
better approximation comes at the cost of getting 541 examples for authority, in contrast
to 1,003 with PR. Note that authority occurs only 30 times in Semcor.
The table also shows that for this word the distributions of senses in Semcor and
Senseval-test have important differences (sense 5 gets 3.3% and 34.3% respectively), although the most frequent sense is the same. For the Web and automatic distributions, the
most salient sense is different from that in Semcor, with the Web prior (Web PR column)
assigning only 0.5% to the first sense. Note that the automatic method is able to detect that
sense 5 is salient in the test corpus, while Semcor ranks it only 5th. In general, distribution
discrepancies similar to those in the table can be observed for the other words in the test
set.
To conclude this section, Table 3 shows the number of examples acquired automatically
for each word in the Senseval-2 lexical-sample following three approaches: the Web prior,
the Semcor prior with minimum ratio, and the Automatic prior with minimum ratio. We
can see that retrieving all the examples (Web prior) we get 24,137 examples in average per
word; and respectively 4,700 or 3,400 if we apply the Semcor prior or the Automatic prior.
7. The senses in WordNet are numbered according to their frequency in Semcor, so the first sense in
WordNet is paramount to the most frequent sense in Semcor.

89

Martinez, Lopez de Lacalle & Agirre

3.3 Decision Lists
The supervised learning method used to measure the quality of the corpus is Decision Lists
(DL). This simple method performs reasonably well in comparison with other supervised
methods in Senseval all words (as we will illustrate in Table 6.4), and preliminary experiments showed it to perform better with the automatically retrieved examples than more
sophisticated methods like Support Vector Machines or the Vector Space Model. It is well
known that learning methods perform differently according to several conditions, as showed
for instance by Yarowsky and Florian (2003), who analyzed in depth the performance of
various learning methods (including DL) in WSD tasks.
We think that the main reason for DL to perform better in our preliminary experiments
is that SenseCorpus is a noisy corpus with conflicting features. Decision Lists use the
single most powerful feature in the test context to make predictions, in contrast to other
ML techniques, and this could make them perform better in this corpus. Specially in the
all-words task, with only a few hand-tagged examples per word in most cases, even the
most sophisticate ML algorithms cannot deal with the problem by themselves. While the
best systems in the Senseval-3 lexical-sample rely on complex kernel-based methods, in the
all-words task the top systems are those that find external ways to deal with the sparseness
of data and then apply well-known methods, such as memory based learning or decision
trees (Mihalcea & Edmonds, 2004).
The DL algorithm is described by Yarowsky (1994). In this method, the sense sk with
the highest weighted feature fi is selected, according to its log-likelihood (see Formula 1).
For our implementation, we applied a simple smoothing method: for the cases where the
denominator is zero, we use 0.1 as the denominator. This is roughly equivalent to assigning
a 0.1 probability mass to the rest of senses, and has been shown to be effective enough
compared to more complex methods (Yarowsky, 1994; Agirre & Martinez, 2004a).
P r(sk |fi )
)
j6=k P r(sj |fi )

weight(sk , fi ) = log( P

(1)

3.4 Feature Types
The feature types that we extracted from the context can be grouped in three main sets:
Local collocations: bigrams and trigrams formed with the words around the target. These
features are constituted by lemmas, word-forms, or PoS tags8 . Other local features are those
formed with the previous/posterior lemma/word-form in the context.
Syntactic dependencies: syntactic dependencies were extracted using heuristic patterns,
and regular expressions defined with the PoS tags around the target9 . The following relations were used: object, subject, noun-modifier, preposition, and sibling.
Topical features: we extract the lemmas of the content words both in the whole sentence
and in a ±4-word window around the target. We also obtain salient bigrams in the context,
with the methods and the software described by Pedersen (2001).
8. The PoS tagging was performed with the fnTBL toolkit (Ngai & Florian, 2001).
9. This software was kindly provided by David Yarowsky’s group, from the Johns Hopkins University.

90

On the Use of Automatically Acquired Examples for All-Nouns WSD

The complete feature set was applied for our main experiments on the all-words Senseval3 corpus. However, for our initial experiments in the lexical-sample task only local features
and topical features (without salient bigrams) were applied.

4. Experimental Setting
We already noted in the introduction that lexical-sample evaluations as defined in Senseval
are not realistic: relatively large amounts of training examples are available, those are drawn
from the same corpus as the test examples, and both train and test examples are tagged by
the same team. Besides, developing a system for a handful of words does not necessarily
show that it is scalable. In contrast, all-words evaluations do not provide training data.
Supervised WSD systems typically use Semcor (Miller et al., 1993) for training. This
corpus offers tagged examples for all open-class words occurring in a 350.000 word subset
of the balanced Brown corpus, tagged with WordNet 1.6 senses. In contrast to lexicalsample, some polysemous words like authority only get a handful of examples (30 in this
case, cf. Table 2). Note that the test examples (from Senseval) and Semcor come from
different corpora and thus might be related to different domains, topics or genres. An
added difficulty is posed by the fact that they have been tagged by different teams of
annotators from distinct institutions.
With all this on mind, we designed two sets of experiments: the first set was performed
on a sample of nouns (lexical-sample), and it was used to develop and fine-tune the method
in basic aspects like the effect of the kinds of features and the importance of the prior. We
did not use the training examples, except to measure the impact of the priors. We provide
a comparison with state-of-the-art systems.
The second set of experiment was used to show that our method is scalable, useful
for any noun, and performs in the state-of-the art of WSD in a realistic setting. We thus
selected to apply WSD on all the nouns in running text (all-nouns). In this setting we apply
the best configurations obtained from the first set of experiments, and explore the use of
SenseCorpus alone, combined with priors from Semcor, and also with training data from
Semcor. We provide a comparison of our results with those of state-of-the-art systems.
For lexical-sample evaluation, the test part of the Senseval-2 English lexical-sample
task was chosen, which consisted on instances of 29 nouns, tagged with WordNet 1.7 senses.
The advantage of this corpus was that we could focus on a word-set with enough examples
for testing. Besides, it is a different corpus, and therefore the evaluation is more realistic
than that made using cross-validation over Semcor. In order to factor out pre-processing
and focus on WSD, the test examples whose senses were multiwords or phrasal verbs were
removed. Note that they are not as problematic since they can be efficiently detected with
other methods in a preprocess.
It is important to note that the training part of Senseval-2 lexical-sample was not used
in the construction of the systems, as our goal was to test the performance we could achieve
with minimal resources (i.e. those available for any word). We only relied on the Senseval-2
training prior in preliminary experiments on local/topical features, and as an upperbound
to compare the performance with other types of priors.
For the all-words evaluation we relied on the Senseval-3 all-words corpus (Snyder &
Palmer, 2004). The test data for this task consisted of 5,000 words of text. The data was
91

Martinez, Lopez de Lacalle & Agirre

extracted from two Wall Street Journal articles and one excerpt from the Brown Corpus.
The texts represent three different domains: editorial, news story, and fiction. Overall, 2,212
words were tagged with WordNet 1.7.1. senses (2,081 if we do not include multiwords). From
these, 695 occurrences correspond to polysemous nouns that are not part of multiwords,
and these comprise our testing set.
As the rest of Senseval participants, we had an added difficulty in that WordNet versions
do not coincide. We therefore used one of the freely available mappings between WordNet
versions (Daude, Padró, & Rigau, 2000) to convert the training material from Semcor
(tagged with WordNet 1.6 senses) into WordNet 1.7 and WordNet 1.7.1 versions (depending
on the target corpus). We preferred to use this mapping rather that relying on other
available mappings or converted Semcors. To our knowledge, no comparative evaluation
among mappings has been performed, and Daude et al. show that their mapping obtained
very high scores in an extensive manual evaluation. Note that the versions of Semcor
available in the Web (other than the original one, tagged with WordNet 1.6) have also been
obtained using an automatic mapping.
In both lexical-sample and all-nouns settings, we provide a set of baselines, which are
based on the most frequent heuristic. This heuristic is known to be hard to beat in WSD,
specially for unsupervised systems that do not have access to the priors, and even for
supervised systems in the all-nouns setting.

5. Lexical-Sample Evaluation
We performed four sets of experiments in order to study different factors, and compare our
performance to other state-of-the-art unsupervised systems in the Senseval-2 lexical-sample
task. First we analyzed the results of the systems when using different sets of local and
topical features, as well as substituting or not multiwords. The next experiments were
devoted to measure the effect of the prior on the performance. After that, we compared
our approach with unsupervised systems that participated in Senseval-2. As we mentioned
in the introduction, the results obtained in lexical-sample evaluations are not realistic, in
that we cannot expect to have hand-tagged data for all words in any target corpus. For this
reason we do not report results of supervised systems (which do use the training data). The
next section on all-nouns evaluation, which is more realistic, does compare to supervised
systems
5.1 Local vs. Topical Features, Substitution
Previous work on automatic acquisition of examples (Leacock et al., 1998) has reported
lower performance when using local collocations formed by PoS tags or closed-class words.
In contrast, Kohomban and Lee (2005), in a related approach, used only local features for
WSD because they discriminated better between senses. Given the fact that SenseCorpus
has also been constructed automatically, and the contradictory results on those previous
works, we performed an initial experiment comparing the results using local features, topical
features, and a combination of both. In this case we used SenseCorpus with Senseval training
prior, distributed according to the MR approach, and always substituting the target word.
The results (per word and overall) are given in Table 4.
92

On the Use of Automatically Acquired Examples for All-Nouns WSD

Local Feats.
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Overall

Coverage
94.4
93.4
98.3
100.0
100.0
73.5
100.0
100.0
88.7
98.6
100.0
100.0
98.2
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0
98.3
79.5
93.0
100.0
100.0
100.0
96.7

Precision
57.4
51.2
53.0
81.2
88.7
54.0
56.5
67.7
51.1
60.2
87.5
89.3
29.1
82.5
55.1
19.0
73.4
96.3
80.4
43.2
36.8
80.6
44.4
44.7
37.1
62.5
74.2
53.9
81.5
58.5

Recall
54.2
47.8
52.1
81.2
88.7
39.7
56.5
67.7
45.3
59.4
87.5
89.3
28.6
82.5
55.1
19.0
73.4
96.3
80.4
43.2
36.8
80.6
44.4
43.9
29.5
58.1
74.2
53.9
81.5
56.5

Topical
Feats.
Recall
45.6
43.2
55.9
87.5
88.7
53.7
55.6
51.6
54.2
54.7
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
44.2
38.6
80.6
39.3
40.5
37.5
37.2
72.6
46.1
81.5
56.0

Combined
Subst.
Recall
47.0
46.2
57.2
85.0
88.7
55.9
56.5
54.8
56.1
56.8
87.5
89.3
21.4
82.5
60.2
39.0
75.0
96.3
73.9
43.8
39.5
80.6
40.7
40.5
37.1
38.4
74.2
48.7
81.5
57.0

Combined
No Subst.
Recall
44.9
46.2
57.2
85.0
88.7
57.4
58.9
51.6
58.0
60.4
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
42.9
39.5
80.6
40.7
40.5
37.1
48.8
74.2
48.7
81.5
57.5

Table 4: Results per feature type (local, topical, and combination), using SenseCorpus with
Senseval-2 training prior (MR). Coverage and precision are given only for local
features (topical and combination have full coverage). Combination is shown for
both substitution and no substitution options. The best recall per word is given
in bold.

93

Martinez, Lopez de Lacalle & Agirre

In this experiment, we observed that local collocations achieved the best precision overall, but the combination of all features obtained the best recall. Local features achieve
58.5% precision for 96.7% coverage overall10 , while the topical and combined features have
full-coverage. The table shows clear differences in the results per word, a fact which is
also known for other algorithms using real training data (Yarowsky & Florian, 2003). This
variability is another important factor to focus on all-words settings, where large numbers
of different words are involved.
We also show the results for not substituting the monosemous relative by the target
word when the monosemous relative is a multiword. We can see that the results are mixed,
but that there is an slight overall improvement if we choose not to substitute in those cases.
For the following experiments, we chose to work with the combination of all features with
no substitution, as it achieved the best overall recall.
5.2 Impact of Prior
In order to evaluate the acquired corpus, our first task was to analyze the impact of the
prior. As we mentioned in Section 3.2, when training Decision Lists with the examples in
SenseCorpus, we need to decide the amount of examples for each sense (what can be seen
as the estimation of the prior probabilities of the senses).
Table 5 shows the recall11 attained by DL with each of the four proposed methods to
estimate the priors for each target word, plus the use of the training part of Senseval-2 lexical
sample to estimate the prior. Note that this last estimation method is not realistic, as one
cannot expect to have hand-tagged data for all words in a given target corpus, and should
thus be taken as an upperbound. In fact it is presented in this section for completeness,
and will not be used for comparison with other systems.
The results show constant improvement from the less informative priors to the most
informed ones. Among the three unsupervised prior estimation methods, the best results are
obtained with the automatic ranking, and the worst by the uniform distribution (“no prior”
column), with the distribution of examples as returned by SenseCorpus (“Web prior”) in
the middle. Estimating the priors from hand-tagged data improves the results considerably,
even when the target corpus and estimation corpus are different (“Semcor”), but the best
results overall are obtained when the priors are estimated from the training part of Senseval2 lexical-sample dataset. The results word by word show that each word behaves differently,
which is a well-known behavior in WSD. Note that for all priors except the most informed
one a number of words have performances below 10%, which might indicate that DL trained
on SenseCorpus is very sensitive to badly estimated priors.
Table 6 shows the overall results from Table 5, together with those obtained using the
prior on its own (“prior only”). The results show that the improvement attained by training
on SenseCorpus is most prominent for the unsupervised priors (from 6.5 to 19.7 percentage
points), with lower improvements (around 2.0 percentage points) for the priors estimated
from hand-tagged corpora. These results show clearly that the acquired corpus has use10. Note that due to the sparse data problem, some test examples might not have any feature in common
with the training data. In those cases the DL algorithm does not return any result, and thus the coverage
can be lower than 100%
11. All the results in the following tables are given as recall, as the coverage is always 100% and precision
equals to recall in this case.

94

On the Use of Automatically Acquired Examples for All-Nouns WSD

Unsupervised
Word
art
authority
bar
bum
chair
channel
child
church
circuit
day
detention
dyke
facility
fatigue
feeling
grip
hearth
holiday
lady
material
mouth
nation
nature
post
restraint
sense
spade
stress
yew
Overall

No
prior
34.0
20.9
24.7
36.7
61.3
42.2
40.3
43.8
44.3
15.3
52.1
92.9
19.6
58.8
27.2
11.3
57.8
70.4
24.3
51.7
39.5
80.6
21.9
36.8
26.3
44.8
74.2
38.6
70.4
38.0

Web
prior
61.1
22.0
52.1
18.8
62.9
28.7
1.6
62.1
52.8
2.2
16.7
89.3
26.8
73.8
51.0
8.0
37.5
7.4
79.3
50.8
39.5
80.6
44.4
47.4
9.1
18.6
66.1
52.6
85.2
39.8

Autom.
ranking
45.6
40.0
26.4
57.5
69.4
30.9
34.7
49.7
49.1
12.5
87.5
80.4
22.0
75.0
42.5
28.2
60.4
72.2
23.9
52.3
46.5
80.6
34.1
47.4
31.4
41.9
85.5
27.6
77.8
43.2

Minimally-Supervised
Semcor
prior
55.6
41.8
51.6
5.0
88.7
16.2
54.0
48.4
41.5
48.0
52.1
92.9
26.8
82.5
60.2
16.0
75.0
96.3
80.4
54.2
54.4
80.6
46.7
34.2
27.3
47.7
67.7
2.6
66.7
49.8

Senseval-2
prior
44.9
46.2
57.2
85.0
88.7
57.4
58.9
51.6
58.0
60.4
87.5
89.3
21.4
82.5
60.2
38.0
75.0
96.3
73.9
42.9
39.5
80.6
40.7
40.5
37.1
48.8
74.2
48.7
81.5
57.5

Table 5: Performance (recall) of SenseCorpus on the 29 nouns in Senseval-2 lexical-sample,
using different priors to train DL. Best results for each word in bold.

ful information about the word senses, and that the estimation of the prior is extremely
important.

Prior
no prior
Web prior
autom. ranking
Semcor prior
Senseval2 prior

Type
unsupervised
minimallysupervised

Only prior
18.3
33.3
36.1
47.8
55.6

SenseCorpus
38.0
39.8
43.2
49.8
57.5

Diff.
+19.7
+6.5
+7.1
+2.0
+1.9

Table 6: Performance (recall) on the nouns in Senseval-2 lexical-sample. In each row, results
for a given prior on its own, of SenseCorpus using that prior, and the difference
between both.

95

Martinez, Lopez de Lacalle & Agirre

Method
SenseCorpus (Semcor prior)
UNED
SenseCorpus (Autom. prior)
Kenneth Litkowski-clr-ls
Haynes-IIT2
Haynes-IIT1

Type
minimallysupervised
unsupervised

Recall
49.8
45.1
43.3
35.8
27.9
26.4

Table 7: Results for nouns of our best minimally supervised and fully unsupervised systems (in bold) compared to the unsupervised systems that took part in Senseval-2
lexical-sample.

5.3 Comparison with other Systems
At this point, it is important that we compare the performance of our DL-based approach
to other systems in the state of the art. In this section we compare our best unsupervised
system (the one using Automatic ranking) and the minimally unsupervised system (using
Semcor prior) with those systems participating on Senseval-2 that were deemed as unsupervised. In order to have the results of the other systems, we used the resources available from
the Senseval-2 competition, where the answers of the participating systems in the different
tasks were available12 . This made possible to compare the results on the same test data,
set of nouns and occurrences.
From the 5 systems presented in the Senseval-2 lexical-sample task as unsupervised, the
WASP-Bench system relied on lexicographers to hand-code information semi-automatically
(Tugwell & Kilgarriff, 2001). This system does not use the training data, but as it uses
manually coded knowledge we think it falls in the supervised category.
The results for the other 4 systems and our own are shown in Table 7. We classified the
UNED system (Fernandez-Amoros, Gonzalo, & Verdejo, 2001) as minimally supervised. It
does not use hand-tagged examples for training, but some of the heuristics that are applied
by the system rely on the prior information available in Semcor. The distribution of senses
is used to discard low-frequency senses, and also to choose the first sense as a back-off
strategy. On the same conditions, our minimally supervised system attains 49.8% recall,
nearly 5 points better.
The rest of the systems are fully unsupervised, and they perform significantly worse
than our unsupervised system.

6. All Nouns Evaluation
As we explained in the introduction, the main goal of this research is to develop a WSD
system that is able to tag all nouns in context, not only a sample of them. In the previous
section we explored different settings for our system, adjusting them according to the results
for a handful of words on a lexical-sample task.
12. http://www.senseval.org

96

On the Use of Automatically Acquired Examples for All-Nouns WSD

In this section we will test SenseCorpus in the 695 occurrences of polysemous nouns
present in the Senseval-3 all-words task, and compare our results with the performance of
the systems that participated in the competition. We also present an analysis of the results
according to the frequency of the target nouns.
We have developed three different systems, all based on SenseCorpus, but with different requirements of external information. The less informed system is the unsupervised
system (called SenseCorpus-U), which does not use any hand-coded corpus or prior extracted therein. This system relies on the examples in SenseCorpus following the Automatic Ranking (McCarthy et al., 2004) to train the DL (see Section 3.2). The following
system is minimally-supervised (SenseCorpus-MS), in the sense that it uses the priors
obtained from Semcor to define the distribution of examples from SenseCorpus that are
fed into the DL. Lastly, the most informed system trains the DL with the hand-tagged
examples from Semcor and SenseCorpus (following the Semcor prior), and is known as
SenseCorpus-S. The three systems follow a widely used distinction among unsupervised,
minimally-supervised and supervised systems, and we will compare each of them to similar
systems that participated on Senseval-3.
These systems respond to realistic scenarios. The unsupervised system is called for in
case of languages for which no all-words hand-tagged corpus exists, or in cases where the priors coming from Semcor are not appropriate, as in domain-specific corpora. The minimally
supervised system is useful when there is no hand-tagged corpora, but when there is some
indication of the distribution of senses. Lastly, the supervised system (SenseCorpus-S)
shows the performance of SenseCorpus on the currently available conditions for English,
that is, when an all-words corpus of limited size is available.
In order to measure the real contribution of SenseCorpus, we compare our three systems
to each of the following baselines: SenseCorpus-U vs. the first sense according to the
automatically obtained ranking, SenseCorpus-MS vs. the most frequent sense in Semcor,
and SenseCorpus-S vs. the Decision Lists trained on Semcor. In order to judge the
significance of the improvements, we applied one-tail paired t-test.
6.1 Comparison with Unsupervised Systems in Senseval-3
¿From the systems that participated in the all-words task only three did not rely on any
hand-tagged corpora (not even for estimating prior information). We compare the performance of those systems with our unsupervised system SenseCorpus-U in Table 8. In order
to make a fair comparison with respect to the participants, we removed the answers that did
not correctly guess the lemma of the test instance (discarding errors when pre-processing
the Senseval-3 XML data).
We can see that one of the participating systems was the automatic ranking by McCarthy
et al. (2004) that we used as a baseline. Although we were able to improve this system, our
results are below the best unsupervised system (IRST-DDD-LSI) (Strapparava, Gliozzo, &
Giuliano, 2004). Surprisingly, this unsupervised method is able to obtain better performance
on this dataset than the version that relies on Semcor frequencies (IRST-DDD-0, see next
subsection), but this discrepancy is not explained by the authors. The reasons for the
remarkable results of IRST-DDD-LSI are not clear, and subsequent publications by the
authors do not shed any light on it.
97

Martinez, Lopez de Lacalle & Agirre

Code
IRST-DDD-LSI
SenseCorpus-U
AutoPS (Baseline)
DLSI-UA

Method
LSI
Decision Lists
Automatic Rank.
WordNet Domains

Attempt.
570
680
675
648

Prec.
64.6
45.5
44.6
27.8

Rec.
52.9
44.4
43.3
25.9

F
58.2
45.0
43.9
26.8

p-value
0.001
–
0.001
0.000

Table 8: Performance of all unsupervised systems participating in Senseval-3 all-words for
the 695 polysemous nouns, accompanied by p-values of the one tailed paired t-test
with respect to our unsupervised system (in bold).

Code
SenseCorpus-MS
MFS (Baseline)
IRST-DDD-00
Clr04-aw
KUNLP
IRST-DDD-09

Method
DL
MFS
Domain-driven
Dictionary clues
Similar relative in WordNet
Domain-driven

Attempt.
695
695
669
576
628
346

Prec.
63.9
62.7
55.6
58.7
54.2
69.7

Rec.
63.9
62.7
53.5
48.6
49.0
34.7

F
63.9
62.7
54.5
53.2
51.5
46.3

p-value
–
0.044
0.000
0.000
0.000
0.000

Table 9: Performance of all minimally supervised systems participating in Senseval-3 allwords for the 695 polysemous nouns, accompanied by p-values of the one tailed
paired t-test with respect to SenseCorpus-MS (in bold).

The improvement over the baseline is lower here than in the lexical-sample case, but it
is significant at the 0.99 level (significance is 1−p-value). In order to explore the reasons
for this, we performed further experiments separating the words in different sets according
to their frequency in Semcor, as reported below in Section 6.4.
6.2 Comparison with Minimally Supervised Systems in Senseval-3
There were four systems in Senseval that used Semcor to estimate the sense distribution,
without using the examples of each word for training. We show the performance of these
systems, together with our own and the most frequent sense baseline in Table 9.
The results show that the SenseCorpus examples are able to obtain the best performance
of this kind of systems, well above the rest. The improvement over the Semcor MFS baseline
is significant at the 0.96 level.
6.3 Comparison with Supervised Systems in Senseval-3
Most of the systems that participated in the all-words task were supervised systems that
relied mainly on Semcor. In Table 10 we present the results of the top 10 competing systems
and our system, trained on SenseCorpus and Semcor. We also include the DL system when
trained only in Semcor, as a baseline.
The results show that using SenseCorpus we are able to obtain a significant improvement
of 2.9% points in F-score over the baseline. This score places our system as second, close
98

On the Use of Automatically Acquired Examples for All-Nouns WSD

Code
SenseLearner
SenseCorpus-S
LCCaw
kuaw.ans
R2D2English
GAMBL-AW
upv-eaw.upv-eaw2
Meaning
upv-eaw.upv-eaw
Prob5
Semcor baseline
UJAEN2

Method
Syntactic Patterns
DL

Ensemble
Optim.,TiMBL
Ensemble

DL

Attempt.
695
695
695
695
695
695
695
695
695
691
695
695

Prec.
65.9
65.3
65.3
64.8
64.5
63.3
63.3
63.2
62.9
62.8
62.4
62.4

Rec.
65.9
65.3
65.3
64.7
64.5
63.3
63.3
63.2
62.9
62.4
62.4
62.4

F
65.9
65.3
65.3
64.7
64.5
63.3
63.3
63.2
62.9
62.6
62.4
62.4

p-value
0.313
–
0.166
0.115
0.054
0.013
0.014
0.009
0.007
0.007
0.006
0.002

Table 10: Performance of the top 10 supervised systems participating in Senseval-3 allwords for the 695 polysemous nouns, accompanied by p-values of the one tailed
paired t-test with respect to SenseCorpus-S (in bold).

to the best system for all-nouns. The statistical significance tests score below 90% for the
top 4 systems, and over 95% for the rest of systems. This means that our system performs
similar to the top three systems, but significantly better than the rest.
6.4 Analysis of the Performance by Word Frequency
In previous sections we observed that different words achieve different rates of accuracy.
For instance, the lexical-sample experiments showed that the precision of the unsupervised
system ranged between 12.5% and 87.5% (cf. Table 5). Clearly, there are some words
whose performance is very low when using SenseCorpus. In this section, we will group the
nouns in the Senseval-3 all-nouns task according to their frequency to see whether there is
a correlation between the frequency of the words and the performance of our system. Our
goal is to identify sets of words that can be disambiguated with higher accuracy by this
method. This process would allow us to previously detect the type of words our system can
be applied to, thus providing a better tool to work in combination with other WSD systems
that exploit other properties of language.
For this study, we created separate word sets according to their frequency of occurrence
in Semcor. Table 11 shows the different word-sets, with their frequency ranges, the number
of nouns in each range, and the average polysemy. We can see that the most frequent words
tend to be also the most polysemous. In the case of supervised systems, polysemy and
number of training examples tend to compensate each other, yielding good results for those
kinds of words. That is, polysemous words are more difficult to disambiguate, but they also
have more examples to train in Semcor (Agirre & Martinez, 2000).
Table 12 shows the results for different frequency ranges for the top unsupervised systems in Senseval-3, together with our method. We can see that for all the systems the
performance is very low in the high-frequency range. The best performing system (IRSTDDD-LSI) profits from the use of a threshold and leaves many of these instances unanswered.
Regarding the improvement of SenseCorpus-U over the Automatic Ranking baseline (Au99

Martinez, Lopez de Lacalle & Agirre

Range
0–10
11–20
21–40
41–60
61–80
81–100
101–
Overall

#Nouns
207
101
89
88
54
31
125
695

Avg. Polysemy
3.6
5.1
6.1
6.6
6.9
9.3
9.6
5.4

Table 11: Number of noun occurrences in each of the frequency ranges (in Semcor), with
average polysemy.

0–10
11–20
21–40
41–60
61–80
81–100
101–
overall

DLSI-UA
Att.
F-sc.
188
35.98
96
34.50
75
15.82
82
19.97
54
22.20
31
9.70
122
24.30
648
26.83

IRST-DDD-LSI
Att.
F-sc.
195
67.13
91
69.77
81
57.65
75
55.21
50
57.69
19
36.02
59
35.85
570
58.22

SenseCorpus-U
Att.
F-sc.
198
62.77
98
58.90
89
25.50
85
42.84
54
35.80
31
23.70
125
29.60
680
45.00

AutoPS
Att.
F-sc
198
62.68
98
49.25
86
26.24
85
42.75
54
31.50
31
29.00
123
31.44
675
43.95

Table 12: Results of each of the unsupervised systems in Senseval-3 all words, as evaluated
on the nouns in each Semcor frequency range. Att. stands for number of words
attempted at each range. Best F-score per system given in bold.

toPS), the best results are obtained in the low-frequency range (0-20), when the baseline
scores in the 50-60% F-score range. The results of SenseCorpus are lower than the baseline
for words with frequency higher than 80. This suggests that the system is more reliable for
low-frequency words, and a simple threshold that takes into account the frequency of words
would be indicative of the performance we can expect. The same behavior is also apparent
in the other unsupervised systems, which shows that this is a weak spot for this kind of
systems. We think that future research should focus on those high frequency words.

7. Discussion
In this work we have implemented and evaluated an all-words WSD system for nouns
that is able to reach state-of-the-art performance in all three supervised, unsupervised and
semi-supervised settings. We have produced different systems combining SenseCorpus with
different priors and the actual examples from Semcor. The supervised system, trained
with both hand-tagged (Semcor) and automatically obtained corpora, reaches an F-score of
65.3%, and would rank second in the Senseval-3 all-nouns test data. The semi-supervised
system, using the priors from Semcor but no manually-tagged examples, would rank first
100

On the Use of Automatically Acquired Examples for All-Nouns WSD

on its class, and the unsupervised system second. In all cases, SenseCorpus improves over
the baselines.
The results are remarkable. If we compare our system to those which came out first
in the unsupervised and supervised settings, we see that each of them uses a completely
different strategy. On the contrary, our system, using primarily the automatically acquired
examples, is able to perform in the top ranks in all three settings.
In any case, a deep gap exists among the following three kinds of systems: (i) Supervised
systems with specific training (e.g. Senseval lexical-sample systems), (ii) Supervised systems
with all-words training (e.g. those trained using Semcor), and (iii) Unsupervised systems.
Our algorithm has been implemented as an all-words supervised system, and as an unsupervised system. Although our implementations obtain state-of-the-art performance in their
categories, there are different issues that could be addressed in order to close these gaps,
and make all-words unsupervised performance closer to those of the supervised systems.
We identified three main sources of error: the low quality of the relatives applied to
some words, the different distributions of senses in training and testing data, and the low
performance on high-frequency (and highly polysemous) words. We examine each of them
in turn.
The algorithm suffers from the noise introduced by relatives that are far from the target
word, and do not share the local context with it. Better filtering would be required to
alleviate this problem, and one way to do this could be to retrieve examples only when they
share part of the local context with the target word and discard other examples. Another
interesting aspect of this problem would be to identify the type of words that achieve low
performance with SenseCorpus. We already observed that high-frequency words obtain low
performance, and another study on performance according to the type of relatives would
be useful for a better application of the algorithm.
In order to deal with words that do not have close WordNet relatives, another source
of examples would be to use distributionally similar words. The words would be obtained
by methods such as the one presented by Lin (1998), and the retrieved examples would be
linked to the target senses using the WordNet similarity package (Patwardhan & Pedersen,
2003).
The second main problem of systems that rely on automatic acquisition is the fact that
the sense distributions in training and test data can be very different, and this seriously
affects the performance. Our system relies on an automatically-obtained sense ranking to
alleviate this problem. However, some words still get too many examples for senses that are
not relevant in the domain. In preliminary experiments, we observed the benefit of using
heuristics to filter out these senses, such as using the number of close relatives in WordNet,
with promising results.
Finally, a third problem is observed in Section 6.4, which is the fact that high-frequency
words do not profit from automatically acquired examples. For most unsupervised methods,
these frequent (and very polysemous) words get low performances, and threshold-based
systems usually discard answering them. A straightforward way to improve the F-score of
our system would be to apply a threshold to discard these words and apply another method
or back-off strategy on them.
All in all, detecting the limitations of the system can give us important clues to work
towards an accurate unsupervised all-words system. The literature shows that no single
101

Martinez, Lopez de Lacalle & Agirre

unsupervised system is able to perform well for all words. If we were able to identify the type
of words that were more suited to different algorithms and heuristics, the integration of these
algorithms into one single combined system could be the way to go. For instance, we could
detect in which cases the relatives of a target word are too different to apply the SenseCorpus
approach, or the cases where the automatic ranking has not enough evidence. We have also
observed that simple heuristics such as the number of close relatives in WordNet can be
successfully applied to some sets of words. Meta-learning techniques (Vilalta & Drissi, 2002)
could be very useful to exploit the strengths of unsupervised systems.

8. Conclusions and Future Work
This paper presents evidence showing that a proper use of automatically acquired examples
allows for state-of-the-art performance on WSD of nouns. We have gathered examples for all
nouns in WordNet 1.6 in a resource called SenseCorpus, amounting to 150 million examples,
and made this resource publicly available to the community.
We have used the examples to train a supervised WSD system, in a variety of settings: on
its own, combined with prior information coming from different sources, or combined with
training examples from Semcor. Depending on the knowledge used, we are able to build,
respectively, an unsupervised system that has not seen any hand-labeled training data, a
semisupervised one that only sees the priors in a generic hand-labeled corpus (Semcor),
and a fully-supervised system that also uses the generic hand-labeled corpus (Semcor) as
training data.
The evaluation in both lexical-sample and all-words settings has shown that SenseCorpus
improves over commonly used baselines in all combinations, and achieves state-of-the-art
performance in the all-words Senseval-3 evaluation set for nouns. Previous work on automatic example acquisition has been evaluated on a handful of words. In contrast we have
shown that we are able to scale up to all nouns producing excellent results. In the way, we
have learned that the use of the prior of the senses is crucial to apply the acquired examples
effectively.
In the discussion we have outlined different ways to overcome the limitations of our
system, and each of the proposed lines could improve significantly the current performance.
Although the recent literature shows that there is no unsupervised system that performs
with high precision for all words, we believe that the different systems complement each
other, as they usually perform well for different sets of words. From a meta-learning perspective, we could build a word-expert system that is able to apply the best knowledge
source for the problem: SenseCorpus, hand-tagged examples, simple heuristics, or other
unsupervised algorithms that can be incorporated.
For future work, aside from the proposed improvements, we think that it would be
interesting to apply the method to other testbeds. In order to be applied, the monosemous
relative method requires an ontology and a raw corpus. Such resources can be found in many
specific domains, such as Biomedicine, that do not have the fine-grainedness of WordNet,
and could lead to more practical applications.
102

On the Use of Automatically Acquired Examples for All-Nouns WSD

Acknowledgments
This work has been partially financed by the Ministry of Education (KNOW project, ICT2007-211423) and the Basque Government (consolidated research groups grant, IT-397-07).
Oier Lopez de Lacalle was supported by a PhD grant from the Basque Government. David
Martinez was funded by the Australian Research Council, grant no. DP0663879.

References
Abney, S. (2002). Bootstrapping. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL’02, Philadelphia.
Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2000). Enriching very large ontologies using
the WWW. In Proceedings of the Ontology Learning Workshop, organized by ECAI,
Berlin (Germany).
Agirre, E., Ansa, O., Martinez, D., & Hovy, E. (2001). Enriching WordNet concepts with
topic signatures. In Procceedings of the SIGLEX workshop on WordNet and Other
Lexical Resources: Applications, Extensions and Customizations. In conjunction with
NAACL.
Agirre, E., & Edmonds, P. (Eds.). (2006). Word Sense Disambiguation: Algorithms and
Applications. Springer.
Agirre, E., & Lopez de Lacalle, O. (2004). Publicly available topic signatures for all WordNet nominal senses. In Proceedings of the 4th International Conference on Language
Resources and Evaluation (LREC), Lisbon, Portugal.
Agirre, E., & Martinez, D. (2000). Exploring automatic word sense disambiguation with Decision Lists and the Web. In Procedings of the COLING 2000 Workshop on Semantic
Annotation and Intelligent Content, Luxembourg.
Agirre, E., & Martinez, D. (2004a). Smoothing and word sense disambiguation. In Proceedings of Expaa for Natural Language Processing (EsTAL), Alicante, Spain.
Agirre, E., & Martinez, D. (2004b). Unsupervised WSD based on automatically retrieved
examples: The importance of bias. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, Barcelona, Spain.
Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training.
In Proceedings of the 11h Annual Conference on Computational Learning Theory, pp.
92–100, New York. ACM Press.
Chan, Y., & Ng, H. (2005). Scaling up word sense disambiguation via parallel texts. In
Proceedings of the 20th National Conference on Artificial Intelligence (AAAI 2005),
Pittsburgh, Pennsylvania, USA.
Collins, M., & Singer, Y. (1999). Unsupervised models for named entity classification.
In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, EMNLP/VLC’99, College Park, MD,
USA.
103

Martinez, Lopez de Lacalle & Agirre

Cuadros, M., Padró, L., & Rigau, G. (2006). An empirical study for automatic acquisition
of topic signatures. In Proceedings of Third International WordNet Conference, Jeju
Island (Korea).
Cuadros, M., & Rigau, G. (2006). Quality assessment of large scale knowledge resources. In
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pp. 534–541, Sydney, Australia. Association for Computational Linguistics.
Daconta, M., Obrst, L., & Smith, K. (2005). The Semantic Web: A Guide to the Future of
XML, Web Services, and Knowledge Management. John Wiley & Sons.
Dale, R., Moisl, H., & Somers, H. (2000). Handbook of Natural Language Processing. Marcel
Dekker Inc.
Daude, J., Padró, L., & Rigau, G. (2000). Mapping WordNets using structural information.
In 38th Anual Meeting of the Association for Computational Linguistics (ACL’2000),
Hong Kong.
Edmonds, P., & Kilgarriff, A. (2002). Natural Language Engineering, Special Issue on Word
Sense Disambiguation Systems. No. 8 (4). Cambridge University Press.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Fernandez-Amoros, D., Gonzalo, J., & Verdejo, F. (2001). The UNED systems at Senseval2. In Proceedings of the SENSEVAL-2 Workshop. In conjunction with ACL, Toulouse,
France.
Fujii, A., Inui, K., Tokunaga, T., & Tanaka, H. (1998). Selective sampling for example-based
word sense disambiguation. In Computational Linguistics, No. 24 (4), pp. 573–598.
Humphreys, L., Lindberg, D., Schoolman, H., & Barnett, G. (1998). The Unified Medical
Language System: An informatics research collaboration. Journal of the American
Medical Informatics Association, 1 (5).
Ide, N., & Veronis, J. (1998). Introduction to the special issue on word sense disambiguation:
The state of the art. Computational Linguistics, 24 (1), 1–40.
Jurafsky, D., & Martin, J. (2000). An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice-Hall, Upper Saddle River,
NJ 07458.
Kim, S.-B., Seo, H.-C., & Rim, H.-C. (2004). Information retrieval using word senses: root
sense tagging approach. In SIGIR ’04: Proceedings of the 27th annual international
ACM SIGIR conference on Research and development in information retrieval, pp.
258–265, New York, NY, USA. ACM.
Kohomban, U., & Lee, W. (2005). Learning semantic classes for word sense disambiguation. In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics (ACL’05).
Leacock, C., Chodorow, M., & Miller, G. A. (1998). Using corpus statistics and WordNet
relations for sense identification. In Computational Linguistics, Vol. 24, pp. 147–165.
Lin, D. (1998). Automatic retrieval and clustering of similar words. In Proceedings of
COLING-ACL, Montreal, Canada.
104

On the Use of Automatically Acquired Examples for All-Nouns WSD

Liu, S., Liu, F., Yu, C., & Meng, W. (2004). An effective approach to document retrieval
via utilizing WordNet and recognizing phrases. In SIGIR ’04: Proceedings of the
27th annual international ACM SIGIR conference on Research and development in
information retrieval, pp. 266–272, New York, NY, USA. ACM.
Manning, C. D., & Schütze, H. (1999). Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.
McCarthy, D., Koeling, R., Weeds, J., & Carroll, J. (2004). Finding predominant word senses
in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for
Computational Linguistics (ACL), Barcelona, Spain.
Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. In Proceedings of the
3rd International Conference on Language Resources and Evaluation (LREC), Las
Palmas, Spain.
Mihalcea, R. (2004). Co-training and self-training for word sense disambiguation. In Proceedings of the Conference on Natural Language Learning (CoNLL 2004), Boston,
USA.
Mihalcea, R., & Chklovski, T. (2003). Open Mind Word Expert: Creating large annotated
data collections with Web users’ help. In Proceedings of the EACL 2003 Workshop
on Linguistically Annotated Corpora (LINC 2003), Budapest, Hungary.
Mihalcea, R., Chklovski, T., & Killgariff, A. (2004). The Senseval-3 English lexical sample
task. In Proceedings of the 3rd ACL workshop on the Evaluation of Systems for the
Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Mihalcea, R., & Edmonds, P. (2004). Senseval-3, Third International Workshop on the
Evaluation of Systems for the Semantic Analysis of Text. The Association for Computational Linguistics.
Mihalcea, R., & Moldovan, D. (1999). An automatic method for generating sense tagged
corpora. In Proceedings of AAAI-99, Orlando, FL.
Miller, G. A., Leacock, C., Tengi, R., & Bunker, R. (1993). A semantic concordance.
In Proceedings of the ARPA Human Language Technology Workshop, pp. 303–308,
Princeton, NJ. distributed as Human Language Technology by San Mateo, CA: Morgan
Kaufmann Publishers.
Ngai, G., & Florian, R. (2001). Transformation-Based Learning in the fast lane. In Proceedings of the Second Conference of the North American Chapter of the Association
for Computational Linguistics, pp. 40–47, Pittsburgh, PA, USA.
Patwardhan, S., & Pedersen, T. (2003). The cpan wordnet::similarity package.
http://search.cpan.org/author/SID/WordNet-Similarity-0.03/.

In

Pedersen, T. (2001). A decision tree of bigrams is an accurate predictor of word sense. In
Proceedings of the Second Meeting of the North American Chapter of the Association
for Computational Linguistics (NAACL-01), Pittsburgh, PA.
Pham, T. P., Ng, H. T., & Lee, W. S. (2005). Word sense disambiguation with semisupervised learning. In Proceedings of the 20th National Conference on Artificial
Intelligence (AAAI 2005), pp. 1093–1098, Pittsburgh, Pennsylvania, USA.
105

Martinez, Lopez de Lacalle & Agirre

Ravin, Y., & Leacock, C. (2001). Polysemy: Theoretical and Computational Approaches.
Oxford University Press.
Resnik, P. (2006). Word sense disambiguation in natural language processing applications.
In Agirre, E., & Edmonds, P. (Eds.), Word Sense Disambiguation, chap. 11, pp. 299–
337. Springer.
Snyder, B., & Palmer, M. (2004). The English all-words task. In Proceedings of the 3rd
ACL workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Stevenson, M. (2003). Word Sense Disambiguation: The Case for Combining Knowledge
Sources. CSLI Publications, Stanford, CA.
Stevenson, M., & Clough, P. (2004). Eurowordnet as a resource for cross-language information retrieval. In Proceedings of the Fourth International Conference on Language
Resources and Evaluation, Lisbon, Portugal.
Strapparava, C., Gliozzo, A., & Giuliano, C. (2004). Pattern abstraction and term similarity
for word sense disambiguation: IRST at Senseval-3. In Proceedings of the 3rd ACL
workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL), Barcelona, Spain.
Tugwell, D., & Kilgarriff, A. (2001). WASP-Bench: a lexicographic tool supporting word
sense disambiguation. In Proceedings of the SENSEVAL-2 Workshop. In conjunction
with ACL-2001/EACL-2001, Toulouse, France.
Vickrey, D., Biewald, L., Teyssier, M., & Koller, D. (2005). Word-sense disambiguation for
machine translation. In Proceedings of Human Language Technology Conference and
Conference on Empirical Methods in Natural Language Processing.
Vilalta, R., & Drissi, Y. (2002). A perspective view and survey of meta-learning. In Artificial
Intelligence Review, No. 18 (2), pp. 77–95.
Vossen, P., Rigau, G., Alegrı́a, I., Agirre, E., Farwell, D., & Fuentes, M. (2006). Meaningful
results for information retrieval in the MEANING project. In Proceedings of Third
International WordNet Conference, Jeju Island, Korea.
Wang, X., & Carroll, J. (2005). Word sense disambiguation using sense examples automatically acquired from a second language. In Proceedings of the joint Human Language
Technologies and Empirical Methods in Natural Language Processing conference, Vancouver, Canada.
Wang, X., & Martinez, D. (2006). Word sense disambiguation using automatically translated sense examples. In Proceedings of EACL 2006 Workshop on Cross Language
Knowledge Induction, Trento, Italy.
Weeber, M., Mork, J., & Aronson, A. (2001). Developing a test collection for biomedical
word sense disambiguation. In Proceedings of AMIA Symposium, pp. 746–750.
Yarowsky, D. (1994). Decision lists for lexical ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics, pp. 88–95, Las Cruces, NM.
106

On the Use of Automatically Acquired Examples for All-Nouns WSD

Yarowsky, D. (1995). Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd Annual Meeting of the Association for Computational
Linguistics, pp. 189–196, Cambridge, MA.
Yarowsky, D., & Florian, R. (2003). Evaluating sense disambiguation across diverse parameter spaces. Natural Language Engineering, 8 (2), 293–310.

107

Journal of Artificial Intelligence Research 33 (2008) 179-222

Submitted 01/08; published 10/08

A Rigorously Bayesian Beam Model and an Adaptive Full
Scan Model for Range Finders in Dynamic Environments
Tinne De Laet
Joris De Schutter
Herman Bruyninckx

tinne.delaet@mech.kuleuven.be
joris.deschutter@mech.kuleuven.be
herman.bruyninckx@mech.kuleuven.be

Department of Mechanical Engineering
Katholieke Universiteit Leuven
Celestijnenlaan 300B, box 2420, 3001 Heverlee, Belgium

Abstract
This paper proposes and experimentally validates a Bayesian network model of a range
finder adapted to dynamic environments. All modeling assumptions are rigorously explained, and all model parameters have a physical interpretation. This approach results in
a transparent and intuitive model. With respect to the state of the art beam model this
paper: (i) proposes a different functional form for the probability of range measurements
caused by unmodeled objects, (ii) intuitively explains the discontinuity encountered in the
state of the art beam model, and (iii) reduces the number of model parameters, while maintaining the same representational power for experimental data. The proposed beam model
is called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood and
a variational Bayesian estimator (both based on expectation-maximization) are proposed
to learn the model parameters.
Furthermore, the RBBM is extended to a full scan model in two steps: first, to a
full scan model for static environments and next, to a full scan model for general, dynamic
environments. The full scan model accounts for the dependency between beams and adapts
to the local sample density when using a particle filter. In contrast to Gaussian-based state
of the art models, the proposed full scan model uses a sample-based approximation. This
sample-based approximation enables handling dynamic environments and capturing multimodality, which occurs even in simple static environments.

1. Introduction
In a probabilistic approach, inaccuracies are embedded in the stochastic nature of the model,
particularly in the conditional probability density representing the measurement process.
It is of vital importance that all types of inaccuracies affecting the measurements are incorporated in the probabilistic sensor model. Inaccuracies arise from sensor limitations, noise,
and the fact that most complex environments can only be represented and perceived in a
limited way. The dynamic nature of the environment in particular is an important source
of inaccuracies. This dynamic nature results from the presence of unmodeled and possibly
moving objects and people.
This paper proposes a probabilistic range finder sensor model for dynamic environments.
Range finders, which are widely used in mobile robotics, measure the distances z to objects
in the environment along certain directions θ relative to the sensor. We derive the sensor
c
2008
AI Access Foundation. All rights reserved.

De Laet, De Schutter & Bruyninckx

model in a form suitable for mobile robot localization, i.e.: P (Z = z | X = x, M = m)1 ,
where Z indicates the measured range, X the position of the mobile robot (and of the
sensor mounted on it), and M the environment map. The presented model is however
useful in other applications of range sensors as well.
First, this paper derives a probabilistic sensor model for one beam of a range finder,
i.e. the beam model. In particular, this paper gives a rigorously Bayesian derivation using a Bayesian network model while stating all model assumptions and giving a physical
interpretation for all model parameters. The obtained model is named RBBM, short for
Rigorously Bayesian Beam Model. The innovations of the presented approach are (i) to
introduce extra state variables A = a for the positions of unmodeled objects in the probabilistic sensor model P (z | x, m, a), and (ii) to marginalize out these extra state variables
from the total probability before estimation. The latter is required because extra variables
(exponentially!) increase the computational complexity of state estimation while in a lot
of applications estimating the position of unmodeled objects is not of primary interest. In
summary, the marginalization avoids the increase in complexity to infer the probability
distributions P (x) and P (m), while maintaining the modeling of the dynamic nature of
the environment.
This paper furthermore presents a maximum-likelihood and a variational Bayesian estimator (both based on expectation-maximization) to learn the model parameters of the
RBBM.
Next, the paper presents an extension of the RBBM to a full scan model i.e.: P (z | θ, x, m)
where z and θ contain all the measured distances and beam angles, respectively. This full
scan model accounts for the dependency between beams and adapts to the local sample
density when using a particle filter. In contrast to Gaussian-based state of the art models,
the proposed full scan model uses a sample-based approximation. The sample-based approximation allows us to capture the multi-modality of the full scan model, which is shown
to occur even in simple static environments.
1.1 Paper Overview
This paper is organized as follows. Section 2 gives an overview of the related work. Section 3
(i) presents a Bayesian beam model for range finders founded on Bayesian networks, the
RBBM, (ii) mathematically derives an analytical formula for the probabilistic sensor model
while clearly stating all assumptions, (iii) provides useful insights in the obtained beam
model and (iv) shows that the obtained analytical sensor model agrees with the proposed
Bayesian network. Section 4 presents a maximum likelihood and a variational Bayesian
estimator (both based on expectation-maximization) to learn the model parameters. In
Section 5 the model parameters of the RBBM are learned from experimental data and the
resulting model is compared with the state of the art beam model proposed by Thrun,
Burgard, and Fox (2005), further on called Thrun’s model. Section 6 extends the RBBM
to an adaptive full scan model for dynamic environments. Section 7 discusses the obtained
RBBM and adaptive full scan model and compares them with previously proposed range
finder sensor models.
1. To simplify notation, the explicit mention of the random variable in the probabilities is omitted whenever
possible, and replaced by the common abbreviation P (x) instead of writing P (X = x).

180

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

2. Related Work
Three basic approaches to deal with dynamic environments exist in the literature (Fox,
Burgard, & Thrun, 1999; Thrun et al., 2005): state augmentation, adapting the sensor
model and outlier detection.
In state augmentation the latent states, e.g. the position of moving objects and people in the environment, are included in the estimated states. Wang, Thorpe, and Thrun
(2003) developed an algorithm ‘SLAM with DATMO’, short for SLAM with the detection
and tracking of moving objects. State augmentation however is often infeasible since the
computational complexity of state estimation increases exponentially with the number of
independent state variables to estimate. A closely related solution consists of adapting the
map according to the changes in the environment. Since such approaches assume that the
environment is almost static, they are unable to cope with real dynamics as in populated
environments (Fox et al., 1999). A more recent, related approach proposed by Wolf and
Sukhatme (2004) maintains two coupled occupancy grids of the environment, one for the
static map and one for the moving objects, to account for environment dynamics.
Probabilistic approaches are to some extent robust to unmodeled dynamics, since they
are able to deal with sensor noise. In such approaches however, the sensor noise should
reflect the real uncertainty due to the unmodeled dynamics of the environment. Therefore,
a second approach for dealing with dynamic environments is to adapt the sensor model to
correctly reflect situations in which measurements are affected by the unmodeled environment dynamics. Fox et al. (1999) show that such approaches are only capable to model such
noise on average, and, while these approaches work reliably with occasional sensor blockage,
they are inadequate in situations where more than fifty percent of the measurements are
corrupted.
To handle measurement corruption more effectively, an approach based on outlier detection can be used. This approach uses an adapted sensor model, as explained in the
previous paragraph. The idea is to investigate the cause of a sensor measurement and to
reject measurements that are likely to be affected by unmodeled environment dynamics.
Hähnel, Schulz, and Burgard (2003a) and Hähnel, Triebel, Burgard, and Thrun (2003b)
studied the problem of performing SLAM in environments with many moving objects using
the EM algorithm for filtering out affected measurements. By doing so, they were able to
acquire maps in the environment where conventional SLAM techniques failed. Fox et al.
(1999) propose two different kinds of filters: an entropy filter, suited for an arbitrary sensor, and a distance filter, designed for proximity sensors. These filters detect whether a
measurement is corrupted or not, and discard sensor readings resulting from objects that
are not contained in the map.
This paper focuses on (sonar and laser) range finders, whose physical principle is the
emission of a sound or light wave, followed by the recording of its echo. Highly accurate
sensor models would include physical parameters such as surface curvature and material
absorption coefficient. These parameters are however difficult to estimate robustly in unstructured environments. Hence, the literature typically relies on purely basic geometric
models.
The range finder sensor models available from the literature are traditionally divided
in three main groups: feature-based approaches, beam-based models and correlation-based
181

De Laet, De Schutter & Bruyninckx

methods. Feature-based approaches extract a set of features from the range scan and match
them to features contained in an environmental model. Beam-based models, also known
as ray cast models, consider each distance measurement along a beam as a separate range
measurement. These models represent the one-dimensional distribution of the distance
measurement by a parametric function, which depends on the expected range measurement
in the respective beam directions. In addition, these models are closely linked to the geometry and the physics involved in the measurement process. They often result in overly
peaked likelihood functions due to the underlying assumption of independent beams. The
last group of range finder sensor models, correlation-based methods, build local maps from
consecutive scans and correlate them with a global map. The simple and efficient likelihood
field models or end point model (Thrun, 2001) are related to these correlation-based methods. Plagemann, Kersting, Pfaff, and Burgard (2007) nicely summarize the advantages and
drawbacks of the different range finder sensor models.
Range finder sensor models can also be classified according to whether they use discrete
geometric grids (Hähnel et al., 2003a, 2003b; Fox et al., 1999; Burgard, Fox, Hennig, &
Schmidt, 1996; Moravec, 1988) or continuous geometric models (Thrun et al., 2005; Choset,
Lynch, Hutchinson, Kantor, Burgard, Kavraki, & Thrun, 2005; Pfaff, Burgard, & Fox,
2006). Moravec proposed non-Gaussian measurement densities over a discrete grid of possible distances measured by sonar; the likelihood of the measurements has to be computed for
all possible positions of the mobile robot at a given time. Even simplified models (Burgard
et al., 1996) in this approach turned out to be computationally too expensive for real-time
application. Therefore, Fox et al. proposed a beam model consisting of a mixture of two
physical causes for a measurement: a hit with an object in the map, or with an object
not yet modeled in the map. The last cause accounts for the dynamic nature of the environment. An analogous mixture (Thrun et al., 2005; Choset et al., 2005) adds two more
physical causes: a sensor failure and an unknown cause resulting in a ‘max-range’ measurement and a ‘random’ measurement, respectively. While Thrun et al. and Pfaff et al. use
a continuous model, Choset et al. present the discrete analog of the mixture, taking into
account the limited resolution of the range sensor. Pfaff et al. extend the basic mixture
model for use in Monte Carlo localization. To overcome problems due to the combination of
the limited representational power and the peaked likelihood of the accurate range finder,
they propose an adaptive likelihood model. The likelihood model is smooth during global
localization and more peaked during tracking.
Recently, different researchers tried to tackle the problems associated with beam-based
models, caused by the independence assumptions between beams. Plagemann et al. (2007)
propose a sensor model for the full scan. The model treats the sensor modeling task as a
non-parametric Bayesian regression problem, and solves it using Gaussian processes. It is
claimed that the Gaussian beam processes combine the advantages of the beam-based and
the correlation-based models. Due to the underlying assumption that the measurements
are jointly Gaussian distributed, the Gaussian beam processes are not suited to take into
account the non-Gaussian uncertainty due to the dynamic nature of the environment. An
alternative approach to handle the overly-peaked likelihood functions resulting from the
traditional beam models is proposed by Pfaff, Plagemann, and Burgard (2007). A locationdependent full scan model takes into account the approximation error of the sample-based
representation, and explicitly models the correlations between individual beams introduced
182

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

by the pose uncertainty. The measurements are assumed to be jointly Gaussian distributed
just as Plagemann et al. proposed. While Plagemann et al. represent the covariance
matrix as a parametrized covariance function using Gaussian processes whose parameters
are learned from data, Pfaff et al. learn the full covariance matrix being less restrictive in
this manner. Despite the modeled correlation between beams, the measurements are still
assumed to be jointly Gaussian distributed, which again limits the applicability in dynamic
environments.
This paper proposes a rigorously Bayesian modeling of the probabilistic range sensor
beam model for dynamic environments, referred to as RBBM. Similar to the work of Thrun
et al. (2005) and Pfaff et al. (2006) the sensor model is derived for a continuous geometry.
Unlike previous models of Thrun et al. (2005), Pfaff et al. (2006), Fox et al. (1999) and
Choset et al. (2005), the mixture components are founded on a Bayesian modeling. This
modeling makes use of probabilistic graphical models, in this case Bayesian networks. Such
graphical models provide a simple way to visualize the structure of a probabilistic model,
and can be used to design and motivate new models (Bishop, 2006). By inspection of the
graph, insights of the model, including conditional independence properties are obtained.
Next, inspired by the adaptive full scan models in the literature (Pfaff et al., 2006, 2007;
Plagemann et al., 2007), the RBBM is extended to an adaptive full scan model. The
underlying sample-based approximation of the full scan model, in contrast to the Gaussianbased approximation proposed by Pfaff et al. (2007) and Plagemann et al., enables handling
dynamic environments and capturing multi-modality, which occurs even in simple static
environments.

3. Beam Model
We model the probabilistic beam model P (Z = z | X = x, M = m) for dynamic environments as a Bayesian network. We introduce extra state variables A = a for the positions
of unmodeled objects in the probabilistic sensor model P (z | x, m, a). To prevent an exponential increase of the computational complexity of the state estimation due to the extra
variables, these variables are marginalized out from the total probability before estimation.
The marginalization:

P (z | x, m) =

Z

P (z | x, m, a) P (a) da,

(1)

a

avoids increasing the complexity to infer the conditional probability distributions of interest,
P (x) and P (m), while it maintains the modeling of the dynamic nature of the environment.
Section 3.1 explains which extra state variables are physically relevant, while Section 3.3
explains the marginalization of these extra state variables. Section 3.5 summarizes all assumptions and approximations. Finally, Section 3.6 provides useful insights in the obtained
beam model, called RBBM, and in its derivation. Section 3.7 shows that the obtained analytical expression for the RBBM agrees with the proposed Bayesian network by means of
a Monte Carlo simulation.
183

De Laet, De Schutter & Bruyninckx

p

X

N

M

XN j
n
K

XKi
k

⋆
Zoccl

Z

σm

Figure 1: The Bayesian network for the probabilistic measurement model supplemented
with the deterministic parameters represented by the smaller solid nodes. A
compact representation with plates (the rounded rectangular boxes) is used. A
plate represents a number, indicated in the lower right corner, of independent
nodes of which only a single example is shown explicitly.

184

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

3.1 Bayesian Model
Bayesian networks graphically represent probabilistic relationships between variables in a
mathematical model, to structure and facilitate probabilistic inference computations with
those variables (Jensen & Nielsen, 2007; Neapolitan, 2004). A Bayesian network is defined
as follows: (i) a set of nodes, each with an associated random variable, connected by directed
edges forming a directed acyclic graph (DAG); (ii) each discrete (continuous) random variable has a finite (infinite) set of mutually exclusive states; (iii) each random variable A with
parents B1 , . . . , BN has a conditional probability distribution P (A | B1 , . . . , Bn ) (known as
conditional probability table in the case of discrete variables). Although the definition of
Bayesian networks does not refer to causality, and there is no requirement that the directed
edges represent causal impact, a well-known way of structuring variables for reasoning under
uncertainty is to construct a graph representing causal relations (Jensen & Nielsen, 2007).
In this case the graphical models are also known as generative models (Bishop, 2006), since
they capture the causal process generating the random variables.
In this application, the range sensor ideally measures z ⋆ , the distance to the closest
object in the map. An unknown number n of unmodeled objects, possibly preventing the
measurement of the closest object in the map, are however present in the environment.
Depending on the position of the jth unmodeled object along the measurement beam, xN j ,
the unmodeled object occludes the map or not. The unmodeled object only occludes the
map if it is located in front of the closest object contained in the map. k is the total number
of occluding objects out of the n unmodeled objects. The positions of these occluding
objects on the measurement beam are denoted by {xKi }i=1:k . If the map is occluded by an
⋆
unmodeled object, the range sensor will ideally measure zoccl
= xKc , with xKc the position
of the closest occluding object.
The following extra state variables, a in Eq. (1), are included in the Bayesian model: N
is the discrete random variable indicating the unknown number of unmodeled objects in the
environment; XN j is the continuous random variable for the position of the jth unmodeled
object on the measurement beam; K is the discrete random variable indicating the number
of objects occluding the measurement of the map; XKi is the continuous random variable
⋆
for the position of the ith occluding object on the measurement beam; and Zoccl
is the
continuous random variable indicating the ideal range measurement of the closest occluding
object. Fig. 1 shows the Bayesian network for the probabilistic range finder sensor model
with the variables Z, X and M that occur in the probabilistic sensor model (defined in
⋆
and the
Section 1), all the extra variables N, XN = {XN j }j=1:n , K, XK = {XKi }i=1:k , Zoccl
model parameters p and σm (defined in Section 3.2).
The directed edges in the graphical model represent causal relationships between the
variables. For example, X and M unambiguously determine the measured range Z for
a perfect sensor in the absence of unmodeled occluding objects. The number of occluding
objects K depends on the total number N of unmodeled objects and their positions XN with
respect to the measurement beam. X and M also have a causal impact on K: the larger the
expected measurement z ⋆ , the higher the possibility that one or more unmodeled objects are
occluding the modeled object corresponding to the expected measurement. The positions
along the measurement beam XK of the occluding objects are equal to the positions of
the K of N unmodeled objects occluding the map. Therefore, random variables XK are
185

De Laet, De Schutter & Bruyninckx

P (n)
0.3
0.2
0.1

0 1 2 3 4 5 6 7 8 9 10

n

Figure 2: P (n) (Eq. (2)) for p = 0.65.

not only influenced by K but also by XN . Since the K objects are occluding the map,
their positions along the measurement beam are limited to the interval [0, z ⋆ ], so XK has a
⋆
causal dependency on X and M . The ideal measurement zoccl
of an occluding object is the
⋆
position of the occluding object closest to the sensor, so Zoccl depends on the positions XK
of the occluding objects. Finally, measurement Z also depends on the ideal measurement
⋆
of the occluding object Zoccl
and the number of occluding objects K. In case of occlusion
⋆
(k ≥ 1), zoccl is ideally measured, else (no occlusion, k = 0) z ⋆ is ideally measured.
3.2 Conditional Probability Distributions
Inferring the probability distribution of the extra state variables such as P (n) is often in⋆
feasible. Marginalization of the extra state variables Z, X, M, N, XN , K, XK , Zoccl
avoids
the increase in complexity of the estimation problem, but still takes into account the dynamic nature of the environment. Marginalization requires the modelling of all conditional
probability tables and conditional probability distributions (pdf ) of each random variable
conditionally on its parents.
First of all, some assumptions have to be made for P (n). Assume that the probability
of the number of unmodeled objects decreases exponentially, i.e. P (n) is given by:
P (n) = (1 − p) pn ,

(2)

with p a measure for the degree of appearance of unmodeled objects. More precisely, p is
the probability that at least one unmodeled object is present. While p is indicated in Fig. 1,
Fig. 2 shows the resulting distribution P (n).
Secondly, assume that nothing is known a priori about the position of the unmodeled
objects along the measurement beam. Hence each unmodeled object’s position is assumed
to be uniformly distributed over the measurement beam (Fig. 3):
(
1
if xN j ≤ zmax
P (xN j ) = zmax
(3)
0
otherwise,
186

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

P (xN j | n)

u

1
zmax

z⋆

0

zmax

z

Figure 3: P (xN j | n) (Eq. (3)).

P (k | n, x, m)
0.3
0.2
0.1

0 1 2 3 4 5 6 7 8 9 10

k

Figure 4: P (k | n, x, m) (Eq. (5)) for n = 10 and u = 0.25.

with zmax the maximum range of the range sensor.
Thirdly, assume the positions of the unmodeled objects are independent:
P (xN | n) =

n
Y

P (xN j ) .

(4)

j=1

Next, an expression is needed for the conditional probability: P (k | n, xN , x, m), i.e. the
probability that k of the n unmodeled objects are occluding the map m. An unmodeled
object is occluding the map m if it is located along the measurement beam and in front
of the closest object in the map. It is straightforward to show that P (k | n, xN , x, m) is a
binomial distribution:
!


n

uk (1 − u)n−k if k ≤ n
(5)
P (k | n, xN , x, m) =
k


0
otherwise,
 
n
where u is the probability that an unmodeled object is occluding the map and
=
k
n!
(n−k)!k! is the number of ways of selecting k objects out of a total of n objects. Fig. 4 shows
187

De Laet, De Schutter & Bruyninckx

XN 1

XN 2

XN 3

...

XN j

...

XK1

XK2

...

XKi

...

XN n−2

XN n−1

XN n

XKk

Figure 5: The selection scheme, where each cross eliminates an unmodeled object that is
not occluding the map.

this binomial distribution. Since it was assumed that the positions of the unmodeled objects
were uniformly distributed, u, the probability that an unmodeled object is occluding the
map is:
u = P (xN j < z ⋆ ) =

z⋆
zmax

,

(6)

as depicted in Fig. 3.
Furthermore, an analytical expression for P (xK | xN , k) is necessary. The positions of
the occluding objects xK are equal to the positions of the unmodeled objects xN that are
occluding the map, as shown in Fig. 5. In other words, xKi equals xN j if and only if the
unmodeled object is occluding the map, i.e. if xN j ≤ z ⋆ :

1
⋆

δ (xKi − xN j ) = zmax
z ⋆ δ (xKi − xN j ) if xN j ≤ z
P (xN j ≤z ⋆ )
P (xKi | xN j , k, x, m) =
(7)
0
otherwise,
with δ the Dirac function and xKi the occluding object corresponding to xN j .
In case of occlusion, the range sensor ideally measures the distance to the closest occluding object xKc :
⋆
⋆
P (zoccl
| xK ) = δ (zoccl
− xKc ) .

(8)

While range finders are truly quite deterministic since the measurements are to a great
extent explainable by underlying physical phenomena such as specular reflections, inference,
... these underlying phenomena are complex and therefore costly to model. On top of these
underlying phenomena additional uncertainty on the measurement is due to (i) uncertainty
in the sensor position, (ii) inaccuracies of the world model and (iii) inaccuracies of the sensor
itself. So far only disturbances on the measurements due to unmodeled objects in the environment are included. To capture the additional uncertainty, additional measurement noise
is added. After taking into account the disturbances by unmodeled objects, unexplainable
measurements and sensor failures (Section 3.4), there is no physical reason to expect that
the mean value of the true measurements deviates from the expected measurement and
that the true measurements are distributed asymmetrically around the mean. Therefore
188

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

symmetrical noise with mean value zero is added. Two facts justify the modeling of the
measurement noise as a normal distribution: (i) the normal distribution maximizes the
information entropy among all distributions with known mean and variance, making it the
natural choice of underlying distribution for data summarized in terms of sample mean and
variance; and (ii) if the underlying phenomena are assumed to have a small, independent
effect on the measurement, the central limit theorem states that under certain conditions
(such as being independent and identically distributed with finite variance), the sum of a
large number of random variables is approximately normally distributed. If the measurement noise is modeled by a zero mean Gaussian with standard deviation σm , the conditional
⋆ , k) is:
probability P (z | x, m, zoccl
(
N (z; z ⋆ , σm )
if k = 0
⋆
P (z | x, m, zoccl
, k) =
(9)
⋆
N (z; zoccl , σm ) if k ≥ 1,
⋆ , k) has two main cases, the first for k = 0
where the conditional probability P (z | x, m, zoccl
where no occlusion is present and the sensor is observing the map m, and the second case
for k ≥ 1 where the sensor observes an occluding object. σm is included in the Bayesian
network of Fig. 1.

3.3 Marginalization
This section shows the different steps needed to marginalize out the extra state variables in
Eq. (1), and motivates the approximation that leads to an analytical sensor model.
The product rule rewrites the sensor model P (z | x, m) as:
P (z | x, m) =

P (z, x, m)
P (z, x, m)
=
,
P (x, m)
P (x) P (m)

(10)

since X and M are independent. The numerator is obtained by marginalizing the joint
⋆ ) over x ,
probability of the whole Bayesian network pjoint = P (z, x, m, xN , n, xK , k, zoccl
N
⋆ :
n, xK , k and zoccl
Z
XZ XZ
⋆
pjoint dxN dxK dzoccl
.
(11)
P (z, x, m) =
⋆
zoccl

k

xK

n

xN

Using the chain rule to factorize the joint distribution while making use of the conditional
dependencies in the Bayesian network (Fig. 1) yields:
⋆
⋆
| xK ) P (k | n, xN , x, m)
, k) P (zoccl
pjoint = P (z | x, m, zoccl

P (xK | xN , k, x, m) P (xN | n) P (n) P (x) P (m) .
Substituting (12) and then (11) into (10) gives:
Z
Z
X
⋆
, k)
P (z | x, m, zoccl
P (z | x, m) =
⋆
zoccl

xK

k

X

⋆
P (zoccl
| xK )

⋆
P (k | n, x, m) P (n) P (xK | n, k, x, m) dxK dzoccl
,

n

189

(12)

(13)

De Laet, De Schutter & Bruyninckx

P (xKi | x, m)
1
z⋆

xKi

0

z⋆

zmax

xKi

Figure 6: P (xKi | n, k, x, m) (Eq. (15))

where
P (xK | n, k, x, m) =

Z

P (xK | xN , k, x, m) P (xN | n) dxN .

(14)

xN

Since the binomial distribution P (k | n, xN , x, m) of Eq. (5) is independent of xN , it is
moved out of the integral over xN in (14), and is further on denoted by P (k | n, x, m).
Marginalizing xN Now study the integral over xN in Eq. (14) and focus on xN j , the
position of one unmodeled object. Substituting (3) and (7) results in:
Z z⋆
1
zmax
δ (xKi − xN j )
dxN j
P (xKi | n, k, x, m) =
⋆
zmax
xN j =0 z
(
1
if xKi ≤ z ⋆
z⋆
=
(15)
0
otherwise.
This equation expresses that xKi is uniformly distributed when conditioned on n, k, x
and m as shown in Fig. 6. Since all occluding objects are considered independent:
( k
1
if ∀ 0 ≤ i ≤ k : xKi ≤ z ⋆
z⋆
(16)
P (xK | n, k, x, m) =
0
otherwise.
This equation shows that P (xK | n, k, x, m) is independent of n and thus can be moved out
of the summation over n in Eq. (13):
Z
X
⋆
⋆
⋆
,
(17)
| k, x, m) P (k | x, m) dzoccl
, k) P (zoccl
P (z | x, m, zoccl
P (z | x, m) =
⋆
zoccl

k

with
P (k | x, m) =

X

P (k | n, x, m) P (n) ,

(18)

n

and
⋆
P (zoccl
| n, k, x, m) =

Z

xK

⋆
P (zoccl
| xK ) P (xK | k, x, m) dxK .

190

(19)

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

Marginalizing n First focus on the summation over n in Eq. (18) and substitute (2) and
(5):
P (k | x, m) =



∞ 
X
n
uk (1 − u)n−k (1 − p) pn .
k

(20)

n=k

Appendix A proves that this infinite sum simplifies to:

P (k | x, m) = 1 − p′ p′k ,

(21)

with

p′ =

up
.
1 − (1 − u) p

(22)

Marginalizing xK Now focus on the integral over xK in Eq. (19). Substituting (8) into
this equation results in:
Z
⋆
⋆
P (zoccl | k, x, m) =
δ (zoccl
− xKc ) P (xKc | k) dxKc
xKc

⋆
| k, x, m) .
= P (xKc = zoccl

(23)

⋆
This equation shows that the conditional probability P (zoccl
| k, x, m) represents the prob⋆ , i.e. the probability that the perfect measurement of the nearest occluding object is zoccl
⋆
ability that the nearest occluding object is located at zoccl . This is only the case when one
⋆
of the k objects along the measurement beam is located such that zoccl
is measured, while
all other objects along the measurement beam are located behind the occluding object, or
expressed in probabilities:

⋆
P (zoccl
| k, x, m) =

k
X

⋆
⋆
P (xK6=i ≥ zoccl
| k, x, m) P (xKi = zoccl
| k, x, m) .

(24)

i=1

Since xK is uniformly distributed over [0, z ⋆ ] as shown by Eq. (15), it follows that:
⋆
| k, x, m) =
P (xKi = zoccl
⋆
P (xKi ≥ zoccl
| k, x, m) =

1
,
z⋆
⋆
z ⋆ − zoccl
,
⋆
z

(25)
(26)

and (24) can be written as:
P

⋆
(zoccl

1
| k, x, m) = k ⋆
z



⋆
z ⋆ − zoccl
z⋆

k−1

.

(27)

⋆
Marginalizing k After obtaining expressions for P (k | x, m) (Eq. (21)) and P (zoccl
| k, x, m)
(Eq. (27)) we turn the attention to the summation over k in Eq. (17):
X
⋆
⋆
⋆
P (z | x, m, zoccl
, k) P (zoccl
| k, x, m) P (k | x, m) .
(28)
P (z, zoccl
| x, m) =
k

191

De Laet, De Schutter & Bruyninckx

Split this summation in two parts: one for k = 0, when there is no occlusion, and one
⋆ , k) given by
for k ≥ 1, and substitute the expressions for P (k | x, m) and P (z | x, m, zoccl
Eq. (21) and Eq. (9), respectively:
⋆
⋆
P (z, zoccl
| x, m) = N (z; z ⋆ , σm ) P (zoccl
| k = , x, m) P (k =  | x, m) +
⋆
⋆
N (z; zoccl
, σm ) P (zoccl
| k ≥ , x, m) P (k ≥  | x, m)

⋆
⋆
= N (z; z , σm ) P (zoccl | k = , x, m)  − p′ +
⋆
⋆
N (z; zoccl
, σm ) α (zoccl
| x, m) ,

where

⋆
α (zoccl

| x, m) =
=

⋆
P (zoccl
∞
X
k=1

| k ≥ 1, x, m) P (k ≥ 1 | x, m)

⋆
P (zoccl
| k, x, m) 1 − p′ p′k .

(29)

(30)

Substituting (27) into (30) results in:
⋆
α (zoccl

| x, m) =

∞
X
k=1

1
k ⋆
z



⋆
z ⋆ − zoccl
z⋆

k−1

which is simplified using Eq. (114) in Appendix A:
∞

⋆
α (zoccl

| x, m) =
=

 X
1
k
1 − p′ p′
⋆
z
p′ (1

h

z⋆ 1 −
Substituting (32) into (29) gives:



−

k=1
p′ )

⋆
z ⋆ −zoccl
p′
z⋆




1 − p′ p′k ,

⋆
z ⋆ − zoccl
p′
z⋆

k−1
(32)

i2 .


⋆
⋆
P (z, zoccl
| x, m) = N (z; z ⋆ , σm ) P (zoccl
| k = , x, m)  − p′ +
⋆
N (z; zoccl
, σm )

(31)

z⋆

h

( − p′ ) p′
 ⋆ ⋆
i . (33)
z −zoccl ′
−
p
⋆
z

⋆
⋆
Marginalizing zoccl
Substituting (33) into (17) shows that only the integration over zoccl
still has to be carried out:
Z z⋆
 − p′
⋆
′
⋆
′
⋆
 ⋆ ⋆
i dzoccl
N (z; zoccl
, σm ) h
P (z | x, m) = (1 − p )N (z; z , σm ) + p
.(34)
z −zoccl ′
⋆ =
⋆
zoccl
z −
p
⋆
z

The first term of the right hand side is a Gaussian distribution around the ideal measurement, multiplied with the probability of no occlusion (k = 0). The second term is an
integration over all possible positions of the occluding object of a scaled Gaussian distribu⋆ ). The scaling factor
tion centered at the ideal measurement of the occluding object (zoccl
⋆
represents the probability that the occluding objects are located such that zoccl
is measured.
From Eq. (20) and Eq. (32) it follows that the scaling factor can be written as:
⋆
α (zoccl
| x, m) =

p (1 − p)
h

 i2 ,
⋆
zoccl
zmax 1 − 1 − zmax
p
192

(35)

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

PSfrag
0.45

Finite sum
Approximation

0.5
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

1

2

3

4

5

6

7

z

Figure 7: Comparison of the approximation (Eq.(36)) of the integral in Eq. (34) with a
finite sum approximation with small step size for p = 0.8, zmax = 10, z ⋆ = 5 and
σm = 0.15.

P (z | x, m)

Finite sum
Approximation

1.0

0.90
0.86

0.9

0.82

0.8
0.7

0.78

0.6
4.90 4.95 5.00 5.05 5.10

0.5
0.4
0.3
0.2
0.1
0

1

2

3

4

5

6

7

8

9

10

z

Figure 8: Comparison of the obtained RBBM P (z | x, m) (Eq. (37)) with a finite sum approximation of Eq. (34) with small step size for p = 0.8, zmax = 10, z ⋆ = 5 and
σm = 0.15.

193

De Laet, De Schutter & Bruyninckx

which is independent of z ⋆ .
Until now, no approximations where made to obtain Eq. (34) for the beam model
P (z | x, m). The integral over the scaled Gaussian distributions however, cannot be obtained analytically. Therefore, a first approximation in the marginalization is made by
⋆ ,σ ) ≈
neglecting the noise on the range measurement in case of occlusion, i.e.: N (z; zoccl
m
⋆
δ (z − zoccl ). Using this approximation the second term in the right hand side of Eq. (34)
becomes:
p′ (1 − p′ )
p (1 − p)
(36)
 i2 .
h

2 =

⋆ −z
z
z
z ⋆ 1 − z ⋆ p′
p
zmax 1 − 1 − zmax

Fig. 7 shows the quality of the approximation of the integral in Eq. ( 34) compared to a
finite sum approximation with small step size. The approximation introduces a discontinuity
around z = z ⋆ . Using the proposed approximation for the integral the resulting beam model
is:

(−p′ )
(1 − p′ ) N (z; z ⋆ , σm ) + p′
if z ≤ z ⋆
z ⋆ −z ′ 
⋆
p
−
z
)]
[
(
⋆
z
P (z | x, m) =
(37)
(1 − p′ ) N (z; z ⋆ , σ )
otherwise,
m
as shown in Fig. 8.
The RBBM can be written as a mixture of two components:

P (z | x, m) = π1 Phit (z | x, m) + π2 Poccl (z | x, m) ,

(38)



(39)

with π1 = 1 − p′
π2 = p

′

(40)

⋆

Phit (z | x, m) = N (z; z , σm )

1−p′
 1⋆
z 1− z⋆ −z p′ 2
[ ( z⋆ )]
Poccl (z | x, m) =
0

(41)
if 0 ≤ z ≤ z ⋆

(42)

otherwise.

3.4 Extra Components

Occasionally, range finders produce unexplainable measurements, caused by phantom readings when sonars bounce off walls, or suffer from cross-talk (Thrun et al., 2005). Furthermore additional uncertainty on the measurements is caused by (i) uncertainty in the
sensor position, (ii) inaccuracies of the world model and (iii) inaccuracies of the sensor itself.
These unexplainable measurements are modeled using a uniform distribution spread over
the entire measurement range [0, zmax ]:
(
1
if 0 ≤ z ≤ zmax ,
(43)
Prand (z | x, m) = zmax
0
otherwise.
Furthermore, sensor failures typically produce max-range measurements, modeled as a
point-mass distribution centered around zmax :
(
1 if z = zmax ,
Pmax (z | x, m) = I (zmax ) =
(44)
0 otherwise.
194

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

These two extra components can be added to Eq. (38), resulting in the final RBBM:
P (z | x, m) = π1 Phit (z | x, m) + π2 Poccl (z | x, m) + π3 Prand (z | x, m) + π4 Pmax (z | x, m) ,
(45)
where π3 and π4 are the probabilities that the range finder returns an unexplainable measurement and a maximum reading, respectively. Furthermore,

π1 = 1 − p′ (1 − π3 − π4 ) and
(46)
π2 = p′ (1 − π3 − π4 ),

(47)

while Phit (z | x, m), Poccl (z | x, m), Prand (z | x, m) and Pmax (z | x, m) are given by (41),
(42), (43) and (44) respectively.
3.5 Assumptions and Approximations
This section summarizes the assumptions and approximations made to arrive at the RBBM
of Eq. (45).
Section 3.2 makes four assumptions:
(i) the probability of the number of unmodeled objects decreases exponentially, Eq. (2);
(ii) the unmodeled object’s position is uniformly distributed over the measurement beam
(Fig. 3, Eq. (3));
(iii) the positions of the unmodeled objects are independent, Eq. (4); and
(iv) the measurement noise is zero mean normally distributed with standard deviation
σm (Eq. 9).
Furthermore, Section 3.3 makes one approximation to obtain an analytical expression
by neglecting the noise on the range measurement in case of occlusion (Eq. (34)).
3.6 Interpretation
The following paragraphs give some insights in the RBBM and its derivation.
The mixture representation (45) shows the four possible causes of a range measurement:
a hit with the map, a hit with an unmodeled object, an unknown cause resulting in a
random measurement and a sensor failure resulting in a maximum reading measurement.
The derivation of Section 3.3 shows that the position of each of the occluding objects is
uniformly distributed between the sensor and the ideally measured object in the environment
(Eq. (15), Fig. 6). This is perfectly reasonable considering the assumption of uniformly
distributed unmodeled objects.
⋆
Furthermore, some insights are provided concerning α (zoccl
| x, m) (Eq. (35), Fig. 7),
⋆
the probability that the occluding objects are located such that zoccl
is measured. First
of all, this probability is independent of the location of the ideally measured object in the
environment (z ⋆ ) (except that this probability is zero for z > z ⋆ ). This agrees with intuition,
since one expects the measurements caused by the occluding objects to be independent of z ⋆ ,
the measurement in the case of no occlusion. Second, the probability of sensing unmodeled
objects decreases with the range, as expected. This is easily explained with the following
thought experiment: if two objects are present with the same likelihood in the perception
field of the range finder, and the first object is closest to the range sensor, then the sensor
is more likely to measure the first object. To measure the second object, the second object
195

De Laet, De Schutter & Bruyninckx

should be present and the first object should be absent (Thrun et al., 2005). Moreover, the
rate of decrease of the likelihood of sensing unmodeled objects is only dependent on p, the
degree of appearance of unmodeled objects.
The probability of measuring a feature of the map, and therefore the integral under the
scaled Gaussian (1 − p′ )Phit (z | x, m) (45), decreases with the expected range. This is easily
explained since the probability that the map is not occluded decreases when the feature is
located further away.
Finally, the discontinuity of the RBBM (Fig. 8) was shown to be caused by the only
approximation made (Section 3.5). Since the state of the art range sensors are very accurate, neglecting the measurement noise on the measurement of an occluding object is an
acceptable approximation. This is also shown by the experiments presented in Section 5.
With respect to the state of the art beam model of Thrun et al. (2005), the model proposed here, Eq. (45), has: (i) a different functional form for the probability of range measurements caused by unmodeled objects, (ii) an intuitive explanation for the discontinuity
encountered in the cited paper, and (iii) a reduction in the number of model parameters.
Thrun et al. find that Poccl (z | x, m) has an exponential distribution. This exponential
distribution results from the following underlying assumptions (although not revealed by
the authors): (i) the unmodeled objects are equally distributed in the environment and (ii)
a beam is reflected with a constant probability at any range. The last assumption equals
assuming that the probability that an unmodeled object is located at a certain distance is
constant. This assumption fails to capture that the number of unmodeled object is finite
and that it is more probable to have a limited number of unmodeled objects than a huge
number of them. While we also assume that unmodeled objects are equally distributed in
the environment (Eq. (3)), we assume that the number of unmodeled objects is geometrically distributed (Eq. (2)) capturing the finiteness of the number of unmodeled objects
and the higher probability of a smaller number of unmodeled objects. The modeling of
the finiteness of the number of unmodeled objects and the higher probability of a smaller
number of unmodeled objects results in a quadratic decay of Poccl (z | x, m), instead of the
exponential decay of Poccl (z | x, m) found by Thrun et al..
As stated in the previous paragraph, the discontinuity of the RBBM (Fig. 8) is caused
by an approximation. While Thrun’s model considers the rate of decay of Poccl (z | x, m)
to be independent of π2 , the probability of an occlusion, it is shown here that they both
depend on the same parameter p′ (Eq. (42), Eq. (47)). Therefore the RBBM has fewer
parameters than Thrun’s model.
3.7 Validation
The goal of this section is to show by means of a Monte Carlo simulation2 that the RBBM,
Eq. (45), agrees with the Bayesian network in Fig. 1. A Monte Carlo simulation is an approximate inference method for Bayesian networks. The idea behind the Monte Carlo simulation
is to draw random configurations of the network variables Z, X, M , N , XN = {XN j }j=1:n ,
⋆
K, XK = {XKi }i=1:k and Zoccl
and to do this a sufficient number of times. Random configurations are selected by ancestral sampling (Bishop, 2006), i.e. by successively sampling
2. A Monte Carlo simulation is also known as stochastic simulation in the Bayesian network literature
(Jensen & Nielsen, 2007).

196

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

P (z | x, m)
Finite sum
Approximation
Monte Carlo Simulation

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

1

2

3

4

5

6

7

8

9

10

z

Figure 9: Comparison of the obtained RBBM P (z | x, m) (45), a finite sum approximation
of Eq. (34) with small stepsize and the normalized histogram of 500 samples
obtained by a Monte Carlo Simulation of the proposed Bayesian network (Fig. 1)
for p = 0.8, zmax = 10, z ⋆ = 5, σ = 0.15, π3 = 0.2 and π4 = 0.02.

the states of the variables following the causal model defined by the directed acyclic graph
of the Bayesian network.
Fig. 9 shows that the RBBM agrees with a Monte Carlo simulation with 500 samples of
the proposed Bayesian network.

4. Variational Bayesian Learning of the Model Parameters
The RBBM, Eq. (45), depends on four independent model parameters:


Θ = σm , p′ , π3 , π4 ,

(48)

while zmax is a known sensor characteristic. This set of parameters has a clear physical
interpretation; σm is the standard deviation of the zero mean Gaussian measurement noise
in Eq. (9) governing Phit (z | x, m) (Eq. (41)); p′ , defined in Eq. (21), is the probability that
the map is occluded (P (k ≥ 1 | x, m)); π3 and π4 are the probabilities that the range finder
returns an unexplainable measurement (unknown cause) and a maximum reading (sensor
failure), respectively.
An alternative non-minimal set of parameters containing all the mixing coefficients π =
[π1 , π2 , π3 , π4 ] could be used: Θ′ = [σm , π], provided that the constraint:
S=4
X

πs = 1,

s=1

197

(49)

De Laet, De Schutter & Bruyninckx

Dι

π1 , π2 , π3 , π4

Zι

σm
J

Figure 10: Graphical representation of the mixture measurement model (Eq. (45)) with
latent correspondence variable Dι = {Dι1 , Dι2 , Dι3 , Dι4 } and model parameters
Θ′ = [σm , π1 , π2 , π3 , π4 ].

is taken into account. The set of minimal parameters Θ straightforwardly follows from the
non-minimal set Θ′ since:
p′ =

π2
,
1 − π3 − π4

(50)

as can be seen from Eq. (47).
The physical interpretation of the parameters Θ allows us to initialize them by hand
with plausible values. However, another, more flexible way is to learn the model parameters
from actual data containing J measurements Z = {zι }ι=1:J with corresponding states X =
{xι }ι=1:J and map m. Furthermore, learning the model parameters is also a validation
for the proposed analytical model: if the learning algorithm succeeds in finding model
parameters such that the resulting distribution gives a good explanation of the data, the
analytical model is likely to agree well with reality.
In this paper two different estimators3 , a maximum likelihood (ML) (Dempster, Laird,
& Rubin, 1977; McLachlan & Krishnan, 1997; Bishop, 2006) and a variational Bayesian
(VB) (Beal & Ghahramani, 2003; Bishop, 2006) estimator, are presented to learn the model
parameters from data. Section 4.1 derives a maximum likelihood estimator, which is a
known approach for this problem, but reformulated for the RBBM. This ML estimator
only provides point estimates of the parameters and leads to overfitting since the likelihood
function is generally higher for more complex model structures. Therefore, we propose a
variational Bayesian (VB) estimator in Section 4.2, which is a new approach for learning the
parameters for beam models. The VB estimator is a fully Bayesian learning approach; priors
over the unknown parameters are included, complex (overfitting) models are punished, and
a full probability distribution over the parameters is obtained.
3. This paper approximately follows the notation by Bishop (2006).

198

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

4.1 Maximum Likelihood Learning
A maximum likelihood estimator is proposed to identify the model parameters Θ that
maximize the likelihood of the data Z with corresponding X and map m:
Θ = arg max log P (Z | X, m, Θ) .
Θ

(51)

When using the mixture representation of the RBBM (Eq. (45)), the estimation problem
can be formulated as finding the ML estimates for the parameters Θ′ = [σm , π] provided
that the constraint in Eq. (49) is included. In general it is not known which of the four
possible causes actually caused each of the measurements. In that case the ML estimation
problem is difficult and lacks a closed-form solution. If however, the corresponding causes
of each of the measurements are known, the solution is easily obtained in closed form.
Therefore, introduce a latent correspondence variable d = [d1 , d2 , d3 , d4 ], representing the
unknown cause, using a 1-of-S representation. The elements ds of d give the probability
that the measurement is a result of the s’th cause. The graphical representation of the
mixture formulation including the latent correspondence d variable is shown in Fig. 10.
Although the ML estimation problem lacks a closed-form solution due to the unknown
correspondences, an expectation-maximization approach (EM) can solve the problem by
iterating an expectation and a maximization step. The expectation step calculates an
expectation for the correspondence variables ds while the maximization step computes the
other model parameters under these expectations.
Algorithm 1 ML estimator for model parameters
while convergence criterion not satisfied do
for all zι in Z, with ι = 1 : J, where J = |Z|−1 do
⋆
calculate zm
η = [ π1 Phit (zι | xι , m) + π2 Poccl (zι | xι , m) + π3 Prand (zι | xι , m) +
π4 Pmax (zι | xι , m)]−1
γ (dι1 ) = η π1 Phit (zι | xι , m)
γ (dι2 ) = η π2 Poccl (zι | xι , m)
γ (dι3 ) = η π3 Prand (zι | xι , m)
γ (dι4 ) = η π4 Pmax (zι | xι , m)
end for P
π1 = J −1 Pι γ (dι1 )
π2 = J −1 Pι γ (dι2 )
π3 = J −1 Pι γ (dι3 )
π4 = J −1 ι γ (dι4 )
p′ = 1−ππ32−π4
rP
σm =

ι

⋆ 2
γ(d
P ι1 )(zι −zι )
γ(d
)
ι1
ι

end while
return Θ = [σm , p′ , π3 , π4 ]

199

De Laet, De Schutter & Bruyninckx

The marginal distribution over the correspondence variable d is specified in terms of the
mixing coefficients πs such that:
P (ds = 1) = πs ,
where the parameters π must satisfy the following two conditions:

0 ≤ π ≤ 1,
PS s
s=1 πs = 1.

(52)

(53)

Since d uses a 1-of-S representation, the marginal distribution can be written as:
P (d) =

S
Y

πsds .

(54)

s=1

The EM-algorithm expresses the complete-data log likelihood, i.e. the log likelihood of
the observed and the latent variables:
J
X

(dι1 (log π1 + log Phit (zι | xι , m)) + · · ·
log P Z, D | X, Θ′ , m =
ι=1

dι2 (log π2 + log Poccl (zι | xι , m)) + · · ·

dι3 (log π3 + log Prand (zι | xι , m)) + · · ·
dι4 (log π4 + log Pmax (zι | xι , m))) ,

(55)

where Z = {zι }ι=1:J is the vector containing the observed data and D = {dι } is the vector
containing the matching correspondences.
Expectation step: Taking the expectation of the complete-data log likelihood in Eq. (55)
with respect to the posterior distribution of the latent variables gives:


Q(Θ′ , Θ′old ) = ED log P Z, D | X, Θ′ , m
=

J
X

(γ (dι1 ) (log π1 + log Phit (zι | xι , m)) + · · ·

ι=1

γ (dι2 ) (log π2 + log Poccl (zι | xι , m)) + · · ·
γ (dι3 ) (log π3 + log Prand (zι | xι , m)) + · · ·
γ (dι4 ) (log π4 + log Pmax (zι | xι , m))) ,

(56)

where γ (dιs ) = E [dιs ] is the discrete posterior probability, or responsibility (Bishop, 2006),
of cause s for data point zι . In the E-step, these responsibilities are evaluated using Bayes’
theorem, which takes the form:
π1 Phit (zι | xι , m)
,
Norm
π2 Poccl (zι | xι , m)
γ (dι2 ) = E [dι ] =
,
Norm
π3 Prand (zι | xι , m)
γ (dι3 ) = E [dι ] =
, and
Norm
π4 Pmax (zι | xι , m)
,
γ (dι4 ) = E [dι ] =
Norm
γ (dι1 ) = E [dι ] =

200

(57)
(58)
(59)
(60)

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

where Norm is the normalization constant:
Norm = π1 Phit (zι | xι , m) + π2 Poccl (zι | xι , m) + π3 Prand (zι | xι , m) + π4 Pmax (zι | xι , m) .
(61)
Two measures are derived from the responsibilities:
Js =

J
X

γ (dιs ) , and

(62)

ι=1

z̄s =

J
1 X
γ (dιs ) zι ,
Js

(63)

ι=1

where Js is the effective number of data points associated with cause s, and z̄s is the mean
of the effective data points associated with cause s.
Maximization step: In the M-step the expected complete-data log likelihood in Eq. (56)
is maximized with respect to the parameters Θ′ = [σm , π]:
Θ′new = arg max
Q(Θ′ , Θ′old ).
′
Θ

(64)

Maximization with respect to πs using a Lagrange multiplier to enforce the constraint
P
s πs = 1 results in:
πs =

Js
,
J

(65)

which is the effective fraction of points in the data set explained by cause s. Maximization
with respect to σm results in:
v
u
J
u1 X
t
σm =
γdι1 (zι − zι⋆ )2 .
(66)
J1
ι=1

Algorithm 1 summarizes the equations for the ML estimator, further on called ML-EM
algorithm.
4.2 Variational Bayesian Learning
The ML estimator only provides point estimates of the parameters and is sensitive to
overfitting (Bishop, 2006). Therefore, we propose a variational Bayesian (VB) estimator,
which is a new approach for learning the parameters for beam models. The VB estimator is a
fully Bayesian learning approach; priors over the unknown parameters are included, complex
(overfitting) models are punished, and a full probability distribution over the parameters is
obtained. The VB estimator has only a little computational overhead as compared to the
ML estimator (Bishop, 2006).
The Bayesian approach attempts to integrate over the possible values of all uncertain
quantities rather than optimize them as in the ML approach (Beal, 2003; Beal & Ghahramani, 2003). The quantity that results from integrating out both the latent variables and the
201

De Laet, De Schutter & Bruyninckx

R
parameters is known as the marginal likelihood4 : P (Z) = P (Z | D, Θ) P (D, Θ) d(D, Θ),
where P (D, Θ) is a prior over the latent variables and the parameters of the model. Integrating out the parameters penalizes models with more degrees of freedom, since these
models can a priori model a larger range of data sets. This property of Bayesian integrations
is known as Occam’s razor, since it favors simpler explanations for the data over complex
ones (Jeffreys & Berger, 1992; Rasmussen & Ghahramani, 2000).
Unfortunately, computing the marginal likelihood, P (Z), is intractable for almost all
models of interest. The variational Bayesian method constructs a lower bound on the
marginal likelihood, and attempts to optimize this bound using an iterative scheme that
has intriguing similarities to the standard EM algorithm. To emphasize the similarity with
ML-EM, the algorithm based on variational Bayesian inference is further on called VB-EM.
By introducing the distribution Q over the latent variables the complete log marginal
likelihood can be decomposed as (Bishop, 2006):
log P (Z) = L (Q) + KL (Q||P ) ,

(67)



(68)

where
L (Q) =

Z

Q (D, Θ) log

P (Z, D, Θ)
Q (D, Θ)



d (D, Θ) ,

and KL (Q||P ) is the KL-divergence between Q and P . Since this KL-divergence is always
greater or equal than zero, L (Q) is a lower bound on the log marginal likelihood. Maximizing this lower bound with respect to the distribution Q (D, Θ) is equivalent to minimizing
the KL-divergence. If any possible choice for Q (D, Θ) is allowed, the maximum of the
lower bound would occur when the KL-divergence vanishes, i.e. when Q (D, Θ) is equal
to the posterior distribution P (D, Θ | Z). Working with the true posterior distribution is
however often intractable in practice. One possible approximate treatment considers only a
restricted family of distributions Q (D, Θ) and seeks the member of this family minimizing
the KL-divergence. The variational Bayesian treatment uses a factorized approximation, in
this case between the latent variables D and the parameter Θ:
Q (D, Θ) = QD (D) QΘ (Θ) .

(69)

The variational approach makes a free form (variational) optimization of L (Q) with respect
to the distributions QD (D) and QΘ (Θ), by optimizing with respect to each of the factors
in turn. The general expressions for the optimal factors are (Bishop, 2006):
log Q⋆D (D) = EΘ [log P (Z, D, Θ)] + C te ,
log Q⋆Θ (Θ)

te

= ED [log P (Z, D, Θ)] + C ,

and

(70)
(71)

where ⋆ indicates the optimality. These expressions give no explicit solution for the factors,
because the optimal distribution for one of the factors depends on the expectation computed
with respect to the other factor. Therefore an iterative procedure, similar to EM, that cycles
through the factors and replaces each in turn with a revised optimal estimate is used.
4. To avoid overloading the notation the conditioning on the map m and the positions X = {xι } associated
with the data Z = {zι } is not explicitly written.

202

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

Introducing priors Since the variational Bayesian approach is a fully Bayesian approach,
priors have to be introduced over the parameters Θ′′ = [µ, σm , π]. Remark that in the
variational Bayesian estimator not only the standard deviation σm governing Phit (z | x, m)
(Eq. (41)) is estimated but also the means, referred to as µ further on. Since the analysis is
considerably simplified if conjugate prior distributions are used, a Dirichlet prior is chosen
for the mixing coefficients π:
P (π) = Dir (π|α0 ) ,

(72)

as well as an independent Gaussian-Wishart prior5 for the mean µ and the precision λm =
−1 of the Gaussian distribution P
σm
hit (z | x, m) (Eq. (41)):

P (µ, λm ) = N µ|µ̄ , (βλm )− W (λm |W , ν ) .

(73)

α0 gives the effective prior number of observations associated with each component of the
mixture. Therefore, if the value of α0 is set small, the posterior distribution will be mainly
influenced by the data rather than by the prior.
Expectation step
can be written as:

Using these conjugate priors, it can be shown that the factor Q⋆D (D)

Q⋆D (D) =

J
Y

dι1 dι2 dι3 dι4
rι1
rι2 rι3 rι4 ,

(74)

ι=1

where the quantities rιs are responsibilities analogous to the γιs of Eq. (57) and are given
by:
rιs =

ριs
,
ρι1 + ρι2 + ρι3 + ρι4

(75)

where
log ρι1 = E [log π1 ] + E [log Phit (zι | xι , m)] ,

(76)

log ρι2 = E [log π2 ] + E [log Poccl (zι | xι , m)] ,

(77)

log ρι3 = E [log π3 ] + E [log Prand (zι | xι , m)] , and

(78)

log ρι4 = E [log π4 ] + E [log Pmax (zι | xι , m)] .

(79)

The above equations can be rewritten as:
h
i



log ρι1 = E [log π1 ] + E [log |λm |] − log (π) − Eµ,λm (zι − µ)T λ (zι − µ) , (80)



log ρι2 = E [log π2 ] + log Poccl (zι | xι , m) ,
(81)
log ρι3 = E [log π3 ] + log Prand (zι | xι , m) , and

(82)

log ρι4 = E [log π4 ] + log Pmax (zι | xι , m) ,

(83)

5. The parameters are as defined by Bishop (2006).

203

De Laet, De Schutter & Bruyninckx

in which the expectations can be calculated as follows:
E [log πs ] = Ψ (αs ) − Ψ (α1 + α2 + α3 + α4 ) ,
ν 
+ log 2 + log |W |, and
E [log |λm |] = Ψ
2
i

h
Eµ,λm (zι − µ)T λ (zι − µ)

= β −1 + ν (zι − µ̄)T W (zι − µ̄) ,

(84)
(85)
(86)

where Ψ is the digamma function.
Three measures are derived from the responsibilities:
J
X

Js =

(87)

rιs ,

ι=1

J
1 X
rιs zι , and
Js

z̄s =

(88)

ι=1

J
1 X
rιs (zι − z̄s ) (zι − z̄s )T ,
Js

Cs =

(89)

ι=1

where Js is the effective number of data points associated with cause s, z̄s is the mean of
the effective data points associated with cause s and Cs the covariance of the effective data
points associated with cause s. Due to the similarity with the E-step of the EM-algorithm,
the step of calculating the responsibilities in the variational Bayesian inference is known as
the variational E-step.
Maximization step In accordance with the graphical representation in Fig. 10, it can
be shown that the variational posterior QΘ (Θ) factorizes as Q (π) Q (µ1 , σm ) and that the
first optimal factor is given by a Dirichlet distribution:
Q⋆ (π) = Dir (π|α) ,

(90)

αs = α0 + Js .

(91)

with

The second optimal factor is given by a Gaussian-Wishart distribution:

Q⋆ (µ1 , λm ) = N µ |µ̄, (βλm )− W (λm |W, ν) ,

(92)

with

β = β0 + J1 ,
1
(β0 µ̄0 + J1 z̄1 ) ,
µ̄ =
β
W −1 = W0−1 + J1 C1 +

(93)
(94)

β0 J1
(z̄1 − µ̄0 ) (z̄1 − µ̄0 )T , and
β0 + J1

ν = ν0 + J1 .

(95)
(96)

204

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

These update equations are analogous to the M-step of the EM-algorithm for the maximum
likelihood solution and is therefore known as the variational M-step. The variational M-step
computes a distribution over the parameters (in the conjugate family) rather than a point
estimate as in the case of the maximum likelihood estimator. This distribution over the
parameters allows us to calculate a predictive density P (z | Z).
Due to the use of conjugate priors, the integrals in the predictive density can be calculated analytically:


νβ
α1 St z|µ̄, 1+β
W, ν + α2 Poccl (z | x, m) + α3 Prand (z | x, m) + α4 Pmax (z | x, m)
P (z | Z) =
,
α1 + α2 + α3
(97)
where St (.) is a Student’s t-distribution. When the size J of the data is large, the Student’s
t-distribution approximates a Gaussian and the predictive distribution can be rewritten as:
P (z | Z) =

α1 N (z|µ, λm ) + α Poccl (z | x, m) + α Prand (z | x, m) + α Pmax (z | x, m)
. (98)
α1 + α2 + α3 + α4

If point estimates are desired for the parameters, maximum a posteriori estimates can be
obtained as follows:
αs
α + α + α + α
− 1

2
νβ
W
=
, and
1+β
πˆ2
.
=
1 − πˆ3 − πˆ4

π̂s = E [πs ] =
σm
p′

(99)
(100)
(101)

Algorithm 2 summarizes the equations for the VB-EM estimator.

5. Experiments
The goal of this section is threefold: (i) to learn the model parameters (Eq. (48)) of the
RBBM (Eq. (45)) from experimental data, (ii) to compare the results of the proposed
ML-EM and VB-EM estimator (Section 4), and (iii) to compare the results of the proposed
estimators with the learning approach of Thrun’s model proposed by Thrun et al. (2005). To
this end two experimental setups from different application areas in robotics are used. The
data for the first learning experiment is gathered during a typical mobile robot application
in which the robot is equipped with a laser scanner and is travelling in an office environment.
The data for the second learning experiment is gathered during a typical industrial pickand-place operation in a human populated environment. A laser scanner is mounted on
the industrial robot to make it aware of people and other unexpected objects in the robot’s
workspace.
To see how well the learned model explains the experiment, the learned continuous pdf
P (z | x, m, Θ) of Eq. (45) has to be compared with the discrete pdf of the experimental
data (histogram) H (z). To this end, the learned pdf is first discretized using the same bins
{zf }f =1:F as the experimental pdf. To quantize the difference between the learned and the
205

De Laet, De Schutter & Bruyninckx

Algorithm 2 Variational Bayesian estimator for model parameters
while convergence criterion not satisfied
for all zι in Z, with ι = 1 : J, where J = |Z|−1
⋆
calculate zm

 1
ν
1
ρι1 = exp Ψ (α1 ) − Ψ (α1 + α2 + α3 + αi
4 ) + 2 Ψ 2 + log 2 + log |W | − 2 log (2π) . . .
· · · − 21 β −1 + ν (zι − µ̄)T W (zι − µ̄)
ρι2 = exp [Ψ (α2 ) − Ψ (α1 + α2 + α3 + α4 ) + log Poccl (zι | x, m)]
ρι3 = exp [Ψ (α3 ) − Ψ (α1 + α2 + α3 + α4 ) + log Prand (zι | x, m)]
ρι4 = exp [Ψ (α4 ) − Ψ (α1 + α2 + α3 + α4 ) + log Pmax (zι | x, m)]
η = ρι1 + ρι2 + ρι3 + ρι4
rι1 = η −1 ρι1
rι2 = η −1 ρι2
rι3 = η −1 ρι3
rι4 = η −1 ρι4
end for
P
J1 = Jι=1 rι1
PJ
J2 = ι=1 rι2
P
J3 = Jι=1 rι3
P
J4 = Jι=1 rι4
P
z̄1 = J11 Jj=1 rι1 zι
P
C1 = J11 Jι=1 rι1 (zι − z̄1 ) (zι − z̄1 )T ,
α1 = α0 + J1 .
α2 = α0 + J2 .
α3 = α0 + J3 .
α4 = α0 + J4 .
β = β0 + J1
µ̄ = β1 (β0 µ̄0 + J1 z̄1 )
W −1 = W0−1 + J1 C1 +
ν = ν0 + J1

β0 J 1
β0 +J1

(z̄1 − µ̄0 ) (z̄1 − µ̄0 )T

1
π1 = α1 +α2α+α
3 +α4
α2
π2 = α1 +α2 +α3 +α4
3
π3 = α1 +α2α+α
3 +α4
α4
π4 = α1 +α2 +α3 +α4
p′ = 1−ππ32−π4
− 1

2
νβ
W
σm = 1+β
end while
return {α1 , α2 , α3 , α4 , β, µ̄, W, ν, Θ′′ = [µ, σm , p′ , π3 , π4 ]}

206

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

N

N

800

800

700

700

600

600

500

500

400

400

300

300

200

200

100

100

0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

z[m]

0

0.5

(a) Short range

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

z[m]

(b) Long range

Figure 11: Data for second learning experiment reported by Thrun et al. (2005). These
data consist of two series of measurements obtained with a mobile robot traveling through a typical office environment. From the set of measurements 10000
measurements that are centered around two different expected ranges are selected.

experimental pdf two ‘distance’ measures are used: the discrete KL-divergence:
d1 = KL (H||P ) ≈

F
X

H (zf ) log

f =1

H (zf )
,
P (zf | x, m, Θ)

and the square root of the discrete Hellinger distance:
v
u F 

uX
1
1 2
d2 = DH (H||P ) ≈ t
H (zf ) 2 − P (zf | x, m, Θ) 2 .

(102)

(103)

f =1

The latter is known to be a valid symmetric distance metric (Bishop, 2006).
5.1 First Learning Experiment
In a first learning experiment, the experimental data reported by Thrun et al. (2005) is used.
The data consists of two series of measurements obtained with a mobile robot traveling
through a typical office environment. From the set of measurements, 10000 measurements
that are centered around two different expected ranges, are selected. The two obtained
sets with different expected ranges are shown in Fig. 11. The parameters of the learning
algorithms are listed in Table 1. Fig. 12 and Table 2 show the results of the ML-EM and
VB-EM estimators for the RBBM compared to the results of the ML estimator for Thrun’s
model (Thrun et al., 2005) for these two sets. The results are obtained by running the
learning algorithms for 30 iteration steps.
207

De Laet, De Schutter & Bruyninckx

ML-EM
RBBM
σm,init = 0.5
p′init = 0.4
π3,init = 0.2
π4,init = 0.1

VB-EM
RBBM
α3,init = 18
p′init = 13
βinit = 5000 α4,init = 18
Winit = 12
β0 = 5
µ̄init = xmp
W0 = 50
νinit = 100
µ̄0 = xmp
α1,init = 58
ν0 = 100
1
α2,init = 8
α0 = 1

ML-EM
Thrun’s model
σm,init = 0.5
zhit,init = 0.4
zshort,init = 0.3
zmax,init = 0.1
zrand,init = 0.2
λshort,init = 0.1

Table 1: EM-parameters for first and second learning experiment (all in SI-units). In the
ML approaches, the mean of Phit (z | x, m) is set to xmp , i.e. the most probable
bin of the histogram of the training set H (z).

P (z | x, m)

P (z | x, m)

0.8

1.2
1.0
0.8

ML-EM RBBM
VB-EM RBBM
ML-EM Thrun’s model
Histogram Training set

0.7

ML-EM RBBM
VB-EM RBBM
ML-EM Thrun’s model
Histogram Training set

0.6
0.5
0.4

0.6

0.3

0.4

0.2
0.2

0.0

0.1
0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

z[m]

(a) Short range

0.0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

(b) Long range

Figure 12: Comparison of the results of the ML-EM and VB-EM estimators for the RBBM
and the results of a maximum likelihood estimator for Thrun’s model (Thrun
et al., 2005) for the data of Fig. 11.

The proposed ML-EM and VB-EM estimator outperform the ML-EM estimator for
Thrun’s model for the studied data sets. Despite the reduced number of parameters of
the RBBM compared to Thrun’s model (Section 3.6), the RBBM has at least the same
representational power.
208

z[m]

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

Experiment

short range
long range
average

ML-EM
RBBM
0.5295
0.4366
0.4830

d1 (Eq. (102))
VB-EM
ML-EM
RBBM Thrun’s model
0.5127
0.7079
0.4368
0.5852
0.4747
0.6465

ML-EM
RBBM
0.3166
0.1683
0.2425

d2 (Eq. (103))
VB-EM
ML-EM
RBBM Thrun’s model
0.2971
0.5629
0.2100
0.3481
0.2535
0.4555

Table 2: Discrete KL-divergence (d1 ) and square root Hellinger distance (d2 ) for the first
learning experiment between the training set and the results of the ML-EM and
VB-EM estimators for the RBBM and the ML-EM estimator for Thrun’s model
(Thrun et al., 2005).

(a) Front view

(b) Side view

(c) Zoomed front view

Figure 13: Setup for the second learning experiment with a Sick LMS 200 laser scanner
mounted on the first axis of an industrial Kuka 361 robot.

5.2 Second Learning Experiment
The data for the second learning experiment is gathered during the execution of a typical
industrial pick-and-place operation in a human-populated environment. A Sick LMS 200
laser scanner is mounted on the first axis of an industrial Kuka 361 robot (Fig. 13). The
laser scanner provides measurements of the robot environment and therefore of people and
other unexpected objects in the robot’s workspace. Processing these measurements is a first
step towards making industrial robots aware of their possibly changing environment and as
such of moving these robots out of their cages.
209

De Laet, De Schutter & Bruyninckx

Robot

4

selected ranges
cut out region
environment

2

y[m]

0

-2

-4

-6
-6

-4

-2

0

x[m]

2

4

6

Figure 14: Map build of the robot’s static environment, i.e. without any unexpected objects
or people moving around, by rotating the first axis of the industrial robot. For
safety reasons, people were not allowed to move inside the safety region (circle
with radius 1m). Therefore, measurements smaller than 1m are discarded. The
studied expected ranges in the second learning experiment range from 3.0m to
4.5m in steps of 0.1m and are indicated in the figure by the selected ranges
region.

In a first step, a map (Fig. 14) is build of the robot’s static environment, i.e. without
any unexpected objects or people moving around, by rotating the first axis of the industrial
robot. Next, the robot performs a pick-and-place operation while a number of people are
walking around at random in the robot environment. Different sets of measurements are
acquired each with a different number of people. Similar to the first learning experiment,
measurements are selected centered around different expected ranges from the acquired
data. The studied expected ranges in the second learning experiment range from 3.0m to
4.5m in steps of 0.1m (Fig. 14). For safety reasons, people were not allowed to move closer
than 1m to the robot, i.e. the safety region (Fig. 14). Therefore, measurements smaller
than 1m are discarded.
From the data, the model parameters are learning using the same learning parameters
as in the first learning experiment (Table. 1).
Table 3 shows the Kullback Leibler divergence (Eq. (102)) and the Hellinger distance
(Eq. (103)) averaged for the studied expected range for the different set of measurements
after running the ML-EM and VB-EM estimators for the RBBM and the ML estimator for
Thrun’s model (Thrun et al., 2005). The results were obtained after running each of the
learning algorithms for 30 iteration steps.
210

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

Experiment
number of
people
1
2
3
4
6
8
average

ML-EM
RBBM
1.7911
1.8002
1.7789
1.8277
1.8007
1.7676
1.7944

d1 (Eq. (102))
VB-EM
ML-EM
RBBM Thrun’s model
1.5271
1.9697
1.5172
1.9735
1.5199
1.9606
1.5140
1.9853
1.5168
1.9655
1.5157
1.9498
1.5185
1.9674

ML-EM
RBBM
5.6141
5.7038
5.6033
5.7563
5.6483
5.4989
5.6375

d2 (Eq. (103))
VB-EM
ML-EM
RBBM Thrun’s model
4.3449
6.5582
4.3334
6.6119
4.3468
6.5365
4.2972
6.6744
4.3126
6.5596
4.2843
6.4257
4.3199
6.5611

Table 3: Discrete KL-divergence (d1 ) and square root Hellinger distance (d2 ) averaged for
the studied expected range for the different set of measurements of the second
learning experiment. Distances are between the training set and the results of
the ML-EM and VB-EM estimators for the RBBM and the ML-EM estimator for
Thrun’s model (Thrun et al., 2005). The first column indicates the number of
people walking around in the environment in that particular set of measurements.

The proposed ML-EM and VB-EM estimator outperform the ML-EM estimator for
Thrun’s model for the studied data sets. Despite the reduced number of parameters of
the RBBM compared to Thrun’s model (Section 3.6), the RBBM has at least the same
representational power.

6. Adaptive Full Scan Model
This section extends the RBBM to an adaptive full scan model for dynamic environments;
adaptive, since it automatically adapts to the local density of samples when using samplebased representations; full scan, since the model takes into account the dependencies between individual beams.
In many applications using a range finder, the posterior is approximated by a finite
set of samples (histogram filter, particle filters). The peaked likelihood function associated
with a range finder (small σm due to its accuracy) is problematic when using such finite set
of samples. The likelihood P (z | x, m) is evaluated at all samples, which are approximately
distributed according to the posterior estimate. Basic sensor models typically assume that
the estimate x and the map m are known exactly, that is, they assume that one of the
samples corresponds to the true value. This assumption, however, is only valid in the limit
of infinitely many samples. Otherwise, the probability that a value exactly corresponds to
the true location is virtually zero. As a consequence, these peaked likelihood functions do
not adequately model the uncertainty due to the finite, sample-based representation of the
posterior (Pfaff et al., 2007). Furthermore, the use of a basic range finder model typically
results in even more peaked likelihood models, especially when using a large number of
beams per measurement, due to multiplication of probabilities. In practice, the problem
211

De Laet, De Schutter & Bruyninckx

P (z | x, m)

P (z | x, m)

2.5

2.5
ML-EM RBBM
VB-EM RBBM
ML-EM Thrun’s model
Histogram Training set

2.0

2.0

1.5

1.5

1.0

1.0

0.5

0.5

0.0
1.0

2.0

3.0

4.0

5.0

6.0

7.0

ML-EM RBBM
VB-EM RBBM
ML-EM Thrun’s model
Histogram Training set

8.0

0.0

z[m] 1.0

2.0

(a) Short range, 3 people

2.5

ML-EM RBBM
VB-EM RBBM
ML-EM Thrun’s model
Histogram Training set

2.0

6.0

7.0

8.0

z[m]

1.5

1.0

1.0

0.5

0.5

4.0

5.0

6.0

7.0

8.0

z[m]

ML-EM RBBM
VB-EM RBBM
ML-EM Thrun’s model
Histogram Training set

2.0

1.5

3.0

5.0

P (z | x, m)

2.5

2.0

4.0

(b) Short range, 8 people

P (z | x, m)

0.0
1.0

3.0

8.0

0.0

z[m] 1.0

(c) Long range, 3 people

2.0

3.0

4.0

5.0

6.0

7.0

(d) Long range, 8 people

Figure 15: Comparison of the results of the ML-EM and VB-EM estimators for the RBBM
and the results of a maximum likelihood estimator for two different expected
ranges and two different number of people populating the robot environment.

212

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

of peaked likelihoods, is dealt with in various ways: sub-sampling the measurement (fewer
beams); introducing minimal likelihoods for beams; inflating the measurement uncertainty;
or other means of regularization of the resulting likelihoods. These solutions are not satisfactory however, since the additional uncertainty due to the sample-based representation
is not known in advance. The additional uncertainty strongly varies with the number of
samples and the uncertainty of the estimate (Pfaff et al., 2006). Fox (2003) proposes to
dynamically adapt the number of samples by means of KLD sampling (KLD stands for
Kullback-Leibler divergence). For very peaked likelihoods however, this might result in a
huge number of samples. Lenser and Veloso (2000) and Thrun, Fox, Burgard, and Dellaert (2001) ensure that a critical mass of samples is located at the important parts of the
state space by sampling from the observation model. Sampling from the observation model
however, is often only possible in an approximate and inaccurate way. Pfaff et al. (2006)
introduced an adaptive beam model for dynamic environments, which explicitly takes location uncertainty due to the sample-based representation into account. They compute
the additional uncertainty due to the sample-based representation, using techniques from
density estimation. When evaluating the likelihood function at a sample, they consider a
certain region around the sample, depending on the sample density at that location. Then,
depending on the area covered by the sample, the variance of the Gaussian, σm , governing
the beam model in Eq. (38), is calculated for each sample. As a result, the beam model
automatically adapts to the local density of samples. Such a location dependent model results in a smooth likelihood function during global localization and a more peaked function
during position tracking without changing the number of samples.
Plagemann et al. (2007) and Pfaff et al. (2007) showed that by considering a region
around samples, the individual beams become statistically dependent. The degree of dependency depends on the geometry of the environment and on the size and location of the
considered region. Beam models, such as the RBBM, implicitly assume however that the
beams are independent, that is:
P (z | θ, x, m) =

B
Y

P (zb | θb , x, m) ,

(104)

b=1

where z = {zb }b=1:B and θ = {θb }b=1:B are the vectors containing the measured ranges
and the angles of the different beams respectively; zb is the range measured at the beam
with angle θb ; B is the total number of beams and P (zb | θb , x, m) is for instance the
RBBM (Eq. (45)). By neglecting the dependency between beams, the resulting likelihoods
P (z | θ, x, m) are overly peaked. Models taking into account the dependencies between
beams consider the full range scan and are therefore called full scan models further on.
The full scan models proposed by Plagemann et al. (2007) and Pfaff et al. (2007) both
assume that the beams of a range scan are jointly Gaussian distributed. The off-diagonal
elements of the covariance matrix associated with the Gaussian distribution represent the
dependency. To learn the model parameters, both methods draw samples from the region
around a sample and perform ray-casting using these samples. Plagemann et al. train a
Gaussian process which models the full scan, while Pfaff et al. directly provide a maximum
likelihood estimate for the mean and covariance of the Gaussian.
Section 6.1 shows that the dependency between beams may introduce multi-modality,
even for simple static environments. Multi-variate Gaussian models as proposed by Plage213

De Laet, De Schutter & Bruyninckx

mann et al. (2007) and Pfaff et al. (2007) cannot handle this multi-modality. Therefore, a
new sample-based method for obtaining an adaptive full scan model from a beam model,
able to handle multi-modality, is proposed. Section 6.2 extends the adaptive full scan model
for dynamic environments by taking into account non-Gaussian model uncertainty.
6.1 Sample-based Adaptive Full Scan Model for Static Environments
Plagemann et al. (2007) and Pfaff et al. (2007) estimate the full scan model, P (z | x, m)6 ,
based on a local environment U (x) of the exact estimate x:
Z
P (z | x, m) ≈ P (x̃ | x) Phit (z | x̃, m) dx̃,
(105)
with P (x̃ | x) the distribution representing the probability that x̃ is an element of the
environment U (x), i.e: x̃ ∈ U (x). The environment U (x) is modeled as a circular region
around x. Since this section does not consider the dynamics of the environment, only one
component of the RBBM in Eq. (37) is used: Phit (z | x, m). The marginalization over the
environment U (x) in Eq.(105) introduces dependencies between the measurements zb of the
measurement vector z.
The environment U (x), as explained above, depends on the sample density around
the sample x under consideration. Pfaff et al. (2006) proposed to use a circular region
with diameter dU (x) , which is a weighted sum of the Euclidean distance and the angular
difference. Like Plagemann et al. (2007) and Pfaff et al. (2007), an approximation of the
above likelihood can be estimated online for each sample x by simulating L complete range
scans at locations drawn from U (x) using the given map m of the environment. Contrary
to the multivariate Gaussian approximation proposed by Plagemann et al. and Pfaff et al.,
we propose a sample-based approximation, able to handle multi-modality. Sampling from
the environment U (x) immediately results in a sample-based approximation of P (x̃ | x):
L

P (x̃ | x) ≈

1X
δx̃(l) ,
L

(106)

l=1

where δx̃(l) denotes the delta-Dirac mass located in x̃(l) , and the samples are distributed
according to P (x̃ | x):
x̃(l) ∼ P (x̃ | x) .

(107)

Using this sample-based approximation of P (x̃ | x) the likelihood of Eq. (105) can be approximated as:
P (z | x, m) ≈

L


1X
Phit z | x̃(l) , m .
L

(108)

l=1

Since this sample-based approximation has to be calculated online, the number of samples
has to be limited. If the used environment U (x) is large, the resulting approximation will be
6. To simplify the notation θ and θ are omitted from P (z | θ, x, m) and P (zb | θb , x, m), respectively.

214

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

Figure 16: Panorama taken from the Sick LMS 200 range finder mounted on a Kuka 361
industrial robot. The environment consists of a rectangular ‘room’ with an
object (a Kuka KR 15/2 robot) in the middle. We show that even for this simple
static environment, the presented sample-based full scan model outperforms the
Gaussian-based state of the art models.

bad. To smooth the undesired bumpy behavior due to the limited number of samples, the
measurement noise σm governing Phit (z | x, m) in Eq. (45), is artificially increased depending
on the size of U (x) by multiplying it with a factor:
q
1 + C dU (x) .
(109)
In further experiments, C was set to 20.

6.1.1 Experiment
A simple environment consisting of a rectangular ‘room’ with an object (a Kuka KR 15/2
robot) in the middle (Fig. 16) is used to show that the marginalization over (even small)
U (x) to obtain the true likelihood not only introduces dependencies between the beams but
also multi-modality. The U (x) results from a local uncertainty on the x- and y−position
of 0.01m and a rotational uncertainty of 5◦ . To obtain a reference, a Sick LMS 200 range
finder is used to take a large number of measurements (L = 1500) at random locations
sampled in U (x). To allow for exact positioning, the Sick LMS 200 is placed on a Kuka 361
industrial robot. The Sick LMS 200 range finder is connected to a laptop that controls the
motion of the Kuka 361 industrial robot over the network using Corba-facilities in the Open
Robot Control Software, Orocos (Bruyninckx, 2001; Soetens, 2006). A simplified map of the
environment (Fig. 16) is built to simulate the 150 complete range scans needed to construct
a full scan model. The marginal P (zb | x, m) of two selected beams are studied in more
detail. The marginal likelihoods for the selected beam using the proposed sample-based approximation of Eq. (108) and the Gaussian approximation proposed by Pfaff et al. (2007),
are compared in Fig. 17(b)-17(c). The histogram of the measurements of the selected beam
in this figure clearly shows the multi-modality of the likelihood caused by the dependency
between beams. In contrast to the Gaussian-based state of the art full scan model, the proposed sample-based approximation is able to handle the multi-modality of the range finder
215

De Laet, De Schutter & Bruyninckx

data. Fig. 17(d) shows the difference for all beams between the experimentally obtained
cumulative marginal (L = 1500) and the Gaussian-based and sample-based approximation
for all beams. The mean difference with the experimental data for the sample-based approximation is 2.8 times smaller than the difference for the Gaussian-based approximation,
even for the simple static environment of Fig. 16 and the small U (x).
6.2 Sample-based Adaptive Full Scan Model for Dynamic Environments
The adaptive beam model proposed by Pfaff et al. (2006) is suited for use in dynamic
environments since it uses the four component mixture beam model (Thrun et al., 2005;
Choset et al., 2005). To date however, the adaptive full scan likelihood models of Pfaff et al.
(2007) and Plagemann et al. (2007) have not been adapted for dynamic environments. The
assumption that the beams are jointly Gaussian distributed, unable to capture the nonGaussian uncertainty due to environment dynamics, prevents the straightforward extension
for dynamic environments. In contrast, the sample-based approximation of the full scan
likelihood, as proposed in Section 6.1, can be extended to include environment dynamics.
To this end, replace Phit (z | x, m) in Eq. (105) and Eq. (108) by the full mixture of Eq. (38).
6.2.1 Experiment
Fig. 18(a) and Fig. 18(b) compare the marginals for the selected beams (Fig. 17(a)) obtained
from the adaptive full scan model for dynamic environments using the proposed samplebased approximation and the Gaussian approximation proposed by Pfaff et al. (2007). In
contrast to the Gaussian-based state of the art full scan model, the proposed sample-based
approximation is able to handle the multi-modality of the range finder data. Fig. 18(c) shows
a probability map of the adaptive full scan model (sample-based approximation) suited for
dynamic environments for the example environment of Fig. 16. The probability map plots
P (z | x, m) as a function of the position in the map and shows that the marginalization over
the environment U (x) of a sample in Eq. (105) not only introduces dependency between
beams but also introduces multi-modality.

7. Discussion
This paper proposed and experimentally validated the RBBM, a rigorously Bayesian network model of a range finder adapted to dynamic environments. All modeling assumptions
are rigorously explained, and all model parameters have a physical interpretation. This
approach resulted in a transparent and intuitive model. The rigorous modeling revealed all
underlying assumptions and parameters. This way a clear physical interpretation of all parameters is obtained providing intuition for the parameter choices. In contrast to the model
of Thrun et al. (2005), the assumption underlying the non-physical discontinuity in the
RBBM is discovered. Furthermore, the paper proposes a different functional form for the
probability of range measurements caused by unmodeled objects Poccl (z | x, m) (Eq. (45)),
i.e. quadratic rather than exponential as proposed by Thrun et al. Furthermore, compared
to the work of Thrun et al. (2005), Choset et al. (2005), Pfaff et al. (2006) the RBBM
depends on fewer parameters, while maintaining the same representational power for experimental data. Bayesian modeling revealed that both the rate of decay of Poccl (z | x, m) and
216

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

5

4.5

4

3.5

3.5

3

2.5

P (zb | x, m)

y[m]

3

2.5

2

1.5

2

1.5

1

1

Environment
Some measurements
Samples of U (x)
Example beams

0.5

0
−2.5

Experimental data (1500 samples)
Sample-based approximation
Gaussian approximation (Pfaff et al., 2007)

4

−2

−1.5

−1

−0.5

0

0.5

1

1.5

2

0.5

0

2.5

0

1

x[m]
(a) Environment model

2

3

4

5

6

z[m]

(b) Marginal likelihood for left beam
−3

7

Experimental data (1500 samples)
Sample-based approximation
Gaussian approximation (Pfaff et al., 2007)

2.5

x 10

Sample-based approximation
Gaussian-based approximation (Pfaff et al., 2007)

6

2

∆Pc (z | x, m)

P (zb | x, m)

5

1.5

1

4

3

2

0.5

1

0

0

1

2

3

4

5

6

z[m]

(c) Marginal likelihood for right beam

0

0

50

100

150

200
beam

250

300

350

400

(d) Difference between experimental cumulative
marginal and Gaussian and sample-based approximations

Figure 17: Experimental results for sample-based adaptive full scan model for static environments. (a) models the simple environment of Fig. 16. The range finder
is located at (0.15m, 0.75m). Samples from U (x) (resulting from a local uncertainty on the x- and y−position of 0.01m and a rotational uncertainty of
5◦ ) are shown with black dots, and some simulated measurements are shown in
grey. (b) and (c) show the marginal likelihood P (zb | x, m) for the two selected
beams together with the histogram of the experimentally recorded range finder
data, the Gaussian-based approximation (L = 150) of Pfaff et al. (2007), and
the sample-based approximation (L = 150) of this paper. (d) shows the difference for all beams between the experimentally obtained cumulative marginal
(L = 1500) and the Gaussian-based and sample-based approximation.
217

De Laet, De Schutter & Bruyninckx

0.9

0.7

Large sample approximation
Sample-based approximation
Gaussian appr. (Pfaff, 2007)

0.8

Large sample approximation
Sample-based approximation
Gaussian appr. (Pfaff, 2007)

0.6
0.7
0.5

P (z | x, m)

P (z | x, m)

0.6

0.5

0.4

0.4

0.3

0.3
0.2
0.2
0.1
0.1

0

0

1

2

3

4

5

6

0

0

1

2

z[m]

3

4

5

z[m]

(a) Marginal likelihood for left beam

(b) Marginal likelihood for right beam

(c) Probability map P (z | x, m) from sample-based
approximation

Figure 18: Results for sample-based adaptive full scan model for dynamic environments. (a)
and (b) show the marginal likelihood P (zb | x, m) for the two selected beams of
Fig. 17(a) together with the Gaussian-based approximation (L = 150) of Pfaff
et al. (2007) and the sample-based approximation (L = 150) extended for the
use in dynamic environments. (c) shows the probability map resulting from
the sample-based approximation. The probability map shows P (z | x, m) as a
function of the x- and y− position in the map.

218

6

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

the probability of an occluded measurement π2 depend on one parameter p′ . State of the art
sensor models however, assume independency of these two parameters. Finally, a maximumlikelihood and a variational Bayesian estimator (both based on expectation-maximization)
were proposed to learn the model parameters of the RBBM. Learning the model parameters
from experimental data benefits from the RBBM’s reduced number of parameters. Using
two sets of learning experiments from different application areas in robotics (one reported
by Thrun et al. (2005)) the RBBM was shown to explain the obtained measurements at
least as well as the state of the art model of Thrun et al.
Furthermore, the paper extended the RBBM to an adaptive full scan model in two
steps: first, to a full scan model for static environments and next, to a full scan model for
general, dynamic environments. The full scan model adapts to the local sample density
when using a particle filter, and accounts for the dependency between beams. In contrast
to the Gaussian-based state of the art models of Plagemann et al. (2007) and Pfaff et al.
(2007), the proposed full scan model uses a sample-based approximation, which can cope
with dynamic environments and with multi-modality (which was shown to occur even in
simple static environments).

Acknowledgments
The authors thank the anonymous reviewers for their thorough and constructive reviews.
The authors also thank Wilm Decré, Pauwel Goethals, Goele Pipeleers, Ruben Smits,
Bert Stallaert, Lieboud Van den Broeck, Marnix Volckaert and Hans Wambacq for participating in the experiments. All authors gratefully acknowledge the financial support by
K.U.Leuven’s Concerted Research Action GOA/05/10 and the Research Council K.U.Leuven,
CoE EF/05/006 Optimization in Engineering (OPTEC). Tinne De Laet is a Doctoral Fellow
of the Fund for Scientific Research–Flanders (F.W.O.) in Belgium.

Appendix A. Simplification of Infinite Sum
To goal of this appendix is to prove that


∞ 
X
X
n
n−k
k
n
u (1 − u)
(1 − p) p ,
P (k | n, x, m) P (n) =
k
n

(110)

n=k

can be simplified to:

X
n


P (k | n, x, m) P (n) = 1 − p′ p′k ,

up
with p′ = 1−(1−u)p
.
Expand Eq. (110) and move terms out of the summation so that:

∞ 
X
X
n!
n−k
k k
[(1 − u) p]
.
P (k | n, x, m) P (n) = (1 − p) u p
(n − k)!k!
n

(111)

(112)

n=k

Next introduce variables t = n − k and a e = (1 − u) p:
X
n


∞ 
X
(t + k)! t
P (k | n, x, m) P (n) = (1 − p) u p
e .
t!k!
k k

t=0

219

(113)

De Laet, De Schutter & Bruyninckx

The next step is to prove by induction that:

∞ 
X
1
(t + k)! t
e =
.
k+1
t!k!
(1
−
e)
t=0

(114)

First, show that the above equality holds for k = 0:
∞
X
t!
t=0

t!

t

e =

∞
X

et ,

(115)

t=0

which is the well-known geometric series, so:
∞
X

et =

t=0

1
,
1−e

(116)

showing that equality (114) indeed holds for k = 0. Next it is proved that, if the expression
holds for k − 1, it also holds for k. Introduce variable V for the solution of the infinity sum
for k and split up (114) in two parts:

 X

∞ 
∞ 
∞ 
X
(t + k)! t
(t + k)! (t + k − 1)! t X (t + k − 1)! t
(117)
V =
e =
−
e ,
e +
t!k!
t!k!
t! (k − 1)!
t! (k − 1)!
t=0
t=0
{z
}
|t=0
1
(1−e)k

where the fact is used that the equality (114) holds for k − 1. Simplify the first term of the
summation to:


∞ 
∞ 
X
(t + k)! (t + k − 1)! t X (t + k − 1)! t
−
te .
(118)
e =
t!k!
t! (k − 1)!
t!k!
t=0

t=0

The term in the summation for t = 0 is equal to zero. Hence, introduce the variable t′ = t−1
and simplify:


∞ 
∞ 
X
X
 t′ +1
(t′ + k)! ′
(t + k)! (t + k − 1)! t
t +1 e
(119)
e =
−
t!k!
t! (k − 1)!
(t′ + 1)!k!
′
t=0
t =0

∞  ′
X
(t + k)! t′
= e
e
,
(120)
t′ !k!
′
|t =0
{z
}
V

from which the series V we were looking for is recognized. Substitute the above result in
Eq. (117) so that:
V = eV +

1
(1 − e)k

.

(121)

Solving this equation for V gives:
V =

1
(1 − e)k+1
220

,

(122)

Rigorously Bayesian Beam Model and Adaptive Full Scan Model

proving that equality (114) holds for k if it assumed to hold for k − 1, and closing the proof
by induction.
Now substitute Eq. (114) in Eq. (113):
X

P (k | n, x, m) P (n) =

n

(1 − p) uk pk
[1 − (1 − u) p]k+1

,

(123)

or rewrite as:
X
n

where p′ =

up
1−(1−u)p ,


P (k | n, x, m) P (n) = 1 − p′ p′k ,

(124)

what is exactly what had to be proved.

References
Beal, M. J. (2003). Variational algorithms for approximate Bayesian inference. Ph.D. thesis,
University College London.
Beal, M. J., & Ghahramani, Z. (2003). The variational bayesian em algorithm for incomplete data: with application for scoring graphical model structures. In Valencia
International Meeting on Bayesian Statistics, Tenerife, Canary Islands, Spain.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer.
Bruyninckx, H. (2001). Open RObot COntrol Software. http://www.orocos.org/.
Burgard, W., Fox, D., Hennig, D., & Schmidt, T. (1996). Estimating the absolute position
of a mobile robot using position probability grids. In Proc. of the National Conference
on Artificial Intelligence.
Choset, H., Lynch, K. M., Hutchinson, S., Kantor, G. A., Burgard, W., Kavraki, L. E., &
Thrun, S. (2005). Principles of Robot Motion: Theory, Algorithms, and Implementations. MIT Press.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm (with discussion). Journal of the Royal Statistical Society
(Series B), 39, 1–38.
Fox, D. (2003). Adapting the Sample Size in Particle Filters Through KLD-Sampling. The
International Journal of Robotics Research, 22 (12), 985–1003.
Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization for mobile robots in dynamic
environments. Journal of Artificial Intelligence Research, 11, 391–427.
Hähnel, D., Schulz, D., & Burgard, W. (2003a). Mobile robot mapping in populated environments and sensor planning. Journal of the Advanced Robotics, 17 (7), 579–597.
Hähnel, D., Triebel, R., Burgard, W., & Thrun, S. (2003b). Map building with mobile robots
in dynamic environments. In Proceedings of the 2003 IEEE International Conference
on Robotics and Automation, pp. 1557–1569, Taipeh, Taiwan. ICRA2003.
Jeffreys, W., & Berger, J. (1992). Ockham’s razor and bayesian analysis. American Scientist,
80, 64–72.
221

De Laet, De Schutter & Bruyninckx

Jensen, F. V., & Nielsen, T. D. (2007). Bayesian Networks and Decision Graphs. Springer.
Lenser, S., & Veloso, M. (2000). Sensor resetting localization for poorly modelled mobile
robots. In Proceedings of the 2000 IEEE International Conference on Robotics and
Automation, San Francisco, CA. ICRA2000.
McLachlan, G. J., & Krishnan, T. (1997). The EM algorithm and extensions. John Wiley
& Sons, New York, NY.
Moravec, H. P. (1988). Sensor fusion in certainty grids for mobile robots. AI Magazine, 9,
61–74.
Neapolitan, R. E. (2004). Learning Bayesian Networks. Pearson Prentice Hall, New York,
NY.
Pfaff, P., Burgard, W., & Fox, D. (2006). Robust Monte-Carlo localization using adaptive
likelihood models. In Christensen, H. (Ed.), European Robotics Symposium, Vol. 22,
pp. 181–194, Palermo, Italy. Springer-Verlag Berlin Heidelberg, Germany.
Pfaff, P., Plagemann, C., & Burgard, W. (2007). Improved likelihood models for probabilistic
localization based on range scans. In Proceedings of the 2007 IEEE/RSJ International
Conference on Intelligent Robots and Systems, San Diego, California. IROS2007.
Plagemann, C., Kersting, K., Pfaff, P., & Burgard, W. (2007). Gaussian beam processes: A
nonparametric Bayesian measurement model for range finders. In Robotics: Science
and Systems (RSS), Atlanta, Georgia, USA.
Rasmussen, C., & Ghahramani, Z. (2000). Occam’s razor. In Advances in Neural Information Processing 13, Denver, Colorado. MIT Press.
Soetens, P. (2006). A Software Framework for Real-Time and Distributed Robot and Machine Control. Ph.D. thesis, Department of Mechanical Engineering, Katholieke Universiteit Leuven, Belgium. http://www.mech.kuleuven.be/dept/resources/docs/
soetens.pdf.
Thrun, S., Burgard, W., & Fox, D. (2005). Probabilistic Robotics. MIT Press.
Thrun, S. (2001). A probabilistic online mapping algorithm for teams of mobile robots. The
International Journal of Robotics Research, 20 (5), 335–363.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2001). Robust monte carlo localization for
mobile robots. Artificial Intelligence, 128, 99–141.
Wang, C.-C., Thorpe, C., & Thrun, S. (2003). Online simultaneous localization and mapping with detection and tracking of moving objects: Theory and results from a ground
vehicle in crowded urban areas. In Proceedings of the 2003 IEEE International Conference on Robotics and Automation, Taipeh, Taiwan. ICRA2003.
Wolf, D. F., & Sukhatme, G. S. (2004). Mobile robot simultaneous localization and mapping
in dynamic environments. In Proceedings of the 2004 IEEE International Conference
on Robotics and Automation, pp. 1301–1307, New Orleans, U.S.A. ICRA2004.

222

Journal of Artificial Intelligence Research 33 (2008) 433-464

Submitted 07/08; published 11/08

An Ordinal Bargaining Solution with Fixed-Point Property
Dongmo Zhang
Yan Zhang

dongmo@scm.uws.edu.au
yan@scm.uws.edu.au

Intelligent Systems Laboratory
School of Computing and Mathematics
University of Western Sydney, Australia

Abstract
Shapley’s impossibility result indicates that the two-person bargaining problem has no
non-trivial ordinal solution with the traditional game-theoretic bargaining model. Although
the result is no longer true for bargaining problems with more than two agents, none of the
well known bargaining solutions are ordinal. Searching for meaningful ordinal solutions,
especially for the bilateral bargaining problem, has been a challenging issue in bargaining
theory for more than three decades. This paper proposes a logic-based ordinal solution
to the bilateral bargaining problem. We argue that if a bargaining problem is modeled in
terms of the logical relation of players’ physical negotiation items, a meaningful bargaining
solution can be constructed based on the ordinal structure of bargainers’ preferences. We
represent bargainers’ demands in propositional logic and bargainers’ preferences over their
demands in total preorder.
We show that the solution satisfies most desirable logical
properties, such as individual rationality (logical version), consistency, collective rationality
as well as a few typical game-theoretic properties, such as weak Pareto optimality and
contraction invariance. In addition, if all players’ demand sets are logically closed, the
solution satisfies a fixed-point condition, which says that the outcome of a negotiation is
the result of mutual belief revision. Finally, we define various decision problems in relation
to our bargaining model and study their computational complexity.

1. Introduction
Bargaining has been a central research topic in economics for over five decades and has
become an interesting issue in computer science in recent years (Osborne & Rubinstein,
1990; Rosenschein & Zlotkin, 1994; Muthoo, 1999). In his ground-breaking paper, Nash
(1950) models a bargaining problem as a pair (S, d), where S ⊆ <2 is a subset of twodimensional Euclidean space (feasible set), representing a set of utility pairs that can be
derived from bargainers’ preferences on feasible outcomes, and d is a point in D designated
to be the “disagreement point”. A bargaining solution is then a function that assigns each
bargaining problem (S, d) a point in S (Thomson, 1994).
The Nash bargaining model presumes that bargainers’ preferences are represented by
von Neumann-Morgenstern utility, referred to as cardinal utility (Myerson, 1991). Under
such an assumption, two utility functions can be viewed as the same if one can be derived
from the other by an affine positive transformation. Thus a bargaining solution based on
Nash’s bargaining model should be invariant under any affine positive transformations.
However, traditional economic theory considers bargaining problems in which players’ preferences are represented in ordinal (Calvo & Peters, 2005). Therefore, ideally, a bargaining
c
°2008
AI Access Foundation. All rights reserved.

Zhang & Zhang

solution should be invariant under any order-preserving transformations on utilities. This
property is referred to as ordinal invariance in the game-theoretic literature (Thomson,
1994). A bargaining solution that possesses such a property is called an ordinal solution.
Obviously, ordinal bargaining solutions are more desirable than cardinal solutions because
ordinal information about players’ preferences is easier to elicit than cardinal preferences
and the corresponding solutions can be more robust (Sakovics, 2004; Calvo & Peters, 2005).
However, none of the well known bargaining solutions (Nash, 1950; Kalai & Smorodinsky,
1975; Kalai, 1977; Perles & Maschler, 1981) are ordinal. In fact, Shapley (1969) showed
that for the two-person bargaining problem (bilateral bargaining) there is no non-trivial
(i.e., strongly individual rational) ordinal solution1 . The result is generally referred to as
Shapley’s impossibility result in the game-theoretic literature.
Shapley’s negative result obviously discouraged the investigation of ordinal bargaining,
notwithstanding Shapley himself demonstrated ten years later that ordinal solutions exist
for the three-person bargaining problem (Shubik, 1982). The study of ordinal bargaining
theory did not regain the focus of game theory until very recently. Kibris (2001) provided an
axiomatic characterization for an ordinal solution of the three-person bargaining problem
based on Nash’s bargaining model2 . Safra and Samet (2004) extended the result to the
bargaining problems with more than three players. Rubinstein et al. (1992) and O’Neill et
al. (2004) investigated the ordinal bargaining problem by varying Nash’s bargaining model.
Calvo and Perers (2005) explored the problem of ordinal bargaining in which at least one
player is cardinal. Nevertheless, the problem of ordinal bargaining is still considered to be an
unsolved problem. Most of these pierces of work focus on the existence of ordinal solutions.
None of these proposed solutions gains strong intuitive support. Looking for meaningful
ordinal bargaining solutions is still an outstanding problem in game theory (Sakovics, 2004).
To show the difficulty of ordinal bargaining, let us consider a simple but typical bargaining
scenario:
Example 1 (Muthoo, 1999) Two players, A and B, bargain over the partition of a cake.
Let xi be the share of the cake in percentage to player i (i = A, B). The set of possible
agreements is represented by Ω = {(xA , xB ) : 0 ≤ xA ≤ 100 and xB = 100 − xA }. For each
xi ∈ [0, 100], ui (xi ) is player i’s utility from obtaining a share xi of the cake.
Assume that player A has a linear utility scale of its share, uA (xA ) = xA , and player
B has a utility scale that is proportional to the square of his share, uB (xB ) = x2B . Failure to agree is rated 0 by both A and B. Consider two most influential bargaining solutions: Nash’s solution (Nash, 1950) and Kalai-Smorodinsky’s solution (Kalai & Smorodinsky, 1975). It is easy to calculate that Nash’s bargaining solution to the problem gives the
outcome (33.3, 66.7) and Kalai-Smorodinsky’s solution gives (38.2, 61.8). Both solutions are
in favor of player B. This is because player B is less risk-averse (has concave utility) than
player A (has linear utility). For both Nash’s solution and Kalai-Smorodinsky’s solution,
risk-loving players has advantage in bargaining comparing to risk-neutral and risk-averse
players (see Roth’s book, 1979a, p.35-60). Now consider an order-preserving transformation
√
τ (x) = x on player B’s utility. The transformed utility of player B becomes linear. Under
1. See the work of Thomson (1994) for an easy proof.
2. The original work was not formally published. Kibris (2004) gave a brief note.

434

An Ordinal Bargaining Solution with Fixed-Point Property

the new utility scales, both Nash’s solution and Kalai-Smorodinsky’s solution give (50, 50)
as outcome. This means that none of the solutions is ordinally invariant.
This example clearly shows that the non-linearity of utility functions, which expresses
the risk posture of a player, determines the outcomes of bargaining but collapses under ordinal transformations. In other words, ordinal transformations filter out useful information
that is expressible by cardinal utility but not expressible by ordinal utility. This explains
why “no resolution of the two-player bargaining problem can be made on the basis of ordinal
utility alone .... A satisfactory theory of bilateral bargaining requires knowledge of something more than just an ordering of the bargainers’ preferences” (see Shubik’s book, 1982,
p.94-98).
All in all, ordinal preference is insufficient to fully specify a bargaining situation. A
bargaining model must supply a way to express the information additional to ordinal preferences, such as bargainers’ attitude towards risk. This article aims to demonstrate with an
ordinal solution to the bilateral bargaining problem that the language of logic can be used
to express the knowledge that is required for modeling bargaining with ordinal preferences.
In recent years, the studies on logic-based frameworks of bargaining and negotiation
have received considerable attention in the field of artificial intelligence (AI) (Sycara, 1990;
Kraus, Sycara, & Evenchik, 1998; Parsons, Sierra, & Jennings, 1998; Zhang, Foo, Meyer,
& Kwok, 2004; Meyer, Foo, Kwok, & Zhang, 2004; Zhang, 2005, 2007). These frameworks
utilize logical languages to represent bargaining situations so that physical negotiation items,
bargaining conflicts, players’ beliefs and mutual threats can be explicitly expressed, which
differentiates themselves from the traditional game-theoretic models. In this paper we
propose an ordinal solution to the bilateral bargaining problem based on the logical model
introduced by Zhang and Zhang (2006a).
The organization of the paper is the following. Section 2 presents the formal model of
bargaining. We will use a finite propositional language to describe bargainers’ demands.
The bargainers’ preferences on their demands are sorted in total preorder. A bargaining
problem is then defined as a pair of hierarchies of two parties’ demand sets. The construction
of the bargaining solution, presented in Section 3, is based on the idea that each party tries
to maximize their prior demands to be included in the final agreement while keeping the
outcome to be consistent. The approach can be viewed as an extension of Nebel’s prioritized
base revision to the two-agent setting (Nebel, 1992). Section 4 and Section 5 are devoted
to the discussions on the properties of the proposed solution. We shall prove in Section
4 that the solution satisfies most desirable logical properties, such as the logical version
of Individual Rationality, Consistency and Collective Rationality. More extraordinarily,
we shall show that the solution satisfies a desirable fixed-point condition introduced by
Zhang et al. (2004), which says that the outcome of bargaining is the result of mutual
belief revision. Section 5 focuses on the discussion of the game-theoretic properties of the
solution. We prove that the solution satisfies Weak Pareto Optimality, Restricted Symmetry
and Contraction Invariance. Section 6 is devoted to a discussion of how bargainers’ attitudes
towards risk are represented in our model and how they determine players’ bargaining power.
Section 7 investigates the complexity issues related to the proposed model. We consider four
major decision problems in relation to a bargaining game and provide their computational
435

Zhang & Zhang

complexity results. The final two sections conclude the work with a discussion of the related
work.
The paper is made to be self-contained. However, the reader will find that basic knowledge in belief revision and game-theoretic bargaining theory can be helpful for a better
understanding of the concepts introduced in the paper. For an introductory survey of the
areas, see Gärdenfors’s article (1992) and Thomson’s article (1994), respectively.

2. Representation of Bargaining Problems
As we have seen in Example 1, an ordinal solution to the bilateral bargaining problem
requires information in addition to ordinal preferences. In order to express such extra information, we model a bargaining situation in two aspects: the physical bargaining terms,
described in propositional logic, and the ordering of the bargainers’ preferences on their
bargaining terms, described in total preorder. Since the bilateral bargaining is the most
challenging problem to bargaining theory, we shall restrict ourselves to the bargaining problem with only two players.
2.1 Preliminaries
We assume that each party has a set of negotiation items, referred to as demand set,
described by a finite propositional language L. The language consists of a finite set of
propositional variables and the propositional connectives ¬, ∨,∧, → and ↔ with standard
syntax and semantics. The logical closure operator Cn is defined as Cn(X) = {ϕ : X ` ϕ},
where X is a set of sentences. We say X to be logically closed or a belief set if X = Cn(X).
If X and Y are two sets of sentences, X + Y denotes Cn(X ∪ Y ).
Suppose that X1 and X2 are two sets of sentences. To simplify exploration, we use X−i
to represent the other set among X1 and X2 if Xi is one of them. If D is a vector of two
components, D1 and D2 will represent each component of D.
2.2 Bargaining Games
As shown in the previous section, cardinal utility encodes two types of information: bargainers’ attitude towards risk (via non-linearity of utility functions) and bargainers’ preferences
on possible outcomes (via the ordering of utility values). One may ask whether a theory of
bargaining can be based purely on ordinal information about preferences. In order to investigate such a possibility, Osborne and Rubinstein introduced a different way to represent a
bargaining situation, with which the preference information is separated from the physical
bargaining terms (Osborne & Rubinstein, 1990). Precisely, they define a bargaining problem as a four-tuple (X, D, ≥1 , ≥2 ), where X is a set of feasible outcomes (in physical terms),
D is the disagreement event, and ≥i is a complete transitive reflexive ordering over the set
X ∪ D, representing bargainer i’s preferences. As shown by Example 1, however, simply
describing physical bargaining terms without specifying their relations does not suffice to
lead to an ordinal solution (see also Osborne & Rubinstein’s book, 1990, p.32). In this
paper, we further extend Osborne & Rubinstein’s model in such a way that the physical
negotiation items are represented in logical formulae.
436

An Ordinal Bargaining Solution with Fixed-Point Property

Definition 1 A bargaining game is a pair ((X1 , º1 ), (X2 , º2 )), where Xi (i = 1, 2) is a
logically consistent set of sentences in L and ºi is a complete transitive reflexive order
(total preorder or weak order) over Xi that satisfies the following logical constraint3 :
(LC) If ϕ1 , · · · , ϕn ` ψ, then there is k (1 ≤ k ≤ n) such that ψ ºi ϕk .
We call the pair (Xi , ºi ) the prioritized demand set of player i. For any ϕ, ψ ∈ Xi , ψ Âi ϕ
denotes that ψ ºi ϕ and ϕ 6ºi ψ. ψ ≈i ϕ denotes that ψ ºi ϕ and ϕ ºi ψ.
Intuitively, a bargaining game is the formal representation of a bargaining situation in
which each player describes his demands in logical formulae and expresses his preferences
on his demands in total preorder. We assume that each player has consistent demands. The
preference ordering of each player reflects the degree of entrenchment in which the player
defends his demands. The logical constraint LC says that if the demand ψ is a logical
consequence of the demands ϕ1 , · · · , ϕn , then ψ should not be less entrenched than all the
ϕi because if you fail to defend ψ, at least one of the ϕi has to be dropped (otherwise you
would not have lost ψ). It is easy to see that such an ordering is similar to Gädenfors and
Makinson’s epistemic entrenchment (Gärdenfors & Makinson, 1988). In fact, the logical
constraint LC, introduced by Zhang and Foo (2001), is actually the combination of the
postulates EE2 and EE3 of epistemic entrenchment ordering.
Observation 1 Let º be a total preorder on X. The logical constraint LC is equivalent to
the conjunction of the following conditions:
1. If ϕ ` ψ, then ψ º ϕ.
2. Either ϕ ∧ ψ º ϕ or ϕ ∧ ψ º ψ.
Proof: It is easy to verify that LC implies these two conditions. We now prove that the
conditions imply LC by induction on n. Obviously, LC holds when n = 1. Assume that LC
holds when n = l. Suppose that ϕ1 , · · · , ϕl−1 , ϕl , ϕl+1 ` ψ, then ϕ1 , · · · , ϕl−1 , (ϕl ∧ϕl+1 ) ` ψ.
By the inductive assumption, either ψ º ϕl ∧ ϕl+1 or there is k (1 ≤ k ≤ l − 1) such that
ψ º ϕk . If ψ º ϕl ∧ ϕl+1 , by the condition 2 and transitivity of º, either ψ º ϕl or
ψ º ϕl+1 . Therefore in any case there is k (1 ≤ k ≤ l + 1) such that ψ º ϕk .
¶
We remark that the preference ordering represents how firmly an agent entrenches her
demands rather than her gain or payoff4 . For instance, suppose that p1 represents the
demand of a seller “the price of the good is no less than $10” and p2 denotes “the price
of the good is no less than $8”. Obviously the seller could get higher payoff from p1 than
p2 . However, since p1 implies p2 , she will entrench p2 no less firmly than p1 , i.e., p2 º p1 ,
3. A complete transitive reflexive order, i.e., total preorder or weak order, satisfies the following properties:
• Completeness or totality: ϕ º ψ or ψ º ϕ.
• Reflexivity: ϕ º ϕ.
• Transitivity: if ϕ º ψ and ψ º χ then ϕ º χ.
4. We will define bargainers’ gains in Section 5.1.

437

Zhang & Zhang

because, if she fails to keep p1 , she can still bargain for p2 but the loss of p2 means the loss
of both.
There is another significant difference between our model of bargaining and the gametheoretic model. The game-theoretic model abstracts a bargaining situation into a numerical
game. The demands, preferences and risk posture of a player are all represented in utility
values. In our model, however, these factors are abstracted into logical statements and
ordering on these statement, i.e., a prioritized demand set. Note that we have endowed
the word “demand” with a broad meaning. In our model, a demand of a player can be
everything that is related to the negotiation and that the player wants to keep in the final
agreement. It can be a physical item the player wants to obtain from the other party. It can
also be a piece of knowledge, a belief, a goal, a constraint or even a thread, whatever, the
player wants to keep in the agreement. Different from the logics for rational agency, such
as the BDI logics, we do not distinguish knowledge, belief, goal from the “real” demands.
For instance, a logical tautology can be a demand of a player if the player considers that
it should be included in the final agreement. Consider Example 1 again. Typical demands
of player A are xA ≥ 40, xA ≥ 50, xA ≥ 60, and so on5 , which mean that the player
wants to get a share of the cake no less than 40%, 50%, 60%, · · ·. Player B’s demands
are the opposite, say xB ≥ 40, xB ≥ 50, xB ≥ 60, · · ·. Besides these “real” demands,
there are a set of domain constraints in the players’ demand sets, such as xA + xB ≤ 100,
xA ≥ y → xA ≥ z and xB ≥ y → xB ≥ z whenever y ≥ z. These constraints link all
the demands of the players together, therefore, play an important rule in the determination
of bargaining outcomes. For instance, we cannot have both xA ≥ 55 and xB ≥ 55 in
the final agreement because they are inconsistent with the constraints. One may wonder
what if a player does not include these constraints in her demand set. In this case, the
player would have to accept unreasonable results, such as xA ≥ 55 and xB ≥ 55. In other
words, if “we idealize the bargaining problem by assuming that the two individuals are highly
rational ” (see the work of Nash, 1950, p.155), we should assume that these constraints and
background knowledge are included in the demand set of each player (see more discussions
on this example in Section 6). Again, the ordering of demand sets does not reflect the gains
of a player as we mentioned above. It is very likely that a rational agent could give her
highest priority to the above mentioned constraints and background knowledge since she
might be never going to give up these fundamental rules.
The following example shows that our model is more suitable for discrete domain of
problems.
Example 2 A couple are making their family budget for the next year. The husband wants
to change his car to a new fancy model and have a domestic holiday. The wife is going to
implement her dream of a romantic trip to Europe and suggests to redecorate their kitchen.
Both of them know that they can’t have two holidays in one year. They also realize that
they cannot afford a new car and an overseas holiday in the same year without getting a
loan from the bank. However, the wife does not like the idea of borrowing money.
In order to represent the situation in logic, let c denote “buy a new car”, d stand for
“domestic holiday”, o for “overseas holiday”, k for “kitchen redecoration” and l for “loan”.
5. Note that the player may have different demands at different stages of negotiation.

438

An Ordinal Bargaining Solution with Fixed-Point Property

Then ¬(d ∧ o) means that it is impossible to have both domestic holiday and overseas
holiday. The statement (c ∧ o) → l says that if they want to buy a new car and also have
an overseas holiday, they have to get a loan from the bank.
With the above symbolization, we can express the husband’s demands in the following
set:
X1 = {c, d, ¬(d ∧ o), (c ∧ o) → l}
Similarly, the wife’s demands can be represented by:
X2 = {o, k, ¬(d ∧ o), (c ∧ o) → l, ¬l}
Let us assume that the husband’s preferences over his demands are:
¬(d ∧ o) ≈1 (c ∧ o) → l Â1 c Â1 d
and the wife’s preferences are:
¬(d ∧ o) ≈2 (c ∧ o) → l Â2 o Â2 k Â2 ¬l
Note that both agents give the common beliefs: ¬(d ∧ o) and (c ∧ o) → l the highest
priority. This reflects that the couple are rational. We shall see that these common beliefs
play important role in the determination of bargaining solution.
From the above example, we can also see the differences between our model and Osborne
& Rubinstein’s model (Osborne & Rubinstein, 1990). First, the demand sets X1 and X2
in our model represent players’ physical demands rather than feasible outcomes (note that
the two players share the same feasible set X in Osborne & Rubinstein’s model). In our
model, the feasible set is generated from the demand sets through a procedure of conflict
resolving (see Definition 2 and Example 3). Secondly, we do not have a representation of
disagreements. If a negotiation ends with disagreement, we simply assume that the agreement is empty. Thirdly, in Osborne & Rubinstein’s model, the items in X are independent,
while in our model, the items from both players’ demand sets are related through their
logical relations and the orderings (via LC). This allows us to represent bargainers’ attitude
towards risk.
2.3 Hierarchy of Demands
Before we construct our bargaining solution, we present the basic properties of prioritized
demand sets. Consider a prioritized demand set (X, º) for a single agent. We define
recursively a hierarchy, {X j }+∞
j=1 , of X with respect to the ordering º as follows:
1. X 1 = {ϕ ∈ X : ∀ψ ∈ X(ϕ º ψ)}; T 1 = X\X 1 .
2. X j+1 = {ϕ ∈ T j : ∀ψ ∈ T j (ϕ º ψ)}; T j+1 = T j \X j+1 .
The intuition behind the construction is the following: at each stage of the construction,
we collects all the maximal elements from the current demand set and remove them from
the set for the next stage of the construction. It is easy to see that there exists a number
439

Zhang & Zhang

n such that X =

n
S

X j due to the logical constraint LC6 . Therefore a demand set X can

j=1

be always written as X 1 ∪ · · · ∪ X n , where X j ∩ X k = ∅ for any j 6= k (see Figure 1). Also
most entrenched

X1

EAA¢¢¦
E ¦
E ¦
E¦
E¦

X2
...
Xn

least entrenched

Figure 1: The hierarchy of a demand set.
for any ϕ ∈ X j and ψ ∈ X k , ϕ Â ψ if and only if j < k. In other words, the prioritized
demand set (X, º) uniquely determines a partition of X,

n
S

X j , with a total order over

j=1

the partition. Therefore, the concept of prioritized demand set is equivalent to the concept
of so-called nicely-ordered partition of a belief set7 , introduced by Zhang and Foo (Zhang
& Foo, 2001), if the demand set is logically closed.
In the sequent, we write X ≤k to denote

k
S

Xj.

j=1

2.4 Prioritized Base Revision
Once we have a hierarchy of the demand set for each agent, we are able to define a belief
revision operator for each agent by following Nebel’s prioritized base revision (Nebel, 1992)8 .
We define a revision function ⊗ as follows:
For any demand set (X, ¹) and a set F of sentences,
def

X ⊗F =

\

(H + F ),

H∈X⇓F

where X ⇓ F is defined as: H ∈ X ⇓ F if and only if
1. H ⊆ X,
2. for all k (k = 1, 2, · · ·), H ∩ X k is a maximal subset of X k such that

k
S

(H ∩ X j ) ∪ F

j=1

is consistent.

In other words, H is a maximal subset of X that is consistent with F and gives priority to
the higher ranked items. The following result will be used in Section 4.
Lemma 1 (Nebel, 1992) If X is logically closed, then ⊗ satisfies all AGM postulates.
6. Note that X can be an infinite set even though the language is finite.
7. A nicely-ordered partition of a belief set K is a triple (K, ℘, ≥), where ℘ is a partition of K and ≥ is a
total order on ℘ that satisfies the logical constraint LC. See the work of Zhang and Foo (2001) p.540.
8. The idea of the construction can be traced back to Poole and Brewka’s approach to default logic (Brewka,
1989; Poole, 1988).

440

An Ordinal Bargaining Solution with Fixed-Point Property

3. Bargaining Solution
In Nash’s bargaining model, a bargaining solution is defined as a function that assigns to
each bargaining game a point in the feasible set of the game. However, we do not have such
a simple definition because in our model the set of possible agreements (feasible set) is not
given in a bargaining game. We have to generate the feasible set from the demand sets.
This involves a process of conflict resolving.
3.1 Possible Deals
Whenever the demands from two agents conflict, at least one agent has to make a concession
in order to reach an agreement. The simple way of making a concession is to withdraw a
number of demands. In such sense, a possible agreement is a pair of subsets of two players’
original demand sets such that the collection of remaining demands is consistent. Obviously
each player would like to keep as many original demands as possible. In addition, if a player
has to give up a demand, the player typically gives up the one with the lowest priority. This
idea leads to the following definition of possible deals.
Definition 2 Let G = ((X1 , º1 ), (X2 , º2 )) be a bargaining game. A deal of G is a pair
(D1 , D2 ) satisfying the following conditions: for each i = 1, 2,
1. Di ⊆ Xi ;
2. X1 ∩ X2 ⊆ Di ;
3. for each k (k = 1, 2, · · ·), Di ∩ Xik is a maximal subset of Xik such that

k
S

(Di ∩ Xij ) ∪

j=1

D−i is consistent.
where {Xij }+∞
j=1 is the hierarchy of Xi defined in Section 2.3. The set of all deals of G is
denoted by Ω(G), called the feasible set of the game.
This definition is obviously an analogue of Nebel’s notion ⇓ (see Section 2.4). The only
difference is that the procedure of maximization here is interactive between two agents:
given one player’s demands, the other player always tops up his demands with the highest
prioritized items provided the overall outcome is consistent.
Since we have assumed that both X1 and X2 are consistent, Ω(G) is non-empty. Specifically, if X1 ∪ X2 is consistent, we have Ω(G) = {(X1 , X2 )}.
Example 3 Consider the bargaining game in Example 2. According to the preference orderings of the couple, the game has three possible deals:
D1 = ({¬(d ∧ o), (c ∧ o) → l, c, d}, {¬(d ∧ o), (c ∧ o) → l, k, ¬l}).
D2 = ({¬(d ∧ o), (c ∧ o) → l, c}, {¬(d ∧ o), (c ∧ o) → l, o, k}).
D3 = ({¬(d ∧ o), (c ∧ o) → l}, {¬(d ∧ o), (c ∧ o) → l, o, k, ¬l}).
Therefore Ω(G) = {D1 , D2 , D3 }.
441

Zhang & Zhang

3.2 The Core of Agreement
We have shown how to generate possible deals, which form the set of possible agreements,
i.e., the feasible set, by resolving conflicting demands. Now we are at the same level as the
game-theoretic model that, to define a bargaining solution, we only have to select a deal
from all possible deals. Obviously, if the demands from two parties contradict, there are
multiple possible deals. Different deals would be in favor of different parties. For instance,
in Figure 2, the deal D0 is in favor of player 1 while D00 is in favor of player 2. Therefore,
the conflicts in choosing outcomes still exist. The major concern of a bargaining theory is
how to measure and balance the gain of each negotiating party.

³
1
D0³

X11

X21

...

...

X1k−1

X2k−1

X1k

X2k

X1k+1

X2k+1

...

...

P
i
PD00

Figure 2: Different deals are in favor of different players.
Instead of counting the number of demands that each party can remain from a deal, we
consider the top block demands that a player keeps in the deal (the top levels of demands
in each player’s demand hierarchy) and ignore all the demands that are not included in the
top blocks for the purpose of measuring players’ gains.
Given a deal D, we shall use the maximal number of the top levels of demands in the
deal as the indicator of each player’s gain from the deal, i.e., max{k : Xi≤k ⊆ Di } for
i = 1, 2. For instance, in Figure 2, player 1 remains maximally the top k − 1 levels of her
demands from the deal D00 while player 2 can successfully gain the top k + 1 levels of his
demands from the same deal. With deal D0 , both players can remain the top k levels of
demands.
In order to compare different deals, we refer the gain index of a deal to the gain of the
player whoever receives less from the deal, i.e., min{max{k : X1≤k ⊆ D1 }, max{k : X2≤k ⊆
D2 }}, or equivalently, max{k : X1≤k ⊆ D1 and X2≤k ⊆ D2 }. For instance, in Figure 2, the
gain index of D0 is k while the gain index of D00 is k − 1. Therefore we can say that D0
is better than D00 because, with D0 , both players can remind at least top k blocks in their
demand hierarchies but D00 can’t.
Formally, let
G
πmax
=

and

max

{k : X1≤k ⊆ D1 and X2≤k ⊆ D2 }

(D1 ,D2 )∈Ω(G)

G
≤πmax

γ(G) = {(D1 , D2 ) ∈ Ω(G) : X1

442

G
≤πmax

⊆ D1 & X2

⊆ D2 }

(1)

(2)

An Ordinal Bargaining Solution with Fixed-Point Property

Then γ(G) collects all the “best deals” from all the possible deals of G. This is because none
G
of other deals can contain more than the top πmax
levels of demands from both players.
G
Note that πmax may be infinite when X1 ∪ X2 is consistent. Let
G
≤πmax

(Φ1 , Φ2 ) = (X1

G
≤πmax

, X2

)

(3)

We call Φ = (Φ1 , Φ2 ) the core of the game. Therefore the core contains the top block
demands that all the best deals contain, therefore should be included in the final agreement.
The following lemma gives another way to calculate the core of a game.
G
Lemma 2 πmax
= max{k : X1≤k ∪ X2≤k ∪ (X1 ∩ X2 ) is consistent}.

Proof: Let π = max{k : X1≤k ∪ X2≤k ∪ (X1 ∩ X2 ) is consistent}. It is easy to show
≤π G

≤π G

that X1 max ∪ X2 max ∪ (X1 ∩ X2 ) is consistent because γ(G) is non-empty. Therefore
G
πmax
≤ π. On the other hand, since X1≤π ∪ X2≤π ∪ (X1 ∩ X2 ) is consistent, there exists
a deal (D1 , D2 ) ∈ Ω(G) such that Xi≤π ⊆ Di and X1 ∩ X2 ⊆ Di for each i = 1, 2. Thus
G . We conclude that π = π G .
π ≤ πmax
¶
max

3.3 The Solution
At first sight, a bargaining solution can be easily defined as a function that assigns each
bargaining game G a possible deal in Ω(G). More likely, the solution could select one of
the “best deals” from γ(G). However, due to the multiplicity of γ(G) (see Figure 3), such a
selection is not always feasible if we want the solution to be symmetric to each player. To
show the difficulty, let us consider the following example.
Example 4 Consider a bargaining game G = ((X1 , º1 ), (X2 , º2 )), described in three propositional variables p, q and r, where X1 = {{p}, {r}} and X2 = {{q}, {¬r}} (note that the
demand sets are represented in the form of hierarchy where p Â1 r and q Â2 ¬r (see Section
2.3)). It is easy to know that the game has two possible deals, ({p, r}, {q}) and ({p}, {q, ¬r}),
which are all in γ(G). However, none of the deals can lead to an impartial solution. A reasonable solution to the problem should be ({p}, {q}), which take the intersection of all the
best deals for each player, respectively.
Base on the above intuitive explanation, we are now ready to present our bargaining
solution.
Definition 3 The bargaining solution is the function F defined as follows, which maps a
bargaining game G = ((X1 , º1 ), (X2 , º2 )) to a pair of sets of sentences:
def

F (G) = (

\

D1 ,

(D1 ,D2 )∈γ(G)

\

D2 )

(4)

(D1 ,D2 )∈γ(G)

where γ(G) is defined by Equation (2).
For a better understanding of the construction of our solution, we would like to make
the following remarks:
443

Zhang & Zhang

X11

X21

...

...

πG

πG

X1 max
πG

D

X1 max
³
1
...
0³

X2 max
πG

+1

+1

P
i

PD00
X2 max
iP 000
... P
D

Figure 3: Multiplicity of best deals.
1. The solution gives a prediction of bargaining outcome for each game. Given a game
G, Fi (G) represents all the demands the player i can successfully remain at the end
of bargaining. It also means that these demands are accepted by the other player.
Therefore the final agreement of the bargaining can be defined as F1 (G) ∪ F2 (G)9 .
2. Note that Fi (G) ⊆ Xi for each i = 1, 2. Therefore the solution means a compromise
to each player. Both players may make concessions to their demands in order to
reach the agreement. Obviously, if there is no conflict between the players’ original
demands, i.e., X1 ∪ X2 is consistent, no concession is needed, that is, Fi (G) = Xi
(i = 1, 2).
3. The construction of the solution takes a skeptical view in the sense that, for each
player, a demand item is included in the solution only if it belongs to all the “best
deals”. In other words, the solution gives only a cautious prediction of bargaining
outcome. As a result, the solution is not necessarily a deal if there are multiple
elements in γ(G).
4. The solution is unique to each bargaining game. Like the bargaining problem with
non-convex domain, we have to scarify strict Pareto optimality to gain the uniqueness
(this is the reason we take cautious prediction). However, we will show that the
solution is weakly Pareto optimal (see Section 5.2 for more discussions).
5. In our bargaining model, we do not specify disagreement points. In fact, we assume
that if the solution gives an empty agreement, i.e., F (G) = (∅, ∅), then the negotiation
reaches a disagreement. In other words, (∅, ∅) is the default disagreement point for
any bargaining game.
Example 5 Continue on Example 3. According to the hierarchies of the demand sets shown
in Example 2, the core of the game is:
({¬(d ∧ o), (c ∧ o) → l, c}, {¬(d ∧ o), (c ∧ o) → l, o})
9. Alternatively, we can define the final agreement as Cn(F1 (G) ∪ F2 (G)) if we consider that the outcome
of the negotiation contains all the logical consequences of the demands in the agreement. In addition,
the relation of the items in the agreement should be read as “and” rather than “or”. In other words, all
items in the agreement are accepted by all players.

444

An Ordinal Bargaining Solution with Fixed-Point Property

Therefore γ(G) contains only a single deal, which is D2 (see Example 3). The solution is
then
F (G) = D2 = ({¬(d ∧ o), (c ∧ o) → l, c}, {¬(d ∧ o), (c ∧ o) → l, o, k})
In words, the couple agree upon the commonsense that they can only have one holiday and
they have to get a loan if they want to buy a new car and to go overseas for holiday. The
husband accepts his wife’s suggestion to have holiday in Europe and the wife agrees on buying
a new car.
Now consider the following preference orderings:
¬(d ∧ o) ≈1 (c ∧ o) → l Â1 d Â1 c
¬(d ∧ o) ≈2 (c ∧ o) → l Â2 o Â2 k Â2 ¬l
Therefore the demand hierarchies become:
X1 = {{¬(d ∧ o), (c ∧ o) → l}, {d}, {c}}.
X2 = {{¬(d ∧ o), (c ∧ o) → l}, {o}, {k}, {¬l}}.
Let G0 denote the game. The deals are the same as the original hierarchy as shown
in Example 3. However the solution of the game becomes ({¬(d ∧ o), (c ∧ o) → l}, {¬(d ∧
G0 = 2 and all three deals are included in γ(G0 ). Note that the
o), (c ∧ o) → l, k}) because πmax
final agreement does not include the demands which lead to conflicting (d, o, ¬l) but keeps
the demands which do not lead to conflicting (k). In other words, the solution excludes the
demands that lead to a conflict but keeps the demands that are not involved in any conflicts
even though they are in low priorities.

4. Logical Properties of the Bargaining Solution
In the following two sections, we discuss the properties of the bargaining solution introduced in the previous section. According to Zhang (2007), a bargaining solution satisfies
the axioms Collective Rationality, Scale Invariance, Symmetry and Mutually Comparable
Monotonicity as well as the basic assumptions Individual Rationality, Consistency and Comprehensiveness if and only if it is the logical version of Kalai-Smorodinsky solution (Kalai &
Smorodinsky, 1975). Among these properties, Collective Rationality, Individual Rationality
and Consistency capture the logical properties of a bargaining solution. Scale Invariance,
Symmetry and Mutually Comparable Monotonicity reflect the game-theoretic properties of
a bargaining solution. Comprehensiveness is an idealized assumption for a logic-based bargaining solution. Although there is a significant difference between the bargaining solution
we defined in this paper and the one in that work, these two solutions share most desirable
properties of bargaining solutions.
4.1 Generic Properties of Logic-Based Bargaining Solutions
It is easy to see that the solution we constructed in the previous section satisfies the following
generic properties of a logic-based bargaining solution.
Theorem 1 For any bargaining game G = ((X1 , º1 ), (X2 , º2 )), let F (G) = (F1 (G), F2 (G)).
Then
445

Zhang & Zhang

1. F1 (G) ⊆ X1 and F2 (G) ⊆ X2 .

(Individual Rationality)

2. F1 (G) ∪ F2 (G) is consistent.

(Consistency)

3. If X1 ∪ X2 is consistent, Fi (G) = Xi for all i.

(Collective Rationality)

Proof: The proofs for these properties are straightforward from the definition of the bargaining solution (Definition 3).
¶
Note that the logical version of Individual Rationality (IR) has a different meaning
of its game-theoretic version. The logical version of IR means that each player concerns
only her own demands, i.e., whether and how many of her demands are included in the
final agreement. In contrast, the game-theoretic version of IR concerns about whether each
player can gain no less than disagreement point from a negotiation (see more details in
Section 5.1). The other two properties are quite intuitive.
The following example shows that our solution does not satisfy comprehensiveness, which
requires that ϕ ∈ Fi (G) and ψ ºi ϕ implies ψ ∈ Fi (G) for each i (see the work of Zhang,
2007).
Example 6 Consider a bargaining situation in which player 1’s demand set is X1 = {p, q}
and player 2’s demand set is X2 = {¬p, r}, where p, r, q are propositional variables. Assume
that each player ranks her demands in same level (i.e., both demand sets have a singleton
partition). Based on the assumption, it is easy to know that the solution of the game is
({q}, {r}). Therefore, the solution is not comprehensive (for instance, q ¹ p and q ∈ F1 (G)
but p 6∈ F1 (G)).
Since the solution does not satisfy comprehensiveness, according to Zhang (2007), it is
not the logical version of Kalai-Smorodinsky solution. However, this does not mean that
our solution is less intuitive. Although comprehensiveness is a common restriction in belief
revision and game theory, it is by no means a desirable property of bargaining solution. In
the above example, q and r are not involved in the conflict of the underlying bargaining
game. Thus it is reasonable for the players to keep these irrelevant demands. In addition,
the solution is syntax-dependent. If we represent the demand set as X1 = {p ∧ q} and
X2 = {¬p ∧ r}, the solution will be (∅, ∅).
4.2 Fixed-Point Property
Besides the generic properties, the solution possesses another extraordinary logical property:
the fixed-point property. We consider bargaining or negotiation as mutual persuasion: one
persuades the other to accept her demands. The outcome of negotiation is then the result
of mutual belief revision (Zhang et al., 2004). If it is the case, the negotiation outcome
should satisfy the following fixed-point property.
Theorem 2 For any bargaining game G = ((X1 , º1 ), (X2 , º2 )), if X1 and X2 are logically
closed, the bargaining solution F (G) satisfies the following fixed-point condition:
F1 (G) + F2 (G) = (X1 ⊗1 F2 (G)) ∩ (X2 ⊗2 F1 (G))

(5)

where ⊗i is the prioritized revision operator for player i (see the definition in Section 2.4).
446

An Ordinal Bargaining Solution with Fixed-Point Property

Assume that X1 and X2 are two belief sets (so logically closed), representing the belief
states of two agents. Mutual belief revision between the agents means that each agent takes
part of the other agent’s beliefs to revise his belief set. For instance, if Ψ1 is a subset of X1
and Ψ2 is a subset of X2 , then X1 ⊗1 Ψ2 is the revised belief set of player 1 after he accepts
player 2’s beliefs Ψ2 while X2 ⊗2 Ψ1 is the resulting belief set of player 2 after accepting
Ψ1 . Such an interaction of belief revision can continue until it reaches a fixed point where
the beliefs in common, i.e., (X1 ⊗1 Ψ2 ) ∩ (X2 ⊗2 Ψ1 ), are exactly the beliefs that the agents
mutually accept, that is, Ψ1 + Ψ2 . Note that each agent uses his own way of revision to
rebuilt his belief state. If we view bargaining as mutual belief revision, then the agreement
of bargaining, i.e., F1 (G) + F2 (G), is exactly the common demands the agents accept each
other, i.e., (X1 ⊗1 F2 (G)) ∩ (X2 ⊗2 F1 (G)). In other words, the solution F (G) should be the
fixed-point with respect to the game G. The above theorem shows that this can be true if
the demand sets of a game are logically closed.
To show this theorem, we need a few technical lemmas.
Lemma 3 For any bargaining game G = ((X1 , º1 ), (X2 , º2 )),
1. F1 (G) ⊆ X1 ⊗1 F2 (G);
2. F2 (G) ⊆ X2 ⊗2 F1 (G).
Proof: According to the definition of prioritized base revision, we have X1 ⊗1 F2 (G) =
T
Cn(H ∪ F2 (G)). For any H ∈ X1 ⇓ F2 (G), there is a deal (D1 , D2 ) ∈ Ω(G) such

H∈X1 ⇓F2 (G)

that D1 = H. This is because we can extend the pair (H, F2 (G)) to a deal (H, D2 ) such that
F2 (G) ⊆ D2 . On the other hand, since Φ1 ∪ F2 (G) is consistent, we have Φ1 ⊆ H, where
(Φ1 , Φ2 ) is the core of G. Thus, Φ1 ⊆ D1 and Φ2 ⊆ D2 . It follows that (D1 , D2 ) ∈ γ(G).
Since F1 (G) ⊆ D1 , we have F1 (G) ⊆ H. We conclude that F1 (G) ⊆ X1 ⊗1 F2 (G). The
proof of the second statement is similar.
¶
By this lemma we have,
1. F1 (G) + F2 (G) ⊆ X1 ⊗1 F2 (G);
2. F1 (G) + F2 (G) ⊆ X2 ⊗2 F1 (G).
Note that the above lemma does not require the demand sets X1 and X2 to be logically
closed. However, without the assumption, the following lemmas do not hold.
Lemma 4 Let (Φ1 , Φ2 ) be the core of game G = ((X1 , º1 ), (X2 , º2 )). If X1 and X2 are
logically closed, then
1. X1 ⊗1 F2 (G) = X1 ⊗1 (Φ2 + (X1 ∩ X2 ));
2. X2 ⊗2 F1 (G) = X2 ⊗2 (Φ1 + (X1 ∩ X2 ))
Proof: We only present the proof of the first statement. The second one is similar. Firstly,
we prove that F2 (G) ⊆ Φ1 + Φ2 + (X1 ∩ X2 ). If X1 ∪ X2 is consistent, the result is obviously
true. Therefore we can assume that X1 ∪ X2 is inconsistent.
447

Zhang & Zhang

Assume that ϕ ∈ F2 (G). If ϕ 6∈ Φ1 +Φ2 +(X1 ∩X2 ), we have {¬ϕ}∪Φ1 ∪Φ2 ∪(X1 ∩X2 ) is
≤π G

≤π G

+1

+1

consistent. According to Lemma 2, we have X1 max ∪X2 max ∪(X1 ∩X2 ) is inconsistent.
Since our language is finite and both X1 and X2 are logically closed, the sets X1 ∩ X2 ,
≤π G +1
≤π G +1
X1 max
and X2 max
are all logically closed (the latter two due to LC). Therefore
each set has a finite axiomatization. Let sentence ψ0 axiomatize X1 ∩ X2 , ψ1 axiomatize
≤π G +1
≤π G +1
X1 max and ψ2 axiomatize X2 max . Thus ψ0 ∧ ψ1 ∧ ψ2 is inconsistent. Notice that
ψ0 ∧ψ1 ∈ X1 and ψ0 ∧ψ2 ∈ X2 . It follows that ¬ϕ∨(ψ0 ∧ψ1 ) ∈ X1 and ¬ϕ∨(ψ0 ∧ψ2 ) ∈ X2 .
Since {¬ϕ} ∪ Φ1 ∪ Φ2 ∪ (X1 ∩ X2 ) is consistent, there is a deal (D1 , D2 ) ∈ γ(G) such that
{¬ϕ∨(ψ0 ∧ψ1 )}∪Φ1 ∪(X1 ∩X2 ) ⊆ D1 and {¬ϕ∨(ψ0 ∧ψ2 )}∪Φ2 ∪(X1 ∩X2 ) ⊆ D2 . We know
that ϕ ∈ F2 (G), so ϕ ∈ D1 + D2 . Thus ψ0 ∧ ψ1 ∧ ψ2 ∈ D1 + D2 , which contradicts the fact
that D1 + D2 is consistent. Therefore, we have shown that F2 (G) ⊆ Φ1 + Φ2 + (X1 ∩ X2 ).
Now we prove that X1 ⊗1 F2 (G) = X1 ⊗1 (Φ2 + (X1 ∩ X2 )). By Lemma 3, we have
Φ1 + Φ2 ⊆ X1 ⊗1 F2 (G). It follows that X1 ⊗1 F2 (G) = (X1 ⊗1 F2 (G)) + (Φ1 + Φ2 ).
Furthermore, we yield X1 ⊗1 F2 (G) = (X1 ⊗1 F2 (G)) + (Φ1 + Φ2 ) + (X1 ∩ X2 ) because
X1 ∩ X2 ⊆ F2 (G). Since F2 (G) ⊆ Φ1 + Φ2 + (X1 ∩ X2 ). According to the AGM postulates, we have (X1 ⊗1 F2 (G)) + (Φ1 + Φ2 + (X1 ∩ X2 )) = X1 ⊗1 (Φ1 + Φ2 + (X1 ∩ X2 )).
Therefore X1 ⊗2 F2 (G) = X1 ⊗1 (Φ1 + Φ2 + (X1 ∩ X2 )). In addition, it is easy to
prove that Φ1 ⊆ X1 ⊗1 (Φ2 + (X1 ∩ X2 )). By the AGM postulates again, we have
X1 ⊗1 (Φ2 + (X1 ∩ X2 )) = (X1 ⊗1 (Φ2 + (X1 ∩ X2 )) + Φ1 = X1 ⊗1 (Φ1 + Φ2 + (X1 ∩ X2 )).
Therefore X1 ⊗1 F2 (G) = X1 ⊗1 (Φ2 + (X1 ∩ X2 )).
¶
The following lemma will complete the proof of Theorem 2.
Lemma 5 If X1 and X2 are logically closed, then
(X1 ⊗1 F2 (G)) ∩ (X2 ⊗2 F1 (G)) ⊆ F1 (G) + F2 (G).
Proof: Let (Φ1 , Φ2 ) be the core of G. Let
1
≤πmax

Φ01 = X1

2
≤πmax

and Φ02 = X2

2
1
= max{k : Φ1 ∪ X2≤k ∪
where πmax
= max{k : X1≤k ∪ Φ2 ∪ (X1 ∩ X2 ) is consistent} and πmax
(X1 ∩ X2 ) is consistent}.
i
Note that in the cases when πmax
does not exist, we simply assume that it equals to
0
+∞. We claim that X1 ⊗1 F2 (G) = Φ1 + F2 (G) and X2 ⊗2 F1 (G) = Φ02 + F1 (G). We shall
provide the proof of the first statement. The second one is similar.
Firstly, according to Lemma 2, Φ1 ⊆ Φ01 . Secondly, by Lemma 4, we have X1 ⊗1 F2 (G) =
X1 ⊗1 (Φ2 +(X1 ∩X2 )). Therefore to show X1 ⊗1 F2 (G) = Φ01 +F2 (G), we only need to prove
that X1 ⊗1 (Φ2 +(X1 ∩X2 )) = Φ01 +Φ2 +(X1 ∩X2 ). This is because Φ2 +(X1 ∩X2 ) ⊆ F2 (G),
F2 (G) ⊆ Φ1 + Φ2 + (X1 ∩ X2 ) and Φ1 ⊆ Φ01 . By the construction of prioritized revision, we
can easily verify that Φ01 + Φ2 + (X1 ∩ X2 ) ⊆ X1 ⊗1 (Φ2 + (X1 ∩ X2 )). Therefore we only
have to show the other direction, i.e., X1 ⊗1 (Φ2 + (X1 ∩ X2 )) ⊆ Φ01 + Φ2 + (X1 ∩ X2 ).
If Φ01 = X1 , then X1 ∪ (Φ2 + (X1 ∩ X2 )) is consistent. It follows that X1 ⊗1 (Φ2 +
(X1 ∩ X2 )) ⊆ X1 + (Φ2 + (X1 ∩ X2 )) = Φ01 + Φ2 + (X1 ∩ X2 ), as desired. If Φ01 6= X1 ,
≤π 1 +1
1
, we have X1 max ∪ Φ2 ∪ (X1 ∩ X2 ) is inconsistent.
according to the definition of πmax
1
≤πmax
+1

Therefore there exists ψ ∈ X1

such that ¬ψ ∈ Φ2 + (X1 ∩ X2 ). Now we assume that
448

An Ordinal Bargaining Solution with Fixed-Point Property

ϕ ∈ X1 ⊗1 (Φ2 + (X1 ∩ X2 )). If ϕ 6∈ Φ01 + Φ2 + (X1 ∩ X2 ), then {¬ϕ} ∪ Φ01 ∪ Φ2 ∪ (X1 ∩ X2 )
≤π 1 +1
is consistent. So is {¬ϕ ∨ ψ} ∪ Φ01 ∪ Φ2 ∪ (X1 ∩ X2 ). Notice that ¬ϕ ∨ ψ ∈ X1 max .
There exists H ∈ X1 ⇓ (Φ2 + (X1 ∩ X2 )) such that {¬ϕ ∨ ψ} ∪ Φ01 ⊆ H. Since ϕ ∈
X1 ⊗1 (Φ2 + (X1 ∩ X2 )) and H is logically closed, we have ψ ∈ H, which contradicts the
consistency of H ∪(Φ2 +(X1 ∩X2 )). Therefore X1 ⊗1 (Φ2 +(X1 ∩X2 )) ⊆ Φ01 +Φ2 +(X1 ∩X2 ).
Finally we prove the claim of the lemma. Let ϕ ∈ (X1 ⊗1 F2 (G)) ∩ (X2 ⊗2 F1 (G)).
We then have ϕ ∈ (Φ01 + F2 (G)) ∩ (Φ01 + F2 (G)). For ϕ ∈ Φ01 + F2 (G), there exists a sentence ψ2 such that F2 (G) ` ψ2 and ϕ ∨ ¬ψ2 ∈ Φ01 . Similarly, there exists a sentence ψ1
such that F1 (G) ` ψ1 and ϕ ∨ ¬ψ1 ∈ Φ02 . It turns out that ϕ ∨ ¬ψ1 ∨ ¬ψ2 ∈ Φ01 ∩ Φ02 .
Thus ϕ ∨ ¬ψ1 ∨ ¬ψ2 ∈ X1 ∩ X2 . However, X1 ∩ X2 ⊆ F1 (G) + F2 (G). It follows that
ϕ ∨ ¬ψ1 ∨ ¬ψ2 ∈ F1 (G) + F2 (G). Note that ψ1 ∧ ψ2 ∈ F1 (G) + F2 (G). Therefore we conclude
that ϕ ∈ F1 (G) + F2 (G).
¶
Theorem 2 establishes the link between bargaining theory and belief revision. The link
helps us to understand the reasoning process behind bargaining. It is even more interesting
if we can extend the result into the general multiagent case. However, the main challenge
is how multiple agents mutually revise their beliefs.

5. Game-theoretic Properties of the Bargaining Solution
In game theory, the properties that are considered to be important to a bargaining solution
include individual rationality, Pareto optimality, ordinal invariance (or scale invariance),
symmetry and contraction independence. In our bargaining model, bargainers’ preferences
are represented in total preorder, any order-preserving transformation on the preferences
does not change the order of preferences. Therefore our solution satisfies ordinal invariance
trivially. In this section, we will examine the other properties in the above list with our
bargaining solution. Before presenting the results, we first introduce a few concepts that
are necessary for the game-theoretic analysis of bargaining.
5.1 Strategies and Utilities
Two concepts play essential roles in game-theoretic analysis of bargaining: strategy and
utility. Given a bargaining game G = ((X1 , º1 ), (X2 , º2 )), a strategy profile of the game is
a pair (S1 , S2 ) where S1 ⊆ X1 and S2 ⊆ X2 . The strategy profile can be interpreted as a
pair of proposals of demands from both players in a course of bargaining.
We say a strategy profile S = (S1 , S2 ) to be compatible if
1. X1 ∩ X2 ⊆ S1 and X1 ∩ X2 ⊆ S2
2. S1 ∪ S2 is consistent
Obviously any deal of a game is a compatible strategy profile. The bargaining solution
F (G) is also a compatible strategy profile of G.
Now we consider the gains of each player from a strategy profile. Assume that the
strategy profile (S1 , S2 ) leads to an agreement, the player i’s payoff or utility is defined as:

449

Zhang & Zhang

(

ui (Si ) =

max{k : Xi≤k ⊆ Si },
min{k : Xi≤k = Xi },

if Si 6= Xi ;
otherwise.

In other words, ui (S) counts the number of top block demands that are covered by Si .
Note that the payoff does not count individual demands. Specifically we define the utility
for the default disagreement point (∅, ∅) to be (0, 0).
5.2 Pareto Optimality
Based on the above definition, it is easy to see that our solution satisfies individual rationality (in the sense of game theory) because for any game G, ui (Fi (G)) ≥ 0 for i = 1, 2.
Now we consider Pareto efficiency.
Pareto optimality is one of the most important properties of bargaining solution. We
call a compatible strategy profile (S1 , S2 ) of a game to be (strictly) Pareto optimal if there
does not exist a compatible strategy profile (S10 , S20 ) of the game such that either u1 (S10 ) ≥
u1 (S1 ) & u2 (S20 ) > u2 (S2 ) or u1 (S10 ) > u1 (S1 ) & u2 (S20 ) ≥ u2 (S2 ).
A compatible strategy profile (S1 , S2 ) of a game is weakly Pareto optimal if there does
not exist another compatible strategy profile (S10 , S20 ) of the game such that u1 (S10 ) > u1 (S1 )
and u2 (S20 ) > u2 (S2 ).
Theorem 3 For any bargaining game G, F (G) is weakly Pareto optimal.
Proof: Suppose that there is a compatible strategy profile (S1 , S2 ) of G such that u1 (S1 ) >
G
G . Since
u1 (F1 (G)) and u2 (S2 ) > u2 (F2 (G)). Then u1 (S1 ) > πmax
and u2 (S2 ) > πmax
≤u1 (S1 )
≤u2 (S2 )
(S1 , S2 ) is compatible, X1
∪ X2
∪ (X1 ∩ X2 ) is consistent. It turns out that
k
k
G , which contradicts Lemma 2. Therefore
max{k : X1 ∪ X2 ∪ (X1 ∩ X2 ) is consistent} > πmax
F (G) is weakly Pareto optimal.
¶
Obviously, our solution F does not satisfy strict Pareto optimality. For instance, the
solution of G0 in Example 5 (the second part of the example) is not strictly Pareto optimal
(but it is weakly Pareto optimal). This is not a problem of the solution but the nature of
the problem domain we consider. It is well known in game theory that if the feasible set of
a bargaining game is not convex, there is no guarantee of unique bargaining solution that is
strictly Pareto optimal (Kaneko, 1980; Mariotti, 1996). Therefore, for non-convex domain,
we need a trade-off between the uniqueness of solutions and strict Pareto optimality (Conley
& Wilkie, 1991, 1996; Mariotti, 1998; Xu & Yoshihara, 2006). By using a similar approach
introduced by Zhang (2007), we can map a logically represented bargaining game into a
numerically represented bargaining game. Under such a mapping, the feasible set that
corresponds to any logically represented bargaining game is non-convex unless the demand
sets of the logical bargaining game is consistent. Since our solution is a unique solution, we
cannot expect it to be strictly Pareto optimal.
5.3 Restricted Symmetry
In game theory, a bargaining game is symmetric if the feasible set is invariant under any
permutation of each point in the feasible set (Nash, 1950). However, the concept of symmetry is not easy to be extended to the logic-based bargaining models because a bargaining
450

An Ordinal Bargaining Solution with Fixed-Point Property

problem is represented by its physical items. Permutation of deals does not make any sense.
One may wonder how to judge the fairness of bargaining without the concept of symmetry.
In our point of view, there is no such a thing as fair outcome in negotiation. The outcome
of a negotiation relies on the bargaining power of each party. A bargainer with higher
negotiation power receives more gains from the negotiation. However, it is reasonable to
assume that any negotiation should be based on a fair bargaining procedure or a negotiation
protocol. The construction of our bargaining solution is meant to capture the idea of fair
negotiation protocols. The approach we use in this paper is similar to the idea of bargaining
with an agenda (O’Neill et al., 2004). We consider a negotiation process consists of several
rounds or stages. At each round, the parties are to reach agreements on new issues that
have not been considered in the previous rounds. We assume that each party always place
the higher wanted demands at earlier rounds. All the demands that have been mutually
accepted in the earlier rounds have to remain in the agreements of the negotiation in the
later rounds. Once there is no new agreement being reached, the negotiation procedure
stops. With such a process, a negotiation always terminates at the same level of priority of
demands for all players.
Theorem 4 For any bargaining game G = ((X1 , º1 ), (X2 , º2 )), if X1 and X2 are logically
closed, then there is a natural number n such that
F(G) = (X1 ∩ (X1≤n + X2≤n + (X1 ∩ X2 )), X2 ∩ (X1≤n + X2≤n + (X1 ∩ X2 )))
G . In such a case X ≤n + X ≤n = Φ + Φ .
Proof: In fact, we can prove that n = πmax
1
2
2
1
Obviously if X1 ∪ X2 is consistent, then the result is trivial. Therefore we assume that
X1 ∪ X2 is inconsistent. We only prove the case that F1 (G) = X1 ∩ (Φ1 + Φ2 + (X1 ∩ X2 )).
The proof of the other part is similar.
For any (D1 , D2 ) ∈ γ(G), we have Φ1 ⊆ D1 and Φ2 ⊆ D2 . We prove that X1 ∩(Φ1 +Φ2 +
(X1 ∩ X2 )) ⊆ D1 . If it is not the case, there exists a sentence ϕ ∈ X1 ∩ (Φ1 + Φ2 + (X1 ∩ X2 ))
such that ϕ 6∈ D1 . On the one hand, ϕ ∈ X1 ∩(Φ1 +Φ2 +(X1 ∩X2 )) implies that D1 ∪D2 ` ϕ
because Φ1 + Φ2 + (X1 ∩ X2 ) ⊆ D1 + D2 . On the other hand, ϕ 6∈ D1 implies that
{ϕ} ∪ D1 ∪ D2 is inconsistent due to the maximality of deals. It follows that D1 ∪ D2 ` ¬ϕ.
Therefore D1 ∪ D2 is inconsistent, a contradiction. We have proved that for any deal
(D1 , D2 ) ∈ γ(G), X1 ∩ (Φ1 + Φ2 + (X1 ∩ X2 )) ⊆ D1 . Thus X1 ∩ (Φ1 + Φ2 + (X1 ∩ X2 )) ⊆
T
D1 = F1 (G). The proof of Lemma 4 has shown that the other direction of inclu-

(D1 ,D2 )∈γ(G)

sion F1 (G) ⊆ X1 ∩(Φ1 +Φ2 +(X1 ∩X2 )) holds. Therefore F1 (G) = X1 ∩(Φ1 +Φ2 +(X1 ∩X2 )).
¶
This theorem shows that at the termination of negotiation, each party can remain the
G , including the common demands and their
demands down to the same level, i.e., πmax
logical consequences. However, the solution seemingly excludes the low ranked irrelevant
items. This is due to the assumption of the logical closedness on the demand sets. In fact,
with this assumption, no items are irrelevant because for any two statements ϕ, ψ ∈ Xi , we
always have ϕ → ψ and ψ → ϕ in Xi . This is why we do not assume logical closedness in
general.
Another question that may arise is that a player could gain more negotiation power
than the other if he puts more negotiation items in earlier stages of his agenda (effectively
451

Zhang & Zhang

could gain more if the negotiation does not end up with disagreement). In fact, this is true
and natural if the risk of breakdown is taken into account. If a player places a conflictive
item in an earlier stage in his agenda, the negotiation would terminate sooner. Therefore
the ordering of demands is a part of the strategy of a bargainer. We will discuss this issue
in a separate section (see Section 6)
5.4 Contraction Independence
Contraction Independence, or called Independence of Irrelevant Alternatives (IIA), requires
that if an alternative is judged to be the best compromise for some problem, then it should
still be judged best for any subproblem that contains it (Thomson, 1994). For logic-based
bargaining model, alternatives are not explicitly given. However, we can easily define the
concept of subproblem in terms of bargainers’ prioritized demand sets.
A bargaining game G0 = ((X10 , º01 ), (X20 , º02 )) is a subgame of G = ((X1 , º1 ), (X2 , º2 )),
denoted by G0 v G, if for any i = 1, 2,
1. Xi0 ⊆ Xi ,
2. º0i =ºi ∩(Xi0 × Xi0 ),
3. for any ϕ ∈ Xi , if there is ψ ∈ Xi0 and ϕ º ψ, then ϕ ∈ Xi0 .
In other words, Xi0 is the upper segment of Xi with respect to Xi ’s hierarchy.
Theorem 5 Let G0 v G. If F (G) is a strategy profile of G0 , then F (G0 ) = F (G).
Proof: First, since F (G) is a strategy profile of G0 , we have X1 ∩ X2 ⊆ Xi0 (i = 1, 2).
G0
G . By the
According to the definition of subgame, it is easy to show that πmax
≤ πmax
0
0
G
G
condition that F (G) is a strategy profile of G again, we have πmax = πmax , which means
that G0 and G share the same core (Φ1 , Φ2 ). For each deal (D1 , D2 ) ∈ γ(G), obviously
(D1 ∩ X10 , D2 ∩ X20 ) is a deal of G0 and (D1 ∩ X10 , D2 ∩ X20 ) ∈ γ(G0 ). It follows that
F1 (G0 ) ⊆ D1 and F2 (G0 ) ⊆ D2 . Therefore F1 (G0 ) ⊆ F1 (G) and F2 (G0 ) ⊆ F2 (G).
On the other hand, for each (D10 , D20 ) ∈ γ(G0 ), we can extend it into a deal (D1 , D2 ) of
G such that (D1 , D2 ) ∈ γ(G) because G0 and G share the same core and X1 ∩ X2 ⊆ Xi0
(i = 1, 2). Since F (G) is a strategy profile of G0 , we have F1 (G) ⊆ D1 ∩ X10 and F2 (G) ⊆
D2 ∩ X20 . It follows that F1 (G) ⊆ D10 and F2 (G) ⊆ D20 , which implies F1 (G) ⊆ F1 (G0 ) and
F2 (G) ⊆ F2 (G0 ). We conclude that F (G0 ) = F (G).
¶
Note that the claim of the above theorem is weaker than Nash’s IIA because, for any
two bargaining games G and G0 , the alternatives of G0 being a subset of the alternatives of
G do not guarantee G0 is a subgame of G. However, as it has been pointed out by many
authors, Nash’s IIA in his original form is no longer a plausible assumption for the domain
of non-convex bargaining problems (Conley & Wilkie, 1996; Mariotti, 1998; Zhang, 2007).

6. Bargaining Power and Risk Posture
As we have shown in Section 1, representing bargainers’ preferences in ordinal does not
automatically solve the problem of ordinal bargaining (see Example 1). This is because
452

An Ordinal Bargaining Solution with Fixed-Point Property

ordinal preference has much less expressive power than cardinal utility. It is unable to express risk posture of a player, which determines the player’s bargaining power. A successful
solution to the ordinal bargaining problem should supply an alternative mean to express
bargainers’ attitude towards risk. In this section, we will illustrate with two case studies
how this problem is solved using our framework.
6.1 Case Study I: Bargainer’s Attitude towards Risk
Let us revisit the bargaining game in Example 4, where the demand sets are X1 = {p, r} and
X2 = {q, ¬r}. Consider the following variations of preferences, which reflect the difference
of players’ attitude towards risk (note that different cases lead to different games).
G1 : X1 = {{p}, {r}} and X2 = {{q}, {¬r}}10
G2 : X2 = {{p, r}, {}} and X2 = {{q}, {¬r}}
G3 : X3 = {{p}, {r}} and X2 = {{q, ¬r}, {}}
G4 : X4 = {{p, r}} and X2 = {{q, ¬r}}
It is easy to calculate the solutions of the games:
F (G1 ) = ({p}, {q})
F (G2 ) = ({p, r}, {q})
F (G3 ) = ({p}, {q, ¬r})
F (G4 ) = (∅, ∅)
In G1 , both players are risk-averse, where the players rank the conflicting item r being
the lowest priority. This means that both have the incentive to reach an agreement. In
G2 , player 1 is more aggressive since the conflicting item is highly ranked. We find that
player 1 won the game in this case. This is not surprising. In general, a risk-averse player
would gain disadvantage in negotiation comparing to a risk-lover (see Roth’s book, 1979b,
p.35-60). G3 is symmetrical to G2 . In G4 , both players are aggressive, therefore the game
ends up with disagreement.
From the example, it is clear to see how bargainers’ attitude towards risk are specified
in our model. A risk-averse player would give the demands that likely conflict with the
demands of the other player relatively lower priorities so that an agreement is more likely
to be reached. In contrast, a risk-loving player would more firmly entrench those conflicting
demands. Notice that logic plays a crucial role in the representation. Simply expressing
bargainers’ demands in physical terms without specifying their relation is not sufficient to
lead to an ordinal solution. It is crucial to specify the logical relations between the demands
from all players. In the above example, the contradiction demands r and ¬r play the main
role in the determination of the solutions. This is the main difference of our model from
the game-theoretic models.
6.2 Case Study II: Discretization of Numerical Games
As we have mentioned in the introduction section, risk posture is represented in non-linearity
of utility functions in Nash’s model. In this subsection, we use the example of cake division
(Example 1) to show how the non-linearity of preferences is represented in our model.
10. The demand sets are represented in prioritized partitions. Their corresponding preference orderings are
p º1 r and q º2 ¬r, respectively.

453

Zhang & Zhang

To represent the bargaining problem in logical form, we need to discretize the domain.
Let pn = {Player A receives no less than n percentages of the cake and player B gets
the remain}, where n is a natural number between 0 to 100. In addition, the following
constraints should be acknowledged by both players:
C = {pn+1 → pn : n = 0, 1, 2, · · · , 100}
which says that if player 1 receives no less than n + 1 percents of the cake, then he must
receive no less than n percents.
Then the demands of two players can be represented as
XA = C ∪ {p0 , p1 , p1 , · · · , p100 }
XB = C ∪ {¬p101 , ¬p100 , ¬p99 , · · · , ¬p1 }
Assume that player A arranges his demands according to the linear scale of his share.
The hierarch of his demand set is11 :
{C, {p0 · · · , p5 }, {p6 · · · , p10 }, · · · , {p96 , · · · , p100 }}
Player B arranges his demands according to the square of his share. The hierarch of her
demand set is12 :
{C, {¬p101 , · · · , ¬p79 }, {¬p78 , · · · , ¬p70 }, · · · , {¬p6 , ¬p5 , ¬p4 }, {¬p3 , ¬p2 }, {¬p1 }}
According to the above setting, the solution of the bargaining game is:
(C ∪ {p34 , p33 , · · · , p0 }, C ∪ {¬p42 , ¬p43 , · · · , ¬p101 })
It is easy to calculate that the players agree on the division of the cake to be 34 ≤ xA < 42
and 58 < xB ≤ 66.13 Therefore our solution gives similar prediction as the game-theoretic
solutions. This indicates that the risk posture of the players has been embedded in our
model. In fact, we can easily see that player B is more aggressive because he ranks higher
conflicting items relatively higher than player A does. For instance, player B ranks the
equal share (50/50) at the 7th level while player A ranks it at 11th level.
As we have seen again from the example, the ordering of demands does not reflect
player’s gains from the demands but represents the player’s preference of retaining or abandoning his demands. This is another significant difference between our bargaining model
and the game-theoretic models.
One may ask that whether a player could get advantages by “cheating” in the sense
that if an agent knows the demands and ranking of the other party, the agent can adjust
her demand hierarchy in order to obtain a better outcome. Yes, it is possible. You can tell
your opposite what you want (your demands) but you should not release your ranking on
your demands. Otherwise, you lose your bargaining power. The reason is that your attitude
towards risk has been encoded in your ranking on your demands.
11. This indicates that player A ignores small differences of divisions. For instance, he may consider that
any share between 0-5% means the same to him. In the real negotiation, the player may request 100%,
95%, · · · 5% in sequence by giving up 5% at each round in a bargaining.
12. Player B claims his share in the sequence 100%, 98%, 95%, · · · by dropping his demand in the scale of
square.
13. Note that there is no communication between the players. Therefore a player may give up more then it
is needed (similar to sealed-bid auction).

454

An Ordinal Bargaining Solution with Fixed-Point Property

7. Computational Complexity
In this section, we study the computational properties of the bargaining solution we developed earlier. We assume that readers are familiar with the complexity classes of P,
NP, coNP, ΣP2 and ΠP2 = coΣP2 . The class of DP contains all languages L such that
P
L = L1 ∩ L2 where L1 is in NP and L2 is in coNP. Also the class ∆Pk+1 = P Σk contains
all languages recognizable in polynomial time by a deterministic Turing machine with a
ΣPk oracle. In particular, the class ∆P2 [O(logn)] contains all languages recognizable by a
deterministic Turing machine with O(logn) calls to an NP oracle14 and ∆P3 contains all languages recognisable by a deterministic Turing machine with a ΣP2 oracle. It is well known
that P ⊆ N P ⊆ DP ⊆ ∆P2 ⊆ ΣP2 ⊆ ∆P3 , and these inclusions are generally believed to be
proper (readers may refer to the work of Papadimitriou, 1994, for further details).
Consider a bargaining game G = ((X1 , º1 ), (X2 , º2 )) where X1 and X2 are finite. As we
have mentioned in Section 2.3, we can always write Xi = Xi1 ∪ · · · ∪ Xim , where Xik ∩ Xil = ∅
for any k 6= l. Also for each k < m, if a formula ϕ ∈ Xik , then there does not exist a
ψ ∈ Xil (k < l) such that ϕ ≺i ψ. Therefore, for the convenience of analysis, in the rest
of this section, we will specify a bargaining game as G = (X1 , X2 ), where X1 =
X2 =

n
S
j=1

m
S

i=1

X1i and

X2j , and X11 , · · ·, X1m , and X21 , · · ·, X2n are the partitions of X1 and X2 respectively

and satisfy the properties mentioned above.
The following four major decision problems are most important in order to understand
the computational properties for our bargaining game model developed in the previous
sections: let G = (X1 , X2 ) be a bargaining game, we would like to decide: (1) whether a
given pair (D1 , D1 ) where Di ⊆ Xi (i = 1, 2) is a deal of G; (2) whether a given pair of
propositional formulas (Φ1 , Φ2 ) is a core of G respectively; (3) whether a given formula is
derivable from the core of G; and (4) whether a given strategy profile of G is a solution of
G, First, we have the following result for deciding a deal for a given bargaining game.
Theorem 6 Let G = (X1 , X2 ) be a bargaining game, and D1 ⊆ X1 and D2 ⊆ X2 . Deciding
whether (D1 , D2 ) is a deal of G is DP-complete.
Proof: Membership proof. According to Definition 2, to decide whether (D1 , D2 ) is a deal
of G, for D1 (or D2 ), we need to check: (1) for each k = 1, · · · , m (or for k 0 = 1, · · · , n resp.),
whether D2 ∪

k
S

(D1 ∩ X1j ) (or D1 ∪

j=1

k0
S

(D2 ∩ X2j ) resp.) is consistent; (2) checking whether

j=1

X1 ∩ X2 ⊆ Di (i = 1, 2); and (3) such D1 and D2 are maximal such subsets of X1 and X2
Sk
respectively. For (1), we observe that for each k, the set j=1
(D1 ∩ X1j ) can be computed in
polynomial time, and checking the consistency of D2 ∪

k
S

(D1 ∩ X1j ) is in NP. The same for

j=1

D2 case. It is obvious to see that (2) can be done in polynomial time. Now we consider (3).
In order to check whether D1 and D2 are the maximal subsets of X1 and X2 respectively
satisfying the condition, we consider the complement the problem: assume that D1 (we
can also assume D2 ) is not the maximal subset of X1 satisfying the required conditions,
14. Note that in literatures, different notions have been used to denote this complexity class such as P N P [logn] ,
P||N P and ΘP
2 .

455

Zhang & Zhang

then there exists some k and some ϕ ∈ (X1 \ D1 ) such that D2 ∪

k
S

((D1 ∪ {ϕ}) ∩ X1k ) is

j=1

consistent. Clearly, we can guess such k, formula ϕ and an interpretation S, then check
whether S is a model of D2 ∪

k
S

((D1 ∪ {ϕ}) ∩ X1k ). Obviously, this is in NP. So the original

j=1

problem is in coNP.
Hardness proof. It is known that for given propositional formulas ϕ1 and ϕ2 , deciding whether ϕ1 is satisfiable and ϕ2 is unsatisfiable is DP-complete (Papadimitriou, 1994).
Given two propositional formulas ϕ1 and ϕ2 , we construct in polynomial time a transformation from the ϕ1 ’s satisfiability and ϕ2 ’s unsatisfiability to a deal decision problem of
a game. We simply define a game G = (X1 , X2 ) = ({¬ϕ2 → ϕ1 ∧ p}, {q} ∪ {¬p}), where
p, q are propositional atoms not occurring in ϕ1 and ϕ2 . Note that X1 ∩ X2 = ∅. Let
D1 = X1 and D2 = {q}. Now we show that (D1 , D2 ) is a deal of G, that is, D2 are the
maximal subset of X2 such that X1 ∪ D2 is consistent, if and only if ϕ1 is satisfiable and ϕ2
is unsatisfiable.
(⇒) Clearly, if ϕ1 is satisfiable and ϕ2 is unsatisfiable, then X1 ∪ D2 is consistent, but
X1 ∪ X2 is not consistent. So (D1 , D2 ) is a deal of G.
(⇐) Suppose ϕ1 and ϕ2 are unsatisfiable. Then X1 itself is not consistent. If both ϕ1 and
ϕ2 are satisfiable, then it is observed that X1 ∪ X2 is consistent. So (D1 , D2 ) is not a deal
of G. Finally, suppose ϕ1 is unsatisfiable and ϕ2 is satisfiable. In this case, X1 ∪ X2 is still
consistent. That means, (D1 , D2 ) is not a deal of G either.
¶
As we can see from the definition, the core of a bargaining plays an essential role in
the construction of the bargaining solution. The following theorem provides the complexity
result of its decision problem.
Theorem 7 Let G a bargaining game. Deciding whether a given pair of sets of propositional
formulas (Φ1 , Φ2 ) is the core of G is DP-complete.
S

S

Proof: Membership proof. Let G = (X1 , X2 ), where X1 = X1i , and X1 = X1j . We
outline an algorithm to check whether (Φ1 , Φ2 ) is the core of G: (1) check whether for some
k, Φ1 =

k
S
i=1

X1i and Φ2 =

k
S
i=1

X2i , and Φ1 6=

k+1
S
i=1

X1i or Φ2 6=

k+1
S
i=1

X2i ; (2) check whether

Φ1 ∪ Φ2 is consistent; and (3) check if Φ1 ∪ X1k+1 ∪ Φ2 ∪ X2k+1 is not consistent. Clearly, (1)
can be done in polynomial time, checking (2) is in NP and checking (3) is in coNP. So the
problem is in DP.
Hardness proof. The hardness proof is similar to that as described in the proof of
Theorem 7 with some variations. Given two propositional formulas ϕ1 and ϕ2 , we reduce
the decision problem of ϕ1 ’s satisfiability and ϕ2 ’s unsatisfiability to our problem. Let
G = (X1 , X2 ), where X1 = {¬ϕ2 → ϕ1 ∧ p} ∪ {q}, and X2 = {q} ∪ {¬p}, where p, q are
propositional atoms not occurring in ϕ1 and ϕ2 . We specify a pair of sets of formulas:
(Φ1 , Φ2 ) = ({¬ϕ2 → ϕ1 ∧ p}, {q}).
Now we show that (Φ1 , Φ2 ) is the core of G if and only if ϕ1 is satisfiable and ϕ2 is
unsatisfiable. Suppose that ϕ1 is satisfiable and ϕ2 is unsatifiable. Then Φ1 ∪Φ = {ϕ1 ∧p, q},
which is consistent. On the other hand, Φ1 ∪ {q} ∪ Φ2 ∪ {¬p} = {ϕ1 ∧ p, q, ¬p}, which is
not consistent. So (Φ1 , Φ2 ) is the core of G.
456

An Ordinal Bargaining Solution with Fixed-Point Property

We prove the other direction. (1) Both ϕ1 and ϕ2 are satisfiable. Then Φ1 ∪ Φ2 =
{(ϕ2 ∨ϕ1 )∧(ϕ2 ∨p), q}, which is consistent. However, we can also see that Φ1 ∪{q}∪Φ2 ∪{¬p}
has at leat one model which satisfies ϕ2 , q and ¬p. This implies that (Φ1 , Φ2 ) is no longer
the core of G. (2) Both ϕ1 and ϕ2 are unsatisfiable. In this case, Φ1 ∪ Φ2 is not consistent
any more. So (Φ1 , Φ2 ) is not the core of G. (3) ϕ1 is unsatisfiable and ϕ2 is satisfiable.
Again, under this situation, Φ1 ∪ Φ2 is no longer consistent, and hence it is not the core of
G. This completes our proof.
¶
Recall that the intuition behind the core is that the final agreement maximizes fairly
each agent’s demands without violating the overall consistency. Then it is interesting to
know whether certain information is derivable from the agent’s demands that are in the
final agreement. Let Φ = (Φ1 , Φ2 ) is the core of bargaining game G. We define C(G) ` ϕ
if and only if Φ1 ` ϕ or Φ2 ` ϕ.
Theorem 8 Given a bargaining game G and a propositional formula ϕ. Deciding whether
C(G) ` ϕ is ∆P2 [O(logn)]-complete.
Proof: Membership proof. We outline an algorithm of deciding C(G) ` ϕ as follows: (1)
compute the core (Φ1 , Φ2 ) of G, and (2) checking if Φ1 ` ϕ or Φ2 ` ϕ. From the definition,
Φ = (Φ1 , Φ2 ) is the core of G iff Φ1 =

k
S
j=1

X1j , and Φ2 =

k
S
j=2

X1j , where k is the maximal

number that makes Φ1 ∪ Φ2 consistent. Clearly, such k can be determinated by binay search
with O(logn) NP oracle calls. Then checking Φ1 ` ϕ or Φ2 ` ϕ can be done with two NP
oracle calls. So the problem is in ∆P2 [O(logn)].
Hardness proof. We reduce the ∆P2 [O(logn)]-complete PARITY B
ω (Kobler, Schoning,
& Wagner, 1987; Wagner, 1988) to our problem. An instance of PARITY B
ω is a set of
propositional formulas ϕ1 , · · · , ϕn such that if ϕi is not satisfiable, then for each j ≥ i, ϕj
is not satisfiable. The problem is to decide whether the number of satisfiable formulas is
odd. Without loss of generality, we assume n is an even number. Then we construct in
polynomial time a bargaining game G = (X1 , X2 ) as follows:
S

n/2

X1 = X1 = {¬ϕ2 → p} ∪ {¬ϕ4 → ϕ3 ∧ p} ∪ · · · {¬ϕn → ϕn−1 ∧ p},
S n/2
X2 = X2 = {q1 } ∪ {q2 } ∪ · · · ∪ {qn/2 },
where p, q1 , · · · , qn/2 are propositional atoms not occurring in ϕ1 , · · · , ϕn . Let ϕ = p. Now we
show that C(G) ` p if and only if there is an odd number of satisfiable formulas in ϕ1 , · · · , ϕn .
First, suppose k is an odd number, ϕ1 , · · · , ϕk are satisfiable, and ϕk+1 , · · · , ϕn are not satisfiable. Then it is observed that Φ =

[k/2]
S
j=1

X1j ∪ X2j = {¬ϕ2 → p, ¬ϕ4 → ϕ3 ∧ p, · · · , ¬ϕk+1 →

ϕk ∧ p} ∪ {q1 , · · · , q[k/2] } is consistent, and Φ ∪ {¬ϕk+3 → ϕk+2 ∧ p} ∪ {q[k/2]+1 } is not con[k/2]
S

sistent. So (

j=1

X1j ,

[k/2]
S
j=1

X2j ) is the core of G. Also, since ¬ϕk+1 is not satisfiable, it follows

that ¬ϕk+1 → ϕk ∧ p is reduced to ϕk ∧ p, which is contained in

[k/2]
S
j=1

X1j . So C(G) ` p.

Second, we assume that there is an even number of satisfiable formulas in ϕ1 , · · · , ϕn . Let k
457

Zhang & Zhang

be the even number such that ϕ1 , · · · , ϕk are satisfiable and ϕk+1 , · · · , ϕn are not satisfiable.
In this case, it can be observed that Φ =

k/2
S
j=1

X1j ∪X2j = {¬ϕ2 → p, ¬ϕ4 → ϕ3 ∧p, · · · , ¬ϕk →

ϕk−1 ∧ p} ∪ {q1 , · · · , qk/2 } is consistent, while Φ ∪ {¬ϕk+2 → ϕk+1 ∧ p} ∪ {qk/2+1 } is not
consistent. So (

k/2
S

j=1
k/2
S

from

j=1

X1j ,

k/2
S
j=1

X2j ) is the core of G. Then it is obvious that p cannot be derived

X1j . That is, C(G) 6` p. This completes our proof.

¶

Finally, we consider the decision problem for the solution of a given bargaining game.
The following theorem gives its complexity upper bound.
Theorem 9 Let G be a bargaining game,and (S1 , S2 ) a strategy profile of G. Deciding
whether (S1 , S2 ) is the solution of G is in ∆P3 .
Proof: From Definition 3, we need to check whether S1 =
T
(D1 ,D2 )∈γ(G)

T
(D1 ,D2 )∈γ(G)

D1 and S2 =

D2 , where γ(G) = {(D1 , D2 ) ∈ Ω(G) : Φ1 ⊆ D1 , Φ2 ⊆ D2 }, and (Φ1 , Φ2 ) is

the core of G. Note that for simplicity, here we use the notion of core to represent the
solution.
T
We first consider the complement of deciding S1 =
D1 : checking whether
S1 6=

T
(D1 ,D2 )∈γ(G)

T

(D1 ,D2 )∈γ(G)

D1 . Clearly, S1 6=

T
(D1 ,D2 )∈γ(G)

(D1 ,D2 )∈γ(G)

D1 iff (1) S1 6⊆

T
(D1 ,D2 )∈γ(G)

D1 ; or (2)

D1 6⊆ S1 . We first guess a pair of sets of propositional formulas (Φ1 , Φ2 ), and

check if it is the core of G. According to Theorem 8, we know that this is in ΣP2 . Clearly,
T
(1) holds iff there exists a formula ϕ such that (a) ϕ ∈ S1 , and (b) ϕ 6∈
D1 .
(D1 ,D2 )∈γ(G)

Further, (b) holds iff there exists a deal (D1 , D2 ) such that Φ1 ⊆ D1 , Φ2 ⊆ D2 and ϕ 6∈ D1 .
Now we guess ϕ and (D1 , D2 ), and then check if ϕ ∈ S1 , (D1 , D2 ) is a deal containing the
core, and ϕ 6∈ D1 . From Thorem 7 we know that checking (D1 , D2 ) is a deal can be done
with two NP oracle calls. So task (1) can be solved in ΣP2 .
T
On the other hand, (2) holds iff there exists some ϕ such that ϕ ∈
D1
(D1 ,D2 )∈γ(G)

and ϕ ∈
6 S1 . To solve task (2), we consider its complement: for all ϕ, if ϕ 6∈ S1 , then
T
ϕ 6∈
D1 . Since there are only |X1 | + |X2 | formulas we need to check, checking
(D1 ,D2 )∈γ(G)

all ϕ that are not in S1 can be done in linear time. Then for each ϕ 6∈ S1 , we need to
T
check if ϕ 6∈
D1 , which, as shown above, can be done in ΣP2 . Therefore, for all ϕ
(D1 ,D2 )∈γ(G)

that are not in S1 , there are at most |X1 | + |X2 | checkings of whether ϕ 6∈
P

which is in P Σ2 = ∆P3 . This follows that deciding whether S1 =
Consequently, the original problem is also in ∆P3 .

T

T
(D1 ,D2 )∈γ(G)

(D1 ,D2 )∈γ(G)

D1 ,

D1 is in ∆P3 .
¶

From the proof of Theorem 10, it can be observed that the computation for a bargaining
game is very different from that of Nebel’s prioritized belief revision.
458

An Ordinal Bargaining Solution with Fixed-Point Property

Theorem 10 Let G be a bargaining game and (S1 , S2 ) a strategy profile of G. Deciding
whether (S1 , S2 ) is the solution of G is DP-hard.
Proof: We consider a special game G = (X1 , X2 ) where X1 = X11 ∪ X12 , and X2 = X21 (i.e.
X2 has a singleton partition), and X11 ∪ X21 is consistent but X1 ∪ X2 is not consistent. In
this case, we know that (X11 , X21 ) is the core of G. So our question is reduced to decide
whether S2 = X2 , and S1 is the maximal subset of X1 containing X11 such that S1 ∪ X2 is
consistent. From the proof of Theorem 3, it is easy to see that this is DP-hard.
¶
Obviously, there is a gap between the lower bound and upper bound for the solution
decision problem. This also sheds a light that our bargaining solution cannot be represented
in terms of the traditional belief revision operators.
Theorem 11 Let G be a bargaining game and (S1 , S2 ) a strategy profile of G. Deciding
whether (S1 , S2 ) is the solution of G is in ∆P2 [O(logn)], given that the set of all deals Ω(G)
is provided.
Proof: To decide whether (S1 , S2 ) is a solution of G, we need to do the following: (1)
compute the core (Φ1 , Φ2 ) of G; (2) compute γ(G) from Ω(G) and (Φ1 , Φ2 ); (3) compute
T
T
D10 =
D1 and D20 =
D2 ; and (4) compare whether S1 = D10 and
(D1 ,D2 )∈γ(G)

(D1 ,D2 )∈γ(G)

S2 = D20 . Task (1) can be solved with one ∆P2 [O(logn)] oracle call; having (Φ1 , Φ2 ), task
(2) can be done in polynomial time; and tasks (3) and (4) can be done in polynomial time.
So the problem is in ∆P2 [O(logn)].
¶

8. Related Work
The investigation of ordinal bargaining theory diverges in two directions. The first direction
focuses on the existence of ordinal solution in the Nash bargaining model. As we have
mentioned in the introduction section, Shapley, Kibris, Safra and Samet have shown that
there is a solution to the bargaining problems with three agents or more, which satisfies
ordinal invariance, symmetry and Pareto optimality (Shubik, 1982; Özgür Kibris, 2004;
Safra & Samet, 2004). This result is interesting because it shows a difference between
bilateral bargaining and multilateral bargaining. However, it does not solve the problem
of ordinal bargaining because the solution is not constructive and, more importantly, no
alternative mean is offered to facilitate the representation of bargainers’ risk attitude. Calvo
and Perers investigated the bargaining problems with mixed players: cardinal players and
ordinal players (Calvo & Peters, 2005). A bargaining solution is called utility invariant
if it is ordinally invariant for the ordinal players and cardinally invariant for the cardinal
players. It is proved that there is a solution satisfying utility invariance, Pareto optimality
and individual rationality provided at least one player is cardinal. Obviously, this result is
only peripheral because a utility invariant solution is not necessarily ordinal invariant.
The other direction of the investigation tries to circumvent Shapley’s impossibility result
by altering the Nash bargaining model. Rubinstein et al. reinterpreted the Nash bargaining
solution with respect to ordinal preferences (Rubinstein et al., 1992). By restating Nash’s
459

Zhang & Zhang

axioms, it is proved that the redefined solution, referred to as ordinal-Nash solution, satisfies Pareto optimality, symmetry and contraction independence (ordinal invariance holds
trivially). However, the result is based on the assumption that the preference ordering of
each player is complete, transitive and continuous on the set of finite lotteries over a topological space. It is unclear that how to use such a specific preference language to describe
players’ risk attitudes. More important, the advantages of ordinal preferences, such as ease
of elicitation and robustness of corresponded solutions, may be lost once the preference
ordering is extended to the space of lotteries. O’Neill et al. model a bargaining situation
with a family of Nash bargaining games, parameterized by time (O’Neill et al., 2004). A
bargaining solution is then defined as a function that specifies an outcome at each time.
With the model, bargainers’ risk attitude can be expressed through varying preferences over
time, which is very intuitive. Since a solution is no longer a single point but a function
over time, the construction of the proposed ordinal solution relies on the solution of sets of
simultaneous differential equations.
This work is developed based on a series of previous work of the authors. The fixed-point
condition discussed in this paper was firstly proposed by Zhang et al. (2004)15 . Zhang and
Zhang (2006a, 2006b) presented a logical solution to the bilateral bargaining problem based
on ordinal preferences. However, that solution does not satisfy the fixed point condition.
Zhang (2008) showed that a revision of the solution satisfies the fixed-point condition. The
present paper further develops and systemizes the solution and discusses its logical and
game-theoretic properties.

9. Conclusion
We have presented a bargaining solution to the bilateral bargaining problem based on
the logical representation of bargaining demands and ordinal representation of bargainers’
preferences. We have shown that the solution satisfies most desirable logical properties,
such as individual rationality (logical version), consistency and collective rationality, and
the desirable game-theoretic properties, such as weak Pareto optimality, restricted symmetry
and contraction invariance. The ordinal invariance and game-theoretic version of individual
rationality hold trivially. Due to the discrete nature of logical representation, the solution
is not (strictly) Pareto optimal and does not satisfy symmetry. However, if the demand sets
of two players are logically closed, the solution meets a restricted version of symmetry. In
addition, we have demonstrated that under the logical closedness assumption, the outcome
of a negotiation is the result of mutual belief revision in terms of Nebel’s prioritized belief
base revision. This result established a link between bargaining theory and belief revision.
Such a link would play an important role in the future on the research of multiagent belief
revision and logic-based bargaining theory. Our complexity analysis indicates that the
computation of bargaining solution is more difficult than prioritized belief base revision.
A satisfactory model of bargaining should be able to encode the key factors that determine the bargaining outcome, such as bargainer’s demands, preferences, attitudes towards
15. Jin et al. (2007) also introduced a fixed-point condition for negotiation functions, which says that
under certain conditions, negotiating on the outcome of a negotiation generates the same outcome.
Obviously our bargaining solution satisfies this fixed-point condition because any outcome of bargaining
is consistent, which remains the same in any further negotiation.

460

An Ordinal Bargaining Solution with Fixed-Point Property

risk and so on. Cardinal utility specifies two sorts of information: preference over possible agreements (via the ordering of utility values) and risk attitudes (via the non-linearity
of utility function). However, the second sort of information, which determines players’
bargaining power, may be lost after an ordinal transformation. Meanwhile, a bargaining
model based purely on ordinal information about preferences does not automatically solve
the problem because bargainers’ risk attitude is even inexpressible in such a model. Therefore an ordinal bargaining theory must supply a facility to describe the information other
than ordinal preferences, including risk attitudes. In this paper, we specify a bargaining
situation in logical structure. Bargainer’s demands, goals and beliefs are described in logical
statements. The conflicts of demands between two players can then be identified through
consistency checking. More importantly, bargainer’s attitudes towards risk are expressible
in our model in a natural way: a risk-averse player tends to give a conflicting demand a
relatively lower priority so that an agreement could be more likely reached while a risk-lover
would firmly entrench her demands with less care about whether her demands contradict her
opponent’s.
A few issues are worth further investigation. Firstly, we have shown that our solution
satisfies a set of logical properties and game-theoretic properties. It is interesting to know
whether there is an axiomatic system that exactly characterizes the solution. The main
challenge here is that the construction of our solution is syntax-dependent. If we simply
impose the logical closedness on the demand sets, we will lose a few desirable properties,
such as the inclusion of irrelevant items and computational results. If we do not apply the
assumption, we shall need the axioms to specify the way of logical representation. In other
words, the axioms have to specify how a demand set should be represented syntactically.
Secondly, the present work offers a solution to the bilateral bargaining situations. It
does not supply a model to bargaining agents. Therefore the current framework does not
deal with the issues like “how a demand is formed?”, “why a demand should be ranked
higher than another?” or “how to bargain effectively?”. It is interesting how a logic of
agency can be used or developed to model bargaining agents and how such a logic interacts
with the logic of bargaining.
Finally, a few issues on the computational complexity of the proposed solution remain
unsolved. As we have shown in this paper the membership checking of the solution is DPhard but in ∆P3 . It is not clear yet how this upper bound and lower bound gap can be
closed. We think some new complexity proof technique may be needed for this challenge.

Acknowledgments
The authors wish to thank Norman Foo, Michael Thielscher and the anonymous reviewers
for their comments. This work was partially supported by the Australian Research Council
with project LP0883646.

References
Brewka, G. (1989). Preferred subtheories: An extended logical framework for default reasoning. In Proceedings of the 11th International Joint Conference on Artificial Intelligence
(IJCAI), pp. 1043–1048.
461

Zhang & Zhang

Calvo, E., & Peters, H. (2005). Bargaining with ordinal and cardinal players. Games and
Economic Behavior, 52, 20–33.
Conley, J. P., & Wilkie, S. (1991). The bargaining problem without convexity: extending
the Egalitarian and Kalai-Smorodinsky solutions. Economics Letters, 36 (4), 365–369.
Conley, J. P., & Wilkie, S. (1996). An extension of the Nash bargaining solution to nonconvex problems. Games and Economic Behavior, 13, 26–38.
Gärdenfors, P. (1992). Belief revision: an introduction. In Belief Revision, pp. 1–20. Cambridge University Press.
Gärdenfors, P., & Makinson, D. (1988). Revisions of knowledge systems using epistemic
entrenchment. In Proceedings of the Second Conference on Theoretical Aspect of Reasoning About Knowledge (TARK’88), pp. 83–96.
Jin, Y., Thielscher, M., & Zhang, D. (2007). Mutual belief revision: semantics and computation. In Proceedings of the 22nd AAAI Conference on Artificial Intelligence (AAAI07), pp. 440–445.
Kalai, E. (1977). Proportional solutions to bargaining situations: interpersonal utility comparisons. Econometrica, 45 (7), 1623–1630.
Kalai, E., & Smorodinsky, M. (1975). Other solutions to Nash’s bargaining problem. Econometrica, 43 (3), 513–518.
Kaneko, M. (1980). An extension of the Nash bargaining problem and the Nash social
welfare function. Theory and Decision, 12, 135–148.
Kobler, J., Schoning, U., & Wagner, K. (1987). The difference and truth-table hierarchies
for NP. RAIRO - Theoretical Informatics and Applications, 21, 419–37.
Kraus, S., Sycara, K., & Evenchik, A. (1998). Reaching agreements through argumentation:
a logical model and implementation. Artificial Intelligence, 104, 1–69.
Mariotti, M. (1996). Non-optimal Nash bargaining solutions. Economics Letters, 52, 15–20.
Mariotti, M. (1998). Extending Nash’s axioms to nonconvex problems. Games and Economic Behavior, 22, 377–383.
Meyer, T., Foo, N., Kwok, R., & Zhang, D. (2004). Logical foundations of negotiation:
strategies and preferences. In Proceedings of the 9th International Conference on the
Principles of Knowledge Representation and Reasoning (KR’04), pp. 311–318.
Muthoo, A. (1999). Bargaining Theory with Applications. Cambridge University Press.
Myerson, R. B. (1991). Game Theory: Analysis of Conflict. Harvard University Press.
Nash, J. (1950). The bargaining problem. Econometrica, 18 (2), 155–162.
Nebel, B. (1992). Syntax-based approaches to belief revision. In Gärdenfors (Ed.), Belief
Revision, pp. 52–88. Cambridge University Press.
O’Neill, B., Samet, D., Wiener, Z., & Winter, E. (2004). Bargaining with an agenda. Games
and Economic Behavior, 48, 139–153.
Osborne, M. J., & Rubinstein, A. (1990). Bargaining and Markets. Academic Press.
462

An Ordinal Bargaining Solution with Fixed-Point Property

Özgür Kibris (2004). Ordinal invariance in multicoalitional bargaining. Games and Economic Behavior, 46, 76–87.
Papadimitriou, C. (1994). Computational Complexity. Addison Wesley.
Parsons, S., Sierra, C., & Jennings, N. R. (1998). Agents that reason and negotiate by
arguing. Journal of Logic and Computation, 8 (3), 261–292.
Perles, M. A., & Maschler, M. (1981). The super-additive solution for the Nash bargaining
game. International Journal of Game Theory, 10, 163–193.
Poole, D. (1988). A logical framework for default reasoning. Artif. Intell., 36 (1), 27–47.
Rosenschein, J. S., & Zlotkin, G. (1994). Rules of Encounter: Designing Conventions for
Automated Negotiation among Computers. The MIT Press.
Roth, A. E. (1979a). Axiomatic Models of Bargaining. Springer-Verlag.
Roth, A. E. (1979b). An impossibility result concerning n-person bargaining games. International Journal of Game Theory, 8, 129–132.
Rubinstein, A., Safra, Z., & Thomson, W. (1992). On the interpretation of the Nash bargaining solution and its extension to non-expected utility preferences. Econometrica,
60 (5), 1171–1186.
Safra, Z., & Samet, D. (2004). An ordinal solution to bargaining problems with many
players. Games and Economic Behavior, 46, 129–142.
Sakovics, J. (2004). A meaningful two-person bargaining solution based on ordinal preferences. Economics Bulletin, pp. 1–6.
Sharpley, L. S. (1969). Utility comparison and the theory of games. In Guilbaud, G. T.
(Ed.), La Deécision, pp. 251–263. Paris: Editions du Centre National de la Recherche
Scientifique.
Shubik, M. (1982). Game Theory in the Social Sciences: Concepts and Solutions. MIT
Press, Cambridge.
Sycara, K. P. (1990). Persuasive argumentation in negotiation. Theory and Decision, 28,
203–242.
Thomson, W. (1994). Cooperative models of bargaining. In Aumann, R., & Hart, S. (Eds.),
Handbook of Game Theory, Vol. 2, chap. 35, pp. 1237–1284. Elsevier.
Wagner, K. (1988). Bounded query computations. In Proceedings of the 3rd Conference on
Structure in Complexity Theory, pp. 419–437.
Xu, Y., & Yoshihara, N. (2006). Alternative characterizations of three bargaining solutions
for nonconvex problems. Games and Economic Behavior, 57 (1), 86–92.
Zhang, D. (2005). A logical model of Nash bargaining solution. In Proceedings of the 19th
International Joint Conference on Artificial Intelligence (IJCAI-05), pp. 983–988.
Zhang, D. (2007). Reasoning about bargaining situations. In Proceedings of the 22nd AAAI
Conference on Artificial Intelligence (AAAI-07), pp. 154–159.
Zhang, D. (2008). A fixed-point property of logic-based bargaining solution. In AI 2008,
pp. 30–41. Springer.
463

Zhang & Zhang

Zhang, D., & Foo, N. (2001). Infinitary belief revision. Journal of Philosophical Logic,
30 (6), 525–570.
Zhang, D., Foo, N., Meyer, T., & Kwok, R. (2004). Negotiation as mutual belief revision,.
In Proceedings of the 19th National Conference on Artificial Intelligence (AAAI-04),
pp. 317–322.
Zhang, D., & Zhang, Y. (2006a). A computational model of logic-based negotiation. In
Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06), pp.
728–733.
Zhang, D., & Zhang, Y. (2006b). Logical properties of belief-revision-based bargaining
solution. In AI 2006: Advances in Artificial Intelligence (AI-06), pp. 79–89.

464

Journal of Artificial Intelligence Research 33 (2008)

Submitted 12/07; published 10/08

On Similarities between Inference in Game Theory and
Machine Learning
Iead Rezek

i.rezek@imperial.ac.uk

Department of Clinical Neurosciences, Imperial College
London, SW7 2AZ, UK

David S. Leslie

david.leslie@bristol.ac.uk

Department of Mathematics, University of Bristol
Bristol, BS8 1TW, UK

Steven Reece
Stephen J. Roberts

reece@robots.ox.ac.uk
sjrob@robots.ox.ac.uk

Department of Engineering Science, University of Oxford
Oxford, OX1 3PJ, UK

Alex Rogers
Rajdeep K. Dash
Nicholas R. Jennings

acr@ecs.soton.ac.uk
rkd@ecs.soton.ac.uk
nrj@ecs.soton.ac.uk

School of Electronics and Computer Science, University of Southampton
Southampton, SO17 1BJ, UK

Abstract

In this paper, we elucidate the equivalence between inference in game theory and machine
learning. Our aim in so doing is to establish an equivalent vocabulary between the two
domains so as to facilitate developments at the intersection of both fields, and as proof of
the usefulness of this approach, we use recent developments in each field to make useful
improvements to the other. More specifically, we consider the analogies between smooth
best responses in fictitious play and Bayesian inference methods. Initially, we use these
insights to develop and demonstrate an improved algorithm for learning in games based
on probabilistic moderation. That is, by integrating over the distribution of opponent
strategies (a Bayesian approach within machine learning) rather than taking a simple empirical average (the approach used in standard fictitious play) we derive a novel moderated
fictitious play algorithm and show that it is more likely than standard fictitious play to
converge to a payoff-dominant but risk-dominated Nash equilibrium in a simple coordination game. Furthermore we consider the converse case, and show how insights from game
theory can be used to derive two improved mean field variational learning algorithms. We
first show that the standard update rule of mean field variational learning is analogous
to a Cournot adjustment within game theory. By analogy with fictitious play, we then
suggest an improved update rule, and show that this results in fictitious variational play,
an improved mean field variational learning algorithm that exhibits better convergence in
highly or strongly connected graphical models. Second, we use a recent advance in fictitious
play, namely dynamic fictitious play, to derive a derivative action variational learning algorithm, that exhibits superior convergence properties on a canonical machine learning
problem (clustering a mixture distribution).
c
2008
AI Access Foundation. All rights reserved.

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

1. Introduction
There has recently been increasing interest in research at the intersection of game theory
and machine learning (Shoham, Powers, & Grenager, 2007; Greenwald & Littman, 2007).
Such work is motivated by the observation that whilst these two fields have traditionally
been viewed as disparate research areas, there is actually a great deal of commonality
between them that can be exploited within both fields. For example, insights from the
machine learning literature on graphical models have led to the development of efficient
algorithms for calculating Nash equilibria in large multi-player games (Kearns, Littman, &
Singh, 2001). Similarly, the development of boosting algorithms within machine learning
has been facilitated by regarding them as being engaged in a zero-sum game against a base
learner (Freund & Schapire, 1997; Demiriz, Bennett, & Shawe-Taylor, 2002).
While such interdisciplinary inspiration is promising, unless there is a clearer understanding of the principal connections that exist between the two disciplines, these examples
will remain isolated pieces of research. Thus, the general goal of our work is to explore in a
more formal way the commonalities between game theory and machine learning. In order
to do so, we consider an important problem that is central to both fields; that of making
inferences based on previous observations.
We first consider game theory, where this problem occurs in the context of inferring
the correct strategy to play against an opponent within a repeated game. This is generally
termed learning in games and a common approach is to use an algorithm based on fictitious
play (see Fudenberg & Levine, 1999). Here, we show that an insight from Bayesian inference
(a standard machine learning technique) allow us to derive an improved fictitious play
algorithm. More specifically, we show that by integrating over the distribution of opponent
strategies (a standard approach within machine learning), rather than taking a simple
empirical average (the approach used within the standard fictitious play algorithm), we can
derive a novel moderated fictitious play algorithm. Moreover, we then go on to demonstrate
that this algorithm is more likely than standard fictitious play to converge to a payoffdominant but risk-dominated Nash equilibrium in a simple coordination game1 .
In the second part of the paper, we consider the mean field variational learning algorithm, which is a popular means of making inferences within machine learning. Here we
show that analogies with game theory allows us to suggest two improved variational learning
algorithms. We first show that the standard update rule of mean field variational learning
is analogous to a Cournot adjustment process within game theory. By analogy with fictitious play, we suggest an improved update rule, which leads to an improved mean field
variational learning algorithm, which we term fictitious variational play. By appealing to
game-theoretic arguments, we prove the convergence of this procedure (in contrast standard
mean-field updates can suffer from “thrashing” behaviour (Wolpert, Strauss, & Rajnarayan,
2006) similar to a Cournot process), and we show that this algorithm exhibits better convergence in highly or strongly connected graphical models. Second, we show that a recent
advance in fictitious play, namely dynamic fictitious play (Shamma & Arslan, 2005), can
be used to derive the novel derivative action variational learning algorithm. We demon1. We note here that a form of Bayesian learning in games is known to converge to equilibrium (Kalai
& Lehrer, 1993). However, in that work the players perform Bayesian calculations in the space of all
repeated game strategies, resulting in extremely complex inference problems. In contrast, we consider a
Bayesian extension of fictitious play, using insights from machine learning to aid myopic decision-making.

260

On Similarities between Inference in Game Theory and Machine Learning

strate the properties of this algorithm on a canonical machine learning problem (clustering
a mixture distribution), and show that it again exhibits superior convergence properties
compared to the standard algorithm.
When taken together, our results suggest that there is much to be gained from a closer
examination of the intersection of game theory and machine learning. To this end, in this
paper, we present a range of insights that allow us to derive improved learning algorithms
in both fields. As such, we suggest that these initial first steps herald the possibility of more
significant gains if this area is exploited in the future.
The remainder of the paper is organised as follows. In section 2 we discuss work related
to the interplay of learning in games and machine learning. Then, in section 3 we discuss how
techniques within machine learning can be used in relation to learning in games. We review
the standard stochastic fictitious play algorithm, and then go on to derive and evaluate our
moderated fictitious play algorithm. We then change focus, and in section 4, we show how
techniques within game theory apply to machine learning algorithms. Again, we initially
review the standard mean field variational learning algorithm, and then, by analogy with
fictitious play and the Cournot adjustment, present in section 4.2 our fictitious variational
play algorithm. In section 4.3 we continue this theme and incorporate insights from dynamic
fictitious play to derive and evaluate our derivative action variational learning algorithm.
Finally, we conclude and discuss future directions in section 5.

2. Related Work
The topics of inference and game theory have traditionally been viewed as separate research
areas, and consequently little previous research has exploited their common features to
achieve profitable cross-fertilisation.
One area where progress has been made is in the use of concepts from game theory to
find the optimum of a multi-dimensional function. In this context, Lambert, Epelman, and
Smith (2005) used fictitious play as an optimisation heuristic where players each represent
a single variable and act independently to optimise a global cost function. The analysis
restricts attention to the class of objective functions that are products of these independent
variables, and is thus rather limited in practice.
In a similar vein, Wolpert and co-authors consider independent players who, through
their actions, are attempting to maximise a global cost function (Lee & Wolpert, 2004;
Wolpert, 2004). In this body of work, however, the optimisation is to be carried out
with respect to the joint distributions of the variables chosen by all players. A mean-field
approach is taken, resulting in independent choices for each player; the approach is very
similar in flavour to that presented in Section 4 of this article. However, in this paper we
explicitly use advances in the theory of learning in games to develop improved optimisation
algorithms.
In the context of improving game-theoretical algorithms using techniques from machine
learning and statistics, Fudenberg and Levine (1999) show that fictitious play has an interpretation as a Bayesian learning procedure. However this interpretation shows fictitious
play to be a type of plug-in classifier (Ripley, 2000), and they stop short of using the full
power of Bayesian techniques to improve the method. In contrast, in this article we take a
fully Bayesian approach to deciding the optimal action at each play of the game.
261

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Other articles where cross-over has been attempted, but that do not overlap greatly with
the current article, include that of Demiriz et al. (2002) and Freund and Schapire (1997),
who have interpreted boosting algorithms as zero sum games, and Kearns et al. (2001) who
consider the use of techniques from graphical models (Jordan, Ghahramani, Jaakkola, &
Saul, 1997) to help calculate equilibria in graphical games.

3. Fictitious Play
Fictitious play is an important model for learning in games and the source of the algorithmic
developments presented later in this work. We begin by presenting the notation and terminology used in the standard game-theoretic representation of fictitious play. The reader is
referred to the work of Fudenberg and Levine (1999) for a more extensive discussion.
We will consider strategic-form games with I players that are indexed by i ∈ {1, . . . , I},
and where we use −i to index all players other than player i. By Si we denote the finite set of
pure strategies S i (also known as actions) available to player i, by S the set S1 × S2 × · · · × SI
of pure strategy profiles of all players, and by S−i the set of pure strategy profiles of all
players other than i. Each player’s pay-off function is denoted by Ri : S → R and maps pure
strategy profiles to the real line, i.e. each set of actions selected by the players is associated
with a real number.
This simple model is usually extended to allow players to use mixed strategies π i ∈
∆(Si ), where ∆(Si ) denotes the set of probability distributions over the pure strategy set
Si . Hence each π i is a probability distribution on the discrete space Si . Writing π =
(π 1 , π 2 , . . . , π I ) for the probability distribution on S which is the product of the individual
mixed strategies, π i , we extend the reward functions to the space of mixed strategies by
setting
Ri (π) = Eπ Ri (S)
(1)
where Eπ denotes expectation with respect to the pure strategy profile S ∈ S selected
according to the distribution π. Similarly
Ri (S i , π −i ) = Eπ−i Ri (S i , S −i )

(2)

where Eπ−i denotes expectation with respect to S −i ∈ S−i .
The standard solution concept in game theory is the Nash equilibrium. This is a mixed
strategy profile π such that for each i
Ri (π) ≥ Ri (π̃ i , π −i ) for all π̃ i ∈ ∆(Si ).

(3)

In other words, a Nash equilibrium is a set of mixed strategies such that no player can
increase their expected reward by unilaterally changing their strategy.
If all players receive an identical reward then we have what is known as a partnership
game. In this case, players are acting independently while trying to optimise a global
objective function. This special case is important since it corresponds to a distributed
optimisation problem, where the objective function represents the reward function of the
game. At a Nash equilibrium it is impossible to improve the expected value of this objective
function by changing the probability distribution of a single player. Thus Nash equilibria
correspond to local optima of the objective function.
262

On Similarities between Inference in Game Theory and Machine Learning

Fictitious play proceeds by assuming that during repeated play of a game, every player
monitors the action of their opponent. The players continually update estimates σ of their
opponents’ mixed strategies by taking the empirical average of past action choices of the
other players. Given an estimate of play, a player selects a best response (i.e. an action
that maximizes their expected reward given their beliefs). Thus, at time t, the estimates
are updated according to


1
1
i
(4)
bi (σt−i )
σt+1 = 1 −
σti +
t+1
t+1
where bi (σt−i ), the best response to the other players’ empirical mixed strategies, satisfies
bi (σt−i ) ∈ argmax Ri (S i , σt−i ).

(5)

S i ∈Si

In certain classes of games, including the partnership games mentioned previously, beliefs that evolve according to equation 4 are known to converge to Nash equilibrium. On
the other hand, there also exist games for which non-convergence of equation 4 has been
shown Fudenberg and Levine.
3.1 Stochastic Fictitious Play
Now, one objection to fictitious play has been the discontinuity of the best response function, which means that players almost always play pure strategies, even when beliefs have
converged to a Nash equilibrium in mixed strategies. To overcome such problems, fictitious
play has been generalized to stochastic fictitious play (see Fudenberg & Levine, 1999) which
employs a smooth best response function, defined by
β i (π −i ) = argmax Ri (π i , π −i ) + τ v i (π i )

(6)

π i ∈∆(Si )

where τ is a temperature parameter and v i is a smooth, strictly differentiable concave
function such that as π i approaches the boundary of ∆(Si ) the slope of v i becomes infinite.
One popular choice of smoothing function v i is the entropy function, which results in the
logistic choice function with the noise or temperature parameter τ


1 i i −i
1
i −i
i
R (S , π )
(7)
β (π )(S ) = exp
Z
τ
where the partition function Z ensures that the best response adds to unity. Thus, in
stochastic fictitious play, players choose at every round an action randomly selected using
the smooth best response to their current estimate of the opponents’ probability of play.
The estimates under this process are also known to converge in several classes of games,
including partnership games (Hofbauer & Hopkins, 2005) which will be discussed again in
section 4. Several further extensions of fictitious play have been introduced in attempts
to extend the classes of games in which convergence can be achieved, including weakened
fictitious play (Leslie & Collins, 2005; van der Genugten, 2000) and dynamic fictitious
play (Shamma & Arslan, 2005). We will use these extensions in the second part of the
paper to improve convergence of modifications of variational learning.
263

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

3.2 Moderated Fictitious Play
In fictitious play, each player “learns” by using an empirical average of the past action
choices of the other players to estimate their current mixed strategy. This estimate can be
thought of as the maximum likelihood estimate (MLE) of the opponent’s mixed strategy at
time t under the assumption that all the actions of each player i have been selected using
a multinomial distribution with parameter π −i such that
σt−i = π̂t−i = argmax
π −i

t
Y

P (Su−i ; π −i )

(8)

u=0

where
P (Su−i ; π −i ) can be modelled by a product of multinomial distributions P (Su−i ; π −i ) =
QI
j
−i
j
corresponds to the
j=1,j6=i π (Su ). Fudenberg and Levine note that this choice of πt
maximum a posteriori estimate of the opponent mixed strategies in a Bayesian model.
However, from a machine learning perspective, the logistic choice best response function, given in equation 7, may be viewed as a single layer neural network with a sigmoid
activation function (Bishop, 2006). Substituting the unknown parameter, πt−i , in equation
7 by its maximum likelihood estimate (or by a Bayesian point estimate) fails to take into
account anything that is known about the parameter’s distribution; classifiers using this approach have been called “plug-in” classifiers (Ripley, 2000). From a Bayesian perspective,
better predictions can be obtained by integrating out the parameter’s distribution and thus
computing the posterior predictive best response function (Gelman, Carlin, Stern, & Rubin,
2000). This process is known in the neural network literature as “moderation” (MacKay,
1992).
This suggests a modification of fictitious play that we term moderated fictitious play.
In this, every player uses the posterior predictive best response, obtained by integrating
out over all opponent mixed strategies, weighted by their posterior distribution given the
previously observed actions. As it is conventional to choose to use the posterior mean as
the point estimate, the strategy now chosen by player i is
Z
−i
(9)
) d πt−i
β̃ti = β i (πt−i )P (πt−i | S1:t
−i
) is the posterior probability of the opponents’ mixed strategies πt−i given
where P (πt−i | S1:t
−i
of play from time 1 to t.
the observed history S1:t
Since we model the observed pure strategies of player j as observations of a multinomial
random variable with parameters π j , we place a uniform Dirichlet prior, Dir(π j ; αj0 ), on
each π j , with all parameters αj0k = 1. The posterior distribution of πt−i is therefore again a
product of independent Dirichlet distributions,
Y
−i
P (πt−i | s−i
Dir(πtj ; αjt )
(10)
1:t ; αt ) =
j6=i

P
with αjt (sj ) = 1 + tu=1 I{sju = sj }, where I is an indicator function.
There are multiple approaches to estimating the integral in equation 9. A generally
applicable approach is to sample N opponent mixed strategies, Π−i
n , from the posterior
264

On Similarities between Inference in Game Theory and Machine Learning

distribution and use a Monte Carlo approximation of the integral in equation 9, given by
β̃ti ≈

N
1 X i −i
β (Πn ).
N n=1

(11)

To investigate the effect of moderation we also consider an analytic expression for β̃ti
that makes use of two approximations. The first approximates the distribution in equation
10 by a normal distribution
N (µ; Σ)
(12)
with mean vector
−i
µ = α−i
t /ᾱt

(13)



µ1 (1 − µ1 ) . . .
−µ1 µK
1 

..
Σ = −i 

.
ᾱt
−µK µ1
. . . µK (1 − µK )

(14)

and covariance matrix

P
where K = |S −i | and ᾱ−i
=
t
k αtk (Bernardo & Smith, 1994). The second, given in
MacKay (1992) for the case of two action choices, approximates the integral of a sigmoid
a
1

=
g
τ
1 + exp aτ

with respect to a normal distribution, P (a) = N (a; m, σ 2 ) with mean m and variance σ 2 ,
by the modified sigmoid


Z  
1
a
P (a) d a ≈ g
g
κ(σ)m
(15)
τ
τ
where
κ(σ) =



πσ 2
1+
8

− 21

.

(16)

We see from equations 15 and 16 that the effect of moderation is to scale high rewards
down in proportion to the variance (and thus uncertainty of estimated opponent mixed
strategy) and shift the probability value of any action closer to 0.5 (i.e. down from unity or
up from zero to 0.5). At the onset of play when little is known about the opponent, playing
both actions with equal probability is an intuitively reasonable course of action.
To test the general moderated fictitious play of equation 9 with Dirichlet posterior
distributions, we investigate games with varying degrees of risk dominance, since in these
cases the equilibrium selection of strategies is strongly dependent upon the players’ beliefs
about the other players’ probabilities for each action. We compared the probability of
moderated and stochastic fictitious play converging to the payoff dominated solution for
games in which the payoffs are described by the payoff matrix


(1, 1) (0, r)
R=
(17)
(r, 0) (10, 10)
265

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Probability of Convergence of Moderated and Fictitious Play
Fictitious Play
Moderated Play

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
−25

−20

−15

−10

−5

0

Risk Parameter (r)

Figure 1: Probability of convergence (with 95% confidence intervals) of fictitious and moderated play to the payoff-dominant but risk-dominated equilibrium of the game
represented by the payoff matrix shown in equation 17.

where the factor r determined the degree of risk dominance of the action pair 1/1 and was
set to range from r = −1, −2 · · · − 25.
In this game, it is clearly best if both players choose action 2 (since in doing so they both
receive a reward of 10). However for many learning processes (see Fudenberg & Levine, 1999
and Young, 1998, for example) the players see a few initial actions and become convinced
that the opponent is playing a strategy that means choosing action 2 is bad (since the
penalty for playing 2 against 1 is high). We find that by taking uncertainty over strategies
into account at the start, through the use of our moderated fictitious play algorithm, we
are less likely to get stuck playing action 1, and so convergence to the strategy at (10, 10)
is more likely.
For each value of r we ran 500 plays of the game and measured the convergence rates
for moderated and stochastic fictitious play using matching initial conditions. Both algorithms used the same smooth best response function. Specifically, Boltzmann smooth best
responses with temperature parameter τ = 0.1. We present the results in Figure 1. Stochastic fictitious play converges to the (1,1) equilibrium for all games in which action (2,2) is
risk dominated (i.e. for r < −10). As soon as action (2/2) is no longer risk dominated
then stochastic fictitious play does converge. In contrast, moderated play exhibits a much
smoother overall convergence characteristic. Moderated play achieves convergence to action
(2/2) over a much greater range of values of r, though with varying degrees of probability.
Thus for most risk dominated games examined (r = −25 · · · − 10), moderated play is more
likely to converge to the payoff-dominant equilibrium than stochastic fictitious play.
266

On Similarities between Inference in Game Theory and Machine Learning

Thus, by using an insight from machine learning, and specifically, the standard procedure in Bayesian inference of integrating over the distribution of the opponent’s strategies,
rather than taking an empirical average, we have been able to derive an algorithm based on
stochastic fictitious play with a smoother and thus more predictable convergence behaviour.

4. Variational Learning
Having shown in the first part of the paper how insights from machine learning can be used
to derive improved algorithms for fictitious play, we now consider the converse case. More
specifically, we consider a popular machine learning algorithm, the mean field variational
learning algorithm, and show how this can be viewed as learning in a game where all players
receive identical rewards. We proceed to show how insights from game theory (specifically,
relating variational leaning to a Cournot process and fictitious play) can be used to derive
improved algorithms.
We start by reviewing the mean field variational learning algorithm, and first note that
it and other methods are typically used to infer the probability distribution of some latent
(or hidden) variables, based on the evidence provided by another set of observable variables.
Computing these probability distributions, however, requires an integration step which is
frequently intractable (MacKay, 2003). To tackle this problem one can use Markov chain
Monte Carlo methods (Robert & Casella, 1999) to obtain asymptotically optimal results,
or alternatively use approximate analytical methods, such as variational approaches, if
faster algorithms are preferred. Among the variational methods, the mean field variational
approach to distribution estimation (Jordan et al., 1997) has been applied to real world
problems ranging from bioinformatics (Husmeier, Dybowski, & Roberts, 2004) to finite
element analysis (Liu, Besterfield, & Belytschko, 1988), and now here to games.
4.1 Mean Field Variational Method
The typical starting point in variational learning is the distributional form of a model, postulated to underlie the experimental data generating processes (i.e. the generative model).
The distribution will usually be instantiated with some observations, D, and defined over
a set I of latent variables which are indexed by i = 1, · · · I. We denote the domain of the
latent variable S i by Si , and an element by si ∈ Si . Note that, for ease of exposition later
in the text, we re-use and newly define S as we intend to make the connection to the earlier
definition of S as the strategy profile. We often desire the marginal distribution pi ∈ ∆(Si )
of the latent variable i, taken from the set of all marginal distributions ∆(Si ) over S i .
In the absence of any detailed knowledge about dependence or independence of the
variables, we define the joint distribution p ∈ ∆(S) for the set of all distributions over S,
where S = S1 × S2 × · · · SI is the profile domain of the latent variables. For mathematical
convenience we resort to the logarithm of the density
ℓ(S | D, θ) , log (p(S | D, θ))

(18)

and parameterise the distribution p with θ ∈ Θ.
Due to the intractability of integrating equation 18 with respect to S ∈ S, variational
learning methods (Jordan et al., 1997) approach the problem by finding the distribution,
267

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

q ∈ ∆(S), which minimises the criterion
Z
Z
F = · · · q(s)ℓ(s | D, θ) d s + τ H (q)

(19)

where H(·) is the entropy function
H (q) = −

Z

···

Z

q(s) log q(s) d s.

(20)

This is equivalent to minimising
D(q||p) =

Z

···

Z

q(s) log



q(s)τ
p(s | D, θ)



ds

(21)

and highlights the fact that the variational cost function described in equation 19 is a
Kullback-Leibler (KL) divergence between the marginal log-likelihood log (p(D)) and the
negative free energy (Jordan et al., 1997).
Within variational learning, the “mean field” approach makes the assumption that all
latent variables are independent. Thus, the distribution profile, q, for the latent variables
simplifies to
I
Y
q i (si )
(22)
q(s) ,
i=1

qi

∆(Si ).

where
∈
On the basis of the mean field assumption, the optimal distribution of
variable i is the one that minimises the KL divergence in equation 21, assuming that all
other variables −i adopt the distribution q −i ∈ ∆(S−i ), and can be obtained by partial
differentiation of equation 21. The model-free update equation for q i , under these assumptions (Haft, Hofmann, & Tresp, 1999), takes the general form
 Z

Z
1
@i @i
i @i
@i
i i
· · · q (s )ℓ(s , s | D; θ) d s
q (s ) ∝ exp
(23)
τ
where the index set @i denotes the Markov blanket of variable i (i.e. the set of nodes
neighbouring node i). In a 2 player game this set consist of just the opponent, while in a
graphical game the Markov blanket consists of all players affecting player i’s actions (Kearns
et al., 2001; Pearl, 1988).
The variational algorithm thus proceeds by iterating the update equation 23 until the
KL divergence expressed in equation 21 has converged to a stationary value — possibly a
local extremum. In the case of the EM algorithm, one round of updates by equation 23 will
be interlaced with one round of updates of parameter θ. The update equations for θ are
obtained by differentiation of equation 19 with respect to θ.
From a game-theoretic perspective, the log-probability shown in equation 18, with instantiated observations D, can be interpreted as a global reward function
r(S | θ) ≡ log (p(S | D, θ))

(24)

parameterised by θ. The I latent variables then become synonymous with the players, each
player having a strategy space consisting of all possible values of S i and mixed strategy
268

On Similarities between Inference in Game Theory and Machine Learning

q i . Interpreted in terms of players, the task of probabilistic inference is now a partnership
game, played by I players jointly against nature (Grünwald & Dawid, 2004). The total
expected reward
Z
Z
L(θ) ≡ · · · q(s) log (p(s | D, θ)) d s
(25)
given some set of mixed strategies, is simply the expected log-likelihood. In the Bayesian
setting, with priors on θ, the global reward is the full log-posterior distribution and the
equivalent total expected reward is the marginal log-likelihood, or evidence. If p factorises
then it can often be represented as a graphical model. Within a game-theoretic interpretation, graphical models can be seen as players having their own, local, reward functions,
and such graphical games (Kearns et al., 2001) are an active research area which implicitly
makes use of the analogy between game theory and graphical models.
4.2 Fictitious Variational Play
The variational mean field algorithm, described in the previous section, suggests that the
mixed strategies that maximise equation 24 can be determined by iterating over equation 23
and gradually taking the limit τ → 0. This is analogous to a Cournot adjustment process
in the game theory literature, with the modification that smooth best responses are used
in place of best responses2 . However, a well known shortcoming of the Cournot process’
iteration of best response is that it can often fail to converge and, instead, exhibit cyclic
behaviour. Consider, for instance, the partnership game with reward matrix


1 0
0 1



.

If the players commence play by choosing different actions, the Cournot process fails to
converge and the iterated best responses will exhibit cyclic solutions.
Such cyclic behaviour can indeed be observed in the variational mean field algorithm.
Whilst not commonly reported, cycles can also occur in highly connected graphical models
(e.g. Markov Random Fields or Coupled Hidden Markov Models), and when the probability distribution being approximated does not really support the imposed independence
assumption in the mean field approach (i.e. when random variables are strongly instead of
weakly connected). Clearly, especially in the latter case, even random initialisation cannot
avoid the mean field algorithm’s problem of convergence. This phenomenon is described as
“thrashing” by Wolpert et al.’s (2006).
Example: This simple example illustrates our point. Suppose we have two observations,
D = {y1 , y2 } with y1 = 10 and y2 = −10. We know that each observation is drawn from a
two-component mixture of one dimensional normal distributions, with mixing probability
0.5 and both variances set to 1. The means of the normal distributions are chosen from the
2. In the Cournot adjustment process, players use strategies that are a best response to the action the
opponents used in the previous period. In many cases, this process does not converge (Fudenberg &
Levine, 1999).

269

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

set {−10, 10}. Therefore, we have for
ℓ(µ1 , µ2 | D) = log(0.5(φ(d1 − µ1 ) + φ(d1 − µ2 )))
+ log(0.5(φ(d2 − µ1 ) + φ(d2 − µ2 )))
where φ(yi − µj ) for i, j = 1, 2 denotes the density of yi − µj under a standard normal
distribution. Now by symmetry, it should be clear that lsame ≡ l(10, 10) = l(−10, −10) =
log φ(0) + log φ(20) ≈ −201.8379 and ldif f erent ≡ l(10, −10) = l(−10, 10) = 2 log(0.5(φ(0) +
φ(20))) ≈ −3.2242. From a game theory perspective, this means that the partnership game
we are playing is simply given by the 2 × 2 matrix


−201.8379 −3.2242
.
−3.2242 −201.8379
If we choose two q-distributions of equation 23 for each component mean, initialise them
with q i (µi = 10) = 0.9 = 1 − q i (µi = −10) for component index i = {1, 2} (i.e. each
marginal distribution places weight 0.9 on having the mean at 10), and update them simultaneously, both distributions switch virtually all of their mass to the −10 point. This
shouldn’t be a surprise, given that we clearly need to have µ1 6= µ2 . The problem is that
both components of the mean field approximation jump at the same time. At the next step,
the reverse move will happen. Each time things get more extreme, and continuous cycling
occurs.
In the work by Wolpert et al. (2006) it was suggested that thrashing can be avoided by
adjusting the distributions toward the best responses, instead of setting them to be the best
responses. Here, we use the analogy with game theory to rigorously develop this approach,
and prove its convergence. We start by noting that fictitious play does exactly this; it
modifies the Cournot process by adjusting players’ mixed strategies towards the smooth best
response, instead of setting it to be the smooth best response. This suggests a fictitious playlike modification to the standard mean field algorithm, in which best responses computed
at time t are mixed with the current distributions:

i
i
−i
(26)
qt+1
= (1 − λt )qti + λt βM
F qt
wherePλt is a sequence satisfying the
Robins-Monro conditions
(Bishop, 2006) such

P∞usual
−i
2 < ∞, and β i
denotes
the best response
that ∞
λ
=
∞,
lim
λ
=
0,
λ
q
t→∞ t
t
t=1 t
t=1 t
MF
function for distribution i and is given by equation 23. We call this process variational
fictitious play.
The game theory literature proves that stochastic fictitious play in partnership games
is guaranteed to converge when Si is finite (Leslie & Collins, 2005). This allows us to prove
the following result:
Theorem: If each random variable is discrete, the variational fictitious play converges.
Proof: In the case of discrete random variables, equation 26 defines a generalised weakened
fictitious play process for the game where each player receives reward log(p(s | D, θ)). This
270

On Similarities between Inference in Game Theory and Machine Learning

Standard Mean Field Variational Algorithm

Si−1

i

S

E

p(si = 1)

1
0.8
0.6
0.4
0.2
0
1

i

2

3

4

5

6

Iteration

7

8

9

10

9

10

Fictitious Variational Play

(
0.9
p(E i |S i−1 , S i ) =
0.1

S i−1 6= S i
S i−1 = S i

p(si = 1)

1
0.8
0.6
0.4
0.2
0
1

2

(a)

3

4

5

6

Iteration

7

8

(b)

Figure 2: Comparison of the performance of the standard mean field variational algorithm
and our improved algorithm when applied to an exemplar binary hidden Markov
model.

process is known to converge (Leslie & Collins, 2005).
Remark: While an equivalent theorem is not yet available for continuous random variables,
a similar approach may yield suitable results when combined with recent work of Hofbauer
and Sorin (2006).
Example: As an example of the change the fictitious update scheme has on the standard
variational algorithm, consider the binary hidden Markov model shown in Figure 2a. The
model contains two latent nodes, indexed by i and i − 1, which are jointly parent to the
observed variable E i . Both latent variables take values S i−1 , S i ∈ {0, 1}. The model is
specified such that observation is best explained (with probability 0.9; see Figure 2b), if
the two neighbouring states take different values. Further, the joint prior for states i − 1
and i is the uniform distribution. For simplicity we omit the parameter θ, which encodes
observation state probabilities and the prior distribution.
• Consider initialising so that both the distributions q 1 (s1 = 1) and q 2 (s2 = 1) are close
to 1. The result is a periodic flipping of the state probability distributions, q 1 and
q 2 , at every update iteration of the mean field algorithm (top graph in Figure 2b).
In contrast, the fictitious variational play scheme gradually forces the algorithm to
converge (bottom graph in Figure 2b).
271

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

• Initialisation at random reduces the likelihood of the mean field algorithm’s failure to
converge. However, empirically, we could still observe such cyclic solutions in 50% of
all the random starts of the hidden Markov model training procedure. In contrast,
the modified mean field variational algorithm converges reliably in all cases.
4.3 Derivative Action Variational Algorithm
In the previous sections we have shown that, when viewed from a game theoretic perspective, the smooth best response function is equivalent to the mean field update rule
and, consequently, we were able to apply results from game theory to derive an improved
variational learning algorithm. In this section we go further and use this equivalence to
incorporate a recent development in fictitious play, specifically dynamic fictitious play, into
the variational mean field method.
In dynamic fictitious play (Shamma & Arslan, 2005) the standard best response expression, shown in equation 7, is extended to include an additional term



d


(27)
β qt−i + γ q −i  .
dt
| {z }
new
This additional derivative term acts as an “anticipatory” component of the opponent’s play
and is expected to increase the speed of convergence in the partnership game considered
here. Note that, on convergence, the derivative terms become zero and the fixed points are
exactly the same as for the standard variational algorithm.
Based on the correspondence between the best response function in fictitious play and
the model-free update expression of variational learning in section 4.1, we incorporate this
additional differential term into the update equation of our variational learning algorithm to
derive a derivative action variational algorithm (DAVA) that displays improved convergence
properties compared to the standard variational algorithm.
To illustrate this procedure, we consider the case of applying this derivative action
variational algorithm to the problem of performing clustering (i.e. learning and labelling
a mixture distribution in order to best fit an experimental data set). We chose clustering
as it represents a canonical problem, widely reported in the literature of machine learning.
We consider a standard mixture distribution consisting of K one-dimensional Gaussian
distributions, N (d|µk , βk ) for k = 1, . . . , K, given by
p(d|θ) =

K
X

πk N (d|µk , βk )

(28)

k=1

where θ is the set of distribution parameters {µ1 , β1 , π1 , . · · · , µK , , βK , πK }. Here, µk and βk
are the mean and precision of each Gaussian, and πk is their weighting within the mixture.
The usual approach to learning this distribution (Redner & Walker, 1984) is to assume
the existence of an indicator (or labelling) variable, indexed by i and which takes values
S i ∈ {1, · · · K}, for each sample, di , and formulate the complete likelihood
i

i

p(S , µ1 , . . . , µK , β1 , . . . , βK | d ) ∝

K
Y

k=1

272

δ(S i =k)

πk

N (di |µk , βk )δ(S

i =k)

p(θ)

(29)

On Similarities between Inference in Game Theory and Machine Learning

where p(θ) denote the parameter prior distributions. To obtain analytic coupled update
equations for each member of the parameter set, {π1 , µ1 , β1 , · · · , πK , µK , βK }, we use the
model discussed in Uedaa and Ghahramani (2002), which describes the appropriate choice
of the approximating marginal posterior distribution for each parameter. By evaluating
the integral in equation 23, after replacing the generic latent variables S by the model
parameters θ and the indicator variables S, Uedaa and Ghahramani show that a closed
form expression can be derived for the approximating marginal posterior distributions for
all parameters. To compute the approximate marginal posterior mean distribution, for
example, the set of variables S i and S @i in equation 23 become place holders for µk and
{βk , π, S 1 , · · · , S N }, respectively. In other words, to compute the posterior distribution of
µk , the logarithm of equation 23 must be averaged with respect to the distributions q(βk ),
q(π), and q(S i ), i = 1, · · · N .
The computations result in a closed form solution for the marginal posterior for each
element of the parameter set. Thus, the posterior of the means, µk , is a normal distribution
q µk , N (µk |mk , τk )

(30)

with mean mk and precision βk and where
µk =
λ̄ =


ck bk d¯k + τ0 µ0 τk

P

i

λik

λik = q i (S i = k)
P i i
d¯k =
i λk d

τk = ck bk λ̄k + τ0

and µ0 and τ0 are, respectively, the mean and precision of the Gaussian prior for µk .
The posterior for the precisions is a Gamma distribution
q βk , Γ(βk |bk , ck )

(31)

where
1
λ̄k + α0
2
1P i i
bk =
λ (d −µk )2 )+ 12 λ̄k τk−1 +β0
2 i k
where α0 and β0 are, respectively, are the shape and precision parameters of the Gamma
prior for the precisions βk . Finally, the posterior of the mixture weights is a K-dimensional
Dirichlet distribution
q π , Dir(π|κ)
(32)
ck =

where
κ=

P

i

λik +κ0

and κ0 is the parameter of the Dirichlet prior for π. Finally, the distribution of the component labels si has the form



K
bk ck
1
1 Y δ(S i =k) 12
i
2
i
i
β̃k exp −
π̃k
(d − mk ) +
(33)
q (S ) =
ZS i
2
τk
k=1

273

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

where the normalising constant ZS i is computed over the finite set of states of S i . The
values for π̃k and β̃k are computed using the relations
π̃k =Ψ(κk ) − Ψ (

PK

l=1

κl )

β̃k =Ψ(ck ) + log(bk ).
Ψ is the digamma function.
As noted earlier, the update equations can be interpreted as the best response of a
particular parameter given the values of the others. Thus, the additional derivative term
seen in dynamic fictitious play can, in principle, be included into the variational update
equations 30, 31, 32, and 33. If it is desired, however, to obtain closed form solutions for
the modified best response functions, or update equations, the derivative term can only be
incorporated in the discrete distribution, given by equation 33, as follows.
For notational clarity we will only add the anticipatory component to the estimate of
the means, µk , and use this only for the update of the sk . That is, we will consider only the
d µk
q term to equation 33. Furthermore, we will approximate the derivative
inclusion of the dt
d µk
by
the
discrete
difference in the distributions between iterations t and t − 1
q
dt
d µk
µk
.
q ≈ qtµk − qt−1
dt

(34)

It is also possible to implement a smoothed version of the derivative to provide added
robustness against random fluctuations, as was done in the equivalent fictitious play algorithm described by Shamma and Arslan (2005). When we introduce the derivative term
into equation 33 the update equation for the component labels becomes
i

i

q (S ) ∝

K
Y

k=1

1

δ(S i =k) 2
β̃k
π̃k




2
1
bk ck
i
(1 + γ) d − mkt +
exp −
2
τkt


2
bk ck
1
i
+
γ d − mkt−1 +
2
τkt−1

(35)

where the parameters of qtµk are denoted by mkt and τkt . The differential coefficient, γ,
allows us to control the degree by which the differential term contributes toward the overall
update equation.
In order to demonstrate empirically the convergence properties of the derivative action
variational inference algorithm, we compared the algorithm using either update equation 33
or update equation 35 to test data. To control the problem setting, we generated synthetic
test data by drawing data points from a mixture of 3 Gaussian distributions and then
applying a non-linear mapping function3 . This data is shown in Figure 3 along with the
optimal clustering solution.
We then performed 200 runs comparing the standard variational algorithm (implemented
as described in Penny & Roberts, 2002) with the derivative action variational algorithm.
During each run, both algorithms were started from an identically and randomly initialised
3. Equation 35 generalises to two dimensions or higher by replacing the quadratic term (di − mkt )2 with
the inner product (d~i − m
~ kt )T (d~i − m
~ kt ). This assumes vector valued data samples, d~i ∀i, are Gaussian
distributed with means m
~ k and homoscedastic precision βk .

274

On Similarities between Inference in Game Theory and Machine Learning

(1)
(3)

(2)

(a)

(b)

Figure 3: Synthetic data (a) used for empirical study of the convergence properties of the
derivative action variational inference algorithm and the optimal clustering solution (b) showing the centres of each of the three Gaussians in the mixture.

set of model parameters and at each iteration we measured the value of the Kullback-Leibler
divergence of equation 21. The algorithms were terminated after 60 iterations (chosen to be
well above the number of iterations required to achieve a relative change of Kullback-Leibler
divergence of less than 10−5 ). The differential coefficient, γ, in equation 35 was set to a
constant value throughout. For illustrative purposes we chose the following values for this
coefficient: γ = 0.5, 1.0, 1.5, and 2.0.
We first consider the difference in the convergence rate between the standard variational
algorithm and the derivative action variational algorithm. Due to the nature of both algorithms, different initial conditions can lead to different final clustering solutions. In addition,
the difference in the update rule of the two algorithms means that, although we start both
algorithms with identical initial conditions, they do not necessarily result in identical final
clustering solutions. Thus, we analyse the difference in the Kullback-Leibler divergence at
each iteration, for just those runs that did in fact produce identical clustering solutions at
termination. In Figure 4 we compare these algorithms for the case when γ = 0.5. As can be
clearly seen, the DAVA converges everywhere more quickly than the standard algorithm.
Our experiments also indicate that the choice of the differential coefficient, γ, has a
significant effect on the convergence behaviour of DAVA. This is seen in Figure 5 for the
case when γ = 1.0. Compared to the results in Figure 4, the magnitude of the difference in Kullback-Leibler divergence is much larger, indicating a substantial increase in the
convergence rate.
By comparing the times at which each algorithm reached equilibrium (indicated by a
relative change of Kullback-Leibler divergence of less than 10−5 ), for all four values of γ,
275

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Relative Performance of Derivative
Action Variational Algorithm, γ =0.50
0.2

10th Percentile
50th Percentile
90th Percentile

Difference in KL divergence

0.15

0.1

0.05

0

−0.05
0

10

20

30

40

50

60

Iteration

Figure 4: For comparison of the standard and the derivative action variational algorithm
(γ = 0.5), the Kullback-Leibler divergence values, obtained from both algorithms
and at every iteration, were compared. Shown here are the estimates of the 10th ,
50th (the median) and 90th percentiles of the differences between the standard
KL and the derivative action KL, i.e. KLstandard (t) − KLDAV A (t) at iteration
t. A positive value suggests that the current solution of the standard algorithm
is worse than than of the derivative action algorithm. A zero value implies that
the solutions found by both algorithms are identical. At initialisation the KL
differences are zero as both algorithms have identical initialisation conditions.

the role of the differential coefficient in improving convergence rates becomes apparent. In
particular, Table 1 shows the relative convergence rate improvement shown by the derivative action variational algorithm, compared to the standard variational algorithm. The
results indicate that the median improvement can be as much as 28% when γ = 2.0. In
addition, as the value of the differential coefficient increases, the variance in the convergence
rate increases, and thus, there is a widening gap between the best and worst performance
improvements.
However, this increasing variance does not imply that DAVA is converging to inferior
solutions. This can be seen in Figure 6 which shows an analysis of the quality of the solutions
reached by each algorithm, for all 200 runs (not just those where both algorithms converged
to the same clustering solution). At moderate values of γ (γ ≤ 1.5) the derivative component
can assist in finding a better solution. This is indicated by the proportion of positive KL
divergence differences (in Figure 6). Positive values imply that the standard algorithm often
finds worse solutions compared to DAVA at the particular setting of γ. Increasing the value
276

On Similarities between Inference in Game Theory and Machine Learning

Relative Performance of Derivative
Action Variational Algorithm, γ =1.00
0.2

10th Percentile
50th Percentile
90th Percentile

Difference in KL divergence

0.15

0.1

0.05

0

−0.05
0

10

20

30

40

50

60

Iteration

Figure 5: Estimates of the 10th , 50th (the median) and 90th percentiles of the differences
in the Kullback-Leibler divergence values, at each iteration, after 200 runs of the
standard and the derivative action variational algorithm (γ = 1.0).

of the differential coefficient further increases the variance in the clustering solutions that
are generated by the algorithm.
Having evaluated the performance of the DAVA algorithm on a synthetic data set, and
investigated its performance over a range of values for the derivative coefficient, γ, we now
consider a more realistic experimental setting and apply the algorithm to medical magnetic
resonance data (see Figure 7). This data consists of a 100×100 pixel region of a slice through
a tumour patient’s brain. Data was collected using both T2 and proton density (PD) spin
sequences (shown in Figure 7a), which are used directly to form a two-dimensional feature
space.
A 10 component Gaussian mixture model is fitted to this data space, and as before, we
use the DAVA algorithm derived earlier to learn and label the most appropriate mixture
distribution. Following the synthetic data experiments, the DAVA derivative coefficient
γ was set to 1.0. This being the best compromise between speed and robustness of the
algorithm. We let the algorithms run for 100 iterations and measured the KL divergence
at each iteration to monitor convergence. Figure 7b shows the resulting segmentation from
the DAVA algorithm.
On this dataset, the algorithm converged to K = 5 classes; identical to the Markov Chain
Monte Carlo clustering reported in the work of Roberts, Holmes, and Denison (2001). The
segmentation clearly shows spatial structure which was not incorporated into the segmen277

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Difference in final KL divergences between Standard
and Derivative Action Variational Algorithm
1
γ=0.5
γ=1.0
γ=1.5
γ=2.0

0.9
0.8

Probability

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

<0

0

>0

Sign of final differences (Standard−Derivative Action)

Figure 6: Relative difference in Kullback-Leibler divergence values at equilibrium for the
standard and the derivative action variational algorithms with derivative coefficients: γ = 0.5, 1.0, 1.5, and 2.0.

γ
0.5
1.0
1.5
2.0

10th percentile
0%
0%
-2%
8%

50th percentile
9%
19 %
22%
28%

90th percentile
21 %
39 %
46 %
53 %

Table 1: Relative convergence rate improvement of the derivation action variational algorithm over the standard variational algorithms.

tation process a priori, and the algorithm was approximately 1.5 times faster than the
standard segmentation algorithm4 .
In summary, by adding a derivative term to the variational learning algorithm, we have
produced an algorithm that, in empirical studies, shows improved convergence rates compared to the standard variational algorithm. Indeed, this convergence rate improvement has
been achieved by applying the additional derivative response term to the mean components
of the mixture model parameters only, and, thus, we believe that further improvement is
possible if other parameters are treated similarly.
4. To determine the factor of speedup between the two algorithms we averaged the KL divergence values
of every iteration and for each segmentation algorithm. Based on the mean KL divergence curve the
iteration number at which the algorithm converged could be calculated. This is the iteration at which
the relative change in KL divergence between 2 successive iterations was less than 10−4 . The ratio of
the convergence points of the DAVA and the standard algorithm then produced our indicator of speed.

278

On Similarities between Inference in Game Theory and Machine Learning

Slice of a T2 magnetic resonance image through a tumour in a patient’s brain.

(a)
The proton density image corresponding to the T2 MR image.

(b)
Figure 7: The results of the segmentation using the DAVA algorithm with γ = 1.0. For
this data the segmentation was obtained at approximately half the time it took
the standard segmentation algorithm.

5. Conclusions and Future Work
In this work we have shown an equivalence between game-theoretical learning in partnership games and the variational learning algorithm common in machine learning. In this
comparison, probabilistic inference using the mean field variational learning is seen to be
a Cournot adjustment process in a partnership game. Likewise, the smooth best response
function used by players is a plug-in single layer classifier with a sigmoid activation function.
By exploiting this analysis and the insights derived from it, we have been able to show
that insights from one area may be applied to the other to derive improved fictitious play
and variational learning algorithms. In empirical comparisons, these algorithms showed
improved convergence properties compared to the standard counterparts.
279

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Player 1’s Estimate of Opponent Strategy
1
C
D

0.8
0.6
0.4
0.2
0
0

50

Iterations

100

150

Expected Rewards
5

Player 1
Player 2

4
3
2
1
0
0

50

Iterations

100

150

Figure 8: Using dynamic logistic regression (DLR) for adaptive estimation of opponent
mixed strategies. In a game of repeated prisoner’s dilemma one player changes
strategies from “always co-operate” to “always defect” midway through the game.
Using DLR as model of learning, the other player adapts to the changes based on
the uncertainty between the predicted and observed opponent strategies.

We believe these initial results are particularly exciting. Thus, whilst there still remains
further analysis to be performed (specifically, we would like to prove convergence of moderated fictitious play and compare the performance of our variational algorithms on large real
world data sets), the work clearly shows the value of studying the intersection of machine
learning and game theory.
One conclusion from this work is that almost any machine learning algorithm can be put
to use as a model of players learning from each other. Consider, for instance adaptive classification algorithms which are capable of adapting to changes in the learner’s environment.
Most game-theoretic methods are tuned toward achieving equilibrium and estimating, for
example, Nash mixed strategy profiles. While this is desirable in stationary environments,
such algorithms fail to adapt to non-stationary environments, and are consequently of little practical use in dynamic real-world applications. The research presented in this paper
strongly suggests that adaptive classifiers might prove useful.
As a proof of concept, we have implemented the dynamic logistic regression (DLR),
presented in Penny and Roberts (1999), as a model of play for one player (player 1) in a
game of repeated prisoner’s dilemma. The other player (player 2) was set to play an “always
co-operate” strategy for the first 75 rounds of play. For the second set of 75 rounds, player 2
was set to play a “always defect” strategy. The task of player 1 is to detect this change in the
opponent’s behaviour and compute the best responses according to the updated estimate
of player 2’s strategy. The estimates of player 1 of player 2’s strategy for the entire game
are shown in Figure 8, together with both players’ expected rewards. The DLR adaptively
280

On Similarities between Inference in Game Theory and Machine Learning

changes the one-step ahead predicted opponent strategy on the basis of the uncertainty
that results from incorporation of the most recently observed opponent action (see Penny
& Roberts, 1999 for details; the observations considered in this work map directly to the
observed actions made by the opponent). The decision about which action to play follows
then as usual (i.e. compute the best response according to the updated estimate using
equation 9).
There are two things we would like to point out. First, as implemented here, the input
required for DLR is simply the recently observed opponent action, and the decision made
by the DLR is the action drawn from the best response function. However, DLR also allows
for a vector of inputs, and as a consequence, players can be made to respond not just to
the opponent’s actions, but also to other context or application specific variables. Second,
the DLR estimation described in Penny and Roberts (1999) is fully Bayesian. Missing data
can be naturally embedded in the Bayesian estimation and this is demonstrated in Lowne,
Haw, and Roberts (2006). Mapping this fact back to the use of DLR as model of play
implies that players can keep track of the opponent’s behaviour without the need to follow
or observe their every move. Missing observations, for instance, could result in an increased
uncertainty in the opponent’s predicted play and, within, a reversal toward the appropriate
best response (as it might have existed at the onset of play).
Similar ideas of using dynamic, instead of static, estimators of opponent strategy have
recently been presented in the work of Smyrnakis and Leslie (2008). Extending further
the use of machine learning techniques to allow dynamic estimation of both strategies and
environmental parameters will allow game theoretical learning to become more generally
applicable in real-world scenarios.

Acknowledgments
The authors would like the thank the reviewers, whose suggestions led to significant improvements in both the content and clarity of the final paper. This research was undertaken
as part of the ARGUS II DARP and ALADDIN projects. ARGUS II DARP (Defence
and Aerospace Research Partnership) is a collaborative project involving BAE SYSTEMS,
QinetiQ, Rolls-Royce, Oxford University and Southampton University, and is funded by
the industrial partners together with the EPSRC, MoD and DTI. ALADDIN (Autonomous
Learning Agents for Decentralised Data and Information Systems) is jointly funded by a
BAE Systems and EPSRC (Engineering and Physical Science Research Council) strategic
partnership (EP/C548051/1).

References
Bernardo, J. M., & Smith, A. F. M. (1994). Bayesian Theory. John Wiley and Sons.
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Oxford University Press,
Oxford.
Demiriz, A., Bennett, K. P., & Shawe-Taylor, J. (2002). Linear programming boosting via
column generation. Machine Learning, 46 (1–3), 225–254.
281

Rezek, Leslie, Reece, Roberts, Rogers, Dash & Jennings

Freund, Y., & Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning
and an application to boosting. Journal of Computer and System Sciences, 55 (1),
119–139.
Fudenberg, D., & Levine, D. K. (1999). The Theory of Learning in Games. MIT Press.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2000). Bayesian Data Analysis.
Chapman & Hall/CRC.
Greenwald, A., & Littman, M. L. (2007). Introduction to the special issue on learning and
computational game theory. Machine Learning, 67 (1-2), 3–6.
Grünwald, P. D., & Dawid, A. P. (2004). Game Theory, Maximum Entropy, Minimum
Discrepancy and Robust Bayesian Decision Theory. Annals of Statistics, 32, 1367–
1433.
Haft, M., Hofmann, R., & Tresp, V. (1999). Model-Independent Mean Field Theory as a
Local Method for Approximate Propagation of Information. Computation in Neural
Systems, 10, 93–105.
Hofbauer, J., & Hopkins, E. (2005). Learning in perturbed asymmetric games. Games and
Economic Behavior, 52, 133–157.
Hofbauer, J., & Sorin, S. (2006). Best response dynamics for continuous zero-sum games.
Discrete and Continuous Dynamical Systems, B6, 215–224.
Husmeier, D., Dybowski, R., & Roberts, S. J. (Eds.). (2004). Probabilistic Modeling in
Bioinformatics and Medical Informatics. Advanced Information and Knowledge Processing. Springer Verlag.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1997). An Introduction
to Variational Methods for Graphical Models. In Jordan, M. I. (Ed.), Learning in
Graphical Models. Kluwer Academic Press.
Kalai, E., & Lehrer, E. (1993). Rational Learning Leads to Nash Equilibrium. Econometrica,
61 (5), 1019–1045.
Kearns, M., Littman, M. L., & Singh, S. (2001). Graphical Models for Game Theory. In
Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence,
pp. 253–260.
Lambert, T., Epelman, M. A., & Smith, R. L. (2005). A Fictitious Play Approach to
Large-Scale Optimization. Operations Research, 53 (3), 477–489.
Lee, C. F., & Wolpert, D. H. (2004). Product Distribution Theory for Control of Multi-Agent
Systems. In AAMAS ’04: Proceedings of the Third International Joint Conference on
Autonomous Agents and Multiagent Systems, pp. 522–529, New York, USA.
Leslie, D. S., & Collins, E. J. (2005). Generalised weakened fictitious play. Games and
Economic Behavior, 56 (2), 285–298.
Liu, W., Besterfield, G., & Belytschko, T. (1988). Variational approach to probabilistic
finite elements. Journal of Engineering Mechanics, 114 (12), 2115–2133.
Lowne, D., Haw, C., & Roberts, S. (2006). An adaptive, sparse-feedback EEG classifier
for self-paced BCI. In Proceedings of the Third International Workshop on BrainComputer Interfaces, Graz, Austria.
282

On Similarities between Inference in Game Theory and Machine Learning

MacKay, D. J. C. (1992). The Evidence Framework Applied to Classification Networks.
Neural Computation, 4 (5), 720–736.
MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms. Cambridge University Press.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann Publishers.
Penny, W., & Roberts, S. (1999). Dynamic logistic regression. In Proceedings of the International Joint Conference on Neural Networks (IJCNN’99), Vol. 3, pp. 1562 – 1567.
Penny, W., & Roberts, S. (2002). Bayesian Multivariate Autoregressive Models with Structured Priors. IEE Proceedings on Vision, Signal and Image Processing, 149 (1), 33–41.
Redner, R. A., & Walker, H. F. (1984). Mixture Densities, Maximum Likelihood and the
EM Algorithm. SIAM Review, 26 (2), 195–239.
Ripley, B. (2000). Pattern Recognition and Neural Networks. Cambridge University Press.
Robert, C. P., & Casella, G. (1999). Monte Carlo Statistical Methods. Springer-Verlag: New
York.
Roberts, S., Holmes, C., & Denison, D. (2001). Minimum Entropy data partitioning using
Reversible Jump Markov Chain Monte Carlo. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 23 (8), 909–915.
Shamma, J. S., & Arslan, G. (2005). Dynamic Fictitious Play, Dynamic Gradient Play
and Distributed Convergence to Nash Equilibria. IEEE Transactions on Automatic
Control, 50 (3), 312–327.
Shoham, Y., Powers, R., & Grenager, T. (2007). If multi-agent learning is the answer, what
is the question. Artificial Intelligence, 171 (7), 365–377.
Smyrnakis, M., & Leslie, D. (2008). Stochastic Fictitious Play using particle Filters to
update the beliefs of opponents strategies. In Proceedings of the First International
Workshop on Optimisation in Multi-Agent Systems (OPTMAS) at the Seventh International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2008).
Uedaa, N., & Ghahramani, Z. (2002). Bayesian model search for mixture models based on
optimizing variational bounds. Neural Networks, 15, 1223–1241.
van der Genugten, B. (2000). A Weakened Form of Fictitious Play in Two-Person Zero-Sum
Games. International Game Theory Review, 2 (4), 307–328.
Wolpert, D. H. (2004). Information Theory - The Bridge Connecting Bounded Rational
Game Theory and Statistical Physics. arXiv.org:cond-mat/0402508.
Wolpert, D., Strauss, C., & Rajnarayan, D. (2006). Advances in Distributed Optimization
Using Probability Collectives. Advances in Complex Systems, 9 (4), 383–436.
Young, H. P. (1998). Individual Strategy and Social Structure: An Evolutionary Theory of
Institutions. Princeton University Press.

283

Journal of Artificial Intelligence Research 33 (2008) 109-147

Submitted 11/07; published 9/08

Networks of Influence Diagrams: A Formalism for
Representing Agents’ Beliefs and Decision-Making Processes
Ya’akov Gal

MIT Computer Science and Artificial Intelligence Laboratory
Harvard School of Engineering and Applied Sciences

Avi Pfeffer

gal@csail.mit.edu

avi@eecs.harvard.edu

Harvard School of Engineering and Applied Sciences

Abstract
This paper presents Networks of Influence Diagrams (NID), a compact, natural and
highly expressive language for reasoning about agents’ beliefs and decision-making processes. NIDs are graphical structures in which agents’ mental models are represented as
nodes in a network; a mental model for an agent may itself use descriptions of the mental
models of other agents. NIDs are demonstrated by examples, showing how they can be used
to describe conflicting and cyclic belief structures, and certain forms of bounded rationality. In an opponent modeling domain, NIDs were able to outperform other computational
agents whose strategies were not known in advance. NIDs are equivalent in representation
to Bayesian games but they are more compact and structured than this formalism. In particular, the equilibrium definition for NIDs makes an explicit distinction between agents’
optimal strategies, and how they actually behave in reality.

1. Introduction
In recent years, decision theory and game theory have had a profound impact on the design
of intelligent systems. Decision theory provides a mathematical language for single-agent
decision-making under uncertainty, whereas game theory extends this language to the multiagent case. On a fundamental level, both approaches provide a definition of what it means
to build an intelligent agent, by equating intelligence with utility maximization. Meanwhile, graphical languages such as Bayesian networks (Pearl, 1988) have received much
attention in AI because they allow for a compact and natural representation of uncertainty
in many domains that exhibit structure. These formalisms often lead to significant savings
in representation and in inference time (Dechter, 1999; Cowell, Lauritzen, & Spiegelhater,
2005).
Recently, a wide variety of representations and algorithms have augmented graphical
languages to be able to represent and reason about agents’ decision-making processes. For
the single-agent case, influence diagrams (Howard & Matheson, 1984) are able to represent
and to solve an agent’s decision making problem using the principles of decision theory.
This representation has been extended to the multi-agent case, in which decision problems
are solved within a game-theoretic framework (Koller & Milch, 2001; Kearns, Littman, &
Singh, 2001).
The focus in AI so far has been on the classical, normative approach to decision and
game theory. In the classical approach, a game specifies the actions that are available to
the agents, as well as their utilities that are associated with each possible set of agents’
c
!2008
AI Access Foundation. All rights reserved.

Gal & Pfeffer

actions. The game is then analyzed to determine rational strategies for each of the agents.
Fundamental to this approach are the assumptions that the structure of the game, including
agents’ utilities and their actions, is known to all of the agents, that agents’ beliefs about
the game are consistent with each other and correct, that all agents reason about the game
in the same way, and that all agents are rational in that they choose the strategy that
maximizes their expected utility given their beliefs.
As systems involving multiple, autonomous agents become ubiquitous, they are increasingly deployed in open environments comprising human decision makers and computer
agents that are designed by or represent different individuals or organizations. Examples of
such systems include on-line auctions, and patient care-delivery systems (MacKie-Mason,
Osepayshivili, Reeves, & Wellman, 2004; Arunachalam & Sadeh, 2005). These settings are
challenging because no assumptions can be made about the decision-making strategies of
participants in open environments. Agents may be uncertain about the structure of the
game or about the beliefs of other agents about the structure of the game; they may use
heuristics to make decisions or they may deviate from their optimal strategies (Camerer,
2003; Gal & Pfeffer, 2003b; Rajarshi, Hanson, Kephart, & Tesauro, 2001).
To succeed in such environments, agents need to make a clear distinction between their
own decision-making models, the models others may be using to make decisions, and the
extent to which agents deviate from these models when they actually make their decisions.
This paper contributes a language, called Networks of Influence Diagrams (NID), that makes
explicit the different mental models agents may use to make their decisions. NIDs provide
for a clear and compact representation with which to reason about agents’ beliefs and
their decision-making processes. It allows multiple possible mental models of deliberation
for agents, with uncertainty over which models agents are using. It is recursive, so that
the mental model for an agent may itself contain models of the mental models of other
agents, with associated uncertainty. In addition, NIDs allow agents’ beliefs to form cyclic
structures, of the form, “I believe that you believe that I believe,...”, and this cycle is
explicitly represented in the language. NIDs can also describe agents’ conflicting beliefs
about each other. For example, one can describe a scenario in which two agents disagree
about the beliefs or behavior of a third agent.
NIDs are a graphical language whose building blocks are Multi Agent Influence Diagrams
(MAID) (Koller & Milch, 2001). Each mental model in a NID is represented by a MAID,
and the models are connected in a (possibly cyclic) graph. Any NID can be converted to
an equivalent MAID that will represent the subjective beliefs of each agent in the game.
We provide an equilibrium definition for NIDs that combines the normative aspects of
decision-making (what agents should do) with the descriptive aspects of decision-making
(what agents are expected to do). The equilibrium makes an explicit distinction between
two types of strategies: Optimal strategies represent agents’ best course of action given
their beliefs over others. Descriptive strategies represent how agents may deviate from their
optimal strategy. In the classical approach to game theory, the normative aspect (what
agents should do) and the descriptive aspect (what analysts or other agents expect them
to do), have coincided. Identification of these two aspects makes sense when an agent can
do no better than optimize its decisions relative to its own model of the world. However,
in open environments, it is important to consider the possibility that an agent is deviating
from its rational strategy with respect to its model.
110

Networks of Influence Diagrams

NIDs share a relationship with the Bayesian game formalism, commonly used to model
uncertainty over agents’ payoffs in economics (Harsanyi, 1967). In this formalism, there
is a type for each possible payoff function an agent may be using. Although NIDs are
representationally equivalent to Bayesian games, we argue that they are a more compact,
succinct and natural representation. Any Bayesian game can be converted to a NID in
linear time. Any NID can be converted to a Bayesian game, but the size of the Bayesian
game may be exponential in the size of the NID.
This paper is a revised and expanded version of previous work (Gal & Pfeffer, 2003a,
2003b, 2004), and is organized as follows: Section 2 presents the syntax of the NID language,
and shows how they build on MAIDs in order to express the structure that holds between
agents’ beliefs. Section 3 presents the semantics of NIDs in terms of MAIDs, and provides
an equilibrium definition for NIDs. Section 4 provides a series of examples illustrating
the representational benefits of NIDs. It shows how agents can construct belief hierarchies
of each other’s decision-making in order to represent agents’ conflicting or incorrect belief
structures, cyclic belief structures and opponent modeling. It also shows how certain forms
of bounded rationality can be modeled by making a distinction between agents’ models of
deliberation and the way they behave in reality. Section 5 demonstrates how NIDs can model
“I believe that you believe” type reasoning in practice. It describes a NID that was able
to outperform the top programs that were submitted to a competition for automatic rockpaper-scissors players, whose strategy was not known in advance. Section 6 compares NIDs
to several existing formalisms for describing uncertainty over decision-making processes. It
provides a linear time algorithm for converting Bayesian games to NIDs. Finally, Section 7
concludes and presents future work.

2. NID Syntax
The building blocks of NIDs are Bayesian networks (Pearl, 1988), and Multi Agent Influence
Diagrams (Koller & Milch, 2001). A Bayesian network is a directed acyclic graph in which
each node represents a random variable. An edge between two nodes X1 and X2 implies
that X1 has a direct influence on the value of X2 . Let Pa(Xi ) represent the set of parent
nodes for Xi in the network. Each node Xi contains a conditional probability distribution
(CPD) over its domain for any value of its parents, denoted P (Xi | Pa(Xi )). The topology
of the network describes the conditional independence relationships that hold in the domain
— every node in the network is conditionally independent of its non-descendants given its
parent nodes. A Bayesian network defines a complete joint probability distribution over its
random variables that can be decomposed as the product of the conditional probabilities of
each node given its parent nodes. Formally,
P (X1 , . . . , Xn ) =

n
!
i=1

P (Xi | Pa(Xi ))

We illustrate Bayesian networks through the following example.
Example 2.1. Consider two baseball team managers Alice and Bob whose teams are playing the late innings of a game. Alice, whose team is hitting, can attempt to advance a runner
by instructing him to “steal” a base while the next pitch is being delivered. A successful
111

Gal & Pfeffer

steal will result in a benefit to the hitting team and a loss to the pitching team, or it may
result in the runner being “thrown out”, incurring a large cost to the hitting team and a
benefit to the pitching team. Bob, whose team is pitching, can instruct his team to throw
a “pitch out”, thereby increasing the probability that a stealing runner will be thrown out.
However, throwing a pitch out incurs a cost to the pitching team. The decisions whether
to steal and pitch out are taken simultaneously by both team managers. Suppose that the
game is not tied, that is either Alice’s or Bob’s team is leading in score, and that the identity
of the leading team is known to Alice and Bob when they make their decision.
Suppose that Alice and Bob are using pre-specified strategies to make their decisions
described as follows: when Alice is leading, she instructs a steal with probability 0.75,
and Bob calls a pitch out with probability 0.90; when Alice is not leading, she instructs
a steal with probability 0.65, and Bob calls a pitch out with probability 0.50. There are
six random variables in this domain: Steal and PitchOut represent the decisions for Alice
and Bob; ThrownOut represents whether the runner was thrown out; Leader represents the
identity of the leading team; Alice and Bob represent the utility functions for Alice and Bob.
Figure 1 shows a Bayesian network for this scenario.
Leader

Steal

PitchOut
ThrownOut

Alice

Bob

Figure 1: Bayesian network for Baseball Scenario (Example 2.1)
The CPD associated with each node in the network represents a probability distribution over its domain for any value of its parents. The CPDs for nodes Leader, Steal,
PitchOut, and ThrownOut in this Bayesian network are shown in Table 1. For example, the
CPD for ThrownOut, shown in Table 1d, represents the conditional probability distribution
P (ThrownOut | Steal, PitchOut). According to the CPD, when Alice instructs a runner to
steal a base there is an 80% chance to get thrown out when Bob calls a pitch out and a 60%
chance to get thrown out when Bob remains idle. The nodes Alice and Bob have deterministic CPDs, assigning a utility for each agent for any joint value of the parent nodes Leader,
Steal, PitchOut and ThrownOut. The utility for Alice is shown in Table 2. The utility for
Bob is symmetric and assigns the negative value assigned by Alice’s utility for the same
value of the parent nodes. For example, when Alice is leading, and she instructs a runner
to steal a base, Bob instructs a pitch out, and the runner is thrown out, then Alice incurs
a utility of −60, while Bob incurs a utility of 60.1
1. Note that when Alice does not instruct to steal base, the runner cannot be thrown out, and the utility
for both agents is not defined for this case.

112

Networks of Influence Diagrams

Leader
alice bob none
0.4
0.3
0.3
(a) node Leader

Leader
alice
bob

PitchOut
true f alse
0.90 0.10
0.50 0.50

(c) node PitchOut

Leader
alice
bob

Steal
true f alse
0.75 0.25
0.65 0.35

(b) node Steal

Steal
true
true
f alse
f alse

PitchOut
true
f alse
true
f alse

ThrownOut
true f alse
0.8
0.2
0.6
0.4
0
1
0
1

(d) node ThrownOut

Table 1: Conditional Probability Tables (CPDs) for Bayesian network for Baseball
Scenario (Example 2.1)

Leader
alice
alice
alice
alice
alice
alice
alice
alice
bob
bob
bob
bob
bob
bob
bob
bob

Steal
true
true
true
true
f alse
f alse
f alse
f alse
true
true
true
true
f alse
f alse
f alse
f alse

PitchOut
true
true
f alse
f alse
true
true
f alse
f alse
true
true
f alse
f alse
true
true
f alse
f alse

ThrownOut
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse
true
f alse

Alice
−60
110
−80
110
—
10
—
0
−90
110
−100
110
—
20
—
0

Table 2: Alice’s utility (Example 2.1) (Bob’s utility is symmetric, and assigns negative value to
Alice’s value).

113

Gal & Pfeffer

2.1 Multi-agent Influence Diagrams
While Bayesian networks can be used to specify that agents play specific strategies, they
do not capture the fact that agents are free to choose their own strategies, and they cannot
be analyzed to compute the optimal strategies for agents. Multi-agent Influence Diagrams
(MAID), address these issues by extending Bayesian networks to strategic situations, where
agents must choose the values of their decisions to maximize their own utilities, contingent
on the fact that other agents are choosing the values of their decisions to maximize their
own utilities. A MAID consists of a directed graph with three types of nodes: Chance
nodes, drawn as ovals, represent choices of nature, as in Bayesian networks. Decision nodes,
drawn as rectangles, represent choices made by agents. Utility nodes, drawn as diamonds,
represent agents’ utility functions. Each decision and utility node in a MAID is associated
with a particular agent. There are two kinds of edges in a MAID: Edges leading to chance
and utility nodes represent probabilistic dependence, in the same manner as edges in a
Bayesian network. Edges leading into decision nodes represent information that is available
to the agents at the time the decision is made. The domain of a decision node represents
the choices that are available to the agent making the decision. The parents of decision
nodes are called informational parents. There is a total ordering over each agent’s decisions,
such that earlier decisions and their informational parents are always informational parents
of later decisions. This assumption is known as perfect recall or no forgetting. The CPD
of a chance node specifies a probability distribution over its domain for each value of the
parent nodes, as in Bayesian networks. The CPD of a utility node represents a deterministic
function that assigns a probability of 1 to the utility incurred by the agent for any value of
the parent nodes.
In a MAID, a strategy for decision node Di maps any value of the informational parents,
denoted as pai , to a choice for Di . Let Ci be the domain of Di . The choice for the decision
can be any value in Ci . A pure strategy for Di maps each value of the informational
parents to an action ci ∈ Ci . A mixed strategy for Di maps each value of the informational
parents to a distribution over Ci . Agent α is free to choose any mixed strategy for Di
when it makes that decision. A strategy profile for a set of decisions in a MAID consists of
strategies specifying a complete plan of action for all decisions in the set.
The MAID for Example 2.1 is shown in Figure 2. The decision nodes Steal and PitchOut
represent Alice’s and Bob’s decisions, and the nodes Alice and Bob represent their utilities.
The CPDs for the chance node Leader and ThrownOut are as described in Tables 1a and
1d.
A MAID definition does not specify strategies for its decisions. These need to be computed or assigned by some process. Once a strategy exists for a decision, the relevant
decision node in the MAID can be converted to a chance node that follows the strategy.
This chance node will have the same domain and parent nodes as the domain and informational parents for the decision node in the MAID. The CPD for the chance node will
equal the strategy for the decision. We then say that the chance node in the Bayesian
network implements the strategy in the MAID. A Bayesian network represents a complete
strategy profile for the MAID if each strategy for a decision in the MAID is implemented
by a relevant chance node in the Bayesian network. We then say that the Bayesian network
implements that strategy profile. Let σ represent the strategy profile that implements all
114

Networks of Influence Diagrams

Leader

PitchOut

Steal
ThrownOut

Alice

Bob

Figure 2: MAID for Baseball Scenario (Example 2.1)
decisions in the MAID. The distribution defined by this Bayesian network is denoted by
P σ.
An agent’s utility function is specified as the aggregate of its individual utilities; it is the
sum of all of the utilities incurred by the agent in all of the utility nodes that are associated
with the agent.
Definition 2.2. Let E be a set of observed nodes in the MAID representing evidence that
is available to α and let σ be a strategy profile for all decisions. Let U(α) be the set of all
utility nodes belonging to α. The expected utility for α given σ and E is defined as
"
"
"
U σ (α | E) =
E σ [U | E] =
P σ (u | E) · u
U ∈U(α)

U ∈U(α) u∈Dom(U )

Solving a MAID requires computing an optimal strategy profile for all of the decisions,
as specified by the Nash equilibrium for the MAID, defined as follows.
Definition 2.3. A strategy profile σ for all decisions in the MAID is a Nash equilibrium if
each strategy component σi for decision Di belonging to agent α in the MAID is one that
maximizes the utility achieved by the agent, given that the strategy for other decisions is
σ−i .
σi ∈ argmax U $τi ,σ−i % (α)

(1)

τi ∈∆Si

These equilibrium strategies specify what each agent should do at each decision given the
available information at the decision. When the MAID contains several sequential decisions,
the no-forgetting assumption implies that these decisions can be taken sequentially by the
agent, and that all previous decisions are available as observations when the agent reasons
about its future decisions.
Any MAID has at least one Nash equilibrium. Exact and approximate algorithms have
been proposed for solving MAIDs efficiently, in a way that utilizes the structure of the
network (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller, Meggido, & von Stengel,
115

Gal & Pfeffer

1996; Blum, Shelton, & Koller, 2006). Exact algorithms for solving MAIDs decompose
the MAID graph into subsets of interrelated sub-games, and then proceed to find a set of
equilibria in these sub-games that together constitute a global equilibrium for the entire
game. In the case that there are multiple Nash equilibria, these algorithms will select one
of them, arbitrarily. The MAID in Figure 2 has a single Nash equilibrium, which we can
obtain by solving the MAID: When Alice is leading, she instructs her runner to steal a base
with probability 0.2, and remain idle with probability 0.8, while Bob calls a pitch out with
probability 0.3, and remains idle with probability 0.7. When Bob is leading, Alice instructs
a steal with probability 0.8, and Bob calls a pitch out with probability 0.5.
The Bayesian network that implements the Nash equilibrium strategy profile for the
MAID can be queried to predict the likelihood of interesting events. For example, we can
query the network in Figure 2 and find that the probability that the stealer will get thrown
out, given that agents’ strategies follow the Nash equilibrium strategy profile, is 0.57.
Any MAID can be converted to an extensive form game — a decision tree in which
each vertex is associated with a particular agent or with nature. Splits in the tree represent
an assignment of values to chance and decision nodes in the MAID; leaves of the tree
represent the end of the decision-making process, and are labeled with the utilities incurred
by the agents given the decisions and chance node values that are instantiated along the
edges in the path leading to the leaf. Agents’ imperfect information regarding the actions
of others are represented by the set of vertices they cannot tell apart when they make a
particular decision. This set is referred to as an information set. Let D be a decision in
the MAID belonging to agent α. There is a one-to-one correspondence between values of
the informational parents of D in the MAID and the information sets for α at the vertices
representing its move for decision D.
2.2 Networks of Influence Diagrams
To motivate NIDs, consider the following extension to Example 2.1.
Example 2.4. Suppose there are experts who will influence whether or not a team should
steal or pitch out. There is social pressure on the managers to follow the advice of the
experts, because if the managers’ decision turns out to be wrong they can assign blame to
the experts. The experts suggest that Alice should call a steal, and Bob should call a pitch
out. This advice is common knowledge between the managers. Bob may be uncertain as to
whether Alice will in fact follow the experts and steal, or whether she will ignore them and
play a best-response with respect to her beliefs about Bob. To quantify, Bob believes that
with probability 0.7, Alice will follow the experts, while with probability 0.3, Alice will play
best-response. Alice’s beliefs about Bob are symmetric to Bob’s beliefs about Alice: With
probability 0.7 Alice believes Bob will follow the experts and call a pitch out, and with
probability 0.3 Alice believes that Bob will play the best-response strategy with respect
to his beliefs about Alice. The probability distribution for other variables in this example
remains as shown in Table 1.
NIDs build on top of MAIDs to explicitly represent this structure. A Network of Influence Diagrams (NID) is a directed, possibly cyclic graph, in which each node is a MAID.
To avoid confusion with the internal nodes of each MAID, we will call the nodes of a NID
blocks. Let D be a decision belonging to agent α in block K, and let β be any agent. (In
116

Networks of Influence Diagrams

particular, β may be agent α itself.) We introduce a new type of node, denoted Mod[β, D]
with values that range over each block L in the NID. When Mod[β, D] takes value L, we
say that agent β in block K is modeling agent α as using block L to make decision D.
This means that β believes that α may be using the strategy computed in block L to make
decision D. For the duration of this paper, we will refer to a node Mod[β, D] as a “Mod
node” when agent β and decision D are clear from context.
A Mod node is a chance node just like any other; it may influence, or be influenced
by other nodes of K. It is required to be a parent of the decision D but it is not an
informational parent of the decision. This is because an agent’s strategy for D does not
specify what to do for each value of the Mod node. Every decision D will have a Mod[β, D]
node for each agent that makes a decision in block K, including agent α itself that owns
the decision. If the CPD of Mod[β, D] assigns positive probability to some block L, then
we require that D exists in block L either as a decision node or as a chance node. If D is a
chance node in L, this means that β believes that agent α is playing like an automaton in
L, using a fixed, possibly mixed strategy for D; if D is a decision node in L, this means that
β believes α is analyzing block L to determine the course of action for D. For presentation
purposes, we also add an edge K → L to the NID, labeled {β, D}.
Leader

Mod[Alice, Steal]

Mod[Bob, PitchOut]

Mod[Bob, Steal]

Mod[Alice, PitchOut]

Steal

ThrownOut

PitchOut

Alice

Bob

(a) Top-level Block

Leader

Leader

Top-level

Steal

Bob,STEAL Alice,PITCHOUT

PitchOut
S

(b) block S

(c) block P

P

(d) Baseball NID

Figure 3: Baseball Scenario (Example 2.1)
We can represent Example 2.4 in the NID described in Figure 3. There are three blocks
in this NID. The Top-level block, shown in Figure 3a, corresponds to an interaction between
Alice and Bob in which they are free to choose whether to steal base or call a pitch out,
respectively. This block is identical to the MAID of Figure 2, except that each decision node
includes the Mod nodes for all of the agents. Block S, presented in Figure 3b, corresponds to
a situation where Alice follows the expert recommendation and instructs her player to steal.
117

Gal & Pfeffer

Mod[Bob, Steal]
Top-level
S
0.3
0.7
(a) node
Mod[Bob, Steal]

Mod[Alice, PitchOut]
Top-level
P
0.3
0.7
(b) node
Mod[Alice, PitchOut]

Mod[Bob, PitchOut]
Top-level
1
(c) node
Mod[Bob, PitchOut]

Mod[Alice, Steal]
Top-level
1
(d) node
Mod[Alice, Steal]

Table 3: CPDs for Top-level block of NID for Baseball Scenario (Example 2.1)

In this block, the Steal decision is replaced with a chance node, which assigns probability
1 to true for any value of the informational parent Leader. Similarly, block P, presented in
Figure 3c, corresponds to a situation where Bob instructs his team to pitch out. In this
block, the PitchOut decision is replaced with a chance node, which assigns probability 1 to
true for any value of the informational parent Leader.
The root of the NID is the Top-level block, which in this example corresponds to reality.
The Mod nodes in the Top-level block capture agents’ beliefs over their decision-making
processes. The node Mod[Bob, Steal] represents Bob’s belief about which block Alice is
using to make her decision Steal. Its CPD assigns probability 0.3 to the Top-level block,
and 0.7 to block S. Similarly, the node Mod[Alice, PitchOut] represents Alice’s beliefs about
which block Bob is using to make the decision PitchOut. Its CPD assigns probability 0.3 to
the Top-level block, and 0.7 to block P. These are shown in Table 3.
An important aspect of NIDs is that they allow agents to express uncertainty about the
block they themselves are using to make their own decisions. The node Mod[Alice, Steal]
in the Top-level block represents Alice’s beliefs about which block Alice herself is using to
make her decision Steal. In our example, the CPD of this node assigns probability 1 to
the Top-level. Similarly, the node Mod[Bob, PitchOut] represents Bob’s beliefs about which
block he is using to make his decision PitchOut, and assigns probability 1 to the Top-level
block. Thus, in this example, both Bob and Alice are uncertain about which block the other
agent is using to make a decision, but not about which block they themselves are using.
However, we could also envision a situation in which an agent is unsure about its own
decision-making. We say that if Mod[β, D] at block K equals some block L $= K, and
β owns decision D, then agent β is modeling itself as using block L to make decision
D. In Section 3.2 we will show how this allows to capture interesting forms of bounded
rational behavior. We do impose the requirement that there exists no cycle in which each
edge includes a label {α, D}. In other words, there is no cycle in which the same agent is
modeling itself at each edge. Such a cycle is called a self-loop. This is because the MAID
representation for a NID with a self-loop will include a cycle between the nodes representing
the agent’s beliefs about itself at each block of the NID.
In future examples, we will use the following convention: If there exists a Mod[α, D] node
at block K (regardless of whether α owns the decision) and the CPD of Mod[α, D] assigns
probability 1 to block K, we will omit the node Mod[α, D] from the block description.
In the Top-level block of Figure 3a, this means that both nodes Mod[Alice, Steal] and
Mod[Bob, PitchOut], currently appearing as dashed ovals, will be omitted.
118

Networks of Influence Diagrams

3. NID Semantics
In this section we provide semantics for NIDs in terms of MAIDs. We first show how a
NID can be converted to a MAID. We then define a NID equilibrium in terms of a Nash
equilibrium of the constructed MAID.
3.1 Conversion to MAIDs
The following process converts each block K in the NID to a MAID fragment OK , and then
connects them to form a MAID representation of the NID. The key construct in this process
is the use of a chance node DαK in the MAID to represent the beliefs of agent α regarding
the action that is chosen for decision D at block K. The value of Dα depends on the block
used by α to model decision D, as determined by the value of the Mod[α, D] node.
1. For each block K in the NID, we create a MAID OK . Any chance or utility node N
in block K that is a descendant of a decision node in K is replicated in OK , once for
each agent α, and denoted NαK . If N is not a descendant of a decision node in K, it
is copied to OK and denoted N K . In this case, we set NαK = N K for any agent α.
2. If P is a parent of N in K, then PαK will be made a parent of NαK in OK . The CPD
of NαK in OK will be equal to the CPD of N in K.
3. For each decision D in K, we create a decision node BR[D]K in OK , representing the
optimal action for α for this decision. If N is a chance or decision node which is an
informational parent of D in K, and D belongs to agent α, then NαK will be made an
informational parent of BR[D]K in OK .
4. We create a chance node DαK in OK for each agent α. We make Mod[α, D]K a parent
of DαK . If decision D belongs to agent α, then we make BR[D]K a parent of DαK . If
decision D belongs to agent β $= α, then we make DβK a parent of DαK .
5. We assemble all the MAID fragments OK into a single MAID O as follows: We add
an edge DαL → DβK where L $= K if L is assigned positive probability by Mod[β, D]K ,
and α owns decision D. Note that β may be any agent, including α itself.
6. We set the CPD of DαK to be a multiplexer. If α owns D then the CPD of DαK assigns
probability 1 to BR[D]K when Mod[α, D]K equals K, and assigns probability 1 to
DαL when Mod[α, D]K equals L $= K. If β $= α owns D then the CPD of DαK assigns
probability 1 to DβK when Mod[α, D]K equals K, and assigns probability 1 to DβL
when Mod[α, D]K equals L $= K.
To explain, Step 1 of this process creates a MAID fragment OK for each NID block. All
nodes that are ancestors of decision nodes — representing events that occur prior to the
decisions — are copied to OK . However, events that occur after decisions are taken may
depend on the actions for those decisions. Every agent in the NID may have its own beliefs
about these actions and the events that follow them, regardless of whether that agent owns
the decision. Therefore, all of the descendant nodes of decisions are duplicated for each agent
in OK . Step 2 ensures that if any two nodes are connected in the original block K, then
119

Gal & Pfeffer

the nodes representing agents’ beliefs in OK are also connected. Step 3 creates a decision
node in OK for each decision node in block K belonging to agent α. The informational
parents for the decision in OK are those nodes that represent the beliefs of α about its
informational parents in K. Step 4 creates a separate chance node in OK for each agent α
that represents its belief about each of the decisions in K. If α owns the decision, this node
depends on the decision node belonging to α. Otherwise, this node depends on the beliefs
of α regarding the action of agent β that owns the decision. In the case that α models β as
using a different block to make the decisions, Step 5 connects between the MAID fragments
of each block. Step 6 determines the CPDs for the nodes representing agents’ beliefs about
each other’s decisions. The CPD ensures that the block that is used to model a decision is
determined by the value of the Mod node. The MAID that is obtained as a result of this
process is a complete description of agents’ beliefs over each other’s decisions.
We demonstrate this process by converting the NID of Example 2.4 to its MAID representation, shown in Figure 4. First, MAID fragments for the three blocks Top-level, P, and
S are created. The node Leader appearing in blocks Top-level, P, and S is not a descendant of any decision. Following Step 1, it is created once in each of the MAID fragments,
giving the nodes LeaderT L , LeaderP and LeaderS . Similarly, the node Steal in block S and
the node PitchOut in block P are created once in each MAID fragment, giving the nodes
StealS and PitchOutP . Also in Step 1, the nodes Mod[Alice, Steal]T L , Mod[Bob, Steal]T L ,
Mod[Alice, PitchOut]T L and Mod[Bob, PitchOut]T L are added to the MAID fragment for the
Top-level block.
Step 3 adds the decision nodes BRT L [Steal] and BRT L [PitchOut] to the MAID fragment
L
L
L
, PitchOutTAlice
, StealTAlice
for the Top-level block. Step 4 adds the chance nodes PitchOutTBob
TL
and StealBob to the MAID fragment for the Top-level block. These nodes represent agents’
beliefs in this block about their own decisions or the decisions of other agents. For exL
represents Bob’s beliefs about its decision whether to pitch out, while
ample, PitchOutTBob
TL
PitchOutAlice represents Alice’s beliefs about Bob’s beliefs about this decision. Also followL
L
L
and StealTAlice
→ StealTBob
are added to the
ing Step 4, edges BRT L [PitchOut] → PitchOutTBob
MAID fragment for the Top-level block. These represent Bob’s beliefs over its own decision
L
L
→ StealTBob
is added to the MAID fragment to represent
at the block. An edge StealTAlice
Bob’s beliefs over Alice’s decision at the Top-level block. There are also nodes representing
Alice’s beliefs about her and Bob’s decisions in this block.
L
L
and PitchOutP → PitchoutTAlice
are added to the
In Step 5, edges StealS → StealTBob
MAID fragment for the Top-level block. This is to allow Bob to reason about Alice’s decision
in block S, and for Alice to reason about Bob’s decision in block P. This action unifies the
L
MAID fragments into a single MAID. The parents of StealTBob
are Mod[Bob, Steal]T L , StealS
L
and StealTAlice
. Its CPD is a multiplexer node that determines Bob’s prediction about Alice’s
action: If Mod[Bob, Steal]T L equals S, then Bob believes Alice to be using block S, in which
her action is to follow the experts and play strategy StealS . If Mod[Bob, Steal]T L equals
the Top-level block, then Bob believes Alice to be using the Top-level block, in which
Alice’s action is to respond to her beliefs about Bob. The situation is similar for Alice’s
L
decision StealTAlice
and the node Mod[Alice, Steal]T L with the following exception: When
T
L
Mod[Alice, Steal] equals the Top-level block, then Alice’s action follows her decision node
BRT L [Steal].
In the Appendix, we prove the following theorem.
120

Networks of Influence Diagrams

Theorem 3.1. Converting a NID into a MAID will not introduce a cycle in the resulting
MAID.
Alice

TL

BobTLBob

Bob

ModTL[Bob, Steal]

ModTL[Bob, PitchOut]

ThrownOutTLBob

StealTLBob

PitchOutTLBob

S
Lead
Bob
Leader

TL
BR [PitchOut]
StealS
P
Lead
Bob
Leader

TL
LeadBob
Leader
BRTL[Steal]

PitchOutP

Mod

TL

[Alice, PitchOut]

ModTL[Alice, Steal]

StealTLAlice

PitchOutTLAlice

ThrownOut

Alice

TL

TL
Alice

Bob

Alice

TL
Alice

Figure 4: MAID representation for the NID of Example 2.4
As this conversion process implies, NIDs and MAIDs are equivalent in their expressive
power. However, NIDs provide several advantages over MAIDs. A NID block structure
makes explicit agents’ different beliefs about decisions, chance variables and utilities in the
world. It is a mental model of the way agents reason about decisions in the block. MAIDs
do not distinguish between the real world and agents’ mental models of the world or of each
other, whereas NIDs have a separate block for each mental model. Further, in the MAID,
nodes simply represent chance, decision or utilities, and are not inherently interpreted in
terms of beliefs. A DαK node in a MAID representation for a NID does not inherently
represent agent α’s beliefs about how decision D is made in mental model K, and the
ModK for agent α does not inherently represent which mental model is used to make a
decision. Indeed, there are no mental models defined in a MAID. In addition, there is no
relationship in a MAID between descendants of decisions NαK and NβK , so there is no sense
in which they represent the possibly different beliefs of agents α and β about N .
121

Gal & Pfeffer

Together with the NID construction process described above, a NID is a blueprint
for constructing a MAID that describes agents’ mental models. Without the NID, this
process becomes inherently difficult. Furthermore, the constructed MAID may be large and
unwieldy compared to a NID block. Even for the simple NID of Example 2.4, the MAID of
Figure 4 is complicated and hard to understand.
3.2 Equilibrium Conditions
In Section 2.1, we defined pure and mixed strategies for decisions in MAIDs. In NIDs, we
associate the strategies for decisions with the blocks in which they appear. A pure strategy
for a decision D in a NID block K is a mapping from the informational parents of D to
an action in the domain of D. Similarly, a mixed strategy for D is a mapping from the
informational parents of D to a distribution over the domain of D. A strategy profile for a
NID is a set of strategies for all decisions at all blocks in the NID.
Traditionally, an equilibrium for a game is defined in terms of best response strategies.
A Nash equilibrium is a strategy profile in which each agent is doing the best it possibly can,
given the strategies of the other agents. Classical game theory predicts that all agents will
play a best response. NIDs, on the other hand, allow us to describe situations in which an
agent deviates from its best response by playing according to some other decision-making
process. We would therefore like an equilibrium to specify not only what the agents should
do, but also to predict what they actually do, which may be different.
A NID equilibrium includes two types of strategies. The first, called a best response
strategy, describes what the agents should do, given their beliefs about the decision-making
processes of other agents. The second, called an actually played strategy, describes what
agents will actually do according to the model described by the NID. These two strategies
are mutually dependent. The best response strategy for a decision in a block takes into
account the agent’s beliefs about the actually played strategies of all the other decisions.
The actually played strategy for a decision in a block is a mixture of the best response for
the decision in the block, and the actually played strategies for the decision in other blocks.
Definition 3.2. Let N be a NID and let M be the MAID representation for N. Let σ be an
equilibrium for M. Let D be a node belonging to agent α in block K of N. Let the parents
of D be Pa. By the construction of the MAID representation detailed in Section 3.1, the
K
parents of BR[D]K in M are PaK
α and the domains of Pa and Paα are the same. Let
K
σBR[D]K (pa) denote the mixed strategy assigned by σ for BR[D] when PaK
α equals pa.
K
The best response strategy for D in K, denoted θD (pa), defines a function from values of
Pa to distributions over D that satisfy
K
θD
(pa) ≡ σBR[D]K (pa)

In other words, the best response strategy is the same as the MAID equilibrium when
the corresponding parents take on the same values.
Definition 3.3. Let P σ denote the distribution that is defined by the Bayesian network
that implements σ. The actually played strategy for decision D in K that is owned by
agent α, denoted φK
D (pa), specifies a function from values of Pa to distributions over D
that satisfy
σ
K
φK
D (pa) ≡ P (Dα | pa)
122

Networks of Influence Diagrams

Note here, that DαK is conditioned on the informational parents of decision D rather than its
own parents. This node represents the beliefs of α about decision K. Therefore, the actually
played strategy for D in K represents α’s belief about D in K, given the informational
parents of D.
Definition 3.4. Let σ be a MAID equilibrium. The NID equilibrium corresponding to σ
consists of two strategy profiles θ and φ, such that for every decision D in every block K,
K is the best response strategy for D in K, and φK is the actually played strategy for D
θD
D
in K.
For example, consider the constructed MAID for our baseball example in Figure 4. The
best response strategies in the NID equilibrium specify strategies for the nodes Steal and
PitchOut in the Top-level block that belong to Alice and Bob respectively. For an equilibrium σ of the MAID, the best response strategy for Steal in the Top-level block is the
strategy specified by σ for BRT L [Steal]. Similarily, the best response strategy for Pitchout
in the Top-level block is the strategy specified by σ for BRT L [Pitchout]. The actually played
strategy for Steal in the Top-level is equal to the conditional probability distribution over
L
given the informational parent LeaderT L . Similarly, the actually played strategy
StealTAlice
L
for Pitchout is equal to the conditional probability distribution over PitchoutTBob
given the
TL
informational parent Leader . Solving this MAID yields the following unique equilibrium:
In the NID Top-level block, the CPD for nodes Mod[Alice, Steal] and Mod[Bob, Pitchout]
assigns probability 1 to the Top-level block, so the actually played and best response strategies for Bob and Alice are equal and specified as follows: If Alice is leading, then Alice steals
base with probability 0.56 and Bob pitches out with probability 0.47. If Bob is leading,
then Alice never steals base and Bob never pitches out. It turns out that because the experts may instruct Bob to call a pitch out, Alice is considerably less likely to steal base,
as compared to her equilibrium strategy for the MAID of Example 2.1, where none of the
managers considered the possibility that the other was being advised by experts. The case
is similar for Bob.
A natural consequence of this definition is that the problem of computing NID equilibria
reduces to that of computing MAID equilibria. Solving the NID requires to convert it to its
MAID representation and solving the MAID using exact or approximate solution algorithms.
The size of the MAID is bounded by the size of a block times the number of blocks times
the number of agents. The structure of the NID can then be exploited by a MAID solution
algorithm (Koller & Milch, 2001; Vickrey & Koller, 2002; Koller et al., 1996; Blum et al.,
2006).

4. Examples
In this section, we provide a series of examples demonstrating the benefits of NIDs for
describing and representing uncertainty over decision-making processes in a wide variety of
domains.
4.1 Irrational Agents
Since the challenge to the notion of perfect rationality as the foundation of economic systems presented by Simon (1955), the theory of bounded rationality has grown in different
123

Gal & Pfeffer

directions. From an economic point of view, bounded rationality dictates a complete deviation from the utility maximizing paradigm, in which concepts such as “optimization”
and “objective functions” are replaced with “satisficing” and “heuristics” (Gigerenzer &
Selten, 2001). These concepts have recently been formalized by Rubinstein (1998). From
a traditional AI perspective, an agent exhibits bounded rationality if its program is a solution to the constrained optimization problem brought about by limitations of architecture
or computational resources (Russell & Wefald, 1991). NIDs serve to complement these
two prevailing perspectives by allowing to control the extent to which agents are behaving
irrationally with respect to their model.
Irrationality is captured in our framework by the distinction between best response and
actually played strategies. Rational agents always play a best response with respect to
their models. For rational agents, there is no distinction between the normative behavior
prescribed for each agent in each NID block, and the descriptive prediction of how the agent
actually would play when using that block. In this case, the best response and actually
played strategies of the agents are equal. However, in open systems, or when people are
involved, we may need to model agents whose behavior differs from their best response
strategy. In other words, their best response strategies and actually played strategies are
different. We can capture agent α behaving (partially) irrationally about its decision Dα
in block K by setting the CPD of Mod[α, Dα ] to assign positive probability to some block
L $= K.
There is a natural way to express this distinction in NIDs through the use of the Mod
node. If Dα is a decision associated with agent α, we can use Mod[α, Dα ] to describe which
block α actually uses to make the decision Dα . In block K, if Mod[α, Dα ] is equal to K with
probability 1, then it means that within K, α is making the decision according to its beliefs
in block K, meaning that α will be rational; it will play a best response to the strategies
of other agents, given its beliefs. If, however, Mod[α, Dα ] assigns positive probability to
some block L other than K, it means that there is some probability that α will not play
a best response to its beliefs in K, but rather play a strategy according to some other
block L. In this case, we say α self-models at block K. The introduction of actually played
strategies into the equilibrium definition represents another advantage of NIDs over MAIDs,
in that they explicitly represent strategies for agents that may deviate from their optimal
strategies.
In some cases, making a decision may lead an agent to behave irrationally by viewing
the future in a considerably more positive light than is objectively likely. For example, a
person undergoing treatment for a disease may believe that the treatment stands a better
chance of success than scientifically plausible. In the psychological literature, this effect
is referred to as motivational bias or positive illusion (Bazerman, 2001). As the following
example shows, NIDs can represent agents’ motivational biases in a compelling way, by
making Mod nodes depend on the outcome of decision nodes.
Example 4.1. Consider the case of a toothpaste company whose executives are faced
with two sequential decisions: whether to place an advertisement in a magazine for their
leading brand, and whether to increase production of the brand. Based on past analysis,
the executives know that without advertising, the probability of high sales for the brand in
the next quarter will be 0.5. Placing the advertisement costs money, but the probability
of high sales will rise to 0.7. Increasing production of the brand will contribute to profit
124

Networks of Influence Diagrams

if sales are high, but will hurt profit if sales are low due to the high cost of storage space.
Suppose now that the company executives wish to consider the possibility of motivational
bias, in which placing the advertisement will inflate their beliefs about sales to be high in
the next quarter to probability 0.9. This may lead the company to increase the production
of the brand when it is not warranted by the market and consequently, suffer losses. The
company executives wish to compute their best possible strategy for their two decisions
given the fact that they attribute a motivational bias.
A NID describing this situation is shown in Figure 5c. The Top-level block in Figure 5a
shows the situation from the point of view of reality. It includes two decisions, whether
to advertise (Advertise) and whether to increase the supply of the brand (Increase). The
node Sales represents the amount of sales for the brand after the decision of whether to
advertise, and the node Profit represents the profit for the company, which depends on
the nodes Advertise, Increase and Sales. The CPD of Sales in the Top-level block assigns
probability 0.7 to high if Advertise is true and 0.5 to high if Advertise is f alse, as described
in Table 4a. The utility values for node Profit are shown in Table 4.1. For example, when
the company advertises the toothpaste, increases its supply, and sales are high, it receives
a reward of 70; when the company advertises the toothpaste, does not increase its supply,
and sales are low, it receives a reward of −40. Block Bias, described in Figure 5b, represents
the company’s biased model. Here, the decision to advertise is replaced by an automaton
chance node that assigns probability 1 to Advertise = true. The CPD of Sales in block Bias
assigns probability 0.9 to high if Advertise is true and 0.5 to high if Advertise is f alse, as
described in Table 4b. In the Top-level block, we have the following:
1. The node Mod[Company, Advertise] assigns probability 1 to the Top-level block.
2. The decision node Advertise is a parent of the node Mod[Company, Increase].
3. The node Mod[Company, Increase] assigns probability 1 to block Bias when Advertise
is true, and assigns probability 0 to block Bias when Advertise is f alse.
Intuitively, Step 1 captures the company’s beliefs that it is not biased before it makes the
decision to advertise. Step 2 allows the company’s uncertainty about whether it is biased
to depend on the decision to advertise. Note that this example shows when it is necessary
for a decision node to depend on an agent’s beliefs about a past decision. Step 3 captures
the company’s beliefs that it may use block Bias to make its decision whether to increase
supply, in which it is over confident about high sales.
Solving this NID results in the following unique equilibrium: In block Bias, the company’s actually played and best response strategy is to increase supply, because this is its
optimal action when it advertises and sales are high. In block Top-level, we have the following: If the company chooses not to advertise, it will behave rationally, and its best response
and actually played strategy will be not to increase supply; if the company chooses to advertise, its actually played strategy will be to use block Bias in which it increases supply,
and its best response strategy will be not to increase supply. Now, the expected utility for
the company in the Top-level block is higher when it chooses not to advertise. Therefore,
its best response strategies for both decisions are not to advertise nor to increase supply.
Interestingly, if the company was never biased, it can be shown using backwards induction
125

Gal & Pfeffer

that its optimal action for the first decision is to advertise. Thus, by reasoning about its
own possible irrational behavior for the second decision, the company revised its strategy
for the first decision.
Mod[Company, Advertise]

Advertise
Sales

Advertise
Mod[Company, Increase]

Sales

Increase

Prot

Increase

Prot

(a) Block Top-level

(b) Block Bias

Top-level

Company, INCREASE

Bias

(c) NID

Figure 5: Motivational Bias Scenario (Example 4.1)

Advertise
true
f alse

Sales
low high
0.3
0.7
0.5
0.5

(a) node Sales (Top-level
Block)

Advertise
true
f alse

Sales
low high
0.1
0.9
0.5
0.5

(b) node Sales (Bias Block)

Table 4: CPDs for Top-level block of Motivational Bias NID (Example 4.1)

Example 4.2. Consider the following extension to Example 2.4. Suppose that there are
now two successive pitches, and on each pitch the managers have an option to steal or pitch
out. If Bob pitches out on the first pitch, his utility for pitching out on the second pitch
(regardless of Alice’s action) decreases by 20 units because he has forfeited two pitches.
Bob believes that with probability 0.3, he will succumb to social pressure during the second
pitch and call a pitch out. Bob would like to reason about this possibility when making the
decision for the first pitch.
126

Networks of Influence Diagrams

Advertise
true
true
true
true
f alse
f alse
f alse
f alse

Increase
true
true
f alse
f alse
true
true
f alse
f alse

Sales
high
low
high
low
high
low
high
low

Profit
70
−70
50
−40
80
−60
60
−30

Table 5: Company’s utility (node Profit) for Top-level block of Motivational Bias NID
(Example 4.1)
In this example, each manager is faced with a sequential decision problem: whether to
steal or pitch out in the first and second pitch. The strategy for the second pitch is relevant
to the strategy for the first pitch for each agent. Now, each of the managers, if they were
rational, could use backward induction to compute optimal strategies for the first pitch, by
working backwards from the second pitch. However, this is only a valid procedure if the
managers behave rationally on the second pitch. In the example above, Bob knows that he
will be under strong pressure to pitch out on the second pitch and he wishes to take this
possibility into account, while making his decision for the first pitch.
Mod[Bob, PitchOut2 ]
Top-level
L

0.7
0.3

Table 6: CPD for Mod[Bob, PitchOut2 ] node in Top-level block of Irrational Agent Scenario
(Example 4.2)
We can model this situation in a NID as follows. The Top-level block of the NID is shown
in Figure 6a. Here, the decision nodes Steal1 and PitchOut1 represent the decisions for Alice
and Bob in the first pitch, and the nodes Steal2 and Pitchout2 represent the decisions for
Alice and Bob in the second pitch. The nodes Leader, Steal1 , PitchOut1 and ThrownOut1
are all informational parents of the decision nodes Steal2 and PitchOut2 . For expository
convenience, we have not included the edges leading from node Leader to the utility nodes
in the block. Block L, shown in Figure 6b, describes a model for the second pitch in which
Bob is succumbing to social pressure and pitches out, regardless of who is leading. This is
represented by having the block include a chance node PitchOut2 which equals true with
probability 1 for each value of Leader. The node Mod[Bob, PitchOut2 ] will assign probability
0.3 to block L, and 0.7 probability to the Top-level block, as shown in Table 4.1. The node
Mod[Bob, PitchOut2 ] is not displayed in the Top-level block. By our convention, this implies
that its CPD assigns probability 1 to the Top-level block, in which Bob is reasoning about
the possibility of behaving irrationally with respect to the second pitch. In this way, we
have captured the fact that Bob may behave irrationally with respect to the second pitch,
and that he is reasoning about this possibility when making the decision for the first pitch.
127

Gal & Pfeffer

Leader

Steal1

Bob1

ThrownOut1
PitchOut1

Steal2

Alice1
Bob2

Mod[Bob, PitchOut2]

ThrownOut2

Leader
PitchOut2

Alice2

PitchOut2

(a) Block Top-level
Top level

Bob, PITCHOUT2

L

(c) Irrational NID

Figure 6: Irrational Agent Scenario (Example 4.2)

128

(b) Block L

Networks of Influence Diagrams

There is a unique equilibrium for this NID. Both agents behave rationally for their first
decision so their actually played and best response strategies are equal, and specified as
follows: Alice steals a base with probability 0.49 if she is leading, and never steals a base
if Bob is leading. Bob pitches out with probability 0.38 if Alice is leading and pitches out
with probability 0.51 if Bob is leading. In the second pitch, Alice behaves rationally, and
her best response and actually played strategy are as follows: steal base with probability
0.42 if Alice is leading and never steal base if Bob is leading. Bob may behave irrationally
in the second pitch: His best response strategy is to pitch out with probability 0.2 if Alice
is leading, and pitch out with probability 0.52 if Bob is leading; his actually played strategy
is to pitch out with probability 0.58 if Alice is leading, and with probability 0.71 if Bob is
leading. Note that because Bob is reasoning about his possible irrational behavior in the
second pitch, he is less likely to pitch out in the first pitch as compared to the case in which
Bob is completely rational (Example 2.4).
4.2 Conflicting Beliefs
In traditional game theory, agents’ beliefs are assumed to be consistent with a common prior
distribution, meaning that the beliefs of agents about each other’s knowledge is expressed
as a posterior probability distribution resulting from conditioning a common prior on each
agent’s information state. One consequence of this assumption is that agents’ beliefs can
differ only if they observe different information (Aumann & Brandenburger, 1995). This
result led to theoretic work that attempted to relax the common prior assumption. Myerson
(1991) showed that a game with inconsistent belief structure that is finite can be converted
to a new game with consistent belief structures by constructing utility functions that are
equivalent to the original game in a way that they both assign the same expected utility
to the agents. However, this new game will include beliefs and utility functions that are
fundamentally different to the original game exhibiting the inconsistent belief structure. For
a summary of the economic and philosophical ramifications of relaxing the common prior
assumption, see the work of Morris (1995) and Bonanno and Nehring (1999).
Once we have a language that allows us to talk about different mental models that
agents have about the world, and different beliefs that they have about each other and
about the structure of the game, it is natural to relax the common prior assumption within
NIDs while preserving the original structure of the game.
Example 4.3. Consider the following extension to the baseball scenario of Example 2.1.
The probability that the runner is thrown out depends not only on the decisions of both
managers, but also on the speed of the runner. Suppose a fast runner will be thrown out
with 0.4 probability when Bob calls a pitch out, and with 0.2 probability when Bob does
not call a pitch out. A slow runner will be thrown out with 0.8 probability when Bob calls
a pitch out, and with 0.6 probability when Bob does not call a pitch out.
Now, Bob believes the runner to be slow, but is unsure about Alice’s beliefs regarding
the speed of the runner. With probability 0.8, Bob believes that Alice thinks that the
stealer is fast, and with probability 0.2 Bob believes that Alice thinks that the stealer is
slow. Assume that the distributions for other variables in this example are as described in
Table 1.
129

Gal & Pfeffer

In this example, Bob is uncertain whether Alice’s beliefs about the speed of the runner
conflict with his own. NIDs allow to express this in a natural fashion by having two blocks
that describe the same decision-making process, but differ in the CPD that they assign
to the speed of the runner. Through the use of the Mod node, NIDs can specify agents’
conflicting beliefs about which of the two blocks is used by Alice to make her decision,
according to Bob’s beliefs. The NID and blocks for this scenario are presented in Figure 7.

Mod[Bob, Steal]

Lead

Leader

Speed

Speed

Steal

Steal

PitchOut

Alice

PitchOut
ThrownOut

ThrownOut

Alice

Bob

(a) Top-level Block

Bob

(b) Block L

Top level
Bob,STEAL

L

(c) Conflicting Beliefs NID

Figure 7: Conflicting Beliefs Scenario (Example 4.3)
In the Top-level block, shown in Figure 7a, Bob and Alice decide whether to pitch out or
to steal base, respectively. This block is identical in structure to the Top-level block of the
previous example, but it has an additional node Speed that is a parent of node ThrownOut,
representing the fact that the speed of the runner affects the probability that the runner is
thrown out.
The Top-level corresponds to Bob’s model, in which the runner is slow. The CPD of
the node Speed assigns probability 1 to slow in this block, as shown in Table 7a. Block
L, shown in Figure 7b, represents an identical decision-making process as in the Top-level
block, except that the CPD of Speed is different: it assigns probability 1 to f ast, as shown
in Table 7b. The complete NID is shown in Figure 7c. Bob’s uncertainty in the Toplevel block over Alice’s decision-making process is represented by the node Mod[Bob, Steal],
whose CPD is shown in Table 7c. With probability 0.8, Alice is assumed to be using block
L, in which the speed of the runner is fast. With probability 0.2, Alice is assumed to
be using the Top-level block, in which the speed of the runner is slow. Note that in the
130

Networks of Influence Diagrams

Speed
f ast slow
0
1

Speed
f ast slow
1
0

(a) node Speed
(b) node Speed
(block Top-level) (block L)

Mod[Bob, Steal]
Top-level
L
0.2
0.8
(c) node
Mod[Bob, Steal] (block
Top-level)

Table 7: CPDs for nodes in Conflicting Beliefs NID (Example 4.3)
Top-level block, the nodes Mod[Alice, Steal], Mod[Alice, PitchOut] and Mod[Bob, PitchOut]
are not displayed. By the convention introduced earlier, all these nodes assign probability
1 to the Top-level block and have been omitted from the Top-level block of Figure 7a.
Interestingly, this implies that Alice knows the runner to be slow, even though Bob believes
that Alice believes the runner is fast. When solving this NID, we get a unique equilibrium.
Both agents are rational, so their best response and actually played strategies are equal,
and specified as follows: In block L, the runner is fast, so Alice always steals base, and Bob
always calls a pitch out. In the Top-level block, Bob believes that Alice uses block L with
high probability, in which she seals a base. In the Top-level block the speed of the runner is
slow and will likely be thrown out. Therefore, Bob does not pitch out in order to maximize
its utility given its beliefs about Alice. In turn, Alice does not steal base at the Top-level
block because the speed of the runner is slow at this block.
4.3 Collusion and Alliances
In a situation where an agent is modeling multiple agents, it may be important to know
whether those agents are working together in some fashion. In such situations, the models
of how the other agents make their decisions may be correlated, due to possible collusion.
Example 4.4. A voting game involves 3 agents Alice, Bob, and Carol, who are voting one
of them to be chairperson of a committee. Alice is the incumbent, and will be chairperson
if the vote ends in a draw. Each agent would like itself to be chairperson, and receives
utility 2 in that case. Alice also receives a utility of 1 if she votes for the winner but loses
the election, because she wants to look good. Bob and Carol, meanwhile, dislike Alice and
receive utility -1 if Alice wins.
It is in the best interests of agents Bob and Carol to coordinate, and both vote for the
same person. If Bob and Carol do indeed coordinate, it is in Alice’s best interest to vote for
the person they vote for. However, if Bob and Carol mis-coordinate, Alice should vote for
herself to remain the chairperson. In taking an opponent modeling approach, Alice would
like to have a model of how Bob and Carol are likely to vote. Alice believes that with
probability 0.2, Bob and Carol do not collude; with probability 0.3, Bob and Carol collude
to vote for Bob; with probability 0.4, Bob and Carol collude to vote for Carol. Also, Alice
believes that when they collude, both agents might renege and vote for themselves with
probability 0.1.
This example can easily be captured in a NID. The Top-level block is shown in Figure 8.
There is a node Collude, which will have three possible values: none indicating no collusion;
131

Gal & Pfeffer

Bob and Carol indicating collusion to vote for Bob or Carol respectively. The decision nodes
A, B, C represent the decisions for Alice, Bob and Carol, respectively. The CPD for Collude
is presented in Table 8a. The nodes Mod[Alice, B] and Mod[Alice, C], whose CPD is shown
in Table 8b and 8c respectively, depend on Collude. If Collude is none, Mod[Alice, B] will
assign probability 1 to the Top-level block. If Collude is Bob, Mod[Alice, B] will equal a block
B describing an automaton in which Bob and Carol both vote for Bob. If Collude is Carol,
Mod[Alice, B] will equal a block C, in which Bob and Carol vote for Carol with probability
0.9, and block B with probability 0.1. This accounts for the possibility that when Bob
and Carol have agreed to vote for Carol, Bob might renege. The CPD for Mod[Alice, B]
is similar, and is described in Table 8b. The CPD for Mod[Alice, C] is symmetric, and is
described in Table 8c.
Collude

Mod[Alice, B]

Mod[Alice, C]

A

B

C

Alice

Bob

Caroll

Figure 8: Top-level block of Collusion Scenario (Example 4.4)

Collude
none Bob Carol
0.2
0.3
0.5
(a) node Collude

Mod[Alice, B]
Top-level
B
C

none
1
0
0

Collude
Bob Carol
0
0
1
0
0.1
0.9

(b) node Mod[Alice, B]

Mod[Alice, C]
Top-level
B
C

none
1
0
0

Collude
Bob Carol
0
0
0.9
0.1
0
1

(c) node Mod[Alice, C]

Table 8: CPDs for Top-level block of Collusion Scenario (Example 4.4)
In the unique NID equilibrium for this example, all agents are rational so their actually
played and best response strategies are equal. In the equilibrium, Alice always votes for
Carol because she believes that Bob and Carol are likely to collude and vote for Carol.
132

Networks of Influence Diagrams

In turn, Carol votes for herslef or for Bob with probability 0.5, and Bob always votes for
himself. By reneging, Bob gives himself a chance to win the vote, in the case that Carol
votes for him.
Moving beyond this example, one of the most important issues in multi-player games is
alliances. When players form an alliance, they will act for the benefit of the alliance rather
than purely for their own self-interest. Thus an agent’s beliefs about the alliance structure
affects its models of how other agents make their decisions. When an agent has to make a
decision in such a situation, it is important to be able to model its uncertainty about the
alliance structure.
4.4 Cyclic Belief Structures
Cyclic belief structures are important in game theory, where they are used to model agents
who are symmetrically modeling each other. They are used to describe an infinite regress of
“I think that you think that I think...” reasoning. Furthermore, cyclic belief structures can
be expressed in economic formalisms, like Bayesian games, so it is vital to allow them in
NIDs in order for NIDs to encompass Bayesian games. Cyclic belief structures can naturally
be captured in NIDs by including a cycle in the NID graph.
Example 4.5. Recall Example 4.3, in which Alice and Bob had conflicting beliefs about
the speed of the runner. Suppose that Bob believes that the runner is slow, and that with
probability 0.8, Alice believes that the runner is fast, and is modeling Bob as reasoning
about Alice’s beliefs, and so on...
We model this scenario using the cyclic NID described in Figure 9c. In the Top-level
block, shown in Figure 9b, Bob believes the runner to be slow and is modeling Alice as
using block L to make her decision. In block L, Alice believes the runner to be fast, and is
modeling Bob as using the Top-level block to make his decision. Bob’s beliefs about Alice
in the Top-level block are represented by the CPD of node Mod[Bob, Steal], shown in Table
9c, which assigns probability 1 to block L.
In block L, the CPD of Speed, shown in Table 9b assigns probability 1 to f ast. Alice’s
beliefs about Bob in block L are represented by the CPD of node Mod[Alice, PitchOut],
shown in Table 9d, which assigns probability 1 to block L. In the Top-level block, the CPD
of Speed assigns probability 1 to slow, shown in Table 4.4a. The NID equilibrium for this
scenario is as follows. In both blocks L and Top-level, Alice does not steal base, and Bob
does not pitch out, regardless of who is leading.

5. Application: Opponent Modeling
In some cases, agents use rules, heuristics, patterns or tendencies when making decisions.
One of the main approaches to game playing with imperfect information is opponent modeling, in which agents try to learn the patterns exhibited by other players and react to their
model of others. NIDs provide a solid, coherent foundation for opponent modeling.
Example 5.1. In the game of RoShamBo (commonly referred to as Rock-Paper-Scissors),
players simultaneously choose one of rock, paper, or scissors. If they choose the same item,
the result is a tie; otherwise rock crushes scissors, paper covers rock, or scissors cut paper,
as shown in Table 10.
133

Gal & Pfeffer

Speed
f ast slow
0
1

Speed
f ast slow
1
0

(a) node Speed
(b) node Speed
(block Top-level) (block L)

Mod[Bob, Steal]
Top-level
L
1
0
(c) node
Mod[Bob, Steal]
(block Top-level)

Mod[Alice, PitchOut]
Top-level
L
1
0
(d) node
Mod[Alice, PitchOut]
(block L)

Table 9: CPDs for nodes in Cyclic NID (Example 4.5)

Mod[Bob, Steal]

Mod[Alice, PitchOut]

Speed

Speed

Steal

Steal

PitchOut
ThrownOut

PitchOut
ThrownOut

Alice

Alice

Bob

(a) Block L

Bob

(b) Block Top-level

Top level
Bob,STEAL

Alice,PITCHOUT

L

(c) Cyclic NID

Figure 9: Cyclic Baseball Scenario (Example 4.5)

rock
paper
scissors

rock
(0, 0)
(1, −1)
(−1, 1)

paper
(−1, 1)
(0, 0)
(1, −1)

scissors
(1, −1)
(−1, 1)
(0, 0)

Table 10: Payoff Matrix for Rock-paper-scissors

134

Networks of Influence Diagrams

The game has a single Nash equilibrium in which both players play a mixed strategy
over {rock, paper, scissors} with probability { 13 , 13 , 13 }. If both players do not deviate from
their equilibrium strategy, they are guaranteed an expected payoff of zero. In fact, it is
easy to verify that a player who always plays his equilibrium strategy is guaranteed to
get an expected zero payoff regardless of the strategy of his opponent. In other words,
sticking to the equilibrium strategy guarantees not to lose a match in expectation, but it
also guarantees not to win it!
However, a player can try and win the game if the opponents are playing suboptimally.
Any suboptimal strategy can be beaten, by predicting the next move of the opponent and
then employing a counter-strategy. The key to predicting the next move is to model the
strategy of the opponent, by identifying regularities in its past moves.
Now consider a situation in which two players play repeatedly against each other. If
a player is able to pick up the tendencies of a suboptimal opponent, it might be able to
defeat it, assuming the opponent continues to play suboptimally. In a recent competition (Billings, 2000), programs competed against each other in matches consisting of 1000
games of RoShamBo. As one might expect, Nash equilibrium players came in the middle
of the pack because they broke even against every opponent. It turned out that the task
of modeling the opponent’s strategy can be surprisingly complex, despite the simple structure of the game itself. This is because sophisticated players will attempt to counter-model
their opponents, and will hide their own strategy to avoid detection. The winning program,
called Iocaine Powder (Egnor, 2000), did a beautiful job of modeling its opponents on multiple levels. Iocaine Powder considered that its opponent might play randomly, according
to some heuristic, or it might try to learn a pattern used by Iocaine Powder, or it might
play a strategy designed to counter Iocaine Powder learning its pattern, or several other
possibilities.
5.1 A NID for Modeling Belief Hierarchies
Inspired by “Iocaine Powder”, we constructed a NID for a player that is playing a match
of RoShamBo and is trying to model his opponent. Suppose that Bob wishes to model
Alice’s play using a NID. The block Top-level of the NID, shown in Figure 10a, is simply a
MAID depicting a RoShamBo round between Bob and Alice. Both players have access to a
predictor P, an algorithm that is able to predict the next move in a sequence as a probability
distribution over the possible moves. The only information available to the predictor is the
history of past moves for Alice and Bob.
Alice may be ignoring P, and playing the Nash Equilibrium strategy. Bob has several
alternative models of Alice’s decision. According to block Automaton, shown in Figure 10c,
Alice always follows the signal P. In block B1, shown in Figure 10b, Bob is modeling Alice
as using block Automaton to make her decision. This is achieved by setting the CPD of
Mod[Bob, Alice] in block B1 to assign probability 1 to Automaton. We can analyze the NID
rooted at block B1 to determine Bob’s best response to Alice. For example, if Bob thinks,
based on the history, that P is most likely to tell Alice to play rock, then Bob would play
paper. Let us denote this strategy as BR(P).
However, Alice can also model Bob by assigning probability 1 to Mod[Alice, Bob] in block
A1. In this way, Alice is reasoning about Bob modeling Alice as following the predictor P.
135

Gal & Pfeffer

P

Mod[Alice, Bob]

alice

bob

bob

alice

Mod[Bob, Alice]

P

alice

bob

bob

alice

P

(a) Blocks Top-level, A1,A2

(b) Blocks B1,B2

Alice

(c) Block Automaton

Top-level

Bob,ALICE

A2

Bob, ALICE

Alice, BOB
Bob, ALICE
B2

Bob,ALICE

A1

Alice, BOB

B1
Bob, ALICE
Automaton

(d) RoShamBo NID

Figure 10: RoShamBo Scenario (Example 5.1)

136

Networks of Influence Diagrams

When we analyze the NID originating in block A1, shown in Figure 10a, we will determine
Alice’s best-response to Bob’s model of her as well as Bob’s best-response to his model of
Alice. Since Alice believes that Bob plays BR(P) as a result of Bob’s belief that Alice plays
according to P, she will therefore play a best response to BR(P), thereby double-guessing
Bob. Alice’s strategy in block A1 is denoted as BR(BR(P)). Following our example, in
block A1 Alice does not play rock at all, but scissors, in order to beat Bob’s play of paper.
Similarly, in block B2, Bob models Alice as using block A1 to make her decisions, and in
block A2, Alice models Bob as using block B2 to make his decision. Therefore, solving the
NID originating in block B2 results in a BR(BR(BR(P))) strategy for Bob. This would
prompt Bob to play rock in B2 in our example, in order to beat scissors. Lastly, solving
the NID originating in block A2 results in a BR(BR(BR(BR(P)))) strategy for Alice.
This would prompt Alice to play paper in block A2, in order to beat rock. Thus, we have
shown that for every instance of the predictor P, Alice might play one of the three possible
strategies. Any pure strategy can only choose between rock, paper, or scissors for any
given P, so this reasoning process terminates.
The entire NID is shown in Figure 10d. In block Top-level, Bob models Alice as using
one of several possible child blocks: block Automaton, in which Alice follows her predictor;
block A1, in which Alice is second-guessing her predictor; or block A2, in which Alice is
triple-guessing her predictor. Bob’s uncertainty over Alice’s decision-making processes is
captured in the Mod[Bob, Alice] node in block Top-level. Analyzing the Top-level block of
this NID will extract Bob’s best response strategy given his beliefs about Alice’s decisionmaking processes.
To use this NID in practice, it is necessary to compute the MAID equilibrium and extract
Bob’s best-response strategy at the Top-level block. To this end, we need to estimate the
values of the NID parameters, represented by the unknown CPDs at each of its blocks,
and solve the NID. These parameters include Mod[Bob, Alice], representing Bob’s beliefs
in the Top-level block regarding which block Alice is using; and node P, representing the
distributions governing the signals for Alice and Bob, respectively.2 To this end, we use
an on-line version of the the EM algorithm that was tailored for NIDs. We begin with
random parameter assignments to the unknown CPDs. We then revise the estimate over
the parameters of the NID given the observations at each round. Then Bob plays the bestresponse strategy of the MAID representation for the NID given the current parameter
setting. Interleaving learning and using the NID to make a decision helps Bob to adapt to
Alice’s possibly changing strategy.
5.2 Empirical Evaluation
We evaluated the NID agent against the ten top contestants from the first automatic
RoShamBo competition. All of these agents used an opponent modeling approach, that
is, they learned some signal of their opponent’s play based on the history of prior rounds.
Contestants can be roughly classified according to three dimensions: the type of signal used
(probabilistic vs. deterministic); the type of reasoning used (pattern vs. meta-reasoners);
and, their degree of exploration versus exploitation of their model. Probabilistic agents
2. Technically, the CPDs for the nodes representing prior history are also missing. However, they are
observed at each decision-making point in the interaction and their CPDs do not affect players’ utilities.

137

Gal & Pfeffer

estimated a distribution over the strategies of their opponents while deterministic agents
predicted their opponents’ next move with certainty. Pattern reasoners directly modeled
their opponents as playing according to some rule or distribution, and did not reason about
the possibility that their opponents were modeling themselves. In contrast, meta-reasoners
attempted to double- or triple-guess their opponents’ play. Exploitative agents played a best
response to their model of their opponents, while explorative agents deviated, under certain
conditions, from their best response strategy to try and learn different behavioral patterns
of their opponents. Iocaine Powder used the strategy of reverting to the Nash equilibrium
when it was losing. Because this made it impossible to evaluate whether our NID model
could learn Iocaine powder’s reasoning process, we turned off this strategy. Also, we limited
all contestants’ strategies to depend on the last 100 rounds of play, in order to allow a fair
comparison with the NID agent that only used four rounds of play. We did not limit them
to four rounds because they were not originally designed to use such a short history. Our
purpose was to show that explicitly reasoning and learning about mental models can make
a difference, and not to optimize learning the model of the signal.
Figure 11 shows the performance of the RoShamBo NID when playing 10 matches of
3,000 rounds with each contestant. The overall standings were determined by ordering
the total scores for each contestant in all rounds played (+1 for winning a round against a
contestant by the NID player; −1 for losing a round; 0 for ties). Therefore, it was important
for each player to maximize its win against the weaker opponents, and minimize its loss to
stronger opponents. The x-axis includes the contestant number while the y-axis describes
the difference between the average score of the RoShamBo NID and the contestant; error
bars indicate a single standard deviation difference.
As shown by the figure, the RoShamBo NID was able to defeat all contestant in all
matches, including a version of Iocaine Powder. The best performance for the NID was
achieved when playing pattern reasoners that used deterministic signals (Contestants 3, 5
and 6). Each of these contestants directly predicted their opponents’ play as a function of
the history, without reasoning about their opponents’ model of themselves. Consequently,
it was difficult for them to detect change in the strategies of adaptive opponents, such as
the RoShamBo NID. In addition, the use of deterministic signals made it harder for these
contestants to capture probabilistic players like the NID algorithm.
The RoShamBo NID also outperformed those contestants that attempted to trick their
opponents, by reasoning about the possibility that the opponents are double- and tripleguessing their model (Contestants 4 and 1). This shows that the NID was able to determine
the level of reasoning employed by its opponents.

6. Relationship with Economic Models
In this section, we describe the relationship between NIDs and several existing formalisms
for representing uncertainty over decision-making processes. NIDs share a close relationship with Bayesian games (Harsanyi, 1967), a game-theoretic framework for representing
uncertainty over players’ payoffs. Bayesian games capture the beliefs agents have about
each other as well as define an equilibrium that assigns a best response strategy for each
agent given its beliefs. Bayesian games are quite powerful in their ability to describe belief
hierarchies and cyclic belief structures.
138

Networks of Influence Diagrams

400

350

Average Score Difference

300

250

200

150

100

50

0

0

1

2

3

4

5
Contestant

6

7

Opponent type
Iocaine Powder
Probabilistic, Pattern, Exploitative
Deterministic, Pattern, Exploitative
Probabilistic, Meta, Exploitative
Probabilistic, Pattern, Exploitative

8

9

10

Number
1
2, 9
3, 6, 5
1, 4
7, 8

Figure 11: Difference in average outcomes between NID player and opponents

In a Bayesian game, each agent has a discrete type embodying its private information.
Let N be a set of agents. For each agent i a Bayesian game includes a set of possible types
Ti , a set of possible actions Ci , a conditional distribution pi and a utility function ui . Let
T = ×i∈N Ti and let C = ×i∈N Ci . For each agent i, let T−i = ×j&=i Tj denote the set of
all possible types other than those of agent i. The probability distribution pi is a function
from ti to ∆T−i , that is, pi (.|ti ) specifies for each type ti ∈ Ti a joint distribution over
the types of the other agents. The utility function ui is a function from C × T to the real
numbers. It is a standard assumption that the game, including agents’ strategies, utilities
and type distributions, is common knowledge to all agents.
The solution concept most commonly associated with Bayesian games is a Bayesian
Nash equilibrium. This equilibrium maps each type to a mixed strategy over its actions
that is the agent’s best response to the strategies of the other agents, given its beliefs about
their types. Notice that in a Bayesian game, an agent’s action can depend on its own types
but not on the types of the other agents, because they are unknown to that agent when it
analyzes the game. It is assumed that each agent knows its own type, and that this type
subsumes all of the agent’s private information before the game begins. Because the types
of other agents are unknown, each agent maximizes its expected utility given its distribution
over other types.
Let N−i denote all of the agents in the Bayesian game apart from agent i. Let σi (.|ti )
denote a random strategy for agent i given that its type is ti . A Bayesian Nash equilibrium
139

Gal & Pfeffer

is any mixed strategy profile σ such that for any agent i and type ti ∈ Ti we have
#
σi (.|ti ) ∈ argmaxτ ∈∆Ci t−i ∈T−i pi (t−i |ti )·
$%
&
#
c∈C
j∈N−i σj (cj |tj ) τ (ci )ui (t, c)

(2)

Bayesian games have been used extensively for modeling interaction in which agents have
private information, such as auction mechanisms (Myerson, 1991) and they can be used to
express uncertainty over agents’ decision-making models. In general, Bayesian games are
just as expressive as NIDs. As we show, any Bayesian game can be converted into a NID
in time and space linear in the size of the Bayesian game. Conversely, any NID can be
converted into a Bayesian game, because any NID can be converted to a MAID, which can
in turn be converted to an extensive form game. The extensive form game can be converted
to a normal form game which is a trivial Bayesian game with only one type per agent.
However, in the worst case, the size of the extensive form game will be exponential in the
number of informational parents for decision nodes in the MAID, and the size of the normal
form game will be exponential in the size of the extensive form game. Of course, this is a
brute force conversion; more compact conversions may be possible.
We now consider more formally the question of whether Bayesian games can be represented by NIDs. The idea is to align each type in a Bayesian game with a decision in a
NID block. The resulting best response strategy for the decision in the NID equilibrium
will equal the Bayes Nash equilibrium strategy for the type.
Definition 6.1. Let B be a Bayesian game and N a NID. We say that N is equivalent to
B if there exists an injective mapping f from types in B to (block,agent) pairs in N , such
that the following conditions hold:
1. For any Bayesian Nash equilibrium σ of B, there exists a NID equilibrium of N , such
that for every type ti , if f maps ti to (K, α), the best-response and actually-played
strategies for α in K are equal to σi (.|ti ).
2. For any NID equilibrium of N , there exists a Bayesian Nash equilibrium σ of B such
for every (K, α) in the image of f , σi (.|ti ) where ti = f −1 (K, α) is equal to the
best-response and actually-played strategies for α in K.
The following theorem is proved in Appendix 8.
Theorem 6.2. Every Bayesian game can be represented by an equivalent NID whose size
is linear in the size of the Bayesian game.
In this section, we will use the term Bayesian games to specify a representation that
includes type distributions and utility functions that are presented explicitly. NIDs enjoy the
same advantages over fully specified Bayesian games that graphical models typically enjoy
over unstructured representations. In general, NIDs may be exponentially more compact
than Bayesian games because Bayesian games require, for every type of every agent, a full
joint distribution over the types of all other agents. In addition, the utility function in a
Bayesian game specifies a utility for each joint combination of types and actions of every
player. These distributions and utility functions are exponential in the number of players.
In NIDs, because they are based on MAIDs, the type distributions can be decomposed
140

Networks of Influence Diagrams

into a product of small conditional distributions, and the utility functions can be additively
decomposed into a sum of small functions that depend only on a small number of actions.
In addition, Bayesian games are representationally obscure. First, types in Bayesian
games are atomic entities that capture all the information available to an agent in a single
variable. A type is used to capture both an agent’s beliefs about the way the world works
(including its preferences), and its private information. For example, in poker, both the
player’s beliefs about the other player’s tendency to bluff and her knowledge of what cards
she has received are captured by a type. We believe that these two aspects are fundamentally
different; one describes the actual state of the world and the other describes what is going on
in a player’s head. Conflating these two aspects leads to confusion. In NIDs, the two aspects
are differentiated. Private information about the world is represented by informational
parents, whereas mental models are represented by blocks.
Second, a type in a Bayesian game does not decompose different aspects of information
into variables. Thus in poker, the hand must be represented by a single variable, whereas in
NIDs it can be represented by different variables representing each of the cards. A final point
is that in Bayesian games all of the uncertainty must be folded into the utility functions
and the distribution over agents’ types. Consider the scenario in which two agents have
conflicting beliefs about a chance variable, such as in Example 4.3. In a NID, there will
be a separate block for each possible mental model that differs in the CPD assignments
for the chance variable. In contrast, each type in the Bayesian game would sum over the
distribution over the chance variable. Looking at the Bayesian game, we would not know
whether the reason for the different utility functions is because the agent has different beliefs
about the chance variable, or whether it is due to different preferences of the agent.
NIDs also exhibit a relationship with more recent formalisms for games of awareness,
in which agents may be unaware of other players’ strategies or of the structure of the
game (Halpern & Rego, 2006; Feinberg, 2004). A game description in this formalism shows
how players’ awareness about each other’s strategies changes over time. A game of awareness
includes a set of extensive form game descriptions, called augmented games, that represent
an analyst’s beliefs about the world, as well as separate descriptions for each game that
may become true according to agents’ subjective beliefs. The analyst’s augmented game is
considered to be the actual description of reality, while each subjective augmented game can
differ from the analyst’s game in agents’ utility functions, their decisions, and the strategies
available to agents at each of their decisions. A history for an agent in an augmented game
is a sub-path in the tree leading to a node in which the agent makes a move. Awareness is
modeled by a function that maps an agent-history pair in one augmented game to another
augmented game which the agent considers possible given the history. Uncertainty over
agents’ awareness in an augmented game can be quantified by having nature choose a
move in the tree leading to agents’ information sets. The definition of Nash equilibrium
is extended to include a set of strategies for each agent-game pairthat the agent considers
to be possible, given a history and the best-response strategies used by other agents in the
augmented game. This formalism can capture an analyst’s model about agents’ awareness
as well as agents’ model about their own, or other agents’ awareness.
There are fundamental differences between NIDs and games of awareness. First, like
Bayesian games, the equilibrium conditions for this representation do not allow for agents to
deviate from their best-response strategies. Second, they require the presence of a modeler
141

Gal & Pfeffer

agent, that in reality, is modeling its uncertainty about levels of awareness of other agents.
NIDs allow for such a modeler agent, but they do not require it. This allows to capture
situations where no agent has certain knowledge of reality, such as in the Baseball NID of
Example 2.4. Third, each augmented game of awareness is represented as an extensive form
game, that as we have shown above, may be exponentially larger than the MAID used to
represent each decision-making model in a NID. Lastly, agents’ awareness over each other’s
strategies is just one type of reasoning that can be captured by a NID. Other types of
reasoning processes were described in Section 4.
Lastly, Gmytrasiewicz and Durfee (2001) have developed a framework for representing
uncertainty over decision-making using a tree structure in which the nodes consist of payoff
matrices for a particular agent. Like Bayesian games, uncertainty is folded into the payoff
matrices. Each agent maintains its own tree, representing its model of the decision-making
processes used by other agents. Like traditional representations, this language assumes that
all agents behave rationally. In addition, it assumes that each agent believes others to use
a fixed strategy, that is folded into the environment.

7. Conclusion
We have presented a highly expressive language for describing agents’ beliefs and decisionmaking processes in games. Our language is graphical. A model in our language is a
network of interrelated models, where each mental model itself is a graphical model of a
game. An agent in one mental model may believe that another agent (or possibly itself) uses
a different mental model to make decisions; it may have uncertainty about which mental
model is used. We presented semantics for our language in terms of multi-agent influence
diagrams. We analyzed the relationship between our language and Bayesian games. They
are equally expressive, but NIDs may be exponentially more compact.
We showed how our language can be used to describe agents who play irrationally, in the
sense that their actual play does not correspond to the best possible response given their
beliefs about the world and about other agents. This is captured by a novel equilibrium
concept that captures the interaction between what agents should do and what they actually
do. We also showed how to express situations in which agents have conflicting beliefs,
including situations in which the agents do not have a common prior distribution over the
state of the world. Finally, we showed how to capture cyclic reasoning patterns, in which
agents engage in infinite chains of “I think that you think that I think...” reasoning.
A vital question is the use of our language to learn about agents’ behavior and reasoning
processes. As we have shown, our language can be used to learn non-stationary strategies
in rock-paper-scissors. In other work, we have shown how models that were inspired by
NIDs can learn people’s play in negotiation games (Gal, Pfeffer, Marzo, & Grosz, 2004; Gal
& Pfeffer, 2006). The focus of our continuing work will be to develop a general method for
learning models in NIDs.

Acknowledgments
Thank you very much for the useful comments provided by the anonymous reviewers and
editor. Thanks to Barbara Grosz and Whitman Richards for their invaluable guidance.
142

Networks of Influence Diagrams

Thanks to Adam Juda for reading a prior draft of this work. This work was supported by
an NSF Career Award IIS-0091815 and AFOSR under contract FA9550-05-1-0321.

8. Appendix A
Theorem 3.1: Converting a NID into a MAID will not introduce a cycle in the resulting
MAID.
Proof. First, let us ignore the edges added by step 5 of the construction, and focus on the
MAID fragment OK constructed from a single block K. Since the block is acyclic, we can
number the nodes of the block with integers in topological order. We now number the nodes
of OK as follows. For a node Nα that derives from a chance or utility node N in K, Nα
gets the same number as N . A node BR[D]K gets the same number as D. A node DαK ,
where α owns D, gets the same number as D plus 1/3. A node DαK , where α does not own
D, gets the same number as D plus 2/3. By construction, if P is a parent of N in OK , P
has a lower number than N .
Now let us consider the entire constructed MAID O. Suppose, by way of contradiction,
that there is a cycle in O. It follows from the above argument that it must consist entirely
of edges between fragments added by step 5. Since all such edges emanate from a node
DαK where α owns D, and end at a node DαL , all nodes in the cycle must refer to the same
decision D, and must belong to the agent who owns D. Thus the cycle must be of the form
DαK1 , . . . , DαKn , DαK1 where α owns D. Since an edge has been added from DαKi to DKi+1 in
O, α must be modeling itself in block Ki as using block Ki+1 to make decision D. Therefore
there is a self-loop in the NID, which is a contradiction.
Theorem 6.2: Every Bayesian game can be represented as an equivalent NID whose size
is linear in the size of the Bayesian game.
Proof. Given a Bayesian game B, we construct a NID N as follows. The set of agents in N
is equal to the set of agents in B. For each type ti of agent i in B there is a corresponding
block in N labeled ti . The block ti contains a decision node Dj and utility node Uj for
every agent j. Dj has no informational parents. The domain of Dj is the set of choices Cj
for agent j in B. We add a new chance node Qi in block ti whose domain is the set T−i .
Each node Mod[i, Dj ] where j $= i will have the node Qi as a parent. The parents of Ui are
all the decision nodes as well as the node Qi . For an agent j $= i, Uj has only the parent
Dj . For each agent j we define a distinguished action c∗j ∈ Cj .
We set the CPD for nodes in ti as follows:
1. The CPD of Mod[i, Di ] assigns probability 1 to ti .
2. The CPD of Qi assigns probability pi (t−i | ti ), as defined in B, for each type profile
t−i ∈ T−i .
3. The CPD of a node Mod[i, Dj ] where j $= i assigns probability 1 to block tj when the
jth element of the value of the parent node Qi equals tj . This projects the probability
distribution Qi in B to the node Mod[i, Dj ] representing i’s beliefs about which block
agent j is using in the NID.
143

Gal & Pfeffer

4. The CPD of Ui assigns probability 1 to ui (t, c), as defined in B, given that Qi equals
t, and D equals c.
5. The CPD of Uj assigns probability 1 to utility 1 when Dj = c∗j , and probability 1 to
0 otherwise.
6. The CPD of Mod[j, Dk ], for all k, when j $= i, assigns probability 1 to ti .
Our construction is accompanied by the injective mapping f that maps a type ti to the
(block,agent) pair (ti , i).
Let M be the constructed MAID for N. To prove condition 1 of Definition 6.1, let τ be
a Bayes Nash equilibrium of B. For each agent, τi is a conditional probability distribution
τi (· | ti ). We define the strategy profile σ in M as follows. σBR[Di ]ti = τi (· | ti ) for decisions
owned by agent i, and σBR[Dj ]ti assigns probability 1 to c∗j when j $= i.
We claim the following:
1. σ is a MAID equilibrium in M, according to Definition 2.3.
2. In the resulting NID equilibrium, the best response strategy for i in ti is τi (· | ti ).
3. In the resulting NID equilibrium, the actually played strategy is the same as the best
response strategy.
Claim 3 is true because Mod[i, Di ] assigns probability 1 to ti .
Note that there are no informational parents in N. Therefore, by the definition of NID
ti
= σBR[Di ,ti ] = τi (· | ti ). Therefore, Claim 2 is
equilibrium, the best response strategy θD
i
true.
To prove Claim 1, note first that in block ti , the utility node Uj , where j $= i, is fully
determined by Dj , because Dj is the sole parent of Uj . Also, player j is not self-modeling
at Dj , because the CPD of node Mod[j, Dj ] assigns probability 1 to ti . The same holds in
M: the decision node BR[Dj ]ti is the sole parent of Ujti . Therefore, in any equilibrium for
M, the strategy for BR[Dj ]ti will assign probability 1 to the distinguished action c∗j that
causes Ujti to be 1.
In block ti , the CPD of Mod[i, Dj ] assigns probability 0 to ti . This means that player
j is not using block ti to make its decision, according to i’s beliefs. Therefore, BR[Dj ]ti
is independent of Uiti , and the equilibrium strategies for BR[Di ]ti are independent of the
distinguished action chosen for the BR[Di ]tj .
By the definition of MAID equilibrium of Definition 2.3, the strategy profile σ is an
equilibrium if each σi maximizes EUσ (i). We need to show that maximizing this is equivalent
to maximizing the right hand side of Equation 2. There is a utility Uiti and decision node
BR[Di ]ti in every block ti . Let ctii denote a choice for agent i at decision BR[Di ]ti in block
t"

ti . Let t(i denote a block corresponding to a different type t(i for agent i. Let cii be a choice
"
for the agent in decision BR[Di ]ti at block t(i and c−i all the choices for other decisions
"
BR[D−i ]ti . By the construction of M, Uiti is d-separated from BR[Di ]ti given BR[Di ]ti and
BR[D−i ]ti .
As a result, we can optimize Uiti separately from all other utility nodes belonging to
agent i, considering only BR[Di ]ti . We then get that the utility for i in M given the
144

Networks of Influence Diagrams

strategy profile σ can be written as
E σ [Uiti ] =

"

σiti (ctii )

"
c−i

t
ci i

σ−i (c−i )

"
t
ui i

P σ (Uiti = ui | ctii , c−i ) · utii

(3)

We now condition on agent i’s beliefs about the decisions of other agents in block ti . Let
Mod[i, D−i ]ti denote the set of nodes Mod[i, Dj ]ti where j $= i, and let the tuple t−i refer
to the block label profile for blocks T−i . We now obtain
"
t
ci i

σiti (ctii )

"
c−i

σ−i (c−i )

"
t−i

P (Mod[i, D−i ]ti = t−i )

"
t
ui i

(utii | ctii , c−i , t−i ) · utii

(4)

Now observe that the role of Mod[i, D−i ]ti is to determine which choices for decisions
BR[D−i ]ti are relevant for the utility of player i. In particular, if Mod[i, Dj ]ti is equal to
tj , then it is j’s choice in block tj that player i needs to consider when it makes decision
t
BR[Di ]ti . Let ci −i denote the relevant choices for BR[D−i ]ti when Mod[i, D−i ]ti = t−i .
Since other choice variables are irrelevant, we can marginalize them out and obtain
#

t

ci i

σiti (ctii )

#

t

t

−i
c−i

#

−i
σ−i (c−i
)

#

t−i

P (Mod[i, D−i ]ti = t−i )

(5)

t

−i
P σ (Uiti = ui | ctii , c−i
) · utii

t

ui i

Rearranging terms, we rewrite Equation 5.
(
# '%
tj tj
ti = t )
P
(Mod[i,
D
]
σ
(c
)
−i
−i
t−i
c
j&=i j
j
t
ti ti #
t
t
t
−i
σi (ci ) uti P σ (Ui i = ui | cii , c−i
) · ui i

#

(6)

i

t

t

By our construction, P (Mod[i, D−i ]ti = t−i ) is pi (t−i | ti ) as defined in B, σjj (cjj ) is
#
t−i
) · utii is ui (t, c). We therefore get
τj (cj | tj ) as defined in B, and uti P σ (Uiti = ui | ctii , c−i
i

"
t−i

pi (t−i | ti )

"
c




!
j&=i



τj (cj | tj ) τi (ci | ti )ui (t, c)

(7)

Therefore σ is a MAID equilibrium of M if and only if τ is a Bayesian Nash equilibrium of
B. Claim 1 is established and therefore Condition 1 of Definition 6.1 is satisfied.
Finally, to prove Condition 2, given a NID equilibrium of N we construct a MAID
equilibrium σ for M by copying the best response strategies, and then construct strategies
τ for B in exactly the reverse manner to above. The previous reasoning applies in reverse
to show that τ is a Bayes Nash equilibrium of B and the best response and actually played
strategies for N are equal to τ .

145

Gal & Pfeffer

References
Arunachalam, R., & Sadeh, N. M. (2005). The supply chain trading agent competition.
Electronic Commerce Research and Applications, 4, 63–81.
Aumann, R., & Brandenburger, A. (1995). Epistemic conditions for Nash equilibrium.
Econometrica, 63 (5), 1161–1180.
Bazerman, M. (2001). Judgment in Managerial Decision Making. Wiley Publishers.
Billings, D. (2000). The first international RoShamBo programming competition. International Computer Games Association Journal, 23 (1), 3–8.
Blum, B., Shelton, C. R., & Koller, D. (2006). A continuation method for Nash equilibria
in structured games. Journal of Artificial Intelligence Research, 25, 457–502.
Bonanno, G., & Nehring, K. (1999). How to make sense of the common prior assumption
under incomplete information. International Journal of Game Theory, 28, 409–434.
Camerer, C. F. (2003) In Behavioral Game Theory. Experiments in Strategic Interaction,
chap. 2. Princeton University Press.
Cowell, R. G., Lauritzen, S. L., & Spiegelhater, D. J. (2005). Probabilistic Networks and
Expert Systems. Springer.
Dechter, R. (1999). Bucket elimination: A unifying framework for reasoning. Artificial
Intelligence, 113 (1-2), 41–85.
Egnor, D. (2000). Iocaine Powder. International Computer Games Association Journal,
23 (1), 3–8.
Feinberg, Y. (2004). Subjective reasoning — games with unawareness. Tech. rep. 1875,
Stanford University.
Gal, Y., & Pfeffer, A. (2003a). A language for modeling agents’ decision making processes
in games. In Proc. 2nd International Joint Conference on Autonomous Agents and
Multi-agent Systems (AAMAS).
Gal, Y., & Pfeffer, A. (2003b). A language for opponent modeling in repeated games. In
Workshop on Game Theory and Decision Theory, AAMAS.
Gal, Y., & Pfeffer, A. (2004). Reasoning about rationality and belief. In Proc. 3rd International Joint Conference on Autonomous Agents and Multi-agent Systems (AAMAS).
Gal, Y., & Pfeffer, A. (2006). Predicting people’s bidding behavior in negotiation. In Proc.
5th International Joint Conference on Autonomous Agents and Multi-agent Systems
(AAMAS).
Gal, Y., Pfeffer, A., Marzo, F., & Grosz, B. (2004). Learning social preferences in games.
In Proc. 19th National Conference on Artificial Intelligence (AAAI).
Gigerenzer, G., & Selten, R. (Eds.). (2001). Bounded Rationality: The Adaptive Toolbox.
MIT Press.
Gmytrasiewicz, P., & Durfee, E. H. (2001). Rational communication in multi-agent environments. Autonomous Agents and Multi-Agent Systems, 4 (3), 233–272.
146

Networks of Influence Diagrams

Halpern, J., & Rego, L. (2006). Extensive games with possibly unaware players. In Proc.
5th International Joint Conference on Autonomous Agents and Multi-agent Systems
(AAMAS).
Harsanyi, J. C. (1967). Games with incomplete information played by ’Bayesian’ players.
Management Science, 14, 159–182, 320–334, 486–502.
Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. In Readings on the Principles
and Applications of Decision Analysis, pp. 721–762.
Kearns, M., Littman, M., & Singh, S. (2001). Graphical models for game theory. In Proc.
17th Conference on Uncertainty in Artificial Intelligence (UAI).
Koller, D., Meggido, N., & von Stengel, B. (1996). Efficient computation of equilibria for
extensive two-person games. Games and Economic Behavior, 14 (2), 247–259.
Koller, D., & Milch, B. (2001). Multi-agent influence diagrams for representing and solving
games. In Proc. 17th International Joint Conference on Artificial Intelligence (IJCAI).
MacKie-Mason, J. K., Osepayshivili, A., Reeves, D. M., & Wellman, M. P. (2004). Price
prediction strategies for market-based scheduling. In Proc. of 18th International Conference on Automated Planning and Scheduling.
Morris, S. (1995). The common prior assumption in economic theory. Economic Philosophy,
pp. 227–253.
Myerson, R. (1991). Game Theory. Harvard University Press.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems. Morgan Kaufmann.
Rajarshi, D., Hanson, J. E., Kephart, J. O., & Tesauro, G. (2001). Agent-human interactions
in the continuous double auction. In Proc. 17th International Joint Conference on
Artificial Intelligence (IJCAI).
Rubinstein, A. (1998). Modeling Bounded Rationality. MIT Press.
Russell, S., & Wefald, E. (1991). Do the Right Thing: Studies in Limited Rationality. MIT
Press.
Simon, H. A. (1955). A behavioral model of rational choice. Quarterly Journal of Economics,
69, 99–118.
Vickrey, D., & Koller, D. (2002). Multi-agent algorithms for solving graphical games. In
Proc. 18th National Conference on Artificial Intelligence (AAAI).

147

Journal of Artificial Intelligence Research 33 (2008) 33–77

Submitted 09/07; published 09/08

ICE: An Expressive Iterative Combinatorial Exchange
Benjamin Lubin
Adam I. Juda
Ruggiero Cavallo
Sébastien Lahaie
Jeffrey Shneidman
David C. Parkes

blubin@eecs.harvard.edu
adamjuda@post.harvard.edu
cavallo@eecs.harvard.edu
slahaie@eecs.harvard.edu
jeffsh@eecs.harvard.edu
parkes@eecs.harvard.edu

School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138

Abstract
We present the design and analysis of the first fully expressive, iterative combinatorial
exchange (ICE). The exchange incorporates a tree-based bidding language (TBBL) that is
concise and expressive for CEs. Bidders specify lower and upper bounds in TBBL on their
value for different trades and refine these bounds across rounds. These bounds allow price
discovery and useful preference elicitation in early rounds, and allow termination with an
efficient trade despite partial information on bidder valuations. All computation in the
exchange is carefully optimized to exploit the structure of the bid-trees and to avoid enumerating trades. A proxied interpretation of a revealed-preference activity rule, coupled
with simple linear prices, ensures progress across rounds. The exchange is fully implemented, and we give results demonstrating several aspects of its scalability and economic
properties with simulated bidding strategies.

1. Introduction
Combinatorial exchanges combine and generalize two different mechanisms: double auctions
and combinatorial auctions. In a double auction (DA), multiple buyers and sellers trade
units of an identical good (McAfee, 1992). In a combinatorial auction (CA), a single seller
has multiple heterogeneous items up for sale (de Vries & Vohra, 2003; Cramton, Shoham,
& Steinberg, 2006). Each buyer in a CA may have complementarities (“I want A and B”)
or substitutabilities (“I want A or B”) between goods, and is provided with an expressive
bidding language to describe these preferences. A common goal in the design of both DAs
and CAs is to implement the efficient allocation, which is the allocation that maximizes
total social welfare.
A combinatorial exchange (CE) (Parkes, Kalagnanam, & Eso, 2001) is a combinatorial
double auction that brings together multiple buyers and sellers to trade multiple heterogeneous goods. CEs have potential use in wireless spectrum allocation (Cramton, Kwerel, &
Williams, 1998; Kwerel & Williams, 2002), airport takeoff and landing slot allocation (Ball,
Donohue, & Hoffman, 2006; Vossen & Ball, 2006), and in financial markets (Saatcioglu,
Stallaert, & Whinston, 2001). In all of these domains there are incumbents with property
rights, and it is necessary to facilitate a complex multi-way reallocation of resources. Another potential application domain for CEs is to allocate resources in shared distributed
c
2008
AI Access Foundation. All rights reserved.

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

systems, such as PlanetLab (Fu, Chase, Chun, Schwab, & Vahdat, 2003). CEs can also find
use in task allocation in robot teams, making them a potentially powerful tool to the multiagent systems community (Gerkey & Mataric, 2002; Bererton, Gordon, & Thrun, 2003;
Dias, Zlot, Kalra, & Stentz, 2006). Finally, CEs have promise as mechanisms for expressive
sourcing by multiple bid-takers, perhaps representing different profit centers within an organization; see the associated work on expressive sourcing using one-sided CAs (Sandholm,
2007).
This paper presents the design of the first fully expressive, iterative combinatorial exchange (ICE). In designing an iterative exchange, we share the motivation of earlier work on
iterative CAs: we wish to mitigate elicitation costs by focusing bidders, in this case through
price discovery and activity rules, on their values for relevant trades. This is important
because determining the value on even a single potential trade can be a challenging problem in complex domains (Sandholm & Boutilier, 2006; Compte & Jehiel, 2007). Moreover,
bidders often wish to reveal as little information as possible to avoid leaking information to
competitors. In describing the central design principles that support the ICE mechanism,
we highlight the following aspects:
• A bidder interacts with ICE by first defining a structured representation of his valuation for different trades. Defined in the tree-based bidding language (TBBL), this
concisely defines the set of trades of interest to the bidder. The bidder must annotate
the tree with initial lower and upper bounds on his value for different trades.
• Having lower and upper bounds on valuations allows the exchange to identify both a
provisional trade and provisional payments in each round, and to generate a provisional clearing price on each item in the market. In each round of ICE, each bidder
is required to tighten the bounds on his TBBL bid so as to make precise which trade
is most preferred given the current prices.
• ICE is a hybrid between a demand-revealing process and a direct-revelation mechanism, with simple (linear) prices guiding preference elicitation but bids submitted
through direct claims about valuation functions in the TBBL language, and these
expressive bids being finally used to clear the exchange.
When ICE terminates, a payment rule is used to determine the payments made, and
received, by each participant. While suggesting that these payments be defined in a way
that seeks to mitigate opportunities for manipulation in the exchange, ICE is agnostic to
the particular payment rule that is adopted. For a given rule, the prices that are quoted
in each round are defined in part to approximate these payments, when aggregated across
the provisional trade suggested for a bidder. For concreteness, we adopt the Threshold
rule (Parkes et al., 2001) in defining final payments, which minimizes the ex post regret
for truthful bidding across all budget-balanced payment rules, when holding the bids from
other participants fixed; see also the work of Milgrom (2007). 1 This is not to say that
1. We are not aware of the existence of mechanism design solutions for approximately efficient, but truthful
(i.e., with truthful bidding as a dominant-strategy equilibrium) and budget-balanced, sealed bid (i.e.,
non iterative) CEs. Nevertheless, it is true that any payment rules that are developed can be leveraged
directly within ICE and would allow ICE to inherit truthful bidding (i.e., revising TBBL bounds to
remain consistent with a bidder’s true valuation) in an ex post Nash equilibrium, just as can be achieved
in iterative Generalized Vickrey auctions (Mishra & Parkes, 2007).

34

ICE: An Iterative Combinatorial Exchange

incentive issues related to payment rules are not important in the design of successful CEs.
Rather, this is just orthogonal to the design of ICE and not the main focus of our work. We
do propose novel activity rules, which are themselves designed to mitigate opportunities for
strategic behavior.
We highlight the following technical contributions made in this work:
• The tree-based bidding language (TBBL) extends earlier CA bidding languages to support bidders that wish to simultaneously buy and sell, the specification of valuation
bounds, and the use of generalized ‘choose’ operators to provide more concise representations than OR* and LGB (Boutilier & Hoos, 2001; Nisan, 2006). TBBL can
be directly encoded within a mixed-integer programming (MIP) formulation of the
winner determination problem.
• Despite quoting prices on items and not bundles of items, ICE is able to converge
to the efficient trade with straightforward (i.e., non-strategic) bidders. Efficiency is
established through duality theory when prices are sufficiently accurate. Otherwise, a
direct proof based on reasoning about the upper and lower valuation bounds is always
available, even when the combinatorics of the instance preclude a duality-based proof.
• Preference elicitation is performed through the combination of two novel activity rules.
The first is a modified revealed-preference activity rule (MRPAR), and requires each
bidder to make precise which trade is most preferred in each round. The second is
a delta improvement activity rule (DIAR), and requires each bidder to refine his bid
to improve price accuracy or prove that no improvement is possible. When coupled
together these rules ensure that useful progress towards determining the efficient trade
is made in each round.
To summarize, there are three main reasons to prefer explicit value representations over
repeated demand reports in the context of an iterative CE: (a) a provisional allocation can
be computed from round 1, since both upper and lower bounds on value are available, (b) the
combinatorics of the domain can be directly handled in clearing the exchange and efficiency
is not limited by adopting simple (linear) prices, (c) proofs of (approximate) efficiency
are available by reasoning directly with bounds on valuations and despite adopting simple
(linear) prices.
The exchange is fully implemented in Java (with a C-based MIP solver). We present
scalability results showing performance across a wide number of bidders, goods and valuation complexity as well as benchmarks that provide a qualitative understanding of the
characteristics of our mechanism. Our experimental results (with straightforward bidders)
show that the exchange quickly converges to the efficient trade, taking an average of only
7 rounds for an example domain with 100 goods of 20 different types and 8 bidders with
valuation functions containing an average of 112 TBBL nodes. In this same domain, we
find that bidders can leave upwards of 62% of their maximum attainable value undefined
when the efficient trade is known, and 56% once final payments are determined, indicating
that bidders are able to leave large amounts of their value space unrefined. The exchange
terminates on these problems in an average of 8.5 minutes on a 3.2GHz dual-processor dualcore workstation with 8GB of memory. This includes the time for all winner determination,
pricing, and activity rules, as well as the time to simulate agent bidding strategies.
35

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

1.1 Related Work
Many ascending-price one-sided CAs are known in the literature (Parkes & Ungar, 2000a;
Wurman & Wellman, 2000; Ausubel & Milgrom, 2002; de Vries, Schummer, & Vohra,
2007; Mishra & Parkes, 2007). Direct elicitation approaches, in which bidders respond to
explicit queries about their valuations, have also been proposed for one-sided CAs (Conen &
Sandholm, 2001; Hudson & Sandholm, 2004; Lahaie & Parkes, 2004; Lahaie, Constantin, &
Parkes, 2005). Of particular relevance here are the ascending CAs that are designed to work
with simple prices on items (Dunford, Hoffman, Menon, Sultana, & Wilson, 2003; Kwasnica,
Ledyard, Porter, & DeMartini, 2005). In computing (approximately competitive) linear
prices, we generalize and extend these methods. Building on the work of Rassenti, Smith,
and Bulfin (1982), these earlier papers consider bids on bundles individually, and find prices
that are exact on winning bids and minimize the pricing error to losing bids. Generalizing
to the TBBL expressive language, we propose instead to compute prices that minimize the
worst-case pricing error over all bidders (rather than bids on individual trades), considering
the most preferred trade consistent with the TBBL bid of each bidder. As in the work
of Dunford et al. (2003) and Kwasnica et al. (2005) we incorporate additional tie-breaking
stages, in our case to lexicographically minimize the error and then to find prices that closely
approximate the provisional payments. This latter step appears to be novel.
Linear prices are important in practical applications. Such prices are adopted by the
FCC in their wireless spectrum auctions (Cramton, 2006), within clock auctions for the
procurement of electricity generation (Cramton, 2003), and are an essential part of the
proposed design for an airport landing slot auction at Laguardia airport (Ball et al. 2007).
Linear competitive equilibrium prices exist in two-sided markets with indivisibilities for
the assignment problem in which each agent will buy or sell a single item (but may be
interested in multiple different items) (Shapley & Shubik, 1972). But in general linear,
competitive equilibrium prices will not exist in combinatorial markets with nonconvexities;
see the work of Kelso and Crawford (1982), Bikhchandani and Mamer (1997), Bikhchandani
and Ostroy (2002), and O’Neill, Sotkiewicz, Hobbs, Rothkopf, and Stewart (2005) for related
discussions.
ICE has a “proxied” architecture in the sense that bidders submit and refine bounds
on TBBL bids directly to the exchange, with this information used to drive price dynamics
and ultimately to clear the exchange. Earlier work has considered proxied approaches,
but in application to one-sided ascending-price CAs (Parkes & Ungar, 2000b; Ausubel &
Milgrom, 2002). Given its focus on simple, linear prices, ICE can be considered to provide
a two-sided generalization of the clock-proxy design of Ausubel, Cramton, and Milgrom,
which has an initial stage of linear price discovery followed by a “best-and-final” sealedbid stage (Ausubel et al., 2006). Activity rules have been shown to be very important
in practice. For instance, the Milgrom-Wilson activity rule that requires a bidder to be
active on a minimum percentage of the quantity of the spectrum for which it is eligible
to bid is a critical component of the auction rules used by the FCC for wireless spectrum
auctions (Milgrom, 2004). ICE adopts a variation on the clock-proxy auction’s revealedpreference activity rule.
It is well known that exact efficiency together with budget balance is not possible because
of the Myerson-Satterthwaite impossibility result (Myerson & Satterthwaite, 1983). Given
36

ICE: An Iterative Combinatorial Exchange

this, Parkes et al. study sealed-bid combinatorial exchanges and introduced the Threshold
payment rule (Parkes et al., 2001); see the work of Milgrom (2007) and Day and Raghavan (2007) for a recent discussion. Double auctions in which truthful bidding is in a dominant strategy equilibrium are known for unit demand settings (McAfee, 1992) and also for
slightly more expressive domains (Babaioff & Walsh, 2005; Chu & Shen, 2007). However,
no truthful, budget-balanced mechanisms with useful efficiency properties are known for
the general CE problem.
Voucher-based schemes have been proposed as an alternative method to extend onesided CAs to exchanges (Kwerel & Williams, 2002). Such mechanisms collect all goods
from sellers and then run a one-sided auction in which sellers can “buy-back” their own
goods with vouchers used to provide a seller with a share of the revenue collected on their
own goods. Although voucher-based schemes can facilitate the design of exchanges through
one-sided auction technology, the ICE design offers the nice advantage of providing equal
and symmetric expressiveness to all participants. We are not aware of any previous studies
of fully expressive iterative CEs. Smith, Sandholm, and Simmons previously studied iterative CEs, but handle only limited expressiveness and adopt a direct-query based approach
with an enumerative internal data structure that does not scale (Smith et al., 2002). A
novel feature in their earlier design (not supported here) is item discovery, where the items
available to trade need not be known in advance. Earlier work has also considered sealed-bid
combinatorial exchanges for the purpose of contingent trades in financial markets, including
aspects of expressiveness and winner determination (Saatcioglu et al., 2001).
Several bidding languages for CAs have previously been proposed, arguably the most
compelling of which allow bidders to explicitly represent the logical structure of their valuation over goods via standard logical operators. We refer to these as “logical bidding
languages” (Nisan, 2006). Closest in generality to TBBL is the LGB language (Boutilier &
Hoos, 2001), which allows for arbitrarily nested levels, combining goods and trades by the
standard propositional logic operators, and also provides a k-of operator, used to represent
a willingness to pay for any k trades it quantifies over; see also the work of Rothkopf, Pekeč,
and Harstad (1998) for a restricted tree-based bidding language. In a key insight, Boutilier
specifies a MIP formulation for Winner Determination (WD) using LGB , and provides positive empirical performance results using a commercial solver, suggesting the computational
feasibility of moving to this more expressive logical language (Boutilier, 2002). TBBL shares
some structural elements with the LGB language but has important differences in its semantics. In LGB , the semantics are those of propositional logic, with the same items in an
allocation able to satisfy a tree in multiple places. Although this can make LGB especially
concise in some settings, the semantics that we propose provide representational locality, so
that the value of one component in a tree can be understood independently from the rest
of the tree.
1.2 Outline
Section 2 introduces preliminary concepts, defining the efficient trade and competitive equilibrium prices. Section 3 defines a sealed-bid CE, introducing TBBL and providing the MIP
that is used to solve winner determination. Section 4 extends TBBL to allow for valuation
bounds and defines the MRPAR and DIAR activity rules. The main theoretical results are
37

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

also described as well as our method to determine price feedback in each round. Section 5
gives a number of illustrative examples of the operation of ICE. Section 6 presents our main
experimental results. We conclude in Section 7. The Appendix provides an algorithm for
each of the two activity rules together with details on the bidding logic used by simulated
bidding agents.

2. Preliminaries
The basic environment considers a set of bidders, N = {1, . . . , n}, who are interested in
trading multiple units of distinct, indivisible goods, where the set of different types of goods
is denoted G = {1, . . . , m}. Each bidder has an initial endowment of goods and a valuation
for different trades. Let x0 = (x01 , . . . , x0n ) denote the initial endowment of goods, with
x0i = (x0i1 , . . . , x0im ) and x0ij ∈ Z+ to indicate the number of units of good type j ∈ G
initially held by bidder i ∈ N . A trade λ = (λ1 , . . . , λn ) denotes the change in allocation,
with λi = (λi1 , . . . , λimP
) and λ
ij ∈ Z denoting the change in the number of units of item j
P
to bidder i. Let M = i∈N j∈G x0ij denote the total supply in the exchange. We write
i ∈ λ to denote that bidder i is active in the trade, i.e., buys or sells at least one item.
2.1 The Efficient Trade
Each bidder has a value vi (λi ) ∈ R for his component of trade λ. This value can be positive
or negative, and represents the change in value between the final allocation x0i + λi and
the initial allocation x0i . The valuation and initial allocation information is private to each
bidder, and we assume that there are no externalities, so that each bidder’s value depends
′
only on his individual trade. We assume free disposal,
P so that vi (λi ) ≥ vi (λi ) for trade
′
′
λi ≥ λi , i.e., for which λij ≥ λij for all j. Let v(λ) = i vi (λi ).
Utility is modeled as quasi-linear, with ui (λi , p) = vi (λi ) − p for trade λi and payment
p ∈ R. This implies that bidders are modeled as being risk neutral and assumes that
there are no budget constraints. The payment, p, can be negative, indicating the bidder
may receive a payment for the trade. We use the term payoff interchangeably with utility.
Because of quasi-linearity, any Pareto optimal (i.e., efficient) trade will maximize the social
welfare, which is equivalent to the total increase in value to all bidders due to the trade.
Given an instance of the CE problem, defined by tuple (v, x0 ), i.e., a valuation profile
v = (v1 , . . . , vn ) and an initial allocation x0 = (x01 , . . . , x0n ), the efficient trade λ∗ , is defined
as follows:
Definition 1 Given CE instance (v, x0 ), the efficient trade λ∗ solves
X
max
vi (λi )
(λ1 ,...,λn )

s.t.

(1)

i

λij + x0ij ≥ 0,
X
λij = 0,

∀i, ∀j

(2)

∀j

(3)

i

λij ∈ Z
Constraints (2) ensure that no bidder sells more items than he has in his initial allocation.
By free disposal, we can impose strict balance in the supply and demand of goods at the
38

ICE: An Iterative Combinatorial Exchange

solution in constraints (3), i.e., we can allocate unwanted items to any bidder. We adopt
F(x0 ) to denote the set of feasible trades, given these constraints and given an initial
allocation x0 , and Fi (x0 ) for the set of feasible trades to bidder i. Note that valuation
function vi cannot be explicitly represented as a value for each possible trade to bidder i,
because the number of such trades scales as O(sm ), where s is the maximal number of units
of any item in the market and there are m different items. The TBBL language (introduced
in Section 3) leads to a concise formulation of the efficient trade problem as a mixed-integer
program.
The initial allocation x0i may be private to agent i. We assume throughout that bidders
are truthful in revealing this information, which we motivate by supposing that participants
cannot sell items that they do not actually own (or pay a suitably high penalty if they do).
2.2 Competitive Equilibrium Prices
Linear prices, π = (π1 , . . . , πm ), define
Pa price πj on each good so that the price to bidder
i on a trade λ is defined as pπ (λi ) = j λij πj = λi · π. Such prices play an important role
in ICE. Of particular interest is the set of competitive equilibrium prices:
Definition 2 Linear prices π are competitive equilibrium (EQ) prices for CE problem
(v, x0 ) if there is some feasible trade λ ∈ F(x0 ) such that:
vi (λi ) − pπ (λi ) ≥ vi (λ′i ) − pπ (λ′i ),

∀λ′i ∈ Fi (x0 ),

(4)

for every bidder i. We say that such a trade, λ, is supported by prices π.
Theorem 1 (Bikhchandani & Ostroy, 2002) Any trade λ supported by competitive equilibrium prices π is an efficient trade.
In practice, exact EQ prices are unlikely to exist. Instead, it is useful to define the
concept of approximate EQ prices and an approximately efficient trade:
Definition 3 Linear prices π are δ-approximate competitive equilibrium (EQ) prices for
CE problem (v, x0 ) and δ ∈ R≥0 , if there is some feasible trade λ ∈ F(x0 ) such that:
vi (λi ) − pπ (λi ) + δ ≥ vi (λ′i ) − pπ (λ′i ),

∀λ′i ∈ Fi (x0 ),

(5)

for every bidder i.
At δ-approximate EQ prices, there is some trade for which every bidder i is within δ ≥ 0
of maximizing his utility. Furthermore, say that trade λ is z-approximate if the total value
of the trade is within z of the total value of the efficient trade.
Theorem 2 Any trade λ supported by δ-approximate EQ prices π is a 2 min(M, n2 )δapproximate efficient trade.
Proof: Fix instance (v, x0 ) and consider (λ, π). For any trade λ′ 6= λ we have
X
X
[vi (λ′i ) − pπ (λ′i )],
[vi (λi ) − pπ (λi ) + δ] ≥
i∈λ∪λ′

i∈λ∪λ′

39

(6)

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

by δ-EQ prices and because
values and P
prices are zero for bidders P
that do notP
participate
P
′′
π λ′′ =
π (λ′ ) = 0 (since
π (λ ) =
p
p
p
in
a
trade.
We
have
′
i
i
i
i λi · π =
i
i∈λ∪λ′
P
Pi∈λ∪λ
P
P P ′′
0 )). Then,
λ′′ij = 0, with i λ′′ij = 0 for all j, for all λ′′i ∈ F(xP
= j πj i P
Pi j λij πjP
′
′
∗
∗
(λi ) + i∈λ∪λ′ δi ≥ i vi (λi ). Fix λ := λ , for efficient trade λ . Then, i vi (λi ) +
i viP
∆ ≥ i vi (λ∗i ), where
∆=

X

n
δi ≤ min(2A#(x0 ), n)δ ≤ min(2 min(M, n), n)δ = 2 min(M, )δ
2
′

(7)

i∈λ∪λ

Here A#(x0 ) is the maximal number of bidders that can trade in a feasible trade given x0 .
The second inequality follows because no more bidders can trade than there are number of
goods to trade or bidders in the market and thus A#(x0 ) ≤ min(M, n).


3. Step One: A TBBL-Based Sealed-Bid Combinatorial Exchange
We first flesh out the details for a non-iterative, TBBL-based CE in which each bidder
submits a sealed bid in the TBBL language.
Bidding language. The tree-based bidding language (TBBL) is designed to be expressive
and concise, to be entirely symmetric with respect to buyers and sellers, and to easily provide
for bidders that are both buying and selling goods; i.e., ranging from simple swaps to highly
complex trades. Bids are expressed as annotated bid trees, and define a bidder’s change in
value for all possible trades. The main feature of TBBL is that it has a general “intervalchoose” logical operator on internal nodes coupled with a rich semantics for propagating
values within the tree. Leaves of the tree are annotated with traded items and all nodes are
annotated with changes in values (either positive or negative). TBBL is designed such that
these changes in value are expressed on trades rather than the total value of allocations.
Examples are provided in Figures 1 and 2.
Consider bid tree Ti from bidder i. Let β ∈ Ti denote a node in the tree, and let
vi (β) ∈ R denote the value specified at node β (perhaps negative). Let Leaf (Ti ) ⊆ Ti be
the subset of nodes representing the leaves of Ti and let Child (β) ⊆ Ti denote the children
of node β. All nodes except leaves are labeled with the interval-choose operator ICyx (β).
Each leaf β is labeled as a buy or sell, with units qi (β, j) ∈ Z for the good j associated with
leaf β, and qi (β, j ′ ) = 0 otherwise. The same good j may simultaneously occur in multiple
leaves of the tree, given the semantics of the tree described below.
The IC operator defines a range on the number of children that can be, and must
be, satisfied for node β to be satisfied: an ICyx (β) node (where x and y are non-negative
integers) indicates that the bidder is willing to pay for the satisfaction of at least x and
at most y of his children. With suitable values for x and y the operator can include many
logical connectors. For instance: ICnn (β) on node β with n children is equivalent to an AND
operator; ICn1 (β) is equivalent to an OR operator; and IC11 (β) is equivalent to an XOR
operator.2
We say that the satisfaction of an ICyx (β) node is defined by the following two rules:
2. This equivalence implies that TBBL can directly express the XOR, OR and XOR/OR languages (Nisan,
2006).

40

ICE: An Iterative Combinatorial Exchange

R1 Node β with ICyx (β) may be satisfied only if at least x and at most y of its children
are satisfied.
R2 If some node β is not satisfied, then none of its children may be satisfied.
One can consider R1 as a “first pass” that defines a set of candidates for satisfaction. This
candidate set is then refined by R2. Besides defining how value is propagated, by virtue
of R2 our logical operators act as constraints on what trades are acceptable and provide
necessary and sufficient conditions.3
Given a tree Ti , the (change in) value of a trade λ is defined as the sum of the values on
all satisfied nodes, where the set of satisfied nodes is chosen to provide the maximal total
value. Let sat i (β) ∈ {0, 1} denote whether node β in tree Ti of bidder i is satisfied, with
sat i = {sat i (β), ∀β ∈ Ti }. For solution sat i to be valid for tree Ti and trade λi , written
sat i ∈ valid (Ti , λi ), then rules R1 and R2 must hold for all internal nodes β ∈ {Ti\Leaf (Ti )}
with ICyx (β):
x sat i (β) ≤

X

sat i (β ′ ) ≤ y sat i (β)

(8)

β ′ ∈Child(β)

Equation (8) enforces the interval-choose constraints, by ensuring that no more and no
less than the appropriate number of children are satisfied for any node that is satisfied. The
constraint also ensures that any time a node other than the root is satisfied, its parent is
also satisfied. We further require, for sat i ∈ valid (Ti , λi ), that the total increase in quantity
of each item across all satisfied leaves is no greater than the total number of units awarded
in the trade:
X

qi (β, j)sat i (β) ≤ λij ,

∀j ∈ G

(9)

β∈Leaf (Ti )

By free disposal, we allow here for a trade to assign additional units of an item overand-above that required in order to activate leaves in the bid tree. This works for sellers
as well as buyers: for sellers a trade is negative and this requires that the total number of
items indicated sold in the tree is at least the total number of items “traded away” from
the bidder in the trade.
Given these constraints, the total value of trade λi , given bid-tree Ti from bidder i, is
defined as the solution to an optimization problem:
vi (Ti , λi ) = max
sat i

X

vi (β)sat i (β)

(10)

β∈Ti

s.t. (8), (9)
Example 1 Consider an airline operating out of a slot-controlled airport that already owns
several morning landing slots, but none in the evening. In order to expand its business the
airline wishes to acquire at least two and possibly three of the evening slots. However, it
needs to offset the cost of this purchase by selling one of its morning slots. Figure 1 shows
a TBBL valuation tree for expressing this kind of swap.
41

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

AND

IC3
2

Buy 4pm $1

Buy 6pm $4

XOR

Buy 8pm $3

Sell 5am $-2

Sell 7am $-9

Sell 9am $-5

Figure 1: A simple TBBL tree for an airline interested in trading landing slots.
In working through numerous examples we frequently found it very cumbersome to
capture even simple trades in languages that specified values on allocations, as is the case
with all existing languages. Indeed, in earlier work, we demonstrate natural instances for
which TBBL is exponentially more concise than OR* and LGB (Cavallo et al. 2005). In
fact, TBBL’s conciseness is incomparable to OR* and LGB but can be extended in simple
ways to strictly dominate these earlier languages.
Winner Determination. The problem of determining an efficient trade given bids is
called the winner determination (WD) problem. The WD problem in CAs (and thus also
in CEs) is NP-hard (Rothkopf et al., 1998). The approach we adopt here is to formulate the problem as a mixed-integer program (MIP), and solve with branch-and-cut algorithms (Nemhauser & Wolsey, 1999). A similar approach has proved successful for solving
the WD problem in CAs (de Vries & Vohra, 2003; Boutilier, 2002; Sandholm, 2006).
Given some tree Ti , it is useful to adopt notation β ∈ λi to denote a node β ∈ Ti that is
satisfied by trade λi . We can now formulate the WD problem for bid trees T = (T1 , . . . , Tn )
and initial allocation x0 :
WD(T, x0 ) : max
λ,sat

XX
i

vi (β)sat i (β)

β∈Ti

s.t. (2), (3)
sat i ∈ valid (Ti , λi ),

∀i

sat i (β) ∈ {0, 1}, λij ∈ Z,
where sat = (sat 1 , . . . , sat n ). The tree structure is made explicit in this MIP formulation:
we have decision variables to represent the satisfaction of nodes and capture the logic
of the TBBL language through linear constraints; a related approach approach has been
considered in application to LGB (Boutilier, 2002). By doing this, there are O(nB + mn)
variables and constraints, where B is the maximal number of nodes in any bid tree. The
formulation determines the trade λ while simultaneously determining the value to all bidders
by activating nodes in the bid trees.
Payments. Given reported valuation functions v̂ = (v̂1 , . . . , v̂n ) from each bidder, the
Vickrey-Clarke-Groves (VCG) (e.g. Krishna, 2002) mechanism collects the following pay3. R1 naturally generalizes the approach taken in LGB , where an internal node is satisfied according to its
operator and the subset of its children that are satisfied. The semantics of LGB , however, treat logical
operators only as a way of specifying when “added value” (positive or negative) results from attaining
combinations of goods. Our use of R2 also imposes constraints on acceptable trades.

42

ICE: An Iterative Combinatorial Exchange

ments from each bidder:
pvcg,i = v̂i (λ∗i ) − (V (v̂) − V−i (v̂)),

(11)

where λ∗ is the efficient trade, V (v̂) is the reported value of this trade and V−i (v̂) is
the reported value of the efficient trade in the economy without bidder i, where v−i =
(v1 , . . . , vi−1 , vi+1 , . . . , vn ). Let us refer to ∆vcg,i = V (v̂) − V−i (v̂) as the VCG discount.
The problem with the VCG mechanism in the context of a CE is that it may run at a budget deficit with the total payments negative. An alternative payment method is provided
by the Threshold rule (Parkes et al., 2001):
pthresh,i = v̂i (λ∗i ) − ∆thresh,i ,

(12)

where the discounts ∆thresh,i P
are picked to minimize maxi (∆vcg,i − ∆thresh,i ) subject to
∆thresh,i ≤ ∆vcg,i for all i and i ∆thresh,i ≤ V (v̂). Threshold payments are exactly budget
balanced and minimize the maximal deviation from the VCG outcome across all balanced
rules.
Bidder 1
Bidder 2

AND

IC3
1
XOR

Buy C $6
Buy A $10

Sell A $-4

Buy B $5

Sell C $-3

Sell B $-8

Figure 2: Two bidders and three items {A, B, C}. The efficient trade is for bidder 1 to sell
A and buy C.

Example 2 Consider the two bidders in Figure 2. Bidder 1 will potentially sell one of his
items (A or B) if he can get Bidder 2’s item, C, at the right price. Bidder 2 is interested in
buying one or both of Bidder 1’s items and also selling his own item. We consider each of
the possible trades: If Bidder 1 trades A for C he gets $2 of value and Bidder 2 gets $7. If
Bidder 1 trades B for C he gets $-2 of value and Bidder 2 gets $2. And if no trade occurs
both bidders get $0 value. Therefore the efficient trade is to swap A for C.
Because the efficient trade creates a surplus of $9 and removing either bidder results in
the null trade, both bidders have a Vickrey discount of $9. Thus if we use VCG payments,
Bidder 1 pays $2-$9=$-7 and Bidder 2 pays $7-$9=$-2 and the exchange runs at a deficit.
The Threshold payment rule chooses payments that minimally deviate from VCG while
maintaining budget balance. This minimization reduces the discounts to $4.50, and thus
Bidder 1 pays $2-$4.50=$-2.50 and Bidder 2 pays $7-$4.50=$2.50.

4. Step Two: Making the Exchange Iterative
Having defined a sealed-bid, TBBL-based exchange we can now modify the design to make
it iterative. Rather than provide an exact valuation for all interesting trades, a bidder
43

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Figure 3: ICE system overview
annotates a single TBBL tree with upper and lower bounds on his valuation. The ICE
mechanism then proceeds in rounds, as illustrated in Figure 3.
ICE is a proxied design in which each bidder has a proxy to facilitate his valuation
refinement. In each round, a bidder responds to prices in interacting with his proxy agent
in order to tighten the bounds on his TBBL tree and meet the activity rules. The exchange
chooses a provisional valuation profile (denoted v α = (v1α , . . . , vnα ) in the figure), with the
valuation viα for each bidder picked to fall within the bidder’s current valuation bounds
(and to tend towards the lower valuation bound as progress is made towards determining
the final trade). Then, the exchange computes a provisional trade λα and checks whether
the conditions for moving to a last-and-final round are satisfied. Approximate equilibrium
prices are then computed based on valuation profile v α and trade λα and a new round
begins. In the last-and-final round, the final payments and the trade are computed in terms
of lower valuations; the semantics are that these lower bounds guarantee that a bidder
is willing to pay at least this amount (or receive a payment of this amount) in order to
complete the trade.
Let v i and v i denote the lower and upper valuation functions reported by bidder i in
a particular round of ICE, and adopt WD(v) to denote the WD problem for valuation
profile v = (v1 , . . . , vn ). ICE is parameterized by a target approximation error ∆∗ ∈ (0, 1],
which requires that the total value from the optimal trade λ given the current lower-bound
valuation profile (i.e., λ solves WD(v)) is close to the total value from the efficient trade λ∗i :
P
v(λ)
∗
i vi (λi )
P
EFF(λ) =
∗ ) = v(λ∗ ) ≥ ∆
v
(λ
i i i

(13)

However, the true valuation v and thus the trade λ∗ are uncertain within ICE and thus we
will later introduce techniques to estblish this bound.
In each round, ICE goes through the following steps:
1. If this is the last-and-final round, then implement the trade that solves WD(v) and
collect Threshold payments defined on valuations v. STOP.
ELSE,
44

ICE: An Iterative Combinatorial Exchange

2. Solve WD(v) to obtain λ. Use valuation bounds and prices to determine a lowerbound, ω eff , on the allocative efficiency EFF(λ) of λ. If ω eff ≥ ∆∗ then the next
round will be designated the last-and-final round.
3. Set α ∈ [0, 1], with α tending to 1 as ω eff tends to 1, and provisional valuation profile
v α = (v1α , . . . , vnα ), where viα (λi ) = αv i (λi )+(1−α)v i (λi ), expressed with a TBBL tree
in which the value on node β ∈ Ti is viα (β) = αv i (β) + (1 − α)v i (β).
4. Solve WD(v α ) to find provisional trade λα , and determine Threshold payments for
provisional valuation profile, v α .
α
5. Compute linear prices, π ∈ Rm
≥0 , that are approximate CE prices given valuations v
and trade λα , breaking ties to best approximate the provisional Threshold payments
and finally to minimize the difference in price between items.

6. Report (λαi , π) to each bidder i ∈ N , and whether or not the next round is last-andfinal.
In transitioning to the next round, the proxy agents are responsible for guiding bidders
to make refinements to their lower- and upper-bound valuations in order to meet activity
rules that ensure progress towards the efficient trade across rounds. In what follows, we (a)
extend TBBL to capture lower and upper valuation bounds, (b) describe our two activity
rules, (c) explain how we compute price feedback, (d) provide our main theoretical results.
In developing theoretical and experimental results about ICE we assume straightforward
bidders, so that bidders refine upper and lower bounds on valuations to keep their true
valuation consistent with the bounds.
Extending TBBL. We first extend TBBL to allow bidder i to report a lower and upper
bound (v i (β), v i (β)) on the value of each node β ∈ Ti , which in turn induces valuation
functions v i (Ti , λi ) and v i (Ti , λi ), using the exact same semantics as in (10). The bounds
on a trade can be interpreted as bounding the payment that the bidder considers acceptable.
The bidder commits to complete the trade for a payment less than or equal to the lowerbound and to refuse to complete a trade for any payment greater than the upper-bound. The
exact value, and thus true willingness-to-pay, remains unknown except when v i (β) = v i (β)
on all nodes. We say that bid-tree Ti for bidder i is well-formed if v i (β) ≤ v i (β) for all
nodes β ∈ Ti . In this case we also have v i (Ti , λi ) ≤ v i (Ti , λi ) for all trades λi . We refer
to the difference v i (β) − v i (β) as the value uncertainty on node β. The efficient trade can
often be determined with only partial information about bidder valuations. Consider the
following simple variant on Example 2:
Example 3 The structure of the bidders’ trees in Figure 4 is the same as in Example 2 but
the nodes are annotated with bounds. Let x ∈ [3, 8] denote Bidder 1’s true value for “buy
C” and y ∈ [−4, −1] denote Bidder 2’s true value for “sell C.” The three feasible trades
are: (1) trade A and C, (2) trade B and C, (3) no trade. The first trade is already provably
efficient. Fixing x and y, its minimal value is −4+9+x−y and this is at least −5+7+x−y,
the value of the second trade. Moreover, its worst-case value is −4 + 9 + 3 − 4 ≥ 0, the value
of the null trade.
45

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Bidder 1
Bidder 2

AND

IC3
1
XOR

Buy C $8
$3
Buy A $12
$9

Sell A $-3
$-4

Sell B

Buy B $7
$3

Sell C $-1
$-4

$-5
$-10

Figure 4: Two bidders, each with partial value information defined on their bid tree. One
can already prove that the efficient trade is for bidder 1 to sell A and buy C.
4.1 Activity Rules
Activity rules are used to guide the preference elicitation process in each round of ICE.
Without an activity rule, a rational bidder would likely wait until the very last moment to
revise his valuation information, free-riding on the price discovery enabled by the bids of
other participants. If every bidder were to behave this way then the exchange would reduce
to a sealed-bid mechanism and lose its desirable properties.4 Thus, activity rules are critical
in mitigating opportunities for strategic behavior.5
ICE employs two activity rules. In presenting our activity rules, we will not specify the
explicit consequences of failing to meet an activity rule. One simple possibility is that the
default action is to automatically set the upper valuation bound on every node in a bid
tree to the maximum of the “provisional price on a node”6 and the lower-bound value on
that node. This is entirely analogous to when a bidder in an ascending-clock auction stops
bidding at a price: he is not permitted to bid at a higher price again in future rounds.
Modified Revealed-Preference Activity Rule (MRPAR). The first rule, MRPAR,
is based on a simple idea. We require bidders to refine their valuation bounds in each round,
so that there is some trade that is optimal (i.e., maximizes surplus) for the bidder given the
current prices and for all possible valuations consistent with the bounds. MRPAR is loosely
based around the revealed-preference based activity rule, advocated for the clock-proxy
auction in a one-sided CA (Ausubel et al., 2006).
Let vi′ ∈ Ti for TBBL tree Ti denote that valuation vi′ is consistent with the value bounds
in the tree. If the bounds are tight everywhere, then vi′ is exactly the valuation function
defined by tree Ti . A simple variant (RPAR), requires that there is enough information in
valuation bounds to establish that one trade is weakly preferred to all other trades at the
prices, i.e.
∃λ̆i ∈ Fi (x0 ) s.t. vi′ (λ̆i ) − pπ (λ̆i ) ≥ vi′ (λ′i ) − pπ (λ′i ),

∀vi′ ∈ Ti , ∀λ′i ∈ Fi (x0 )

(RPAR)

Note that a bidder can always meet this rule by defining an exact valuation v̂i and tight
value bounds on every node in his bid tree; in this case, trade λ̆i ∈ arg maxλi ∈Fi (x0 ) [v̂i (λi ) −
4. This problem has been evocatively described as the “snake in the grass” problem. See Kwerel’s forward
in Milgrom’s book (2004).
5. There is no conflict here with our assumption about straightforward bidding: we design for the strategic
case despite assuming straightforward bidding to provide for tractable theoretical and experimental
analysis; moreover, the presence of activity rules helps to motivate straightforward bidding.
6. The provisional price on a node is defined as the minimal total price across all feasible trades for which
the subtree rooted at the node is satisfied.

46

ICE: An Iterative Combinatorial Exchange

pπ (λi )] satisfies RPAR. We say that prices π are strict EQ prices for (v α , λα ) when:
viα (λαi ) − pπ (λαi ) > viα (λ′i ) − pπ (λ′i ),

∀λ′i ∈ Fi (x0 ) \ {λαi } ,

(14)

for every bidder i ∈ N .
Theorem 3 If prices π are strict EQ prices for provisional valuation profile v α and trade
λα , and every bidder i retains viα in his bid tree after meeting RPAR, then trade λα is
efficient when all bidders are straightforward.
Proof: Fix bidder i. Let λ̆i denote the trade that satisfies RPAR. Because viα is consistent
with the revised bid tree of bidder i, we have:
viα (λ̆i ) − pπ (λ̆i ) ≥ viα (λ′i ) − pπ (λ′i ),

∀λ′i ∈ Fi (x0 ).

(15)

Moreover, we must have λ̆i = λαi , because viα (λαi ) − pπ (λαi ) > viα (λ′i ) − pπ (λ′i ) by the
strictness of prices. Instantiating RPAR with this trade, and with true valuations vi ∈ Ti
(since bidders are straightforward), we have:
vi (λαi ) − pπ (λαi ) ≥ vi (λ′i ) − pπ (λ′i ),

∀λ′i ∈ Fi (x0 ),

(16)

from which prices pπ are EQ prices with respect to true valuations. The efficiency claim
then follows from the welfare theorem, Theorem 1.

In particular, the provisional trade is efficient given strict EQ prices when every bidder
meets the rule without modifying his bounds in any way. Strict EQ prices are required to
prevent problems involving ties:
Buyer

Seller

XOR

Buy A $8

OR

$4
Buy B $4
$2

$-6
Sell A $-9
$-20

$-2
Sell B $-6
$-10

Figure 5: An Example to illustrate the failure of the simple RPAR rule without strict EQ
prices. True values are shown in bold and are such that the efficient outcome is
no trade.
Example 4 The TBBL trees shown in Figure 5 will have no trade occur at the truthful
valuation (which is indicated in bold between the value bounds). However, suppose α = 0 so
that at the provisional valuations it is efficient for A to be traded. Prices π = (6, 2) are EQ
(but not strict EQ) prices given v α and λα , with the buyer indifferent between buying A and
buying B and the seller indifferent between selling A, selling A and B, or making no sale.
The buyer passes RPAR without changing his bounds because the bounds already establish
that he (weakly) prefers A to B, and prefers A to no trade, at all possible valuations.
Similarly, the seller passes RPAR without changing his bounds because the bounds establish
that he weakly prefers no trade to selling any combination of A and B given the current
prices. Thus, we have no activity even though the current provisional trade is inefficient.
47

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

In order to better handle these sorts of ties, we slightly strengthen RPAR to modified
RPAR (MRPAR), which requires that there exists some λ̆i ∈ Fi (x0 ) such that
θiπ (λ̆i , λ′i , vi′ ) ≥ 0,
and either λ̆i =

λαi

or

θiπ (λ̆i , λαi , vi′ )

> 0,

∀vi′ ∈ Ti , ∀λ′i ∈ Fi (x0 )
∀vi′

∈ Ti .

(17)
(18)

where θiπ (λi , λ′i , vi′ ) = vi′ (λi ) − pπ (λi ) − (vi′ (λ′i ) − pπ (λ′i )) denotes the profit to bidder i for
trade λi over λ′i given vi′ and prices π. (17) is RPAR and the additional requirements enforce
that the satisfying trade λ̆i is either λαi or strictly preferred to λαi . This need to show a
strict preference over λαi prevents the deadlock shown in Example 4. The seller has shown
only a weak preference for not trading over selling A. With MRPAR, the seller must also
show that he strictly prefers λ̆i , in this case by reducing the upper-bounds on both A and
B, thus ensuring progress.
The actual rule adopted in ICE is δ-MRPAR, parameterized with accuracy parameter
δ ≥ 0, and providing a relaxation of MRPAR which is useful even when there are no exact
EQ prices defined with respect to (λα , v α ) in some round.
Definition 4 Given provisional trade λα , linear prices π, and accuracy parameter δ ≥ 0,
δ-MRPAR requires that every bidder i refines his value bounds so that his TBBL tree Ti
satisfies:
θiπ (λαi , λ′i , vi′ ) ≥ −δ,

∀vi′ ∈ Ti , ∀λ′i ∈ Fi

(19)

or, that there is some λ̆i ∈ Fi (x0 ) such that
θiπ (λ̆i , λ′i , vi′ ) ≥ 0,

∀vi′ ∈ Ti , ∀λ′i ∈ Fi (x0 )

(20)

θiπ (λ̆i , λαi , vi′ )

∀vi′

(21)

> δ,

∈ Ti

It is a simple matter to check that δ-MRPAR reduces to MRPAR for δ = 0. Phrasing
the description to allow for the rule to be interpreted with and without the δ relaxation,
δ-MRAPR requires that each bidder must adjust his valuation bounds to establish that the
provisional trade is [within δ of ] maximizing profit for all possible valuations (19), or some
other trade satisfies RPAR (20) and is strictly preferred [by at least δ] to the provisional
trade (21). Just as for RPAR, one can show that a bidder can always meet δ-MRPAR (for
any δ) by defining an exact valuation.7
Lemma 1 If every bidder i meets δ-MRPAR without precluding viα from his updated bid
tree, and prices π are δ-approximate EQ prices with respect to provisional valuation profile
v α and trade λα , and bidders are straightforward, then the provisional trade is a
2 min(M, n2 )δ-approximate efficient trade.
7. Let vi denote this valuation. If δ-MRPAR is not satisfied via (19) then λ̆i ∈ arg maxλi ∈Fi (x0 ) [vi (λi ) −
pπ (λ)] will satisfy δ-MRPAR. This satisfies (20) by construction. Now, let λ′i denote the trade with
π
α
π
′
π
′
α
π
α
vi (λ′i ) − pπ (λ′i ) > vi (λα
i ) − p (λi ) + δ. We have vi (λ̆i ) − p (λ̆i ) ≥ vi (λi ) − p (λi ) > vi (λi ) − p (λi ) + δ,
and (21).

48

ICE: An Iterative Combinatorial Exchange

Proof: Fix bidder i. By δ-EQ, we have θiπ (λαi , λ′i , viα ) ≥ −δ for all λ′i ∈ Fi (x0 ). Consider
any λ̆i 6= λαi . Because viα remains in the bid tree, we must have θiπ (λ̆i , λαi , viα ) ≤ δ and
δ-MRPAR cannot be satisfied via (20) and (21). Therefore, δ-MRPAR is satisfied for every
bidder via (19) and with provisional trade λα the satisfying trade. Therefore we prove that
prices, π, are δ-approximate EQ prices for all valuations, and including the true valuation
since bidders are straightforward and this is within their bounds. The efficiency of the trade
follows from Theorem 2.

This in turn provides a simple proof for the efficiency of ICE when approximate CE
prices exist upon termination. Suppose that ICE is defined to terminate as soon as prices
are δ-accurate and v α is retained in the bid tree by all bidders in meeting the activity rule,
or when quiescence is reached and no bidder refines his bounds in meeting the rule. In this
variation, the provisional trade λα is the trade finally implemented.
Theorem 4 ICE with δ-MRPAR is 2 min(M, n2 )δ-efficient when prices are δ-accurate with
respect to (v α , λα ) upon termination and bidders are straightforward.
Proof: When ICE terminates either (a) prices are δ-accurate and v α is retained in the
bid tree by all bidders and we can appeal directly to Lemma 1, or (b) no bidder refines his
bounds in meeting δ-MRPAR, in which case viα remains in the space of valuations consistent
with the bid tree for each bidder.

We also have the following simple corollary, which considers the property of ICE for a
domain in which approximately accurate EQ prices exist:
Corollary 1 ICE with δ-MRPAR is 2 min(M, n2 )δ-efficient when δ-accurate competitive
equilibrium prices exist for all valuations in the valuation domain and when all bidders are
straightforward.
Specializing to domains in which exact EQ prices exist (e.g., for unit-demand preferences
as in the assignment model of Shapley and Shubik, 1972; see also the work of Bikhchandani
and Mamer, 1997) then ICE with MRPAR is efficient for straightforward bidders.
XOR

Buy A $8
$2

Buy B $5
$4

XOR

Buy A $8=v
$2=x

(a) Passes δ-MRPAR

Buy B $8=y
$4=w

(b) Fails δ-MRPAR

Figure 6: δ-MRPAR where the provisional trade is “Buy A”, πA = 3, πB = 4 and δ = 2
Example 5 To illustrate the δ-MRPAR rule consider a single bidder with a valuation tree
as in Figure 6(a). Suppose the provisional trade λαi allocates A to the bidder, and with prices
πA = 3, πB = 4 and δ = 2. Here the bidder has satisfied δ-MRPAR because the guaranteed
$2-$3=$-1 payoff from A is within δ of the possible $5-$4=$1 payoff from B. Now consider
Figure 6(b), with a relaxed upper-bound on “buy B” of $8. Now the bidder fails δ-MRPAR
because the guaranteed $-1 payoff from A is not within δ of the possible payoff from B of
$8-$4=$4. Let [x, v] and [w, y] denote the lower and upper bounds, on “buy A” and “buy
B” respectively, as revised in meeting the rule. To pass the rule, the bidder has two choices:
49

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

• Demonstrate λαi is the best response. To do so the bidder will need to adjust x and y
to make x − 3 ≥ y − 4 − 2 ⇒ y − x ≤ 3; e.g., values x = $2, y = $5 solve this, as in
Figure 6(a), as do many other possibilities.
• OR Demonstrate that another trade (e.g., “buy B”) is more than $2 better than λαi ,
i.e., w − 4 > v − 3 + 2 ⇒ w − v > 3, and “buy B” is weakly better than the null trade,
i.e., w − 4 ≥ 0. For instance, if the bidder’s true values are vA = $3, vB = $8 then
x ≤ 3 ≤ v and w ≤ 8 ≤ y and the rule cannot be satisfied in the first case. But, the
buyer can establish that “buy B” is his best-response, e.g., by setting v = $4, w = $7,
or v = $3, w = $6.
Remark: Computation and Bidder Feedback. The definition of MRPAR naively
suggests that checking for compliance requires explicitly considering all valuations vi′ ∈ Ti
and all trades λ′i ∈ Fi (x0 ). Fortunately, this is not necessary. We present in the Appendix a
method to check MRPAR given prices π, provisional trade λαi and bid tree Ti by solving three
MIPs. Moreover, we explain that the solution to these MIPs also provides nice feedback for
bidders. ICE can automatically identify a set of nodes at which a bidder needs to increase
his lower bound and a set of nodes at which a bidder needs to decrease his upper bound in
meeting MRPAR.
Delta Improvement Activity Rule (DIAR). With only δ-MRPAR, it is quite possible
for ICE to get stuck, with all bidders satisfying the activity rule without changing their
bounds, but with the prices less than δ accurate (with respect to (λα , v α )). Therefore, we
need an activity rule that will continue to drive a reduction in value uncertainty, i.e., the
gap between upper bound values and lower bound values, even in the face of inaccurate
prices, and ideally in a way that remains “price-directed” in the sense of using prices to
determine which trades (and in turn which nodes in TBBL trees) each bidder should be
focused on.
We introduce for this purpose a second (and novel) activity rule (DIAR), which fills
this role by requiring bidders to reveal information so as to improve price accuracy and,
in the limit, full information on the nodes that matter. Defined this way, the DIAR rule
very nicely complements the δ-MRPAR rule. Because we can establish the efficiency of
the provisional trade directly via the valuation bounds, as we will see in Section 4.3, we
do not actually need fully accurate prices in order to close the exchange. Thus, the DIAR
rule does not imply that bidders will reveal full information. Rather, the presence of DIAR
ensures both good performance in practice as well as good theoretical properties. In our
experiments we enable DIAR in all rounds of ICE, and it fires in parallel with δ-MRPAR.
In practice, we see that most of the progress in refining valuation information occurs due to
δ-MRPAR, and that all the progress in early rounds occurs due to δ-MRPAR. Experimental
support for this is provided in Section 6.8
Before providing the specifics of DIAR, we identify a node β ∈ Ti in the bid tree of
bidder i as interesting for some fixed instance (v, x0 ), when the node is satisfied in some
feasible trade. We have the following simple lemma:
8. In a variation on the way ICE is defined, DIAR could be used only in rounds in which the price error for
the provisional valuation and trade is greater than the error associated with δ-MRPAR. This is because
δ-MRPAR is sufficient for approximate efficiency when prices are accurate enough.

50

ICE: An Iterative Combinatorial Exchange

Lemma 2 If there is no value uncertainty on any interesting nodes in the bid trees of any
biders, and bidders are straightforward, then λα is efficient.
Proof: No value uncertainty and thus exact information about the value on all interesting
nodes implies that the difference in value is exactly known between all pairs of feasible
trades because for all uninteresting nodes, either the node is never satisfied in any trade
(and thus its value does not matter) or the node is satisfied in every trade and thus its actual
value does not matter in defining the difference in value between pairs of trades. Only the
difference in value between pairs of trades is important in determining the efficient trade.

DIAR focuses a bidder in particular on interesting nodes that correspond to trades for
which the pricing error is large, and where this error could still be reduced by refining the
valuation bounds on the node. Given prices π and provisional trade λαi , the main focus of
k
DIAR is the following upper-bound δ i , on the amount by which prices π might misprice
some trade λki ∈ Fi (x0 ) with respect to bidder i’s true valuation:
k

δ i = max
[vi′ (λki ) − pπ (λki ) − (vi′ (λαi ) − pπ (λαi ))]
′
vi ∈Ti

(22)

We call this the “DIAR error on trade λki ”, and note that it depends on the current
prices as well as the current bid tree and provisional trade, but not the true valuation which
is unknown to the center. The DIAR error provides an upper bound on the additional payoff
that the bidder could achieve from trade λki over trade λαi . If we order trades, λ1i , λ2i , . . ., so
1
that λ1i has maximal DIAR error, then δ i ≥ δi , where δi = maxλ′i ∈Fi (x0 ) [viα (λ′i ) − pπ (λ′i ) −
(viα (λαi ) − pπ (λαi ))] is the pricing error with respect to the provisional trade and provisional
valuation profile. This is the error that the pricing algorithm is designed to minimize in
each round, and the same error that is used in Theorem 2 in reference to δ-accurate prices.
Thus, we see that the maximal DIAR error also bounds the amount by which prices are
1
approximate EQ prices, and that if δ i ≤ 0 for all bidders i then the current prices π are
exact EQ prices with respect to (λα , v α ).
To satisfy DIAR a bidder must reduce the DIAR error on the trade with the largest error
for which the error can be reduced (some error may just be intrinsic given the current prices
and not because of uncertainty about the bidder’s valuation), or establish by providing exact
value information throughout the tree that none of the DIAR error on any trades is due
to value uncertainty. Figure 7 illustrates the difference between MRPAR and DIAR. A
bidder can satisfy MRPAR by making it clear that the lower bound on payoff from some
trade is greater than the upper bound on all other trades, but still leave large uncertainty
about value. DIAR requires that a bidder also refine this upper bound if it is on a node
that corresponds to a trade for which the DIAR error (and thus potentially the actual
approximation in prices) is large. The rule is illustrated in Figure 8.
DIAR is parameterized by some ǫ ≥ 0. We refer to the formal rule as ǫ-DIAR:
Definition 5 To satisfy ǫ-DIAR given provisional trade λαi and prices π, the bidder must
modify his valuation bounds to:
(a) reduce the DIAR error on some trade, λji ∈ Fi (x0 ), by at least ǫ and
k

(b) prove that error δ i cannot be improved by ǫ for all trades λki ∈ Fi (x0 ) for 1 ≤ k < j,
k

or (c) establish that δ i cannot be improved by ǫ on any trade λki ∈ Fi (x0 ).
51

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Figure 7: Stylized effect of MRPAR and
DIAR on the bounds of the λαi
and λ∗i trades

Figure 8: Trades for bidder i, ordered with
DIAR error reducing from left to
right. The bidder must reduce,
by at least ǫ, the DIAR error
on the trade with the greatest
error for which this is possible
and prove (via valuation bounds)
that it is impossible to improve
by ǫ any trades with larger error.

In particular, even if the bidder is in case (c) above, he will still be forced to narrow his
bounds and progress will be made towards bounding efficiency. In practice, we define the ǫ
parameter to be large at the start and smaller in later rounds.
XOR

$6
Buy A $4
$2

$8
Buy B $5
$3

XOR

$10
Buy C $10
$4

$6
Buy A $4
$2

(a) Fails DIAR

$7
Buy B $5
$3

Buy C

$10
$10
$9.01

(b) Passes DIAR

Figure 9: Respecting DIAR where the provisional trade is “Buy A”, πA = 4, πB = 5, πC = 6
and ǫ = 1
Example 6 Consider the tree in Figure 9(a) when the provisional trade is “buy A”, prices
π = ($4, $5, $6) and DIAR parameter ǫ = 1. The DIAR error on each trade, defined via
(22), and listed in decreasing order, are:
1

C → δ = ($10 − $6) − (−$2) = $6
2

B → δ = ($8 − $5) − (−$2) = $5
3

∅ → δ = ($0 − $0) − (−$2) = $2
4

A → δ = ($2 − $4) − (−$2) = $0,
where −$2 = $2 − $4 is the worst-case profit from the provisional trade. Now, we see that
1
δ cannot be made smaller by lowering the upper-bound on leaf “buy C” because this bound
52

ICE: An Iterative Combinatorial Exchange

is already tight against the truthful value of $10. Instead the bidder must demonstrate that
a decrease of ǫ = 1 is impossible by raising the lower bound on “buy C” to 9.01. However
2
δ can be decreased by ǫ = 1, by reducing the upper-bound on “buy B” from 8 to 7, giving
us the tree in Figure 9(b).
Lemma 3 When ICE incorporates DIAR, a straightforward bidder must eventually reveal
complete value information on all interesting nodes in his bid tree as ǫ → 0.
Proof: Fix provisional trade λαi and consider trade, λ1i ∈ Fi (x0 ) 6= λαi , with the maximal
DIAR error. Continue to assume straightforward bidders. Recall that vi (β) denotes a
bidder’s true value on node β in his TBBL tree. By case analysis on nodes β ∈ Ti , meeting
the DIAR rule on this trade as ǫ → 0 requires:
(i) Nodes β ∈ λ1i \ λαi . Decrease the upper-bound to vi (β), the true value, to reduce the
error. Increase the lower-bound to vi (β) to prove that further progress is not possible.
(ii) Nodes β ∈ λαi \ λ1i . Increase the lower-bound to vi (β), the true value, to reduce
the error. Decrease the upper-bound to vi (β) to prove that further progress is not
possible.
(iii) Nodes β ∈ λαi ∩ λ1i . No change is required.
(iv) Nodes β ∈
/ λ1i ∪ λαi . No change is required.
Continue to fix some λαi , and consider now the impact of DIAR as ǫ → 0 and as the
rule is met for successive trades, moving from λ1i to λ2i and onwards. Eventually, the value
bounds on all nodes β ∈
/ λαi but in at least one other feasible trade are driven to truth by
(i), and the value bounds on all nodes β ∈ λαi but not in at least one other feasible trade
are driven to truth by (ii). Noting that the null trade is always feasible, the bidder will
ultimately reveal complete value information except on nodes that are not satisfied in any
feasible trade.

Putting this together we have the following simple theorem, which considers the convergence property of ICE when DIAR is the only activity rule.
Theorem 5 ICE with the ǫ-DIAR rule will terminate with the efficient trade when all
bidders are straightforward and as ǫ → 0.
Proof: Immediate by Lemma 2 and Lemma 3.

In practice, we use both δ-MRPAR and DIAR and the role of DIAR is to ensure convergence in instances for which there do not exist good, supporting EQ prices. The use
of DIAR does not lead, in any case, to full revelation of bidder valuations because we can
prove efficiency directly in terms of valuation bounds on different trades (see Section 4.3).
Remark: Computation and Bidder Feedback. We present in the Appendix a method
to check ǫ-DIAR given prices π, provisional trade λαi , the bidder’s bid tree from the past
round and proposed new bid tree by solving two MIPs. Moreover, the solution to these
MIPs also provides nice feedback for bidders. ICE can automatically identify the trade,
and in turn the corresponding nodes in the bid tree, for which the bidder must provide
more information.
53

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

4.2 Generating Linear Prices
Given the provisional trade λα , provisional valuations v α , and given that provisional payments have also been determined (according to the payment rule, such as Threshold,
adopted in the exchange), approximate clearing prices are computed in each round according to the following rules:
I: Accuracy (ACC). First, we compute prices that minimize the maximal error in the
best-response constraints across all bidders.
II: Fairness (FAIR). Second, we break ties to prefer prices that minimize the maximal
deviation from provisional payments across all bidders.
III: Balance (BAL). Third, we break ties to prefer prices that minimize the maximal
difference in price across all items.
Taken together, these steps are designed to promote the informativeness of prices in
driving progress across rounds. Balance is well motivated in domains where items are more
likely to be similar in value than dissimilar, preferring prices to be similar across items
and rejecting extremal prices. Note that these prices may ascend or descend from round
to round – but that they will in general tend towards increasing accuracy, as we shall see
experimentally in Section 6.
Buyer

Seller

AND

Buy A $8

AND

Buy B $8

Sell A $-6

Sell B $-6

Figure 10: A simple example to illustrate pricing. ACC prices AB between $12 and $16,
FAIR narrows this to $14 and BAL requires A = $7, B = $7
Example 7 Consider the example in Figure 10 with one buyer interested in buying AB and
one seller interested in selling AB. Here the buyer’s and seller’s values for each item are
8 and -6 respectively. The efficient outcome given these values is for the trade to complete.
ACC requires 12 ≤ πA +πB ≤ 16, and thus allows a range of prices. The Threshold payment
splits the difference, so that the buyer pays 14 to the seller and so FAIR adds the constraint
πA + πB = 14. Finally, BAL requires πA = πB = 7.
Each of the three stages occur in turn. In the interest of space, here we only present the
basic formulation of the Accuracy stage: We define maximally accurate EQ prices by first
considering the following LP:

s.t.

∗
δacc
= min δacc
π,δacc
X
X
πj λ′ij ≤ viα (λαi ) −
πj λαij + δacc ,
viα (λ′i ) −
j

j

δacc ≥ 0,
πj ≥ 0,

∀j ∈ G
54

∀i, ∀λ′i ∈ Fi (x0 )

(23)

ICE: An Iterative Combinatorial Exchange

These prices minimize the maximal loss in payoff across all bidders for trade λα compared to the trade that a bidder would most prefer given provisional valuation v α , i.e.,
minimize the maximal value of θiπ (λ∗i , λαi , viα ), where λ∗i = arg maxλi ∈Fi (x0 ) [viα (λi ) − pπ (λi )].
Prices that solve this LP are then refined lexicographically, fixing the worst-case pricing error (ACC) and then working down to try to additionally minimize the next largest pricing
error and so on. Given maximally accurate prices, this then triggers a series of lexicographical refinements to best approximate the payments (FAIR) without reducing the pricing
accuracy, and eventually a series of lexicographical refinements to try to maximally balance prices across distinct items (BAL). In addition to further improving the quality of the
prices, this process also ensures uniqueness of prices.
Each of the Accuracy, Fairness and Balance problems have an exponential number of
constraints because the price accuracy constraints (23) (which are carried forward into
the subsequent stages) are defined over all trades λ′i ∈ Fi (x0 ) and all bidders i. It is
therefore infeasible to even write these problems down. Rather than solve them explicitly,
we use constraint generation (e.g. Bertsimas & Tsitsiklis, 1997) and dynamically generate
a sufficient subset of constraints. Constraint generation (CG) considers a relaxed program
that only contains a manageable subset of the constraints, and solves this to optimality.
Given a solution to this relaxed program, a subproblem is used to either prove that the
solution is optimal to the full program, or find a “violated constraint” in the full problem
that is then introduced and the (now strengthened) relaxed program resolved. In this case
the subproblem is a variation of the winner determination IP from Section 3, and can be
concisely formulated and solved via branch-and-cut.9
4.3 Establishing Bounds on Efficiency
Consider some round t in ICE. The round starts with the announcement of prices, denote
them π t , and the provisional trade. The round ends with every bidder having met the
δ-MRPAR and ǫ-DIAR activity rules. The question to address is: what can be established
about the efficiency of the trade defined on lower-bound valuations at the end of the round?
It is perhaps unsurprising that MRPAR by itself is sufficient to provide efficiency claims
when prices are suitably accurate. What is interesting is that the coupling of MRPAR
with DIAR ensures that ICE converges to a provably efficient trade in all cases, with
efficiency often established independently of prices by reasoning directly about lower and
upper valuation bounds. For the theoretical analysis of convergence to efficiency, we assume
straightforward bidders, by which we mean a bidder that always retains his true valuation
within the valuation bounds. (All results could equivalently be phrased in terms of efficiency
claims with respect to reported valuations.)
At the closing of each round, ICE makes a determination about whether to move to
the last-and-final round. Bidders are notified when this occurs. This last-and-final round
9. The pricing step is the most computationally intensive of all steps in ICE and therefore heavily optimized.
In practice, we have found it useful to employ heuristics to seed the set of constraints used in CG. We
have also developed algorithmic techniques to speed the search for the appropriate set of constraints
in the context of lexicographic refinement: provisional Locking of multiple lexicographic values for each
CG check, and lazy constraint checks in which only a subset of the conditions for CG are routinely
checked, even though the complete set is eventually enforced. Please see the technical report at www.
eecs.harvard.edu/~blubin/ice for complete details of the pricing method.

55

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

provides a final opportunity for bidders to update their lower valuation bound information
(without exceeding their upper bounds). The exchange finally terminates with the efficient
trade and payments determined with respect to the lower valuation bounds: it is these
lower bounds that can be considered to beP
the ultimate bid submitted by each bidder when
ICE terminates. Let λ ∈ arg maxλ∈F (x0 ) i v i (λi ) denote the trade that is optimal given
the lower bound valuations. As explained in Section 4, ICE is parameterized by target
approximation error, ∆∗ , providing a lower-bound on the relative efficiency of λ to the
efficient trade λ∗ for true valuations. The challenge is to obtain useful bounds on the
relative efficiency EFF(λ) of trade λ. We provide two methods, one of which is price-based
and uses duality theory and the second of which directly reasons about the bounds on bidder
valuations. We now consider each in turn.
A price-based proof of efficiency. We have already seen in Section 2.2 that a bound on
the efficiency of provisional trade λα can sometimes be established via prices. This provides
a simple method to establish a bound on the efficiency of trade λ. Fix some δ ≥ 0. For v α
denoting the provisional valuation profile at the start of round t, and λα the corresponding
provisional trade, we know that if
(a) bidders meet δ-MRPAR while leaving v α within their bounds,
(b) prices π t were δ-approximate EQ prices for v α and λα , and
(c) λα is equal to λ, i.e., the efficient trade given the refined lower bound valuations,
then trade
λ is a 2 min(M, n2 )δ-approximation
to the efficient trade λ∗ by Theorem 2. We
P
P
have i vi (λi ) + 2 min(M, n2 )δ ≥ i vi (λ∗i ), and then,
P
2 min(M, n2 )
2 min(M, n2 )
vi (λi )
P
P
δ,
≥
1
−
δ
≥
1
−
EFF(λ) = P i
∗
∗
maxλ∈F (x0 ) i v i (λ)
i vi (λ )
i vi (λi )

(24)

which we define as ω price . Conditioned on (a–c) being met, so that the bound is available,
it will satisfy ω price ≥ ∆∗ for a small enough δ parameter. When the bound is not available
we set ω price := 0.
A direct proof of efficiency. We also provide a complementary, direct, method to
establish the relative efficiency of λ by working with the refined valuation bounds at the
end of round t. First, given a bid tree Ti , it is useful to define the perturbed valuation
with respect to a trade λi , by assigning the following values to each node β:

v i (β) , if β ∈ sat i (λi )
ṽi (β) =
(25)
v i (β) , otherwise,
where β ∈ sat i (λi ) if and only if node β is satisfied given tree Ti and lower bound valuations
v i on nodes, and given trade λi . The valuation function ṽi associated with TBBL tree Ti
is defined to minimize the value on nodes satisfied by trade λi and maximize the value on
other nodes. With this concept, and given the valuation bounds, we can now establish the
following bound,


 ′

v(λ)
v(λ)
v (λ)
ṽ(λ)
,
(26)
≥
min
= min
=
EFF(λ) =
∗
′
′
′
v(λ ) v′ ∈T,λ′ ∈F (x0 ) v (λ )
λ′ ∈F (x0 ) ṽ(λ )
ṽ(λ̃)
56

ICE: An Iterative Combinatorial Exchange

Figure 11: Determining an efficiency bound based on lower and upper valuations.
which
we define as ω direct . Notation ṽ = (ṽ1 , . . . , ṽn ), and λ̃ is the trade that maximizes
P
i ṽi (λi ) across all feasible trades. The first inequality holds because the domain of the
minimization includes v ∈ T and trade λ′ = λ∗ . The first equality holds because for any
λ′ 6= λ, the worst-case efficiency for λ occurs when the value v ′ ∈ T is selected to minimize
the value on nodes λ \ λ′ , maximize the value on nodes λ′ \ λ, and minimize the value on
shared nodes, λ′ ∩λ. Whatever the choice of λ′ , this valuation is provided through perturbed
valuation ṽ. For the final equality, ṽ(λ) = v(λ) by definition, and the optimal trade λ′ is
that which maximizes the value of the denominator, i.e., trade λ̃. Figure 11 schematically
illustrates the various trades and values used in this bound, and in particular provides
some graphical intuition for why ṽ(λ̃) − v(λ) ≥ ṽ(λ∗ ) − v(λ) = maxv′ ∈T [v ′ (λ∗ ) − v ′ (λ)] ≥
v(λ∗ ) − v(λ).
Combining together. Given the above methods we can establish lower-bound ω eff =
max(ω price , ω direct ) on the relative efficiency of trade λ. ICE is defined to move to the
last-and-final round when either of the following hold:
(a) the error bound ω eff ≥ ∆∗
(b) there is no trade even at optimistic (i.e., upper-bound) valuations.
Combining this with Theorem 5, we immediately get our main result.
Theorem 6 When ICE incorporates δ-MRPAR and ǫ-DIAR and when all bidders are
straightforward, then the exchange terminates with a trade that is within target approximation error ∆∗ , for any ∆∗ ≥ 0 as ǫ → 0.
The use of ǫ-DIAR by itself is sufficient to establish this result. However, it is the use
of prices and MRPAR that drives most elicitation in practice, particularly as we fix δ in
δ-MRPAR to a tiny constant in actual use. Empirical support for this, along with the
quality of the price-based bound and the direct efficiency bounds, is provided in Section 6.
For the ǫ parameter in ǫ-DIAR, we find that a simple rule:
ǫ :=

1 X X v i∈N (β) − v i (β)
,
2n
|Ti |
i

(27)

β∈Ti

works well. This tends towards zero as more value information is revealed by participants.
One last element of the design of ICE is the precise method by which the provisional
valuation profile v α = αv + (1 − α)v is constructed. This is important because it is then
57

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

used to determine the provisional trade and price feedback. A simple approach that works
well is to define α := max(0.5, ω eff ). We find that the lower bound of 0.5 is a useful heuristic
for early rounds when ω eff is likely to be small, making ICE adopt a provisional valuation
in the middle of the valuation bounds when not much is known. The effect is then to push
α towards 1 and thus v α towards v as the efficiency bound ω eff improves.10

5. Illustrative Examples
In this section we illustrate the behavior of the exchange on two simple examples. These
examples are provided to give a qualitative feel for its behavior. To construct the examples
we populate ICE with very simple, automated bidding agents. These agents use MIPguided heuristics to minimize the amount of information revealed in the course of passing
the activity rules, while maintaining their true value within their lower- and upper-bounds
(i.e., they act in a ‘straightforward’ way). Their reluctance to reveal information models
a basic tenet of our design, that it is costly for participants to refine and then reveal
information about their values for different trades. A detailed explanation of the operation
of these bidding agents is provided in the Appendix.
In this section, and also in presenting our main experimental results, we do not move to
a last-and-final round. Rather, the bidding agents are programmed to continue to improve
their bids past the round at which efficiency is already proved (and when a last-and-final
round would ordinarily be declared), and until payments are within some desired accuracy
tolerance. We do this to avoid the need to program agents with a strategy for how to bid
in the last-and-final round.
Prices
9.5

3.5

9

A
B
Efficient Allocation

8.5
3
8
2.5

7.5

Pessimistic
Alpha
Optimistic

2

Price

Average Allocation Value

Allocation Value
4

7
6.5

1.5

6
1
5.5
Provable
Efficiency

0.5
0
0

0.2

0.4
0.6
% Complete

Provable
Efficiency

5
0.8

4.5
0

1

(a) Allocative value

0.2

0.4
0.6
% Complete

0.8

1

(b) Prices

Figure 12: AgentA: A $8, AgentB: B $8, AgentAB: A AND B $10.
10. In some domains, it may also be important to require that payments (rather than just the efficiency
of trade λ) be accurate enough before moving to the last-and-final round. A bound on payments can
be computed in an analogous way to that on efficiency. Whether this is required in practice is likely
domain-specific and to depend, for instance, on whether the payments tend to be accurate anyway by
the time the trade is approximately accurate, and also on the impact on strategic behavior.

58

ICE: An Iterative Combinatorial Exchange

Example 8 Consider a market with a no-reserve seller of two items A and B, and three
buyers. AgentA demands A with a value of $8, AgentB demands B with a value of $8, and
AgentAB demands A AND B with a value of $10. Figure 12(a) shows that very quickly
the exchange discovers the correct trade. A price between $5 and $8 will be accurate in
this situation, and we can see that the prices in Figure 12(b) quickly meet this condition.
Fairness drives the prices towards $6, which will be the eventual Threshold payments to
AgentA and AgentB. Balance ensures that the prices remain the same for the two items.

Prices

Allocation Value

25

0.7

20

0.5
0.4

15

Pessimistic
Alpha
Optimistic

0.3

Price

Average Allocation Value

0.6

A
B
Efficient Allocation

10

0.2

5
0.1
0
0

Provable
Efficiency

Provable
Efficiency
0.2

0.4
0.6
% Complete

0.8

0
0

1

(a) Allocative value

0.2

0.4
0.6
% Complete

0.8

1

(b) Prices

Figure 13: Seller A -$10, Swapper: swap B for A $8, Buyer B $4
Example 9 Consider an example with a Seller offering A for a reserve of $10, a “Swapper”
who is willing to pay $8 if he can swap his B for A, and a Buyer willing to pay $4 for B.
In this more complex example, it takes 4 rounds, as illustrated in Figure 13(a), for a trade
to be found in the pessimistic trade. Revelation drives progress towards a completed trade,
and as we can see in Figure 13(b), this is reflected in falling prices on the goods. Thus we
can see that the price feedback is providing accurate information to the participants: only
when the price eventually becomes low enough do the buying bidders actually want a trade
to occur– and that is also when the exchange’s provisional trade switches. It is also worth
noting that the greater valuations the Seller and Swapper place on good A result in a net
higher price than that for good B.

6. Experimental Analysis
In this section we report the results of a set of experiments that are designed to provide a
proof-of-concept for ICE. The results illustrate the scalability of ICE to realistic problem
sizes and provide evidence of the effectiveness of the elicitation process and the techniques
to bound the efficiency of the provisional trade.
59

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Implementation. First, a brief aside on our experimental implementation. ICE is approximately 20,000 lines of extremely tight Java code, broken up into the functional packages
described in Table 1.11 The prototype is modular so that researchers may easily replace
components for experimentation.12 Because of ICE’s complexity, it is essential that the
code be constructed in a rigid hierarchy that avoids obscuring the high level logic behind
the details of generating, running and integrating the results of MIPs. To this end, the
system is written in a series of progressively more abstract“mini-languages” each of which
defines a clean, understandable API to the next higher level of logic. Our hierarchy provides
a way to hide the extremely delicate steps needed to handle the numerical issues that come
out of trying to repeatedly solve coupled optimization problems, where the constraints in
one problem may be defined in terms of slightly inaccurate results from an earlier problem.
Most of the constraints presented in this paper must be carefully relaxed and monitored
in order to handle these numerical precision issues. At the bottom of this hierarchy the
MIP specification is fed into our generalized back-end optimization solver interface13 (we
currently support CPLEX and the LGPL-licensed LPSolve), that handles machine loadbalancing and parallel MIP/LP solving. This concurrent solving capability is essential, as
we need to handle tens of thousands of comparatively simple MIPs/LPs.
Component
Agent
Model
Bidding Language
Exchange Driver & Communication
Activity/Closing Rule Engines
WD Engine
Pricing Engine
MIP Builders
Framework & Instrumentation
JOpt
Instance Generator

Purpose
Strategic behavior and information revelation decisions
XML support to load goods and true valuations
Implements TBBL
Controls exchange, and coordinates agent behavior
MRPAR, DIAR and Closing Rules
Logic for WD
Logic for three pricing stages
Translates from engines into our optimization APIs
Wire components together & Gather data
Our Optimization API wrapping CPLEX
Random Problem Generator

Lines
2001
1353
2497
1322
1830
685
1317
2206
2642
2178
497

Table 1: Exchange components and code breakdown

Experimental set-up. In the experiments, the δ-parameter in MRPAR is set to near
zero and both the MRPAR and DIAR activity rule fire in every round. The rule used
to define the ǫ-parameter in DIAR is exactly as described in Section 4.1. We adopt the
same straightforward bidding agents that were employed in Section 5 (see the Appendix
for details). In simulation, we adopt the Threshold payment rule and terminate ICE when
the per-agent error in payment relative to the correct payment is within 5% of the average
per-agent value for the efficient trade. On typical instances, this incurs an additional 4
rounds beyond those that would be required if we had a last-and-final round. All timing
is wall clock time, and does not separately count the large number of parallel threads of
execution in the system. The experiments were run on a dual-processor dual-core Pentium
11. Code size is measured in physical source line of code (SLOC).
12. Please contact the authors for access to the source code.
13. http://www.eecs.harvard.edu/econcs/jopt

60

ICE: An Iterative Combinatorial Exchange

IV 3.2GHz with 8GB of memory and CPLEX 10.1. All results are averaged over 10 trials.
The problem instances are available at http://www.eecs.harvard.edu/~blubin/ice.
Our instance generator begins by generating a set G of good types. Next, for each j ∈ G it
creates s ≥ 1 copies of each good type, forming a total potential supply in the market of s|G|
goods (exactly how many units are in supply depends on the precise structure of bid trees).
Each unit is assigned to one of the bidders uniformly at random. The generator creates a
bid tree Ti for each bidder by recursively growing it, starting from the root and adopting
two phases. For the tree above depthLow, each node receives a number of children drawn
uniform between outDegreeLow and outDegreeHigh (a percentage of which are designated
as leaves), resulting in an exponential growth in the number of nodes during this phase. By
the width at some depth we refer to the number of nodes at that depth. Below this point,
we carefully control the expected number of children at each node in order to make the
expected width conform to a triangle distribution over depth from depthLow to depthMid
to depthHigh: we linearly increase the expected width at each depth between depthLow and
depthMid to a fixed multiple (ξ) of the width at depthLow, and then linearly decrease the
expected width back to zero by depthHigh.14 This provides complex and deep trees without
inherently introducing an exponential number of nodes.
Each internal node must be assigned the parameters for its interval choose operator. We
typically choose y with a high-triangle distribution between 1 and the number of children and
x with a low-triangle distribution between 1 and y. This will bias towards the introduction
of IC operators that permit a wide choice in the number of children. Each internal node is
also assigned a bonus drawn according to a uniform distribution. Each leaf node is assigned
as a “buy” node with a probability ψ ∈ [0, 1], and then a specific good type for that node
is chosen from among those good types for sale in the market. The node is assigned a
quantity by drawing from a low-triangle distribution between 1 and the total number in
existence.15 A unit value for the node is then drawn from a specific “buy” distribution,
typically uniform, which is multiplied by the quantity and assigned as the node’s bonus.
The leaf nodes assigned as “sell” nodes have their goods and bonuses determined similarly,
this time with goods selected from among those previously assigned to the bidder.16
6.1 Experimental Results: Scalability
The first set of results that we present focuses on the computational properties of ICE.
Figure 14 shows the runtime performance of the system as we increase the number of
bidders while holding all other parameters constant. In this example, 100 goods in 20 types
are being traded by bidders with an average of 104 node trees. The graph shows the total
wall clock time for all parts of the system. While we see super-linear growth in solve time
14. Note that by setting depthLow =depthMid =depthHigh one can still grow a full tree of a given depth by
eliminating phase 2.
15. The total number of goods of a given type in existence may not actually be available for purchase at any
price given the structure of seller trees. Thus a bias towards small quantities in “buy” nodes and large
quantities in “sell” nodes produces more interesting problem instances.
16. In our experiments, we vary 2 ≤ |G| ≤ 128, 1 ≤ d ≤ 128, 2 ≤ |N | ≤ 20, 2 ≤ outDegreeLow ≤ 8,
2 ≤ outDegreeHigh ≤ 8, 2 ≤ depthLow ≤ 6, 2 ≤ depthMid ≤ 6, 2 ≤ depthHigh ≤ 8, set a balanced buy
probability ψ = 0.5, and set width multiplier during the second phase to ξ = 2. In these examples,
buy node bonuses were drawn uniformly from [10, 100], sell nodes bonuses were drawn uniformly from
[−100, −10] and internal nodes bonuses uniformly from [−25, 25].

61

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

4

10

3000

Good Scalability (Mean of 10 runs)

Agent Scalability (Mean of 10 runs)

Concurrent CPU Time (s)

Concurrent CPU Time (s)

2500

2000

1500

1000

3

10

500

2

0
0

5

10

15

20

10 0
10

25

1

2

10

10

Agents

3

10

Good Types

Figure 14: Effect of the number of bidders
on the run-time of ICE

Figure 15: Effect of the number of good
types on the run-time of ICE

with the number of bidders, the constants of this growth are such that markets with large
numbers of bidders can be efficiently solved (solving for 20 bidders in around 40 minutes).
The error bars in all plots are for the standard error of the statistic.
In Figure 15 we can see the effect of varying the number of types of goods (retaining
5 units of each good in the supply) on computation time. For this example we adopt
10 bidders, and the same tree generation parameters. A likely explanation for eventual
concavity of the run-time performance is suggested by the decrease in the average (item)
price upon termination of ICE as the number of types of goods are increased (see Figure 16).
The average price provides a good proxy for the competitiveness of the market. Adding
120
Mean Linear Price (Mean of 10 runs)

Mean Linear Price

100

80

60

40

20

0 0
10

1

2

10

10

3

10

Goods

Figure 16: Effect of the number of goods on the average item price upon termination of
ICE.
62

ICE: An Iterative Combinatorial Exchange

4

4

10

10

3

3

10
Concurrent CPU Time (s)

Concurrent CPU Time (s)

10

2

10

1

10

2

10

1

10

Node Out Degree Scalability (Mean of 10 runs)
0

10 0
10

Power law fit
1

0.4243 x

Tree Depth Scalability (Mean of 10 runs)

1.59
0

2

10
10
Number of Nodes in Tree

10 0
10

3

10

Figure 17: Effect of bid-tree size on runtime of ICE: Varying the nodeout degree.

Power law fit
1

0.7553 x
2

10
10
Number of Nodes in Tree

1.45
3

10

Figure 18: Effect of bid-tree size on runtime of ICE: Varying the tree
depth.

goods to the problem will initially make the winner determination problem more difficult,
but only until there is a large over-supply, at which point the outcome is easier to determine.
Figures 17 and 18 illustrate the change in run time with the size of bid trees. Here we
use only the first phase of our tree-generator to avoid confounding the effects of size with
structural complexity. In both experiments, 100 goods in 20 types were being traded by 10
bidders. In Figure 17 we vary the number of children of any given node while in Figure 18
we vary the depth of the tree. Increasing the branching factor and/or tree depth results in
an exponential growth in tree size, which necessarily corresponds to an exponential growth
in runtime. However, if we account for this by instead plotting against the number of nodes
in the trees, we see that both graphs indicate a near-polynomial increase in runtime with
tree size. We fit a polynomial function to this data of the form y = Axb , indicating that
this growth is approximately of degree 1.5 in the range of tree sizes considered in these
experiments.
6.2 Empirical Results: Economic Properties
The second set of results that we present focus on the economic properties of ICE: the
efficiency of trade across rounds, the effectiveness of preference elicitation, and the accuracy
and stability of prices. For this set of experiments we average over 10 problem instances,
each with 8 bidders, a potential supply of 100 goods in 20 types, and bid trees with an
average of 104 nodes.
Figure 19 plots the true efficiency of the trades computed at pessimistic (lower bounds
v), provisional (α-valuation v α ) and optimistic (upper bounds v) valuations across rounds.
In this graph and those that follow, the x-axis indicates the number of rounds completed
as a percentage of the total number of rounds until termination which enables results to
be aggregated across multiple instances, each of which can have a different number of total
63

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

100

10000

80

8000

70

7000

60
Mean
Provable 95%
Efficiency
SE

50
40
30

6000
5000
4000
Mean
Provable 95%
Efficiency
SE

3000
2000

Pessimistic
Alpha
Optimistic

20
10
0

MRPAR
DIAR

9000

Mean Slack Revealed

% Efficient

90

0.2

0.4
0.6
% Complete

0.8

1000
0
0

1

Figure 19: Efficiency of the optimistic, provisional, and pessimistic trades
across rounds.

0.2

0.4
0.6
% Complete

0.8

1

Figure 20: Average reduction in value uncertainty due to each rule.

rounds.17 The vertical (dashed) line indicates the average percentage complete when the
trade is provably 95% efficient. The exchange remains open past this point while payments
converge (and because we simulate the outcome of the last-and-final round by continuing
progress with our straightforward bidding agents). The two lines on either side represent
one standard error of this statistic.
In Figure 19, we see that the exchange quickly converges to highly efficient trades, taking
an average of 6.8 rounds to achieve efficiency. In general, the optimistic trade (i.e., computed
from upper bounds v) has higher (true) efficiency than the pessimistic one (i.e., computed
from lower bounds v), while the efficiency of the provisional trade λα is typically better than
both. This justifies the design decision to adopt the provisional valuations and provisional
trade in driving the exchange dynamics. It also suggests that exchanges with the traditional
paradigm of improving bids (i.e., increasing lower bound claims on valuations) would allow
little useful feedback in early rounds: the efficiency of the pessimistic trade—all that would
be available without information about the upper-bounds of bidder valuations—is initially
very poor.
Figure 20 shows the average amount of revelation caused by MRPAR and DIAR in each
round of ICE. Revelation is measured here in terms of the absolute tightening of upper and
lower bounds, summed across the bid trees. The MRPAR activity rule is the main driving
force behind the revelation of information and the vast majority of revelation (in absolute
terms) occurs within the first 25% of rounds. DIAR plays a role in making progress towards
identifying the efficient trade but only once MRPAR has substantially reduced the value
uncertainty and despite firing in every round. One can think of MRPAR as our rocket’s
main engine, and DIAR as a thruster for mid-course correction. ICE determines the efficient
17. Each data point represents the average across the 10 instances, and is determined by averaging the
underlying points in its neighborhood. Error-bars indicate the standard error (SE) of this mean. Thus,
the figures are essentially a histogram rendered as a line graph.

64

ICE: An Iterative Combinatorial Exchange

80

30
Price Volatility (Mean of 10 runs)

% Regret in Price (Mean of 10 runs)

70

% Regret in Price

% Difference from final price

25
60
50
40
30

Mean
Provable 95%
Efficiency
SE

20

20

15

Mean
Provable 95%
Efficiency
SE

10

5

10
0
0

0.2

0.4
0.6
% Complete

0.8

0
0

1

Figure 21: Price trajectory: Closeness of
prices in each round to the final
prices

0.2

0.4
0.6
% Complete

0.8

1

Figure 22: Regret in best-response by bidders due to price inaccuracy relative to final prices.

trade when the average node in a TBBL tree still retains a gap between the upper and lower
bounds on value at the node equal to around 62% of the maximum (true) value that a node
could contribute to a bidder’s value, roughly the maximum marginal value contributed by a
node over all feasible trades. We see that ICE is successful in directing preference elicitation
to information that is relevant to determining the efficient trade.
We now provide two different views on the effectiveness of prices. Figure 21 shows the
mean percentage absolute difference between the prices computed in some round and the
prices computed in the final round. Prices quickly converge. In our experiments we have
driven the exchange beyond the efficient solution in order to converge to the Threshold
payments, but we see that most of the price information is already available at the point of
efficiency. Figure 22 provides information about the quality of the price feedback. We plot
the ‘regret’, averaged across bidders and runs, from the best-response trade as determined
from intermediate prices in comparison to the best-response to final prices, where the regret
is defined in terms of lost payoff at those final prices. Define the regret to bidder i for his
best response λ′i = arg maxλi ∈Fi (x0 ) [vi (λi ) − pπ̂ (λi )], to prices π̂, given that the final prices
are π, as:


′ ) − pπ (λ′ )
v
(λ
i i
i
 × 100%.
Regreti (λ′i , π) = 1 −
(28)
max vi (λi ) − pπ (λi )
λi ∈Fi (x0 )

As the payoff from trade λ′i , when evaluated at prices π, approaches that from the bestresponse trade at prices π, then Regreti (λ′i , π) approaches 0%. Figure 22 plots the average
regret across all bidders as a function of the number of rounds completed in ICE. The regret
is low: 11.2% when averaged across all rounds before the efficient trade is determined and
7.0% when averaged across all rounds. That regret falls across rounds also shows that prices
become more and more informative as the rounds proceed.
65

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

100
90
80

% Efficient

70
60
50
40
30
20
10
0
0

Closing Rule Prediction
Truth
0.2

0.4
0.6
% Complete

0.8

1

Figure 23: Comparison between the actual efficiency of the pessimistic trade and the ω direct
bound.
Finally, we present experimental results that relate to the two methods that ICE employs
to bound the final efficiency of the pessimistic trade. The total pricing error across all
bidders in each round as determined within pricing in terms of (λα , v α ), and normalized
here by the total true value of the efficient trade, is already small (at 8.5%) in initial rounds
and falls to around 3% by final rounds of ICE. This suggests that a price-based bound is
quite informative, although note that this is defined in terms of the error given (λα , v α ) and
does not immediately map to a price-based accuracy claim for true valuations and for the
current trade λ defined on lower bound valuations. Figure 23 compares the actual efficiency
of the pessimistic trade λ in each round with that estimated by the ω direct bound on efficiency
that is available to the exchange. This confirms that the direct bound is reasonably tight,
and very effective in bounding the true efficiency regardless of the accuracy of the prices.

7. Conclusions
In this work we designed and implemented a scalable and highly expressive iterative combinatorial exchange. The design includes many interesting features, including: a new treebased language for combinatorial exchange environments, a new method to construct approximate linear prices from expressive languages, a proxied architecture with optimistic
and pessimistic valuations coupled with price-based activity rules to drive preference elicitation, and a direct method to estimate the final efficiency of the trade in terms of valuation
bounds. By adopting proxy agents that receive direct, expressive claims on upper and
lower valuations bounds we are able to form claims about efficiency despite using only
linear prices. These bounds also allow for good progress in early rounds, and even when
there is no efficient trade at lower bound (pessimistic) values. Experimental results with
automated, simple bidding agents indicate good behavior in terms of both scalability and
economic properties.
66

ICE: An Iterative Combinatorial Exchange

There are many intriguing opportunities for future work. It will be especially interesting
to instantiate special-cases of the ICE design to domains for which there exist strategyproof,
static (two-sided) combinatorial market designs. This would bring straightforward bidding
strategies into an ex post Nash equilibrium. For example, it should be possible to integrate
methods such as trade-reduction (McAfee, 1992) and its generalizations (Babaioff & Walsh,
2005; Chu & Shen, 2007) in domains with restricted expressiveness. We can also consider
ICE as a combinatorial auction rather than exchange, where a direct appeal to VCG payments would provide incentive compatibility. The other two major directions for future work
are to: (a) modify the design to allow bidders to refine the structure, not just the valuation
bounds on their TBBL tree, across rounds; (b) extend ICE to work in a dynamic environment with a changing bidder population, for instance maintaining linear price feedback
and periodically clearing. Recent progress in on-line mechanism design includes truthful,
dynamic double auctions for very simple expressiveness (Blum, Sandholm, & Zinkevich,
2006; Bredin, Parkes, & Duong, 2007), but does not extend to the kind of expressiveness
and price sophistication present in ICE; see the work of Parkes (2007) for a recent survey.
Lastly, the incentive properties of ICE are very much dependent on the payment rule used
which argues for further analysis of the Threshold rule and its alternatives.

Acknowledgments
This work is supported in part by NSF grant IIS-0238147. An earlier version of this paper
appeared in the Proc. 6th ACM Conference on Electronic Commerce, 2005. The TBBL language is also described in a workshop paper (Cavallo et al., 2005). The primary authors of
this paper are Benjamin Lubin, Adam Juda and David Parkes. Thanks to the anonymous
reviewers and associate editor of JAIR for extremely helpful comments. Nick Elprin, Loizos
Michael and Hassan Sultan contributed to earlier versions of this work. Our thanks to the
students in Al Roth’s class (Econ 2056) who participated in a trial of this system and to
Cynthia Barnhart for airline domain expertise. We would also like to thank Evan Kwerel
and George Donohue for early motivation and encouragement. Some of the computation
used in the preparation of this manuscript was performed on the Crimson Grid in the Harvard School of Engineering and Applied Sciences. Finally, this paper’s genesis is from CS
286r “Topics at the Interface of Computer Science and Economics” taught at Harvard in
Spring 2004. Thanks to all the students for their many early, innovative ideas.

Appendix A. Computation for MRPAR
In this section we show that MRPAR can be computed by solving a sequence of 3 MIPs. We
begin by considering the special case of δ = 0. The general case follows almost immediately.
Define a candidate passing trade, λL
i , as:
λL
i ∈ arg

max v i (λi ) − pπ (λi )

λi ∈Fi (x0 )

breaking ties
(i) to maximize v i (λi ) − v i (λi )
(ii) in favor of λαi
67

(29)

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

This can be computed by solving one MIP to maximize v i (λi ) − pπ (λi ), followed by
a second MIP in which this objective is incorporated as a constraint and v i (λi ) − v i (λi )
becomes the objective. Given perturbed valuation ṽi , defined with respect to trade λL
i (as
in Section 4), we can define a witness trade, λU
,
as:
i
λU
i ∈ arg

max ṽi (λi ) − pπ (λi ).

(30)

λi ∈Fi (x0 )

This can be found by solving a third MIP. Given prices π, provisional trade λαi and
bid tree Ti , the computational MRPAR rule (C-MRPAR) for the case of δ = 0 can now be
defined as:
π L
U
π U
(1) v i (λL
i ) − p (λi ) ≥ ṽi (λi ) − p (λi ) ,and
α
L
π L
α
π α
(2) λL
i = λi , or v i (λi ) − p (λi ) > ṽi (λi ) − p (λi )

We now establish that C-MRPAR is equivalent to MRPAR, as defined by (19)–(21).
Lemma 4 Given trades λi and λ′i , prices π, and tree Ti , we have θiπ (λi , λ′i , vi′ ) ≥ 0, ∀vi′ ∈ Ti
if and only if v i (λi ) − pπ (λi ) ≥ ṽi (λ′i ) − pπ (λ′i ), where ṽi is defined with respect to trade λi .
Proof: Direction (⇒) is immediate since ṽi ∈ Ti . Consider direction (⇐) and suppose, for
π ′
′
contradiction, that v i (λi ) − pπ (λi ) ≥ ṽi (λ′i ) −
Pp (λi ) but′ there exists some vi ∈ Ti such that
π
′
′
π
′
′
vi (λi ) − p (λi ) < vi (λi ) − p (λi ). Subtract β∈λi ∩λ′ [vi (β) − v i (β)] from both sides, where
i
β ∈ λi indicates that node β is satisfied by trade λi , to get
X
X
X
X
v i (β) − pπ (λi ) <
vi′ (β) +
vi′ (β) −
vi′ (β) +
X

⇒

X

⇒

vi′ (β) −

β∈λi ∩λ′i

v i (β) +

β∈λi \λ′i

X

vi′ (β) +

β∈λ′i \λi

X

β∈λi ∩λ′i

β∈λi ∩λ′i

β∈λi ∩λ′i

β∈λi \λ′i

X

v i (λi ) − p (λi ) <

X

v i (β) − pπ (λi ) <

v i (β) +

β∈λ′i \λi

ṽi (λ′i )

−p

π

X

v i (β) − pπ (λ′i )

(31)

β∈λi ∩λ′i

β∈λi ∩λ′i

β∈λi ∩λ′i
π

vi′ (β) +

X

v i (β) − pπ (λ′i ) (32)

β∈λi ∩λ′i

(λ′i ),

(33)

which is a contradiction.



Lemma 5 Given trade λi , prices π, and tree Ti then θiπ (λi , λ′i , vi′ ) ≥ 0, ∀vi′ ∈ Ti , ∀λ′i ∈
π U
Fi (x0 ), if and only if v i (λi ) − pπ (λi ) ≥ ṽi (λU
i ) − p (λi ), where ṽi is defined with respect to
U
trade λi and λi is the witness trade.
0
Proof: Direction (⇒) is immediate since ṽi ∈ Ti and λU
i ∈ Fi (x ). Consider direction
π
U
(⇐) and suppose, for contradiction, that v i (λi ) − p (λi ) ≥ ṽi (λi ) − pπ (λU
i ) but there exists
′
0
′
π
′
′
some λi ∈ Fi (x ) and vi ∈ Ti such that θi (λi , λi , vi ) < 0. By Lemma 4, this means
v i (λi ) − pπ (λi ) < ṽi (λ′i ) − pπ (λ′i ). But, we have a contradiction because
π U
v i (λi ) − pπ (λi ) ≥ ṽi (λU
i ) − p (λi )

=

max

0
λ′′
i ∈Fi (x )

ṽi (λ′′i )

−p

(34)
π

(λ′′i )

≥

ṽi (λ′i )

−p

π

(λ′i )

(35)


68

ICE: An Iterative Combinatorial Exchange

Theorem 7 C-MRPAR is equivalent to δ-MRPAR for δ = 0.
Proof: Comparing (17) and (18) with C-MRPAR, and given Lemmas 4 and 5, all that is
left to show is that it is sufficient to check λL
i , as the only candidate to pass MRPAR. That
is, we need to show that if there is some λ̆i ∈ Fi (x0 ) that satisfies MRPAR then λL
i satisfies
MRPAR. We argue as follows:
1. Trade λ̆i must solve maxλi ∈Fi (x0 ) [v i (λi ) − pπ (λi )]. Otherwise, there is some λ′i with
v i (λ′i ) − pπ (λ′i ) > v i (λ̆i ) − pπ (λ̆i ). A contradiction with (17).
2. Trade λ̆i must also break ties in favor of maximizing v i (λi ) − v i (λi ). Otherwise, there
is some λ′i with the same profit as λ̆i at v i , with v i (λ′i ) − v i (λ′i ) > v i (λ̆i ) − v i (λ̆i ). This
implies v i (λ′i ) − v i (λ̆i ) > v i (λ′i ) − v i (λ̆i ), and θiπ (λ′i , λ̆i , v i ) > θiπ (λ′i , λ̆i , v i ). But, since
λ′i has the same profit as λ̆i at v i we have θiπ (λ′i , λ̆i , v i ) = 0 and so θiπ (λ′i , λ̆i , v i ) > 0.
This is a contradiction with (17).
3. Proceed now by case analysis. Either λ̆i = λαi , in which case we are done because
this will be explicitly selected as candidate passing trade λL
i . For the other case, let
ΛL
denote
all
feasible
solutions
to
(29)
and
consider
the
difficult
case when |ΛL
i
i | > 1.
L
′
We argue that if λ̆i ∈ Λi satisfies MRPAR, then so does any other trade λi ∈ ΛL
i ,
with λ′i 6= λ̆i . By MRPAR, we have θiπ (λ̆i , λ′i , vi′ ) ≥ 0, ∀vi′ ∈ Ti . In particular,
ṽi (λ̆i )−pπ (λ̆i ) ≥ ṽi (λ′i )−pπ (λ′i ), where ṽi is defined with respect to λ̆i , and equivalently,
v i (λ̆i ) − pπ (λ̆i ) ≥ ṽi (λ′i ) − pπ (λ′i ).

(36)

v i (λ̆i ) − pπ (λ̆i ) = v i (λ′i ) − pπ (λ′i ),

(37)

On the other hand,

′
since both are in ΛL
i . Taking (36) together with (37), we must have that λi satisfies
no uncertain value nodes in Ti not also satisfied in λ̆i . Moreover, since v i (λ̆i ) −
v i (λ̆i ) = v i (λ′i ) − v i (λ′i ), both trades must satisfy exactly the same uncertain value
nodes. Finally, by (37) the profit from all fixed value nodes in Ti must be the same
in both trades. We conclude that the profit is the same for all vi′ ∈ Ti for λ̆i and λ′i
at the current prices and MRPAR is satisfied by either trade.


To understand the importance of the tie-breaking rule (i) in selecting the candidate
passing trade, λL
i , in C-MRPAR, consider the following example for MRPAR with δ = 0:
Example 10 A bidder has XOR(+A, +B) and a value of 5 on the leaf +A and a value range
of [5,10] on leaf +B. Suppose prices are currently 3 for each of A and B and λαi = +B.
The MRPAR rule is satisfied because the market knows that however the remaining value
uncertainty on +B is resolved the bidder will always (weakly) prefer +B to +A and +B is
λαi . Notice that both +A and +B have the same pessimistic utility, but only +B can satisfy
MRPAR. But +B has maximal value uncertainty, and therefore this is selected over +A by
C-MRPAR.
69

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

To understand the importance of selecting, and evaluating, λU
i with respect to ṽi rather
than v i , consider the following example (again for δ = 0). It illustrates the role of “shared
uncertainty” in the tree, which occurs when multiple trades share a node with uncertain
value and the value, although uncertain, will be resolved in the same way for both trades.
Example 11 A bidder has XOR(+A, +B) and value bounds [5, 10] on the root node and a
value of 1 on leaf +A. Suppose prices are currently 3 for each of A and B and λαi = +B.
The MRPAR rule is satisfied because the bidder strictly prefers +A to +B, whichever way
the uncertain value on the root node is ultimately resolved. C-MRPAR selects λL
i as “buy
π (λL ) = 5 + 1 − 3 = 3. At valuation v , the witness trade “buy B”
)
−
p
A”, with payoff v i (λL
i
i
i
would be selected and have payoff 10 − 3 = 7 and seem to violate MRPAR. But, whichever
way the uncertain value at the root is resolved it will affect +A and +B in the same way.
This is addressed by setting ṽi (β) = v i (β) = 5 on the root node, the same value adopted
in determining the payoff from λL
i . Evaluated at ṽi , the witness is “buy A” and (1) of
C-MRPAR is trivially satisfied while (2) is satisfied since 3 > 5 − 3 = 2.
For δ-MRPAR with δ > 0, we adopt a slight variation, with a δ-C-MRPAR procedure
defined as:
(1) Check θiπ (λαi , λ′i , vi′ ) ≥ −δ for all vi′ ∈ Ti , all λ′i ∈ Fi (x0 ) directly, by application of
Lemma 5 with valuation ṽi defined with respect to trade λαi , and test
π U
v i (λαi ) − pπ (λαi ) ≥ ṽi (λU
i ) − p (λi ) − δ

(38)

(2) If this is not satisfied then fall back on C-MRPAR to verify (20) and (21), with
α
candidate passing trade λL
i modified from (29) to drop tie-breaking in favor of λi and
L
π
L
with the second step of C-MRPAR modified to require v i (λi ) − p (λi ) > ṽi (λαi ) −
pπ (λαi ) + δ, again with ṽi defined with respect to λL
i .
The argument adopted in the proof of Theorem 7 remains valid in establishing that it
α
is sufficient to consider λL
i , as defined in δ-C-MRPAR, in the case that λi does not pass
the activity rule.

Appendix B. Computation for DIAR
The ǫ-DIAR rule can be verified by solving two MIPs. The first optimization problem
identifies the trade with maximal DIAR error for which the current bounds refinement has
improved this error by at least ǫ:
∆Pi =

max [ṽi0 (λi ) − pπ (λi ) − (v 0i (λαi ) − pπ (λαi ))]

λi ∈Fi (x0 )

(39)

s.t. (ṽi0 (λi ) − pπ (λi ) − (v 0i (λαi ) − pπ (λαi )))
= −C +

max

λi ∈Fi (x0 )

− (ṽi1 (λi ) − pπ (λi ) − (v 1i (λαi ) − pπ (λαi ))) ≥ ǫ

(40)

ṽi0 (λi )

(41)

π

− p (λi )

s.t. ṽi0 (λi ) − v 0i (λαi ) − ṽi1 (λi ) + v 1i (λαi ) ≥ ǫ,
70

(42)

ICE: An Iterative Combinatorial Exchange

where ṽi0 and ṽi1 are defined with respect to λαi , v 0 and v 1 represent valuations defined
before and after the bidder’s refinement respectively, and C = v 0i (λαi ) − pπ (λαi ). Note that
the problem could be infeasible, in which case we define ∆Pi := −∞.
The second optimization identifies the trade with maximal DIAR error for which v 1 still
allows for the possibility of valuation bounds that provide an ǫ error reduction over v 0 :
∆Fi =

max [ṽi0 (λi ) − pπ (λi ) − (v 0i (λαi ) − pπ (λαi ))]

λi ∈Fi (x0 )

(43)

s.t. (ṽi0 (λi ) − pπ (λi ) − (v 0i (λαi ) − pπ (λαi )))
− (v 1i (λi ) − pπ (λi ) − (v̆i1 (λαi ) − pπ (λαi ))) ≥ ǫ
= −C +

max ṽi0 (λi ) − pπ (λi )

λi ∈Fi (x0 )

s.t. ṽi0 (λi ) − v 0i (λαi ) − v 1i (λi ) + v̆i1 (λαi ) ≥ ǫ,

(44)
(45)
(46)

where ṽi is defined with respect to λαi , and v̆i is similarly defined with respect to λi . The
second term in (44) recognizes that it remains possible to decrease the value on λi to the
new lower-bound v 1i (λi ), while increasing the value on λαi to the new upper-bound v 1i (λαi )
except on those nodes that are shared with λi , giving v̆i1 (λαi ). We see that (46) is equivalent
to:
X
X
[v 1i (β) − v 0i (β)] ≥ ǫ,
[v 0i (β) − v 1i (β)] +
β∈λi \λα
i

β∈λα
i \λi

which calculates the amount of refinement that is still possible in service of reducing the
DIAR error. Note the problem could be infeasible, in which case we define ∆Fi := −∞. We
ultimately compare the two solutions, and the bidder passes DIAR if and only if ∆Pi ≥ ∆Fi .

Appendix C. The Automated Bidding Agents and Bidder Feedback
The bidding agents that are used for the simulation experiments are designed to minimize
the amount of information revealed in order to pass the activity rules and while remaining
straightforward so that the true valuation is consistent with lower and upper valuations. In
summarizing the behavior of the bidding agents, there are three things to explain: (a) the
method that we adopt in place of the last-and-final round; (b) the feedback that is provided
by ICE to bidders in meeting MRPAR and DIAR; and (c) the logic that is followed by the
bidding agents. Rather than define a method for bidding agents to adjust their bounds in
a last-and-final round, we keep ICE open in simulation past the point in which it would
ordinarily go to last-and-final. Past this point, the bidding agents continue to refine their
bounds and ICE terminates when the payments are within some desired accuracy. Each
bidding agent in this phase reduces its uncertainty by some multiplicative factor on all
nodes that are active in the current provisional trade or in any of the provisional trades for
the economies with bidder i removed. This is adopted for simulation purposes only.
Our bidding agents operate in a loop, heuristically modifying their valuation bounds in
trying to meet MRPAR and DIAR and querying the proxy for advice. The proxy provides
guidance to help the bidding agent further refine its valuation so it can meet the activity
rule. For both MRPAR and DIAR, the optimization problems that are solved in checking
whether a bidder has satisfied the activity rule also provide information that can guide the
71

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

U
bidder. First consider MRPAR and recall that λL
i is the candidate passing trade and λi is
the witness trade. The following lemma is easy, and stated without proof:

Lemma 6 When MRPAR is not satisfied for the current valuation bounds, a bidder must
U
increase a lower bound on at least one node in {λL
i \ λi }, or decrease an upper bound on
U
L
at least one node in {λi \ λi }, in order to meet the activity rule.
Once a simple bidder makes some changes on some subset of these nodes, the bidder
can inquire if he has passed the activity rule. The proxy can then respond “yes” or can
revise the set of nodes on which the bidding agent should refine its valuation bounds. A
similar functionality is provided for DIAR. This time the trade that solves the second MIP
(with DIAR error ∆Fi ) is provided as feedback, together with information about how much
the bidder must either further reduce the error, or further constrain the possibilities on this
trade, to satisfy DIAR. The bidding agent can determine from this information which nodes
it must modify, and by how much in total, and is free to decide how much to modify each
node to satisfy the rule. The key to our agent design is the following lemma:
Lemma 7 The trade with which a straightforward bidder passes MRPAR (for δ = 0) must
be a trade that is weakly preferred by the bidder to all other trades for his true valuation.
Proof: By contradiction. Suppose true valuation vi ∈ Ti and trade λ̆i meets MRPAR but
is not a weakly preferred trade at the true valuation and prices π. Then, there exists a
trade λ∗i ∈ Fi (x0 ) such that θiπ (λ∗i , λ̆i , vi ) > 0. But, this is a contradiction with MRPAR
since θiπ (λ̆i , λ′i , vi′ ) ≥ 0 for all vi′ ∈ Ti and all λ′i ∈ Fi (x0 ), including vi′ = vi and λ′i = λ∗i . 
We use this observation to define a procedure UpdateMRPAR by which a bidder
can intelligently refine its valuation bounds to meet MRPAR. Let λ̆i be the trade with
which we hope to pass MRPAR, and define ui (λi , π) = vi (λi ) − pπ (λi ), ui (λi , π) = v i (λi ) −
pπ (λi ), ũi (λi , π) = ṽi (λi ) − pπ (λi ), where ṽi is defined with respect to candidate passing
trade λ̆i . The high-level approach is as follows:
function UpdateMRPAR
λ̆i ∈ arg maxλi ∈Fi (x0 ) ui (λi , π)
if ui (λ̆i , π) < 0 then
reduce slack on λ̆i by ui (λ̆i , π)
end if
λU
i ∈ arg maxλi ∈Fi (x0 ) ũi (λi , π)
while ui (λ̆i , π) < ũi (λU
i , π) do
U
Heuristically reduce upper bounds on λU
i \ λ̆i by ũi (λi , π) − ui (λ̆i , π)
If remaining slack heuristically reduce lower bounds on λ̆i \ λU
i
λU
i ∈ arg maxλi ∈Fi (x0 ) ũi (λi , π)
end while
if λ̆i 6= λαi then
while ui (λ̆i , π) ≤ ũi (λαi , π) do
Heuristically reduce upper bounds on λαi \ λ̆i by ũi (λαi , π) − ui (λ̆i , π)
If remaining slack heuristically reduce lower bounds on λ̆i \ λαi
72

ICE: An Iterative Combinatorial Exchange

end while
end if
return λ̆i
end function
The bidding agent makes use of a couple of optimization modalities that are exposed
by the proxy to the bidder. The procedure first chooses the most preferred trade at truth
as the trade to pass MRPAR with λ̆i ; the bidding agent requests that the proxy finds this
trade by solving a MIP. If the trade has negative profit, then the bidding agent attempts to
demonstrate positive profit for this trade. Next, the bidding agent enters a loop, wherein it
repeatedly requests the proxy to run a MIP that calculates a witness trade λU
i with respect
to λ̆i . As long as this witness has more profit than that of what should be the most preferred
trade, the bidding agent adjust bounds so as to reverse this mis-ordering. Lastly, because
the bidding agent must pass MRPAR, not merely RPAR, the bidding agent attempts to
show a strict preference for λ̆i over λαi when they are not identical.
In meeting DIAR, the bidding agent responds to the ∆F ≥ 0 and ǫ ≥ 0 parameter
provided by the proxy as follows. Let λF be the trade chosen in the maximization that
calculates ∆F . The high-level approach is as follows:
function updateDIAR
while Proxy says we still have not passed DIAR do
if λF or λα can be modified to reduce DIAR error by ǫ over last round then
Heuristically reduce the upper-bound slack in λF \ λα
Heuristically reduce the lower-bound slack in λα \ λF
else
Heuristically reduce the upper-bound slack in λα \ λF
Heuristically reduce the lower-bound slack in λF \ λα
end if
end while
end function
The bidding agent attempts to make the current failing trade pass DIAR if possible by
reducing the error with respect to that trade. Otherwise, it reduces bounds to prove that
DIAR could not be made to pass on that trade and loops on to the next trade.

References
Ausubel, L., Cramton, P., & Milgrom, P. (2006). The clock-proxy auction: A practical
combinatorial auction design. In Cramton et al. (Cramton et al., 2006), chap. 5.
Ausubel, L. M., & Milgrom, P. (2002). Ascending auctions with package bidding. Frontiers
of Theoretical Economics, 1, 1–42.
Babaioff, M., & Walsh, W. E. (2005). Incentive-compatible, budget-balanced, yet highly
efficient auctions for supply chain formation. Decision Support Systems, 39, 123–149.
Ball, M., Donohue, G., & Hoffman, K. (2006). Auctions for the safe, efficient, and equitable
allocation of airspace system resources. In Cramton et al. (Cramton et al., 2006),
73

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

chap. 20.
Ball, M. O., Ausubel, L. M., Berardino, F., Cramton, P., Donohue, G., Hansen, M., &
Hoffman, K. (2007). Market-based alternatives for managing congestion at new york’s
laguardia airport. In Proceedings of AirNeth Annual Conference.
Bererton, C., Gordon, G., & Thrun, S. (2003). Auction mechanism design for multi-robot
coordination. In Proc. 17th Annual Conf. on Neural Information Processing Systems
(NIPS’03).
Bertsimas, D., & Tsitsiklis, J. (1997). Introduction to Linear Optimization. Athena Scientific.
Bikhchandani, S., & Mamer, J. W. (1997). Competitive equilibrium in an exchange economy
with indivisibilities. Journal of Economic Theory, 74, 385–413.
Bikhchandani, S., & Ostroy, J. M. (2002). The package assignment model. Journal of
Economic Theory, 107 (2), 377–406.
Blum, A., Sandholm, T., & Zinkevich, M. (2006). Online algorithms for market clearing.
Journal of the ACM, 53, 845–879.
Boutilier, C. (2002). Solving concisely expressed combinatorial auction problems. In In
Proceedings of the 18th National Conference on Artificial Intelligence, pp. 359–366.
Boutilier, C., & Hoos, H. (2001). Bidding languages for combinatorial auctions. In Proc.
17th International Joint Conference on Artificial Intelligence, pp. 1121–1217.
Bredin, J., Parkes, D. C., & Duong, Q. (2007). Chain: A dynamic double auction framework
for matching patient agents. Journal of Artificial Intelligence Research, 30, 133–179.
Cavallo, R., Parkes, D. C., Juda, A. I., Kirsch, A., Kulesza, A., Lahaie, S., Lubin, B.,
Michael, L., & Shneidman, J. (2005). TBBL: A Tree-Based Bidding Language for
Iterative Combinatorial Exchanges. In Multidisciplinary Workshop on Advances in
Preference Handling (IJCAI).
Chu, L. Y., & Shen, Z. M. (2007). Truthful double auction mechanisms for e-marketplace.
Operations Research. To appear.
Compte, O., & Jehiel, P. (2007). Auctions and information acquisition: Sealed-bid or Dynamic Formats?. Rand Journal of Economics, 38 (2), 355–372.
Conen, W., & Sandholm, T. (2001). Preference elicitation in combinatorial auctions.. In
Wellman, & Shoham (Wellman & Shoham, 2001), pp. 256–259.
Cramton, P. (2003). Electricity Market Design: The Good, the Bad, and the Ugly. In
Proceedings of the Hawaii International Conference on System Sciences.
Cramton, P. (2006). Simultaneous ascending auctions. In Cramton et al. (Cramton et al.,
2006), chap. 3.
Cramton, P., Kwerel, E., & Williams, J. (1998). Efficient relocation of spectrum incumbents.
Journal of Law and Economics, 41, 647–675.
Cramton, P., Shoham, Y., & Steinberg, R. (Eds.). (2006). Combinatorial Auctions. MIT
Press.
74

ICE: An Iterative Combinatorial Exchange

Day, R., & Raghavan, S. (2007). Fair payments for efficient allocations in public sector
combinatorial auctions. Management Science, 53 (9), 1389.
de Vries, S., Schummer, J., & Vohra, R. V. (2007). On ascending Vickrey auctions for
heterogeneous objects. Journal of Economic Theory, 132, 95–118.
de Vries, S., & Vohra, R. V. (2003). Combinatorial auctions: A survey. Informs Journal on
Computing, 15 (3), 284–309.
Dias, M., Zlot, R., Kalra, N., & Stentz, A. (2006). Market-based multirobot coordination:
A survey and analysis. Proceedings of the IEEE, 94, 1257–1270.
Dunford, M., Hoffman, K., Menon, D., Sultana, R., & Wilson, T. (2003). Testing linear
pricing algorithms for use in ascending combinatorial auctions. Tech. rep., SEOR,
George Mason University. Submitted to INFORMS J.Computing.
Fu, Y., Chase, J., Chun, B., Schwab, S., & Vahdat, A. (2003). Sharp: an architecture for
secure resource peering. In Proceedings of the 19th ACM symposium on Operating
systems principles, pp. 133–148. ACM Press.
Gerkey, B. P., & Mataric, M. J. (2002). Sold!: Auction methods for multi-robot coordination.
IEEE Transactions on Robotics and Automation, Special Issue on Multi-robot Systems,
18, 758–768.
Hudson, B., & Sandholm, T. (2004). Effectiveness of query types and policies for preference
elicitation in combinatorial auctions. In Proc. 3rd Int. Joint. Conf. on Autonomous
Agents and Multi Agent Systems, pp. 386–393.
Kelso, A. S., & Crawford, V. P. (1982). Job matching, coalition formation, and gross
substitutes. Econometrica, 50, 1483–1504.
Krishna, V. (2002). Auction Theory. Academic Press.
Kwasnica, A. M., Ledyard, J. O., Porter, D., & DeMartini, C. (2005). A new and improved
design for multi-object iterative auctions. Management Science, 51, 419–434.
Kwerel, E., & Williams, J. (2002). A proposal for a rapid transition to market allocation
of spectrum. Tech. rep., FCC Office of Plans and Policy.
Lahaie, S., Constantin, F., & Parkes, D. C. (2005). More on the power of demand queries in
combinatorial auctions: Learning atomic languages and handling incentives. In Proc.
19th Int. Joint Conf. on Artificial Intell. (IJCAI’05).
Lahaie, S., & Parkes, D. C. (2004). Applying learning algorithms to preference elicitation.
In Proc. 5th ACM Conf. on Electronic Commerce (EC-04), pp. 180–188.
McAfee, R. P. (1992). A dominant strategy double auction. J. of Economic Theory, 56,
434–450.
Milgrom, P. (2004). Putting Auction Theory to Work. Cambridge University Press.
Milgrom, P. (2007). Package auctions and package exchanges (2004 Fisher-Schultz lecture).
Econometrica, 75, 935–966.
Mishra, D., & Parkes, D. C. (2007). Ascending price Vickrey auctions for general valuations.
Journal of Economic Theory, 132, 335–366.
75

Lubin, Juda, Cavallo, Lahaie, Shneidman & Parkes

Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms for bilateral trading.
Journal of Economic Theory, 28, 265–281.
Nemhauser, G., & Wolsey, L. (1999). Integer and Combinatorial Optimization. WileyInterscience.
Nisan, N. (2006). Bidding languages for combinatorial auctions. In Cramton et al. (Cramton
et al., 2006), chap. 9.
Nisan, N., Roughgarden, T., Tardos, E., & Vazirani, V. (Eds.). (2007). Algorithmic Game
Theory. Cambridge University Press.
O’Neill, R. P., Sotkiewicz, P. M., Hobbs, B. F., Rothkopf, M. H., & Stewart, Jr., W. R.
(2005). Efficient market-clearing prices in markets with nonconvexities. European
Journal of Operations Research, 164, 269–285.
Parkes, D. C. (2007). On-line mechanisms. In Nisan et al. (Nisan, Roughgarden, Tardos,
& Vazirani, 2007), chap. 16. To appear.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance with
Vickrey-based payment schemes in exchanges. In Proc 17th International Joint Conference on Artificial Intelligence, pp. 1161–1168.
Parkes, D. C., & Ungar, L. H. (2000a). Iterative combinatorial auctions: Theory and practice. In Proc. 17th National Conference on Artificial Intelligence (AAAI-00), pp.
74–81.
Parkes, D. C., & Ungar, L. H. (2000b). Preventing strategic manipulation in iterative
auctions: Proxy agents and price-adjustment. In Proc. 17th National Conference on
Artificial Intelligence (AAAI-00), pp. 82–89.
Rassenti, S. J., Smith, V. L., & Bulfin, R. L. (1982). A combinatorial mechanism for airport
time slot allocation. Bell Journal of Economics, 13, 402–417.
Rothkopf, M. H., Pekeč, A., & Harstad, R. M. (1998). Computationally manageable combinatorial auctions. Management Science, 44 (8), 1131–1147.
Saatcioglu, K., Stallaert, J., & Whinston, A. B. (2001). Design of a financial portal. Communications of the ACM, 44, 33–38.
Sandholm, T. (2006). Optimal winner determination algorithms. In Cramton et al. (Cramton et al., 2006), chap. 14.
Sandholm, T. (2007). Expressive Commerce and Its Application to Sourcing: How We
Conducted $35 Billion of Generalized Combinatorial Auctions. AI Magazine, 28 (3),
45.
Sandholm, T., & Boutilier, C. (2006). Preference elicitation in combinatorial auctions. In
Cramton et al. (Cramton et al., 2006), chap. 10.
Shapley, L. S., & Shubik, M. (1972). The assignment game I: The core. Int. Jounral of
Game Theory, 1, 111–130.
Smith, T., Sandholm, T., & Simmons, R. (2002). Constructing and clearing combinatorial
exchanges using preference elicitation. In AAAI-02 workshop on Preferences in AI
and CP: Symbolic Approaches.
76

ICE: An Iterative Combinatorial Exchange

Vossen, T. W. M., & Ball, M. O. (2006). Slot trading opportunities in collaborative ground
delay programs. Transportation Science, 40, 15–28.
Wellman, M. P., & Shoham, Y. (Eds.). (2001). Proc. 3rd ACM Conf. on Electronic Commerce (EC-01), New York, NY. ACM.
Wurman, P. R., & Wellman, M. P. (2000). AkBA: A progressive, anonymous-price combinatorial auction. In Proc. 2nd ACM Conf. on Electronic Commerce (EC-00), pp.
21–29.

77

