Journal of Artificial Intelligence Research 30 (2007) 525-564

Submitted 1/07; published 12/07

A Framework for Kernel-Based Multi-Category Classification
Simon I. Hill

sih22@eng.cam.ac.uk

Department of Engineering,
University of Cambridge,
Cambridge, UK

Arnaud Doucet

arnaud@cs.ubc.ca

Depts. of Statistics and Computer Science
University of British Columbia,
Vancouver, Canada

Abstract
A geometric framework for understanding multi-category classification is introduced, through
which many existing ‘all-together’ algorithms can be understood. The structure enables
parsimonious optimisation, through a direct extension of the binary methodology. The
focus is on Support Vector Classification, with parallels drawn to related methods.
The ability of the framework to compare algorithms is illustrated by a brief discussion
of Fisher consistency. Its utility in improving understanding of multi-category analysis is
demonstrated through a derivation of improved generalisation bounds.
It is also described how this architecture provides insights regarding how to further
improve on the speed of existing multi-category classification algorithms. An initial example of how this might be achieved is developed in the formulation of a straightforward
multi-category Sequential Minimal Optimisation algorithm. Proof-of-concept experimental
results have shown that this, combined with the mapping of pairwise results, is comparable
with benchmark optimisation speeds.

1. Introduction
The problem of extending classification methods from the standard dichotomous framework
to a more general ‘polychotomous’ arrangement is one which has been considered by a
number of authors. Essentially, the task is to learn from some training data how best to
assign one of M possible classes to subsequent input data, where M is known beforehand.
The key contribution of this work is to introduce an overarching framework for understanding multi-category kernel-based classification methods. In particular this is a framework which makes the assumptions and constructions used in individual approaches clear.
As a result it enables the operation of most existing multi-category methods to be transparently compared and contrasted in an intuitive and consistent manner. Further, the insight
afforded by the architecture suggests ways of developing more efficient algorithms and of
bringing together the best of existing techniques.
The central idea behind this approach is to introduce an (M − 1)-dimensional space
which is divided into M class-specific regions. The aim is to learn a (M − 1)-dimensional
function f (·) which lies in the class region corresponding to the class of its argument. As
will be shown this is a straightforward generalisation of the M = 2 case, in which the two
c
2007
AI Access Foundation. All rights reserved.

Hill & Doucet

class-specific regions are f (·) ≥ 0 and f (·) < 0. Indeed, in this framework, unlike many
other approaches, the binary case is not treated as a special case.
Discussion of this is done primarily in a Support Vector Classification (SVC) context
initially, and then extended to other methodologies. The geometric structure employed
is introduced in more detail in Section 2, together with a derivation of the optimisation
problem, which is shown to be a generalisation of the standard ‘all-together’ optimisation
problems overviewed by Hsu and Lin (2002). This is discussed along with a review of
existing Support Vector (SV) multi-category methods in Section 3.
Following this we consider overall algorithm performance with Section 4 discussing
Fisher consistency, and Section 5 looking at generalisation bounds. Section 6 then discusses
other methodologies, in particular ν-Support Vector Classification (ν-SVC), Least Squares
Support Vector Classification (LS-SVC), Lagrangian Support Vector Classification (LSVC),
Proximal Support Vector Classification (PSVC), and Bayes Point Machines (BPM). This
is followed by a return to the SVC problem and a Sequential Minimal Optimisation (SMO)
algorithm is derived in Section 7. Issues related to the details of how best to implement
the SMO algorithm (e.g. point selection) are discussed, as are options for improving the
speed of convergence. These are implemented for several examples in Section 8, in an initial
experimental exercise.

2. Setting up the Multi-Category Problem
In this Section the key geometric construction will be presented, as will mechanisms for
using this to formulate an optimisation problem. Finally, extensions to the generic structure
will be described. The basic construction is described in Subsection 2.1. Following this,
Subsection 2.2 describes example empirical SV loss cases, Subsection 2.3 discusses how
relative class knowledge can be incorporated and Subsection 2.4 details an overview of the
derivation of the SV optimisation problem.
2.1 The Geometric Construction
In the binary classification case, class determination of some input from the set X is often
performed by considering the sign of an underlying real-valued function f : X → R (Vapnik,
1998, for example). In progressing to the M -class case, the underlying vector-valued func
T
tion f : X → RM −1 will be found, where f = f1 . . . fM −1
. The basic idea behind
the use of an (M − 1)-dimensional space is to be able to introduce M equally separable
class-target vectors. The class of input x will be determined by identifying that class-target
vector to which f (x) is closest.
This can be seen to effectively be what takes place in binary SV classification, where
classes, denoted A and B, have class targets y(A) = −1 and y(B) = +1. Consider now that
a third class, C, is a possibility. A one-dimensional numerical label is insufficient for the
classes to be equidistant, and in the case that little is known about the relationship between
the classes then the logical arrangement would be to compare every class to every other in
an equivalent way. In order to do this then class targets must be equidistant in some sense.
526

A Framework for Kernel-Based Multi-Category Classification

A two-dimensional arrangement as illustrated in Figure 1 allows this. Here the classtarget vectors are
y(A) =

h

−

√

3
2

iT

− 21

, y(B) =

h

√

3
2

− 21

iT

, y(C) =



0 1

T

.

(1)

where ky(ϑ)k = 11 for all classes ϑ ∈ Θ (with Θ = {A, B, . . . } denoting the set of possible
classes) as this improves tractability later. These are example class-target vectors, however,


Class C

0
1



Class Boundary

1
2



p

3
1

Class Boundary


Class A

Class B

1
2



p

3
1



Class Boundary

Figure 1: Possible class labels for classification into three. The class-target vectors corresponding to classes A, B and C are shown. The class boundaries are given by
solid lines.
in general it is important to understand that the optimisation methods which will be described are applicable regardless of their rotation. Indeed, although the apparent Cartesian
coordinate asymmetry may not appear intuitive, the important consideration is the relative
positioning of class-target vectors with respect to each other. The optimisation procedure
has no dependence on any particular orientation. This will be proven for SV methods as
part of the derivation of the optimisation process in Section 2.4.
The same approach to that described for M = 3 is taken when considering larger values
of M . While typically M = 3 will be used in this work as an example case, extensions to
higher values of M follow without further consideration. An example of how target vectors
might easily be found in higher dimensions is discussed by Hill and Doucet (2005).
1. Note that in this work k · k denotes the 2-norm of a vector, i.e. kyk =
y
normalisation will imply kyk

527

q

2
y12 + · · · + yM
−1 and, further,

Hill & Doucet

2.2 SV Empirical Multi-Category Loss
In setting up the classification process, each class is assigned a subset of the (M − 1)dimensional output space. In particular, in the most straightforward approach, these subsets
are the Voronoi regions associated with the class targets. As a result, class boundaries can be
found by forming hyperplanes between class regions which consist of all points equidistant
from the two relevant class targets. For an input x, the classifier output is given by the
function h which is found by observing in which of the regions f (x) lies i.e.
h(x) = The class associated with the region in which f (x) lies.

(2)

In describing empirical loss the vectors perpendicular to the hyperplane dividing the
region between y(A) and y(B) will typically be used2 . Define
vA (B) =

y(B) − y(A)
ky(B) − y(A)k

(3)

These vectors are illustrated for class C in Figure 2 in which a margin ε has been introduced
and defined as ε = yT (ϑ)vθ (ϑ) for all θ, ϑ ∈ {A, B, C} and θ 6= ϑ. Note that here the
dependency on θ, ϑ is not explicitly noted when referring to ε as it is constant. Discussions
of when this might not be the case are presented later. This definition of vectors v is used
as the aim will be to measure distance in a direction perpendicular to the class boundaries
and this can be done through an inner product with the relevant vector v.
This margin is used for all cases in finding the empirical loss. While there are several
different ways to combine individual loss components, the fundamental starting point is that
illustrated in Figure 2. Here a training point x with class
 C has f (x) which falls outside the
T
required region. This is penalised by ε − vB (C)f (x) in an analogous way to the binary
SV classification empirical loss of (1 − yf (x)). Indeed in the binary case vB (A) = y(A) and
vA (B) = y(B) and ε = 1. As a further parallel, just as there is a region of zero loss in the
binary case when y · f (x) > 1, so too is there a region of zero loss here, above the dotted
lines.
Consider now that training data {(xi , ϑi ) : i ∈ {1, . . . , N }} is to be used to learn how
best to classify some new input x. Denote the indicator function by I(·); the empirical loss
for a polychotomous classification problem given by Allwein, Schapire, and Singer (2001),
Crammer and Singer (2001a), and Lee et al. (2001, 2004) is then,
ℓEM P =

N
X

I(h(xi ) 6= ϑi ),

(4)

i=1

namely the number of misclassified training samples. As with dichotomous SV techniques,
some loss will be used which bounds ℓEM P , thus generating a straightforward optimisation
problem.
In setting up multi-category SV classification, this is an approach used by many different
authors, however their exact empirical loss functions have differed. The most prevalent can
be understood within the framework of Figures 1 and 2, four of these are illustrated in
Figure 3 for an object of class C. These four loss functions involve either adding together
2. An exception to this is the case presented by Lee, Lin, and Wahba (2001, 2004), as discussed by Hill and
Doucet (2005, App. C).

528

A Framework for Kernel-Based Multi-Category Classification

"

y(C )
vB (C )

T

vB (C )f (x)

vA (C )

f (x)

Figure 2: Elements involved in determining empirical loss associated with a training sample
of class C. Note that the unlabelled solid lines are the class boundaries, the region
above the dotted line is the region of zero loss for training samples of class C.

T

T

Contour Linear Summed Error Surface for y=[0 1]
2.5

2

2

1.5

1.5
f2(x)

f2(x)

Contour Quadratic Summed Error Surface for y=[0 1]
2.5

1
0.5
0
−0.5
−3

1
0.5
0

−2

−1

0
f (x)

1

2

−0.5
−3

3

−2

−1

1

2

2
1.5
f2(x)

f2(x)

1

2

3

Contour Linear Maximal Error Surface for y=[0 1]T
2.5

1.5
1
0.5

1
0.5

0
−0.5
−3

0
f (x)
1

Contour Quadratic Maximal Error Surface for y=[0 1]T
2.5

0
−2

−1

0
f (x)

1

2

−0.5
−3

3

1

−2

−1

0
f (x)

1

2

3

1

Figure 3: Four possible loss functions for the three class problem (see Figure 1). The loss
functions are shown with respect to the target vector y = [0 1]T . Traditional
additive losses are shown at top, (see equations (6) and (5)), possible variants
following proposals by Crammer and Singer (2001a) at bottom (see equations (8)
and (7)). In all cases the class boundary is shown by a dot-dash line.

all margin infringements, or taking the largest such infringement. Both linear and quadratic
versions of these two options are illustrated. Algebraically, the summed loss for training
529

Hill & Doucet

point i can be expressed,
ℓSL,i =

X

θ∈(Θ−ϑi )

ℓSQ,i =

X

θ∈(Θ−ϑi )

 
ε − f T (xi )vθ (ϑi ) , 0

max


max

 2
ε − f T (xi )vθ (ϑi ) , 0

(5)
(6)

where SL stands for summed linear loss and SQ for summed quadratic loss. These are the
top two Subfigures in Figure 3. Using the same notation, the maximal loss for training
point i can be expressed,
	
max ε − f T (xi )vθ (ϑi ), 0
θ∈(Θ−ϑi )
n
2 o
=
max
max ε − f T (xi )vθ (ϑi ), 0

ℓM L,i =
ℓM Q,i

max



θ∈(Θ−ϑi )

(7)
(8)

where ML stands for maximal linear and MQ for maximal quadratic. These are the bottom
two Subfigures in Figure 3. From these expressions it is apparent that the ith summand of
the empirical loss (equation (4)) is bound by ε12 × ℓSQ,i, ε12 × ℓM Q,i, 1ε × ℓSL,i and 1ε × ℓM L,i .
While all of these loss arrangements can be cast in a transparent way into a SV framework,
in this work only ℓSL,i will initially be focussed on, as it has been most commonly adopted,
albeit implicitly, in previous contributions. ℓSQ,i will be discussed with respect to LSVC in
Subsection 6.3.
In terms of the practioner’s preferred approach, however, clearly the choice must be in
line with the underlying probabilistic model of the data. It seems unlikely that there will
be one best choice for all implementations. In the case that the practioner has no particular
idea about a model and just wishes to use some methodology to ‘get a feel’ for the data,
then presumably it is optimal to use the most computationally efficient approach as often
these approaches will converge to very similar results. To this end the approach outlined in
this paper is of interest as it describes methods which can potentially be used to speed all
loss cases.
2.3 Relative Class Knowledge
While the framework developed has been based on the assumption that all classes are to
be treated equally, this may not be desirable in some cases. There may be some prior
knowledge suggesting that some classes are, in some sense, closer to each other, and thus
more likely to be mistaken for each other. There may also be some reason for preferring to
err on the side of choosing one class over the others or over another at the cost of overall
accuracy.
A classical example of deeming it more important to choose one class over another
comes from the binary case of detection by radar. In military combat it is clearly extremely
important to detect incoming missiles or planes. As a result it is understandable that
a classification algorithm may be set up to return many more false positives than false
negatives. Hence errors made when classing enemy weaponry as unimportant are far more
heavily penalised than errors made in classifying nonthreatening phenomena as weaponry.
530

A Framework for Kernel-Based Multi-Category Classification

There are two ways to introduce relative class knowledge in the framework presented.
The first of these is the traditional method of error weighting, as introduced to the ‘alltogether’ SV framework by Lee et al. (2001). In this solution each different type of misclassification (e.g. classifying an input as θ instead of ϑ) has its error weighted by some
amount; Dθ (ϑ).
This approach of incorporating weights could equivalently be viewed as varying the
length of the vectors v, i.e. vθ (ϑ) → Dθ (ϑ)vθ (ϑ). An alternative, and possibly complementary, approach is to allocate to each class an unequal volume in the (M − 1)-dimensional
output space. This can be enabled by varying the angle between the class boundaries and
hence the orientation of the vectors v, i.e. vθ (ϑ) → Rθ (ϑ)vθ (ϑ) where Rθ (ϑ) is some rotation matrix. In doing this it may also be useful to incorporate a set of variable ε values
which, for some class ϑ are denoted {εθ (ϑ) : θ ∈ (Θ − ϑ)}, that is εθ (ϑ) is the size of the
margin on the ϑ side of the (ϑ, θ) boundary. Clearly the greater the volume allocated to
the class the more diverse the input vectors can be which are mapped to it.
Training Points

Found f(x)

8
Class A
Class B
Class C

Class A
Class B
Class C
Class Boundary

5

6

0

f2(x)

x

2

4

2

−5

0

−2
−10

−4
−10

−8

−6

−4

−2

0

2

4

6

8

−10

−8

−6

x1

(a) Classes in feature space

−4

−2

0
f1(x)

2

4

6

8

10

(b) Output result

Figure 4: A simple example illustrating a potential case for differently sized class areas. In
this arrangement the target area for class A could be increased.

Unfortunately it is not obvious how to construct a principled approach to determining
these different volumes. The key issue is the region of support that each class has in the
feature space. For instance in the case illustrated in Figure 4 it is not possible to find a linear
projection from the feature space which will separate the classes into the standard class
regions. However, by changing the class region sizes such a projection would be possible.
This may have the advantage of avoiding a more complicated feature space (possibly of
higher dimension).
2.4 Derivation of the SVC Optimisation Problem
Standard SV mappings of inputs to a higher dimensional feature space, Φ : X → F are used
in order to estimate the (M − 1)-dimensional function f (·). The mth element of f (·) is a
linear function in this feature space, characterised by weight vector wm and offset bm . To
531

Hill & Doucet

summarise,





f (x) = 







hΦ(x), w1 iF
hΦ(x), w2 iF
..
.
Φ(x), w(M −1)



F



 
 
+
 

b1
b2
..
.
b(M −1)





 = ψ(x) + b.


(9)

It is important to realise that, although some class separation is achieved by each component,
fm (·), accurate classification can only really be accomplished through the use of all elements,
together.
The optimisation problem which follows from the discussion in the previous Subsections
can be written (in standard SV form) as,

N
M
−1
X
X
X
1
Dθ (ϑi )ξi,θ 
kwm k2F + C
Minimise 
2
m=1
i=1 θ∈(Θ−ϑi )
 PM −1
m=1 (hΦ(xi ), wm iF + bm ) vθ,m (ϑi ) ≥ εθ (ϑi ) − ξi,θ , for i = 1, ..., N , θ ∈ (Θ − ϑi )
Subject to
ξi,θ ≥ 0, for i = 1, ..., N , θ ∈ (Θ − ϑi )
(10)


where the slack variable ξi,θ quantifies the empirical loss involved in mistaking the class
of point xi (which is ϑi ) for θ(6= ϑi ). C quantifies the trade-off between regularisation
(introduced by kwm k2F ) and this empirical loss, and vθ,m (ϑ) is the mth element of vθ (ϑ).
Framing equation (10) in terms of a Lagrangian gives,
N
M −1
X
1 X
2
L=
kwm kF + C
2
m=1

−

N
X

X

Dθ (ϑi )ξi,θ −

M
−1
X

αi,θ

X

ri,θ ξi,θ

i=1 θ∈(Θ−ϑi )

i=1 θ∈(Θ−ϑi )

X

N
X

(hΦ(xi ), wm iF + bm ) vθ,m (ϑi ) − εθ (ϑi ) + ξi,θ

m=1

i=1 θ∈(Θ−ϑi )

!

(11)

where {αi,θ , ri,θ : i ∈ (1, . . . , N ), θ ∈ (Θ − ϑi )} are Lagrangian multipliers. It is standard
in SV methodology to find the optimal solution to this by first finding the Wolfe dual,
and then maximising with respect to the dual variables, namely the Lagrangian multipliers
(Cristianini & Shawe-Taylor, 2000; Vapnik, 1998, for example). First let V(ϑ) denote a
(M − 1) × (M − 1) matrix with columns given by the vectors vθ (ϑ),
V(ϑ) =



vA (ϑ) vB (ϑ) . . . vθ6=ϑ (ϑ) . . .

∗T (ϑ).
and represent the mth row of V(ϑ) by vm



(12)

Lemma 1 The dual to the Lagrangian presented in equation (11) is,
N

N

N

X
1 XX T T
LD = −
αTi ε(ϑi )
αi V (ϑi )V(ϑj )αj K(xi , xj ) +
2
i=1

i=1 j=1

532

(13)

A Framework for Kernel-Based Multi-Category Classification

where,
αi =
ε(ϑi ) =





αi,A αi,B . . . αi,θ6=ϑi . . .

T

εA (ϑi ) εB (ϑi ) . . . εθ6=ϑi (ϑi ) . . .

(14)
T

(15)

and the kernel function has been denoted K(xi , xj ) = hΦ(xi ), Φ(xj )iF . The derivation of
equation (13) also introduces the constraints that,
CDθ (ϑi ) ≥ αi,θ ≥ 0, ∀i, θ ∈ (Θ − ϑi )

(16)

N
X

(17)

V(ϑi )αi = 0.

i=1

The derivation of this is presented in a technical report by the authors Hill and Doucet
(2005, App. A).
It also remains to confirm that this optimisation problem has a unique maximum, that
is that the problem is unimodal. This will be the case if it can be shown that the quadratic
term in equation (13) is effectively equivalent to a quadratic expression involving a positive
definite matrix. This is the case, as shown by Hill and Doucet (2005, App. B).
A final issue to consider is that of rotational invariance to the structuring of the problem
— as initially raised in Subsection 2.1. Note that the only influence of rotational orientation
in equation (13) is through the summation term αTi VT (ϑi )V(ϑj )αj K(xi , xj ). Consider now
that the chosen orientation is rotated in some way as described by a rotation matrix R,
this quadratic term then becomes,
αTi VT (ϑi )RT RV(ϑj )αj K(xi , xj ) = αTi VT (ϑi )V(ϑj )αj K(xi , xj )

(18)

due to the fact that rotation matrices are orthonormal. There is one further aspect that
should be considered, namely the constraints in equation (17), however it is clear that
these will not be affected by rotation either. Hence the optimisation problem is rotationally
invariant.
A related issue is that the geometric structure implicitly introduces ordinal regression
along the (M −1) axes. That is, when looking for example, at the three-class case illustrated
in Figure 1 there are essentially two real valued outputs i.e. f1 (·) and f2 (·). Now, along any
horizontal or vertical line for which one of these is held constant, the other is outputing a
value, and the region into which this value falls determines the class assignment. This gives
the impression of ordinal regression as using ranges of a single value output to determine
between more than two classes is the essence of that approach.
This raises two questions — is the methodology presented subject to the same problems
as ordinal regression? And, when looking at the structure in this way, does the fact that it
can potentially be arranged quite asymmetrically, and appears arbitrary cause concern?
The answer to both questions is ‘No’. The very aim of structuring the ouput space in
this way has been to avoid the situation encountered in ordinal regression in which classes
are not all equivalently compared against each other. Furthermore, it should be clear from
the rotational invariance of the structure, that the particular orientation chosen is not going
to affect the optimisation problem in any way whatsoever.
533

Hill & Doucet

Note that with the introduced terminology then the function f (·) can be expressed,
f (x) =

N
X

V(ϑi )αi K(x, xi ) + b.

(19)

i=1

This is clearly a very natural extension of the binary framework, a comparison with previous
similar contributions forms the next Section. The offset, b can be determined through
realising that the non-extremal Lagrangian coefficients αi,θ lie on the edge of the zero-loss
region — this is analogous to finding b in the two-class case.

3. Discussion of Previous SV Approaches
There are three main methods for applying binary classification techniques to the more
generic multi-category problem. These are the one-against-all method, the pairwise coupling
method and the method of Error Correcting Output Codes (ECOCs). As will be discussed
here, all have been applied in conjunction with SV methods. An extensive literature review
forms a large part of work by Rifkin and Klautau (2004); by contrast, in this Section the
various methods are discussed with respect to the approach described in Section 2.
Essentially, while the one-against-all and pairwise methods can be made to work well,
they invariably require a heuristic component to resolve issues associated with combining
results. Many authors have tried to overcome this by framing the problem as a single
optimisation, as is also done in Section 2, however these approaches are substantially slower
to converge. A key contribution of this work is to demonstrate that a consistent3 result can
be obtained by mapping pairwise results in an ad hoc way into the framework of Figure 1
and ‘fine-tuning’ the result, to a consistent final optimum. This provides a combination of
fast training and consistency.
Contributions such as that by Rifkin and Klautau (2004) argue that one-against-all and
pairwise methods can be made to perform practically as well as other methods. This may
well be the case, depending on the implicit model behind the heuristics involved and should
come as no surprise. Indeed, for many quick black-box implementations this approach may
well be optimal.
Often however it is desirable to have a clear understanding of the optimisation process
and a significant contribution of the framework presented is that within it many single optimisation SV methods can be understood and compared directly. Further, it is a framework
in which multi-category versions of other algorithms can be formulated and understood in
a consistent way, as will be discussed in Section 6. Finally, the very fact that so many
different efforts have been made to find a method involving a single optimisation which is
competitive in terms of speed is in itself evidence of a desire by the research community to
overcome heuristic solutions.
In this Section the one-against-all method is reviewed in Subsection 3.1, pairwise coupling in Subsection 3.2 and ECOCs in Subsection 3.3. Efforts to develop single optimisation
3. Here, and in subsequent usage, we use ‘consistent’ and ‘consistency’ to refer to the fact that while some
approaches can be quite ambiguous in exactly which class to output, hence the need for a heuristic to
choose, an ‘all-together’ solution should not suffer from this i.e. the results are consistent with each
other. Fisher consistency is discussed in Section 4.

534

A Framework for Kernel-Based Multi-Category Classification

approaches, known as ‘all-together’ methods are discussed in Subsection 3.4. How they relate to the framework presented is also clarified therein.
3.1 The One-Against-All Method
The one-against-all method has received the most attention from the SV community, and
was also the earliest approach considered. The idea is to generate M classifiers. Of these,
classifier i determines whether or not an input belongs to class i. An obvious stumblingblock is the case that more than one classifier determines that a particular input should
belong to its class of interest. Hence it is important to have in place some technique for
either avoiding or resolving this problem.
Early implementations (Schölkopf, Burges, & Vapnik, 1995; Blanz, Schölkopf, Bülthoff,
Burges, Vapnik, & Vetter, 1996) used the underlying real-valued output, choosing the highest such output to indicate the strongest ‘likelihood’ of class membership. A variant on this
was introduced by Mayoraz and Alpaydın (1999) in an attempt to make these outputs more
reliably comparable; see also Wang, Vuurpijl, and Schomaker (2000).
As an aside, note that a function f (·) found in the framework proposed in Section
2 can be used to produce a one-against-all classifier. To see this consider the function
fθ (·) = yT (θ)f (·) where y(θ) is the class-target vector for class θ. A new input x would
then be classed θ if fθ (x) is the largest such scalar function.
3.2 The Pairwise Coupling Method
The pairwise coupling method involves finding M (M2 −1) different classifiers, each of which
compares one class against another. Given an input, the output class is decided through a
vote or some similar method. One particular problem to be addressed is the circular one
in which, for example, the AB classifier chooses class A, the AC classifier class C and the
BC classifier class B.
A few authors (Hastie & Tibshirani, 1998; Platt, Cristianini, & Shawe-Taylor, 2000;
Fürnkranz, 2002) have proposed heuristics for resolving any such problems, however the
most substantive purely SV approach seems to be that by Kreßel (1999). This technique
considers a classifier vote and, in the case of a tie, the real-valued outputs are referred to.
This has the downside that the M (M2 −1) classifiers are found independently, and so such a
comparison is not always meaningful. Nonetheless, as is shown by Kreßel and supported by
Allwein et al. (2001), Hsu and Lin (2002) and Rifkin and Klautau (2004), this appears to be
an effective methodology and faster than ‘all-together’ methods by a considerable margin.
An alternative approach is presented by Allwein et al. (2001) in their work unifying pairwise,
one-against-all and class codes — this is discussed in Subsection 3.3.
It is interesting to note that a function f (·) found in the framework proposed in Section
2 can be used to find a classifier between two classes A and B. To see this consider the
T (B)f (·) where v (B) is as defined in equation (3). A new input x
function fAB (·) = vA
A
would then be classed A if fAB (x) ≤ 0 and B otherwise. Results such as these will be used
in Section 7 to construct the ‘best of both worlds’ approach.
535

Hill & Doucet

3.3 Class Codes
The underlying idea behind ECOCs is to assign to each class a particular binary code and
then to train individual classifiers to identify each bit in the code (Sejnowski & Rosenberg,
1987; Dietterich & Bakiri, 1995). Eight classes can, for example, be each assigned a 3bit code ([−1, −1, −1], [−1, −1, +1], etc.). In general at least ⌈log2 M ⌉ bits are required.
Dietterich and Bakiri (1995) propose using more than M bits in order that small errors can
be corrected.
ECOC SV methods have been described by Kindermann, Leopold, and Paaß (2000) and
Rennie and Rifkin (2001). Minimum length code methods have been presented by Sebald
(2000), Sebald and Bucklew (2001) and by Suykens and Vandewalle (1999). However these
implementations often have the problem that classes are treated inconsistently. This is due
to the fact that such codes will have smaller Hamming distances between some classes than
others. Their approach becomes very much like that of utilising Ordinal Regression (see,
for example, Crammer and Singer (2001b) or Herbrich, Graepel, and Obermayer (2000b)
for more, in a SV context) to perform the classification and performance becomes dependent on the ordering of the labels (Weston, 1999). Essentially ordinal regression performs
classification based on the region of R in which some scalar output lies. For instance, a
class of A may be assigned to some input x if f (x) ∈ [a, b), class B if f (x) ∈ [b, c), and so
on.
To see the parallel between this and coding approaches consider the four-class case with
minimum length codes, such that,
y(A) =
y(C) =




−1 −1
+1 −1





y(B) =
y(D) =





−1 +1
+1 +1





.

Two functions f1 and f2 need to be found which corresspond to the first and second elements
of the codes respectively. However class D is clearly further from class A than from classes
B and C. Hence the comparison between classes is again no longer consistent. Although
this is less extreme than ordinal regression the main problem that classes are not compared
in an equivalent way remains. For this reason, as well as the lack of a computational or
accuracy advantage, these methods have not been particularly popular.
Allwein et al. (2001) have shown that pairwise and one-against-all methods can be viewed
as special cases of ECOC approaches4 . Indeed it is only when the code length equals (the
one-against-all case) or exceeds M that the inconsistency problem described above can be
made to disappear. Even when viewing pairwise and one-against-all approaches as special
cases of ECOC Allwein et al. (2001) still must employ a heuristic (in this case code-based)
to find the final answer.
3.4 ‘All-Together’ Methods
A consistent result can be obtained by arranging the multi-category problem such that
there is a single optimisation to perform. These are described by Hsu and Lin (2002) as
‘all-together’ methods and a number of authors (Bredensteiner & Bennett, 1999; Crammer
4. An exception to the binary nature of the code in this formulation being the case of pairwise comparison
when code word elements take values from {−1, 0, 1}.

536

A Framework for Kernel-Based Multi-Category Classification

& Singer, 2001a; Guermeur, 2000, 2002; Weston & Watkins, 1999; Weston, 1999; Vapnik,
1998) present a variety of such methods. To see that many of these relate to that described
in Section 2, note that their aim is to find M functions {fϑ′ (·) : ϑ ∈ Θ} such that, class(x) =
arg maxϑ {fϑ′ (x) : ϑ ∈ Θ}. Weston and Watkins (1999) aim to find functions of the form
fϑ′ (·) = hΦ(·), wϑ′ i + b′ϑ by
N
X
X
1X
′
ξi,θ
kwθ′ k2F + C
2
i=1 θ∈(Θ−ϑi )
θ∈Θ

 

′
′ , θ ∈ (Θ − ϑ )
Φ(xi ), wϑi F + bϑi ≥ hΦ(xi ), wθ′ iF + bθ + 2 − ξi,θ
i
Subject to
′
ξi,θ ≥ 0

Minimising

(20)

and it has been shown in detail by the authors (Hill & Doucet, 2005, App. C) that this
optimisation arrangement is identical to that in Section 2 when fϑ′ (·) = yT (ϑ)f (·) where f (·)
is as introduced in Section 2. Furthermore, the other ‘all-together’ approaches mentioned,
with the exception of Crammer and Singer (2001a), have been shown by Guermeur (2002)
to converge to the same solution. However a key problem with these algorithms is that their
resulting kernel expressions can be quite complicated in that the framing of the optimisation
process leads to convoluted expressions.
As an alternative Crammer and Singer (2001a) propose an ‘all-together’ one-against-all
method with the maximal loss ℓM L,i in equation (7) (see also Figure 3) for which they give
a new optimisation algorithm. In their comparative work Hsu and Lin (2002) find it hard to
draw definitive conclusions about the Crammer and Singer (2001a) approach in comparison
to the others as they note variable performance with regard to optimisation times required.
They also suggest algorithmic improvements to the more traditional methods, but eventually
conclude that, of available techniques, pairwise coupling methods (see Section 3.2) are much
faster and appear more suitable for practical use.
While the standard form of the methodology introduced in Section 2 results in an optimal
solution equivalent to other ‘all-together’ approaches two key points differentiate it. The
first is that it has increased flexibility in that it can incorporate the approaches described in
Subsection 2.3 without any increased computational effort. The second is that it can easily
take advantage of the relatively much faster pairwise methods. This is discussed further in
Subsection 7.4.
3.4.1 Another ‘All-Together’ Approach
Lee et al. (2001, 2004) have presented a unique ‘all-together’ approach which uses M -length
target codes. For classes Θ = {A, B, . . . } these take the form

y′′ (A) =
y′′ (B) =





1

−1
M −1

−1
M −1

1

...
−1
M −1

537

−1
M −1

...



−1
M −1



Hill & Doucet

and so on. The resulting optimisation problem posed is to
N
X
X
1X
′′
ξi,θ
kwθ′′ k2F + C
2
i=1 θ∈(Θ−ϑi )
θ∈Θ

′′
′′
hΦ(xi ), wθ iF + b′′θ − yi′′ (θ) ≤ ξi,θ
 P

′′
′′
Subject to
hΦ(xi ), wθ iF + bθ = 0
 ′′ θ∈Θ
ξi,θ ≥ 0

Minimise

(21)




T
′′ i + b′′
′′ i + b′′
hΦ(·), wA
where output is given by f ′′ (·) =
hΦ(·),
w
...
=
A
B F
B
F
 ′′
T
′′
′′
′′
′′
W Φ(·) + b with W = wA wB . . . , and the class membership determined by
observing which element of f ′′ (·) is maximal. This approach can be understood in the
framework of Section 2 when, similarly to Subsection 3.4 fϑ′′ (·) = yT (ϑ)f (·) where f (·) is as
introduced in Section 2. This has been discussed further by Hill and Doucet (2005, App.
C), where it is shown that setting vA (B) = −y(A) and ε = M1−1 causes the optimisation
problem in equation (21) to be the same as the generic approach in equation (10).

4. A Brief Look at Fisher Consistency
The Lee et al. (2001, 2004) (Subsubsection 3.4.1) approach has the key feature that it
is Fisher consistent5 . This has recently been discussed in the context of multicategory
classification by Tewari and Bartlett (2007), Zhang (2004a, 2004b). In this Section we
simply aim to show how some of the key results of these authors can be understood in the
framework presented in Section 2.
In considering Fisher consistency in the multicategory case we first define the vector
T

T 
(22)
= P (ϑ = A|x) P (ϑ = B|x) . . .
p(x) = pA (x) pB (x) . . .

this is analogous to the vector p of Tewari and Bartlett (2007), or the vector P(·|X) of
Zhang
(2004b, Eqn.(7)). We also express the empirical loss function, by ℓ (ϑ, f (x)) =
P
T
θ∈(Θ−ϑ) max ε − vθ (ϑ)f (x) , 0 , cf. equation (5), and define the vector ℓ by,
ℓ(f (x)) =



ℓ (A, f (x)) ℓ (B, f (x)) . . .

T

.

(23)

With this notation then the ‘ℓ-risk’, in the terminology of Tewari and Bartlett (2007) is
given by,


EX ×Θ [ℓ (ϑ, f (x))] = EX EΘ|X [ℓ (ϑ, f (x))]
(24)


= EX pT (x)ℓ (f (x))

and the optimal classification rule is to choose class ϑ∗ = arg maxθ [pθ (x)]. A Fisher
consistent multicategory classifier will have g(x) = Y T f (x), cf. Subsection 3.4, such that,
ϑ∗ = arg max [pθ (x)] = arg max [gθ (x)]
θ

θ

(25)

5. This is referred to as classification calibrated by Tewari and Bartlett (2007) and infinite-sample consistent
by Zhang (2004b).

538

A Framework for Kernel-Based Multi-Category Classification

Tewari and Bartlett (2007) illustrate the two class case of this with reference to equation
(24), in that they plot ℓ (A, f (x)) against ℓ (B, f (x)) where y(A) = −1 and y(B) = +1.
Bartlett, Jordan, and McAuliffe (2004) have shown that provided that this plot is differentiable at f (x) = 0 then consistency is attained. If this is not the case then there is more
than one tangent to the plot at f (x) = 0. This is noteworthy because for a particular x,
the value of f (x) which minimises the inner product of equation (24) is determined by the
point at which a line with slope − ppBA (x)
(x) is tangent. If the plot is not differentiable then
sample x with pA (x) > pB (x), and sample x′ with pA (x′ ) < pB (x′ ) may both have the same
value f (x) = f (x′ ) = 0 even though f minimises the ℓ-risk.
Whereas such a plot is a straightforward 2D plot, in considering a three class case we
must turn to 3D surfaces and consider tangent planes with normal given by p(x) e.g. Tewari
and Bartlett (2007, Figs.2&3). In these it is illustrated that most ‘all-together’ methods
are inconsistent including those by Weston and Watkins (1999), Weston (1999) and by
Crammer and Singer (2001a). However, it is also the case that the approach of Lee et al.
(2001, 2004) is consistent.
In order to better understand what is happening, consider the similarities between the
method introduced by Weston and Watkins (1999), Weston (1999), and that of Lee et al.
(2001, 2004) (§3.4). They both have the same additive approach to forming the loss function
(in contrast to the maximum approach of Crammer & Singer, 2001a). The key difference
is in their choice of vectors v. While Weston and Watkins (1999), Weston (1999) use, for
example vA (B) ∝ [y(B) − y(A)], Lee et al. (2001, 2004) use e.g. vA (B) = −y(A). In fact
we can also consider using other such vectors — either some combination of these, or more
extreme versions. Examples of resulting loss functions are shown in Figure 5, cf. Figure 3.
What becomes interesting in terms of Fisher consistency is what happens when these
plots of loss contours for all classes are overlaid. Before doing this consider the plots for
class C in Figure 5. In particular consider the Lee et al. (2001, 2004) (LLW) case. Here we
can clearly identify four regions — that with zero loss, that with loss due only to class A,
that with loss due only to class B and that with combined loss. In overlaying contour plots
we will similarly seek to identify regions.
This is presented in Figure 6, with the regions seperated by solid black lines. It can
be seen that the regions identified in the Weston and Watkins (1999), Weston (1999) and
Lee et al. (2001, 2004) plots correspond to planes (and edges) in Figures 2(b) and 3(b) of
Tewari and Bartlett (2007). Further, in keeping with their discussion it becomes clear that
potential inconsistency problems occur when such region boundaries intersect on the class
boundaries. The reason that the Lee et al. (2001, 2004) case manages to avoid this is that
region boundaries coincide in this particular setting. The scenarios illustrated in Figure 6
correspond to the following example vectors v, from left to right, top to bottom;

−0.2y(B) − y(A)




−y(A)



y(B) − 5y(A)
vA (B) ∝
y(B) − 2y(A)





y(B) − y(A)


y(B) − 0.5y(A)

539

The
The
The
The
The
The

‘Excess LLW’ case.
‘LLW’ case.
‘LLW to WW A’ case.
‘LLW to WW B’ case.
‘WW’ case.
‘Excess WW’ case.

(26)

Hill & Doucet

Excess LLW

Lee Lin and Wahba

Excess WW

3

3

3

2

2

2

1

1

1

0

0

0

−1

−1

−1

−2

−2

−2

−3
−3

−2

−1

0

1

2

3

−3
−3

−2

−1

0

1

2

3

−3
−3

−2

−1

0

1

2

3

Figure 5: A further illustration of losses with respect to class C. These correspond to
changing vectors v. Recall that the Weston and Watkins (1999), Weston (1999)
(WW) case had loss contours parallel to class boundaries (Figure 3)

While it is clear that all cases from ‘LLW to WW A’ onwards in Figure 6 will be inconsistent,
a question remains over the ‘Excess LLW’ case. To further investigate this we have created
plots similar to those in Figures 2 and 3 of Tewari and Bartlett (2007), as shown in Figure
7. From this it is clear from the reverse view that the labelled ‘Point of Interest’ is again
going to pose a consistency problem.
These results give quick geometric insight into why it is that the Lee et al. (2001, 2004)
approach appears to be the only Fisher consistent approach involving summed linear losses.
We have not performed a similar investigation of the effect of changing v within the context
of the Crammer and Singer (2001a) framework as it seems clear that there will always be a
problem at the central point for all reasonable choice of vectors v, cf. Tewari and Bartlett
(2007, Fig. 2a).

5. Generalisation Bounds
An important aspect of many kernel based algorithms such as SVC, is that Structural Risk
Minimisation (SRM) ideas can be applied in order to obtain distribution-free bounds on
performance. Such an approach underlies the initial work on SVC in particular, and results
in ideas such as the Vapnik Chervonenkis (VC) dimension.
In this section we build on the body of work which is concerned with bounding the
performance of multicategory classifiers. This was originally published by Guermeur (2002),
but it is important to realise that this paper draws heavily from the work of Elisseeff,
Guermeur, and Paugam-Moisy (1999). Further insight is also to be found in work by
Paugam-Moisy, Elisseeff, and Guermeur (2000).
540

A Framework for Kernel-Based Multi-Category Classification

Excess LLW

Lee Lin and Wahba

LLW to WW A

3

3

3

2

2

2

1

1

1

0

0

0

−1

−1

−1

−2

−2

−2

−3
−3

−2

−1

0

1

2

3

−3
−3

−2

LLW to WW B

−1

0

1

2

3

−3
−3

Weston and Watkins
3

3

2

2

2

1

1

1

0

0

0

−1

−1

−1

−2

−2

−2

−2

−1

0

1

2

3

−3
−3

−2

−1

0

1

−1

0

1

2

3

2

3

Excess WW

3

−3
−3

−2

2

3

−3
−3

−2

−1

0

1

Figure 6: Region Identification — These six different cases represent a rotation of the vectors v, starting at the top left the progression passes through the Lee et al. (2001,
2004) case (middle top), then to the bottom left and through the Weston and
Watkins (1999), Weston (1999) case (middle bottom).

(a) Front View

(b) Reverse View

Figure 7: Loss surfaces of the ‘Excess LLW’ case in Figure 6

541

Hill & Doucet

By using the geometric approach presented above, it becomes possible to reduce the
multidimensional bounding problem to a scalar problem, and thus to fully utilise the more
traditional approaches to bounding. These approaches are also drawn on by Elisseeff et al.
(1999), however by viewing the problem in the manner proposed here it becomes possible
to adopt them virtually unchanged. The key references for this work are by Bartlett (1998)
and Williamson, Smola, and Schölkopf (2001).
The finalP
result of this working is to demonstrate that a the bound derived is dependent
−1
2
on the term M
i=1 kwi kF , cf. equation (10). This is in keeping with the results of Guermeur
(2002), and Elisseeff et al. (1999), as well as traditional two-class bound analyses (Schölkopf
& Smola, 2002). Note that some of the notation in this Section is inconsistent with that
used elsewhere in this paper, however the difference should be apparent.
5.1 Basic Definitions
We have as a starting reference the canonical function of Elisseeff et al. (1999), PaugamMoisy et al. (2000), and Guermeur (2002), rewritten in the present notation. In doing this
we introduce also the M −dimensional vector yc which has elements,

−1 if the input has class other than θ
c
yθ =
+1 if the input has class θ.
We further introduce the function g : X → RM ,
g(·) = Y T f (·)

(27)

where Y is a matrix with columns of class target vectors y, cf. Section 3.
Definition 1 (The Original Canonical Function) Define R1 (x) to be an index such
that gR1 (x) (x) = maxθ gθ (x) and R2 (x) to be an index such that gR2 (x) (x) = maxθ6=R1 (x) gθ (x).
The canonical function ∆g : X → RM , is then given by,
( 
 1
T
1
T
if θ = R1 (x)
2 gθ − gR2 (x)  = 2 [y(θ) − y(R2 (x))] f (x) = κvR2 (x) (θ)f (x)
∆gθ (x) =
T
1
1
T
otherwise.
2 gθ − gR1 (x) = 2 [y(θ) − y(R1 (x))] f (x) = κvR1 (x) (θ)f (x)
(28)
where κ is a constant of proportionality.
T
Clearly, if this example has been classified correctly, all the terms κvR
(θ)f (x) should
1 (x)
T
be negative and κvR2 (x) (R1 (x))f (x) should be positive. Paugam-Moisy et al. (2000), Guermeur (2002) define the margin by ε = minθ yθc ∆gθ (x), however, recall that vA (B) =
−vB (A), and so ∆gR1 (x) (x) = −∆gR2 (x) (x). Hence, if R1 (x) or R2 (x) is the actual class
c
c
of x then yR
∆gR1 (x) (x) = yR
∆gR2 (x) (x). In the case that neither of them are the
1 (x)
2 (x)
correct class then it can be demonstrated that the margin is not going to be determined by
∆gR1 (x) (x) uniquely, and so this term does not need to be ever considered. This being the
case then the margin is simply given by minθ6=R1 (x) κvθT (R1 (x))f (x).
This definition of Paugam-Moisy et al. (2000), Guermeur (2002) is somewhat nonintuitive, as it does not make reference to the actual class of the point, merely to R1 (x),
which may not be equal to the class ϑ. This is equivalent to defining the margin of the

542

A Framework for Kernel-Based Multi-Category Classification

two-class classifier as the absolute value of the function f (x). This is not something which
appears in the mainstream texts of Vapnik (1998, p. 402), Schölkopf and Smola (2002, p.
142) or Hastie, Tibshirani, and Friedman (2001, p. 110), for instance. However the general
ideas in these and other texts, and the approach in Section 2 can be related to in Definition
2, which is from Paugam-Moisy et al. (2000, Defn. 6) and Guermeur (2002, Defn. 5). In
anticipation of this we introduce an alternative canonical function ∆f (x, ϑ),
∆f (x, ϑ) = κ · VT (ϑ)f (x)

(29)

where V(ϑ) is as given in equation (12). If x is correctly classified then all elements of ∆f
should be positive.
Definition 2 (Empirical Margin Risk) Conceptually, the empirical margin risk is the
fraction of training samples whose positioning in the (M −1)− dimensional space lies outside
their region of zero-loss. Formally this is expressed for some fixed margin ε > 0 and some
training set S = {xi , ϑi }N
i=1 as,
RSε (f ) =

1
|{(xi , ϑi ) : ∃θ ∈ (Θ − ϑi ) , ∆fθ (xi , ϑi ) < ε}|
N

(30)

A further definition which will be used is that of a pseudo-metric. In this definition note
1
P
that ℓp denotes the norm kxkℓp = ( i |xi |p ) p .
Definition 3 (Pseudo-Metric) Let F : X × Θ → RM −1 be a set of functions and (f , f ) ∈
,S
F 2 . For a set S of points in X × Θ, define the pseudo-metric dF
ℓ∞ ,ℓ1 by,
,S
dF
ℓ∞ ,ℓ1 (f , f ) = max

(x,ϑ)∈S

X

θ∈(Θ−ϑ)



fθ (x, ϑ) − f θ (x, ϑ) .

(31)

We now define the covering number (Vapnik, 1998; Schölkopf & Smola, 2002, for example).

Definition 4 (Covering Number) Let F , dF ,S be a pseudo-metric space, and B(f̂ , r)

the closed ball in F with radius r and centre f̂ . The covering number N ε, F, dF ,S of a
set F ∈ F is the smallest cardinality of set F such that,
[
F⊂
B(f̂, ε).
(32)
f̂ ∈F

The sets F satisfying this property are called ε−covers of F: each element in F is at a
distance less than ε of an element in F. With |S| = 2N then define also,
ε

ε

,S
Np,q
, F, 2N = sup N
, F, dF
ℓp ,ℓq .
2
2
S∈X 2N
543

Hill & Doucet

5.2 Presentation of Bounds
We will use this final definition in consideration of,
ε

∆f (x, ϑ) =



ε · sign [minθ ∆fθ (x, ϑ)] ,
minθ ∆fθ (x, ϑ),

if | minθ ∆fθ (x, ϑ)| ≥ ε
otherwise,

(33)

which is analogous to the definition of ∆gε used by Elisseeff et al. (1999, §4), Paugam-Moisy
et al. (2000, §6) and Guermeur (2002, §2). This can be used to define the set of scalar-valued
functions,
∆F ε = {∆f ε : f ∈ F}
(34)
leading to the Theorem 1, below. The advantage of the approach presented here is that
the function ∆f ε is a scalar and, as such, it is a lot more straightforward to use the proof
structure outlined by Bartlett (1998, Lemma 4) cf. Elisseeff et al. (1999, Cor. 2), PaugamMoisy et al. (2000, Cor. 1), Guermeur (2002, Thm. 1). This is elaborated on, and the two
different approaches contrasted by Hill (2007, §4.2,§A.2)
Theorem 1 With probability at least (1 − δ), for every value of ε in (0, 1], the risk R(f ) of
a function f computed by a numerical M −class discriminant model F trained on a set of
size N (denoted SN ), is bounded above by
R(f ) ≤

RSε N (f )

+

s

 
 

ε
2
1
1
ε
, ∆F , 2N
+ log
log 2N∞,1
+
2N
2
εδ
N

(35)

Proof of Theorem 1
The starting point of the proof is equivalent to that in Elisseeff et al. (1999, Eqn. (5)),
namely, for any λ,
PSN










1
sup R(f ) − RSε N (f ) ≥ λ ≤ 2×PSN ,SeN sup RSεe (f ) − RSε N (f ) ≥ λ −
. (36)
N
N
f ∈F
f ∈F

Now the aim is to bound the right-hand side of this and the starting point is to consider all
permutations σ over (X × Θ)2N such that σ realises a transposition between two elements
of the same ranking in SeN and SN . Let U be the uniform distribution over the set of all
such permutations σ and so,
PSN ,SeN









1
1
ε
ε
ε
ε
≤ sup U σ : sup RSeσ (f ) − RSNσ (f ) ≥ λ −
.
sup RSe (f ) − RSN (f ) ≥ λ −
N
N
N
N
f ∈F
f ∈F
S ,Se
N

N

ε

ε

Denote a 2ε −cover of the set ∆F ε by ∆F , with elements ∆f . Through identical reasoning
to that of Bartlett (1998, proof of Lemma 4) by defining,
 ε o

 ε
1 n  ε σ σ


σ
,
A ∆f , SN

 i : ∆f (xi , ϑi ) − ε ≥
N
2
544

A Framework for Kernel-Based Multi-Category Classification

then the above inequality leads to,




1
PSN ,SeN sup RSεe (f ) − RSε N (f ) ≥ λ −
N
N
f ∈F
(
)



 
1
ε
ε
σ
σ
≤ sup U σ : sup
− A ∆f , SeN
≥λ−
A ∆f , SN
ε
ε
N
e
∆f ∈∆F
SN , SN
  





1
ε
ε eσ
ε 
σ

≥λ−
sup U σ : A ∆f , SN − A ∆f , SN
≤ ∆F
.
ε
ε
N
∆f ∈∆F


ε
Now by definition ∆F  = N∞,1 2ε , ∆F ε , 2N and so it can be seen that this leads to




1
PSN ,SeN sup RSεe (f ) − RSε N (f ) ≥ λ −
N
N
f ∈F
!
(37)
ε

X
1
1
ε
≤ N∞,1
(ai − bi ) βi ≥ λ −
, ∆F , 2N × sup P
2
N
N
(ai ,bi )
i

where βi ∈ {−1, +1}, P (βi = −1) = P (βi = +1) = 0.5 and they are Independent, and
Identically Distributed (IID). Meanwhile




(
 ε σ eσ

ei , ϑi − ε ≥ 2ε
1 if ∆f x
ai =
0 otherwise

and

bi =

(



 ε

if ∆f (xσi , ϑσi ) − ε ≥
0 otherwise.
1

ε
2

Now the right-hand term in equation (37) can be bounded using Hoeffding’s inequality
(Elisseeff et al., 1999, Theorem 5) such that,


2 !



ε

1
1
, ∆F ε , 2N × exp −2N λ −
.
PSN ,SeN sup RSεe (f ) − RSε N (f ) ≥ λ −
≤ N∞,1
N
N
2
N
f ∈F
(38)

This can be rearranged to demonstrate that,
s

 



ε


1
1
1
ε
ε
ε
λ≤
, ∆F , 2N
− log 2PSN ,SeN sup R e (f ) − RSN (f ) ≥ λ −
log 2N∞,1
+
SN
2N
2
N
N
f ∈F

and so, from equation (36)
s



 



ε
1
1
ε
+ .
, ∆F ε , 2N
− log PSN sup R(f ) − RSN (f ) ≥ λ
log 2N∞,1
λ≤
2N
2
N
f ∈F




Now, with probability at least (1 − δ) where δ = PSN supf ∈F R(f ) − RSε N (f ) ≥ λ , it is
the case that R(f ) − RSε N (f ) ≤ λ. Hence, with probability at least (1 − δ)
r
ε

i
1 h 
1
ε
R(f ) ≤ RSN (f ) +
log 2N∞,1
, ∆F ε , 2N
− log (δ) + .
(39)
2N
2
N
545

Hill & Doucet

which is analogous to the result of Theorem 4 of Elisseeff et al. (1999). Using this together
with Proposition 8 of Bartlett (1998) demonstrates that,
s
 
 

ε
2
1
1
ε
ε
R(f ) ≤ RsN (f ) +
, ∆F , 2N
+ log
(35)
log 2N∞,1
+
2N
2
εδ
N
This concludes the proof of Theorem 1.
5.2.1 Bounding N∞,1

ε
ε
2 , ∆F , 2N



using Entropy Numbers

While the generalised risk of Theorem
 1 can be bounded by an expression involving the
covering number, N∞,1 2ε , ∆F ε , 2N , it is not clear how to determine this number exactly,
and a standard approach is to bound it. This is done by following the ideas of Williamson
et al. (2001). The first step is to define entropy numbers Guermeur (2002, Defns. 7 & 8),
Williamson et al. (2001, eqns. 7-10).
Definition 5 (Entropy Numbers and Operator Norm) Given a pseudo-metric space
,S
F ,S
(F , dF
ℓ∞ ,ℓ1 ) then, the nth entropy number of a set F ⊂ F with respect to dℓ∞ ,ℓ1 , is

o
n

,S
≤
n
(40)
εn (F) , inf ε > 0 : N ε, F, dF
ℓ∞ ,ℓ1

The entropy number of an operator T : F → M follows from the introduction of a unit ball
in F , denoted UF . The nth entropy number of T is defined as,
εn (T ) , εn (T (UF ))

(41)

kT k = sup kT (f )kM .

(42)

and the operator norm is given by,
f ∈UF

To understand the entropy number of T more explicitly, denote T (UF ) by a set M ∈ M ,
and assume some metric dM . With
these then, in keeping


	with equation (40), the entropy
M
number is given by εn (T ) , inf ε > 0 : N ε, M, d
≤n .

Note that from the first
 definition it is clear to see that should εn (F) be bounded
 part of this
F ,S
by some ε, then N ε, F, dℓ∞ ,ℓ1 ≤ n; Guermeur (2002, Thm. 3), Williamson et al. (2001,

ε
ε , 2N , it is
Prop. 12). Note also that from Definition
4,
in
order
to
bound
N
,
∆F
∞,1
2

,S
sufficient to bound N 2ε , ∆F ε , dF
ℓ∞ ,ℓ1 , as discussed in the following Theorem.
 

,S
Theorem 2 (Bound on log N 2ε , ∆F ε , dF
) The log of the covering number
ℓ∞ ,ℓ1


PM −1
,S
2
N 2ε , ∆F ε , dF
m=1 kwm kF ,
ℓ∞ ,ℓ1 of a set F ∈ F can be bounded by a term proportional to
i.e.
M
−1

 ε
X
,S
kwm k2F
(43)
≤
r
log N
, ∆F ε , dF
ℓ∞ ,ℓ1
2
m=1

for some r > 0.

546

A Framework for Kernel-Based Multi-Category Classification

Proof of Theorem 2
The proof begins with the fact, as highlighted by Williamson et al. (2001, Thm. 10) (see
also Guermeur, 2002, Thm. 4), that Maurey’s Theorem can be used to bound entropy
numbers. For this Theorem note that ℓqp is a vector space containing vectors of dimension
1
P
q and norm kf kℓqp = ( qi=1 |fi |p ) p . Furthermore T (F , M ) denotes the set of all bounded
operators between the normed spaces (F , k · kF ) and (M , k · kM ).
Maurey’s Theorem considers that T ∈ T (H , ℓq∞ ) where H is a Hilbert space. It then
states that there exists a constant c > 0 such that, for all n, q ∈ N,
v


u
u log 1 + q
t
log n+1
.
(44)
εn (T ) ≤ ckT k
log n + 1
While Guermeur (2002) must rely on a generalisation of Maurey’s Theorem to matrix
output spaces, it is only directly applicable to vector output spaces. This is mentioned by
Guermeur (2002), but it is claimed there that it is not a problem, as an extension can be
derived, although this is not done. In the current formulation of the problem however, this
theorem can be used directly to bound the entropy number, and, as stated, it can also be
used to bound the covering number of an operator. These steps are as follows,
v


u
u log 1 + N
t
log n+1
ε
= ckT k
(45)
εn (T ) ≤
2
log n + 1
ε

,S
N
≤ n
(46)
, T (UF ) , dM
ℓ∞ ,ℓ1
2



ε
ε
,S
,S
N
≤ N
.
(47)
, ∆F ε , dF
, T (UF ) , dM
ℓ
ℓ
∞ ,ℓ1
∞ ,ℓ1
2
2

It remains to demonstrate the third of these and, in particular, we aim to do this for


T (f ) = minθ ∆fθ (x1 , ϑ1 ) minθ ∆fθ (x2 , ϑ2 ) . . . minθ ∆fθ (xN , ϑN ) .
(48)
This mapping T : F → M is to a vector space which has a norm
kakM ,S = max |ai |.

(49)

1≤i≤|S|

For this case Maurey’s Theorem is clearly directly applicable, as are equations (45) and
(46). The expressions in equations (48) and (49) are far simpler than their counterparts in
Guermeur (2002, §2.3) due to the scalar form of ∆f ε in equation (33). For more on the
comparison between the two approaches, see Hill (2007, §4.2.1,§A.2.1). In proving equation
(47), consider first,





M ,S

dℓ∞ ,ℓ1 T (f ), T (f ) = max min ∆fθ (x, ϑ) − min ∆f θ (x, ϑ)
(x,ϑ)∈S

θ

θ

and, meanwhile,





ε
ε
 ε

,S
ε
∆f
(x,
ϑ)
−
∆f
=
max
(x,
ϑ)
∆f
,
∆f
dF


ℓ∞ ,ℓ1
(x,ϑ)∈S

547

Hill & Doucet

and it is clear to see that,






ε
 ε


max ∆f (x, ϑ) − ∆f (x, ϑ) ≤ max min ∆fθ (x, ϑ) − min ∆f θ (x, ϑ)
θ
θ
(x,ϑ)∈S
(x,ϑ)∈S



ε
M
,S
F ,S
dℓ∞ ,ℓ1 ∆f ε , ∆f ≤ dℓ∞ ,ℓ1 T (f ), T (f )

(50)

which means that, provided f , f ∈ UF , then equation (47) is correct. An extended version
of this derivation is presented by Hill (2007, §B.2).
All that remains is to bound kT k. Now, from equation (49),


 T



kT (f )kM ,S = max min vθ (ϑi )WΦ(xi ) .
(51)
θ6=ϑi

i≤i≤|S|

Through the Cauchy-Schwarz inequality (Guermeur, 2002, §A.2) and with ΛX being the
radius of a ball including Φ(X ), then


T
(52)
kT (f )kM ,S ≤ ΛX max min kvθ (ϑ)Wk2
θ6=ϑ

ϑ

this means that,


min kvθT (ϑ)Wk2
θ6=ϑ

kT k ≤ΛX max
ϑ
v
uM −1
uX
kwm k2F .
≤ΛX t



(53)

m=1

In this we have made the assumption that f ∈ UF , however if this is not the case it is
straightforward to arrive at an analogous solution. More on this can be found in Williamson
et al. (2001, §V.B), Elisseeff et al. (1999, Prop. 4) or Guermeur (2002, Prop. 1), for example.
Combining this result with equation (45)
v


v
u
uM −1
N
u
log
1
+
uX
t
log n+1
ε
kwm k2F
εn (TF ) ≤ ≤ cΛX t
2
log n + 1
m=1
which can be rearranged to give,

4c2 Λ2X log(1 + N )
log n ≤
ε2

PM −1
m=1

kwm k2F

−1

when n ≥ 1, which is always going to be the case as this is a bound on a covering number.
Equation (46) then gives,
 ε
 4c2 Λ2 log(1 + N ) PM −1 kw k2
m F
MF ,S
X
m=1
log N
≤
, TF (UF ) , dℓ∞ ,ℓ1
−1
2
2
ε
and so, finally, from equation (47)


log N

ε

2

, ∆F

ε

,S
, dF
ℓ∞ ,ℓ1



4c2 Λ2X log(1 + N )
≤
ε2
548

PM −1
m=1

kwm k2F

− 1.

A Framework for Kernel-Based Multi-Category Classification

This demonstrates the result in equation (43) of Theorem 2 and moreover shows that the
constant r is given by,
4c2 Λ2X log(1 + N )
.
(54)
r=
ε2
This concludes the proof of Theorem 2
5.3 Summary of Generalisation Bounds
In Subsection 5.2, Theorem 1 showed that the risk R(f ) of a function f is bounded with
probability at least (1 − δ) by,
R(f ) ≤

RSε N (f )

+

s

 
 

ε
2
1
1
ε
, ∆F , 2N
+ log
log 2N∞,1
+ .
2N
2
εδ
N

(35)




,S
Where, from Definition 4; N∞,1 2ε , ∆F ε , 2N = supS∈X 2N N 2ε , ∆F ε , dF
ℓ∞ ,ℓ1 , and from
Theorem 2
M
−1

 ε
X
,S
kwm k2F
(43)
≤
r
, ∆F ε , dF
log N
ℓ∞ ,ℓ1
2
m=1
where r is positive and given by equation (54). As a result
v
" M −1
u
 #
u 1
X
4
1
ε
t
2
R(f ) ≤ RSN (f ) +
kwm kF + log
r
+ .
2N
εδ
N
m=1

(55)

As mentioned in the derivation of this result, the methodology employed has been more in
keeping with the two-class derivation than the bound derived by Guermeur (2002). This
is due to the use of the scalar function ∆f ε , as introduced in equation (33). The use of
this function is a logical consequence of viewing the problem in the geometric framework
of Section 2. It allows T to be a mapping to a vector space, as it is in the two-class case,
rather than to a matrix space, as it is in the work by Guermeur (2002).
Not only does the use of ∆f ε simplify the working, but the final result is that the
derived bound is tighter than that of Guermeur (2002). This has been rederived in the
present notation by Hill (2007, App. A) in which the assumption that Maurey’s Theorem
is applicable directly is maintained. In presenting it here we first define Θ to be the set of
M (M −1)
/ Θ and the
unique class combinations. If the class pair (φ, ϕ) ∈ Θ then (ϕ, φ) ∈
2
equivalent expression to equation (55) is,
v


u
 
u
X
4 
1
u 1 
kvφT (ϕ)Wk2 + log
r
+ .
R(f ) ≤ RSε N (f ) + t
2N
εδ
N
(φ,ϕ)∈Θ

where now
r=

8c2 κ2 Λ2X M (M − 1) log(1 + N )
.
ε2
549

(56)

Hill & Doucet

6. Other Kernel-Based Methods
In this section the use of the framework presented in Section 2 is described with respect to
ν-SVC, LS-SVC, LSVC, PSVC, and BPM.
6.1 ν-Support Vector Classification
In this case the two-class optimisation problem (Schölkopf & Smola, 2002) is to
!

N
X
1
yi [hΦ(xi ), wiF + b] ≥ ε − ξi
2
ξi − νε , Subject to
kwkF +
Minimise
ξi ≥ 0, and, ε ≥ 0
2

(57)

i=1

and the extension to the polychotomous case is straightforward, namely to


N
M
−1
X
X
X X
X
1
Minimise 
ξi,θ −
νεθ (ϑ)
kwm k2F +
2
m=1
i=1 θ∈(Θ−ϑi )
ϑ∈Θ θ∈(Θ−ϑ)
 PM −1
m=1 vθ,m (ϑi ) [hΦ(xi ), wm iF + bm ] ≥ ε − ξi,θ
Subject to
ξi,θ ≥ 0, and, ε ≥ 0.

(58)

Following the usual Lagrangian dual approach results in the final aim being to maximise
LD = −

N N M −1
1 XX X T T
αi V (ϑi )V(ϑj )αj K(xi , xj )
2

(59)

i=1 j=1 m=1

subject to 0 ≤ αi,θ ≤ 1, ∀i, θ ∈ (Θ − ϑi ),
The output is as given in equation (19).

PN

i=1 V(ϑi )αi

= 0, and

PN P
i=1

θ∈(Θ−ϑi ) αi,θ

> ν.

6.2 Least Squares Support Vector Classification
LS-SVC as developed at length by Van Gestel, Suykens, Baesens, Viaene, Vanthienen,
Dedene, De Moor, and Vandewalle (2001) is much the same as standard SVC, except that
the empirical loss is now taken to be quadratic; see the top-left corner of Figure 3, and
equation (6). Multiclass versions have been published (Van Gestel, Suykens, Lanckriet,
Lambrechts, De Moor, & Vandewalle, 2002) which rely on coding schemes as discussed in
Subsection 3.3. The two-class case aims to
!
N
X
1
(60)
ξi2 , Subject to yi [hΦ(xi ), wiF + b] = 1 − ξi
kwk2F + C
Minimise
2
i=1

An alternative multi-category extension to the coding approach exists, i.e.


N
M
−1
X
X
X
1
2 
Minimise 
ξi,θ
kwm k2F + C
2
m=1

Subject to

M
−1
X

i=1 θ∈(Θ−ϑi )

vθ,m (ϑi ) [hΦ(xi ), wm iF + bm ] = εθ (ϑi ) − ξi,θ .

m=1

550

(61)

A Framework for Kernel-Based Multi-Category Classification

Now, define

Zm =





α′ =

αT1

...

αTN

T


∗T (ϑ ) . . . Φ(x )v∗T (ϑ )
Φ(x1 )vm
N
1
N
m

T
.
Z′ = ZT1 . . . ZTM −1



ε′ =

V′ =



εT (ϑ1 ) . . .

εT (ϑN )

V(ϑ1 ) . . .

V(ϑN )

T



With these definitions then it can be shown (Van Gestel et al., 2001) that the optimisation
problem becomes equivalent to finding α′ and b to satisfy,


0
V′ T

V′
T
Z′ Z′ + CI



b
α′



=



0
ε′



.

(62)

The classifier is found by solving these linear equations. Note that finding Z′ T Z′ does not
require reference to the feature space, but only kernel evaluations. The final output is again
as in equation (19).
6.3 Lagrangian Support Vector Classification
As introduced by Mangasarian and Musicant (2001), the LSVC is an algorithm which has
its strength in that it is computationally efficient, and easy to implement. It again uses a
quadratic empirical loss, as illustrated in the top-left corner of Figure 3, and detailed in
equation (6). The method for two-class classification aims to
N

Minimise

X

1
ξi2
kwk2F + b2 + C
2
i=1

!

, Subject to yi [hΦ(xi ), wiF + b] ≥ 1 − ξi .

(63)

This can be reformulated to a multi-category problem resulting in,


1
Minimise 
2
Subject to

M
−1
X

M
−1
X

m=1




kwm k2F + b2m + C

N
X

X

i=1 θ∈(Θ−ϑi )



2 
ξi,θ

(64)

vθ,m (ϑi ) [hΦ(xi ), wm iF + bm ] ≥ εθ (ϑi ) − ξi,θ .

m=1

The dual to this is,


N
N N
X
1
1 XX T T
T
αi ε(ϑi ) −
αi V (ϑi )V(ϑj )αj [K(xi , xj ) + 1] +
αi
LD = −
2
2C

(65)

i=1

i=1 j=1

which needs to be maximised subject to αi,θ ≥ 0 for all i, and all θ ∈ (Θ − ϑi ). Once that
P
has been done then b = N
i=1 V(ϑi )αi . The final solution again takes the form of equation
(19).
551

Hill & Doucet

6.4 Proximal Support Vector Classification
Following on from the LSVC method, the PSVC approach was developed by Fung and
Mangasarian (2001b, 2001a). While they have presented a multi-category approach it is a
one-against-all algorithm, not an ‘all-together’ one. The two-class aim is to
!
N
X

1
2
2
2
ξi , Subject to yi [hΦ(xi ), wiF + b] = 1 − ξi . (66)
kwkF + b + C
Minimise
2
i=1

which is, once more, the same as that for LS-SVC in Subsection 6.2 except for the b2 term.
This can be reformulated to a multi-category problem resulting in,


M
−1
N
X
X
X


1
2 
ξi,θ
kwm k2F + b2m + C
Minimise 
2
m=1
i=1 θ∈(Θ−ϑi )
(67)
M
−1
X
vθ,m (ϑi ) [hΦ(xi ), wm iF + bm ] = εθ (ϑi ) − ξi,θ .
Subject to
m=1


T
∗T
Now, define v∗ ′ = v1∗T (ϑ1 ) . . . v1∗T (ϑN ) v2∗T (ϑ1 ) . . . vN
(ϑN ) , and with this
then as for LS-SVC the optimisation problem has an exact solution,


T −1 ′
T
α′ = I + Z′ Z′ + v∗ ′ v∗ ′
ε

where everything is as defined in Section 6.2 and b =
solution takes the form of equation (19).

PN

i=1 V(ϑi )αi .

(68)
As before, the final

6.5 Bayes Point Machines
BPMs were introduced by Herbrich, Graepel, and Campbell (2000a) and the ideas can be
extended to a multi-category problem. In short they consider what they term Version
Space, V. In the two-class case this is the region in which a weight vector w can lie without
inducing any classification errors on the training set.
Within version space a uniform distribution is assumed over all possible linear (in feature
space) classifiers, h, outside it is assumed zero. The Bayes point classifier is then given by
i
h
(69)
hbp = arg min EX EH|{xi ,yi }N [ℓ (h(X), H(X))]
i=1

h∈H

where ℓ(·, ·) is some loss function (typically the zero-one loss function is used) and the inner
expectation is over classifiers H ∈ H. One problem with this definition is that it is not usual
that there is any knowledge about PX and so evaluation of EX is impossible. With some
assumptions about the form of PX (see Herbrich et al., 2000a, for more) it can, however,
be shown that the centre of mass,
wcm =

Ew|{xi ,yi }N [w]
i=1

kEw|{xi ,yi }N [w]k
i=1

552

(70)

A Framework for Kernel-Based Multi-Category Classification

is a good approximation to wbp . Eventually the problem becomes to identify V, which is
some contiguous and convex space, and then to find wcm given that there is a uniform
distribution assumed over the weight vectors in this space.
Note that version space is defined by
V = {w : yi hΦ(xi ), wi > 0, kwk = 1, ∀i} ,

(71)

When considering multiple classes then the condition yi hΦ(xi ), wi > 0 becomes VT (ϑi )WΦ(xi ) >

T
0 0 ... 0
where the inequality indicates component-wise inequalities, and the maT

has been introduced. As a result the version space is given
trix W = w1 . . . wM −1
by
n
o

T
V = (w1 , w2 , . . . , wM −1 ) : VT (ϑi )WΦ(xi ) > 0 0 . . . 0
, kwm k = 1 ∀m, i ,
(72)
which is identical in form to equation (71). Extensions of the kernel billiards algorithm
described by Herbrich et al. (2000a) can be used to find Wcm , which is analogous to wcm
in equation (70). Their method for including training errors can also be seamlessly incorporated.

7. Implementation through Sequential Minimal Optimisation
The geometric construction introduced in Section 2 allows insight into the multi-category
problem, which should motivate alternative approaches to efficiently solve the ‘all-together’
optimisation problem. One possibility for SVC is presented here, based on a vector-valued
version of SMO which was first introduced for binary classification by Platt (1999). This has
several advantages, including the fact that it is reasonably straightforward to understand,
relatively easy to implement, quite efficient and flexible, in addition to being well established
and known.
SMO optimises with respect to two points at a time, denote these c and d. With this
notation, and with Kij = K(xi , xj ), then the dual Lagrangian in equation (13) becomes
 1
1
L = αTc ε(ϑc ) + αTd ε(ϑd ) − αTc VT (ϑc )V(ϑc )αc Kcc − αTd VT (ϑd )V(ϑd )αd Kdd
2
2
− αTc VT (ϑc )V(ϑd )αd Kcd − αTc VT (ϑc )zc − αTd VT (ϑd )zd + constant.

(73)

P
T ∗
where zc is a vector with elements zc,m = N
i=1,i6=c,d αi vm (ϑi )Kic and similarly for zd,m .
As shown by Hill and Doucet (2005, §6); by expressing αc in terms of αd , through the
constraint in equation (17), and finding a minimum by setting ∇αd L(αd ) = 0, then an
SMO update takes the form,
= αold
αnew
d +
d



V−1 (ϑd )
ψ(xc ) − ψ(xd ) + V−T (ϑd )ε(ϑd ) − V−T (ϑc )ε(ϑc ) (74)
Kdd + Kcc − 2Kdc

where, V−1 (ϑd ) exists provided that the vectors {vθ (ϑd ) : θ ∈ (Θ − ϑd )} are linearly independent, which is nearly always the case, although it is possible to conceive of pathological
cases. Recall that ψ was introduced in equation (9) cf. (19).
553

Hill & Doucet

7.1 Clipping
Recall that all elements of αc and αd are upper and lower bounded (equation (16)), and
hence clipping may be required. This is best understood through Figure 8, which relates
Illustration of Clipping Considerations
1.2

α d,2
1

0.8

α∆

0.6
Start point

αd,2

α old
d

0.4

α new
d

0.2

End point

0

α d,1
−0.2
−0.2

0

0.2

0.4

0.6

αd,1

0.8

1

1.2

1.4

1.6

(a) A vector illustration of a proposed (b) Constraints within which the update must be
update
made

Figure 8: The proposed update for a three-class problem. The new point is shown in Subfigure 8(b) to be outside the allowed regions. These correspond to the overall limits
of 0 and CD1 (ϑd ) (upright box) and limits imposed by αc considerations (tilted
box).

to the three-class case, however the ideas are generically applicable. Given the update in
new or αnew lie outside their constraints
equation (74) of the form αnew
= αold
d
d + α∆ , if αc
d
old
new
then the line between αd and αd is traced back along until this is no longer the case.
Ultimately some κ ∈ [0, 1) is found such that
αnew,clipped
= αold
d + κα∆ .
d
As the optimisation surface is convex, improvements are still made with every update.
7.2 Updating Non-Extremal Components
Often, updates of the form of equation (74) involve vectors αold
and αold
which have
c
d
extremal components (i.e. components at their constraint-introduced limits). This can
lead to a computational bottleneck as any update which suggests that these components
should lie further outside the allowed region will result in the clipping procedure returning
the original vectors.
To avoid this consider again that the two points to update are labelled c and d, and
denote the number of non-extremal components of each as Pc and Pd respectively. An
update is likely possible6 if Pd > M − 1 − Pc and, this being the case, let Pd + Pc + 1 − M
6. Otherwise there is only one solution, the current one.

554

A Framework for Kernel-Based Multi-Category Classification

non-extremal components of αd be grouped into a new vector αd . The remaining elements of
both αc and αd are dependent on these. Owing to the linearity of the relationship between
αc and αd , as introduced by the constraint in equation (17) then it becomes apparent that
e d + Ad αd and αc = α
e c + Ac αd
αd = α

(75)

e d , Ad , α
e c , and Ac . Of these α
e c and α
e d contain the
describe dependencies, for some α
extremal components which will not be updated, together with zeros, and Ac and Ad are
matrices consisting of ones and zeros which map the variable components back to their
original positions in the vectors αc and αd . It can be shown (Hill & Doucet, 2005, App.
E), that the SMO update in this case is,
= αold
αnew
d
d +

(ATd VT (ϑd )V(ϑd )Ad )−1 ATd VT (ϑd )
Kdd + Kcc − 2Kdc


× ψ(xc ) − ψ(xd ) + V−T (ϑd )ε(ϑd ) − V−T (ϑc )ε(ϑc ) .

(76)

Again, clipping can be performed as in Subsection 7.1. Note that in this expression only the
evaluations of ψ(·) actually change during the optimisation process. All others, especially
the matrix-valued numerator may be held in memory to speed the procedure, where possible.
7.3 Point Selection
It remains to select points c and d. Platt (1999), presents a number of heuristics, however the
improvements suggested by Keerthi, Shevade, Bhattacharyya, and Murthy (2001) appear
to be more efficient and will form the basis of that overviewed here. In the binary case
the essential approach is to identify points requiring the highest and lowest offsets b in
order that their underlying function f (·) is as might be expected, i.e. it has the sign of the
relevant point. When considering two classes, A and B in the multi-category arrangement
a directly analogous approach can be taken in that the problem is reduced to a two-class
problem across their mutual boundary, and a comparable scalar metric can be found.
The starting point in this methodology is to construct the Lagrangian which governs
the dual optimisation problem as given in equation (13);
N

L=

N

N

i=1

i=1

N

X
X
1 XX T T
αTi δ i
αTi ε(ϑi ) −
αi V (ϑi )V(ϑj )αTj K(xi , xj ) −
2
i=1 j=1

+

N
X

X

µi,θ (αi,θ − CDθ (ϑi )) −

N
X

αTi VT (ϑi )η m

i=1

i=1 θ∈(Θ−ϑi )

where {δi,θ , µi,θ : i ∈ {1, . . . , N }, θ ∈ (Θ − ϑi )} and {ηm : m ∈ {1, . . . , (M − 1)}} are
Lagrangian multipliers. Differentiating this with respect to αi and setting the result equal
to zero implies that,
VT (ϑi ) (ψ(xi ) + η) = ε(ϑi ) + δ i − µi
(77)
and, hence,
η = V−T (ϑi ) (ε(ϑi ) + δ i − µi ) − ψ(xi ).
555

(78)

Hill & Doucet

while KKT conditions not satisfied.
for all combinations of two classes (denoted A and B).
Perform two-class SMO along the direction vB (A) with
updates given by equation (76) and iup , ilow found through (79)

Table 1: Algorithm Pseudo-Code

Consider the update of two points of classes A and B and let ϑi = A, in doing this recall
the equality, as discussed by Keerthi et al. (2001), of η and b. With this in mind it becomes
apparent that an equivalent metric for updating is the difference between the respective η
values across the boundary between the two classes. To find the perpendicular distance,
take the inner product with the perpendicular to the boundary, for instance vB (A)
T
T
vB
(A)η = εB (A) + δi,B − µi,B − vB
(A)ψ(xi ).

(79)

This expression is now directly comparable to the equivalent key starting point in the point
selection process as it is directly analogous to the scalar used by Keerthi et al. (2001). Indeed
T (A)η.
the parameters bup and blow used there are equivalent to the extreme values of vB
7.4 Multi-Category SMO Summary
Following from the above it becomes possible to put together a complete approach;
1. Select an initial two classes to consider denoted generically A and B.
2. From these two classes determine the two points with maximum and minimum values
T (A)η as in equation (79). These will be denoted i
of vB
up and ilow respectively as they
correspond to those associated with bup and blow in the work by Keerthi et al. (2001).
3. Perform updates as outlined in equations (74) and (76) until convergence by some
criteria (e.g. updates are all below some threshold) is achieved. Point selection is
made following the standard two loop approach of a for loop attempting to update all
points and a while loop considering only non-extremal ones. Updates are attempted
with respect to either iup or ilow and these maximal and minimal points are updated
after each iteration.
4. Once convergence has been achieved for these two classes then select another two
classes and repeat steps 2 and 3. Do this until all possible combinations of classes
have been attempted.
5. Repeat the entire process until no updates are made. At this point the Karush-KuhnTucker (KKT) conditions should be very nearly satisfied, and should be checked to
ensure that they are at least within some acceptable limit of satisfaction.
This is summarised in the pseudocode of Table 1.
This approach is clearly closely related to the structure of a pairwise coupling algorithm
(Subsection 3.2) however now with a single optimisation problem as a focus. Clearly the
556

A Framework for Kernel-Based Multi-Category Classification

algorithm may be more computationally intense than that of Kreßel (1999) for two reasons.
First, each stage updates involves matrix multiplications instead of scalar ones. Second, as
indicated by step 5, more than one pass may be required. On the other hand, there might
be some reduction in overall iterations required in a particular class-class combination as
each optimisation is not starting from ‘scratch’, rather updates from previous combinations
may have had a positive impact.
Experimentally however it has been observed that the traditional pairwise coupling approach is more computationally efficient, and this has also been noted in the literature (Hsu
& Lin, 2002; Rifkin & Klautau, 2004). As alluded to in Subsection 3.2, a combined approach is possible, which will be referred to as the combined pairwise, all-together algorithm.
Broadly, this is as follows;
1. Perform the pairwise optimisation described by Kreßel (1999). This optimisation
requires the implementation of M (M2 −1) standard SV classifiers with the slight change
that instead of using the standard 2-class ε value of 1, use the ε values corresponding
to the particular pairwise optimisation.
2. Map the results into the classification spatial arrangement (Figures 1 or 2 for example).
This can be done easily by observing that in the product V(ϑi )αi of equation (19)
the element αi,θ multiplies vθ (ϑi ) — recall that this is perpendicular to the boundary
between θ and ϑi . As such then the result of the binary classification optimisation
between classes ϑi and θ can be used to directly provide the value αi,θ . Note that the
constraint in equation (17) is still satisfied.
3. Finalise the ‘all-together’ single optimisation following the steps outlined earlier in
this Subsection.
In short the bulk of the optimisation is performed with the standard pairwise methodology.
The geometrical approach detailed in Section 2 is used to manipulate the output such
that a unified consistent result can be obtained with little additional computational effort.
This has the clear advantage that a practitioner can be sure of exactly on what basis the
classification is being made without having to resort to ad hoc heuristics.

8. Examples
Extensive investigations into comparative performance of multi-category SVM methods
have been detailed by Hsu and Lin (2002), and they present current benchmark training
times. As discussed, their work has found that pairwise coupling approaches are far more
computationally efficient than others. This has also been found to be the case for the first
SMO algorithm proposed in Subsection 7.4 and the main aim in this Section is to investigate
the performance of the combined pairwise, ‘all-together’ algorithm. Both standard binary
and the described multi-category SMO were coded in a straightforward way. No dynamic
caching or low-level code refinements were used in this initial proof-of-concept investigation
as it was felt that such detailed optimisations are best done together in a consistent way,
as in the dedicated comparative work of Hsu and Lin (2002).
The datasets used were obtained from the University of California repository (Blake &
Merz, 1998). For illustrative purposes the training and test output results on the DNA
557

Hill & Doucet

dataset are presented in Figure 9. Here it is clear to see how the pairwise result has been
Found f(x) Final
4

2

2
f (x)

0

2

2

f (x)

Found f(x), After Initial Mapping
4

−2

−4
−4

0

−2

−2

0
f1(x)

2

−4
−4

4

−2

4

2

2

2

0

−2

−4
−4

2

4

2

4

Found f(x) Final

4

f (x)

2

f (x)

Found f(x), After Initial Mapping

0
f1(x)

0

−2

−2

0
f (x)

2

4

1

−4
−4

−2

0
f (x)
1

Figure 9: DNA data outputs for training and test data cases. The mapped pairwise and
optimised ‘all-together’ results are shown. Margins analogous to those in the twoclass case are clearly visible and are shown by dashed lines. Training data forms
the top row, test data the bottom. The more numerous, ‘N’ case is given by green
triangles, ‘EI’ by blue circles, and ‘IE’ by red squares. The stars are indicative of
the class target vectors.

mapped into the classification plane of Figure 1, and what changes are made in performing
the ‘all-together’ additional optimisation. In short the ‘N ’ class appears to have intermingled a little more with the ‘EI’ class and less with the ‘IE’ class. As well the ‘all-together’
outputs fill the corners of the margin intersections more completely, while the pairwise
outputs tend to cut them off. This has been often observed in other implementations.
The training time is heavily dependent on the tolerance to within which convergence is
desired. This value, referred to as τ by Keerthi et al. (2001) indicates the variation allowed
between bup and blow as discussed in Subsections 7.3 and 7.4. The effect of this has been
additionally investigated for two values of τ , and the results are tabulated in Tables 2 and
3. In these experiments Gaussian kernels were used and appropriate values of σ and C were
chosen by trial and error such that output accuracies (where accuracy refers to percentage
classification error rate) of the ‘all-together’ implementation were comparable to those of
Hsu and Lin (2002).
The actual accuracies recorded are given in the Table, however recall that, as noted in
Section 3.1, the optimisation problem being solved is the generic ‘all-together’ one and, as
such, judicious choices of σ and C should mean that the same accuracy rates are achievable
by all such algorithms. Clearly as the implicit model behind the pairwise approach is slightly
different it may indeed be able to achieve slightly different accuracy results. With this in
mind the aim here has not been to incessantly tweak hyperparameters to achieve marginally
superior results, but simply to look at the big picture of performance.
558

A Framework for Kernel-Based Multi-Category Classification

Problem
DNA
Vehicle
Satimage
Segment
Vowel
Letter

M
3
4
6
7
11
26

N
2000
766
4435
2079
891
15000

Pair
0.8
0.4
3.0
2.4
0.7
129.0

τ = 0.03C
All
Alone
1.1
1.5
2.7
5.3
10.8
41.8
13.2
47.9
3.5
13.3
129.9
2119.2

Pair
1.1
0.5
3.6
3.2
1.0
142.3

τ = 0.001C
All
Alone
3.7
11.7
3.5
3.9
9.0
27.6
16.2
42.0
18.5
22.8
1373.7
5573.4

Table 2: Optimisation times (seconds) for various example problems. Columns present results obtained using the pairwise algorithm and the ‘all-together’ SMO algorithm
discussed. In all cases ‘Pair’ refers to pairwise optimisation time results, meanwhile ‘All’ denotes additional refinement time i.e. that required to progress from
the pairwise result to the ‘all-together’ result. Finally ‘Alone’ identifies time taken
by the ‘all-together’ algorithm without initial pairwise optimisation.

Problem
DNA
Vehicle
Satimage
Segment
Vowel
Letter

M
3
4
6
7
11
26

N
2000
766
4435
2079
891
15000

τ = 0.03C
ER(Pair) ER(All)
4.4
4.6
15.0
18.8
10.6
10.8
3.0
2.6
3.0
3.0
8.8
8.8

τ = 0.001C
ER(Pair) ER(All)
4.6
4.5
17.5
20.0
9.7
9.2
3.0
3.0
3.0
3.0
8.9
8.8

Table 3: Optimisation error rates (percentages) for various example problems. Columns
present experimentally obtained results using the pairwise and ‘all-together’ multicategory SMO algorithms discussed. ‘ER(Pair)’ refers to the test error rate of the
pairwise method and ‘ER(All)’ to that of the ‘all-together’ algorithm.

In continuing with this mindset, no class weightings were introduced, and target vectors were set to be equidistant. Clearly it may well be the case that these could actually
be perturbed, and class weights introduced to improve performance, with no additional
computational effort, however in this initial investigation this has not been done.
The experiments were all run on a 2.8GHz P4 with 1GB RAM7 . From Tables 2 and 3
the following points become apparent,
1. The optimisation times presented here are of magnitudes similar to those of Hsu and
Lin. Although it has not been the aim of this work to produce highly refined optimal code, and although such comparisons are always going to be problematic in
terms of implementation specifics, this result is, in itself, positive. Generally, the
most accurate implementation of the algorithm presented in the preceding sections
(when τ = 0.001C) convergence times are similar to those of Hsu and Lin for their
‘all-together’ implementation. Briefly, their optimisation times were; for DNA, 13.5s,
for vehicle 88.6s, for satimage 48.2s, for segment 66.4s, for vowel 14.1s, and for letter
8786.2s. As such we consider the advantage obtained here through extra computational power as roughly equivalent to the effect of their extra coding.
7. Hsu and Lin (2002) had a 500MHz P3 with 384MB RAM.

559

Hill & Doucet

It is worth noting that there is additional intrinsic value in the intuitiveness, flexibility
and its ease of implementation of the presented algorithm, something the standard
SMO algorithm is well known for. As highlighted, no additional computational effort
is required to alter class regions or introduce class weights (Subsection 2.3), neither
of which have been considered by Hsu and Lin (2002).
2. It is possible to approximately quantify the relative effect of combining the pairwise
and ‘all-together’ algorithms in context. In short it typically halves them, although
the variation on this is quite large. This result appears roughly consistent for both
values of τ .
3. As anticipated, error rate results do not strongly favour the pairwise or ‘all-together’
methods; this is always going to be a case-by-case issue.

9. Conclusion
A geometric framework for understanding multi-category classification has been introduced,
through which many existing ‘all-together’ algorithms can be understood. The structure
allows the derivation of a parsimonious optimisation function, which is a direct extension
of the binary SV classification optimisation function. This can be seen in that no special
case considerations need be made in order that the mathematics reduce to the standard
result when the number of classes, M = 2. Further, the framework enables considerable
generalisation of the problem and incorporation of relative class knowledge without any
additional computational complexity. As far as actual optimisation results are concerned,
the virtues of the proposed framework, in fact, apply to the other ‘all-together methods as
well.
It has been found by Hsu and Lin (2002) and Rifkin and Klautau (2004), among others,
that the pairwise SV method converges with a substantial speed advantage over existing
multi-category methods. However pairwise results require some heuristic to combine them.
This can be avoided by mapping them to the geometric framework described and ‘finetuning’ to obtain the consistent ‘all-together’ solution. This refining can be performed by
any multi-category ‘all-together’ algorithm.
The ability of the framework to compare algorithms has been illustrated by a brief
discussion of Fisher consistency. This has shown graphically illustrated how different loss
structures compare and how most result in Fisher inconsistent optimisation problems.
Generalisation bounds have been derived with the aim of the framework presented which
are tighter than those previously presented in the literature. These have also benefited from
a simpler derivation than those previously presented due to the fact that well-known scalar
methods developed for the two class case have been directly applicable. Previously there
was a need to extend them to more cumbersome vector methods.
In addition to providing a more generic and flexible framework, this architecture may
well provide insights regarding how to further improve on the speed of existing multicategory SV classification algorithms (whether coupled with a pairwise optimisation, or not).
An initial example of how this might be achieved has been developed in the formulation of a
straightforward multi-category SMO variant algorithm. The proof-of-concept experimental
results have shown that this, combined with the mapping of pairwise results, is already
560

A Framework for Kernel-Based Multi-Category Classification

comparable with the optimisation speeds achieved by Hsu and Lin (2002) in their benchmark
work, despite the fact that their implementation code is highly refined and includes features
such as dynamic caching. Future efforts based on the geometric framework described should
be able to outperform existing standards.

References
Allwein, E. L., Schapire, R. E., & Singer, Y. (2001). Reducing multiclass to binary: A unifying approach for margin classifiers. Journal of Machine Learning Research, 1 (113-141).
Bartlett, P. L. (1998). The sample complexity of pattern classification with neural networks: The size of the weights is more important than the size of the network. IEEE
Transactions on Information Theory, 44 (2), 525–536.
Bartlett, P. L., Jordan, M. I., & McAuliffe, J. D. (2004). Large margin classifiers: Convex
loss, low noise and convergence rates. Advances in Neural Information Processing
Systems, 16.
Blake, C. L., & Merz, C. J. (1998). UCI repository of machine learning databases..
http://www.ics.uci.edu/∼mlearn/MLRepository.html.
Blanz, V., Schölkopf, B., Bülthoff, H., Burges, C. J. C., Vapnik, V. N., & Vetter, T. (1996).
Comparison of view-based object recognition algorithms using realistic 3D models.
In von der Malsburg, C., von Seelen, W., Vorbrüggen, J. C., & Sendhoff, B. (Eds.),
Artificial Neural Networks, Vol. 1112 of Springer Lecture Notes in Computer Science,
pp. 251–256, Berlin.
Bredensteiner, E. J., & Bennett, K. P. (1999). Multicategory classification by support vector
machines. Computational Optimizations and Applications, 12, 53–79.
Crammer, K., & Singer, Y. (2001a). On the algorithmic implementation of multiclass
kernel-based vector machines. Journal of Machine Learning Research, 2, 265–292.
Crammer, K., & Singer, Y. (2001b). Pranking with ranking. In Advances in Neural Information Processing, Vol. 14.
Cristianini, N., & Shawe-Taylor, J. (2000). An Introduction to Support Vector Machines
and Other Kernel-Based Learning Methods (1st edition). Cambridge University Press.
Dietterich, T., & Bakiri, G. (1995). Solving multiclass learning problems via error-correcting
output codes. Journal of Artificial Intelligence Research, 2, 263–286.
Elisseeff, A., Guermeur, Y., & Paugam-Moisy, H. (1999). Margin error and generalization capabilities of multi-class discriminant systems. Tech. rep. NC2-TR-1999-051-R,
NeuroCOLT2.
Fung, G., & Mangasarian, O. L. (2001a). Multicategory proximal support vector machine
classifiers. Tech. rep. 01-06, Data Mining Institute.
Fung, G., & Mangasarian, O. L. (2001b). Proximal support vector machine classifiers. In
Proceedings KDD-2001, pp. 77–86, San Francisco.
Fürnkranz, J. (2002). Round robin classification. Journal of Machine Learning, 2, 721–747.
561

Hill & Doucet

Guermeur, Y. (2000). Combining discriminant models with new multi-class SVMs. Tech.
rep., NeuroCOLT2.
Guermeur, Y. (2002). Combining discriminant models with new multi-class SVMs. Pattern
Analysis and Applications, 5, 168–179.
Hastie, T., & Tibshirani, R. (1998). Classification by pairwise coupling. In Michael, M.
J. K., Jordan, I., & Solla, S. A. (Eds.), Advances in Neural Information Processing,
Vol. 10, pp. 507–513. MIT Press.
Hastie, T., Tibshirani, R., & Friedman, J. (2001). The Elements of Statistical Learning.
Springer.
Herbrich, R., Graepel, T., & Campbell, C. (2000a). Bayes point machines. Journal of
Machine Learning Research, 1, 245–279.
Herbrich, R., Graepel, T., & Obermayer, K. (2000b). Large margin rank boundaries for
ordinal regression. Advances in Large Margin Classifiers, 1, 115–132.
Hill, S. I. (2007). Notes on the generalisation performance and Fisher consistency of multicategory classifiers. Tech. rep. CUED/F-INFENG/TR.583, Engineering Dept, University of Cambridge.
Hill, S. I., & Doucet, A. (2005). A framework for kernel-based multi-category classification.
Tech. rep. CUED/F-INFENG/TR.508, Engineering Dept., University of Cambridge.
Hsu, C.-W., & Lin, C.-J. (2002). A comparison of methods for multi-class support vector
machines. IEEE Transactions on Neural Networks, 13, 415–425.
Keerthi, S. S., Shevade, S. K., Bhattacharyya, C., & Murthy, K. R. K. (2001). Improvements
to Platt’s SMO algorithm for SVM classifier design. Neural Computation, 13, 637–649.
Kindermann, J., Leopold, E., & Paaß, G. (2000). Multi-class classification with error correcting codes. In Leopold, E., & Kirsten, M. (Eds.), Treffen der GI-Fachgruppe 1.1.3
Maschinelles Lernen. GMD Report 114.
Kreßel, U. H.-G. (1999). Pairwise classification and support vector machines. In Schölkopf,
B., Burges, C. J. C., & Smola, A. J. (Eds.), Advances in Kernel Methods: Support
Vector Learning. MIT Press.
Lee, Y., Lin, Y., & Wahba, G. (2001). Multicategory support vector machines. Tech. rep.
1043, Department of Statistics, University of Wisconsin.
Lee, Y., Lin, Y., & Wahba, G. (2004). Multicategory support vector machines, theory,
and application to the classification of microarray data and satellite radiance data.
Journal of the American Statistical Association, 99, 659–672.
Mangasarian, O. L., & Musicant, D. R. (2001). Lagrangian support vector machines. Journal
of Machine Learning Research, 1, 161–177.
Mayoraz, E., & Alpaydın, E. (1999). Support vector machines for multi-class classification. In Proceedings of the International Workshop on Artifical Neural Networks
(IWANN99).
Paugam-Moisy, H., Elisseeff, A., & Guermeur, Y. (2000). Generalization performance of
multiclass discriminant models..
562

A Framework for Kernel-Based Multi-Category Classification

Platt, J. C. (1999). Fast training of support vector machines using sequential minimal
optimization. In Schölkopf, B., Burges, C. J. C., & Smola, A. J. (Eds.), Advances in
Kernel Methods - Support Vector Learning, pp. 185–208. MIT Press, Cambridge, MA.
Platt, J. C., Cristianini, N., & Shawe-Taylor, J. (2000). Large margin DAGs for multiclass
classification. In Solla, S. A., Lean, T. K., & Müller, K.-R. (Eds.), Advances in Neural
Information Processing, Vol. 12, pp. 547–553. MIT Press.
Rennie, J. D. M., & Rifkin, R. (2001). Improving multiclass text classification with the
support vector machine. Memo AIM-2001-026, Massachusetts Institute of Technology
Artificial Intelligence Laboratory.
Rifkin, R., & Klautau, A. (2004). In defense of one-vs-all classification. Journal of Machine
Learning Research, 5, 101–141.
Schölkopf, B., Burges, C. J. C., & Vapnik, V. N. (1995). Extracting support data for a given
task. In Fayyad, U. M., & Uthurusamy, R. (Eds.), Proceedings, First International
Conference on Knowledge Discovery and Data Mining, pp. 252–257, Menlo Park, CA.
AAAI Press.
Schölkopf, B., & Smola, A. J. (2002). Learning with Kernels. MIT Press.
Sebald, D. J. (2000). Nonlinear Signal Processing for Digital Communications using Support
Vector Machines and a New Form of Adaptive Decision Feedback Equalizer. Ph.D.
thesis, University of Wisconsin-Madison.
Sebald, D. J., & Bucklew, J. A. (2001). Support vector machines and the multiple hypothesis
test problem. IEEE Transactions on Signal Processing, 49 (11), 2865–2872.
Sejnowski, T. J., & Rosenberg, C. R. (1987). Parallel networks that learn to pronounce
English text.. Journal of Complex Systems, 1, 145–168.
Suykens, J. A. K., & Vandewalle, J. (1999). Multiclass least squares support vector machines. In Proceedings of the International Joint Conference on Neural Networks
(IJCNN’99), Washington DC, USA.
Tewari, A., & Bartlett, P. L. (2007). On the consistency of multiclass classification methods.
Journal of Machine Learning Research, 8, 1007–1025.
Van Gestel, T., Suykens, J. A. K., Baesens, B., Viaene, S., Vanthienen, J., Dedene, G.,
De Moor, B., & Vandewalle, J. (2001). Benchmarking least squares support vector
machine classifiers. Machine Learning, 54 (1), 5–32.
Van Gestel, T., Suykens, J. A. K., Lanckriet, G., Lambrechts, A., De Moor, B., & Vandewalle, J. (2002). Multiclass LS-SVMs: Moderated outputs and coding-decoding
schemes. Neural Processing Letters, 15, 45–58.
Vapnik, V. N. (1998). Statistical Learning Theory. Wiley.
Wang, F., Vuurpijl, L. G., & Schomaker, L. R. B. (2000). Support vector machines for
the classification of western handwritten capitals. In Schomaker, L. R. B., & Vuurpijl, L. G. (Eds.), Proceedings of the 7th International Workshop on Frontiers in
Handwriting Recognition, pp. 167–176.
Weston, J. A. E. (1999). Extensions to the Support Vector Method. Ph.D. thesis, University
of London.
563

Hill & Doucet

Weston, J. A. E., & Watkins, C. (1999). Support vector machines for multi-class pattern recognition. In Proceedings of the 7th European Symposium On Artificial Neural
Networks.
Williamson, R. C., Smola, A. J., & Schölkopf, B. (2001). Generalization performance of
regularization networks and support vector machines via entropy numbers of compact
operators. IEEE Transactions on Information Theory, 47 (6), 2516–2532.
Zhang, T. (2004a). An infinity-sample theory for multi-category large margin classification.
Advances in Neural Information Processing, 16.
Zhang, T. (2004b). Statistical analysis of some multi-category large margin classification.
Journal of Machine Learning Research, 5, 1225–1251.

564

Journal of Articial Intelligence Research 30 (2007) 659-684

Submitted 06/07; published 12/07

Learning to Play Using Low-Complexity Rule-Based Policies:
Illustrations through Ms. Pac-Man
István Szita
András L®rincz

szityu@eotvos.elte.hu
andras.lorincz@elte.hu

Dept. of Information Systems
Eötvös University, Hungary, H-1117

Abstract
In this article we propose a method that can deal with certain combinatorial reinforcement learning tasks. We demonstrate the approach in the popular Ms. Pac-Man game. We
dene a set of high-level observation and action modules, from which rule-based policies
are constructed automatically. In these policies, actions are temporally extended, and may
work concurrently. The policy of the agent is encoded by a compact decision list. The components of the list are selected from a large pool of rules, which can be either hand-crafted
or generated automatically. A suitable selection of rules is learnt by the cross-entropy
method, a recent global optimization algorithm that ts our framework smoothly. Crossentropy-optimized policies perform better than our hand-crafted policy, and reach the score
of average human players. We argue that learning is successful mainly because (i) policies
may apply concurrent actions and thus the policy space is suciently rich, (ii) the search is
biased towards low-complexity policies and therefore, solutions with a compact description
can be found quickly if they exist.

1. Introduction
During the last two decades, reinforcement learning (RL) has reached a mature state, and
has been laid on solid foundations. We have a large variety of algorithms, including valuefunction-based, direct policy search and hybrid methods. For reviews on these subjects,
see, e.g., the books of Bertsekas and Tsitsiklis (1996) and Sutton and Barto (1998). The
basic properties of many such algorithms are relatively well understood, e.g. conditions
for convergence, complexity, the eect of various parameters, although it is needless to say
that there are still lots of important open questions. There are also plenty of test problems
(like various maze-navigation tasks, pole-balancing, car on the hill etc.) on which the
capabilities of RL algorithms have been demonstrated, and the number of large-scale RL
applications is also growing steadily. However, current RL algorithms are far from being
out-of-the-box methods, so there is still need for more demonstrations showing that RL can
be ecient in complex tasks.
We think that games (including the diverse set of classical board games, card games, modern computer games, etc.) are ideal test environments for reinforcement learning. Games
are intended to be interesting and challenging for human intelligence and therefore, they
are ideal means to explore what articial intelligence is still missing. Furthermore, most
games t well into the RL paradigm: they are goal-oriented sequential decision problems,
where each decision can have long-term eects. In many cases, hidden information, random
events, unknown environment, known or unknown players account for (part of) the diculty
c 2007 AI Access Foundation. All rights reserved.
°

Szita & L®rincz

of playing the game. Such circumstances are in the focus of reinforcement learning. Games
are also attractive for testing new methods: the decision space is huge in most cases, so
nding a good strategy is a challenging task.
There is another great advantage of using games as test problems: the rules of the games
are xed, so the danger of `tailoring the task to the algorithm'  i.e., to tweak the rules
and/or the environment so that they meet the capabilities of the proposed RL algorithm 
is reduced, compared, e.g., to various maze navigation tasks.
RL has been tried in many classical games, including checkers (Samuel, 1959), backgammon (Tesauro, 1994), and chess (Baxter, Tridgell, & Weaver, 2001). On the other hand,
modern computer games got into the spotlight only recently, and there are not very many
successful attempts to learn them with AI tools. Notable exceptions are, for example, roleplaying game Baldur's Gate (Spronck, Sprinkhuizen-Kuyper, & Postma, 2003), real-time
strategy game Wargus (Ponsen & Spronck, 2004), and possibly, Tetris (Szita & L®rincz,
2006). These games pose new challenges to RL, for example, many observations have to be
considered in parallel, and both the observation space and the action space can be huge.
In this spirit, we decided to investigate the arcade game Ms. Pac-Man. The game is
interesting on its own as it is largely unsolved, but also imposes several important questions
in RL, which we will overview in Section 8. We will provide hand-coded high-level actions
and observations, and the task of RL is to learn how to combine them into a good policy. We
will apply rule-based policies, because they are easy to interpret and enable one to include
human domain-knowledge easily. For learning, we will apply the cross-entropy method, a
recently developed general optimization algorithm. We will show that the hybrid approach
is more successful than either tabula rasa learning or a hand-coded strategy alone.
In the next section we introduce the Ms. Pac-Man game briey and discuss how it can
be formalized as a reinforcement learning task. In sections 3 and 4, we shall shortly describe
the cross-entropy optimization method and rule-based policies, respectively. In section 5,
details of the learning experiments are provided, and in section 6 we present our results.
Section 7 provides a review of related literature, and nally, in section 8 we summarize and
discuss our approach with an emphasis on the implications for other RL problems.

2. Pac-Man and Reinforcement Learning
The video-game Pac-Man was rst released in 1979, and reached immense success. It is
considered to be one of the most popular video games to date (Wikipedia, 2006).
The player maneuvers Pac-Man in a maze (see Fig. 1), while Pac-Man eats the dots in
the maze. In this particular maze there are 174 dots,1 each one is worth 10 points. A level
is nished when all the dots are eaten. To make things more dicult, there are also four
ghosts in the maze who try to catch Pac-Man, and if they succeed, Pac-Man loses a life.
Initially, he has three lives, and gets an extra life after reaching 10,000 points.
There are four power-up items in the corners of the maze, called power dots (worth
40 points). After Pac-Man eats a power dot, the ghosts turn blue for a short period (15
seconds), they slow down and try to escape from Pac-Man. During this time, Pac-Man is
1. The maze of the original Pac-Man game is slightly dierent. This description applies to the opensource Pac-Man implementation of Courtillat (2001). The two versions are about equivalent in terms of
complexity and entertainment value.

660

Learning to play Ms. Pac-Man

Figure 1: A snapshot of the Pac-Man game
able to eat them, which is worth 200, 400, 800 and 1600 points, consecutively. The point
values are reset to 200 each time another power dot is eaten, so the player would want to
eat all four ghosts per power dot. If a ghost is eaten, his remains hurry back to the center
of the maze where the ghost is reborn. At certain intervals, a fruit appears near the center
of the maze and remains there for a while. Eating this fruit is worth 100 points.
Our investigations are restricted to learning an optimal policy for the rst level, so the
maximum achievable score is 174 · 10 + 4 · 40 + 4 · (200 + 400 + 800 + 1600) = 13900 plus
100 points for each time a fruit is eaten.
In the original version of Pac-Man, ghosts move on a complex but deterministic route, so
it is possible to learn a deterministic action sequence that does not require any observations.
Many such patterns were found by enthusiastic players. In most of Pac-Man's sequels, most
notably in Ms. Pac-Man, randomness was added to the movement of the ghosts. This way,
there is no single optimal action sequence, observations are necessary for optimal decision
making. In other respects, game play was mostly unchanged.
In our implementation, ghosts moved randomly in 20% of the time and straight towards
Pac-Man in the remaining 80%, but ghosts may not turn back (following Koza, 1992, Chapter
12). To emphasize the presence of randomness, we shall refer to our implementation as a
Ms. Pac-Man-clone.

2.1 Ms. Pac-Man as an RL Task
Ms. Pac-Man meets all the criteria of a reinforcement learning task. The agent has to make
a sequence of decisions that depend on its observations. The environment is stochastic
(because the paths of ghosts are unpredictable). There is also a well-dened reward function
(the score for eating things), and actions inuence the rewards to be collected in the future.
661

Szita & L®rincz

The full description of the state would include (1) whether the dots have been eaten (one
bit for each dot and one for each power dot), (2) the position and direction of Ms. Pac-Man,
(3) the position and direction of the four ghosts, (4) whether the ghosts are blue (one bit for
each ghost), and if so, for how long they remain blue (in the range of 1 to 15 seconds) (5)
whether the fruit is present, and the time left until it appears/disappears (6) the number
of lives left. The size of the resulting state space is astronomical, so some kind of function
approximation or feature-extraction is necessary for RL.
The action space is much smaller, as there are only four basic actions: go north/south/east/west. However, a typical game consists of multiple hundreds of steps, so the
number of possible combinations is still enormous. This indicates the need for temporally
extended actions.
We have a moderate amount of domain knowledge on Ms. Pac-Man: for one, it is quite
easy to dene high-level observations and action modules that are potentially useful. On
the other hand, constructing a well-performing policy seems much more dicult. Therefore,
we provide mid-level domain knowledge to the algorithm: we use domain knowledge to
preprocess the state information and to dene action modules. On the other hand, it will be
the role of the policy search reinforcement learning to combine the observations and modules
into rule-based policies and nd their proper combination.

3. The Cross-Entropy Method
Our goal is to optimize rule-based policies by performing policy search in the space of
all legal rule-based policies. For this search we apply the cross-entropy method (CEM), a
recently published global optimization algorithm (Rubinstein, 1999). It aims to nd the
(approximate) solution for global optimization tasks in the following form

x∗ := arg max f (x).
x

where f is a general objective function (e.g., we do not need to assume continuity or dierentiability). Below we summarize the mechanism of this method briey (see also section 7.2
for an overview of applications).

3.1 An Intuitive Description
While most optimization algorithms maintain a single candidate solution x(t) in each time
step, CEM maintains a distribution over possible solutions. From this distribution, solution
candidates are drawn at random. This is essentially random guessing, but with a nice trick
it is turned into a highly eective optimization method.
3.1.1 The Power of Random Guessing

Random guessing is an overly simple `optimization' method: we draw many samples from
a xed distribution g , then select the best sample as an estimation of the optimum. In
the limit case of innitely many samples, random guessing nds the global optimum. We
have two notes here: (i) as it has been shown by Wolpert and Macready (1997), for the
most general problems, uniform random guessing is not worse than any other method, (ii)
nonetheless, for practical problems, uniform random guessing can be extremely inecient.
662

Learning to play Ms. Pac-Man

Thus, random guessing is safe to start with, but as one proceeds with the collection of
experience, it should be limited as much as possible.
The eciency of random guessing depends greatly on the distribution g from which the
samples are drawn. For example, if g is sharply peaked around x∗ , then very few samples
may be sucient to get a good estimate. The case is the opposite, if the distribution is
sharply peaked around x 6= x∗ : a tremendous number of examples may be needed to get a
good estimate of the global optimum. Naturally, nding a good distribution is at least as
hard as nding x∗ .
3.1.2 Improving the Efficiency of Random Guessing

After drawing moderately many samples from distribution g , we may not be able to give
an acceptable approximation of x∗ , but we may still obtain a better sampling distribution.
The basic idea of CEM is that it selects the best few samples, and modies g so that it
becomes more peaked around them. Consider an example, where x is a 0-1 vector and g is
a Bernoulli distribution for each coordinate. Suppose that we have drawn 1000 samples and
selected the 10 best. If we see that in the majority of the selected samples, the ith coordinate
is 1, then CEM shifts the Bernoulli distribution of the corresponding component towards 1.
Afterwards, the next set of samples is drawn already from the modied distribution.
The idea seems plausible: if for the majority of the best-scoring samples the ith coordinate
was 1, and there is a structure in the tness landscape, then we may hope that the ith
coordinate of x∗ is also 1. In what follows, we describe the update rule of CEM in a more
formal way and sketch its derivation.

3.2 Formal Description of the Cross-Entropy Method
We will pick g from a family of parameterized distributions, denoted by G , and describe an
algorithm that iteratively improves the parameters of this distribution g .
Let N be the number of samples to be drawn, and let the samples x(1) , . . . , x(N ) be
drawn independently from distribution g . For each γ ∈ R, the set of high-valued samples,

L̂γ := {x(i) | f (x(i) ) ≥ γ, 1 ≤ i ≤ N },
provides an approximation to the level set

Lγ := {x | f (x) ≥ γ}.
Let Uγ be the uniform distribution over the level set Lγ . For large values of γ , this distribution will be peaked around x∗ , so it would be suitable for random sampling. This raises two
potential problems: (i) for large γ values L̂γ will contain very few points (possibly none),
making accurate approximation impossible, and (ii) the level set Lγ is usually not a member
of the parameterized distribution family.
The rst problem is easy to avoid by choosing lower values for γ . However, we have to
make a compromise, because setting γ too low would inhibit large improvement steps. This
compromise is achieved as follows: CEM chooses a ratio ρ ∈ [0, 1] and adjusts L̂γ to be the
set of the best ρ · N samples. This corresponds to setting γ := f (x(ρ·N ) ), provided that the
samples are arranged in decreasing order of their values. The best ρ · N samples are called
the elite samples. In practice, ρ is typically chosen from the range [0.02, 0.1].
663

Szita & L®rincz

The other problem is solved by changing the goal of the approximation: CEM chooses
the distribution g from the distribution family G that approximates best the empirical distribution over L̂γ . The best g is found by minimizing the distance of G and the uniform
distribution over the elite samples. The measure of distance is the cross-entropy distance
(often called Kullback-Leibler divergence). The cross-entropy distance of two distributions
g and h is dened as
Z
g(x)
DCE (g||h) = g(x) log
dx
h(x)
The general form of the cross-entropy method is summarized in Table 1. It is known that
under mild regularity conditions, the CE method converges with probability 1 (Margolin,
2004). Furthermore, for a suciently large population, the global optimum is found with
high probability.
input: G
input: g0 ∈ G
input: N
input: ρ
input: T
for t from 0 to T − 1,
for i from 1 to N ,
draw x(i) from distribution gt
compute fi := f (x(i) )
sort fi -values in descending order
γt+1 := fρ·N
Et+1 := {x(i) | f (x(i) ) ≥ γt+1 }
gt+1 := arg ming∈G DCE (g||Uniform(Et+1 ))
end loop

%
%
%
%
%
%

parameterized distrib. family
initial distribution
population size
selection ratio
number of iterations
CEM iteration main loop

% draw N samples
% evaluate them
% level set threshold
% get elite samples
% get nearest distrib. from G

Table 1: Pseudo-code of the general cross-entropy method

3.3 The Cross-Entropy Method for Bernoulli Distribution
For many parameterized distribution families, the parameters of the minimum cross-entropy
member can be computed easily from simple statistics of the elite samples. We provide the
formulae for Bernoulli distributions, as these will be needed for the policy learning procedure
detailed in the next section. Derivations as well as a list of other discrete and continuous
distributions that have simple update rules can be found in the tutorial of de Boer, Kroese,
Mannor, and Rubinstein (2004).
Let the domain of optimization be D = {0, 1}m , and each component be drawn from
independent Bernoulli distributions, i.e., G = Bernoullim . Each distribution g ∈ G is parameterized with an m-dimensional vector p = (p1 , . . . , pm ). When using g for sampling,
664

Learning to play Ms. Pac-Man

component j of the sample x ∈ D will be
½
1, with probability pj ;
xj =
0, with probability 1 − pj .
After drawing N samples x(1) , . . . , x(N ) and xing a threshold value γ , let E denote the set
of elite samples, i.e.,
E := {x(i) | f (x(i) ) ≥ γ}
With this notation, the distribution g 0 with minimum CE-distance from the uniform distribution over the elite set has the following parameters:

p0 := (p01 , . . . , p0m ), where
P
P
(i)
(i)
x(i) ∈E χ(xj = 1)
x(i) ∈E χ(xj = 1)
0
P
=
pj :=
ρ·N
x(i) ∈E 1

(1)

In other words, the parameters of g 0 are simply the component wise empirical probabilities
of 1's in the elite set. For the derivation of this rule, see the tutorial of de Boer et al. (2004).
Changing the distribution parameters from p to p0 can be too coarse, so in some cases,
applying a step-size parameter α is preferable. The resulting algorithm is summarized in
Table 2.
input: p0 = (p0,1 , . . . , p0,m )
input: N
input: ρ
input: T
for t from 0 to T − 1,
for i from 1 to N ,
draw x(i) from Bernoullim (pt )
compute fi := f (x(i) )
sort fi -values in descending order
γt+1 := fρ·N
Et+1 := {x(i) | f (x(i) ) ≥ γt+1 }
¡P
¢
(i)
p0j :=
x(i) ∈E χ(xj = 1) /(ρ · N )
pt+1,j := α · p0j + (1 − α) · pt,j
end loop

%
%
%
%
%

initial distribution parameters
population size
selection ratio
number of iterations
CEM iteration main loop

% draw N samples
% evaluate them
%
%
%
%

level set threshold
get elite samples
get parameters of nearest distrib.
update with step-size α

Table 2: Pseudo-code of the cross-entropy method for Bernoulli distributions
We will also need to optimize functions over D = {1, 2, . . . , K}m with K > 2. In the
simplest case, distributions over this domain can be parameterized
by m · K parameters:
PK
p = (p1,1 , . . . , p1,K ; . . . ; pm,1 , . . . , pm,K ) with 0 ≤ pj,k ≤ 1 and k=1 pj,k = 1 for each j (this
is a special case of the multinomial distribution).
The update rule of the parameters is essentially the same as Eq. 1 for the Bernoulli case:
P
P
(i)
(i)
(i) ∈E χ(xj = k)
x(i) ∈E χ(xj = k)
x
0
P
=
.
(2)
pj,k :=
ρ·N
x(i) ∈E 1
665

Szita & L®rincz

Note that constraint

PK

0
k=1 pj,k

= 1 is satised automatically for each j .

4. Rule-Based Policies
In a basic formulation, a rule is a sentence of the form  if [Condition] holds, then do
[Action]. A rule-based policy is a set of rules with some mechanism for breaking ties, i.e.,
to decide which rule is executed, if there are multiple rules with satised conditions.
Rule-based policies are human-readable, it is easy to include domain knowledge, and they
are able to represent complex behaviors. For these reasons, they are often used in many
areas of articial intelligence (see section 7.3 for a short overview of related literature).
In order to apply rule-based policies to Ms. Pac-Man, we need to specify four things:
(1) what are the possible actions (2) what are the possible conditions and how are they
constructed from observations, (3) how to make rules from conditions and actions, and
(4) how to combine the rules into policies. The answers will be described in the following
sections.

4.1 Action Modules
While dening the action modules for Ms. Pac-Man, we listed only modules that are easy
to implement but are considered potentially useful (see Table 3). This way, we kept human work at a minimum, but still managed to formalize a part of our domain knowledge
about the problem. As a consequence, this list of action modules is by no means optimal: some actions could be more eective with a more appropriate denition, others
may be superuous. For example, there are four dierent modules for ghost avoidance:
FromGhost escapes from the nearest ghost, without considering the position of the other
ghosts; ToLowerGhostDensity tries to take into account the inuence of multiple ghosts;
FromGhostCenter moves out from the geometrical center of ghosts, thus, it is able to avoid
being surrounded and trapped, but, on the other hand, it can easily bump into a ghost
while doing so; and nally, ToGhostFreeArea considers the whole board in search of a safe
location, so that the agent can avoid being shepherded by the ghosts. All of these modules
may have their own strengths and weaknesses, and possibly a combination of them is needed
for success. There can also be actions, which are potentially useful, but were not listed here
(for example, moving towards the fruit).
Note also that the modules are not exclusive. For example, while escaping from the
ghosts, Ms. Pac-Man may prefer the route where more dots can be eaten, or she may want to
head towards a power dot. Without the possibility of concurrent actions, the performance of
the Ms. Pac-Man agent may be reduced considerably (which is investigated in experimental
section 5.3).
We need a mechanism for conict resolution, because dierent action modules may suggest dierent directions. We do this by assigning priorities to the modules. When the agent
switches on an action module, she also decides about its priority. This is also a decision,
and learning this decision is part of the learning task.2
2. Action priorities are learnt indirectly: each rule has a xed priority, and when an action is switched
on by a rule, it also inherits this priority. The same action can be switched on by dierent rules with
dierent priorities. The mechanism is described in detail in section 4.6.

666

Learning to play Ms. Pac-Man

Table 3: List of action modules used for rule construction.
Name

Description

ToDot
ToPowerDot
FromPowerDot

Go towards the nearest dot.
Go towards the nearest power dot.
Go in direction opposite to the nearest power
dot.
Go towards the nearest edible (blue) ghost.
Go in direction opposite to the nearest ghost.
Go towards the maximally safe junction. For all
four directions, the safety of the nearest junction is estimated in that direction. If Ms. PacMan is n steps away from the junction and the
nearest ghost is k steps away, then the safety
value of this junction is n − k . A negative value
means that Ms. Pac-Man possibly cannot reach
that junction.
Go in a direction which maximizes the Euclidean
distance from the geometrical center of ghosts.
Go further in the current direction, or choose a
random available action (except turning back) if
that is impossible.
go in the direction where the cumulative ghost
density decreases fastest. Each ghost denes a
density cloud (with radius = 10 and linear decay), from which the cumulative ghost density is
calculated.
Choose a location on the board where the minimum ghost distance is largest, and head towards
it on the shortest path.

ToEdGhost
FromGhost
ToSafeJunction

FromGhostCenter
KeepDirection
ToLowerGhostDensity

ToGhostFreeArea

667

Szita & L®rincz

Table 4: List of observations used for rule construction. Distances denote the length of the
shortest path, unless noted otherwise. Distance to a particular object type is +∞
if no such object exists at that moment.
Name

Description

Constant
NearestDot
NearestPowerDot
NearestGhost
NearestEdGhost
MaxJunctionSafety

Constant 1 value.
Distance of nearest dot.
Distance of nearest power dot.
Distance of nearest ghost.
Distance of nearest edible (blue) ghost.
For all four directions, the safety of the nearest
junction in that direction is estimated, as dened
in the description of action ToSafeJunction.
The observation returns the value of the maximally safe junction.
Euclidean distance from the geometrical center
of ghosts.
Euclidean distance from the geometrical center
of uneaten dots.
Each ghost denes a density cloud (with radius
= 10 and linear decay). Returns the value of the
cumulative ghost density.
travelling salesman distance to ghosts: the
length of the shortest route that starts at
Ms. Pac-Man and reaches all four ghosts (not
considering their movement).

GhostCenterDist
DotCenterDist
GhostDensity
TotalDistToGhosts

We implemented this with the following mechanism: a decision of the agent concerns
action modules: the agent can either switch on or, switch o an action module. That is, in
principle, the agent is able to use any subset of the action modules, instead of selecting a
single one at each time step. Basically, the module with highest priority decides the direction
of Ms. Pac-Man. If there are more than one equally ranked directions, then lower-priority
modules are checked. If the direction cannot be decided after checking switched-on modules
in the order of decreasing priority (for example, no module is switched on, or two directions
are ranked equally by all switched-on modules), then a random direction is chosen.
Ms. Pac-Man can make decisions each time she advances a whole grid cell (the above
mechanism ensures that she never stands still), according to 25 game ticks or approx. 0.2
seconds of simulated game time.

4.2 Observations, Conditions and Rules
Similarly to actions, we can easily dene a list of observations which are potentially useful
for decision making. The observations and their descriptions are summarized in Table 4.
668

Learning to play Ms. Pac-Man

Modules could have been improved in many ways, for example, checking whether there is
enough time to intercept edible ghosts when calculating NearestEdGhost or taking into
consideration the movement of ghosts when calculating NearestGhost, NearestEdGhost or
MaxJunctionSafety. We kept the implementation of the modules as simple as possible.
We designed reasonable modules, but no eort was made to make the module denitions
optimal, complete or non-redundant.
Now we have the necessary tools for dening the conditions of a rule. A typical condition
is true if its observations are in a given range. We note that the status of each action module
is also important for proper decision making. For example, the agent may decide that if a
ghost is very close, then she switches o all modules except the escape module. Therefore
we allow conditions that check whether an action module is `on' or `o'.
For the sake of simplicity, conditions were restricted to have the form [observation]
< [value], [observation] > [value], [action]+, [action]-, or the conjunction of such
terms. For example,

(NearestDot<5) and (NearestGhost>8) and (FromGhost+)
is a valid condition for our rules.
Once we have conditions and actions, rules can be constructed easily. In our implementation, a rule has the form  if [Condition], then [Action]. For example,

if (NearestDot<5) and (NearestGhost>8) and (FromGhost+)
then FromGhostCenter+
is a valid rule.

4.3 Constructing Policies from Rules
Decision lists are standard forms of constructing policies from single rules. This is the
approach we pursue here, too. Decision lists are simply lists of rules, together with a
mechanism that decides the order in which the rules are checked.
Each rule has a priority assigned. When the agent has to make a decision, she checks her
rule list starting with the ones with highest priority. If the conditions of a rule are fullled,
then the corresponding action is executed, and the decision-making process halts.
Note that in principle, the priority of a rule can be dierent from the priority of action
modules. However, for the sake of simplicity, we make no distinction: if a rule with priority
k switches on an action module, then the priority of the action module is also taken as k .
Intuitively, this makes sense: if an important rule is activated, then its eect should also be
important. If a rule with priority k switches o a module, then it is executed, regardless of
the priority of the module.
It may be worth noting that there are many possible alternatives for ordering rules and
actions:

• Each rule could have a xed priority, as a part of the provided domain knowledge
(Spronck, Ponsen, Sprinkhuizen-Kuyper, & Postma, 2006).
• The priority of a rule could be a free parameter that should be learned by the CEM
method.
669

Szita & L®rincz

• Instead of absolute priorities, the agent could also learn the relative ordering of rules
(Timuri, Spronck, & van den Herik, 2007).
• The order of rules could be determined by some heuristic decision mechanism. For
example, the generality of the rule  e.g., rules with few/many conditions and large/small domains  could be taken into account. Such heuristics have been used in linear
classier systems (see e.g. the work of Bull & Kovacs, 2005)
In principle, one would like to nd interesting solutions using the computer with minimal
bias from `domain knowledge'. In this regard, the eciency of our simple priority management method was satisfactory, so we did not experiment with other priority heuristics.

4.4 An Example
Let us consider the example shown in Table 5. This is a rule-based policy for the Ms. PacMan agent.
Table 5: A hand-coded policy for playing Ms. Pac-Man. Bracketed numbers denote
priorities, [1] is the highest priority.
[1]
[1]
[2]
[2]
[3]
[3]
[3]
[3]

if
if
if
if
if
if
if
if

NearestGhost<4 then FromGhost+
NearestGhost>7 and JunctionSafety>4 then FromGhostNearestEdGhost>99 then ToEdGhostNearestEdGhost<99 then ToEdGhost+
Constant>0 then KeepDirection+
FromPowerDot- then ToPowerDot+
GhostDensity<1.5 and NearestPowerDot<5 then FromPowerDot+
NearestPowerDot>10 then FromPowerDot-

The rst two rules manage ghost avoidance: if a ghost is too close, then the agent should
ee, and she should do so until she gets to a safe distance. Ghost avoidance has priority
over any other activities. The next two rules regulate that if there is an edible ghost on
the board, then the agent should chase it (the value of NearestEdGhost is innity (> 99) if
there are no edible ghosts, but it is ≤ 41 on our board, if there are). This activity has also
relatively high priority, because eating ghosts is worth lots of points, but it must be done
before the blueness of the ghosts disappears, so it must be done quickly. The fth rule says
that the agent should not turn back, if all directions are equally good. This rule prevents
unnecessary zigzagging (while no dots are being eaten), and it is surprisingly eective. The
remaining rules tweak the management of power dots. Basically, the agent prefers to eat
a power dot. However, if there are blue ghosts on the board, then a power dot resets the
score counter to 200, so it is a bad move. Furthermore, if ghost density is low around the
agent, then most probably it will be hard to collect all of the ghosts, so it is preferable to
wait with eating the power dot.
670

Learning to play Ms. Pac-Man

4.5 The Mechanism of Decision Making
The mechanism of decision making is depicted in Fig 2. In short, the (hidden) state-space
is the world of the Ms. Pac-Man and the Ghosts. The dynamics of this (hidden) statespace determines the vector of observations, which can be checked by the conditions. If the
conditions of a rule are satised, the corresponding action module is switched on or o. As
a consequence, multiple actions may be in eect at once. For example, the decision depicted
in Fig. 2 sets two actions to work together.

Figure 2: Decision-making mechanism of the Ms. Pac-Man agent. At time step t,
the agent receives the actual observations and the state of her action modules. She
checks the rules of her script in order, and executes the rst rule with satised
conditions.
Initially, each action module is in switched-o state. After a module has been switched
on, it remains so until it is either explicitly switched o or another module of the same
priority is switched on and replaces it.

4.6 Learning Rule-Based Policies by CEM
We will apply CEM for searching in the space of rule-based policies. Learning is composed of
three phases: (1) the generation of random policies drawn according to the current parameter
set, (2) evaluation of the policies, which consists of playing a game of Ms. Pac-Man to
measure the score, and (3) updating the parameter set using the CEM update rules.
4.6.1 Drawing Random Scripts from a Predefined Rule-Base

Suppose that we have a predened rule-base containing K rules (for example, the one listed
in Appendix A). A policy has m rule slots. Each slot can be lled with any of the K rules,
671

Szita & L®rincz

or left empty. As a result, policies could contain up to m rules, but possibly much less. Each
rule slot has a xed priority, too, from the set {1, 2, 3}.3 The priority of a rule slot does not
change during learning. Learning can, however, push an important rule to a high-priority
slot from a low-priority one, and vice versa.
For each 1 ≤ i ≤ m, slot i was lled with a rule from the rule-base with probability pi ,
and left empty with probability 1 − pi . If it was decided that a slot should
PKbe lled, then
a particular rule j (1 ≤ j ≤ K ) was selected with probability qi,j , where j=1 qi,j = 1 for
each slot i ∈ {1, . . . , m}. As a result, policies could contain up to m rules, but possibly much
less. Both the pi values and the qi,j values are learnt simultaneously with the cross-entropy
method (Table 2), using the update rules (1) and (2), respectively. This gives a total of
m + m · K parameters to optimize (although the eective number of parameters is much
less, because the qi,j values of unused slots are irrelevant). Initial probabilities are set to
pi = 1/2 and qi,j = 1/K .
4.6.2 Drawing Random Rules without a Predefined Rule-Base

We studied situations with lessened domain knowledge; we did not use a predened rulebase. Script generation was kept the same, but the rule-base of K rules was generated
randomly. In this case we generated dierent rule-bases for each of the m rule slots; the low
ratio of meaningful rules was counteracted by increased rule variety.
A random rule is a random pair of a randomly drawn condition set and a randomly
drawn action. Random condition sets contained 2 conditions. A random action is constructed as follows: an action module is selected uniformly from the set of modules listed in
Table 3, and switched on or o with probability 50%. The construction of a random condition starts with the uniformly random selection of a module from either Table 3 or Table
4. If the selected module is an action, then the condition will be [action]- or [action]+
with equal probability. If the selected module is an observation, then the condition will be
[observation]<[value] or [observation]>[value] with equal probability, where [value]
is selected uniformly from a ve-element set. The values in this set were determined separately for each observation module as follows: we played 100 games using a xed policy and
recorded the histogram of values for each observation. Subsequently, the ve-element set
was determined so that it would split the histogram into regions of equal area. For example,
the value set for NearestGhost was {12, 8, 6, 5, 4}.
The design of the random rule generation procedure contains arbitrary elements (e.g.
the number of conditions in a rule, the number of values an observation can be compared
to). The intuition behind this procedure was to generate rules that are suciently versatile,
but the ratio of meaningless rules (e.g. rules with unsatisable conditions) is not too large.
However, no optimization of any form was done at this point.

5. Description of Experiments
According to our assumptions, the eectiveness of the above described architecture is based
on three pillars: (1) the human domain knowledge provided by the modules and rules; (2)
3. According to our preliminary experiments, the quality of the learned policy did not improve by increasing
the priority set or the number of the slots.

672

Learning to play Ms. Pac-Man

the eectiveness of the optimization algorithm; (3) the possibility of concurrent actions.
Below, we describe a set of experiments that were designed to test these assumptions.

5.1 The Full Architecture
In the rst experiment, random rules are used. In their construction, we use all the modules
dened in sections 4.1 and 4.2. In the second experiment, rules were not generated randomly,
but were hand-coded. In this case, the role of learning is only to determine which rules should
be used.
5.1.1 Learning with Random Rule Construction

In the rst experiment, the rule-base was generated randomly, as described in section 4.6.2.
The number of rule slots was xed to m = 100 (priorities were distributed evenly), each one
containing K = 100 randomly generated rules. The values for K and m were selected by
coarse search over parameter space.
The parameters of CEM were as follows: population size N = 1000, selection ratio
ρ = 0.05, step size α = 0.6.4 These values for ρ and α are fairly standard for CEM, and
we have not tried varying them. In each step, the probabilities of using a rule slot (that
is, the values pi , but not qi,j ) were slightly decreased, by using a decay rate of β = 0.98.
With larger decay rate, useful rules were also annulled too often. On the other hand, smaller
decay did not aect the performance, but many superuous rules were left in the policies.
The score of a given policy has huge variance due to the random factors in the game.
Therefore, to obtain reliable tness estimations, the score of each policy was averaged over
3 subsequent games. Learning lasted for 50 episodes, which was sucient to tune each
probability close to either 0 or 1. We performed 10 parallel training runs. This experiment
type is denoted as CE-RandomRB.
5.1.2 Learning with Hand-Coded Rules

In the second experiment we constructed a rule-base of K = 42 hand-coded rules (shown in
Appendix A) that were thought to be potentially useful. These could be placed in one of
the m = 30 rule slots.5 Other parameters of the experiment were identical to the previous
one. This experiment type is denoted as CE-FixedRB.

5.2 The Eect of the Learning Algorithm
In the following experiment, we compared the performance of CEM to simple stochastic
gradient optimization. This single comparison is not sucient to measure the eciency
of CEM; it serves to provide a point of reference. The comparison is relevant, because
these algorithms are similar in complexity and both of them move gradually towards the
best samples that were found so far. The dierence is that SG maintains a single solution
4. Note that α is the per-episode learning rate. This would correspond to a per-instance learning rate of
α0 = α/(ρ · N ) = 0.012 for an on-line learning algorithm.
5. In contrast to the previous experiment, all of the rules are meaningful and potentially useful. Therefore
there is no need for a large pool of rules, and a much lower m can be used. We found that the algorithm
is fairly insensitive to the choice of m; signicant changes in performance can be observed if parameter
m is modied by a factor of 3.

673

Szita & L®rincz

candidate at a time, whereas CEM maintains a distribution over solutions. Thus, CEM
maintains a memory over solutions and becomes less fragile to occasional wrong parameter
changes.
The particular form of stochastic gradient search was the following: the initial policy was
drawn at random (consisting of 6 rules). After that, we generated 100 random mutation of
the current solution candidate at each step, and evaluated the obtained policies. The bestperforming mutation was chosen as the next solution candidate. Mutations were generated
using the following procedure: (1) in each rule, each condition was changed to a random new
condition with probability 0.05; (2) in each rule, the action was changed to a random new
action with probability 0.05. The listed parameter values (number of rules in policy, number
of mutated policies, probabilities of mutation) were the results of coarse parameter-space
optimization.
The number of episodes was set to 500. This way, we evaluated the same number of
dierent policies (50,000) as in the CEM experiments. Both the random rule-base and the
xed rule-base experiments were repeated using the stochastic gradient method, executing
10 parallel training runs. The resulting policies are denoted as SG-RandomRB and SGFixedRB, respectively.

5.3 The Eect of Parallel Actions
According to our assumptions, the possibility of parallel actions plays a crucial role in the
success of our architecture. To conrm this assumption, we repeated previous experiments
with concurrent actions disabled. If the agent switches on an action module, all other
action modules are switched o automatically. These experiment types are denoted as
CE-RandomRB-1action, CE-FixedRB-1action, SG-RandomRB-1action and SGFixedRB-1action.

5.4 Baseline Experiments
In order to isolate and assess the contribution of learning, we performed two additional
experiments with dierent amounts of domain knowledge and no learning. Furthermore, we
asked human subjects to play the game.
5.4.1 Random Policies

In the rst non-learning experiment, we used the rule-base of 42 hand-coded rules (identical
to the rule-base of CE-FixedRB). Ten rules were selected at random, and random priorities
were assigned to them. We measured the performance of policies constructed this way.
5.4.2 Hand-Coded Policy

In the second non-learning experiment, we hand-coded both the rules and the priorities, that
is, we hand-coded the full policy. The policy is shown in Table 5, and has been constructed
by some trial-and-error. Naturally, the policy was constructed before knowing the results of
the learning experiments.
674

Learning to play Ms. Pac-Man

Table 6: Ms. Pac-Man results. See text for details. Abbreviations: CE: learning with
the cross-entropy method, SG: learning with stochastic gradient, randomRB:
randomly generated rule-base, fixedRB: xed, hand-coded rule-base, 1action:
only one action module can work at a time.
Method

Avg. Score

(25%/75% percentiles)

6382
4135
5449

(6147/6451)
(6682/9369)
(3356/5233)
(4843/6090)

CE-randomRB-1action
CE-fixedRB-1action
SG-randomRB-1action
SG-fixedRB-1action

5417
5631
2267
4415

(5319/5914)
(5705/5982)6
(1770/2694)
(3835/5364)

Random policy
Hand-coded policy
Human play

676
7547
8064

(140/940)
(6190/9045)
(5700/10665)

CE-randomRB
CE-fixedRB
SG-randomRB
SG-fixedRB

8186

5.4.3 Human Play

In the nal experiment, ve human subjects were asked to play the rst level of Ms. PacMan and we measured their performance. Each of the subjects has played Pac-Man and/or
similar games before, but none of them was an experienced player.

6. Experimental Results
Human experiments were performed on the rst level of an open-source Pac-Man clone of
Courtillat (2001). For the other experiments we applied the Delphi re-implementation of
the code.
In all learning experiments, 10 parallel learning runs were executed, each one for 50
episodes. This training period was sucient to tune all probabilities close to either 0 or
1, so the learned policy could be determined unambiguously in all cases. Each obtained
policy was tested by playing 50 consecutive games, giving a total of 500 test games per
experiment. In the non-learning experiments the agents played 500 test games, too, using
random policies and the hand-coded policy, respectively. Each human subject played 20
games, giving a total of 100 test games. Results are summarized in Table 6. We provide
25% and 75% percentile values instead of the variances, because the distribution of scores
is highly non-Gaussian.
6. The fact that the average is smaller than the 25% percentile is caused by a highly skewed distribution
of scores. In most games, the agent reached a score in the range 5800 ± 300, except for a few games
with extremely low score. These few games did not aect the 25% percentile but lowered the average
signicantly.

675

Szita & L®rincz

[1]
[1]
[2]
[2]
[2]
[3]

if
if
if
if
if
if

NearestGhost<3 then FromGhost+
MaxJunctionSafety>3 then FromGhostNearestEdGhost>99 then ToPowerDot+
NearestEdGhost<99 then ToEdGhost+
GhostDensity<1.5 and NearestPowerDot<5 then FromPowerDot+
Constant>0 then ToCenterofDots+

Figure 3: Best policy learned by CE-fixedRB. Average score over 50 games: 9480.
[1]
[1]
[1]
[2]
[2]
[2]
[3]

if
if
if
if
if
if
if

MaxJunctionSafety>2.5 and ToLowerGhostDensity- then FromGhostNearestGhost<6 and MaxJunctionSafety<1 then FromGhost+
NearestGhost>6 and FromGhostCenter- then ToEdGhost+
ToEdGhost- and CenterOfDots>20 then ToEdGhost+
ToEdGhost- and NearestEdGhost<99 then ToEdGhost+
NearestDot>1 and GhostCenterDist>0 then KeepDirection+
ToGhostFreeArea- and ToDot- then ToPowerDot+

Figure 4: Best policy learned by CE-randomRB. Average score over 50 games: 7199.
Note the presence of always-true (and thus, superuous) conditions like
ToLowerGhostDensity-, FromGhostCenter-, ToGhostFreeArea- or ToDot-.

Fig. 3 shows the best individual policy learned by CE-fixedRB, reaching 9480 points
on average. Ghost avoidance is given highest priority, but is only turned on when a ghost
is very close. Otherwise Ms. Pac-Man concentrates on eating power dots and subsequently
eating the blue ghosts. She also takes care not to eat any power dot while there are blue
ghosts on the board, because otherwise she would miss the opportunity to eat the 1600-point
ghost (and possibly several others, too). With lowest priority setting, the agent looks for
ordinary dots, although this rule is in eect only when the previous rules can not decide on
a direction (for example, in the endgame when there are no power dots left and all ghosts
are in their original form).
Policies learnt by CE-randomRB behave similarly to the ones learnt by CE-fixedRB,
although the behavior is somewhat obscured by superuous conditions and/or rules, as
demonstrated clearly on the example policy shown in Fig. 4. Because of the noise generated
by the random rules, the algorithm often fails to learn the correct priorities of various
activities.
The eect of enabling/disabling concurrent actions is also signicant. It is instructive
to take a look at the best policy learned by CE-fixedRB-1action shown in Fig. 5: the
agent has to concentrate on eating ghosts, as it is the major source of reward. However, she
cannot use modules that are necessary for ghost avoidance and long-term survival.
The results also show that CEM performs signicantly better than stochastic gradient
learning. We believe, however, that this dierence could be lowered with a thorough search
over the parameter space. SG and many other global optimization methods like evolutionary
methods or simulated annealing could reach similar performances to CEM. According to
de Boer et al. (2004) and the applications cited in section 7.2, an advantage of CEM is
676

Learning to play Ms. Pac-Man

[2] if NearestEdGhost>99 then ToPowerDot+
[2] if NearestEdGhost<99 then ToEdGhost+

Figure 5: Best policy learned by CE-fixedRB-1action. Average score over 50 games:
6041.

that it maintains a distribution of solutions and can reach robust performance with very
little eort, requiring little or no tuning of the parameters: there is a canonical set of
parameters (0.01 ≤ ρ ≤ 0.1, 0.5 ≤ α ≤ 0.8, population as large as possible) for which
the performance of the method is robust. This claim coincides with our experiences in the
parameter-optimization process.
Finally, it is interesting to analyze the dierences between the tactics of human and
computer players. One fundamental tactic of human players is that they try to lure the
ghosts close to Ms. Pac-Man such that all ghosts are very close to each other. This way,
all of them can be eaten fast when they turn blue. No such behavior evolved in any of our
experiments. Besides, there are other tactics that CEM has no chance to discover, because
it is lacking the appropriate sensors. For example, a human player can (and does) calculate
the time remaining from the blue period, the approximate future position of ghosts, and so
on.

7. Related Literature
In this section, we review literature on learning the Pac-Man game, and on various components of our learning architecture: the cross-entropy method, rule-based policies, and
concurrent actions.

7.1 Previous Work on (Ms.) Pac-Man
Variants of Pac-Man have been used previously in several studies. A direct comparison of
performances is possible only in a few cases, however, because simplied versions of the
game are used in most of the other studies.
Koza (1992) uses Ms. Pac-Man as an example application for genetic programming. It
uses dierent score value for the fruit (worth 2000 points instead of the 100 points used
here), and the shape of the board (and consequently, the number of dots) is also dierent,
therefore scores cannot be directly compared. However, Koza reports (on p. 355) that The
Pac Man could have scored an additional 9000 points if he had captured all four monsters
on each of the four occasions when they turned blue. This score, the only one reported,
translates to approximately 5000 points in our scoring system.
Lucas (2005) also uses the full-scale Ms. Pac-Man game as a test problem. He trains
a neural network position evaluator with hand-crafted input features. For the purposes of
training, he uses an evolutionary strategy approach. The obtained controller was able to
reach 4781 ± 116 points, averaged over 100 games.
Bonet and Stauer (1999) restrict observations to a 10 × 10 window centered at Ms. PacMan, and uses a neural network and temporal-dierence learning to learn a reactive con677

Szita & L®rincz

troller. Through a series of increasingly dicult learning tasks, they were able to teach basic
pellet-collecting and ghost-avoidance behaviors in greatly simplied versions of the game:
they used simple mazes containing no power dots and only one ghost.
Gallagher and Ryan (2003) denes the behavior of the agent as a parameterized nite
state automata. The parameters are learnt by population-based incremental learning, an
evolutionary method similar to CEM. They run a simplied version of Pac-Man; they had a
single ghost and had no power dots, which takes away most of the complexity of the game.
Tiong (2002) codes rule-based policies for Pac-Man by hand, but uses no learning to
improve them. His tests, similarly to ours, are based on the Pac-Man implementation of
Courtillat (2001), but he limits the number of ghosts to 1. The best-performing rule set
reaches 2164 points on average out of the maximal 2700. However, the results are not likely
to scale up well with increasing the number of ghosts: the ghost is eaten only 1.4 times on
average (out of the possible 4 times per game).7

7.2 The Cross-Entropy Method
The cross-entropy method of Rubinstein (1999) is a general algorithm for global optimization
tasks, bearing close resemblance to estimation-of-distribution evolutionary methods (see e.g.
the paper of Muehlenbein, 1998). The areas of successful application range from combinatorial optimization problems like the optimal buer allocation problem (Allon, Kroese, Raviv,
& Rubinstein, 2005), DNA sequence alignment (Keith & Kroese, 2002) to independent process analysis (Szabó, Póczos, & L®rincz, 2006).
The cross-entropy method has several successful reinforcement learning applications, too:
Dambreville (2006) uses CEM for learning an input-output hierarchical HMM that controls
a predator agent in a partially observable grid world; Menache, Mannor, and Shimkin (2005)
use radial basis function approximation of the value function in a continuous maze navigation task, and use CEM to adapt the parameters of the basis functions; and nally, Mannor,
Rubinstein, and Gat (2003) apply CEM to policy search in a simple grid world maze navigation problem. Recently, the cross-entropy method has also been applied successfully to
the game Tetris by Szita and L®rincz (2006).

7.3 Rule-Based Policies
The representation of policies as rule sequences is a widespread technique for complex problems like computer games. As an example, many of the Pac-Man-related papers listed above
use rule-based representation.
Learning classier systems (Holland, 1986) are genetic-algorithm based methods to
evolve suitable rules for a given task. Bull (2004) gives an excellent general overview and
pointers to further references. The Hayek machine of Baum (1996) is a similar architecture,
where agents (corresponding to simple rules) dene an economical system: they make bids
for executing tasks in the hope that they can obtain rewards. Schaul (2005) applies this
architecture for the Sokoban game.
Dynamic scripting (Spronck et al., 2006) is another prominent example of using and
learning rule-based policies. It uses a hand-coded rule-base and a reinforcement-learning7. Results are cited from section 3.6.

678

Learning to play Ms. Pac-Man

like principle to determine the rules that should be included in a policy. Dynamic scripting
has successful applications in state-of-the-art computer games like the role-playing game
Neverwinter Nights (Spronck et al., 2006) and the real-time strategy game Wargus (Ponsen
& Spronck, 2004).

7.4 Concurrent Actions
In traditional formalizations of RL tasks, the agent can select and execute a single action at
a time. The only work known to us that handles concurrent actions explicitly is that of Rohanimanesh and Mahadevan (2001). They formalize RL tasks with concurrent actions in the
framework of semi-Markov decision processes and present simple grid world demonstrations.

8. Summary and Closing Remarks
In this article we have proposed a method that learns to play Ms. Pac-Man. We have dened
a set of high-level observation and action modules with the following properties: (i) actions
are temporally extended, (ii) actions are not exclusive, but may work concurrently. Our
method can uncover action combinations together with their priorities. Thus, our agent can
pursue multiple goals in parallel.
The decision of the agent concerns whether an action module should be turned on (if
it is o) or o (if it is on). Furthermore, decisions depend on the current observations and
may depend on the state of action modules. The policy of the agent is represented as a
list of if-then rules with priorities. Such policies are easy to interpret and analyze. It is
also easy to incorporate additional human knowledge. The cross-entropy method is used for
learning policies that play well. Learning is biased towards low-complexity policies, which
is a consequence of both the policy representation and the applied cross entropy learning
method. For CEM, higher complexity solutions are harder to discover and special means
should be used to counteract premature convergence. For solutions of higher complexities,
noise injection has been suggested in our previous work (Szita & L®rincz, 2006). Learned
low complexity policies reached better score than a hand-coded policy or the average human
players.
The applied architecture has the potentials to handle large, structured observation- and
action-spaces, partial observability, temporally extended and concurrent actions. Despite
its versatility, policy search can be eective, because it is biased towards low-complexity
policies. These properties are attractive from the point of view of large-scale applications.

8.1 The Role of Domain Knowledge
When demonstrating the abilities of an RL algorithm, it is desirable that learning starts from
scratch, so that the contribution of learning is clearly measurable. However, the choices of
test problems are often misleading: many `abstract' domains contain considerable amount
of domain knowledge implicitly. As an example, consider grid world navigation tasks, an
often used class of problems for tabula rasa learning.
In a simple version of the grid world navigation task, the state is an integer that uniquely
identies the position of the agent, and the atomic actions are moves to grid cells north/south/east/west from the actual cell. More importantly, the unique identication of the
679

Szita & L®rincz

position means that the moves of the agent do not change the direction of the agent and
the task is in laboratory coordinate framework, sometimes called allocentric coordinates,
and not in egocentric coordinates. The concepts of north, south, etc. correspond to very
high-level abstraction, they have a meaning to humans only, so they must be considered as
part of the domain knowledge. The domain knowledge provided by us is similar to the grid
world in the sense that we also provide high-level observations in allocentric form, such as
`distance of nearest ghost is d' or `Ms. Pac-Man is at position (11, 2)'. Similarly, action `go
north' and action `go towards the nearest power dot' are essentially of the same level.
The implicit presence of high-level concepts becomes even more apparent as we move
from abstract MDPs to the `real world'. Consider a robotic implementation of the maze
task: the full state information, i.e. its own state as well as the state of the environment is
not available for the robot. It sees only local features and it may not see all local features at
a time. To obtain the exact position, or to move one unit's length in the prescribed direction,
the robot has to integrate information from movement sensors, optical/radar sensors etc.
Such information fusion, although necessary, is not a topic of reinforcement learning. Thus,
in this task, there is a great amount of domain knowledge that needs to be provided before
our CE based policy search method could be applied.
In our opinion, the role of human knowledge is that it selects the set of observations and
actions that suit the learning algorithm. Such extra knowledge is typically necessary for most
applications. Nonetheless, numerous (more-or-less successful) approaches exist for obtaining
such domain knowledge automatically. According to one approach, the set of observations
is chosen from a rich (and redundant) set of observations by some feature selection method.
The cross-entropy method seems promising here, too (see the paper of Szita, 2006, for an
application to feature selection from brain fMRI data at the 2006 Pittsburgh Brain Activity
Interpretation Competition). According to a dierent approach, successful combinations
of lower level rules can be joined into higher level concepts/rules. Machine learning has
powerful tools here, e.g. arithmetic coding for data compression (Witten, Neal, & Cleary,
1987). It is applied in many areas, including the writing tool Dasher developed by Ward and
MacKay (2002). Such extensions are to be included into the framework of reinforcement
learning.

8.2 Low-Complexity Policies
The space of legal policies is huge (potentially innite), so it is an interesting question how
search can be eective in such huge space. Direct search is formidable. We think that an
implicit bias towards low-complexity policies can be useful and this is what we studied here.
By low-complexity policy, we mean the following: The policy may consist of very many
rules, but in most cases, only a few of them are applied concurrently. Unused rules do not
get rewarded nor do they get punished unless they limit a useful rule, so the eective length
of policies is biased towards short policies. This implicit bias is strengthened by an explicit
one in our work: in the absence of explicit reinforcement, the probability of applying a rule
decays, so indierent rules get wiped out quickly. It seems promising to use frequent low
complexity rule combinations as building blocks in a continued search for more powerful but
still low-complexity policies.
680

Learning to play Ms. Pac-Man

The bias towards short policies reduces the eective search space considerably. Moreover, for many real-life problems, low-complexity solutions exist (for an excellent analysis of
possible reasons, see the paper of Schmidhuber, 1997). Therefore, search is concentrated on
a relevant part of the policy space, and pays less attention to more complex policies (which
are therefore less likely according to Occam's razor arguments.)

Acknowledgments
Please send correspondence to András L®rincz. The authors would like to thank the anonymous reviewers for their detailed comments and suggestions for improving the presentation
of the paper. This material is based upon work supported partially by the European Oce
of Aerospace Research and Development, Air Force Oce of Scientic Research, Air Force
Research Laboratory, under Contract No. FA-073029. This research has also been supported
by an EC FET grant, the `New Ties project' under contract 003752. Any opinions, ndings
and conclusions or recommendations expressed in this material are those of the authors and
do not necessarily reect the views of the European Oce of Aerospace Research and Development, Air Force Oce of Scientic Research, Air Force Research Laboratory, the EC,
or other members of the EC New Ties project.

Appendix A. The Hand-Coded Rule-Base
Below is a list of rules of the hand-coded rule-base used in the experiments.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

Constant>0 then ToDot+
Constant>0 then ToCenterofDots+
NearestGhost<4 then FromGhost+
NearestGhost<3 then FromGhost+
NearestGhost<5 then FromGhost+
NearestGhost>5 then FromGhostNearestGhost>6 then FromGhostNearestGhost>7 then FromGhostConstant>0 then ToSafeJunction+
MaxJunctionSafety<3 then ToSafeJunction+
MaxJunctionSafety<1 then ToSafeJunction+
MaxJunctionSafety>3 then ToSafeJunctionMaxJunctionSafety>3 then FromGhostMaxJunctionSafety>5 then ToSafeJunctionMaxJunctionSafety>5 then FromGhostConstant>0 then KeepDirection+
Constant>0 then ToEdGhost+
NearestGhost<4 then ToPowerDot+
NearestEdGhost<99 then ToPowerDotNearestEdGhost<99 and NearestPowerDot<5 then FromPowerDot+
NearestEdGhost<99 then FromPowerDot+
NearestEdGhost>99 then FromPowerDotNearestEdGhost>99 then ToPowerDot+
GhostDensity>1 then ToLowerGhostDensity+
GhostDensity<0.5 then ToLowerGhostDensity681

Szita & L®rincz

26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42

if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if
if

NearestPowerDot<2 and NearestGhost<5 then ToPowerDot+
NearestGhost>7 and MaxJunctionSafety>4 then FromGhostGhostDensity<1.5 and NearestPowerDot<5 then FromPowerDot+
NearestPowerDot>10 then FromPowerDotTotalDistToGhosts>30 then FromPowerDot+
MaxJunctionSafety<3 then FromGhost+
MaxJunctionSafety<2 then FromGhost+
MaxJunctionSafety<1 then FromGhost+
MaxJunctionSafety<0 then FromGhost+
Constant>0 then FromGhostCenter+
NearestGhost<4 then FromGhost+
NearestGhost>7 and MaxJunctionSafety>4 then FromGhostNearestEdGhost>99 then ToEdGhostNearestEdGhost<99 then ToEdGhost+
FromPowerDot- then ToPowerDot+
GhostDensity<1.5 and NearestPowerDot<5 then FromPowerDot+
NearestPowerDot>10 then FromPowerDot-

References
Allon, G., Kroese, D. P., Raviv, T., & Rubinstein, R. Y. (2005). Application of the crossentropy method to the buer allocation problem in a simulation-based environment.
Annals of Operations Research, 134, 137151.
Baum, E. B. (1996). Toward a model of mind as a laissez-faire economy of idiots. In
Proceedings of the 13rd International Conference on Machine Learning, pp. 2836.
Baxter, J., Tridgell, A., & Weaver, L. (2001). Machines that learn to play games, chap.
Reinforcement learning and chess, pp. 91116. Nova Science Publishers, Inc.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientic.
Bonet, J. S. D., & Stauer, C. P. (1999). Learning to play Pac-Man using incremental
reinforcement learning.. [Online; accessed 09 October 2006].
Bull, L. (2004). Applications of Learning Classier Systems, chap. Learning Classier Systems: A Brief Introduction, pp. 313. Springer.
Bull, L., & Kovacs, T. (2005). Foundations of Learning Classier Systems, chap. Foundations
of Learning Classier Systems: An Introduction, pp. 314. Springer.
Courtillat, P. (2001). NoN-SeNS Pacman 1.6 with C sourcecode.. [Online; accessed 09
October 2006].
Dambreville, F. (2006). Cross-entropic learning of a machine for the decision in a partially
observable universe. Journal of Global Optimization. To appear.
de Boer, P.-T., Kroese, D. P., Mannor, S., & Rubinstein, R. Y. (2004). A tutorial on the
cross-entropy method. Annals of Operations Research, 134, 1967.
682

Learning to play Ms. Pac-Man

Gallagher, M., & Ryan, A. (2003). Learning to play pac-man: An evolutionary, rule-based
approach. In et. al., R. S. (Ed.), Proc. Congress on Evolutionary Computation, pp.
24622469.
Holland, J. H. (1986). Escaping brittleness: The possibilities of general-purpose learning
algorithms applied to parallel rule-based systems. In Mitchell, Michalski, & Carbonell
(Eds.), Machine Learning, an Articial Intelligence Approach. Volume II, chap. 20,
pp. 593623. Morgan Kaufmann.
Keith, J., & Kroese, D. P. (2002). Sequence alignment by rare event simulation. In Proceedings of the 2002 Winter Simulation Conference, pp. 320327.
Koza, J. (1992). Genetic programming: on the programming of computers by means of
natural selection. MIT Press.
Lucas, S. M. (2005). Evolving a neural network location evaluator to play Ms. Pac-Man. In
IEEE Symposium on Computational Intelligence and Games, pp. 203210.
Mannor, S., Rubinstein, R. Y., & Gat, Y. (2003). The cross-entropy method for fast policy
search. In 20th International Conference on Machine Learning.
Margolin, L. (2004). On the convergence of the cross-entropy method. Annals of Operations
Research, 134, 201214.
Menache, I., Mannor, S., & Shimkin, N. (2005). Basis function adaptation in temporal
dierence reinforcement learning. Annals of Operations Research, 134 (1), 215238.
Muehlenbein, H. (1998). The equation for response to selection and its use for prediction.
Evolutionary Computation, 5, 303346.
Ponsen, M., & Spronck, P. (2004). Improving adaptive game AI with evolutionary learning.
In Computer Games: Articial Intelligence, Design and Education.
Rohanimanesh, K., & Mahadevan, S. (2001). Decision-theoretic planning with concurrent
temporally extended actions. In Proceedings of the 17th Conference on Uncerainty in
Articial Intelligence, pp. 472479.
Rubinstein, R. Y. (1999). The cross-entropy method for combinatorial and continuous
optimization. Methodology and Computing in Applied Probability, 1, 127190.
Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. IBM
Journal of Research and Development, 6, 211229.
Schaul, T. (2005). Evolving a compact concept-based Sokoban solver. Master's thesis, École
Polytechnique Fédérale de Lausanne.
Schmidhuber, J. (1997). A computer scientist's view of life, the universe, and everything.
In Freksa, C., Jantzen, M., & Valk, R. (Eds.), Foundations of Computer Science:
Potential - Theory - Cognition, Vol. 1337 of Lecture Notes in Computer Science, pp.
201208. Springer, Berlin.
683

Szita & L®rincz

Spronck, P., Ponsen, M., Sprinkhuizen-Kuyper, I., & Postma, E. (2006). Adaptive game ai
with dynamic scripting. Machine Learning, 63 (3), 217248.
Spronck, P., Sprinkhuizen-Kuyper, I., & Postma, E. (2003). Online adaptation of computer
game opponent AI. In Proceedings of the 15th Belgium-Netherlands Conference on
Articial Intelligence, pp. 291298.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press,
Cambridge.
Szabó, Z., Póczos, B., & L®rincz, A. (2006). Cross-entropy optimization for independent
process analysis. In ICA, pp. 909916.
Szita, I. (2006). How to select the 100 voxels that are best for prediction  a simplistic
approach. Tech. rep., Eötvös Loránd University, Hungary.
Szita, I., & L®rincz, A. (2006). Learning Tetris using the noisy cross-entropy method. Neural
Computation, 18 (12), 29362941.
Tesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6 (2), 215219.
Timuri, T., Spronck, P., & van den Herik, J. (2007). Automatic rule ordering for dynamic
scripting. In The Third Articial Intelligence and Interactive Digital Entertainment
Conference, pp. 4954.
Tiong, A. L. K. (2002). Rule set representation and tness functions for an articial pac man
playing agent. Bachelor's thesis, Department of Information Technology and Electrical
Engineering.
Ward, D. J., & MacKay, D. J. C. (2002). Fast hands-free writing by gaze direction. Nature,
418, 838540.
Wikipedia (2006). Pac-Man  Wikipedia, the free encyclopedia. Wikipedia. [Online;
accessed 20 May 2007].
Witten, I. A., Neal, R. M., & Cleary, J. G. (1987). Arithmetic coding for data compression.
Communications of the ACM, 30, 520540.
Wolpert, D. H., & Macready, W. G. (1997). No free lunch theorems for optimization. IEEE
Transactions on Evolutionary Computation, 1, 6782.

684

Journal of Artificial Intelligence Research 30 (2007) 181-212

Submitted 03/07; published 10/07

Knowledge Derived From Wikipedia
For Computing Semantic Relatedness
Simone Paolo Ponzetto
Michael Strube

PONZETTO @ EML - RESEARCH . DE
STRUBE @ EML - RESEARCH . DE

EML Research gGmbH, Natural Language Processing Group
Schloss-Wolfsbrunnenweg 33, 69118 Heidelberg, Germany
http://www.eml-research.de/nlp

Abstract
Wikipedia provides a semantic network for computing semantic relatedness in a more structured
fashion than a search engine and with more coverage than WordNet. We present experiments on
using Wikipedia for computing semantic relatedness and compare it to WordNet on various benchmarking datasets. Existing relatedness measures perform better using Wikipedia than a baseline
given by Google counts, and we show that Wikipedia outperforms WordNet on some datasets. We
also address the question whether and how Wikipedia can be integrated into NLP applications as
a knowledge base. Including Wikipedia improves the performance of a machine learning based
coreference resolution system, indicating that it represents a valuable resource for NLP applications. Finally, we show that our method can be easily used for languages other than English by
computing semantic relatedness for a German dataset.

1. Introduction
While most advances in Natural Language Processing (NLP) have been made recently by investigating data-driven methods, namely statistical techniques, we believe that further advances crucially
depend on the availability of world and domain knowledge. This is essential for high-level linguistic tasks which require language understanding capabilities such as question answering (e.g., Hovy,
Gerber, Hermjakob, Junk, & Lin, 2001) and recognizing textual entailment (Bos & Markert, 2005;
Tatu, Iles, Slavick, Novischi, & Moldovan, 2006, inter alia). However, there are not many domainindependent knowledge bases available which provide a large amount of information on named
entities (the leaves of the taxonomy) and contain continuously updated knowledge for processing
current information.
In this article we approach the problem from a novel1 perspective by making use of a wide
coverage online encyclopedia, namely Wikipedia. We use the “encyclopedia that anyone can edit”
to compute semantic relatedness by taking the system of categories in Wikipedia as a semantic
network. That way we overcome the well known knowledge acquisition bottleneck by deriving a
knowledge resource from a very large, collaboratively created encyclopedia. Then the question is
whether the quality of the resource is high enough to be used successfully in NLP applications.
By performing two different evaluations we provide an answer to that question. We do not only
show that Wikipedia derived semantic relatedness correlates well with human judgments, but also
that such information can be used to include lexical semantic information in a NLP application,
namely coreference resolution, where world knowledge has been considered important since early
1. This article builds upon and extends Ponzetto and Strube (2006a) and Strube and Ponzetto (2006).
c
2007
AI Access Foundation. All rights reserved.

P ONZETTO & S TRUBE

research (Charniak, 1973; Hobbs, 1978), but has been integrated only recently by means of WordNet
(Harabagiu, Bunescu, & Maiorano, 2001; Poesio, Ishikawa, Schulte im Walde, & Vieira, 2002).
We begin by introducing Wikipedia and measures of semantic relatedness in Section 2. In
Section 3 we show how semantic relatedness measures can be ported to Wikipedia. We then evaluate our approach using datasets designed for evaluating such measures in Section 4. Because all
available datasets are small and seem to be assembled rather arbitrarily we perform an additional
extrinsic evaluation by means of a coreference resolution system in Section 5. In Section 6 we show
that relatedness measures computed using Wikipedia can be easily ported to a language other than
English, i.e. German. We give details of our implementation in Section 7, present related work in
Section 8 and conclude with future work directions in Section 9.

2. Wikipedia and Semantic Relatedness Measures
In this section we describe the structure of Wikipedia and present the measures we use for computing
semantic relatedness within its categorization network.
2.1 Wikipedia
Wikipedia is a multilingual web based encyclopedia. Being a collaborative open source medium, it
is edited by volunteers. Wikipedia provides a very large domain-independent encyclopedic repository. The English version, as of 14 February 2006, contains 971,518 articles with 18.4 million
internal hyperlinks2 .
The text in Wikipedia is highly structured. Apart from article pages being formatted in terms of
sections and paragraphs, various relations exists between the pages themselves. These include:
Redirect pages: These pages are used to redirect the query to the actual article page containing
information about the entity denoted by the query. This is used to point alternative expressions
for an entity to the same article, and accordingly models synonymy. Examples include CAR
and SICKNESS3 redirecting to the AUTOMOBILE and DISEASE pages respectively, as well
as U.S.A., U.S., USA, US, ESTADOS UNIDOS and YANKEE LAND all redirecting to the
UNITED STATES page.
Disambiguation pages: These pages collect links for a number of possible entities the original
query could be pointed to. This models homonymy. For instance, the page BUSH contains
links to the pages SHRUB, BUSH LOUISIANA, GEORGE H.W. BUSH and GEORGE W.
BUSH.
Internal links: Articles mentioning other encyclopedic entries point to them through internal hyperlinks. This models article cross-reference. For instance, the page ‘PATAPHYSICS contains links to the term inventor, ALFRED JARRY, followers such as RAYMOND QUENEAU,
as well as distinctive elements of the philosophy such as NONSENSICAL and LANGUAGE.
Since May 2004 Wikipedia provides also a semantic network by means of its categories: articles can be assigned one or more categories, which are further categorized to provide a so-called
2. Wikipedia can be downloaded at http://download.wikimedia.org. In our experiments we use the English
and German Wikipedia database dump from 19 and 20 February 2006, except where otherwise stated.
3. In the following we use Sans Serif for words and queries, CAPITALS for Wikipedia pages and S MALL C APS for
concepts and Wikipedia categories.

182

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Categories

Fundamental

Information

Technology

Systems

Information systems

Top 10

Knowledge

Nature

Society

Thought

Science

Organizations

Natural sciences

Interdisciplinary fields

Abstraction

Computer science

Information science

Belief

Mathematics

Philosophy

Applied mathematics

Computational science

Life

Cybernetics

Artificial life

Branches of philosophy

Logic

Mathematical logic

Biology

Metaphysics

Neuroscience

Cognitive science

Linguistics

Computational linguistics

Artificial intelligence

Artificial intelligence applications

Cognition

Cognitive architecture

Ontology

Pataphysics

Speech recognition

Natural language processing

Figure 1: Wikipedia category network. The top nodes in the network (C ATEGORIES , F UNDAMEN TAL , T OP 10) are structurally identical to the more content bearing categories.

“category tree”. In practice, this “tree” is not designed as a strict hierarchy, but allows multiple categorization schemes to coexist simultaneously. The category system is considered a directed acyclic
graph, though the encyclopedia editing software does not prevent the users to create cycles in the
graph (which nevertheless should be avoided according to the Wikipedia categorization guidelines).
Due to this flexible nature, we refer to the Wikipedia “category tree” as the category network. As of
February 2006, 94% of the articles have been categorized into 103,759 categories. An illustration
of some of the higher regions of the hierarchy is given in Figure 1.
The strength of Wikipedia lies in its size, which could be used to overcome the limited coverage
and scalability issues of current knowledge bases. But the large size represents also a challenge: the
search space in the Wikipedia category graph is very large in terms of depth, branching factor and
multiple inheritance relations. Problems arise also in finding robust methods for retrieving relevant
183

P ONZETTO & S TRUBE

information. For instance, the large amount of disambiguation pages requires an efficient algorithm
for disambiguating queries, in order to be able to return the desired articles.
Since Wikipedia exists only since 2001 and has been considered a reliable source of information for an even shorter amount of time (Giles, 2005), researchers in NLP have only begun recently
to work with its content or use it as a resource. Wikipedia has been used successfully for applications such as question answering (Ahn, Jijkoun, Mishne, Müller, de Rijke, & Schlobach, 2004;
Ahn, Bos, Curran, Kor, Nissim, & Webber, 2005; Lo & Lam, 2006, inter alia), named entity disambiguation (Bunescu & Paşca, 2006), text categorization (Gabrilovich & Markovitch, 2006) and
computing document similarity (Gabrilovich & Markovitch, 2007).
2.2 Taxonomy Based Semantic Relatedness Measures
Approaches to measuring semantic relatedness that use lexical resources transform that resource
into a network or graph and compute relatedness using paths in it. An extensive overview of lexical
resource-based approaches to measuring semantic relatedness is presented in Budanitsky and Hirst
(2006).
2.2.1 T ERMINOLOGY
Semantic relatedness indicates how much two concepts are semantically distant in a network or
taxonomy by using all relations between them (i.e. hyponymic/hypernymic, antonymic, meronymic
and any kind of functional relations including is-made-of, is-an-attribute-of, etc.). When limited
to hyponymy/hyperonymy (i.e. isa) relations, the measure quantifies semantic similarity instead
(see Budanitsky & Hirst, 2006, for a discussion of semantic relatedness vs. semantic similarity).
In fact, two concepts can be related but are not necessarily similar (e.g. cars and gasoline, see
Resnik, 1999). While the distinction holds for a lexical database such as WordNet, where the
relations between concepts are semantically typed, it cannot be applied when computing metrics in
Wikipedia. This is because the category relations in Wikipedia are neither typed nor show a uniform
semantics. The Wikipedia categorization guidelines state that “categories are mainly used to browse
through similar articles”. Therefore users assign categories rather liberally without having to make
the underlying semantics of the relations explicit.
In the following, we use the more generic term of semantic relatedness, as it encompasses both
WordNet and Wikipedia measures. However, it should be noted that when applied to WordNet, the
measures below indicate semantic similarity, as they make use only of the subsumption hierarchy.
2.2.2 PATH BASED M EASURES
These measures compute relatedness as a function of the number of edges in the path between two
nodes c1 and c2 the words w1 and w2 are mapped to. Rada, Mili, Bicknell, and Blettner (1989)
traverse MeSH, a term hierarchy for indexing articles in Medline, and compute semantic distance
straightforwardly in terms of the number of edges between terms in the hierarchy. Accordingly,
semantic relatedness is defined as the inverse score of the semantic distance (pl henceforth).
Since the edge counting approach relies on a uniform modeling of the hierarchy, researchers
started to develop measures for computing semantic relatedness which abstract from this problem.
Leacock and Chodorow (1998) propose a normalized path-length measure which takes into account
the depth of the taxonomy in which the concepts are found (lch). Wu and Palmer (1994) present
184

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

instead a scaled measure which takes into account the depth of the nodes together with the depth of
their least common subsumer (wup).
2.2.3 I NFORMATION C ONTENT BASED M EASURES
The measure of Resnik (1995) computes the relatedness between the concepts as a function of their
information content, given by their probability of occurrence in a corpus (res). Relatedness is modeled as “the extent to which they [the concepts] share information”, and is given by the information
content of their least common subsumer. Similarly to the path-length based measures, more elaborate measure definitions based on information content have been later developed. This includes
the measures from Jiang and Conrath (1997) and Lin (1998), hereafter referred to respectively as
jcn and lin, which have been both shown to correlate better with human judgments than Resnik’s
measure.
2.2.4 T EXT OVERLAP BASED M EASURES
Lesk (1986) defines the relatedness between two words as a function of text (i.e. gloss) overlap.
The extended gloss overlap (lesk) measure of Banerjee and Pedersen (2003) computes the overlap
score by extending the glosses of the concepts under consideration to include the glosses of related
concepts in a hierarchy. Given two glosses g1 and g2 taken as definitions for the words w1 and w2 ,
P
the overlap score overlap(g1 , g2 ) is computed as n m2 for n phrasal m-word overlaps (Banerjee
& Pedersen, 2003). The overlap score is computed using a non-linear function, as the occurrences
of words in a text collection are known to approximate a Zipfian distribution.

3. Computing Semantic Relatedness with Wikipedia
Wikipedia based semantic relatedness computation is described in the following Subsections:
1. Retrieve two unambiguous Wikipedia pages which a pair of words, w1 , w2 (e.g. king and
rook) refer to, namely pages = {p1 , p2 } (Section 3.1).
2. Connect to the category network by parsing the pages and extracting the two sets of categories
C1 = {c1 | c1 is category of p1 } and C2 = {c2 | c2 is category of p2 } the pages are assigned
to (Section 3.2).
3. Compute the set of paths between all pairs of categories of the two pages, namely paths =
{pathc1 ,c2 | c1 ∈ C1 , c2 ∈ C2 } (Section 3.2).
4. Compute semantic relatedness based on the two pages extracted (for text overlap based measures) and the paths found along the category network (for path length and information content based measures) (Section 3.3).
3.1 Page Retrieval and Disambiguation
Given a pair of words, w1 and w2 , page retrieval for page p is accomplished by
1. querying the page titled as the word w,
2. following all redirects (e.g. CAR redirecting to AUTOMOBILE),
185

P ONZETTO & S TRUBE

3. resolving ambiguous page queries. This is due to many queries in Wikipedia returning a
disambiguation page. For instance, querying king returns the Wikipedia disambiguation page
KING, which points to other pages including MONARCH, KING (CHESS), KING KONG,
KING-FM (a broadcasting station), B.B. KING (the blues guitarist) and MARTIN LUTHER
KING.
We choose an approach to disambiguation which maximizes relatedness, namely we let the page
queries disambiguate each other (see Figure 2). If a disambiguation page p1 for querying word w1
is hit, we first get all the hyperlinks in page p2 obtained by querying the other word w2 without
disambiguating. This is to bootstrap the disambiguation process, since it could be the case that both
queries are ambiguous, e.g. king and rook. We then take the other word w2 and all the Wikipedia
internal links of page p2 as a lexical association list L2 = {w2 } ∪ {l2 | l2 is a link in p2 } to be used
for disambiguation – i.e., we use the term list {rook, rook (chess), rook (bird), rook (rocket),
. . . } for disambiguating the page KING. Links such as rook (chess) are split to extract the label
between parentheses – i.e., rook (chess) splits into rook and chess. If a link in p1 contains any
occurrence of a disambiguating term l2 ∈ L2 (i.e. the link to KING (CHESS) in the KING page
containing the term chess extracted from the ROOK page), the linked page is returned (KING
(CHESS)), else we return the first article linked in the disambiguation page (MONARCH).
This disambiguation strategy provides a less accurate solution than following all disambiguation
page links. Nevertheless it realizes a more practical solution as many of those pages contain a large
number of links (e.g. 34 and 13 for the KING and ROOK pages respectively).
3.2 Category Network Search
Given the pages p1 and p2 , we extract the lists of categories C1 and C2 they belong to (i.e. both
KING (CHESS) and ROOK (CHESS) belong to the C HESS PIECES category). Given the category
sets C1 and C2 , for each category pair hc1 , c2 i, c1 ∈ C1 , c2 ∈ C2 we look for all paths connecting
the two categories c1 and c2 . We perform a depth-limited search of maximum depth of 4 for a
least common subsumer. We additionally limit the search to any category of a level greater than
2, i.e. we do not consider the levels between 0 and 2 (where level 0 is represented by the top
node C ATEGORIES of Figure 1). We noticed that limiting the search improves the results. This is
probably due to the upper regions of the Wikipedia category network being too strongly connected
(see Figure 1). Accordingly, the value of the search depth was established during system prototyping
by finding the depth search value which maximizes the correlation between the relatedness scores of
the best performing Wikipedia measure and the human judgments given in the datasets from Miller
and Charles (1991) and Rubenstein and Goodenough (1965).
3.3 Relatedness Measure Computation
Finally, given the set of paths found between all category pairs, we compute the network based
measures by selecting the paths satisfying the measure definitions, namely the shortest path for
path-based measures and the path with the most informative least common subsumer for information
content based measures.
In order to apply Resnik’s measure to Wikipedia we couple it with an intrinsic information content measure relying on the hierarchical structure of the category network (Seco, Veale, & Hayes,
2004), rather than computing the information content from the probabilities of occurrence of the
186

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

function GET-PAGES(w1 , w2 ) returns pages
1: pages ← {∅}
2: pages ← pages ∪ GET-UNAMBIGUOUS-PAGE(w1 , w2 )
3: pages ← pages ∪ GET-UNAMBIGUOUS-PAGE(w2 , w1 )
4: return pages
function GET-UNAMBIGUOUS-PAGE(w1 , w2 ) returns page
1: page ← getArticleT itled(w1 )
2: while page is a redirection page do
3:
page ← f ollowRedirect(page)
4: end while
5: while page is a disambiguation page do
6:
l0 ← first link in page
otherP age ← getArticleT itled(w2 ),
L1 = {l1 | l1 is a link in page}
L2 = {w2 } ∪ {l2 | l2 is a link in otherP age}
7:
for each li ∈ L1
8:
for each lj ∈ L2
9:
if MATCHES?(li ,lj ) then
10:
page ← getArticleT itled(li ), goto (5)
11:
end if
12:
end for
13:
end for
14:
page ← getArticleT itled(l0 )
15: end while
16: return page
function MATCHES?(l1 , l2 ) returns true or false
1: T1 ← SPLIT-BY-PARENTHESIS(l1 )
T2 ← SPLIT-BY-PARENTHESIS(l2 )
2: for each ti ∈ T1
3:
for each tj ∈ T2
4:
if ORTHOGRAPHICALLY-MATCHES(ti , tj ) then
5:
return true
6:
end if
7:
end for
8: end for
9: return false
Figure 2: Algorithm for Wikipedia page retrieval and disambiguation

187

P ONZETTO & S TRUBE

concepts in a corpus. Seco et al. (2004) show that this method correlates better with human judgments than the original approach from Resnik (1995). The intrinsic information content of a category node n in the hierarchy is given as a function of its child nodes, namely
ic(n) = 1 −

log(hypo(n) + 1)
log(C)

(1)

where hypo(n) is the number of hyponyms of node n and C equals the total number of conceptual
nodes in the hierarchy.
Gloss overlap measures are computed from article pages, since no relevant text is given in
the category pages. In order to adapt the Lesk measure to Wikipedia (Equation 2), gloss overlap
measures (gloss) are computed from the first paragraph of the pages. The relatedness score is given
by applying a double normalization step to the overlap score. We first normalize by the sum of text
lengths and then take the output as the value of the hyperbolic tangent function in order to minimize
the role of outliers skewing the score distribution.
overlap(t1 , t2 )
lesk wikipedia(t1 , t2 ) = tanh
length(t1 ) + length(t2 )




(2)

4. Experiments
This section describes an evaluation of our methodology based on experiments with word pair lists.
We compare the performance of WordNet and Wikipedia based relatedness measures on datasets
which have been extensively used in the literature as standard benchmark tests. In addition, we
evaluate the performance of the relatedness measures derived from Wikipedia using different versions of the online encyclopedia between February and May 2007.
4.1 Experiments for English
We evaluate the relatedness measures on four standard datasets, namely Miller and Charles’ (1991)
list of 30 noun pairs (hereafter referred to as M&C), Rubenstein and Goodenough’s (1965) 65 word
pair synonymity list (R&G) of which M&C is a subset, the WordSimilarity-353 Test Collection
(353-TC) from Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, and Ruppin (2002)4 , and
finally the 2,682 pairs from the nominal only subset (KLEB) of the reader based lexical cohesion
dataset from Beigman Klebanov and Shamir (2006). As the 353-TC dataset is partitioned into
training and testing subsets, we experiment both with the full list (353 word pairs) and its test
data subset (153 pairs). Similarly, as the KLEB dataset contains a relatively large amount of noun
pairs, it was split into two 50-50% partitions for performing machine learning based experiments
on learning the relatedness of words.
4.1.1 E VALUATION
Following the literature on semantic relatedness, we evaluate performance by taking the Pearson
product-moment correlation coefficient r between the relatedness measure scores and the corresponding human judgments. For each dataset we report the correlation computed on all pairs (all).
In the case of word pairs where at least one of the words could not be found in the lexical resource
4. Available at http://www.cs.technion.ac.il/∼gabr/resources/data/wordsim353/wordsim353.html.

188

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Dataset
M&C
R&G
353-TC
full
353-TC
test
KLEB
full
KLEB
test

all
non-miss
all
non-miss
all
non-miss
all
non-miss
all
non-miss
all
non-miss

Google
jaccard
0.33
0.33
0.22
0.22
0.08
0.08
0.10
0.10
0.02
0.02
0.03
0.03

WordNet 2.1
wup
lch
res
0.77
0.82 0.78
0.77
0.82 0.78
0.82
0.86 0.81
0.82
0.86 0.81
0.30
0.34 0.34
0.32
0.36 0.35
0.28
0.35 0.38
0.30
0.38 0.39
0.15
0.15 0.18
0.15
0.15 0.18
0.15
0.14 0.18
0.15
0.14 0.18

pl
0.72
0.72
0.78
0.78
0.28
0.27
0.29
0.28
0.07
0.10
0.06
0.09

lesk
0.37
0.37
0.35
0.35
0.21
0.21
0.21
0.21
0.14
0.14
0.16
0.16

pl
0.47
0.50
0.51
0.53
0.45
0.45
0.50
0.50
0.29
0.30
0.31
0.31

Wikipedia (February 2006)
wup
lch
res
gloss
0.43
0.45
0.23
0.46
0.49
0.49
0.29
0.47
0.50
0.52
0.30
0.46
0.54
0.55
0.34
0.47
0.48
0.49
0.38
0.21
0.48
0.49
0.39
0.21
0.55
0.57
0.46
0.23
0.55
0.57
0.46
0.23
0.28
0.30
0.18
0.13
0.29
0.31
0.19
0.13
0.30
0.32
0.19
0.15
0.31
0.32
0.18
0.15

SVM

0.62

0.38

Table 1: Results on correlation with human judgments of relatedness measures
(i.e. WordNet or Wikipedia) the relatedness score is set to 0. In addition, we report the correlation
score obtained by disregarding such pairs containing missing words (non-miss). As a baseline, we
compute for each word pair w1 and w2 the Google correlation coefficient by taking the Jaccard
similarity coefficient (Salton & McGill, 1983) on page hits.
jaccard =

Hits(w1 AND w2 )
Hits(w1 ) + Hits(w2 ) − Hits(w1 AND w2 )

This co-occurrence distributional similarity measure serves as baseline. We choose the Jaccard
similarity coefficient because it is a combinatorial measure which does not take into account the
actual word distributions (Lee, 1999) — which we do not have here, as we take only Google hits.
This models also the usage of other similarity coefficients, e.g. van Rijsbergen (1979) shows that
Dice and Jaccard’s coefficients are monotonic in each other.
4.1.2 E XPERIMENTAL SETTING
Experiments were performed for each measure on all datasets. For the 353-TC and the KLEB, we
experiment on integrating different measures by performing regression using a Support Vector Machine (Vapnik, 1995) to estimate the functional dependence of the human relatedness judgments on
multiple relatedness scores. The learner was trained and tested using all available Google, WordNet
and Wikipedia scores. We used an RBF kernel with degree 3. Feature selection was performed
to find the optimal feature space using a genetic algorithm (Mierswa, Wurst, Klinkenberg, Scholz,
& Euler, 2006) through cross-validation on the training data. In addition, we performed model
selection for optimal parameter estimation as a grid search (Hsu, Chang, & Lin, 2006).
4.1.3 D ISCUSSION
Table 1 shows the correlation coefficients of the different measures with human judgments. Best
performance per dataset is highlighted in bold5 . Both WordNet and Wikipedia perform better than
5. Differences in performance are statistically significant at 95% significance level (p = 0.05). For computing statistical significance we performed a paired t-test on each dataset for pairs of corresponding relatedness measures (e.g.
between the WordNet and Wikipedia path measures). Additionally, we performed the test between each WordNet
and Wikipedia measure and the Google baseline, and between the SVM combined measure and the best performing
measure on the 353-TC and KLEB test datasets. The only statistically non-significant differences in performance
were found between the lesk and the Wikipedia gloss measure on the M&C dataset.

189

P ONZETTO & S TRUBE

the Google baseline. While WordNet performs extremely well on the M&C and R&G datasets,
its performance drastically decreases when applied to the 353-TC and KLEB datasets. Wikipedia
however does not perform as well on the M&C and R&G datasets but outperforms WordNet on
353-TC and KLEB. In the case of the KLEB full dataset, we report a performance competitive with
a state-of-the-art WordNet based measure (information content induced from taxonomy and gloss
information, Beigman Klebanov, 2006). This is not due to coverage, because in the 353-TC dataset
there are only 2 pairs containing at least one word not present in WordNet, where these amount to
13 for Wikipedia. In the KLEB dataset 114 pairs are missing in WordNet while 150 are missing in
Wikipedia. The problems seem to be caused rather by sense proliferation in WordNet. The measures
are in fact computed by looking at all possible sense pairs for the given words (as no word senses are
given), and taking the best scoring (e.g. shortest, most informative) path. This allows for unplausible
paths to be returned. It should be noted however that this is not caused by WordNet itself, as it has
to provide sense coverage, but rather by the relatedness measures. In fact, no sense disambiguation
apart from the one performed by the measures themselves is possible. Using Wikipedia pages as
entry points, we have access to the page texts and hyperlinks, which can be used to disambiguate
and subsequently limit and focus the search. As an example, using fertility to disambiguate egg, we
correctly return the Wikipedia page OVUM, whereas the shortest path in WordNet makes use of the
second sense for egg, namely ‘oval reproductive body of a fowl (especially a hen) used as food’.
In addition to the problem of sense proliferation, WordNet seems to suffer in principle of a
link proliferation problem, e.g., the shortest path between egg and fertility traverses the hierarchy
through one of the root nodes (i.e. E NTITY). One could suggest to limit the search in WordNet as
we did in Wikipedia, though it should be noted that this is supposed to be taken care by the measures
themselves, e.g. by scaling by the depth of the path nodes.
Besides, Wikipedia performs better than WordNet in the present experimental setting because
the 353-TC and the KLEB datasets model semantic relatedness, rather than similarity. The WordNet measures we used (all except for lesk) are instead designed to quantify similarity, thus yielding
a poor performance. This is supported by the 353-TC annotation guidelines, which define similarity as ‘belonging to the same domain or representing features of the same concept’, as well as
by Beigman Klebanov (2006) reporting competitive results (Pearson correlation coefficient r =
0.47) with her WordNet-based relatedness measure. 353-TC contains also highly rated word pairs
such as cell and phone or energy and crisis which are closely related, since they tend to occur frequently together, but not similar, as they share few or no properties at all. Finally, additional support
is given by the fact that the most competitive results given by Wikipedia are on the KLEB dataset,
which is specifically designed with relatedness in mind (Beigman Klebanov & Shamir, 2006).
Finkelstein et al. (2002) suggest that integrating a word-vector based relatedness measure with
a WordNet based one is useful, as it accounts for word co-occurrences and helps recovering from
cases in which the words cannot be found in the available resources, e.g. dictionary or ontology.
Accordingly, on the 353-TC and KLEB test sets we report the best performance by integrating
all available measures and performing feature selection. On the 353-TC data, the score of r =
0.62 outperforms the combined WordNet–word-vector measure of Finkelstein et al. (2002) (r =
0.55), as well as the score of r = 0.38 on the KLEB test data outperforming the score of r =
0.32 obtained by using the best performing (Wikipedia-based) relatedness measure (lch). Instead of
integrating a word-vector based relatedness measure with a WordNet based one, our results indicate
that a competitive performance can be achieved also by using a different knowledge base such as
Wikipedia.
190

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

From the analysis above, one could conclude that Wikipedia yields better relatedness scores than
WordNet when applied to datasets designed to evaluate the relatedness of words. In practice, we
believe that it is extremely difficult to perform a fair comparison of the two knowledge sources when
limiting the application to such small datasets. In addition, it is not always clear which linguistic
notion (i.e. similarity vs. relatedness) underlies the datasets (cf. 353-TC). This is the reason why we
do not perform additional experiments making use of other datasets from synonymity tests such as
the 80 TOEFL (Landauer & Dumais, 1997), 50 ESL (Turney, 2001) or 300 Reader’s Digest Word
Power Game (Jarmasz & Szpakowicz, 2003) questions. These datasets pose also a problem since
they contain verbs, which are unlikely to be found in an encyclopedic resource such as Wikipedia.
These are all reasons why we evaluate in Section 5 our approach by applying it to a real-world NLP
task, namely coreference resolution, where the relatedness between hundreds of thousands of word
pairs has to be computed, thus providing a more reliable evaluation.
4.2 Evaluation of Wikipedia Throughout Time
One of the most appealing features of Wikipedia is not only that it provides a large coverage knowledge base, but also that it shows a quasi-exponential growth with respect to the number of articles
(Table 2)6 . We evaluated whether such growth rate affects our methodology for computing the
relatedness of words. The experiments with the word pairs datasets, using the Wikipedia English
database dump from 19 February 2006, were repeated using the dumps from 25 September 2006
and 27 May 2007. The performance of our Wikipedia-based relatedness measures on the M&C,
R&G, 353-TC and KLEB datasets are presented in Tables 3 and 4. As highlighted in Figure 3, the
only notable differences in performance between different Wikipedia versions are on the M&C and
R&G dataset. Nevertheless a simple one-tailed paired sample t-test at the 0.05 level reveals that
none of the variations between different Wikipedia versions are statistically significant. Qualitative
analysis reveals that the improvements are due to few queries getting correctly disambiguated – i.e.
lad correctly disambiguates to BOY in the September 2006 and May 2007 Wikipedia, rather than
SECT, as in the February version where the LAD disambiguation page contains SECT as the first
link used for disambiguation occurring in “Lad is a sub-sect of the Jainist Digambara sect”. These
differences are accidental and can be easily spotted by a significance test.
WordNet 2.1
#word-sense pairs/#articles
#synsets/#categories

207,016
117,597

English Wikipedia
Feb. 06
Sep. 06
May 07
971,518 1,403,207 1,803,843
103,759
165,744
244,376

Table 2: Statistics on WordNet and Wikipedia

These results show that our method is robust (thus replicable) disregarding the Wikipedia version. However, we did not observe any improvement despite the quasi-exponential growth of
Wikipedia, because the articles added to Wikipedia did not provide crucial information with respect to our experiments.
6. See for instance http://en.wikipedia.org/wiki/Wikipedia:Modelling Wikipedia’s growth.

191

P ONZETTO & S TRUBE

Dataset
M&C
R&G
353-TC
full
353-TC
test
KLEB
full
KLEB
test

all
non-miss
all
non-miss
all
non-miss
all
non-miss
all
non-miss
all
non-miss

pl
0.59
0.63
0.57
0.60
0.47
0.48
0.52
0.52
0.26
0.28
0.27
0.29

wup
0.54
0.61
0.59
0.65
0.49
0.50
0.57
0.57
0.26
0.28
0.26
0.29

Wikipedia
lch
res
0.58 0.31
0.64 0.41
0.60 0.30
0.66 0.37
0.51 0.35
0.53 0.36
0.60 0.49
0.61 0.49
0.27 0.13
0.29 0.15
0.28 0.14
0.31 0.17

gloss
0.58
0.60
0.52
0.54
0.25
0.26
0.26
0.26
0.11
0.11
0.12
0.13

SVM

0.62

0.37

Table 3: Correlation of Wikipedia scores (September 2006) with human judgments

Dataset
M&C
R&G
353-TC
full
353-TC
test
KLEB
full
KLEB
test

all
non-miss
all
non-miss
all
non-miss
all
non-miss
all
non-miss
all
non-miss

pl
0.57
0.57
0.58
0.58
0.48
0.48
0.54
0.54
0.31
0.32
0.28
0.29

wup
0.54
0.54
0.58
0.58
0.52
0.52
0.63
0.63
0.31
0.33
0.29
0.30

Wikipedia
lch
res
0.57 0.31
0.57 0.31
0.61 0.38
0.61 0.38
0.53 0.41
0.54 0.41
0.64 0.60
0.64 0.60
0.33 0.19
0.34 0.20
0.30 0.18
0.31 0.18

gloss
0.55
0.55
0.53
0.53
0.20
0.20
0.22
0.22
0.10
0.10
0.11
0.11

SVM

0.66

0.37

Table 4: Correlation of Wikipedia scores (May 2007) with human judgments

5. Case Study: Coreference Resolution
We extend a machine learning based coreference resolver with features capturing different semantic knowledge sources. These features represent relatedness scores mined from WordNet and
Wikipedia. Coreference resolution provides an application to evaluate the performance of the relatedness measures we previously evaluated using only datasets of limited size. This extrinsic evaluation provides a better insight on the usefulness of Wikipedia relatedness measures for NLP applications than the intrinsic evaluation described in Section 4.
5.1 Machine Learning Based Coreference Resolution and Semantic Knowledge
The last years have seen a boost of work devoted to the development of machine learning based
coreference resolution systems (Soon, Ng, & Lim, 2001; Ng & Cardie, 2002; Yang, Zhou, Su,
& Tan, 2003; Luo, Ittycheriah, Jing, Kambhatla, & Roukos, 2004, inter alia). While machine
learning has proved to yield performance rates fully competitive with rule based systems, current
coreference resolution systems are mostly relying on rather shallow features, such as the distance
between the coreferent expressions, string matching, and linguistic form. These shallow features
are not sufficient for correctly identifying many of the coreferential relations between expressions
192

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Correlation coefficient (Pearson r)

0.7

0.8

M&C
R&G
353-TC full
353-TC test
353-TC SVM
KLEB full
KLEB test
KLEB SVM

0.7

Correlation coefficient (Pearson r)

0.8

0.6

0.5

0.4

0.3

0.2
Jan 06

M&C
R&G
353-TC full
353-TC test
353-TC SVM
KLEB full
KLEB test
KLEB SVM

0.6

0.5

0.4

0.3

Mar 06

May 06

Jul 06

Sep 06

Nov 06

Jan 07

Mar 07

May 07

Jul 07

0.2
Jan 06

Mar 06

May 06

Jul 06

Wikipedia database dump

Sep 06

Nov 06

Jan 07

Mar 07

May 07

Jul 07

Wikipedia database dump

(a) All pairs

(b) Non-missing pairs

Figure 3: Performance variation of Wikipedia-based relatedness measures throughout time

in a text. As an example, consider a fragment from the Automatic Content Extraction (ACE) 2003
data.
But frequent visitors say that given the sheer weight of the country’s totalitarian ideology and
generations of mass indoctrination, changing this country’s course will be something akin to
turning a huge ship at sea. Opening North Korea up, even modestly, and exposing people to the
idea that Westerners – and South Koreans – are not devils, alone represents an extraordinary
change. [...] as his people begin to get a clearer idea of the deprivation they have suffered,
especially relative to their neighbors. “This is a society that has been focused most of all on
stability, [...]”.

In order to correctly resolve the coreferent expressions highlighted in bold (which are all annotated
as coreferent in the ACE data), lexical semantic and encyclopedic knowledge is required, i.e., that
North Korea is a country, that countries consist of people and are societies. The resolution
requires a knowledge base (e.g. generated from Wikipedia) look-up and reasoning on the content
relatedness holding between the different expressions (e.g. as a path measure along the links of the
WordNet and Wikipedia semantic networks). In the following we explore the scenario of including
knowledge mined from WordNet and Wikipedia for coreference resolution. We start with a machine
learning based baseline system taken from Ponzetto and Strube (2006b), which includes the set of
shallow linguistic features from Soon et al. (2001) as well as semantic parsing information in terms
of semantic role labeling (Gildea & Jurafsky, 2002; Carreras & Màrquez, 2005, SRL henceforth),
and analyze the performance variations given by including the previously discussed relatedness
measures in the feature set. An overview of the system we present in the remainder of the section is
given in Figure 4.
5.2 Coreference Resolution Using Semantic Knowledge Sources
This subsection presents our coreference resolution system which uses semantic relatedness features
induced from WordNet and Wikipedia to capture information from these knowledge sources.
193

Preprocessing
pipeline

Raw
text

Baseline feature extractor
(Ponzetto and Strube, 2006b)

PoS tagger
Semantic feature
Chunker

NER

extractor

MaxEnt
classifier

Text annotated
with coreference chains

P ONZETTO & S TRUBE

WordNet
Wikipedia

SEMANTICS
Figure 4: Overview of the coreference resolution system for extrinsic evaluation of WordNet and
Wikipedia relatedness measures. We start with a baseline system from Ponzetto and
Strube (2006b) which includes the features from Soon et al. (2001) and semantic role
information. We then include at different times features from WordNet and Wikipedia
and register performance variations.

5.2.1 C ORPORA U SED
To establish a competitive coreference resolver, the system was initially prototyped using the MUC6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system
with the ACE 2003 Training Data corpus (Mitchell, Strassel, Przybocki, Davis, Doddington, Grishman, Meyers, Brunstain, Ferro, & Sundheim, 2003)7 . Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where split into 60-20-20% document-based partitions for training,
development, and testing, and later per-partition merged (MERGED) for system evaluation. The
distribution of coreference chains and referring expressions is given in Table 5.
5.2.2 L EARNING A LGORITHM
For learning coreference decisions, we used a Maximum Entropy (Berger, Della Pietra, & Della Pietra, 1996) model, implemented using the MALLET library8 . Coreference resolution is viewed as a
binary classification task: given a pair of REs, the classifier has to decide whether they are coreferent or not. The MaxEnt model produces a probability for each category y (coreferent or not)
of a candidate pair, conditioned on the context x in which the candidate occurs. The conditional
probability is calculated by:
7. We used the training data corpus only, as the availability of the test data is restricted to ACE participants. Therefore,
the results we report cannot be compared directly with those using the official test data.
8. http://mallet.cs.umass.edu

194

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

TRAIN.
DEVEL
TEST
TOTAL
TOTAL (%)

TRAIN.
DEVEL
TEST
TOTAL
TOTAL (%)

#coref chains
587
201
228
1,016

BNEWS (147 docs – 33,479 tokens)
#pronouns
#common nouns #proper names
876 (36.1%)
572 (23.6%)
980 (40.3%)
315 (33.4%)
163 (17.3%)
465 (49.3%)
291 (30.7%)
238 (25.1%)
420 (44.2%)
1,482
973
1,865
34.3%
22.5%
43.2%

#coref chains
904
399
354
1,657

NWIRE (105 docs – 57,205 tokens)
#pronouns
#common nouns
1037 (24.3%)
1210 (28.3%)
358 (20.3%)
485 (27.5%)
329 (21.6%)
484 (31.7%)
1,724
2,179
22.8%
28.8%

#proper names
2023 (47.4%)
923 (52.2%)
712 ( 46.7%)
3,658
48.4%

Table 5: Partitions of the ACE 2003 training data corpus

"

#

1 X
λi fi (x, y)
p(y|x) =
Zx i

where fi (x, y) is the value of feature i on outcome y in context x, and λi is the weight associated
with i in the model. Zx is a normalization constant. The features used in our model are all binaryvalued feature functions (or indicator functions), e.g.

fWIKI PL (WIKI PL = 0.1, COREF) =


1 if candidate pair is coreferent and





semantic relatedness of their lexical


heads is 0.1 using the pl measure








0 otherwise

We use the L-BFGS algorithm (Malouf, 2002) to estimate the parameters of the Maximum Entropy
model. To prevent the model from overfitting, we employ a tunable Gaussian prior as a smoothing
method. Training is performed using multi-conditional learning (McCallum, Pal, Druck, & Wang,
2006), a state-of-the-art hybrid method combining generative and discriminative methods for model
parameter estimation.
We apply a set of preprocessing components including a POS tagger (Giménez & Màrquez,
2004), NP chunker (Kudoh & Matsumoto, 2000) and the Alias-I LingPipe Named Entity Recognizer9 to the text in order to identify the noun phrases, which are further taken as referring expressions (REs) to be used for instance generation. Therefore, we use automatically extracted noun
phrases, rather than assuming perfect NP chunking. This is in contrast to other related works in
coreference resolution (e.g., Luo et al., 2004; Kehler, Appelt, Taylor, & Simma, 2004).
Instances are created following Soon et al. (2001). We create a positive training instance from
each pair of adjacent coreferent REs. Negative instances are obtained by pairing the anaphoric
9. http://alias-i.com/lingpipe

195

P ONZETTO & S TRUBE

REs with any RE occurring between the anaphor and the antecedent. During testing each text is
processed from left to right: each RE is paired with any preceding RE from right to left, until a pair
labeled as coreferent is output, or the beginning of the document is reached. The classifier imposes
a partitioning on the available REs by clustering each set of expressions labeled as coreferent into
the same coreference chain.
5.2.3 BASELINE S YSTEM F EATURES
Following Ng and Cardie (2002), our core baseline system reimplements the Soon et al. (2001)
system. The system uses twelve features. Given a potential antecedent REi and a potential anaphor
REj the features are computed as follows10 .
(a) Lexical features
1. STRING MATCH T if REi and REj have the same spelling, else F.
2. ALIAS T if one RE is an alias of the other; else F.
(b) Grammatical features
3. I PRONOUN T if REi is a pronoun; else F.
4. J PRONOUN T if REj is a pronoun; else F.
5. J DEF T if REj starts with the; else F.
6. J DEM T if REj starts with this, that, these, or those; else F.
7. NUMBER T if both REi and REj agree in number; else F.
8. GENDER U if either REi or REj have an undefined gender. Else if they are both defined
and agree T; else F.
9. PROPER NAME T if both REi and REj are proper names; else F.
10. APPOSITIVE T if REj is in apposition with REi ; else F.
(c) Semantic features
11. WN CLASS U if either REi or REj have an undefined WordNet semantic class. Else if
they both have a defined one and it is the same T; else F.
(d) Distance features
12. DISTANCE how many sentences REi and REj are apart.
In addition to the 12 features from Soon et al. (2001), we employ SRL features taken from Ponzetto and Strube (2006b). Semantic roles are taken from the output of the ASSERT parser (Pradhan,
Ward, Hacioglu, Martin, & Jurafsky, 2004), an SVM based semantic role tagger which uses a full
syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, and output them as PropBank arguments (Palmer, Gildea, & Kingsbury, 2005). It
10. Possible values are U(nknown), T(rue) and F(alse). Note that in contrast to Ng and Cardie (2002) we interpret ALIAS
as a lexical feature, as it solely relies on string comparison and acronym string matching.

196

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only when the two
phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic
roles filled by a constituent, where each semantic argument label is always defined with respect to
a predicate. Given such level of semantic information available at the RE level, we introduce two
new features.
(e) SRL features
13. I SEMROLE the semantic role argument-predicate pairs of REi .
14. J SEMROLE the semantic role argument-predicate pairs of REj .
For the ACE 2003 data, 11,406 of 32,502 automatically extracted noun phrases were tagged with
2,801 different argument-predicate pairs. Our baseline feature set is obtained by starting with all
the features from Soon et al. (2001), plus the SRL features, and removing those selected using a
backward feature selection (see Subsection 5.3.2).
5.2.4 W ORD N ET F EATURES
The WN CLASS feature from the baseline system is very noisy, because of the lack of coverage,
sense proliferation and ambiguity11 . We accordingly enrich the semantic information available to
the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen,
Patwardhan, & Michelizzi, 2004). The measures we use include path length based measures (Rada
et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information
content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998).
In our case, the measures are obtained by computing the similarity scores between the head
lemmata (for common nouns, e.g. house) or full NPs (for named entities, e.g. George W. Bush)
of each potential antecedent-anaphor pair. In order to deal with the sense disambiguation problem,
we factorize over all possible sense pairs: given a candidate pair, we take the cross product of each
antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARITY, we
compute the similarity score for all synset pairs, and create the following features.
15. WN SIMILARITY BEST the highest similarity score from all hSENSEREi ,n , SENSEREj ,m i
synset pairs.
16. WN SIMILARITY AVG the average similarity score from all hSENSEREi ,n , SENSEREj ,m i
synset pairs.
Pairs containing REs which cannot be mapped to WordNet synsets are assumed to be maximally
dissimilar, i.e. their similarity score is set to 0.
5.2.5 W IKIPEDIA F EATURES
We include features derived from Wikipedia. Pages and paths are retrieved following the procedure
described in Section 3. As in the case of WordNet, we query the head lemmata of common nouns
or the full NPs for named entities. Given a candidate coreference pair REi/j and the disambiguated
11. Following Soon et al. (2001) we mapped each RE to the first WordNet sense of the head noun.

197

P ONZETTO & S TRUBE

Wikipedia pages pREi/j they point to, obtained by querying pages titled as tREi/j , we extract the
following features:
17. I/J GLOSS CONTAINS U if no Wikipedia page titled tREi/j is available. Else T if the first
paragraph of text of pREi/j contains tREj/i ; else F.
18. I/J RELATED CONTAINS U if no Wikipedia page titled as tREi/j is available. Else T if at
least one Wikipedia hyperlink of pREi/j contains tREj/i ; else F.
19. I/J CATEGORIES CONTAINS U if no Wikipedia page titled as tREi/j is available. Else T
if the list of categories pREi/j belongs to contains tREj/i ; else F.
20. GLOSS OVERLAP the overlap score between the first paragraph of text of pREi and pREj
computed using Equation 2.
Additionally, we use the Wikipedia category graph. We use the relatedness measures described
in Subsections 2.2.2 and 2.2.3. Given pREi/j and the lists of categories CREi/j they belong to, we
factorize over all possible category pairs. That is, we take the cross product of each antecedent and
anaphor category to form pairs of ‘Wikipedia synsets’. For each measure WIKI RELATEDNESS,
we compute the relatedness score for all category pairs, and create the following features.
21. WIKI RELATEDNESS BEST the highest relatedness score from all hCREi ,n , CREj ,m i category pairs.
22. WIKI RELATEDNESS AVG the average relatedness score from all hCREi ,n , CREj ,m i category pairs.
5.3 Experiments
5.3.1 P ERFORMANCE M ETRICS
We report in the following tables the MUC score (Vilain, Burger, Aberdeen, Connolly, & Hirschman,
1995). Scores in Table 6 are computed for all noun phrases appearing in either the key or the system
response, whereas Tables 7 and 8 refer to scoring only those phrases which appear in both the key
and the response. We therefore discard those responses not present in the key, as the only referring
expressions annotated in the ACE 2003 data belong to the categories Person (i.e. humans), Organization (e.g. corporations, agencies), Facility (i.e. buildings), Location (e.g. geographical areas and
landmasses) and Geo-political entities (i.e. nations, regions, their government and people, cf. the
‘North Korea’ example above)12 . This makes it impossible to perform ‘full’ coreference resolution
including, e.g. the identification of referential expressions, and implies that the results establish the
upper limit of the improvements given by our semantic features.
We also report the accuracy score for all three types of ACE mentions, namely pronouns, common nouns and proper names. Accuracy is the percentage of REs of a given mention type correctly
resolved divided by the total number of REs of the same type having a direct antecedent given in the
key. A RE is said to be correctly resolved when both it and its direct antecedent identify mentions
which belong to the same coreference class in the key.
12. Cf. the ACE 2003 Entity Detection and Tracking (EDT) Annotation Guidelines available at http://projects.
ldc.upenn.edu/ace/docs/EDT-Guidelines-V2-5.pdf: “We do not identify mentions of animals or
most inanimate objects at this time”.

198

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

original
Soon et al.
duplicated
baseline

R
58.6
64.9

MUC-6
P
F1
67.3 62.3
65.6

65.3

R
56.1

MUC-7
P
F1
65.5 60.4

55.1

68.5

61.1

Table 6: Results on MUC
5.3.2 F EATURE S ELECTION
For determining the relevant feature sets we follow an iterative procedure similar to the wrapper
approach for feature selection (Kohavi & John, 1997) using the development data. The feature
selection algorithm performs a hill-climbing search along the feature space. In the case of the baseline system we perform backward feature selection, since we are interested in obtaining a minimal
feature set which provides the most competitive baseline13 . We start with a model based on all
available features (Ponzetto & Strube, 2006b). Then we train models obtained by removing one
feature at a time. We choose the worst performing feature, namely the one whose removal gives the
largest improvement based on the MUC score F-measure on the development data, and remove it
from the model. We then train classifiers removing each of the remaining features separately from
the enhanced model. The process is iteratively run as long as significant improvement is observed.
For evaluating WordNet and Wikipedia features, we perform instead forward greedy feature
selection: we start with the minimal set of previously kept baseline features and iteratively add
those features from WordNet or Wikipedia which give the best MUC score F-measure improvement
on the development data for each selection step. This is because we are interested in evaluating the
additional contribution of such information, and avoid external factors such as improvements due to
further removal of the baseline features. A summary of the features selected by the backward and
forward feature selections is given in Table 9.
5.3.3 R ESULTS
Table 6 compares the results between our duplicated Soon et al. (2001) baseline and the original
system on the MUC data. We assume that the slight improvements of our system are due to the
use of current preprocessing components and another classifier. Tables 7 and 8 show a comparison
of the performance between our baseline system (Ponzetto & Strube, 2006b) and the ones incremented with semantic features mined from WordNet and Wikipedia on the ACE data. Statistically
significant performance improvements are highlighted in bold14 .
13. This is under the assumption that adding the SRL features to the core baseline features from Soon et al. (2001) could
not yield the best performance. In practice, analysis of feature selection on the development data highlights that
Soon et al.’s (2001) features such as J DEM, PROPER NAME and WN CLASS (BNEWS), J DEM (NWIRE) and
DISTANCE (MERGED) are indeed removed from the baseline feature set when SRL information is included. None
of the best performing features of Soon et al. (2001) (STRING MATCH, ALIAS and APPOSITIVE) is removed,
thus supporting their feature relevance analysis (pp. 534–535).
14. We take performance variations between different experimental runs to be statistically significant in case the changes
in the MUC F-measure are statistically significant at the 0.05 level or higher. We follow Soon et al. (2001) in
performing a simple one-tailed, paired sample t-test between the baseline system’s MUC score F-measure and each
of the other systems’ F-measure scores on the test documents.

199

P ONZETTO & S TRUBE

baseline
+WordNet
+Wikipedia

R
50.5
59.1
58.3

P
82.0
82.4
81.9

BNEWS
F1
Ap
62.5 44.2
68.8 43.1
68.1 41.2

Acn
17.4
40.9
38.9

Apn
58.0
64.6
62.3

R
56.3
62.4
60.7

P
86.7
81.4
81.8

NWIRE
F1
Ap
68.3 43.8
70.7 45.4
69.7 44.1

Acn
35.0
43.0
40.1

Apn
71.6
68.7
71.6

Table 7: Results on the ACE 2003 data (BNEWS and NWIRE sections)

baseline
+WordNet
+Wikipedia

R
54.5
60.6
59.4

P
85.4
79.4
82.2

F1
66.5
68.7
68.9

Ap
40.5
42.4
38.9

Acn
30.1
43.2
41.4

Apn
73.0
66.0
74.5

Table 8: Results ACE (merged BNEWS/NWIRE)
5.3.4 D ISCUSSION
The tables show that semantic features improve system recall, rather than acting as a ‘semantic filter’
improving precision. Semantics therefore seems to trigger a response in cases where more shallow
features do not seem to suffice. A one-tailed, paired sample t-test reveals that on the BNEWS and
MERGED sections the difference in performance between WordNet and Wikipedia are not statistically significant (p < 0.05), thus proving that Wikipedia is indeed competitive with WordNet in the
coreference resolution scenario.
WordNet and Wikipedia features tend to consistently increase performance on common nouns
on all dataset partitions. WordNet features are able to improve by 23.5%, 8% and 13.1% the accuracy rate for common nouns on the BNEWS, NWIRE and MERGED datasets (+35, +28 and +65
correctly resolved common nouns out of 149 and 349 and 498 respectively), whereas employing
Wikipedia yields slightly smaller improvements (+21.5%, +5.1% and +11.3% accuracy increase
on the same datasets). The accuracy on common nouns shows that features derived from Wikipedia
are competitive with the ones from WordNet. The performance gap on all three datasets is relatively
small, which indicates the usefulness of using an encyclopedic knowledge base as a replacement for
a lexical taxonomy.
If semantic relatedness clearly helps for common nouns, it does not always improve the performance on proper names, where features such as string matching and alias suffice, cf. the performance degradation induced by WordNet on the NWIRE and MERGED datasets. This suggests that
the semantic information we use tends to play a role mostly for common noun resolution, where
surface features cannot account for complex preferences and semantic knowledge is required. Nevertheless, Wikipedia exhibits in general a better performance for the resolution of proper names than
WordNet, as it yields results which are always at least as good as the baseline. This is not surprising, as Wikipedia contains a larger amount of information about named entities than WordNet. In
particular, qualitative analysis on the development data shows that Wikipedia is useful for instance
for identifying cases of REs coreferent with the same geo-political discourse entity, e.g. Yemeni
and Yemen or American and United States, thanks to the feature of redirection, i.e. YEMENI
redirects to YEMEN. While from a linguistic point of view it is far from clear whether such cases
represent genuine cases of coreference, redirection helps to cover meronymy.
200

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

backward
feature
selection

forward
feature
selection

starting
features
removed
WordNet
features
added
Wikipedia
features
added

BNEWS
WN CLASS
PROPER NAME
J DEM
jcn average
jcn best
pl best
wup average
wup average
pl best
pl average
gloss

NWIRE

MERGED

J DEM

DISTANCE

jcn best
lin average
pl best

lch best
pl best
pl average

wup average
lch best

pl best
pl average

Table 9: Feature selection
Feature selection improves the results15 . This is due to the fact that our full feature set is extremely redundant: in order to explore the usefulness of the knowledge sources we included overlapping features (i.e. using best and average similarity/relatedness measures at the same time), as
well as features capturing the same phenomenon from different points of view (i.e. using multiple
measures at the same time). In order to yield the desired performance improvements, it turns out
to be essential to filter out irrelevant features. For instance, in the case of Wikipedia none of the
I/J GLOSS, RELATED or CATEGORIES CONTAINS features survives the feature selection process (see Table 9). On the other hand, in all cases for both WordNet and Wikipedia at least one
best and one average measure is always included among the selected features. This suggests that
including information about all available senses (in terms of average relatedness measures) provides sensible information to handle ambiguity. Finally, multiple measures are included in most of
the selected feature sets, e.g. the selected features for WordNet on the BNEWS data include both
the measure from Wu and Palmer (1994) and the one from Jiang and Conrath (1997), indicating
that rather than having a best overall measure, competitive results can be obtained by integrating
different ones.

6. Experiments for German
Except for Gurevych (2005) who ported semantic relatedness measures to German using GermaNet
(Lemnitzer & Kunze, 2002), the topic of semantic relatedness has been explored almost exclusively
for the English language using WordNet. Research about semantic relatedness in languages other
than English has been hindered by differences in the structure and organization of the respective
wordnets and by a large variation in coverage. For instance, Gurevych and Niederlich (2005a) had
to implement an API specifically designed for GermaNet access. Furthermore, GermaNet is much
smaller than WordNet and it did not even cover all word pairs in the relatively small dataset provided
by Rubenstein and Goodenough (1965) which Gurevych and Niederlich (2005b) translated into
German. In contrast, the structure and organization of Wikipedia is the same across all languages,
so that semantic relatedness measures developed for English can be applied to other languages
15. We experienced during system prototyping that simply including the full feature set without performing feature
selection gave worst or no significant performance variations at all.

201

P ONZETTO & S TRUBE

GermaNet 4.0
#word-sense pairs/#articles
#synsets/#categories

60,646
41,777

German Wikipedia
Feb. 06
Jun. 06 Sep. 06
387,586 410,586 471,065
25,035
28,656
33,130

Table 10: Statistics on GermaNet and Wikipedia

without changing the methods for accessing them. The coverage of the German Wikipedia is also
considerably large as shown in Table 10.
To our knowledge there exist no datasets for evaluating semantic relatedness measures in languages other than English. Only Gurevych translated the dataset by Rubenstein and Goodenough
(1965) into German and supplied it with judgments by native speakers of German. Gurevych (2005)
evaluated several semantic relatedness measures on the R&G dataset using GermaNet as knowledge
source. Because GermaNet did not cover all words in the 65 word pairs they report only results for
57 word pairs with the best results obtained by res followed by lin. In Table 11 we adopt the results
reported by Gurevych and supply them with numbers computed for all 65 word pairs including the
ones which are not covered by GermaNet. As in the case of the results given in Table 1, word pairs
where at least one of the words could not be found in GermaNet are assigned a relatedness score of
016 .
The numbers we obtain in our experiments using Wikipedia compare well with the numbers
they reported. In contrast to our results for the English R&G dataset, Wikipedia performs as good as
GermaNet when considering all pairs – viz., no statistically significant difference using a one-tailed
paired sample t-test (p < 0.05) between the respective best performing measures, GermaNet lin and
Wikipedia wup – and shows a performance only slightly below when considering available pairs
only. The German R&G dataset is the one on which Wikipedia shows the best overall performance,
and the only one on which we have a larger coverage than the corresponding wordnet (4 missing
pairs versus 8).
As previously pointed out in the analysis of the performance on the 353-TC English dataset,
Wikipedia is able to yield a competitive performance on datasets specifically designed to capture
semantic relatedness, rather than the stricter notion of similarity. This seems to be supported in the
present scenario as well, as the data in Gurevych and Niederlich (2005b) are rated explicitly for
semantic relatedness, using the definition from Budanitsky and Hirst (2006) that we summarized in
Section 2.2.1.

7. Implementation Details
Wikipedia is freely available for download, and can be accessed using robust Open Source applications, e.g. the MediaWiki software17 , integrated within a Linux, Apache, MySQL and PHP (LAMP)
software bundle. We briefly present in the following the main components of the Application Programming Interface (API) we developed as part of the present work18 . The architecture of the
WikiRelate! API (Ponzetto & Strube, 2007a) consists of the following modules:
16. The complete list of German word pairs with the corresponding human judgments and automatically computed
measures can be obtained from Gurevych and Niederlich (2005b).
17. http://www.mediawiki.org
18. The WikiRelate! software can be downloaded from http://www.eml-research.de/nlp.

202

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Dataset
all
non-miss

R&G

Google
jaccard
0.26
0.26

GermaNet 4.0
lin
res lesk
0.66 0.64 0.49
0.73 0.76 0.53

Wikipedia (February 2006)
pl
wup lch
res gloss
0.58 0.65 0.64 0.62 0.33
0.62 0.70 0.69 0.67 0.34
Wikipedia (June 2006)
0.65 0.64 0.58 0.33
0.69 0.69 0.62 0.35

all
non-miss

0.58
0.61

all
non-miss

Wikipedia (September 2006)
0.59 0.65 0.64 0.50 0.38
0.63 0.70 0.69 0.55 0.40

Table 11: Results for German R&G Dataset

1. RDBMS: at the lowest level, the encyclopedia content is stored into a relational database
management system (e.g. MySQL).
2. MediaWiki: a suite of PHP routines for interacting with the RDBMS.
3. WWW-Wikipedia Perl library19 : responsible for querying MediaWiki, parsing and structuring the returned encyclopedia pages.
4. XML-RPC server: an intermediate communication layer between Java and the Perl routines.
5. Java wrapper library: provides a simple façade to create and access the encyclopedia page
objects and compute the relatedness scores.
The information flow of the API is summarized by the sequence diagram in Figure 5. The higher
input/output layer the user interacts with is provided by a Java API from which Wikipedia can be
queried. The API provides factory classes for querying Wikipedia, in order to retrieve the encyclopedia entries as well the relatedness scores for word pairs. In practice, the Java library works as a
wrapper in order to provide a simple user access in terms of a façade. The library is responsible
for issuing HTTP requests to an XML-RPC daemon which provides a layer for calling Perl routines
from the Java API. Perl routines take care of the bulk of querying encyclopedia entries to the MediaWiki software (which in turn queries the database) and efficiently parsing the text responses into
structured objects.

8. Related Work
In this section we relate our work to the existing body of literature on computing semantic relatedness and its application to various NLP tasks.
19. http://search.cpan.org/dist/WWW-Wikipedia/

203

204
4. Relatedness score computation

3. Wikipedia pages lookup

loop: foreach word

2. Create category tree Java data structure

1. Retrieve Wikipedia category tree

: XML-RPC daemon

Perl object

Wiki markup text

: HTTP request

: WWW-Wikipedia

: Perl module call

: Category extraction and path search

XML-RPC response

: HTTP request

: Create graph from category tree query

Result set

: SQL query (categories and links)

: Java wrapper library

Result set

: article lookup

: Database

: SQL query (page)

: MediaWiki

PHP Article object

: PHP module call

: Webserver

P ONZETTO & S TRUBE

Figure 5: WikiRelate! API processing sequence diagram. Wikipedia pages and relatedness measures are accessed through a Java API façade. The wrapper communicates with a Perl
library designed for Wikipedia access and parsing through an XML-RPC server. WWWWikipedia in turn accesses the database where the encyclopedia is stored by means of
appropriate queries to MediaWiki.

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

8.1 Computing Semantic Relatedness
Research in the area of computing semantic relatedness and similarity can be generally divided into
two categories – firstly measures of distributional similarity using largely unstructured information
such as text and secondly approaches using structured lexical databases such as WordNet.
Measures of distributional similarity (Landauer & Dumais, 1997; Lee, 1999; Dagan, 2000; Turney, 2001; Weeds & Weir, 2005, inter alia) are based on the distributional hypothesis, i.e. on the hypothesis that similar words appear in similar contexts and hence have similar meaning. For reasons
discussed in detail by Budanitsky and Hirst (2006, pp.41–44), measures of distributional similarity
and measures of semantic relatedness are distinct, because (1) measures of semantic relatedness
cover relations between concepts while measures of distributional similarity capture relations between words; (2) semantic relatedness is a symmetric relation while distributional similarity is a
potentially asymmetric relationship; (3) measures of semantic relatedness depend on a predefined
knowledge source which is created by humans and may be presumed “true, unbiased and complete”
(Budanitsky & Hirst, 2006, p.43); measures of distributional similarity depend entirely on corpora
causing problems of imbalance and data sparseness; this problem may only be overcome by using
representative, very large corpora; however, computing distributional similarity does not scale well
(Gorman & Curran, 2006). Budanitsky and Hirst (2006) conclude that measures of distributional
similarity cannot replace measures of semantic relatedness and similarity.
Approaches using structured lexical databases can be traced back to work by Rada et al. (1989)
who measured semantic similarity in MeSH, a term hierarchy for indexing articles in Medline. They
compute semantic similarity straightforwardly in terms of the numbers of edges between terms in
the hierarchy. Research in this area proceeded in two directions. Firstly, different knowledge sources
were proposed. Early on, WordNet was used to provide a broad coverage lexical database (Resnik,
1993; Wu & Palmer, 1994; Resnik, 1995). Later Jarmasz and Szpakowicz (2003) explored the use
of Roget’s Thesaurus for computing semantic similarity. Secondly, major advances were achieved
by developing more sophisticated measures of semantic similarity and relatedness.
In this article we propose a new knowledge source for computing semantic relatedness, i.e.
Wikipedia and its associated categorization network. We believe that many NLP applications will
benefit from using Wikipedia and evaluate this hypothesis by including Wikipedia based semantic
relatedness measures as features in a state-of-the-art coreference resolution system.
8.2 Using Semantic Relatedness in Coreference and Other NLP Applications
Vieira and Poesio (2000), Harabagiu et al. (2001), and Markert and Nissim (2005) explore the use of
WordNet for different coreference resolution subtasks, such as resolving bridging references, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems
which infer coreference relations from a set of potential antecedents by means of a WordNet search.
Our approach to WordNet here is to cast the search results in terms of semantic similarity measures.
Their output can be used as features for a learner. These measures are not specifically developed for
coreference resolution but simply taken ‘off-the-shelf’ and applied to our task without any specific
tuning — e.g. in contrast to Harabagiu et al. (2001), who weight WordNet relations differently in
order to compute the confidence measure of the path.
Semantic relatedness measures have been proven to be useful in many applications in Natural
Language Processing such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan,
Banerjee, & Pedersen, 2005), information retrieval (Finkelstein et al., 2002), information extraction
205

P ONZETTO & S TRUBE

pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), paraphrase detection (Mihalcea, Corley, & Strapparava, 2006) and spelling correction
(Budanitsky & Hirst, 2006).

9. Conclusions
In this article we investigated the use of Wikipedia for computing semantic relatedness and its application to a real-world NLP task, coreference resolution. We assumed the Wikipedia category
graph to represent a semantic network modeling relations between concepts, and we computed their
relatedness from it. Even if the categorization feature has been introduced into Wikipedia only
three years ago, our results indicate that semantic relatedness computed using the Wikipedia category network consistently correlates better with human judgments than a simple baseline based
on Google counts. It is also competitive with WordNet for datasets specifically modeling semantic
relatedness human judgments. Because all available dataset are small and seem to be assembled
rather arbitrarily we perform an extrinsic evaluation with an NLP application, i.e. a coreference resolution system, where we register for some datasets no statistically significant differences between
the improvements given by features induced from WordNet and the ones from Wikipedia.
Wikipedia provides a large amount of information as encyclopedic entries at the leaves of the
category network, e.g. named entities. The encyclopedia gets continuously updated and the derived
knowledge can be used to analyze current information. The text and the category network both
provide semi-structured information and can be mined with more precision than unstructured data
gathered from the web. Unfortunately, the Wikipedia categorization still suffers from some limitations, i.e., it cannot be considered a fully-fledged ontology, as the relations between categories
are not semantically-typed. In the near future we will concentrate on making the semantic relations
between concepts explicit in the Wikipedia category network (Ponzetto & Strube, 2007b). The availability of explicit semantic relations will allow for inducing semantic similarity rather than semantic
relatedness measures, which may be more suitable for coreference resolution. What is most interesting about our results is that they indicate that a collaboratively created folksonomy can actually
be used in NLP applications with the same benefit as hand-crafted taxonomies or ontologies.
Acknowledgments. This work has been funded by the Klaus Tschira Foundation, Heidelberg,
Germany. The first author has been supported by a KTF grant (09.003.2004). We thank three
anonymous JAIR reviewers for their extensive reviews and our colleague Vivi Nastase for useful
feedback.

References
Ahn, D., Jijkoun, V., Mishne, G., Müller, K., de Rijke, M., & Schlobach, S. (2004). Using Wikipedia
at the TREC QA track. In Proceedings of the Thirteenth Text REtrieval Conference, Gaithersburg, Md., 16–19 November 2004.
Ahn, K., Bos, J., Curran, J. R., Kor, D., Nissim, M., & Webber, B. (2005). Question answering with
QED at TREC-2005. In Proceedings of the Fourteenth Text REtrieval Conference, Gaithersburg, Md., 15–18 November 2005.
Banerjee, S., & Pedersen, T. (2003). Extended gloss overlap as a measure of semantic relatedness. In
Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco,
206

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Mexico, 9–15 August 2003, pp. 805–810.
Beigman Klebanov, B. (2006). Semantic relatedness: Computational investigation of human data. In
Proceedings of the 3rd Midwest Computational Linguistics Colloquium, Urbana-Champaign,
Ill., 20-21 May 2006.
Beigman Klebanov, B., & Shamir, E. (2006). Reader-based exploration of lexical cohesion. Language Resources and Evaluation, 40(2), 109–126.
Berger, A., Della Pietra, S. A., & Della Pietra, V. J. (1996). A maximum entropy approach to natural
language processing. Computational Linguistics, 22(1), 39–71.
Bos, J., & Markert, K. (2005). Recognising textual entailment with logical inference. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical
Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8 October 2005, pp.
628–635.
Budanitsky, A., & Hirst, G. (2006). Evaluating WordNet-based measures of semantic distance.
Computational Linguistics, 32(1), 13–47.
Bunescu, R., & Paşca, M. (2006). Using encyclopedic knowledge for named entity disambiguation. In Proceedings of the 11th Conference of the European Chapter of the Association for
Computational Linguistics, Trento, Italy, 3–7 April 2006, pp. 9–16.
Carreras, X., & Màrquez, L. (2005). Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the 9th Conference on Computational Natural Language Learning,
Ann Arbor, Mich., USA, 29–30 June 2005, pp. 152–164.
Charniak, E. (1973). Jack and Janet in search of a theory of knowledge. In Advance Papers from the
Third International Joint Conference on Artificial Intelligence, Stanford, Cal., pp. 337–343,
Los Altos, Cal. W. Kaufmann.
Chinchor, N. (2001). Message Understanding Conference (MUC) 7. LDC2001T02, Philadelphia,
Penn: Linguistic Data Consortium.
Chinchor, N., & Sundheim, B. (2003).
Message Understanding Conference (MUC) 6.
LDC2003T13, Philadelphia, Penn: Linguistic Data Consortium.
Dagan, I. (2000). Contextual word similarity. In Dale, R., Moisl, H., & H., S. (Eds.), Handbook of
Natural Language Processing, pp. 459–476. New York, N.Y.: Marcel Dekker Inc.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin, E. (2002).
Placing search in context: The concept revisited. ACM Transactions on Information Systems,
20(1), 116–131.
Gabrilovich, E., & Markovitch, S. (2006). Overcoming the brittleness bottleneck using Wikipedia:
Enhancing text categorization with encyclopedic knowledge. In Proceedings of the 21st National Conference on Artificial Intelligence, Boston, Mass., 16–20 July 2006, pp. 1301–1306.
Gabrilovich, E., & Markovitch, S. (2007). Computing semantic relatedness using Wikipedia-based
explicit semantic analysis. In Proceedings of the 20th International Joint Conference on
Artificial Intelligence, Hyderabad, India, 6–12 January 2007, pp. 1606–1611.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics,
28(3), 245–288.
207

P ONZETTO & S TRUBE

Giles, J. (2005). Internet encyclopedias go head to head. Nature, 438, 900–901.
Giménez, J., & Màrquez, L. (2004). SVMTool: A general POS tagger generator based on support
vector machines. In Proceedings of the 4th International Conference on Language Resources
and Evaluation, Lisbon, Portugal, 26–28 May 2004, pp. 43–46.
Gorman, J., & Curran, J. R. (2006). Scaling distributional similarity to large corpora. In Proceedings
of the 21st International Conference on Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics, Sydney, Australia, 17–21 July 2006, pp.
361–368.
Gurevych, I. (2005). Using the structure of a conceptual network in computing semantic relatedness.
In Proceedings of the 2nd International Joint Conference on Natural Language Processing,
Jeju Island, South Korea, 11–13 October 2005, pp. 767–778.
Gurevych, I., & Niederlich, H. (2005a). Accessing GermaNet data and computing semantic relatedness. In Companion Volume to the Proceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics, Ann Arbor, Mich., 25–30 June 2005, pp. 5–8.
Gurevych, I., & Niederlich, H. (2005b). Measuring semantic relatedness of GermaNet word senses.
Tech. rep., EML Research gGmbH.
Harabagiu, S. M., Bunescu, R. C., & Maiorano, S. J. (2001). Text and knowledge mining for
coreference resolution. In Proceedings of the 2nd Conference of the North American Chapter
of the Association for Computational Linguistics, Pittsburgh, Penn., 2–7 June 2001, pp. 55–
62.
Hobbs, J. R. (1978). Resolving pronominal references. Lingua, 44, 311–338.
Hovy, E., Gerber, L., Hermjakob, U., Junk, M., & Lin, C.-Y. (2001). Question answering in Webclopedia. In Proceedings of the Thirteenth Text REtrieval Conference, Gaithersburg, Md.,
13–16 November 2001.
Hsu, C.-W., Chang, C.-C., & Lin, C.-J. (2006). A Practical Guide to Support Vector Classification.
http://www.csie.ntu.edu.tw/∼cjlin/papers/guide/guide.pdf.
Jarmasz, M., & Szpakowicz, S. (2003). Roget’s Thesaurus and semantic similarity. In Proceedings of the International Conference on Recent Advances in Natural Language Processing,
Borovets, Bulgaria, 10–12 September 2003, pp. 212–219.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the 10th International Conference on Research in Computational
Linguistics (ROCLING).
Kehler, A., Appelt, D., Taylor, L., & Simma, A. (2004). The (non)utility of predicate-argument
frequencies for pronoun interpretation. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Association for Computational Linguistics,
Boston, Mass., 2–7 May 2004, pp. 289–296.
Kim, S. N., & Baldwin, T. (2005). Automatic interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International Joint Conference on Natural Language
Processing, Jeju Island, South Korea, 11–13 October 2005, pp. 945–956.
Kohavi, R., & John, G. H. (1997). Wrappers for feature subset selection. Artificial Intelligence
Journal, 97(1-2), 273–324.
208

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Kohomban, U. S., & Lee, W. S. (2005). Learning semantic classes for word sense disambiguation.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,
Ann Arbor, Mich., 25–30 June 2005, pp. 34–41.
Kudoh, T., & Matsumoto, Y. (2000). Use of Support Vector Machines for chunk identification. In
Proceedings of the 4th Conference on Computational Natural Language Learning, Lisbon,
Portugal, 13–14 September 2000, pp. 142–144.
Landauer, T. K., & Dumais, S. T. (1997). A solution to Plato’s problem: The Latent Semantic
Analysis theory of the acquisition, induction, and representation of knowledge. Psychological
Review, 104, 211–240.
Leacock, C., & Chodorow, M. (1998). Combining local context and WordNet similarity for
word sense identification. In Fellbaum, C. (Ed.), WordNet. An Electronic Lexical Database,
chap. 11, pp. 265–283. Cambridge, Mass.: MIT Press.
Lee, L. (1999). Measures of distributional similarity. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics, College Park, Md., 20–26 June 1999, pp.
25–31.
Lemnitzer, L., & Kunze, C. (2002). GermaNet – representation, visualization, application. In
Proceedings of the 3rd International Conference on Language Resources and Evaluation,
Las Palmas, Canary Islands, Spain, 29–31 May 2002, pp. 1485–1491.
Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries: How to tell
a pine cone from an ice cream cone. In Proceedings of the 5th Annual Conference on Systems
Documentation, Toronto, Ontario, Canada, pp. 24–26.
Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings of the 15th International Conference on Machine Learning, Madison, Wisc., 24–27 July 1998, pp. 296–304.
Lo, K. K., & Lam, W. (2006). Using semantic relations with world knowledge for question answering. In Proceedings of the Fifteenth Text REtrieval Conference, Gaithersburg, Md., 14–17
November 2006.
Luo, X., Ittycheriah, A., Jing, H., Kambhatla, N., & Roukos, S. (2004). A mention-synchronous
coreference resolution algorithm based on the Bell Tree. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Linguistics, Barcelona, Spain, 21–26 July 2004,
pp. 136–143.
Malouf, R. (2002). A comparison of algorithms for maximum entropy parameter estimation. In
Proceedings of the 6th Conference on Computational Natural Language Learning, Taipei,
Taiwan, 31 August – 1 September 2002, pp. 49–55.
Markert, K., & Nissim, M. (2005). Comparing knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3), 367–401.
McCallum, A., Pal, C., Druck, G., & Wang, X. (2006). Multi-conditional learning: Generative/discriminative training for clustering and classification. In Proceedings of the 21st National Conference on Artificial Intelligence, Boston, Mass., 16–20 July 2006, pp. 433–439.
Mierswa, I., Wurst, M., Klinkenberg, R., Scholz, M., & Euler, T. (2006). YALE: Rapid prototyping
for complex data mining tasks. In Proceedings of the 12th ACM SIGKDD International
209

P ONZETTO & S TRUBE

Conference on Knowledge Discovery and Data Mining, Philadelphia, Penn., 20–23 August
2006, pp. 935–940.
Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st National Conference on Artificial
Intelligence, Boston, Mass., 16–20 July 2006, pp. 775–780.
Miller, G. A., & Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and
Cognitive Processes, 6(1), 1–28.
Mitchell, A., Strassel, S., Przybocki, M., Davis, J., Doddington, G., Grishman, R., Meyers, A.,
Brunstain, A., Ferro, L., & Sundheim, B. (2003). TIDES extraction (ACE) 2003 multilingual
training data. LDC2004T09, Philadelphia, Penn.: Linguistic Data Consortium.
Ng, V., & Cardie, C. (2002). Improving machine learning approaches to coreference resolution. In
Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,
Philadelphia, Penn., 7–12 July 2002, pp. 104–111.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1), 71–105.
Patwardhan, S., Banerjee, S., & Pedersen, T. (2005). SenseRelate::TargetWord – A generalized
framework for word sense disambiguation. In Proceedings of the 20th National Conference
on Artificial Intelligence, Pittsburgh, Penn., 9–13 July 2005.
Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet::Similarity – Measuring the relatedness of concepts. In Companion Volume to the Proceedings of the Human Language
Technology Conference of the North American Chapter of the Association for Computational
Linguistics, Boston, Mass., 2–7 May 2004, pp. 267–270.
Poesio, M., Ishikawa, T., Schulte im Walde, S., & Vieira, R. (2002). Acquiring lexical knowledge
for anaphora resolution. In Proceedings of the 3rd International Conference on Language
Resources and Evaluation, Las Palmas, Canary Islands, Spain, 29–31 May 2002, pp. 1220–
1225.
Ponzetto, S. P., & Strube, M. (2006a). Exploiting semantic role labeling, WordNet and Wikipedia
for coreference resolution. In Proceedings of the Human Language Technology Conference
of the North American Chapter of the Association for Computational Linguistics, New York,
N.Y., 4–9 June 2006, pp. 192–199.
Ponzetto, S. P., & Strube, M. (2006b). Semantic role labeling for coreference resolution. In Companion Volume to the Proceedings of the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Trento, Italy, 3–7 April 2006, pp. 143–146.
Ponzetto, S. P., & Strube, M. (2007a). An API for measuring the relatedness of words in Wikipedia.
In Companion Volume to the Proceedings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic, 23–30 June 2007, pp. 49–52.
Ponzetto, S. P., & Strube, M. (2007b). Deriving a large scale taxonomy from Wikipedia. In Proceedings of the 22nd National Conference on Artificial Intelligence, Vancouver, B.C., Canada,
22–26 July 2007, pp. 1440–1447.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J. H., & Jurafsky, D. (2004). Shallow semantic parsing using Support Vector Machines. In Proceedings of the Human Language Technology
210

K NOWLEDGE D ERIVED F ROM W IKIPEDIA

Conference of the North American Chapter of the Association for Computational Linguistics,
Boston, Mass., 2–7 May 2004, pp. 233–240.
Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development and application of a metric to
semantic nets. IEEE Transactions on Systems, Man and Cybernetics, 19(1), 17–30.
Resnik, P. (1993). Selection and Information: A Class-based Approach to Lexical Relationships.
Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania,
Philadelphia, Penn.
Resnik, P. (1995). Using information content to evaluate semantic similarity in a taxonomy. In
Proceedings of the 14th International Joint Conference on Artificial Intelligence, Montréal,
Canada, 20–25 August 1995, Vol. 1, pp. 448–453.
Resnik, P. (1999). Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language. Journal of Artificial Intelligence
Research, 11, 95–130.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates of synonymy. Communications of
the ACM, 8(10), 627–633.
Salton, G., & McGill, M. (1983). Introduction to Modern Information Retrieval. New York, N.Y.:
McGraw-Hill.
Seco, N., Veale, T., & Hayes, J. (2004). An intrinsic information content metric for semantic similarity in WordNet. In Proceedings of the 16th European Conference on Artificial Intelligence,
Valencia, Spain, 23–27 August 2004, pp. 1089–1090.
Soon, W. M., Ng, H. T., & Lim, D. C. Y. (2001). A machine learning approach to coreference
resolution of noun phrases. Computational Linguistics, 27(4), 521–544.
Stevenson, M., & Greenwood, M. (2005). A semantic approach to IE pattern induction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, Ann
Arbor, Mich., 25–30 June 2005, pp. 379–386.
Strube, M., & Ponzetto, S. P. (2006). WikiRelate! Computing semantic relatedness using Wikipedia.
In Proceedings of the 21st National Conference on Artificial Intelligence, Boston, Mass., 16–
20 July 2006, pp. 1419–1424.
Tatu, M., Iles, B., Slavick, J., Novischi, A., & Moldovan, D. (2006). COGEX at the Second Recognizing Textual Entailment Challenge. In Proceedings of the Second PASCAL Recognising
Textual Entailment Challenge Workshop, Venice, Italy, 10 April 2006, pp. 104–109.
Turney, P. (2001). Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings
of the 12th European Conference on Machine Learning, Freiburg, Germany, 3–7 September,
2001, pp. 491–502.
van Rijsbergen, C. (1979). Information Retrieval. London, U.K.: Butterworths.
Vapnik, V. (1995). The Nature of Statistical Learning Theory. Springer-Verlag, Berlin, Germany.
Vieira, R., & Poesio, M. (2000). An empirically-based system for processing definite descriptions.
Computational Linguistics, 26(4), 539–593.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). A model-theoretic
coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference
(MUC-6), pp. 45–52, San Mateo, Cal. Morgan Kaufmann.
211

P ONZETTO & S TRUBE

Weeds, J., & Weir, D. (2005). Co-occurrence retrieval: A flexible framework for lexical distributional similarity. Computational Linguistics, 31(4), 439–475.
Wu, Z., & Palmer, M. (1994). Verb semantics and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Computational Linguistics, Las Cruces, N.M., 27–30
June 1994, pp. 133–138.
Yang, X., Zhou, G., Su, J., & Tan, C. L. (2003). Coreference resolution using competition learning
approach. In Proceedings of the 41st Annual Meeting of the Association for Computational
Linguistics, Sapporo, Japan, 7–12 July 2003, pp. 176–183.

212

Journal of Artificial Intelligence Research 30 (2007) 413-456

Submitted 05/2007; published 11/2007

Individual and Domain Adaptation
in Sentence Planning for Dialogue
Marilyn Walker

lynwalker@gmail.com

Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Amanda Stent

amanda.stent@gmail.com

Department of Computer Science, Stony Brook University
Stony Brook, NY 11794, USA

François Mairesse

f.mairesse@sheffield.ac.uk

Department of Computer Science, University of Sheffield,
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Rashmi Prasad

rjprasad@linc.cis.upenn.edu
Institute for Research in Cognitive Science, University of Pennsylvania,
3401 Walnut Street, Suite 400A, Philadelphia, PA 19104, USA

Abstract
One of the biggest challenges in the development and deployment of spoken dialogue
systems is the design of the spoken language generation module. This challenge arises
from the need for the generator to adapt to many features of the dialogue domain, user
population, and dialogue context. A promising approach is trainable generation, which
uses general-purpose linguistic knowledge that is automatically adapted to the features of
interest, such as the application domain, individual user, or user group. In this paper
we present and evaluate a trainable sentence planner for providing restaurant information
in the MATCH dialogue system. We show that trainable sentence planning can produce
complex information presentations whose quality is comparable to the output of a templatebased generator tuned to this domain. We also show that our method easily supports
adapting the sentence planner to individuals, and that the individualized sentence planners
generally perform better than models trained and tested on a population of individuals.
Previous work has documented and utilized individual preferences for content selection, but
to our knowledge, these results provide the first demonstration of individual preferences for
sentence planning operations, affecting the content order, discourse structure and sentence
structure of system responses. Finally, we evaluate the contribution of different feature
sets, and show that, in our application, n-gram features often do as well as features based
on higher-level linguistic representations.

1. Introduction
One of the most robust findings of studies of human-human dialogue is that people adapt
their interactions to match their conversational partners’ needs and behaviors (Goffman,
1981; Brown & Levinson, 1987; Pennebaker & King, 1999). People adapt the content of
their utterances (Garrod & Anderson, 1987; Luchok & McCroskey, 1978). They choose
syntactic structures to match their partners’ syntax (Levelt & Kelter, 1982; Branigan,
Pickering, & Cleland, 2000; Reitter, Keller, & Moore, 2006; Stenchikova & Stent, 2007),
c
2007
AI Access Foundation. All rights reserved.

Walker, Stent, Mairesse, & Prasad

and adapt their choice of words and referring expressions (Clark & Wilkes-Gibbs, 1986;
Brennan & Clark, 1996). They also adapt their speaking rate, amplitude, and clarity of
pronunciation (Jungers, Palmer, & Speer, 2002; Coulston, Oviatt, & Darves, 2002; Ferguson
& Kewley-Port, 2002).
However, it is beyond the state of the art to reproduce this type of adaptation in
the spoken language generation module of a dialogue system, i.e. the components that
handle response generation and information presentation. A standard generation system
includes modules for content planning, sentence planning, and surface realization (Kittredge,
Korelsky, & Rambow, 1991; Reiter & Dale, 2000). A content planner takes as input a
communicative goal; it selects content to realize that goal and organizes that content into
a content plan. A sentence planner takes as input a content plan. It decides how the
content is allocated into sentences, how the sentences are ordered, and which discourse cues
to use to express the relationships between content elements. It outputs a sentence plan.
Finally, a surface realizer determines the words and word order for each sentence in the
sentence plan. It outputs a text or speech realization for the original communicative goal.
The findings from human-human dialogue suggest that adaptation could potentially be
useful at any stage of the generation pipeline. Yet to date, the only work on adaptation
to individual users utilizes models of the user’s knowledge, needs, or preferences to adapt
the content for content planning (Jokinen & Kanto, 2004; Rich, 1979; Wahlster & Kobsa,
1989; Zukerman & Litman, 2001; Carenini & Moore, 2006), rather than applying models
of individual linguistic preferences as to the form of the output, as determined by sentence
planning or surface realization.
However, consider the alternative realizations for a restaurant recommendation in Figure 1. Columns A and B contain human ratings of the quality of the realizations from users
A and B. The differences in the rating feedback suggest that each user has different perceptions as to the quality of the potential realizations. Data from an experiment collecting
feedback from users A and B, for 20 realizations of 30 different recommendation content
plans (600 examples), shows that the feedback of the two users are easily distinguished: a
paired t-test supports the hypothesis that the two samples are sampled from distinct distributions (t = 17.4, p < 0.001). These perceptual differences appear to be more general:
when we examined the user feedback from the evaluation experiment described by Rambow,
Rogati, and Walker (2001) where 60 users rated the output of 7 different spoken language
generators for 20 content plans, we again found significant differences in user perceptions
of utterance quality (F = 1.2, p < 0.002). This led us to hypothesize that individualized
sentence planners for dialogue systems might be of high utility.
In addition to our own studies, we also find evidence in other work that individual
variation is inherent to many aspects of language generation, including content ordering,
referring expression generation, syntactic choice, lexical choice, and prosody generation.
• It is common knowledge that individual authors can be identified from the linguistic
features of their written texts (Madigan, Genkin, Lewis, Argamon, Fradkin, & Ye,
2005; Oberlander & Brew, 2000).
• An examination of a weather report corpus for five weather forecasters showed individual differences in lexical choice for expressing specific weather-related concepts
(Reiter & Sripada, 2002).
414

Individual and Domain Adaptation in Dialogue

Alt Realization
6
Chanpen Thai has the best overall quality among the selected restaurants since it is a Thai restaurant, with good service, its price is 24
dollars, and it has good food quality.
7
Chanpen Thai has the best overall quality among the selected restaurants because it has good service, it has good food quality, it is a Thai
restaurant, and its price is 24 dollars.
4
Chanpen Thai has the best overall quality among the selected restaurants. It has good food quality, with good service, it is a Thai restaurant, and its price is 24 dollars.
9
Chanpen Thai is a Thai restaurant, with good food quality, its price
is 24 dollars, and it has good service. It has the best overall quality
among the selected restaurants.
5
Chanpen Thai has the best overall quality among the selected restaurants. It has good service. It has good food quality. Its price is 24
dollars, and it is a Thai restaurant.
3
Chanpen Thai has the best overall quality among the selected restaurants. Its price is 24 dollars. It is a Thai restaurant, with good
service. It has good food quality.
10 Chanpen Thai has the best overall quality among the selected restaurants. It has good food quality. Its price is 24 dollars. It is a Thai
restaurant, with good service.
2
Chanpen Thai has the best overall quality among the selected restaurants. Its price is 24 dollars, and it is a Thai restaurant. It has good
food quality and good service.
1
Chanpen Thai has the best overall quality among the selected restaurants. This Thai restaurant has good food quality. Its price is 24
dollars, and it has good service.
8
Chanpen Thai is a Thai restaurant, with good food quality. It has
good service. Its price is 24 dollars. It has the best overall quality
among the selected restaurants.

A
1

B
4

AVG
2.5

2

5

3.5

2

4

3

2

4

3

3

2

2.5

3

3

3

3

3

3

4

4

4

4

3

3.5

4

2

3

Figure 1: Some alternative realizations for the content plan in Figure 4, with feedback from
Users A and B, and the mean (AVG) of their feedback (1=worst and 5=best).

• Rules learned for generating nominal referring expressions perform better when individual speakers are provided as a feature to the learning algorithm (Jordan & Walker,
2005), and an experiment evaluating choice of referring expression shows only 70%
agreement among native speakers as to the best choice (Yeh & Mellish, 1997). Chai,
Hong, Zhou, and Prasov (2004) show that there are also individual differences in
gesture when generating multimodal references, and the corpus study of accented
pronouns reported by Kothari (2007) suggests that accentuation is also partly determined by individual linguistic style.
• Automatic evaluation techniques applied to human-generated reference outputs for
machine translation and automatic summarization perform better when multiple outputs are provided for comparison (Papenini, Roukos, Ward, & Zhu, 2002; Nenkova,
Passonneau, & McKeown, 2007): this can be attributed to the large variation in what
humans generate given particular content to express. This is also reflected in the finding that human subjects produce many different valid content orderings when asked to
order a specific set of content items to produce the best possible summary (Barzilay,
Elhadad, & McKeown, 2002; Lapata, 2003).
415

Walker, Stent, Mairesse, & Prasad

In the past, linguistic variation among individuals was considered a problem for generation researchers to work around, rather than a potential area of study (McKeown, Kukich,
& Shaw, 1994; Reiter, 2002; Reiter, Sripada, & Robertson, 2003). In part, this was due
to the hand-crafting of generation components and resources. It is impossible to encode
by hand, for each individual, rules for sentence planning and realization. Furthermore, if
domain experts don’t agree on the best way to express a domain concept, how can the
generation dictionary be encoded? It is difficult simply to get good output that respects
all the interacting domain and linguistic constraints even with considerable handcrafting of
rules (Kittredge, Korelsky, & Rambow, 1991).
Modeling individual differences can also be a problem for statistical methods when
learning paradigms are used that assume there is a single correct output (Lapata, 2003;
Jordan & Walker, 2005; Hardt & Rambow, 2001) inter alia. We believe that the simplest
way to deal with the inherent variability in possible generation outputs is to treat generation
as a ranking problem as we explain below, with techniques that overgenerate using user or
domain-independent rules, and then filter or rank the possibilities using domain or userspecific corpora or feedback (Langkilde & Knight, 1998; Langkilde-Geary, 2002; Bangalore
& Rambow, 2000; Rambow, Rogati, & Walker, 2001). This approach has an advantage for
dialogue systems because it also affords joint optimization of the generator and the textto-speech engine (Bulyko & Ostendorf, 2001; Nakatsu & White, 2006). There are many
problems in generation to which ranking models and individualization could be applied, such
as text planning, cue word selection, or referring expression generation (Mellish, O’Donnell,
Oberlander, & Knott, 1998; Litman, 1996; Di Eugenio, Moore, & Paolucci, 1997; Marciniak
& Strube, 2004). However, only recently has any work in generation acknowledged that
there are individual differences and tried to model them (Guo & Stent, 2005; Mairesse &
Walker, 2005; Belz, 2005; Lin, 2006).
This article describes SPaRKy (Sentence Planning with Rhetorical Knowledge), a sentence planner that uses rhetorical relations and adapts to the user’s individual sentence
planning preferences.1 SPaRKy has two components: a randomized sentence plan generator (SPG) that produces multiple alternative realizations of an information presentation,
and a sentence plan ranker (SPR) that is trained (using human feedback) to rank these
alternative realizations (See Figure 1). As mentioned above, previous work has documented
and utilized individual preferences for content selection, but to our knowledge, our results
provide the first demonstration of individual preferences for sentence planning operations,
affecting the content ordering, discourse structure, sentence structure, and sentence scope
of system responses. We also show that some of the learned preferences are domain-specific.
Section 2 compares our approach and results with previous work. Section 3 provides an
overview of the MATCH system architecture, which can generate dialogue system responses
using either SPaRKy, or a domain-specific template-based generator described and evaluated in previous work (Stent, Walker, Whittaker, & Maloor, 2002; Walker et al., 2004).
Sections 4, 5 and 6 describe SPaRKy in detail; they describe the SPG, the automatic
generation of features used in training the SPR, and how boosting is used to train the SPR.
Sections 7 and 8 present both quantitative and qualitative results:
1. A Java version of SPaRKy can be downloaded from www.dcs.shef.ac.uk/cogsys/sparky.html

416

Individual and Domain Adaptation in Dialogue

1. First, we show that SPaRKy learns to select sentence plans that are significantly
better than a randomly selected sentence plan, and on average less than 10% worse
than a sentence plan ranked highest by human judges. We also show that, in our
experiments, simple n-gram features perform as well as features based on higher-level
linguistic representations.
2. Second, we show that SPaRKy’s SPG can produce realizations that are comparable
to that of MATCH’s template-based generator, but that there is a gap between the
realization that the SPR selects when trained on multiple users and those selected by
a human.
3. Third, we show that when SPaRKy is trained for particular individuals, it performs
better than when trained on feedback from multiple individuals. These are the first
results suggesting that individual sentence planning preferences exist, and that they
can be modeled by a trainable generation system. We also show that in most cases
the performance of the individualized SPRs are statistically indistinguishable from
MATCH’s template-based generator, but for compare-2, User B prefers SPaRKy,
while for compare-3, User A prefers the template-based generator.
4. Fourth, we show that the differences in the learned models make sense in terms of
previous rule-based approaches to sentence planning. We analyze the qualitative
differences between the learned group and individual models, and show that SPaRKy
learns specific rules about the interaction between content items and sentence planning
operations, and rules that model individual differences, that would be difficult to
capture with a hand-crafted generator.
We sum up and discuss future work in Section 9.

2. Related Work
We discuss related work on adaptation in generation using the standard generation architecture which contains modules for content planning (Section 2.1), sentence planning
(Section 2.2) and surface realization (Section 2.3) (Kittredge, Korelsky, & Rambow, 1991;
Reiter & Dale, 2000).
2.1 Adaptation in Content Planning
There has been significant research on the use of user models and discourse context to adapt
the content of information presentations in dialogue (Joshi, Webber, & Weischedel, 1984,
1986; Chu-Carroll & Carberry, 1995; Zukerman & Litman, 2001) inter alia, but only the
user models (not the information presentation strategies) are sensitive to particular individuals. Several studies have investigated the use of quantitative models of user preferences in
selection of content for recommendations and comparisons (Carenini & Moore, 2006; Walker
et al., 2004; Polifroni & Walker, 2006), and Moore, Foster, Lemon, and White (2004) use
such models for referring expression generation, sentence planning and some surface realization. Elhadad, Kan, Klavans, and McKeown (2005) applied group models (physician,
lay person) and individual user models to the task of summarizing medical information.
417

Walker, Stent, Mairesse, & Prasad

McCoy (1989) used context information to design helpful system-generated corrections.
Other work has looked at the use of statistical techniques for adapting content selection
and content ordering methods to particular domains (Barzilay, Elhadad, & McKeown, 2002;
Duboue & McKeown, 2003; Lapata, 2003), but not to individual users.
2.2 Adaptation in Sentence Planning
The first trainable sentence planner was SPoT, a precursor to SPaRKy that output information gathering utterances in the travel domain (Walker, Rambow, & Rogati, 2002).
Evaluations of SPoT demonstrated that it performed as well as a template-based generator
developed for the travel domain and field-tested in the DARPA Communicator evaluations
(Rambow, Rogati, & Walker, 2001; Walker et al., 2002). Information gathering utterances
are considerably simpler than information presentations: they do not usually exhibit any
complexities in rhetorical structure, and there is little interaction between domain-specific
content items and sentence structures. Thus the SPoT generator did not produce utterances with variation in rhetorical structure; it learned to optimize speech-act ordering and
sentence structure choices, but it did not adapt to individuals.
2.3 Adaptation in Surface Realization
Work on adaptation in surface realization has mainly focused on decisions such as lexical and
syntactic choice, using models of a target text, but not individual text models, although
recent research has also shown that n-gram models trained on user-specific corpora can
adapt generators to reproduce individualized lexical and syntactic choices (Lin, 2006; Belz,
2005). Paiva and Evans (2004) present a technique for training a generator by learning the
relationship between particular generation decisions and text variables that can be measured
in the output corpus. This technique was applied to generator decisions such as the form
of referring expression and syntactic structure, and was used to capture stylistic, rather
than individual, differences. Gupta and Stent (2005) use discourse context and speaker
knowledge for referring expression generation in dialogue.
User models have also been used to adapt surface realization. The approach of learning
a ranking from user feedback has been applied to multimedia presentation planning (Stent
& Guo, 2005) and to the joint optimization of the syntactic realizer and the text-to-speech
engine (Nakatsu & White, 2006). This work does not look at individual differences.
Research has also focused on other factors that affect stylistic variation – how realization
choices reflect personality, politeness, emotion or domain specific style (Hovy, 1987; DiMarco
& Foster, 1997; Walker, Cahn, & Whittaker, 1997; André, Rist, van Mulken, Klesen, &
Baldes, 2000; Bouayad-Agha, Scott, & Power, 2000; Fleischman & Hovy, 2002; Piwek,
2003; Porayska-Pomsta & Mellish, 2004; Isard, Brockmann, & Oberlander, 2006; Gupta,
Walker, & Romano, 2007; Mairesse & Walker, 2007). None of this work has attempted to
reproduce individual stylistic variation.

418

Individual and Domain Adaptation in Dialogue

3. Overview of MATCH’s Spoken Language Generator

Dialog Manager
Communicative
goal

SPUR text planner

Content
plan
SPaRKy
Text−plan trees
(tp−trees)

Sentence plan
generator

Pairs of
(sentence plan [sp−tree],
dependency tree [d−tree])

Template−based
generator

Sentence plan
ranker

Text

Ranked list of
(sp−tree, d−tree) pairs

RealPro surface realizer

Text

Figure 2: Architecture of MATCH’s Spoken Language Generator.

MATCH (Multimodal Access To City Help) is a multimodal dialogue system for finding
restaurants and entertainment options in New York City (Johnston, Bangalore, Vasireddy,
Stent, Ehlen, Walker, Whittaker, & Maloor, 2002). Information presentations in MATCH
include route descriptions, as well as user-tailored recommendations and comparisons of
restaurants. Figure 2 shows MATCH’s architecture for spoken language generation (SLG).
The content planning module is the SPUR text planner (Section 3.1) (Walker et al., 2004).
There are two modules for producing text or spoken dialogue responses from SPUR’s output: a highly engineered domain-specific template-based realizer (Section 3.2); and the
SPaRKy sentence planner followed by the RealPro surface realizer (Lavoie & Rambow,
1997) (Section 3.3). Example template-based and SPaRKy outputs for each dialogue strategy are in Figure 3. Both SPUR and SPaRKy are trainable, and produce different output
depending on the user and discourse context.
419

Walker, Stent, Mairesse, & Prasad

Strategy
recommend

System
Template

recommend

SPaRKy

compare-2

Template

compare-2

SPaRKy

compare-3

Template

compare-3

SPaRKy

Realization
Caffe Cielo has the best overall value among the selected
restaurants. Caffe Cielo has good decor and good service.
It’s an Italian restaurant.
Caffe Cielo, which is an Italian restaurant, with good decor
and good service, has the best overall quality among the
selected restaurants.
Caffe Buon Gusto’s an Italian restaurant. On the other
hand, John’s Pizzeria’s an Italian, Pizza restaurant.
Caffe Buon Gusto is an Italian restaurant, and John’s
Pizzeria is an Italian , Pizza restaurant.
Among the selected restaurants, the following offer exceptional overall value. Uguale’s price is 33 dollars. It has
good decor and very good service. It’s a French, Italian
restaurant. Da Andrea’s price is 28 dollars. It has good
decor and very good service. It’s an Italian restaurant.
John’s Pizzeria’s price is 20 dollars. It has mediocre decor
and decent service. It’s an Italian, Pizza restaurant.
Da Andrea, Uguale, and John’s Pizzeria offer exceptional
value among the selected restaurants. Da Andrea is an Italian restaurant, with very good service, it has good decor,
and its price is 28 dollars. John’s Pizzeria is an Italian ,
Pizza restaurant. It has decent service. It has mediocre
decor. Its price is 20 dollars. Uguale is a French, Italian
restaurant, with very good service. It has good decor, and
its price is 33 dollars.

AVG
4

4

2
4
4.5

4

Figure 3: Template outputs and a sample SPaRKy output for each dialogue strategy. AVG
= Averaged score of two human users.

3.1 SPUR
The input to SPUR is a high-level communicative goal from the MATCH dialogue
manager and its output is a content plan for a recommendation or comparison. SPUR
selects and organizes the content to be communicated based on the communicative goal,
a conciseness parameter, and a decision-theoretic user model. It produces targeted recommendations and comparisons: the restaurants mentioned and the attributes selected for
each restaurant are those the user model predicts the user will want to know about. Thus
SPUR can produce a wide variety of content plans.
Figure 4 shows a sample content plan for a recommendation. This content plan gives rise
to the alternate realizations for recommendations for Chanpen Thai in Figure 1. Following
a bottom-up approach to text-planning (Marcu, 1997; Mellish, O’Donnell, Oberlander, &
Knott, 1998), each content plan consists of a set of assertions that must be communicated
to the user and a set of rhetorical relations that hold between those assertions that may be
communicated as well. Each rhetorical relation designates one or more facts as the nuclei
of the relation, i.e. the main point, and the other facts as satellites, i.e. the supplementary
facts (Mann & Thompson, 1987). Three rhetorical relations (Mann & Thompson, 1987) are
used by SPUR: the justify relation for the recommendation strategy, and the contrast
and elaboration relations for the comparison strategies. The relations in Figure 4 specify
that the nucleus (1) is the claim being made in the recommendation, and that the satellites
(assertions 2 to 5) provide justifying evidence for the claim.
420

Individual and Domain Adaptation in Dialogue

relations:justify(nuc:1, sat:2); justify (nuc:1, sat:3 ); justify(nuc:1, sat:4);
justify(nuc:1, sat:5)
content: 1. assert(best (Chanpen Thai))
2. assert(is (Chanpen Tai, cuisine(Thai)))
3. assert(has-att(Chanpen Thai, food-quality(good)))
4. assert(has-att(Chanpen Thai, service(good)))
5. assert(is (Chanpen Thai, price(24 dollars)))
Figure 4: A content plan for a recommendation.

3.2 Template-Based Generator
In order to produce utterances from the content plans produced by SPUR, we first implemented and evaluated a template-based generator for MATCH (Stent, Walker, Whittaker,
& Maloor, 2002; Walker et al., 2004). The template-based generator was designed to
make it possible to evaluate algorithms for user-specific content selection based on SPUR’s
decision-theoretic user model. It performs sentence planning, including some discourse cue
insertion, clause combining and referring expression generation. It produces one high quality output for any content plan for our three dialogue strategies: recommend, compare-2
and compare-3. Recommendations and comparisons are one form of evaluative argument,
so its realization strategies are based on guidelines from argumentation theory for producing
effective evaluative arguments, as summarized by Carenini and Moore (2000). Because the
templates are highly tailored to this domain, the template-based generator can be expected
to perform well in comparison to SPaRKy.
Following the argumentation guidelines, the template-based generator realizes recommendations with the nucleus ordered first, followed by the satellites. The satellites are
ordered to maximize the opportunity for aggregation. To produce the most concise recommendations given the content to be communicated, phrases with identical verbs and subjects
are grouped, so that lists and coordination can be used to aggregrate the assertions about
the subject. Figure 5 provides examples of aggregration as the number of assertions varies
according to SPUR’s conciseness parameter (Z-value).
The realization template for comparisons focuses on communicating both the elaboration
and the contrast relations. Figure 6 contains a content plan for comparisons. The nucleus
is the assertion (1) that Above and Carmine’s are exceptional restaurants. The satellites
(assertions 2 to 7 representing the selected attributes for each restaurant) elaborate on the
claim in the nucleus (assertion 1). Contrast relations hold between assertions 2 and 3,
between 4 and 5, and between 6 and 7. One way to communicate the elaboration relation
is to structure the comparison so that all the satellites are grouped together, following the
nucleus. To communicate the contrast relation, the satellites are produced in a fixed order,
with a parallel structure maintained across options (Prevost, 1995; Prince, 1985). The
satellites are initially ordered in terms of their evidential strength, but then are reordered
to allow for aggregation. Figure 7 illustrates aggregation for comparisons with varying
numbers of assertions.
421

Walker, Stent, Mairesse, & Prasad

Z
1.5
0.7
0.3
-0.5
-0.7

-1.5

Output
Komodo has the best overall value among the selected restaurants. Komodo’s a Japanese,
Latin American restaurant.
Komodo has the best overall value among the selected restaurants. Komodo’s a Japanese,
Latin American restaurant.
Komodo has the best overall value among the selected restaurants. Komodo’s price is
$29. It’s a Japanese, Latin American restaurant.
Komodo has the best overall value among the selected restaurants. Komodo’s price is
$29 and it has very good service. It’s a Japanese, Latin American restaurant.
Komodo has the best overall value among the selected restaurants. Komodo’s price is
$29 and it has very good service and very good food quality. It’s a Japanese, Latin
American restaurant.
Komodo has the best overall value among the selected restaurants. Komodo’s price is
$29 and it has very good service, very good food quality and good decor. It’s a Japanese,
Latin American restaurant.

Figure 5: Recommendations for the East Village Japanese Task, for different settings of the
conciseness parameter Z.

strategy: compare3
items:
Above, Carmine’s
relations: elaboration(nuc:1,sat:2);
elaboration(nuc:1,sat:3);
elaboration(nuc:1,sat:4);
elaboration(nuc:1,sat:5);
elaboration(nuc:1,sat:6); elaboration(nuc:1,sat:7); contrast(nuc:2,nuc:3);
contrast(nuc:4,nuc:5); contrast(nuc:6,nuc:7)
content: 1. assert(exceptional(Above,Carmine’s))
2. assert(has-att(Above, decor(good)))
3. assert(has-att(Carmine’s, decor(decent)))
4. assert(has-att(Above, service(good)))
5. assert(has-att(Carmine’s, service(good)))
6. assert(has-att(Above, cuisine(New American)))
7. assert(has-att(Carmine’s, cuisine(Italian)))
Figure 6: A content plan for a comparison.

3.3 SPaRKy
Like the template-based generator, SPaRKy takes as input any of the content plans produced by SPUR. Figure 2 shows that SPaRKy has two modules: the sentence plan generator (SPG), and the sentence plan ranker (SPR). The SPG uses a set of clause-combining
operations (Figure 12); it produces a large set of alternative realizations of an input content
plan (See Figure 1). The SPR ranks the alternative realizations using a model learned from
users’ ratings of a training set of content plans. The SPG is described in Section 4. The
features used to train the SPR are described in Section 5; the procedure for training the
SPR is described in Section 6.
Because SPaRKy is trained using user feedback, rather than being handcrafted, it can
be trained to be an individualized spoken language generator. As discussed above, the
422

Individual and Domain Adaptation in Dialogue

Z
1.5
0.7
0.3

-0.5

-0.7

-1.5

Output
Among the selected restaurants, the following offer exceptional overall value. Komodo
has very good service.
Among the selected restaurants, the following offer exceptional overall value. Komodo
has very good service and good decor.
Among the selected restaurants, the following offer exceptional overall value. Komodo’s
price is $29. It has very good food quality, very good service and good decor. Takahachi’s
price is $27. It has very good food quality, good service and decent decor.
Among the selected restaurants, the following offer exceptional overall value. Komodo’s
price is $29. It has very good food quality, very good service and good decor. Takahachi’s
price is $27. It has very good food quality, good service and decent decor. Japonica’s
price is$37. It has excellent food quality, good service and decent decor
Among the selected restaurants, the following offer exceptional overall value. Komodo’s
price is $29. It has very good food quality, very good service and good decor. Takahachi’s
price is $27. It has very good food quality, good service and decent decor. Japonica’s
price is $37. It has excellent food quality, good service and decent decor. Shabu-Tatsu’s
price is $31. It has very good food quality, good service and decent decor.
Among the selected restaurants, the following offer exceptional overall value. Komodo’s
price is $29. It has very good food quality, very good service and good decor. Takahachi’s
price is $27. It has very good food quality, good service and decent decor. Japonica’s
price is $37. It has excellent food quality, good service and decent decor. Shabu-Tatsu’s
price is $31. It has very good food quality, good service and decent decor. Bond Street’s
price is $51. It has excellent food quality, good service and very good decor. Dojo’s price
is $14. It has decent food quality, mediocre service and mediocre decor.

Figure 7: Comparisons for the East Village Japanese Task, for different settings of the
conciseness parameter Z.

feedback from the two users in Figure 1 suggests that each user has different perceptions
as to the quality of the potential realizations. A significant part of Sections 7 and 8
are dedicated to examining the differences between a model trained on averaged feedback,
shown as AVG in Figure 1, and those trained on individual feedback from users A and B.

4. Sentence Plan Generation
The input to SPaRKy’s SPG is a content plan from SPUR. Content plans for a
sample recommendation and comparison were in Figure 4 and Figure 6. Figure 1 shows
alternative SPaRKy realizations for the recommendation in Figure 4, while Figure 8 shows
alternative SPaRKy realizations for the comparison in Figure 6. Content plans specify
which assertions to include in an information presentation, and the rhetorical relations
holding between them, but not the order of assertions or how to express the rhetorical
relations between them. This task is known as discourse planning. The SPG has two stages
of processing; first it does discourse planning, and then it does sentence planning.
4.1 Discourse Planning
Discourse planning algorithms can be characterized as: schema-based (McKeown, 1985;
Kittredge, Korelsky, & Rambow, 1991); top-down algorithms using plan operators (Moore
& Paris, 1993); or bottom-up approaches that use, for example, constraint satisfaction
algorithms (Marcu, 1996, 1997) or genetic algorithms (Mellish, O’Donnell, Oberlander, &
423

Walker, Stent, Mairesse, & Prasad

Alt Realization
11 Above and Carmine’s offer exceptional value among the selected restaurants.
Above, which is a New American restaurant, with good decor, has good service.
Carmine’s, which is an Italian restaurant, with good service, has decent decor.
12 Above and Carmine’s offer exceptional value among the selected restaurants.
Above has good decor, and Carmine’s has decent decor. Above and Carmine’s
have good service. Above is a New American restaurant. On the other hand,
Carmine’s is an Italian restaurant.
13 Above and Carmine’s offer exceptional value among the selected restaurants.
Above is a New American restaurant. It has good decor. It has good service.
Carmine’s, which is an Italian restaurant, has decent decor and good service.
14 Above and Carmine’s offer exceptional value among the selected restaurants.
Above has good decor while Carmine’s has decent decor, and Above and
Carmine’s have good service. Above is a New American restaurant while
Carmine’s is an Italian restaurant.
20 Above and Carmine’s offer exceptional value among the selected restaurants.
Carmine’s has decent decor but Above has good decor, and Carmine’s and
Above have good service. Carmine’s is an Italian restaurant. Above, however,
is a New American restaurant.
25 Above and Carmine’s offer exceptional value among the selected restaurants.
Above has good decor. Carmine’s is an Italian restaurant. Above has good
service. Carmine’s has decent decor. Above is a New American restaurant.
Carmine’s has good service.

A
2

B
2

AVG
2

3

2

2.5

3

3

3

4

5

4.5

2

3

2.5

NR

NR

NR

Figure 8: Some alternative realizations for the compare-3 plan in Figure 6, with feedback
from Users A and B, and the mean (AVG) of their feedback (1=worst and 5=best).
NR = Not generated or ranked.
justify

infer

nucleus: <1>
assert−reco−
best

satellite: <2>
assert−reco−
cuisine

satellite: <3>
assert−reco−
food−quality

satellite: <4>
assert−reco−
service

satellite: <5>
assert−reco−
price

Figure 9: A tp-tree for the plan of Figure 4, used to generate Alternatives 1, 3, 4, 5, 6, 7
and 10 in Figure 1.

Knott, 1998). In SPaRKy, the SPG takes a bottom-up approach to discourse planning
using principles from Centering Theory (Grosz, Joshi, & Weinstein, 1995). Content items
are grouped because they talk about the same thing, but the linear order between and
among the groupings is left unspecified. The centering constraints have the result that
Alt-25 in Figure 8, which repeatedly changes the discourse center, are never generated.
The discourse planning stage produces one or more text-plan trees (tp-trees). A tp-tree
for the recommend plan in Figure 4 is in Figure 9, and tp-trees for the compare-3 plan in
Figure 6 are in Figure 10. In a tp-tree, each leaf represents a single assertion and is labeled
424

Individual and Domain Adaptation in Dialogue

elaboration
infer

nucleus:<1>assert-com-list_exceptional

contrast

contrast

nucleus:<4>assert-com-service

nucleus:<2>assert-com-decor

contrast
nucleus:<6>assert-com-cuisine

nucleus:<5>assert-com-service

nucleus:<3>assert-com-decor

nucleus:<7>assert-com-cuisine

elaboration
nucleus:<1>assert-com-list_exceptional

contrast
infer

infer
nucleus:<2>assert-com-decor

nucleus:<3>assert-com-decor
nucleus:<7>assert-com-cuisine
nucleus:<5>assert-com-service

nucleus:<6>assert-com-cuisine

nucleus:<4>assert-com-service

Figure 10: Tp-trees for the comparisons shown as alternatives 12 and 14 (top) and alternatives 11 and 13 (bottom) in Figure 8.

with a speech act. Interior nodes are labeled with rhetorical relations. In addition to the
rhetorical relations in the content plan, the SPG uses the relation infer for combinations
of speech acts for which there is no rhetorical relation expressed in the content plan (Marcu,
1997). The infer relation is similar to the joint relation in RST; it joins multiple satellites
in a mononuclear relation or the nuclei in a multinuclear relation.
Each simple assertion, or leaf, in a tp-tree is associated with one or more syntactic
realizations (d-trees), using a dependency tree representation, called DSyntS (Figure 11)
(Melčuk, 1988; Lavoie & Rambow, 1997). The association between the simple assertions
and any potential d-trees specifying their syntactic realizations is specified in a hand-crafted
generation dictionary. Leaves of some d-trees in the generation dictionary are variables,
which are instantiated from the content plan, e.g. Thai replaces a cuisine type variable.
4.2 Sentence Planning
During sentence planning, the SPG assigns assertions to sentences, orders the sentences,
inserts discourse cues, and performs referring expression generation. It uses a set of clausecombining operations that operate on tp-trees and incrementally transform the elementary
d-trees associated with their leaves into a single lexico-structural representation. The output
from this process is two parallel structures: (1) a sentence plan tree (sp-tree), a binary tree
with leaves labeled with the assertions from the input tp-tree, and interior nodes labeled with
clause-combining operations; and (2) one or more d-trees which reflect parallel operations
on the predicate-argument representations.
The clause-combining operations are general operations similar to aggregation operations used in other research (Rambow & Korelsky, 1992; Danlos, 2000). The operations and
425

Walker, Stent, Mairesse, & Prasad

assert-com-cuisine

assert-com-food quality

BE3 [class:verb ]
(
I Chanpen Thai [number:sg class:proper noun article:no-art person:3rd ]
II restaurant [class:common noun article:indef ]
(
Thai [class:adjective ]
)
)
HAVE1 [class:verb ]
(
I Chanpen Thai [number:sg class:proper noun article:no-art person:3rd ]
II quality [class:common noun article:no-art ]
(
ATTR good [class:adjective ]
ATTR food [class:common noun ]
)
)

Figure 11: Example d-trees from the generation dictionary used by the SPG.
examples of their use are given in Figure 12. They are applied in a bottom-up left-to-right
fashion, with the choice of operation constrained by the rhetorical relation holding between
the assertions to be combined (Scott & de Souza, 1990), as specified in Figure 12.
In addition to ordering assertions, a clause-combining operation may insert cue words
between assertions. Figure 13 gives the list of cue words used by the SPG. The choice of
cue-word is determined by the type of rhetorical relation2 .
The SPG generates a random sample of possible sp-trees for each tp-tree, up to a prespecified number of sp-trees, by randomly selecting among the clause-combining operations
according to a probability distribution that favors preferred operations. Table 14 shows the
probability distribution used in our experiments, which is hand-crafted based on assumed
preferences for operations such as merge, relative-clause and with-reduction, and
is one way in which some knowledge can be injected into the random process to bias it
towards producing higher quality sentence plans.3
The SPG handles referring expression generation by converting a proper name to a pronoun when the same proper name appears in the previous utterance. Referring expression
generation rules are applied locally, across adjacent utterances, rather than globally across
the entire presentation at once (Brennan, Friedman, & Pollard, 1987). Referring expressions
are manipulated in the d-trees, either intrasententially during the incremental creation of
the sp-tree, or intersententially, if the full sp-tree contains any period operations. The
2. An alternative approach is for the cue-word to impose a constraint on the rhetorical relation that must
hold (Webber, Knott, Stone, & Joshi, 1999; Forbes, Miltsakaki, Prasad, Sarkar, Joshi, & Webber, 2003).
3. This probability distribution could be learned from a corpus (Marcu, 1997; Prasad, Joshi, Dinesh, Lee,
& Miltsakaki, 2005).
4. If an infer relation holds and both clauses contain the have possession predicate, the second clause is
arbitrarily selected for reduction. If a justify relation holds, it is the satellite of the RST relation that
always undergoes reduction, if the syntactic constraints are satisfied.
5. If an infer relation holds, any clause is arbitrarily selected for reduction. If a justify relation holds, the
clause that undergoes relative clause formation is the satellite clause. This is motivated by the fact that
relative clause formation is generally seen to occur when the modifying relative clause provides additional
information about the noun it modifies, but where the additional/elaborated information does not have
the same informational status as the information in the main clause.

426

Individual and Domain Adaptation in Dialogue

Operation

Rel

Description

Merge

infer
or
contrast

Withreduction

justify or
infer

Relativeclause

justify or
infer

Two clauses can be combined if
they have identical matrix verbs
and identical arguments and adjuncts except one.
The nonidentical arguments are coordinated.
Two clauses with identical subject arguments can be identified
if one of the clauses has a havepossession matrix verb.
The
possession clause undergoes withparticipial clause formation and
is attached to the non-reduced
clause.4
Two clauses with an identical
subject can be identified. One
clause is attached to the subject
of the other clause as a relative
clause.5

justify, inCuefer or conwordconjunction trast

Cuewordinsertion
(on
the
other
hand)

contrast

Period

justify,
contrast,
infer
or
elaboration

Two clauses are conjoined with a
cue word (coordinating or subordinating conjunction). The order
of the arguments of the connective is determined by the order of
the nucleus (N) and the satellite
(S), yielding two distinct operations, cue-word-conjunctionns and cue-word-conjunctionsn.
cue-word insertion combines
clauses by inserting a cue word
at the start of the second clause
(Carmine’s is an Italian restaurant. HOWEVER, Above is a
New American restaurant), resulting in two separate sentences.
Two clauses are joined by a period.

Sample 1st
arg
Chanpen
Thai
has
good service.

Sample
2nd arg
Chanpen
Thai
has
good
food
quality.

Result

Chanpen
Thai is a
Thai restaurant.

Chanpen
Thai
has
good
food
quality.

Chanpen Thai is
a Thai restaurant,
with good food
quality.

Chanpen
Thai
has
the
best
overall quality
among
the selected
restaurants.
Chanpen
Thai
has
the
best
overall quality
among
the selected
restaurants.

Chanpen
Thai
is
located
in
Midtown
West.

Chanpen
Thai,
which is located
in Midtown West,
has the best overall
quality among the
selected
restaurants.
Chanpen Thai has
the best overall
quality among the
selected
restaurants, since it is a
Thai
restaurant,
with good service.

Chanpen
Thai
is
a
Thai
restaurant,
with
good
service.

Chanpen Thai has
good service and
good food quality.

Penang has
very
good
decor.

Baluchi’s
has
mediocre
decor.

Penang has very
good decor.
On
the other hand,
Baluchi’s
has
mediocre decor.

Chanpen
Thai is a
Thai restaurant,
with
good
food
quality.

Chanpen
Thai
has
good
service.

Chanpen Thai is
a Thai restaurant,
with good food
quality.
It has
good service.

Figure 12: Clause combining operations and examples.

third and fourth sentences for Alt 13 in Figure 8 show the conversion of a named restaurant
(Carmine’s) to a pronoun.
The sp-trees for Alts 6 and 8 in Figure 1 are shown in Figs. 15 and 16. Leaf labels are
concise names for assertions in the content plan, e.g. assert-reco-best is the claim (labelled
1) in Figure 4. Because combination operations can switch the order of their arguments,
from satellite before nucleus (SN) to nucleus before satellite (NS), the labels on the interior
nodes indicate whether this occurred, and specify the rhetorical relation that the operation
realizes. These labels keep track of the operations and substitutions used in constructing
the tree and are subsequently used in the tree feature set described in Section 5, one of the
427

Walker, Stent, Mairesse, & Prasad

RST relation
justify
contrast
infer
elaboration

Aggregation operator
with-reduction, relative-clause, cue-word conj. because, cue-word
conj. since, period
merge, cue-word insert. however, cue-word conj. while, cue-word conj.
and, cue-word conj. but, cue-word insert. on the other hand, period
merge, cue-word conj. and, period
period

Figure 13: RST relation constraints on aggregation operators.
Aggregation operator
merge, with-reduction, relative-clause
cue-word conj. because, cue-word conj. since, cue-word conj. while,
cue-word conj. and, cue-word conj. but
cue-word insert. however, cue-word insert. on the other hand
period

Probability
0.80
0.10
0.09
0.01

Figure 14: Probability distribution of aggregation operators. The final operation is randomly chosen from the selected set with a uniform distribution.

feature sets tested for training the SPR. For example, the label at the root of the tree in
Figure 15 (CW-SINCE-NS-justify) specifies that the cw-conjunction operation was
used, with the since cue word, with the nucleus first (NS), to realize the justify relation.
Similarly, the bottom left-most interior node (WITH-NS-infer) indicates that the withreduction operation was used, with the nucleus before the satellite (NS), to realize the
infer relation.
Figure 17 shows a d-tree for the content plan in Figure 4. This d-tree shows that the
SPG treats the period operation as part of the lexico-structural representation for the
d-tree. The d-tree is split into multiple d-trees at these nodes before being sent to RealPro
for surface realization.
Note that a tp-tree can have very different realizations, depending on the operations of
the SPG. For example, the tp-tree in Figure 9 yields both Alt 6 and Alt 2 in Figure 1. Alt
CW−SINCE−NS−justify

assert−reco−
best

CW−CONJUNCTION−infer

WITH−NS−infer

assert−reco−
cuisine

CW−CONJUNCTION−infer

assert−reco− assert−reco−
service
price

assert−reco−
food−quality

Figure 15: Sentence Plan Tree (SP-tree) for Alternative 6 of Figure 1.

428

Individual and Domain Adaptation in Dialogue

PERIOD−justify

assert−reco−
best

PERIOD−infer

WITH−NS−infer

assert−reco−
food−quality

PERIOD−infer

assert−reco−
cuisine

assert−reco−
service

assert−reco−
price

Figure 16: Sentence Plan Tree (SP-tree) for Alternative 8 of Figure 1.
PERIOD_justify

HAVE1
PERIOD_infer
Champen_Thai

PERIOD_infer

Champen_Thai
Champen_Thai

restaurant

best

service

with

dollar
price

quality

good

restaurant

BE3

good
Thai

overall

AMONG1

selected

HAVE1

BE3

quality

24

Champen_Thai’s

food

Figure 17: Dependency tree for alternative 8 in Figure 1.
2 is highly rated, with an average human rating of 4. However, Alt 6 is a poor realization
of this plan, with an average human rating of 2.5.
To summarize, SPaRKy’s SPG transforms an input content plan into a set of alternative
pairs of sentence-plan trees and d-trees. First, assertions in the input content plan are
grouped using principles from centering theory. Second, assertions are assigned to sentences
and discourse cues inserted using clause combining operations. Third, decisions about the
realization of referring expressions are made on the basis of recency. The rhetorical relations
and clause-combining operations are domain-independent.
SPaRKy uses two types of domain-dependent knowledge: the probability distribution
over clause-combining operations, and the d-trees that are input to the RealPro surface
realizer. In order to use SPaRKy in a new domain, it might be necessary to:
429

Walker, Stent, Mairesse, & Prasad

• add new rhetorical relations if the content planner used additional rhetorical relations;
• modify the probability distribution over clause-combining operations, either by hand
or by learning from a corpus;
• construct a new set of d-trees to capture the syntactic structure of sentences in the
domain, unless we used a surface realizer that could take logical forms or semantic
representations as input.

5. Feature Generation
To train or use the SPR, each potential realization generated by the SPG, along with its
corresponding sp-tree and d-tree, is encoded as a set of real-valued features (binary features
are modeled with values 0 and 1) from three feature types:
• N-Gram features – simple word n-gram features generated from the realization of
SPG outputs;
• Concept features – concept n-gram features generated from named entities in the
realization of SPG outputs;
• Tree features – these features represent structural configurations in the sp-trees and
d-trees output by the SPG.
These features are automatically generated as described below.
5.1 N-Gram Features
N-gram features capture information about lexical selection and lexical ordering in the realizations output by SPaRKy. A two-step approach is used to generate these features. First,
a domain-specific rule-based named-entity tagger (using MATCH’s lexicons for restaurant,
cuisine type and location names) replaces specific tokens with their types, e.g. Babbo with
restname. Then, unigram, bigram and trigram features and their counts are automatically
generated. The tokens begin and end indicate the beginning and end of a realization.
N-gram feature names are prefixed with n-gram. For example, ngram-cuisinenamerestaurant-with counts the occurrences of cuisine type followed by “restaurant” and
“with” (as in the realization “Italian restaurant with”); ngram-begin-restname-which
counts occurrences of realizations starting with a restaurant’s name followed by “which”.
We also count words per presentation, and per sentence in a presentation.
5.2 Concept Features
Concept features capture information about the concepts selected for a presentation, and
their linear order in the realization. A two-step approach is used to generate these features.
First, a named-entity tagger marks the names of items in our restaurant database, e.g.
Uguale. Then, unigram, bigram and trigram features and their counts are automatically
generated from the sequences of concepts in the sentence plan for the realization. As with
the n-gram features, the tokens begin and end indicate the beginning and end of a realization.
430

Individual and Domain Adaptation in Dialogue

Concept feature names are prefixed with conc. For example, conc-decor-claim is
set to 1 if the claim is expressed directly after information about decor, while the feature
conc-begin-service characterizes utterances starting with information about service. In
the concept n-gram features, we use ’*’ to separate individual features. We also count
concepts per presentation, and per sentence in a presentation.
5.3 Tree Features
Tree features capture declaratively the way in which merge, infer and cue-word operations are applied to the tp-trees, and were inspired by the parsing features used by Collins
(2000). They count the occurrences of certain structural linguistic configurations in the
sp-trees and associated d-trees that the SPG generated. Tree feature names are prefixed
with r for “rule” (sp-tree) or s for “sentence” (d-tree).
Several feature templates are used to generate tree features. Local feature templates
record structural configurations local to a particular node (its ancestors, daughters etc.);
global feature templates, used only for sp-tree features, record properties of the entire sp-tree.
There are four types of local feature template: traversal features, sister features, ancestor
features and leaf features. Traversal, sister and ancestor features are generated for all nodes
in sp-trees and d-trees; leaf features are generated for sp-trees only. The value of each
feature is the count of the described configuration in the tree. We discard features that
occur fewer than 10 times to avoid those specific to particular content plans.
For each node in the tree, traversal features record the preorder traversal of the
subtree rooted at that node, for all subtrees of all depths. Feature names are the concatenation of the prefix trav-, with the names of the nodes (starting with the current
node) on the traversal path. ’*’ is used to separate node names. An example is r-travwith-ns-infer*assert-reco-food-quality*assert-reco-cuisine (with value 1) of the
bottom-left subtree in Figure 16.
Sister features record all consecutive sister nodes. Names are the concatenation of
the prefix sis-, with the names of the sister nodes. An example is r-sis-assert-recobest*cw-conjunction-infer (with value 1) of the tree in Figure 15.
For each node in the tree, ancestor features record all the initial subpaths of the path
from that node to the root. Feature names are the concatenation of the prefix anc- with the
names of the nodes (starting with the current node). An example is r-anc-assert-recocuisine*with-ns-infer*cw-conjunction-infer (with value 1) of the tree in Figure 15.
Leaf features record all initial substrings of the frontier of the sp-tree. Names are the
concatenation of the prefix leaf-, with the names of the frontier nodes (starting with the
current node). For example, the sp-tree of Figure 15 has value 1 for leaf-assert-recobest and also for leaf-assert-reco-best*leaf-assert-reco-cuisine, and the sp-tree
of Figure 16 has value 1 for leaf-assert-reco-food-quality*assert-reco-cuisine.
Global features apply only to the sp-tree. They record, for each sp-tree and for each
operation labeling a non-frontier node, (1) the minimal number of leaves dominated by a
node labeled with that rule in that tree (MIN); (2) the maximal number of leaves dominated
by a node labeled with that rule (MAX); and (3) the average number of leaves dominated
by a node labeled with that rule (AVG). For example, the sp-tree in Figure 15 has value
431

Walker, Stent, Mairesse, & Prasad

4 for cw-conjunction-infer-max, value 2 for cw-conjunction-infer-min and value 3
for cw-conjunction-infer-avg.

6. Training the Sentence Plan Ranker
The SPR ranks alternative information presentations using a model learned from user ratings of a set of training data. The training procedure is as follows:
• For each content plan in the training data, the SPG generates a set of alternative
sentence plans using a random selection of sentence planning operators (Section 4);
• Features are automatically generated from the surface realizations and sentence plans
so that each alternative sentence plan is represented in terms of a number of realvalued features (Section 5);
• Feedback as to the perceived quality of the realization of each alternative sentence
plan is collected from one or more users;
• The RankBoost boosting method (Freund, Iyer, Schapire, & Singer, 1998) learns a
function from the featural representation of each realization to its feedback, that
attempts to duplicate the rankings in the training examples.
We use RankBoost for three reasons. First, it produces a ranking over the input alternatives rather than a selection of one best alternative. Second, it can handle many sparse
features. Third, the function that it learns is a rule-based model showing the effect of
each feature on the ranking of the competing examples. These models can be inspected
and compared. This allows us to qualitatively analyze the models (Section 8) in order to
understand the preferences of individuals, and the differences between SPRs for individuals
vs. groups.
This section describes the training of the SPR in detail. The SPUR content planner
produces content plans for three dialogue strategies:
• recommend: recommend an entity from a set of entities
• compare-2: compare two entities
• compare-3: compare three or more entities
For each dialogue strategy, we start with a set of 30 representative content plans from
SPUR. The SPG was parameterized to produced up to 20 distinct (sp-tree, d-tree) pairs
for each content plan. Each of these was realized by RealPro. Separately, we also obtained
output for each content plan from our template-based generator (Section 3.2).
Both the SPaRKy realizations and the template-based realizations were randomly ordered and placed on a series of Web pages. These 1830 realizations were then rated on a
scale from 1 to 5 by the first two authors of this paper, neither of whom had implemented
the template-based realizer or the SPG. The raters worked on this rating task during sessions of one hour at a time for several hours a day, over a period of a week. They were
instructed to look at all 21 realizations for a particular content plan before rating any of
them, to try to use the whole rating scale, and to indicate their spontaneous rating without
432

Individual and Domain Adaptation in Dialogue

repeatedly re-labelling the alternative realizations. They did not discuss their ratings or
the basis for their ratings at any time. Given the cognitive load and long duration of this
rating task, it was impossible for the raters to keep track of which realizations came from
SPaRKy and which from the template-based generator, and likely to be impossible to do
more than generate a “gestalt” evaluation of each alternative.
Each (sp-tree, d-tree, realization) triple is an example input for RankBoost; the ratings
are used as feedback. The experiments below examine two uses of the ratings. First, we
train and test an SPR with the average of the ratings of the two users, i.e. we consider
the two users as representing a single user group. Second, we train and test individualized
SPRs, one for each user.
The SPR is trained using the RankBoost algorithm (Freund, Iyer, Schapire, & Singer,
1998), which we describe briefly here. First, the training corpus is converted into a set T
of ordered pairs of examples x, y:
T = {(x, y)| x, y are alternatives for the same plan,
x is preferred to y by user ratings}
Each alternative realization x is represented by a set of m indicator functions hs (x)
for 1 ≤ s ≤ m. The indicator functions are calculated by thresholding the feature values
(counts) described in Section 5. For example, one indicator function is:
h100 (x) =



1 if leaf-assert-reco-best(x) ≥ 1
0 otherwise

So h100 (x) = 1 if the leftmost leaf is the assertion of the claim as in Figure 15. A single
parameter αs is associated with each indicator function, and the “ranking score” for an
example x is calculated as
X
αs hs (x)
F (x) =
s

This score is used to rank competing sp-trees of the same content plan with the goal of
duplicating the ranking found in the training data. Training is the process of setting the
parameters αs to minimize the following loss function:
RankLoss =

1 X
eval(F (x) ≤ F (y))
|T | (x,y)∈T

The eval function returns 1 if the ranking scores of the (x, y) pair are misordered (so that x
is ranked higher than y even though in the training data y is ranked higher than x), and 0
otherwise. In other words, the RankLoss is the percentage of misordered pairs. As this loss
function is minimized, the ranking errors (cases where ranking scores disagree with human
judgments) are reduced. Initially all parameter values are set to zero. The optimization
method then greedily picks a single parameter at a time – the parameter which will make
the most impact on the loss function – and updates the parameter value to minimize the
loss.
In the experiments described below, we use two evaluation metrics:
433

Walker, Stent, Mairesse, & Prasad

• RankLoss: The value of the training method’s loss function;
• TopRank: The difference between the human rating of the top realization for each
content plan and the human rating of the realization that the SPR predicts to be the
top ranked.

7. Quantitative Results
In this section, we describe three experiments with SPaRKy:
1. Feature sets for trainable sentence planning: We examine which features (ngram, concept, tree, all) lead to the best performance for the sentence planning task,
and find that n-gram features sometimes perform as well as all the features.
2. Comparison with template-based generation: We show that the performance of
a trainable sentence planner using the best performing feature set is more consistent
than that of a template-based generator, although overall a template-based generator
still performs better.
3. Individualized sentence planners: We show that people have quite specific individual preferences regarding the three tasks of sentence planning: information ordering, sentence aggregation, and use of discourse cues; and furthermore, that a trainable
sentence planner can model these individual preferences. Moreover we show that in
some cases the individualized sentence planners are better than, or statistically indistinguishable from, the template-based generator.
We report results below separately for comparisons between two entities and among three
or more entities. These two types of comparison are generated using different strategies in
the SPG, and produce text that is very different both in terms of length and structure.
7.1 Feature Sets for Trainable Sentence Planning
Using a cross-validation methodology, we repeatedly train the SPR on a random 90% of
the corpus, and test on the remaining 10%. Here, we use the averaged feedback from user
A and user B as feedback. Figure 18 repeats the examples in Figure 1, here showing both
the user rankings and the rankings for a ranking function that was learned by the trained
SPRs for both users A and B and for the AVG user.
Table 1 shows RankLoss for each feature set (Section 5). Paired t-tests comparing the
ranking loss for different feature sets show surprisingly few performance differences among
the features. Using all the features (All) always produces the best results, but the differences
are not always significant.
The n-gram features give results comparable to all the features for both compare-2
and recommend. An analysis of the learned models suggests that one reason that ngram features perform well is because there are individual lexical items that are uniquely
associated with many of the combination operators, such as the lexical item with for the
with-ns operator. This means that the detailed representations of the content and structure
of an information presentation as represented by the tree features are equivalent to n-gram
features in this application domain.
434

Individual and Domain Adaptation in Dialogue

Alt Realization
6
Chanpen Thai has the best overall quality among the selected restaurants since it is a Thai restaurant, with good
service, its price is 24 dollars, and it has good food quality.
7
Chanpen Thai has the best overall quality among the selected restaurants because it has good service, it has good
food quality, it is a Thai restaurant, and its price is 24
dollars.
4
Chanpen Thai has the best overall quality among the selected restaurants. It has good food quality, with good
service, it is a Thai restaurant, and its price is 24 dollars.
9
Chanpen Thai is a Thai restaurant, with good food quality,
its price is 24 dollars, and it has good service. It has the
best overall quality among the selected restaurants.
5
Chanpen Thai has the best overall quality among the selected restaurants. It has good service. It has good food
quality. Its price is 24 dollars, and it is a Thai restaurant.
3
Chanpen Thai has the best overall quality among the selected restaurants. Its price is 24 dollars. It is a Thai
restaurant, with good service. It has good food quality.
10 Chanpen Thai has the best overall quality among the selected restaurants. It has good food quality. Its price is 24
dollars. It is a Thai restaurant, with good service.
2
Chanpen Thai has the best overall quality among the selected restaurants. Its price is 24 dollars, and it is a Thai
restaurant. It has good food quality and good service.
1
Chanpen Thai has the best overall quality among the selected restaurants. This Thai restaurant has good food
quality. Its price is 24 dollars, and it has good service.
8
Chanpen Thai is a Thai restaurant, with good food quality.
It has good service. Its price is 24 dollars. It has the best
overall quality among the selected restaurants.

A
1

B
4

SPRA
0.16

SPRB
0.65

SPRAV G
0.58

2

5

0.38

0.54

0.42

2

4

0.53

0.62

0.53

2

4

0.47

0.53

0.63

3

2

0.59

0.32

0.46

3

3

0.64

0.40

0.62

3

3

0.67

0.46

0.58

4

4

0.75

0.50

0.74

4

3

0.64

0.52

0.45

4

2

0.81

0.29

0.73

Figure 18: Some alternative realizations for the content plan in Figure 4, with feedback
from users A and B (1=worst and 5=best) and rankings from the trained SPRs
for users A and B and mean(A,B) ([0, 1]).

The concept features always perform worse than all the features, indicating that the
linear ordering of concepts only accounts for some of the variation in rating feedback. For
the two types of comparison, performance using the concept features approaches that of the
other feature sets. However, for recommendations, performance using the concept features
is much worse than that using n-gram features or all the features. In the qualitative analysis
presented in Section 8, we discuss some aspects of the models for recommendations that
might account for this large difference in performance.
Table 2 shows results with all the features using the TopRank evaluation metric, calculated for two-fold cross-validation, to be comparable with previous work (Walker, Rambow,
& Rogati, 2002; Stent, Prasad, & Walker, 2004).6 We evaluated SPaRKy on the test sets by
comparing three data points for each content plan: Human (the score of the best sentence
plan that SPaRKy’s SPG can produce as selected by the human users); SPaRKy (the
score of the SPR’s top-ranked selected sentence); and Random (the score of a sentence plan
6. The TopRank metric is sensitive to the distribution of ranking feedback and SPR scores in the test set,
which means that it is sensitive to the number of cross-validation folds.

435

Walker, Stent, Mairesse, & Prasad

Feature set/Strategy
Random Baseline
Concept
N-Gram
Tree
All

compare-2
0.50
0.16 (p < .000)
0.14 (p < .161)
0.14 (p < .087)
0.13

compare-3
0.50
0.16 (p < .021)
0.15 (p < .035)
0.16 (p < .007)
0.14

recommend
0.50
0.32 (p < .000)
0.21 (p < .197)
0.22 (p < .001)
0.20

Table 1: AVG model’s ranking error with different feature sets, for all strategies. Results
are averaged over 10-fold cross-validation, testing over the mean feedback. p values
in parentheses indicate the level of significance of the decrease in accuracy when
compared to the model using all the features. Cases where different feature sets
perform as well as all the features are marked in bold.

randomly selected from the alternative sentence plans). For all three presentation types,
a paired t-test comparing SPaRKy to Human to Random showed that SPaRKy was significantly better than Random (df = 59, p < .001) and significantly worse than Human
(df = 59, p < .001). The difference between the SPaRKy scores and the Human scores
indicates how much performance could be improved if the SPR were perfect at replicating
the Human ratings.
User
AVG
AVG
AVG

Strategy
recommend
compare-2
compare-3

SPaRKy
3.6 (0.77)
4.0 (0.66)
3.6 (0.68)

Human
3.9 (0.55)
4.4 (0.54)
4.0 (0.49)

Random
2.8 (0.81)
2.8 (1.30)
2.7 (1.20)

Table 2: TopRank scores for recommend, compare-2 and compare-3 (N = 180), using
all the features, for SPaRKy trained on AVG feedback, with standard deviations.

7.2 Comparison with Template Generation
User
AVG
AVG
AVG

Strategy
recommend
compare-2
compare-3

SPaRKy
3.6 (0.59)
3.9 (0.52)
3.4 (0.38)

Human
4.4 (0.37)
4.6 (0.39)
4.6 (0.35)

Template
4.2 (0.74)
3.6 (0.75)
4.1 (1.23)

Table 3: TopRank scores for MATCH’s template-based generator, SPaRKy(AVG) and
Human. N = 180, with standard deviations.

436

Individual and Domain Adaptation in Dialogue

As described above, the raters also rated the single output of the template-based generator for MATCH for each content plan in the training data. Table 3 shows the mean
TopRank scores for the template-based generator’s output (Template), compared to the
best plan the trained SPR selects (SPaRKy), and the best plan as selected by a human
oracle (Human). In each fold, both SPaRKy and the Human oracle select the best of 10
sentence plans for each text plan, while the template-based generator produces a single
output with a single human-rated score. A paired t-test comparing Human with Template
shows that there are no significant differences between them for recommend or compare3, but that Human is significantly better for compare-2 (df = 29, t = 4.8, p < .001). The
users evidently did not like the compare-2 template. A paired t-test comparing SPaRKy
to Template shows that the template-based generator is significantly better for both recommend and compare-3 (df = 29, t = 2.1, p < .05), while there is a trend for SPaRKy
to be better for compare-2 (df = 29, t = 2.0, p = .055).
Also, the standard deviation for Template strategies is wider than for Human or SPaRKy,
indicating that while the template-based generator performs well overall, it performs poorly
on some inputs. One reason for this might be that SPUR’s decision-theoretic user model
selects a wide range and number of content items for different users, and for conciseness
settings (See Figures 5 and 7). This means that it is difficult to handcraft a template-based
generator to handle all the different cases well.
The gap between the Human scores (produced by the SPG but selected by a human
rather than by the SPR) and the Template scores shows that the SPG produces sentence
plans as good as those of the template-based generator, but the accuracy of the SPR needs
to be improved. Below, Section 7.3 shows that when the SPR is trained for individuals,
SPaRKy’s performance is indistinguishable from the template-based generator in most
cases.
7.3 Comparing Individualized Models to Group Models
We discussed in Section 1 that the differences in the rating feedback from users A and B for
competing realizations (See Figure 1) suggest that each user has different perceptions as to
the quality of the potential realizations. To quantify the utility and the feasibility of training
individualized SPRs, we first examine the feasibility of training models for individual users.
The results in Table 1 are based on a corpus of 600 examples, rated by each user,
which may involve too much effort for most users. We would like to know whether a highperforming individualized SPR can be trained from less labelled data. Figure 19 plots
ranking error rates as a function of the amount of training data. This data suggests that
error rates around 0.20 could be acquired with a much smaller training set, i.e. with a
training set of around 120 examples, which is certainly more feasible.
recommend
A’s test data
B’s test data
AVG’s test data

A’s model
0.17
0.52
0.31

B’s model
0.52
0.17
0.31

AVG model
0.29
0.27
0.20

Table 4: Ranking error for various configurations with the recommend strategy.
437

Walker, Stent, Mairesse, & Prasad

0.45
User A
User B
0.4

Average test error

0.35

0.3

0.25

0.2

0.15
0

50

100

150

200

250

300

Number of sentences in the training set

Figure 19: Variation of the testing error for both users as a function of the number of
training utterances.

compare-2
A’s test data
B’s test data
AVG’s test data

A’s model
0.16
0.23
0.17

B’s model
0.26
0.11
0.16

AVG model
0.20
0.13
0.13

Table 5: Ranking error for various configurations with the compare-2 strategy.

compare-3
A’s test data
B’s test data
AVG’s test data

A’s model
0.13
0.26
0.17

B’s model
0.30
0.14
0.20

AVG model
0.18
0.18
0.14

Table 6: Ranking error for various configurations with the compare-3 strategy.
We then examine if trained individualized SPRs are accurate. The results in Tables 4, 5
and 6 show RankLoss for several training and testing configurations for each strategy (using
10-fold cross-validation). We compare the two individualized models with models trained
on A and B’s mean feedback (AVG). For each model, we test on its own test data, and on
test data for the other models. This shows how well a model might ‘fit’ if customizing an
SPR to a new domain or user group. For example, if we train a model for recommendations
438

Individual and Domain Adaptation in Dialogue

using feedback from a group of users, and then deploy this system to an individual user, we
might expect model fit differences similar to those in Table 4.
Of course, there may be strongly conflicting preferences in any group of users. For
example, consider the differences in the ratings for users A and B and the average ratings
in Figure 1. Alt-1 and Alt-7 are equivalent using the average feedback, but user A dislikes
Alt-7 and likes Alt-1 and vice versa for user B. Column 3 of Table 4 shows that the average
model, when used in an SPR for user A or user B has a much higher ranking error (.29
and .27 respectively) than that of an SPR customized to user A (.17 error) or customized
to user B (.17 error).
An examination of Tables 4, 5 and 6 shows that in general, there are striking differences
between models trained and tested on one individual’s feedback (RankLoss ranges from 0.11
to 0.17) and cross-tested models (RankLoss ranges from 0.13 to 0.52). Also, the average
(AVG) models always perform more poorly for both users A and B than individually-tailored
models. As a baseline for comparison, a model ranking sentence alternatives randomly
produces an error rate of 0.5 on average; Table 4 shows that models trained on one user’s
data and tested on the other’s can perform as badly as the random model baseline. This
suggests that the differences in the users’ ratings are not random noise.
In some cases, the average model also performs significantly worse than the individual
models even when tested on feedback from the “average” user (the diagonal in Tables 4, 5
and 6). This suggests that in some cases it is harder to get a good model for the average
user case, possibly because the feedback is more inconsistent. For recommendations, the
performance of each individual model is significantly better than the average model (df = 9,
t = 2.6, p < .02). For compare-2 the average model is better than user A’s (df = 9, t = 2.3,
p < .05), but user B’s model is better than the average model (df = 9, t = 3.1, p < .01).
User
A
A
A
B
B
B

Strategy
recommend
compare-2
compare-3
recommend
compare-2
compare-3

SPaRKy
3.5 (0.87)
3.8 (0.98)
3.1 (1.02)
4.4 (0.70)
4.4 (0.69)
4.4 (0.62)

Human
3.9 (0.61)
4.3 (0.73)
3.6 (0.80)
4.7 (0.46)
4.7 (0.53)
4.8 (0.40)

Template
3.9 (1.05)
4.2 (0.64)
3.9 (1.19)
4.5 (0.76)
3.1 (1.21)
4.2 (1.34)

Table 7: TopRank scores for the Individualized SPaRKy as compared with MATCH’s
template-based generator as rated separately by Users A and B, and individual
User A and User B Human Oracles. Standard Deviations are in parentheses. N =
180.

We can also compare the template-based generator to the individualized SPaRKy generators using the TopRank metric (See Table 7). All comparisons are done with paired
t-tests using the Bonferroni adjustment for multiple comparisons.
For recommend, there are no significant differences between SPaRKy and Template
for User A (df = 59, t = 2.3, p = .07), or for User B (df = 59, t = 1.6, p = .3). There
439

Walker, Stent, Mairesse, & Prasad

are also no significant differences for either user between Template and Human (df = 59,
t < 1.5, p > 0.4).
For compare-2, there are large differences between Users A and B. User A appears to
like the template for compare-2 (average rating is 4.2) while User B does not (average rating is 3.1). For User A, there are no significant differences between SPaRKy and Template
(df = 59, t = 2.3, p = .07), and between Template and Human (df = 59, t = 0.1, p = .09),
but User B strongly prefers SPaRKy to Template (df = 59, t = 7.7, p < .001).
For compare-3, there are also large differences between Users A and B. User A likes the
template for compare-3 (average rating 3.9), and strongly prefers it to the individualized
SPaRKy (average rating 3.1) (df = 59, t = 3.4, p < .004). User B also likes the template
(average rating 4.2), but there are no significant differences with SPaRKy (average rating
4.4) (df = 59, t = 1.0, p = .95).
For both users, and for every strategy, even with individually trained SPRs, there is still
a significant gap between SPaRKy and Human scores, indicating that the performance of
the SPR could be improved (df = 59, t = 3.0, p < .006).
These results demonstrate that trainable sentence planning can produce output comparable to or better than that of a template-based generator, with less programming effort and
more flexibility.

8. Qualitative Analysis
An important aspect of RankBoost is that the learned models are expressed as rules: a qualitative examination of the learned models may highlight individual differences in linguistic
preferences, and help us understand why SPaRKy’s SPG can produce sentence plans that
are better than those produced by the template-based generator, and why the individually
trained SPRs usually select sentence plans that are as good as the templates. To qualitatively compare the learned ranking models for the individualized SPRs, we assess both
which linguistic aspects of an utterance (which features) are important to an individual, and
how important they are. We evaluate whether an individual is oriented towards a particular
feature by examining which features’ indicator functions hs (x) have non-zero values. We
evaluate how important a feature is to an individual by examining the magnitude of the
parameters αs .
There are two potential problems with this approach, The first problem is that the
feature templates produce thousands of features, some of which are redundant, so that
differences in each model’s indicator functions can be spurious. Therefore, to allow more
meaningful qualitative comparisons between models, one of a pair of perfectly correlated
features is eliminated.
The second problem arises from RankBoost’s greedy algorithm. The selection of which
parameter αs to set on any round of boosting is highly dependent on the training set, so
that the models derived from a single episode of training are highly variable. To compare
indicator functions independently of the training set, we adopt a bootstrapping method to
identify a feature set for each user that is independent of a particular training episode. By
repeatedly randomly selecting 10 alternatives for training and 10 for testing for each content
plan, we created 50 different training sets for each user. We then average the α values of the
features selected by RankBoost over these 50 training runs, and conduct experiments using
440

Individual and Domain Adaptation in Dialogue

Model
AVG

A

B

Strategy
recommend
compare-2
compare-3
recommend
compare-2
compare-3
recommend
compare-2
compare-3

Tree
45
37
63
50
35
47
47
45
47

Feature Type
N-Gram Concept Leaf
36
9
7
46
12
1
29
4
1
29
14
4
51
10
3
37
11
1
34
9
6
36
13
1
34
9
6

Global
3
4
3
3
1
4
4
5
4

Table 8: Features in the top 100 with the highest average α for each user model.
only the 100 features for each user with the highest average α magnitude. In Section 8.1 we
discuss differences in the types of feature that are selected by the bootsrapping algorithm
just outlined. Section 8.2 discusses differences in models produced using the tree features
for user A and user B, while section 8.3 discusses differences between the average model
and the individual models.
8.1 Types of Bootstrapped Features
The bootstrapping process selects a total of 100 features for each strategy and for each
type of feedback (individual or averaged). We found differences in the features along both
dimensions.
Table 8 shows the number of features of each type that were in the top 100 (averaged
over 50 training runs). Only 9 features are shared by the three strategies for the AVG
model; these shared features are usually n-gram features. For User A, 6 features are shared
by the three strategies (mostly n-gram features). For User B, there are no features shared
by the three strategies.
We also found that some features capture specific interactions between domain-specific
content items and syntactic structure, which are difficult to model in a rule-based or
template-based generator. An example is Rule (1) in Figure 20 which significantly lowers the ranking of any sentence plan in which neighborhood information (assert-reconbhd) is combined with subsequent content items via the with-ns operation. Among the
bootstrapped features for the average user, 16 features for compare-2 count interactions
between domain-specific content and syntactic structure. For compare-3, 22 features count
such interactions, and the bootstrapped features for recommend include 39 such features.
We examine some of the models derived from these features in detail below.
8.2 Differences in Individual Models
To further analyze individual linguistic preferences for information presentation strategies, we now qualitatively compare the two models for Users A and B. We believe that this
qualitative analysis provides additional evidence that the differences in the users’ ranking
preferences are not random noise. We identify differences among the features selected by
441

Walker, Stent, Mairesse, & Prasad

N
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

Condition
r-anc-assert-reco-nbhd*with-ns-infer ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 3.1
r-anc-assert-reco*with-ns-infer*cw-conjunction-infer ≥ 1
leaf-assert-reco-best*assert-reco-price ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 2.8
r-trav-with-ns-infer*assert*assert ≥ 1
r-anc-cw-conjunction-infer*cw-conjunction-infer ≥ 1
with-ns-infer-min-leaves-under ≥ 1
r-anc-assert-reco*with-ns-infer ≥ 1
cw-conjunction-infer-max-leaves-under ≥ 3.5
r-trav-with-ns-infer*assert-reco*assert-reco ≥ 1
r-anc-assert*with-ns-infer ≥ 1
r-anc-with-ns-infer*relative-clause-infer ≥ 1
r-anc-assert*with-ns-infer*relative-clause-infer ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 4.1
r-anc-assert-reco-cuisine*with-ns-infer*period-infer ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 2.2
r-anc-assert-reco-food-quality*merge-infer ≥ 1
r-anc-assert-reco*merge-infer ≥ 2.5
r-anc-assert-reco-decor*merge-infer ≥ 1
r-anc-assert*merge-infer ≥ 2.5
r-trav-merge-infer ≥ 1.5
r-trav-with-ns-infer*assert-reco-service*assert-reco-food-quality ≥
1
leaf-assert-reco-food-quality*assert-reco-cuisine ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 3.8
leaf-assert-reco-food-quality ≥ 1
s-trav-have1*propernoun-restaurant*II-quality*attr-among1 ≥ 1
s-anc-attr-with*have1 ≥ 1

α
-1.26
-0.58
-0.33
-0.29
-0.27
-0.22
-0.17
-0.13
-0.11
-0.07
-0.07
-0.03
-0.01
-0.01
-0.01
0.10
0.15
0.18
0.20
0.22
0.25
0.27
0.40
0.46
0.46
0.60
0.68
0.71

Figure 20: A subset of rules and corresponding α values of User A’s model, ordered by α.
RankBoost, and their α values, using models derived using bootstrapping over the tree features only, since they are easier to interpret qualitatively. Of course many different models
are possible. User A’s model consists of 109 rules; a subset are in Figure 20. User B’s
model consists of 90 rules, a subset of which are shown in Figure 21. We first consider
how the individual models account for the rating differences for Alt-6 and Alt-8 from Figure 1 (repeated in Figure 18 with ratings from the trained SPRs), and then discuss other
differences.
Comparing Alt-6 and Alt-8: Alt-6 is highly ranked by User B but not by User A.
Alt-6 instantiates Rule 21 of Figure 21, expressing User B’s preferences about linear order
of the content. (Alt-6’s sp-tree is in Figure 15.) Rule 21 increases the rating of examples
in which the claim, i.e. assert-reco-best (Chanpen Thai has the best overall quality), is
realized first. Thus, unlike user A, user B prefers the claim at the beginning of the utterance
(the ordering of the claim is left unspecified by argumentation theory (Carenini & Moore,
2000)). Rule 22 increases the rating of examples in which the initial claim is immediately
442

Individual and Domain Adaptation in Dialogue

N
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24

Condition
r-sis-assert-reco-relative-clause-infer ≥ 1
r-sis-period-infer-assert-reco ≥ 1
r-anc-assert-reco-nbhd*with-ns-infer ≥ 1
r-anc-assert-reco*period-infer*period-infer ≥ 1.5
r-anc-assert-reco-food-quality*with-ns-infer*relative-clause-infer ≥ 1
r-anc-assert-reco-cuisine*with-ns-infer*relative-clause-infer ≥ 1
r-anc-assert-reco*period-infer ≥ 1
leaf-assert-reco-price ≥ 1
r-anc-assert*period-infer*period-infer ≥ 1.5
leaf-assert-reco-decor ≥ 1
r-anc-assert*relative-clause-infer*period-infer ≥ 1.5
r-trav-relative-clause-infer*assert-reco*with-ns-infer ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 3.1
cw-conjunction-infer-avg-leaves-under ≥ 3.3
cw-conjunction-infer-avg-leaves-under ≥ 2.2
r-anc-assert*relative-clause-infer*period-infer ≥ 1
leaf-assert-reco-service ≥ 1
s-trav-attr-with ≥ 1
r-anc-assert-reco-cuisine*with-ns-infer*cw-conjunction-infer ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 3.6
leaf-assert-reco-best ≥ 1
leaf-assert-reco-best*assert-reco-cuisine ≥ 1
cw-conjunction-infer-avg-leaves-under ≥ 2.8
r-trav-with-ns-infer*assert-reco-cuisine*assert-reco-food-quality ≥ 1

α
-1.01
-0.71
-0.50
-0.49
-0.41
-0.39
-0.35
-0.32
-0.26
-0.14
-0.07
-0.05
-0.03
-0.03
-0.01
0.03
0.07
0.18
0.27
0.36
0.47
0.50
0.52
0.76

Figure 21: A subset of rules and corresponding α values of User B’s model, ordered by α.
followed by the type of cuisine (assert-reco-cuisine). These rules interact with Rule 19
in Figure 21, which specifies a preference for information following assert-reco-cuisine
to be combined via the with-ns operation, and then conjoined (cw-conjunction-infer)
with additional evidence. Alt-6 also instantiates Rule 23 in User B’s model, with an α value
of .52 associated with multiple uses of the cw-conjunction-infer operation.
User A’s low rating of Alt-6 arises from A’s dislike of the with-ns operation (Rules 3,
8, 9, 11 and 12) and the cw-conjunction-infer operation (Rules 3, 5, 7, 10 and 15) in
Figure 20. (Contrast User B’s Rule 23 with User A’s Rules 5 and 17.) Alt-6 also fails to
instantiate A’s preference for food quality and cuisine information to occur first (Rules 24
and 26). Finally, user A also prefers the claim assert-reco-best to be realized in its own
sentence (Rule 27).
By contrast, Alt-8 is rated highly by User A but not by User B (see Figure 1). Even
though Alt-8 instantiates the negatively evaluated with-ns operation (Rules 3, 8, 9 and 11
in Figure 20), there are no instances of cw-conjunction-infer (Rules 3, 5, 7, 10 and 15).
Moreover Alt-8 follows A’s ordering preferences (Rules 24 and 26) which describe sp-trees
with assert-reco-food-quality on the left frontier, and trees where it is followed by
assert-reco-cuisine. (See Alt-8’s sp-tree in Figure 16.) Rule 27 also increases the rating
of Alt-8 with its large positive α reflecting the expression of the claim in its own sentence.
443

Walker, Stent, Mairesse, & Prasad

On the other hand, Alt-8 is rated poorly by User B; it violates B’s preferences for
linear order (remember that Rules 21 and 22 specify that B prefers the claim first, followed
by cuisine information). Also, B’s model has rules that radically decrease the ranking of
examples using the period-infer operation (Rules 2, 4, 7 and 9).
Thus, Alt-6 and Alt-8 show that users A and B prefer different combination operators,
and different ordering of content, e.g. B likes the claim first and A likes recommendations
with food quality first followed by cuisine. As mentioned above, previous work on the
generation of evaluative arguments states that the claim may appear first or last (Carenini
& Moore, 2000). The relevant guideline for producing effective evaluative arguments states
that “placing the main claim first helps users follow the line of reasoning, but delaying the
claim until the end of the argument can also be effective if the user is likely to disagree with
the claim.” The template-based generator for MATCH always placed the claim first, but
this analysis suggests that this may not be effective for user A.
Other similarities and differences: There are also individual differences in preferences for particular operations, and for specific content operation interactions. For example,
User A’s model demotes examples where the with-ns operation has been applied (Rules
3, 6 and 8 in Figure 20), while User B generally likes examples where with-ns has been
used (Rule 18 in Figure 21). However, neither A nor B like with-ns when used to combine
other content with neighborhood information. In User A’s model the α value is -1.26, while
in User B’s model the value is -0.50 (see Rule 1 in Figure 20 and Rule 3 in Figure 21.)
These rules capture a specific interaction in the sp-tree between domain-specific content
and the with-ns-infer combination operation. Utterances instantiating these rules place
information in an adjunctival with-clause following the clause realizing the restaurant’s
neighborhood. There is no constraint on the type of information in the with-clause. In
utterance (1) below, the with-clause realizes the restaurant’s food quality, whereas in (2) it
contains information about the restaurant’s service.
(1) Mont Blanc has very good service, its price is 34 dollars, and it is located in Midtown
West, with good food quality. It has the best overall quality among the selected
restaurants.
(2) Mont Blanc is located in Midtown West, with very good service, its price is 34 dollars,
and it has good food quality. It has the best overall quality among the selected
restaurants.
Moreover, both users like with-ns when it combines cuisine and food-quality information as in example (3) (Rule 23 in Figure 20 and Rule 24 in Figure 21).
(3) Komodo has the best overall quality among the selected restaurants since it is a
Japanese, Latin American restaurant, with very good food quality, it has very good
service, and its price is 29 dollars.
But User B radically reduces the rating of the cuisine, food-quality combination when
it is combined with further information using the relative-clause-infer operation, as in
example (5) (Rules 5 and 6 in Figure 21).
444

Individual and Domain Adaptation in Dialogue

(4) Bond Street has very good decor. This Japanese, Sushi restaurant, with excellent
food quality, has good service. It has the best overall quality among the selected
restaurants.
Example (4) is an interesting contrast with example (3). Example (4) instantiates Rule
24 in Figure 21, but it also instantiates a number of negatively valued features. As discussed
above, User B prefers examples where the claim is expressed first (Rule 21 in Figure 21),
and User B’s model explicitly reduces the rating of examples where information about decor
is expressed first (Rule 10 in Figure 21).
In general, User A likes the merge-infer operation (Rules 19, 21 and 22), especially when applied with assert-reco-food-quality (Rule 18), and assert-reco-decor
(Rule 20). User A strongly prefers to hear about food quality first (Rule 26 in Figure 20),
followed by cuisine information (Rule 24). In contrast, User B has rules that reduce the
rating of examples with price or decor first (Rules 8 and 10 in Figure 21). User B also
has no preferences for merge-infer but likes the cw-conjunction operation (Rule 20
in Figure 21). Finally, User B dislikes the relative-clause-infer operation in general
(Rule 1), and its combination with the with-ns operation (Rule 12) or the period-infer
operation (Rule 11).
In addition to other evidence discussed above as to individual differences in language
generation, we believe that the fact that these model differences are interpretable shows that
the differences in user perception of the quality of system utterances are true individual
differences, and not random noise.
8.3 Average Model Differences
Table 22 shows a subset of rules that have the largest α magnitudes for an example
AVG model using the same 100 feature bootstrapping process described above. Section 8.2
presented results that the average model performs statistically worse for recommendations
than either of the individual models. This may be due to the fact that the average model is
essentially trying to learn from contradictory feedback from the two users. To see whether
an examination of the models provides support for this hypothesis, we first examine how the
learned model ranks Alt-6 And Alt-8 as shown in Figure 18 in the column SPRAV G . The
average feedback for Alt-6 is 2.5 while the average feedback for Alt-8 is 3, but the trained
SPR ranks Alt-8 second highest and Alt-6 fifth out of 10.
The mid-value ranking of Alt-6 arises from a number of interacting rules, some of which
are similar to User B’s and some of which are similar to User A’s. Alt-6 instantiates Rules
26 and 27 in Figure 22 which increase the ranking of sentence plans in which the claim,
i.e. assert-reco-best is realized first, and sentence plans where the claim is immediately
followed by information about the type of cuisine (assert-reco-cuisine). These rules are
identical to B’s Rules 21 and 22 in Figure 21. Rule 18 additionally increases the ranking of
sentence plans where cuisine information is followed by service information, which applies
to Alt-6 to further increase its ranking. However Rule 3 lowers the ranking of Alt-6, since
it combines more than 3 different assertions into a single DSyntS tree.
Alt-8 is highly ranked by SPRAV G , largely as a result of several rules that increase its
ranking. Rule 31 specifies an increase in ranking for sentence plans that have the claim in
its own sentence, which is true of Alt-8 but not of Alt-6. This rule also appears as Rule 27
445

Walker, Stent, Mairesse, & Prasad

N
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31

Condition
s-anc-attr-with*locate ≥ −∞
s-trav-have1*i-restaurant*cuisine-type*ii-quality*attr-good*attrfood ≥ −∞
s-trav-propernoun-restaurant ≥ 2.5
r-anc-cw-conjunction-infer*cw-conjunction-infer*period-justify ≥ −∞
r-sis-assert-reco-relative-clause-infer ≥ −∞
r-anc-assert-reco-decor*with-ns-infer*period-infer*period-justify
≥ −∞
r-anc-assert-reco-cuisine*with-ns-infer*relative-clause-infer ≥ −∞
period-justify-avg-leaves-under ≥ 5.5
cw-conjunction-infer-avg-leaves-under ≥ 3.1
r-anc-assert-reco-nbhd*with-ns-infer ≥ −∞
r-sis-cw-conjunction-infer-relative-clause-infer ≥ −∞
period-infer-avg-leaves-under ≥ 3.4
r-anc-assert-reco-food-quality*merge-infer ≥ −∞
s-trav-propernoun-restaurant ≥ 5.5
r-anc-assert-reco-decor*merge-infer ≥ −∞
s-anc-attr-with*i-restaurant*have1 ≥ −∞
r-anc-assert-reco-decor*with-ns-infer ≥ −∞
leaf-assert-reco-best*assert-reco-cuisine*assert-reco-service ≥ −∞
s-trav-propernoun-restaurant ≥ 3.5
r-anc-assert-reco-cuisine*with-ns-infer*period-infer*period-justify
≥ −∞
leaf-assert-reco-food-quality ≥ −∞
period-infer-avg-leaves-under ≥ 3.2
r-sis-merge-infer-assert-reco ≥ −∞
period-justify-avg-leaves-under ≥ 6.5
s-anc-attr-with*have1 ≥ −∞
leaf-assert-reco-best*assert-reco-cuisine ≥ −∞
leaf-assert-reco-best ≥ −∞
merge-infer-max-leaves-under ≥ −∞
leaf-assert-reco-food-quality*assert-reco-cuisine ≥ −∞
merge-infer-max-leaves-under ≥ 2.5
s-trav-have1*propernoun-restaurant*ii-quality*attr-among1 ≥ −∞

αs
-0.87
-0.81
-0.81
-0.77
-0.74
-0.62
-0.62
-0.60
-0.54
-0.45
-0.40
0.14
0.15
0.19
0.19
0.22
0.26
0.26
0.29
0.29
0.32
0.36
0.42
0.48
0.49
0.50
0.50
0.51
0.77
0.96
0.97

Figure 22: A subset of the rules with the largest α magnitudes that were learned for ranking
recommendations given AVG feedback.

in A’s model in Figure 20. Alt-8 also instantiates Rules 21 and 29 which which are identical
to user A’s ordering preferences (Rules 24 and 26 in Figure 20) These rules describe sptrees with assert-reco-food-quality on the left frontier, and trees where it is followed
by assert-reco-cuisine. (See Alt-8’s sp-tree in Figure 16.) Rule 3 also applies to Alt-8,
reducing its ranking due to the number of content items it realizes.
Other similarities and differences: There are many rules in the average model that
are similar to either A or B’s models or both, and the average model retains a number of
446

Individual and Domain Adaptation in Dialogue

preferences seen in the individual models. For example, Rules 1 and 10 both reduce the
ranking of any sentence plan where neighborhood information is combined with subsequent
information using the with-ns combination operator. Rule 1 expresses this in terms of
the lexical items in the d-tree, whereas Rule 10 expresses it in terms of semantic features
derived from the sp-tree. Examples 1 and 2 in Section 8.2 illustrate this interaction.
Some of the rules are more similar to User A. For example, Rules 4 and 9 (like A’s Rules
2 and 5 in Figure 20) reduce the rating of sentence plans that use the operation cw-conjinfer. In addition, Rules 22, 23, 24, and 28 expresses preferences for merging information,
which are very similar to A’s Rules 19, 21 and 22. Rule 15 expresses a preference for
information about the atmosphere (assert-reco-decor) to be combined using the merge
operation, as specified in A’s Rule 20. Rule 20 in Figure 22 is also similar to A’s Rule 16 with
assert-reco-cuisine combined with subsequent information with the with-ns operation.
Other rules are more similar to B’s model. For example, Rule 5 reduces the ranking of
sentence plans using the relative clause operation, which was also specified in User B’s
Rule 1, and Rules 16 and 25 indicate a general preference for use of the with-ns operation,
which was a strong preference in User B’s model (see B’s Rule 18 in Figure 21).
Note that in some cases, the learned model tries to account for both A’s and B’s preferences, even when these contradict one another. For example, Rule 27 specifies a preference
for the claim to come first, as in B’s Rule 21, whereas Rule 26 is the same as A’s 24, specifying a preference for food quality and cuisine information to be expressed first. Thus the
model does suggest that a reduction in performance may arise from trying to account for
the contradictory preferences of users A and B.

9. Conclusions
This article describes SPaRKy, a two-stage sentence planner that generates many alternative realizations of input content plans and then ranks them using a statistical model
trained on human feedback. We demonstrate that the training technique developed for
SPoT (Walker, Rambow, & Rogati, 2002), generalizes easily to new domains, and that it
can be extended to handle the rhetorical structures required for more complex types of
information presentation.
One of the most novel contributions of this paper is to show that trainable generation
can be used to train sentence planners tailored to individual users’ preferences. Previous work modeling individuals has mainly applied to content planning. While studies of
human-human dialogue suggest that modeling other types of individual differences could be
valuable for spoken language generation, in the past, linguistic variation among individuals
was considered a problem for generation (McKeown, Kukich, & Shaw, 1994; Reiter, 2002;
Reiter, Sripada, & Robertson, 2003). Here, we show that users have different perceptions of
the quality of alternative realizations of a content plan, and that individualized models perform better than those trained for groups of users. Our qualitative analysis indicates that
trainable sentence generation is sensitive to variations in domain application, presentation
type, and individual human preferences about the arrangement of particular content types.
These are the first results showing that individual preferences apply to sentence planning.
We also compared SPaRKy to the template-based generator described in Section 3.2:
this generator is highly tuned to this domain and was previously shown to produce high
447

Walker, Stent, Mairesse, & Prasad

quality outputs in a user evaluation (Stent, Prasad, & Walker, 2004). When SPaRKy is
trained for a group of users, then template-based generation is better for recommend and
compare-3, but in most cases the performance of the individualized SPRs are statistically
indistinguishable from MATCH’s template-based generator: the exceptions are that, for
compare-2, User B prefers SPaRKy, while for compare-3 User A prefers the templatebased generator. In all cases, the Human scores (outputs produced by the SPG but selected
by a human) are as good or better than the template-based generator, even for complex
information presentations such as extended comparisons.
These results show that there is a gap between the performance of the trained SPR
and human performance. This suggests that it might be possible to improve the SPR with
different feature sets or a different ranking algorithm. We leave a comparison with other
ranking algorithms to future work. Here, we report results for many different feature sets
(n-gram, concept and tree) and investigate their effect on performance. Table 1 shows that
a combination of the three feature sets performs significantly better for recommend and
compare-3 than the tree features from our earlier work (Walker, Rambow, & Rogati, 2002;
Stent, Prasad, & Walker, 2004; Mairesse & Walker, 2005). Interestingly, in some cases,
simple features like n-grams perform as well as features representing linguistic structure
such as the tree features. This might be because particular lexical items, e.g. with, are
often uniquely associated with a combination operator, e.g. the with-ns operator, which
was shown to have impact on user perceptions of utterance quality (Section 8). More work
is needed to determine whether these performance similarities are simply due to the fact
that the variation of form generated by SPaRKy’s SPG is limited. Other work has also
examined tradeoffs between n-gram features and linguistically complex features in terms
of tradeoffs between time and accuracy (Pantel, Ravichandran, & Hovy, 2004). Although
SPaRKy is trained offline, the time to compute features and rank SPG outputs remains
an issue when using SPaRKy in a real-time spoken dialogue system.
A potential limitation of our approach is the time and effort required to elicit user
feedback for training the system, as described in Section 6. In Section 7.3 we showed that
RankLoss error rates of around 0.20 could be acquired with a much smaller training set, i.e.
with a training set of around 120 examples. However typical users would probably not want
to provide ratings of 120 examples. Future work should explore alternative training regimes
perhaps by utilizing ratings from several users. For example, we could identify examples
that most distinguish our existing users, and just present these examples to new users.
Also, instead of users rating information presentations before using MATCH, perhaps a
method for users to rate information presentations while using MATCH could be developed,
i.e. in the course of a dialogue with MATCH when a recommendation or comparison
is presented to the user, the system could display on the screen a rating form for that
presentation. Another approach would be to train from a different type of user feedback
collected automatically by monitoring the user’s behavior, e.g. measures of cognitive load
such as reading time.
Another limitation is that SPaRKy’s dictionary is handcrafted, i.e. the associations
between simple assertions and their syntactic realizations (d-trees) are specified by hand, like
all generators. Recent work has begun to address this limitation by investigating techniques
for learning a generation dictionary automatically from different types of corpora, such
448

Individual and Domain Adaptation in Dialogue

as user reviews (Barzilay & Lee, 2002; Higashinaka, Walker, & Prasad, 2007; Snyder &
Barzilay, 2007).
A final limitation is that we only use two individuals to provide a proof-of-concept argument for the value of user-tailored trainable sentence planning. We have argued throughout
this paper that the individual differences we document are more general, are not particular
to users A and B, and are not the result of random noise in user feedback. Nevertheless,
we hope that future work will test these results against a larger population of individuals
in order to provide further support for these arguments and in order to characterize the full
range of individual differences in preferences for language variation in dialogue interaction.

Acknowledgments
This work was partially funded by a DARPA Communicator Contract MDA972-99-30003, by a Royal Society Wolfson Research Merit Award to M. Walker, and by a Vice
Chancellor’s studentship to F. Mairesse.

References
André, E., Rist, T., van Mulken, S., Klesen, M., & Baldes, S. (2000). Embodied Conversational Agents, chap. The automated design of believable dialogues for animated
presentation teams, pp. 220–255. MIT Press.
Bangalore, S., & Rambow, O. (2000). Exploiting a probabilistic hierarchical model for
generation. In Proc. of the International Conference on Computational Linguistics.
Barzilay, R., Elhadad, N., & McKeown, K. R. (2002). Inferring strategies for sentence
ordering in multidocument news summarization. Journal of Artificial Intelligence
Research, 17, 35–55.
Barzilay, R., & Lee, L. (2002). Bootstrapping lexical choice via multiple-sequence alignment.
In Proc. of the Conference on Empirical Methods for Natural Language Processing.
Belz, A. (2005). Corpus-driven generation of weather forecasts. In Proc. 3rd Corpus Linguistics Conference.
Bouayad-Agha, N., Scott, D., & Power, R. (2000). Integrating content and style in documents: a case study of patient information leaflets. Information Design Journal, 9 (2),
161–176.
Branigan, H., Pickering, M., & Cleland, A. (2000). Syntactic coordination in dialogue.
Cognition, 75, B13–B25.
Brennan, S. E., & Clark, H. H. (1996). Conceptual pacts and lexical choice in conversation.
Journal of Experimental Psychology: Learning, Memory And Cognition, 22 (6), 1482–
1493.
449

Walker, Stent, Mairesse, & Prasad

Brennan, S. E., Friedman, M. W., & Pollard, C. J. (1987). A centering approach to pronouns. In Proc. of the Annual Meeting of the Association for Computational Linguistics.
Brown, P., & Levinson, S. (1987). Politeness: Some Universals in Language Usage. Cambridge University Press.
Bulyko, I., & Ostendorf, M. (2001). Joint prosody prediction and unit selection for concatenative speech synthesis. In Proc. of the International Conference on Acoustic Speech
and Signal Processing.
Carenini, G., & Moore, J. D. (2000). A strategy for generating evaluative arguments. In
Proc. of the International Natural Language Generation Conference.
Carenini, G., & Moore, J. D. (2006). Generating and evaluating evaluative arguments.
Artificial Intelligence Journal, 170 (11), 925–952.
Chai, J., Hong, P., Zhou, M., & Prasov, Z. (2004). Optimization in multimodal interpretation. In Proc. of the Annual Meeting of the Association for Computational Linguistics.
Chu-Carroll, J., & Carberry, S. (1995). Response generation in collaborative negotiation.
In Proc. of the Annual Meeting of the Association for Computational Linguistics.
Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a collaborative process. Cognition,
22, 1–39.
Collins, M. (2000). Discriminative reranking for natural language parsing. In Proc. of the
International Conference on Machine Learning.
Coulston, R., Oviatt, S., & Darves, C. (2002). Amplitude convergence in children’s conversational speech with animated personas. In Proc. of the International Spoken Language
Processing Conference.
Danlos, L. (2000). G-TAG: A lexicalized formalism for text generation inspired by tree
adjoining grammar. In Abeillé, A., & Rambow, O. (Eds.), Tree Adjoining Grammars:
Formalisms, Linguistic Analysis, and Processing. CSLI Publications.
Di Eugenio, B., Moore, J. D., & Paolucci, M. (1997). Learning features that predict cue
usage. In Proc. of the Annual Meeting of the Association for Computational Linguistics.
DiMarco, C., & Foster, M. E. (1997). The automated generation of Web documents that
are tailored to the individual reader. In Proc. of the AAAI Spring Symposium on
Natural Language Processing on the World Wide Web.
Duboue, P. A., & McKeown, K. R. (2003). Statistical acquisition of content selection rules
for natural language generation. In Proc. of the Conference on Empirical Methods in
Natural Language Processing.
450

Individual and Domain Adaptation in Dialogue

Elhadad, N., Kan, M.-Y., Klavans, J., & McKeown, K. (2005). Customization in a unified
framework for summarizing medical literature. Journal of Artificial Intelligence in
Medicine, 33 (2), 179–198.
Ferguson, S. H., & Kewley-Port, D. (2002). Vowel intelligibility in clear and conversational
speech for normal-hearing and hearing-impaired listeners. Journal of the Acoustical
Society of America, 112 (1), 259–271.
Fleischman, M., & Hovy, E. (2002). Emotional variation in speech-based natural language
generation. In Proc. of the International Natural Language Generation Conference.
Forbes, K., Miltsakaki, E., Prasad, R., Sarkar, A., Joshi, A., & Webber, B. (2003). DLTAG system: Discourse parsing with a lexicalized tree adjoining grammar. Journal
of Logic, Language and Information, 12 (3), 261–279.
Freund, Y., Iyer, R., Schapire, R., & Singer, Y. (1998).
An efficient boosting algorithm for combining preferences.
In Machine Learning: Proceedings
of the Fifteenth International Conference.
Extended version available from
http://www.research.att.com/ schapire.
Garrod, S., & Anderson, A. (1987). Saying what you mean in dialogue: A study in conceptual and semantic coordination. Cognition, 27, 181–218.
Goffman, E. (1981). Forms of Talk. University of Pennsylvania Press, Philadelphia, Pennsylvania, USA.
Grosz, B. J., Joshi, A. K., & Weinstein, S. (1995). Centering: A framework for modeling
the local coherence of discourse. Computational Linguistics, 21 (2), 203–225.
Guo, H., & Stent, A. (2005). Trainable adapatable multimedia presentation generation. In
Proc. of the International Conference on Multimodal Interfaces. Demo paper.
Gupta, S., Walker, M., & Romano, D. (2007). How rude are you?: Evaluating politeness
and affect in interaction. In Proc. of the Second International Conference on Affective
Computing and Intelligent Interaction.
Gupta, S., & Stent, A. (2005). Automatic evaluation of referring expression generation using
corpora. In Proc. of the Workshop on Using Corpora in Natural Language Generation.
Hardt, D., & Rambow, O. (2001). Generation of VP ellipsis: A corpus-based approach. In
Proc. of the Annual Meeting of the Association for Computational Linguistics.
Higashinaka, R., Walker, M., & Prasad, R. (2007). An unsupervised method for learning generation lexicons for spoken dialogue systems by mining user reviews. ACM
Transactions on Speech and Language Processing, 4 (4).
Hovy, E. (1987). Some pragmatic decision criteria in generation. In Kempen, G. (Ed.),
Natural Language Generation, pp. 3–17. Martinus Nijhoff.
Isard, A., Brockmann, C., & Oberlander, J. (2006). Individuality and alignment in generated
dialogues. In Proc.of the International Natural Language Generation Conference.
451

Walker, Stent, Mairesse, & Prasad

Johnston, M., Bangalore, S., Vasireddy, G., Stent, A., Ehlen, P., Walker, M., Whittaker,
S., & Maloor, P. (2002). MATCH: An architecture for multimodal dialogue systems.
In Proc. of the Annual Meeting of the Association for Computational Linguistics.
Jokinen, K., & Kanto, K. (2004). User expertise modelling and adaptivity in a speech-based
e-mail system. In Proc. of the Annual Meeting of the Association for Computational
Linguistics.
Jordan, P., & Walker, M. (2005). Learning content selection rules for generating object
descriptions in dialogue. Journal of Artificial Intelligence Research, 24, 157–194.
Joshi, A. K., Webber, B., & Weischedel, R. M. (1986). Some aspects of default reasoning
in interactive discourse. Tech. rep. MS-CIS-86-27, University of Pennsylvania.
Joshi, A. K., Webber, B. L., & Weischedel, R. M. (1984). Preventing false inferences. In
Proc. of the International Conference on Computational Linguistics.
Jungers, M. K., Palmer, C., & Speer, S. R. (2002). Time after time: The coordinating
influence of tempo in music and speech. Cognitive Processing, 1–2, 21–35.
Kittredge, R., Korelsky, T., & Rambow, O. (1991). On the need for domain communication
knowledge. Computational Intelligence, 7 (4), 305–314.
Kothari, A. (2007). Accented pronouns and unusual antecedents: A corpus study. In Proc.
of the 8th SIGdial Workshop on Discourse and Dialogue.
Langkilde, I., & Knight, K. (1998). Generation that exploits corpus-based statistical knowledge. In Proc. of the International Conference on Computational Linguistics and
Meeting of the Association for Computational Linguistics.
Langkilde-Geary, I. (2002). An empirical verification of coverage and correctness for a
general-purpose sentence generator. In Proc. of the International Natural Language
Generation Conference.
Lapata, M. (2003). Probabilistic text structuring: Experiments with sentence ordering. In
Proc. of the Annual Meeting of the Association for Computational Linguistics.
Lavoie, B., & Rambow, O. (1997). A fast and portable realizer for text generation systems.
In Proc. of the Conference on Applied Natural Language Processing.
Levelt, W. J. M., & Kelter, S. (1982). Surface form and memory in question answering.
Cognitive Psychology, 14, 78–106.
Lin, J. (2006). Using distributional similarity to identify individual verb choice. In Proc. of
the International Natural Language Generation Conference.
Litman, D. (1996). Cue phrase classification using machine learning. Journal of Artificial
Intelligence Research, 5, 53–94.
452

Individual and Domain Adaptation in Dialogue

Luchok, J. A., & McCroskey, J. C. (1978). The effect of quality of evidence on attitude
change and source credibility. The Southern Speech Communication Journal, 43, 371–
383.
Madigan, D., Genkin, A., Lewis, D., Argamon, S., Fradkin, D., & Ye, L. (2005). Author
identification on the large scale. In Proc. of the Meeting of the Classification Society
of North America.
Mairesse, F., & Walker, M. (2007). PERSONAGE: Personality generation for dialogue. In
Proc. of the Annual Meeting of the Association for Computational Linguistics.
Mairesse, F., & Walker, M. (2005). Learning to personalize spoken generation for dialogue
systems. In Proc. Interspeech.
Mann, W., & Thompson, S. (1987). Rhetorical structure theory: Description and construction of text structures. In Kempen, G. (Ed.), Natural Language Generation, pp.
83–96. Martinus Nijhoff.
Marciniak, T., & Strube, M. (2004). Classification-based generation using TAG. In Proc.
of the International Natural Language Generation Conference.
Marcu, D. (1996). Building up rhetorical structure trees. In Proc. of the Conference on
Artificial Intelligence and Conference on Innovative Applications of Artificial Intelligence.
Marcu, D. (1997). From local to global coherence: a bottom-up approach to text planning.
In Proc. of the Conference on Artificial Intelligence.
McCoy, K. F. (1989). Generating context-sensitive responses to object related misconceptions. Artificial Intelligence, 41 (2), 157–195.
McKeown, K., Kukich, K., & Shaw, J. (1994). Practical issues in automatic document
generation. In Proc. of the Conference on Applied Natural Language Processing.
McKeown, K. R. (1985). Discourse strategies for generating natural language text. Artificial
Intelligence, 27 (1), 1–42.
Mellish, C., O’Donnell, M., Oberlander, J., & Knott, A. (1998). An architecture for opportunistic text generation. In Proc. of the Ninth International Workshop on Natural
Language Generation.
Melčuk, I. A. (1988). Dependency Syntax: Theory and Practice. State University of New
York Press, Albany, New York.
Moore, J. D., & Paris, C. L. (1993). Planning text for advisory dialogues: Capturing
intentional and rhetorical information. Computational Linguistics, 19 (4), 651–694.
Moore, J. D., Foster, M. E., Lemon, O., & White, M. (2004). Generating tailored, comparative descriptions in spoken dialogue. In Proc. of the Seventeenth International
Florida Artificial Intelligence Research Society Conference.
453

Walker, Stent, Mairesse, & Prasad

Nakatsu, C., & White, M. (2006). Learning to say it well: Reranking realizations by
predicted synthesis quality. In Proc. of the Annual Meeting of the Association for
Computational Linguistics.
Nenkova, A., Passonneau, R. J., & McKeown, K. (2007). The pyramid method: incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing, 4 (2).
Oberlander, J., & Brew, C. (2000). Stochastic text generation. Philosophical Transactions
of the Royal Society of London, Series A, 358, 1373–1385.
Paiva, D. S., & Evans, R. (2004). A framework for stylistically controlled generation. In
Proc. of the International Natural Language Generation Conference.
Pantel, P., Ravichandran, D., & Hovy, E. (2004). Towards terascale knowledge acquisition.
In Proc. of the International Conference on Computational Linguistics.
Papenini, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). Bleu: A method for automatic
evaluation of machine translation. In Proc. of the Annual Meeting of the Association
for Computational Linguistics.
Pennebaker, J. W., & King, L. A. (1999). Linguistic styles: Language use as an individual
difference. Journal of Personality and Social Psychology, 77, 1296–1312.
Piwek, P. (2003). A flexible pragmatics-driven language generator for animated agents. In
Proc. of the European Meeting of the Association for Computational Linguistics.
Polifroni, J., & Walker, M. (2006). An analysis of automatic content selection algorithms
for spoken dialogue system summaries. In Proc. of the IEEE/ACL Conference on
Spoken Language Technology.
Porayska-Pomsta, K., & Mellish, C. (2004). Modelling politeness in natural language generation. In Proc. of the International Natural Language Generation Conference.
Prasad, R., Joshi, A., Dinesh, N., Lee, A., & Miltsakaki, E. (2005). The Penn Discourse
TreeBank as a resource for natural language generation. In Proc. of the Corpus
Linguistics Workshop on Using Corpora for Natural Language Generation.
Prevost, S. (1995). A Semantics of Contrast and Information Structure for Specifying
Intonation in Spoken Language Generation. Ph.D. thesis, University of Pennsylvania.
Prince, E. F. (1985). Fancy syntax and shared knowledge. Journal of Pragmatics, 9 (1),
65–81.
Rambow, O., Rogati, M., & Walker, M. (2001). Evaluating a trainable sentence planner for
a spoken dialogue travel system. In Proc. of the Annual Meeting of the Association
for Computational Linguistics.
Rambow, O., & Korelsky, T. (1992). Applied text generation. In Proc. of the Conference
on Applied Natural Language Processing.
454

Individual and Domain Adaptation in Dialogue

Reiter, E. (2002). Should corpora be gold standards for NLG?. In Proc. of the International
Natural Language Generation Conference.
Reiter, E., & Dale, R. (2000). Building Natural Language Generation Systems. Cambridge
University Press.
Reiter, E., & Sripada, S. (2002). Human variation and lexical choice. Computational
Linguistics, 28, 545–553.
Reiter, E., Sripada, S., & Robertson, R. (2003). Acquiring correct knowledge for natural
language generation. Journal of Artificial Intelligence Research, 18, 491–516.
Reitter, D., Keller, F., & Moore, J. D. (2006). Computational modeling of structural priming in dialogue. In Proc. of the Joint Conference on Human Language Technologies
and Meeting of the North American Chapter of the Association for Computational
Linguistics.
Rich, E. (1979). User modelling via stereotypes. Cognitive Science, 3, 329–354.
Scott, D. R., & de Souza, C. S. (1990). Getting the message across in RST-based text
generation. In Dale, R., Mellish, C., & Zock, M. (Eds.), Current Research in Natural
Language Generation, pp. 47–73. Academic Press.
Snyder, B., & Barzilay, R. (2007). Database-text alignment via structured multilabel classification. In Proc. of the International Joint Conference on Artificial Intelligence.
Stenchikova, S., & Stent, A. (2007). Measuring adaptation between dialogs. In Proc. of the
8th SIGdial Workshop on Discourse and Dialogue.
Stent, A., & Guo, H. (2005). A new data-driven approach for multimedia presentation
generation. In Proc. EuroIMSA.
Stent, A., Prasad, R., & Walker, M. (2004). Trainable sentence planning for complex
information presentation in spoken dialog systems. In Proc. of the Annual Meeting of
the Association for Computational Linguistics.
Stent, A., Walker, M., Whittaker, S., & Maloor, P. (2002). User-tailored generation for
spoken dialogue: An experiment. In Proc. of the International Conference on Spoken
Language Processing.
Wahlster, W., & Kobsa, A. (1989). User models in dialogue systems. In User Models in
Dialogue Systems, pp. 4–34. Springer Verlag, Berlin.
Walker, M., Rambow, O., & Rogati, M. (2002). Training a sentence planner for spoken
dialogue using boosting. Computer Speech and Language: Special Issue on Spoken
Language Generation, 16 (3-4), 409–433.
Walker, M. A., Cahn, J. E., & Whittaker, S. J. (1997). Improvising linguistic style: Social
and affective bases for agent personality. In Proc. of the First Conference on Automous
Agents.
455

Walker, Stent, Mairesse, & Prasad

Walker, M. A., et al. (2002). DARPA communicator: Cross-system results for the 2001
evaluation. In Proc. of the International Spoken Language Processing Conference.
Walker, M. A., et al. (2004). Generation and evaluation of user tailored responses in multimodal dialogue. Cognitive Science, 28 (5), 811–840.
Webber, B., Knott, A., Stone, M., & Joshi, A. (1999). What are little trees made of?: A
structural and presuppositional account using lexicalized tag. In Proc. of the Annual
Meeting of the Association for Computational Linguistics.
Yeh, C.-L., & Mellish, C. (1997). An empirical study on the generation of anaphora in
Chinese. Computational Linguistics, 23-1, 169–190.
Zukerman, I., & Litman, D. (2001). Natural language processing and user modeling: Synergies and limitations. User Modeling and User-Adapted Interaction, 11 (1-2), 129–158.

456

Journal of Artificial Intelligence Research 30 (2007) 565-620

Submitted 3/07; published 12/07

Probabilistic Planning via Heuristic Forward Search
and Weighted Model Counting
Carmel Domshlak

DCARMEL @ IE . TECHNION . AC . IL

Technion - Israel Institute of Technology,
Haifa, Israel

Jörg Hoffmann

J OERG .H OFFMANN @ DERI . AT

University of Innsbruck, DERI,
Innsbruck, Austria

Abstract
We present a new algorithm for probabilistic planning with no observability. Our algorithm,
called Probabilistic-FF, extends the heuristic forward-search machinery of Conformant-FF to problems with probabilistic uncertainty about both the initial state and action effects. Specifically,
Probabilistic-FF combines Conformant-FF’s techniques with a powerful machinery for weighted
model counting in (weighted) CNFs, serving to elegantly define both the search space and the
heuristic function. Our evaluation of Probabilistic-FF shows its fine scalability in a range of probabilistic domains, constituting a several orders of magnitude improvement over previous results in
this area. We use a problematic case to point out the main open issue to be addressed by further
research.

1. Introduction
In this paper we address the problem of probabilistic planning with no observability (Kushmerick,
Hanks, & Weld, 1995), also known in the AI planning community as conditional (Majercik &
Littman, 2003) or conformant (Hyafil & Bacchus, 2004) probabilistic planning. In such problems
we are given an initial belief state in the form of a probability distribution over the world states W ,
a set of actions (possibly) having probabilistic effects, and a set of alternative goal states WG ⊆ W .
A solution to such a problem is a single sequence of actions that transforms the system into one
of the goal states with probability exceeding a given threshold θ. The basic assumption of the
problem is that the system cannot be observed at the time of plan execution. Such a setting is
useful in controlling systems with uncertain initial state and non-deterministic actions, if sensing is
expensive or unreliable. Non-probabilistic conformant planning may fail due to non-existence of a
plan that achieves the goals with 100% certainty. Even if there is such a plan, that plan does not
necessarily contain information about what actions are most useful to achieve only the requested
threshold θ.
The state-of-the-art performance of probabilistic planners has been advancing much more slowly
than that of deterministic planners, scaling from 5-10 step plans for problems with ≈20 world states
to 15-20 step plans for problems with ≈100 world states (Kushmerick et al., 1995; Majercik &
Littman, 1998; Hyafil & Bacchus, 2004). Since probabilistic planning is inherently harder than its
deterministic counterpart (Littman, Goldsmith, & Mundhenk, 1998), such a difference in evolution
rates is by itself not surprising. However, recent developments in the area (Onder, Whelan, & Li,
2006; Bryce, Kambhampati, & Smith, 2006; Huang, 2006), and in particular our work here, show
that dramatic improvements in probabilistic planning can be obtained.
c
2007
AI Access Foundation. All rights reserved.

D OMSHLAK & H OFFMANN

In this paper we introduce Probabilistic-FF, a new probabilistic planner based on heuristic forward search in the space of implicitly represented probabilistic belief states. The planner is a natural
extension of the recent (non-probabilistic) conformant planner Conformant-FF (Hoffmann & Brafman, 2006). The main trick is to replace Conformant-FF’s SAT-based techniques with a recent
powerful technique for probabilistic reasoning by weighted model counting (WMC) in propositional CNFs (Sang, Beame, & Kautz, 2005). In more detail, Conformant-FF does a forward search
in a belief space in which each belief state corresponds to a set of world states considered to be
possible. The main trick of Conformant-FF is the use of CNF formulas for an implicit representation of belief states. Implicit, in this context, means that formulas φ(a) encode the semantics of
executing action sequence a in the initial belief state, with propositional variables corresponding to
facts with time-stamps. Any actual knowledge about the belief states has to be (and can be) inferred
from these formulas. Most particularly, a fact p is known to be true in a belief state if and only if
φ(a) → p(m), where m is the time endpoint of the formula. The only knowledge computed by
Conformant-FF about belief states are these known facts, as well as (symmetrically) the facts that
are known to be false. This suffices to do STRIPS-style planning, that is, to determine applicable
actions and goal belief states. In the heuristic function, FF’s (Hoffmann & Nebel, 2001) relaxed
planning graph technique is enriched with approximate SAT reasoning.
The basic ideas underlying Probabilistic-FF are:
(i) Define time-stamped Bayesian networks (BNs) describing probabilistic belief states.
(ii) Extend Conformant-FF’s belief state CNFs to model these BNs.
(iii) In addition to the SAT reasoning used by Conformant-FF, use weighted model-counting to
determine whether the probability of the (unknown) goals in a belief state is high enough.
(iv) Introduce approximate probabilistic reasoning into Conformant-FF’s heuristic function.
Note the synergetic effect: Probabilistic-FF re-uses all of Conformant-FF’s technology to recognize
facts that are true or false with probability 1. This fully serves to determine applicable actions, as
well as detect whether part of the goal is already known. In fact, it is as if Conformant-FF’s CNFbased techniques were specifically made to suit the probabilistic setting: while without probabilities
one could imagine successfully replacing the CNFs with BDDs, with probabilities this seems much
more problematic.
The algorithms we present cover probabilistic initial belief states given as Bayesian networks,
deterministic and probabilistic actions, conditional effects, and standard action preconditions. Our
experiments show that our approach is quite effective in a range of domains. In contrast to the
SAT and CSP based approaches mentioned above (Majercik & Littman, 1998; Hyafil & Bacchus,
2004), Probabilistic-FF can find 100-step plans for problem instances with billions of world states.
However, such a comparison is not entirely fair due to the different nature of the results provided; the
SAT and CSP based approaches provide guarantees on the length of the solution. The approach most
closely related to Probabilistic-FF is implemented in POND (Bryce et al., 2006): this system, like
Probabilistic-FF, does conformant probabilistic planning for a threshold θ, using a non-admissible,
planning-graph based heuristic to guide the search. Hence a comparison between Probabilistic-FF
and POND is fair, and in our experiments we perform a comparative evaluation of Probabilistic-FF
and POND. While the two approaches are related, there are significant differences in the search
566

P ROBABILISTIC -FF

space representation, as well as in the definition and computation of the heuristic function.1 We run
the two approaches on a range of domains partly taken from the probabilistic planning literature,
partly obtained by enriching conformant benchmarks with probabilities, and partly obtained by
enriching classical benchmarks with probabilistic uncertainty. In almost all cases, Conformant-FF
outperforms POND by at least an order of magnitude. We make some interesting observations
regarding the behavior of the two planners; in particular we identify a domain – derived from the
classical Logistics domain – where both approaches fail to scale. The apparent reason is that neither
approach is good enough at detecting how many times, at an early point in the plan, a probabilistic
action must be applied in order to sufficiently support a high goal threshold at the end of the plan.
Devising methods that are better in this regard is the most pressing open issue in this line of work.
The paper is structured as follows. The next section provides the technical background, formally
defining the problem we address and illustrating it with our running example. Section 3 details how
probabilistic belief states are represented as time-stamped Bayesian networks, how these Bayesian
networks are encoded as weighted CNF formulas, and how the necessary reasoning is performed
on this representation. Section 4 explains and illustrates our extension of Conformant-FF’s heuristic function to the probabilistic settings. Section 5 provides the empirical results, and Section 6
concludes. All proofs are moved into Appendix A.

2. Background
The probabilistic planning framework we consider adds probabilistic uncertainty to a subset of
the classical ADL language, namely (sequential) STRIPS with conditional effects. Such STRIPS
planning tasks are described over a set of propositions P as triples (A, I, G), corresponding to the
action set, initial world state, and goals. I and G are sets of propositions, where I describes a
concrete initial state wI , while G describes the set of goal states w ⊇ G. Actions a are pairs
(pre(a), E(a)) of the precondition and the (conditional) effects. A conditional effect e is a triple
(con(e), add(e), del(e)) of (possibly empty) proposition sets, corresponding to the effect’s condition, add, and delete lists, respectively. The precondition pre(a) is also a proposition set, and
an action a is applicable in a world state w if w ⊇ pre(a). If a is not applicable in w, then
the result of applying a to w is undefined. If a is applicable in w, then all conditional effects
e ∈ E(a) with w ⊇ con(e) occur. Occurrence of a conditional effect e in w results in the world
state w ∪ add(e) \ del(e).
If an action a is applied to w, and there is a proposition q such that q ∈ add(e) ∩ del(e′ ) for
(possibly the same) occurring e, e′ ∈ E(a), then the result of applying a in w is undefined. Thus,
we require the actions to be not self-contradictory, that is, for each a ∈ A, and every e, e′ ∈ E(a),
if there exists a world state w ⊇ con(e) ∪ con(e′ ), then add(e) ∩ del(e′ ) = ∅. Finally, an action
sequence a is a plan if the world state that results from iterative execution of a’s actions, starting in
wI , leads to a goal state w ⊇ G.
2.1 Probabilistic Planning
Our probabilistic planning setting extends the above with (i) probabilistic uncertainty about the
initial state, and (ii) actions that can have probabilistic effects. In general, probabilistic planning
1. POND does not use implicit belief states, and the probabilistic part of its heuristic function uses sampling techniques,
rather than the probabilistic reasoning techniques we employ.

567

D OMSHLAK & H OFFMANN

tasks are quadruples (A, bI , G, θ), corresponding to the action set, initial belief state, goals, and
acceptable goal satisfaction probability. As before, G is a set of propositions. The initial state is
no longer assumed to be known precisely. Instead, we are given a probability distribution over the
world states, bI , where bI (w) describes the likelihood of w being the initial world state.
Similarly to classical planning, actions a ∈ A are pairs (pre(a), E(a)), but the effect set E(a)
for such a has richer structure and semantics. Each e ∈ E(a) is a pair (con(e), Λ(e)) of a propositional condition and a set of probabilistic outcomes. Each probabilistic outcome ε ∈ Λ(e) is a
triplet (P r(ε), add(ε), del(ε)), where add and delete lists are as before, and P r(ε) is the probability that outcome ε occurs as a result of effect e. Naturally,
P we require that probabilistic effects
define probability distributions over their outcomes, that is, ε∈Λ(e) P r(ε) = 1. The special case of
deterministic effects e is modeled this way via Λ(e) = {ε} and P r(ε) = 1. Unconditional actions
are modeled as having a single effect e with con(e) = ∅. As before, if a is not applicable in w,
then the result of applying a to w is undefined. Otherwise, if a is applicable in w, then there exists
exactly one effect e ∈ E(a) such that con(e) ⊆ w, and for each ε ∈ Λ(e), applying a to w results
in w ∪ add(ε) \ del(ε) with probability P r(ε). The likelihood [b, a] (w′ ) of a world state w′ in the
belief state [b, a], resulting from applying a probabilistic action a in b, is given by
[b, a] (w′ ) =

X

w⊇pre(a)

b(w)

X

ε∈Λ(e)


P r(ε) · δ w′ = w ∪ s \ s′ , s ⊆ add(ε), s′ ⊆ del(ε) ,

(1)

where e is the effect of a such that con(e) ⊆ w, and δ(·) is the Kronecker step function that takes
the value 1 if the argument predicate evaluates to TRUE, and 0 otherwise.
Our formalism covers all the problem-description features supported by the previously proposed
formalisms for conformant probabilistic planning (Kushmerick et al., 1995; Majercik & Littman,
1998; Hyafil & Bacchus, 2004; Onder et al., 2006; Bryce et al., 2006; Huang, 2006), and it corresponds to what is called Unary Nondeterminism (1ND) normal form (Rintanen, 2003). We note
that there are more succinct forms for specifying probabilistic planning problems (Rintanen, 2003),
yet 1ND normal form appears to be most intuitive from the perspective of knowledge engineering.
Example 1 Say we have a robot and a block that physically can be at one of two locations. This
information is captured by the propositions r1 , r2 for the robot, and b1 , b2 for the block, respectively. The robot can either move from one location to another, or do it while carrying the block.
If the robot moves without the block, then its move is guaranteed to succeed. This provides us
with a pair of symmetrically defined deterministic actions {move-right, move-lef t}. The action move-right has an empty precondition, and a single conditional effect e = ({r1 }, {ε}) with
P r(ε) = 1, add(ε) = {r2 }, and del(ε) = {r1 }. If the robot tries to move while carrying the block,
then this move succeeds with probability 0.7, while with probability 0.2 the robot ends up moving
without the block, and with probability 0.1 this move of the robot fails completely. This provides us
with a pair of (again, symmetrically defined) probabilistic actions {move-b-right, move-b-lef t}.
The action move-b-right has an empty precondition, and two conditional effects specified as in
Table 1.
Having specified the semantics and structure of all the components of (A, bI , G, θ) but θ, we are
now ready to specify the actual task of probabilistic planning in our setting. Recall that our actions
transform probabilistic belief states to belief states. For any action sequence a ∈ A∗ , and any belief
568

P ROBABILISTIC -FF

E(a)

con(e)

e

r1 ∧ b 1

e′

¬r1 ∨ ¬b1

Λ(e)

P r(ε)

add(ε)

del(ε)

ε1
ε2
ε3
ε′1

0.7
0.2
0.1
1.0

{r2 , b2 }
{r2 }
∅
∅

{r1 , b1 }
{r1 }
∅
∅

Table 1: Possible effects and outcomes of the action move-b-right in Example 1.
state b, the new belief state [b, a] resulting from applying a at b is given by


a = hi
b,
[b, a] = [b, a] ,
.
a = hai, a ∈ A


′
′
′
[[b, a] , a ] , a = hai · a , a ∈ A, a 6= ∅

(2)

In such setting, achieving G with certainty is typically unrealistic. Hence, θ specifies the required
lower bound on the probability of achieving G. A sequence of actions a is called a plan if we have
ba (G) ≥ θ for the belief state ba = [bI , a].
2.2 Specifying the Initial Belief State
Considering the initial belief state, practical considerations force us to limit our attention only to
compactly representable probability distributions bI . While there are numerous alternatives for
compact representation of structured probability distributions, Bayes networks (BNs) (Pearl, 1988)
to date is by far the most popular such representation model.2 Therefore, in Probabilistic-FF we
assume that the initial belief state bI is described by a BN NbI over our set of propositions P.
As excellent introductions to BNs abound (e.g., see Jensen, 1996), it suffices here to briefly
define our notation. A BN N = (G, T ) represents a probability distribution as a directed acyclic
graph G, where its set of nodes X stands for random variables (assumed discrete in this paper),
and T , a set of tables of conditional probabilities (CPTs)—one table TX for each node X ∈ X .
For each possible value x ∈ Dom(X) (where Dom(X) denotes the domain of X), the table TX
lists the probability of the event X = x given each possible value assignment to all of its immediate
ancestors (parents) P a(X) in G. Thus, the table size is exponential in the in-degree of X. Usually, it
is assumed either that this in-degree is small (Pearl, 1988), or that the probabilistic dependence of X
on P a(X) induces a significant local structure allowing a compact representation of TX (Shimony,
1993, 1995; Boutilier, Friedman, Goldszmidt, & Koller, 1996). (Otherwise, representation of the
distribution as a BN would not be a good idea in the first place.) The joint probability of a complete
assignment ϑ to the variables X is given by the product of |X | terms taken from the respective
CPTs (Pearl, 1988):
Y
Y
P r (ϑ[X] | ϑ[P a(X)]) =
TX (ϑ[X] | ϑ[P a(X)]) ,
P r(ϑ) =
X∈X

X∈X

where ϑ[·] stands for the partial assignment provided by ϑ to the corresponding subset of X .
2. While BNs are our choice here, our framework can support other models as well, e.g. stochastic decision trees.

569

D OMSHLAK & H OFFMANN

In Probabilistic-FF we allow NbI to be described over the multi-valued variables underlying
the planning problem. This significantly simplifies the process of specifying NbI since the STRIPS
3
propositions P do
Sknot correspond to the true random variables underlying problem specification.
Specifically, let i=1 Pi be a partition of P such that each proposition set Pi uniquely corresponds
to the domain of a multi-valued variable underlying our problem. That is, for every world state w
and every Pi , if |Pi | > 1, then there is exactly one proposition q ∈ Pi that holds in w. The variables
of the BN NbI describing our initial belief state bI are X = {X1 , . . . , Xk }, where Dom(Xi ) = Pi
if |Pi | > 1, and Dom(Xi ) = {q, ¬q} if Pi = {q}.
Example 2 For an illustration of such NbI , consider our running example, and say the robot is
known to be initially at one of the two possible locations with probability P r(r1 ) = 0.9 and
P r(r2 ) = 0.1. Suppose there is a correlation in our belief about the initial locations of the robot and
the block. We believe that, if the robot is at r1 , then P r(b1 ) = 0.7 (and P r(b2 ) = 0.3), while if the
robot is at r2 , then P r(b1 ) = 0.2 (and P r(b2 ) = 0.8). The initial belief state BN NbI is then defined
over two variables R (“robot”) and B (“block”) with Dom(R) = {r1 , r2 } and Dom(B) = {b1 , b2 },
respectively, and it is depicted in Figure 1.
r1
0.9

r2
0.1

R

// B

r1
r2

b1
0.7
0.2

b2
0.3
0.8

Figure 1: Bayes network NbI for Example 1.
It is not hard to see that our STRIPS-style actions a ∈ A can be equivalently specified in terms
of the multi-valued variables X . Specifically, if |Pi | > 1, then no action a can add a proposition
q ∈ Pi without deleting some other proposition q ′ ∈ Pi , and thus, we can consider a as setting
Xi = q. If |Pi | = 1, then adding and deleting q ∈ Pi has the standard semantics of setting Xi = q
and Xi = ¬q, respectively. For simplicity of presentation, we assume that our actions are not selfcontradictory at the level of X as well—if two conditional effects e, e′ ∈ E(a) can possibly occur in
some world state w, then the subsets of X affected by these two effects have to be disjoint. Finally,
our goal G directly corresponds to a partial assignment to X (unless our G is self-contradictory,
requiring q ∧ q ′ for some q, q ′ ∈ Pi .)

3. Belief States
In this section, we explain our representation of, and reasoning about, belief states. We first explain
how probabilistic belief states are represented as time-stamped BNs, then we explain how those
BNs are encoded and reasoned about in the form of weighted CNF formulas. This representation
of belief states by weighted CNFs is then illustrated on the belief state from our running example in
Figure 2. We finally provide the details about how this works in Probabilistic-FF.
3. Specifying NbI directly over P would require identifying the multi-valued variables anyway, followed by connecting
all the propositions corresponding to a multi-valued variable by a complete DAG, and then normalizing the CPTs of
these propositions in a certain manner.

570

P ROBABILISTIC -FF

ε1 ∨ ε2
r
ε3 ∨ε′1 1
r2

r1 r2
0.9 0.1
R(0)



B(0)

II
II
II
II
II
$$
::
uu
uu
u
u
uu
uu

b1 b2
r1 0.7 0.3
r2 0.2 0.8

Y(1)

r1
0
1
0

r2
1
0
1

// R
n66 (1)
n
n
nnn
nnn
n
n
ε1 ε2 ε3 ε′1
nn
nnn
r1 ∧ b1 0.7 0.2 0.1 0
PPP
PPP
0 0 0 1
PPP othrw
PPP
PPP
P((
// B(1)

b1 b2
ε1
0 1
b 1 0
¬ε1 1
b2 0 1

r1 r2
r1 1 0
r2 1 0
// R(2)

// B(2)

b1 b2
b1 1 0
b2 0 1

Figure 2: Bayes network Nba for our running Example 1-2 and action sequence
a = hmove-b-right, move-lef ti.

3.1 Bayesian Networks
Probabilistic-FF performs a forward search in a space of belief states. The search states are belief
states (that is, probability distributions over the world states w), and the search is restricted to belief
states reachable from the initial belief state bI through some sequences of actions a. A key decision
one should make is the actual representation of the belief states. Let bI be our initial belief state
captured by the BN NbI , and let ba be a belief state resulting from applying to bI a sequence of
actions a. One of the well-known problems in the area of decision-theoretic planning is that the
description of ba directly over the state variables X becomes less and less structured as the number
of (especially stochastic) actions in a increases. To overcome this limitation, we represent belief
states ba as a BN Nba that explicitly captures the sequential application of a starting from bI , trading
the representation size for the cost of inference, compared to representing belief states directly as
distributions over world states. Below we formally specify the structure of such a BN Nba , assuming
that all the actions a are applicable in the corresponding belief states of their application, and later
showing that Probabilistic-FF makes sure this is indeed the case. We note that these belief-state
BNs are similar in spirit and structure to those proposed in the AI literature for verifying that a
probabilistic plan achieves its goals with a certain probability (Dean & Kanazawa, 1989; Hanks &
McDermott, 1994; Kushmerick et al., 1995).
Figure 2 illustrates the construction of Nba for our running example with a = hmove-b-right,
move-lef ti. In general, let a = ha1 , . . . , am i be a sequence of actions, numbered according to their
appearance on a. For 0 ≤ t ≤ m, let X(t) be a replica of our state variables X , with X(t) ∈ X(t)
571

D OMSHLAK & H OFFMANN

corresponding to X ∈ X . The variable set of Nba is the union of X(0) , . . . , X(m) , plus some
additional variables that we introduce for the actions in a.
First, for each X(0) ∈ X(0) , we set the parents P a(X(0) ) and conditional probability tables
TX(0) to simply copy these of the state variable X in NbI . Now, consider an action at from a, and
let at = a. For each such action we introduce a discrete variable Y(t) that S
“mediates” between the
variable layers X(t−1) and X(t) . The domain of Y(t) is set to Dom(Y(t) ) = e∈E(a) Λ(e), that is, to
the union of probabilistic outcomes of all possible effects of a. The parents of Y(t) in Nba are set to
P a(Y(t) ) =

[ 

e∈E(a)

	
X(i−1) | con(e) ∩ Dom(X) 6= ∅ ,

(3)

and, for each π ∈ Dom(P a(Y(t) )), we set
TY (t) (Y(i)

(
P r(ε),
= ε | π) =
0,

con (e(ε)) ⊆ π
,
otherwise

(4)

where e(ε) denotes the effect e of a such that ε ∈ Λ(e).
We refer to the set of all such variables Y(t) created for the actions of a as Y. Now, let EX (a) ⊆
E(a) be the probabilistic effects of a that affect a variable X ∈ X . If EX (a) = ∅, then we set
P a(X(t) ) = {X(t−1) }, and
(
1, x = x′ ,
.
(5)
TX(t) (X(t) = x | X(t−1) = x′ ) =
0, otherwise
Otherwise, if EX (a) 6= ∅, let xε ∈ Dom(X) be the value provided to X by ε ∈ Λ(e), e ∈ EX (a).
Recall that the outcomes of effects E(a) are all mutually exclusive. Hence, we set P a(X(t) ) =
{X(t−1) , Y(t−1) }, and

′

TX(i) (X(i) = x | X(i−1) = x , Y(i−1)



1,
= ε) = 1,


0,

e(ε) ∈ EX (a) ∧ x = xε ,
e(ε) 6∈ EX (a) ∧ x = x′ , ,
otherwise

(6)

where e(ε) denotes the effect responsible for the outcome ε.
It is not hard to verify that Equations 4-6 capture the frame axioms and probabilistic semantics of ourSactions. In principle, this accomplishes our construction of Nba over the variables
Xba = Y m
t=0 X(t) . We note, however, that the mediating variable Y(t) are really needed only
for truly probabilistic actions. Specifically, if at is a deterministic action a, let EX (a) ⊆ E(a) be
the conditional effects of a that add and/or delete propositions associated with the domain of a variable X ∈ X . If EX (a) = ∅, then we set P a(X(t) ) = {X(t−1) }, and TX(t) according to Equation 5.
Otherwise, we set
o
[n
′
P a(X(t) ) = {X(t−1) }
X(t−1)
| con(e) ∩ Dom(X) 6= ∅ ,
(7)
e∈EX (a)

and specify TX(t) as follows. Let xe ∈ Dom(X) be the value that (the only deterministic outcome
of) the effect e ∈ EX (a) provides to X. For each π ∈ Dom(P a(X(t) )), if there exists e ∈ EX (a)
572

P ROBABILISTIC -FF

such that con(e) ⊆ π, then we set
TX(t) (X(t)

(
1,
= x | π) =
0,

x = xe ,
otherwise

(8)

x = π[X(t−1) ],
otherwise

(9)

Otherwise, we set
TX(t) (X(t)

(
1,
= x | π) =
0,

Due to the self-consistency of the action, it is not hard to verify that Equations 8-9 are consistent,
and, together with Equation 5, capture the semantics of the conditional deterministic actions. This
special treatment of deterministic actions is illustrated in Figure 2 by the direct dependencies of
X(2) on X(1) .
Proposition 1 Let (A, NbI , G, θ) be a probabilistic planning problem, and a be an m-step sequence
of actions applicable in bI . Let P r be the joint probability distribution induced by Nba on its
variables Xba . The belief state ba corresponds to the marginal distribution of P r on X(m) , that is:
ba (X ) = P r(X(m) ), and if G(m) is a partial assignment provided by G to X(m) , then the probability
ba (G) that a achieves G starting from bI is equal to P r(G(m) ).
As we already mentioned, our belief-state BNs are constructed along the principles outlined
and used by Dean and Kanazawa (1989), Hanks and McDermott (1994), and Kushmerick et al.
(1995), and thus the correctness of Proposition 1 is immediate from these previous results. At this
point, it is worth bringing attention to the fact that all the variables in X(1) , . . . , X(m) are completely
deterministic. Moreover, the CPTs of all the variables of Nba are all compactly representable due to
either a low number of parents, or some local structure induced by a large amount of context-specific
independence, or both. This compactness of the CPTs in Nba is implied by the compactness of the
STRIPS-style specification of the planning actions. By exploiting this compactness of the action
specification, the size of the Nba ’s description can be kept linear in the size of the input and the
number of actions in a.
Proposition 2 Let (A, NbI , G, θ) be a probabilistic planning problem described over k state variables, and a be an m-step sequence of actions from A. Then, we have |Nba | = O(|NbI |+mα(k+1))
where α is the largest description size of an action in A.
The proof of Proposition 2, as well as the proofs of other formal claims in the paper, are relegated
to Appendix A, pp. 613.
3.2 Weighted CNFs
Given the representation of belief states as BNs, next we should select a mechanism for reasoning
about these BNs. In general, computing the probability of a query in BNs is known to be #Pcomplete (Roth, 1996). In addition, it is not hard to verify, using an analysis similar to the ones
of Darwiche (2001) and Brafman and Domshlak (2006), that the networks arising in our work
will typically exhibit large tree-width. While numerous exact algorithms for inference with BNs
have been proposed in the literature (Darwiche, 2000; Dechter, 1999; Zhang & Poole, 1994), the
classical algorithms do not scale well on large networks exhibiting high tree-width. On the positive
573

D OMSHLAK & H OFFMANN

side, however, an observation that guides some recent advances in the area of probabilistic reasoning
is that real-world domains typically exhibit a significant degree of deterministic dependencies and
context-specific independencies between the problem variables. Targeting this property of practical
BNs already resulted in powerful inference techniques (Chavira & Darwiche, 2005; Sang et al.,
2005). The general principle underlying these techniques is to
(i) Compile a BN N into a weighted propositional logic formula φ(N ) in CNF, and
(ii) Perform an efficient weighted model counting for φ(N ) by reusing (and adapting) certain
techniques that appear powerful in enhancing backtracking DPLL-style search for SAT.
One observation we had at the early stages of developing Probabilistic-FF is that the type of
networks and type of queries we have in our problems make this machinery for solving BNs by
weighted CNF model counting very attractive for our needs. First, in Section 3.1 we have already
shown that the BNs representing our belief states exhibit a large amount of both deterministic nodes
and context-specific independence. Second, the queries of our interest correspond to computing
probability of the “evidence” G(m) in Nba , and this type of query has a clear interpretation in terms
of model counting (Sang et al., 2005). Hence, taking this route in Probabilistic-FF, we compile our
belief state BNs to weighted CNFs following the encoding scheme proposed by Sang et al. (2005),
and answer probabilistic queries using Cachet (Sang, Bacchus, Beame, Kautz, & Pitassi, 2004), one
of the most powerful systems to date for exact weighted model counting in CNFs.
In general, the weighted CNFs and the weights of such formulas are specified as follows. Let
V = {V1 , . . . , Vn } be a set of propositional variables with Dom(Vi ) = {vi , ¬vi }, and let ̟ :
S
0+ be a non-negative, real-valued weight function from the literals of V. For any
i Dom(Vi ) → R
partial assignment π to V,Q
the weight ̟(π) of this assignment is defined as the product of its literals’
weights, that is, ̟(π) = l∈π ̟(l). Finally, a propositional logic formula φ is called weighted if it
is defined over such a weighted set of propositional variables. For any weighted formula φ over V,
the weight ̟(φ) is defined as the sum of the weights of all the complete assignments to V satisfying
φ, that is,
X
̟(φ) =
̟(π)δ (π |= φ),
π∈Dom(V)

where Dom(V) = ×i Dom(Vi ). For instance, if for all variables Vi we have ̟(vi ) = ̟(¬vi ) = 1,
then ̟(φ) simply stands for the number of complete assignments to V that satisfy φ.
Given an initial belief state BN NbI , and a sequence of actions a = ha1 , . . . , am i applicable in
bI , here we describe how the weighted CNF encoding φ(Nba ) (or φ(ba ), for short) of the belief state
ba is built and used in Probabilistic-FF. First, we formally specify the generic scheme introduced
by Sang et al. (2005) for encoding a BN N over variables X into a weighted CNF φ(N ). The
encoding formula φ(N ) contains two sets of variables. First, for each variable Z ∈ X and each
value z ∈ Dom(Z), the formula φ(N ) contains a state proposition with literals {z, ¬z}, weighted
as ̟(z) = ̟(¬z) = 1. These state propositions act in φ(ba ) as regular SAT propositions. Now,
for each variable Z ∈ Xba , let Dom(Z) = {z1 , . . . , zk } be an arbitrary fixed ordering of Dom(Z).
Recall that each row TZ [i] in the CPT of Z corresponds to an assignment ζi (or a set of such assignments) to P a(Z). Thus, the number of rows in TZ is upper bounded by the number of different
assignments to P a(Z), but (as it happens in our case) it can be significantly lower if the dependence of Z on P a(Z) induces a substantial local structure. Following the ordering of Dom(Z) as
above, the entry TZ [i, j] contains the conditional probability of P r(zj | ζi ). For every CPT entry
574

P ROBABILISTIC -FF

procedure basic-WMC(φ)
if φ = ∅ return 1
if φ has an empty clause return 0
select a variable V ∈ φ
return basic-WMC(φ|v ) · ̟(v) + basic-WMC(φ|¬v ) · ̟(¬v)
Figure 3: Basic DPPL-style weighted model counting.
TZ [i, j] but the last one (i.e., TZ [i, k]), the formula φ(N ) contains a chance proposition with literals
{hzji i, ¬hzji i}. These chance variables aim at capturing the probabilistic information from the CPTs
of Nba . Specifically, the weight of the literal hzji i is set to P r(zj | ζi , ¬z1 , . . . , ¬zj−1 ), that is to
conditional probability that the entry is true, given that the row is true, and no prior entry in the row
is true:
TZ [i, j]
Pj−1
1 − k=1 TZ [i, k]


i
̟ ¬hzj i = 1 − ̟ hzji i

̟ hzji i =

(10)

Considering the clauses of φ(N ), for each variable Z ∈ X , and each CPT entry TZ [i, j], the
formula φ(N ) contains a clause

i
ζi ∧ ¬hz1i i ∧ · · · ∧ ¬hzj−1
i ∧ hzji i → zj ,
(11)

where ζi is a conjunction of the literals forming the assignment ζi ∈ Dom(P a(Z)). These clauses
ensure that the weights of the complete assignments to the variables of φ(N ) are equal to the probability of the corresponding atomic events as postulated by the BN N . To illustrate the construction
in Equations 10-11, let boolean variables A and B be the parents of a ternary variable C (with
Dom(C) = {C1 , C2 , C3 }) in some BN, and let P r(C1 |A, ¬B) = 0.2, P r(C2 |A, ¬B) = 0.4, and
P r(C3 |A, ¬B) = 0.4. Let the raw corresponding to the assignment A, ¬B to P a(C) be the i-th
row of the CPT TC . In the encoding of this BN, the first two entries of this raw of TC are captured
by a pair of respective chance propositions
hC1i i, and hC2i i. According
to Equation 10, the weights


0.4
i
i
of these propositions are set to ̟ hC1 i = 0.2, and ̟ hC1 i = 1−0.2 = 0.5. Then, according to
Equation 11, the encoding contains three clauses

¬A ∨ B ∨ ¬hC1i i ∨ C1

¬A ∨ B ∨ hC1i i ∨ ¬hC2i i ∨ C2

¬A ∨ B ∨ hC1i i ∨ hC2i i ∨ C3

Finally, for each variable Z ∈ X , the formula φ(N ) contains a standard set of clauses encoding
the “exactly one” relationship between the state propositions capturing the value of Z. This accomplishes the encoding of N into φ(N ). In the next Section 3.3 we illustrate this encoding on the
belief state BN from our running example.
The weighted CNF encoding φ(ba ) of the belief state BN Nba provides the input to a weighted
model counting procedure. A simple recursive DPPL-style procedure basic-WMC underlying Cachet (Sang et al., 2004) is depicted in Figure 3, where the formula φ|v is obtained from φ by setting
575

D OMSHLAK & H OFFMANN

the literal v to true. Theorem 3 by Sang et al. (2005) shows that if φ is a weighted CNF encoding
of a BN N , and P r(Q|E) is a general query with respect to N , query Q, and evidence E, then we
have:
basic-WMC(φ ∧ Q ∧ E)
P r(Q|E) =
,
(12)
basic-WMC(φ ∧ E)
where query Q and evidence E can in fact be arbitrary formulas in propositional logic. Note that,
in a special (and very relevant to us) case of empty evidence, Equation 12 reduces to P r(Q) =
basic-WMC(φ∧Q), that is, a single call to the basic-WMC procedure. Corollary 3 is then immediate
from our Proposition 1 and Theorem 3 by Sang et al. (2005).
Corollary 3 Let (A, bI , G, θ) be a probabilistic planning task with a BN NbI describing bI , and a
be an m-step sequence of actions applicable in bI . The probability ba (G) that a achieves G starting
from bI is given by:
ba (G) = WMC (φ(ba ) ∧ G(m)) ,
(13)
where G(m) is a conjunction of the goal literals time-stamped with the time endpoint m of a.
3.3 Example: Weighted CNF Encoding of Belief States
We now illustrate the generic BN-to-WCNF encoding scheme of Sang et al. (2005) on the belief
state BN Nba from our running example in Figure 2.
For 0 ≤ i ≤ 2, we introduce time-stamped state propositions r1 (i), r2 (i), b1 (i), b2 (i). Likewise,
we introduce four state propositions ε1 (1), ε2 (1), ε3 (1), ε′1 (1) corresponding to the values of the
variable Y(1) . The first set of clauses in φ(ba ) ensure the “exactly one” relationship between the
state propositions capturing the value of a variable in Nba :

ε1 (1) ∨ ε2 (1) ∨ ε3 (1) ∨ ε′1 (1) ,

1≤i<j≤4 :

(¬yi (1) ∨ ¬yj (1)) ,
0≤i≤2 :

(14)

(r1 (i) ∨ r2 (i)) , (¬r1 (i) ∨ ¬r2 (i))
(b1 (i) ∨ b2 (i)) , (¬b1 (i) ∨ ¬b2 (i))
Now we proceed with encoding the CPTs of Nba . The root nodes have only one row in their
CPTs so their chance propositions can be identified with the corresponding state variables (Sang
et al., 2005). Hence, for the root variable R(0) we need neither additional clauses nor special
chance propositions, but the state proposition r1 (0) of φ(ba ) is treated as a chance proposition
with ̟ (r1 (0)) = 0.9.
Encoding of the variable B(0) is a bit more involved. The CPT TB(0) contains two (content-wise
different) rows corresponding to the “given r1 ” and “given r2 ” cases, and both these cases induce
a non-deterministic dependence of B(0) on R(0) . To encode the content of TB(0) we introduce
two chance variables hb1 (0)1 i and hb1 (0)2 i with ̟(hb1 (0)1 i) = 0.7 and ̟(hb1 (0)2 i) = 0.2. The
positive literals of hb1 (0)1 i and hb1 (0)2 i capture the events “b1 given r1 ” and “b1 given r2 ”, while
the negations ¬hb1 (0)1 i and ¬hb1 (0)2 i capture the complementary events “b2 given r2 ” and “b2
given r2 ”, respectively. Now consider the “given r1 ” row in TB(0) . To encode this row, we need
576

P ROBABILISTIC -FF



φ(ba ) to contain r1 (0) ∧ hb1 (0)1 i → b1 (0) and r1 (0) ∧ ¬hb1 (0)1 i → b2 (0). Similar encoding
is required for the row “given r2 ”, and thus the encoding of TB 0 introduces four additional clauses:


¬r1 (0) ∨ ¬hb1 (0)1 i ∨ b1 (0) , ¬r1 (0) ∨ hb1 (0)1 i ∨ b2 (0)


(15)
¬r2 (0) ∨ ¬hb1 (0)2 i ∨ b1 (0) , ¬r2 (0) ∨ hb1 (0)2 i ∨ b2 (0)

Having finished with the NbI part of Nba , we proceed with encoding the variable Y(1) corresponding to the probabilistic action move-b-right. To encode the first row of TY(1) we introduce three chance propositions hε1 (1)1 i, hε2 (1)1 i, and hε3 (1)1 i; in general, no chance variables are needed for the last entries of the CPT rows. The weights of these chance propositions
0.2
= 0.6(6), and
are set according to Equation 10 to ̟ hε1 (1)1 i = 0.7, ̟ hε2 (1)1 i = 1−0.7

0.1
1
̟ hε3 (1) i = 1−0.9 = 0.1. Using these chance propositions, we add to φ(ba ) four clauses as in
Equation 11, notably the first four clauses of Equation 16 below.
Proceeding the second row of TY(1) , observe that the value of R(0) and B(0) in this case fully
determines the value of Y(1) . This deterministic dependence can be encoded without using any
chance propositions using the last two clauses in Equation 16.

¬r1 (0) ∨ ¬b1 (0) ∨ ¬hε1 (1)1 i ∨ ε1 (1) ,

¬r1 (0) ∨ ¬b1 (0) ∨ hε1 (1)1 i ∨ ¬hε2 (1)1 i ∨ ε2 (1) ,

¬r1 (0) ∨ ¬b1 (0) ∨ hε1 (1)1 i ∨ hε2 (1)1 i ∨ ¬hε3 (1)1 i ∨ ε3 (1) ,
(16)

¬r1 (0) ∨ ¬b1 (0) ∨ hε1 (1)1 i ∨ hε2 (1)1 i ∨ hε3 (1)1 i ∨ ε′1 (1) ,


r1 (0) ∨ ¬ε′1 (1) , b1 (0) ∨ ¬ε′1 (1)

Using the state/chance variables introduced for R0 , B 0 , and Y(1) , we encode the CPTs of R(1)
and B(1) as:
R(1) : (¬ε1 (1) ∨ r2 (1)) , (¬ε2 (1) ∨ r2 (1)) ,

(¬ε3 (1) ∨ ¬r1 (0) ∨ r1 (1)) , ¬ε′1 (1) ∨ ¬r1 (0) ∨ r1 (1) ,

(¬ε3 (1) ∨ ¬r1 (0) ∨ r1 (1)) , ¬ε′1 (1) ∨ ¬r2 (0) ∨ r2 (1)

B(1) : (¬ε1 (1) ∨ b2 (1)) ,

(17)

(ε1 (1) ∨ ¬b1 (0) ∨ b1 (1)) ,
(ε1 (1) ∨ ¬b2 (0) ∨ b2 (1))
Since the CPTs of both R(1) and B(1) are completely deterministic, their encoding as well is using
no chance propositions. Finally, we encode the (deterministic) CPTs of R(2) and B(2) as:
R(2) : (r1 (2))
B(2) : (¬b1 (1) ∨ b1 (2))

(18)

(¬b2 (1) ∨ b2 (2))
where the unary clause (r1 (2)) is a reduction of (¬r1 (1) ∨ r1 (2)) and (¬r2 (1) ∨ r1 (2)). This accomplishes our encoding of φ(ba ).
577

D OMSHLAK & H OFFMANN

3.4 From Conformant-FF to Probabilistic-FF
Besides the fact that weighted model counting is attractive for the kinds of BNs arising in our context, the weighted CNF representation of belief states works extremely well with the ideas underlying Conformant-FF (Hoffmann & Brafman, 2006). This was outlined in the introduction already;
here we give a few more details.
As stated, Conformant-FF does a forward search in a non-probabilistic belief space in which
each belief state corresponds to a set of world states considered to be possible. The main trick of
Conformant-FF is the use of CNF formulas for an implicit representation of belief states, where
formulas φ(a) encode the semantics of executing action sequence a in the initial belief state. Facts
known to be true or false are inferred from these formulas. This computation of only a partial
knowledge constitutes a lazy kind of belief state representation, in comparison to other approaches
that use explicit enumeration (Bonet & Geffner, 2000) or BDDs (Bertoli, Cimatti, Pistore, Roveri,
& Traverso, 2001) to fully represent belief states. The basic ideas underlying Probabilistic-FF are:
(i) Define time-stamped Bayesian Networks (BN) describing probabilistic belief states (Section 3.1 above).
(ii) Extend Conformant-FF’s belief state CNFs to model these BN (Section 3.2 above).
(iii) In addition to the SAT reasoning used by Conformant-FF, use weighted model-counting to
determine whether the probability of the (unknown) goals in a belief state is high enough
(directly below).
(iv) Introduce approximate probabilistic reasoning into Conformant-FF’s heuristic function (Section 4 below).
In more detail, given a probabilistic planning task (A, bI , G, θ), a belief state ba corresponding to
some applicable in bI m-step action sequence a, and a proposition q ∈ P, we say that q is known
in ba if ba (q) = 1, negatively known in ba if ba (q) = 0, and unknown in ba , otherwise. We begin
with determining whether each q is known, negatively known, or unknown at time m. Re-using the
Conformant-FF machinery, this classification requires up to two SAT tests of φ(ba ) ∧ ¬q(m) and
φ(ba ) ∧ q(m), respectively. The information provided by this classification is used threefold. First,
if a subgoal g ∈ G is negatively known at time m, then we have ba (G) = 0. On the other extreme,
if all the subgoals of G are known at time m, then we have ba (G) = 1. Finally, if some subgoals of
G are known and the rest are unknown at time m, then we accomplish evaluating the belief state ba
by testing whether
ba (G) = WMC (φ(ba ) ∧ G(m)) ≥ θ.

(19)

Note also that having the sets of all (positively/negatively) known propositions at all time steps up
to m allows us to significantly simplify the CNF formula φ(ba ) ∧ G(m) by inserting into it the
corresponding values of known propositions.
After evaluating the considered action sequence a, if we get ba (G) ≥ θ, then we are done.
Otherwise, the forward search continues, and the actions that are applicable in ba (and thus used to
generate the successor belief states) are actions whose preconditions are all known in ba .
578

P ROBABILISTIC -FF

4. Heuristic Function
The key component of any heuristic search procedure is the heuristic function. The quality (informedness) and computational cost of that function determine the performance of the search. The
heuristic function is usually obtained from solutions to a relaxation of the actual problem of interest (Pearl, 1984; Russell & Norvig, 2004). In classical planning, a successful idea has been to
use a relaxation that ignores the delete effects of the actions (McDermott, 1999; Bonet & Geffner,
2001; Hoffmann & Nebel, 2001). In particular, the heuristic of the FF planning system is based on
the notion of relaxed plan, which is a plan that achieves the goals while assuming that all delete
lists of actions are empty. The relaxed plan is computed using a Graphplan-style (Blum & Furst,
1997) technique combining a forward chaining graph construction phase with a backward chaining
plan extraction phase. The heuristic value h(w) that FF provides to a world state w encountered
during the search is the length of the relaxed plan from w. In Conformant-FF, this methodology was
extended to the setting of conformant planning under initial state uncertainty (without uncertainty
about action effects). Herein, we extend Conformant-FF’s machinery to handle probabilistic initial
states and effects. Section 4.1 provides background on the techniques used in FF and ConformantFF, then Sections 4.2 and 4.4 detail our algorithms for the forward and backward chaining phases in
Probabilistic-FF, respectively. These algorithms for the two phases of the Probabilistic-FF heuristic
computation are illustrated on our running example in Sections 4.3 and 4.5, respectively.
4.1 FF and Conformant-FF
We specify how relaxed plans are computed in FF; we provide a coarse sketch of how they are
computed in Conformant-FF. The purpose of the latter is only to slowly prepare the reader for what
is to come: Conformant-FF’s techniques are re-used for Probabilistic-FF anyway, and hence will be
described in full detail as part of Sections 4.2 and 4.4.
Formally, relaxed plans in classical planning are computed as follows. Starting from w, FF
builds a relaxed planning graph as a sequence of alternating proposition layers P (t) and action
layers A(t), where P (0) is the same as w, A(t) is the set of all actions whose preconditions are
contained in P (t), and P (t + 1) is obtained from P (t) by including the add effects (with fulfilled
conditions) of the actions in A(t). That is, P (t) always contains those facts that will be true if one
would execute (the relaxed versions of) all actions at the earlier layers up to A(t − 1). The relaxed
planning graph is constructed either until it reaches a propositional layer P (m) that contains all
the goals, or until the construction reaches a fixpoint P (t) = P (t + 1) without reaching the goals.
The latter case corresponds to (all) situations in which a relaxed plan does not exist, and because
existence of a relaxed plan is a necessary condition for the existence of a real plan, the state w is
excluded from the search space by setting h(w) = ∞. In the former case of G ⊆ P (m), a relaxed
plan is a subset of actions in A(1), . . . , A(m) that suffices to achieve the goals (under ignoring the
delete lists), and it can be extracted by a simple backchaining loop: For each goal in P (m), select
an action in A(1), . . . , A(m) that achieves this goal, and iterate the process by considering those
actions’ preconditions and the respective effect conditions as new subgoals. The heuristic estimate
h(w) is then set to the length of the extracted relaxed plan, that is, to the number of actions selected
in this backchaining process.
Aiming at extending the machinery of FF to conformant planning, in Conformant-FF, Hoffmann and Brafman (2006) suggested to extend the relaxed planning graph with additional fact layers uP (t) containing the facts unknown at time t, and then to reason about when such unknown
579

D OMSHLAK & H OFFMANN

facts become known in the relaxed planning graph. As the complexity of this type of reasoning is
prohibitive, Conformant-FF further relaxes the planning task by ignoring not only the delete lists,
but also all but one of the unknown conditions of each action effect. That is, if action a appears
in layer A(t), and for effect e of a we have con(e) ⊆ P (t) ∪ uP (t) and con(e) ∩ uP (t) 6= ∅,
then con(e) ∩ uP (t) is arbitrarily reduced to contain exactly one literal, and reasoning is done as if
con(e) had this reduced form from the beginning.
V
This relaxation converts implications ( c∈con(e)∩uP (t) c(t)) → q(t + 1) that the action effects
induce between unknown propositions into their 2-projections that take the form of binary implications c(t) → q(t + 1), for arbitrary c ∈ con(e) ∩ uP (t). Due to the layered structure of the
planning graph, the set of all these binary implications c(t) → q(t + 1) can be seen as forming a
directed acyclic graph Imp. Under the given relaxations, this graph captures exactly all dependencies between the truth of propositions over time. Hence, checking whether a proposition q becomes
known at time t can be done as follows. First, backchain over the implication edges of Imp that end
in q(t), and collect the set support(q(t)) of leafs4 that are reached. Then, if Φ is the CNF formula
describing the possible initial states, test by a SAT check whether
_
Φ→
l
l∈support(q(t))

This test will succeed if and only if at least one of the leafs in support(q(t)) is true in every possible
initial state. Under the given relaxations, this is the case if and only if, when applying all actions in
the relaxed planning graph, q will always be true at time t.5
The process of extracting a relaxed plan from the constructed conformant relaxed planning
graph is an extension of FF’s respective process with machinery that selects actions responsible for
relevant paths in Imp. The overall Conformant-FF heuristic machinery is sound and complete for
relaxed tasks, and yields a heuristic function that is highly informative across a range of challenging
domains (Hoffmann & Brafman, 2006).
In this work, we adopt Conformant-FF’s relaxations, ignoring the delete lists of the action effects, as well as all but one of the propositions in the effect’s condition. Accordingly, we adopt the
following notations from Conformant-FF. Given a set of actions A, we denote by |+
1 any function
+
from A into the set of all possible actions, such that |1 maps each a ∈ A to the action similar to a
but with empty delete lists and with all but one conditioning propositions of each effect removed;
+
+
for |+
denote the action set obtained by applying |+
1 (a), we write a|
1 . By A|1 we
1 to all the actions
	
+
+
a
we
denote
by
a|
of A, that is, A|+
=
a|
|
a
∈
A
.
For
an
action
sequence
1 the sequence of
1
1
+
actions obtained by applying |1 to every action along a, that is,
(
hi,
a = hi
a|+
.
1 =
+
+
′
ha|1 i · a |1 , a = hai · a′
For a probabilistic planning task (A, bI , G, θ), the task (A|+
1 , bI , G, θ) is called a relaxation of
+
+
(A, bI , G, θ). Finally, if a|1 is a plan for (A|1 , bI , G, θ), then a is called a relaxed plan for
(A, bI , G, θ).
4. Following the Conformant-FF terminology, by “leafs” we refer to the nodes having zero in-degree.
5. Note here that it would be possible to do a full SAT check, without any 2-projection (without relying on Imp), to see
whether q becomes known at t. However, as indicated above, doing such a full check for every unknown proposition
at every level of the relaxed planning graph for every search state would very likely be too expensive, computationally.

580

P ROBABILISTIC -FF

In the next two sections we describe the machinery underlying the Probabilistic-FF heuristic
estimation. Due to the similarity between the conceptual relaxations used in Probabilistic-FF and
Conformant-FF, Probabilistic-FF inherits almost all of Conformant-FF’s machinery. Of course,
the new contributions are those algorithms dealing with probabilistic belief states and probabilistic
actions.
4.2 Probabilistic Relaxed Planning Graphs
Like FF and Conformant-FF, Probabilistic-FF computes its heuristic function in two steps, the first
one chaining forward to build a relaxed planning graph, and the second step chaining backward to
extract a relaxed plan. In this section, we describe in detail Probabilistic-FF’s forward chaining step,
building a probabilistic relaxed planning graph (or PRPG, for short). In Section 4.4, we then show
how one can extract a (probabilistic) relaxed plan from the PRPG. We provide a detailed illustration
of the PRPG construction process on the basis of our running example; since the illustration is
lengthy, it is moved to a separate Section 4.3.
The algorithms building a PRPG are quite involved; it is instructive to first consider (some
of) the key points before delving into the details. The main issue is, of course, that we need to
extend Conformant-FF’s machinery with the ability to determine when the goal set is sufficiently
likely, rather than when it is known to be true for sure. To achieve that, we must introduce into
relaxed planning some effective reasoning about both the probabilistic initial state, and the effects
of probabilistic actions. It turns out that such a reasoning can be obtained by a certain weighted
extension of the implication graph. In a nutshell, if we want to determine how likely it is that a fact
q is true at a time t, then we propagate certain weights backwards through the implication graph,
starting in q(t); the weight of q(t) is set to 1, and the weight for any p(t′ ) gives an estimate of the
probability of achieving q at t given that p holds at t′ . Computing this probability exactly would,
of course, be too expensive. Our estimation is based on assuming independence of the various
probabilistic events involved. This is a choice that we made very carefully; we experimented widely
with various other options before deciding in favor of this technique.
Any simplifying assumption in the weight propagation constitutes, of course, another relaxation,
on top of the relaxations we already inherited from Conformant-FF. The particularly problematic
aspect of assuming independence is that it is not an under-estimating technique. The actual weight
of a node p(t′ ) – the probability of achieving q at t given that p holds at t′ – may be lower than our
estimate. In effect, the PRPG may decide wrongly that a relaxed plan exists: even if we execute
all relaxed actions contained in the successful PRPG, the probability of achieving the goal by this
execution may be less than the required threshold. In other words, we lose the soundness (relative
to relaxed tasks) of the relaxed planning process.
We experimented with an alternative weight propagation method, based on an opposite assumption, that the relevant probabilistic events always co-occur, and that hence the weights must be
propagated according to simple maximization operations. This propagation method yielded very
uninformative heuristic values, and hence inacceptable empirical behaviour of Probabilistic-FF,
even in very simple benchmarks. In our view, it seems unlikely that an under-estimating yet informative and efficient weight computation exists. We further experimented with some alternative
non under-estimating propagation schemes, in particular one based on assuming that the probabilistic events are completely disjoint (and hence weights should be added); these schemes gave better
581

D OMSHLAK & H OFFMANN

performance than maximization, but lagged far behind the independence assumption in the more
challenging benchmarks.
Let us now get into the actual algorithm building a PRPG. A coarse outline of the algorithm is as
follows. The PRPG is built in a layer-wise fashion, in each iteration extending the PRPG, reaching
up to time t, by another layer, reaching up to time t + 1. The actions in the new step are those whose
preconditions are known to hold at t. Effects conditioned on unknown facts (note here the reduction
of effect conditions to a single fact) constitute new edges in the implication graph. In difference to
Conformant-FF, we don’t obtain a single edge from condition to add effect; instead, we obtain edges
from the condition to “chance nodes”, where each chance node represents a probabilistic outcome of
the effect; the chance nodes, in turn, are linked by edges to their respective add effects. The weights
of the chance nodes are set to the probabilities of the respective outcomes, the weights of all other
nodes are set to 1. These weights are “static weights” which are not “dynamically” modified by
weight propagation; rather, the static weights form an input to the propagation.
Once all implication graph edges are inserted at a layer, the algorithm checks whether any new
facts become known. This check is done very much like the corresponding check in Conformant-FF,
by testing whether the disjunction of the support leafs for a proposition p at t + 1 is implied by the
initial state formula. The two differences to Conformant-FF are: (1) Only leafs are relevant whose
dynamic weight is 1 (otherwise, achieving a leaf is not guaranteed to accomplish p at t + 1). (2)
Another reason for p to become known may be that all outcomes of an unconditional effect (or an
effect with known condition) result in achievement of p at time t + 1. We elegantly formulate the
overall test by a single implication test over support leafs whose dynamic weight equals their own
weight.
Like FF’s and Conformant-FF’s algorithms, the PRPG process has two termination criteria. The
PRPG terminates positively if the goal probability is high enough at time t; the PRPG terminates
negatively if, from t to t + 1, nothing has changed that may result in a higher goal propability at
some future t′ . The goal probability in a layer t is computed based on weighted model counting over
a formula derived from the support leafs of all goals not known to be true. The criteria for negative
termination check: whether any new facts have become known or unknown (not negatively known);
whether any possibly relevant new support leafs have appeared; and whether the goal probability
has increased. If neither is the case, then we can stop safely—if the PRPG terminates unsuccessfully
then we have a guarantee that there is no relaxed plan, and that the corresponding belief is hence a
dead end.
Let us get into the details. Figure 4 depicts the main routine for building the PRPG for a belief
state ba . As we already specified, the sets P (t), uP (t), and A(t) contain the propositions that are
known to hold at time t (hold at t with probability 1), the propositions that are unknown to hold at
time t (hold at t with probability less than 1 but greater than 0), and actions that are known to be
applicable at time t, respectively. The layers t ≥ 0 of PRPG capture applying the relaxed actions
starting from ba . The layers −m to −1 of PRPG correspond to the m-step action sequence a leading
from the initial belief state to the belief state in question ba . We inherit the latter technique from
Conformant-FF; in a sense, the PRPG “reasons about the past”. This may look confusing at first
sight, but it has a simple reason. Imagine the PRPG starts at level 0 instead. Then, to check whether
a proposition becomes known, we have to do SAT tests regarding support leafs against the belief
state formula, φ(ba ), instead of the initial state formula (similarly for weighted model counting
to test whether the goal is likely enough). Testing against φ(ba ) is possible, but very expensive
582

P ROBABILISTIC -FF

procedure build-PRPG(a, A, φ(NbI ), G, θ, |+
1 ),
returns a Bool saying if there is a relaxed plan for the belief state
given by a = ha−m , . . . , a−1 i, and
builds data structures from which a relaxed plan can be extracted
Φ := φ(NbI ), Imp := ∅
P (−m) := {p | p is known in Φ}, uP (−m) := {p | p is unknown in Φ}
for t := −m · · · − 1 do
A(t) := {at |+
1 } ∪ N OOP S
build-timestep(t, A(t))
endfor
t := 0
while get-P(t, G) < θ do
A(t) := {a|+
1 | a ∈ A, pre(a) ⊆ P (t)} ∪ N OOP S
build-timestep(t, A(t))
if P (t + 1) = P (t) and
uP (t + 1) = uP (t) and
∀p ∈ uP (t + 1) : uP (−m) ∩ support(p(t + 1)) = uP (−m) ∩ support(p(t)) and
get-P(t + 1, G) = get-P(t, G) then
return FALSE
endif
t := t + 1
endwhile
T := t, return TRUE

Figure 4: Main routine for building a probabilistic relaxed planning graph (PRPG).
computationally.6 The negative-index layers chain the implication graph all the way back to the
initial state, and hence enable us to perform SAT tests against the – typically much smaller – initial
state formula.
Returning to Figure 4, the PRPG is initialized with an empty implication set Imp, P (−m)
and uP (−m) are assigned the propositions that are known and unknown in the initial belief state,
and a weighted CNF formula Φ is initialized with φ(NbI ). Φ is the formula against which implication/weighted model checking tests are run when asking whether a proposition becomes known/whether
the goal is likely enough. While the PRPG is built, Φ is incrementally extended with further clauses
to capture the behavior of different effect outcomes.
The for loop builds the sets P and uP for the a’s time steps −m · · · − 1 by iterative invocation
of the build-timestep procedure that each time expands PRPG by a single time level. At each
iteration −m ≤ t ≤ −1, the sets P (t + 1) and uP (t + 1) are made to contain the propositions
that are known/unknown after applying the relaxed version of the action at ∈ a (remember that
a = ha1 , . . . , am i). To simplify the presentation, each action set A(t) contains a set of dummy
actions N OOP S that simply
transport all

	 the propositions from time layer t to time layer t+1. More
formally, N OOP S = noopp | p ∈ P , where pre(noopp ) = ∅, E(noopp ) = {({p}, {ε})}, and
ε = (1.0, {p}, ∅)}).
6. In Conformant-FF, this configuration is implemented as an option; it significantly slows down the search in most
domains, and brings advantages only in a few cases.

583

D OMSHLAK & H OFFMANN

The subsequent while loop constructs the relaxed planning graph from layer 0 onwards by,
again, iterative invocation of the build-timestep procedure. The actions in each layer t ≥ 0 are
relaxations of those actions whose preconditions are known to hold at time t with certainty. This
iterative construction is controlled by two termination tests. First, if the goal is estimated to hold at
layer t with probability higher than θ, then we know that a relaxed plan estimate can be extracted.
Otherwise, if the graph reaches a fix point, then we know that no relaxed (and thus, no real) plan
from bI exists. We postpone the discussion of these two termination criteria, and now focus on the
time layer construction procedure build-timestep.
procedure build-timestep(t, A),
builds P (t + 1), uP (t + 1), and the implication edges from t to t + 1,
as induced by the action set A
P (t + 1) := P (t), uP (t + 1) := ∅
for all effects e of an action a ∈ A, con(e) ∈ P (t) ∪ uP (t) do
for all ε ∈ Λ(e) do
uP (t + 1) := uP (t + 1) ∪ add(ε)
introduce new fact ε(t) with ̟(ε(t)) = P r(ε)
Imp := Imp ∪ {(ε(t), p(t + 1)) | p ∈ add(ε)}
endfor
if con(e) ∈ uP (t)
Sthen
Imp := Imp ∪ ε∈Λ(e) {(con(e)(t), ε(t))}
else
 V
Φ := Φ ∧ ∨ε∈Λ(e) ε(t) ∧ ε,ε′ ∈Λ(e) (¬ε(t) ∨ ¬ε′ (t))
endif
endfor
for all p ∈ uP (t + 1) do
build-w-impleafs(p(t + 1), Imp)
support(p(t + 1)) := {l | l ∈ leafs(Imp→p(t+1) ) ∧ ̟p(t+1) (l) = ̟(l)}
W
if Φ → l∈support(p(t+1)) l then P (t + 1) := P (t + 1) ∪ {p} endif
endfor
uP (t + 1) := uP (t + 1) \ P (t + 1)

Figure 5: Building a time step of the PRPG.
The build-timestep procedure is shown in Figure 5. The first for loop of build-timestep proceeds
over all outcomes of (relaxed) actions in the given set A that may occur at time t. For each such
probabilistic outcome we introduce a new chance proposition weighted by the conditional likelihood
of that outcome.7 Having that, we extend Imp with binary implications from this new chance
proposition to the add list of the outcome. If we are uncertain about the condition con(e) of the
corresponding effect at time t, that is, we have con(e) ∈ uP (t), then we also add implications
from con(e) to the chance propositions created for the outcomes of e. Otherwise, if con(e) is
known at time t, then there is no uncertainty about our ability to make the effect e to hold at time
t. In this case, we do not “ground” the chance propositions created for the outcomes of e into the
implication graph, but simply extend the running formula Φ with clauses capturing the “exactly
one” relationship between these chance propositions corresponding to the alternative outcomes of e
7. Of course, in our implementation we have a special case treatment for deterministic actions, using no chance nodes
(rather than a single “chance node” with static weight 1).

584

P ROBABILISTIC -FF

at time t. This way, the probabilistic uncertainty about the outcome of e can be treated as if being a
property of the initial belief state bI ; This is the only type of knowledge we add into the knowledge
base formula Φ after initializing it in build-PRPG to φ(NbI ).
Notation
Impv→u
Imp→u
leafs(Imp′ )
E(Imp′ )

Description
The graph containing exactly all the paths from node v to node u in Imp.
The subgraph of Imp formed by node u and all the ancestors of u in Imp.
The set of all zero in-degree nodes in the subgraph Imp′ of Imp.
The set of time-stamped action effects responsible for the implication edges
of the subgraph Imp′ of Imp.
Table 2: Overview of notations around the implication graph.

The second for loop checks whether a proposition p, unknown at time t, becomes known at
time t + 1. This part of the build-timestep procedure is somewhat more involved; Table 2 provides
an overview of the main notations used in the follows when discussing the various uses of the
implication graph Imp.
First thing in the second for loop of build-timestep, a call to build-w-impleafs procedure associates each node v(t′ ) in Imp→p(t+1) with an estimate ̟p(t+1) (v(t′ )) on the probability of achieving
p at time t + 1 by the effects E(Impv(t′ )→p(t+1) ), given that v holds at time t′ . In other words, the
dynamic weight (according to p(t + 1)) of the implication graph nodes is computed. Note that v(t′ )
can be either a time-stamped proposition q(t′ ) for some q ∈ P, or a chance proposition ε(t′ ) for
some probabilistic outcome ε.
We will discuss the build-w-impleafs procedure in detail below. For proceeding to understand
the second for loop of build-timestep, the main thing we need to know is the following lemma:
Lemma 4 Given a node v(t′ ) ∈ Imp→p(t+1) , we have ̟p(t+1) (v(t′ )) = ̟ (v(t′ )) if and only if,
given v at time t′ , the sequence of effects E(Impv(t′ )→p(t+1) ) achieves p at t + 1 with probability 1.
In words, v(t′ ) leads to p(t + 1) with certainty iff the dynamic weight of v(t′ ) equals its static
weight. This is a simple consequence of how the weight propagation is arranged; it should hold true
for any reasonable weight propagation scheme (“do not mark a node as certain if it is not”). A full
proof of the lemma appears in Appendix A on pp. 613.
Re-consider the second for loop of build-timestep. What happens is the following. Having
finished the build-w-impleafs weight propagation for p at time t + 1, we
1. collect all the leafs support(p(t + 1)) of Imp→p(t) that meet the criteria of Lemma 4, and
2. check (by a call to a SAT solver) whether the knowledge-base formula Φ implies the disjunction of these leafs.
If the implication holds, then the examined fact p at time t is added to the set of facts known at time
t. Finally, the procedure removes from the set of facts that are known to possibly hold at time t + 1
all those facts that were just proven to hold at time t + 1 with certainty.
To understand the above, consider the following. With Lemma 4, support(p(t + 1)) contains
exactly the set of leafs achieving which will lead to p(t + 1) with certainty. Hence we can basically
585

D OMSHLAK & H OFFMANN

procedure build-w-impleafs (p(t), Imp)
top-down propagation of weights ̟p(t) from p(t) to all nodes in Imp→p(t)
̟p(t) (p(t)) := 1
for decreasing time steps t′ := (t − 1) . . . (−m) do
for all chance nodes ε(t′ ) ∈ Imp→p(t)
 do

Q
1 − ̟p(t) (r(t′ + 1))
α := r∈add(ε),r(t′ +1)∈Imp
→p(t)
̟p(t) (ε(t′ )) := ̟ (ε(t′ )) · (1 − α)
endfor
for all fact nodes q(t′ ) ∈ Imp→p(t) do
α := 1
′
for all a ∈ A(t
∈ E(a), con(e) = q do
i
h ), eP
̟p(t) (ε(t′ ))
α := α · 1 − ε∈Λ(e),ε(t′ )∈Imp
→p(t)
endfor
̟p(t) (q(t′ )) := 1 − α
endfor
endfor

Figure 6: The build-w-impleafs procedure for weight back-propagation over the implication graph.
use the same implication test as in Conformant-FF. Note, however, that the word “basically” in the
previous sentence hides a subtle but important detail. In difference to the situation in ConformantFF, support(p(t + 1)) may contain two kinds of nodes: (1) proposition nodes at the start layer of
the PRPG, i.e., at layer −m corresponding to the initial belief; (2) chance nodes at later layers of
the PRPG, corresponding to outcomes of effects that have no unknown conditions. This is the point
where the discussed above updates onWthe formula Φ are needed—those keep track of alternative
effect outcomes. Hence testing Φ → l∈support(p(t+1)) l is the same as testing whether either: (1) p
is known at t + 1 because it is always triggered with certainty by at least one proposition true in the
initial world; or (2) p is known at t + 1 because it is triggered by all outcomes of an effect that will
appear with certainty. We get the following result:
Lemma 5 Let (A, NbI , G, θ) be a probabilistic planning task, a be a sequence of actions applicable
in bI , and |+
1 be a relaxation function for A. For each time step t ≥ −m, and each proposition p ∈
P, if P (t) is constructed by build-PRPG(a, A, φ(NbI ), G, θ, |+
1 ), then p at time t can be achieved
+
by a relaxed plan starting with a|1
(1) with probability > 0 (that is, p is not negatively known at time t) if and only if p ∈ uP (t)∪P (t),
and
(2) with probability 1 (that is, p is known at time t) if and only if p ∈ P (t).
This is a consequence of the arguments outlined above. The full proof of Lemma 5 is given in
Appendix A on pp. 614.
Let us now consider the weight-propagating8 procedure build-w-impleafs depicted in Figure 6.
This procedure performs a layered, top-down weight propagation from a given node9 p(t) ∈ Imp
8. The weight propagation scheme of the build-w-impleafs procedure is similar in nature to this used in the heuristics
module of the recent probabilistic temporal planner Prottle of Little, Aberdeen, and Thiébaux (2005).
9. Note that the “t” here will be instantiated with t + 1 when called from build-timestep.

586

P ROBABILISTIC -FF

down to the leafs of Imp→p(t) . This order of traversal ensures that each node of Imp→p(t) is processed only after all its successors in Imp→p(t) . For the chance nodes ε(t′ ), the dynamic weight
̟p(t) (ε(t′ )) is set to
1. the probability that the outcome ε takes place at time t′ given that the corresponding action
effect e(ε) does take place at t′ , times
2. an estimate of the probability of achieving p at time t by the effects E(Impε(t′ )→p(t) ).
The first quantity is given by the “global”, static weight ̟ (ε(t′ )) assigned to ε(t′ ) in the first for
loop of build-timestep. The second quantity is derived from the dynamic weights ̟p(t) (r(t′ + 1))
for r ∈ add(ε), computed in the previous iteration of the outermost for loop of build-w-impleafs.
Making a heuristic assumption that the effect sets E(Impr(t′ +1)→p(t) ) for different r ∈ add(ε) are
all pairwise independent, α is then set to the probability of failure to achieve p at t by the effects
E(Impε(t′ )→p(t) ). This computation of α for ε(t′ ) is decomposed over the artifacts of ε, and this
is where the weight propagation starts taking place. For the fact nodes q(t′ ), the dynamic weight
̟p(t) (q(t′ )) is set to the probability that some action effect conditioned on q at time t′ allows
(possibly indirectly) achieving the desired fact p at time t. Making again the heuristic assumption
of independence between various such effects conditioned on q at t′ , computing ̟p(t) (q(t′ )) is
decomposed over the outcomes of these effects.
procedure get-P (t, G)
estimates the probability of achieving G at time p.
if G 6⊆ P (t) ∪ uP (t) then return 0 endif
if G ⊆ P (t) then return 1 endif
for g ∈ G \ P (t) do
for each l ∈ leafs(Imp→g(t) ), introduce a chance proposition hlg i with weight ̟g(t) (l)
W
V
ϕg := ( l∈leafs(Imp→g(t) ) l) ∧ l∈leafs(Imp→g(t) )∩uP (−m) (¬l ∨ hlg i)
endfor
V
return WMC(Φ ∧ g∈G\P (t) ϕg )

Figure 7: Estimating the goal likelihood at a given time step.
What remains to be explained of the build-PRPG procedure are the two termination criteria of
the while loop constructing the planning graph from the layer 0 onwards. The first test is made by
a call to the get-P procedure, and it checks whether the PRPG built to the time layer T contains
a relaxed plan for (A, NbI , G, θ). The get-P procedure is shown in Figure 7. First, if one of the
subgoals is negatively known at time t, then, from Lemma 5, the overall probability of achieving
the goal is 0. On the other extreme, if all the subgoals are known at time t, then the probability of
achieving the goal is 1. The correctness of the latter test is implied by Lemma 5 and non-interference
of relaxed actions. This leaves us with the main case in which we are uncertain about some of the
subgoals. This uncertainty is either due to dependence of these subgoals on the actual initial world
state, or due to achieving these subgoals using probabilistic actions, or due to both. The uncertainty
about the initial state is fully captured by our weighted CNF formula φ(NbI ) ⊆ Φ. Likewise, the
outcomes’ chance propositions ε(t′ ) introduced into the implication graph by the build-timestep
procedure are “chained up” in Imp to the propositions on the add lists of these outcomes, and
587

D OMSHLAK & H OFFMANN

“chained down” in Imp to the unknown (relaxed) conditions of these outcomes, if any. Therefore,
if some action outcome ε at time t′ < t is relevant to achieving a subgoal g ∈ G at time t, then
the corresponding node ε(t′ ) must appear in Imp→g(t) , and its weight will be back-propagated by
build-w-impleafs(g(t), Imp) down to the leafs of Imp→g(t) . The get-P procedure then exploits
these back-propagated estimates by, again, taking a heuristic assumption of independence between
achieving different subgoals. Namely, the probability of achieving the unknown sub-goals G \ P (t)
is estimated by weighted model counting over the formula Φ, conjoined with probabilistic theories
ϕg of achieving each unknown goal g in isolation. To understand the formulas ϕg , consider that, in
order to make g true at t, we must achieve at least one of the leafs l of Imp→g(t) ; hence the left part of
the conjunction. On the other hand, if we make l true, then this achieves g(t) only with (estimated)
probability ̟g(t) (l); hence the right part of the conjunction requires us to “pay the price” if we set
l to true.10
As was explained at the start of this section, the positive PRPG termination test may fire even if
the real goal probability is not high enough. That is, get-P may return a value higher than the real
goal probability, due to the approximation (independence assumption) done in the weight propagation. Of course, due to the same approximation, it may also happen that get-P returns a value lower
than the real goal probability.
The second PRPPG termination test comes to check whether we have reached a point in the
construction of PRPG that allows us to conclude that there is no relaxed plan for (A, NbI , G, θ) that
starts with the given action sequence a. This termination criterion asks whether, from time step t
to time step t + 1, any potentially relevant changes have occurred. A potentially relevant change
would be if the goal-satisfaction probability estimate get-P grows, or if the known and unknown
propositions grow, of if the support leafs of the latter propositions in Imp that correspond to the
initial belief state grow.11 If none occurs, then the same would hold in all future iterations t′ > t,
implying that the required goal satisfaction probability θ would never be reached. In other words,
the PRPG construction is complete.
Theorem 6 Let (A, NbI , G, θ) be a probabilistic planning task, a be a sequence of actions appli+
cable in bI , and |+
1 be a relaxation function for A. If build-PRPG(a, A, φ(NbI ), G, θ, |1 ) returns
+
FALSE, then there is no relaxed plan for (A, bI , G, θ) that starts with a|1 .
Note that Theorem 6 holds despite the approximation done during weight propagation, making
the assumption of probabilistic independence. For Theorem 6 to hold, the only requirement on the
weight propagation is this: if the real weight still grows, then the estimated weight still grows. This
requirement is met under the independence assumption. It would not be met under the assumption of
co-occurence, propagating weights by maximization operations, and thereby conservatively underestimating the weights. With that propagation, if the PRPG fails then we cannot conclude that there
is no plan for the respective belief. This is another good argument (besides the bad quality heuristics
we observed empirically) against using the conservative estimation.
10. If we do not introduce the extra chance propositions hlg i, and instead assign the weight ̟g(t) (l) to l itself, then the
outcome is not correct: we have to “pay” also for setting l to false.
11. To understand the latter, note that PRPG can always be added with more and more replicas of probabilistic actions
irrelevant to achieving the goals, and having effects with known conditions. While these action effects (since they are
irrelevant) will not influence our estimate of goal-satisfaction probability, the chance propositions corresponding to
the outcomes of these effects may become the support leafs of some unknown proposition p. In the latter case, the
set of support leafs support(p(t′ )) will infinitely grow with t′ → ∞, while the projection of support(p(t′ )) on the
initial belief state (that is, support(p(t)) ∩ uP (t)) is guaranteed to reach a fix point.

588

P ROBABILISTIC -FF

The full proof to Theorem 6 is given in Appendix A on pp. 615. The theorem finalizes our
presentation and analysis of the process of constructing probabilistic relaxed planning graphs.
4.3 Example: PRPG Construction
To illustrate the construction of a PRPG by the algorithm in Figures 4-7, let us consider a simplification of our running Examples 1-2 in which
(i) only the actions {move-b-right, move-lef t} constitute the action set A,
(ii) the goal is G = {r1 , b2 }, and the required lower bound on the probability of success θ = 0.9,
(iii) the initial belief state bI is given by the BN NbI as in Example 2, and
(iv) the belief state ba evaluated by the heuristic function corresponds to the actions sequence
a = hmove-b-righti.
The effects/outcomes of the actions A considered in the construction of PRPG are described in
Table 3, where embr is a re-notation of the effect e in Table 1, the effect e′ in Table 1 is effectively
ignored due to the emptiness of its add effects.
a

E(a)

con(e)

con(e)|+
1

Λ(e)

P r(ε)

add(ε)

0.7
0.2
0.1
1.0

{r2 , b2 }
{r2 }
∅
{r1 }

1.0
1.0
1.0
1.0

{r1 }
{r2 }
{b1 }
{b2 }

embr

{r1 , b1 }

{r1 }

aml (move-lef t)

eml

{r2 }

{r2 }

εmbr
1
εmbr
2
εmbr
3
εml

noopr1
noopr2
noopb1
noopb2

er1

{r1 }
{r2 }
{b1 }
{b2 }

{r1 }
{r2 }
{b1 }
{b2 }

εr1
εr2
εb1
εb2

ambr

(move-b-right)

er2
eb1
eb2

Table 3: Actions and their |+
1 relaxation for the PRPG construction example.
The initialization phase of the build-PRPG procedure results in Φ = φ(NbI ), Imp := ∅,
P (−1) = ∅, and uP (−1) = {r1 , r2 , b1 , b2 }. The content of uP (−1) is depicted in the first column
of nodes in Figure 8. The first for loop of build-PRPG (constructing PRPG for the “past” layers
corresponding to a) makes a single iteration, and calls the build-timestep procedure with t = −1
and A(-1) = {ambr } ∪ N OOP S. (In what follows, using the names of the actions we refer to their
mbr is empty, and thus it adds no
|+
1 relaxations as given in Table 3.) The add list of the outcome ε3
nodes to the implication graph. Other than that, the chance nodes introduced to Imp by this call to
build-timestep appear in the second column of Figure 8. The first outer for loop of build-timestep
results in Imp given by columns 1-3 of Figure 8, uP (0) = uP (−1), and no extension of Φ.
In the second outer for loop of build-timestep, the weight propagating procedure build-w-impleafs
is called for each unknown fact p(0) ∈ uP (0) = {r1 (0), r2(0) , b1 (0), b2 (0)}, generating the “p(0)oriented” weights as in Table 4. For each p(0) ∈ uP (0), the set of supporting leafs support(p(0)) =
589

D OMSHLAK & H OFFMANN


WV



00 εml (0)

54






mbr (-1)
mbr
ε

 1 AA

ε1AA (0)
ML
ML



]\
]\












 mbr
  εmbr (0)

 88 2
 qε88 2 (-1)

;
;; 


;;
 qqq
  rrrr
;;


;;
 qq
;;
rrrr
;;
qqq

 r
 ;
 r
 01;;

;; // r1 (0)
;; // r1 (1)
// 1
// ε 1 (-1)
r1 (-1)


ε (0)
;;
;;

;;
;;

;
RS
HI ;
HI ;;




// εr2 (0)
// εr2 (-1)
// r2 (0)
// r2 (1)
r2 (-1)





b1 (-1)



// εb1 (-1)



b2 (-1)



// εb2 (-1)



!

// b1 (0)



// εb1 (0)



// b2 (0)



// εb2 (0)



!



mbr
ε1 (1)

ML
]\



mbr
ε2 (1)
;; 
;;
;;
;;
;;
;;
;;
HI ;;
 r

// 2
// r2 (2)
ε (1)

// b1 (1)



// εb1 (1)



// b2 (1)



// εb2 (1)



// b1 (2)
!

// b2 (2)

Figure 8: The implication graph Imp. The odd columns of nodes depict the sets of unknown propositions uP (t). The even columns of nodes depict the change propositions introduced for
the probabilistic outcomes of the actions A(t).

{p(−1)}, none of them is implied by Φ = NbI , and thus the set of known facts P (0) remains equal
to P (−1) = ∅, and uP (−1) equal to = uP (−1).

̟r1 (0)
̟r2 (0)
̟b1 (0)
̟b2 (0)

t′ = 0
t′ = −1
mbr
mbr
r
r
r1 r2 b1 b2 ε1
ε2
ε 1 ε 2 εb1 εb2
1
1
1
0.7 0.2
1
1
1
1 0.7
1

r1 r2 b1 b2
1
0.9 1
1
0.7
1

Table 4: The columns in the table correspond to the nodes in the implication graph Imp, and each
row provides the weights ̟p(0) for some p(0) ∈ uP (0). An entry in the row of p(0) is
empty if and only if the node associated with the corresponding column does not belong
to the implication subgraph Imp→p(0) .
Having finished with the for loop, the build-PRPG procedure proceeds with the while loop that
builds the “future” layers of PRPG. The test of goal (un)satisficing get-P(0, G) < θ evaluates to
TRUE as we get get-P(0, G) = 0.63 < 0.9, and thus the loop proceeds with its first iteration.
To see the former, consider the implication graph Imp constructed so far (columns 1-3 in Fig590

P ROBABILISTIC -FF

ure 8). For our goal G = {r1 , b2 } we have leafs(Imp→r1 (0) ) = {r1 (−1)}, and leafs(Imp→b2 (0) ) =
{r1 (−1), b2 (−1)}. As {r1 (0), b2 (0)} ⊂ uP (0) and Φ = φ(NbI ), we have
get-P(0, G) = WMC (φ(NbI ) ∧ ϕr1 ∧ ϕb2 ) ,
where
ϕr1 = (hr1,r1 i) ∧ (r1 ↔ hr1,r1 i) ,
ϕb2 = (hr1,b2 i ∨ hb2,b2 i) ∧ (r1 (−1) ↔ hr1,b2 i) ∧ (b2 (−1) ↔ hb2,b2 i) ,

(20)

and
̟ (hr1,r1 i) = ̟r1 (0) (r1 (−1)) = 1
̟ (hb2,b2 i) = ̟b2 (0) (b2 (−1)) = 1

.

(21)

̟ (hr1,b2 i) = ̟b2 (0) (r1 (−1)) = 0.7
Observe that the two models of φ(NbI ) consistent with r2 immediately falsify the sub-formula
φ(NbI ) ∧ ϕr1 . Hence, we have

get-P(0, G) = WMC φ(NbI ) ∧ ϕr1 ∧ ϕb2 |r1 (−1)=1,b1 (−1)=1 +

WMC φ(NbI ) ∧ ϕr1 ∧ ϕb2 |r1 (−1)=1,b2 (−1)=1

= bI (r1 , b1 ) · ̟ (hr1,r1 i) · ̟ (hr1,b2 i) + bI (r1 , b2 ) · ̟ (hr1,r1 i) · ̟ (hr1,b2 i) · ̟ (hb2,b2 i)

= 0.63 · 1 · 0.7 + 0.27 · 1 · 0.7 · 1
= 0.63
In the first iteration of the while loop, build-PRPG calls the build-timestep procedure with
t = 0 and A(0) = {ambr , aml } ∪ N OOP S. The chance nodes introduced to Imp by this call to
build-timestep appear in the forth column of Figure 8. The first outer for loop of build-timestep
results in Imp given by columns 1-5 of Figure 8, uP (1) = uP (0), and no extension of Φ. As
before, in the second for loop of build-timestep, the build-w-impleafs procedure is called for each
unknown fact p(1) ∈ uP (1) = {r1 (1), r2(1) , b1 (1), b2 (1)}, generating the “p(1)-oriented” weights.
The interesting case here is the case of weight propagation build-w-impleafs(r1 (1), Imp), resulting
in weights
̟r1 (1) (r1 (1)) = 1

̟r1 (1) (εr1 (-1)) = 1

ml

̟r1 (1) (ε (0)) = 1
r1

̟r1 (1) (ε (0)) = 1
̟r1 (1) (r1 (0)) = 1
̟r1 (1) (r2 (0)) = 1

⇒

̟r1 (1) (εr2 (-1)) = 1
̟r1 (1) (εmbr
1 (-1))
mbr
̟r1 (1) (ε2 (-1))

= 0.7

⇒

̟r1 (1) (r1 (-1)) = 1
̟r1 (1) (r2 (-1)) = 1

= 0.2

for the nodes in Imp→r1 (1) . From that, the set of supporting leafs of r1 (1) is assigned to support(r1 (1)) =
{r1 (−1), r2 (−1)}, and since Φ = φ(NbI ) does implies r1 (−1) ∨ r2 (−1), the fact r1 is concluded
to be known at time 1, and is added to P (1). For all other nodes p(1) ∈ uP (1) we still have
support(p(1)) = {p(−1)}, and thus they all remain unknown at time t = 1 as well. Putting
things together, this call to the build-w-impleafs procedure results with P (1) = {r1 (1)}, and
591

D OMSHLAK & H OFFMANN

uP (1) = {r2(1) , b1 (1), b2 (1)}. The while loop of the build-PRPG procedure proceeds with checking the fixpoint termination test, and this immediately fails due to P (1) 6= P (0). Hence, the while
loop proceeds with the next iteration corresponding to t = 1.
The test of goal (un)satisficing get-P(1, G) < θ still evaluates to TRUE because we have
get-P(1, G) = 0.899 < 0.9. Let us follow this evaluation of get-P(1, G) in detail as well. Considering the implication graph Imp constructed so far up to time t = 1 (columns 1-5 in Figure 8), and
having G ∩ uP (1) = {b2 (1)}, leafs(Imp→b2 (1) ) = {r1 (−1), b2 (−1)}, and (still) Φ = φ(NbI ), we
obtain
get-P(1, G) = WMC (φ(NbI ) ∧ ϕb2 ) ,
with
ϕb2 = (hr1,b2 i ∨ hb2,b2 i) ∧ (r1 (−1) ↔ hr1,b2 i) ∧ (b2 (−1) ↔ hb2,b2 i) ,

(22)

While the structure of ϕb2 in Equation 22 is identical to this in Equation 20, the weights associated
with the auxiliary chance propositions are different, notably
̟ (hb2,b2 i) = ̟b2 (1) (b2 (−1)) = 1

.

̟ (hr1,b2 i) = ̟b2 (1) (r1 (−1)) = 0.91

(23)

The difference in ̟ (hr1,b2 i) between Equation 21 and Equation 23 stems from the fact that r1 (−1)
supports b2 (1) not only via the effect embr at time −1 but also via the a different instance of the
same effect at time 0. Now, the only model of φ(NbI ) that falsify ϕb2 is the one that sets both r1
and b2 to false. Hence, we have
get-P(1, G) = bI (r1 , b1 ) · ̟ (hr1,b2 i) +
bI (r1 , b2 ) · ̟ (hr1,b2 i) · ̟ (hb2,b2 i) +
bI (r2 , b2 ) · ̟ (hb2,b2 i)
= 0.63 · 0.91 + 0.27 · 0.91 · 1 + 0.08 · 1
= 0.899
Having verified get-P(1, G) < θ, the while loop proceeds with the construction for time t = 2,
and calls the build-timestep procedure with t = 1 and A(1) = {ambr , aml } ∪ N OOP S. The chance
nodes introduced to Imp by this call to build-timestep appear in the sixth column of Figure 8. The
first outer for loop of build-timestep results in Imp given by columns 1-7 of Figure 8, and


mbr
mbr
Φ = φ(NbI ) ∧ εmbr
(1)
∨
ε
(1)
∨
ε
(1)
∧
1
2
3

 
 

mbr
mbr
mbr
mbr
mbr
∧ ¬εmbr
(1)
∨
¬ε
(1)
∧
¬ε
(1)
∨
¬ε
(1)
∧
¬ε
(0)
∨
¬ε
(0)
1
2
1
3
2
3

(24)

Next, the build-w-impleafs procedure is called as usual for each unknown fact p(2) ∈ uP (2) =
{r2(2) , b1 (2), b2 (2)}. The information worth detailing here is that now we have leafs(Imp→b2 (2) ) =
mbr
{b2 (−1), r1 (−1), εmbr
1 (1)}, and support(b2 (2)) = {b2 (−1), ε1 (1)}. However, we still have Φ →
W
l∈support(p(2)) l for no p(2) ∈ uP (2), and thus the set of known facts P (2) remains equal to
P (1) = {r1 }.
592

P ROBABILISTIC -FF

Returning from the call to the build-w-impleafs procedure, build-PRPG proceeds with checking
the fixpoint termination condition. This time, the first three equalities of the condition do hold, yet
the condition is not satisfied due to get-P(2, G) > get-P(t, G). To see the latter, notice that we have
get-P(2, G) = WMC (Φ ∧ ϕb2 ) ,
where Φ is given by Equation 24,


ϕb2 = hr1,b2 i ∨ hb2,b2 i ∨ εmbr
1 (1) ∧ (r1 (−1) ↔ hr1,b2 i) ∧ (b2 (−1) ↔ hb2,b2 i) ,

(25)

and

̟ (hb2,b2 i) = ̟b2 (1) (b2 (−1)) = 1

.

̟ (hr1,b2 i) = ̟b2 (1) (r1 (−1)) = 0.91
̟(εmbr
1 (1))

=

̟b2 (1) (εmbr
1 (1))

(26)

= 0.7

It is not hard to verify that
get-P(2, G) = get-P(1, G) + bI (r2 , b1 ) · ̟(εmbr
1 (1))
= 0.899 + 0.02 · 0.7
= 0.913
Note that now we do have get-P(2, G) ≥ θ, and therefore build-PRPG aborts the while loop by
passing the goal satisficing test, and sets T = 2. This finalizes the construction of PRPG, and thus,
our example.
4.4 Extracting a Probabilistic Relaxed Plan
If the construction of the PRPG succeeds in reaching the goals with the estimated probability of success get-P(T, G) exceeding θ, then we extract a relaxed plan consisting of A′ ⊆ A(0), . . . , A(T −
1), and use the size of A′ as the heuristic value of the evaluated belief state ba .
Before we get into the technical details, consider that there are some key differences between
relaxed (no delete lists) probabilistic planning on the one hand, and both relaxed classical and relaxed qualitative conformant planning on the other hand. In relaxed probabilistic planning, it might
make sense to execute the same action numerous times in consecutive time steps. In fact, this
might be essential – just think of throwing a dice in a game until a “6” appears. In contrast, in the
relaxed classical and qualitatively uncertain settings this is not needed – once an effect has been
executed, it remains true forever. Another complication in probabilistic planning is that the required
goal-achievement probability is specified over a conjunction (or, possibly, some more complicated
logical combination) of different facts. While increasing the probability of achieving each individual sub-goal g ∈ G in relaxed planning will always increase the overall probability of achieving G,
choosing the right distribution of effort among the sub-goals to pass the required threshold θ for the
whole goal G is a non-trivial problem.
A fundamental problem is the aforementioned lack of guarantees of the weight propagation.
On the one hand, the construction of PRPG and Lemma 5 imply that a|+
1 concatenated with an
R
arbitrary linearization a of A(0), . . . , A(T − 1) is executable in bI . On the other hand, due to
the independence assumption made in the build-w-impleafs procedure, get-P(T, G) ≥ θ does not
593

D OMSHLAK & H OFFMANN

R
imply that the probability of achieving G by a|+
1 concatenated with a exceeds θ. A “real” relaxed
plan, in that sense, might not even exist in the constructed PRPG.
Our answer to the above difficulties is to extract relaxed plans that are correct relative to the
weight propagation. Namely, we use an implication graph “reduction” algorithm that computes
a minimal subset of that graph which still – according to the weight propagation – sufficiently
supports the goal. The relaxed plan then corresponds to that subset. Obviously, this “solves” the
difficulty with the lack of “real” relaxed plans; we just do the relaxed plan extraction according to
the independence assumption (besides ignoring deletes and removing all but one condition of each
effect). The mechanism also naturally takes care of the need to apply the same action several times:
this corresponds to several implication graph edges which are all needed in order to obtain sufficient
weight. The choice of how effort is distributed among sub-goals is circumvented in the sense that
all sub-goals are considered in conjunction, that is, the reduction is performed once and for all. Of
course, there remains a choice in which parts of the implication graph should be removed. We have
found that it is a useful heuristic to make this choice based on which actions have already been
applied on the path to the belief. We will detail this below.
Making another assumption on top of the previous relaxations can of course be bad for heuristic
quality. The “relaxed plans” we extract are not guaranteed to actually achieve the desired goal probability. Since the relaxed plans are used only for search guidance, per se this theoretical weakness is
only of marginal importance. However, an over-estimation of goal probability might result in a bad
heuristic because the relaxed plan does not include the right actions, or does not apply them often
enough. In Section 5, we will discuss an example domain where Probabilistic-FF fails to scale for
precisely this reason.
Figure 9 shows the main routine extract-PRPlan for extracting a relaxed plan from a given
PRPG (note that T is the index of the highest PRPG layer, c.f. Figure 4). The sub-routines of
extract-PRPlan are shown in Figures 10-11. At a high level, the extract-PRPlan procedure consists
of two parts:

1. Reduction of the implication graph, aiming at identifying a set of time-stamped action effects
that can be ignored without decreasing our estimate of goal-achievement probability get-P(T, G)
below the desired threshold θ, and
2. Extraction of a valid relaxed plan ar such that (schematically) constructing PRPG with ar instead
of the full set of A(0), . . . , A(T ) would still result in get-P(T, G) ≥ θ.
The first part is accomplished by the reduce-implication-graph procedure, depicted in Figure 10.
As of the first step in the algorithm, the procedure considers only the parts of the implication graph
that are relevant to achieving the unknown sub-goals. Next, reduce-implication-graph performs a
greedy iterative elimination of actions from the “future” layers 0, . . . , T −1 of PRPG until the probability estimate get-P(T, G) over the reduced set of actions goes below θ. While, in principle, any action from A(0), . . . , A(T −1) can be considered for elimination, in reduce-implication-graph we examine only repetitions of the actions that already appear in a. Specifically, reduce-implication-graph
iterates over the actions a in a|+
1 , and if a repeats somewhere in the “future” layers of PRPG, then
one such repetition a(t′ ) is considered for removal. If removing this repetition of a is found safe
with respect to achieving θ,12 then it is effectively removed by eliminating all the edges in Imp that
are induced by a(t′ ). Then the procedure considers the next repetition of a. If removing another
12. Note here that the formula for WMC is constructed exactly as for the get-P function, c.f. Figure 7.

594

P ROBABILISTIC -FF

procedure extract-PRPlan(P RP G(a, A, φ(NbI ), G, θ, |+
1 )),
selects actions from A(0), . . . , A(T − 1)
Imp′ := reduce-implication-graph()
extract-subplan(Imp′ )
sub-goal(G ∩ P (T ))
for decreasing time steps t := T, . . . , 1 do
for all g ∈ G(t) do
if ∃a ∈ A(t − 1), e ∈ E(a), con(e) ∈ P (t − 1), ∀ε ∈ Λ(e) : g ∈ add(ε) then
add-to-relaxed-plan one such a at time t
sub-goal(pre(a) ∪ con(e))
else
Imp g(t) := construct-support-graph(support(g(t)))
extract-subplan(Imp g(t) )
endif
endfor
endfor

Figure 9: Extracting a probabilistic relaxed plan.
copy of a is not safe anymore, then the procedure breaks the inner loop and considers the next
action.
procedure reduce-implication-graph()
operates on the PRPG;
returns a sub-graph of Imp.
Imp′ := ∪g∈G\P (T ) Imp→g(T )
for all actions a ∈ a|+
1 do
for all edges (ε(t′ ), p(t′ + 1)) ∈ Imp′′ , induced by a(t′ ) ∈ A(t′ ), for some t′ ≥ 0 do
Imp′′ := Imp′
remove from Imp′′ all the edges induced by a ∈ A(t′ )
for all g ∈ G \ P (t) do
for each l ∈ leafs(Imp′′ →g(T ) ), introduce a chance proposition hlg i with weight ̟g(T ) (l)
V
W
ϕg := ( l∈leafs(Imp′′
) l) ∧ l∈leafs(Imp′′
)∩uP (−m) (¬l ∨ hlg i)
→g(T )

→g(T )

endfor
V
if WMC(Φ ∧ g∈G\P (T ) ϕg ) ≥ θ then Imp′ := Imp′′ else break endif
endfor
endfor
return Imp′

Figure 10: The procedure reducing the implication graph.
To illustrate the intuition behind our focus on the repetitions of the actions from a, let us consider the following example of a simple logistics-style planning problem with probabilistic actions.
Suppose we have two locations A and B, a truck that is known to be initially in A, and a heavy and
uneasy to grab package that is known to be initially on the truck. The goal is to have the package
unloaded in B with a reasonably high probability, and there are two actions we can use – moving
the truck from A to B (am ), and unloading the package (au ). Moving the truck does not necessarily
595

D OMSHLAK & H OFFMANN

move the truck to B, but it does that with an extremely high probability. On the other hand, unloading the bothersome package succeeds with an extremely low probability, leaving the package on the
truck otherwise. Given this data, consider the belief state ba corresponding to “after trying to move
the truck once”, that is, to the action sequence ham i. To achieve the desired probability of success,
the PRPG will have to be expanded to a very large time horizon T , allowing the action au to be
applied sufficiently many times. However, the fact “truck in B” is not known in the belief state ba ,
and thus the implication graph will also contain the same amount of applications of am . Trimming
away most of these applications of am will still keep the probability sufficiently high.
The reader might ask at this point what we hope to achieve by “trimming away most of the
applications of am ”. The point is, intuitively, that the implication graph reduction mechanism is
a means to understand what has been accomplished already, on the path to ba . Without such an
understanding, the relaxed planning can be quite indiscriminative between search states. Consider
the above example, and assume we have not one but two troubled packages, P 1 and P 2, on the
truck, with unload actions au1 and au2 . The PRPG for ba contains copies of au1 and au2 at layers up
to the large horizon T . Now, say our search starts to unload P 1. In the resulting belief, the PRPG
still has T steps because the situation has not changed for P 2. Each step of the PRPG still contains
copies of both au1 and au2 – and hence the heuristic value remains the same as before! In other
words, without an implication graph reduction technique, relevant things that are accomplished
may remain hidden behind other things that have not yet been accomplished. In the above example,
this is not really critical because, as soon as we have tried an unload for each of P 1 and P 2, the
time horizon T decreases by one step, and the heuristic value is reduced. It is, however, often
the case that some sub-task must be accomplished before some other sub-task can be attacked. In
such situations, without implication graph reduction, the search staggers across a huge plateau until
the first task is completed. We observed this in a variety of benchmarks, and hence designed the
implication graph reduction to make the relaxed planning aware of what has already been done.
Of course, since our weight propagation may over-estimate true probabilities, and hence overestimate what was achieved in the past, the implication graph reduction may conclude prematurely
that a sub-task has been “completed”. This leads us to the main open question in this research; we
will get back to this at the end of Section 5, where we discuss this in the context of an example
where Probabilistic-FF’s performance is bad.
Let us get back to explaining the extract-PRPlan procedure. After the implication graph reduction, the procedure proceeds with the relaxed plan extraction. The process makes use of proposition
sets G(1), . . . , G(T ), which are used to store time-stamped sub-goals arising at layers 1 ≤ t ≤ T
during the relaxed plan extraction. The sub-routine extract-subplan (Figure 11)
1. adds to the constructed relaxed plan all the time-stamped actions responsible for the edges of the
reduced implication graph Imp′ , and
2. subgoals everything outside the implication graph that condition the applicability of the effects
responsible for the edges of Imp′ .
Here and in the later phases of the process, the sub-goals are added into the sets G(1), . . . , G(T ) by
the sub-goal procedure that simply inserts each given proposition as a sub-goal at the first layer of
its appearance in the PRPG. Having accomplished this extract-and-subgoal pass of extract-subplan
over Imp′ , we also subgoal all the goal conjuncts known at time T .
In the next phase of the process, the sub-goals are considered layer by layer in decreasing order
of time steps T ≥ t ≥ 1. For each sub-goal g at time t, certain supporting actions are selected into
596

P ROBABILISTIC -FF

procedure extract-subplan(Imp′ )
actions that are helpful for achieving uncertain goals G ∩ uP (T ) and
subgoals all the essential conditions of these actions
for each edge (ε(t), p(t + 1)) ∈ Imp′ such that t ≥ 0 do
if action a and its effect e ∈ E(a) be responsible for ε at time t time
add-to-relaxed-plan a at time t
sub-goal((pre(a) ∪ con(e)) ∩ P (t))
endif endfor
procedure sub-goal(P )
inserts the propositions in P as sub-goals
at the layers of their first appearance in the PRPG
for all p ∈ P do
t0 := argmint {p ∈ P (t)}
if t0 ≥ 1 then G(t0 ) := G(t0 ) ∪ {p} endif
endfor
procedure construct-support-graph(support(g(t)))
takes a subset support(g(t)) of leafs(Imp→g(t) ) weighted according to g(t);
returns a sub-graph Imp′ of Imp.
′
Imp := ∅
open := support(g(t))
while open 6= ∅ do
open := open \ {p(t′ )}
choose a ∈ A(t′ ), e ∈ E(a), con(e) = {p} such that
∀ε ∈ Λ(e) : (p(t′ ), ε(t′ )) ∈ Impg(t) ∧ ̟g(t) (ε(t′ )) = ̟(ε(t′ ))
for each ε ∈ Λ(e) do
choose q ∈ add(ε) such that ̟g(t) (q(t′ + 1)) = 1
Imp′ := Imp′ ∪ {(p(t′ ), ε(t′ )), (ε(t′ ), q(t′ + 1))}
open := open ∪ {q(t′ + 1)}
endfor endwhile
return Imp′

Figure 11: Sub-routines for extract-PRPlan.
the relaxed plan. If there is an action a and some effect e ∈ E(a) that are known to be applicable
at time t − 1, and guarantee to achieve g with certainty, then a is added to the constructed relaxed
plan at t − 1. Otherwise, we
1. use the construct-support-graph procedure to extract a sub-graph Imp g(t) consisting of a set of
implications that together ensure achieving g at time t, and
2. use the already discussed procedure extract-subplan to
(a) add to the constructed relaxed plan all the time-stamped actions responsible for the edges
of Imp g(t) , and
(b) subgoal everything outside this implication graph Imp g(t) that condition the applicability
of the effects responsible for the edges of Imp g(t) .
597

D OMSHLAK & H OFFMANN

Processing this way all the sub-goals down to G(1) finalizes the extraction of the relaxed plan
estimate. Section 4.5 provides a detailed illustration of this process on the PRPG constructed in
Section 4.3. In any event, it is easy to verify that the relaxed plan we extract is sound relative to our
weight propagation, in the following sense.
Proposition 7 Let (A, NbI , G, θ) be a probabilistic planning task, a be a sequence of actions ap+
plicable in bI , and |+
1 be a relaxation function for A such that build-PRPG(a, A, φ(NbI ), G, θ, |1 )
returns TRUE. Let A(0)s , . . . , A(T − 1)s be the actions selected from A(0), . . . , A(T − 1) by
extract-PRPlan. When constructing a relaxed planning graph using only A(0)s , . . . , A(T − 1)s ,
then get-P(T, G) ≥ θ.
Proof: By construction: reduce-implication-graph leaves enough edges in the graph so that the
weight propagation underlying get-P still concludes that the goal probability is high enough.
4.5 Example: Extracting a Relaxed Plan from PRPG
We illustrate the process of the relaxed plan extraction on the PRPG as in Figure 8, constructed for
the belief state and problem specification as in example in Section 4.3. In this example we have
T = 2, G ∩ uP (2) = {b2 }, and thus the implication graph Imp gets immediately reduced to its
sub-graph Imp′ depicted in Figure 12a. As the plan a to the belief state in question consists of only a
single action ambr , the only action instances that are considered for elimination by the outer for loop
of reduce-implication-graph are ambr (0) and ambr (1). If ambr (0) is chosen to be examined, then the
implication sub-graph Imp′′ = Imp′ is further reduced by removing all the edges due to ambr (0),
and the resulting Imp′′ appears13 in Figure 12b. The Φ and ϕb2 components of the evaluated formula
Φ ∧ ϕb2 are given by Equation 24 and Equation 25, respectively, and the weights associated with
the chance propositions in Equation 25 over the reduced implication graph Imp′′ are
̟ (hb2,b2 i) = ̟b2 (1) (b2 (−1)) = 1
̟ (hr1,b2 i) = ̟b2 (1) (r1 (−1)) = 0.7

.

(27)

mbr
̟(εmbr
1 (1)) = ̟b2 (1) (ε1 (1)) = 0.7

The weight model counting of Φ ∧ ϕb2 evaluates to 0.724 < θ, and thus Imp′′ does not replace Imp′ .
The only alternative action removal is this of ambr (1), and it can be seen from the example in Section 4.3 that this attempt for action elimination will also result in probability estimate lower than θ.
Hence, the only effect of reduce-implication-graph on the PRPG processed by the extract-PRPlan
procedure is the reduction of the implication graph to only the edges relevant to achieving {b2 } at
time T = 2. The reduced implication sub-graph Imp′ returned by the reduce-implication-graph
procedure is depicted in Figure 12a.
Next, the extract-subplan procedure iterates over the edges of Imp′ and adds to the initially
empty relaxed plan applications of ambr at times 0 and 1. The action ambr has no preconditions,
∈ E(ambr ) is known at time 1. Hence, extract-subplan
and the condition r1 of the effect εmbr
1
invokes the sub-goal procedure on {r(1)}, and the latter is added into the proposition set G(1). The
subsequent call sub-goal(G ∩ P (T )) = sub-goal({r1 }) leads to no further extensions of G(2), G(1)
13. The dashed edges in Figure 12b can be removed from Imp′′ either now or at a latter stage if Imp′′ is chosen to replace
Imp′ .

598

P ROBABILISTIC -FF



mbr
ε88 1 (-1)
qq
]\
qqq
q
q
qq


// εr1 (-1)
r1 (-1)



b2 (-1)



// εb2 (-1)



// r1 (0)



mbr
ε88 1 (0)
rrr
]\
rrr
r
r

! //
b2 (0)



// εb2 (0)





mbr
ε1 (1)

! //
b2 (1)

]\



// εb2 (1)



! //
b2 (2)

(a)


mbr
ε88 1 (-1)
qq
]\
qqq
q
q
qq

 r
r1 (-1) _ _ _ _// ε 1 (-1)_ _ _ _// r1 (0)

b2 (-1)



// εb2 (-1)



!

// b2 (0)



mbr
ε1 (1)



// εb2 (0)



// b2 (1)



// εb2 (1)



]\

!

// b2 (2)

(b)


00 ml
ε (0)54

r1 (-1)



// εr1 (-1)



r2 (-1)



// εr2 (-1)



WV

 r
 01

// r1 (0)
// ε 1 (0)





RS

// r1 (1)

// r2 (0)

(c)
Figure 12: Illustrations for various steps of the relaxed plan extraction from the PRPG constructed
in Section 4.3, and, in particular, from the implication graph of the latter, depicted in
Figure 8.

as we already have r1 ∈ G(1). Hence, the outer for loop of extract-PRPlan starts with G(2) = ∅,
and G(1) = {r1 }.
Since G(2) is empty, the first sub-goal considered by extract-PRPlanis r1 from G(1). For r1
at time 1, no action effect at time 0 passes the test of the if statement—the condition r2 of εml
is not known at time 0, and the same is true14 for εr1 . Hence, the subgoal r1 (1) is processed
by extracting a sub-plan to support achieving it with certainty. First, the construct-support-graph
procedure is called with support(r1 (1)) = {r1 (−1), r2 (−1)} (see Section 4.3). The extracted sub14. In fact, it is easy to see from the construction of the sub-goal procedure that if p belongs to G(t), then the condition
of the noop’s effect εp cannot be known at time t − 1.

599

D OMSHLAK & H OFFMANN

graph Imp r1 (1) of the original implication graph Imp is depicted in Figure 12c, and invoking the
procedure extract-subplan on Imp r1 (1) results in adding (i) application of aml at time 0, and (ii)
no new subgoals. Hence, the proposition sets G(1), G(2) get emptied, and thus we end up with
extracting a relaxed plan hambr (0), aml (0), ambr (1)i.

5. Empirical Evaluation
We have implemented Probabilistic-FF in C, starting from the Conformant-FF code. With θ = 1.0,
Probabilistic-FF behaves exactly like Conformant-FF (except that Conformant-FF cannot handle
non-deterministic effects). Otherwise, Probabilistic-FF behaves as described in the previous sections, and uses Cachet (Sang et al., 2005) for the weighted model counting. To better home in on
strengths and weaknesses of our approach, the empirical evaluation of Probabilistic-FF has been
done in two steps. In Section 5.1 we evaluate Probabilistic-FF on problems having non-trivial uncertain initial states, but only deterministic actions. In Section 5.2 we examine Probabilistic-FF
on problems with probabilistic action effects, and with both sources of uncertainty. We compare
Probabilistic-FF’s performance to that of the probabilistic planner POND (Bryce et al., 2006). The
reasons for choosing POND as the reference point are twofold. First, similarly to Probabilistic-FF,
POND constitutes a forward-search planner guided by a non-admissible heuristic function based
on (relaxed) planning graph computations. Second, to our knowledge, POND clearly is the most
efficient probabilistic planner reported in the literature.15
The experiments were run on a PC running at 3GHz with 2GB main memory and 2MB cache
running Linux. Unless stated otherwise, each domain/problem pair was tried at four levels of desired probability of success θ ∈ {0.25, 0.5, 0.75, 1.0}. Each run of a planner was time-limited by
1800 seconds of user time. Probabilistic-FF was run in the default configuration inherited from FF,
performing one trial of enforced hill-climbing and switching to best-first search in case of failure. In
domains without probabilistic effects, we found that Probabilistic-FF’s simpler relaxed plan extraction developed for that case (Domshlak & Hoffmann, 2006), performs better than the one described
in here. We hence switch to the simpler version in these domains.16
Unlike Probabilistic-FF, the heuristic computation in POND has an element of randomization;
namely, the probability of goal achievement is estimated via sending a set of random particles
through the relaxed planning graph (the number of particles is an input parameter). For each problem instance, we averaged the runtime performance of POND over 10 independent runs. In special
cases where POND timed out on some runs for a certain problem instance, yet not on all of the
10 runs, the average we report for POND uses the lower-bounding time threshold of 1800s to replace the missing time points. In some cases, POND’s best-case performance differs a lot from
its average performance; in these cases, the best-case performance is also reported. We note that,
following the suggestion of Dan Bryce, POND was run in its default parameter setting, and, in par15. In our experiments we have used a recent version 2.1 of POND that significantly enhances POND2.0 (Bryce et al.,
2006). The authors would like to thank Dan Bryce and Rao Kambhampati for providing us with a binary distribution
of POND2.1.
16. Without probabilistic effects, relaxed plan extraction proceeds very much like in Conformant-FF, with an additional
straightforward backchaining selecting support for the unknown goals. The more complicated techniques developed
in here to deal with relaxed plan extraction under probabilistic effects appear to have a more unstable behavior than
the simpler techniques. If there are probabilistic effects, then the simple backchaining is not meaningful because it
has no information on how many times an action must be applied in order to sufficiently support the goal.

600

P ROBABILISTIC -FF

θ = 0.25
t/|S|/l

θ = 0.5
t/|S|/l

θ = 0.75
t/|S|/l

θ = 1.0
t/|S|/l

70/71/140
70/70/138

1.39/19 /18
0.28/6/5

4.02/36/35
0.76/13/12

8.06/54/53
1.54/22/21

4.62/71 /70
4.32/70/69

Cube-uni-15
Cube-cub-15

6/90/3375
6/90/3375

3.25/145/26
0.56/41/8

3.94/150/34
1.16/70/13

5.00/169/38
1.95/109/18

25.71/296/42
26.35/365/42

Bomb-50-50
Bomb-50-10
Bomb-50-5
Bomb-50-1

2550/200/> 2100
510/120/> 260
255/110/> 255
51/102/> 251

0.01/1/0
0.00/1/0
0.00/1/0
0.00/1/0

0.10/17/16
0.89/248/22
1.70/468/27
2.12/662/31

0.25/37/36
4.04/778/62
4.80/998/67
6.19/1192/71

0.14/51/50
1.74/911/90
2.17/1131/95
2.58/1325/99

Log-2
Log-3
Log-4

3440/1040/> 2010
3690/1260 /> 3010
3960/1480/> 4010

0.90/117/54
2.85/159/64
2.46/138/75

1.07/152/62
8.80/328/98
8.77/391/81

1.69/205/69
4.60/336/99
6.20/377/95

1.84/295/78
4.14/364/105
8.26/554/107

Grid-2
Grid-3
Grid-4

2040/825 /> 3610
2040/841 /> 3610
2040/857 /> 3610

0.07/39/21
16.01/1629/76
28.15/2167/96

1.35/221/48
15.8/1119/89
51.58/2541/111

6.11/1207/69
82.24/3974/123
50.80/2541/115

6.14/1207/69
66.26/3974/123
193.47/6341/155

Rovers-7
RoversP-7
RoversPP-7
RoversPPP-7

393/97 /> 63 ∗ 38
393/133 /> 63 ∗ 38
393/133 /> 63 ∗ 38
395/140 /> 63 ∗ 38

0.01/ 37/18
2.15/942/65
8.21/948/65
25.77/950/67

0.01/ 37/18
2.23/983/75
12.48/989/75
41.18/996/79

0.01/ 37/18
2.37/1008/83
12.53/994/77
0.01/UNSAT

0.01/ 37/18
2.29/1008/83
16.20/1014/83
0.01/UNSAT

Instance

#actions/#facts/#states

Safe-uni-70
Safe-cub-70

Table 5: Empirical results for problems with probabilistic initial states. Times t in seconds, search
space size |S| (number of calls to the heuristic function), plan length l.

ticular, this includes the number of random particles (64) selected for computing POND’s heuristic
estimate (Bryce et al., 2006).
5.1 Initial State Uncertainty and Deterministic Actions
We now examine the performance of Probabilistic-FF and POND in a collection of domains with
probabilistic initial states, but with deterministic action effects. We will consider the domains one
by one, discussing for each a set of runtime plots. For some of the problem instances, Table 5 shows
more details, providing features of the instance size as well as detailed results for Probabilistic-FF,
including the number of explored search states and the plan length.
Our first three domains are probabilistic versions of traditional conformant benchmarks: “Safe”,
“Cube”, and “Bomb”. In Safe, out of n combinations one opens the safe. We are given a probability
distribution over which combination is the right one. The only type of action in Safe is trying a
combination, and the objective is to open the safe with probability ≥ θ. We experimented with
two probability distributions over the n combinations, a uniform one (“Safe-uni”) and a distribution
that declines according to a cubic function (“Safe-cub”). Table 5 shows that Probabilistic-FF can
solve this very efficiently even with n = 70. Figure 13 compares between Probabilistic-FF and
POND, plotting their time performance on an identical linear scale, where x-axes show the number
of combinations.
From the graphs it is easy to see that Probabilistic-FF outperforms POND by at least an order of
magnitude on both Safe-uni and Safe-cub. But a more interesting observation here is not necessarily
the difference in time performance, but the relative performance of each planner on Safe-uni and
Safe-cub. Note that Safe-cub is somewhat “easier” than Safe-uni in the sense that, in Safe-cub, fewer
combinations must be tried to guarantee a given probability θ of opening the safe. This because the
601

D OMSHLAK & H OFFMANN

PFF

POND2.1

70

70
p=0.25
p=0.50
p=0.75
p=1.00

60

50

50

40

40

Time (sec)

Time (sec)

60

p=0.25
p=0.50
p=0.75
p=1.00

30

30

20

20

10

10

0

0
10

30

50

70

10

30

#combinations

50

70

#combinations

(a) Uniform prior distribution over the combinations.
PFF

POND2.1

70

70
p=0.25
p=0.50
p=0.75
p=1.00

60

50

50

40

40

Time (sec)

Time (sec)

60

p=0.25
p=0.50
p=0.75
p=1.00

30

30

20

20

10

10

0

0
10

30

50

70

10

#combinations

30

50

70

#combinations

(b) Cubic decay prior distribution over the combinations.
Figure 13: The Safe domain, Probabilistic-FF (left) vs. POND (right).
dominant part of the probability mass lies on the combinations at the head of the cubic distribution
(the last combination has probability 0 to be the right combination, and thus it needs not be tried
even when θ = 1.0). The question is now whether the heuristic functions of Probabilistic-FF and
POND exploit this difference between Safe-uni and Safe-cub. Table 5 and Figure 13 provide an
affirmative answer for this question for the heuristic function of Probabilistic-FF. The picture with
POND was less clear as the times spent by POND on (otherwise identical) instances of Safe-uni and
Safe-cub were roughly the same.17
Another interesting observation is that, for both Probabilistic-FF and POND, moving from θ =
1.0 to θ < 1.0, that is, from planning with qualitative uncertainty to truly probabilistic planning,
17. On Safe-cub with n = 70 and θ ∈ {0.75, 1.0}, POND undergoes an exponential blow-up that is not shown in the
graphs since these data points would obscure the other data points; anyway, we believe that this blow-up is due only
to some unfortunate troubles with numerics.

602

P ROBABILISTIC -FF

PFF

POND2.1

30

1800
p=0.25
p=0.50
p=0.75
p=1.00

25

p=0.25
p=0.50
p=0.75
p=1.00

1600
1400
1200
Time (sec)

Time (sec)

20

15

10

1000
800
600
400

5
200
0

0
5

7

9
11
N for Grid NxNxN

13

15

5

7

9
11
N for Grid NxNxN

13

15

13

15

(a) Uniform prior distribution over the initial position.
PFF

POND2.1

30

1800
p=0.25
p=0.50
p=0.75
p=1.00

25

p=0.25
p=0.50
p=0.75
p=1.00

1600
1400
1200
Time (sec)

Time (sec)

20

15

10

1000
800
600
400

5
200
0

0
5

7

9
11
N for Grid NxNxN

13

15

5

7

9
11
N for Grid NxNxN

(b) Cubic decay prior distribution over the initial position.
Figure 14: The Cube domain, Probabilistic-FF (left) vs. POND (right).

typically did not result in a performance decline. We even get improved performance (except for
θ = 0.75 in Safe-uni). The reason seems to be that the plans become shorter. This trend can be
observed also in most other domains. The trend is particularly remarkable for Probabilistic-FF, since
moving from θ = 1.0 to θ < 1.0 means to move from a case where no model counting is needed
to a case where it is needed. (In other words, Probabilistic-FF automatically “specializes” itself for
the qualitative uncertainty, by not using the model counting. To our knowledge, the same is not true
of POND, which uses the same techniques in both cases.)
In Cube, the task is to move into a corner of a 3-dimensional grid, and the actions correspond
to moving from the current cube cell to one of the (up to 6) adjacent cube cells. Again, we created
problem instances with uniform and cubic distributions (over the initial position in each dimension),
and again, Probabilistic-FF scales well, easily solving instances on a 15 × 15 × 15 cube. Within
our time limit, POND was capable of solving Cube problems with cube width ≤ 13. Figure 14
603

D OMSHLAK & H OFFMANN

compares between Probabilistic-FF and POND in more detail, plotting their time performance on
different linear scales (with x-axes capturing the width of the grid in each dimension), and showing
at least an order of magnitude advantage for Probabilistic-FF. Note that,
• Probabilistic-FF generally becomes faster with decreasing θ (with decreasing hardness of
achieving the objective), while θ does not seem to have a substantial effect on the performance
of POND,
• Probabilistic-FF exploits the relative easiness of Cube-cub (e.g., see Table 5), while the time
performance of POND on Cube-cub and Cube-uni is qualitatively identical.
We also tried a version of Cube where the task is to move into the grid center. Probabilistic-FF is
bad at doing so, reaching its performance limit at n = 7. This weakness in the Cube-center domain
is inherited from Conformant-FF. As detailed by Hoffmann and Brafman (2006), the reason for the
weakness lies in the inaccuracy of the heuristic function in this domain. There are two sources of
this inaccuracy. First, to solve Cube-center in reality, one must start with moving into a corner in
order to establish her position; in the relaxation, without delete lists, this is not necessary. Second,
the relaxed planning graph computation over-approximates not only what can be achieved in future
steps, but also what has already been achieved on the path to the considered belief state. For even
moderately long paths of actions, the relaxed planning graph comes to the (wrong) conclusion that
the goal has already been achieved, so the relaxed plan becomes empty and there is no heuristic
information.
Next we consider the famous Bomb-in-the-Toilet domain (or Bomb, for short). Our version
of Bomb contains n bombs and m toilets, where each bomb may be armed or not armed independently with probability 1/n, resulting in huge numbers of initially possible world states. Dunking a
bomb into an unclogged toilet disarms the bomb, but clogs the toilet. A toilet can be unclogged by
flushing it. Table 5 shows that Probabilistic-FF scales nicely to n = 50, and becomes faster as m
increases. The latter is logical and desirable as having more toilets means having more “disarming
devices”, resulting in shorter plans needed. Figures 15 and 16 compare between Probabilistic-FF
and POND, plotting the time performance of Probabilistic-FF on a linear scale, and that of POND
on a logarithmic scale. The four pairs of graphs correspond to four choices of number of toilets
m ∈ {50, 10, 5, 1}. The x-axes in all these graphs correspond to the number of potentially armed
bombs, where we checked problems with n ∈ {5, 10, 25, 50}. Figure 15 shows that this time
Probabilistic-FF is at least four orders of magnitude faster than POND; At the extremes, while the
hardest combination of n = 50, m = 1, and θ = 0.75 took Probabilistic-FF less than 7 seconds,
POND timed-out on most of the problem instances. In addition,
• In Bomb as well, Probabilistic-FF exhibit the nice pattern of improved performance as we
move from non-probabilistic (θ = 1.0) to probabilistic planning (specifically, θ ≤ 0.5; for
θ ≤ 0.25, the initial state is good enough already).
• While the performance of Probabilistic-FF improves with the number of toilets, POND seems
to exhibit the inverse dependence, that is, being more sensitive to the number of states in the
problem (see Table 5) rather to the optimal solution depth.
Finally, we remark that, though length-optimality is not explicitly required in probabilistic conformant planning, for all of Safe, Cube, and Bomb, Probabilistic-FF’s plans are optimal (the shortest
possible).
604

P ROBABILISTIC -FF

PFF

POND2.1

10
p=0.25
p=0.50
p=0.75
p=1.00

1000

8
100

Time (sec)

Time (sec)

6

4

10

1

2

0.1
p=0.25
p=0.50
p=0.75
p=1.00

0

0.01
5

10

25
# bombs

50

5

10

25
# bombs

50

(a) 50 toilets
PFF

POND2.1

10
p=0.25
p=0.50
p=0.75
p=1.00

1000

8
100

Time (sec)

Time (sec)

6

4

10

1

2

0.1
p=0.25
p=0.50
p=0.75
p=1.00

0

0.01
5

10

25
# bombs

50

5

10

25
# bombs

50

(b) 10 toilets
Figure 15: The Bomb domain, Probabilistic-FF (left) vs. POND (right).

Our next three domains are adaptations of benchmarks from deterministic planning: “Logistics”,
“Grid”, and “Rovers”. We assume that the reader is familiar with these domains. Each Logistics-x
instance contains 10 cities, 10 airplanes, and 10 packages, where each city has x locations. The
packages are with chance 0.88 at the airport of their origin city, and uniformly at any of the other
locations in that city. The effects of all loading and unloading actions are conditional on the (right)
position of the package. Note that higher values of x increase not only the space of world states, but
also the initial uncertainty. Grid is the complex grid world run in the AIPS’98 planning competition (McDermott, 1998), featuring locked positions that must be opened with matching keys. Each
Grid-x here is a modification of instance nr. 2 (of 5) run at AIPS’98, with a 6 × 6 grid, 8 locked
positions, and 10 keys of which 3 must be transported to a goal position. Each lock has x possible, uniformly distributed shapes, and each of the 3 goal keys has x possible, uniformly distributed
initial positions. The effects of pickup-key, putdown-key, and open-lock actions are conditional.
605

D OMSHLAK & H OFFMANN

PFF

POND2.1

10
p=0.25
p=0.50
p=0.75
p=1.00

1000

8
100

Time (sec)

Time (sec)

6

4

10

1

2

0.1
p=0.25
p=0.50
p=0.75
p=1.00

0

0.01
5

10

25
# bombs

50

5

10

25
# bombs

50

(c) 5 toilets
PFF

POND2.1

10
p=0.25
p=0.50
p=0.75
p=1.00

1000

8
100

Time (sec)

Time (sec)

6

4

10

1

2

0.1
p=0.25
p=0.50
p=0.75
p=1.00

0

0.01
5

10

25
# bombs

50

5

10

25
# bombs

50

(d) 1 toilet
Figure 16: The Bomb domain, Probabilistic-FF (left) vs. POND (right).

Finally, our last set of problems comes from three cascading modifications of instance nr. 7 (of
20) of the Rovers domain used at the AIPS’02 planning competition. This problem instance has 6
waypoints, 3 rovers, 2 objectives, and 6 rock/soil samples. From Rovers to RoversPPP we modify
the instance/domain as follows.
• Rovers is the original AIPS’02 problem instance nr. 7, and we use it hear mainly for comparison.
• In RoversP, each sample is with chance 0.8 at its original waypoint, and with chance 0.1
at each of the others two waypoints. Each objective may be visible from 3 waypoints with
uniform distribution (this is a probabilistic adaptation of the domain suggested by Bryce &
Kambhampati, 2004).
606

P ROBABILISTIC -FF

Sandcastle

Sandcastle

0.5
PFF
POND

PFF
POND (min)
POND (avg)
100

0.4

10
Time (sec)

Time (sec)

0.3

0.2

1

0.1

0.1

0

0.01
0.2

0.3

0.4

0.5

0.6
θ

0.7

0.8

0.9

0.2

(a)

0.3

0.4

0.5

0.6
θ

0.7

0.8

0.9

(b)

Figure 17: Probabilistic-FF and POND on problems from (a) Sand-Castle, and (b) SlipperyGripper.

• RoversPP enhances RoversP by conditional probabilities in the initial state, stating that whether
or not an objective is visible from a waypoint depends on whether or not a rock sample (intuition: a large piece of rock) is located at the waypoint. The probability of visibility is much
higher if the latter is not the case. Specifically, the visibility of each objective depends on the
locations of two rock samples, and if a rock sample is present, then the visibility probability
drops to 0.1.
• RoversPPP extends RoversPP by introducing the need to collect data about water existence.
Each of the soil samples has a certain probability (< 1) to be “wet”. For communicated
sample data, an additional operator tests whether the sample was wet. If so, a fact “knowthat-water” contained in the goal is set to true. The probability of being wet depends on the
location of the sample.
We show no runtime plots for Logistics, Grid, and Rovers, since POND runs out of either time or
memory on all considered instances of these domains. Table 5 shows that the scaling behavior of
Probabilistic-FF in these three domains is similar to that observed in the previous domains. The
goals in the RoversPPP problem cannot be achieved with probabilities θ ∈ {0.75, 1.0}. This is
proved by Probabilistic-FF’s heuristic function, providing the correct answer in split seconds.
5.2 Probabilistic Actions
Our first two domains with probabilistic actions are the famous “Sand-Castle” (Majercik & Littman,
1998) and “Slippery-Gripper” (Kushmerick et al., 1995) domains. The domains are simple, but they
posed the first challenges for probabilistic planners; our performance in these domains serves an
indicator of the progress relative to previous ideas for probabilistic planning.
In Sand-Castle, the states are specified by two boolean variables moat and castle, and state
transitions are given by two actions dig-moat and erect-castle. The goal is to erect the castle.
607

D OMSHLAK & H OFFMANN

1D-walkgrid

2D-walkgrid

PFF
POND

1000

100

100

10

10

Time (sec)

Time (sec)

1000

1

1

0.1

0.1

0.01

0.01

PFF
POND
5

6

7

8

9

10

3

Grid width

(a)

4

5

6
7
Grid width

8

9

10

(b)

Figure 18: Probabilistic-FF and POND on problems from (a) 1D-WalkGrid with θ = 0.9, and (b)
2D-WalkGrid with θ = 0.01.

Building a moat with dig-moat might fail with probability 0.5. Erecting a castle with erect-castle
succeeds with probability 0.67 if the moat has already been built, and with probability 0.25, otherwise. If failed, erect-castle also destroys the moat with probability 0.5. Figure 17(a) shows that
both Probabilistic-FF and POND solve this problem in less than a second for arbitrary high values
of θ, with the performance of both planners being almost independent of the required probability of
success.
Slippery-Gripper is already a bit more complicated domain. The states in Slippery-Gripper
are specified by four boolean variables grip-dry, grip-dirty, block-painted, and block-held, and
there are four actions dry, clean, paint, and pickup. In the initial state, the block is neither painted
nor held, the gripper is clean, and the gripper is dry with probability 0.7. The goal is to have a
clean gripper holding a painted block. Action dry dries the gripper with probability 0.8. Action
clean cleans the gripper with probability 0.85. Action paint paints the block with probability 1,
but makes the gripper dirty with probability 1 if the block was held, and with probability 0.1 if it
was not. Action pickup picks up the block with probability 0.95 if the gripper is dry, and with
probability 0.5 if the gripper is wet.
Figure 17(b) depicts (on a log-scale) the relative performance of Probabilistic-FF and POND
on Slippery-Gripper as a function of growing θ. The performance of Probabilistic-FF is nicely flat
around 0.06 seconds. This time, the comparison with POND was somewhat problematic, because,
for any fixed θ, POND on Slippery-Gripper exhibited a huge variance in runtime. In Figure 17(b)
we plot the best runtimes for POND, as well as its average runtimes. The best run-times for POND
for different values of θ vary around a couple of seconds, but the average runtimes are significantly
worse. (For some high values of θ POND timed-out on some sample runs, and thus the plot provides
a lower bound on the average runtimes.)
In the next two domains, “1D-WalkGrid” and “2D-WalkGrid”, the robot has to pre-plan a sequence of conditional movements taking it from a corner of the grid to the farthest (from the initial
608

P ROBABILISTIC -FF

position) corner (Hyafil & Bacchus, 2004). In 1D-WalkGrid the grid is one-dimensional, while
in 2D-WalkGrid the grid is two-dimensional. Figure 18(a) depicts (on a log-scale) a snapshot of
the relative performance of Probabilistic-FF and POND on one-dimensional grids of width n and
θ = 0.9. The robot is initially at (1, 1), should get to (1, n), and it can try moving in each of the two
possible directions. Each of the two movement actions moves the robot in the right direction with
probability 0.8, and keeps it in place with probability 0.2. It is easy to see from Figure 18(a) that the
difference between the two planners in this domain is substantial—while runtime of ProbabilisticFF grows only linearly with x, the same dependence for POND is seemingly exponential.
The 2D-WalkGrid domain is already much more challenging for probabilistic planning. In all
2D-WalkGrid problems with n × n grids the robot is initially at (1, 1), should get to (n, n), and it
can try moving in each of the four possible directions. Each of the four movement actions advances
the robot in the right direction with probability 0.8, in the opposite direction with probability 0,
and in either of the other two directions with probability 0.1. Figure 18(a) depicts (on a log-scale)
a snapshot of the relative performance of Probabilistic-FF and POND on 2D-WalkGrid with very
low required probability of success θ = 0.01, and this as a function of the grid’s width n. The
plot shows that Probabilistic-FF still scales well with increasing n (though not linearly anymore),
while POND time-outs for all grid widths n > 3. For higher values of θ, however, Probabilistic-FF
does reach the time-out limit on rather small grids, notably n = 6 and n = 5 for θ = 0.25 and
θ = 0.5, respectively. The reason for this is that Probabilistic-FF’s heuristic function is not good
enough at estimating how many times, at an early point in the plan, a probabilistic action must be
applied in order to sufficiently support a high goal threshold at the end of the plan. We explain this
phenomenon in more detail at the end of this section, where we find that it also appears in a variant
of the well-known Logistics domain.
Our last set of problems comes from the standard Logistics domain. Each problem instance
x-y-z contains x locations per city, y cities, and z packages. We will see that Probabilistic-FF
scales much worse, in Logistics, in the presence of probabilistic effects than if there is “only” initial
state uncertainty (we will explain the reason for this at the end of this section). Hence we use much
smaller instances than the ones used above in Section 5.1. Namely, to allow a direct comparison to
previous results in this domain, we closely follow the specification of Hyafil and Bacchus (2004).
We use instances with configurations x-y-z = 2-2-2, 4-2-2, and 2-2-4, and distinguish between two
levels of uncertainty.
• L-x-y-z correspond to problems with uncertainty only in the outcome of the load and unload
actions. Specifically, the probabilities of success for load are 0.875 for trucks and 0.9 for
airplanes, and for unload, 0.75 and 0.8, respectively.
• LL-x-y-z extends L-x-y-z with independent uniform priors for each initial location of a
package within its start city.
Figure 19 depicts (on a log scale) runtimes of Probabilistic-FF and POND on L-2-2-2, L-4-2-2,
and L-2-2-4, as a function of growing θ. On these problems, both planners appear to scale well,
with the runtime of Probabilistic-FF and the optimal runtime of POND being roughly the same,
and the average runtime of POND somewhat degrading from 2-2-2 to 4-2-2 to 2-2-4. This shows
that both planners are much more efficient in this domain than the previously known SAT and CSP
based techniques. However, moving to LL-x-y-z changes the picture for both planners. The results
are as follows:
609

D OMSHLAK & H OFFMANN

L-2-2-2

L-4-2-2

100

PFF
POND (min)
POND (avg)

1

0.1

10

Time (sec)

10

Time (sec)

Time (sec)

100
PFF
POND (min)
POND (avg)

10

0.01
0.01

L-2-2-4

100
PFF
POND (min)
POND (avg)

1

0.1

0.25

0.5
θ

(a)

0.75

0.95

0.01
0.01

1

0.1

0.25

0.5
θ

(b)

0.75

0.95

0.01
0.01

0.25

0.5

0.75

0.95

θ

(c)

Figure 19: Probabilistic-FF and POND on problems from Logistics (a) L-2-2-2, (b) L-4-2-2, and
(c) L-2-2-4.

1. On LL-2-2-2, the runtimes of Probabilistic-FF were identical to those on L-2-2-2, and the
optimal runtimes of POND only slightly degraded to 2−8 seconds. However, for all examined
values of θ, some runs of POND resulted in timeouts.
2. On LL-4-2-2, the runtimes of Probabilistic-FF were identical to those on L-4-2-2 for θ ∈
{0.01, 0.25, 0.5, 0.75}, yet Probabilistic-FF time-outed on θ = 0.95. The optimal runtimes
of POND degraded from those for L-4-2-2 only to 9 − 18 seconds, and again, for all values
of θ, some runs of POND resulted in timeouts.
3. On LL-2-2-4, Probabilistic-FF experienced hard times, finishing in 0.19 seconds for θ =
0.01, and time-outing for all other examined values of θ. The optimal runtimes of POND
degraded from those for L-2-2-4 to 120 − 700 seconds, and here as well, for all values of θ,
some runs of POND resulted in timeouts.
We also tried a variant of LL-x-y-z with non-uniform priors over the initial locations of the packages, but this resulted in a qualitatively similar picture of absolute and relative performance.
The LL-x-y-z domain remains challenging, and deserves close attention in the future developments for probabilistic planning. In this context, it is interesting to have a close look at what the
reasons for the failure of Probabilistic-FF is. It turns out that Probabilistic-FF is not good enough
at estimating how many times, at an early point in the plan, a probabilistic action must be applied
in order to sufficiently support a high goal threshold at the end of the plan. To make this concrete,
consider a Logistics example with uncertain effects of all load and unload actions. Consider a package P that must go from a city A to a city B. Let’s say that P is initially not at A’s airport. If the
goal threshold is high, this means that, to be able to succeed, the package has to be brought to A’s
airport with a high probability before loading it onto an airplane. This is exactly the point where
Probabilistic-FF’s heuristic function fails. The relaxed plan contains too few actions unloading P
at A’s airport. The effect is that the search proceeds too quickly to loading P onto a plane and
bringing it to B. Once the search gets to the point where B should be unloaded to its goal location, the goal threshold cannot be achieved no matter how many times one unloads P. At this point,
610

P ROBABILISTIC -FF

Probabilistic-FF’s enforced hill-climbing enters a loop and eventually fails because the relaxed plan
(which over-estimates the past achievements) becomes empty.18
The challenge here is to devise methods that are better at recognizing how many times P has
to be unloaded at A’s airport in order to sufficiently support the goal threshold. The error made by
Probabilistic-FF lies in that our propagation of weights on the implication graph over-estimates the
goal probability. Note here that this is much more critical for actions that must be applied early on
in the plan, than for actions that are applied later. If an action a appears early on in a plan, then
the relaxed plan, when a is executed, will be long. Recall that the weight propagation proceeds
backwards, from the goal towards the current state. At each single backwards step, the propagation
makes an approximation that might lose precision of the results. Over several backwards steps,
these imprecisions accumulate. Hence the quality of the approximation decreases quickly over the
number of backwards steps. The longer the distance between goal and current state is, the more
information is lost. We have observed this phenomenon in detailed experiments with different
weight propagation schemes, that is, with different underlying assumptions. Of the propagation
schemes we tried, the independence assumption, as presented in this paper, was by far the most
accurate one. All other schemes failed to deliver good results even for much shorter distances
between the goal and the current state.
It is interesting to consider how this issue affects POND, which uses a very different method for
estimating the probability of goal achievement: instead of performing a backwards propagation and
aggregation of weight values, POND sends a set of random particles through the relaxed planning
graph in a forward fashion, and stops the graph building if enough particles end up in the goal. From
our empirical results, it seems that this method suffers from similar difficulties as Probabilistic-FF,
but not to such a large extent. POND’s optimal runtimes for LL-x-y-z are much higher than those
for L-x-y-z. This indicates that it is always challenging for POND to “recognize” the need for
applying some action a many times early on in the plan. More interestingly, POND never times-out
in L-x-y-z, but it does often time-out in LL-x-y-z. This indicates that, to some extent, it is a matter
of chance whether or not POND’s random particles recognize the need for applying an action a
many times early on in the plan. An intuitive explanation is that the “good cases” are those where
sufficiently many of the particles failed to reach the goal due to taking the “wrong effect” of a.
Based on this intuition, one would expect that it helps to increase the number of random particles in
POND’s heuristic function. We did so, running POND on LL-x-y-z with an increased number of
particles, 200 and 600 instead of the default value of 64. To our surprise, the qualitative behavior of
POND did not change, time-outing in a similar number of cases. It is unclear to us what the reason
for this phenomenon is. Certainly, it can be observed that the situation encoded in LL-x-y-z is not
solved to satisfaction by either of Probabilistic-FF’s weight propagation or POND’s random particle
methods, in their current configurations.
At the time of writing, it is unclear to the authors how better methods could be devised. It seems
unlikely that a weight propagation – at least one that does not resort to expensive reasoning – exists
which manages long distances better than the independence assumption. An alternative way out
might be to simply define a weaker notion of plans that allows to repeat certain kinds of actions –
18. This does not happen in the above L-2-2-2, L-4-2-2, and L-2-2-4 instances simply because they are too small and a
high goal probability can be achieved without thinking too much about the above problem; if one increases the size
of these instances, the problem appears. The problem appears earlier in the presence of initial state uncertainty – even
in small instances such as LL-2-2-2, LL-4-2-2, and LL-2-2-4 – because with uncertainty about the start position of
the packages one needs to try unloading them at the start airports more often.

611

D OMSHLAK & H OFFMANN

throwing a dice or unloading a package – arbitrarily many times. However, since our assumption is
that we do not have any observability during plan execution, when executing such a plan there would
still arise the question how often an action should be tried. Since Logistics is a fairly well-solved
domain in simpler formalisms – by virtue of Probabilistic-FF, even in the probabilistic setting as
long as the effects are deterministic – we consider addressing this problem as a quite pressing open
question.

6. Conclusion
We developed a probabilistic extension of Conformant-FF’s search space representation, using
a synergetic combination of Conformant-FF’s SAT-based techniques with recent techniques for
weighted model counting. We further provided an extension of conformant relaxed planning with
approximate probabilistic reasoning. The resulting planner scales well on a range of benchmark domains. In particular it outperforms its only close relative, POND, by at least an order of magnitude
in almost all of the cases we tried.
While this point may be somewhat obvious, we would like to emphasize that our achievements
do not solve the (this particular) problem once and for all. Probabilistic-FF inherits strengths and
weaknesses from FF and Conformant-FF, like domains where FF’s or Conformant-FF’s heuristic
functions yield bad estimates (e.g. the mentioned Cube-center variant). What’s more, the probabilistic setting introduces several new potential impediments for FF’s performance. For one thing,
weighted model counting is inherently harder than SAT testing. Though this did not happen in our
set of benchmarks, there are bound to be cases where the cost for exact model counting becomes
prohibitive even in small examples. A promising way to address this issue lies in recent methods
for approximate model counting (Gomes, Sabharwal, & Selman, 2006; Gomes, Hoffmann, Sabharwal, & Selman, 2007). Such methods are much more efficient than exact model counters. They
provide high-confidence lower bounds on the number of models. The lower bounds can be used in
Probabilistic-FF in place of the exact counts. It has been shown that good lower bounds with very
high confidecne can be achieved quickly. The challenge here is to extend the methods – which are
currently designed for non-weighted CNFs – to handle weighted model counting.
More importantly perhaps, in the presence of probabilistic effects there is a fundamental weakness in Probabilistic-FF’s – and POND’s – heuristic information. This becomes a pitfall for performance even in a straightforward adaptation of the Logistics domain, which is otherwise very easy
for this kind of planners. As outlined, the key problem is that, to obtain a high enough confidence
of goal achievement, one may have to apply particular actions several times early on in the plan.
Neither Probabilistic-FF’s nor POND’s heuristics are good enough at identifying how many times.
In our view, finding techniques that address this issue is currently the most important open topic in
this area.
Apart from addressing the latter challenge, we intend to work towards applicability in real-word
settings. Particularly, we will look at the space application settings that our Rovers domain hints at,
at medication-type treatment planning domains, and at the power supply restoration domain (Bertoli,
Cimatti, Slaney, & Thiébaux, 2002).
612

P ROBABILISTIC -FF

Acknowledgments
The authors would like to thank Dan Bryce and Rao Kambhampati for providing a binary distribution of POND2.1. Carmel Domshlak was partially supported by the Israel Science Foundations
grant 2008100, as well as by the C. Wellner Research Fund. Some major parts of this research have
been accomplished at the time that Jörg Hoffmann was employed at the Intelligent Information
Systems Institute, Cornell University.

Appendix A. Proofs
Proposition 2 Let (A, NbI , G, θ) be a probabilistic planning problem described over k state variables, and a be an m-step sequence of actions from A. Then, we have |Nba | = O(|NbI |+mα(k+1))
where α is the largest description size of an action in A.
Proof: The proof is rather straightforward, and it exploits the local structure of Nba ’s CPTs. The
first nodes/CPTs layer X(0) of Nba constitutes an exact copy of NbI . Then, for each 1 ≤ t ≤ m, the
t-th layer of Nba contains k + 1 node {Y(t) } ∪ X(t) .
First, let us consider the “action node” Y(t) . While specifying the CPT TY (t) in a straightforward
manner as if prescribed by Eq. 4 might result in an exponential blow up, the same Eq. 4 suggests
that the original description of at is by itself a compact specification of TY (t) . Therefore, TY (t)
can be described in space O(α), and this description can be efficiently used for answering queries
TY (t) (Y(i) = ε | π) as in Eq. 4. Next, consider the CPT TX(t) of a state-variable node X(t) ∈ X(t) .
This time, it is rather evident from Eq. 5 that TX(t) can be described in space O(α) so that queries
TX(t) (X(t) = x | X(t−1) = x′ ) could be efficiently answered. Thus, summing up for all layers
1 ≤ t ≤ m, the description size of |Nba | = O(|NbI | + mα(k + 1))
Lemma 4 Given a node v(t′ ) ∈ Imp→p(t) , we have ̟p(t) (v(t′ )) = ̟ (v(t′ )) if and only if, given v
at time t′ , the sequence of effects E(Impv(t′ )→p(t) ) achieves p at t with probability 1.
Proof: The proof of Lemma 4 is by a backward induction on the time layers of Impv(t′ )→p(t) . For
time t, the only node of Imp→p(t) time-stamped with t is p(t) itself. For this node we do have
̟p(t) (p(t)) = ̟ (p(t)) = 1, but, given p at time t, an empty plan corresponding to (empty)
E(Impp(t)→p(t) ) trivially “re-establishes” p at t with certainty. Assuming now that the claim holds
for all nodes of Imp→p(t) time stamped with t′ + 1, . . . , t, we now show that it holds for the nodes
time stamped with t′ .
It is easy to see that, for any node v(t′ ) ∈ Imp→p(t) , we get ̟p(t) (v(t′ )) = ̟ (v(t′ )) only if
α goes down to zero. First, consider the chance nodes ε(t′ ) ∈ Impv→p(t) . For such a node, lb is
set to zero if and only if we have ̟p(t) (r(t′ + 1)) = 1 for some r ∈ add(ε). However, by our
inductive assumption, in this and only in this case the effects E(Impε(t′ )→p(t+1) ) achieve p at t with
probability 1, given the occurrence of ε at time t′ .
Now, consider the fact nodes q(t′ ) ∈ Impv→p(t) . For such a node, α can get nullified only by
some effect e ∈ E(a), a ∈ A(t′ ), con(e) = q. The latter happens if only if, for all possible outcomes of e, (i) the node ε(t′ ) belongs to Imp→p(t) , and (ii) and the estimate ̟p(t) (ε(t′ )) = ̟(ε(t′ )).
In other words, by our inductive assumption, given any outcome ε ∈ Λ(e) at time t′ , the effects E(Impε(t′ )→p(t) ) achieve p at t with probability 1. Thus, given q at time t′ , the effects
E(Impq(t′ )→p(t) ) achieve p at t with probability 1 independently of the actual outcome of e. Alternatively, if for q(t′ ) we have lb > 0, then for each effect e conditioned on q(t), there exists an
613

D OMSHLAK & H OFFMANN

outcome ε of e such that, according to what we just proved for the chance nodes time-stamped with
t′ , the effects E(Impε(t′ )→p(t+1) ) do not achieve p at t with probability 1. Hence, the whole set of
effects E(Impq(t′ )→p(t+1) ) does not achieve p at t with probability 1.
Lemma 5 Let (A, NbI , G, θ) be a probabilistic planning task, a be a sequence of actions applicable
in bI , and |+
1 be a relaxation function for A. For each time step t ≥ −m, and each proposition p ∈
P, if P (t) is constructed by build-PRPG(a, A, φ(NbI ), G, θ, |+
1 ), then p at time t can be achieved
by a relaxed plan starting with a|+
1
(1) with probability > 0 (that is, p is not negatively known at time t) if and only if p ∈ uP (t)∪P (t),
and
(2) with probability 1 (that is, p is known at time t) if and only if p ∈ P (t).
Proof: The proof of the “if” direction is by a straightforward induction on t. For t = −m the claim
is immediate by the direct initialization of uP (−m) and P (−m). Assume that, for −m ≤ t′ < t,
if p ∈ uP (t′ ) ∪ P (t′ ), then p is not negatively known at time t′ , and if p ∈ P (t′ ), then p is known
at time t′ .
First, consider some p(t) ∈ uP (t) ∪ P (t), and suppose that p is egatively know at time t. By
the inductive assumption, and the property of the PRPG construction that uP (t − 1) ∪ P (t − 1) ⊆
uP (t) ∪ P (t), we have p 6∈ uP (t − 1) ∪ P (t − 1). Therefore, p has to be added into uP (t) (and
then, possibly, moved from there to P (t)) in the first for loop of the build-timestep procedure.
However, if so, then there exists an action a ∈ A(t − 1), e ∈ E(a), and ε ∈ Λ(e) such that (i)
con(e) ∈ uP (t − 1) ∪ P (t − 1), and (ii) p ∈ add(ε). Again, by the assumption of the induction we
have that pre(a) is known at time t − 1, and con(e) is not negatively known at time t − 1. Hence, the
non-zero probability of ε occurring at time t implies that p can be achieved at time t with probability
greater than 0, contradicting that p is negatively know at time t.
Now, let us consider some p(t) ∈ P (t). Notice that, for t > −m, we have p(t) ∈ P (t) if and
only if
_
l .
(28)
Φ −→
l∈support(p(t))

Thus, for each world state w consistent with bI , we have either q ∈ w for some fact proposition
q(−m) ∈ support(p(t)), or, for some effect e of an action a(t′ ) ∈ A(t′ ), t′ < t, we have con(e) ∈
P (t′ ) and {ε(t′ ) | ε ∈ Λ(e)} ⊆ support(p(t)). In this first case, Lemma 4 immediately implies that
the concatenation of a|+
1 with an arbitrary linearization of the (relaxed) actions A(0), . . . , A(t − 1)
achieves p at t with probability 1, and thus p is known at time t. In the second case, our inductive
assumption implies that con(e) is known at time t, and together with Lemma 4 this again implies that
the concatenation of a|+
1 with an arbitrary linearization of the (relaxed) actions A(0), . . . , A(t − 1)
achieves p at t with probability 1.
The proof of the “only if” direction is by induction on t as well. For t = −m this claim is
again immediate by the direct initialization of P (−m). Assume that, for −m ≤ t′ < t, if p is not
negatively known at time t′ , then p ∈ uP (t′ ) ∪ P (t′ ), and if p is known at time t′ , then p ∈ P (t′ ).
First, suppose that p is not negatively known at time t, and yet we have p 6∈ uP (t) ∪ P (t). From
our inductive assumption plus that A(t − 1) containing all the NOOP actions for propositions in
uP (t − 1) ∪ P (t − 1), we know that p is negatively known at time t − 1. If so, then p can become
not negatively known at time t only due to some ε ∈ Λ(e), e ∈ E(a), such that pre(a) is known
614

P ROBABILISTIC -FF

at time t − 1, and con(e) is not negatively known at time t − 1. By our inductive assumption, the
latter conditions imply con(e) ∈ uP (t − 1) ∪ P (t − 1), and pre(a) ∈ P (t − 1). But if so, then p
has to be added to uP (t) ∪ P (t) by the first for loop of the build-timestep procedure, contradicting
our assumption that p 6∈ uP (t) ∪ P (t).
Now, let us consider some p known at time t. By our inductive assumption, P (t − 1) contains
all the facts known at time t − 1, and thus A(t − 1) is the maximal subset of actions A|+
1 applicable
at time t − 1. Let us begin with an exhaustive classification of the effects e of the actions A(t − 1)
with respect to our p at time t.
(I) ∀ε ∈ Λ(e) : p ∈ add(ε), and con(e) ∈ P (t − 1)
(II) ∀ε ∈ Λ(e) : p ∈ add(ε), and con(e) ∈ uP (t − 1)
(III) ∃ε ∈ Λ(e) : p 6∈ add(ε) or con(e) 6∈ P (t − 1) ∪ uP (t − 1)
If the set (I) is not empty, then, by the construction of build-w-impleafs(p(t), Imp), we have
{ε(t − 1) | ε ∈ Λ(e)} ⊆ support(p(t)),
for each e ∈ (I). Likewise, by the construction of build-timestep (notably, by the update of Φ), for
each e ∈ (I), we have
_
Φ −→
ε(t − 1).
{ε(t−1)|ε∈Λ(e)}

Putting these two facts together, we have that Eq. 28 holds for p at time t, and thus we have p ∈ P (t).
Now, suppose that the set (I) is empty. It is not hard to verify that no subset of only effects (III)
makes p known at time t. Thus, the event “at least one of the effects (II) occurs” must hold with
probability 1. First, by the construction of build-w-impleafs(p(t), Imp), we have
[
support (p(t)) ⊇
support (con(e)(t − 1))
e∈(II)

Then, and 4 from Lemma 4 we have that the event “at least one of the effects (II) occurs” holds with
probability 1 if and only if
_
Φ −→
l
e∈(II)
l∈support(con(e)(t−1))

Putting these two facts together, we have that Eq. 28 holds for p at time t, and thus we have p ∈ P (t).

Theorem 6 Let (A, NbI , G, θ) be a probabilistic planning task, a be a sequence of actions appli+
cable in bI , and |+
1 be a relaxation function for A. If build-PRPG(a, A, φ(NbI ), G, θ, |1 ) returns
+
FALSE, then there is no relaxed plan for (A, bI , G, θ) that starts with a|1 .
Proof: Let t > 0 be the last layer of the PRPG upon the termination of build-PRPG. For every
−m ≤ t′ ≤ t, by the construction of PRPG and Lemma 5, the sets P (t′ ) and uP (t′ ) contain all
(and only all) propositions that are known (respectively unknown) after executing all the actions in
the action layers up to and including A(t′ − 1).
615

D OMSHLAK & H OFFMANN

First, let us show that if build-PRPG returns FALSE, then the corresponding termination criterion would hold in all future iterations. If P (t + 1) = P (t), then we have A(t + 1) = A(t).
Subsequently, since P (t + 1) ∪ uP (t + 1) = P (t) ∪ uP (t) and A(t + 1) = A(t), we have
P (t + 2) ∪ uP (t + 2) = P (t + 1) ∪ uP (t + 1). Given that, we now show that P (t + 2) = P (t + 1)
and uP (t + 2) = uP (t + 1).
Assume to the contrary that there exists p(t + 2) ∈ P (t + 2) such that p(t + 1) 6∈ P (t + 1), that
is p(t + 1) ∈ uP (t + 1). By the construction of the sets P (t + 1) and P (t + 2) in the build-timestep
procedure, we have
_
Φ −→
l ,
l∈support(p(t+2))

_

Φ 6−→

(29)

l

l∈support(p(t+1))

Consider an exhaustive classification of the effects e of the actions A(t + 1) with respect to our p at
time t + 2.
(I) ∀ε ∈ Λ(e) : p ∈ add(ε), and con(e) ∈ P (t + 1)
(II) ∀ε ∈ Λ(e) : p ∈ add(ε), and con(e) ∈ uP (t + 1)
(III) ∃ε ∈ Λ(e) : p 6∈ add(ε) or con(e) 6∈ P (t + 1) ∪ uP (t + 1)
Suppose that the set (I) is not empty, and let e ∈ (I). From P (t) = P (t + 1) we have that con(e) ∈
P (t), and thus {ε(t)
W | ε ∈ Λ(e)} ⊆ support(p(t + 1)).
W By the update of Φ in build-timestep we
then have Φ −→ {ε(t)|ε∈Λ(e)} ε(t), and thus Φ −→ l∈support(p(t+1)) l, contradicting Eq. 29.
Alternatively, assume that the set (I) is empty. Using the arguments similar to these in the proof
of Lemma 5, p(t + 2) ∈ P (t + 2) and p(t + 1) 6∈ P (t + 1) in this case imply that
_
l
Φ −→
e∈(II)
l∈support(con(e)(t+1))

_

Φ 6−→

(30)

l

e∈(II)
l∈support(con(e)(t))

However, A(t + 1) = A(t), uP (t + 1) = uP (t), and P (t + 1) = P (t) together imply that all
the action effects that can possibly take place at time t + 1 are also feasible to take place at time
t. Therefore, since for each e ∈ (II) we have con(e) ∈ uP (t + 1) by the definition of (II), Eq. 30
implies that
[
[
support (con(e)(t + 1)) ∩ uP (−m) 6=
support (con(e)(t)) ∩ uP (−m),
(31)
e∈(II)

e∈(II)

contradicting our termination condition. Hence, we arrived into contradiction with our assumption
that p(t + 1) 6∈ P (t + 1).
Having shown that P (t + 2) = P (t + 1) and uP (t + 2) = uP (t + 1), we now show that the
termination criteria implies that, for each q(t + 2) ∈ uP (t + 2), we have
uP (−m) ∩ support(p(t + 2)) = uP (−m) ∩ support(p(t + 1)).
616

P ROBABILISTIC -FF

Let Ep(t+2) be the set of all effects of actions A(t + 1) such that con(e) ∈ uP (t + 1), and, for each
outcome ε ∈ Λ(e), we have p ∈ add(ε). Given that, we have
uP (−m) ∩ support(p(t + 2)) = uP (−m) ∩

[

support(con(e)(t + 1))

[

support(con(e)(t))

e∈Ep(t+2)

= uP (−m) ∩

,

(32)

e∈Ep(t+2)

= uP (−m) ∩ support(p(t + 1))
where the first and third equalities are by the definition of support sets via Lemma 4, and the second
equation is by our termination condition.
The last things that remains to be shown is that our termination criteria implies get-P(t +
2, G) =get-P(t + 1, G). Considering the simple cases first, if G 6⊆ P (t + 1) ∪ uP (t + 1), from
P (t + 2) ∪ uP (t + 2) = P (t + 1) ∪ uP (t + 1) we have get-P(t + 2, G) =get-P(t + 1, G) = 0. Otherwise, if G ⊆ P (t + 1), from P (t + 2) = P (t + 1) we have get-P(t + 2, G) =get-P(t + 1, G) = 1.
This leaves us with the case of G ⊆ P (t + 1) ∪ uP (t + 1) and G ∩ uP (t + 1) 6= ∅. From
P (t + 2) = P (t + 1), uP (t + 2) = uP (t + 1), and the termination condition, we have
G ∩ uP (t) = G ∩ uP (t + 1) = G ∩ uP (t + 2).
From get-P(t + 1, G) =get-P(t, G) we know that action effects that become feasible only in A(t)
do not increase our estimate of probability of achieving any g ∈ G ∩ uP (t + 1) from time t to time
t + 1. However, from P (t + 1) = P (t), uP (t + 1) = uP (t), and A(t + 1) = A(t), we have that
no action effect will become feasible at time t + 1 if it is not already feasible at time t, and thus
get-P(t + 1, G) =get-P(t, G) will imply get-P(t + 2, G) =get-P(t + 1, G).
To this point we have shown that if build-PRPG returns FALSE, then the corresponding termination criterion would hold in all future iterations. Now, assume to the contrary to the claim of
the theorem that build-PRPG returns FALSE at some iteration t, yet there exists a relaxed plan for
(A, bI , G, θ) that starts with a|+
1 . First, if θ = 1, then Lemma 5 implies that there exists time T
such that G ⊆ P (T ). If so, then the persistence of our “negative” termination condition implies
G ⊆ P (t). However, in this case we would have get-P(t, G) = 1 (see the second if of the get-P
procedure), and thus build-PRPG would return TRUE before ever getting to check the “negative”
termination condition in iteration t. Alternatively, if θ = 0, then build-PRPG would have terminated
with returning TRUE before the “negative” termination condition is checked even once.
This leaves us with the case of 0 < θ < 1 and get-P(t, G) < θ. (get-P(t, G) ≥ θ will
again contradict reaching the negative termination condition at iteration t.) We can also assume that
G ⊆ P (t) ∪ uP (t) because P (t) ∪ uP (t) contains all the facts that are not negatively known at time
t, and thus persistence of the negative termination condition together with G 6⊆ P (t) ∪ uP (t) would
imply that there is no relaxed plan for any θ > 0. Let us consider the sub-goals G ∩ uP (t) 6= ∅.
(1) If for all subgoals g ∈ G ∩ uP (t), the implications in Imp→g(t) are only due to deterministic
outcomes of the effects E(Imp→g(t) ), then the uncertainty about achieving G ∩ uP (t) at time
t is only due to the uncertainty about the initial state. Since the initial
V belief state is reasoned
about with no relaxation, in this case get-P(t, G) = WMC(Φ ∧ g∈G\P (t) ϕg ) provides us
with an upper bound on the probability of achieving our goal G by a|+
1 concatenated with
617

D OMSHLAK & H OFFMANN

an arbitrary linearization of an arbitrary subset of A(0), . . . , A(t − 1). The termination subcondition get-P(t + 1, G) =get-P(t, G) and the persistence of the action sets A(T ), T ≥ t,
imply then that get-P(t, G) provides us with an upper bound on the probability of achieving G
by a|+
1 concatenated with an arbitrary linearization of an arbitrary subset of A(0), . . . , A(T ),
for all T ≥ t. Together with get-P(t, G) < θ, the latter conclusion contradicts our assumption
that a desired relaxed plan exists.
(2) If there exists a subgoal g ∈ G ∩ uP (t) such that some implications in Imp→g(t) are due to truly
probabilistic outcomes of the effects E(Imp→g(t)
actions A(t) in
V ), then repeating the (relaxed) V
A(t + 1) will necessarily result in WMC(Φ ∧ g∈G\P (t+1) ϕg ) > WMC(Φ ∧ g∈G\P (t) ϕg ),
contradicting our termination sub-condition condition get-P(t + 1, G) =get-P(t, G).
Hence, we arrived into contradiction that our assumption that build-PRPG returns FALSE at time t,
yet there exists a relaxed plan for (A, bI , G, θ) that starts with a|+
1.

References
Bertoli, P., Cimatti, A., Pistore, M., Roveri, M., & Traverso, P. (2001). MBP: a model based planner.
In Proc. IJCAI’01 Workshop on Planning under Uncertainty and Incomplete Information,
Seattle, WA.
Bertoli, P., Cimatti, A., Slaney, J., & Thiébaux, S. (2002). Solving power supply restoration problems with planning via symbolic model-checking. In Proceedings of the 15th European Conference on Artificial Intelligence (ECAI), pp. 576–580, Lion, France.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90(1-2), 279–298.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129(1–2),
5–33.
Bonet, B., & Geffner, H. (2000). Planning with incomplete information as heuristic search in belief
space. In Proceedings of the 5th International Conference on Artificial Intelligence Planning
and Scheduling Systems (AIPS), pp. 52–61, Breckenridge, CO.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence
in Bayesian networks. In Proceedings of the Twelfth Conference on Uncertainty in Artificial
Intelligence (UAI), pp. 115–123, Portland, OR.
Brafman, R. I., & Domshlak, C. (2006). Factored planning: How, when, and when not. In Proceedings of the 18th National Conference on Artificial Intelligence (AAAI), pp. 809–814, Boston,
MA.
Bryce, D., & Kambhampati, S. (2004). Heuristic guidance measures for conformant planning. In
Proceedings of the 14th International Conference on Automated Planning and Scheduling
(ICAPS), pp. 365–374, Whistler, BC, Canada.
Bryce, D., Kambhampati, S., & Smith, D. (2006). Sequential Monte Carlo in probabilistic planning
reachability heuristics. In Proceedings of the 16th International Conference on Automated
Planning and Scheduling (ICAPS), pp. 233–242, Cumbria, UK.
618

P ROBABILISTIC -FF

Chavira, M., & Darwiche, A. (2005). Compiling Bayesian networks with local structure. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI), pp.
1306–1312, Edinburgh, Scotland.
Darwiche, A. (2000). Recursive conditioning. Artificial Intelligence, 125(1-2), 5–41.
Darwiche, A. (2001). Constant-space reasoning in dynamic Bayesian networks. International Journal of Approximate Reasoning, 26(3), 161–178.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation. Computational Intelligence, 5, 142–150.
Dechter, R. (1999). Bucket elimination: A unified framework for reasoning. Artificial Intelligence,
113, 41–85.
Domshlak, C., & Hoffmann, J. (2006). Fast probabilistic planning through weighted model counting. In Proceedings of the 16th International Conference on Automated Planning and
Scheduling (ICAPS), pp. 243–252, Cumbria, UK.
Gomes, C. P., Hoffmann, J., Sabharwal, A., & Selman, B. (2007). From sampling to model counting.
In Proceedings of the 20th International Joint Conference on Artificial Intelligence (IJCAI07), Hyderabad, India.
Gomes, C. P., Sabharwal, A., & Selman, B. (2006). Model counting: A new strategy for obtaining good bounds. In Proceedings of the 21th National Conference on Artificial Intelligence
(AAAI-06), pp. 54–61, Boston, MA.
Hanks, S., & McDermott, D. (1994). Modeling a dynamic and uncertain world I: Symbolic and
probabilistic reasoning about change. Artificial Intelligence, 66(1), 1–55.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through heuristic
search. Journal of Artificial Intelligence Research, 14, 253–302.
Hoffmann, J., & Brafman, R. (2006). Conformant planning via heuristic forward search: A new
approach. Artificial Intelligence, 170(6–7), 507–541.
Huang, J. (2006). Combining knowledge compilation and search for efficient conformant probabilistic planning. In Proceedings of the 16th International Conference on Automated Planning
and Scheduling (ICAPS), pp. 253–262, Cumbria, UK.
Hyafil, N., & Bacchus, F. (2004). Utilizing structured representations and CSPs in conformant
probabilistic planning. In Proceedings of the European Conference on Artificial Intelligence
(ECAI), pp. 1033–1034, Valencia, Spain.
Jensen, F. (1996). An Introduction to Bayesian Networks. Springer Verlag, New York.
Kushmerick, N., Hanks, S., & Weld, D. (1995). An algorithm for probabilistic planning. Artificial
Intelligence, 78(1-2), 239–286.
Little, I., Aberdeen, D., & Thiébaux, S. (2005). Prottle: A probabilistic temporal planner. In Proceedings of the 20th National Conference on Artificial Intelligence (AAAI-05), pp. 1181–
1186, Pittsburgh, PA.
Littman, M. L., Goldsmith, J., & Mundhenk, M. (1998). The computational complexity of probabilistic planning. Journal of Artificial Intelligence Research, 9, 1–36.
619

D OMSHLAK & H OFFMANN

Majercik, S. M., & Littman, M. L. (1998). MAXPLAN: A new approach to probabilistic planning. In Proceedings of the 4th International Conference on Artificial Intelligence Planning
Systems (AIPS), pp. 86–93, Pittsburgh, PA.
Majercik, S. M., & Littman, M. L. (2003). Contingent planning under uncertainty via stochastic
satisfiability. Artificial Intelligence, 147(1-2), 119–162.
McDermott, D. (1998). The 1998 AI Planning Systems Competition. AI Magazine, 2(2), 35–55.
McDermott, D. V. (1999). Using regression-match graphs to control search in planning. Artificial
Intelligence, 109(1-2), 111–159.
Onder, N., Whelan, G. C., & Li, L. (2006). Engineering a conformant probabilistic planner. Journal
of Artificial Intelligence Research, 25, 1–15.
Pearl, J. (1984). Heuristics - Intelligent Search Strategies for Computer Problem Solving. AddisonWesley.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference.
Morgan Kaufmann, San Mateo, CA.
Rintanen, J. (2003). Expressive equivalence of formalisms for planning with sensing. In Proceedings of the 13th International Conference on Automated Planning and Scheduling (ICAPS),
pp. 185–194, Trento, Italy.
Roth, D. (1996). On the hardness of approximate reasoning. Artificial Intelligence, 82(1-2), 273–
302.
Russell, S., & Norvig, P. (2004). Artificial Intelligence: A Modern Approach (2 edition). Pearson.
Sang, T., Bacchus, F., Beame, P., Kautz, H., & Pitassi, T. (2004). Combining component caching
and clause learning for effective model counting. In (Online) Proceedings of the 7th International Conference on Theory and Applications of Satisfiability Testing (SAT), Vancouver, BC,
Canada.
Sang, T., Beame, P., & Kautz, H. (2005). Solving Bayes networks by weighted model counting. In
Proceedings of the 20th National Conference on Artificial Intelligence (AAAI), pp. 475–482,
Pittsburgh, PA.
Shimony, S. E. (1993). The role of relevance in explanation I: Irrelevance as statistical independence. International Journal of Approximate Reasoning, 8(4), 281–324.
Shimony, S. E. (1995). The role of relevance in explanation II: Disjunctive assignments and approximate independence. International Journal of Approximate Reasoning, 13(1), 27–60.
Zhang, N. L., & Poole, D. (1994). A simple approach to Bayesian network computations. In
Proceedings of the 10th Canadian Conference on Artificial Intelligence, pp. 171–178, Banff,
Alberta, Canada.

620

Journal of Artificial Intelligence Research 30 (2007) 361-412

Submitted 07/07; published 11/07

Natural Events
John Bell

jb@dcs.qmul.ac.uk

Department of Computer Science,
Queen Mary, University of London,
London E1 4NS, UK

Abstract
This paper develops an inductive theory of predictive common sense reasoning. The
theory provides the basis for an integrated solution to the three traditional problems of
reasoning about change; the frame, qualification, and ramification problems. The theory is
also capable of representing non-deterministic events, and it provides a means for stating
defeasible preferences over the outcomes of conflicting simultaneous events.

1. Introduction
A great deal has been written on the logical representation of common sense reasoning
about change since the publication of McCarthy and Hayes’s (1969) seminal paper, and
many theories have been proposed; see, for example, the monographs by Sandewall (1994),
Shanahan (1997), and Reiter (2001).
Most theories treat events1 deductively, along the lines of the representation of actions
used in the planner strips (Fikes & Nilsson, 1971). Each event type is defined by its
preconditions and effects. For example, in the blocks world, the preconditions for unstacking
block x from block y are that x is on y, x is clear (no block is on top of it), and the robot hand
is empty. The effects are that the hand is holding x, y is clear, and each of the preconditions
is false. Change is then a matter of deduction. If a particular event (a token of an event
type) occurs and its particular preconditions hold, then its particular effects are deduced,
and so necessarily follow, from it. Events of this kind will be called deductive events and
the view that natural events can be represented deductively will be called Deductionism.
Deductive event types can be thought of as invariable regularities (or uniformities) of sequence. Viewed in this way the strips representation of events can be seen to be descended
from those considered by Hume and Mill in their discussions of causation. Hume (1739, Bk
I, Pt III) suggests that we inductively acquire knowledge of regularities of succession of the
form: A-type events are followed by B-type events. We then consider that A-type events
cause B-type events, because whenever we see an A-type event we expect that it will be
followed by a B-type event. Mill (1898, Bk III, Ch 5) complicates this picture by considering
assemblages of conditions. A single assemblage might consist of an A-type event together
with certain conditions which must be present (positive conditions) and certain conditions
which must be absent (negative conditions). For example, an assemblage concerning the
lighting of matches might include the striking of the match, the presence of oxygen, and
the absence of dampness in the match head.
1. Events are assumed to include the physical actions of agents; whether intentional or unintentional.
c
2007
AI Access Foundation. All rights reserved.

Bell

Mill (1898) thought that it was possible, at least in principle, to define assemblages which
are detailed enough to ensure their effects: “For every event there is some combination of
objects or events, some given concurrence of circumstances, positive and negative, the
occurrence of which is always followed by that phenomenon” (p. 214).
However, Hume (1777, pp. 36-38) had already argued against this possibility. It is always
possible that a regularity, no matter how long it has continued in the past, will not continue
in the future. Consequently no set of sentences which report what has been observed ever
logically implies anything about what has not been observed. As Goodman (1954, p. 59)
puts it, what has happened imposes no logical restrictions on what will happen. So, if
Deductionism is to be plausible, it is necessary to assume that Nature is uniform; that the
future will resemble the past, that past regularities will continue. Now, clearly, Uniformity
cannot be justified by appealing to experience, and it is difficult to see how else it can be
justified; see, Goodman’s discussion (pp. 61-62). Without such a justification, Deductionism
should be regarded as being suspect in theory.
Deductionism is also suspect in practice, as it is impossible in practice to define preconditions which, together with the occurrence of the event, are sufficient to ensure that
its effects will follow. For example, Russell (1913, p. 7) considers the problem of conflicting
events: “I put my penny in the slot, but before I can draw out my ticket there is an earthquake which upsets the machine and my calculations. In order to be sure of the expected
effect, we must know that there is nothing in the environment to interfere with it. But this
means that the supposed cause is not, by itself, adequate to insure the effect”. Russell also
observes that we cannot usefully solve the problem by complicating preconditions because:
“as soon as we include the environment, the probability of repetition is diminished, until, at
last, when the whole environment is included, the probability of repetition becomes almost
nil” (pp. 7-8).
The problem of specifying preconditions which are always sufficient also arises in Mackie’s
account of causal regularities (invariable regularities of sequence). For example, “at least
part of the answer [to the question of what caused a particular fire] is that there is a set
of conditions (of which some are positive and some are negative), including the presence
of inflammable material, the absence of a suitably placed sprinkler, and no doubt quite a
number of others” (Mackie, 1975, p. 16). The list of conditions is incomplete because, even
if causal regularities hold “in the objects”, they are seldom, if ever, known in full: “Causal
knowledge progresses gradually towards the formulation of such regularities, but it hardly
ever gets there. Causal regularities as known are typically incomplete . . . What we know
are certain elliptical or gappy universal propositions” (Mackie, 1974, p. 66).
The sufficient-preconditions problem becomes more acute when we consider formal representations which are intended to be of practical use, because the preconditions have to
be computationally tractable. McCarthy (1977, p. 1040) gives the example of using a boat
to cross a river. Given that the boat is a rowing boat, that it is equipped with oars, and is
manned by an oarsman, it can be used to convey two passengers across the river; provided
that the boat does not leak, and provided that it does not hit a rock, and provided that
it is not hit by another boat, and provided that it is not overturned by a hippopotamus,
or sunk by a meteorite, or vapourized by a thermonuclear blast, etc. It seems that the list
of qualifications which need to be added is limited only by the limits of our imagination.
Accordingly, McCarthy calls the sufficient-preconditions problem the qualification problem.
362

Natural Events

In response, Deductionists might argue that representations are abstractions and that
their approach works well for simple domains, in which it can be assumed that qualifications do not arise. In such domains, they might argue, the uniformity assumption is
reasonable, and so deductive theories do provide a useful representation. This may well be
true, but, theories of this kind cannot readily be extended to more complex (less uniform)
domains because the additional complexity of the required preconditions quickly becomes
overwhelming. Thus the Deductionist approach may be appropriate for certain applications,
such as the mathematical analysis of high-level programming languages in which elementary commands are viewed as abstract operations on data. But it is inappropriate for
the representation of predictive reasoning about natural events, the events of our everyday
experience, because these are too irregular to be treated deductively. Moreover, the Deductionist abstraction is better thought of as an idealization, and a problematic one at that.
If the preconditions of a deductive event hold on occurrence, then its effects are logically
guaranteed to follow, and so no natural force can intervene to prevent them from doing so.
Deductive events are thus not natural, but supernatural. This idealization creates technical
difficulties when it comes to the representation of events with variable effects (including
non-deterministic and context-dependent effects) as these effects should not always be deduced, and to the representation of conflicting events (events whose effects are individually
consistent but jointly inconsistent) as their joint effects cannot consistently be deduced.
Consequently Deductionist theories of these phenomena (some of which are discussed in the
sequel) face unnecessary, self-imposed, difficulties. It is difficult to escape the conclusion
that, in representing natural events deductively, Deductionism starts off on the wrong foot.
But this conclusion is hardly surprising when we consider that our predictive reasoning
about natural events is inductive, rather than deductive, in nature.2 The major purpose of
predictive reasoning is to support practical reasoning; that is, reasoning about what to do.
Predictive reasoning is normally based on partial knowledge (or incomplete belief), both of
causal regularities, as Mackie observes, and of the contexts in which the events concerned
occur. It also tends to be conjectural in that it seeks to produce reasonable conclusions on
the basis of what is known. As a result it tends to produce conclusions which are both supradeductive (which may not be deducible from the known) and defeasible (which may turn
out to be wrong).3 Accordingly, definitions of (practical, non-omniscient) rationality are
typically couched in terms of the utility of the expected (rather than the actual) outcomes
of actions. Russell and Norvig (2003, p. 36) illustrate this point as follows: “I am walking
along the Champs Elysées one day and I see an old friend across the street. There is no
traffic nearby and I am not otherwise engaged, so being rational, I start to cross the street.
Meanwhile at 33,000 feet, a cargo door falls off a passing airliner, and before I make it to
2. In philosophy, the term ‘inductive reasoning’ is applied to any form of qualitative non-deductive reasoning. It thus includes enumerative induction, in which a general rule is inferred from a non-exhaustive
set of inferences (for example, all of the emeralds which have been observed have been green, therefore
all emeralds are green). This is the form of reasoning which underlies our knowledge of regularities.
When it comes to inductive events the idea (as explained in the text below) is to produce reasonable
conclusions about their outcomes on the basis of partial information. In AI, inductive reasoning of this
kind is formalized in non-monotonic logics.
3. J. K. Galbraith once remarked that there are two kinds of forecasters. Those who don’t know, and those
who know that they don’t know.

363

Bell

the other side of the street I am flattened. Was I irrational to cross the street? It is unlikely
that my obituary would read ‘Idiot attempts to cross street’”.
This change in perspective results in a substantial simplification of the problem of specifying preconditions. We are no longer concerned with invariable regularities of sequence,
with necessary connections between events and their effects, but rather with regularities
of sequence which normally hold; with “fairly dependable regularities of sequence” (Russell, 1913, p. 8), with expected connections between events and their effects. Consequently
we can define preconditions which, together with their associated events, form conditions
which are normally sufficient for the associated effects, and which are otherwise minimal
in the sense that no part of them is redundant. Preconditions of this kind tend to be both
tractable (simple) and useful (to occur frequently in practice).
We can now give a Humeian account of predictions involving natural events in terms of
(fairly) dependable regularities and the expectations that they engender. If an event occurs,
its preconditions obtain, and we are not aware of anything which will prevent the effects
from following, then we form a clear expectation (we “know”) that the effects will follow,
and so it is rational (reasonable) to predict them. For example, if block A is unstacked
from block B, the preconditions obtain, and we are not aware of a preventer, then we
clearly expect, and so predict, that A will no longer be on B. More complex cases involve
conflicting events. If, in such a case, we have a clear expectation of the outcome, then it is
rational to predict it. For example, we clearly expect, and so predict, that the airliner door
will crush, rather than bounce off, the intrepid pedestrian. In this example we consider that
two conflicting outcomes are possible, but that only one of them is probable. However, in
other cases our expectations are unclear; we are torn between conflicting expectations and
so do not “know” what to expect. In such cases it seems reasonable to adopt a cautious
approach, and restrict our predictions to those effects that we clearly expect. For example,
if a fair coin is tossed, then we do not have a clear expectation as to which side it will
land. We “half” expect that it will land on heads and we “half” expect that it will land on
tails. We consider that two conflicting outcomes are equally probable. So caution dictates
that we should predict that the coin lands on one side or the other, but that we should
not predict which of the two sides it will land on. Note that, as expectations are based on
incomplete knowledge, the predictions which are based on them are defeasible. For example,
if, unbeknown to us, block A is glued to block B when A is unstacked from B, then the
event does not have the effects that we predict it will have.
We can thus begin by thinking of natural events as defeasible strips events, as stripslike events whose effects do not always follow them (when they occur and their preconditions
are true), and inferring their effects inductively. Accordingly, events of this kind will be
called inductive events, and the view that natural events should be represented inductively
will be called Inductionism. So if ‘logic’ is understood to include both deductive and inductive inference, then the Inductionist objection to Deductionism can be stated succinctly:
Deductionism is a logical mistake.
This paper can be seen as an argument for Inductionism. It begins by presenting a basic
theory of inductive events and then uses this as the basis for a more comprehensive theory
of natural events.
The formal language in which the theory is expressed is defined in the next section, and
the basic theory of inductive events is then given in Section 3. The theory builds on the
364

Natural Events

ideas of McCarthy (1986, §9), Lifschitz (1987), and Shoham (1988), and is logico-pragmatic
in nature; that is, it consists of a set of axioms together with a formal pragmatics, which,
given a formal theory containing the axioms, interprets it in a particular way, and in doing
so, generates the predictions of the theory. The basic theory of inductive events provides
the basis of a solution to the qualification problem and integrates this with the basis of
a solution to the complementary frame problem (McCarthy & Hayes, 1969, p. 487); that
is, the problem of inferring what is unchanged by the occurrence of an event (or, more
generally, by the occurrence of several simultaneously occurring events).
In Section 4, the basic theory of inductive events is extended by introducing a distinction
between inductive events which are primary and those which are secondary. Whereas
primary events occur independently, secondary events are invoked by other (primary or
secondary) events in appropriate contexts, and are causally dependent on them. In the
simplest case, a primary event invokes a secondary event and is the only event to do so.
In which case, the secondary event succeeds (is followed by its effects) only if the primary
event which invoked it succeeds. This extension to the basic theory makes it possible for
inductive events to have additional context-dependent effects, thereby providing the basis
for a solution to the ramification problem (Ginsberg & Smith, 1988); that is, the problem
of representing the indirect, context-dependent, effects of events. For example, if an agent
is holding a block and the agent moves, then the move-agent event invokes a causally
dependent move-block event, with the effect that the block moves only if the agent does.
This extension also makes it possible to represent events with non-deterministic effects. For
example, the non-deterministic event of tossing a fair coin can be represented by having the
event invoke two conflicting deterministic events, one of which has the effect that the coin
lands on heads, the other that it lands on tails.
The theory of natural events is completed in Section 5, which deals with the problem
of representing defeasible preferences over the outcomes of conflicting simultaneous events.
When two events conflict we often have a clear expectation about the outcome. For example,
if two agents attempt to go through a door simultaneously and only one of them can succeed,
then it is reasonable to expect that the stronger one will do so. However this expectation
is defeasible. The stronger agent may fail for some independent reason (the agent may slip,
say), in which case the preference is reversed and we expect that the weaker agent will
succeed (although the weaker agent may also slip, etc.). In order to represent defeasible
asymmetric expectations of this kind event preferences are introduced, and the formal
pragmatics of the basic theory is refined in order to interpret them correctly.
A philosophical justification of the theory of natural events is given in Section 6, and
related work is discussed in Section 7.
Although causal notions underlie much of the development of the theory of natural
events, there is no explicit reference to causation in the theory. This is because it is intended
to provide the basis for a definition of sufficient causation which forms part of a larger theory
of causation (Bell, 2004, 2006, 2008).4
4. According to the theory, the occurrence of event e in context c is a sufficient cause of effect φ if the occurrence of e in c is sufficient to ensure φ; for instance, if e succeeds at time t and φ is a logical consequence of
e’s effects at t+1, then e is a sufficient cause of φ. The definition of causation is then obtained by requiring that sufficient causes also satisfy a refinement of Lewis’s (1986, Ch. 21) counterfactual-dependence

365

Bell

2. The Event Language EL
The theory of events is expressed in the event language EL, which has been developed
in order to represent and reason about events and their effects, on the basis of partial
information, at successive points in time. This section begins with an informal introduction
and then gives a formal account.
In order to represent epistemic partiality in a natural and economical way EL is based
on Kleene’s (1952, §64) strong three-valued logic.5 Kleene introduced the truth value ‘undefined’ in order to accommodate undecidable mathematical sentences. However he also
suggested that ‘undefined’ could be interpreted as ‘unknown’, where: “‘unknown’ is a category into which we can regard any proposition as falling, whose value we either do not
know or choose for the moment to disregard; and it does not the exclude the other two
possibilities ‘true’ and ‘false’” (p. 335). Thus understood, ‘undefined’ is not a truth value
on a par with ‘true’ and ‘false’, and its introduction is intended as a practical, logically
conservative, way of reasoning with partial information; rather than a revolutionary attack
on classical logic.6
In keeping with this interpretation, the truth value of a sentence should be classical
(either ‘true’ or ‘false’) if enough is known to determine it. The formal semantics for
the propositional case can thus be given as follows. A model, M , consists of a possibly
partial evaluation function, V , which assigns at most one classical truth value to each
atomic proposition. The truth (|=) and falsity (=|) of sentences in M is then defined by the
following truth and falsity conditions:
M |= p iff V (p) = true
M =| p iff V (p) = false
M |= ¬φ iff M =| φ
M =| ¬φ iff M |= φ
condition: the occurrence of event e is a cause of effect φ in context c iff (i) e is a sufficient cause of φ
in c, and (ii) φ depends on e in the closest context to c in which e is the only sufficient cause of φ.
5. Kleene’s logic will be familiar to readers with a background in philosophical logic (it is, for example, used
by Kripke, 1975, as a basis for his theory of truth), and the choice to use it here is likely to appear a
natural one to them. However, Kleene’s logic may be unfamiliar to readers in the ‘reasoning about actions
and change’ community, and they may well wonder why I have not used a more established classical
language such as the Situation Calculus (McCarthy & Hayes, 1969). A full justification of my choice
would involve a lengthy comparison of languages. In short, it is simpler to acknowledge that epistemic
partiality is a ubiquitous feature of predictive reasoning and to deal with it directly, as in Kleene’s
logic, rather than indirectly in a classical logic; by means of syntactic encoding and circumscription (as
in the Situation Calculus), or by using modal logic (as in TK, Shoham, 1988). The representation of
partiality in Kleene’s logic is also optimal; because there is no cost associated with the representation
of what is unknown. By contrast, partiality in classical reasoning requires the consideration of a class
of models (or possible worlds) which is large enough to ensure that unwanted “noise” (arbitrary, but
compulsory, assignments of classical truth values to sentences whose truth values are not determined by
the theory in question) is eliminated. This profligacy is significant when considering the contemplated
model-building implementation of event theories (Bell, 1996, §1); see the remarks on implementation in
Section 8. Finally, as indicated in the introduction and Footnote 4, the theory of events is intended to
form part of a larger theory of causation, in which EL is embedded in a partial modal language.
6. Confirmed classicists can thus rest assured that they are not being threatened with anything radical,
such as “the Bolshevik menace of Brouwer and Weyl” (remark on Intuitionism attributed to F.P. Ramsey
by Blackburne, 1994).

366

Natural Events

M |= φ ∧ ψ iff M |= φ and M |= ψ
M =| φ ∧ ψ iff M =| φ or M =| ψ

So a sentence ¬φ is true if φ is false, is false if φ is true, and is undefined otherwise; and
the sentence φ ∧ ψ is true if φ and ψ are both true, false if either is false, and is undefined
otherwise. Note that when the evaluation function is total this semantics is equivalent to
the semantics for classical propositional calculus. So the essential difference between the two
semantics is the classical assumption that the evaluation function is total; the additional
requirement that V assigns at least one classical truth value to each atomic proposition.
Further operators can be defined as in classical logic. In particular, inclusive disjunction
is defined as: φ ∨ ψ =Df ¬(¬φ ∧ ¬ψ); so φ ∨ ψ is true if either disjunct is true, is false if
both disjuncts are false, and is undefined otherwise. And exclusive disjunction is defined
as: φ ⊕ ψ =Df (φ ∧ ¬ψ) ∨ (¬φ ∧ ψ); so φ ⊕ ψ is true if the truth values of φ and ψ are both
defined and are different, false if the truth values of φ and ψ are both defined and are the
same, and is undefined otherwise.7
Kleene’s logic can perhaps be called “demi-classical”, as it becomes classical when the
truth values of all of the constituent atomic sentences are classical. Unsurprisingly then,
the use of Kleene’s logic does not, of itself, solve any of the problems of predictive reasoning
beyond that of representing partiality. There is, for example, no reliance on a special
“causal” notion of consequence, such as that of Linear Logic (Girard, 1987).
The expressiveness of Kleene’s language is greatly enhanced by adding a classicallyvalued “definedness” operator to it. The sentence Dφ is true if φ is defined (is either true
or false), and is false otherwise:

M |= Dφ iff either M |= φ or M =| φ
M =| Dφ iff neither M |= φ nor M =| φ

Further classically-valued operators can now be defined as follows:

Tφ =Df Dφ ∧ φ
φ → ψ =Df ¬Tφ ∨ Tψ

Fφ =Df Dφ ∧ ¬φ
Uφ =Df ¬Dφ
φ ≡ ψ =Df (Tφ ∧ Tψ) ∨ (Fφ ∧ Fψ) ∨ (Uφ ∧ Uψ)

Thus, for sentences φ and ψ: Tφ is true if φ is true, and is false otherwise; Fφ is true if φ
is false, and is false otherwise; Uφ is true if φ is undefined, and is false otherwise; φ → ψ is

7. Readers who are unfamiliar with Kleene’s logic may wish to check the definitions against the semantics.
For example, M |= φ ∨ ψ iff M |= ¬(¬φ ∧ ¬ψ) iff M =| ¬φ ∧ ¬ψ iff [M =| ¬φ or M =| ¬ψ] iff [M |= φ or
M |= ψ].

367

Bell

true if ψ is true or φ is not, and is false otherwise;8 and φ ≡ ψ is true if φ and ψ have the
same truth value, and is false otherwise.9
The first-order extension, given by Kleene, is straightforward. The universal sentence
∀xφ is true if φ is true for all assignments to x, false if φ is false for some such assignment, and
is undefined otherwise. The existential quantifier is defined as in classical logic: ∃xφ =Df
¬∀x¬φ; thus ∃xφ is true if φ is true for some assignment to x, is false if φ is false for all
such assignments, and is undefined otherwise.
In order to represent change, events and time points are added as an additional sort.
For simplicity, the order of time is assumed to be discrete and linear.
An object atom is an atom of the form r(u1 , . . . , un )(t), where r is an object relation
symbol, the ui are object terms, and t is a time-point term. For example, the object atoms
At(O, L)(1) and ¬At(O, L)(2) state respectively that object O is at location L at time 1,
and that O is not at L at time 2.
An event atom is an atom of the form r(u1 , . . . , un )(t), where r is an event relation
symbol, the ui are event terms, and t is a time-point term. For example, the event atom
Occ(Move(O, L1, L2))(3) states that the event consisting of object O moving from location
L1 to location L2 occurs at time 3.
The intuition behind the fact-event distinction is that events are active “agents” (causes)
of change, while facts are passive “patients” of change which persist through time until
affected by some event (until some event causes them to change).10
In order to represent the persistence of facts, second-order quantification over object
relations and second-order relations are added to EL. An object-relation atom is an atom
of the form r(u1 , . . . , un )(t), where r is a second-order relation symbol, the ui are object
relation symbols or object terms, and t is a time-point term. For example, the objectrelation atom Inert(At, hO, Li)(4) states that the At relation is inert for objects O and L at
time 4.
8. The conditional φ → ψ captures much of the flavour of classical material implication. This can be
emphasized by defining the weaker conditional of Kleene’s logic: φ ⊃ ψ =Df ¬φ ∨ ψ. This conditional is
inadequate, at least for present purposes, because it is undefined, rather than false, when φ is true and ψ
is undefined. This is not the case for the stronger conditional, →, which could equally have been defined
as: Tφ ⊃ Tψ. This definition makes it clear that a conditional φ → ψ states a constraint which must
be satisfied if φ is true, but which can otherwise be ignored. The conditional does not quite capture
all of the meaning of classical material implication, as it does not satisfy the (implicitly understood)
condition that if ψ is false, then so is φ. If desired, it is possible to define a stronger conditional, which
better represents classical material implication, as follows: φ −→ ψ =Df (φ → ψ) ∧ (¬ψ → ¬φ); or,
equivalently, φ −→ ψ =Df (Tφ ⊃ Tψ) ∧ (Fψ ⊃ Fφ). The equivalence operator can then be defined as
follows: φ ≡ ψ =Df (φ −→ ψ) ∧ (ψ −→ φ).
9. The addition of a truth-value designation operator is not new. Bochvar (1939) added a truth operator to
a different system of connectives, and the undefined operator U is a special case of Rosser and Turquette’s
(1952) Jk operator. Barringer, Cheng, and Jones (1984) give a natural deduction system for Kleene’s logic
and the defined operator D. Their system is readily extended to include the operators defined above; the
additional introduction and elimination rules for connectives such as → and ≡ simply fold and unfold the
definitions. Similar languages have been used as a basis for the formalization of non-monotonic reasoning
(Doherty, 1996).
10. The fact-event distinction is similar to McCarthy’s (1986, §9) fluent-event distinction. The case for
adding events to the ontology of facts is argued by Davidson (1980). Lewis (1986, Ch. 23) goes further
and treats facts as events.

368

Natural Events

A temporally-indexed relation is any relation whose atoms are temporally indexed; whose
atoms are of the form r(u1 , . . . , un )(t). The primitive atemporal relations of EL are temporal
precedence, ‘<’, and identity, ‘=’. Further atemporal relations can be defined in terms of
temporally-indexed relations as follows:
r(u1 , . . . , un ) =Df ∀t r(u1 , . . . , un )(t) .
For example, ∀t Phys(At)(t) can be abbreviated to Phys(At), which states that At is (eternally) a physical relation.
The formal semantics of EL can be sketched as follows. Models contain a set of objects,
a set of event types, a time frame consisting of a set of time points ordered by a (discrete
and linear) precedence relation, and functions for interpreting terms and relations. The
twin notions of satisfaction and violation of a formula, by a variable assignment in a model,
are defined by means of a parallel recursion. The truth (falsity) of a sentence in a model
is then defined in terms of satisfaction (violation) by all assignments for that model. As in
classical logic, a model M is said to be a model of a sentence φ (a set of sentences Θ) if φ
is true in M (if every sentence in Θ is true in M ), and a set of sentences Θ semantically
entails a sentence φ iff all models of Θ are also models of φ.
In the remainder of this section the formal syntax and semantics of EL are defined.
Readers may wish to skip to the next section and return to consult the details as necessary.
Definition 1 The four sorts of EL are identified by the following letters: O (objects), T
(time points), E (events), and R (object relations). The vocabulary of EL consists of the
symbols ‘<’, ‘=’, ‘¬’, ‘D’, ‘∧’, ‘∀’, ‘(’, and ‘ )’, and the following countable sets of symbols:
• CS (constants of each sort S),
• VS (variables of each sort S),
• FS (function symbols of each arity n ≥ 1 of each sort S),
• RO , RE , RR (relation symbols of each arity n ≥ 0 of sorts O, E,and R).
The sets RO and CR are required to be the same set. Otherwise the above sets are required to
be mutually disjoint. Furthermore, VR is assumed to contain variables of each arity n ≥ 0.
Definition 2 The terms of EL are defined as follows.
• termS = CS ∪ VS ∪ {f (u1 , . . . , un ) : n-ary f ∈ FS , ui ∈ termS } for S ∈ {O, T, R}.
• termE = CE ∪ VE ∪ {f (u1 , . . . , un ) : n-ary f ∈ FE , ui ∈ termO }.
Definition 3 EL is the minimal set which satisfies the following conditions.
• If t, t′ ∈ termT then t < t′ ∈ EL.
• If S is any sort and u, u′ ∈ termS , then u = u′ ∈ EL.
• If u1 , . . . , un ∈ termO , rO is an n-ary relation symbol in RO , and t ∈ termT , then
rO (u1 , . . . , un )(t) ∈ EL.
369

Bell

• If S is of sort E or R, u1 , . . . , un ∈ termS and w1 , . . . , wm ∈ termO (where m = 0 if
n = 0), rS is an n + m-ary relation symbol in RS , and t ∈ termT , then rS (u1 , . . . , un ,
w1 , . . . , wm )(t) ∈ EL.
• If u1 , . . . , un ∈ termO , vR is an n-ary variable in VR , and t ∈ termT , then
vR (u1 , . . . , un )(t) ∈ EL.
• If φ, ψ ∈ EL, then ¬φ ∈ EL, Dφ ∈ EL, and (φ ∧ ψ) ∈ EL.
• If S is any sort, v ∈ VS and φ ∈ EL, then ∀vφ ∈ EL.
The members of EL are called formulas (of EL). Those formulas in which no variable occurs
free are called sentences (of EL).
Models for EL consist of a set O of objects, a set E of event types, a temporal frame
hT , ≺T i (where T is a set of time points and ≺T is the before-after relation on T ), and
interpretation functions for terms and relations. For simplicity, time is assumed to be
isomorphic to the integers. The denotations of terms are always defined and do not vary
over time. By contrast, temporally-indexed relations may be partial and may vary over
time. Consequently each temporally-indexed relation is interpreted by a function from time
points to partial characteristic functions. Where defined, the partial characteristic function
associated with a time point maps instances of the relation to {true, false}.
Definition 4 A model for EL is a structure hO, E, hT , ≺T i, F, R, Vi, where:
• O, E and T are mutually disjoint, non-empty, countable sets,
• ≺T is a binary relation on T which is isomorphic to the integers,
• R = hRO , RE , RR i. RO is a set of partial functions of each arity n ≥ 0 of type On →
{true, false}. For hS, Si ∈ {hE, Ei, hR, T → RO i}, RS is a set of partial functions of
each arity n + m ≥ 0 of type S n × Om → {true, false}.
• F = hFO , FT , FE , FR i. For each hS, Si ∈ {hO, Oi, hT, T i, hR, T → RO i}, FS is a set
of functions of each arity n ≥ 1 of type S n → S. FE is a set of functions of each
arity n ≥ 1 of type On → E.
C , V C , V C , V C i, hV F , V F , V F , V F i, hV R , V R , V R ii is an interpretation function
• V = hhVO
T
E
R
O
T
E
R
O
E
R
such that: VSC : CS → S for each hS, Si ∈ {hO, Oi, hT, T i, hE, Ei, hR, T → RO i},
R = VC.
VSF : FS → FS , VSR : RS → (T → RS ), and VO
R

Definition 5 A variable assignment for an EL model is a function g = hgO , gT , gE , gR i,
where for hS, Si ∈ {hO, Oi, hT, T i, hE, Ei}, gS : VS → S, and gR : VR → (T → RO ). For
EL-model M , with interpretation function V and variable assignment g for M , the term
evaluation function Vg is defined, on the terms and relation symbols of EL, as follows:


gS (u)


 V C (u)
S
Vg (u) =
 VSF (f )(Vg (u1 ), . . . , Vg (un ))


 R

VS (u)

370

if
if
if
if

u ∈ VS ,
u ∈ CS ,
u = f (u1 , . . . , un ) ∈ termS ,
u ∈ RS .

Natural Events

Table 1: Satisfaction and violation conditions for EL (see Definition 6)
M, g |= t < t′ iff hVg (t), Vg (t′ )i ∈ ≺T
M, g =| t < t′ iff hVg (t), Vg (t′ )i ∈
/ ≺T
M, g |= u = u′ iff Vg (u) is Vg (u′ )
M, g =| u = u′ iff Vg (u) is not Vg (u′ )
M, g |= u(u1 , . . . , un )(t) iff (Vg (u)(Vg (t)))(Vg (u1 ), . . . , Vg (un )) = true
M, g =| u(u1 , . . . , un )(t) iff (Vg (u)(Vg (t)))(Vg (u1 ), . . . , Vg (un )) = false
M, g |= ¬ψ iff M, g =| ψ
M, g =| ¬ψ iff M, g |= ψ
M, g |= Dψ iff either M, g |= ψ or M, g =| ψ
M, g =| Dψ iff neither M, g |= ψ nor M, g =| ψ
M, g |= ψ ∧ χ iff M, g |= ψ and M, g |= χ
M, g =| ψ ∧ χ iff M, g =| ψ or M, g =| χ
M, g |= ∀vψ iff M, g′ |= ψ for every g′ such that g ≈v g′
M, g =| ∀vψ iff M, g′ =| ψ for some g′ such that g ≈v g′

Definition 6 Let M be an EL model, g be a variable assignment for M , and let g ≈v g′
indicate that variable assignment g′ differs from g at most on the assignment to variable v.
Then g satisfies an EL-formula φ in M (written M, g |= φ) or violates φ in M (written
M, g =| φ) according to the clauses given in Table 1.
Let M be an EL model. Then a formula φ is true in M (written M |= φ) if M, g |= φ
for all variable assignments g; a formula φ is false in M (written M =| φ) if M, g =| φ
for all variable assignments g; M is a model of a sentence φ iff φ is true in M ; and M is
a model of a set of sentences Θ iff M is a model of every sentence in Θ.
A set of sentences Θ (semantically) entails a sentence φ (written Θ |= φ) iff every model
of Θ is also a model of φ.

3. Inductive Events
The formal theory of natural events is introduced in stages, beginning, in this section, with
the basic theory of inductive events.
Definition 7 The theory of inductive events, ΘInd , consists of the axioms given in Table 2;
thus ΘInd = {(1), (2), (3)}. An event theory is any set of EL sentences which contains ΘInd .
Axiom (1) defines the notion of success, and states that event e succeeds at time t iff it
is true that e occurs at t, the preconditions of e are true at t, and the effects of e are true
at t+1.11 The presence of the truth operator in this axiom ensures that the relation Succ
11. Thus defined, the success or failure of an event is a simple, objective, matter of whether its occurrence is
accompanied by its preconditions and is followed by its effects. So when speaking of the success or failure

371

Bell

Table 2: The theory of inductive events, ΘInd
∀e, t(Succ(e)(t) ≡ T(Occ(e)(t) ∧ Pre(e)(t) ∧ Eff(e)(t+1)))

(1)

∀R T(Phys(R) ⊕ Theo(R))

(2)

∀R, x, t(Inert(R, hxi)(t) ≡ (Phys(R) ∧ (R(x)(t) ≡ R(x)(t+1))))

(3)

is classical in the sense that every instance of it is either true or false. It is thus possible to
reason classically about success and failure on the basis of partial information.
In view of the fact-event distinction, it is also necessary to represent inertia; that is,
the temporal persistence of facts which are not changed by events. The definition of inertia
begins with a distinction between physical facts (represented by physical relations) and
theoretical facts (represented by theoretical relations). Intuitively, physical facts are facts
about the world as we directly observe it, whereas theoretical facts are the product of
our more complex reflection on (theorizing about) the physical facts.12 For example, in
a representation of the blocks world the locations of blocks might be represented by the
object relation At. This can then be used to define the relation Clear, which is true of a
location at a point in time if there are no blocks at that location at that point in time. In
this theory, the relation At is naturally classified as a physical relation (as it represents the
physical locations of blocks) while the relation Clear is naturally classified as a theoretical
relation (as it represents a, comparatively complex, property of locations which is defined in
terms of the locations of blocks). Note that, as theoretical facts are (ultimately) defined in
terms of physical facts, the theoretical facts supervene on the physical facts; that is, fixing
the physical facts at any point in time also fixes the theoretical facts at that point in time.
In event theories, physical and theoretical relations are identified by means of the secondorder predicates Phys and Theo respectively. Thus Axiom (2) states that every object
relation is either a physical relation or a theoretical relation. As a matter of notational
convenience the convention is adopted that any object relation which is not declared to be
a theoretical relation is a physical one. This convention is enforced by the formal pragmatics
discussed below.
The inertia of physical facts is defined by Axiom schema (3); which, for simplicity, will
henceforth be called an axiom. In the axiom, R is an n-ary object relation symbol, x is
a vector x1 , . . . , xn of object variables, and hxi is the list hx1 , . . . , xn i. The axiom states
that R is inert for the objects referred to by x at time t iff R is a physical relation and
of an event no end or purpose (no teleology) is implied; we could equally talk of an event occurrence
being complete or incomplete. It is natural to talk informally of intentional actions succeeding or failing,
for example of an agent succeeding or failing in their intention to move to a particular location. But no
attempt is made to represent this intentionality formally.
12. Physical facts can be thought of as Quine’s (1995, Chs. 2-3) observational predications. These are
compounds of more primitive observation sentences, which are “the human equivalent of bird-calls and
apes’ cries” (p. 22). For example, the observational sentences ‘Black’ (or ‘That’s black’) and ‘Dog’
(or ‘That’s a dog’) might be combined in the observational predication ‘Black dog’ (or ‘The dog is
black’). Theoretical facts are the result of more complex compounding, involving logical connectives
and, especially, reification.

372

Natural Events

the truth values of the object atoms R(x)(t) and R(x)(t+1) are equivalent. Note that the
relation Inert is classical in event theories; because Phys is classical (by Axiom (2)) and the
right-hand equivalence is classically valued. Note also that, for event theories which do not
contain occurrences of the Theo relation, Axiom (2) is unnecessary and Axiom (3) can be
simplified accordingly.
We turn now to the intended interpretation of event theories.
On the intended interpretation, Axiom (1) is used to generate the expected outcomes
of events. If Pre(e)(t) and Occ(e)(t) are both true, and it is consistent to assume that
the success atom Succ(e)(t) is true, then this success assumption is made, and the axiom
is used to conclude that the expected effects, Eff(e)(t+1), are true. Thus interpreted the
axiom states that, when accompanied by their preconditions, occurring events are normally
sufficient for (are normally followed by) their effects. When interpreted in this way, the
axiom amounts to a common sense law of change.
Similarly, on the intended interpretation, Axiom (3) is used to generate the expected
persistence of physical facts. If R is a physical relation, the object atom R(x)(t) is defined,
and it is consistent to assume that the inertia atom Inert(R, hxi)(t) is true, then this inertia
assumption is made, and the axiom is used to conclude that the truth value of R(x) persists
from t to t+1.13 Thus interpreted, the axiom states that physical facts normally persist,
and so it amounts to a common sense law of inertia.
The intended interpretations of axioms (1) and (3) often conflict. For example, if an
unstack-A-from-B event occurs, its preconditions are true, and no other relevant facts or
events are involved, then it is consistent to assume that the unstack event succeeds, and it
is consistent to assume that the fact that A is on B is inert, but the assumptions cannot
both be made because the success of the unstack event implies that A is no longer on B.
In such cases I suggest that change should always have priority over inertia, that success
assumptions should always have priority over inertia assumptions. This conflict resolution
principle can be defended by appealing to our regularity-based (Humeian) expectation of
change. Thus, in the case at hand, experience has taught us that unstack events of the
sort described normally succeed, and so we form a clear expectation that the effects will
follow. By contrast, giving priority to inertia would, contrary to expectation, result in
nothing changing, and adopting a neutral stance would, contrary to expectation, produce
an unclear outcome.
The intended interpretation of event theories is enforced by their formal pragmatics,
which defines the class of preferred models of any given event theory. In this section the
notion of preference1 is defined. This is later refined to preference2 in Section 5.
In order to enforce the convention that object-relations are physical unless stated otherwise, the preferred1 models of an event theory should all be models of the theory in which
the (positive) domain of the Phys relation is as large as it can be. Let us say that a model
M is a Phys-maximal model of an event theory Θ if M is a model of Θ and, for any model
M ′ of Θ, if {R : M |= Phys(R)} ⊆ {R : M ′ |= Phys(R)} then M = M ′ . Then the preferred1
models of Θ should all be Phys-maximal models of Θ.
13. If R(x)(t) is not defined, then, as will become clear, this atom can safely be ignored. For if Inert(R, hxi)(t)
can consistently be assumed, then the formal pragmatics (in particular, the minimization of evidential
atoms at t+1) ensures that R(x)(t+1) is undefined, thereby satisfying the axiom.

373

Bell

Beyond this requirement, we can get a clearer idea of what a preferred1 model of an
event theory should look like by considering an inductive version of the “canonical” example,
known as the “Yale Shooting Problem” (Hanks & McDermott, 1987). At time 1 a gun is
loaded and pointed at Fred. Nothing relevant happens at time 2. At time 3 the gun is fired.
If the gun is still loaded at time 3, then, in the absence of further information, we expect
that the shot will prove fatal and that Fred will no longer be alive at time 4. This example
can be represented formally by the theory Θ1 = ΘInd ∪ {(4), (5), (6)}, where:
∀t(Pre(Shoot )(t) ≡ (Alive(t) ∧ Loaded (t)))

(4)

∀t(Eff(Shoot)(t) ≡ ¬Alive(t))

(5)

Alive(1) ∧ Loaded (1) ∧ Occ(Shoot )(3)

(6)

Thus the preconditions for a Shoot event are that the gun is loaded and the victim is alive
(Axiom (4)), and its effect (if successful) is that the victim is not alive (Axiom (5)).
For model M and time point t, let M/t denote the set of all object or occurs literals
with temporal index t′ ≤ t which are true in M , and let M (t) = M/t \ M/t−1.14 So M/t
can be thought of as the history that is represented by M up to (and including) t. And,
at t, M (t) can be thought of as representing what is known at the present moment, as the
evidential context on which predictions about t+1 are based. We can also think of there
being a dynamic conjectural context at t, which consists of the set of success and inertia
assumptions which correspond to our expectations about t+1. Assumptions are added to
the conjectural context if they are consistent with the current context (the union of the
evidential context and the current conjectural context) and the background theory (the laws
of the given event theory).
The generation of a preferred1 model, M , of Θ1 should proceed as follows:
M/1 = {Alive(1), Loaded (1)} ,
M/2 = M/1 ∪ {Alive(2), Loaded (2)} ,
M/3 = M/2 ∪ {Alive(3), Loaded (3), Occ(Shoot )(3)} ,
M/4 = M/3 ∪ {¬Alive(4), Loaded (4)} ,
M/5 = M/4 ∪ {¬Alive(5), Loaded (5)} , . . . .
Thus the only atoms which should be in M/1, and so in the evidential context M (1), are
those required by the boundary conditions of Θ1 , which are stated by Axiom (6). This
restriction of the evidential context (“Ockham’s razor”) is appropriate because prediction
should be based on all and only the available evidence. Now, Alive is not declared to be a
theoretical relation in Θ1 , so by the notational convention, Phys(Alive) should be true in M .
And it is consistent in the current context M (1)∪∅ (given the background theory Θ1 \{(6)})
to assume Inert(Alive)(1), so this assumption should be added to the conjectural context.
Consequently Alive(2) should be in M (2) by the inertia axiom (Axiom (3)). Similarly
Phys(Loaded ) should be true in M , and it is consistent in the current context M (1) ∪
{Inert(Alive)(1)} to assume Inert(Loaded )(1), so this assumption should be added to the
conjectural context. By inertia, Loaded (2) should be in M (2). And, in accordance with
14. As usual, a literal is either an atom, α, or its negation, ¬α.

374

Natural Events

Ockham’s razor, no other atoms should be in M (2). By analogous reasoning beginning
with the current context M (2) ∪ ∅, Alive(3) and Loaded (3) should be in M (3), as should
the remaining boundary condition Occ(Shoot )(3). And, by Ockham’s razor, no other atoms
should be in M (3). Now, in the current context M (3) ∪ ∅ it is consistent to assume either
Succ(Shoot )(3) or Inert(Alive)(3). However both cannot be assumed; for if they were, then
it would follow by the axioms for change ((1) and (5)) and inertia that both Alive(4) and
¬Alive(4) would be in M (4). In keeping with the principle that change is preferred to inertia,
Succ(Shoot )(3) should be assumed and added to the conjectural context, and so ¬Alive(4)
should be in M (4). It is consistent in the current context M (3)∪{Succ(Shoot )(3)} to assume
Inert(Loaded )(4), so by inertia, Loaded (4) should be in M (4). And, by Ockham’s razor, no
further atoms should be in M (4). The remainder of M/∞ should then be generated by
repeated applications of the inertia axiom and Ockham’s razor.
The example suggests that event theories should be interpreted chronologically. This
fits naturally with our experience of “time’s arrow”; of the asymmetry between the past
(which is fixed) and the future (which is open, which is yet to exist). In particular, our
understanding of events in terms of dependable regularities of sequence is founded on this
asymmetry. The example also suggests that at each successive time point (at each new
present moment) we should first fix the evidential context and then generate the appropriate
conjectural context. The evolving context and the background theory then produce the
expected changes and persistences. Fixing the evidential context consists of minimizing it;
that is, restricting it to those object and event literals which are required by the boundary
conditions or the earlier interpretation of the theory. Generating the conjectural context
consists of maximizing success and inertia assumptions (that is, assuming them when they
are suggested by the evidential context and they are consistent with the current context
and the background theory) giving priority to the former in case of conflict. The definition
of a preferred1 model of an event theory should thus reflect the prioritized chronological
minimaximization involved in its intended interpretation.
We begin by defining the preference relation ≺1 . In this definition (and in the subsequent
definition of ≺2 ) ‘fewer’ and ‘more’ should be understood in terms of set inclusion rather
than cardinality.15 In keeping with the above discussion, let an evidential atom be either
an object atom or an event atom other than a success atom, and let a conjectural atom be
either a success atom or an inertia atom.
Definition 8 (Preference1 ) Let M and M ′ be models which differ at most on the interpretation of temporally-indexed relations. Then M is preferred1 to M ′ (written M ≺1 M ′ )16
iff there is a time point t such that M and M ′ agree before t, and at t:
1. fewer evidential atoms are defined in M , and M and M ′ agree on the truth values of
all evidential atoms which are defined in M ; or
2. M and M ′ differ only on conjectural atoms, and more success atoms are true in M ;
or
15. Thus if At(T, v, t, M ) = {α : α is an atom of type T with truth value v at time t in model M }, then
fewer atoms of type T have truth value v at time t in model M than do so in model M ′ if At(T, v, t, M ) ⊂
At(T, v, t, M ′ ). Similarly, but with ⊃ replacing ⊂, in the case for ‘more’.
16. This way of writing preferences is based on the comparison of evidential contexts.

375

Bell

3. M and M ′ differ only on inertia atoms, and more inertia atoms are true in M .
For example, suppose that M and M ′ are models which differ at most on temporally
indexed relations. (1) If M and M ′ agree before time 2 and disagree then only in that
Occ(Shoot )(2) is undefined in M and true in M ′ , then M is preferred1 to M ′ by clause 1
of the definition. (2) If M and M ′ agree before time 3 and disagree then only in that
Succ(Shoot )(3) is true in M and false in M ′ , then M is preferred1 to M ′ by clause 2 of the
definition. (3) If M and M ′ agree before time 3 and disagree then only in that Succ(Shoot )(3)
and ¬Inert(Alive)(3) are true in M whereas ¬Succ(Shoot )(3) and Inert(Alive)(3) are true
in M ′ , then M is preferred1 to M ′ by clause 2 of the definition. (4) If M and M ′ agree
before time 2 and disagree then only in that Inert(Alive)(2) is true in M and false in M ′ ,
then M is preferred1 to M ′ by clause 3 of the definition.
The preferred1 models of an event theory Θ should thus be obtained by focussing on the
class of all Phys-maximal models of Θ and then selecting the ≺1 -minimal models of Θ from
it. Accordingly, the definition of the preferred1 models of an event theory and the definition
of the predictions based on them are instances of the following generic definitions.
Definition 9 (Preferred Models, Prediction) A model M is said to be a preferredi
model of an event theory Θ if M is a Phys-maximal model of Θ and there is no other model
M ′ of Θ which is preferred i to M (which is such that M ′ ≺i M ). If Θ is an event theory and
φ is a sentence, then Θ predictsi φ (written Θ |≈i φ) iff all preferred i models of Θ are also
models of φ.
In keeping with the discussion in the introduction, the definition of prediction is cautious.
A clearer picture of this emerges if we consider the preferredi models of a given event theory
at a more abstract level.
Definition 10 (Equivalence, Determinism, Representative Preferred Model) Let
Θ be an event theory, and let M and M ′ be preferred i models of Θ. Then M and M ′ are
said to be preferencei equivalent (written M ∼i M ′ ) if M and M ′ agree on the interpretation
of all evidential and conjectural atoms.17 An event theory Θ is deterministici if it has a
single ∼i -equivalence class, and is non-deterministici otherwise. The representative member
of each ∼i -equivalence class is called a representative preferredi model of Θ.
Each ∼i -equivalence class of preferredi models of an event theory represents a possible
history which is defined by the theory. So deterministici theories define a single possible
history, and predictions can safely be based on it. However, non-deterministici theories
define more than one possible history, and so caution dictates that their predictions should
be restricted to those sentences which are true in all of the possible histories that they
define.18 The representative preferredi models of an event theory provide a concrete way of
thinking about possible histories.
We can now return to the (inductive version of the) Yale Shooting Problem.
17. Any two such models may differ on the interpretation of terms, or on the truth values of atoms not
considered in the definition of preferencei .
18. It is possible to define a risky notion of prediction based on a single ∼i -equivalence class c for event theory
Θ. Thus Θ |≈ ci φ iff φ is true in all models in c. This relation can be used to obtain information about a
particular possible history, but it does not serve as a basis for reliable prediction in non-deterministici
theories because it does not take other, equally possible, histories into account.

376

Natural Events

Example 1 As before, let Θ1 = ΘInd ∪ {(4), (5), (6)}. Then Θ1 is deterministic 1 . The
evidential literals which are true in its representative preferred 1 model agree with those in
the set M/∞ discussed earlier. Thus Θ1 predicts 1 that the Shoot event succeeds at time 3,
with the effect that Fred is not alive at time 4.
Proposition Θ1 |≈1 Succ(Shoot )(3) ∧ ¬Alive(4).
Proof By Definition 9, it is sufficient to prove that the conclusion follows in all preferred 1
models of Θ1 . So, let M be a preferred 1 model of Θ1 . Then, by Definition 9, Phys(Alive)
and Phys(Loaded ) are true in M .19 By Axiom (6), Alive(1) and Loaded (1) are both true
in M . By Definition 8.3, Inert(Alive)(1) is true in M .20 So it follows by Axiom (3) that
Alive(2) is true in M . By similar reasoning, Inert(Loaded )(1) and Loaded (2) are also
true in M (Axiom (3), Definition 8.3). As Alive(2) is true in M , it follows by inertia
(Axiom (3), Definition 8.3) that Alive(3) is true in M . Similarly, as Loaded (2) is true in
M , it follows by inertia that Loaded (3) is true in M . So, by Axiom (4), Pre(Shoot )(3) is
true in M . By Axiom (6), the occurs atom Occ(Shoot )(3) is true in M . By Definition 8.2,
Succ(Shoot )(3) is true in M . It follows, by Axiom (1), that Eff(Shoot )(4) is true in M , and
so, by Axiom (5), ¬Alive(4) is true in M .
The Yale Shooting Problem is of interest because, as Hanks and McDermott (1987)
show, it poses problems for theories which do not take account of time’s arrow. The example
suggests that reasoning about inertia should be chronological. A related example involving
reasoning about change was suggested by Lifschitz (1987, p. 37). The point of his example
can be illustrated by adding a second shot to the Yale Shooting Problem. Let Θ′1 =
Θ1 ∪ {Occ(Shoot )(4)}. Then we expect that, as before, the first shot will succeed and that
the second shot will fail (because Fred is no longer alive when the second shot occurs).
And, indeed, this is what transpires in all preferred1 models of Θ′1 . However, if Θ′1 were not
interpreted chronologically, then there would be preferred models of it in which the second
shot succeeds and the first shot fails; as the success of the second shot requires that Fred is
alive at time 4.
The inductive version of the Yale Shooting Problem considered here (in which the Shoot
event is treated inductively rather than deductively) also illustrates the need to give priority
to change (Succ assumptions) over inertia (Inert assumptions) in case of conflict. Without
19. As M is a preferred1 model of Θ1 it follows by Definition 9 that M is a Phys-maximal model of Θ1 . So,
if Phys(Alive) were not true in M , then there would be a model M ′ of Θ1 in which Phys(Alive) and all
of the Phys atoms which are true in M were true. But then M would not be a Phys-maximal model of
of Θ1 , contradicting the assumption that it is. An analogous argument justifies all subsequent appeals
to Definition 9 regarding the relation Phys.
20. If Inert(Alive)(1) were not true in M , then there would be a model M ′ of Θ1 in which Inert(Alive)(1)
was true and which was therefore preferred1 to M on this basis by clause 3 of Definition 8 at time 1.
(M and M ′ would disagree at most on the interpretation of temporally-indexed relations, M and M ′
would agree on the interpretation of all temporally-indexed relations at all time points before time 1, M
and M ′ would agree on the interpretation all evidential and success atoms at time 1, and in M ′ more
inertia atoms with temporal index 1 (all of those which are true in M together with Inert(Alive)(1))
would be true.) But then it would follow by Definition 9 that M would not be a preferred1 model of Θ1 ,
contradicting the assumption that it is. An analogous argument justifies all subsequent appeals to some
clause n of the definition of preferencei regarding the truth value of some atom with temporal index t.

377

Bell

this requirement there would be preferred models of Θ1 in which Inert(Alive)(3) is true and
Succ(Shoot )(3) is false, and so, contrary to expectation, Fred remains alive at time 4.
In subsequent examples it will often be assumed that different names (whether constants
or functional expressions) denote different individuals. In order to enforce this convention,
uniqueness of names axioms (Lifschitz, 1987, p. 50) are used. Let f1 , . . . , fn be functions
returning values of the same sort, and let x1 , . . . , y1 , . . . be variables of the appropriate sorts.
Then U [f1 , . . . , fn ] is the conjunction of the axioms in the set:
{∀x1 , . . . , xk , y1 , . . . , yl ¬fi (x1 , . . . , xk ) = fj (y1 , . . . , yl ) : 1 ≤ i < j ≤ n} ∪
{∀x1 , . . . , xk , y1 , . . . , yk (fi (x1 , . . . , xk ) = fi (y1 , . . . , yk ) → (x1 = y1 ∧ . . . ∧ xk = yk )) :
1 ≤ i ≤ n} .
These axioms express the fact that the functions f1 , . . . , fn are injections with different
ranges. This notation is extended to constants by treating them as 0-ary functions. Thus, for
example, given U [A, B, L1, L2] and U [Move], it follows that the constants A, B, etc., denote
different objects, and that the functional expressions Move(A, L1, L2) and Move(B, L1, L2)
denote different events.
The next example illustrates the need to restrict the inertia axiom to physical relations.
Example 2 Block B is moved from location L1 to location L2. We expect that L1 will be
clear as a result. This example can be represented as follows:
∀x, l, l′ , t(Pre(Move(x, l, l′ ))(t) ≡ At(x, l)(t))
′

′

′

(7)

∀x, l, l , t(Eff(Move(x, l, l ))(t) ≡ (At(x, l )(t) ∧ ¬At(x, l)(t)))

(8)

∀l, t(Clear(l)(t) ≡ ¬T∃xAt(x, l)(t))

(9)

Theo(Clear)

(10)

U [B, L1, L2] ∧ At(B, L1)(1) ∧ Occ(M ove(B, L1, L2))(1)

(11)

Axioms (7) and (8) define the preconditions and effects of move events. Axiom (9) defines a
location to be clear if it is not true that there exists a block which is at that location. The use
of the truth operator in this definition allows for the fact that the At relation may be partial;
a location is considered to be clear if none of the blocks whose locations are defined are at
the location. Axiom (10) declares Clear to be a theoretical relation. Finally, Axiom (11)
states the boundary conditions.
Let Θ2 = ΘInd ∪{(7), . . . , (11)}. Then Θ2 has a single representative preferred 1 model in
which Clear(L1)(2) is true; because, in the model, the move event succeeds, thereby vacating
L1, and no other object replaces B at L1. However, non-replacement at L1 depends on
the fact that Clear is a theoretical relation, and so is exempt from the law of inertia. If
Θ′2 = Θ2 \ {(10)}, then in every preferred 1 model of Θ′2 an object mysteriously replaces B
at L1.
Proposition Θ2 |≈1 Clear(L1)(2), but Θ′2 |≈1 ¬Clear(L1)(2).
Proof For the first part, let M be a preferred 1 model of Θ2 . Then, by axioms (7) and (11),
At(B, L1)(1), Pre(Move(B, L1, L2))(1), and Occ(Move(B, L1, L2))(1) are all true in M .
378

Natural Events

By Definition 8.2, Succ(Move(B, L1, L2))(1) is true in M . So it follows, by axioms (1) and
(8) that At(B, L2)(2) and ¬At(B, L1)(2) are both true in M . As At(B, L1)(1) is true in M ,
it follows by Axiom (9) that ¬Clear(L1)(1) is true in M . By Axiom (10) Theo(Clear) is
true in M and so, by Axiom (2), ¬Phys(Clear) is true in M . It follows by Axiom (3) that
¬Inert(Clear, hL1i)(1) is true in M (consequently ¬Clear(L1)(2) is no longer true in M by
inertia). By Definition 8.1 it follows that, for any x other than B, At(x, L1)(2) is undefined
in M . So, as ¬At(B, L1)(2) is true in M , it follows that for any x, ¬T∃xAt(x, L1)(2) is
true in M . And so it follows by Axiom (9) that Clear(L1)(2) is true in M .
For the second part, let M be a preferred 1 model of Θ′2 . Then, as before, the atoms
At(B, L1)(1), ¬Clear(L1)(1), Succ(Move(B, L1, L2))(1), At(B, L2)(2) and ¬At(B, L1)(2)
are all true in M . However (in the absence of Axiom (10)) it follows by Definition 9 that
Phys(Clear) is true in M . By Definition 8.3, Inert(Clear, hL1i)(1) is true in M . So it
follows by Axiom (3) that ¬Clear(L1)(2) is true in M .
The restriction of the inertia axiom can be justified in terms of the physical-theoretical
distinction as follows. The law of inertia is a law of physical inertia; its task is to represent
the persistence of those physical facts which are not changed by events. Applying it to
theoretical relations (as in Θ′2 ) results in mysterious consequences; which arise because
maintaining the inertia of a theoretical fact (¬Clear(L1)) introduces an additional real
change in a physical fact (a change in the At relation). Moreover, as theoretical facts
supervene on physical facts, changes (persistences) in theoretical facts supervene on changes
(persistences) in physical facts. So it is sufficient to represent changes (persistences) in
physical facts, and to let changes (persistences) in theoretical facts “take care of themselves”.
This is done in the case of Θ2 , where a change in the At relation results in a change in the
Clear relation. In more complex examples, several blocks may be moved to or from a
location simultaneously, and each move event may or may not succeed. In such cases, the
axioms for change and inertia represent changes and persistences in the At relation, and
changes (persistences) in the Clear relation take care of themselves once the new At facts
are fixed; once the real changes have occurred and the dust has settled.21
21. Another, more artificial, example involves the interaction between Goodman’s (1954, p. III.4) predicate
Grue and common sense inertia. Call an object “grue” if it is green at time t and t is before time 2 or it
is blue thereafter. Now suppose that object O is green at time 1 and that we don’t know of any events
which occur at time 1 which affect O. Then it seems natural to conclude by inertia that O is green at
time 2. However, as O is green at time 1, O is also grue at time 1, so it is equally reasonable to predict
that O will be grue (that is, blue) at time 2. The example can be represented formally by the theory
ΘG , which consists of ΘInd together with the following axioms:
∀x, t(Grue(x)(t) ≡ ((t < 2 ∧ Green (x)(t)) ∨ (t ≥ 2 ∧ Blue(x)(t)))) ,
∀x, t(Green(x)(t) ≡ ¬Blue(x)(t)) ,
Green(O)(1) .
Then there are intended preferred1 models of ΘG in which Green(O)(2) is true, and unintended preferred1
models of ΘG in which Blue(O)(2) is true. This problem can be solved by declaring that Grue is a
theoretical predicate (in keeping with Goodman’s doctrine of entrenchment and Quine’s advocation of
similarity), and so should not be projected by the law of inertia.

379

Bell

4. Primary and Secondary Events
The theory of inductive events provides the basis of an integrated solution to the qualification problem and the frame problem. On the intended interpretation of the success axiom,
events are, given their preconditions, normally sufficient for their effects. On the intended
interpretation of the inertia axiom, physical facts which are not affected by events persist.
However, whereas the effects of successful inductive events are certain and invariable, the
effects of natural events may be uncertain and they, or some of them at least, may vary
according to the context in which the events occur.
It may seem that context-dependent effects, or ramifications, can be represented by
domain axioms. However, the following example, which is based on Lifschitz’s (1990) lampcircuit example and Baker’s (1991) ice-cream example, shows that this approach is too
simplistic.
Example 3 Ollie is at location L1, he is holding block B, and he moves to location L2.
We expect that, as a result, Ollie will reach L2. Moreover, as Ollie is holding the block when
he moves, we expect that it will move with him to L2.
It may seem that this example can be represented by the event theory Θ3 = ΘInd ∪
{(12), . . . , (16)}; where:
∀x, l, l′ , t(Pre(Move(x, l, l′ ))(t) ≡ At(x, l)(t))

(12)

∀x, l, l′ , t(Eff(Move(x, l, l′ ))(t) ≡ At(x, l′ )(t))

(13)

′

′

′

∀x, l, l , t((At(x, l)(t) ∧ ¬l = l ) → ¬At(x, l )(t))

(14)

∀x, y, l, t((At(x, l)(t) ∧ Holding(x, y)(t)) → At(y, l)(t))

(15)

U [O, B, L1, L2]
∧ At(O, L1)(1) ∧ Holding (O, B)(1) ∧ Occ(M ove(O, L1, L2))(1)

(16)

Thus the effects of Move have been simplified. The fact that the moved object is no longer
where it was is now to be inferred from Axiom (14), which states that no object can be at two
different locations simultaneously, together with the appropriate inequality. The intention is
to use Axiom (15) to infer that Ollie’s movement results in the movement of block B. For,
given that Ollie is holding B when he gets to L2, it follows from the axiom that B is at L2
also; and so, by axioms (14) and (16), B is not at L1.
However, there are two representative preferred 1 models of Θ3 , which can be partially
described as follows:
M1 ⊇ {At(O, L2)(2), Holding (O, B)(2), At(B, L2)(2)} ,
M2 ⊇ {At(O, L2)(2), At(B, L1)(2)} .
As change is preferred to inertia, Ollie succeeds in moving to L2 in both models. In M1 ,
the fact that Ollie is holding B is inert, and so it follows that the block moves with him to
L2 as expected. In M2 , the fact that B is at L1 is inert, and so, contrary to expectation, B
remains at L1.
One reaction to this failure is to seek to strengthen axioms such as Axiom (15) by making
them “causally” directed, so that they can be interpreted “causally” (positively, as in M1 ),
380

Natural Events

rather than declaratively (positively as in M1 , or contrapositively as in M2 ). However it
seems to me that this response (which is discussed further in Section 7) is mistaken because
it misdiagnoses the problem; taking it to be a logical problem rather than a representational
one.
Let us consider the problem posed by the example afresh. The theory Θ3 has two
representative preferred models only one of which corresponds to our expectation. What
accounts for the asymmetry in our expectation, and how is the formal symmetry to be
broken?
The intended positive interpretation of Axiom (15) depends on appropriate reasoning
about inertia, and in particular on the appropriate use of the inertia axiom; it is necessary
to conclude that Ollie keeps hold of the block, rather than concluding that it remains at L1.
But it seems odd to be using the inertia axiom when reasoning about change; to be using
the inertia axiom (together with Axiom (15)) to get a block to move without there being
an event which causes it to move. In doing so we are violating the fundamental intuition
which underlies the fact-event distinction; which has it that events are the only causes of
physical change.
This consideration provides the key to the correct understanding of the problem. We
expect that the block will move when Ollie does because we are told that he is holding the
block when he moves and are not told that he releases it. Consequently we discount the
possibility of the block remaining at L1 because an additional event would be required in
order to account for this. However the movement of the block is itself an additional event
which has the additional effect that the block is at L2. The missing, symmetry-breaking,
causal element in this example is thus an event, and the choice between a move event and
a release event seems clear. But note that the block’s moving differs from Ollie’s moving.
The block moves only because Ollie moves and only because he is holding it when he does
so.
In order to reflect this difference, a distinction is drawn between primary and secondary
events. Primary and secondary events are inductive events of the kind that we have been
considering so far. However while primary events occur independently, secondary events
are invoked (in the non-mystical Computer Science sense in which one program (procedure,
process, . . . ) is said to invoke another) by other events, and are causally dependent on
them in the sense that a secondary event can succeed only if it is invoked invoked by a
successful event.22 Given this distinction, ramifications can be represented by invoking
appropriate secondary events in appropriate contexts. Thus, in Example 3, Ollie’s moving
can be represented as a primary event which, because he is holding block B when he moves,
invokes the secondary move-B event. The move-B event occurs because it is invoked, and
it succeeds only if the move-Ollie event does. Note that secondary events may, in turn,
invoke other events which are causally dependent on them. For instance, if in the current
example block B ′ is placed on top of block B, then the invoked move-B event should in turn
invoke a move-B ′ event. There can thus be tertiary events, and events of ever higher order.

22. This condition should perhaps be called “success dependence” or “effect dependence” in order to distinguish it from counterfactual dependence (Lewis, 1986, Ch. 21).

381

Bell

Table 3: The theory of invocation, ΘInv
∀e, e′ , t(Inv1 (e, e′ )(t) → (Occ(e)(t) ∧ Occ(e′ )(t)))
′

′

′

(17)
′

′

∀e, t((Succ(e)(t) ∧ ∃e Inv1 (e , e)(t)) → ∃e (Inv1 (e , e)(t) ∧ Succ(e )(t)))
′

′

′

′′

′′

′′

′

(18)

∀e, e , t(Inv(e, e )(t) ≡ (Inv1 (e, e )(t) ∨ ∃e (Inv1 (e, e )(t) ∧ Inv(e , e )(t))))

(19)

∀e, t ¬TInv(e, e)(t)

(20)

However, for the sake of convenience, all invoked events will be referred to as ‘secondary
events’.23
Invocations are represented in EL by invocation atoms. An invocation atom is an event
atom of the form Inv1 (e, e′ )(t), which states that event e directly invokes event e′ at time t.
Secondary events are typically invoked by invocation axioms of the form:
∀e, e′ , t((Occ(e)(t) ∧ φ) → Inv1 (e, e′ )(t)) ;
where φ is a formula which distinguishes those contexts in which e invokes e′ . The properties
of secondary events are stated by axioms (17)-(20) given in Table 3. Axiom (17) requires that
both the invoking and the invoked events occur. Axiom (18) represents (causal) dependence,
and states that a secondary event succeeds only if one of the events which invoked it
succeeds. The axiom is stated in this way in order to allow for cases in which a secondary
event is invoked by more than one event. Axioms (19) and (20) ensure that invocation is
acyclic. This is achieved by defining the auxiliary (indirect invocation) relation Inv to be
the transitive closure of the (direct invocation) relation Inv1 (Axiom (19)) and requiring
that Inv is irreflexive (Axiom (20)).24
Events which invoke others can be thought of in two ways: as elementary inductive
events, or as complex events which have a causal structure. The invocation graph for an
23. Primary and secondary events are so called by loose analogy with the philosophical distinction between
primary and secondary qualities. Primary qualities (such as size, shape and motion) are the fundamental
qualities used in science. By contrast, secondary qualities are sensory qualities (such as colour, taste,
smell, felt warmth or texture, and sound) which exist only in certain contexts (to individual observers
under specific conditions) and which are causally dependent on primary qualities. Similarly, tertiary
qualities are qualities which an object has in virtue of its secondary qualities; for example a flower
may be attractive to a butterfly because of its colour, or a wine may be expensive because of its taste
(Blackburne, 1994).
24. The axiomatization of secondary events is intended to be minimal. For example, there is no prohibition
on an event occurring as both a primary event and as a secondary event. If this were to happen, then it
would follow from Axiom (18) that the event would have secondary status. In certain circumstances it
may be desirable to define the order of an event, and to require that each event has exactly one order.
This can be done by adding axioms such as the following:
∀e, t(Ord(e, n)(t) ≡ ((n = 1 ∧ Occ(e)(t) ∧ ¬T∃e′ Inv1 (e′ , e)(t))
∨ (n > 1 ∧ ∃e′ (Ord(e′ , n−1)(t) ∧ Inv1 (e′ , e)(t))))) ,
∀e, n, m, t((Ord(e, n)(t) ∧ ¬n = m) → ¬TOrd(e, m)(t)) .

382

Natural Events

event e (at some time point t) is a directed acyclic graph whose initial vertex is e, whose
remaining vertices are the events invoked by e (either directly or indirectly), and whose edges
represent the direct invocation relation. Event e’s success graph (at t) is the subgraph of
e’s invocation graph which consists of all those chains in the invocation graph which begin
with e and which consist entirely of successful events. The direct effects of e are its defined
(invariant) effects. The indirect effects of e are the effects of all other events in its success
subgraph. So, in particular, successful inductive events can be thought of as having all of
the effects of the successful secondary events that they invoke (either directly or indirectly).
Thus, in the working example, the move-Ollie event has the direct effect that he moves,
and can be thought of as having the indirect context-dependent effects that B and B ′ move
with him. Note that the effects of an event may now be context-dependent in two ways. An
event may invoke different events in different contexts (recall that invocation axioms may
be context-dependent), and so its invocation graph may vary according to the context in
which it occurs. Moreover, the same invocation graph may result in different success graphs
if the context varies in other ways. For instance, in the working example, move-B ′ may be
invoked in two different contexts (in both of which B ′ is on B), and succeed in one (the
one we have been contemplating) but fail in another (say because B is also being held by
another agent).
Definition 11 The theory of invocation, ΘInv , consists of the axioms given in Table 3;
thus ΘInv = {(17), . . . , (20)}.
As invocation atoms are event atoms, the pragmatics given in the last section can be
used without change.
We can now give a formal version of an extension of the block-carrying example.
Example 4 Ollie is at location L1. He is holding block B1, block B2 is stacked on B1, and
block B3 is stacked on B2. Ollie moves to location L2. We expect that the move event will
succeed and that the stack of blocks will move with him. However if, for some independent
reason, B2 does not move, then we expect that B3 will also remain at L1.
The first part of this example can be represented by the event theory Θ4 = ΘInd ∪ ΘInv ∪
{(12), . . . , (15)} ∪ {(21), . . . , (24)}, where:
∀x, y, l, t((On(x, y)(t) ∧ At(y, l)(t)) → At(x, l)(t))
′

(21)

′

∀x, y, l, l , t((Occ(Move(x, l, l ))(t) ∧ Holding (x, y)(t)) →
Inv1 (Move(x, l, l′ ), Move(y, l, l′ ))(t))
′

(22)

′

∀x, y, l, l , t((Occ(Move(x, l, l ))(t) ∧ On(y, x)(t)) →
Inv1 (Move(x, l, l′ ), Move(y, l, l′ ))(t))

(23)

U [O, B1, B2, L1, L2] ∧ U [Move] ∧ At(O, L1)(1) ∧ Holding (O, B1)(1)
∧ On(B2, B1)(1) ∧ On(B3, B2)(1) ∧ Occ(M ove(O, L1, L2))(1)

(24)

Axiom (21) states that if object x is on object y then x is at the same location as y is, while
axioms (22) and (23) are invocation axioms representing ramifications. Axiom (22) states
that a move-x event invokes a move-y event in contexts in which x is holding y. Similarly,
383

Bell

Axiom (23) states that a move-x event invokes a move-y event in contexts in which y is on
x.
There is a single representative preferred 1 model of Θ4 , in which Ollie’s movement
successfully invokes the movement of B1 (because Ollie is holding B1 when he moves), this
in turn successfully invokes the movement of B2 (because B2 is on B1 when B1 moves),
and this in turn successfully invokes the movement of B3 (because B3 is on B2 when B2
moves). Thus, in accordance with expectation, Θ4 predicts that Ollie succeeds in moving to
L2 with the entire stack of blocks. Note that the success of each invoked event depends on
the success of the event which invoked it. Thus, for example, if Θ4 is extended such that B2
remains at L1, then the extended theory predicts that B3 also remains at L1.
Proposition Θ4 |≈1 At(B3, L2)(2), and Θ4 ∪ {At(B2, L1)(2)} |≈ 1 At(B3, L1)(2).
Proof For the first part, let M be a preferred 1 model of Θ4 . Then Occ(Move(O, L1, L2))(1)
and Holding(O, B1)(1) are true in M (Axiom (24)). So it follows (axioms (17), (22))
that Inv1 (Move(O, L1, L2), Move (B1, L1, L2))(1) and Occ(Move(B1, L1, L2))(1) are true
in M . So, as On(B2, B1)(1) is true in M (Axiom (24)), it follows (axioms (17), (23)),
that Inv1 (Move(B1, L1, L2), Move (B2, L1, L2))(1) and Occ(Move(B2, L1, L2))(1) are true
in M . And so, as On(B3, B2)(1) is true in M (Axiom (24)), it follows (axioms (17),
(23)) that Inv1 (Move(B2, L1, L2), Move (B3, L1, L2))(1) and Occ(Move(B3, L1, L2)(1) are
true in M . No further invocation atoms with temporal index 1 are defined in M (Definition 8.1), so the four events Move(O, L1, L2), Move(B1, L1, L2), Move(B2, L1, L2),
Move(B3, L1, L2), which occur at time 1 in M , are linked by a chain of invocations at
time 1 in M .25 Moreover, the preconditions of each of the four events are true in M (axioms (12), (15), (21), (24)). As Move(O, L1, L2) is the only primary event at time 1 in
M , it follows (Definition 8.2) that Succ(Move(O, L1, L2))(1) is true in M . Moreover, as
Move(B1, L1, L2) is directly invoked by a successful event at time 1 in M , it follows (Definition 8.2) that Succ(Move(B1, L1, L2))(1) is true in M . Similar reasoning shows that success
is propagated down the rest of the invocation chain; that, in turn, Succ(Move(B2, L1, L2))(1)
and hence Succ(Move(B3, L1, L2))(1) are true in M . So, as Succ(Move(B3, L1, L2))(1) is
true in M , it follows (axioms (1), (13)) that At(B3, L2)(2) is true in M .
For the second part, let M be a preferred 1 model of Θ′4 = Θ4 ∪ {At(B2, L1)(2)}.
Then, as above, the events Move(O, L1, L2), Move(B1, L1, L2), Move(B2, L1, L2), and
Move(B3, L1, L2) all occur at time 1 in M , their preconditions are all true at time 1 in
M , and they are all linked by a unique invocation chain at time 1 in M . As above, the
first two events in the chain succeed. However, as M is a model of Θ′4 , At(B2, L1)(2) is
true in M . So it follows (axioms (14), (24)) that ¬At(B2, L2)(2) is true in M . It therefore follows (axioms (1) and (13)) that ¬Succ(Move(B2, L1, L2))(1) is true in M . So, as
Move(B2, L1, L2) is the only event which directly invokes Move(B3, L1, L2) at time 1 in
M , it follows that ¬T∃e(Inv1 (e, Move(B3, L1, L2))(1) ∧ Succ(e)(1)) is true in M . And
so ¬T(Succ(Move(B3, L1, L2))(1) ∧ ∃e Inv1 (e, Move(B3, L1, L2))(1)) is true in M (Axiom (18)). As Inv1 (Move(B2, L1, L2), Move (B3, L1, L2))(1) is true in M and the Succ
25. The unique-names axiom for Move in Axiom (24) ensures that the events are distinct. In view of
axioms (19) and (20) this appeal to the unique-names axiom is not strictly necessary. However, in
this and in subsequent examples involving multiple events it is simpler to add unique-names axioms for
events, and then assume in the proofs that distinct event terms denote distinct events.

384

Natural Events

relation is classical (Axiom (1)), it follows that ¬Succ(Move(B3, L1, L2))(1) is true in M .
However, Phys(At) and At(B3, L1)(1) are true in M (axioms (15), (21), (24), Definition 9), so it follows by inertia (Axiom (3), Definition 8.3) that At(B3, L1)(2) is true in
M.
Note that with the introduction of secondary events as the missing causal elements in
this example, the inertia axiom and the domain axioms ((14), (15) and (21)) are confined to
their proper tasks; namely, representing inertia, and defining or constraining the At relation
respectively.
In philosophical terms, invocations provide a means for representing contemporaneous
sufficient causation between events. Thus if event e directly invokes event e′ at time t,
then e is a sufficient cause of e′ at t. The direct invocation relation, Inv1 , thus represents
causal directedness (causal priority) between contemporaneous events. Axioms (17) and
(18) respectively ensure that contemporaneous causation occurs between actual events, and
that an invoked event is efficacious only if it is invoked by an efficacious event. This account
of contemporaneous sufficient causation can be thought of in terms of regularities, however
the reasoning is now more complex; for example, we might discover that event e directly
invokes event e′ by noting that whenever e occurs and condition φ is true e′ also occurs,
and that whenever this is the case e′ succeeds only if e does.
Contemporaneous (sufficient) causation between events is naturally required to be asymmetric (axioms (19) and (20)). However, it has been suggested that there are also cases of
symmetric contemporaneous causation between events. Taylor (1975) gives the example of
a locomotive and a caboose which are coupled together in such a way that the locomotive
moves iff the caboose does. An analogous example, suggested by Denecker, Dupré, and
Belleghem (1998), involves a pair of gears which are interlocked, so that each gear rotates
iff the other does.
As will become clear, it is better to view these as cases involving symmetric constraints
on contemporaneous events, rather than symmetric causation between them. Now, clearly,
symmetric constraints cannot be represented as invocations.26 However, they can be represented on an individual basis by adding particular axioms. For example, the symmetric
constraint on the rotation of the gears can be represented by the following axiom:
∀g, g′ , t(Intl (g, g′ )(t) → ((Occ(Rot (g))(t) ≡ Occ(Rot (g′ ))(t))
∧ (Succ(Rot (g))(t) ≡ Succ(Rot (g′ ))(t)))) .
The first conjunct of the consequent of this axiom is required in order to ensure that the
rotations of pairs of interlocked gears co-occur; without it, it would be possible for one
of the rotate events to occur and fail without the other occurring. Note also that, when
introducing the problem, Denecker et al. (1998, p. 34) require that the representation of the
behaviour of the interlocked gears should not be such that they can rotate spontaneously.
This can present a problem for theories based on classical logic; because of the semantics of
26. An attempt to do so in the case of the gears would be to use the following axiom:
∀g, g ′ , t((Occ(Rot (g))(t) ∧ Intl (g, g ′ )(t)) → Inv1 (Rot (g), Rot (g ′ ))(t)) .
But clearly if Intl(g, g ′ )(t), Intl(g ′ , g)(t) and Occ(e)(t) hold, then a contradiction results by the above
axiom and axioms (17), (19) and (20).

385

Bell

Table 4: The theory of symmetrically constrained events, ΘSCon
∀e, t ¬TSCon(e, e)(t)
′

′

′

′

(25)
′

∀e, e , t(SCon(e, e )(t) → SCon(e , e)(t))

(26)
′

′

∀e, e , t(SCon(e, e )(t) → ((Occ(e)(t) ≡ Occ(e )(t)) ∧ (Succ(e)(t) ≡ Succ(e )(t)))) (27)
∀e, e′ , t(SCSet(e, e′ )(t) ≡
(SCon(e, e′ )(t) ∨ ∃e′′ (SCon(e, e′′ )(t) ∧ SCSet(e′′ , e′ )(t))))
′

(28)

′

∀e, e , t(EInv(e, e )(t) ≡
(Inv1 (e, e′ )(t) ∧ ∃e′′ (SCSet(e′ , e′′ )(t) ∧ ¬TSCSet(e, e′′ )(t)))) (29)
∀e, e′ , n, t(IPath(e, e′ , n)(t) ≡
((n = 0 ∧ e = e′ ∧ ∃e′′ EInv(e′′ , e)(t))
∨ (n > 0 ∧ ∃e′′ (IPath(e, e′′ , n−1)(t) ∧ SCon(e′′ , e′ )(t))))) (30)
∀e, e′ , t((SCon(e, e′ )(t)
∧ ∃e′′ , n(IPath(e′′ , e, n)(t)
∧ ∀e′′′ , m(IPath(e′′′ , e′ , m)(t) → m > n)) → Inv1 (e, e′ )(t))) (31)

classical biconditionals. However this potential pitfall is effortlessly avoided in event theories
because of the accurate representation of partiality; for if only Intl (g, g′ )(t) is given, then
Occ(Rot (g))(t) and Occ(Rot (g′ ))(t) are both undefined.
A more general treatment of symmetric constraints on events can be given by introducing
the symmetric constraint relation SCon. This relation represents the co-occurrence of and
co-dependence between pairs of events. Accordingly we expect SCon to be irreflexive,
symmetric, and to hold between pairs of co-occurring events which are also co-dependent.
These properties are stated by axioms (25)-(27) in Table 4.
When considered in isolation, the fact that two events are symmetrically constrained
provides no compelling evidence that either is the cause of the other. For example, in the
gears case, if all we know is that g and g′ are interlocked and that g is rotating, then it is
reasonable to conclude that g′ is also rotating, but it is not reasonable to conclude that the
rotation of g is the cause of the rotation of g′ or vice versa. Indeed, the two events may not
even be causally connected; each of the gears might be rotating because the shaft that it is
attached to is being driven.
However, if we have additional, external, information about the direction of causation,
then we can use it to infer the direction of causation between pairs of symmetrically constrained events.27 For example, if we know that gear g is being driven (say by rotating
the shaft that it is attached to), and we don’t know that gear g′ is being driven, then it
is reasonable to conclude that the driving of g is causally prior to the rotation of g, and
that the rotation of g is in turn causally prior to the rotation of g′ . More formally, if we
have just Inv1 (Drive(g), Rot (g))(t) and SCon(Rot (g), Rot (g′ ))(t), then it seems reasonable
27. Taylor makes a similar appeal to an external cause in his discussion of the locomotive-caboose example.

386

Natural Events

to conclude that Inv1 (Rot (g), Rot (g′ ))(t). The external invocation chain is thus extended
across the symmetric constraint link. The construction of invocation chains of this kind is
defined by axioms (28)-(31) given in Table 4.
Axiom (28) states that, at time t, events e and e′ are in the same symmetrically constrained set iff they are symmetrically constrained or they both occur in a chain of symmetric
constrained events.
Axiom (29) defines the conditions for an external invocation of a symmetrically constrained event. Thus, at time t, event e externally invokes event e′ iff e invokes e′ and e′ is
in a symmetric constraint set which does not include e.
An invocation path in a symmetrically constrained set begins with an externally invoked
event and consists of a chain of events each of which symmetrically constrains its neighbour.
The length of an invocation path is determined by the number of links that it contains; so
that an invocation path consisting only of an externally invoked event has length 0, one
consisting of such an event and its neighbour has length 1, etc. Accordingly, Axiom (30)
states the conditions under which there is an invocation path of length n between events e
and e′ .
Finally, Axiom (31) defines invocation paths in symmetrically constrained sets. It does
so by requiring that an invocation link exist between symmetrically constrained events e
and e′ whenever the shortest invocation path to e in their symmetrically constrained set is
shorter than the shortest invocation path to e′ in that set.
Definition 12 The theory of symmetric constraints, ΘSCon , consists of the axioms given
in Table 4; thus ΘSCon = {(25), . . . , (31)}.
These ideas are illustrated by the following elaboration of the gears example.
Example 5 Five interlocking gears, G1 , . . . , G5 , are arranged in a row. If only G1 is
driven, then this invokes the rotation of G1 , and, for each hGi , Gi+1 i pair, the rotation of
Gi invokes the rotation of Gi+1 . However if both G1 and G5 are driven, then the rotation
of each invokes the rotation of its neighbour, and, in turn, each of these rotations invokes
the rotation of G3 .
The first part of this example can be represented by the event theory Θ5 = ΘInd ∪ ΘInv ∪
ΘSCon ∪ {(32), . . . , (39)}, where:
∀g, t(Pre(Drive(g))(t) ≡ Free(g)(t))

(32)

∀g, t(Eff(Drive(g))(t) ≡ Rotd (g)(t))

(33)

∀g, t(Pre(Rot (g))(t) ≡ Free(g)(t))

(34)

∀g, t(Eff(Rot (g))(t) ≡ Rotd (g)(t))

(35)

′

′

′

∀g, g , t(Intl (g, g )(t) ≡ Intl (g , g)(t))

(36)

∀g, t(Occ(Drive(g))(t) → Inv1 (Drive(g), Rot (g))(t))
′

′

′

∀g, g , t(Intl (g, g )(t) → SCon(Rot (g), Rot (g ))(t))

(37)
(38)

U [G1 , . . . , G5 ] ∧ U [Drive, Rot ]
∧

5
^

i=1

Free(Gi )(1) ∧

4
^

Intl (Gi , Gi+1 )(1) ∧ Occ(Drive(G1 ))(1)

i=1

387

(39)

Bell

For the sake of simplicity the preconditions and effects of drive events are the same as
those of rotate events and all objects are assumed to be gears. Thus a gear can be driven
(can rotate) if it is free to do so, and the effect of its being driven (rotating) is that it has
rotated (in a direction and by a degree which are, again for simplicity, not represented).
Axiom (36) states that the Intl relation (which represents pairs of interlocked gears) is
symmetric. Axiom (37) states that a drive-g event invokes a rotate-g event, and Axiom (38)
states that if gears g and g′ are interlocked, then their rotation is symmetrically constrained.
The theory Θ5 has a single representative preferred 1 model in which all five gears rotate and in which there is a single invocation chain hDrive(G1 ), Rot (G1 ), Rot (G2 ), Rot (G3 ),
Rot (G4 ), Rot (G5 )i.
Moreover, the extended theory Θ5 ∪ {Occ(Drive(G5 ))(1)}, has a single preferred 1 model
in which all five gears rotate, and in which there are two invocation chains; hDrive(G1 ),
Rot (G1 ), Rot (G2 ), Rot (G3 )i and hDrive(G5 ), Rot (G5 ), Rot (G4 ), Rot (G3 )i.
Proposition Θ5 |≈1 Succ(Rot (Gi ))(1) for 1 ≤ i ≤ 5, and Θ5 |≈1 Inv(Rot (G1 ), Rot (G5 ))(1).
Moreover, if Θ′5 = Θ5 ∪{Occ(Drive(G5 ))(1)}, then Θ′5 |≈1 Succ(Rot (Gi ))(1) where 1 ≤ i ≤ 5,
but now both Θ′5 |≈1 Inv(Rot (G1 ), Rot (G3 ))(1) and Θ′5 |≈1 Inv(Rot (G5 ), Rot (G3 ))(1).
Proof Let M be a preferred 1 model of Θ5 . Then it follows, by axioms (17), (37) and
(39), that Occ(Drive(G1 ))(1), Inv1 (Drive(G1 ), Rot (G1 ))(1), and Occ(Rot (G1 ))(1) are true
in M . By Axiom (39), Intl (G1 , G2 )(1) is true in M , so it follows by Axiom (38) that
SCon(Rot (G1 ), Rot (G2 ))(1) is true in M . And so it follows, by axioms (26) and (27), that
SCon(Rot (G2 ), Rot (G1 ))(1) and Occ(Rot (G2 ))(1) are true in M . Similar reasoning shows
that for each i such that 1 ≤ i ≤ 4, both SCon(Rot (Gi ), Rot (Gi+1 ))(1) and SCon(Rot (Gi+1 ),
Rot (Gi ))(1) are true in M , and that, for each i such that 1 ≤ i ≤ 5, Occ(Gi )(1) is true in M .
By axioms (32), (34) and (39), Pre(Drive(G1 ))(1) is true in M , as is each Pre(Rot (Gi ))(1)
where 1 ≤ i ≤ 5. So by Definition 8.2, Succ(Drive(G1 ))(1) is true in M , as is each
Succ(Rot (Gi ))(1) where 1 ≤ i ≤ 5.
By Axiom (28), the set SI = {Rot (Gi ) : 1 ≤ i ≤ 5} is a symmetric constraint set at
time 1 in M , and by Definition 8.1 it is the only such set. So it follows by Axiom (29)
that EInv(Drive(G1 ), Rot (G1 ))(1) is true in M . Moreover it follows, by Definition 8.1
and Axiom (29), that Rot (G1 ) is the only initially invoked event in SI. So it follows by
Axiom (30) that hRot (G1 ), Rot (G1 )i is an influence path of length 0 in SI, and that the
shortest influence path to G2 in SI is hRot (G1 ), Rot (G2 )i which is of length 1. So it follows
by Axiom (31) that Inv1 (Rot (G1 ), Rot (G2 ))(1) is true in M . By analogous reasoning, the
shortest influence path to G3 in SI is the path hRot (G1 ), Rot (G2 ), Rot (G3 )i which is of
length 2, and so Inv1 (Rot (G2 ), Rot (G3 ))(1) is true in M . Similar reasoning shows that
Inv1 (Rot (G3 ), Rot (G4 ))(1) and Inv1 (Rot (G4 ), Rot (G5 ))(1) are true in M . So it follows by
Axiom (19) that Inv(Rot (G1 ), Rot (G5 ))(1) is true in M .
Now let M be a preferred 1 model of Θ′5 . By reasoning analogous to that given above,
Succ(Drive(G1 ))(1) is true in M , as is each Succ(Rot (Gi ))(1) where 1 ≤ i ≤ 5. We also
have that SI = {Rot (Gi ) : 1 ≤ i ≤ 5} is the only symmetric constraint set at time 1
in M . However this time Rot (G1 ) and Rot (G5 ) are both initially invoked events in SI.
The shortest influence path to G2 in SI is the path hRot (G1 ), Rot (G2 )i of length 1, and
so, as before, Inv1 (Rot (G1 ), Rot (G2 ))(1) is true in M . Analogous reasoning shows that
388

Natural Events

hRot (G5 ), Rot (G5 )i is an influence path in SI of length 0 and that the shortest influence path
to G4 in SI is the path hRot (G5 ), Rot (G4 )i of length 1, and so Inv1 (Rot (G5 ), Rot (G4 ))(1)
is true in M . Moreover, the shortest influence paths to Rot (G3 ) in SI are the paths
hRot (G1 ), Rot (G2 ), Rot (G3 )i and hRot (G5 ), Rot (G4 ), Rot (G3 )i, each of length 2. So it follows by Axiom (31) that Inv1 (Rot (G2 ), Rot (G3 ))(1) and Inv1 (Rot (G4 ), Rot (G3 ))(1) are true
in M . Thus by Axiom (19), Inv(Rot (G1 ), Rot (G3 ))(1) and Inv(Rot (G5 ), Rot (G3 ))(1) are
both true in M .
Secondary events can also be used to represent non-deterministic effects.
Non-deterministic effects may arise because of uncertainty in the preconditions. Suppose, for example, that the initial positions of blocks B and B ′ are uncertain, either B is
on B ′ or conversely, and that B is moved. Intuitively the resulting location of B ′ should be
uncertain. If B ′ was on B, then B ′ should have moved to the same location as B, otherwise
B ′ should have remained where it was. It is easy to see how Example 4 can be adapted
to represent this example faithfully. If B ′ is on B initially, then the move-B event invokes
a secondary move-B ′ event (Axiom (23)) which results in B ′ moving with B, otherwise
the location of B ′ remains unchanged. The resulting theory thus has two representative
preferred1 models; one in which B ′ moves, and one in which B ′ does not move.
non-deterministic effects may also arise because the outcome of the events in question
is uncertain. If successful, a non-deterministic event is not regularly followed by a definite
effect, but rather by any one of a set of mutually exclusive possible effects; as in the classic
example of tossing a fair coin. In the terms used in the introduction, our expectations
regarding the outcome of such events are unclear. In order to see why secondary events are
needed to represent these, consider the following attempt to represent coin-tossing.
Example 6 Suppose that a fair coin is showing heads initially and that the coin is tossed.
As the coin is fair, the result should be uncertain; the coin may show heads or it may show
tails. Let Θ6 = ΘInd ∪ {(40), (41), (42)} where:
∀t(Pre(Toss)(t) ≡ (Heads(t) ⊕ Tails(t)))

(40)

∀t(Eff(Toss)(t) ≡ (Heads(t) ⊕ Tails(t)))

(41)

Heads(1) ∧ ¬Tails(1) ∧ Occ(Toss)(1)

(42)

Then, contrary to intention, Θ6 is deterministic 1 . Θ6 has a single representative preferred 1
model in which the Toss event succeeds and its effect Heads(2) ⊕ Tails(2) is true. However,
as Heads(1) is true in the model, the normal application of the inertia axiom removes the
uncertainty by determining that Heads(2) is true.
If, as in this example, one of the alternative effects of an event preserves the status
quo, then inertia will favour that outcome and so determine the outcome of the event. The
fact that inertia intervenes in this way in the example suggests that something is missing
from the representation of the toss event. As defined, the event does not do what it is
intended to do. In succeeding it does not ensure that there are two distinct outcomes.
Metaphorically speaking, it does not introduce a fork at this point in history, resulting in
two alternative futures. There seems to be a hidden causal element which accounts for our
intuitive understanding of the example but which is missing from the formalization of it.
389

Bell

I suggest that the missing component is the causal structure of the toss event, and that
this can be faithfully represented by two conflicting secondary events; that tossing the coin
invokes two conflicting deterministic events, one resulting in the coin showing heads, the
other resulting in the coin showing tails.
When formalizing this and subsequent examples involving nondeterministic events, the
following abbreviation is useful:
Inv1 (e, {e1 , . . . , en })(t) =Df

Inv1 (e, e1 )(t) ∧ . . . ∧ Inv1 (e, en )(t) .

Example 7 Let Θ7 = ΘInd ∪ ΘInv ∪ {(40), . . . , (46)} where:
∀t(Pre(TossH )(t) ≡ Pre(Toss)(t)) ∧ ∀t(Eff(TossH )(t) ≡ Heads(t))

(43)

∀t(Pre(TossT )(t) ≡ Pre(Toss)(t)) ∧ ∀t(Eff(TossT )(t) ≡ Tails(t))

(44)

∀t(Occ(Toss)(t) → Inv1 (Toss, {TossH , TossT })(t))

(45)

U [Toss, TossH , TossT ]

(46)

Then there are two representative preferred 1 models of Θ7 . In both models, the primary
event Toss succeeds and invokes the conflicting secondary events TossH and TossT . In one
of the models, TossH succeeds and TossT fails. In the other model, TossT succeeds and
TossH fails.
Proposition Θ7 |≈1 Heads(2) ⊕ Tails(2), Θ7 |6≈1 Heads(2), and Θ7 |6≈1 Tails(2).
Proof For the first part, let M be a preferred 1 model of Θ7 . Then the Toss event occurs
at time 1 in M (Axiom (42)) and its occurrence invokes the secondary TossH and TossT
events (Axiom (45)), which also occur at time 1 in M (Axiom (17)). The preconditions of
all three events are true at time 1 in M (axioms (40), (42)-(44)). However, in view of their
effects (axioms (41), (43), (44)), all three events cannot succeed at time 1 in M . As Toss
invokes the other two events, and is the only event which does so (Definition 8.1), TossH
or TossT can only succeed if Toss does (Axiom (18)). Moreover, it is consistent to assume
that Toss does succeed at time 1 in M , so it follows (Definition 8.2) that Toss does succeed
at time 1 in M , with the effect that Heads(2) ⊕ Tails(2) is true in M (axioms (1), (41)).
For the second part it is sufficient to show that there is a preferred 1 model of Θ7 in which
Heads(2) is false. So, let M be an EL model in which ∀R Phys(R) is true, which satisfies
Axiom (46), and which satisfies the following conditions. The only evidential atoms which
are defined in M are those which satisfy the following set of literals:
{Heads(1), ¬Tails (1), Occ(Toss)(1), Inv1 (Toss, TossH )(1), Occ(TossH )(1),
Inv1 (Toss, TossT )(1), Occ(TossT )(1)} ∪ {Tails(t) : t ≥ 2} ∪ {¬Heads(t) : t ≥ 2} ;
thus, for example, Heads(1) is true in M and Tails(1) is false in M . The success atoms in
the set:
{Succ(Toss)(1), Succ(TossT )(1)}
are both true in M , and every other success atom is false in M . Finally the inertia atoms
in the set:
{Inert(Heads)(0), Inert(Tails)(0), Inert(Heads)(1), Inert(Tails)(1)}
390

Natural Events

are all false in M , and every other inertia atom is true in M . Clearly M exists. Moreover,
inspection shows that M is a preferred 1 model of Θ7 . In particular, the success of Toss at
time 1 in M is required (on the grounds given in the proof of the first part) as is the success
of either TossH or TossT at time 1 in M (axioms (1), (41), (43), (44), Definition 8.2).
In the given model, TossT succeeds at time 1, and the combined effect of the two successful
events is that Heads(2) is false in M (axioms (1), (41), (44)).
The proof of the third part is similar; but with TossH (rather than TossT ) succeeding
at time 1 in the given model.
A similar problem is posed by the Russian Shooting Problem (Sandewall, 1994). A
revolver is loaded with a single bullet and the cylinder is spun. The result should be
uncertain, as the cylinder could come to rest with the bullet in any one of six possible
positions. However in the naive representation, inertia will, once again, favour the outcome
in which the bullet returns to, and so effectively remains in, its original position. A faithful
representation of the expected outcome of the spin event can be obtained by having it invoke
six conflicting secondary spin events, with the result that the bullet is equally likely to come
to rest in any of the six possible positions.
More generally, a non-deterministic event can be seen as invoking a set of conflicting
deterministic events, each resulting in one of its possible outcomes.
When there are more than two possible outcomes, the following general form of exclusive
alternation is useful:
φ1 | φ2 | . . . | φn =Df

n
_

m−1
^

(

m=1 i=1

¬φi ∧ φm ∧

n
^

¬φj ) .

j=m+1

Thus φ1 | φ2 | . . . | φn is true if exactly one of the alternatives is true and the remainder are
all false, is false if all of the alternatives are false or if more than one of them is true, and
is undefined otherwise.28
The techniques for representing ramifications and non-deterministic events can readily
be combined to represent events with conditional effects, such as crossing the points on
a railway line. Suppose that point P has entry point PE , left exit point PL, and right
exit point PR. Suppose further that if P is set to ‘left’, then a train crossing P should
emerge at PL, otherwise if P is set to ‘right’, then the train should emerge at PR. Then
the preconditions for crossing P can simply be that the train is at PE and that P is set to
either ‘left’ or ‘right’, and the effect can be that the train is at either PL or PR. If P is set
to ‘left’ when the train crosses, then this event should invoke a cross-P -left event, which
has the precondition that the train is at PE and that P is set to ‘left’, and the effect that
the train is at PL. Similarly, if P is set to ‘right’ when the train crosses, then this event
should invoke a cross-P -right event with the effect that the train emerges at PR.

5. Event Preferences
Conflicts are the reductio ad absurdum of Deductionism. If two events conflict, then their
effects cannot, on pain of inconsistency, be deduced. Consequently, Deductionists wishing
28. So, if n = 2, then exclusive alternation is just exclusive disjunction. But if n > 2, then the two notions
differ; for example, the exclusive disjunction φ1 ⊕ (φ2 ⊕ φ3 ) is true if φ1 ∧ φ2 ∧ φ3 is.

391

Bell

to represent simultaneous events are forced to police their events and to regulate their
effects. Gelfond, Lifschitz, and Rabinov (1991) suggest that this can be done by means of
cancellations. Suppose, for example, that we have a bowl full of soup. If only one side
of the bowl is lifted, then the soup is spilt. However, if both sides of the bowl are lifted
simultaneously, then the soup is not spilt. These interactions are represented by means
of two elementary lift events (lift-left-side, lift-right-side) and their composition (lift-bothsides). If either of the elementary events occurs in isolation, then it has the effect that the
soup is spilt. But if the complex event occurs, then the spill-effects of its component actions
should be cancelled, with the result that the bowl is lifted and the soup is not spilt.
However cancellations are only appropriate if we ignore the possibility of failure. Suppose
that in reality one of the elementary lift events fails because a hand slips. Then the effects
of the other component event should not be cancelled and the soup should be spilt. But
how can a cancellation be cancelled?
Conflicts do not pose the same problem for inductive events. When faced with conflicting events we do not expect that they will both succeed. Some, and possibly all, of our
expectations regarding these events are uncertain. This is reflected in the formal theory:
conflicting effects give rise to conflicting success atoms, resulting in failure rather than inconsistency. However the possibility of failure raises the problem of “over-weak” predictions
(which amusingly complements the “over-strong” problem of Deductionism). If two inductive events conflict (and there are no other interactions involving them), then each succeeds
in a preferred model in which the other fails, consequently nothing more definite than the
disjunction of their effects is predicted. This is appropriate when the events are of equal
status; indeed, it provides the basis for the representation of non-deterministic events given
in the previous section. But it is not appropriate when we expect that one of the events will
succeed. Suppose, for example, that Stan and Ollie attempt to move to the same location
simultaneously but that only one of them can succeed. Suppose further that Ollie’s success
is more likely than Stan’s, say because he is bigger. Then we normally expect that Ollie will
succeed and that Stan will fail. However, if there are abnormal independent circumstances
which lead us to expect that Ollie will fail, then these do not lead us to expect that Stan will
fail also. Indeed, under the circumstances, we expect that Stan will succeed. For example,
if Ollie trips, then we expect that Stan will succeed; although, of course, he may also trip,
etc.
Asymmetric expectations of this kind can be thought of as event preferences, as preferences over the outcomes of events. Thus if events e and e′ conflict, and we expect that
e will succeed, then the success of e is preferred to that of e′ . However, as the examples
show, preferences of this kind should be defeasible in order not to prejudice the success of
the non-preferred event should the preferred event fail for some independent reason.
Event preferences can be represented in EL by preference atoms, event atoms of the
form Pref(e, e′ )(t). In keeping with the above discussion, these should be interpreted as
stating that the success of event e is normally preferred to that of event e′ should they
conflict at time t. The temporal index accommodates the possibility that event preferences
may vary over time; as in Example 10 below.
The only logical restriction on event preferences is that they are required to be asymmetric (Axiom (47) in Table 2). Further conditions, such as transitivity, can, of course, be
added where appropriate.
392

Natural Events

Table 5: The theory of event preferences, ΘPref
∀e, e′ , t(Pref(e, e′ )(t) → ¬TPref(e′ , e)(t))

(47)

Definition 13 The theory of event preferences is given by the axioms in Table 5; thus
ΘPref = {(47)}. The theory of natural events, ΘNE , consists of the axioms given in tables 2,
3, 4, and 5; thus ΘNE = ΘInd ∪ ΘInv ∪ ΘSCon ∪ ΘPref .
The intended interpretation of event preferences cannot be enforced by adding the axiom:
∀e, e′ , t((Pref(e, e′ )(t) ∧ Pre(e)(t) ∧ Occ(e)(t) ∧ Succ(e′ )(t)) → Succ(e)(t)) .
Adding this axiom would ensure that if e and e′ were to conflict, then e would succeed (and
so e′ would fail). However, if e were to fail for some independent reason, then the axiom
would have the undesirable effect of forcing the failure of e′ .
Clearly, if event preferences are to be interpreted correctly, then a more flexible approach
is needed, and so it is necessary to extend the pragmatics of event theories which contain
them. In doing so the aim is to produce a consistent interpretation of the applicable event
preferences where possible, and to ignore them otherwise.
To begin with, a distinction is drawn between preferential events, the events to which
the preferences can consistently be applied, and non-preferential events, all of the remaining
events under consideration. An event e is said to be supported in model M at time t if e
occurs at t and e’s preconditions are true in M at t (if Pre(e)(t) and Occ(e)(t) are both true
in M ). If e is supported at some time point in a model, and the time point and model are
clear from the context, then e will simply be said to be supported. Now, if the applicable
event preferences form an acyclic chain Pref(e, e′ )(t), Pref(e′ , e′′ )(t), . . . , then the supported
events occurring in them can be ordered lexicographically; thus given that e, e′ and e′′ are
all supported, e has order 1, e′ order 2, and e′′ order 3. More generally, for model M and
time point t, a supported event e may (or may not) be assigned a preference rank as follows:
• e has preference rank 1 if there is some event e′ such that Pref(e, e′ )(t) is true and
there is no supported event e′ such that Pref(e′ , e)(t) is true, and
• e has preference rank n if e does not have a preference rank m < n and there is a
supported event e′ with preference rank n − 1 which is directly preferred to e (which
is such that Pref(e′ , e)(t) is true and there is no supported event e′′ such that the
preferences Pref(e, e′′ )(t) and Pref(e′′ , e′ )(t) are both true).
Let e be a supported event (at time t in model M ). Then e is a preferential event (at t in
M ) if e has a preference rank (at t in M ), otherwise e is a non-preferential event (at time
t in model M ).
Now suppose that at time t models M and M ′ agree on event preferences, preferential
events, and non-preferential events. Then, at t:
393

Bell

• M is better than M ′ on preferential events if there is some preference rank n such
that M and M ′ agree on the success of all events with preference rank m < n, and
more events with preference rank n succeed in M ,
• M is as good as M ′ on preferential events if M is better than M ′ on preferential
events, or M and M ′ agree on the success of preferential events,
• M is better than M ′ on non-preferential events if more non-preferential events succeed
in M ,
• M is as good as M ′ on non-preferential events if M is better than M ′ on nonpreferential events, or M and M ′ agree on the success of non-preferential events.
The definition of a preferred model can now be refined.
Definition 14 (Preference2 ) Let M and M ′ be models which differ at most on the interpretation of temporally-indexed relations. Then M is preferred2 to M ′ (written M ≺2 M ′ )
iff there is a time point t such that M and M ′ agree before t, and at t:
1. fewer evidential atoms are defined in M , and M and M ′ agree on the truth values of
all evidential atoms which are defined in M ; or
2. M and M ′ differ only on conjectural atoms, and either
(a) M is better than M ′ on preferential events, and M is as good as M ′ on nonpreferential events; or
(b) M is as good as M ′ on preferential events, and M is better than M ′ on nonpreferential events; or
3. M and M ′ differ only on inertia atoms, and more inertia atoms are true in M .
Note that preference2 reduces to preference1 when event theories do not include event
preferences.
Four examples are now given. The first illustrates the interpretation of event preferences.
Example 8 Suppose that three agents, Stan, Ollie, and Charlie, are at locations L1, L2 and
L3 respectively. It is assumed that at most one of the agents can be at a location at a point
in time, so if any two of them attempt to move to the same location simultaneously, then at
most one of them can succeed. If Ollie and Stan both attempt to move to location L4 then,
as Ollie is bigger, Ollie’s success is expected. Similarly, if Stan and Charlie both attempt
to move to L4 simultaneously, then, as Stan is bigger, Stan’s success is expected. However,
Charlie is much faster than Ollie, so if they both attempt to move to L4 simultaneously,
then Charlie’s success is expected. Now, suppose that in fact Stan and Ollie both attempt
to move to L4 simultaneously, then, as indicated, Ollie’s success is expected. However, if
Charlie also attempts to move to L4, then the result is uncertain because the preferences
among the move events can no longer be interpreted consistently.
394

Natural Events

Let Θ8 = ΘInd ∪ ΘPref ∪ {(12), (13), (14)} ∪ {(48), . . . , (52)}; where:
∀x, y, l, t((At(x, l)(t) ∧ ¬x = y) → ¬At(y, l)(t))

(48)

Pref(Move(O, L1, L4), Move (S, L2, L4))(1)

(49)

Pref(Move(S, L2, L4), Move (C, L3, L4))(1)

(50)

Pref(Move(C, L3, L4), Move (O, L1, L4))(1)

(51)

U [O, S, C, L1, L2, L3, L4] ∧ U [Move]
∧ At(O, L1)(1) ∧ At(S, L2)(1) ∧ At(C, L3)(1)
∧ Occ(Move(O, L1, L4))(1) ∧ Occ(Move(S, L2, L4))(1)

(52)

Then Θ8 has a single representative preferred 2 model in which Ollie succeeds in moving to
L4. However the extended theory Θ′8 = Θ8 ∪ {Occ(Move(C, L3, L4))(1)} has three representative preferred 2 models, and a different member of the trio succeeds in each of them.
Proposition Θ8 |≈2 At(O, L4)(2). However Θ′8 |6≈2 At(O, L4)(2), Θ′8 |6≈2 At(S, L4)(2), and
Θ′8 |6≈2 At(C, L4)(2).
Proof For the first part, let M be a preferred 2 model of Θ8 . Then (Definition 14.1) the only
events which occur at time 1 in M are Move(O, L1, L4) and Move(S, L2, L4). If follows
from axioms (12) and (52) that both of these events are both supported at time 1 in M . In
view of their effects, only one of these events can succeed (axioms (1), (13), (48), (52)).
By axioms (49)-(51) and Definition 14.1, the only preference atoms which are true in M
are Pref(Move(O, L1, L4), Move (S, L2, L4))(1), Pref(Move(S, L2, L4), Move (C, L3, L4))(1),
and Pref(Move(C, L3, L4), Move (O, L1, L4))(1). In view of the first of these, there is an
event e such that Pref(Move(O, L1, L4), e)(1) is true in M . And, as Move(C, L3, L4) is not
supported at time 1 in M , there is no supported event e such that Pref(e, Move(O, L1, L4))(1)
is true in M . So Move(O, L1, L4) has preference rank 1 at time 1 in M . Moreover,
Move(S, L2, L4) does not have preference rank 1 at time 1 in M , because Move(O, L1, L4)
is a supported event at time 1 in M and Pref(Move(O, L1, L4), Move (S, L2, L4))(1) is true
in M . So, as Move(O, L1, L4) is directly preferred to Move(S, L2, L4) at time 1 in M ,
it follows Move(S, L2, L4) has preference rank 2 at time 1 in M . It therefore follows by
Definition 14.2(a) that Succ(Move(O, L1, L4))(1) is true in M . Consequently the effect
At(O, L4)(2) is true in M (axioms (1), (13)).
For the second part, let M be an EL model in which ∀R Phys(R) is true, and which
satisfies U [O, S, C, L1, L2, L3, L4] and U [Move]. Suppose further that M satisfies the following conditions; where Loc = {O, S, C, L1, L2, L3, L4}. The only object or event atoms
which are defined in M are those which satisfy the following sets of literals:
{At(O, L1)(t) : t ≥ 1}, {¬At(O, l)(t) : t ≥ 1, l ∈ Loc, l 6= L1},
{At(S, L2)(1)}, {At(S, L4)(t) : t ≥ 2)},
{¬At(S, l)(1) : l ∈ Loc, l 6= L2}, {¬At(S, l)(t) : t ≥ 2, l ∈ Loc, l 6= L4},
{At(C, L3)(t) : t ≥ 1} ∪ {¬At(C, l)(t) : t ≥ 1, l ∈ Loc, l 6= L3},
{Occ(Move(O, L1, L4))(1), Occ(Move(S, L2, L4))(1), Occ(Move(C, L3, L4))(1)},
395

Bell

{Pref(Move(O, L1, L4), Move (S, L2, L4))(1),
Pref(Move(S, L2, L4), Move (C, L3, L4))(1),
Pref(Move(C, L3, L4), Move (O, L1, L4))(1)}.
The success atom Succ(Move(S, L2, L4))(1) is true in M and all other success atoms are
false in M . Finally, the inertia atoms in the following sets are all false in M :
{Inert(At, hO, li)(0) : l ∈ Loc}, {Inert(At, hC, li)(0) : l ∈ Loc},
{Inert(At, hS, li)(0) : l ∈ Loc}, {Inert(At, hS, li)(1) : l ∈ {L2, L4}},
and all other inertia atoms are true in M . Clearly M exists. Moreover, inspection shows
that M is a preferred 2 model of Θ′8 . In particular, the success of Move(S, L2, L4) at time 1
can be justified as follows. The only three events which are supported at time 1 in M are
Move(O, L1, L4), Move(S, L2, L4), and Move(C, L3, L4) (axioms (12), (52), definition of
Θ′8 , Definition 14.1). And the only preference atoms which are true in M are those in the
set given above (axioms (49)-(51), Definition 14.1). So none of the three supported events
has preference rank 1 at time 1 in M ; Move(O, L1, L4) does not because Move(C, L3, L4)
is supported and is preferred to it, Move(S, L2, L4) does not because Move(O, L1, L4) is
supported and is preferred to it, and Move(C, L3, L4) does not because Move(S, L2, L4) is
supported and is preferred to it. It follows that all three events are non-preferential at time 1
in M . At most one of these events can succeed at time 1 in M (axioms (1), (13), (48), (52)).
And it follows by Definition 14.2(b) that one of them does succeed. In M , Move(S, L2, L4)
succeeds at time 1, with the effect that At(S, L4)(2) and consequently ¬At(O, L4)(2) are both
true in M ((1), (13), (48), (52)).
The given model also establishes the fourth part of the proposition. For the third part
a preferred 2 model of Θ′8 can be given in which either Move(O, L1, L4) or Move(C, L3, L4)
succeeds at time 1.
The next example shows how event preferences and secondary events can be combined
in order to represent implicit cancellation and the implicit cancellation thereof.
Example 9 The soup-bowl example can be represented as follows:
∀t(Pre(LiftL)(t) ≡ ¬HoldingL(t)) ∧ ∀t(Eff(LiftL)(t) ≡ HoldingL(t))

(53)

∀t(Pre(LiftR)(t) ≡ ¬HoldingR(t)) ∧ ∀t(Eff(LiftR)(t) ≡ HoldingR(t))

(54)

∀t(Pre(Spill )(t) ≡ ¬Spilt (t))

(55)

∀t(Eff(Spill )(t) ≡ (Spilt (t) ∧ (HoldingL(t) ⊕ HoldingR(t))))

(56)

∀e, t((Occ(e)(t) ∧ (e = LiftL ∨ e = LiftR) ∧ ¬Spilt(t)) → Inv1 (e, Spill )(t))

(57)

∀t(Pre(Lift)(t) ≡ (Pre(LiftL)(t) ∧ Pre(LiftR)(t)))

(58)

∀t(Eff(Lift)(t) ≡ (Eff(LiftL)(t) ∧ Eff(LiftR)(t)))

(59)

∀t(Occ(Lift)(t) ≡ (Occ(LiftL)(t) ∧ Occ(LiftR)(t)))

(60)

∀tPref(Lift, Spill )(t)

(61)

U [Lift, LiftL, LiftR, Spill ]
∧ ¬HoldingL(1) ∧ ¬HoldingR(1) ∧ ¬Spilt(1) ∧ Occ(Lift)(1)
396

(62)

Natural Events

Thus axioms (53)-(56) define elementary lift-left (LiftL), lift-right (LiftR) and spill events.
Axiom (57) states that if the soup is not spilt, then the occurrence of either elementary lift
event invokes a spill event. Axioms (58)-(60) define the complex lift-both event (Lift), and
Axiom (61) states that the success of a lift-both event is normally preferred to that of a spill
event.
Let Θ9 = ΘInd ∪ ΘInv ∪ ΘPref ∪ {(53), . . . , (62)}. Then Θ9 has a single representative
preferred 2 model in which the Lift event succeeds and the Spill event fails; with the result
that the soup is not spilt. Thus the success of the Lift event implicitly cancels the (secondary)
Spilt effect of its component LiftL and LiftR events. Moreover, the extended theory Θ9 ∪
{¬Succ(Lift)(1)} has two representative preferred 2 models. In one LiftL succeeds (and LiftR
fails), in the other LiftR succeeds (and LiftL fails). So in both Spill succeeds with the effect
that the soup is spilt. The implicit cancellation of the Spilt effect is thus itself implicitly
cancelled.
Proposition Θ9 |≈2 ¬Spilt(2), and Θ9 ∪ {¬Succ(Lift)(1)} |≈2 Spilt (2).
Proof For the first part, let M be a preferred 2 model of Θ9 . Then the events Lift, LiftL,
and LiftR are all supported at time 1 in M (axioms (53), (54), (58), (60), (62)). Moreover
the LiftL and LiftR events both invoke the Spill event at time 1 (axioms (57), (62)), and this
is also supported at time 1 in M (axioms (17), (55), (62)). Now the Lift and Spill events
conflict at time 1 in M ; if Lift succeeds, then Spill fails, and vice-versa (axioms (1), (53),
(54), (56), (59)). And the preference atom Pref(Lift, Spill )(1) is the only preference atom
with temporal index 1 which is true in M (Axiom (61), Definition 14.1). Consequently, at
time 1 in M , Lift and Spill have preference ranks 1 and 2 respectively, and LiftL and LiftR
are non-preferential. The success of Lift implies the success of its component events (axioms (1), (53), (54), (59)). So (Definition 14.2(a)) Lift succeeds (and Spill fails) at time 1
in M . And so, as Phys(Spilt ) and ¬Spilt (1) are true in M (Axiom (62), Definition 9), it
follows by inertia (Axiom (3), Definition 14.3) that ¬Spilt(2) is true in M .
For the second part, let M be a preferred 2 model of Θ9 ∪ {¬Succ(Lift)(1)}. Then, as
above, the events Lift, LiftL, LiftR, and Spill are all supported at time 1 in M . As Lift
fails (when supported) at time 1 in M , one of its component lift-events must also fail at
time 1 in M (axioms (1), (53), (54), (59)). However the other component event succeeds
at time 1 in M (Definition 14.2(b)), and consequently so does Spill (axioms (1), (53) (54),
(56), Definition 14.2(a)), with the effect that Spilt(2) is true in M (axioms (1), (56)).
The next example shows how event preferences and secondary events can be used to
represent changing expectations regarding the outcome of non-deterministic events.
Example 10 A race between a fast horse and a strong horse will be run the day after
tomorrow. The course is currently dry. Given that this remains the case, the fast horse
is expected to win. However if it were to rain tomorrow, then the strong horse would be
expected to win.
This example can be represented by the theory Θ10 = ΘInd ∪ ΘInv ∪ ΘPref ∪ {(63), . . . ,
(71)}; where:
∀t(Pre(RaceFS )(t) ≡ Occ(RaceFS )(t))
397

(63)

Bell

∀t(Eff(RaceFS )(t) ≡ (WinnerF (t) ⊕ WinnerS (t)))

(64)

∀t(Pre(WinF )(t) ≡ Pre(RaceFS )(t)) ∧ ∀t(Eff(WinF )(t) ≡ WinnerF (t))

(65)

∀t(Pre(WinS )(t) ≡ Pre(RaceFS )(t)) ∧ ∀t(Eff(WinS )(t) ≡ WinnerS (t))

(66)

∀t(Occ(RaceFS )(t) → Inv1 (RaceFS , {WinF , WinS })(t))

(67)

∀t(Pre(Rain)(t) ≡ Occ(Rain)(t)) ∧ ∀t(Eff(Rain)(t) ≡ ¬Dry(t)))

(68)

∀t(Dry(t) → Pref(WinF , WinS )(t))

(69)

∀t(¬Dry(t) → Pref(WinS , WinF )(t))

(70)

U [RaceFS , WinF , WinS , Rain] ∧ Dry(1) ∧ Occ(RaceFS )(3)

(71)

Thus Axiom (64) states that the outcome of a successful race between the two horses
(RaceFS ) results in either the fast horse winning (WinnerF ) or the strong horse winning
(WinnerS ). The causal structure of the RaceFS event (the fact that it involves the competition between two horses) is represented by its invocation of the conflicting WinF and WinS
events (axioms (64)-(67)). Rain results in the course being wet (not dry) (Axiom (68)). If
the course is dry, then the fast horse is expected to win (Axiom (69)), otherwise the slow
horse is expected to win (Axiom (70)).
Proposition Θ10 |≈2 WinnerF (4), and Θ10 ∪ {Occ(Rain)(2)} |≈2 WinnerS (4).
Proof For the first part, let M be a preferred 2 model of Θ10 . Then Phys(Dry) and Dry(1)
are true in M (Axiom (71), Definition 9), and so it follows by inertia (Axiom (3), Definition 14.3) that Dry(3) is true in M . Moreover, the event RaceFS occurs at time 3 in
M and invokes the WinF and WinS events (axioms (67), (71)). All three events are supported at time 3 in M (axioms (17), (63), (65), (66), (71)). As RaceFS invokes WinF
and WinS , they can succeed only if it does (Axiom (18), Definition 14.1). So it follows
(Definition 14.2(b)) that RaceFS succeeds at time 3 in M . The success of exactly one of the
two invoked events at time 3 is consistent with the success of RaceFS (axioms (64)-(66)).
As Dry(3) is true in M it follows (Axiom (69)) that Pref(WinF , WinS )(3) is true in M .
Moreover this is the only preference atom with temporal index 3 which is true in M (Definition 14.1). So, as WinF and WinS are both supported at time 3 in M , it follows that
WinF has preference rank 1 and WinS has preference rank 2 at time 3 in M . Consequently
(Definition 14.2(a)) WinF succeeds in M at time 3, with the effect that WinnerF (4) is true
in M (axioms (1) and (65)).
For the second part, let M be a preferred 2 model of Θ10 ∪ {Occ(Rain)(2)}. Then the
Rain event succeeds at time 2 in M and so its effect ¬Dry(3) is true in M (Axiom (68),
Definition 14.2(b)). As in the proof of the first part, RaceFS and one of the two conflicting
secondary events, WinS and WinF , succeed at time 3 in M . As ¬Dry(3) is true in M , it
follows (Axiom (70)) that Pref(WinS , WinF )(3) is true in M . Moreover, this is the only
preference atom with temporal index 3 which is true in M (Definition 14.1). So, as WinS
and WinF are both supported at time 3 in M , it follows that WinS has preference rank 1
and WinF has preference rank 2 at time 3 in M . So it follows (Definition 14.2(a)) that
WinS succeeds at time 3 in M , with the effect that WinnerS (4) is true in M (axioms (1)
and (66)).
398

Natural Events

More generally, event preferences can be combined with secondary events in order to
give a qualitative representation of conditional probabilities. A probability judgment of
the form P (Succ(e)(t)|Succ(e′ )(t)) = n states that the probability of event e succeeding at
time t, given that event e′ does, is n. Judgments of this kind can be represented in EL by
event atoms of the form Prob(e, e′ , n)(t). Conditional probabilities can then be translated
into event preferences by means of the following axiom:
∀e, e′ , e′′ , n, m, t((Prob(e, e′ , n)(t) ∧ Prob(e′′ , e′ , m)(t) ∧ n > m) → Pref(e, e′′ )(t))

(72)

together with the appropriate invocations. The use of this technique is illustrated by the
final example, which is a probabilistic extension of an example attributed to Reiter (Shanahan, 1997, p. 290).
Example 11 A chessman is placed haphazardly on a chessboard. It may end up on (within)
a single square, but it is more likely that it will overlap four squares, and more likely still
that it will overlap just two squares. This situation can be represented by the theory Θ11 =
ΘInd ∪ ΘInv ∪ ΘPref ∪ {(72), (73), . . . , (80)}, where:
∀t(Pre(Place)(t) ≡ Occ(Place)(t))

(73)

∀t(Eff(Place)(t) ≡ (One(t) | Two(t) | Four (t)))

(74)

∀t(Pre(Place1 )(t) ≡ Pre(Place)(t)) ∧ ∀t(Eff(Place1 )(t) ≡ One(t))

(75)

∀t(Pre(Place2 )(t) ≡ Pre(Place)(t)) ∧ ∀t(Eff(Place2 )(t) ≡ Two(t))

(76)

∀t(Pre(Place4 )(t) ≡ Pre(Place)(t)) ∧ ∀t(Eff(Place4 )(t) ≡ Four (t))

(77)

∀t(Occ(Place)(t) → Inv1 (Place, {Place1 , Place2 , Place4 })(t))

(78)

∀t(Prob(Place1 , Place, 0.2)(t) ∧ Prob(Place2 , Place, 0.5)(t)
∧ Prob(Place4 , Place, 0.3)(t))
U [Place, Place1 , Place2 , Place4 ] ∧ Occ(Place)(1)

(79)
(80)

Thus Axiom (74) states the three mutually-exclusive outcomes of placing the chessman,
Axiom (78) states that an occurrence of the Place event invokes the three conflicting secondary events, Place1 , Place2 , and Place4 , Axiom (79) associates a conditional probability
judgment with each of them, and Axiom (72) translates these judgments into qualitative
event preferences.
Proposition Θ11 |≈2 Two(2).
Proof Let M be a preferred 2 model of Θ11 . Then the Place event occurs and invokes
the three secondary events Place1 , Place2 , and Place4 at time 1 in M (axioms (78),
(80)). All four events are supported at time 1 in M (axioms (17), (73),(75)-(77),(80)).
But, in view of their effects they cannot all succeed at time 1 in M (axioms (1), (74),
(75)-(77)). Now the preference atoms Pref(Place2 , Place4 )(1), Pref(Place4 , Place1 )(1), and
Pref(Place2 , Place1 )(1) are all true in M (axioms (72), (79)) and these are the only preference atoms with temporal index 1 which are defined in M (Definition 14.1). Consequently,
at time 1 in M , the three events Place2 , Place4 , and Place1 have preference ranks 1, 2
and 3 respectively, and Place is non-preferential. However, as Place invokes the other three
399

Bell

events and is the only event which does so ((78), Definition 14.1), they can succeed only if
it does (Axiom (18)). So it follows that Place succeeds at time 1 in M (Definition 14.2(b)).
The success of Place2 at time 1 is consistent with the success of Place (axioms (1), (74),
(76)). Consequently, given its preference rank, Place2 succeeds at time 1 in M (Definition 14.2(a)), with the effect that Two(2) is true in M (axioms (1), (76)).

6. Philosophical Justification
In this section the justification of formal theories of prediction is discussed and a justification
of the theory of events is given.
Goodman (1954, §III.2) discusses the related problem of the justification of formal theories of enumerative induction, and suggests that we start by considering how we justify
a deductive inference. Clearly we can do so by showing that it conforms to a set of valid
general rules of deduction.29 But then the question arises as to how we justify the rules
themselves. To suggest that we do so by appealing to some more fundamental “underlying”
rules simply postpones the question and so invites a regress.30 But if we cannot give a
foundational justification of deduction, then how can we proceed? Goodman suggests that
we can do so by showing that the rules for deduction conform with particular deductive
inferences which we actually make and sanction. This is circular, but virtuously so: “The
point is that rules and particular inferences alike are justified by being brought into agreement with one another. A rule is amended if it yields an inference we are unwilling to
accept; an inference is rejected if it violates a rule we are unwilling to amend. The process of justification is the delicate one of making mutual adjustments between rules and
accepted inferences; and in the agreement achieved lies the only justification needed for either” (p. 64). Note however that there is no general consensus as to what counts as a valid
deductive argument. Classical logicians accept the inference from ¬¬φ to φ as valid, Intuitionists do not. Similarly, Intuitionists accept the inference from φ to ψ ⊃ φ as valid, but
Relevantists do not. These differences can be explained by the fact that those concerned are
attempting to formalize different notions of validity; as there tends to be agreement among
those who agree on a given intuitive notion of deduction.31 This suggests that a justification
of a given form of deductive inference will be partly philosophical and partly empirical, as
it will consist of an analysis of the concept of deductive validity and the consideration of
examples of deductive inference.
Similarly then, in order to justify a formal theory of prediction, we should seek to
show, by means of a combination of conceptual analysis and empirical evidence, that its
predictions agree with those which we actually make and consider to be reasonable. In the
case of a logico-pragmatic theory this amounts to arguing that its pragmatics is appropriate.
29. Or, equally, that it conforms to a notion of entailment defined in terms of a given formal semantics, such
as the Tarski semantics for classical predicate logic.
30. Given a formal semantics and accompanying notion of entailment, we can justify a set of rules by proving
that they are both sound and complete relative to the semantics. But then again the question arises as
to how the semantics and notion of entailment are justified.
31. For example, a valid argument is one in which, if you accept the premises, then you must (on pain of
inconsistency) accept the conclusion; a valid argument is one which admits of a constructive proof of the
conclusion given the premises; a valid argument is one in which the premises are all required in order to
establish the conclusion.

400

Natural Events

Let Θ be a logico-pragmatic theory of prediction. Then an extension of Θ is any theory
Θ′ which includes Θ and which satisfies certain stated restrictions. For instance, we might
require that Θ′ is a semantic extension of Θ, meaning that the additional axioms in Θ′ are
intended to be interpreted semantically (rather than pragmatically); thus, for example, Θ5
is a semantic extension of ΘInv . We can tentatively define the intended models of a given
extension Θ′ of Θ to be those models of Θ′ which accord with our expectations given Θ′ .32
We can then say that a pragmatics is pragmatically sound for a class of extensions of Θ
if, for every extension Θ′ in the class, the pragmatics selects all of the intended models
of Θ′ , and that the pragmatics is pragmatically complete for a class of extensions of Θ if,
for every extension Θ′ in the class, the pragmatics selects only the intended models of Θ′ .
Thus pragmatic soundness ensures that the theory produces only those predictions which
we would consider to be reasonable, and pragmatic completeness ensures that the theory
produces all such predictions.
In the case of event theories it seems to be appropriate to concentrate on the pragmatic soundness and completeness of preference1 for semantic extensions of ΘInd ; for, given
that preference1 does have these properties, the pragmatic soundness and completeness of
preference2 for extensions of ΘInd which include event preferences but are otherwise semantic seems to be uncontentious.
In order to argue for the pragmatic soundness of preference1 we have to show that the
restrictions that it imposes are all necessary. Now the intuitive notion of prediction that
event theories endeavour to formalize is that of a context-dependent activity. Prediction
takes place at a point in time, “the present”, the past is considered to be fixed and the future
is considered to be open. Moreover prediction should be based on all and only the available
evidence. It then consists of making regularity-based speculations about change given the
context, and then (quietly) assuming that facts which are not affected by the changes
will persist. These properties are captured by the restrictions imposed by preference1 .
Location in time and the idea of a closed past and open future are captured by preferring
models of an event theory in which it is interpreted chronologically. The restriction of
the evidential context and subsequent preference for change over inertia is captured by
prioritorized minimaximization at the present time point; first restricting the evidential
context to that required by the earlier interpretation of the theory, then assuming whenever
possible that events succeed, and finally assuming wherever possible that facts persist.
Finally, the physical-theoretical distinction is necessary when a theory contains both kinds
of relations, as inertia is a property of the physical world and so should be restricted to
physical relations. The need for doing so is illustrated by Example 2.
The need to maximize inertia assumptions chronologically when attempting to represent
inductive reasoning about inertia was already clear to Hanks and McDermott (1987) in their
discussion of the Yale Shooting Problem. Moreover, the inductive extension of this example
(in which the events are inductive rather than deductive), given as Example 1, illustrates the
need to prefer change to inertia at any given time point. However, it might be objected that
it is not clear that earlier events should succeed in favour of later ones; that is, that success
32. The question of what counts as an intended model of a theory can be a vexed one; see, for example, the
discussions by Sandewall (1994, pp. 68-69) and Collins, Hall, and Paul (2004, pp. 32-39). However, in
the following discussion, it is sufficient to focus on cases where there is general agreement on expected
outcomes.

401

Bell

assumptions should be made chronologically. But if we consider the extension of Example 1
in which a second, later, shot occurs (as in the discussion of Θ′1 following the example),
then it seems clear that we expect that the first shot will prove fatal, with the consequence
that the second shot fails because Fred is already dead when it occurs; although, of course,
if the first shot were to fail for some independent reason, then we would expect the second
shot to succeed. When predicting that the first shot will succeed we do not consider that
its success may be jeopardized by a later shot. To allow later events to influence earlier
events in this way would be to allow a mysterious form of backwards causation.
A long-standing objection to the chronological assumption of inertia is that this does
not fit with the generation of explanations. The standard example of this is Kautz’s (1986)
Stolen Car Problem; where a car is parked, left unattended, and discovered to be stolen at
some later point in time. It has since become part of the folklore that the chronological
assumption of inertia is pragmatically unsound because it results in the conclusion that
the car was stolen just before this was discovered, when, intuitively, it seems reasonable to
conclude that the car could have been stolen at any earlier point at which it was unattended.
In consequence, proponents of the chronological maximization of inertia have been prompted
to qualify its application; for example, Sandewall (1994) suggests that certain occluded
relations should be exempt from the law of inertia, at least for certain intervals of time.33
However it seems to me that there is a better response, namely to argue that examples
involving explanation are irrelevant when considering the pragmatic soundness of a theory
of prediction because prediction and explanation are different forms of reasoning (Bell,
1998).34
Another long-standing objection to the chronological assumption of inertia involves nondeterminism. Thus it is claimed, as illustrated by Example 6, that the chronological minimization of inertia can determine the outcome of non-deterministic events, thereby eliminating intended models. However, rather than seek to weaken the preference criterion, I suggest
that we should take care to represent the causal structure of non-deterministic events (the
fact that they introduce branching histories) correctly; as illustrated by Example 7.
On the basis of these considerations it seems reasonable to conjecture that preference1
is pragmatically sound.
33. This idea is different from the physical-theoretical distinction. To say that a fact is occluded at a point
in time is, indeed, to say that it is not subject to the law of inertia at that point in time. However, the
occluded fact could naturally be classified as a physical one, such as those represented by the At relation
in Example 2.
34. Prediction is a form of inductive reasoning; given an epistemic context, the task is to produce reasonable conclusions on the basis of it. By contrast, explanation is a form of abductive reasoning; given a
conclusion and an epistemic context which does not imply it, the task is to generate appropriate explanations of the conclusion. Doing so when reasoning about events and their effects involves extending the
epistemic context in appropriate ways, so that the conclusion can be induced (predicted) from each of
the extensions. In order to ensure that an explanation is appropriate it is reasonable to require that an
extension should be such that some event therein causes the conclusion. Consequently I suggest (Bell,
2001, ‘Causal counterfactuals’) that explanations are best dealt with counterfactually in the more comprehensive setting provided by the theory of causation sketched in Footnote 4. On this view, an event
occurrence ǫ together with conditions φ explain ψ at a world w iff ǫ causes ψ at all closest ǫ ∧ φ-worlds
to w. Thus, in the case of the Stolen Car Problem, the occurrence of an additional steal event at any
time point in the interval during which the car is unattended is a cause of, and so explains, the absence
of the car at the next time point.

402

Natural Events

In order to argue for the pragmatic completeness of preference1 we have to show that
the restrictions that it imposes are sufficient. Examples involving ramifications, such as
Lifschitz’s (1990) lamp-circuit and Baker’s (1991) ice-cream eating pedestrian appear to
show otherwise. The point of these examples is illustrated by Example 3 where unintended
models are selected by the pragmatics. However, I suggest that the problem lies not with
the definition of preference1 but with the fact that we need to represent the causal nature
of ramifications (as arising from events which are contemporaneously caused (ultimately)
by primary events) correctly; as illustrated by Example 4.35
In the absence of counterexamples it seems reasonable to conjecture that preference1 is
pragmatically complete; although, of course, it is always possible that some new example
will show that preference1 is too liberal. If so, then this will not be disastrous for the
theory proposed here. On analysis, the example will reveal some further property of prediction which is not captured by preference1 , and preference1 can be refined accordingly. In
Goodman’s terms, this would simply be part of the delicate process of bringing theory and
practice into agreement.36
Rather than seeking for justifications of this kind, Sandewall (1994) proposes a radically
different methodology. He begins by suggesting a series of ontological characteristics of
instances of predictive reasoning. These include context-free inertia (I), alternative results
(A), ramification (D), concurrency (C), surprises (S), and normality (N). Examples of
reasoning which include several of these characteristics can be classified as belonging to
the corresponding family; for example, the inductive version of the Yale Shooting Problem
(Example 1 above) belongs to the family IN, as it involves reasoning about inertia and about
the normal outcome of the shoot event. Sandewall then proposes a formal pragmatics, the
trajectory semantics, which defines the class of intended models for any given example of the
family IAD, and uses this to prove the correctness of (to provide validations for) various
formal pragmatics for theories expressed in appropriate formal languages. For example, a
simple form of chronological minimization called PCM is proved to be valid for the family
IAD; by showing that, for any given example of the IAD family, PCM selects exactly the
same models as the trajectory semantics does. Thus the range of applicability of PCM is
established; that is, PCM is proved to be applicable for all instances of prediction which
have the characteristics of the IAD family.
35. The discussion of the gears example, culminating in Example 5, shows that the theory of events is not
restricted by the presence of symmetric constraints, but rather that it can be used to represent our
reasoning about the direction of causation among symmetrically constrained events.
36. A sceptical reader might object that all I have done is to show that the theory works for a few “toy”
examples. However, this is to overlook the care taken to represent prediction accurately, and to misunderstand the motivation behind the choice of examples. The examples referred to in the justification
given above were not chosen because they are easily represented, but because they are well-know benchmark examples which are specifically designed to probe for weaknesses; in Goodman’s (1954, p. 18) terms,
they represent “clinically pure cases that . . . display to best advantage the symptoms of a widespread and
destructive malady”. So the fact that a theory represents them correctly provides significant empirical
evidence in its favour. If the representations are also intuitively convincing, then they provide significant
evidence in favour of the conceptual basis of the theory.
A related objection is that the theory has only been shown to work for small-scale examples, and
there is no guarantee that it will “scale up” easily to larger examples. But I know of no inherent
limitations of scale. In particular, the definitions of events (their preconditions and effects) can be
extended in a modular way, and the fact that events are inductive, rather than deductive, means that a
given theory can be extended such that additional events occur without fear of contradiction.

403

Bell

However this methodology is limited in two ways.
Firstly, there is the question of how the trajectory semantics itself is justified. It provides
a formal definition of the intended models of any given example (of the IAD family), but
how can we be sure that the models that it selects for any given example correspond to the
ones that we would select? It is not sufficient to simply stipulate that this is the case. It
is possible to justify a formal pragmatics by proving it to be equivalent to another (as in
the case of PCM and trajectory semantics), however at some point a formal pragmatics has
to be squared with our intuitions by means of an argument of the kind employed above.
Thus the best that we can hope for is a thesis relating a formal theory of prediction and
our intuitive notion of it.37 As Kripke once remarked, there is no mathematical substitute
for philosophy.
Secondly, as the trajectory semantics is restricted to the IAD family (essentially to
strips events), it would need to be extended to the IADCSN family before it could be
applied to event theories.
Nevertheless, it would be worthwhile to attempt to undertake a mathematical assessment of event theories relative to some other formal theory; such as the above extension of
trajectory semantics. If possible, mathematical investigations of this kind provide an additional means of justifying formal theories. If the attempt to prove an equivalence between
two theories fails, then this will typically highlight inadequacies in one or both of them,
and so will suggest that the intuitions behind them need to be refined. Alternatively, if
it is possible to prove that the theories are equivalent, then this would provide mutuallysupporting evidence for the robustness of the intuitions underlying each of them; as it would
suggest that, despite appearances to the contrary, the two formal theories capture the same
properties of our intuitive notion of predictive causal reasoning.38 There is no philosophical
substitute for mathematics.

7. Related Work
The theory of natural events presented in this paper has been developed over many years,
and earlier versions of parts of it have appeared elsewhere. These earlier fragments have
been revised and combined here into a unified whole.
An earlier version of the theory of inductive events (Section 3) was suggested in previous research (Bell, 1998), and its model-building implementation was discussed by White,
Bell, and Hodges (1998). The theory has its intellectual origins in the work of McCarthy
(1980, 1986), who suggested that circumscription could be used to approach the qualification problem and the frame problem. This proposal was developed by Shoham (1988),
who introduced the notion of chronological minimization in a classical, temporal, modal
language. Shoham’s theory offers the promise of a simple and intuitive approach to the two
problems. However, his theory is limited in many ways. In particular, it is propositional
and has no fact-event distinction, so it is not possible to state general axioms for change and
37. Similarly, the Chuch-Turing thesis is a thesis, rather than a theorem, as it claims an equivalence between
an informal intuitive notion, effective computability, and a formal theory of computability (recursive
functions, Turing machines).
38. Just as the equivalence results between the rival formalizations of effective computability (recursive
functions, Turing Machines, etc.) provide mutually-supportive evidence of the soundness of the intuitions
which underlie each of them.

404

Natural Events

inertia in it. Also, Shoham requires that his theories meet a number of syntactic restrictions, including the restriction that no two causal rules conflict (see his Definition 4.7(7)).
His reason for doing so is to ensure that his theories are deterministic, so that the process
of chronological minimization interprets them correctly. But this makes it impossible to
express problems involving both (inductive) change and inertia, such as the inductive version of the Yale Shooting Problem (discussed in Section 3 above) in his theory. Lifting the
restriction would allow the problem to be expressed, but the chronological minimization
of the theory would have the counter-intuitive result that Fred remained alive after the
shooting in one class of models, and died in another. In short, chronological minimization
is too simple. The theory of inductive events can thus be thought of as a generalization and
refinement of Shoham’s theory which fulfills its promise.
The first general common sense theory of change and inertia was proposed by Lifschitz
(1987). The axioms that he defines (in the Situation Calculus) are substantially different
from those of the theory of inductive events. However an important similarity is his restriction of his inertia axiom, on the basis of a distinction between primitive and defined
fluents (the Situation Calculus counterparts of object relations). He later (Lifschitz, 1990,
p. 371) says that this distinction should be regarded as a technical trick, and suggests an
alternative, more principled, distinction based on frames (McCarthy & Hayes, 1969). However it seems that his primitive-defined distinction can be justified by identifying it with
the physical-theoretical distinction introduced in Section 3.39
Secondary events (Section 4) were suggested in previous research (Bell, 1999, 2000).
Their use in the representation of ramifications should be compared with Thielscher’s (1997)
treatment. Thielscher views the problem of ramifications as a logical one, which arises (as
discussed in Section 4) because of the lack of “causal directedness” in material conditionals.
His solution starts with a deductive strips-like representation of events. The ramifications
of an event are then brought about by applying a series of causal constraints until a stable
state is reached. Causal constraints can be thought of as directed conditionals between two
single effects, stating the circumstances under which the first causes the second. Thus, the
problem posed by Example 3 is solved by having Ollie at L2 as a direct effect of moving from
L1 to L2, and having the fact that he is at L2 and is holding the block cause the additional
effect of its being at L2 also. While this may have the same effect as the invocation of
secondary events in some cases, the two approaches are radically different. Thielscher’s
causal constraints are deductive in nature, so, once begun, their application runs to its
conclusion without possible interruption. There is thus no possibility of representing failure
at any stage of the indirect-effect-propagation process. So, as it stands, his solution is limited
to deterministic deductive events, which either occur in isolation or which do not (directly
or indirectly) conflict with each other. Sandewall’s (1996) Causal Propagation Semantics is
similar to Thielscher’s approach and suffers from the same limitations. By contrast, on my
approach, success is propagated along an invocation chain, producing the associated effects,
but at some point an event may fail, in which case the propagation terminates; as illustrated
39. Readers familiar with Goodman’s (1954, Ch. 3) paradox will know that the logical complexity of predicates is relative to a choice of language. But here, like Quine (Footnote 12), we can appeal to ordinary
language and its scientific refinements, and refrain from venturing into the fly-bottle. (Wittgenstein,
1953, §309: “What is your aim in philosophy?—To shew the fly the way out of the fly-bottle”.)

405

Bell

by Example 4 above. This representation of ramifications can thus be freely combined with
non-deterministic events and with conflicting events.
Several formal theories of events employ a primitive causation relation; notable examples
are the A-language family originating with Gelfond and Lifschitz (1993) and Lin’s (1995)
extension of (Toronto) Situation Calculus (Lin & Reiter, 1994). The appearance of an
unanalyzed causation relation in a formal theory of events seems to beg the question; as
an appeal is made to a more complex notion (causation) in order to give an analysis of a
simpler one (change). However, rather than view the causation relation as an appeal to
full-blown causation (see Footnote 4), it is better to view it as a means of encoding detailed
causal knowledge. In Lin’s theory the ternary relation Caused (p, v, s) “is true if the fluent p
is caused (by something unspecified) to have the truth value v in the situation s” (p. 1986).
In keeping with this interpretation, two axioms are given:
Caused (p, true, s) ⊃ Holds(p, s) ,
Caused (p, f alse, s) ⊃ ¬Holds(p, s) .
Thus if fluent p is caused to have the value true (false) in situation s, then p holds (does
not hold) in situation s. By way of illustration, Lin discusses the example of a spring
loaded suitcase. If both of its locks are up, the suitcase opens. Initially the suitcase is
closed, one lock is up, the other is down, and the down-lock is flipped. Naturally we expect
that the suitcase will open as a result. However, attempts to formalize this example using
an ordinary domain axiom and inertia fail; as in the lamp circuit example, the other lock
remains up in the intended models, but there are also unintended models in which the
suitcase remains closed and the other lock changes position as a result. Consequently Lin
proposes a causal domain axiom:
up(L1, s) ∧ up(L2, s) ⊃ Caused (open, true, s) ;

(81)

which states that if locks L1 and L2 are both up in situation s, then the suitcase is Caused
to be open in s. The intention is that axioms such as this are interpreted “causally”
(positively), and this effect is achieved by circumscribing the Caused relation and by adding
an inertia axiom which states that fluents which are not Caused to change persist. Thus
according to Lin’s account, ramifications arise as a result of causally-directed domain axioms
such as (81), in which facts cause other facts to change. Indeed, Lin (p. 1986) says of (81)
that:
[T]he physical spring loaded mechanism behind the causal rule has been abstracted away. For all we care, it may just as well be that the device is not
made of spring, but of bombs that will blow open the suitcase each time the two
locks are in the up position. It then seems natural to say that the fluent open
is caused to be true by the fact that the two locks are both in the up position.
However this seems odd from a common sense point of view, which has it that events
cause change and that facts are otherwise inert. Moreover, as the actual cause has been
abstracted away, it is difficult to see how Lin’s account could be extended in order to include
the treatment of qualifications; for example, the suitcase may fail to open when both locks
are up because the mechanism is rusty, someone is sitting on the suitcase, etc. By contrast,
406

Natural Events

it is obvious how to represent the problem using primary and secondary events; in a context
in which one lock is up and the other is down, flipping the down-lock invokes a secondary
event which, if it succeeds, has the additional effect that the suitcase is open. Moreover,
as noted above, the introduction of secondary events makes it possible to represent more
complex examples of ramifications, such as those of Example 4.
Secondary events should also be compared with the natural actions proposed by Reiter
(1996), Lin (1998), and Pinto (1998). In addition to actions initiated by agents, they suggest
that there are also natural actions which arise due to the nature of the world (the Laws
of Nature, etc.), and which are guaranteed to succeed. Their use is illustrated by Pinto’s
treatment of Lifschitz’s (1990) lamp-circuit example. If the switches in a circuit are in the
same position (both up or both down) then the lamp is on, otherwise it is off. Initially the
switches are in opposite positions and one of them is flipped. Clearly we expect the lamp to
be on as a result. However attempts to formalize this example using domain axioms with an
inertia axiom fail. In the intended models the other switch retains its position by inertia and
so the lamp comes on as a result of the flip event. However there are also unintended models
in which the lamp remains off by inertia and consequently the other switch mysteriously
changes position as a result; this problem is essentially the same as that posed by Example 3.
In Pinto’s treatment of the problem, the agent flipping a switch results in the natural action
of current flowing in the circuit, and this in turn results in the lamp being on. The natural
action of the current flowing resolves the conflict between alternative uses of the inertia
axiom by bringing about an intermediary state in which both switches are up (as the flowing
current does not affect their position) and in which the inertia axiom can only be applied in
the intended way (as the flowing current is guaranteed to have the effect that the lamp is on).
As this proposal aims to solve the causal-directedness problem by introducing additional
events it can, perhaps, be seen as lying somewhere between Thielscher’s “logical” approach
and my “representational” approach. Unlike natural events, natural actions are deductive.
So it is difficult to see how natural actions could be used to represent qualifications. For
example, the natural action of the current flowing is guaranteed to succeed (turning the
lamp on), but in reality the current might not flow if the wire loses its conductivity, is cut,
etc. But clearly this complication can be represented using natural events. In a context
in which the switches are in opposite positions, flipping a switch invokes a secondary event
which, if it succeeds, results in the lamp being on; the lamp’s coming on is thus a defeasible
secondary effect of flipping the switch. Moreover, while natural actions are independent
of the actions of agents, secondary events are causally dependent on (at least one of) the
events which invoke them.
Event preferences (Section 5) were introduced in previous research (Bell, 2001, ‘Simultaneous events’). The presentation has been substantially improved, and, as the examples
show, the combination of event preferences and secondary events provides interesting new
possibilities.
Recently Vo and Foo (2005) have suggested an inductive theory of reasoning about action
which, they suggest, provides the basis for a unified solution to the frame, qualification, and
ramification problems. Their theory is based on the theory of argumentation developed by
Bondarenko, Dung, Kowalski, and Toni (1997), and so it differs radically from mine in terms
of its technical realization. At the conceptual level there are interesting similarities. Like
me, Vo and Foo suggest that event occurrences should be minimized and that the inertia of
407

Bell

fluents (their counterpart of object-relations) should be maximized (p. 448). Furthermore,
in order to integrate their solution to the frame and qualification problems, they, in effect
(p. 493), adopt the principle that change is preferred to inertia (Bell, 1998). However, in
their theory there is no suggestion that the minimization and maximization should be done
chronologically. Indeed, they suggest that not doing so is a strength of their approach as
it enables them to provide explanations by “reasoning backwards” (or antichronologically)
in examples such as the Stolen Car Problem. However, as reasoning chronologically is an
essential feature of predictive reasoning, I suspect that this will prove to be a problem for
Vo and Foo’s theory. Instead I suggest (Footnote 34) that explanations should be treated
as counterfactual causes.

8. Conclusion
I began by arguing that Deductionism is a logical mistake, and have made a case for Inductionism. This began with the basic theory of inductive events, which provides the basis
for an integrated solution to the qualification and frame problems. I then introduced the
distinction between primary and secondary events in order to represent the causal structure
of natural events, thereby providing the basis for a solution to the ramification problem
and to the problem of representing non-determinism. Finally, I introduced event preferences, which can be used to express defeasible preferences over the outcomes of conflicting
simultaneous events. The development of the theory illustrates the benefit of starting off
on the right foot, with inductive, rather than deductive, events. The basic theory is simple
and intuitive, and its extensions require no more than the addition of a few axioms and the
refinement of one clause of the formal pragmatics.
In simple cases there may be little to choose between deductive and inductive theories of events; as both can produce predictions which are wrong. However the inductive
representation of natural events is more accurate because it reflects their defeasibility. The
representation of inductive events makes it possible to define primary and secondary events,
and defeasible event preferences. These in turn make it possible to give accurate representations of ramifications, non-determinism, and conflicting events. And this in turn makes it
possible to represent complex cases accurately. For instance, in Example 8, if any two of the
stooges attempt to move to a location, then one is expected to succeed, however if all three
do, then none succeeds. As always with inductive events, the example can be elaborated.
Thus if two stooges attempt the move and the preferred one fails for some independent
reason (he slips, say), then the other stooge normally succeeds (unless he also slips). Or in
the case where all three attempt the move, if Ollie slips, then Stan is expected to succeed.
But if he also slips, then Charlie is expected to succeed; but may also slip. Moreover, the
example can readily be combined with others. For example, each stooge could carry a stack
of blocks; where each block moves only if the block beneath it, or the stooge holding it,
moves. I know of no other theory of events which can represent reasoning of this subtlety
and complexity.
In future work, the model-building implementation of event theories will be investigated.
The general idea was outlined in previous research (Bell, 1995), and White et al. (1998)
describe the implementation of (an earlier version of) the theory of inductive events. Essentially the idea is to build finite initial parts of the representative preferred models of a given
408

Natural Events

event theory chronologically, as suggested by the informal discussion of the Yale Shooting
Problem in Section 3.
As suggested in the introduction (and in Footnote 4), the theory of natural events forms
part of a larger theory of causation (Bell, 2004, 2006, 2008). Event theories are used to
represent detailed, regularity-based, causal knowledge about events; expressed in the form
of preconditions and effects, invocations, and event preferences. This is used as the basis of
a general definition of sufficient causation, which is combined with a refinement of Lewis’s
(1986, Ch. 21) counterfactual-dependence condition to give the definition of causation.

Acknowledgments
I am grateful to everyone who has commented on this work, especially the anonymous
referees, Wilfrid Hodges, Robert Miller, Edmund Robinson, Murray Shanahan, and Graham
White.

References
Baker, A. B. (1991). Nonmonotonic reasoning in the framework of the situation calculus.
Artificial Intelligence, 49, 5–23.
Barringer, H., Cheng, J., & Jones, C. (1984). A logic covering undefinedness in program
proofs. Acta Informatica, 21, 251–269.
Bell, J. (2001). Causal counterfactuals. In Working Notes of Common Sense 2001. Available
at: www.cs.nyu.edu/cs/faculty/davise/commonsense01/.
Bell, J. (1995). Pragmatic reasoning; a model-based theory. In Pólos, L., & Masuch, M.
(Eds.), Applied Logic; How, What and Why, pp. 1–27. Kluwer. A selection of papers
from the Applied Logic Conference, Amsterdam, 1992.
Bell, J. (1996). A model-based approach to predictive causal reasoning. In Doherty (Doherty,
1996), pp. 169–195.
Bell, J. (1998). Chronological minimization and explanation. In Miller, R., & Shanahan,
M. (Eds.), Working papers of Common Sense ’98; the fourth symposium on logical
formalizations of common sense reasoning.
Bell, J. (1999). Primary and secondary events. In Thielscher, M. (Ed.), Working notes of
the 4th workshop on Nonmonotonic Reasoning, Action and Change, pp. 65–72.
Bell, J. (2000). Primary and secondary events. Electronic Transactions on Artificial Intelligence discussion paper. Available at: www.ida.liu.se/ext/etai/rac/. Subsequent
version available at: www.dcs.qmul.ac.uk/˜jb, 2001.
Bell, J. (2001). Simultaneous events: Conflicts and preferences. In Proceedings of the 6th
European Conference on Symbolic and Quantitative Approaches to Reasoning with
Uncertainty (ECSQARU 2001), pp. 714–725. Springer.
Bell, J. (2004). Causation and causal conditionals. In Proceedings of the 9th International
Conference on the Principles of Knowledge Representation and Reasoning (KR 2004),
pp. 2–11. AAAI Press.
409

Bell

Bell, J. (2006). Causation as production. In Proceedings of the 17th European Conference
on Artificial Intelligence (ECAI 2006), pp. 327–331. IOS Press.
Bell, J. (2008). A common sense theory of causation. Forthcoming.
Blackburne, S. (1994). The Oxford Dictionary of Philosophy. Oxford University Press,
Oxford.
Bochvar, D. A. (1939). On a three-valued logical calculus and its application to the analysis
of contradictories. Matematiceskij sbornik, 4.
Bondarenko, A., Dung, P. M., Kowalski, R. A., & Toni, F. (1997). An abstract
argumentation-theoretic approach to default reasoning. Artificial Intelligence, 93,
63–101.
Collins, J., Hall, N., & Paul, L. A. (Eds.). (2004). Causation and Counterfactuals. MIT
Press, Cambridge, Massachusetts.
Davidson, D. (1980). Essays on Actions and Events. Oxford University Press, Oxford.
Denecker, M., Dupré, D. T., & Belleghem, K. V. (1998). An inductive definition approach
to ramifications. Electronic Transactions on Artificial Intelligence, 2, 25–67.
Doherty, P. (Ed.). (1996). Partiality, Modality, and Nonmonotonicity. CSLI Publications,
CSLI, Stanford University, Palo Alto.
Fikes, R. E., & Nilsson, N. J. (1971). strips, a new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189–208.
Gelfond, M., & Lifschitz, V. (1993). Representing action and change by logic programs.
Journal of Logic Programming, 17, 301–322.
Gelfond, M., Lifschitz, V., & Rabinov, A. (1991). What are the limitations of the situation
calculus?. In Boyer, R. (Ed.), Essays in Honour of Woody Bledsoe, pp. 167–179.
Kluwer Academic Publishers.
Ginsberg, M. L., & Smith, D. E. (1988). Reasoning about action II: The qualification
problem. Artificial Intelligence, 35, 311–342.
Girard, J.-Y. (1987). Linear logic. Theoretical Computer Science, 50:1, 1–102.
Goodman, N. (1954). Fact, Fiction, and Forecast. Harvard University Press, Cambridge
Mass. References are to the 4th edition, 1983.
Hanks, S., & McDermott, D. (1987). Nonmonotonic logic and temporal projection. Artificial
Intelligence, 33 (3), 379–412.
Hume, D. (1739). A Treatise of Human Nature. Clarendon Press, Oxford. References are
to the Clarendon edition, 1978.
Hume, D. (1777). An Enquiry Concerning Human Understanding. Clarendon Press, Oxford.
References are to the Clarendon edition, 1975.
Kautz, H. (1986). The logic of persistence. In Proceedings of the 5th National Conference
on Artificial Intelligence (AAAI-86), pp. 401–405.
Kleene, S. C. (1952). Introduction to Metamathematics. North-Holland, Amsterdam.
Kripke, S. (1975). Outline of a theory of truth. Journal of Philosophy, 72, 690–716.
410

Natural Events

Lewis, D. (1986). Philosophical Papers, Vol. II. Oxford University Press, Oxford.
Lifschitz, V. (1987). Formal theories of action. In Brown, F. (Ed.), The Frame Problem in
Artificial Intelligence, pp. 35–58. Morgan Kaufmann.
Lifschitz, V. (1990). Frames in the space of situations. Artificial Intelligence, 46, 365–376.
Lin, F., & Reiter, R. (1994). State constraints revisited. Journal on Logic and Computation,
4(5), 655–678.
Lin, F. (1995). Embracing causality in specifying the indirect effects of actions. In Proceedings of the 14th International Joint Conference Artificial Intelligence (IJCAI ’95), pp.
1985–1991.
Lin, F. (1998). On the relationships between static and dynamic causal rules in the situation
calculus. In Ortiz, C. (Ed.), Working Papers of the AAAI’98 Spring Symposium on
Prospects for a Commonsense Theory of Causation, pp. 38–43.
Mackie, J. L. (1974). The Cement of the Universe. Clarendon Press, Oxford.
Mackie, J. L. (1975). Causes and conditions. In Sosa, E. (Ed.), Causes and Conditionals, pp.
15–38. Oxford University Press, Oxford. First published in American Philosophical
Quarterly 2, 1965, pages 245-264.
McCarthy, J. (1977). Epistemological problems of artificial intelligence. In Proceedings of the
5th International Joint Conference Artificial Intelligence (IJCAI ’95), pp. 1038–1044.
McCarthy, J. (1980). Circumscription – a form of nonmonotonic reasoning. Artificial
Intelligence, 13, 27– 39.
McCarthy, J. (1986). Applications of circumscription to formalizing commonsense knowledge. Artificial Intelligence, 28, 89–116.
McCarthy, J., & Hayes, P. J. (1969). Some philosophical problems from the standpoint of
artificial intelligence. In Michie, D., & Meltzer, B. (Eds.), Machine Intelligence 4, pp.
463–502. Edinburgh University Press.
Mill, J. S. (1898). A System of Logic. Longmans, London. References are to the 8th edition,
1941.
Pinto, J. (1998). Causality in theories of action. In Miller, R., & Shanahan, M. (Eds.),
Working papers of the 4th Symposium on Logical Formalizations of Commonsense
Reasoning, pp. 349–364.
Quine, W. V. (1995). From Stimulus to Science. Harvard University Press, Cambridge,
Massachusetts.
Reiter, R. (1996). Natural actions, concurrency and continuous time in the situation calculus. In Proceedings of the 5th International Conference on Principles of Knowledge
Representation and Reasoning (KR’96.), pp. 2–13.
Reiter, R. (2001). Knowledge in Action; Logical Foundations for Specifying and Implementing Dynamical Systems. MIT Press, Cambridge, Mass.
Rosser, J., & Turquette, A. (1952). Many-valued Logics. North-Holland, Amsterdam.
Russell, B. (1913). On the notion of cause. Proceedings of the Aristotelian Society, 13, 1–26.
411

Bell

Russell, S., & Norvig, P. (2003). Artificial Intelligence; A Modern Approach. Prentice Hall,
New Jersey. 2nd Edition.
Sandewall, E. (1994). Features and Fluents; The Representation of Knowledge About Dynamical Systems. Oxford University Press, Oxford.
Sandewall, E. (1996). Assessments of ramification methods that use domain constraints.
In Proceedings of the 5th International Conference on Principles of Knowledge Representation and Reasoning (KR’96.), pp. 99–110.
Shanahan, M. (1997). Solving the Frame Problem; A Mathematical Investigation of the
Common Sense Law of Inertia. MIT Press, Cambridge, Mass.
Shoham, Y. (1988). Reasoning About Change. MIT Press, Cambridge Mass.
Taylor, R. (1975). The metaphysics of causation. In Sosa, E. (Ed.), Causation and Conditionals, pp. 39–43. Oxford University Press.
Thielscher, M. (1997). Ramification and causality. Artificial Intelligence, 98, 317–364.
Vo, Q. B., & Foo, N. Y. (2005). Reasoning about action: An argumentation-theoretic
approach. Journal of Artificial Intelligence Research, 24, 465–518.
White, G., Bell, J., & Hodges, W. (1998). Building models of prediction theories. In Cohn,
A., Schubert, L., & Shapiro, S. (Eds.), Proc. KR’98, pp. 557–568, San Francisco.
Morgan Kaufmann.
Wittgenstein, L. (1953). Philosophical Investigations. Blackwell, Oxford.

412

Journal of Artificial Intelligence Research 30 (2007) 249-272

Submitted 12/06; published 10/07

Topic and Role Discovery in Social Networks
with Experiments on Enron and Academic Email
Andrew McCallum
Xuerui Wang

mccallum@cs.umass.edu
xuerui@cs.umass.edu

Department of Computer Science
University of Massachusetts
140 Governors Drive
Amherst, MA 01003 USA

Andrés Corrada-Emmanuel

acorrada@physics.umass.edu

Department of Physics
University of Massachusetts
666 North Pleasant Street
Amherst, MA 01003 USA

Abstract
Previous work in social network analysis (SNA) has modeled the existence of links
from one entity to another, but not the attributes such as language content or topics
on those links. We present the Author-Recipient-Topic (ART) model for social network
analysis, which learns topic distributions based on the direction-sensitive messages sent
between entities. The model builds on Latent Dirichlet Allocation (LDA) and the AuthorTopic (AT) model, adding the key attribute that distribution over topics is conditioned
distinctly on both the sender and recipient—steering the discovery of topics according to
the relationships between people. We give results on both the Enron email corpus and
a researcher’s email archive, providing evidence not only that clearly relevant topics are
discovered, but that the ART model better predicts people’s roles and gives lower perplexity
on previously unseen messages. We also present the Role-Author-Recipient-Topic (RART)
model, an extension to ART that explicitly represents people’s roles.

1. Introduction
Social network analysis (SNA) is the study of mathematical models for interactions among
people, organizations and groups. With the recent availability of large data sets of human
interactions (Shetty & Adibi, 2004; Wu, Huberman, Adamic, & Tyler, 2003), the popularity
of services like MySpace.com and LinkedIn.com, and the salience of the connections among
the 9/11 hijackers, there has been growing interest in social network analysis.
Historically, research in the field has been led by social scientists and physicists (Lorrain
& White, 1971; Albert & Barabási, 2002; Watts, 2003; Wasserman & Faust, 1994), and
previous work has emphasized binary interaction data, with directed and/or weighted edges.
There has not, however, previously been significant work by researchers with backgrounds
in statistical natural language processing, nor analysis that captures the richness of the
language contents of the interactions—the words, the topics, and other high-dimensional
specifics of the interactions between people.
c
2007
AI Access Foundation. All rights reserved.

McCallum, Wang, & Corrada-Emmanuel

Using pure network connectivity properties, SNA often aims to discover various categories of nodes in a network. For example, in addition to determining that a node-degree
distribution is heavy-tailed, we can also find those particular nodes with an inordinately
high number of connections, or with connections to a particularly well-connected subset
(group or block) of the network (Nowicki & Snijders, 2001; Kemp, Griffiths, & Tenenbaum,
2004; Kemp, Tenenbaum, Griffiths, Yamada, & Ueda, 2006; Kubica, Moore, Schneider, &
Yang, 2002; Airoldi, Blei, Fienberg, & Xing, 2006; Kurihara, Kameya, & Sato, 2006). Furthermore, using these properties we can assign “roles” to certain nodes (Lorrain & White,
1971; Wolfe & Jensen, 2004). However, it is clear that network properties are not enough
to discover all the roles in a social network. Consider email messages in a corporate setting,
and imagine a situation in which a tightly knit group of users trade email messages with
each other in a roughly symmetric fashion. Thus, at the network level they appear to fulfill
the same role. But perhaps, one of the users is in fact a manager for the whole group—a
role that becomes obvious only when one accounts for the language content of the email
messages.
Outside of the social network analysis literature, there has been a stream of new research
in machine learning and natural language models for clustering words in order to discover
the few underlying topics that are combined to form documents in a corpus. Probabilistic
Latent Semantic Indexing (Hofmann, 2001) and Latent Dirichlet Allocation (Blei, Ng, &
Jordan, 2003) robustly discover multinomial word distributions of these topics. Hierarchical
Dirichlet Processes (Teh, Jordan, Beal, & Blei, 2004) can determine an appropriate number
of topics for a corpus. The Author-Topic Model (Steyvers, Smyth, Rosen-Zvi, & Griffiths,
2004) learns topics conditioned on the mixture of authors that composed a document.
However, none of these models are appropriate for SNA, in which we aim to capture the
directed interactions and relationships between people.
The paper presents the Author-Recipient-Topic (ART) model, a directed graphical model
of words in a message generated given their author and a set of recipients. The model is
similar to the Author-Topic (AT) model, but with the crucial enhancement that it conditions the per-message topic distribution jointly on both the author and individual recipients,
rather than on individual authors. Thus the discovery of topics in the ART model is influenced by the social structure in which messages are sent and received. Each topic consists
of a multinomial distribution over words. Each author-recipient pair has a multinomial
distribution over topics. We can also easily calculate marginal distributions over topics
conditioned solely on an author, or solely on a recipient, in order to find the topics on
which each person is most likely to send or receive.
Most importantly, we can also effectively use these person-conditioned topic distributions to measure similarity between people, and thus discover people’s roles by clustering
using this similarity.1 For example, people who receive messages containing requests for
photocopying, travel bookings, and meeting room arrangements can all be said to have the
role “administrative assistant,” and can be discovered as such because in the ART model
they will all have these topics with high probability in their receiving distribution. Note that
1. The clustering may be either external to the model by simple greedy-agglomerative clustering, or internal
to the model by introducing latent variables for the sender’s and recipient’s roles, as described in the
Role-Author-Recipient-Topic (RART) model toward the end of this paper.

250

Topic and Role Discovery in Social Networks

we can discover that two people have similar roles even if in the graph they are connected
to very different sets of people.
We demonstrate this model on the Enron email corpus comprising 147 people and 23k
messages, and also on about 9 months of incoming and outgoing mail of the first author,
comprising 825 people and 14k messages. We show not only that ART discovers extremely
salient topics, but also gives evidence that ART predicts people’s roles better than AT and
SNA. Also, we show that the similarity matrix produced by ART is different from both the
SNA matrix and the AT matrix in several appropriate ways. Furthermore, we find that the
ART model gives a significantly lower perplexity on previously unseen messages than AT,
which shows that ART is a better topic model for email messages.
We also describe an extension of the ART model that explicitly captures roles of people,
by generating role associations for the author and recipient(s) of a message, and conditioning
the topic distributions on the role assignments. The model, which we term Role-AuthorRecipient-Topic (RART), naturally represents that one person can have more than one role.
We describe several possible RART variants, and describe experiments with one of these
variants.
The importance of modeling the language associated with social network interactions
has also recently been demonstrated in the Group-Topic (GT) model (Wang, Mohanty, &
McCallum, 2006). Unlike ART, which discovers roles, GT discovers groups. Like ART,
it uses text data to find interesting and useful patterns that would not be possible with
edge relations alone. GT simultaneously clusters entities into groups that share similar
interaction patterns, and also clusters text (or other attributes) of their interactions into
topics—doing so in such a way that clustering in each dimension informs the other. When
applied to the voting records and corresponding text of resolutions from the U.S. Senate and
the U.N., the Group-Topic model shows that incorporating the votes results in more salient
topic clusters, and that different groupings of legislators emerge from different topics. Both
role discovery and group discovery are primary areas of SNA research.

2. Author-Recipient-Topic Models
Before describing the ART model, we first describe three related models. Latent Dirichlet
Allocation (LDA) is a Bayesian network that generates a document using a mixture of topics
(Blei et al., 2003). In its generative process, for each document d, a multinomial distribution
θ over topics is randomly sampled from a Dirichlet with parameter α, and then to generate
each word, a topic z is chosen from this topic distribution, and a word, w, is generated
by randomly sampling from a topic-specific multinomial distribution φz . The robustness of
the model is greatly enhanced by integrating out uncertainty about the per-document topic
distribution θ.
The Author model, also termed a Multi-label Mixture Model (McCallum, 1999), is a
Bayesian network that simultaneously models document content and its authors’ interests
with a 1-1 correspondence between topics and authors. For each document d, a set of
authors ad is observed. To generate each word, an author, z, is sampled uniformly from
the set, and then a word, w, is generated by sampling from an author-specific multinomial
distribution φz . The Author-Topic (AT) model is a similar Bayesian network, in which
each author’s interests are modeled with a mixture of topics (Steyvers et al., 2004). In
251

McCallum, Wang, & Corrada-Emmanuel

Author-Topic Model
(AT)

Author-Recipient-Topic Model
(ART)

Latent Dirichlet Allocation
(LDA)

Author Model
(Multi-label Mixture Model)

[Blei, Ng, Jordan, 2003]

[McCallum 1999]

[Rosen-Zvi, Griffiths, Steyvers, Smyth 2004]

α

ad

ad

θ

z

x
α

z

θ

ad

φ

β

w
T

Nd
D

φ

β

w
A

α

z

φ

Nd
D

θ

z

A,A

β

w
T

rd

x

A

β

[This paper]

Nd
D

φ

w
T

Nd
D

Figure 1: Three related models, and the ART model. In all models, each observed word,
w, is generated from a multinomial word distribution, φz , specific to a particular
topic/author, z, however topics are selected differently in each of the models.
In LDA, the topic is sampled from a per-document topic distribution, θ, which
in turn is sampled from a Dirichlet over topics. In the Author Model, there is
one topic associated with each author (or category), and authors are sampled
uniformly. In the Author-Topic model, the topic is sampled from a per-author
multinomial distribution, θ, and authors are sampled uniformly from the observed
list of the document’s authors. In the Author-Recipient-Topic model, there is
a separate topic-distribution for each author-recipient pair, and the selection of
topic-distribution is determined from the observed author, and by uniformly sampling a recipient from the set of recipients for the document.

its generative process for each document d, a set of authors, ad , is observed. To generate
each word, an author x is chosen uniformly from this set, then a topic z is selected from a
topic distribution θx that is specific to the author, and then a word w is generated from a
topic-specific multinomial distribution φz . However, as described previously, none of these
models is suitable for modeling message data.
An email message has one sender and in general more than one recipients. We could
treat both the sender and the recipients as “authors” of the message, and then employ the
AT model, but this does not distinguish the author and the recipients of the message, which
is undesirable in many real-world situations. A manager may send email to a secretary and
vice versa, but the nature of the requests and language used may be quite different. Even
more dramatically, consider the large quantity of junk email that we receive; modeling the
topics of these messages as undistinguished from the topics we write about as authors would
be extremely confounding and undesirable since they do not reflect our expertise or roles.
Alternatively we could still employ the AT model by ignoring the recipient information
of email and treating each email document as if it only has one author. However, in this
case (which is similar to the LDA model) we are losing all information about the recipients,
and the connections between people implied by the sender-recipient relationships.
252

Topic and Role Discovery in Social Networks

SYMBOL
T
D
A
V
Nd

DESCRIPTION
number of topics
number of email messages
number of email accounts (senders and recipients)
number of unique words (vocabulary size)
number of word tokens in message d
Table 1: Notation used in this paper

Thus, we propose an Author-Recipient-Topic (ART) model for email messages. The
ART model captures topics and the directed social network of senders and recipients by
conditioning the multinomial distribution over topics distinctly on both the author and one
recipient of a message. Unlike AT, the ART model takes into consideration both author
and recipients distinctly, in addition to modeling the email content as a mixture of topics.
The ART model is a Bayesian network that simultaneously models message content,
as well as the directed social network in which the messages are sent. In its generative
process, for each message d, an author, ad , and a set of recipients, rd , are observed. To
generate each word, a recipient, x, is chosen uniformly from rd , and then a topic z is
chosen from a multinomial topic distribution θad x , where the distribution is specific to the
author-recipient pair (ad , x). This distribution over topics could also be smoothed against a
distribution conditioned on the author only, although we did not find that to be necessary
in our experiments. Finally, the word w is generated by sampling from a topic-specific
multinomial distribution φz . The result is that the discovery of topics is guided by the
social network in which the collection of message text was generated.
The graphical model representations for all models are shown in Figure 1. In the ART
model, given the hyper-parameters α and β, an author ad , and a set of recipients rd for
each message d, the joint distribution of the topic mixture θij for each author-recipient pair
(i, j), the word mixture φt for each topic t, a set of recipients x, a set of topics z and a set
of words w in the corpus is given by:
P (Θ, Φ, x, z, w|α, β, a, r) =

A Y
A
Y

p(θij |α)

i=1 j=1

T
Y

Nd
D Y
Y
p(φt |β)
(P (xdi |rd )P (zdi |θad xdi )P (wdi |φzdi ))

t=1

d=1 i=1

Integrating over Θ and Φ, and summing over x and z, we get the marginal distribution
of a corpus:
P (w|α, β, a, r)
ZZ Y
Nd X
A Y
A
T
D Y
A
T
Y
Y
X
=
p(θij |α)
p(φt |β)
(P (xdi |rd )
(P (zdi |θad xdi )P (wdi |φzdi )))dΦdΘ
i=1 j=1

t=1

d=1 i=1 xdi =1

zdi =1

2.1 Inference by Gibbs Sampling
Inference on models in the LDA family cannot be performed exactly. Three standard approximate inference methods have been used to obtain practical results: variational methods
253

McCallum, Wang, & Corrada-Emmanuel

Algorithm 1 Inference and Parameter Estimation in ART
1: initialize the author and topic assignments randomly for all tokens
2: repeat
3:
for d = 1 to D do
4:
for i = 1 to Nd do
5:
draw xdi and zdi from P (xdi , zdi |x−di , z−di , w, α, β, a, r)
6:
update nad xdi zdi and mzdi wdi
7:
end for
8:
end for
9: until the Markov chain reaches its equilibrium
10: compute the posterior estimates of θ and φ

(Blei et al., 2003), Gibbs sampling (Griffiths & Steyvers, 2004; Steyvers et al., 2004; RosenZvi, Griffiths, Steyvers, & Smyth, 2004), and expectation propagation (Griffiths & Steyvers,
2004; Minka & Lafferty, 2002). We choose Gibbs sampling for its ease of implementation.
Note that we adopt conjugate priors (Dirichlet) for the multinomial distributions, and thus
we can easily integrate out θ and φ, analytically capturing the uncertainty associated with
them. In this way we facilitate the sampling—that is, we need not sample θ and φ at
all. One could estimate the values of the hyper-parameters of the ART model, α and β,
from data using a Gibbs EM algorithm (Andrieu, de Freitas, Doucet, & Jordan, 2003). In
some applications, topic models are very sensitive to hyper-parameters, and it is extremely
important to set the right values for the hyper-parameters. However, in the particular applications discussed in this paper, after trying out many different hyper-parameter settings,
we find that the sensitivity to hyper-parameters is not very strong. Thus, again for simplicity, we use fixed symmetric Dirichlet distributions (α = 50/T and β = 0.1) in all our
experiments.
We need to derive P (xdi , zdi |x−di , z−di , w, α, β, a, r), the conditional distribution of a
topic and recipient for the word wdi given all other words’ topic and recipient assignments,
x−di and z−di , to carry out the Gibbs sampling procedure for ART. We begin with the joint
probability of the whole data set, and by the chain rule, the above conditional probability
can be obtained with ease:
αz + nad xdi zdi − 1
βw + mzdi wdi − 1
P (xdi , zdi |x−di , z−di , w, α, β, a, r) ∝ PT di
PV di
t=1 (αt + nad xdi t ) − 1
v=1 (βv + mzdi v ) − 1
where nijt is the number of tokens assigned to topic t and the author-recipient pair (i, j),
and mtv represent the number of tokens of word v assigned to topic t.
The posterior estimates of θ and φ given the training set can be calculated by
αz + nijz
βw + mtw
, φ̂tw = PV
θ̂ijz = PT
t=1 (αt + nijt )
v=1 (βv + mtv )

(1)

Detailed derivation of Gibbs sampling for ART is provided in Appendix A. An overview
of the Gibbs sampling procedure we use is shown in Algorithm 1.
254

Topic and Role Discovery in Social Networks

3. Related Work
The use of social networks to discover “roles” for people (or nodes) in a network goes back
over three decades to the work of Lorrain and White (1971). It is based on the hypothesis
that nodes in a network that relate to other nodes in “equivalent” ways must have the
same role. This equivalence is given a probabilistic interpretation by Holland, Laskey,
and Leinhardt (1983): nodes assigned to a class/role are stochastically equivalent if their
probabilities of relationships with all other nodes in the same class/role are the same.
The limitation of a single class/role label for each node in a network is relaxed in recent
work by Wolfe and Jensen (2004). They consider a model that assigns multiple role labels
to a given node in the network. One advantage of multiple labels is that, in this factored
model, fewer parameters are required to be estimated than in a non-factored model using a
label obliged to represent more values. They find that, two labels with three values (giving
32 = 9 possible labelings for each node) is a better estimator for synthetic data produced
by a two-label process than a model using one label with nine possible values. This is, of
course, the advantage of mixture models, such as LDA and the ART model presented here.
The study of email social networks has been hampered by the unavailability of a public
corpus. The research that has been published has used email to-from logs. Logs are easier
to obtain and are less intrusive on user’s privacy. This means that previous research has
focused on the topological structure of email networks, and the dynamics of the email
traffic between users. Wu et al. (2003) look at how information flowed in an email network
of users in research labs (mostly from HP Labs). They conclude that epidemic models of
information flow do not work for email networks and thus identifying hubs in the network
may not guarantee that information originating at a node reaches a large fraction of the
network. This finding serves as an example that network properties are not sufficient to
optimize flow in an email network. Adamic and Adar (2004) study the efficiency of “local
information” search strategies on social networks. They find that in the case of an email
network at HP Labs, a greedy search strategy works efficiently as predicted by Kleinberg
(2000) and Watts, Dodds, and Newman (2002).
All these approaches, however, limit themselves to the use of network topology to discover roles. The ART model complements these approaches by using the content of the
“traffic” among nodes to create language models that can bring out differences invisible at
the network level.
As discussed in the introduction, we have also recently developed a model for group
discovery. In addition to relation-edge data, our Group-Topic (GT) model also takes into
consideration the textual attributes of relations, and allows the discovery of groups to be
guided by emerging textual topics and vice-versa (Wang et al., 2006). Experiments on
voting data show the Group-Topic model’s joint inference improves both the groups and
topics discovered. Other modalities of information can be combined to discover hidden
structure. For example, time and text are combined in the Topics over Time (TOT) model
(Wang & McCallum, 2006), which finds trends in time-sensitive topics using a continuous
distribution over time-stamps. Dynamic Topic Models (Blei & Lafferty, 2006b) incorporate
time into topic models through transitions in a Markov process. The ART model could be
easily extended to incorporate temporal information.
255

McCallum, Wang, & Corrada-Emmanuel

As discussed earlier, the ART model is a direct offspring of Latent Dirichlet Allocation
(Blei et al., 2003), the Multi-label Mixture Model (McCallum, 1999), and the AuthorTopic Model (Steyvers et al., 2004), with the distinction that ART is specifically designed
to capture language used in a directed network of correspondents. Another more recent
model that associates topics with people is the Author-Persona-Topic (APT) model (Mimno
& McCallum, 2007). APT is designed specifically to capture the expertise of a person,
modeling expertise as a mixture of topical intersections, and is demonstrated on the task
of matching reviewers to submitted research papers.
New topic models have been actively studied in recent years for many different tasks,
including joint modeling of words and research paper citations (Erosheva, Fienberg, &
Lafferty, 2004), capturing correlations among topics (Blei & Lafferty, 2006a; Li & McCallum,
2006), taking advantage of both topical and syntactic dependencies (Griffiths, Steyvers, Blei,
& Tenenbaum, 2004), and discovering topically-relevant phrases by Markov dependencies
in word sequences (Wang, McCallum, & Wei, 2007). Many of these models could be easily
combined with the ART model, and would likely prove useful.

4. Experimental Results
We present results with the Enron email corpus and the personal email of one of the authors
of this paper (McCallum). The Enron email corpus, is a large body of email messages
subpoenaed as part of the investigation by the Federal Energy Regulatory Commission
(FERC), and then placed in the public record. The original data set contains 517,431
messages, however MD5 hashes on contents, authors and dates show only 250,484 of these
to be unique.
Although the Enron email data set contains the email folders of 150 people, two people
appear twice with different usernames, and we remove one person who only sent automated
calendar reminders, resulting in 147 people for our experiments. We hand-corrected variants
of the email addresses for these 147 users to capture the connectivity of as much of these
users’ emails as possible. The total number of email messages traded among these users is
23,488. We did not model email messages that were not received by at least one of the 147
users.
In order to capture only the new text entered by the author of a message, it is necessary
to remove “quoted original messages” in replies. We eliminate this extraneous text by a
simple heuristic: all text in a message below a “forwarded message” line or time stamp is
removed. This heuristic certainly incorrectly looses words that are interspersed with quoted
email text. Only words formed as sequences of alphabetic characters are kept, which results
in a vocabulary of 22,901 unique words. To remove sensitivity to capitalization, all text is
downcased.
Our second data set consists of the personal email sent and received by McCallum
between January and September 2004. It consists of 13,633 unique messages written by 825
authors. In a typical power-law behavior, most of these authors wrote only a few messages,
while 128 wrote ten or more emails. After applying the same text normalization filter
(lowercasing, removal of quoted email text, etc.) that was used for the Enron data set, we
obtained a text corpus containing 457,057 word tokens, and a vocabulary of 22,901 unique
words.
256

Topic and Role Discovery in Social Networks

(a) Enron authors

(b) Enron author-recipient pairs

(c) McCallum authors

(d) McCallum author-recipient pairs

Figure 2: Power-law relationship between the frequency of occurrence of of an author (or
an author-recipient pair) and the rank determined by the above frequency of
occurrence. In the author plots, we treat both the sender and the recipients as
authors.

By conditioning topic distributions on author-recipient pairs instead of authors, the data
we have may look sparser considering that we have substantially more author-recipient pairs
than authors. However, as shown in Figure 2, we can find that the number of emails of
an author-recipient pair and its rank determined by the count still follow a power-law
behavior, as for authors. For example, in the McCallum data set, 500 of possible 680,625
author-recipient pairs are responsible for 70% of the email exchange. That is, even though
the data are sparser for the ART model, the power-law behavior makes it still possible to
obtain a good estimation of the topic distributions for prominent author-recipient pairs.
We initialize the Gibbs chains on both data sets randomly, and find that the results are
very robust to different initializations. By checking the perplexity, we find that usually the
Gibbs chain converges after a few hundred iterations, and we run 10,000 iterations anyway
to make sure it converges.
4.1 Topics and Prominent Relations from ART
Table 2 shows the highest probability words from eight topics in an ART model trained
on the 147 Enron users with 50 topics. The quoted titles are our own interpretation of
257

McCallum, Wang, & Corrada-Emmanuel

Topic 5
“Legal Contracts”
section
0.0299
party
0.0265
language
0.0226
contract
0.0203
date
0.0155
enron
0.0151
parties
0.0149
notice
0.0126
days
0.0112
include
0.0111
M.Hain
0.0549
J.Steffes
J.Dasovich
0.0377
R.Shapiro
D.Hyvl
0.0362
K.Ward

Topic 17
“Document Review”
attached
0.0742
agreement
0.0493
review
0.0340
questions
0.0257
draft
0.0245
letter
0.0239
comments
0.0207
copy
0.0165
revised
0.0161
document
0.0156
G.Nemec
0.0737
B.Tycholiz
G.Nemec
0.0551
M.Whitt
B.Tycholiz
0.0325
G.Nemec

Topic 27
“Time Scheduling”
day
0.0419
friday
0.0418
morning
0.0369
monday
0.0282
office
0.0282
wednesday
0.0267
tuesday
0.0261
time
0.0218
good
0.0214
thursday
0.0191
J.Dasovich
0.0340
R.Shapiro
J.Dasovich
0.0289
J.Steffes
C.Clair
0.0175
M.Taylor

Topic 45
“Sports Pool”
game
0.0170
draft
0.0156
week
0.0135
team
0.0135
eric
0.0130
make
0.0125
free
0.0107
year
0.0106
pick
0.0097
phillip
0.0095
E.Bass
0.3050
M.Lenhart
E.Bass
0.0780
P.Love
M.Motley
0.0522
M.Grigsby

Topic 34
“Operations”
operations
0.0321
team
0.0234
office
0.0173
list
0.0144
bob
0.0129
open
0.0126
meeting
0.0107
gas
0.0107
business
0.0106
houston
0.0099
S.Beck
0.2158
L.Kitchen
S.Beck
0.0826
J.Lavorato
S.Beck
0.0530
S.White

Topic 37
“Power Market”
market
0.0567
power
0.0563
price
0.0280
system
0.0206
prices
0.0182
high
0.0124
based
0.0120
buy
0.0117
customers
0.0110
costs
0.0106
J.Dasovich
0.1231
J.Steffes
J.Dasovich
0.1133
R.Shapiro
M.Taylor
0.0218
E.Sager

Topic 41
“Government Relations”
state
0.0404
california
0.0367
power
0.0337
energy
0.0239
electricity
0.0203
davis
0.0183
utilities
0.0158
commission
0.0136
governor
0.0132
prices
0.0089
J.Dasovich
0.3338
R.Shapiro
J.Dasovich
0.2440
J.Steffes
J.Dasovich
0.1394
R.Sanders

Topic 42
“Wireless”
blackberry
0.0726
net
0.0557
www
0.0409
website
0.0375
report
0.0373
wireless
0.0364
handheld
0.0362
stan
0.0282
fyi
0.0271
named
0.0260
R.Haylett
0.1432
T.Geaccone
T.Geaccone 0.0737
R.Haylett
R.Haylett
0.0420
D.Fossum

Table 2: An illustration of several topics from a 50-topic run for the Enron email data set.
Each topic is shown with the top 10 words and their corresponding conditional
probabilities. The quoted titles are our own summary for the topics. Below are
prominent author-recipient pairs for each topic. For example, Mary Hain was an
in-house lawyer at Enron; Eric Bass was the coordinator of a fantasy football league
within Enron. In the “Operations” topic it is satisfying to see Beck, who was the
Chief Operating Officer at Enron; Kitchen was President of Enron Online; and
Lavorato was CEO of Enron America. In the “Government Relations” topic, we
see Dasovich, who was a Government Relation Executive, Shapiro, who was Vice
President of Regulatory Affairs, Steffes, who was Vice President of Government
Affairs, and Sanders, who was Vice President of WholeSale Services. In “Wireless”
we see that Haylett, who was Chief Financial Officer and Treasurer, was an avid
user of the Blackberry brand wireless, portable email system.

258

Topic and Role Discovery in Social Networks

a summary for the topics. The clarity and specificity of these topics are typical of the
topics discovered by the model. For example, Topic 17 (Document Review) comes from
the messages discussing review and comments on documents; Topic 27 (Time Scheduling)
comes from the messages negotiating meeting times.
Beneath the word distribution for each topic are the three author-recipient pairs with
highest probability of discussing that topic—each pair separated by a horizontal line, with
the author above the recipient. For example, Hain, the top author of messages in the “Legal
Contracts” topic, was an in-house lawyer at Enron. By inspection of messages related to
“Sports Pool”, Eric Bass seems to have been the coordinator for a fantasy football league
among Enron employees. In the “Operations” topic, it is satisfying to see Beck, who was the
Chief Operating Officer at Enron; Kitchen was President of Enron Online; and Lavorato was
CEO of Enron America. In the “Government Relations” topic, we see Dasovich, who was
a Government Relation Executive, Shapiro, who was Vice President of Regulatory Affairs,
Steffes, who was Vice President of Government Affairs, and Sanders, who was Vice President
of WholeSale Services. In “Wireless” we see that Haylett, who was Chief Financial Officer
and Treasurer, was an avid user of the Blackberry brand wireless, portable email system.
Results on the McCallum email data set are reported in Table 3.
4.2 Stochastic Blockstructures and Roles
The stochastic equivalence hypothesis from SNA states that nodes in a network that behave
stochastically equivalently must have similar roles. In the case of an email network consisting
of message counts, a natural way to measure equivalence is to examine the probability that
a node communicated with other nodes. If two nodes have similar probability distribution
over their communication partners, we should consider them role-equivalent. Lacking a true
distance measure between probability distributions, we can use some symmetric measure,
such as the Jensen-Shannon (JS) divergence, to obtain a symmetric matrix relating the
nodes in the network. Since we want to consider nodes/users that have a small JS divergence
as equivalent, we can use the inverse of the divergence to construct a symmetric matrix in
which larger numbers indicate higher similarity between users.
Standard recursive graph-cutting algorithms on this matrix can be used to cluster users,
rearranging the rows/columns to form approximately block-diagonal structures. This is the
familiar process of ‘blockstructuring’ used in SNA. We perform such an analysis on two
data sets: a small subset of the Enron users consisting mostly of people associated with the
Transwestern Pipeline Division within Enron, and the entirety of McCallum’s email.
We begin with the Enron TransWestern Pipeline Division. Our analysis here employed
a “closed-universe” assumption—only those messages traded among considered authors in
the data set were used.
The traditional SNA similarity measure (in this case JS divergence of distributions on
recipients from each person) is shown in the left matrix in Figure 3. Darker shading indicates
that two users are considered more similar. A related matrix resulting from our ART model
(JS divergence of recipient-marginalized topic distributions for each email author) appears
in the middle of Figure 3. Finally, the results of the same analysis using topics from the
AT model rather than our ART model can be seen on the right. The three matrices are
similar, but have interesting differences.
259

McCallum, Wang, & Corrada-Emmanuel

Topic 5
“Grant Proposals”
proposal
0.0397
data
0.0310
budget
0.0289
work
0.0245
year
0.0238
glenn
0.0225
nsf
0.0209
project
0.0188
sets
0.0157
support
0.0156
smyth
0.1290
mccallum
mccallum
0.0746
stowell
mccallum
0.0739
lafferty
mccallum
0.0532
smyth
pereira
0.0339
lafferty

Topic 31
“Meeting Setup”
today
0.0512
tomorrow
0.0454
time
0.0413
ll
0.0391
meeting
0.0339
week
0.0255
talk
0.0246
meet
0.0233
morning
0.0228
monday
0.0208
ronb
0.0339
mccallum
wellner
0.0314
mccallum
casutton
0.0217
mccallum
mccallum
0.0200
casutton
mccallum
0.0200
wellner

Topic 38
“ML Models”
model
0.0479
models
0.0444
inference
0.0191
conditional
0.0181
methods
0.0144
number
0.0136
sequence
0.0126
learning
0.0126
graphical
0.0121
random
0.0121
casutton
0.0498
mccallum
icml04-webadmin 0.0366
icml04-chairs
mccallum
0.0343
casutton
nips04workflow
0.0322
mccallum
weinman
0.0250
mccallum

Topic 41
“Friendly Discourse”
great
0.0516
good
0.0393
don
0.0223
sounds
0.0219
work
0.0196
wishes
0.0182
talk
0.0175
interesting
0.0168
time
0.0162
hear
0.0132
mccallum
0.0558
culotta
mccallum
0.0530
casutton
mccallum
0.0274
ronb
mccallum
0.0255
saunders
mccallum
0.0181
pereira

Table 3: The four topics most prominent in McCallum’s email exchange with Padhraic
Smyth, from a 50-topic run of ART on 9 months of McCallum’s email. The topics provide an extremely salient summary of McCallum and Smyth’s relationship
during this time period: they wrote a grant proposal together; they set up many
meetings; they discussed machine learning models; they were friendly with each
other. Each topic is shown with the 10 highest-probability words and their corresponding conditional probabilities. The quoted titles are our own summary for
the topics. Below are prominent author-recipient pairs for each topic. The people
other than smyth also appear in very sensible associations: stowell is McCallum’s
proposal budget administrator; McCallum also wrote a proposal with John Lafferty and Fernando Pereira; McCallum also sets up meetings, discusses machine
learning and has friendly discourse with his graduate student advisees: ronb, wellner,
casutton, and culotta; he does not, however, discuss the details of proposal-writing
with them.

Consider Enron employee Geaccone (user 9 in all the matrices in Figure 3). According
to the traditional SNA role measurement, Geaccone and McCarty (user 8) have very similar
roles, however, both the AT and ART models indicate no special similarity. Inspection of
the email messages for both users reveals that Geaconne was an Executive Assistant, while
McCarty was a Vice-President—rather different roles—and, thus the output of ART and
AT is more appropriate. We can interpret these results as follows: SNA analysis shows that
they wrote email to similar sets of people, but the ART analysis illustrates that they used
very different language when they wrote to these people.
260

Topic and Role Discovery in Social Networks

16 : teb.lokey
15 : steven.harris
14 : kimberly.watson
13 : paul.y’barbo
12 : bill.rapp
11 : kevin.hyatt
10 : drew.fossum
9 : tracy.geaccone
8 : danny.mccarty
7 : shelley.corman
6 : rod.hayslett
5 : stanley.horton
4 : lynn.blair
3 : paul.thomas
2 : larry.campbell
1 : joe.stepenovitch

16

16

15

15

14

14

13

13

12

12

11

11

10

10

9

9

8

8

7

7

6

6

5

5

4

4

3

3

2

2

1

1 2 3 4 5 6 7 8 910111213141516

1
1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16

1

2

3

4

5

6

7

8

9 10 11 12 13 14 15 16

Figure 3: Left: SNA Inverse JS Network. Middle: ART Inverse JS Network. Right: AT
Inverse JS Network. Darker shades indicate higher similarity.

Comparing ART against AT, both models provide similar role distance for Geaccone
versus McCarty, but ART and AT show their differences elsewhere. For example, AT
indicates a very strong role similarity between Geaconne and Hayslett (user 6), who was
her boss (and CFO & Vice President in the Division); on the other hand, ART more
correctly designates a low role similarity for this pair—in fact, ART assigns low similarity
between Geaconne and all others in the matrix, which is appropriate because she is the only
executive assistant in this small sample of Enron employees.
Another interesting pair of people is Blair (user 4) and Watson (user 14). ART predicts
them to be role-similar, while the SNA and AT models do not. ART’s prediction seems more
appropriate since Blair worked on “gas pipeline logistics” and Watson worked on “pipeline
facility planning”, two very similar jobs.
McCarty, a Vice-President and CTO in the Division, also highlights differences between
the models. The ART model puts him closest to Horton (user 5), who was President of the
Division. AT predicts that he is closest to Rapp (user 12), who was merely a lawyer that
reviewed business agreements, and also close to Harris (user 15), who was only a mid-level
manager.
Using ART in this way emphasizes role similarity, but not group membership. This
can be seen by considering Thomas (user 3, an energy futures trader), and his relation
to both Rapp (user 12, the lawyer mentioned above), and Lokey (user 16, a regulatory
affairs manager). These three people work in related areas, and both ART and AT fittingly
indicate a role similarity between them, (ART marginally more so than AT). On the other
hand, traditional SNA results (Figure 3 left) emphasizes group memberships rather than
role similarity by placing users 1 through 3 in a rather distinct block structure; they are the
only three people in this matrix who were not members of the Enron Transwestern Division
group, and these three exchanged more email with each other than with the people of the
Transwestern Division. In separate work we have also developed the Group-Topic (GT)
model, which explicitly discovers groups in a way that leverages accompanying text (Wang
et al., 2006). In the future we may also develop a model that integrates both ART and
SNA metrics to jointly model both role and group memberships.
Based on the above examples, and other similar examples, we posit that the ART model
is more appropriate than SNA and AT in predicting role similarity. We thus would claim
261

McCallum, Wang, & Corrada-Emmanuel

120

100

80

60

40

20

0
0

20

40

60

80

100

120

Figure 4: SNA Inverse JS Network for a 10 topic run on McCallum Email Data. Darker
shades indicate higher similarity. Graph partitioning was calculated with the 128
authors that had ten or more emails in McCallum’s Email Data. The block from
0 to 30 are people in and related to McCallum’s research group at UMass. The
block from 30 to 50 includes other researchers around the world.

that the ART model yields more appropriate results than the SNA model in predicting
role-equivalence between users, and somewhat better than the AT model in this capacity.
We also carried out this analysis with the personal email for McCallum to further validate
the difference between the ART and SNA predictions. There are 825 users in this email
corpus, while only 128 wrote ten or more emails. We perform the blockstructure analysis
with these 128 users, shown in Figure 4. The blocks discovered are quite meaningful, e.g.,
the block from 0 to 30 are people in and related to McCallum’s research group at UMass,
and the block from 30 to 50 includes other researchers around the world.
Table 4 shows the closest pairs in terms of JS divergence, as calculated by the ART
model and the SNA model. The difference in quality between the ART and SNA halves of
the table is striking.
Almost all the pairs predicted by the ART model look reasonable while many of those
predicted by SNA are the opposite. For example, ART matches editor and reviews, two
email addresses that send messages managing journal reviews. User mike and mikem are
actually two different email addresses for the same person. Most other coreferent email
262

Topic and Role Discovery in Social Networks

Pairs considered most alike by ART
User Pair
Description
editor reviews
Both journal review management
mike mikem
Same person! (manual coreference error)
aepshtey smucker
Both students in McCallum’s class
coe laurie
Both UMass admin assistants
mcollins tom.mitchell Both ML researchers on SRI project
mcollins gervasio
Both ML researchers on SRI project
davitz freeman
Both ML researchers on SRI project
mahadeva pal
Both ML researchers, discussing hiring
kate laurie
Both UMass admin assistants
ang joshuago
Both on organizing committee for a conference
Pairs considered most alike by SNA
User Pair
Description
aepshtey rasmith
Both students in McCallum’s class
donna editor
Spouse is unrelated to journal editor
donna krishna
Spouse is unrelated to conference organizer
donna ramshaw
Spouse is unrelated to researcher at BBN
donna reviews
Spouse is unrelated to journal editor
donna stromsten
Spouse is unrelated to visiting researcher
donna yugu
Spouse is unrelated grad student
aepshtey smucker
Both students in McCallum’s class
rasmith smucker
Both students in McCallum’s class
editor elm
Journal editor and its Production Editor
Table 4: Pairs considered most alike by ART and SNA on McCallum email. All pairs
produced by the ART model are accurately quite similar. This is not so for the
top SNA pairs. Many users are considered similar by SNA merely because they
appear in the corpus mostly sending email only to McCallum. However, this
causes people with very different roles to be incorrectly declared similar—such as
McCallum’s spouse and the JMLR editor.

addresses were pre-collapsed by hand during preprocessing; here ART has pointed out a
mistaken omission, indicating the potential for ART to be used as a helpful component
of an automated coreference system. Users aepshtey and smucker were students in a class
taught by McCallum. Users coe, laurie and kate are all UMass CS Department administrative
assistants; they rarely send email to each other, but they write about similar things. User
ang is Andrew Ng from Stanford; joshuago is Joshua Goodman of Microsoft Research; they
are both on the organizing committee of a new conference along with McCallum.
On the other hand, the pairs declared most similar by the SNA model are mostly
extremely poor. Most of the pairs include donna, and indicate pairs of people who are
similar only because in this corpus they appeared mostly sending email only to McCallum,
and not others. User donna is McCallum’s spouse. Other pairs are more sensible. For
263

McCallum, Wang, & Corrada-Emmanuel

User Pair
editor reviews
jordan mccallum
mccallum vanessa
croft mccallum
mccallum stromsten
koller mccallum
dkulp mccallum
blei mccallum
mccallum pereira
davitz mccallum

Description
Both journal editors
Both ML researchers
A grad student working in IR
Both UMass faculty, working in IR
Both ML researchers
Both ML researchers
Both UMass faculty
Both ML researchers
Both ML researchers
Both working on an SRI project

Table 5: Pairs with the highest rank difference between ART and SNA on McCallum email.
The traditional SNA metric indicates that these pairs of people are different, while
ART indicates that they are similar. There are strong relations between all pairs.

example, aepshtey, smucker and rasmith were all students in McCallum’s class. User elm is
Erik Learned-Miller who is correctly indicated as similar to editor since he was the Production
Editor for the Journal of Machine Learning Research.
To highlight the difference between the SNA and ART predictions, we present Table 5,
which was obtained by using both ART and SNA to rank the pairs of people by similarity,
and then listing the pairs with the highest rank differences between the two models. These
are pairs that SNA indicated were different, but ART indicated were similar. In every case,
there are role similarities between the pairs.
4.3 Perplexity Comparison between AT and ART
Models for natural languages are often evaluated by perplexity as a measure of the goodness
of fit of models. The lower perplexity a language model has, the better it predicts the unseen
words given the words we previously saw.
The perplexity of a previously unseen message d consisting of words wd can be defined
as follows, when the author ad and the recipient(s) rd are given:


log(p(wd |ad , rd ))
Perplexity(wd ) = exp −
,
Nd
where (θ̂ and φ̂ defined in Equation 1)
p(wd |ad , rd ) =

Nd
Y
i=1

T
1 XX
θ̂ad rt ψ̂twdi
|rd | r∈r
d

!
.

t=1

We randomly split our data sets into a training set (9/10) and a test set (the remaining
1/10). In the test sets, 92.37% (Enron) and 84.51% (McCallum) of the author-recipient pairs
also appear in the training sets. Ten Markov chains are run with different initializations,
264

Topic and Role Discovery in Social Networks

(a) Enron data set

(b) McCallum data set

Figure 5: Perplexity comparison of AT and ART on two data sets. We plot the information
rate (logarithm of perplexity) here. The difference between AT and ART is
significant under one-tailed t-test (Enron data set: p-value < 0.01 except for 10
topics with p-value = 0.018; McCallum data set: p-value < 1e − 5).

and the samples at the 2000th iteration are used to estimate θ̂ and φ̂ by Equation 1. We
report the average information rate (logarithm of perplexity) with different number of topics
on two data sets in Figure 5.
As clearly shown in the figure, ART has significantly better predictive power than AT
over a large number of randomly selected test documents on both data sets under onetailed t-test. Particularly on the Enron data set, ART uses much fewer number of topics
to achieve the best predictive performance. We can also find that the lowest perplexity
obtained by ART is not achievable by AT with any parameter setting on both data sets.
Both these results provide evidence that ART discovers meaningful topics in the context of
a social network and is indeed more appropriate to message data than AT.
Here we do not compare perplexity between ART and LDA, however AT (which ART
dominates in perplexity) has already been shown to have better perplexity than LDA
(Rosen-Zvi, Griffiths, Smyth, & Steyvers, 2005). Due to the much simpler model structure,
the author model (McCallum, 1999) has much worse perplexity. Measured on both data
sets, the information rates (log perplexity) are larger than 10, whereas ART’s information
rates are mostly between 8 and 9.

5. Role-Author-Recipient-Topic Models
To better explore the roles of authors, an additional level of latent variables can be introduced to explicitly model roles. Of particular interest is capturing the notion that a person
can have multiple roles simultaneously—for example, a person can be both a professor and
a mountain climber. Each role is associated with a set of topics, and these topics may
overlap. For example, professors’ topics may prominently feature research, meeting times,
grant proposals, and friendly relations; climbers’ topics may prominently feature mountains,
climbing equipment, and also meeting times and friendly relations.
265

McCallum, Wang, & Corrada-Emmanuel

Role-Author-Recipient-Topic
Model 1
(RART1)
rd

ad

γ

x
γ

ψ

Role-Author-Recipient-Topic
Model 2
(RART2)

g

ψ

ad

rd

gd

hd

θ

α

θ

α

z

R,R

β

w
T

rd

gd

hd

h

z

φ

ψ

ad

A

h

R,R

β

γ

A

A

α

Role-Author-Recipient-Topic
Model 3
(RART3)

Nd

θ

z

R,R

φ

β

w
T

Nd

D

D

φ

w
T

Nd
D

Figure 6: Three possible variants for the Role-Author-Recipient-Topic (RART) model.

We incorporate into the ART model a new set of variables that take on values indicating
role, and we term this augmented model the Role-Author-Recipient-Topic (RART) model.
In RART, authors, roles and message-contents are modeled simultaneously. Each author
has a multinomial distribution over roles. Authors and recipients are mapped to some
role assignments, and a topic is selected based on these roles. Thus we have a clustering
model, in which appearances of topics are the underlying data, and sets of correlated topics
gather together clusters that indicate roles. Each sender-role and recipient-role pair has
a multinomial distribution over topics, and each topic has a multinomial distribution over
words.
As shown in Figure 6, different strategies can be employed to incorporate the “role”
latent variables. First in RART1, role assignments can be made separately for each word in
a document. This model represents that a person can change role during the course of the
email message. In RART2, on the other hand, a person chooses one role for the duration of
the message. Here each recipient of the message selects a role assignment, and then for each
word, a recipient (with corresponding role) is selected on which to condition the selection
of topic. In RART3, the recipients together result in the selection of a common, shared
role, which is used to condition the selection of every word in the message. This last model
may help capture the fact that a person’s role may depend on the other recipients of the
message, but also restricts all recipients to a single role.
We describe the generative process of RART1 in this paper in detail, and leave the other
two for exploration elsewhere. In its generative process for each message, an author, ad ,
and a set of recipients, rd , are observed. To generate each word, a recipient, x, is chosen
at uniform from rd , and then a role g for the author, and a role h for the recipient x are
chosen from two multinomial role distributions ψad and ψx , respectively. Next, a topic z is
chosen from a multinomial topic distribution θgh , where the distribution is specific to the
266

Topic and Role Discovery in Social Networks

Role 3
“IT Support at UMass CS”
olc (lead Linux sysadmin)
gauthier (sysadmin for CIIR group)
irsystem (mailing list CIIR sysadmins)
system (mailing list for dept. sysadmins)
allan (prof., chair of computing committee)
valerie (second Linux sysadmin)
tech (mailing list for dept. hardware)
steve (head of dept. of IT support)

0.2730
0.1132
0.0916
0.0584
0.0515
0.0385
0.0360
0.0342

Role 4
“Working on the SRI CALO Project”
pereira (prof. at UPenn)
0.1876
claire (UMass CS business manager)
0.1622
israel (lead system integrator at SRI)
0.1140
moll (prof. at UMass)
0.0431
mgervasio (computer scientist at SRI)
0.0407
melinda.gervasio (same person as above) 0.0324
majordomo (SRI CALO mailing list)
0.0210
collin.evans (computer scientist at SRI)
0.0205

Table 6: An illustration of two roles from a 50-topic, 15-group run for the McCallum email
data set. Each role is shown with the most prominent users (their short descriptions in parenthesis) and the corresponding conditional probabilities. The quoted
titles are our own summary for the roles. For example, in Role 3, the users are all
employees (or mailing lists) of the IT support staff at UMass CS, except for allan,
who, however, was the professor chairing the department’s computing committee.

author-role recipient-role pair (g, h). Finally, the word w is generated by sampling from a
topic-specific multinomial distribution φz .
In the RART1 model, given the hyper-parameters α, β and γ, an author ad , and a set
of recipients rd for each message d, the joint distribution of the topic mixture θij for each
author-role recipient-role pair (i, j), the role mixture ψk for each author k, the word mixture
φt for each topic t, a set of recipients x, a set of sender roles g, a set of recipient roles h, a
set of topics z and a set of words w is given by (we define R as the number of roles):
P (Θ, Φ, Ψ, x, g, h, z, w|α, β, γ, a, r)
=

R Y
R
Y
i=1 j=1

p(θij |α)

T
Y
t=1

p(φt |β)

A
Y
k=1

p(ψk |γ)

Nd
D Y
Y

P (xdi |rd )P (gdi |ad )P (hdi |xdi )P (zdi |θgdi hdi )P (wdi |φzdi )

d=1 i=1

Integrating over Ψ, Θ and Φ, and summing over x, g, h and z, we get the marginal
distribution of a corpus, similar to what we showed for ART.
To perform inference on RART models, the Gibbs sampling formulae can be derived in
a similar way as in Appendix A, but in a more complex form.

6. Experimental Results with RART
Extensive experiments have been conducted with the RART1 model. Because we introduce
two sets of additional latent variables (author role and recipient role), the sampling procedure at each iteration is significantly more complex. To make inference more efficient,
we can instead perform it in two distinct parts. One strategy we have found useful is to
first train an ART model, and use a sample to obtain topic assignments and recipient assignments for each word token. Then, in the next stage, we treat topics and recipients as
observed (locked). Although such a strategy may not be recommended for arbitrary graphical models, we feel this is reasonable here because we find that a single sample from Gibbs
267

McCallum, Wang, & Corrada-Emmanuel

Role
Role
Role
Role
Role

allan (James Allan)
10 (grant issues)
13 (UMass CIIR group)
2 (natural language researcher)
3 (IT Support at UMass CS)
4 (working on SRI CALO Project)

0.4538
0.2813
0.0768
0.0326
0.0306

Role
Role
Role
Role
Role

pereira (Fernando Pereira)
2 (natural language researcher)
4 (working on SRI CALO Project)
6 (proposal writing)
10 (grant issues)
8 (guests at McCallum’s house)

0.5749
0.1519
0.0649
0.0444
0.0408

Table 7: An illustration of the role distribution of two users from a 50-topic, 15-group run
for the McCallum email data set. Each user is shown with his most prominent
roles (their short descriptions in parenthesis) and the corresponding conditional
probabilities. For example, considering user pereira (Fernando Pereira), his top
five role assignments are all appropriate, as viewed through McCallum’s email.

sampling on the ART model yields good assignments. The following results are based on a
15-group, 50-topic run of RART1 on McCallum email data set.
Our results show that the RART model does indeed automatically discover meaningful
person-role information by its explicit inclusion of a role variable. We show the most
prominent users in two roles in Table 6. For instance, the users most prominent in Role 3
are all employees (or mailing lists) of the IT support staff at UMass CS, except for allan,
who, however, was the professor chairing the department’s computing committee. Role
4 seems to represent “working on the SRI CALO project.” Most of its top prominent
members are researchers working on CALO project, many of them at SRI. The sender
majordomo sends messages from an SRI CALO mailing list. Users claire and moll were,
however, unrelated with the project, and we do not know the reason they appear in this
role. The users mgervasio and melinda.gervasio are actually the same person; satisfyingly
RART found that they have very similar role distributions.
One objective of the RART model is to capture the multiple roles that a person has.
The role distribution of two users are shown in Table 7. For example, user allan (James
Allan) mentioned above has a role in “IT support,” but also has a role as a “member of the
Center for Intelligent Information Retrieval,” as a “grant proposal writer,” and as a “natural
language researcher.” Although not a member of the “SRI CALO Project,” allan’s research
is related to CALO, and perhaps this is the reason that CALO appears (weakly) among his
roles. Consider also user pereira (Fernando Pereira); his top five role assignments are all
exactly appropriate, as viewed through McCallum’s email.
As expected, one can observe interesting differences in the sender versus recipient topic
distributions associated with each role. For instance, in Role 4 “SRI CALO,” the top three
topics for a sender role are Topic 27 “CALO information,” Topic 11 “mail accounts,” and
Topic 36 “program meetings,” but for its recipient roles, most prominent are Topic 48 “task
assignments,” Topic 46 “a CALO-related research paper,” and Topic 40 “java code”.

7. Conclusions
We have presented the Author-Recipient-Topic model, a Bayesian network for social network
analysis that discovers discussion topics conditioned on the sender-recipient relationships in
268

Topic and Role Discovery in Social Networks

a corpus of messages. To the best of our knowledge, this model combines for the first time
the directionalized connectivity graph from social network analysis with the clustering of
words to form topics from probabilistic language modeling.
The model can be applied to discovering topics conditioned on message sending relationships, clustering to find social roles, and summarizing and analyzing large bodies of
message data. The model would form a useful component in systems for routing requests,
expert-finding, message recommendation and prioritization, and understanding the interactions in an organization in order to make recommendations about improving organizational
efficiency.
The Role-Author-Recipient-Topic (RART) models explicitly capture the multiple roles
of people, based on messages sent and received. Future work will develop models that
explicitly capture both roles and groups.

Acknowledgments
Some of the material in this paper was presented in part at the 19th International Joint Conference on Artificial Intelligence (IJCAI 2005) in Edinburgh, Scotland, July 30–August 5,
2005. This work was supported in part by the Center for Intelligent Information Retrieval,
the Central Intelligence Agency, the National Security Agency, the National Science Foundation under NSF grant #IIS-0326249, and by the Defense Advanced Research Projects
Agency, through the Department of the Interior, NBC, Acquisition Services Division, under
contract #NBCHD030010.

Appendix A. Gibbs Sampling Derivation for ART
We need to derive P (xdi , zdi |x−di , z−di , w, α, β, a, r), the conditional distribution of a topic
and recipient for the word wdi given all other words’ topic and recipient assignments, x−di
and z−di , to carry out the Gibbs sampling procedure for ART. We begin with the joint
probability of the whole data set. Note here that we can take advantage of conjugate priors
to simplify the integrals.

=

P (x, z, w|α, β, a, r)
ZZ Y
Nd
A Y
A
T
D Y
Y
Y
p(θij |α)
p(φt |β)
P (xdi |rd ) · P (zdi |θad xdi )P (wdi |φzdi )dΦdΘ
t=1

i=1 j=1

=

∝

d=1 i=1

! A A T
P
D
T
Y
Γ( Tt=1 αt ) Y αt −1 Y Y Y nijt
1 Nd
(
)
θijt
θijt dΘ
QT
|rd |
t=1 Γ(αt ) t=1
i=1 j=1
i=1 j=1 t=1
d=1
! T V
P
Z Y
T
V
Γ( Vv=1 βv ) Y βv −1 Y Y mtv
×
φtv
φtv dΦ
QV
v=1 Γ(βv ) v=1
t=1
t=1 v=1
A Z Y
T
T Z Y
V
A Y
Y
Y
αt +nijt −1
θijt
dθij
φβtvv +mtv −1 dφt
Z Y
A Y
A

i=1 j=1

t=1

t=1

v=1

269

McCallum, Wang, & Corrada-Emmanuel

QV
T
A Y
A QT
Y
Y
v=1 Γ(βv + mtv )
t=1 Γ(αt + nijt )
∝
PT
P
V
i=1 j=1 Γ( t=1 (αt + nijt )) t=1 Γ( v=1 (βv + mtv ))
where |rd | is the number of recipients in message d, nijt is the number of tokens assigned to
topic t and the author-recipient pair (i, j), and mtv represent the number of tokens of word
v assigned to topic t.
Using the chain rule, we can obtain the conditional probability conveniently. We define
w−di as all word tokens except the token wdi .

=

∝

P (xdi , zdi |x−di , z−di , w, α, β, a, r)
P (xdi , zdi , wdi |x−di , z−di , w−di , α, β, a, r)
P (x, z, w|α, β, a, r)
∝
P (wdi |x−di , z−di , w−di , α, β, a, r)
P (x−di , z−di , w−di |α, β, a, r)
Γ(βwdi +mzdi wdi )
Γ(αzdi +nad xdi zdi )
Γ(αzdi +nad xdi zdi −1)
Γ(βwdi +mzdi wdi −1)
P
P
Γ( T
Γ( V
(αt +nad xdi t ))
(βv +mzdi v ))
PT t=1
PV v=1
Γ( t=1 (αt +nad xdi t )−1) Γ( v=1 (βv +mzdi v )−1)

αz + nad xdi zdi − 1
βw + mzdi wdi − 1
∝ PT di
PV di
t=1 (αt + nad xdi t ) − 1
v=1 (βv + mzdi v ) − 1

If one wants, further manipulation can turn the above formula into separated update
equations for the topic and recipient of each token, suitable for random or systematic scan
updates:
P (xdi |x−di , z, w, α, β, a, r) ∝
P (zdi |x, z−di , w, α, β, a, r) ∝

αz + nad xdi zdi − 1
PT di
t=1 (αt + nad xdi t ) − 1
βw + mzdi wdi − 1
αz + nad xdi zdi − 1
PT di
PV di
t=1 (αt + nad xdi t ) − 1
v=1 (βv + mzdi v ) − 1

References
Adamic, L., & Adar, E. (2004). How to search a social network. http://arXiv.org/abs/condmat/0310120.
Airoldi, E., Blei, D., Fienberg, S., & Xing, E. (2006). Stochastic blockmodels of mixedmembership: General formulation and nested variational inference. In ICML Workshop on Statistical Network Analysis.
Albert, R., & Barabási, A.-L. (2002). Statistical mechanics of complex networks. Reviews
of Modern Physics, 74 (1), 47–97.
Andrieu, C., de Freitas, N., Doucet, A., & Jordan, M. (2003). An introduction to MCMC
for machine learning. Machine Learning, 50, 5–43.
Blei, D., & Lafferty, J. (2006a). Correlated topic models. In Advances in Neural Information
Processing Systems 18.
Blei, D. M., & Lafferty, J. D. (2006b). Dynamic topic models. In Proceedings of the 23rd
International Conference on Machine Learning.
Blei, D. M., Ng, A. Y., & Jordan, M. J. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 993–1022.
270

Topic and Role Discovery in Social Networks

Erosheva, E., Fienberg, S., & Lafferty, J. (2004). Mixed membership models of scientific
publications. Proceedings of the National Academy of Sciences, 101(Suppl. 1).
Griffiths, T., Steyvers, M., Blei, D., & Tenenbaum, J. (2004). Integrating topics and syntax.
In Advances in Neural Information Processing Systems (NIPS) 17.
Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings of the National
Academy of Sciences, 101 (suppl. 1), 5228–5235.
Hofmann, T. (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42 (1), 177–196.
Holland, P., Laskey, K. B., & Leinhardt, S. (1983). Stochastic blockmodels: Some first
steps. Social Networks, 5, 109–137.
Kemp, C., Tenenbaum, J. B., Griffiths, T. L., Yamada, T., & Ueda, N. (2006). Learning
systems of concepts with an infinite relational model. In Proceedings of the 21st
National Conference on Artificial Intelligence.
Kemp, C., Griffiths, T. L., & Tenenbaum, J. (2004). Discovering latent classes in relational
data. Tech. rep., MIT AI Memo 2004-019.
Kleinberg, J. (2000). Navigation in a small world. Nature, 406, 845.
Kubica, J., Moore, A., Schneider, J., & Yang, Y. (2002). Stochastic link and group detection.
In Proceedings of the 18th National Conference on Artificial Intelligence, pp. 798–804.
Kurihara, K., Kameya, Y., & Sato, T. (2006). A frequency-based stochastic blockmodel.
In Workshop on Information Based Induction Sciences.
Li, W., & McCallum, A. (2006). Pachinko allocation: DAG-structured mixture models of
topic correlations. In Proceedings of the 23rd International Conference on Machine
Learning.
Lorrain, F., & White, H. C. (1971). The structural equivalence of individuals in social
networks. Journal of Mathematical Sociology, 1, 49–80.
McCallum, A. (1999). Multi-label text classification with a mixture model trained by EM.
In the 16th National Conference on Artificial Intelligence Workshop on Text Learning.
Mimno, D., & McCallum, A. (2007). Expertise modeling for matching papers with reviewers.
In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining, pp. 500–509.
Minka, T., & Lafferty, J. (2002). Expectation-propagation for the generative aspect model.
In Proceedings of the 18th Conference on Uncertainty in Artificial Intelligence.
Nowicki, K., & Snijders, T. A. (2001). Estimation and prediction for stochastic blockstructures. Journal of the American Statistical Association, 96 (455), 1077–1087.
271

McCallum, Wang, & Corrada-Emmanuel

Rosen-Zvi, M., Griffiths, T., Smyth, P., & Steyvers, M. (2005). Learning author-topic
models from text corpora. Submitted to Journal of Machine Learning Research.
Rosen-Zvi, M., Griffiths, T., Steyvers, M., & Smyth, P. (2004). The author-topic model
for authors and documents. In Proceedings of the 20th Conference on Uncertainty in
Artificial Intelligence.
Shetty, J., & Adibi, J. (2004). The Enron email dataset database schema and brief statistical
report. Tech. rep., Information Sciences Institute.
Steyvers, M., Smyth, P., Rosen-Zvi, M., & Griffiths, T. (2004). Probabilistic author-topic
models for information discovery. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
Teh, Y. W., Jordan, M. I., Beal, M. J., & Blei, D. M. (2004). Hierarchical Dirichlet processes.
Tech. rep., UC Berkeley Statistics.
Wang, X., & McCallum, A. (2006). Topics over time: A non-markov continuous-time model
of topical trends. In Proceedings of the 12th ACM SIGKDD International Conference
on Know ledge Discovery and Data Mining, pp. 424–433.
Wang, X., McCallum, A., & Wei, X. (2007). Topical n-grams: Phrase and topic discovery, with an application to information retrieval. In Proceedings of the 7th IEEE
International Conference on Data Mining.
Wang, X., Mohanty, N., & McCallum, A. (2006). Group and topic discovery from relations
and their attributes. In Advances in Neural Information Processing Systems 18, pp.
1449–1456.
Wasserman, S., & Faust, K. (1994). Social Network Analysis: Methods and Applications.
Cambridge University Press.
Watts, D. J. (2003). Six Degrees: The Science of a Connected Age. W. W. Norton &
Company.
Watts, D. J., Dodds, P. S., & Newman, M. E. J. (2002). Identify and search in social
networks. Science, 296 (5571), 1302–1305.
Wolfe, A. P., & Jensen, D. (2004). Playing multiple roles: Discovering overlapping roles in
social networks. In the 21st International Conference on Machine Learning Workshop
on Statistical Relational Learning and its Connections to Other Fields.
Wu, F., Huberman, B. A., Adamic, L. A., & Tyler, J. R. (2003). Information flow in social
groups. http://arXiv.org/abs/cond-mat/0305305.

272

Journal of Artificial Intelligence Research 30 (2007) 501-523

Submitted 06/07; published 12/07

On the Semantics of Logic Programs with Preferences
Sergio Greco

greco@deis.unical.it

DEIS, Università della Calabria
via P. Bucci, 87030 Rende - Italy

Irina Trubitsyna

irina@deis.unical.it

DEIS, Università della Calabria
via P. Bucci, 87030 Rende - Italy

Ester Zumpano

zumpano@deis.unical.it

DEIS, Università della Calabria
via P. Bucci, 87030 Rende - Italy

Abstract
This work is a contribution to prioritized reasoning in logic programming in the presence
of preference relations involving atoms. The technique, providing a new interpretation for
prioritized logic programs, is inspired by the semantics of Prioritized Logic Programming
and enriched with the use of structural information of preference of Answer Set Optimization Programming. Specifically, the analysis of the logic program is carried out together
with the analysis of preferences in order to determine the choice order and the sets of
comparable models. The new semantics is compared with other approaches known in the
literature and complexity analysis is also performed, showing that, with respect to other
similar approaches previously proposed, the complexity of computing preferred stable models does not increase.

1. Introduction
The increased interest in preferences is witnessed by an extensive number of proposals and
systems for preference handling (Grell, Konczak, & Schaub, 2005; Van Nieuwenborgh &
Vermeir, 2003; Wakaki, Inoue, Sakama, & Nitta, 2003, 2004). The literature distinguishes
static from dynamic preferences. Static preferences are fixed at the time a theory is specified, i.e. they are “external” to the logic program, whereas dynamic preferences appear
within the logic program and are determined “on the fly”. The most common form of
preference consists in specifying preference conditions among rules (Brewka, 1996; Brewka
& Eiter, 1999, 2000; Delgrande, Schaub, & Tompits, 2000a, 2000b, 2003; Gelfond & Son,
1997; Schauba & Wang, 2001; Van Nieuwenborgh & Vermeir, 2002, 2004; Wang, Zhou, &
Lin, 2000; Zhang & Foo, 1997), whereas, some recent proposals admit the expression of
preference relations among atoms (Brewka, Niemela, & Truszczynski, 2003; Brewka, 2004;
Sakama & Inoue, 2000; Wakaki et al., 2003). More sophisticated forms of preferences also
allow the specification of priorities between conjunctions (disjunctions) of literals (Brewka
et al., 2003; Delgrande et al., 2000a; Sakama & Inoue, 2000) and numerical penalties for
suboptimal options (Brewka, 2004).
This work is a contribution to prioritized reasoning in logic programming in the presence
of preference conditions involving atoms. In particular, priorities are applied by following
the natural ordering defined by dependencies, as proposed in the Answer Set Optimizac
°2007
AI Access Foundation. All rights reserved.

Greco, Trubitsyna, & Zumpano

tion (ASO) semantics (Brewka et al., 2003), and the comparison strategy, proposed in the
Preferred Stable Model (PSM) semantics (Sakama & Inoue, 2000), is reviewed by also introducing the concept of comparable models. The next example describes the intuition at
the basis of the proposed approach.
Example 1 The following prioritized program hP 1 , Φ1 i, inspired by a program presented
by Brewka et al. (2003), describes different menus and the preferences among drinks and
desserts:
P1 :

fish ⊕ beef ←
red ⊕ white ←
pie ⊕ ice-cream ←
← fish, white
← beef, pie
← fish, ice-cream

Φ1 :

%1 : white > red ← fish
%2 : red > white ← beef
%3 : pie > ice-cream ← red

The symbol ⊕ denotes exclusive disjunction, i.e. if the body of the rule is true exactly
one atom in the head is true, whereas a rule with empty head defines a constraint, i.e. a
rule which is satisfied only if the body is false. The first three rules of P 1 select the main
dish, the drink and the dessert; the last three rules are constraints and state that a feasible
solution cannot contain i) fish and white or ii) beef and pie or iii) fish and ice-cream.
Prioritized rules in Φ1 introduce preferences among drinks (%1 , %2 ) and desserts (%3 ).
The program P 1 has three stable models: M1 = {fish, red, pie}, M2 = {beef, white,
ice-cream} and M3 = {beef, red, ice-cream}. The PSM returns M1 as the unique preferred model; whereas the ASO technique, following the natural ordering of preference rules
(%1 and %2 precede %3 ), derives that M3 is the unique solution. Thus, the two approaches
provide different results.
2
The structure of preference rules in the above example suggests that i) fish and beef are
alternative options for the main dish and ii) the choice of drink depends on the selected main
dish and precedes the choice of dessert. The second conclusion is based on the observation
that %1 and %2 provide opposite valuations of the choice of drink and they define two different
classes of models (menus), which should be considered separately. In other words, the model
M1 (associated with the menu containing fish) should not be compared with the models
M2 and M3 (associated with the menus containing beef). Consequently, both M1 and M3
should be preferred.
Observe that in the above example the PSM semantics derives that M1 is the preferred
model as M1 is preferable to M3 (due to the presence of rule %3 ), M3 is preferable to M2
(due to the presence of rule %2 ) and, transitively, that M1 is also preferable to M2 . It is
worth noting that the use of the transitive closure makes the comparison of models much
more complex as two models cannot be compared directly. On the other hand the ASO
semantics is sensitive to syntactic changes of programs. This fact is illustrated by means of
the following example.
Example 2 Consider the prioritized program hP2 , Φ2 i, an extension of the prioritized program defined in Example 1:
502

On the Semantics of Logic Programs with Preferences

P2 :

fish ⊕ beef ←
red ⊕ white ⊕ beer ←
pie ⊕ ice-cream ←
← fish, white
← beef, pie
← fish, ice-cream
← beer

Φ2 :

%b1 : beer > white > red ← fish
%b2 : beer > red > white ← beef
%b3 : pie > ice-cream ← red

This program is equivalent to the one reported in Example 1 as, even if it contains an additional choice (beer), this option is not feasible, for the presence of the constraint ← beer.
The set of stable models associated with the program P2 , coincides with those reported
for the program P 1 in Example 1, and consists of: M1 = {fish, red, pie}, M2 = {beef,
white, ice-cream} and M3 = {beef, red, ice-cream}. Considering the set of preference
rules, note that both preferences regarding the choice of drink have beer as the best option,
but there is no stable model containing it. Intuitively, as the two problems hP 1 , Φ1 i in Example 1 and hP2 , Φ2 i are equivalent they must have the same preferred models. The ASO
semantics is sensitive to the program change and gives M1 and M3 as a solution, whereas,
for the equivalent program in Example 1, the returned preferred model is M3 only. No
change occurs in the set of preferred models for the PSM semantics.
2
Thus, in this paper we present a new semantics, inspired by the PSM and ASO semantics, which seems to better capture the intuitive meaning of programs and avoids the above
mentioned problems.
1.1 Contribution
The paper provides a new semantics for prioritized logic programs enriching the one proposed by Sakama and Inoue (2000) with additional information gained from the structure
of preference rules as proposed by Brewka et al. (2003). In particular, the new semantics
uses different preference relations among stable models and introduces a natural ordering
among preferences that fixes the order of choices, on the basis of the stratification of the
preference program. Each decision is determined by the set of choices belonging to the
corresponding level and provides the subset of models given in input as solution. Once a
decision is made, this output subset becomes the input set of the following decision and so
on. The proposed semantics drives the decision making process taking into account catching
additional information regarding non comparable sets of models so partitioning the set of
models of the program by looking at their alternative decisions. To this end the concept of
incomparability, not taken into account by previous approaches, is introduced.
The paper presents a detailed comparison of the approaches dealing with preference
relations among atoms. Particular attention is devoted to the PSM and the ASO semantics.
The analysis of the complexity of computing preferred answer sets is also performed, showing
that, w.r.t. previous proposals, such as the PSM and the ASO semantics, the complexity
of computing preferred stable models does not increase.
503

Greco, Trubitsyna, & Zumpano

1.2 Plan Of The Paper
The rest of the paper is organized as follows: in Section 2 preliminaries on Disjunctive
and Abductive Logic Programs, Prioritized Logic Programs and Answer Set Optimization
semantics are given; in Section 3 a new interpretation for prioritized logic programs is
presented; in Section 4 complexity results are provided; in Section 5 a comparison of the
presented semantics with the PSM and ASO semantics is performed, and other approaches,
known in the literature, are briefly described; finally, Section 6 outlines our conclusions.

2. Preliminaries
We assume familiarity with relational database theory, disjunctive logic programs, disjunctive deductive databases, (disjunctive) stable model semantics and computational complexity (Eiter, Gottlob, & Mannila, 1997b; Gelfond & Lifschitz, 1988, 1991; Papadimitriou,
1994).
2.1 Background
A (disjunctive) logic program is a finite set of rules r of the form a1 ∨ · · · ∨ ak ← b1 , ..., bm ,
not c1 , ..., not cn with k + m + n > 0, where a1 , ..., ak , b1 , ..., bm , c1 , ..., cn are atoms. The
disjunction a1 ∨ · · · ∨ ak , denoted by head(r), is called head of r, while the conjunction
b1 , ..., bm , not c1 , ..., not cn , denoted by body(r), is called body of r.
The Herbrand Universe UP of a program P is the set of all constants appearing in P 1 ,
and its Herbrand Base BP is the set of all ground atoms constructed from the predicates
appearing in P and the constants from UP . A term (resp. an atom, a rule or a program) is
ground if no variables occur in it. A rule r0 is a ground instance of a rule r, if r0 is obtained
from r by replacing every variable in r with some constant in UP ; ground(P) denotes the
set of all ground instances of the rules in P.
The intuitive meaning of the previous disjunctive rule is that if body(r) is true, i.e.
b1 , ..., bm are true and c1 , ..., cn are false, then head(r) must be true, i.e at least one of
a1 , ..., ak has to be true (otherwise r is not satisfied). Rules with empty head, called denials
or constraints, will be used to define constraints and are satisfied only if the body is false. In
this paper exclusive disjunction, denoted by ⊕, is used in the head; the statement head(r) =
a1 ⊕ ... ⊕ ak is true, if exactly one of a1 , ..., ak is true, i.e. a disjunctive rule of the form
a1 ⊕ · · · ⊕ ak ← body is a shorthand for the rule a1 ∨ · · · ∨ ak ← body and (k × (k − 1)/2)
constraints of the form ← ai , aj , body with 1 ≤ i < j ≤ k. The solution of a logic program P
is given in terms of stable model (answer set) semantics (Gelfond & Lifschitz, 1988, 1991).
An interpretation M for P is a model of P if M satisfies all rules in ground(P). The
minimal model semantics, defined for positive P, assigns to P the set of its minimal models
MM(P), where a model M for P is minimal, if no proper subset of M is a model for P
(Minker, 1982). The more general disjunctive stable model semantics also applies to programs with (unstratified) negation (Gelfond & Lifschitz, 1991). Disjunctive stable model
semantics generalizes stable model semantics, previously defined for normal programs (Gelfond & Lifschitz, 1988). For any interpretation M , denote with P M the ground positive
program derived from ground(P) by i) removing all rules that contain a negated atom
1. We are considering function free programs.

504

On the Semantics of Logic Programs with Preferences

not a in the body and a ∈ M , and ii) removing all negated atoms from the remaining rules.
An interpretation M is a (disjunctive) stable model of P if and only if M ∈ MM(P M ).
For general P, the stable model semantics assigns to P the set SM(P) of its stable models. It is well known that stable models are minimal models (i.e. SM(P) ⊆ MM(P))
and that for negation free programs, minimal and stable model semantics coincide (i.e.
SM(P) = MM(P)).
2.2 Extended And Abductive Programs
Given an atom p(t), a literal is either p(t) or its strong negation ¬p(t). An extended
program is a program where atoms are replaced by literals. The semantics of extended
disjunctive programs can also be given in terms of stable models by considering p and ¬p as
different predicate symbols and considering the implicit constraint ← p(X), ¬p(X) (Gelfond
& Lifschitz, 1991).
Abductive logic programming is an extension of logic programming to perform abductive
reasoning (Kakas, Kowalski, & Toni, 1992; Inoue & Sakama, 1998). An abductive program
(ALP) is a pair hP, Ai, where P is an extended program and A is a set of literals called
abducibles. hP, Ai can be represented by means of an extended program Γ = P ∪ {g(t) ∨
g 0 (t) ← | g(t) ∈ A} ∪ {g 0 (t) ← not g(t) | g(t) ∈ A}.
Let Γ be an ALP and G a ground atom denoting an observation. Then, a set S ⊆ A is an
explanation of G iff there is a stable model M of Γ such that S = M ∩A and G ∈ M ; a stable
model M is A-minimal if there is no stable model N such that N ∩ A ⊂ M ∩ A. Moreover,
S is a minimal explanation (i.e. there is no explanation S 0 ⊂ S) of G iff Γ ∪ {← not G} has
a consistent A-minimal stable model M such that S = M ∩ A (Inoue & Sakama, 1998).
It has been shown that given an ALP P and a ground atom G denoting an observation,
deciding whether there exists a A-minimal explanation S for G is Σp3 -complete (Eiter,
Gottlob, & Leone, 1997a).
The rest of this section will briefly review the two main approaches for prioritizing reasoning we refer to, i.e. Prioritized Logic Programs and Answer Set Optimization, proposed,
respectively, by Sakama and Inoue (2000) and Brewka et al. (2003).
2.3 Prioritized Logic Programs
A (partial) preference relation º among atoms is defined as follows: given two atoms e1
and e2 , the statement e1 º e2 (called priority) means that e1 has higher priority than e2 .
Moreover, if e1 º e2 and e2 º e3 , then e1 º e3 . A priority statement e1 º e2 states that for
each a1 instance of e1 and for each a2 instance of e2 the preference relation a1 º a2 holds.
A prioritized logic program (PLP) is a pair hP, Φi where P is a disjunctive program and Φ
is a set of priorities. Φ∗ denotes the set of priorities which can be reflexively or transitively
derived from Φ.
The statement e1 Â e2 stands for e1 º e2 and e2 6º e1 . Clearly, if e1 Â e2 , the sets of
ground instantiations of e1 and e2 have an empty intersection.
Definition 1 Given a prioritized logic program hP, Φi, the relation w is defined over the
stable models of P as follows. For any stable models M1 , M2 and M3 of P:
505

Greco, Trubitsyna, & Zumpano

1. M1 w M1 ,
2. M1 w M2 if ∃e1 ∈ M1 − M2 , ∃e2 ∈ M2 − M1 such that (e1 º e2 ) ∈ Φ∗ and 6 ∃e3 ∈
M2 − M1 such that (e3 Â e1 ) ∈ Φ∗ ,
3. if M1 w M2 and M2 w M3 , then M1 w M3 .
If M1 w M2 then M1 is preferable to M2 . Moreover, if M1 w M2 and M2 6w M1 then
M1 = M2 .
2
An interpretation M is a preferred stable model of hP, Φi if M is a stable model of P and
N w M implies M w N for any stable model N (equivalently, if there is no interpretation N
which is a stable model of P and N = M ). The set of preferred stable models of hP, Φi will
be denoted by PSM(hP, Φi). Note that the relation Φ1 ⊆ Φ2 between two PLPs hP, Φ1 i
and hP, Φ2 i does not imply PSM(hP, Φ2 i) ⊆ PSM(hP, Φ1 i).
In PLP priority relations are used to express priorities over atoms, whereas priorities over
more general forms of knowledge (conjunctive, disjunctive knowledge, rules, preconditions)
can be expressed by a simple rewriting of the preference program. For instance, a preference
rule with precondition of the form (e1 º e2 ) ← B is expressed in PLP as e01 º e02 , where
e01 ← e1 , B and e02 ← e2 , B.
The semantics of prioritized programs proposed by Sakama and Inoue (2000) will be
denoted by PSM semantics. A sound and complete procedure that allows preferred answer
sets for a PLP program to be computed using a generate and test algorithm has been
proposed by Wakaki et al. (2003). This algorithm translates a PLP program hP, Φi and
any answer set S of the program P into a single logic program T [P, Φ, S], such that its
answer sets are answer sets of P preferable to S. More details can be found in works
presenting the semantics and the implementation (Sakama & Inoue, 2000; Wakaki et al.,
2004).
The complexity of answering queries over PLP programs is at least one level above
the complexity of answering queries over standard programs (without preferences). In
particular, let hP, Φi be a prioritized logic program, then i) deciding the existence of a
preferred stable model is Σp2 -hard; ii) deciding whether an atom is true in some (resp. all)
preferable stable model of hP, Φi is Σp3 -hard (resp. Πp3 -hard). It is worth noting that in the
original work (Sakama & Inoue, 2000) it has been claimed that the complexity is exactly one
level above the complexity of standard programs, but the proof does not take into account
the transitivity property of the preference relation.
2.4 Answer Set Optimization
An answer set optimization program, denoted as ASO program, is a pair hP, Φi, where P is
a disjunctive program, called Generating Program, and Φ is a Preference Program consisting
of a finite set of rules of the form: a1 > · · · > ak ← b1 , ..., bm , not c1 , ..., not cn where bi s and cj s
are literals and ai s are boolean combinations2 of literals; here a literal is either an atom L or
its (strong) negation ¬L. Φ determines a preference ordering on the answer sets described
by the generating program P.
2. A boolean combination is a formula built of atoms by means of disjunctions, conjunctions, and default
negation.

506

On the Semantics of Logic Programs with Preferences

Definition 2 Let hP, Φi be an ASO program where Φ = {%1 , ..., %n } and S be an answer
set of P, then S induces a satisfaction vector VS = (vS (%1 ), ..., vS (%n )) where:
• vS (%j ) = I, if %j is Irrelevant to S, i.e. i) the body of %j is not satisfied in S or ii) the
body of %j is satisfied, but none of the atoms in the head of %j is satisfied in S.
• vS (%j ) = min{i | S |= ai ∧%j = a1 > · · · > ak ← b1 , .., bm , not c1 , .., not cn }, otherwise. 2
In the comparison of models it is assumed that I is equal to 1 (i.e., vS (%j ) = I is
equivalent to vS (%j ) = 1).
Definition 3 Let S1 and S2 be two answer sets, then i) VS1 ≤ VS2 if vS1 (%i ) ≤ vS2 (%i ) for
every i ∈ [1..n]; ii) VS1 < VS2 if VS1 ≤ VS2 and for some i ∈ [1..n] vS1 (%i ) < vS2 (%i ). In these
cases S1 w S2 and S1 = S2 3 , respectively.
A set of literals S is an optimal model of an ASO program hP, Φi if S is an answer set
of P and there is no answer set S 0 of P such that S 0 = S.
2
The complexity of ASO programs depends on the class of generating programs. For
disjunctive programs the complexity of answering queries over ASO programs is exactly
one level above the complexity of answering queries over standard programs (without preferences), i.e. i) deciding the existence of a preferred stable model is Σp2 -complete; ii) deciding
whether a literal is true in some (resp. all) preferable stable model of hP, Φi is Σp3 -complete
(resp. Πp3 -complete).
The strategy is further extended by introducing meta-preferences among preference
rules: a ranked ASO program is a sequence hP, Φ1 , ..., Φn i consisting of a generating program
P and a sequence of pairwise disjoint preference programs Φi . The rank of a rule % ∈
Φ1 ∪ · · · ∪ Φn , denoted rank(%), is the unique integer i for which % ∈ Φi . Given two answer
sets S1 and S2 , S1 wrank S2 if for every preference rule %0 such that vS1 (%0 ) ≤ vS2 (%0 ) does
not hold, there is a rule %00 such that rank(%00 ) < rank(%0 ) and vS1 (%00 ) < vS2 (%00 ).
Moreover, a procedure deriving the natural ordering of the preference rules is introduced.
Firstly, given a preference program Φ, its dependency graph G(Φ) is defined. The atoms
appearing in Φ form the vertex set of G(Φ). There is a directed edge from a vertex b to a
vertex a in G(Φ) if there is a rule % in Φ such that a appears in the head of r and b appears
in the body of r. If the graph G(Φ) is acyclic, there is a natural ranking of its atoms which
can be defined recursively as follows: rank(a) = 0 for every atom a that has no predecessors
in G(Φ); otherwise rank(a) is the maximum of the ranks of all predecessors of a in G(Φ)
incremented by 1. The rank of a preference rule % is then defined as the maximum rank of
atoms in its head.
The standard semantics of ASO programs, where priorities are examined all together,
will be denoted as ASO semantics. The alternative semantics, where priorities are divided
into strata following the natural order, will be denoted by RASO (ranked ASO) semantics.
3. In the original work (Brewka et al., 2003) the symbols ≥ and > are used instead of w and =.

507

Greco, Trubitsyna, & Zumpano

3. Preferred Answer Sets
In this paper a syntax similar to the one proposed by Brewka et al. (2003) is used. Given
two atoms a1 and a2 , the statement a2 > a1 means that a2 has higher priority than a1 . A
(partial) preference relation > among atoms is defined as follows.
Definition 4 A prioritized program Π is a pair hP, Φi where P is a disjunctive program
and Φ is set of preference rules of the form:
a1 > a2 > · · · > ak ← b1 , ..., bm , not c1 , ..., not cn
where k > 1 and a1 , ..., ak , b1 , ..., bm , c1 , ..., cn are atoms.

(1)
2

In the following the head and body of a preference rule % will be denoted by head(%)
and body(%), respectively. A ground prioritized program, denoted by ground(hP, Φi) =
hground(P), ground(Φ)i is a prioritized program, where each rule r ∈ (P ∪ Φ) with variables
is replaced with the set of its ground instances, i.e the set of rules obtained by replacing
variables with constants.
Intuitively, a preference rule % of the form (1) describes the choice among a1 , ..., ak
(choice options) under the condition specified by the body of %. The head of % introduces
the preference order among atoms: ai is preferred to aj for 1 ≤ i < j ≤ k. As % can be
applied only if body(%) is true, the body of % specifies the decisions which have to precede
this choice. For instance, a > c ← b states that if b is true, then a is preferred to c. A
preference rule with exactly two atoms in the head will be called binary preference rule,
whereas preference rules with empty bodies will be called preference facts. A prioritized
program is said to be in binary form if all its preference rules are binary.
The following example, presenting a classical program proposed by Brewka et al. (2003),
will be used as running example.
Example 3 Consider the prioritized program hP 3 , Φ3 i whose stable models define the
menus of a restaurant:
P 3:

fish ⊕ beef ←
red ⊕ white ⊕ beer ←
pie ⊕ ice-cream ←
← beef, pie
← fish, ice-cream

Φ3 :

%1
%2
%3
%4

:
:
:
:

white > red > beer ← fish
red > white ← beef
beer > white ← beef
pie > ice-cream ← beer

The first three rules of P 3 select the main dish, the drink and the dessert; the constraints
state that a feasible solution cannot contain both beef and pie or both fish and ice-cream;
while the rules in Φ3 introduce preferences among drinks and desserts.
The program P 3 has six stable models:
M1 = {fish, white, pie}
M2 = {fish, red, pie}
M3 = {fish, beer, pie}

M4 = {beef, white, ice-cream}
M5 = {beef, red, ice-cream}
M6 = {beef, beer, ice-cream}

Both techniques proposed by Sakama and Inoue (2000) and Brewka et al. (2003) select
the stable models M1 and M5 as preferred ones, but the motivation is different. Indeed,
the PSM semantics states that M1 w M2 w M3 w M6 w M4 and M5 w M4 , whereas the
508

On the Semantics of Logic Programs with Preferences

(R)ASO semantics states that Mi w M2 w M3 , Mi w M6 and Mi w M4 for i ∈ {1, 5}, i.e. in
the (R)ASO semantics the models M1 and M6 are compared directly, whereas in the PSM
semantics the models M1 and M6 are compared transitively.
2
Before presenting the formal semantics of programs, some preliminary definitions are
needed. A preference rule of the form a1 > a2 > · · · > ak ← body is shorthand for the k − 1
binary rules of the form ai > ai+1 ← body, with i ∈ [1..k − 1] and the set of preferences
established by Φ is given by its transitive closure Φ∗ defined as follows:
Definition 5 Given a prioritized program hP, Φi, the (ground) transitive closure of Φ is
Φ∗ = Φ0 ∪ {a > c ← body1 , body2 | a > b ← body1 ∈ Φ∗ ∧ b > c ← body2 ∈ Φ∗ ∧ a 6= c},
where Φ0 is the set of binary preference rules derived from ground(Φ).
2
Thus, Φ∗ is defined as the set of rules explicitly representing the preference relations
among choice options. In Section 4 we will show that any ground prioritized program
Π = hP, Φi can be rewritten into an ‘equivalent’ program Π̂ = hP̂, Φ̂i such that Φ̂∗ contains
a number of rules which is polynomial in the size of Π.
The structure of prioritized programs can be examined in order to establish the precedence relation among choices made. For instance, the presence of the preference rule
a > c ← b suggests that the selection of b precedes the choice between a and c and thus
establishes the precedence relation between {b} and {a, c}. This idea was used by Brewka
et al. (2003) for determining the natural ordering among preference rules. In more detail,
the relational order among atoms appearing in Φ was captured by means of the corresponding dependency graph G(Φ); and the stratification of preference rules was established by
considering their head atoms.
Unfortunately, the natural ordering among the preference rules can be established only if
the corresponding dependency graph is acyclic. Thus, the presence of two rules coffee >
tea ← pie and pie > ice-cream ← coffee in Φ does not admit the stratification of Φ,
as they introduce two “mutually dependent” choices.
The stratification algorithm proposed in this paper overcomes this problem by introducing the concept of “collapsed graph”, which maps to the same node the options of “mutually
dependent” choices.
Given a (ground) prioritized program Π = hP, Φi, GΠ = (V, E) denotes the dependency
graph whose set of nodes consists of all atoms in P ∪ Φ whereas there is an arc from b to a
labeled φ (resp. p) if there is a rule in Φ (resp. P) containing a in the head and b in the
body (resp. b in either the head or the body). As the body of (preference) facts is empty,
we assume that their bodies contain the built-in atom true, so that every fact a ← can be
C
considered as a rule a ← true. GΠ denotes the (acyclic) collapsed dependency graph derived
from GΠ by replacing maximal sets of mutual dependent nodes (i.e. nodes belonging to the
C
same cycle) with a unique node. Clearly, each node in GΠ is associated with a set of nodes
in GΠ .
C

To each node in GΠ it is possible to assign a level as follows:
C

• For each node A in GΠ with input degree zero, level(A) = 0;
509

Greco, Trubitsyna, & Zumpano

C

• For each node A in GΠ with input degree greater than zero,
C
C
level(A) = max{max{level(B)+1|∃(B, Z, φ) in GΠ }, max{level(B)|∃(B, Z, p) in GΠ }}.
Observe that the function level assigns to each node the maximum distance from some
node with input degree zero. The following definition introduces the concept of stratification
for preference rules.
Definition 6 Stratification. Given a (ground) prioritized program Π = hP, Φi, Φ∗ can be
partitioned into hΦ∗ [0], Φ∗ [1], ..., Φ∗ [n]i subprograms (called strata) such that
C

• For each atom a in Π, level(a) = level(A), where A is the node of GΠ associated with a;
• For each rule % in Φ∗ , level(%) = max{ level(a) | a ∈ Body(%) };
• Φ∗ [i] = { % | % ∈ Φ∗ ∧ level(%) = i } consists of all ground preference rules associated
with the level i.
2
The partition of Φ∗ into hΦ∗ [0], Φ∗ [1], ..., Φ∗ [n]i is called stratification.
The above definition of stratification of preference rules establishes the order in which
preferences are applied by considering both P and Φ. Moreover, the assignment of the level
to each rule differs from the one proposed by Brewka et al. (2003) in two main aspects:
the level of atoms is defined by analyzing the collapsed dependency graph and the level
of rules is established by considering body atoms instead of head atoms. A more detailed
comparison of the two approaches will be presented in Section 5.
Example 4 Consider the prioritized program hP 3 , Φ3 i of Example 3. The transitive closure
Φ∗3 consists of the binary preference rules %1,1 : white > red ← fish, %1,2 : red > beer ←
fish and %1,3 : white > beer ← fish, derived from %1 , and the rules %2 , %3 and %4 . Φ∗3 can
be stratified into the two strata Φ∗3 [1] = {%1,1 , %1,2 , %1,3 , %2 , %3 } and Φ∗3 [2] = {%4 }.
2
The structural analysis performed in our approach goes beyond the stratification process
and tries to understand the comparability of models. To this end the concepts of conflicting
preferences and comparable models are introduced.
Two ground (binary) preferences of the form a > b ← body1 and b > a ← body2
are said to be conflicting. For instance, the preferences %1 : white > red ← fish and
%2 : red > white ← beef of Example 1 are conflicting, whereas the preferences %1 and
%02 : red > water ← beef are not. A set of preferences Φ is said to be conflicting if Φ∗
contains two conflicting preference rules.
The intuition on the basis of our approach is clarified in this example. Suppose there
are two conflicting preferences %1 : a > b ← c and %2 : b > a ← d. The two conflicting
preferences %1 and %2 specify the preference between a and b in two different sets of models,
having a or b, characterized by the presence of c and d, respectively. Thus, c and d, (and
atoms on which c and d depend) define the alternative decisions. Once a decision has been
made, the associated solutions (models) are no longer comparable.
A preference rule % is said to be relevant for a stable model M , if % can be used
to compare M with other stable models, that is if M |= body(ρ) and some of its head
510

On the Semantics of Logic Programs with Preferences

atoms belongs to M . Given a prioritized program hP, Φi and a (ground) preference rule
% : a > b ← body ∈ Φ∗ , the set of stable models, for which % is relevant, is SM(P, %) =
{M | M ∈ SM(P) ∧ M |= body ∧ (a ∨ b)}.
Definition 7 Comparable models. Let hP, Φi be a prioritized program, M1 and M2 two
stable models for P and hΦ∗ [0], ..., Φ∗ [n]i be a stratification of Φ∗ , then
1. M1 and M2 are comparable on Φ∗ [0];
2. M1 and M2 are comparable on Φ∗ [i + 1] for i ∈ [1..n], if
(a) they are comparable on Φ∗ [i], and
(b) there do not exist two conflicting preference rules %1 , %2 ∈ Φ∗ [i] such that
M1 ∈ SM(P, ρ1 ) − SM(P, ρ2 ) and M2 ∈ SM(P, ρ2 ) − SM(P, ρ1 ).
2
Observe that, the second condition in the previous definition of comparable models
states that the presence of two conflicting preference rules in a given level i identifies two
sets of models, for which only one of the two conflicting rules is relevant. Two models,
appearing in the different sets have to be considered separately in the next levels. In other
words, two stable models M1 and M2 , having as relevant preferences the conflicting rules
%1 : a > b ← body1 and %2 : b > a ← body2 in a given level i, are not comparable at levels
greater than i, if M1 ∈ SM(P, ρ1 ) − SM(P, ρ2 ) and M2 ∈ SM(P, ρ2 ) − SM(P, ρ1 ), that is
M1 |= body1 ∧ (a ∨ b) ∧ not body2 and M2 |= body2 ∧ (a ∨ b) ∧ not body1 .
Example 5 Consider the stable models M3 = {fish, beer, pie}, M6 = {beef, beer,
ice-cream}, and the set of preference rules Φ∗3 of Example 4. The stable models M3
and M6 are comparable on Φ∗3 [0] by definition, while they are not comparable on Φ∗3 [1],
because %1,3 is relevant only for M3 (as M3 |= fish ∧ (white ∨ beer) ∧ not beef), %3 is
relevant only for M6 (as M6 |= beef ∧ (white ∨ beer) ∧ not fish), and these conflicting
preferences belong to Φ∗3 [0].
2
Fact 1 Let hP, Φi be a prioritized program without conflicting preferences and hΦ∗ [0],
Φ∗ [1], ..., Φ∗ [n]i be the stratification of Φ∗ . Then, any two models M1 and M2 are comparable
on Φ∗ [i], i ∈ [0..n].
2
Proof. The proof of the above fact follows directly from Definition 7.

2

On the basis of Definition 7 the declarative semantics of prioritized logic programs can
now be provided. This new semantics, denoted with PAS (Preferred Answer Sets), is given
by preferred stable models as follows:
Definition 8 Preference between Answer Sets. Given a prioritized program hP, Φi, the
relation w is defined over the stable models of P as follows. For any pair of stable models
M1 and M2 of P, being hΦ∗ [0], Φ∗ [1], ..., Φ∗ [n]i be the stratification of Φ∗ , M1 w M2 if
1. ∃%1 : (e1 > e2 ) ← body1 ∈ Φ∗ [i] such that e1 ∈ M1 − M2 , e2 ∈ M2 − M1 , such that
M1 and M2 are comparable on Φ∗ [i], and %1 is relevant for M1 and M2 , and
511

Greco, Trubitsyna, & Zumpano

2. 6 ∃%2 : (e3 > e4 ) ← body2 ∈ Φ∗ [j], such that j < i, e3 ∈ M2 − M1 , e4 ∈ M1 − M2 and
%2 is relevant for M1 and M2 .
Moreover, M1 is strictly preferable to M2 (M1 = M2 ) if M1 w M2 and M2 6w M1 .

2

Note also that the relation = could be defined directly by replacing the condition j < i
with j ≤ i in Item 2 of the above definition.
Definition 9 Preferred Answer Sets. An interpretation M is a preferred stable model for
a prioritized program hP, Φi if M is a stable model of P and there does not exist a stable
model N such that N = M . The set of preferred stable models for hP, Φi will be denoted
by PAS(hP, Φi).
2
Note that Definition 8 introduces preferences between pairs of models by also considering
additional information gained from the structure of preference rules.
Example 6 Consider the prioritized program hP3 , Φ3 i of Example 3 and the stratification
hΦ∗3 [0], Φ∗3 [1]i presented in Example 4. We have that
• all models are comparable on Φ∗ [0] by definition and
– owing to %1,1 , %1,2 , %1,3 , M1 = M2 = M3 ;
– owing to %2 , %3 , M5 = M4 and M6 = M4 ;
• as %1,3 and %3 are conflicting, models M1 and M3 , for which %1,3 is relevant, cannot
be compared in Φ∗ [1] with models M4 and M6 , for which %3 is relevant.
Therefore, as discussed in Example 5, M3 and M6 are not comparable on Φ∗3 [1] and,
consequently, the preferred models are: M1 , M5 and M6 .
2
In the previous example the stable model M6 is considered as good as M5 because both
have beef as main dish, the best choice of drink (red wine and beer, respectively) and the
same dessert (ice-cream). Observe that both ASO and PSM semantics discard M6 . As
already stated, the ASO semantics deduces that M1 and M5 are preferable to M6 owing to
%3 , while the PSM semantics states that M1 is preferable to M3 and M3 is preferable to
M6 , owing to %1 , %3 .
b 3i
Example 7 Let hP 3 , Φ3 i be the program of Example 3, consider the program hP 3 , Φ
b 3 is derived from Φ3 by replacing %4 with
where Φ

%04 : pie > ice-cream ←
b ∗ [0] = {%1,1 , %1,2 , %1,3 , %2 , %3 , %0 },
The new ground preference program has the unique level Φ
3
4
where %1,1 , %1,2 and %1,3 are derived from %1 , as shown in Example 4. Due to %04 , the following relations also hold: Mi = Mj for i ∈ {1, 2, 3} and j ∈ {4, 5, 6}. Therefore, M1 is the
unique preferred model. The same result is obtained by both PSM and ASO semantics. 2
512

On the Semantics of Logic Programs with Preferences

4. Complexity
This section provides some results concerning the computational complexity of computing
preferred stable models and answering queries under PAS semantics. We consider here data
complexity where the input domain UP consists of the Herbrand universe (we assume that
all constants occurring in Φ also occur in P). Clearly, the size of the Herbrand base BP as
well as the sizes of ground(P) and ground(Φ) are polynomial in the size of UP .
The following results demonstrate that allowing preferences among atoms in the semantics proposed here increases the complexity and expressivity of the language by one level
in the polynomial hierarchy. Thus the use of additional information does not increase the
computational complexity of the proposed approach with respect to the above mentioned
techniques (Brewka et al., 2003; Sakama & Inoue, 2000).
Proposition 1 Let Π = hP, Φi be a prioritized program, then there exists a program Π̂ =
hP̂, Φ̂i equivalent to hground(P), Φ∗ i such that i) the stratification of Φ̂ can be computed in
polynomial time, and ii) hP̂, Φ̂i can be derived from hground(P), Φ∗ i in polynomial time.
Proof. We start by considering the program Π0 = hground(P), Φ0 i, where Φ0 is the binary
C
ground version of Φ. The size of the graphs GΠ0 and GΠ0 is polynomial in the size of Π0
C
and can be computed in polynomial time. As the assignment of levels to nodes in GΠ0 can
be done in polynomial time, the assignment of levels to atoms and rules in Φ0 can also be
done in polynomial time.
Let hΦ0 [0], ..., Φ0 [n]i be the stratification of Φ0 . We generate a new ground prioritized
program hP̂, Φ̂i which is equivalent to hground(P), Φ∗ i and such that the size of Φ̂ is polynomial in the size of Φ0 .
Initially, assign to each ground atom ai appearing in the head of a preference rule in Φ0
a unique index i. Let a1 , ..., ap be the (indexed) atoms appearing in the head of rules in Φ0 ,
P̂ denotes the program ground(P) ∪ P Φ where
P Φ = {b(i, j, l) ← bodyi,j | ai > aj ← bodyi,j ∈ Φ0 [l] } ∪
{b(i, j, l) ← b(i, k, l1), b(k, j, l2), l = max(l1, l2) | i, j, k ∈ [1..p], l1 , l2 ∈ [1..n]}
and b is a new predicate symbol. Then, Φ̂ denotes the new set of ground preference rules
defined as follows:
Φ̂ = {ai > aj ← b(i, j, l) | i, j ∈ [1..p] ∧ l ∈ [0..n] }
The stratification of Φ̂ is obtained by associating to each stratum l the preference rules
whose body atom has the value of the level attribute equal to l, that is Φ̂[l] = {ai > aj ←
b(i, j, l) | ai > aj ← b(i, j, l) ∈ Φ̂}.
In order to show the equivalence between hground(P), Φ∗ i and hP̂, Φ̂i, observe that the
set of stable models of ground(P) and P̂ are ‘equivalent’, i.e. for each M ∈ SM(P̂) there
is a stable model N ∈ SM(ground(P)) such that N = M − {b(i, j, l) | b(i, j, l) ∈ M }
and for each N ∈ SM(ground(P)) there is a stable model M ∈ SM(P̂) such that N =
M − {b(i, j, l) | b(i, j, l) ∈ M }, as the rules in ground(P) do not contain atoms of the form
b(i, j, l) in their bodies.
513

Greco, Trubitsyna, & Zumpano

Moreover, let N be a stable model of ground(P) and M be the corresponding stable
model of P̂ (N ⊆ M ), for each ground preference rule ai > aj ← bodyi,j in Φ∗ [l] whose
body is true in N , there is a ground rule ai > aj ← b(i, j, l) in Φ̂[l] whose body is also true
in M and vice versa. Therefore the two sets Φ̂[l] and Φ∗ [l] are equivalent, for all l ∈ [1..n].
Clearly, the program hP̂, Φ̂i is derived from hground(P), Φ∗ i in polynomial time.
2
In the following, for the sake of simplicity of presentation, we continue to refer to the
program hP, Φi and to the stratification of Φ∗ .
Proposition 2 Let hP, Φi be a prioritized program, M1 and M2 two stable models for P,
and hΦ∗ [0], Φ∗ [1], ..., Φ∗ [n]i a stratification of Φ∗ . The problem of checking whether, for a
given k ≤ n, M1 and M2 are comparable on Φ∗ [0], ..., Φ∗ [k] can be solved in polynomial time.
Proof. Obviously M1 and M2 are comparable in Φ∗ [0]. Assuming that M1 and M2 are
comparable for a given level j < k, M1 and M2 are comparable for the level j + 1 if there
are no two conflicting preference rules ρ1 = a > b ← body1 and ρ2 = b > a ← body2 in Φ∗ [j]
such that M1 |= body1 ∧ (a ∨ b) ∧ not body2 and M2 |= body2 ∧ (a ∨ b) ∧ not body1 . This check
can be done in polynomial time as the number of rules in Φ∗ [j] is polynomial in the size of
UP . Moreover, as the maximum value of k is bounded by the size of Φ∗ (which is bounded
by the set of atoms in BP ), the global complexity is also polynomial.
2
Corollary 1 Let hP, Φi be a prioritized program, M1 and M2 two interpretations for P.
The problem of checking whether M1 = M2 can be solved in polynomial time.
Proof. Straightforward from Definition 8 and Proposition 2.

2

Lemma 1 Let hP, Φi be a prioritized program and M an interpretation for P. The problem
of deciding whether M is a preferred stable model for hP, Φi is in Πp2 .
Proof. Consider the complementary problem of deciding whether M is not a preferred
stable model for hP, Φi. In such a case it is sufficient to first check if M is a stable model.
If M is a stable model it is sufficient to guess an interpretation N and to check that i) N
is a stable model for P and ii) N = M . The check on part i) (as well as the check that
M is a stable model) can be done by means of a N P oracle as the problem of deciding
whether an interpretation is a stable model for a disjunctive program is coN P-complete,
whereas the check on part ii) can be done in polynomial time (see Corollary 1). Therefore,
the complexity of the complementary problem is N P N P and, consequently, the complexity
of the original problem is coN P N P .
2
Theorem 1 Let hP, Φi be a prioritized program. Then
1. Deciding whether a ground atom G is true in some preferred stable models of hP, Φi
is Σp3 -complete;
2. Deciding whether a ground atom G is true in all preferred stable models of hP, Φi is
Πp3 -complete.
514

On the Semantics of Logic Programs with Preferences

Proof. Membership: We first demonstrate that deciding whether G is true in some
preferred stable model of hP, Φi is in Σp3 . This result suffices to prove that the complementary problem, consisting in deciding whether A is true in all preferred stable models, is
Πp3 -complete.
To show the membership it is sufficient to guess an interpretation M containing G and
to check whether M is a preferred stable model. From Lemma 1 the problem of deciding
whether M is a preferred stable model is in Πp2 and can be solved by means of a Σp2
oracle.
Therefore, deciding whether there exists a preferred stable model containing G is in
Σp2
N P = Σp3 .
Hardness: Given an abductive logic program consisting of a disjunctive program P and a
set of abducibles (positive) atoms A, the ground abductive logic program derived prom P
and A is
Γ = ground(P) ∪ { g 0 (t) ← not g(t) | g(t) ∈ ground(A) }
∪ { g 0 (t) ⊕ g(t) ← | g(t) ∈ ground(A) }
Let
Φ = { g 0 (t) > g(t) ← | g(t) ∈ ground(A) }
hΓ, Φi denotes the prioritized program derived from P and A.
For any two stable models M, N ∈ SM(Γ), M w N with respect to Φ means that
a preference p0 (u) > p(u) ← ∈ Φ such that p0 (u) ∈ M and p(u) ∈ N exists and no
preference q 0 (v) > q(v) ← ∈ Φ such that q 0 (v) ∈ N and q(v) ∈ M exists. This implies
that M ∩ ground(A) ⊆ N ∩ ground(A) and, consequently, that preferred stable models are
A-minimal.
Therefore, the problem of deciding whether hΓ, Φi has an A-minimal explanation for a
goal G is equivalent to deciding whether hΓ, Φi has a preferred stable model containing G.
Consequently, as the problem of deciding whether a A-minimal explanation S for G exists
is Σp3 -complete, the problem of deciding whether a preferred stable model M for hΓ, Φi
containing G exits is also Σp3 -hard, whereas the problem of deciding whether all preferred
2
stable models of hP, Φi contain G is Πp3 -hard.
Corollary 2 Let hP, Φi be a disjunction-free, prioritized program. Then deciding
whether a ground atom is true in some (all) preferred stable models of hP, Φi is Σp2 -complete
(Πp2 -complete).
Proof. The complexity is one level lower as the problem of deciding whether an interpretation M is a stable model for a disjunction-free program is polynomial.
2

5. Analysis And Comparison
This section compares the semantics introduced here with the PSM and (R)ASO semantics
and briefly discusses other recently proposed semantics.
The PSM semantics is very elegant and compares pairs of models on the basis of their
common preferences and not on the basis of their degree of satisfaction. It does not consider the natural ordering between preference rules and, in some cases, as in Example 1
and 3, compares (and consequently discards) models which in the PAS approach are not
515

Greco, Trubitsyna, & Zumpano

comparable. An interesting feature of the PSM technique is the application of transitive
property in order to derive additional preference relations among problem solutions so that
new, not immediately visible, preference relations are captured. However, as the test of the
transitive property cannot be performed by a direct comparison of two models, this lies in
a more complex implementation.
The (R)ASO technique is a very powerful tool as it determines the preferred models
by evaluating the degree of satisfaction of all preference rules. Thus, it compares two
models even in the absence of common preferences; and the preference relation between
the two models can be established directly. In more detail, the RASO technique considers
the structure of preference rules by associating a degree of satisfaction to choice options
and introduces a natural ordering among preferences. As in the case of PSM semantics,
the (R)ASO semantics also compares and, consequently, discards models which are not
comparable using the PAS technique. For instance, for the program hP1 , Φ1 i, presented in
the Introduction, RASO discards M1 , having the second best option of drink, even if this is
the unique possible choice in the presence of fish.
More specifically, the preference relation w used in the PSM approach is a preorder
relation as it is reflexive and transitive; it determines equivalent answer set classes and
establishes the partial preference order among the above mentioned classes. Consequently
the preferred answer sets are those appearing in the preferred classes. It should be noted
that PSM semantics requires the use of the transitive property in order to derive, on the
basis of relations obtained by the direct comparison of pairs of models, new preference
relations. On the contrary, the (R)ASO semantics uses a strict preference relation which
is just asymmetric and it does not require the application of the transitive property to
compare solutions.
The PAS semantics, proposed here, compares two solutions on the basis of their common preferences by introducing the concept of comparable models and by considering a
refinement of the natural order among preference choices. Thus, it can be seen as an extension of PSM semantics that also uses additional information derived from the structure of
preference rules, but which instead of comparing models transitively, compares models by
considering the transitive closure of the (ground) preference rules.
A novelty of PAS semantics is the consideration of the structural information of preference rules. It introduces the concept of comparable models in order to avoid comparing
models which (in our opinion) should not be compared as they are associated with alternative decisions. Moreover, it proposes a refinement of the natural order among preferences in
order to define the order of choices. RASO semantics establishes the relational order among
atoms appearing in Φ by means of the corresponding dependency graph G(Φ) and cannot
treat the case of “mutually dependent” choices. The stratification algorithm, proposed in
C
this paper, overcomes this problem by considering the collapsed graph GΠ , which is acyclic
by construction and is not sensitive to “syntactic” changes. Moreover, in RASO semantics
the stratification of preference rules is established by considering their head atoms; whereas
in PAS semantics levels are assigned to rules on the basis of the body atoms, following the
intuition that they describe the contexts of choices. Thus, the stratification proposed here
always assigns preference facts to the first level because the level of a rule is fixed by looking
at the level of body atoms.
Some of the advantages of the adopted approach are clarified by the following example.
516

On the Semantics of Logic Programs with Preferences

Example 8 The problem defined by means of the prioritized program hP8 , Φ8 i consists in
selecting the colors of the trousers and the shirt, having only black or blue trousers (r1 )
and white, yellow or red shirts (r2 ) available. The fashion consultant suggests that blue
trousers are better than black ones (%1 ); a white shirt is better than a yellow shirt (%2 );
and in the case of black trousers a white shirt is preferred to a red one (%3 ). Moreover,
blue trousers do not go with a white shirt (c1 ) and a red shirt does not go with blue
trousers (c2 ).
P8 :

r1
r2
c1
c2

:
:
:
:

black ⊕ blue ←
white ⊕ yellow ⊕ red ←
← blue, white
← red, blue

Φ8 :

%1 : blue > black ←
%2 : white > yellow ←
%3 : white > red ← black

The program P8 has four stable models: M1 = {black, white}, M2 = {black, yellow},
M3 = {blue, yellow} and M4 = {black, red}. In order to define the stratification of
preference rules, both RASO and PAS semantics firstly assign the level to atoms: first level
to blue, black and yellow and second level to white and red. In the second step the
RASO approach, by considering the maximum level of head atoms, assigns %1 to the first
level and %2 and %3 to the second level, whereas PAS defines the level of preferences on the
basis of body atoms and assigns %1 and %2 to the first level and %3 to the second level. Note
that in this case the order of %2 is relevant for determining the preferred models. In fact,
RASO gives only M3 , while PAS returns M1 and M3 as preferred models.
2
A formal comparison of the three semantics can be carried out only for the class of
programs where the specific definition of stratification is not significant. Moreover, as
the PSM semantics is defined only for prioritized programs hP, Φi where Φ consists only
of binary facts (preference rules are rewritten into preference facts), in the following the
comparison is carried out by considering programs whose preference rules consist of only
facts. For such a class of programs the closure Φ∗ used in the PSM and PAS semantics
coincide.
Given a prioritized program hP, Φi we denote with GSEM = (V, ESEM ) a preference
graph on the stable models of P, where V = SM(P) denotes the set of stable models
of P and ESEM denotes the preference relation = defined by the semantics SEM ∈
{PSM, ASO, PAS}. In particular, ESEM consists of the arcs (Mi , Mj ) such that Mi = Mj
holds in the SEM semantics. Therefore, the comparison of the different semantics can be
performed by analyzing the corresponding preference graphs. A stable model Mi is preferred
under the SEM semantics if there is no arc (Mj , Mi ) in GSEM .
The following example shows the relation between the PSM and PAS semantics.
Example 9 Consider the prioritized program hP 9 , Φ9 i below
P 9 : fish ⊕ beef ⊕ pork ⊕ chicken ←
white ← fish
red ← beef
beer ← pork
water ← chicken
517

Φ9 : fish > beef ←
chicken > pork ←
red > white ←
red > water ←
beer > water ←

Greco, Trubitsyna, & Zumpano

The program has four stable models: M1 = {fish, white}, M2 = {beef, red}, M3 =
{pork, beer} and M4 = {chicken, water}, whereas the direct preference relations, for the
PSM semantics, are as follows: M1 w M2 , M2 w M1 , M3 w M4 , M4 w M3 and M2 w M4 .
Consequently, we have that the graph GPSM consists of four nodes (M1 , M2 , M3 and
M4 ) and four arcs: M1 = M3 , M1 = M4 , M2 = M3 and M2 = M4 . Therefore, the preferred
models are M1 and M2 .
Regarding the PAS semantics, we have that only the relation M2 = M4 holds and, thus,
there are three preferred stable models, namely M1 and M2 and M3 .
2
Theorem 2 For any prioritized program hP, Φi such that Φ consists in preference facts,
PSM(hP, Φi) ⊆ PAS(hP, Φi).
Proof. Consider the graphs GPSM = (V, EPSM ) and GPAS = (V, EPAS ). Both graphs are
acyclic and EPAS ⊆ EPSM . As the two graphs are acyclic, by adding edges which do not
create cycles, the number of nodes without incoming edges decreases. Therefore, the set of
nodes without incoming edges in GPAS contains all nodes without incoming edges in GPSM
and, consequently, PSM(hP, Φi) ⊆ PAS(hP, Φi).
2
We now analyze the relation between ASO and PAS semantics. First of all note that,
as observed in the Introduction, the ASO semantics is sensitive to syntax changes. For
instance, the prioritized program
a⊕b⊕c←

ρ1 = a > b > c ←
ρ2 = b > a ←

has two preferred stable models: M1 = {a} and M2 = {b}. However, for the program below
ρ01 = a > b ←
ρ001 = b > c ←
ρ2 = b > a ←

a⊕b⊕c←

derived from the rewriting of the rule ρ1 , M3 = {c} is also a preferred model.
Thus, we consider a special class of constraints which is not sensitive to “syntactic”
changes. Since every (ground) prioritized program Π = hP, Φi, such that Φ∗ can be partitioned into n strata, with n > 1, can be rewritten into a program Π̂ = hP̂, Φ̂i, such that
Π̂ and Π are equivalent under the PAS semantics (as shown in the proof of Proposition
1), and may not be equivalent under the RASO semantics (as all rules in Φ̂ belong to the
unique stratum 0), we continue to consider programs Π = hP, Φi where Φ∗ consists of a
single stratum and, in particular, of facts.
Given a prioritized program hP, Φi such that Φ consists only of facts, we denote with
Φ+ = {a1 > · · · > an ← | ai > ai+1 ← ∈ Φ∗ for i ∈ [1..n-1] ∧ n is maximum }
the set of preference rules which can be obtained from the “merging” of ground preference facts.
Lemma 2 Let hP, Φi be a prioritized program such that Φ consists only of preference facts
and ground(Φ) = Φ+ . Then, ASO(hP, Φi) ⊆ PAS(hP, Φi).
518

On the Semantics of Logic Programs with Preferences

Proof. Consider the two graphs GASO = (V, EASO ) and GPAS = (V, EPAS ). (M1 , M2 ) ∈
EPAS means that M1 = M2 , i.e. that
i) there is a ground rule %1 : e1 > e2 ← ∈ Φ∗ such that e1 ∈ M1−M2 , e2 ∈ M2−M1 , and
ii) there is no ground rule %2 : e3 > e4 ← ∈ Φ∗ , such that e3 ∈ M2 −M1 and e4 ∈ M1 −M2 .
This implies that if ground(Φ) = Φ+
i) there must be a ground rule σ1 : · · · > e1 > · · · > e2 > · · · ← ∈ Φ+ such that
e1 ∈ M1 − M2 , e2 ∈ M2 − M1 , and
ii) there must not be ground rule σ2 : · · · > e3 > · · · > e4 > · · · ← ∈ Φ+ , such that
e3 ∈ M2 − M1 and e4 ∈ M1 − M2 .
Therefore, M1 = M2 also with respect to the ASO semantics, and the graph GASO contains an arc (M1 , M2 ). Consequently, as EPAS ⊆ EASO , ASO(hP, Φ+ i) ⊆ PAS(hP, Φ+ i). 2
To find a tight relation between the two semantics, we consider a further restriction of
Φ+ which is obtained by deleting atoms which do not appear in any model from the ground
preference rules:
+

b = {a1 > · · · > an ← | ai > ai+1 ← ∈ Φ∗ for i ∈ [1..n-1] ∧ n is maximum ∧
Φ
∃M ∈ SM(P) s.t. ai , ai+1 ∈ M }

Theorem 3 Let hP, Φi be a prioritized program such that Φ consists only of preference
b + . Then, ASO(hP, Φi) = PAS(hP, Φi).
facts and ground(Φ) = Φ
b + is derived from Φ+ by
Proof. ASO(hP, Φi) ⊆ PAS(hP, Φi) derives from Lemma 2, as Φ
deleting nodes which do not appear in any model and do not influence the relation = in the
ASO semantics.
To show that ASO(hP, Φi) ⊇ PAS(hP, Φi) consider the relation in the ASO semantics.
M1 = M2 means that
b + such that e1 ∈ M1 − M2 ,
1. there is a ground rule σ1 : · · · > e1 > · · · > e2 > · · · ← ∈ Φ
e2 ∈ M2 − M1 , and

2. there is no ground rule σ2 : · · · > e3 > · · · > e4 > · · ·
e3 ∈ M2 − M1 and e4 ∈ M1 − M2 .

← ∈ Φ+ , such that

b+
This implies that if ground(Φ) = Φ

1. there must be a ground rule %1 : e1 > e2 ← ∈ Φ∗ such that e1 ∈ M1 − M2 , e2 ∈
M2 − M1 , and
2. there is no ground rule %2 : e3 > e4 ← ∈ Φ∗ , such that e3 ∈ M2 −M1 and e4 ∈ M1 −M2 .
519

Greco, Trubitsyna, & Zumpano

Therefore, M1 = M2 also holds with respect to the PAS semantics. Consequently,
as EASO ⊆ RPAS , ASO(hP, Φ+ i) ⊇ PAS(hP, Φ+ i).
2
An extension of the ASO semantics has been proposed by Brewka (2004) and Brewka,
Niemela, and Truszczynski (2005). In more detail, Brewka (2004) provided a preference
description language, allowing to express complex preferences by combining qualitative and
quantitative penalty based preferences, whereas Brewka et al. (2005) proposed a framework to specify problem solutions (outcomes) and preferences among them. The latter
proposal combines ideas from answer-set programming, answer-set optimization and CPnets (Boutilier, Brafman, Domshlak, Hoos, & Poole, 2004). The semantics that we have
proposed in this paper is different from both those proposed by Brewka (2004) and Brewka
et al. (2005), as in some cases it returns different results (see Examples 2 and 4).

6. Other Approaches
Besides the approaches managing preferences among atoms, some other works proposed in
the literature specify preferences among rules.
Early proposals expressing preferences on rules focus on Default Logic (Brewka & Eiter,
2000; Delgrande et al., 2000b; Rintanen, 1998), whereas more recently the emphasis has
been given to logic programs. In this regard, different proposals have been developed for
representing and reasoning about user preferences such as ordered logic programs (Delgrande et al., 2000a; Van Nieuwenborgh & Vermeir, 2002, 2004) and preferred answer sets
of extended logic programs (Brewka & Eiter, 1999). Most of the approaches propose an
extension of Gelfond and Lifschitz’s extended logic programming by adding preference information (Delgrande et al., 2003; Wang et al., 2000; Zhang & Foo, 1997). Other proposals
attempt to extend the well founded semantics to logic programs with preferences (Brewka,
1996; Schauba & Wang, 2001), and an extension of van-Gelder’s alternating fixpoint theory
for logic programs with priorities has been proposed by Wang et al. (2000).
Gelfond and Son (1997) have proposed a methodology of reasoning with prioritized
default in the language of logic programming under answer set semantics. This approach
enables the specification of preferences among rules and allows the definition of a set of
default rules which must be satisfied as well as a second set of default rules which could be
ignored.
Ordered logic programs have been introduced by Delgrande et al. (2000a) as extended
logic programs whose rules are subject to a strict partial order with both static and dynamic
preferences. This approach is fully prescriptive as it enforces the ordering information during
the construction of the answer set. The original program is transformed into a second
extended logic program in which preferences are taken into account in the sense that the
answer sets obtained by evaluating the transformed theory correspond to the preferred
answer sets of the original theory.
Another methodology in which logic programs containing preferences on the set of rules
can be translated into logic programs under stable model semantics has been proposed by
Delgrande et al. (2003).
520

On the Semantics of Logic Programs with Preferences

7. Conclusions
In this paper the case of preferences involving atoms in logic programming has been studied.
In particular, the behavior of the technique proposed by Sakama and Inoue (2000) and
Brewka et al. (2003) has been analyzed and a semantics, interpreting each preference rule as
a tool for representing a choice over alternative options, has been proposed. Specifically, the
proposed approach extends the PSM semantics by considering a refinement of the natural
order among preferences and introduces the concept of comparable models. Preferences and
logic programs are examined together in order to determine the choice order and the sets
of models which can be compared.
The new semantics has been compared with the PSM and the ASO semantics. Complexity analysis has also been performed showing that the use of additional information,
regarding the preference order and the sets of non comparable models, does not increase the
complexity of computing preferred stable models. Although the semantics presented here
has the same complexity as other approaches proposed in the literature, the advantage lies
in the fact that it seems to better capture the intuitive meaning of prioritized programs by
also considering the structural information of preference rules.
Prioritized reasoning in logic programming under the PAS semantics can be easily
implemented on the top of deductive systems based on stable model semantics such as
DeRes, DLV, Smodels (Cholewinski, Marek, & Truszczynski, 1996; Leone, Pfeifer, Faber,
Calimeri, & Dell’Armi, 2002; Syrjanen & Niemela, 2001). An architecture and a system
prototype implementing prioritized reasoning (with different semantics) on the top of the
DLV system has been presented by Caroprese, Trubitsyna, and Zumpano (2007).

Acknowledgments
A preliminary version of the papers has been presented by Greco, Trubitsyna, and Zumpano
(2006). The authors would like to thank the anonymous referees for their useful suggestions
and Filippo Furfaro for his comments.

References
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., Poole, D. (2004). CP-nets: A tool
for representing and reasoning with conditional ceteris paribus preference statements.
Journal of Artificial Intelligence Research, 21, 135-191.
Brewka, G. (1996). Well-Founded Semantics for Extended Logic Programs with Dynamic
Preferences. Journal of Artificial Intelligence Research, 4, 19-36.
Brewka, G., Eiter, T. (1999). Preferred Answer Sets for Extended Logic Programs. Artificial
Intelligence, 109(1-2), 297-356.
Brewka, G., Eiter, T. (2000). Prioritizing Default Logic. Intellectics and Computational
Logic, Kluwer, 27-45.
Brewka, G. (2002). Logic programming with ordered disjunction. Proceedings 18th National
Conference on Artificial Intelligence (AAAI/IAAI), 100-105.
521

Greco, Trubitsyna, & Zumpano

Brewka, G., Niemela, I., Truszczynski, M. (2003). Answer Set Optimization. Proceedings
18th International Joint Conference on Artificial Intelligence (IJCAI), 867-872.
Brewka, G. (2004). Complex Preferences for Answer Set Optimization, Proceedings 9th
International Conference on Principles of Knowledge Representation and Reasoning
(KR), 213-223.
Brewka, G., Niemela, I., Truszczynski, M. (2005). Prioritized Component Systems. Proceedings 20th National Conference on Artificial Intelligence (AAAI), 596-601.
Caroprese, L., Trubitsyna, I., Zumpano, E. (2007). Implementing Prioritized Reasoning in
Logic Programming. Proceedings International Conference on Enterprice Information
Systems (ICEIS), 94-100.
Cholewinski, P., Marek, V. W., Truszczynski, M. (1996). Default Reasoning System DeReS.
Proceedings 5th International Conference on Principles of Knowledge Representation
and Reasoning (KR), 518-528.
Delgrande, J., P., Schaub, T., Tompits, H. (2000). Logic Programs with Compiled Preferences. Proceedings 14th European Conference on Artificial Intelligence (ECAI), 464-468.
Delgrande, J., P., Schaub, T., Tompits, H. (2000). A Compilation of Brewka and Eiter’s
Approach to Prioritization. Proceedings European Workshop on Logics in Artificial
Intelligence (JELIA), 376-390.
Delgrande, J., P., Schaub, T., Tompits, H. (2003). A Framework for Compiling Preferences
in Logic Programs. Theory and Practice of Logic Programming, 3(2), 129-187.
Eiter, T., Gottlob, G., Leone, N. (1997). Abduction from Logic Programs: Semantics and
Complexity. Theoretical Computer Science 189(1-2), 129–177.
Eiter, T., Gottlob, G., Mannila, H. (1997). Disjunctive Datalog. ACM Transaction On
Database Systems, 22(3), 364–418, 1997.
Gelfond, M., Lifschitz, V. (1988). The Stable Model Semantics for Logic Programming,
Proceedings International Conference on Logic Programming (ICLP), 1070–1080.
Gelfond, M., Lifschitz, V. (1991). Classical Negation in Logic Programs and Disjunctive
Databases, New Generation Computing, 9, 365–385.
Gelfond, M., Son, T.C. (1997). Reasoning with prioritized defaults. Proc. 3rd International
Workshop on Logic Programming and Knowledge Representation (LPKR), 164-223.
Greco, S., Trubitsyna, I., Zumpano, E. (2006). On the Semantics of Logic Programs with
Preferences. Proceedings 10th European Conference on Logics in Artificial Intelligence
(JELIA), 203-215.
Grell, S., Konczak, K., Schaub, T. (2005). nomore<: A System for Computing Preferred
Answer Sets. Proceedings 8th International. Conference on Logic Programming and
Nonmonotonic Reasoning (LPNMR), 394-398.
Janhunen, T., Niemela, I., Simons, P., You, J.-H. (2000). Unfolding partiality and disjunctions in stable model semantics, Proceedings 7th International Conference on Principles of Knowledge Representation and Reasoning (KR), 411-419.
522

On the Semantics of Logic Programs with Preferences

Inoue, K., Sakama, S. (1998). Negation as Failure in the Head. Journal of Logic Programming, 35(1), 39-78.
Kakas, A. C., Kowalski, R. A., Toni, F. (1992). Abductive Logic Programming. Journal of
Logic anc Computation, 2(6), 719-770.
Leone, N., Pfeifer, G., Faber, W., Calimeri, F., Dell’Armi, T., Eiter, T., Gottlob, G., Ianni,
G., Ielpa, G., Koch, K., Perri, S., Polleres, A. (2002). The DLV System. Proceedings
8th European Conference on Logics in Artificial Intelligence (JELIA), 537-540, 2002.
Minker, J. (1982). On Indefinite Data Bases and the Closed World Assumption, Proc. 6-th
Conf. on Automated Deduction, 292-308, 1982.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Rintanen J. (1998). Complexity of Prioritized Default Logics, Journal of Artificial Intelligence Research, 9, 423-461.
Sakama, C., Inoue, K. (2000). Priorized logic programming and its application to commonsense reasoning. Artificial Intelligence, 123, 185-222.
Schaub, T., Wang , K. (2001). A Comparative Study of Logic Programs with Preference.
Proceedings 17th International Joint Conference on Artificial Intelligence (IJCAI),
597-602.
Syrjanen, T., and Niemela, I. (2001). The Smodels System. Proceedings International Conference on Logic Programming and Nonmonotonic Reasoning (LPNMR), 434-438.
Van Nieuwenborgh, D., Vermeir, D. (2002). Preferred Answer Sets for Ordered Logic
Programs. Proceedings 10th European Conference on Logics in Artificial Intelligence
(JELIA), 432-443.
Van Nieuwenborgh, D., Vermeir, D. (2002). Ordered Diagnosis, Proceedings 10th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning
(LPAR), 244-258.
Van Nieuwenborgh, D., Heymans, S., Vermeir, D. (2004). On Programs with Linearly Ordered Multiple Preferences. Proceedings International Conference on Logic Programming (ICLP), 180-194.
Wakaki, T., Inoue, K., Sakama, C., Nitta, K. (2003). Computing Preferred Answer Sets
in Answer Set Programming. Proceedings 10th International Conference on Logic for
Programming, Artificial Intelligence, and Reasoning (LPAR), 259-273.
Wakaki, T., Inoue, K., Sakama, C., Nitta, K. (2004). The PLP System. Proceedings 9th
European Conference on Logics in Artificial Intelligence (JELIA), 706-709.
Wang, K., Zhou, L., Lin, F. (2000). Alternating Fixpoint Theory for Logic Programs with
Priority. Proceedings First International Conference on Computational Logic, 164-178.
Zhang, Y., Foo, N. (1997). Answer sets for prioritized logic programs. Proceedings International Logic Programming Symposium (ILPS), 69-83.

523

Journal of Artificial Intelligence Research 30 (2007) 1-50

Submitted 11/06; published 9/07

Learning Semantic Definitions of Online Information Sources
Mark James Carman
Craig A. Knoblock

mark@bradipo.net
knoblock@isi.edu

University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292

Abstract
The Internet contains a very large number of information sources providing many types
of data from weather forecasts to travel deals and financial information. These sources can
be accessed via Web-forms, Web Services, RSS feeds and so on. In order to make automated
use of these sources, we need to model them semantically, but writing semantic descriptions
for Web Services is both tedious and error prone. In this paper we investigate the problem
of automatically generating such models. We introduce a framework for learning Datalog
definitions of Web sources. In order to learn these definitions, our system actively invokes
the sources and compares the data they produce with that of known sources of information.
It then performs an inductive logic search through the space of plausible source definitions
in order to learn the best possible semantic model for each new source. In this paper we
perform an empirical evaluation of the system using real-world Web sources. The evaluation
demonstrates the effectiveness of the approach, showing that we can automatically learn
complex models for real sources in reasonable time. We also compare our system with
a complex schema matching system, showing that our approach can handle the kinds of
problems tackled by the latter.

1. Introduction
Recent years have seen an explosion in the quantity and variety of information available
online. One can find shopping data (prices and availability of goods), geospatial data
(weather forecasts, housing information), travel data (flight pricing and status), financial
data (exchange rates and stock quotes), and that just scratches the surface of what is
available. The aim of this work is to make use of that vast store of information.
As the amount of information has increased, so too has its reuse across Web portals
and applications. Developers have realised the importance of managing content separately
from presentation, leading to the development of XML as a self-describing data format.
Content in XML is far easier to manipulate than HTML, simplifying integration across
different sources. Standards have also emerged for providing programmatic access to data
(like SOAP and REST) so that developers can easily build programs (called Mash-Ups) that
combine content from different sites in real-time. Many portals now provide such access to
their data and some even provide syntactic definitions (in WSDL) of the input and output
these data sources expect. Missing, however, are semantic descriptions of what each source
does, which is required in order to support automated data integration.
c
2007
AI Access Foundation. All rights reserved.

Carman & Knoblock

1.1 Structured Querying
Given all the structured sources available, we would like to combine the data dynamically to
answer specific user requests (as opposed to statically in the case of Mash-Ups). Dynamic
data requests can be expressed as queries such as those shown below. Such queries may
require access to multiple (publicly available) data sources and combine information in ways
that were not envisaged by its producers.
1. Tourism: Get prices and availability for all three star hotels within 100 kilometers of
Trento, Italy that lie within 1 kilometer of a ski resort that has over 1 meter of snow.
2. Transportation: Determine the time that I need to leave work in order to catch a bus
to the airport to pick up my brother who is arriving on Qantas flight 205.
3. Disaster Prevention: Find phone numbers for all people living within 1 mile of the
coast and below 200 feet of elevation.
It is clear even from this small set of examples just how powerful the ability to combine
data from disparate sources can be. In order to give these queries to an automated system,
we must first express them formally in a query language such as SQL or Datalog (Ullman,
1989). In Datalog the first query might look as follows:
q(hotel, price) :accommodation(hotel, 3*, address), available(hotel, today, price),
distance(address, hTrento,Italyi, dist1), dist1 < 100km,
skiResort(resort, loc1), distance(address, loc1, dist2),
dist2 < 1km, snowCondiditions(resort, today, height), height > 1m.
The expression states that hotel and price pairs are generated by looking up three star hotels
in a relational table called accommodation, then checking the price for tomorrow night in a
table called available. The address of each hotel is used to calculate the distance to Trento,
which must be less than 100 kilometers. The query also checks that there is a skiResort
within 1 kilometer of the hotel, and that the snowConditions for today show more than 1
meter of snow.
1.2 Mediators
A system capable of generating a plan to answer such a query is called an Information
Mediator (Wiederhold, 1992). In order to generate a plan, mediators look for sources that
are relevant to the query. In this case, relevant sources might be:
1. The Italian Tourism Website: to find all hotels near ‘Trento, Italy’.
2. A Ski Search Engine: to find ski resorts near each hotel.
3. A Weather Provider: to find out how much snow has fallen at each ski resort.
For a mediator to know which sources are relevant, it needs to know what information each
source provides. While XML defines the syntax (formatting) used by a source, the semantics
(intended meaning) of the information the source provides must be defined separately.
This can be done using Local-as-View (LAV) source definitions in Datalog (Levy, 2000).
Essentially, source definitions describe queries that if given to a mediator, will return the
same data as the source provides. Example definitions are shown below. The first states
2

Learning Semantic Definitions of Information Sources on the Internet

that the source hotelSearch takes four values as input (inputs are prefixed by the $-symbol),
and returns a list of hotels which lie within the given distance of the input location. For
each hotel it also returns the address as well as the price for a room on the given date.
(Note that the source only provides information for hotels in Italy.)
hotelSearch($location, $distance, $rating, $date, hotel, address, price) :country(location, Italy), accommodation(hotel, rating, address),
available(hotel, date, price), distance(address, location, dist1),
dist1 < distance.
findSkiResorts($address, $distance, resort, location) :skiResort(resort, location), distance(address, location, dist1),
dist1 < distance.
getSkiConditions($resort, $date, height) :snowCondiditions(resort, date, height).
In order to generate a plan for answering the query, a mediator performs a process called
query reformulation (Levy, 2000), whereby it transforms the query into a new query over
(in terms of) the relevant information sources.1 (A source is relevant if it refers to the same
relations as the query.) The resulting plan in this case is shown below.
q(hotel, price) :hotelSearch(hTrento,Italyi, 100km, 3*, today, hotel, address, price),
findSkiResorts(address, 1km, resort, location),
getSkiConditions(resort, today, height), height > 1m.
In this work, the questions of interest are: where do all the definitions for these information
sources come from and more precisely, what happens when we want to add new sources to
the system? Is it possible to generate these source definitions automatically?
1.3 Discovering New Sources
In the example above, the mediator knows of a set of relevant sources to use to successfully
answer the query. If instead, one of those sources is missing or doesn’t have the desired
scope (e.g. getSkiConditions doesn’t provide data for Trento), then the mediator first needs
to discover a source providing that information. As the number and variety of information
sources increase, we will undoubtedly rely on automated methods for discovering them and
annotating them with semantic descriptions. In order to discover relevant sources, a system
might inspect a service registry2 (such as those defined in UDDI), or perform keywordbased search over a Web index (such as Google or del.icio.us). The research community has
looked at the problem of discovering relevant services, developing techniques for classifying
services into different domains (such as weather and flights) using service metadata (Heß &
Kushmerick, 2003) and clustering similar services together to improve keyword-based search
(Dong, Halevy, Madhavan, Nemes, & Zhang, 2004). These techniques, although useful, are
not sufficient for automating service integration.
1. The complexity of query reformulation is known to be exponential, although efficient algorithms for
performing it do exist (Pottinger & Halevy, 2001).
2. Note that technically, a service is different from a source. A service is an interface providing access to
multiple operations, each of which may provide information. If an operation does not affect the “state
of the world” (e.g. by charging somebody’s credit card), then we call it an information source. For this
paper, however, we use the term service to refer only to information sources.

3

Carman & Knoblock

1.4 Labeling Service Inputs and Outputs
Once a relevant service is discovered, the problem shifts to modeling it semantically. Modeling sources by hand can be laborious, so automating the process makes sense. Since different
services often provide similar or overlapping data, it should be possible to use knowledge of
previously modeled services to learn descriptions for new ones.
The first step in modeling a source is to determine what type of data it requires as
input and produces as output. This is done by assigning semantic types (like zipcode,
telephone number, temperature, and so on) to the attributes of a service. Semantic types
restrict the possible values of an attribute to a subset of the corresponding primitive type.
The research community has investigated automating the assignment process by viewing it
as a classification problem (Heß & Kushmerick, 2003). In their system, Heß and Kushmerick
trained a Support Vector Machine (SVM) on metadata describing different sources. The
system, given a source such as the following:
getWeather($zip, temp)
uses the labels getWeather, zip and temp (and any other available metadata) to assign
types to the input and output attributes, e.g.: zip →zipcode, temp →temperature. Note that
additional metadata is often useful for distinguishing between possible assignments. (If, for
example, the name of the operation had been listEmployees, then temp may have referred
to a temporary employee rather than a temperature.)
In subsequent work, researchers developed a more comprehensive system that used both
metadata and output data to classify service attributes (Lerman, Plangprasopchok, &
Knoblock, 2006). In that system, a Logistic Regression based classifier first assigns semantic types to input parameters. Examples of those input types are then used to invoke
the service, and the output is given to a pattern-language based classifier, that assigns types
to the output parameters. The authors argue that classification based on both data and
metadata is far more accurate than that based on metadata alone. Using an example, it is
easy to see why. Consider the following tuples produced by our getWeather source:
h90292, 25◦ Ci, h10274, 15◦ Ci, h60610, 18◦ Ci, ...
Given the data, the classifier can be certain that temp really does refers to a temperature,
and indeed can even assign it to a more specific type, temperatureC (in Celsius).
While the problem of determining the semantic types of a service’s attributes is very
interesting, and there is room for improvement on current techniques, we assume for the
purposes of this work that it has already been solved.
1.5 Generating a Definition
Once we know the parameter types, we can invoke the service, but we are still unable to
make use of the data it returns. To do that, we need also to know how the output attributes
relate to the input (i.e. a definition for the source). For example, for the getWeather service
we need to know whether the temperature being returned is the current temperature, the
predicted high temperature for tomorrow or the average temperature for this time of year.
Such relationships can be described by the following definitions:
getWeather($zip, temp) :- currentTemp(zip, temp).
getWeather($zip, temp) :- forecast(zip, tomorrow, temp).
getWeather($zip, temp) :- averageTemp(zip, today, temp).
4

Learning Semantic Definitions of Information Sources on the Internet

The relations used in these definitions would be defined in a domain ontology (or schema).
In this paper we describe a system capable of learning which if any of these definitions
is the correct. The system leverages what it knows about the domain, i.e. the domain
ontology and a set of known information sources, to learn what it does not know, namely
the relationship between the attributes of a newly discovered source.
1.6 Outline
This paper presents a comprehensive treatment of methods for learning semantic descriptions of Web information sources. It extends our previous work on the subject (Carman &
Knoblock, 2007) by presenting detailed descriptions of the methods for both enumerating
the search space and evaluating the individual candidate definitions. We provide additional
details regarding the evaluation methodology and the results generated.
The paper is structured as follows. We start with an example to motivate the source
induction problem and then formulate the problem concisely. We discuss our approach
to learning definitions for sources in terms of other known sources of information (section
3). We give details of the search procedure for generating candidate definitions (section 4)
and of the evaluation procedure for scoring candidates during search (section 5). We then
describe extensions to the basic algorithm (section 6) before discussing the evaluation setup
and the experiments (section 7), which demonstrate the capabilities of our system. Finally,
we contrast our approach with prior work.

2. Problem
We now describe in detail the problem of learning definitions for newly discovered services.
We start with a concrete example of what is meant by learning a source definition. In the
example there are four types of data (semantic types), namely: zipcodes, distances, latitudes
and longitudes. There are also three known sources of information. Each of the sources
has a definition in Datalog as shown below. The first service, aptly named source1, takes
in a zipcode and returns the latitude and longitude coordinates of its centroid. The second
service calculates the great circle distance (the shortest distance over the earth’s surface)
between two pairs of coordinates, and the third converts a distance from kilometres into
miles by multiplying the input by the constant 1.609.
source1($zip, lat, long) :- centroid(zip, lat, long).
source2($lat1, $long1, $lat2, $long2, dist) :greatCircleDist(lat1, long1, lat2, long2, dist).
source3($dist1, dist2) :- multiply(dist1, 1.609, dist2).
The goal in this example is to learn a definition for a newly discovered service, called source4.
This service takes two zipcodes as input and returns a distance value as output:
source4($zip1, $zip2, dist)
The system we will describe uses this type signature (input and output type information)
to search for an appropriate definition for the source. The definition discovered in this case
might be the following conjunction of calls to the individual sources:
source4($zip1, $zip2, dist):source1($zip1, lat1, long1), source1($zip2, lat2, long2),
source2($lat1, $long1, $lat2, $long2, dist2), source3($dist2, dist).
5

Carman & Knoblock

The definition states that source’s output distance can be calculated from the input zipcodes,
by giving those zipcodes to source1, taking the resulting coordinates and calculating the
distance between them using source2, and then converting that distance into miles using
source3. To test whether this definition is correct, the system must invoke both the new
source and the definition to see if the values generated agree with each other. The following
table shows such a test:
$zip1
80210
60601
10005

$zip2
90266
15201
35555

dist (actual)
842.37
410.31
899.50

dist (predicted)
843.65
410.83
899.21

In the table, the input zipcodes have been selected randomly from a set of examples, and
the output from the source and the definition are shown side by side. Since the output
values are quite similar, once the system has seen a sufficient number of examples, it can
be confident that it has found the correct semantic definition for the source.
The definition above is given in terms of the source relations, but could also have been
written in terms of the domain relations (the relations used to define sources 1 to 3). To
convert the definition into that form, one simply needs to replace each source relation by
its definition as follows:
source4($zip1, $zip2, dist):centroid(zip1, lat1, long1), centroid(zip2, lat2, long2),
greatCircleDist(lat1, long1, lat2, long2, dist2), multiply(dist1, 1.609, dist2).
Written in this way, the new semantic definition makes sense at an intuitive level: the source
is simply calculating the distance in miles between the centroids of the two zipcodes.
2.1 Problem Formulation
Having given an example of the Source Definition Induction Problem, we now describe the
problem more formally, but before doing so, we introduce some concepts and notation. (We
note that our focus in this paper is on learning definitions for information-providing, as
opposed to “world-altering” services.)
• The domain of a semantic data-type t, denoted D[t], is the (possibly infinite) set of
constant values {c1 , c2 , ...}, which constitute the set of values for variables of that
type. For example D[zipcode] = {90210, 90292, ...}
• An attribute is a pair hlabel, semantic data-typei, e.g. hzip1, zipcodei. The type of
attribute a is denoted type(a) and the corresponding domain D[type(a)] is abbreviated
to D[a].
• A scheme is an ordered (finite) set of attributes ha1 , ..., an i with unique labels, where
n is referred to as the arity of the scheme. An example scheme might be hzip1 :
zipcode, zip2 : zipcode, dist : distancei. The domain of a scheme A, denoted
D[A], is the Cartesian product of the domains of the attributes in the scheme {D[a1 ]×
... × D[an ]}, ai ∈ A.
• A tuple over a scheme A is an element from the set D[A]. A tuple can be represented
by a set of name-value pairs, such as {zip1 = 90210, zip2 = 90292, dist = 8.15}
6

Learning Semantic Definitions of Information Sources on the Internet

• A relation is a named scheme, such as airDistance(zip1, zip2, dist). Multiple relations may share the same scheme.
• An extension of a relation r, denoted E[r], is a subset of the tuples in D[r]. For example, E[airDistance] might be a table containing the distance between all zipcodes in
California. (Note that the extension of a relation may only contain distinct tuples.)
• A database instance over a set of relations R, denoted I[R], is a set of extensions
{E[r1 ], ..., E[rn ]}, one for each relation r ∈ R.
• A query language L is a formal language for constructing queries over a set of relations.
We denote the set of all queries that can be written using the language L over the set
of relations R returning tuples conforming to a scheme A as LR,A .
• The result set produced by the execution of a query q ∈ LR,A on a database instance
I[R] is denoted EI [q].
• A source is a relation s, with a binding pattern βs ⊆ s, which distinguishes input
attributes from output attributes. (The output attributes of a source are denoted by
the complement of the binding pattern3 , βsc ≡ s\βs .)
• A view definition for a source s is a query vs written in some query language LR,s .
The Source Definition Induction Problem is defined as a tuple:
hT, R, L, S, V, s∗ i
where T is a set of semantic data-types, R is a set of relations, L is a query language, S is a
set of known sources, V is a set of view definitions (one for each known source), and s∗ is
the new source (also referred to as the target).
Each semantic type t ∈ T must be provided with a set of examples values Et ⊆ D[t].
(We do not require the entire set D[t], because the domain of many types may be partially
unknown or too large to be enumerated.) In addition, a predicate eqt (t, t) is available for
checking equality between values of each semantic type to handle the case where multiple
serialisations of a variable represent the same value.
Each relation r ∈ R is referred to as a global relation or domain predicate and its
extension is virtual, meaning that the extension can only be generated by inspecting every
relevant data source. The set of relations R may include some interpreted predicates, such
as ≤, whose extension is defined and not virtual.
The language L used for constructing queries could be any query language including
SQL and XQuery (the XML Query Language). In this paper we will use a form of Datalog.
Each source s ∈ S has an extension E[s] which is the complete set of tuples that can be
produced by the source (at a given moment in time). We require that the corresponding view
definition vs ∈ V (written in LR,s ) is consistent with the source, such that: E[s] ⊆ EI [vs ],
(where I[R] is the current virtual database instance over the global relations). Note that
we do not require equivalence, because some sources may provide incomplete data.
The view definition for the source to be modeled s∗ is unknown. The solution to the
Source Definition Induction Problem is a view definition v ∗ ∈ LR,s∗ for the source s∗ such
that E[s∗ ] ⊆ EI [v ∗ ], and there is no other view definition v 0 ∈ LR,s∗ that better describes
(provides a tighter definition for) the source s∗ , i.e.:
¬∃v 0 ∈ LR,s∗ s.t. E[s∗ ] ⊆ EI [v 0 ] ∧ |EI [v 0 ]| < |EI [v ∗ ]|
3. The ‘\’-symbol denotes set difference.

7

Carman & Knoblock

Given limited available computation and bandwidth, we note that it may not be possible to
guarantee that this optimality condition holds for a particular solution; thus in this paper
we will simply strive to find the best solution possible.
2.2 Implicit Assumptions
A number of assumptions are implicit in the problem formulation. The first is that there
exists a system capable of discovering new sources and more importantly classifying (to
good accuracy) the semantic types of their input and output. Systems capable of doing this
were discussed in section 1.4.
The second assumption has to do with the representation of each source as a relational
view definition. Most sources on the Internet provide tree structured XML data. It may
not always be obvious how best to flatten that data into a set of relational tuples, while
preserving the intended meaning of the data. Consider a travel booking site which returns
a set of flight options each with a ticket number, a price and a list of flight segments that
constitute the itinerary. One possibility for converting this data into a set of tuples would
be to break each ticket up into individual flight segment tuples (thereby obscuring the
relationship between price and the number of flight segments). Another would be to create
one very long tuple for each ticket with room for a number of flight segments (thereby
creating tuples with many null values). In this case, it is not obvious which, if either, of
these options is to be preferred. Most online data sources can, however, be modeled quite
naturally as relational sources; and by first tackling the relational problem, we can develop
techniques that can later be applied to the more difficult semi-structured case.
A third assumption is that the set of domain relations suffices for describing the source to
be modeled. For instance, consider the case where the domain model only contains relations
describing financial data, and the new source provides weather forecasts. Obviously, the
system would be unable to find an adequate description of the behavior of the source and
would not learn a model of it. From a practical perspective, this limitation is not a big
problem, since a user can only request data (write queries to a mediator) using the relations
in the domain model anyway. (Thus any source which cannot be described using those
relations would not be needed to answer user requests.) In other words, the onus is on
the domain modeler to model sufficient relations so as to be able to describe the types of
queries a user should be able to pose to the system and consequently, the types of sources
that should be available. That said, an interesting avenue for future research would be to
investigate the problem of automating (at least in part) the process of expanding the scope
of the domain model (by adding attributes to relations, or inventing new ones), based on
the types of sources discovered.
2.3 Problem Discussion
A number of questions arise from the problem formulation, the first being where the domain
model comes from. In principle, the set of semantic types and relations could come from
many places. It could be taken from standard data models for the different domains, or it
might just be the simplest model possible that aptly describes the set of known sources. The
domain model may then evolve over time as sources are discovered for which no appropriate
model can be found. Somewhat related is the question of how specific the semantic types
8

Learning Semantic Definitions of Information Sources on the Internet

ought to be. For example, is it sufficient to have one semantic type distance or should
one distinguish between distance in meters and distance in feet? Generally speaking, a
semantic type should be created for each attribute that is syntactically dissimilar to all
other attributes. For example, a phone number and a zipcode have very different syntax,
thus operations which accept one of the types as input are unlikely to accept the other. In
practice, one might create a new semantic type whenever a trained classifier can recognise
the type based on its syntax alone. In general, the more semantic types there are, the
harder the job of the system classifying the attributes, and the easier the job of the system
tasked with learning a definition for the source.
Another question to be considered is where the definitions for the known sources come
from. Initially such definitions would need to be written by hand. As the system learns
definitions for new sources, they too would be added to the set of known sources, making
it possible to learn ever more complicated definitions.
In order for the system to learn a definition for a new source, it must be able to invoke
that source and thus needs examples of the input types. The more representative the set of
examples available, the more efficient and accurate the learning process will be. An initial
set of examples will need to be provided by the domain modeler. Then, as the system learns
over time, it will generate a large number of examples of different semantic types (as output
from various sources), which can be retained for future use.
Information Integration research has reached a point where mediator technology4 is
becoming mature and practical. The need to involve a human in the writing of source
definitions is, however, the Achilles’ Heel of such systems. The gains in flexibility that come
with the ability to dynamically reformulate user queries are often partially offset by the
time and skill required to write definitions when incorporating new sources. Thus a system
capable of learning definitions automatically could greatly enhance the viability of mediator
technology. This motivation alone seems sufficient for pursuing the problem.

3. Approach
The approach we take to learning semantic models for information sources on the Web is
twofold. Firstly, we choose to model sources using the powerful language of conjunctive
queries. Secondly, we leverage the set of known sources in order to learn a definition for the
new one. In this section we discuss these aspects in more detail.
3.1 Modeling Language
The source definition language L is the hypothesis language in which new definitions will
need to be learnt. As is often the case in machine learning, we are faced with a trade-off
with respect to the expressiveness of this language. If the hypothesis language is too simple,
then we may not be able to model real services using it. On the other hand, if the language
is overly complex, then the space of possible hypotheses will be so large that learning will
not be feasible. The language we choose is that of conjunctive queries in Datalog, which is
4. Influential Information Integration systems include TSIMMIS (Garcia-Molina, Hammer, Ireland, Papakonstantinou, Ullman, & Widom, 1995), SIMS (Arens, Knoblock, & Shen, 1996), InfoMaster (Duschka,
1997), and Ariadne (Knoblock, Minton, Ambite, Ashish, Muslea, Philpot, & Tejada, 2001).

9

Carman & Knoblock

a highly expressive relational query language. In this section we argue why a less expressive
language is not sufficient for our purposes.
Researchers interested in the problem of assigning semantics to Web Services (Heß &
Kushmerick, 2003) have investigated the problem of using Machine Learning techniques to
classify services (based on metadata characteristics) into different semantic domains, such
as weather and flights, and the operations they provide into different classes of operation,
such as weatherForecast and flightStatus. From a relational perspective, we can consider
the different classes of operations as relations. For instance, consider the definition below:
source($zip, temp) :- weatherForecast(zip, tomorrow, temp).
The source provides weather data by selecting tuples from a relation called weatherForecast,
which has the desired zipcode and date equal to tomorrow. This query is referred to as a
select-project query because its evaluation can be performed using the relational operators
selection and projection. So far so good, we have been able to use a simple classifier to learn
a simple definition for a source. The limitation imposed by this restricted (select-project)
modeling language becomes obvious, however, when we consider slightly more complicated
sources. Consider a source that provides the temperature in Fahrenheit as well as Celsius.
In order to model such a source using a select-project query, we would require that the
weatherForecast relation be extended with a new attribute as follows:
source($zip, tempC, tempF):- weatherForecast(zip, tomorrow, tempC, tempF).
The more attributes that could conceivably be returned by a weather forecast operation
(such as dewpoint, humidity, temperature in Kelvin, latitude, etc.), the longer the relation
will need to be to cover them all. Better, in this case, would be to introduce a second
relation convertCtoF that makes explicit the relationship between the temperature values.
If, in addition, the source limits its output to zipcodes in California, a reasonable definition
for the source might be:
source($zip, tempC, tempF):weatherForecast(zip, tomorrow, tempC), convertCtoF(tempC, tempF),
state(zip, California).
This definition is no longer expressed in the language of select-project queries, because it now
involves multiple relations and joins across them. Thus from this simple example, we see that
modeling services using simple select-project queries is not sufficient for our purposes. What
we need are select-project-join queries, also referred to as conjunctive queries.5 The reader
has already been introduced to examples of conjunctive queries throughout the previous
sections. Conjunctive queries form a subset of the logical query language Datalog and can
be described more formally as follows:
A conjunctive query over a set of relations R is an expression of the form:
q(X0 ) :- r1 (X1 ), r2 (X2 ), ..., rl (Xl ).
where each ri ∈ R is a relation and Xi is an ordered set of variable names of size
arity(ri ).6 Each conjunct ri (Xi ) is referred to as a literal. The set of variables
S
in the query, denoted vars(q) = li=0 Xi , consists of distinguished variables
X0 (from the head of the query), and existential variables vars(q)\X0 , (which
5. Evaluating a select-project-join query requires additional relational operators: natural join and rename.
6. Note that a conjunctive query can also be expressed in first order logicSas follows:
l
∀X0 ∃Y s.t. r1 (X1 ) ∧ r2 (X2 ) ∧ ... ∧ rl (Xl ) → q(X0 ) where X0 ∪ Y = i=1 Xi

10

Learning Semantic Definitions of Information Sources on the Internet

only appear in the body). A conjunctive query is said to be safe if all the
S
distinguished variables appear in the body, i.e. X0 ⊆ li=1 Xi .
3.2 More Expressive Languages
Modeling sources using conjunctive queries implies that aggregate operators like MIN and
ORDER cannot be used in source definitions. The functionality of most sources can be
described without such operators. Some sources can only be described poorly, however.
Consider a hotel search service that returns the 20 closest hotels to a given location:
hotelSearch($loc, hotel, dist) :accommodation(hotel, loc1), distance(loc, loc1, dist).
According to the definition, the source should return all hotels regardless of distance. One
cannot express the fact that only the closest hotels will be returned. The reason for not
including aggregate operators in the hypothesis language is that the search space associated
with learning definitions is prohibitively large. (Thus we leave aggregate operators to future
work as discussed in section 9.2.)
Similarly, source definitions cannot contain disjunction, which rules out union and recursive queries. Again, this simplifying assumption holds for most information sources
and greatly reduces the search space. It means however, that a weather service providing
forecasts only for cities in the US and Canada would be modeled as:
s($city, temp) :- forecast(city, country, tomorrow, temp).
Since the definition does not restrict the domain of the country attribute, when confronted
with a request for the forecast in Australia, a mediator would proceed to call the service,
oblivious to any restriction on that attribute.
We also do not allow negation in the queries because source definitions very rarely
require it, so including it would needlessly complicate the search. In those rare cases where
the negation of a particular predicate is useful for describing certain types of sources, the
negated predicate can be included (as a distinct predicate) in the search. For instance, we
might use “≥” to describe a source, even though strictly speaking it is the negation of “<”.
3.3 Leveraging Known Sources
Our approach to the problem of discovering semantic definitions for new services is to
leverage the set of known sources when learning a new definition. Broadly speaking, we do
this by invoking the known sources (in a methodical manner) to see if any combination of
the information they provide matches the information provided by the new source. From a
practical perspective, this means in order to model a newly discovered source semantically,
we require some overlap in the data being produced by the new source and the set of known
sources. One way to understand this is to consider a new source producing weather data.
If none of the known sources produce any weather information, then there is no way for
the system to learn whether the new source is producing historical weather data, weather
forecasts - or even that it is describing weather at all. (In principle, one could try to guess
what the service is doing based on the type signature alone, but there would be no guarantee
that the definition was correct, making it of little use to a mediator.) Given this overlapping
data requirement, one might claim that there is little benefit in incorporating new sources.
We detail some of the reasons why this is not the case below.
11

Carman & Knoblock

The most obvious benefit of learning definitions for new sources is redundancy. If the
system is able to learn that one source provides exactly the same information as a currently
available source, then if the latter suddenly becomes unavailable, the former can be used
in its place. For example, if a mediator knows of one weather source providing current
conditions and learns that a second source provides the same or similar data, then if the
first goes down for whatever reason (perhaps because an access quota has been reached),
weather data can still be accessed from the second.
The second and perhaps more interesting reason for wanting to learn a definition for
a new source is that the new source may provide data which lies outside the scope of (or
simply was not present in) the data provided by the other sources. For example, consider a
weather service which provides temperature values for zipcodes in the United States. Then
consider a second source that provides weather forecasts for cities worldwide. If the system
can use the first source to learn a definition for the second, the amount of information
available for querying increases greatly.
Binding constraints on a service can make accessing certain types of information difficult
or inefficient. In this case, discovering a new source providing the same or similar data but
with a different binding pattern may improve performance. For example, consider a hotel
search service that accepts a zipcode and returns a set of hotels along with their star rating:
hotelSearch($zip, hotel, rating, street, city, state):accommodation(hotel, rating, street, city, state, zip).
Now consider a simple query for the names and addresses of all five star hotels in California:
q(hotel, street, city, zip):- accommodation(hotel, 5*, street, city, California, zip).
Answering this query would require thousands of calls to the known source, one for every
zipcode in California, and a mediator could only answer the query if there was another
source providing those zipcodes. In contrast, if the system had learnt a definition for a new
source which provides exactly the same data but with a different binding pattern (such as
the one below), then answering the query would require only one call to a source:
hotelsByState($state, $rating, hotel, street, city, zip):accommodation(hotel, rating, street, city, state, zip).
Often the functionality of a complex source can be described in terms of a composition of
the functionality provided by other simpler services. For instance, consider the motivating
example from section 2, in which the functionality provided by the new source was to calculate the distance in miles between two zipcodes. The same functionality could be achieved
by performing four different calls to the available sources. In that case, the definition learnt
by the system meant that any query regarding the distance between zipcodes could be handled more efficiently. In general, by learning definitions for more complicated sources in
terms of simpler ones, the system can benefit from computation, optimisation and caching
abilities of services providing complex functionality.
Finally, the newly discovered service may be faster to access than the known sources
providing similar data. For instance, consider a geocoding service that takes in an address
and returns the latitude and longitude coordinates of the location. Because of the variety
in algorithms used to calculate the coordinates, it’s not unreasonable for some geocoding
services to take a very long time (upwards of one second) to return a result. If the system
were able to discover a new source providing the same geocoding functionality, but using
12

Learning Semantic Definitions of Information Sources on the Internet

a faster algorithm, then it could locate and display many more addresses on a map in the
same amount of time.

4. Inducing Definitions
In this section we describe an algorithm for generating candidate definitions for a newly
discovered source. The algorithm forms the first phase in a generate and test methodology
for learning source definitions. We defer discussion of the testing phase to later in the paper.
We start by briefly discussing work on relational rule learning and then describe how our
algorithm builds upon these ideas.
4.1 Inductive Logic Programming
The language of conjunctive queries is a restricted form of first-order logic. In the Machine
Learning community, systems capable of learning models using first-order representations
are referred to as Inductive Logic Programming (ILP) systems or relational rule learners.
Because of the expressiveness of the modeling language, the complexity of learning is much
higher than for propositional rule learners (also called attribute-value learners), which form
the bulk of Machine Learning algorithms. Given our relational modeling of services, many
of the techniques developed in ILP should also apply to our problem.
The First Order Inductive Learner (foil) is a well known ILP search algorithm (CameronJones & Quinlan, 1994). It is capable of learning first-order rules to describe a target predicate, which is represented by a set of positive examples (tuples over the target relation,
denoted E + ) and optionally also a set of negative examples (E − ). The search for a viable
definition in foil starts with an empty clause7 and progressively adds literals to the body
(antecedent) of the rule, thereby making the rule more specific. This process continues until
the definition (denoted h) covers only positive examples and no negative examples:
E + ∩ EI [h] 6= ∅

and E − ∩ EI [h] = ∅

Usually a set of rules is learnt in this manner by removing the positive examples covered
by the first rule and repeating the process. (The set of rules is then interpreted as a union
query.) Search in foil is performed in a greedy best-first manner, guided by an information
gain-based heuristic. Many extensions to the basic algorithm exist, most notably those that
combine declarative background knowledge in the search process such as focl (Pazzani &
Kibler, 1992). Such systems are categorised as performing a top down search because they
start from an empty clause (the most general rule possible) and progressively specialize
the clause. Bottom up approaches, on the other hand, such as golem (Muggleton & Feng,
1990), perform a specific to general search starting from the positive examples of the target.
4.2 Search
We now describe the actual search procedure we use to generate candidate definitions for
a new source. The procedure is based on the top-down search strategy used in foil. The
algorithm takes as input a type signature for the new source and uses it to seed the search for
7. We use the terms clause and query interchangeably to refer to a conjunctive query in Datalog. An empty
clause is a query without any literals in the body (right side) of the clause.

13

Carman & Knoblock

input : A predicate signature s∗
output: The best scoring view definition vbest
invoke target with set of random inputs;
vbest ← empty clause s∗ ;
3 add vbest to empty queue;
4 while queue 6= ∅ ∧ time() < timeout ∧ i++ < limit do
5
v0 ← best definition from queue;
6
forall v1 ∈ expand(v0 ) do
7
insert v1 into queue;
8
while ∆eval(v1 ) > 0 do
9
forall v2 ∈ constrain(v1 ) do
10
insert v2 into queue;
11
if eval(v2 ) ≥ eval(v1 ) then v1 ← v2 ;
12
end
13
end
14
if eval(v1 ) ≥ eval(vbest ) then vbest ← v1 ;
15
end
16 end
17 return vbest ;
Algorithm 1: Best-first search through the space of candidate source definitions.
1

2

candidate definitions. (We will refer to the new source relation as the target predicate and
the set of known source relations as source predicates.) The space of candidate definitions is
enumerated in a best-first manner, with each candidate tested to see if the data it returns
is similar to the target. Pseudo-code describing the procedure is given in Algorithm 1.8
The first step in the algorithm is to invoke the new source with a representative set of
input tuples to generate examples of output tuples that characterise the functionality of the
source. This set of invocations must include positive examples (invocations for which output
tuples were produced) and if possible, also negative tuples (inputs for which no output was
returned). The algorithm’s ability to induce the correct definition for a source depends
greatly on the number of positive examples available. Thus a minimum number of positive
invocations of the source is imposed, meaning that the algorithm may have to invoke the
source repeatedly using different inputs until sufficient positive invocations can be recorded.
Selecting appropriate input values so as to successfully invoke a service is easier said than
done. We defer discussion of the issues and difficulties involved in successfully invoking the
new source to section 6.1, and assume for the moment that the induction system is able to
generate a table of values that represent its functionality.
The next step in the algorithm is to initialise the search by adding an empty clause to
the queue of definitions to expand. The rest of the algorithm is simply a best-first search
procedure. At each iteration the highest scoring but not yet expanded definition (denoted
v0 ) is removed from the queue and expanded by adding a new predicate to the end of the
8. The implementation of this algorithm used in the experiments of section 7.3 is available at:
http://www.isi.edu/publications/licensed-sw/eidos/index.html

14

Learning Semantic Definitions of Information Sources on the Internet

clause (see the next section for an example). Each candidate generated (denoted v1 ) is
added to the queue. The algorithm then progressively constrains the candidate by binding
variables of the newly added predicate, (see section 4.4). The eval function (see section
5.3) evaluates the quality of each candidate produced. The procedure stops constraining
the candidate when the change in the evaluation function (∆eval) drops to zero. It then
compares v1 to the previous best candidate vbest and updates the latter accordingly.
In principle the algorithm should terminate when the “perfect” candidate definition is
discovered - one that produces exactly the same data as the target. In practice that never
occurs because the sources are incomplete (don’t perfectly overlap with each other) and are
noisy. Instead the algorithm terminates when either the queue becomes empty, a time limit
is reached or a maximum number of iterations has been performed.
4.3 An Example
We now run through an example of the process of generating candidate definitions. Consider
a newly discovered source, which takes in a zipcode and a distance, and returns all the
zipcodes that lie within the given radius (along with their respective distances). The target
predicate representing this source is:
source5($zip1, $dist1, zip2, dist2)
Now assume that there are two known sources. The first is the source for which the definition
was learnt in the example from section 2, namely:
source4($zip1, $zip2, dist):centroid(zip1, lat1, long1), centroid(zip2, lat2, long2),
greatCircleDist(lat1, long1, lat2, long2, dist2), multiply(dist1, 1.609, dist2).
The second source isn’t actually a source but an interpreted predicate:
≤ (dist1, dist2).
The search for a definition for the new source might then proceed as follows. The first
definition to be generated is the empty clause:
source5($ , $ , , ).
The null character ( ) represents the fact that none of the inputs or outputs have any
restrictions placed on their values. Prior to adding the first literal (source predicate), the
system will check whether any output attributes echo the input values. In this case, given
the semantic types, two possibilities need to be checked:
source5($zip1, $ , zip1, ).
source5($ , $dist1, , dist1).
Assuming neither of these possibilities is true (i.e. improves the score), then literals will
be added one at a time to refine the definition. A literal is a source predicate with an
assignment of variable names to its attributes. A new definition must be created for every
possible literal that includes at least one variable already present in the clause. (For the
moment we ignore the issue of binding constraints on the sources being added.) Thus many
candidate definitions would be generated, including the following:
source5($zip1, $dist1, , ) :- source4($zip1, $ , dist1).
source5($zip1, $ , zip2, )
:- source4($zip1, $zip2, ).
source5($ , $dist1, , dist2) :- ≤(dist1, dist2).
15

Carman & Knoblock

Note that the semantic types in the type signature of the target predicate limit greatly the
number of candidate definitions that can be produced. The system then evaluates each of
these candidates in turn, selecting the best one for further expansion. Assuming the first of
the three has the best score, it would be expanded by adding another literal, forming more
complicated candidates such as the following:
source5($zip1, $dist1, , dist2) :- source4($zip1, $ , dist1), ≤(dist1, dist2).
This process continues until the system discovers a definition which perfectly describes the
source, or is forced to backtrack because no literal improves the score.
4.4 Iterative Expansion
The sources used in the previous example all had relatively low arity. On the Internet this is
rarely the case, with many sources producing a large number of attributes of the same type.
This is a problem because it causes an exponential number of definitions to be possible at
each expansion step. Consider for instance a stock price service, which provides the current,
high, low, market-opening and market-closing prices for a given ticker symbol. The type
signature for that service would be:
stockprice($ticker, price, price, price, price, price)
If the definition to which this predicate is to be added already contains k distinct price
variables, then the number of ways in which the price attributes of the new relation can be
P
assigned variable names is 5i=0 5i k i , which is prohibitively large even for moderate k.9 To
limit the search space in the case of such high-arity predicates, we first generate candidates
with a minimal number of bound variables in the new literal and progressively constrain
the best performing of these definitions within each expansion. (High arity predicates are
handled in a similar fashion in foil, Quinlan and Cameron-Jones, 1993.) For example,
consider using the source above to learn a definition for a new source with signature:
source6($ticker, price, price)
We start by adding literals to an empty definition as before. This time though, instead of
generating a literal for every possible assignment of variable names to the attributes of each
relation, we generate only the simplest assignments such that all of the binding constraints
are met. (This is the expand procedure referred to in Algorithm 1.) In this example, the
ticker symbol input of the stockprice source would need to be bound, generating a single
definition:
source6($tic, , ) :- stockprice($tic, , , , , ).
This definition would be evaluated, and then more constrained definitions are generated by
equating a variable from the same literal to other variables from the clause. (This is the
constrain procedure from Algorithm 1.) Two such definitions are shown below:
source6($tic, pri1, ) :- stockprice($tic, pri1, , , , ).
source6($tic, pri1, ) :- stockprice($tic, , pri1, , , ).
The best of these definitions would then be selected and constrained further, generating
definitions such as:
i
9. Intuitively, one can assign variable
 names to i attributes using k labels in k different ways. One can
choose i of the 5 attributes in 5i ways, and one can do that for i = 0, 1, .., 5. See Weber, Tausend, and
Stahl (1995) for a detailed discussion of the size of the hypothesis space in ILP.

16

Learning Semantic Definitions of Information Sources on the Internet

source6($tic, pri1, pri2) :- stockprice($tic, , pri1, pri2, , ).
source6($tic, pri1, pri2) :- stockprice($tic, , pri1, , pri2, ).
In this way, the best scoring literal can be found without the need to iterate over all of the
possible assignments of variables to attributes.
4.5 Domain Predicates vs. Source Predicates
In the examples of sections 4.3 and 4.4, the decision to perform search over the source
predicates rather than the domain predicates was made in an arbitrary fashion.10 In this
section we justify that decision. If one were to perform the search over the domain predicates
rather than the source predicates, then testing each definition would require an additional
query reformulation step. For example, consider the following candidate definition for
source5 containing the domain predicate centroid:
source5($zip1, $ , , ) :- centroid(zip1, , ).
In order to evaluate this candidate, the system would need to first treat the definition as
a query and reformulate it into a set of rewritings (that together form a union query) over
the various sources as follows:
source5($zip1, $ , , ) :- source4($zip1, $ , ).
source5($zip1, $ , , ) :- source4($ , $zip1, ).
This union query can then be executed against the available sources (in this case just
source4) to see what tuples the candidate definition returns. In practice however, if the
definitions for the known sources contain multiple literals (as they normally do) and the
domain relations are of high-arity (as they often are), then the search over the space of
conjunctions of domain predicates is often much larger than the corresponding search over
the space of conjunctions of source predicates. This is because multiple conjunctions of
domain predicates (candidate definitions) end up reformulating to the same conjunction of
source predicates (union queries). For example, consider the following candidate definitions
written in terms of the domain predicates:
source5($zip1, $ , , ) :- centroid(zip1, lat, ), greatCircleDist(lat, , , , ).
source5($zip1, $ , , ) :- centroid(zip1, , lon), greatCircleDist( , lon, , , ).
source5($zip1, $ , , ) :- centroid(zip1, lat, lon), greatCircleDist(lat, lon, , , ).
All three candidates would reformulate to the same query over the sources (shown below),
and thus are indistinguishable given the sources available.
source5($zip1, $ , , ) :- source4($zip1, $ , ).
In general, the number of candidate definitions that map to the same reformulation can be
exponential in the number of hidden variables present in the definitions of the known sources.
For this reason, we simplify the problem and search the space of conjunctions of source
predicates. In some sense, performing the search over the source predicates can be seen as
introducing a “similarity heuristic” which focuses the search toward definitions with similar
structure to the definitions of the available sources. We note that the definitions produced
can (and will) later be converted to queries over the global predicates by unfolding and
10. A note on the difference between domain, source and interpreted predicates. Domain predicates are
invented by a domain expert for use in modeling a particular information domain. They define a common
schema that can be used for describing information from different sources. Source predicates represent
the sources available in the system. Interpreted predicates, (such as “≤”), are a special type of domain
predicate, that can be treated as source predicates, since their meaning is interpreted (understood).

17

Carman & Knoblock

possibly tightening them to remove redundancies. We will discuss the process of tightening
the unfoldings in section 6.6.
4.6 Limiting the Search
The search space generated by this top-down search algorithm may be very large even for
a small number of sources. The use of semantic types limits greatly the ways in which
variables within each definition can be equated (aka the join paths) and thus goes a long
way to reduce the size of the search space. Despite this reduction, as the number of sources
available increases, the search space becomes so large that techniques for limiting it must
be used. We employ some standard (and other not so standard) ILP techniques for limiting
this space. Such limitations are often referred to as inductive search bias or language bias
(Nédellec, Rouveirol, Adé, Bergadano, & Tausend, 1996).
An obvious way to limit the search is to restrict the number of source predicates that
can occur in a definition. Whenever a definition reaches the maximum length, backtracking
can be performed, allowing the search to escape from local minima that may result from
greedy enumeration. The assumption here is that shorter definitions are more probable
than longer ones, which makes sense since service providers are likely to provide data in
the simplest form possible. Moreover, the simpler the definition learnt, the more useful
it will be to a mediator, so we decide to trade completeness (the ability to express longer
definitions) for improved accuracy over shorter definitions.
The second restriction placed on candidate definitions is to limit the number of times
the same source predicate appears in a given candidate. This makes sense because the
definitions of real services tend not to contain many repeated predicates. Intuitively, this
is because most services provide raw data without performing many calculations upon it.
Repeated use of the same predicate in a definition is more useful for describing some form
of calculation than raw data itself. (Exceptions to this rule exist, for example predicates
representing unit conversion functionality such as Fahrenheit to Celsius, may necessarily
occur multiple times in the definition of a source.)
The third restriction limits the complexity of the definitions generated by reducing the
number of literals that do not contain variables from the head of the clause. Specifically,
it limits the level of existential quantification (sometimes also referred to as the depth,
Muggleton and Feng, 1990) of each variable in a clause. This level is defined to be zero
for all distinguished variables (those appearing in the head of the clause). For existential
variables it is defined recursively as one plus the lowest level of any variable appearing in the
same literal. For example, the candidate definition shown below has a maximum existential
quantification level of three because the shortest path from the last literal to the head literal
(via join variables) passes through two other literals:
source5($zip1, $ , , ) :- source4($zip1, $ , d1), source3($d1, d2), source3($d2, ).
The effect of this bias is to concentrate the search around simpler but highly connected
definitions, where each literal is closely linked to the input and output of the source.
The fourth restriction placed on source definitions is that they are executable. More
specifically, it should be possible to execute them from left to right, meaning that the inputs
of each source appear either in the target predicate (head of the clause) or in one of the
literals to the left of that literal. For example, of the two candidate definitions shown below,
18

Learning Semantic Definitions of Information Sources on the Internet

only the first is executable. The second definition is not, because zip2 is used as input for
source4 in the first literal, without first being bound to a value in the head of the clause:
source5($zip1, $ , zip2, ) :- source4($zip1, $zip2, ).
source5($zip1, $ , , ) :- source4($zip1, $zip2, dist1), source4($zip2, $zip1, dist1).
This restriction serves two purposes. Firstly, like the other biases, it limits the size of the
search space. Secondly, it makes it easier to evaluate the definitions produced. In theory,
one could still evaluate the second definition above by generating lots of input values for
zip2, but that would require a lot of invocations for minimal gain.
The last restriction reduces the search space by limiting the number of times the same
variable can appear in any given literal in the body of the clause. Definitions in which the
same variable appears multiple times in a given literal, such as in the following example
which returns the distance between a zipcode and itself, are not very common in practice:
source5($zip1, $ , , dist2) :- source4($zip1, $zip1, dist2).
Explicitly preventing such definitions from being generated makes sense because sources
requiring them are so rare, that it is better to reduce the search space exponentially by
ignoring them, than to explicitly check for them each time.

5. Scoring Definitions
We now proceed to the problem of evaluating the candidate definitions generated during
search. The basic idea is to compare the output produced by the source with the output
produced by the definition on the same input. The more similar the set of tuples produced,
the higher the score for the candidate. The score is then averaged over a set of different
input tuples to see how well the candidate definition describes the new source.
In the motivating example of section 2, the source for which a definition was being learnt
(the definition is repeated below) only produced one output tuple hdisti for every input
tuple hzip1, zip2i:
source4($zip1, $zip2, dist):centroid(zip1, lat1, long1), centroid(zip2, lat2, long2),
greatCircleDist(lat1, long1, lat2, long2, dist2),
multiply(dist1, 1.6093, dist2).
This fact made it simple to compare the output of the service with the output of the induced
definition. In general however, the source to be modeled (and the candidate definitions
modeling it) may produce multiple output tuples for each input tuple. Take for example
source5 from section 4.3, which produces the set of output tuples hzip2, dist2i containing
all the zipcodes which lie within a given radius of the input zipcode hzip1, dist1i. In
such cases, the system needs to compare a set of output tuples with the set produced by
the definition to see if any of the tuples are the same. Since both the new source and the
existing sources may not be complete, the two sets may simply overlap, even if the candidate
definition correctly describes the new source. Assuming that we can count the number of
tuples that are the same, we need a measure that tells us how well a candidate hypothesis
describes the data returned by a source. One such measure is the following:
score(s, v, I) =

1 X |Os (i) ∩ Ov (i)|
|I| i∈I |Os (i) ∪ Ov (i)|
19

Carman & Knoblock

where s is the new source, v is a candidate source definition, and I ⊆ D[βs ] is the set of
input tuples used to test the source (βs is the set of input attributes of source s). Os (i)
denotes the set of tuples returned by the new source when invoked with input tuple i. Ov (i)
is the corresponding set returned by the candidate definition. Using relational projection
(π) and selection (σ) operators and the notation introduced in section 2.1, these sets can
be written as follows. (Note that βsc represents the output attributes of s.)
Os (i) ≡ πβsc (σβs =i (E[s]))

and

Ov (i) ≡ πβsc (σβs =i (EI [v]))

If we view this hypothesis testing as an information retrieval task, we can consider recall
to be the number of common tuples, divided by the number of tuples produced by the
source, and precision to be the number of common tuples divided by the number of tuples
produced by the definition. The above measure takes both precision and recall into account
by calculating the average Jaccard similarity between the sets. The table below gives an
example of how this score is calculated for each input tuple.
input tuple
i∈I
ha, bi
hc, di
he, f i
hg, hi
hi, ji

actual output
tuples Os (i)
{hx, yi, hx, zi}
{hx, wi, hx, zi}
{hx, wi, hx, yi}
∅
∅

predicted output
tuples Ov (i)
{hx, yi}
{hx, wi, hx, yi}
{hx, wi, hx, yi}
{hx, yi}
∅

Jaccard similarity
for tuple i
1/2
1/3
1
0
#undef!

The first two rows of the table show inputs for which the predicted and actual output
tuples overlap. In the third row, the definition produces exactly the same set of tuples
as the source being modeled and thus gets the maximum score. In the fourth row, the
definition produced a tuple, while the source didn’t, so the definition was penalised. In the
last row, the definition correctly predicted that no tuples would be output by the source.
Our score function is undefined at this point. From a certain perspective the definition
should score well here because it has correctly predicted that no tuples be returned for that
input, but giving a high score to a definition when it produces no tuples can be dangerous.
Doing so may cause overly constrained definitions that can generate very few output tuples
to score well. At the same time, less constrained definitions that are better at predicting the
output tuples on average may score poorly. For example, consider a source which returns
weather forecasts for zipcodes in Los Angeles:
source($zip, temp) :- forecast(zip, tomorrow, temp), UScity(zip, Los Angeles).
Now consider two candidate definitions for the source. The first returns the temperature
for a zipcode, while the second returns the temperature only if it is below 0◦ C:
v1 ($zip, temp) :- forecast(zip, tomorrow, temp).
v2 ($zip, temp) :- forecast(zip, tomorrow, temp), temp < 0 ◦ C .
Assume that the source and candidates are invoked using 20 different randomly selected
zipcodes. For most zipcodes, the source will not return any output, because the zipcode
will lie outside of Los Angeles. The first candidate will likely return output for all zipcodes,
while the second candidate would, like the source, only rarely produce any output. This is
because the temperature in most zipcodes will be greater than zero, and has nothing to do
20

Learning Semantic Definitions of Information Sources on the Internet

with whether or not the zipcode is in Los Angeles. If we score definitions highly when they
correctly produce no output, the system would erroneously prefer the second candidate over
the first (because the latter often produces no output). To prevent that from happening,
we simply ignore inputs for which the definition correctly predicts zero tuples. This is the
same as setting the score to be the average of the other values.
Returning our attention to the table, after ignoring the last row, the overall score for
this definition would be calculated as 0.46.
5.1 Partial Definitions
As the search proceeds toward the correct definition for the service, many semi-complete
(unsafe) definitions will be generated. These definitions will not produce values for all
attributes of the target tuple but only a subset of them. For example, the candidate:
source5($zip1, $dist1, zip2, ) :- source4($zip1, $zip2, dist1).
produces only one of the two output attributes produced by the source. This presents a
problem, because our score is only defined over sets of tuples containing all of the output
attributes of the new source. One solution might be to wait until the definitions become
sufficiently long as to produce all outputs, before comparing them to see which one best
describes the new source. There are, however, two reasons why this would not make sense:
• The space of safe definitions is too large to enumerate, and thus we need to compare
partial definitions to guide the search toward the correct definition.
• The best definition that the system can generate may well be a partial one, as the set
of known sources may not be sufficient to completely model the source.
The simplest way to compute a score for a partial definition is to compute the same function
as before, but instead of using the raw source tuples, projecting them over the subset of
attributes that are produced by the definition. This revised score is shown below. (Note
that the projection is over v\βs , which denotes the subset of output attributes of s which
are produced by the view definition v. Note also that the projection is not distinct, i.e.
multiple instances of the same tuple may be produced.)
score2 (s, v, I) =

1 X |πv\βs (Os (i)) ∩ Ov (i)|
|I| i∈I |πv\βs (Os (i)) ∪ Ov (i)|

This revised score is not very useful however, as it gives an unfair advantage to definitions
that do not produce all of the output attributes of the source. This is because it is far easier
to correctly produce a subset of the output attributes than to produce all of them. Consider
for example the two source definitions shown below. The two definitions are identical except
that the second returns the output distance value dist2, while the first does not:
source5($zip1, $dist1, zip2, )
:- source4($zip1, $zip2, dist2), ≤(dist2, dist1).
source5($zip1, $dist1, zip2, dist2):- source4($zip1, $zip2, dist2), ≤(dist2, dist1).
Since the two are identical, the projection over the subset will in this case return the same
number of tuples. This means that both definitions would get the same score although the
second definition is clearly better than the first since it produces all of the required outputs.
We need to be able to penalise partial definitions in some way for the attributes they
don’t produce. One way to do this is to first calculate the size of the domain |D[a]| of each
21

Carman & Knoblock

of the missing attributes. In the example above, the missing attribute is the distance value.
Since distance is a continuous value, calculating the size of its domain is not obvious. We
can approximate the size of its domain by:
|D[distance]| ≈

max − min
accuracy

where accuracy is the error-bound on distance values. (We will discuss error-bounds further
in section 5.4.) Note that the cardinality calculation may be specific to each semantic type.
Armed with the domain size, we can penalise the score for the definition by dividing it
by the product of the size of the domains of all output attributes not generated by the
definition. In essence, we are saying that all possible values for these extra attributes have
been “allowed” by this definition. This technique is similar to techniques used for learning
without explicit negative examples (Zelle, Thompson, Califf, & Mooney, 1995).
The set of missing output attributes is given by the expression βsc \v, thus the penalty
for missing attributes is just the size of the domain of tuples of that scheme, i.e.:
penalty = |D[βsc \v]|
Using this penalty value we can calculate a new score, which takes into account the missing
attributes. Simply dividing the projected score by the penalty would not adhere to the
intended meaning of compensating for the missing attribute values, and thus may skew the
results. Instead, we derive a new score by introducing the concept of typed dom predicates
as follows:
A dom predicate for a semantic data-type t, denoted domt , is a single arity
relation whose extension is set to be the domain of the datatype, i.e. E[domt ] =
D[t]. Similarly, a dom predicate for a scheme A, denoted domA , is a relation
over A whose extension is E[domA ] = D[A].
Dom predicates were introduced by Duschka to handle the problem of query reformulation
in the presence of sources with binding constraints (Duschka, 1997). (In that work the
predicates were not typed, although typing would have resulted in a more efficient algorithm.) Here we use them to convert a partial definition v into a safe (complete) definition
v 0 . We can do this simply by adding a dom predicate to the end of the view definition that
generates values for the missing attributes. For the example above, v 0 would be:
source5($zip1, $dist1, zip2, x) :source4($zip1, $zip2, dist2), ≤(dist2, dist1), domdistance (x).
where x is a new variable of type distance. The new view definition v 0 is safe, because
all the variables in the head of the clause also appear in the body. In general, we can
turn any unsafe view definition v into a safe definition v 0 by appending a dom predicate
domβsc \v (x1 , ..., xn ), where each xi is a distinguished variable (from the head of the clause)
corresponding to an output attribute of v 0 that wasn’t bound in v. Now we can use this
complete definition to calculate the score as before:
score3 (s, v, I) = score(s, v 0 , I) =

22

1 X |Os (i) ∩ Ov0 (i)|
|I| i∈I |Os (i) ∪ Ov0 (i)|

Learning Semantic Definitions of Information Sources on the Internet

which can be rewritten (by expanding the denominator) as follows:
score3 (s, v, I) =

|Os (i) ∩ Ov0 (i)|
1 X
|I| i∈I |Os (i)| + |Ov0 (i)| − |Os (i) ∩ Ov0 (i)|

We can then remove the references to v 0 from this equation by considering:
Ov0 (i) = Ov (i) × E[domβsc \v ] = Ov (i) × D[βsc \v]
Thus the size of the set is given by |Ov0 (i)| = |Ov (i)||D[βsc \v]| and the size of the intersection
can be calculated by taking the projection over the output attributes produced by v:
|Os (i) ∩ Ov0 (i)| = |πv\βs (Os (i) ∩ Ov (i) × D[βsc \v])| = |πv\βs (Os (i)) ∩ Ov (i)|
Substituting these cardinalities into the score function given above, we arrive at the following
equation for the penalised score:
score3 (s, v, I) =

|πv\βs (Os (i)) ∩ Ov (i)|
1 X
|I| i∈I |Os (i)| + |Ov (i)||D[βsc \v]| − |πv\βs (Os (i)) ∩ Ov (i)|

5.2 Binding Constraints
Some of the candidate definitions generated during the search may have different binding
constraints from the target predicate. For instance in the partial definition shown below,
the variable zip2 is an output of the target source, but an input to source4 :
source5($zip1, $dist1, zip2, ) :- source4($zip1, $zip2, dist1).
From a logical perspective, in order to test this definition correctly, we need to invoke
source4 with every possible value from the domain of zipcodes. Doing this is not practical
for two reasons: firstly, the system may not have a complete list of zipcodes at its disposal.
Secondly and far more importantly, invoking source4 with thousands of different zipcodes
would take a very long time and would probably result in the system being blocked from
further use of the service. So instead of invoking the same source thousands of times,
we approximate the score for this definition by sampling from the domain of zipcodes
and invoking the source using the sampled values. We then compensate for this sampling
by scaling (certain components of) the score by the ratio of the sampled zipcodes to the
entire domain. Considering the example above, if we randomly choose a sample (denoted
δ[zipcode]) of say 20 values from the domain of zipcodes, then the set of tuples returned by
the definition will need to be scaled by a factor of |D[zipcode]|/20.
A more general equation for computing the scaling factor (denoted SF ) is shown below.
Note that the sampling may need to be performed over a set of attributes. (Here βv \βs
denotes the input attributes of v which are outputs of s.)
SF =

|D[βv \βs ]|
|δ[βv \βs ]|

We now calculate the effect of this scaling factor on the overall score as follows. We denote
the set of tuples returned by the definition given the sampled input as Õv (i). This value
23

Carman & Knoblock

when scaled will approximate the set of tuples that would have been returned had the
definition been invoked with all the possible values for the additional input attributes:
|Ov (i)| ≈ |Õv (i)| ∗ SF
Assuming the sampling is performed randomly over the domain of possible values, the
intersection between the tuples produced by the source and the definition should scale in
the same way. Thus the only factor not affected by the scaling in the score defined previously
is |Os (i)|. If we divide throughout by the scaling factor we have a new scoring function:
score4 (s, v, I) =

|πv\βs (Os (i)) ∩ Õv (i)|
1 X
|I| i∈I |Os (i)|/SF + |Õv (i)||D[βsc \v]| − |πv\βs (Os (i)) ∩ Õv (i)|

The problem with this approach is that often the sampled set of values is too small and as a
result it does not intersect with the set of values returned by the source, even though a larger
sample would have intersected in some way. Thus our sampling introduces unfair distortions
into the score for certain definitions, causing them to perform poorly. For example, consider
again source5 and assume that for scalability purposes, the service places a limit on the
maximum value for the input radius dist1. (This makes sense, as otherwise the user could
set the input radius to cover the entire US, and a tuple for every possible zipcode would
need to be returned.) Now consider the sampling performed above. If we randomly choose
only 20 zipcodes from the set of all possible zipcodes, the chance of the sample containing
a zipcode which lies within a 300 mile radius of a particular zipcode (in the middle of the
desert) is very low. Moreover, even if one pair of zipcodes (out of 20) results in a successful
invocation, this will not be sufficient for learning a good definition for the service.
So to get around this problem we bias the sample such that, whenever possible, half of
the values are taken from positive examples of the target (those tuples returned by the new
source) and half are taken from negative examples (those tuples not returned by the source).
By sampling from both positive and negative tuples, we guarantee that the approximation
generated will be as accurate as possible given the limited sample size. We denote the set
of positive and negative samples as δ + [βv \βs ] and δ − [βv \βs ], and use these values to define
positive and total scaling factors as shown below. (The numerator for the positive values is
different from before, as these values have been taken from the output of the new source.)
SF + =

|πβv \βs (πv\βs (Os (i)))|
|δ + [βv \βs ]|

The total scaling factor is the same value as before, but calculated slightly differently:
SF =

|D[βv \βs ]|
|δ + [βv \βs ]| + |δ − [βv \βs ]|

The score can then be approximated accordingly by taking into account these new scaling
factors. The intersection now needs to be scaled using the positive scaling factor:
|πv\βs (Os (i)) ∩ Ov (i)| ≈ |πv\βs (Os (i)) ∩ Õv (i)| ∗ SF +
This new scaling results in a new function for evaluating the quality of a view definition:
score5 (s, v, I) =

|πv\βs (Os (i)) ∩ Õv (i)| ∗ SF +
1 X
|I| i∈I |Os (i)| + |Õv (i)||D[βsc \v]| ∗ SF − |πv\βs (Os (i)) ∩ Õv (i)| ∗ SF +
24

Learning Semantic Definitions of Information Sources on the Internet

5.3 Favouring Shorter Definitions
Now that we have derived a score for comparing the data that a source and candidate
produce, we can define the evaluation function eval used in Algorithm 1. As mentioned
in section 4.6, shorter definitions for the target source should be preferred over longer and
possibly less accurate ones. In accordance with this principle, we scale the score by the
length of the definition, so as to favour shorter definitions as follows:
eval(v) = ω length(v) ∗ score5 (s, v, I)
Here length(v) is the length of the clause and ω < 1 is a weighting factor. Setting the
weighting factor to be a little less than 1 (such as 0.95) helps to remove logically redundant
definitions, which can sometimes be hard to detect, but often return almost exactly the same
score as their shorter equivalent. We will discuss the problem of generating non-redundant
clauses in section 6.3.
5.4 Approximating Equality
Until now, we have ignored the problem of deciding whether two tuples produced by the
target source and the definition are the same. Since different sources may serialize data in
different ways and at different levels of accuracy, we must allow for some flexibility in the
values that the tuples contain. For instance, in the example from section 2, the distance
values returned by the source and definition did not match exactly, but were “sufficiently
similar” to be accepted as the same value.
For numeric types like temperature or distance it makes sense to use an error bound (like
±0.5◦ C) or a percentage error (such as ±1%) to decide if two values can be considered the
same. This is because the sensing equipment (in the case of temperature) or the algorithm (in
the case of distance) will have some error bound associated with the values it produces. We
require that an error bound for each numeric type be provided in the problem specification.
(Ideally, these bounds would be learnt automatically from examples.)
For certain nominal types like company names, where values like hIBM Corporationi
and hInternational Business Machines Corp.i represent the same value, simplistic equality
checking using exact or substring matches is not sufficient for deciding whether two values
correspond to the same entity. In this case, string edit distances such as the JaroWinkler
score do better at distinguishing strings representing the same entity from those representing
different ones (Bilenko, Mooney, Cohen, Ravikumar, & Fienberg, 2003). A machine learning
classifier could be trained on a set of such examples to learn which of the available string
edit distances best distinguishes values of that type and what threshold to set for accepting
a pair as a match. We require that this pair of similarity metric and threshold (or any
combinations of metrics) be provided in the problem specification.
In other cases, enumerated types like months of the year might be associated with a
simple equality checking procedure, so that values like hJanuaryi, hJani and h1i can be
found equal. The actual equality procedure used will depend on the semantic type and we
assume in this work that such a procedure is given in the problem definition. We note that
the procedure need not be 100% accurate, but only provide a sufficient level of accuracy
to guide the system toward the correct source description. Indeed, the equality rules could
also be generated offline by training a classifier.
25

Carman & Knoblock

Complex types such as date present a bigger problem when one considers the range of
possible serializations, including values like h5/4/2006i or hThu, 4 May 2006i or h2006-05-04i.
In such cases specialized functions are not only required to check equality between values
but also to break the complex types up into their constituent parts (in this case day, month
and year ). The latter would form part of the domain model.
In some cases, deciding whether two values of the same type can be considered equal
depends not only on the type, but also on the relations they are used in. Consider the
two relations shown below. The first provides the latitude and longitude coordinates of the
centroid for a zipcode, while the second returns the coordinates for a particular address:
centroid(zipcode, latitude, longitude)
geocode(number, street, zipcode, latitude, longitude)
Given the different ways of calculating the centroid of a zipcode (including using the center
of mass or the center of population density) an error bound of 500 meters might make sense
for equating latitude and longitude coordinates. For a geocoding service, on the other hand,
an error bound of 50 meters may be more reasonable. In general, such error bounds should
be associated with the set of global relations, instead of just the semantic types, and could
be learnt accordingly. When the relations contain multiple attributes, then the problem
of deciding whether two tuples refer to the same entity is called record linkage (Winkler,
1999). An entire field of research is devoted to tackling this problem. Due to the complexity
of the problem and the variety of techniques that have been developed to handle it, we do
not investigate it further here.

6. Extensions
In this section we discuss extensions to the basic algorithm needed for handling real data
sources, as well as ways to reduce the size of the hypothesis space and improve the quality
of the definitions produced.
6.1 Generating Inputs
The first step in the source induction algorithm is to generate a set of tuples which will
represent the target relation during the induction process. In other words, the system must
try to invoke the new source to gather some example data. Doing this without biasing the
induction process is easier said than done. The simplest approach to generating input values
is to select constants at random from the set of examples given in the problem specification.
The problem with this approach is that in some cases the new source will not produce any
output for the selected inputs. Instead the system may need to select values according to
some distribution over the domain of values in order for the source to invoke correctly. For
example, consider a source providing posts of used cars for sale in a certain area. The source
takes the make of the car as input, and returns car details:
usedCars($make, model, year, price, phone)
Although there are over a hundred different car manufacturers in the world, only a few
of them produce the bulk of the cars. Thus invoking the source with values like Ferrari,
Lotus and Aston Martin will be less likely to return any tuples, when compared with more
common brands such as Ford and Toyota (unless the source is only providing data for sports
cars of course). If a distribution over possible values is available, the system can first try
26

Learning Semantic Definitions of Information Sources on the Internet

the more common values, or more generally, it can choose values from that set according to
the distribution. In this particular example, it might not be too difficult to query the source
with a complete set of car manufacturers until one of the invocations returns some data.
In general, the set of examples may be very large (such as the 40,000+ zipcodes in the US)
and the number of “interesting” values in that set (the ones likely to return results) may
be very small, in which case taking advantage of prior knowledge about the distribution of
possible values makes sense. It should be noted also that during execution the system will
receive a lot of output data from the different sources it accesses. This data can be recorded
to generate distributions over possible values for the different types.
The problem of generating viable input data for a new source becomes yet more difficult
if the input required is not a single value but a tuple of values. In this case the system
can first try to invoke the source with random combinations of attribute values from the
examples of each type. Invoking some sources (such as source5 ) is easy because there is no
explicit restriction on the combination of input values:
source5($zip, $distance, zip, distance)
In other cases, such as a geocoding service the combination of possible input values is highly
restricted:
USGeocoder($number, $street, $zipcode, latitude, longitude)
Randomly selecting input values independently of one another is unlikely to result in any
successful invocations. (In order for the invocation to succeed, the randomly generated
tuple must correspond to an address that actually exists.) In such cases, after failing to
invoke the source a number of times, the system can try to invoke other sources (such as the
hotel lookup service below), which produce tuples containing the required attribute types:
HotelSearch($city, hotel, number, street, zipcode)
In general, this process of invoking sources to generate input for other sources can be chained
until a set of viable inputs is generated.
We note that the problem of synthesizing viable input data is itself a difficult and
interesting research problem. Our combined approach of utilizing value distributions and
invoking alternative services performs well in our experiments (see section 7.3), but an area
of future work is to develop a more general solution.
6.2 Dealing with Sources
In order to minimise source accesses, which can be very expensive in terms of both time and
bandwidth, all requests to the individual sources are cached in a local relational database.
This implementation means that there is an implicit assumption in this work that the
output produced by the services is constant for the duration of the induction process. This
could be problematic if the service being modeled provides (near) real-time data with an
update frequency of less than the time it takes to induce a definition. For a weather
prediction service, updated hourly, this may not present much of a problem, since the
difference between predicted temperatures may vary only slightly from one update to the
next. For a real-time flight status service providing the coordinates of a given aircraft
every five minutes, the caching may be problematic as the location of the plane will vary
greatly if it takes, for example, one hour to induce a definition. In theory one could test
for such variation systematically by periodically invoking the same source with a previously
27

Carman & Knoblock

successful input tuple to see if the output has changed, and update the caching policy
accordingly.
6.3 Logical Optimisations
Evaluating definitions can be expensive both in terms of time (waiting for sources to return data) and computation (calculating joins over large tables). Thus it makes sense to
check each candidate for redundancy before evaluating it. To decide which definitions are
redundant, we use the concept of query containment:
A query q1 ∈ LR,A is said to be contained in another query q2 ∈ LR,A if for any
database instance I, the set of tuples returned by the first query is a subset of
those returned by the second, i.e. ∀I EI [q1 ] ⊆ EI [q2 ]. We denote containment
by q1 v q2 . Two queries are considered logically equivalent if q1 v q2 ∧ q2 v q1 .
For the conjunctive queries learnt in this paper, testing for query containment reduces to
the problem of finding a containment mapping (Chandra & Merlin, 1977).11 We can use
this test to discover logically equivalent definitions such as the following, (which contain a
reordering of the same literals):
source($zip, temp):- getCentroid($zip, lat, lon), getConditions($lat, $lon, temp).
source($zip, temp):- getConditions($lat, $lon, temp), getCentroid($zip, lat, lon).
Such equivalence checking can be performed efficiently if a canonical ordering of predicate
and variable names is chosen a priori. Whenever logically equivalent definitions are discovered, the search can backtrack, thereby avoiding entire sub-trees of equivalent clauses.
Similarly, we can test for and skip logically redundant clauses such as the following (which
is equivalent to a shorter definition without the second literal):
source($zip, ):- getCentroid($zip, lat, long), getCentroid($zip, lat, ).
Again, such redundancy checking can be performed efficiently (Levy, Mendelzon, Sagiv, &
Srivastava, 1995) resulting in little computational overhead during search.
6.4 Functional Sources
More information may be known about the functionality of certain sources than is expressed
by their source definitions. For example, sources like Multiply and Concatenate, which are
implemented locally, will be known to be complete. (A source is considered complete, if
it returns all of the tuples implied by its definition, i.e. E[s] = EI [v].) Whenever such
information is available, the induction system can take advantage of it to improve search
efficiency. To explain how, we define a class of sources that we call functional sources,
which are complete and for any input tuple return exactly one output tuple. This is slightly
more restrictive than the standard ILP concept of determinate literals (Cameron-Jones &
Quinlan, 1994), which for every input tuple return at most one output tuple. Multiply and
Concatenate are both examples of functional sources. The system takes advantage of the fact
that functional sources place no restrictions on their input. Whenever a functional source is
added to a candidate definition, the score for that definition doesn’t change providing all the
source’s inputs and none of its outputs are bound. (The set of tuples returned by the new
11. If the queries contain interpreted predicates, then containment testing is a little more involved (Afrati,
Li, & Mitra, 2004).

28

Learning Semantic Definitions of Information Sources on the Internet

definition is the same as before, but with a few new attributes corresponding to the outputs
of the source.) Thus the new definition does not need to be evaluated, but can be added to
the queue (of definitions to expand) as is, which becomes particularly advantageous when
a source’s input arity is high.
6.5 Constants
Constants are often used in source descriptions to define the scope of a service. Consider a
weather service that provides reports for zipcodes only in California:
calWeather($zip, $date, temp) :- forecast(zip, date, temp), USstate(zip, California).
If a mediator receives a query asking for the forecast for Chicago, it will know that this
source is not relevant to the query since Chicago is not in California. Although constants
in source descriptions can be very useful, simply introducing them into the hypothesis
language could cause the search space to grow prohibitively. (For states, the branching
factor would be 50, while for zipcodes it would be in excess of 40,000.) Obviously a generate
and test methodology does not make sense when the domain of the semantic type is large.
Alternatively, one can explicitly check for repeated values in the tuples returned by the
new source (i.e. for constants in the head of the clause) and in the join of the source
and definition relations (i.e. for constants in the body of the clause). For example, in the
definition below the join of the source relation hzip, date, tempi with the definition relation
hzip, date, temp, statei would produce only tuples with state equal to California. So that
constant could be added to the definition.
source($zip, $date, temp) :- forecast(zip, date, temp), USstate(zip, state).
More complicated detection procedures would be required for discovering constants in interpreted predicates (i.e. range restrictions over numeric attributes).
6.6 Post-Processing
After a definition has been learnt for a new source, it may be possible to tighten that
definition by removing logical redundancies from its unfolding. Consider the following
definition involving calls to two hotel sources, one to check availability and the other to
check its rating:
source($hotel, address, rating):HotelAvailability($hotel, address, price), HotelRating($hotel, rating, address).
The unfolding of that definition (in terms of the definitions of the hotel sources) contains
two references to an accommodation relation:
source($hotel, address, rating):accommodation(hotel, , address), available(hotel, today, price),
accommodation(hotel, rating, address).
The first literal is redundant and can be removed from the unfolding. In general, the same
rules used to discover redundancy in candidate definitions can be used to remove redundant
literals from the unfolding. Moreover, since this post-processing step needs to be performed
only once, time can be spent searching for more complicated forms of redundancy.
29

Carman & Knoblock

7. Evaluation
In this section we describe our evaluation of the source induction algorithm. We first
describe the experimental setup used and then the experiments performed. Finally, we
compare the induction algorithm with a particular complex schema matching system.
7.1 Experimental Setup
The source induction algorithm defined in this paper was implemented in a system called
eidos, which stands for Efficiently Inducing Definitions for Online Sources. eidos implements the techniques and optimisations discussed in sections 4 through 6. (Certain
extensions from section 6 were only partially implemented: the implementation currently
checks for constants only in the head of the clause and does not perform any tightening of
the definitions.) All code was written in Java and a MySQL database was used for caching
the results of source invocations.
eidos was tested on 25 different problems involving real services from several domains
including hotels, financial data, weather and cars. The domain model used was the same
for each problem and included over 70 different semantic types, ranging from common
ones like zipcode to more specific types such as stock ticker symbols. The data model
also contained 36 relations (excluding interpreted predicates), which were used to model 33
different services. All of the modeled services are publicly available information sources.
We note here that the decision to use the same set of known sources for each problem
(regardless of the domain) was important in order to make sure that the tests were realistic.
This decision made the problem more difficult than the standard schema matching/mapping
scenario in which the source schema is chosen, because it provides data that is known a
priori to be relevant to the output schema.
In order to give a better sense of the problem setting and the complexity of the known
sources available, we list ten below (ordered by arity). Due to space limitations we don’t
show the complete list nor their definitions, only the input/output types for each source.
Note that all the sources share the semantic types latitude and longitude, which means that
the search space associated with these sources alone is very large.
1

WeatherConditions($city,state,country,latitude,longitude,time,time,timeoffset,
datetime,temperatureF,sky,pressureIn,direction,speedMph,humidity,temperatureF)
2 WeatherForecast($city,state,country,latitude,longitude,timeoffset,day,date,
temperatureF,temperatureF,time,time,sky,direction,speedMph,humidity)
3 GetDistance($latitude,$longitude,$latitude,$longitude,distanceKm)
4 USGeocoder($street,$zipcode,city,state,latitude,longitude)
5 ConvertDMS($latitudeDMS,$longitudeDMS,latitude,longitude)
6 USGSEarthquakes(decimal,timestamp,latitude,longitude)
7 GetAirportCoords($iata,airport,latitude,longitude)
8 CountryCode($latitude,$longitude,countryAbbr)
9 GetCentroid($zipcode,latitude,longitude)
10 Altitude($latitude,$longitude,distanceM)

In order to induce definitions for each problem, the source (and each candidate definition) was invoked at least 20 times using random inputs. Whenever possible, the system
attempted to generate 10 positive examples of the source (invocations for which the source
returned some tuples) and 10 negative examples (inputs which produced no output). To
30

Learning Semantic Definitions of Information Sources on the Internet

ensure that the search terminated, the number of iterations of the algorithm including backtracking steps was limited to 30. A search time limit of 20 minutes was also imposed. The
inductive search bias used during the experiments is shown below, and a weighting factor
(defined in section 5.3) of 0.9 was used to direct the search toward shorter definitions.
Search Bias
Maximum clause length = 7
Maximum predicate repetition = 2
Maximum variable level = 5
Executable candidates only
No variable repetition within a literal
In the experiments, different procedures were used to decide equality between values of the
same type as discussed in section 5.4. Some of the equality procedures used for different
types are listed below. The accuracy bounds and thresholds used were chosen to maximize
overall performance of the learning algorithm. (In practice, a meta-learning algorithm
could be used to determine the best accuracy bounds for different attribute types.) For all
semantic types not listed below, substring matching (checking if one string contained the
other) was used to test equality between values.
Types
latitudes, longitudes
distances, speeds, temperatures, prices
humidity, pressure, degrees
decimals
companies, hotels, airports
dates

Equality Procedure
accuracy bound of ±0.002
accuracy bound of ±1%
accuracy bound of ±1.0
accuracy bound of ±0.1
JaroWinkler score ≥ 0.85
specialised equality procedure

The experiments were run on a dual core 3.2 GHz Pentium 4 with 4 GB of RAM (although
memory was not a limiting factor in any of the tests). The system was running Windows
2003 Server, Java Runtime Environment 1.5 and MySQL 5.0.
7.2 Evaluation Criteria
In order to evaluate the induction system, one would like to compare for each problem the
definition generated by the system with the ideal definition for that source (denoted vbest
and v ∗ respectively). In other words, we would like an evaluation function, which rates
the quality of each definition produced with respect to a hand-written definition for the
source (i.e. quality : vbest × v ∗ → [0, 1]). The problem with this is twofold. Firstly, it is
not obvious how to define such a similarity function over conjunctive queries and many
different possibilities exist (see Markov and Marinchev, 2000, for a particular example).
Secondly, working out the best definition by hand, while taking into account the limitations
of the domain model and the fact that the available sources are noisy, incomplete, possibly
less accurate, and even serialise data in different ways, may be extremely difficult, if even
possible. So in order to evaluate each of the discovered definitions, we instead count the
number of correctly generated attributes in each definition. An attribute is said to be
correctly generated, if:
31

Carman & Knoblock

• it is an input, and the definition correctly restricts the domain of possible values for
that attribute, or
• it is an output, and the definition correctly predicts its value for given input tuples.
Consider the following definition that takes two input values and returns the difference and
its square root (providing the difference is positive):
source($A, $B, C, D) :- sum(B, C, A), product(D, D, C), A ≥ B.
and imagine that the induction system managed to learn only that the source returns the
difference between the input and the output, i.e.:
source($A, $B, C, ) :- sum(B, C, A).
We say that the first input attribute A is not correctly generated as it is an input and is not
constrained with respect to the input attribute B (the inequality is missing). The input B
is deemed correctly generated as it is present in the sum relation (only one input is penalised
for the missing inequality). The output C is deemed correctly generated with respect to
the inputs, and the missing attribute D isn’t generated at all. (Note that if the ordering of
variables in the sum relation had been different, say sum(A, B, C), then C would have been
generated, but not correctly generated.)
Given a definition for correctly generated attributes, one can define expressions for
precision and recall over the attributes contained in a source definition. We define precision
to be the ratio of correctly generated attributes to the total number of attributes generated
by a definition, i.e.:
precision =

# of correctly generated attributes
total # of generated attributes

We define recall to be the ratio of generated attributes to the total number of attributes
that would have been generated by the ideal definition, given the sources available. (In
some cases no sources are available to generate values for an attribute in which case, that
attribute is not included in the count.)
recall =

# of correctly generated attributes
total # of attributes that should have been generated

Note that we defined precision and recall at the schema level in terms of the attributes
involved in a source definition. They could also be defined at the data level in terms of
the tuples being returned by the source and the definition. Indeed, the Jaccard similarity
used to score candidate definitions is a combination of data-level precision and recall values.
The reason for choosing schema level metrics in our evaluation is that they better reflect
the semantic correctness of the learnt definition, in so far as they are independent of the
completeness (amount of overlap) between the known and target sources.
Returning to our example above, the precision for the simple definition learnt would be
2/3 and the recall would be 2/4. Note that, if the product relation had not been available
in our domain model (in which case attribute D could never have been generated), recall
would have been higher at 2/3.
32

Learning Semantic Definitions of Information Sources on the Internet

7.3 Experiments
The definitions learnt by the system are described below. Overall the system performed
very well and was able to learn the intended definition (ignoring missing join variables and
superfluous literals) in 19 out of the 25 problems.
7.3.1 Geospatial Sources
The first set of problems involved nine geospatial data sources providing a variety of location
based information. The definitions learnt by the system are listed below. They are reported
in terms of the source predicates rather than the domain relations (i.e. the unfoldings)
because the corresponding definitions are much shorter. This makes it easier to understand
how well the search algorithm is performing.
1
2
3

4
5
6

7
8
9

GetInfoByZip($zip0,cit1,sta2,_,tim4) :GetTimezone($sta2,tim4,_,_), GetCityState($zip0,cit1,sta2).
GetInfoByState($sta0,cit1,zip2,_,tim4) :GetTimezone($sta0,tim4,_,_), GetCityState($zip2,cit1,sta0).
GetDistanceBetweenZipCodes($zip0,$zip1,dis2) :GetCentroid($zip0,lat1,lon2), GetCentroid($zip1,lat4,lon5),
GetDistance($lat1,$lon2,$lat4,$lon5,dis10), ConvertKm2Mi($dis10,dis2).
GetZipCodesWithin($_,$dis1,_,dis3) :<(dis3,dis1).
YahooGeocoder($str0,$zip1,cit2,sta3,_,lat5,lon6) :USGeocoder($str0,$zip1,cit2,sta3,lat5,lon6).
GetCenter($zip0,lat1,lon2,cit3,sta4) :WeatherConditions($cit3,sta4,_,lat1,lon2,_,_,_,_,_,_,_,_,_,_,_),
GetZipcode($cit3,$sta4,zip0).
Earthquakes($_,$_,$_,$_,lat4,lon5,_,dec7,_) :USGSEarthquakes(dec7,_,lat4,lon5).
USGSElevation($lat0,$lon1,dis2) :ConvertFt2M($dis2,dis1), Altitude($lat0,$lon1,dis1).
CountryInfo($cou0,cou1,cit2,_,_,cur5,_,_,_,_) :GetCountryName($cou0,cou1), GoCurrency(cur5,cou0,_),
WeatherConditions($cit2,_,cou1,_,_,_,_,_,_,_,_,_,_,_,_,_).

The first two sources provide information about zipcodes, such as the name of the city, the
state and the timezone. They differ in their binding constraints, with the first taking a
zipcode as input, and the second taking a state. The second source returns many output
tuples per input value, making it harder to learn the definition, even though the two sources
provide logically the same information. The induced definitions are the best possible given
the known sources available. (None of them provided the missing output attribute, a telephone area-code.) The third source calculates the distance in miles between two zipcodes,
(it is the same as source4 from section 2). The correct definition was learnt for this source,
but for the next source, which returned zipcodes within a given radius, a reasonable definition could not be learnt within the time limit.12 Ignoring binding constraints, the intended
12. The recall for this problem is 1/4 because the input attribute dis1 is determined to be the only correctly
generated attribute (it is constrained with respect to the output attribute dis3 ), while all four attributes
should have been generated (the output attribute dis3 is not generated by the < predicate). The precision
is 1/1 because dis1 is the only generated attribute, and it is correctly generated.

33

Carman & Knoblock

definition was the same as the third, but with a restriction that the output distance be less
than the input distance. Thus it would have been far easier for eidos to learn a definition
for the fourth source in terms of the third. Indeed, when the new definition for the third
source was added to the set of known sources, the system was able to learn the following:
4’ GetZipCodesWithin($zip0,$dis1,zip2,dis3) :GetDistanceBetweenZipCodes($zip0,$zip2,dis3), <(dis3,dis1).

The ability of the system to improve its learning ability over time as the set of known
sources increases is a key benefit of the approach.
Source five is a geocoding service provided by Yahoo. (Geocoding services map addresses to latitude and longitude coordinates.) eidos learnt that the same functionality
was provided by a service called USGeocoder. Source six is a simple service providing the
latitude/longitude coordinates and the city and state for a given zipcode. Interestingly,
the system learnt that the source’s coordinates were better predicted by a weather conditions service (discussed in section 7.3.3), than by the GetCentroid source from the third
definition. Note that when the new source definition is unfolded it will contain extraneous predicates related to weather information.13 The additional predicates do not interfere
with the usefulness of the definition, however, as a query reformulation algorithm will still
use the source to answer the same queries regardless. (Thus precision and recall scores
are not affected.) A post-processing step to remove extraneous predicates is possible, but
would require additional information in the domain model.14 The seventh source provided
earthquake data within a bounding box which it took as input. In this case, the system
discovered that the source was indeed providing earthquake data, (lat4 and lon5 are the coordinates of the earthquake and dec7 is its magnitude). It didn’t manage, however, to work
out how the input coordinates related to the output. The next source provided elevation
data in feet, which was found to be sufficiently similar to known altitude data in metres.
Finally, the system learnt a definition for a source providing information about countries
such as the currency used, and the name of the capital city. Since known sources were not
available to provide this information, the system ended up learning that weather reports
were available for the capital of each country.
Problem
1
2
3
4
5
6
7
8
9

# Candidates
25
24
888
3
50
40
11
15
176

# Invocations
5068
9804
11136
11176
13148
15162
14877
177
28784

Time (s)
85
914
449
25
324
283
18
72
559

log10 (Score)
-1.36
-1.08
-0.75
0.25
-0.45
-7.61
-6.87
-8.58
-5.77

Precision
4/4
4/4
3/3
1/1
6/6
5/5
3/3
3/3
4/4

Recall
4/4
4/4
3/3
1/4
6/6
5/5
3/9
3/3
4/4

13. The unfolding is shown below. The conditions predicate could be removed without affecting its meaning:
GetCenter($zip0, lat1, lon2, cit3, sta4):- municipality(cit3, sta4, zip2, tim3), country( , cou5, ),
northAmerica(cou5), centroid(zip2, lat1, lon2), conditions(lat1, lon2, , , , , , , , , , , ),
timezone(tim3, , ), municipality(cit3, sta4, zip0, ).
14. In particular, universally quantified knowledge would be needed in the domain model, e.g.:
∀lat, lon ∃x1 , ..., x11 s.t. conditions(lat, long, x1 , ..., x11 )

34

Learning Semantic Definitions of Information Sources on the Internet

The table shows some details regarding the search performed to learn each of the definitions listed above. For each problem, it shows the number of candidates generated prior
to the winning definition, along with the time and number of source invocations required
to learn the definition. (The last two values should be interpreted with caution as they
are highly dependent on the delay in accessing sources, and on the caching of data in the
system.) The scores shown in the fifth column are a normalised version of the scoring
function used to compare the definitions during search. (Normalisation involved removing
the penalty applied for missing outputs.) The scores can be very small, so the logarithm
of the values is shown (hence the negative values).15 These scores can be interpreted as
the confidence the system has in the definitions produced. The closer the value is to zero,
the better the definition’s ability to produce the same tuples as the source. We see that
the system was far more confident about the definitions one through five, than the latter
ones.16 The last two columns give the precision and recall value for each problem. The
average precision for these problems was 100%. (Note that a high precision value is to be
expected, given that the induction algorithm relies on finding matching tuples between the
source and definition.) The average recall for the geospatial problems was also very high at
84%.
7.3.2 Financial Sources
Two sources were tested that provided financial data. The definitions generated by eidos
for these sources are shown below.
10 GetQuote($tic0,pri1,dat2,tim3,pri4,pri5,pri6,pri7,cou8,_,pri10,_,_,pri13,_,com15):YahooFinance($tic0,pri1,dat2,tim3,pri4,pri5,pri6,pri7,cou8),
GetCompanyName($tic0,com15,_,_),Add($pri5,$pri13,pri10),Add($pri4,$pri10,pri1).
11 YahooExchangeRate($_,$cur1,pri2,dat3,_,pri5,pri6) :GetCurrentTime(_,_,dat3,_), GoCurrency(cur1,cou5,pri2),
GoCurrency(_,cou5,pri5), Add($pri2,$pri5,pri12), Add($pri2,$pri6,pri12).

The first financial service provided stock quote information, and the system learnt that the
source returned exactly the same information as a stock market service provided by Yahoo.
It was also able to work out that the previous day’s close plus today’s change was equal to
the current price. The second source provided the rate of exchange between the currencies
given as input. In this case, the system did not fare well. It was unable to learn the intended
result, which involved calculating the exchange rate by taking the ratio of the values for the
first and second currency.
Problem
10
11

# Candidates
2844
367

# Invocations
16671
16749

Time (s)
387
282

log10 (Score)
-8.13
-9.84

Precision
12/13
1/5

Recall
12/12
1/4

Details regarding the search spaces for the two problems are shown above. The average
precision and recall for these problems were much lower at 56% and 63% respectively,
because the system was unable to learn the intended definition in the second problem.
15. The positive value for problem 4 results from an approximation error.
16. Low scores and perfect precision and recall (problems 6, 8 and 9) indicate very little overlap between
the target and the known sources. The fact that the system learns the correct definition in such cases is
testimony to the robustness of the approach.

35

Carman & Knoblock

7.3.3 Weather Sources
On the Internet, there are two types of weather information services, those that provide
forecasts for coming days, and those that provide details of current weather conditions. In
the experiments, a pair of such services provided by Weather.com were used to learn definitions for a number of other weather sources. The first set of definitions, which correspond
to sources that provide current weather conditions, are listed below:
12 NOAAWeather($ica0,air1,_,_,sky4,tem5,hum6,dir7,spe8,_,pre10,tem11,_,_) :GetAirportInfo($ica0,_,air1,cit3,_,_),
WeatherForecast($cit3,_,_,_,_,_,_,_,_,_,_,_,sky4,dir7,_,_),
WeatherConditions($cit3,_,_,_,_,_,_,_,_,tem5,sky4,pre33,_,spe8,hum6,tem11),
ConvertIn2mb($pre33,pre10).
13 WunderGround($sta0,$cit1,tem2,_,_,pre5,pre6,sky7,dir8,spe9,spe10) :WeatherConditions($cit1,sta0,_,_,_,_,_,_,dat8,tem9,sky7,pre5,dir8,spe13,_,tem2),
WeatherForecast($cit1,sta0,_,_,_,_,_,_,tem24,_,_,_,_,_,spe10,_),
ConvertIn2mb($pre5,pre6),<(tem9,tem24),ConvertTime($dat8,_,_,_,_),<(spe9,spe13).
14 WeatherBugLive($_,cit1,sta2,zip3,tem4,_,_,dir7,_,_) :WeatherConditions($cit1,sta2,_,_,_,_,_,_,_,tem4,_,_,dir7,_,_,_),
GetZipcode($cit1,$sta2,zip3).
15 WeatherFeed($cit0,$_,tem2,_,sky4,tem5,_,_,pre8,lat9,_,_) :WeatherConditions($cit0,_,_,lat9,_,_,_,_,_,_,sky4,pre8,dir12,_,_,tem5),
WeatherForecast($cit0,_,_,_,_,_,_,_,_,tem2,_,_,_,dir12,_,_).
16 WeatherByICAO($ica0,air1,cou2,lat3,lon4,_,dis6,_,sky8,_,_,_,_) :Altitude($lat3,$lon4,dis6), GetAirportInfo($ica0,_,air1,cit6,_,cou8),
WeatherForecast($cit6,_,cou8,_,_,_,_,_,_,_,_,_,sky8,_,_,_),
GetCountryName($cou2,cou8).
17 WeatherByLatLon($_,$_,_,_,_,lat5,lon6,_,dis8,_,_,_,_,_,_) :Altitude($lat5,$lon6,dis8).

In the first problem, the system learnt that source 12 provided current conditions at airports,
by checking the weather report for the cities in which each airport was located. This
particular problem demonstrates some of the advantages of learning definitions for new
sources described in section 3.3. Once the definition has been learnt, if a mediator receives
a request for the current conditions at an airport, it can generate an answer for that query
by executing a single call to the newly modeled source, (without needing to find a nearby
city). The system performed well on the next three sources (13 to 15) learning definitions
which cover most of the attributes of each. On the last two problems, the system did not
perform as well. In the case of source 16, the system spent most of its time learning which
attributes of the airport were being returned (such as its country, coordinates, elevation,
etc.). In the last case, the system was only able to learn that the source was returning some
coordinates along with their elevation. We note here that different sources may provide
data at different levels of accuracy. Thus the fact that the system is unable to learn a
definition for a particular source could simply mean that the data being returned by that
source wasn’t sufficiently accurate for the system to label it a match.
In addition to current weather feeds, the system was run on two problems involving
weather forecast feeds. It did very well on the first problem, matching all bar one of the
attributes (the country) and finding that the order of the high and low temperatures was
36

Learning Semantic Definitions of Information Sources on the Internet

inverted. It did well also for the second problem, learning a definition for the source that
produced most of the output attributes.
18 YahooWeather($zip0,cit1,sta2,_,lat4,lon5,day6,dat7,tem8,tem9,sky10) :WeatherForecast($cit1,sta2,_,lat4,lon5,_,day6,dat7,tem9,tem8,_,_,sky10,_,_,_),
GetCityState($zip0,cit1,sta2).
19 WeatherBugForecast($_,cit1,sta2,_,day4,sky5,tem6,_) :WeatherForecast($cit1,sta2,_,_,_,tim5,day4,_,tem6,_,tim10,_,sky5,_,_,_),
WeatherConditions($cit1,_,_,_,_,tim10,_,tim5,_,_,_,_,_,_,_,_).

Details regarding the number of candidates generated in order to learn definitions for the
different weather sources are shown below. The average precision of the definitions produced
was 91%, while the average recall was 62%.
Problem
12
13
14
15
16
17
18
19

# Candidates
277
1989
98
199
102
45
119
116

# Invocations
579
426
2499
754
946
7669
13876
14857

Time (s)
233
605
930
292
484
1026
759
1217

log10 (Score)
-2.92
-6.35
-13.37
-6.48
-29.69
-26.71
-5.74
-12.56

Precision
8/9
6/9
5/5
5/6
6/7
3/3
10/10
5/5

Recall
8/11
6/10
5/8
5/10
6/9
3/13
10/11
5/7

7.3.4 Hotel Sources
Definitions were also learnt for sources providing hotel information from Yahoo, Google and
the US Fire Administration. These definitions are shown below.
20 USFireHotelsByCity($cit0,_,_,sta3,zip4,cou5,_) :HotelsByZip($zip4,_,_,cit0,sta3,cou5).
21 USFireHotelsByZip($zip0,_,_,cit3,sta4,cou5,_) :HotelsByZip($zip0,_,_,cit3,sta4,cou5).
22 YahooHotel($zip0,$_,hot2,str3,cit4,sta5,_,_,_,_,_) :HotelsByZip($zip0,hot2,str3,cit4,sta5,_).
23 GoogleBaseHotels($zip0,_,cit2,sta3,_,_,lat6,lon7,_) :WeatherConditions($cit2,sta3,_,lat6,lon7,_,_,_,_,_,_,_,_,_,_,_),
GetZipcode($cit2,$sta3,zip0).

The system performed well on three out of the four problems. It was unable in the time
allocated to discover a definition for the hotel attributes (name, street, latitude and longitude) returned by the Google Web Service. The average precision for these problems was
90% while the average recall was 60%.
Problem
20
21
22
23

# Candidates
16
16
43
95

# Invocations
448
1894
3137
4931

Time (s)
48
5
282
1161
37

log10 (Score)
-4.00
-2.56
-2.81
-7.50

Precision
4/4
4/4
5/5
3/5

Recall
4/6
4/6
5/9
3/6

Carman & Knoblock

7.3.5 Cars and Traffic Sources
The last problems on which the system was tested were a pair of traffic related Web Services.
The first service, provided by Yahoo, reported live traffic data (such as accidents and
construction work) within a given radius of the input zipcode. No known sources were
available which provided such information, so not surprisingly, the system was unable to
learn a definition for the traffic related attributes of that source. (Instead, the system
discovered a relationship between the input zipcode and the output longitude that wasn’t
correct, so precision for this problem was zero.)
24 YahooTraffic($zip0,$_,_,lat3,lon4,_,_,_) :GetCentroid($zip0,_,lon4), CountryCode($lat3,$lon4,_).
25 YahooAutos($zip0,$mak1,dat2,yea3,mod4,_,_,pri7,_) :GoogleBaseCars($zip0,$mak1,_,mod4,pri7,_,_,yea3),
ConvertTime($dat2,_,dat10,_,_), GetCurrentTime(_,_,dat10,_).

The second problem involved a classified used-car listing from Yahoo that took a zipcode
and car manufacturer as input. eidos was able to learn a good definition for that source,
taking advantage of the fact that some of the same cars (defined by their make, model, year
and price) were also listed for sale on Google’s classified car listing.
Problem
24
25

# Candidates
81
55

# Invocations
29974
405

Time (s)
1065
815

log10 (Score)
-11.21
-5.29

Precision
0/3
6/6

Recall
0/4
6/6

Since the system failed on the first problem (it found some incorrect/non-general relationships between different attributes), but succeeded on the second problem to find the best
possible definition, the average precision and recall for these problems were both 50%.
7.3.6 Overall Results
Across the 25 problems, eidos managed to generate definitions with high accuracy (average
precision was 88%) and a large number of attributes (average recall was 69%). These results
are promising, especially considering that all problems involved real data sources with in
some cases very small overlap between the data produced by the target and that provided
by the known sources (as evidenced by low logarithmic scores). In addition to minimal
overlap, many sources provided incomplete tuples (i.e. tuples containing multiple “NULL”
or “N/A” values) as well as erroneous or inaccurate data, making the problem all the more
difficult. The high average precision and recall lead us to believe that the Jaccard measure
is doing a good job of distinguishing correct from incorrect definitions in the presence of
data sources that are both noisy (inconsistent) and incomplete (missing tuples and values).
Comparing the different domains, one can see that the system performed better on
problems with fewer input and output attributes (such as the geospatial problems), which
was to be expected given that the resulting search space is much smaller.
38

Learning Semantic Definitions of Information Sources on the Internet

7.4 Empirical Comparison
Having demonstrated the effectiveness of eidos in learning definitions for real information
services, we now show that the system is capable of handling the same problems as a
well-known complex schema matching system.
The iMAP system (Dhamanka, Lee, Doan, Halevy, & Domingos, 2004) (discussed in
section 8.4) is a schema matcher that can learn complex (many-to-one) mappings between
the concepts of a source and a target schema. It uses a set of special purpose searchers to
learn different types of mappings. The eidos system, on the other hand, uses a generic
search algorithm to solve a comparable problem. Since the two systems can be made to
perform a similar task, we show that eidos is capable of running on one of the problem
domains used in the evaluation of iMAP. We chose the particular domain of online cricket
databases because it is the only one used in the evaluation that involved aligning data from
two independent data sources. (All other problems involved generating synthetic data by
splitting a single database into a source and target schema, which would not have been as
interesting for eidos.)
Player statistics from two online cricket databases (cricketbase.com and cricinfo.com)
were used in the experiments. Since neither of the sources provided programmatic access
to their data, the statistics data was extracted from HTML pages and inserted into a
relational database. The extraction process involved flattening the data into a relational
model and a small amount of data cleaning. (The resulting tables are similar but not
necessarily exactly the same as those used in the iMAP experiments.) The data from the
two websites was used to create three data sources representing each website. The three
sources representing cricinfo.com were then used to learn definitions for the three sources
representing cricketbase.com. Other known sources were available to the system, including
functionality for splitting apart comma-separated lists, adding and multiplying numbers,
and so on. The definitions learnt to describe the cricketbase services are shown below:
1 CricbasePlayers($cou0,nam1,_,dat3,_,unk5,unk6) :CricinfoPlayer($nam1,dat3,_,_,_,lis5,nam6,unk5,unk6), contains($lis5,cou0),
CricinfoTest($nam6,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_,_).
2 CricbaseTest($_,nam1,cou2,_,_,_,cou6,cou7,dec8,cou9,_,_,_,cou13,cou14,dec15,_,
dec17,cou18,_,_) :CricinfoTest($nam1,_,_,cou2,cou6,cou18,cou7,_,dec8,dec15,_,cou9,_,_,_,cou14,
cou13,_,_,_,_,dec17).
3 CricbaseODI($_,nam1,cou2,_,_,_,cou6,cou7,dec8,cou9,cou10,_,cou12,cou13,_,dec15,
dec16,dec17,cou18,cou19,_) :CricinfoODI($nam1,_,_,cou2,cou6,_,cou10,_,dec8,_,cou7,cou18,cou19,cou9,_,_,
cou13,cou12,dec15,_,dec16,dec17).

The first source provided player profiles by country. The second and third sources provided
detailed player statistics for two different types of cricket (Test and One-Day-International
respectively). The system easily found the best definition for the first source. The definition
involved looking for the player’s country in a list of teams that he played for. eidos did
not perform quite as well on the second and third problems. There were two reasons for
this. Firstly, the arity of these sources was much higher with many instances of the same
semantic type (count and decimal ), making the space of possible alignments much larger.
39

Carman & Knoblock

(Because of the large search space, a longer timeout of 40 minutes was used.) Secondly,
a high frequency of null values (the constant “N/A”) in the data for some of the fields
confused the algorithm, and made it harder for it to discover overlapping tuples with all of
the desired attributes.
Problem
1
2
3

# Candidates
199
1162
3114

# Invocations
3762
1517
4299

Time (s)
432
1319
2127

log10 (Score)
-3.95
-4.70
-6.28

Precision
5/5
8/11
8/14

Recall
5/5
8/16
8/16

Details of the search performed to learn the definitions are shown above. The average
precision for these problems was 77% while the average recall was lower at 66%. These
values are comparable to the quality of the matchings reported for iMAP.17 These results are
very good, considering that eidos searches in the space of many-to-many correspondences,
(trying to define the set of target attributes contemporaneously), while iMAP searches the
spaces of one-to-one and many-to-one correspondences. Moreover, eidos first invokes the
target source to generate representative data (a task not performed by iMAP) and then
performs a generic search for reasonable definitions without relying on specialised search
algorithms for different types of attributes (as is done in iMAP).

8. Related Work
In this section we describe how the work in this paper relates to research performed by
the Machine Learning, Database and the Semantic Web communities. Before doing that,
we describe some early work performed by the Artificial Intelligence community. We also
discuss how our algorithm differs from standard ILP techniques, and in particular why a
direct application of such techniques was not possible for our problem.
8.1 An Early Approach
The first work concerned with learning models for describing operations available on the
Internet was performed (in the pre-XML era) on a problem called category translation
(Perkowitz & Etzioni, 1995; Perkowitz, Doorenbos, Etzioni, & Weld, 1997). This problem
consisted of an incomplete internal world model and an external information source with
the goal being to characterize the information source in terms of the world model. The
world model consisted of a set of objects O, where each object o ∈ O belonged to a certain
category (e.g. people) and was associated with a set of attributes ha1 (o), ..., an (o)i, made up
of strings and other objects. A simple relational interpretation of this world model would
consider each category to be a relation, and each object to be a tuple. The information
source, meanwhile, was an operation that took in a single value as input and returned a
single tuple as output. The category translation problem can be viewed as a simplification
of the source definition induction problem, whereby:
17. The actual values for precision and recall in the cricket domain are not quoted, but an accuracy range
of 68-92% for simple matches (one-to-one correspondences between source and target fields) and 50-86%
for complex matches (many-to-one correspondences) across synthetic and real problems was given.

40

Learning Semantic Definitions of Information Sources on the Internet

• The extensions of the global relations are explicit. (There is one source per global
relation, and it doesn’t have binding constraints, i.e. R = S.)
• The information provided by the sources does not change over time.
• The new source takes a single value as input and returns a single tuple as output.
In order to find solutions to instances of the category translation problem, the authors employed a variant of relational path-finding (Richards & Mooney, 1992), which is an extension
on the foil algorithm, to learn models of the external source. The technique described in
this paper for solving instances of the source induction problem is similar in that it too is
based on a foil-like inductive search algorithm.
8.2 Direct Application of ILP techniques
Researchers became interested in the field of Inductive Logic Programming in the early
nineties, resulting in a number of different ILP systems being developed including foil
(Cameron-Jones & Quinlan, 1994), progol (Muggleton, 1995) and aleph18 . Ideally, one
would like to apply such “off-the-shelf” ILP systems to the source definition induction
problem. A number of issues, however, limit the direct applicability of these systems. The
issues can be summarised as follows:
•
•
•
•

Extensions of the global relations are virtual.
Sources may be incomplete with respect to their definitions.
Explicit negative examples of the target are not available.
Sources may serialise constants in different ways.

The first issue has to do with the fact that all ILP systems assume that there is an extensional
definition of the target predicate and extensional (or in some cases intentional) definitions
of the (source) predicates that will be used in the definition for the target. In other words,
they assume that tables already exist in some relational database to represent both the
new source and the known sources. In our case, we need to generate such tables by first
invoking the services with relevant inputs. One could envisage invoking each of the sources
with every possible input and using the resulting tables to perform induction. Such a direct
approach would not be feasible for two reasons. Firstly, a complete set of possible input
values may not be known to the system. Secondly, even if it is possible to generate a
complete set of viable inputs to a service, it may not be practical to query the source with
such a large set of tuples. Consider source4 from section 2, which calculates the distance in
miles between two zipcodes. Given that there are over 40,000 zipcodes in the US, generating
an extensional representation of this source would require performing more than a billion
invocations! Performing such a large number of invocations does not make sense when a
small number of example invocations would suffice for characterising the functionality of
the source. In this paper we have developed an efficient algorithm that only queries the
sources as needed in order to evaluate individual candidate definitions.
The second issue regarding the incompleteness of the sources causes a problem when a
candidate is to be evaluated. Since the set of tuples returned by each known source may only
18. See the Aleph Manual by Ashwin Srinivasan, which is available at:
http://web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/aleph.html

41

Carman & Knoblock

be a subset of those implied by its own definition, so too will be the set of tuples returned
by the candidate hypothesis when executed against those sources. This means that when
the system tries to evaluate a hypothesis by comparing those tuples with the output of the
new source, it cannot be sure that a tuple which is produced by the new source but not by
the hypothesis is in fact not logically implied by it. This fact is taken into account in our
evaluation function for scoring candidate definitions, discussed in section 5.
The third issue regarding the lack of explicit negative examples for the target predicate
also affects the evaluation of candidate hypotheses. The classic approach to dealing with
this problem is to assume a closed world, in which all tuples (over the head relation) which
are not explicitly declared to be positive must be negative. Since the new source may in
fact be incomplete with respect to the best possible definition for it, this assumption does
not necessarily hold. In other words, just because a particular tuple is produced when the
candidate definition is executed and that same tuple is not returned by the new source does
not necessarily mean that the candidate definition is incorrect.
The fourth issue has to do with the fact that the data provided by different sources may
need to be reconciled, in the sense that different serialisations of (strings representing) the
same value (such as “Monday” and “Mon” for instance) must be recognized. Since ILP
systems have been designed to operate over a single database containing multiple tables,
the issue of heterogeneity in the data is not handled by current systems. In section 5.4 we
discussed how this heterogeneity is resolved in our system.
8.3 Machine Learning Approaches
Since the advent of services on the Internet, researchers have been investigating ways to
model them automatically. Primarily, interest has centered on using machine learning
techniques to classify the input and output types of a service, so as to facilitate service
discovery. Heß & Kushmerick proposed using a Support Vector Machine to classify the
input and output attributes into different semantic types based on metadata in interface
descriptions (Heß & Kushmerick, 2003, 2004). Their notion of semantic types (such as
zipcode) as opposed to syntactic types (like integer ) went some way toward defining the
functionality that a source provides. Recently, other researchers (Lerman et al., 2006)
proposed the use of logistic regression for assigning semantic types to input parameters
based on metadata, and a pattern language for assigning semantic types to the output
parameters based on the data the source produces. This work on classifying input and
output attributes of a service to semantic types forms a prerequisite for the work in this
article. For the purposes of this paper, we have assumed that this problem has been solved.
In addition to classifying the input/output attributes of services, Heß & Kushmerick
investigated the idea of classifying the services themselves into different service types. More
precisely, they used the same classification techniques to assign service interfaces to different semantic domains (such as weather and flights) and the operations that each interface
provides to different classes of operation (such as weatherForecast and flightStatus). The
resulting source description (hypothesis) language is limited to select-project queries, which
are not sufficiently expressive to describe many of the sources available on the Internet.
According to that approach, since every operation must be characterized by a particular operation class, operations that provide overlapping (non-identical) functionality would
42

Learning Semantic Definitions of Information Sources on the Internet

need to be assigned different classes as would operations which provide composed functionality (such as, for example, an operation that provides both weather and flight data). The
need for an exhaustive set of operation classes (and accompanying training data) is a major
limitation of that approach, not shared by the work described in this paper, which relies on
a more expressive language for describing service operations.
One way to eliminate the need for a predefined set of operation types is to use unsupervised clustering techniques to generate the (operation) classes automatically from examples
(WSDL documents). This idea was implemented in a system called Woogle (Dong et al.,
2004). The system clustered service interfaces together using a similarity score based on the
co-occurrence of metadata labels. It then took advantage of the clusters produced to improve keyword-based search for Web Services. An advantage of this unsupervised approach
is that no labeled training data is required, which can be time-consuming to generate. Such
clustering approaches, however, while useful for service discovery, suffer the same limitations
as the previous approach when it comes to expressiveness.
8.4 Database Approaches
The database community has long been interested in the problem of integrating data from
disparate sources. Specifically, in the areas of data warehousing (Widom, 1995) and information integration (Wiederhold, 1996), researchers are interested in resolving semantic
heterogeneity which exists between different databases so that the data can be combined or
accessed via a single interface. The schema mapping problem is the problem of determining
a mapping between the relations contained in a source schema and a particular relation in
a target schema. A mapping defines a transformation which can be used to populate the
target relation with data from the source schema. Mappings may be arbitrarily complex
procedures, but in general they will be declarative queries in SQL or Datalog. The complexity of these queries makes the schema mapping problem far more difficult than the highly
investigated schema matching problem (Rahm & Bernstein, 2001), which involves finding
1-to-1 correspondences between fields of a source and target schema.
The source definition induction problem can be viewed as a type of schema mapping
problem, in which the known sources define the source schema and the unknown source
specifies the target relation. In order to solve a schema mapping problem, one typically
takes advantage of all available auxiliary information (including source and target data
instances, labels from the respective schemas, and so on). Such problems are generally
simpler, however, because the data (the extensions of the relations) in the source and target
schema are usually explicitly available. In source induction, that data is hidden behind a
service interface, which has binding constraints, and the data itself can be extremely large
or even (in the case of sources providing mathematical functions) infinite. Thus making the
problem considerably more difficult.
The schema integration system CLIO (Yan, Miller, Haas, & Fagin, 2001) helps users
build SQL queries that map data from a source to a target schema. In CLIO, foreign keys
and instance data are used to generate integration rules semi-automatically. Since CLIO
relies heavily on user involvement, it does not make sense to compare it directly with the
automated system developed in this paper.
43

Carman & Knoblock

Another closely related problem is that of complex schema matching, the goal of which is
to discover complex (many-to-one) mappings between two relational tables or XML schemas.
This problem is far more complicated than basic (one-to-one) schema matching because:
• The space of possible correspondences between the relations is no longer the Cartesian
product of the source and target relations, but the powerset of the source relation times
the target relation.
• Many-to-one mappings require a mapping function, which can be simple like concatenate(x,y,z), or an arbitrarily complex formula such as z = x2 + y.
The iMAP system (Dhamanka et al., 2004) tries to learn such many-to-one mappings between the concepts of a set of source relations and a target relation. It uses a set of special
purpose searchers to learn different types of mappings (such as mathematical expressions,
unit conversions and time/date manipulations). It then uses a meta-heuristic to control
the search being performed by the different special purpose searchers. If one views both
the source schema and the functions available for use in the mappings (such as concatenate(x,y,z), add(x,y,z), etc.) as the set of known sources in the source definition induction
problem, then the complex schema matching and source induction problems are somewhat
similar. The main differences between the problems are:
• The data associated with the source schema is explicit (and static) in complex schema
matching, while it is hidden (and dynamic) in source induction.
• In general, the set of known sources in a source induction problem will be much larger
(and the data they provide may be less consistent), than the set of mapping functions
and source relations in a complex schema matching problem.
In this paper we develop a general framework for handling the source induction problem.
Since iMAP provides functionality which is similar to that of our system, we perform a
simple empirical comparison in section 7.4.
8.5 Semantic Web Approach
The stated goal of the Semantic Web (Berners-Lee, Hendler, & Lassila, 2001) is to enable
machine understanding of Web resources. This is done by annotating those resources with
semantically meaningful metadata. Thus the work described in this paper is very much
in line with the Semantic Web, in so far as we are attempting to discover semantically
meaningful definitions for online information sources. De facto standards for annotating
services with semantic markup have been around for a number of years. These standards
provide service owners with a metadata language for adding declarative statements to service
interface descriptions in an attempt to describe the semantics of each service in terms of
the functionality (e.g. a book purchase operation) or data (e.g. a weather forecast) that it
provides. Work on these languages is related to this article from two perspectives:
• It can be viewed as an alternative approach to gaining knowledge as to the semantics
of a newly discovered source (providing it has semantic metadata associated with it).
• Semantic Web Service annotation languages can be seen as a target language for the
semantic descriptions learnt in this paper.
44

Learning Semantic Definitions of Information Sources on the Internet

If a Web Service is already semantically annotated, heterogeneity may still exist between
the ontology used by the service provider and that used by the consumer, in which case the
learning capabilities described in this paper may be required to reconcile those differences.
More importantly, we are interested in the vast number of sources for which semantic
markup is currently unavailable. The work in this article complements that of the Semantic
Web community by providing a way of automatically annotating sources with semantic
information; thereby relieving service providers of the burden of manually annotating their
services. Once learnt, Datalog source definitions can be converted to Description Logicbased representations such as is used in OWL-S (Martin, Paolucci, McIlraith, Burstein,
McDermott, McGuinness, Parsia, Payne, Sabou, Solanki, Srinivasan, & Sycara, 2004) and
WSMO (Roman, Keller, Lausen, de Bruijn, Lara, Stollberg, Polleres, Feier, Bussler, &
Fensel, 2005). The reason we use Datalog in this paper (rather than Description Logics) is
that most mediator-based integration systems rely on it as a representation language.

9. Discussion
In this paper we have presented a completely automatic approach to learning definitions for
online services. Our approach exploits the definition of sources that have either been given
to the system or learned previously. The resulting framework is a significant advance over
prior approaches that have focused on learning only the inputs and outputs or the class of
a service. We have demonstrated empirically the viability of the approach.
The key contribution of this article is a procedure for learning semantic definitions for
online information services that is:
• Fully automated : Definitions are learnt in a completely automated manner without
the need for any user intervention.
• More expressive: The query language for defining sources is that of conjunctive
queries, which is far more expressive than previous attribute-value approaches.
• Sufficiently robust: The procedure is able to learn definitions in the presence of noisy
and incomplete data, and thus is sufficiently robust to handle real data sources.
• Data-access efficient: The procedure samples data from live sources, invoking them
sparingly and only as required, making it highly efficient in terms of source accesses.
• Evolving: The procedure’s ability to learn definitions improves over time as each new
definition is learnt and added to the set of known sources.
9.1 Application Scenarios
There are a number of different application scenarios for a system that is capable of learning
definitions for online sources. They generally involve providing semantic definitions to data
integration systems, which then exploit and integrate the available sources.
The most obvious application for our work would be a system (depicted on the left side
of Figure 1) that crawls the Web, searching for information sources. Upon finding a source,
the system would use a classifier to assign semantic types to it, followed by the inductive
learner to generate a definition for it. The definition could then be used to annotate the
source for the Semantic Web, or by a mediator for answering queries. Importantly, this
entire process could run with minimal user involvement.
45

Carman & Knoblock

Figure 1: Architecture diagrams for three different application scenarios.

A more challenging application scenario (shown in the center of Figure 1) would involve
real-time service discovery. Consider the case where a mediator is unable to answer a
particular query because the desired information lies out of scope of the sources available.
A search is then performed based on the “missing conjuncts” (relation names and constants)
from the query using a specialised Web Service search engine, such as Woogle (Dong et al.,
2004). The services returned would be annotated with semantic types and, if possible,
semantic definitions. After the definitions are provided to the mediator, it would complete
the query processing and return an answer to the user. This scenario may seem a little farfetched until one considers a specific example: imagine a user interacting with a geospatial
browser (an online atlas). If the user turns on a particular information layer, such as ski
resorts, but no source is available for the current field of view (of, for instance, Italy), then
no results would be displayed. In the background a search could be performed and a new
source discovered, which provides ski resorts all over Europe. The relevant data could then
be displayed, with the user unaware that a search has been performed.
Perhaps the most likely application scenario (to the right of Figure 1) for a source
induction system would be a mixed initiative one. In this case a human would annotate
the different operations of a service interface with semantic definitions. At the same time,
the system would attempt to induce definitions for the remaining operations, and prompt
the user with suggestions for them. In this scenario the classifier may not be needed,
since attributes of the same name in the different operations would likely have the same
semantic type. Moreover, since the definitions learnt by the system may in some cases
contain erroneous or superfluous predicates, the user could also be involved in a process of
checking and improving the definitions discovered.
46

Learning Semantic Definitions of Information Sources on the Internet

9.2 Opportunities for Further Research
A number of future directions for this work will allow these techniques to be applied more
broadly. We now discuss two such directions, improving the search algorithm and extending
the query language.
As the number of known sources grows, so too will the search space, and it will be necessary to develop additional heuristics to better direct the search toward the best definition.
Many heuristic techniques have been developed in the ILP community and some may be
applicable to the source induction problem. More pressing perhaps is the need to develop a
robust termination condition for halting the search once a “sufficiently good” definition has
been discovered. As the number of available sources increases, the simple timeout used in
the experiments will be ineffective as certain (more complicated) definitions will necessarily
take longer to learn than others.
Another way to increase the applicability of this work is to extend the query language so
that it better describes the sources available. Often online sources do not return a complete
set of results but rather cut off the list at some maximum cardinality. For example the
YahooHotel source described in section 7.3.4 returns a maximum of 20 hotels near a given
location, and orders them according to distance. In this case, recognising the specific
ordering on the tuples produced would be very useful to a mediator. A second useful
extension to the query language would be the ability to describe sources using the procedural
construct if-then-else. This construct is needed to describe the behaviour of some sources
on certain inputs. For example, consider the YahooGeocoder from section 7.3.1, which takes
as input a tuple containing a street name, number, and zipcode. If the geocoder is unable
to locate the corresponding address in its database (because it doesn’t exist), instead of
returning no tuples, it returns the centroid of the zipcode. Describing such behavior is only
possible using procedural constructs.

Acknowledgments
This research is based upon work supported in part by the Defense Advanced Research
Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010. The U.S. Government is authorized
to reproduce and distribute reports for Governmental purposes notwithstanding any copyright annotation thereon. The views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official policies or
endorsements, either expressed or implied, of any of the above organizations or any person
connected with them.

References
Afrati, F. N., Li, C., & Mitra, P. (2004). On containment of conjunctive queries using arithmetic comparisions. In 9th International Conference on Extending Database Technology (EDBT 2004) Heraklion-Crete, Greece.
47

Carman & Knoblock

Arens, Y., Knoblock, C. A., & Shen, W.-M. (1996). Query reformulation for dynamic
information integration. Journal of Intelligent Information Systems - Special Issue on
Intelligent Information Integration, 6 (2/3), 99–130.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientific American,
284 (5), 34–43.
Bilenko, M., Mooney, R. J., Cohen, W. W., Ravikumar, P., & Fienberg, S. E. (2003).
Adaptive name matching in information integration.. IEEE Intelligent Systems, 18 (5),
16–23.
Cameron-Jones, R. M., & Quinlan, J. R. (1994). Efficient top-down induction of logic
programs. SIGART Bulletin, 5 (1), 33–42.
Carman, M. J., & Knoblock, C. A. (2007). Learning semantic descriptions of web information sources. In Proceedings of the Twentieth International Joint Conference on
Artificial Intelligence (IJCAI-07) Hyderabad, India.
Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation of conjunctive queries
in relational data bases. In Proceedings of the 9th ACM Symposium on Theory of
Computing (STOC), pp. 77–90 Boulder, Colorado.
Dhamanka, R., Lee, Y., Doan, A., Halevy, A., & Domingos, P. (2004). imap: Discovering
complex semantic matches between database schemas. In SIGMOD ’04: Proceedings
of the 2004 ACM SIGMOD International Conference on Management of Data.
Dong, X., Halevy, A. Y., Madhavan, J., Nemes, E., & Zhang, J. (2004). Simlarity search
for web services. In Proceedings of VLDB.
Duschka, O. M. (1997). Query Planning and Optimization in Information Integration. Ph.D.
thesis, Department of Computer Science, Stanford University.
Garcia-Molina, H., Hammer, J., Ireland, K., Papakonstantinou, Y., Ullman, J., & Widom,
J. (1995). Integrating and accessing heterogeneous information sources in tsimmis. In
Proceedings of the AAAI Symposium on Information Gathering, pp. 61-64.
Heß, A., & Kushmerick, N. (2003). Learning to attach semantic metadata to web services.
In 2nd International Semantic Web Conference (ISWC).
Heß, A., & Kushmerick, N. (2004). Iterative ensemble classification for relational data:
A case study of semantic web services. In 15th European Conference on Machine
Learning (ECML2004) Pisa, Italy. Springer.
Knoblock, C. A., Minton, S., Ambite, J. L., Ashish, N., Muslea, I., Philpot, A., & Tejada,
S. (2001). The ariadne approach to web-based information integration. International
Journal of Cooperative Information Systems, 10 (1-2), 145–169.
Lerman, K., Plangprasopchok, A., & Knoblock, C. A. (2006). Automatically labeling data
used by web services. In Proceedings of the 21st National Conference on Artificial
Intelligence (AAAI).
48

Learning Semantic Definitions of Information Sources on the Internet

Levy, A. Y. (2000). Logic-based techniques in data integration. In Minker, J. (Ed.), LogicBased Artificial Intelligence. Kluwer Publishers.
Levy, A. Y., Mendelzon, A. O., Sagiv, Y., & Srivastava, D. (1995). Answering queries using
views. In Proceedings of the 14th ACM SIGACT-SIGMOD-SIGART Symposium on
Principles of Database Systems, pp. 95–104 San Jose, Calif.
Markov, Z., & Marinchev, I. (2000). Metric-based inductive learning using semantic height
functions. In Proceedings of the 11th European Conference on Machine Learning
(ECML 2000). Springer.
Martin, D., Paolucci, M., McIlraith, S., Burstein, M., McDermott, D., McGuinness, D.,
Parsia, B., Payne, T., Sabou, M., Solanki, M., Srinivasan, N., & Sycara, K. (2004).
Bringing semantics to web services: The owl-s approach. In Proceedings of the First
International Workshop on Semantic Web Services and Web Process Composition
(SWSWPC 2004).
Muggleton, S., & Feng, C. (1990). Efficient induction of logic programs. In Proceedings of
the 1st Conference on Algorithmic Learning Theory.
Muggleton, S. (1995). Inverse entailment and Progol. New Generation Computing, Special
issue on Inductive Logic Programming, 13 (3-4), 245–286.
Nédellec, C., Rouveirol, C., Adé, H., Bergadano, F., & Tausend, B. (1996). Declarative
bias in ILP. In De Raedt, L. (Ed.), Advances in Inductive Logic Programming, pp.
82–103. IOS Press.
Pazzani, M. J., & Kibler, D. F. (1992). The utility of knowledge in inductive learning.
Machine Learning, 9, 57–94.
Perkowitz, M., & Etzioni, O. (1995). Category translation: Learning to understand information on the internet. In Proceedings of the Fourteenth International Joint Conference
on Artificial Intelligence (IJCAI-95).
Perkowitz, M., Doorenbos, R. B., Etzioni, O., & Weld, D. S. (1997). Learning to understand information on the internet: An example-based approach. Journal of Intelligent
Information Systems, 8 (2), 133–153.
Pottinger, R., & Halevy, A. Y. (2001). Minicon: A scalable algorithm for answering queries
using views. VLDB Journal, 10 (2-3).
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: A midterm report. In Machine
Learning: ECML-93, European Conference on Machine Learning, Proceedings, Vol.
667, pp. 3–20. Springer-Verlag.
Rahm, E., & Bernstein, P. (2001). A survey of approaches to automatic schema matching.
VLDB Journal, 10 (4).
Richards, B. L., & Mooney, R. J. (1992). Learning relations by pathfinding. In National
Conference on Artificial Intelligence, pp. 50–55.
49

Carman & Knoblock

Roman, D., Keller, U., Lausen, H., de Bruijn, J., Lara, R., Stollberg, M., Polleres, A.,
Feier, C., Bussler, C., & Fensel, D. (2005). Web service modeling ontology. Applied
Ontology, 1 (1), 77–106.
Ullman, J. D. (1989). Principles of Database and Knowledge-Base Systems, Vol. 2. Computer Science Press, Rockville, Maryland.
Weber, I., Tausend, B., & Stahl, I. (1995). Language series revisited: The complexity of
hypothesis spaces in ILP. In Proceedings of the 8th European Conference on Machine
Learning, Vol. 912, pp. 360–363. Springer-Verlag.
Widom, J. (1995). Research problems in data warehousing. In CIKM ’95: Proceedings of
the fourth International Conference on Information and Knowledge Management, pp.
25–30. ACM Press.
Wiederhold, G. (1992). Mediators in the architecture of future information systems. Computer, 25 (3), 38–49.
Wiederhold, G. (Ed.). (1996). Intelligent Integration of Information. Kluwer Academic
Publishers, Boston MA.
Winkler, W. (1999). The state of record linkage and current research problems. Tech. rep.,
Statistical Research Division, U.S. Bureau of the Census, Washington, DC.
Yan, L. L., Miller, R. J., Haas, L. M., & Fagin, R. (2001). Data-driven understanding
and refinement of schema mappings. In SIGMOD ’01: Proceedings of the 2001 ACM
SIGMOD International Conference on Management of data.
Zelle, J. M., Thompson, C. A., Califf, M. E., & Mooney, R. J. (1995). Inducing logic
programs without explicit negative examples. In Proceedings of the Fifth International
Workshop on Inductive Logic Programming.

50

Journal of Artificial Intelligence Research 30 (2007) 621-657

Submitted 03/07; published 12/07

Query-time Entity Resolution
Indrajit Bhattacharya

indrajbh@in.ibm.com

IBM India Research Laboratory
Vasant Kunj, New Delhi 110 070, India

Lise Getoor

getoor@cs.umd.edu

Department of Computer Science
University of Maryland, College Park, MD 20742 USA

Abstract
Entity resolution is the problem of reconciling database references corresponding to
the same real-world entities. Given the abundance of publicly available databases that
have unresolved entities, we motivate the problem of query-time entity resolution: quick
and accurate resolution for answering queries over such ‘unclean’ databases at query-time.
Since collective entity resolution approaches — where related references are resolved jointly
— have been shown to be more accurate than independent attribute-based resolution for
off-line entity resolution, we focus on developing new algorithms for collective resolution
for answering entity resolution queries at query-time. For this purpose, we first formally
show that, for collective resolution, precision and recall for individual entities follow a
geometric progression as neighbors at increasing distances are considered. Unfolding this
progression leads naturally to a two stage ‘expand and resolve’ query processing strategy.
In this strategy, we first extract the related records for a query using two novel expansion
operators, and then resolve the extracted records collectively. We then show how the
same strategy can be adapted for query-time entity resolution by identifying and resolving
only those database references that are the most helpful for processing the query. We
validate our approach on two large real-world publication databases where we show the
usefulness of collective resolution and at the same time demonstrate the need for adaptive
strategies for query processing. We then show how the same queries can be answered in
real-time using our adaptive approach while preserving the gains of collective resolution. In
addition to experiments on real datasets, we use synthetically generated data to empirically
demonstrate the validity of the performance trends predicted by our analysis of collective
entity resolution over a wide range of structural characteristics in the data.

1. Introduction
With the growing abundance of publicly available data in digital form, there has been intense research on data integration. A critical component of the data integration process is
the entity resolution problem, where uncertain references in the data to real-world entities
such as people, places, organizations, events, etc., need to be resolved according to their
underlying real-world entities. Entity resolution is needed in order to solve the ‘deduplication’ problem, where the goal is to identify and consolidate pairs of records or references
within the same relational table that are duplicates of each other. It also comes up as the
‘fuzzy match’ problem, where tuples from two heterogeneous databases with different keys,
and possibly different schemas, need to be matched and consolidated. It goes by different
c
2007
AI Access Foundation. All rights reserved.

Bhattacharya & Getoor

names even within the data mining and database communities, including record linkage,
object consolidation, and reference reconciliation.
The problem has a long history, and recent years have seen significant and fruitful
research on this problem. However, in spite of the widespread research interest and the
practical nature of the problem, many publicly accessible databases remain unresolved, or
partially resolved, at best. The popular publication databases, CiteSeer and PubMed, are
representative examples. CiteSeer contains several records for the same paper or author,
while author names in PubMed are not resolved at all. This is due to a variety of reasons,
ranging from rapid and often uncontrolled growth of the databases and the computational
and other expenses involved in maintaining resolved entities.
Yet, millions of users access and query such databases everyday, mostly seeking information that, implicitly or explicitly, requires knowledge of the resolved entities. For example,
we may query the CiteSeer database of computer science publications looking for books by
‘S Russell’ (Pasula, Marthi, Milch, Russell, & Shpitser, 2003). This query would be easy
to answer if all author names in CiteSeer were correctly mapped to their entities. But,
unfortunately, this is not the case. According to CiteSeer records, Stuart Russell and Peter
Norvig have written more than 100 different books together. One of the main reasons behind databases containing unresolved entities is that entity resolution is generally perceived
as an expensive process for large databases. Also, maintaining a ‘clean’ database requires
significant effort to keep pace with incoming records. Alternatively, we may be searching
different online social network communities for a person named ‘Jon Doe’. In this case,
each online community may individually have records that are clean. Even then, query
results that return records from all of the sources aggregated together may have multiple
representations for the same ’Jon Doe’ entity. Additionally, in both cases, it is not sufficient
to simply return records that match the query name, ‘S. Russell’ or ‘Jon Doe’ exactly. In
order to retrieve all the references correctly, we may need to retrieve records with similar
names as well, such as ’Stuart Russel’ or ’John Doe’. And, most importantly, for the results
to be useful, we need to partition the records that are returned according to the real-world
entities to which they correspond. Such on-the-fly partitioning of returned results is also
necessary when accessing third-party or external databases that do not provide full access
possibly due to privacy and other concerns, and can be accessed only via specific query
interfaces.
In this paper, we propose an alternative solution for answering entity resolution queries,
where we obviate the need for maintaining resolved entities in a database. Instead, we
investigate entity resolution at query-time, where the goal is to enable users to query an
unresolved or partially resolved database and resolve the relevant entities on the fly. A user
may access several databases everyday and she does not want to resolve all entities in every
database that she queries. She only needs to resolve those entities that are relevant for a
particular query. For instance, when looking for all books by ‘Stuart Russell’ in CiteSeer,
it is not useful to resolve all of the authors in CiteSeer. Since the resolution needs to be
performed at query-time, the requirement is that the resolution process needs to be quick,
even if it is not entirely accurate.
Though entity resolution queries have not been addressed in the literature, there has
been significant progress on the general entity resolution problem. Recent research has focused on the use of additional relational information between database references to improve
622

Query-time Entity Resolution

resolution accuracy (Bhattacharya & Getoor, 2004; Singla & Domingos, 2004; Dong, Halevy,
& Madhavan, 2005; Ananthakrishna, Chaudhuri, & Ganti, 2002; Kalashnikov, Mehrotra, &
Chen, 2005). This improvement is made possible by resolving related references or records
jointly, rather than independently. Intuitively, this corresponds to the notion that figuring
out that two records refer to the same underlying entity may in turn give us useful information for resolving other record pairs that are related. Imagine that we are trying to decide if
two authors ‘Stuart Russell’ and ‘S Russell’ are the same person. We can be more confident
about this decision if we have already decided that their co-authors ‘Peter Norvig’ and ‘P.
Norvig’ are the same person.
As others have done, in our earlier work (Bhattacharya & Getoor, 2004, 2007), we have
demonstrated using extensive experiments on multiple real and synthetic datasets that
collective resolution significantly improves entity resolution accuracy over attribute-based
and naive relational baselines. However, its application for query-time entity resolution is
not straight-forward, and this is precisely the problem that we focus on in this paper. The
first difficulty is that collective resolution works for a database as a whole and not for a
specific query. Secondly, the accuracy improvement comes at a considerable computation
cost arising from the dependencies between related resolutions. This added computational
expense makes its application in query-time resolution challenging.
In this paper, which builds on and significantly extends the work presented in Bhattacharya, Licamele, and Getoor (2006), we investigate the application of collective resolution for queries. First, we formally analyze how accuracies of different decisions in collective
resolution depend on each other and on the structural characteristics of the data. The recursive nature of the dependency leads naturally to a recursive ‘expand and resolve’ strategy
for processing queries. The relevant records necessary for answering the query are extracted
by a recursive expansion process and then collective resolution is performed only on the extracted records. Using our analysis, we show that the recursive expansion process can be
terminated at reasonably small depths for accurately answering any query; the returns fall
off exponentially as neighbors that are further away are considered.
However, the problem is that this unconstrained expansion process can return too many
records even at small depths; and thus the query may still be impossible to resolve in
real-time. We address this issue using an adaptive strategy that only considers the most
informative of the related records for answering any query. This significantly reduces the
number of records that need to be investigated at query time, but, most importantly, does
not compromise on the resolution accuracy for the query.
Our specific contributions in this paper are as follows:
1. First, we motivate and formulate the problem of query-time entity resolution. Our
entity resolution approach is based on a relational clustering algorithm. To the best
of our knowledge, clustering based on queries in the presence of relations has received
little attention in the literature.
2. For collective resolution using relational clustering, we present an analysis of how the
accuracy of different resolution decisions depends on each other and on the structural
characteristics of the data. We introduce the notion of precision and recall for individual entities, and show how they follow a geometric progression as neighbors at
increasing distances are considered and resolved. Our analysis shows that collective
623

Bhattacharya & Getoor

use of relationships can sometimes hurt entity resolution accuracy. This has not been
previously reported in the literature. Our analysis additionally demonstrates the convergent nature of resolution performance for the recursive query-resolution strategy
that we propose.
3. For resolving queries collectively, we propose a two-phase ‘expand and resolve’ algorithm. It first extracts the related records for a query using two novel expansion
operators, and then resolves the query by only considering the extracted records. We
then improve on this algorithm using an adaptive approach that selectively considers
only the ‘most informative’ ones among the related records for a query. This enables
collective resolution at query-time without compromising on resolution accuracy for
the query.
4. We present experimental results on two large real-world datasets where our strategy
enables collective resolution in seconds. We compare against multiple baselines to
show that the accuracy achieved using collective query resolution is significantly higher
than those achieved using traditional approaches.
5. We also use synthetically generated data to demonstrate the gains of collective query
resolution over a wide range of attribute and relational characteristics. We additionally show that the empirical results are in agreement with the trends predicted by our
analysis of collective resolution.
The rest of the paper is organized as follows. In Section 2, we formalize the relational
entity resolution problem and entity resolution queries, and also illustrate these with an
example. In Section 3, we briefly review the relational clustering algorithm that we employ
for collective entity resolution and then, in Section 4, investigate how resolution accuracy
for related entities depend on each other for collective resolution using this algorithm. In
Section 5, we extend collective resolution for queries, and describe and analyze an unconstrained recursive strategy for collectively resolving a query. We then modify this approach
in Section 6 and present our adaptive algorithm that extracts only the ‘most informative’
references for resolving a query. We present experimental results on real and synthetic data
in Section 7, review related work in Section 8 and finally conclude in Section 9.

2. Entity Resolution and Queries: Formulation
In this section, we formally introduce the entity resolution problem and also entity resolution
queries, and illustrate them using a realistic example — that of resolving authors in a
citation database such as CiteSeer or PubMed.
In the simplest formulation of the entity resolution problem, we have a collection of
references, R = {ri }, with attributes {R.A1 , . . . , R.Ak }. Let E = {ej } be the unobserved
domain entities. For any particular reference ri , we denote the entity to which it maps
as E(ri ). We will say that two references ri and rj are co-referent if they correspond to
the same entity, E(ri ) = E(rj ). Note that in the case of an unresolved database, this
mapping E(R) is not provided. Further, the domain entities E and even the number of
such entities is not known. However, in many domains, we may have additional information
about relationships between the references. To model relationships in a generic way, we use
624

Query-time Entity Resolution

h1

h2

A Mouse Immunity Model

r1
W Wang

r2
C Chen

r3

r4

r5

A Ansari

W Wang

A Ansari

h 3 Measuring Protien−bound Fluxetine
r6
r7
r8
L Li

C Chen

A Better Mouse Immunity Model

h 4 Autoimmunity in Biliary Cirrhosis
r9
r 10
W W Wang

W Wang

A Ansari

Figure 1: An example set of papers represented as references connected by hyper-edges.
References are represented as ovals shaded according to their entities. Each paper
is represented as a hyper-edge (shown as a rectangle) spanning multiple references.

a set of hyper-edges H = {hi }. Each hyper-edge connects multiple references. To capture
this, we associate a set of references hi .R with each hyper-edge hi . Note that each reference
may be associated with zero or more hyper-edges.
Let us now look at a sample domain to see how it can be represented in our framework.
Consider a database of academic publications similar to DBLP, CiteSeer or PubMed. Each
publication in the database has a set of author names. For every author name, we have a
reference ri in R. For any reference ri , ri .N ame records the observed name of the author
in the publication. In addition, we can have attributes such as R.Email to record other
information for each author reference that may be available in the paper. Now we come to
the relationships for this domain. All the author references in any publication are connected
to each other by a co-author relationship. This can be represented using a hyper-edge hi ∈ H
for each publication and by having rj ∈ hi .R for each reference rj in the publication. If
publications have additional information such as title, keywords, etc, they are represented
as attributes of H.
To illustrate, consider the following four papers, which we will use as a running example:
1. W. Wang, C. Chen, A. Ansari, “A mouse immunity model”
2. W. Wang, A. Ansari, “A better mouse immunity model”
3. L. Li, C. Chen, W. Wang,“Measuring protein-bound fluxetine”
4. W. W. Wang, A. Ansari, “Autoimmunity in biliary cirrhosis”
To represent them in our notation, we have 10 references {r1 , . . . , r10 } in R, one for each
author name, such that r1 .N ame = ‘W Wang’, etc. We also have 4 hyper-edges {h1 , . . . , h4 }
in H, one for each paper. The first hyper-edge h1 connects the three references r1 , r2 and
r3 corresponding to the names ‘W. Wang’ , ‘C. Chen’ and ‘A. Ansari’. This is represented
pictorially in Figure 1.
Given this representation, the entity resolution task is defined as the partitioning
or clustering of the references according to the underlying entity-reference mapping E(R).
Two references ri and rj should be assigned to the same cluster if and only if they are
625

Bhattacharya & Getoor

coreferent, i.e., E(ri ) = E(rj ). To illustrate, assume that we have six underlying entities for
our example. This is illustrated in Figure 1 using a different shading for each entity. For
example, the ‘Wang’s of papers 1, 2 and 4 are names of the same individual but the ’Wang’
from paper 3 is a reference to a different person. Also, the ‘Chen’s from papers 1 and 3 are
different individuals. Then, the correct entity resolution for our example database with 10
references returns 6 entity clusters: {{r1 , r4 , r9 }, {r8 }, {r2 }, {r7 }, {r3 , r5 , r10 }, {r6 }}. The
first two clusters correspond to two different people named ‘Wang’, the next two to two
different people named ‘Chen’, the fifth to ‘Ansari’ and the last to ‘Li’.
Any query to a database of references is called an entity resolution query if answering
it requires knowledge of the underlying entity mapping E(R). We consider two different
types of entity resolution queries. Most commonly, queries are specified using a particular
value a for an attribute R.A of the references that serves as a ‘quasi-identifier’ for the
underlying entities. Then the answer to the query Q(R.A = a) should partition or group
all references that have r.A = a according to their underlying entities. For references to
people, the name often serves as a weak or noisy identifier. For our example bibliographic
domain, we consider queries specified using R.N ame. To retrieve all papers written by
some person named ‘W. Wang’, we issue a query using R.N ame and ‘W. Wang’. Since
names are ambiguous, treating them as identifiers leads to undesirable results. In this case,
it would be incorrect to return the set {r1 , r4 , r8 } of all references with name ‘W Wang’ as
the answer to our query. This answer does not indicate that r8 is not the same person as
the other two. Additionally, the answer should include the reference r9 for ‘W W Wang’,
that maps to the same entity as the author of the first paper. Therefore, the correct answer
to the entity resolution query on ‘W Wang’ should be the partition {{r1 , r4 , r9 }, {r8 }}.
Entity resolution queries may alternatively be specified using a specific reference. Imagine a CiteSeer user looking at a paper that contains some author name. The user may be
interested in looking up other papers written by the same author, even though they may
not know who that author is precisely. Then the correct answer to a query on the reference
r is the group of references that are coreferent to r, or, in other words, correspond to the
same underlying entity. In our example, consider a query specified using the reference r1
corresponding to the name ‘W. Wang’ in the first paper. Then the correct answer to the
query is the set of references {r1 , r4 , r9 }. To distinguish it from the first type of entity
resolution query, note that it does not include the cluster {r8 } corresponding to the other
entity that also has name ‘W. Wang’. This second query type may be answered by first
reducing it to an instance of the first type as Q(R.A = r1 .A), and then selecting the entity
corresponding to reference r1 . We denote this as σE(R)=E(r1 ) (Q(R.A = r1 .A)). In the rest
of this paper, we focus only on queries of the first type.

3. Collective Entity Resolution and Relational Clustering
Although entity resolution for queries has not been studied in the literature, the general
entity resolution problem has received a lot of attention. We review related work in detail in
Section 8. In this section, we briefly review the different categories of proposed approaches
before discussing how they may be adapted for query-time entity resolution.
In most entity resolution applications, data labeled with the underlying entities is hard
to acquire. Our focus is on unsupervised approaches for resolving entities. Traditionally,
626

Query-time Entity Resolution

attributes of individual references, such as names, affiliation, etc., for person references, are
used for comparing references. A similarity measure is generally employed over attributes,
and only those pairs of references that have attribute similarity above a certain threshold
are considered to be co-referent. This attribute-based entity resolution approach (A)
often runs into problems. In our example, it is hard to infer with just attributes that
references r1 and r8 are not co-referent although they have the same name, while r1 and r9
are co-referent although their names are different.
When relations between references are available, they may also be taken into account
for computing similarities in the naive relational entity resolution approach (NR)
(Ananthakrishna et al., 2002; Bhattacharya & Getoor, 2007). For computing similarities
between two references, this approach additionally considers the attributes of the related
references when comparing the attributes of their related references. In our example, this
approach returns a higher similarity between r1 (‘W. Wang’) and r9 (‘W. W. Wang’) than
the attribute-based approach, since they have co-authors r3 and r10 with very similar (identical, in this case) names. Although this approach can improve performance in some cases,
it does not always work. For instance, the two ‘W. Wang’ references r1 and r8 are not
co-referent, though they both have co-authors with identical names ‘C. Chen’.
Instead of considering the attribute similarities of the related references, the collective
entity resolution approach (Pasula et al., 2003; Bhattacharya & Getoor, 2004; Singla
& Domingos, 2004; McCallum & Wellner, 2004; Li, Morie, & Roth, 2005; Dong et al.,
2005; Kalashnikov et al., 2005) takes into account the resolution decisions for them. In our
previous example, the correct evidence to use for the pair of references r1 and r8 is that
their co-author references do not map to the same entity, although they have similar names.
Therefore, in order to resolve the ‘W. Wang’ references in the collective resolution approach,
it is necessary to resolve the ‘C. Chen’ references as well, instead of considering the similarity
of their attributes. The collective entity resolution approach has recently been shown to
improve entity resolution accuracy over the previous approaches but is computationally
more challenging. The references cannot be resolved independently. Instead, any resolution
decision is affected by other resolutions through hyper-edges.
In earlier work (Bhattacharya & Getoor, 2004, 2006, 2007), we developed a relational
clustering algorithm (RC-ER) for collective entity resolution using relationships. The goal
of this approach is to cluster the references according to their entities taking the relationships
into account. We associate a cluster label r.C with each reference to denote its current
cluster membership. Starting from an initial set of clusters C = {ci } of references, the
algorithm iteratively merges the pair of clusters that are the most similar. To capture the
collective nature of the cluster assignment, the similarity measure between pairs of clusters
considers the cluster labels of the related references. The similarity of two clusters ci and
cj is defined as a linear combination of their attribute similarity simA and their relational
similarity simR :
sim(ci , cj ) = (1 − α) × simA (ci , cj ) + α × simR (ci , cj )

(1)

where α (0 ≤ α ≤ 1) is the combination weight. The interesting aspect of the collective
approach is the dynamic nature of the relational similarity. The similarity between two
references depends on the current cluster labels of their related references, and therefore
changes when related references change clusters. In our example, the similarity of the two
627

Bhattacharya & Getoor

clusters containing references ‘W. Wang’ and ‘W. W. Wang’ increases once their co-author
references named ‘A. Ansari’ are assigned to the same cluster. We now briefly review how
the two components of the similarity measure are defined.
Attribute Similarity: For each reference attribute, we use a similarity measure that
returns a value between 0 and 1 for two attribute values indicating the degree of similarity
between them. Several sophisticated similarity measures have been developed for names,
and popular TF-IDF schemes may be used for other textual attributes such as keywords.
The measure that works best for each attribute may be chosen. Finally, a weighted linear
combination of the similarities over the different attributes yields the combined attribute
similarity between two reference clusters.
Relational Similarity: Relational similarity between two clusters considers the similarity of their ‘cluster neighborhoods’. The neighborhood of each cluster is defined by the
hyper-edges associated with the references in that cluster. Recall that each reference r is
associated with one or more hyper-edges in H. Therefore, the hyper-edge set c.H for a
cluster c of references is defined as
c.H =

[

{h | h ∈ H ∧ r ∈ h.R}

(2)

r∈R∧r.C=c

This set defines the hyper-edges that connect a cluster c to other clusters, and are the
ones that relational similarity needs to consider. To illustrate, when all the references in
our running example have been correctly clustered as in Figure 1(b), the hyper-edge set
for the larger ‘Wang’ cluster is {h1 , h2 , h4 }, which are the hyper-edges associated with the
references r1 , r4 and r9 in that cluster.
Given the hyper-edge set for any cluster c, the neighborhood N br(c) of that cluster c is
the set of clusters labels of the references spanned by these hyper-edges:
N br(c) =

[

{cj | cj = r.C}

(3)

h∈c.H,r∈h

For our example ‘Wang’ cluster, its neighborhood consists of the ‘Ansari’ cluster and one
of the ‘Chen’ clusters, which are connected by its edge-set. Then, the relational similarity
measure between two clusters, considers the similarity of their cluster neighborhoods. The
neighborhoods are essentially sets (or multi-sets) of cluster labels and there are many possible ways to define the similarity of two neighborhoods (Bhattacharya & Getoor, 2007).
The specific similarity measure that we use for our experiments in this paper is Jaccard
similarity1 :
simR (ci , cj ) = Jaccard(N br(ci ), N br(cj ))
(4)
Clustering Algorithm: Given the similarity measure for a pair of clusters, a greedy
relational clustering algorithm can be used for collective entity resolution. Figure 2 shows
high-level pseudo-code for the complete algorithm. The algorithm first identifies the candidate set of potential duplicates using a ‘blocking’ approach (Hernández & Stolfo, 1995;
Monge & Elkan, 1997; McCallum, Nigam, & Ungar, 2000). Next, it initializes the clusters
1. Jaccard similarity for two sets A and B is defined as Jaccard(A, B) =

628

|A∩B|
|A∪B|

Query-time Entity Resolution

1.
2.

Algorithm RC-ER (Reference set R)
Find similar references in R using blocking
Initialize clusters using bootstrapping

3.
4.

For clusters ci , cj such that similar(ci , cj )
Insert hsim(ci , cj ), cj , cj i into priority queue

5.
6.
7.
8.
9.
10.
11.
12.
13.
14.

While priority queue not empty
Extract hsim(ci , cj ), ci , cj i from queue
If sim(ci , cj ) less than threshold, then stop
Merge ci and cj to new cluster cij
Remove entries for ci and cj from queue
For each cluster ck such that similar(cij , ck )
Insert hsim(cij , ck ), cij , ck i into queue
For each cluster cn neighbor of cij
For ck such that similar(ck , cn )
Update sim(ck , cn ) in queue
Figure 2: High-level description of the relational clustering algorithm

of references, identifies the ‘similar’ clusters — or potential merge-candidates — for each
cluster, inserts all the merge-candidates into a priority queue and then iterates over the
following steps. At each step, it identifies the current ‘closest pair’ of clusters from the candidate set and merges them to create a new cluster. It identifies new candidate pairs and
updates the similarity measures for the ‘related’ cluster pairs. This is the key step where
evidence flows from one resolution decision to other related ones and this distinguishes relational clustering from traditional clustering approaches. The algorithm terminates when
the similarity for the closest pair falls below a threshold or when the list of potential candidates is exhausted. The algorithm is efficiently implemented to run in O(nk log n) time
for n references where each ‘block’ of similar names is connected to k other blocks through
the hyper-edges.
3.1 Issues with Collective Resolution for Queries
In previous work, we (and others) have shown that collective resolution using relationships
improves entity resolution accuracy significantly for offline cleaning of databases. So, naturally, we would like to use the same approach for query-time entity resolution as well.
However, while the attribute-based and naive relational approaches discussed earlier can
be applied at query-time in a straight-forward fashion, that is not the case for collective
resolution. Two issues come up when using collective resolution for queries. First, the
set of references that influence the resolution decisions for a query need to be identified.
When answering a resolution query for ‘S. Russell’ using the attribute-based approach, it is
sufficient to consider all papers that have ‘S. Russell’ (or, similar names) as author name.
For collective resolution, in contrast, the co-authors of these author names, such as ‘P.
629

Bhattacharya & Getoor

Norvig’ and ‘Peter Norvig’, also need to be clustered according to their entities. This in
turn requires clustering their co-authors and so on. So the first task is to analyze these
dependencies for collective resolution and identify the references in the database that are
relevant for answering a query. But this is not enough. The set of references influencing a
query may be extremely large, but the query still needs to be answered quickly even though
the answer may not be completely accurate. So the second issue is performing the resolution
task at query-time. These are the two problems that we address in the next few sections.

4. Analysis of Collective Resolution using Relational Clustering
For collective entity resolution, we have seen that resolution performance for the query
becomes dependent on the resolution accuracy of the related entities. Before we can analyze
which other references influence entity resolution for the query and to what extent, we need
to analyze the nature of this dependence for collective resolution in general. In this section,
we identify the structural properties of the data that affect collective entity resolution and
formally model the interdependent nature of the resolution performance. This analysis
also helps us to understand when collective resolution using relational clustering helps,
and, equally importantly, when it has an adverse effect as compared against traditional
attribute-based resolution.
The goal of an entity resolution algorithm is to partition the set R = {ri } of references
into a set of clusters C = {ci } according to the underlying entities E = {ei }. The accuracy of the resolution depends on how closely the separation of the references into clusters
corresponds to the underlying entities. We consider two different measures of performance.
The first measure is recall for each entity. For any entity ei , recall counts how many pairs
of references corresponding to ei are correctly assigned to the same computed cluster. The
second measure is precision for each computed cluster. For any cluster ci , precision counts
how many pairs of references assigned to ci truly correspond to the same underlying entity.
(Alternatively, imprecision measures how many pairs of references assigned to the cluster
do not correspond to the same entity.) In the next two subsections, we analyze how these
two performance metrics are influenced, first, by the attribute values of the references, and
then, by the observed relationships between them.
4.1 Influence of Attributes
First, consider an entity resolution algorithm that follows the traditional attribute-based approach and the analysis of its performance. Such an algorithm only considers the attributes
of individual references. It uses a similarity measure defined over the domain of attributes,
and considers pair-wise attribute similarity between references for resolving them. Let us
define two references to be ǫ-similar if their attribute-similarity is at least ǫ. Then, given
a resolution threshold ǫ, the attribute-based approach assigns a pair of references to the
same cluster if and only if they are ǫ-similar. To illustrate using our example, using any
similarity measure defined over names and an appropriately determined similarity threshold
ǫ, the attribute-based approach would assign the three ‘W. Wang’ references (r1 , r4 , r8 ) to
one cluster c1 and the ‘W. W. Wang’ reference (r9 ) to a different cluster c2 . This resolution
of the Wang references is not perfect in terms of precision or recall, since references r1 , r4
and r9 map to one entity e1 and r8 maps to a second entity e2 . Cluster c1 has precision less
630

Query-time Entity Resolution

than 1, since it incorrectly includes references for two different entities, and recall is less
than 1 for entity e1 , since its references are dispersed over two different clusters.
In order to analyze the performance of this attribute-based resolution approach given
an arbitrary dataset, we now characterize a dataset in terms of the attribute values of
its references. Intuitively, the attribute-based approach works well when the references
corresponding to the same entity are similar in terms of their attributes, and when the
references corresponding to different entities are not. To capture this formally, we define
two probabilities that measure the attribute-similarity of references that map to the same
entity, and the attribute-similarity of those that map to different entities:
• attribute identification probability aI (e, ǫ): the probability that a pair of references chosen randomly from those corresponding to entity e are ǫ-similar to each
other.
• attribute ambiguity probability aA (e1 , e2 , ǫ): the probability that a pair of references chosen randomly such that one corresponds to entity e1 and the other to entity
e2 are ǫ-similar to each other.
To illustrate using the four ‘Wang’ references, r1 , r4 and r9 correspond to the same
entity e1 and r8 corresponds to a different entity e2 . Also, assume that for some similarity
measure for names and an appropriate threshold ǫ, references r1 , r4 and r8 are ǫ-similar to
each other. Then, of the 3 pairs of references corresponding to entity e1 , only one (r1 and
r4 ) is ǫ-similar, so that the attribute identification probability aI (e1 , ǫ) for entity e1 is 0.33.
On the other hand, of the three pairs of references such that one maps to e1 and the other
to e2 , two (r1 and r8 , r4 and r8 ) are ǫ-similar. This means that the attribute ambiguity
probability aA (e1 , e2 , ǫ) between e1 and e2 is 0.66.
As can be seen from the above example, the performance of the attribute-based clustering algorithm can be represented in terms of these two probabilities. For any specified
threshold ǫ, the pairs of references for any entity that are correctly recalled are the ones that
are ǫ-similar, which is exactly what aI (e, ǫ) captures. Therefore, the recall for any domain
entity e is R(e, ǫ) = aI (e, ǫ). On the other hand, consider the cluster assignment for all the
references that correspond to two entities e1 and e2 . The pairs that are incorrectly clustered
together are those that correspond to two different entities, and yet are ǫ-similar. This is
what aA (e1 , e2 , ǫ) captures. Therefore the imprecision of the cluster assignment of reference
pairs corresponding to entities e1 and e2 is I(e1 , e2 , ǫ) = aA (e1 , e2 , ǫ). Alternatively, the
precision is given by P (e1 , e2 , ǫ) ≡ 1 − I(e1 , e2 , ǫ) = 1 − aA (e1 , e2 , ǫ).
4.2 Influence of Relationships
Now, we consider the collective entity resolution approach that additionally makes use of
relationships, and analyze its impact on entity resolution accuracy. Recall that we have
a set H = {hj } of observed co-occurrence relationships between the references. Such cooccurrences between references are useful for entity resolution when they result from strong
ties or relations between their underlying entities. Specifically, we assume that references to
any entity ei co-occur frequently with references to a small set of other entities {e1i , . . . , eki },
which we call the entity neighbors, denoted N (ei ), of entity ei .
631

Bhattacharya & Getoor

W.W. Wang
W. Wang

h4

A. Ansari
A. Ansari

W. Wang
W. Wang

h1

h1
h3

C. Chen
C. Chen

Figure 3: Illustration of (a) identifying relation and (b) ambiguous relation from running
example. Dashed lines represent co-occurrence relations.

Assuming such a neighborhood relationship among the underlying entities allows us
to analyze the performance of the relational clustering approach. For those reference pairs
that are ǫ-similar in terms of attributes, the attribute evidence is enough for resolution. But
now, unlike attribute-based clustering, any pair of references that are δ-similar in terms of
attributes, for some δ < ǫ, are considered as candidates for being clustered together. Not all
of them actually get assigned to the same cluster. For reference pairs that are in the ring of
uncertainty between ǫ and δ, their relationships play a role in determining if they are similar
enough, and consequently, if they should be clustered together. Specifically, if references ri
and rj co-occur through hyper-edge h and references ri′ and rj′ co-occur through hyper-edge
h′ , then the relational similarity of the pair (ri , ri′ ) is more when (rj , rj′ ) belong to the same
cluster. In general, multiple such relationships may be needed for tipping the balance, but
for simplicity, we assume for now that a single pair of related references is sufficient. In
other words, ri and ri′ get assigned to the same cluster if rj and rj′ are in the same cluster.
We now analyze the impact that this approach has on entity resolution performance.
Without loss of generality, assume that the (rj , rj′ ) pair get clustered together first by the
relational clustering algorithm. This results in the other pair (ri , ri′ ) also getting clustered
at some later iteration by considering this relational evidence. To see if this is accurate, we
consider two situations, as we did with attribute evidence. The first is shown in Figure 3(a),
where both pairs truly correspond to the same entity. Then the collective resolution decision
is correct and we say that hyper-edges h and h′ are identifying relationships for that entity.
Formally,
IRel(h, h′ , e) ≡ ∃ ri , rj ∈ h.R, ri′ , rj′ ∈ h′ .R,
E(ri ) = E(ri′ ) = e, E(rj ) = E(rj′ )

(5)

On the other hand, we may have a different scenario, in which both pairs of references correspond to two different entities. This second scenario is depicted in Figure 3(b). Then the
first decision to resolve (rj , rj′ ) as co-referent is incorrect, and relational evidence obtained
through hyper-edges h and h′ consequently leads to the incorrect resolution of (ri , ri′ ). In
this situation, collective resolution hurts accuracy, and we say that h and h′ form ambiguous
relationships for both pairs of entities, whose references may be incorrectly clustered as a
result of these relationships. Formally,
IAmb(h, h′ , e, e′ ) ≡ ∃ ri , rj ∈ h.R, ri′ , rj′ ∈ h′ .R,
632

Query-time Entity Resolution

E(ri ) = e, E(ri′ ) = e′ , e 6= e′ ,
E(rj ) 6= E(rj′ )

(6)

In general, a reference ri can have a co-occurrence relation h that includes more than
one other reference. We may think of this as multiple co-occurrence pairs involving ri .
Cluster labels of all these other references in the pairs influence resolution decisions for ri .
When resolving ri with another reference ri′ that participates in co-occurrence relation h′ ,
the fraction of common cluster labels between h and h′ determines whether or not ri and
ri′ will be clustered together. If they are assigned to the same cluster, h and h′ are labeled
identifying or ambiguous relationships based on whether ri and ri′ are actually co-referent
or not.
Formally, we define:
• identifying relationship probability rI (e, δ): the probability that a randomly chosen pair of δ-similar references corresponding to entity e has identifying relationships
h and h′ with some other entity.
• ambiguous relationship probability rA (e1 , e2 , δ): the probability that a pair of
δ-similar references, chosen randomly such that one corresponds to entity e1 and the
other to entity e2 , has ambiguous relationships h and h′ with some other pair of
entities.
To illustrate these probabilities using our example, we have two ‘Wang’ entities, e1
that has references r1 , r4 and r9 , and e2 that has reference r8 . Assume that the attribute
threshold δ is such that all six pairs are considered potential matches. Of the three pairs of
references corresponding to e1 , all of them have identifying relationships with the ‘Ansari’
entity. So, rI (e1 , δ) = 1. To measure the relational ambiguity between the two ‘Wang’
entities, we consider the 3 possible pairs (r1 and r8 , r4 and r8 , r9 and r8 ). Of these only
one (r1 and r8 ) pair has ambiguous relationships with two different ‘Chen’ entities. So,
rA (e1 , e2 , δ) = 0.33.
Given these two probabilities, we can analyze the performance of our relational clustering algorithm that combines attribute and relational evidence for collective entity resolution.
It is not hard to see that the recall for any entity depends recursively on the recall of its
neighbor entities. Any pair of references for entity e is resolved correctly on the basis of
attributes alone with probability aI (e, ǫ) (the identifying attribute probability). Furthermore, it may still be resolved correctly in the presence of identifying relationships with a
neighbor entity, if the related reference pair for the neighbor is resolved correctly. Denoting
as R(e, ǫ, δ) the recall for entity e and that for its neighbors as R(N (e), ǫ, δ), we have:
R(e, ǫ, δ) = aI (e, ǫ) + (1 − aI (e, ǫ)) × rI (e, δ) × R(N (e), ǫ, δ)

(7)

On the other hand, consider a pair of entities e1 and e2 . The cluster assignment for
a pair of references corresponding to e1 and e2 is imprecise on the basis of its attributes
alone with probability aA (e1 , e2 , ǫ). Even otherwise, the cluster assignment can go wrong
by considering relational evidence. This happens in the presence of ambiguous relationships
with references corresponding to another pair of entities, if those references are also clustered
633

Bhattacharya & Getoor

together incorrectly. So the imprecision I(e1 , e2 , ǫ, δ) of the cluster assignment of reference
pairs corresponding to entities e1 and e2 turns out to be:
I(e1 , e2 , ǫ, δ) = aA (e1 , e2 , ǫ) + (1 − aA (e1 , e2 , ǫ)) × rA (e1 , e2 , δ) × I(N (e1 ), N (e2 ), ǫ, δ) (8)
In general, any entity e has multiple neighbors ei in its neighborhood N (e). To formalize the performance dependence on multiple neighbors, assume that if a co-occurrence
involving references corresponding to e is chosen at random, the probability of selecting a
co-occurrence with a reference corresponding to ei is pei . Then recall is given as:
|N (e)|

R(e) = aI (e) + (1 − aI (e)) × rI (e) ×

X

pei R(ei )

(9)

i=1

Note that we have dropped ǫ and δ for notational brevity. For defining imprecision, observe
that a reference corresponding to any neighbor ei1 of e1 may co-occur with a reference for
any neighbor ej2 of e2 with probability pei 1 pej 2 . Then imprecision is given as:
|N (e1 )| |N (e2 )|

I(e1 , e2 ) = aA (e1 , e2 ) + (1 − aA (e1 , e2 )) × rA (e1 , e2 ) ×

X

X

i=1

j=1

pei 1 pej 2 I(ei1 , ej2 )

(10)

Given similarity thresholds ǫ and δ, relational clustering increases recall beyond that
achievable using attributes alone. This improvement is larger when the probability of identifying relationships is higher. On the flip side, imprecision also increases with relational
clustering. Typically, a low attribute threshold ǫ that corresponds to high precision is used,
and then recall is increased using relational evidence. When the probability of ambiguous
relations rA is small, the accompanying increase in imprecision is negligible, and performance is improved overall. However, the higher the ambiguous relationship probability
rA , the less effective is relational clustering. Thus the balance between ambiguous and
identifying relations determines the overall benefit of collective resolution using relational
clustering. When rA is high compared to rI , imprecision increases faster than recall, and
overall performance is adversely affected compared to attribute-based clustering. Eq. (9)
and Eq. (10) quantify this dependence of resolution performance for any entity on the nature
of its relationships with other entities. In the next section, we will use these equations to
design and analyze a relational clustering algorithm for answering entity resolution queries.

5. Collective Resolution for Queries
Our analysis of collective resolution using relational clustering showed that the resolution accuracy for any underlying entity depends on the resolution accuracy for its related/neighboring entities. For the problem of answering entity resolution queries, the goal
is not to resolve all the entities in the database. We need to resolve entities for only those
references that are retrieved for the query. We have seen that collective resolution leads to
potential performance improvements over attribute-based resolution. We now investigate
how collective resolution can be applied for answering queries to get similar improvements.
The obvious hurdle is illustrated by the expressions for performance metrics in Eq. (9) and
Eq. (10). They show that in order to get performance benefits for resolving the query using
634

Query-time Entity Resolution

relational clustering, we need to resolve the neighboring entities as well. Furthermore, to
resolve the neighboring entities, we need to resolve their neighboring entities, and so on.
These other entities that need to be resolved can be very large in number, and resolving
them is expensive in terms of query-processing time. Also, none of them are actually going
to be retrieved as part of the answer to the query. So it is critical to identify and resolve
those entities that contribute the most for improving resolution accuracy for the query.
We propose a two-stage query processing strategy, consisting of an extraction phase, for
identifying all the relevant references that need to be resolved for answering the query, and
a resolution phase, where the relevant references that have been extracted are collectively
resolved using relational clustering. Unfolding Eq. (9) and Eq. (10) starting from the query
entities leads to a natural expansion process. In this section, we describe the extraction
process using two novel expansion operators and, in parallel, we analyze the improvement
in resolution accuracy that is obtained from considering co-occurrences.
Recall that an entity resolution query Q(R.A = a) is specified using an attribute A
and a value a for it. The answer to the query consists of a partitioning of all references
r that have r.A = a or some value δ-similar to a. The correct answer to the query, in
general, involves references from multiple entities {eq }. We measure resolution accuracy for
the query using two metrics as before. For each of the query entities eq , we measure recall
R(eq ) and imprecision I(eq , e′ ) with respect to any other entity e′ . Entity e′ may or may
not belong to {eq }.
Before going into the details of our algorithm for collective resolution of queries, we
briefly recall the accuracy of the attribute-based strategy of resolving a query. This approach
considers all references r with r.A δ-similar to a, and resolves them using their attributes
only. The recall that results from this approach is R(eq , δ) = aI (eq , δ), and the imprecision
is given by I(eq , e′ , δ) = aA (eq , e′ , δ).
We propose two expansion operators for constructing the relevant set for an entity
resolution query. We denote as level-0 references all references that are δ-similar to the
query attribute. These are the references that the user is interested in, and the goal is
to resolve these correctly. The first operator we introduce is the attribute expansion
operator XA , or A-expansion for short. Given an attribute A and a value a for that
attribute, XA (a, δ) returns all references r whose attributes r.A exactly match a or are δsimilar to a. For a query Q(R.A = a), the level-0 references can be retrieved by expanding
Q as:
Rel0 (Q) = XA (a, δ)
The first step in Figure 4 shows A-expansion for the query Q(R.N ame = W.W ang) in our
example. It retrieves the four references (r1 ,r4 ,r8 ,r9 ) that have name ‘W. Wang’ or ‘W. W.
Wang’.
To consider co-occurrence relations, we construct the level-1 references by including
all references that co-occur with level-0 references. For this, we use our second operator,
which we call hyper-edge expansion XH , or H-expansion. For any reference r, XH (r)
returns all references that share a hyper-edge with r, and for a set R of references XH (R)
S
returns r∈R XH (r). Collective entity resolution requires that we consider all co-occurring
references for each reference. This is achieved by performing H-expansion on the references
635

Bhattacharya & Getoor

Q
R.Name=W_Wang

1

r 11 A_Ansari

Rel (Q)

0

Rel (Q)

r 9 W_W_Wang

r 10 A_Ansari

r 4 W_Wang

r 5 A_Ansari

r 54 A_Ansari

r 3 A_Ansari

r 23 C_Chen
...

r 1 W_Wang

r 2 C_Chen

r 8 W_Wang

r 7 C_Chen
r 6 L_Li

Rel2 (Q)

...

r 89 C_Chen
r 16 L_Li
...
r 66 L_Li

Figure 4: Relevant set for query Q(R.N ame = W.W ang) using H-expansion and Aexpansion alternately

at level-0 to retrieve the level-1 references:
Rel1 (Q) = XH (Rel0 (Q))
Figure 4 illustrates this operation in our example, where XH (r1 ) retrieves references ‘C.
Chen’ (r2 ) and ‘A. Ansari’ (r3 ), and so on.
To perform collective resolution for the query, we additionally need to resolve the references at level-1. One option for level-1 references is attribute-based resolution using a
conservative ǫ-similarity to keep imprecision to a minimum. We can use our analysis technique from before to evaluate the performance for this approach. Expanding from Eq. (9),
and substituting aI (eiq , ǫ) for the recall of each neighboring entity eiq for eq , the recall for
any query entity is:
R(eq , ǫ, δ) = aI (eq , ǫ) + (1 − aI (eq , ǫ)) × rI (eq , δ) ×

k
X

e

pi q aI (eiq , ǫ)

i=1

Similarly, on substituting aA (eiq , ej , ǫ) in Eq. (10) for the imprecision of each neighboring
entity eiq , we get the following expression for imprecision:
I(eq , e′ , ǫ, δ) = aA (eq , e′ , ǫ) + (1 − aA (eq , e′ , ǫ)) × rA (eq , e′ , δ) ×

k X
l
X

e

′

′

pi q pej aA (eiq , e j , ǫ)

i=1 j=1

To appreciate more easily the implications of considering first-order neighbors, we may
assume that the attribute identification probability and the attribute ambiguity probability
are the same for all the entities involved, i.e., aI (e, ǫ) = aI (ǫ) and aA (e, e′ , ǫ) = aA (ǫ). Then,
P
using ki=1 pei = 1 for any entity e, the expression for recall simplifies to
R(eq , ǫ, δ) = aI (ǫ) + (1 − aI (ǫ)) × rI (δ) × aI (ǫ)
= aI (ǫ)[1 + (1 − aI (ǫ))rI (δ)]
636

Query-time Entity Resolution

Similarly, the expression for imprecision simplifies to
I(eq , e′ , ǫ, δ) = aA (ǫ)[1 + (1 − aA (ǫ))rA (δ)]
So we can see that attribute-clustering of the first level neighbors potentially increases
recall for any query entity eq , but imprecision goes up as well. However, when the balance between rA and rI is favorable, the increase in imprecision is insignificant and much
smaller than the corresponding increase in recall, so that there is an overall performance
improvement.
Can we do better than this? We can go a step further and consider co-occurrence
relations for resolving the level-1 references as well. So, instead of considering attributebased resolution for references in level-1 as before, we perform collective resolution for them.
We consider all of their δ-similar references, which we call level-2 references (Rel2 (Q)), using
A-expansion:
Rel2 (Q) = XA (Rel1 (Q))
Note that we have overloaded the A-expansion operator for a set R of references: XA (R) =
r∈R XA (r.A). The level-3 references are the second order neighbors that co-occur with
level-2 references. They are retrieved using H-expansion on the level-2 references:

S

Rel3 (Q) = XH (Rel2 (Q))
Finally, as with the level-1 references earlier, we resolve the level-3 references using ǫsimilarity of their attributes alone.
In order to evaluate the impact on resolution accuracy for the query, we unfold the
recursions in Eq. (9) and Eq. (10) up to two levels, and now substitute aI (eiq , ǫ) for recall
and aA (ei , ej , ǫ) for imprecision for the second order neighbors. The trend in the expressions
becomes clearly visible if we assume, as before, that aI and aA is identical for all entities, and,
additionally, rI and rA are also the same, i.e., rI (e1 , e2 , ǫ) = rI (ǫ) and rA (e1 , e2 , δ) = rA (δ).
Then, we can work through a few algebraic steps to get the following expressions for recall
and precision for any query entity eq :
R(eq ) = aI [1 + (1 − aI )rI + (1 − aI )2 rI2 ]
′

I(eq , e ) = aA [1 + (1 − aA )rA + (1 −

2
aA )2 rA
]

(11)
(12)

We can continue to unfold the recursion further and grow the relevant set for the query.
Formally, the expansion process alternates between A-expansion and H-expansion:
Reli (Q) = XA (Q)
XH (Reli−1 (Q))
XA (Reli−1 (Q))

for i = 0
for odd i
for even i

As we proceed recursively and consider higher order co-occurrences for the query, additional terms appear in the expressions for precision and recall. But this does not imply
that we need to continue this process to arbitrary levels to get optimum benefit. Using
our simplifying assumptions about the attribute and relational probabilities, the expressions for both recall and imprecision for nth order co-occurrences turns out to be geometric
637

Bhattacharya & Getoor

progressions with n + 1 terms. The common ratio for the two geometric progressions are
(1 − aI (ǫ))rI (δ) and (1 − aA (ǫ))rA (δ) respectively. Typically, both of these ratios are significantly smaller than 1, and therefore converge very quickly with increasing co-occurrence
level. So the improvement in resolution accuracy for the query Q falls off quickly with
expansion depth, and we can terminate the expansion process at some cut-off depth d∗
without compromising on accuracy:
∗

Rel(Q) =

d
[

Reli (Q)

i=0

Of course, the assumptions about the attribute and relational probabilities being entityindependent do not hold in practice, so that the performance trends for increasing levels of
co-occurrence cannot be exactly captured by geometric progressions with a common ratio
for successive terms. But the converging trends for both of them still hold in general, and
the rate of convergence is still determined by the four probabilities aI , aA , rI and rA for the
entities that are encountered during the expansion process. Intuitively, smaller values for
rI and rA indicate less sensitivity to co-occurrences, and the convergence is quicker. On
the other hand, higher values of aI and aA mean that more entities are resolved based on
attributes alone — correctly or incorrectly — and the impact of co-occurrence relations is
smaller. Therefore convergence is quicker for higher values of aI and aA .
Apart from imposing a cutoff on the expansion depth, the size of the relevant set can
also be significantly reduced by restricting attribute expansion beyond level-0 to exact
e (r). This only considers references with exactly the same attribute as
A-expansion XA
r and disregards other δ-similar references. Interestingly, we can show that the restricted
strategy that alternates between exact A-expansion and H-expansion does not reduce recall
significantly.

6. Adaptive Query Expansion
The limited depth query expansion strategy proposed in the previous section is an effective
approach that is able to answer queries quickly and accurately for many domains. However,
for some domains, the size of the relevant set that is generated can be extremely large even
for small expansion depths, and as a result, the retrieved references cannot be resolved
at query-time. In this section, we propose adaptive strategies based on estimating the
‘ambiguity’ of individual references that makes our algorithm even more efficient while
preserving accuracy.
The main reason behind this explosive growth of the relevant set with increasing levels
is that our query expansion strategy from the previous section is unconstrained — it treats
all co-occurrences as equally important for resolving any entity. It blindly expands all
references in the current relevant set, and also includes all new references generated by an
expansion operation. Given the limited time to process a query, this approach is infeasible
for domains that have dense relationships. Our solution is to identify the references that are
likely to be the most helpful for resolving the query, and to focus on only those references.
To illustrate using our example from Figure 4, observe that ‘Chen’ and ‘Li’ are significantly
more common or ‘ambiguous’ names than ‘Ansari’ — even different ‘W. Wang’ entities are
likely to have collaborators named ‘Chen’ or ‘Li’. Therefore, when h-expanding Rel0 (rq ) for
638

Query-time Entity Resolution

‘W. Wang’, ‘Ansari’ is more informative than ‘Chen’ or ‘Li’. Similarly, when n-expanding
Rel1 (rq ), we can choose not to expand the name ‘A. Ansari’ any further, since two ‘A.
Ansari’ references are very likely to be coreferent. But we need more evidence for the
‘Chen’s and the ‘Li’s.
To describe this formally, the ambiguity of a value a for an attribute A is the probability that any two references ri and rj in the database that have ri .A = rj .A = a are not
coreferent: Amb(a) = P (E(ri ) 6= E(rj ) | ri .A = rj .A = a). The goal of adaptive expansion
is to add less ambiguous references to the relevant set and to expand the most ambiguous
references currently in the relevant set. We first define adaptive versions of our two expansion operators treating the ambiguity estimation process as a black-box, and then look at
ways to estimate ambiguity of references.
6.1 Adaptive Expansion Operators
The goal of adaptive expansion is to selectively choose the references to expand from the
current relevant set, and also the new references that are included at every expansion step.
For adaptive hyper-edge expansion, we set an upper-bound hmax on the number of new
references that h-expansion at a particular level can generate. Formally, we want
|XH (Reli (Q))| ≤ hmax |Reli (Q)|. The value of hmax may depend on depth i but should be
small enough to rule out full h-expansion of the current relevant set. Then, given hmax , our
strategy is to choose the least ambiguous references from XH (Reli (Q)), since they provide
the most informative evidence for resolving the references in Reli (Q). To achieve this, we
sort the h-expanded references in increasing order of ambiguity and select the first k from
them, where k = hmax |Reli (Q)|.
i−1
i
Reladapt
(Q, hmax ) = LeastAmb(k, XH (Reladapt
(Q)))

(13)

The setting for adaptive attribute expansion is very similar. For some positive number amax , exact a-expansion of Reli (Q) is allowed to include at most amax |Reli (Q)| references. Note that now the selection preference needs to be flipped — more ambiguous names
e (Reli (Q)) in decreasing
need more evidence, so they are expanded first. So we can sort XA
order of ambiguity and select the first k from the sorted list, where k = amax |Reli (Q)|. But
this could potentially retrieve only references for the most ambiguous name, totally ignoring
references with any other name. To avoid this, we choose the top k ambiguous references
from Reli (Q) before expansion, and then expand the references so chosen.
i
e
i
Reladapt
(Q, nmax ) = XA
(M ostAmb(k, Reladapt
(Q)))

(14)

Though this cannot directly control the number of new references added, µr × k is a reasonable estimate, where µr is the average number of references per name.
6.2 Ambiguity Estimation
The adaptive expansion scheme proposed in this section is crucially dependent on the estimates of name ambiguity. We now describe one possible scheme that worked quite well.
Recall that we want to estimate the probability that two randomly picked references with
value a for attribute A correspond to different entities. For a reference attribute A1 , denoted
639

Bhattacharya & Getoor

1.
2.

Algorithm Query-time Resolve (R.Name name)
RSet = RelevantFrontier(name)
RC-ER(RSet)

1.
5.
3.
4.
5.
6.
7.
8.
9.
10.
10.
11.

Algorithm FindRelevantRefs(R.Name name)
Initialize RSet to {}
Initialize depth to 0
Initialize FrontierRefs to {}
While depth < d*
If depth is even or 0
R = XA (FrontierRefs)
else
R = XH (FrontierRefs)
FrontierRefs = R
Add FrontierRefs to RSet
Increment depth
Return RSet
Figure 5: High-level description of the query-time entity resolution algorithm

R.A1 , a naive estimate for the ambiguity of a value of n for the attribute is:
Amb(r.A1 ) =

|σR.A1 =r.A1 (R)|
,
|R|

where |σR.A1 =r.A1 (R)| denotes the number of references with value r.A1 for A1 . This estimate is clearly not good since the number of references with a certain attribute value does
not always match the number of different entity labels for that attribute. We can do much
better if we have an additional attribute A2 . Given A2 , the ambiguity for value of A1 can
be estimated as
|δ(πR.A2 (σR.A1 =r.A1 (R)))|
Amb(r.A1 | r.A2 ) =
,
|R|
where |δ(πR.A2 (σR.A1 =r.A1 (R)))| is the number of distinct values observed for A2 in references with R.A1 = r.A1 . For example, we can estimate the ambiguity of a last name by
counting the number of different first names observed for it. This provides a better estimate
of the ambiguity of any value of an attribute A1 , when A2 is not correlated with A1 . When
multiple such uncorrelated attributes Ai are available for references, this approach can be
generalized to obtain better ambiguity estimates.
Putting everything together, high-level pseudo code for the query-time entity resolution
algorithm is shown in Figure 5. The algorithm works in two stages — first, it identifies the
relevant set of references given an entity name as a query, and then it performs relational
clustering on the extracted relevant references. The relevant references are extracted using
the recursive process that we have already seen. The relevant references at any depth i are
obtained by expanding the relevant references at depth i−1, the expansion being dependent
640

Query-time Entity Resolution

of whether it is an odd step or an even step. The actual expansion operator that is used
may either be unconstrained or adaptive.

7. Empirical Evaluation
For experimental evaluation of our query-time resolution strategies, we used both realworld and synthetically generated datasets. First, we describe our real datasets and the
experiments performed on them and then we move on to our experiments on synthetic data.
7.1 Experiments on Real Data
For real-world data, we used two citation datasets with very different characteristics. The
first dataset, arXiv, contains papers from high energy physics and was used in KDD Cup
20032 . It has 58,515 references to 9,200 authors, contained in 29,555 publications. The number of author references per publication ranges from 1 to 10 with an average of 1.90. Our
second dataset is the Elsevier BioBase database3 of publications from biology used in the
recent IBM KDD-Challenge competition. It includes all publications under ‘Immunology
and Infectious Diseases’ between years 1998 and 2001. This dataset contains 156,156 publications with 831,991 author references. The number of author references per publication
is significantly higher than arXiv and ranges from 1 to 100 (average 5.3). All names in this
database only have initials for first and middle names (if available), unlike arXiv, which has
both initialed and complete names. The number of distinct names in BioBase is 303,693,
with the number of references for any name ranging from 1 to 193 (average 2.7). Unlike
arXiv, BioBase includes keywords, topic classification, language, country of correspondence
and affiliation of the corresponding author as attributes of each paper, all of which we use
as attributes for resolution in addition to author names. BioBase is diverse in terms of
these attributes, covering 20 languages, 136 countries, 1,282 topic classifications and 7,798
keywords.
For entity resolution queries in arXiv, we selected all ambiguous names that correspond
to more than one author entity. This gave us 75 queries, with the number of true entities
for the selected names varying from 2 to 11 (average 2.4). For BioBase, we selected as
queries the top 100 author names with the highest number of references. The average
number of references for each of these 100 names is 106, and the number of entities for the
selected names ranges from 1 to 100 (average 32), thereby providing a wide variety of entity
resolution settings over the queries.
7.1.1 Relevant Set Size Vs. Resolution Time
We begin by exploring the growth rate of the relevant set for a query over expansion depth
in the two datasets. Figure 6(a) plots the size of the relevant set for a sample query on the
name ‘T. Lee’ for arXiv and ‘M. Yamashita’ for BioBase. The growth rate for the arXiv
query is moderate. The number of references with name ‘T. Lee’ is 7, which is the number
of relevant references at depth 0, and the size grows to 7,500 by depth 7. In contrast, for
BioBase the plots clearly demonstrate the exponential growth of the relevant references
2. http://www.cs.cornell.edu/projects/kddcup/index.html
3. http://help.sciencedirect.com/robo/projects/sdhelp/about biobase.htm

641

Bhattacharya & Getoor

800

900

BioBase: similar
BioBase: exact
arXive: exact

700

700
time (secs)

# references
(in thousands)

600
500
400
300

600
500
400
300

200

200

100

100

0

0
0

(a)

BioBase
arXiv

800

1

2

3

4

expansion depth

5

6

7

0
(b)

10

20
30
40
50
#references (in thousands)

60

70

Figure 6: (a) Size of the relevant set for increasing expansion depth for sample queries
in arXiv and BioBase (b) Execution time of RC-ER with increasing number of
references

with depth for both name expansion strategies. There are 84 relevant references at depth
0. When references are expanded using name similarity expansion, there are 722 relevant
references at depth 1, 65,000 at depth 3 and more than 586,000 at depth 5. This is for a
very restricted similarity measure where two names are considered similar only if their first
initials match, and the last names have the same first character and differ overall by at most
2 characters. A more liberal measure would result in a significantly faster growth. We also
observe that for exact expansion, the growth is slower but we still have 45,000 references at
depth 3, 384,000 at depth 5 and 783,000 by depth 7. It is interesting to note that the growth
slows down beyond depth 5; but this is because most of the references in the entire dataset
are already covered at that depth (BioBase has 831,991 references in total). The growth
rates for these two examples from arXiv and BioBase are typical for all of our queries in
these two datasets.
Next, in Figure 6(b), we observe how the relational clustering algorithm RC-ER scales
with increasing number of references in the relevant set. All execution times are reported
on a Dell Precision 870 server with 3.2GHz Intel Xeon processor and 3GB of memory. The
plot shows that the algorithm scales well with increasing references, but the gradient is
different for the two datasets. This is mainly due to the difference in the average number of
references per hyper-edge. This suggests that for arXiv, RC-ER is capable of handling the
relevant sets generated using unconstrained expansion. But for BioBase, it would require
up to 600 secs for 40,000 references, and up to 900 secs for 65,000. So it is clearly not
possible to use RC-ER with unconstrained expansion for query-time resolution in BioBase
even for depth 3.
7.1.2 Entity Resolution Accuracy for Queries
In our next experiment, we evaluate several algorithms for entity resolution queries. We
compare entity resolution accuracy of the pair-wise co-reference decisions using the F1
measure (which is the harmonic mean of precision and recall). For a fair comparison, we
consider the best F1 for each of these algorithms over all possible thresholds for determining
642

Query-time Entity Resolution

Table 1: Average entity resolution accuracy (F1) for different algorithms over 75 arXiv
queries and 100 BioBase queries

A
A*
NR
NR*
RC-ER Depth-1
RC-ER Depth-3

arXiv
0.721
0.778
0.956
0.952
0.964
0.970

BioBase
0.701
0.687
0.710
0.753
0.813
0.820

duplicates. For the algorithms, we compare attribute-based entity resolution (A), naive
relational entity resolution (NR) that uses attributes of related references, and our relational
clustering algorithm for collective entity resolution (RC-ER) using unconstrained expansion
up to depth 3. We also consider transitive closures over the pair-wise decisions for the first
two approaches (A* and NR*). For attribute similarity, we use the Soft TF-IDF with
Jaro-Winkler similarity for names, which has been shown to perform the best for namebased resolution (Bilenko, Mooney, Cohen, Ravikumar, & Fienberg, 2003), and TF-IDF
similarity for the other textual attributes.
The average F1 scores over all queries are shown in Table 1 for each algorithm in the
two datasets. It shows that RC-ER improves accuracy significantly over the baselines.
For example in BioBase, the improvement is 21% over A and NR, 25% over A* and 13%
over NR*. This demonstrates the potential benefits of collective resolution for answering
queries, and validates recent results in the context of offline entity resolution (Bhattacharya
& Getoor, 2004, 2007; Singla & Domingos, 2004; Dong et al., 2005; McCallum & Wellner,
2004). In our earlier work (Bhattacharya & Getoor, 2007) we have demonstrated using
extensive experiments on real and synthetic datasets how our relational clustering algorithm
(RC-ER) improves entity resolution performance over traditional baselines in the context
of offline data cleaning, where the entire database is cleaned as a whole. The numbers
in Table 1 confirm that similar improvements can be obtained for localized resolution as
well. As predicted by our analysis, most of the accuracy improvement comes from the
depth-1 relevant references. For 56 out of the 100 BioBase queries, accuracy does not
improve beyond the depth-1 relevant references. For the remaining 44 queries, the average
improvement is 2%. However, for 8 of the most ambiguous queries, accuracy improves by
more than 5%, the biggest improvement being as high as 27% (from 0.67 to 0.85 F1). Such
instances are fewer for arXiv, but the biggest improvement is 37.5% (from 0.727 to 1.0).
On one hand, this shows that considering related records and resolving them collectively
leads to significant improvement in accuracy. On the other hand, it also demonstrates that
while there are potential benefits to considering higher order neighbors, they fall off quickly
beyond depth 1. This also serves to validate our analysis of collective query resolution in
Section 4.
643

Bhattacharya & Getoor

Table 2: Average query processing time with unconstrained expansion

A
A*
NR
NR*
RC-ER Depth-1
RC-ER Depth-3

1

arXiv
0.41
0.41
0.43
0.428
0.45
1.36

BioBase
9.35
9.59
28.54
28.69
11.88
606.98

1

depth 1
depth 2

depth 1
depth 2

0.9

0.7
recall

precision

0.8
0.9

0.6
0.5

0.8

0.4
0.3
0.7

0.2
1

0.8

0.6

0.4

0.2

0

1

0.8

similarity threshold

(a)

0.6
0.4
similarity threshold

0.2

0

(b)
1

1

0.9
0.8
recall

precision

0.9

0.8

0.7
0.6
0.5

0.7

0.4

depth 1
depth 2

0.6
1

0.8

depth 1
depth 2

0.3
0.6

0.4

0.2

0

1

similarity threshold

(c)

0.8

0.6
0.4
similarity threshold

0.2

0

(d)

Figure 7: Average precision and recall at different similarity thresholds for (a-b) BioBase
and (c-d) arXiv

The last two rows of Table 1 show the converging nature of entity resolution performance
with increasing depth. We verify this explicitly for precision and recall in Figure 7. The
top two plots show average precision and recall over BioBase queries at different similarity
thresholds for RC-ER. The bottom two plots show the same for arXiv. We can see that
the precision curve at depth 1 coincides with or stays marginally above the precision curve
at depth 3 for both BioBase and arXiv. The recall curves show the opposite trend — recall
644

Query-time Entity Resolution

marginally improves for depth 3. This is in agreement with our derived expressions for
precision and recall for increasing depth in Eq. (12). The difference in recall between depths
2.
1 and 3 can be quantified as aI (1 − aI )2 rI2 , and the difference in precision as aA (1 − aA )2 rA
The explanation for the small difference between average precision and recall in these two
plots is that both of these factors, when averaged over all queries, are significantly smaller
than 1 for arXiv and BioBase. We will investigate this converging nature of performance in
more detail by varying these structural properties in our experiments with synthetic data
in Section 7.2.
7.1.3 Reducing Time with Adaptive Expansion
The first set of experiments show the effectiveness of our two-phase query processing strategy
in terms of entity resolution performance. The challenge, as we have described earlier, is
in obtaining these benefits in real-time. So, next, we focus on the time that is required to
process these queries in the two datasets using unconstrained expansion up to depth 3. The
results are shown in Table 2. For arXiv, the average processing time for depth-3 expansion
is 1.36 secs, with 406 relevant references on average. This shows that our two-phase strategy
with unconstrained expansion is a practical processing strategy for entity resolution queries
— it resolves the query entities accurately, and extremely quickly as well. However, for
BioBase, the average number of references reached by depth 3 is more that 44,000, and the
time taken to resolve them collectively is more than 10 minutes. This is unacceptable for
answering queries, and next we focus on how the processing time is improved using our
proposed adaptive strategies. Note that the time taken for depth-1 expansion is around 12
secs, which is close to that for the attribute-based baseline (A) and less than the time for
the naive relational algorithm (NR).
Since unconstrained expansion is effective for arXiv, we focus only on BioBase for evaluating our adaptive strategies. For estimating ambiguity of references, we use last names
with first initial as the secondary attribute. This results in very good estimates of ambiguity — the ambiguity estimate for a name is strongly correlated (correlation coeff. 0.8)
with the number of entities for that name. First, we evaluate adaptive H-expansion. Since
H-expansion occurs first at depth 1, for each query, we construct the relevant set with cutoff
depth d∗ = 1, and use adaptive H-expansion for depth 1. The expansion upper-bound hmax
is set to 4. We compare three different adaptive H-expansion strategies: (a) choosing the
least ambiguous references, (b) choosing the most ambiguous references and (c) random
selection. Then, for each query, we evaluate entity resolution accuracy using RC-ER on
the relevant sets constructed using these three adaptive strategies. The average accuracies
for the three strategies over all 100 queries are shown in the first column of Table 3. Least
ambiguous selection, which is the strategy that we propose, clearly shows the biggest improvement and most ambiguous the smallest, while random selection is in between. Notably,
even without many of the depth-1 references, all of them improve accuracy over NR* by
virtue of collective resolution.
We perform a similar set of experiments for evaluating adaptive attribute expansion.
Recall that depth 2 is the lowest depth where adaptive attribute expansion is performed.
So for each query, we construct the relevant set with d∗ = 3 using adaptive A-expansion
at depth 1 and unconstrained H-expansion at depths 1 and 3. The expansion upper-bound
645

Bhattacharya & Getoor

Table 3: Avg. resolution accuracy in F1 with different adaptive expansion strategies

Least Ambiguous
Most Ambiguous
Random

H-expansion
0.790
0.761
0.770

A-expansion
0.815
0.821
0.820

amax is set to 0.2, so that on average 1 out of 5 names are expanded. Again, we compare three
strategies: (a) expanding the least ambiguous names, (b) expanding the most ambiguous
names and (c) random expansion. The average accuracies for the three schemes over all
100 queries are listed in the second column of Table 3. The experiment with adaptive Aexpansion does not bring out the difference between the three schemes as clearly as adaptive
H-expansion. This is because we are comparing A-expansion at depth 2 and, on average,
not much improvement can be obtained beyond depth 1 because of a ceiling effect. But
it shows that almost all the benefit up to depth 3 comes from our proposed strategy of
expanding the most ambiguous names.
The above two experiments demonstrate the effectiveness of the two adaptive expansion
schemes in isolation. Now, we look at the results when we use them together. For each
of the 100 queries, we construct the relevant set Rel(rq ) with d∗ = 3 using adaptive Hexpansion and adaptive exact A-expansion. Since most of the improvement from collective
resolution comes from depth-1 references, we consider two different experiments. In the
first experiment (AX-2), we use adaptive expansion only at depths 2 and beyond, and
unconstrained H-expansion at depth 1. In the second experiment (AX-1), we use adaptive
H-expansion even at depth 1, with hmax = 6. For both of them, we use adaptive expansion
at higher depths 2 and 3 with parameters hmax = 3 at 3 and amax = 0.2 at 2.
Table 4: Comparison between unconstrained and adaptive expansion for BioBase

relevant-set size
time (cpu secs)
accuracy (F1)

Unconstrained
44,129.5
606.98
0.821

AX-2
5,510.52
43.44
0.818

AX-1
3,743.52
31.28
0.820

In Table 4, we compare the two adaptive schemes against unconstrained expansion with
= 3 over all queries. Clearly, accuracy remains almost unaffected for both schemes.
First, we note that AX-2 matches the accuracy of unconstrained expansion, and shows
almost the same improvement over depth 1. This accuracy is achieved even though it
uses adaptive expansion that expands a small fraction of Rel1 (Q), and thereby reduces the
average size of the relevant set from 44,000 to 5,500. More significantly, AX-1 also matches
this improvement even without including many depth-1 references. This reduction in the
size of the relevant set has an immense impact on the query processing time. The average
processing time drops from more than 600 secs for unconstrained expansion to 43 secs for
d∗

646

1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2

1

pR=0.2
pR=0.5
pR=1.0

0.9
0.8
Precision

Recall

Query-time Entity Resolution

0.7
0.6
0.5

pRa=0.0
pRa=0.3
pRa=0.6

0.4
0.3
0.7

0.6
0.5
Sim. Threshold

0.4

0.7

0.65

0.6
0.55
0.5
Sim. Threshold

0.45

Figure 8: Effect of (a) identifying relations on recall and (b) ambiguous relations on precision for collective clustering. Error bars show standard deviation.

AX-2, and further to just 31 secs for AX-1, thus making it possible to use collective entity
resolution for query-time resolution.
7.1.4 Adaptive Depth Selection
As a further improvement, we investigate if processing time can be reduced by setting the
expansion depth d∗ adaptively, depending on the ambiguity of the query name, as compared
to a fixed d∗ for all queries. In a simple setup, we set d∗ to 1 for queries where the number
of different first initials for a last name is less than 10 (out of 26), and explore depth 2 only
for more ambiguous queries. This reduces expansion depth from 2 to 1 for 18 out of the 100
queries. As a result, the average processing time for these queries is reduced by 35% to 11.5
secs from 17.7 secs with no reduction in accuracy. For three of these queries, the original
processing time at depth 2 is greater than 30 secs. In these preliminary experiments, we only
evaluated our original set of 100 queries that are inherently ambiguous. In a more general
setting, where a bigger fraction of queries have lower ambiguity, the impact is expected to
be even more significant.
7.2 Experiments using Synthetic Data
In addition to experiments on real datasets, we performed experiments on synthetically
generated data. This enables us to reason beyond specific datasets, and also to empirically
verify our performance analysis for relational clustering in general, and more specifically for
entity resolution queries. We have designed a generator for synthetic data (Bhattacharya
& Getoor, 2007) that allows us to control different properties of the underlying entities and
the relations between them, and also of the observed co-occurrence relationships between
the entity references. Among other properties, we can control the number of entities,
the average number of neighbor entities per entity, and the number and average size of
observed co-occurrences. Additionally, we can control the ambiguity of entity attributes,
and the number of ambiguous relationships between entities. We present an overview of the
synthetic data generation process in Appendix A.
647

Bhattacharya & Getoor

1

0.9
0.85
Precision

0.8
Recall

0.95

t=0.9
t=0.6
t=0.5

0.9
0.7
0.6
0.5

0.8
0.75
0.7

0.4

0.65

0.3

0.6

0.2

t=0.9
t=0.6
t=0.5

0.55
0

1
2
Expansion Level

3

0

1
2
Expansion Level

3

Figure 9: Change in (a) precision and (b) recall for increasing expansion levels used for
collective clustering. Error bars show standard deviation.

We have performed a number of different experiments on synthetic data. In the first set
of experiments, we investigate the influence of identifying relationships on collective resolution using relational clustering. We generate 500 co-occurrence relations from the same
100 entities and 200 entity-entity relationships, using varying probability of co-occurrences
pR = {0.2, 0.5, 1.0} in the data. The probability of ambiguous relationships is held fixed,
so that higher pR translates to higher probability of identifying co-occurrences in the data.
Figure 8(a) shows recall at different similarity thresholds for three different co-occurrence
probabilities. The results confirm that recall increases progressively with more identifying
relationships at all thresholds. The curves for pR = 0.5 and pR = 1.0 flatten out only when
no further recall is achievable.
Next, we observe the effect of ambiguous relations on the precision of collective resolution using relational clustering. We add 200 binary relationships between 100 entities in
three stages with increasing ambiguous relationship probability (pR
a = {0, 0.3, 0.6}). Then
we perform collective resolution on 500 co-occurrence relations generated from each of these
three settings. In Figure 8(b) we plot precision at different similarity threshold for three different values of pR
a . The plots confirm the progressive decrease in precision for all thresholds
with higher pR
.
For
both experiments, the results are averaged over 200 different runs.
a
Next, we evaluate collective resolution for queries. Recall that the last two rows in Table 1 clearly demonstrate the converging nature of performance over increasing expansion
levels for queries on real datasets. We ran further experiments on synthetic data to verify
this trend. In each run, we generated 2,500 co-occurrence relations from 500 entities having
an average of 2 neighbors per entity. Then we performed localized collective clustering
in each case, using as query the most ambiguous attribute value (that corresponds to the
highest number of underlying entities). In Figure 9(c) and (d), we show how recall and precision change with increasing expansion level for a query. Recall improves with increasing
expansion level, while precision decreases overall, as is predicted by our analysis. Importantly, recall increases at a significantly faster rate than that for the decrease in precision.
In general, the rate of increase/decrease depends on the structural properties of the data,
as we have shown in our analysis. In other experiments, we have seen different rates of
648

Query-time Entity Resolution

change, but the overall trend remains the same. Our analysis also showed that precision
and recall converge quickly over increasing expansion levels. This too is confirmed by the
two plots where the curves flatten out by level 3.
7.3 Current Limitations
Finally, we discuss two of the current limitations of our collective entity resolution approach.
Recall that the similarity measure in Eqn. 1 involves a weighting parameter α for combining
attribute and relational similarity. For all of our experiments, we report the best accuracy
over all values of α for each query. Selecting the optimal value of α for each query is an
unresolved issue. However, our experiments reveal that even a fixed α (α = 0.5) for all
queries brings significant improvements over the baselines.
The second issue is the determination of the termination threshold for RC-ER. Note
that this is an issue for all of the baselines as well, and here we report best accuracy over
all thresholds. This is an area of ongoing research. Preliminary experiments have shown
that the best threshold is often query specific — setting the threshold depending on the
ambiguity of the query results in significantly better accuracy than a fixed threshold for all
queries. For an empirical evaluation, we cleaned the entire arXiv dataset offline by running
RC-ER on all its references together, and terminated at the threshold that maximizes
resolution accuracy over all references. This results in an overall accuracy (F1) of 0.98.
However, the average accuracy measured over the 75 queries in our test set is only 0.87. In
comparison, the best obtainable accuracy when resolving the queries individually each with
a different threshold is 0.97. This suggests that there may be potential benefits to localized
cleaning over its global counterpart in the offline setting.

8. Related Work
The entity resolution problem has been studied in many different areas under different
names — deduplication, record linkage, co-reference resolution, reference reconciliation,
object consolidation, etc. Much of the work has focused on traditional attribute-based
entity resolution. Extensive research has been done on defining approximate string similarity
measures (Monge & Elkan, 1996; Navarro, 2001; Bilenko et al., 2003; Chaudhuri, Ganjam,
Ganti, & Motwani, 2003) that may be used for unsupervised entity resolution. The other
approach uses adaptive supervised algorithms that learn similarity measures from labeled
data (Tejada, Knoblock, & Minton, 2001; Bilenko & Mooney, 2003).
Resolving entities optimally is known to be computationally hard even when only attributes are considered (Cohen, Kautz, & McAllester, 2000). Therefore, efficiency has
received a lot of attention in attribute-based data cleaning. The goal essentially is to avoid
irrelevant and expensive attribute similarity computations using a ‘blocking’ approach without affecting accuracy significantly (Hernández & Stolfo, 1995; Monge & Elkan, 1997; McCallum et al., 2000). The merge/purge problem was posed by Hernández and Stolfo (1995)
with efficient schemes to retrieve potential duplicates without resorting to quadratic complexity. They use a ‘sorted neighborhood method’ where an appropriate key is chosen for
matching. Records are then sorted or grouped according to that key and potential matches
are identified using a sliding window technique. However, some keys may be badly distorted
so that their matches cannot be spanned by the window and such cases will not be retrieved.
649

Bhattacharya & Getoor

The solution they propose is a multi-pass method over different keys and then merging the
results using transitive closure. Monge and Elkan (1997) combine the union find algorithm
with a priority queue look-up to find connected components in an undirected graph. McCallum et al. (2000) propose the use of canopies to first partition the data into overlapping
clusters using a cheap distance metric and then use a more accurate and expensive distance
metric for those data pairs that lie within the same canopy. Chaudhuri et al. (2003) use
an error tolerant index for data warehousing applications for probabilistically looking up
a small set of candidate reference tuples for matching against an incoming tuple. This is
considered ‘probabilistically safe’ since the closest tuples in the database will be retrieved
with high probability. This is also efficient since only a small number of matches needs to
be performed. Swoosh (Benjelloun, Garcia-Molina, Su, & Widom, 2005) has recently been
proposed as a generic entity resolution framework that considers resolving and merging
duplicates as a database operator and the goal is to minimize the number of record-level
and feature-level operations. An alternative approach is to reduce the complexity of individual similarity computations. Gravano, Ipeirotis, Koudas, and Srivastava (2003) propose
a sampling approach to quickly compute cosine similarity between tuples for fast text-joins
within an SQL framework. All of these approaches enable efficient data cleaning when only
attributes of references are considered.
Many recently proposed approaches take relations into account for data integration
(Ananthakrishna et al., 2002; Bhattacharya & Getoor, 2004, 2005; Kalashnikov et al., 2005;
Dong et al., 2005). Ananthakrishna et al. (2002) introduce relational deduplication in data
warehouse applications where there is a dimensional hierarchy over the relations. Kalashnikov et al. (2005) enhance attribute similarity between an ambiguous reference and the
many entity choices for it with relationship analysis between the entities, like affiliation and
co-authorship. In earlier work, we have proposed different measures for relational similarity and a relational clustering algorithm for collective entity resolution using relationships
(Bhattacharya & Getoor, 2004, 2007). Dong et al. (2005) collectively resolve entities of multiple types by propagating relational evidences in a dependency graph, and demonstrate the
benefits of collective resolution in real datasets. Long, Zhang, Wú, and Yu (2006) have proposed a model for general multi-type relational clustering, though it has not been applied
specifically for entity resolution. They perform collective factorization over related matrices
using spectral methods to identify the cluster space that minimizes distortion over relationships and individual features at the same time. All of these approaches that make use of
relationships either for entity matching (where the domain entities are known) or entity
resolution (where the underlying entities also need to be discovered) have been shown to
increase performance significantly over the attribute-based solutions for the same problems.
However, the price they pay is in terms of computational complexity that increases due
to a couple of different reasons. Firstly, the number of potential matches increases when
relationships are considered and individual similarity computations also become more expensive. Secondly, collective resolution using relationships necessitates iterative solutions
that make multiple passes over the data. While some of these approaches have still been
shown to be scalable in practice, they cannot be employed for query-time cleaning in a
straight-forward manner.
The idea of multi-relational clustering also comes up in the Inductive Logic Programming
(ILP) literature. Emde and Wettschereck (1996) have used multi-relational similarity for
650

Query-time Entity Resolution

instance-based classification of representations in first order logic. They define the similarity
of two objects, e.g., of two people, as a combination of the similarity of their attribute
values, such as their age, weight, etc., and the similarity of the objects that they are
related to, such as the companies they work for. This is similar to the naive relational
similarity that we discussed earlier, except that the similarity of the connected objects is
also defined recursively in terms of their connected objects. Kirsten and Wrobel (1998)
have used this recursive relational similarity measure for agglomerative clustering of first
order representations. While recursive comparison of neighbors is shown to be effective in
terms of accuracy of results, the computational challenge is again a major drawback.
Probabilistic approaches that cast entity resolution as a classification problem have been
extensively studied. The groundwork was done by Fellegi and Sunter (1969). Others (Winkler, 2002; Ravikumar & Cohen, 2004) have more recently built upon this work. Adaptive
machine learning approaches have been proposed for data integration (Sarawagi & Bhamidipaty, 2002; Tejada et al., 2001), where active learning requires the user to label informative
examples. Probabilistic models that use relationships for collective entity resolution have
been applied to named entity recognition and citation matching (Pasula et al., 2003; McCallum & Wellner, 2004; Li et al., 2005; Singla & Domingos, 2004). These probabilistic
approaches are superior to similarity-based clustering algorithms in that they associate a
degree of confidence with every decision, and learned models provide valuable insight into
the domain. However, probabilistic inference for collective entity resolution is not known
to be scalable in practice, particularly when relationships are also considered. These approaches have mostly been shown to work for small datasets, and are significantly slower
than their clustering counterparts.
Little work has been done in the literature for query-centric cleaning or relational approaches for answering queries, where execution time is as important as accuracy of resolution. Approaches have been proposed for localized evaluation of Bayesian networks (Draper
& Hanks, 1994), but not for clustering problems. Recently, Chandel, Nagesh, and Sarawagi
(2006) have addressed efficiency issues in computing top-k entity matches against a dictionary in the context of entity extraction from unstructured documents. They process top-k
searches in batches where speed-up is achieved by sharing computation between different
searches. Fuxman, Fazli, and Miller (2005) motivate the problem of answering queries over
databases that violate integrity constraints and address scalability issues in resolving inconsistencies dynamically at query-time. However, the relational aspect of the problem,
which is the major scalability issue that we address, does not come up in any of these settings. In our earlier work on relational clustering(Bhattacharya & Getoor, 2007), we used
the idea of ‘relevant references’ for experimental evaluation on the BioBase dataset. As
we have also discussed here, this dataset has entity labels only for the 100 most frequent
names. Therefore, instead of running collective resolution over the entire BioBase dataset,
we evaluated the 100 names separately, using only the ‘relevant references’ in each case.
The relevant references were the ones directly connected to references having the names of
interest. The concept of focused cleaning, the performance analysis of relational clustering, the expand-resolve strategy and, most importantly, the idea of adaptive expansion for
query-time resolution were not addressed in that paper.
One of the first papers to make use of relational features for classification problem was
by Chakrabarti, Dom, and Indyk (1998). They showed that for the problem of classifying
651

Bhattacharya & Getoor

hyper-linked documents, naive use of relationships can hurt performance. Specifically, if
key terms from neighboring documents are thrown into the document whose topic is to be
classified, classification accuracy degrades instead of improving. The parallel in our scenario
of clustering using relationships is that the naive relational model (NR) may perform worse
than the attribute model (A) in the presence of highly ambiguous relationships. Chakrabarti
et al. (1998) showed that relationships can however be used for improved classification
when the topic labels of the neighboring documents are used as evidence instead of naively
considering the terms that they contain. In our earlier work (Bhattacharya & Getoor, 2004,
2007), we have shown similar results for collective clustering using relationships, where the
cluster labels of neighboring labels lead to improved clustering performance compared to
naive relational and attribute-based clustering. The interesting result that we have shown
in this paper both in theory and empirically is that even collective use of relationships
can hurt clustering accuracy compared to attribute-based clustering. This happens when
relationships between references are dense and ambiguous, and errors that propagate over
relationships exceed the identifying evidence that they provide.

9. Conclusions
In this paper, we have motivated the problem of query-time entity resolution for accessing
unresolved third-party databases. For answering entity resolution queries, we have addressed the challenges of using collective approaches, which have recently shown significant
performance improvements over traditional baselines in the offline setting. The first hurdle
for collective resolution arises from the interdependent nature of its resolution decisions. We
first formally analyzed the recursive nature of this dependency, and showed that the precision and recall for individual entities grow in a geometric progression as increasing levels of
neighbors are considered and collectively resolved. We then proposed a two-stage ‘expand
and resolve’ strategy for answering queries based on this analysis, using two novel expansion
operators. We showed using our analysis that it is sufficient to consider neighbors up to small
expansion depths, since resolution accuracy for the query converges quickly with increasing
expansion level. The second challenge for answering queries is that the computation has to
be quick. To achieve this, we improved on our unconstrained expansion strategy to propose
an adaptive algorithm, which dramatically reduces the size of the relevant references —
and, as a result, the processing time — by identifying the most informative references for
any query. We demonstrated using experiments on two real datasets that our strategies
enable collective resolution at query-time, without compromising on accuracy. We additionally performed various experiments on synthetically generated data over a wide range
of settings to verify the trends predicted by our analysis. In summary, we have addressed
and motivated a critical data integration and retrieval problem, proposed algorithms for
solving it accurately and efficiently, provided a theoretical analysis to validate our approach
and explain why it works, and, finally, shown experimental results on multiple real-world
and synthetically generated datasets to demonstrate that it works extremely well in practice. While we have presented results for bibliographic data, the techniques are applicable
in other relational domains.
While we have shown the dramatic reduction in query processing time that comes with
adaptive expansion, more research is necessary to be able to answer entity resolution queries
652

Query-time Entity Resolution

on the order of milli-seconds, as may be demanded in many scenarios. Interesting directions
of future research include exploring stronger coupling between the extraction and resolution
phases of query processing, where the expansion happens “on-demand” only when the
resolution process finds the residual ambiguity to be high and requires additional evidence
for taking further decisions. This would directly address the problem of determining the
expansion depth. While we have reported some preliminary experiments in this paper, more
work needs to be done on adaptive depth determination depending on ambiguity. In the
same context, we may imagine “soft” thresholds for adaptive expansion, where the expansion
operator automatically determines the number of hyper-edges or names to be expanded so
that the residual ambiguity falls below some specified level. Other interesting extensions
include caching of intermediate resolutions, where the related resolutions performed for any
query are stored and retrieved as and when required for answering future queries.

Acknowledgments
We wish to thank our anonymous reviewers for their constructive suggestions which greatly
improved this paper. This work was supported by the National Science Foundation, NSF
#0423845 and NSF #0438866, with additional support from the ITIC KDD program.

Appendix A
Synthetic Data Generator
We have designed a synthetic data generator that allows us to control different structural
and attribute-based characteristics of the data(Bhattacharya & Getoor, 2007). Here we
present an overview of the generation algorithm.
The generation process has two stages. In the first stage, we create the collaboration
graph among the underlying entities and the entity attributes. In the second, we generate
observed co-occurrence relations from this collaboration graph. A high level description
of the generative process in shown in Figure 10. Next, we describe the two stages of the
generation process in greater detail.
The graph creation stage, in turn, has two sub-stages. First, we create the domain
entities and their attributes and then add relationships between them. For creating entities,
we control the number of entities and the ambiguity of their attributes. We create N entities
and their attributes one after another. For simplicity and without losing generality, each
entity e has a single floating point attribute e.x, instead of a character string. A parameter
pa controls the ambiguity of the entity attributes; with probability pa the attribute of a new
entity is chosen from values that are already in use by existing entities. Then M binary
relationships are added between the created entities. As with the attributes, there is a
parameter controlling the ambiguity of the relationships, as defined in Section 4. For each
binary relationship (ei , ej ), first ei is chosen randomly and then ej is sampled so that (ei , ej )
is an ambiguous relationship with probability pR
a.
Before describing the process of generating co-occurrence relationships from the graph,
let us consider in a little more detail the issue of attribute ambiguity. What finally needs
to be controlled is the ambiguity of the reference attributes. While these depend on the
entity attributes, they are not completely determined by entities. Taking the example
653

Bhattacharya & Getoor

1.
2.
3.
4.
5.
6.
7.

Creation Stage
Repeat N times
Create random attribute x with ambiguity pa
Create entity e with attribute x
Repeat M times
Choose entity ei randomly
Choose entity ej with prob pR
a of an ambiguous relationship (ei , ej )
Set ei = N br(ej ) and ej = N br(ei )

8.
9.
10.
11.
12.
13.
14.
15.
16.

Generation Stage
Repeat R times
Randomly choose entity e
Generate reference r using N (e.x, 1)
Initialize hyper-edge h = hri
Repeat with probability pc
Randomly choose ej from N br(e) without replacement
Generate reference rj using N (ej .x, 1)
Add rj hyper-edge h
Output hyper-edge h

Figure 10: High-level description of synthetic data generation algorithm

of names, two people who have names ‘John Michael Smyth’ and ‘James Daniel Smith’
can still be ambiguous in terms of their observed names in the data depending on the
generation process of observed names. In other words, attribute ambiguity of the references
depends both on the separation between entity attributes and the dispersion created by the
generation process. We make the assumption that for an entity e with attribute e.x, its
references are generated from a Gaussian distribution with mean x and variance 1.0. So,
with very high probability, any reference attribute generated from e.x will be in the range
[e.x − 3, e.x + 3]. So this range in the attribute domain is considered to be ‘occupied’ by
entity e. Any entity has an ambiguous attribute if its occupied range intersects with that
of another entity.
Now we come to the generation of co-occurrence relationships from the entity collaboration graph. In this stage, R co-occurrence relationships or hyper-edges are generated, each
with its own references. For each hyper-edge hri , ri1 , . . . , rik i, two aspects need to be controlled — how many references and which references should be included in this hyper-edge.
This is done as follows. First, we sample an entity ei which serves the initiator entity for
this hyper-edge. Then other entities eij for this hyper-edge are repeatedly sampled (without replacement) from the neighbors of the initiator entity ei . The size of the hyper-edge is
determined using a parameter pc . The sampling step for a hyper-edge is terminated with
probability pc after each selection eij . The process is also terminated when the neighbors
of the initiator entity are exhausted. Finally, references rij need to be generated from each
of the selected entities eij . This is done for each entity e by sampling from its Gaussian
distribution N (e.x, 1).
654

Query-time Entity Resolution

References
Ananthakrishna, R., Chaudhuri, S., & Ganti, V. (2002). Eliminating fuzzy duplicates in
data warehouses. In The International Conference on Very Large Databases (VLDB),
Hong Kong, China.
Benjelloun, O., Garcia-Molina, H., Su, Q., & Widom, J. (2005). Swoosh: A generic approach
to entity resolution. Tech. rep., Stanford University.
Bhattacharya, I., & Getoor, L. (2004). Iterative record linkage for cleaning and integration. In The SIGMOD Workshop on Research Issues on Data Mining and Knowledge
Discovery (DMKD), Paris, France.
Bhattacharya, I., & Getoor, L. (2005). Relational clustering for multi-type entity resolution. In The ACM SIGKDD Workshop on Multi Relational Data Mining (MRDM),
Chicago, IL, USA.
Bhattacharya, I., & Getoor, L. (2006). Mining Graph Data (L. Holder and D. Cook, eds.),
chap. Entity Resolution in Graphs. Wiley.
Bhattacharya, I., & Getoor, L. (2007). Collective entity resolution in relational data. ACM
Transactions on Knowledge Discovery from Data (TKDD), 1 (1).
Bhattacharya, I., Licamele, L., & Getoor, L. (2006). Query-time entity resolution. In The
ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),
Philadelphia, PA, USA.
Bilenko, M., & Mooney, R. (2003). Adaptive duplicate detection using learnable string
similarity measures. In The ACM International Conference on Knowledge Discovery
and Data Mining (SIGKDD), Washington DC, USA.
Bilenko, M., Mooney, R., Cohen, W., Ravikumar, P., & Fienberg, S. (2003). Adaptive name
matching in information integration.. IEEE Intelligent Systems, 18 (5), 16–23.
Chakrabarti, S., Dom, B., & Indyk, P. (1998). Enhanced hypertext categorization using
hyperlinks. In Proceedings of the ACM International Conference on Management of
Data (SIGMOD).
Chandel, A., Nagesh, P. C., & Sarawagi, S. (2006). Efficient batch top-k search for
dictionary-based entity recognition. In The IEEE International Conference on Data
Engineering (ICDE), Washington, DC, USA.
Chaudhuri, S., Ganjam, K., Ganti, V., & Motwani, R. (2003). Robust and efficient fuzzy
match for online data cleaning. In The ACM International Conference on Management
of Data (SIGMOD), San Diego, CA, USA.
Cohen, W., Kautz, H., & McAllester, D. (2000). Hardening soft information sources. In The
ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),
Boston, MA, USA.
Dong, X., Halevy, A., & Madhavan, J. (2005). Reference reconciliation in complex information spaces. In The ACM International Conference on Management of Data
(SIGMOD), Baltimore, MD, USA.
655

Bhattacharya & Getoor

Draper, D., & Hanks, S. (1994). Localized partial evaluation of belief networks. In The
Annual Conference on Uncertainty in Artificial Intelligence (UAI), Seattle, WA, USA.
Emde, W., & Wettschereck, D. (1996). Relational instance based learning. In Proceedings
of the International Conference on Machine Learning (ICML).
Fellegi, I., & Sunter, A. (1969). A theory for record linkage. Journal of the American
Statistical Association, 64, 1183–1210.
Fuxman, A., Fazli, E., & Miller, R. (2005). Conquer: Efficient management of inconsistent
databases. In The ACM International Conference on Management of Data (SIGMOD), Baltimore, MD, USA.
Gravano, L., Ipeirotis, P., Koudas, N., & Srivastava, D. (2003). Text joins for data cleansing and integration in an rdbms. In The IEEE International Conference on Data
Engineering (ICDE), Bangalore, India.
Hernández, M., & Stolfo, S. (1995). The merge/purge problem for large databases. In The
ACM International Conference on Management of Data (SIGMOD), San Jose, CA,
USA.
Kalashnikov, D., Mehrotra, S., & Chen, Z. (2005). Exploiting relationships for domainindependent data cleaning. In SIAM International Conference on Data Mining (SIAM
SDM), Newport Beach, CA, USA.
Kirsten, M., & Wrobel, S. (1998). Relational distance-based clustering. In Proceedings of
the International Workshop on Inductive Logic Programming (ILP).
Li, X., Morie, P., & Roth, D. (2005). Semantic integration in text: From ambiguous names
to identifiable entities. AI Magazine. Special Issue on Semantic Integration, 26 (1).
Long, B., Zhang, Z. M., Wú, X., & Yu, P. S. (2006). Spectral clustering for multi-type relational data. In Proceedings of the 23rd International Conference on Machine Learning
(ICML).
McCallum, A., Nigam, K., & Ungar, L. (2000). Efficient clustering of high-dimensional data
sets with application to reference matching. In The ACM International Conference
On Knowledge Discovery and Data Mining (SIGKDD), Boston, MA, USA.
McCallum, A., & Wellner, B. (2004). Conditional models of identity uncertainty with application to noun coreference. In Advances In Neural Information Processing Systems
(NIPS), Vancouver, BC, Canada.
Monge, A., & Elkan, C. (1996). The field matching problem: Algorithms and applications.
In The ACM International Conference on Knowledge Discovery and Data Mining
(SIGKDD), Portland, OR, USA.
Monge, A., & Elkan, C. (1997). An efficient domain-independent algorithm for detecting
approximately duplicate database records. In The SIGMOD Workshop on Research
Issues on Data Mining and Knowledge Discovery (DMKD), Tuscon, AZ, USA.
Navarro, G. (2001). A guided tour to approximate string matching. ACM Computing
Surveys, 33 (1), 31–88.
656

Query-time Entity Resolution

Pasula, H., Marthi, B., Milch, B., Russell, S., & Shpitser, I. (2003). Identity uncertainty and
citation matching. In Advances in Neural Information Processing Systems (NIPS),
Vancouver, BC, Canada.
Ravikumar, P., & Cohen, W. (2004). A hierarchical graphical model for record linkage.
In The Conference on Uncertainty in Artificial Intelligence (UAI), Banff, Alberta,
Canada.
Sarawagi, S., & Bhamidipaty, A. (2002). Interactive deduplication using active learning.
In Proceedings of the Eighth ACM International Conference on Knowledge Discovery
and Data Mining (SIGKDD), Edmonton, Alberta, Canada.
Singla, P., & Domingos, P. (2004). Multi-relational record linkage. In The SIGKDD Workshop on Multi-Relational Data Mining (MRDM), Seattle, WA, USA.
Tejada, S., Knoblock, C., & Minton, S. (2001). Learning object identification rules for
information integration. Information Systems Journal, 26 (8), 635–656.
Winkler, W. (2002). Methods for record linkage and Bayesian networks. Tech. rep., Statistical Research Division, U.S. Census Bureau, Washington, DC.

657

Journal of Artificial Intelligence Research 30 (2007) 321–359

Submitted 11/06; published 10/07

New Inference Rules for Max-SAT
Chu Min Li

chu-min.li@u-picardie.fr

LaRIA, Université de Picardie Jules Verne
33 Rue St. Leu, 80039 Amiens Cedex 01, France

Felip Manyà

felip@iiia.csic.es

IIIA, Artificial Intelligence Research Institute
CSIC, Spanish National Research Council
Campus UAB, 08193 Bellaterra, Spain

Jordi Planes

jplanes@diei.udl.es

Computer Science Department, Universitat de Lleida
Jaume II, 69, 25001 Lleida, Spain

Abstract
Exact Max-SAT solvers, compared with SAT solvers, apply little inference at each
node of the proof tree. Commonly used SAT inference rules like unit propagation produce
a simplified formula that preserves satisfiability but, unfortunately, solving the Max-SAT
problem for the simplified formula is not equivalent to solving it for the original formula.
In this paper, we define a number of original inference rules that, besides being applied
efficiently, transform Max-SAT instances into equivalent Max-SAT instances which are
easier to solve. The soundness of the rules, that can be seen as refinements of unit resolution
adapted to Max-SAT, are proved in a novel and simple way via an integer programming
transformation. With the aim of finding out how powerful the inference rules are in practice,
we have developed a new Max-SAT solver, called MaxSatz, which incorporates those rules,
and performed an experimental investigation. The results provide empirical evidence that
MaxSatz is very competitive, at least, on random Max-2SAT, random Max-3SAT, MaxCut, and Graph 3-coloring instances, as well as on the benchmarks from the Max-SAT
Evaluation 2006.

1. Introduction
In recent years there has been a growing interest in developing fast exact Max-SAT
solvers (Alber, Gramm, & Niedermeier, 2001; Alsinet, Manyà, & Planes, 2003b, 2005;
de Givry, Larrosa, Meseguer, & Schiex, 2003; Li, Manyà, & Planes, 2005; Xing & Zhang,
2004; Zhang, Shen, & Manyà, 2003) due to their potential to solve over-constrained NPhard problems encoded in the formalism of Boolean CNF formulas. Nowadays, Max-SAT
solvers are able to solve a lot of instances that are beyond the reach of the solvers developed
just five years ago. Nevertheless, there is yet a considerable gap between the difficulty of the
instances solved with current SAT solvers and the instances solved with the best performing
Max-SAT solvers.
The motivation behind our work is to bridge that gap between complete SAT solvers
and exact Max-SAT solvers by investigating how the technology previously developed for
SAT (Goldberg & Novikov, 2001; Li, 1999; Marques-Silva & Sakallah, 1999; Zhang, 1997;
Zhang, Madigan, Moskewicz, & Malik, 2001) can be extended and incorporated into Maxc
2007
AI Access Foundation. All rights reserved.

Li, Manyà & Planes

SAT. More precisely, we focus the attention on branch and bound Max-SAT solvers based on
the Davis-Putnam-Logemann-Loveland (DPLL) procedure (Davis, Logemann, & Loveland,
1962; Davis & Putnam, 1960).
One of the main differences between SAT solvers and Max-SAT solvers is that the former
make an intensive use of unit propagation at each node of the proof tree. Unit propagation,
which is a highly powerful inference rule, transforms a SAT instance φ into a satisfiability
equivalent SAT instance φ′ which is easier to solve. Unfortunately, solving the Max-SAT
problem for φ is, in general, not equivalent to solving it for φ′ ; i.e., the number of unsatisfied
clauses in φ and φ′ is not the same for every truth assignment. For example, if we apply
unit propagation to the CNF formula φ = {x1 , x̄1 ∨ x2 , x̄1 ∨ ¬x2 , x̄1 ∨ x3 , x̄1 ∨ ¬x3 }, we
obtain φ′ = {2, 2}, but φ and φ′ are not equivalent because any interpretation satisfying
¬x1 unsatisfies one clause of φ and two clauses of φ′ . Therefore, if we want to compute an
optimal solution, we cannot apply unit propagation as in SAT solvers.
We proposed in a previous work (Li et al., 2005) to use unit propagation to compute
lower bounds in branch and bound Max-SAT solvers instead of using unit propagation to
simplify CNF formulas. In our approach, we detect disjoint inconsistent subsets of clauses
via unit propagation. It turns out that the number of disjoint inconsistent subsets detected
is an underestimation of the number of clauses that will become unsatisfied when the current
partial assignment is extended to a complete assignment. That underestimation plus the
number of clauses unsatisfied by the current partial assignment provides a good performing
lower bound, which captures the lower bounds based on inconsistency counts that most of
the state-of-the-art Max-SAT solvers implement (Alsinet, Manyà, & Planes, 2003a; Alsinet
et al., 2003b; Borchers & Furman, 1999; Wallace & Freuder, 1996; Zhang et al., 2003), as
well as other improved lower bounds (Alsinet, Manyà, & Planes, 2004; Alsinet et al., 2005;
Xing & Zhang, 2004, 2005).
On the one hand, the number of disjoint inconsistent subsets detected is just a conservative underestimation for the lower bound, since every inconsistent subset φ increases the
lower bound by one independently of the number of clauses of φ unsatisfied by an optimal
assignment. However, an optimal assignment can violate more than one clause of an inconsistent subset. Therefore, we should be able to improve the lower bound based on counting
the number of disjoint inconsistent subsets of clauses.
On the other hand, despite the fact that good quality lower bounds prune large parts of
the search space and accelerate dramatically the search for an optimal solution, whenever
the lower bound does not reach the best solution found so far (upper bound), the solver
continues exploring the search space below the current node. During that search, solvers
often redetect the same inconsistencies when computing the lower bound at different nodes.
Basically, the problem with lower bound computation methods is that they do not simplify
the CNF formula in such a way that the unsatisfied clauses become explicit. Lower bounds
are just a pruning technique.
To overcome the above two problems, we define a set of sound inference rules that
transform a Max-SAT instance φ into a Max-SAT instance φ′ which is easier to solve. In
Max-SAT, an inference rule is sound whenever φ and φ’ are equivalent.
Let us see an example of inference rule: Given a Max-SAT instance φ that contains
three clauses of the form l1 , l2 , ¯l1 ∨ ¯l2 , where l1 , l2 are literals, we replace φ with the CNF
322

New Inference Rules for Max-SAT

formula
φ′ = (φ − {l1 , l2 , ¯l1 ∨ ¯l2 }) ∪ {2, l1 ∨ l2 }.
Note that the rule detects a contradiction from l1 , l2 , ¯l1 ∨ ¯l2 and, therefore, replaces these
clauses with an empty clause. In addition, the rule adds the clause l1 ∨ l2 to ensure the
equivalence between φ and φ′ . For any assignment containing either l1 = 0, l2 = 1, or
l1 = 1, l2 = 0, or l1 = 1, l2 = 1, the number of unsatisfied clauses in {l1 , l2 , ¯l1 ∨ ¯l2 } is 1,
but for any assignment containing l1 = 0, l2 = 0, the number of unsatisfied clauses is 2.
Note that even when any assignment containing l1 = 0, l2 = 0 is not the best assignment
for the subset {l1 , l2 , ¯l1 ∨ ¯l2 }, it can be the best for the whole formula. By adding l1 ∨ l2 ,
the rule ensures that the number of unsatisfied clauses in φ and φ′ is also the same when
l1 = 0, l2 = 0.
That inference rule adds the new clause l1 ∨ l2 , which may contribute to another contradiction detectable via unit propagation. In this case, the rule allows to increase the
lower bound by 2 instead of 1. Moreover, the rule makes explicit a contradiction among
l1 , l2 , ¯l1 ∨ ¯l2 , so that the contradiction does not need to be redetected below the current
node.
Some of the inference rules defined in the paper are already known in the literature (Bansal & Raman, 1999; Niedermeier & Rossmanith, 2000), others are original for
Max-SAT. The new rules were inspired by different unit resolution refinements applied in
SAT, and were selected because they could be applied in a natural and efficient way. In a
sense, we can summarize our work telling that we have defined the Max-SAT counterpart
of SAT unit propagation.
With the aim of finding out how powerful the inference rules are in practice, we have
designed and implemented a new Max-SAT solver, called MaxSatz, which incorporates those
rules, as well as the lower bound defined in a previous work (Li et al., 2005), and performed
an experimental investigation. The results provide empirical evidence that MaxSatz is very
competitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph
3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation 20061 .
The structure of the paper is as follows. In Section 2, we give some preliminary definitions. In Section 3, we describe a basic branch and bound Max-SAT solver. In Section 4, we
define the inference rules and prove their soundness in a novel and simple way via an integer
programming transformation. We also give examples to illustrate that the inference rules
may produce better quality lower bounds. In Section 5, we present the implementation of
the inference rules in MaxSatz. In Section 6, we describe the main features of MaxSatz. In
Section 7, we report on the experimental investigation. In Section 8, we present the related
work. In Section 9, we present the conclusions and future work.

2. Preliminaries
In propositional logic a variable xi may take values 0 (for false) or 1 (for true). A literal li
is a variable xi or its negation x̄i . A clause is a disjunction of literals, and a CNF formula
φ is a conjunction of clauses. The length of a clause is the number of its literals. The size
of φ, denoted by |φ|, is the sum of the length of all its clauses.
1. http://www.iiia.csic.es/˜maxsat06

323

Li, Manyà & Planes

An assignment of truth values to the propositional variables satisfies a literal xi if xi
takes the value 1 and satisfies a literal x̄i if xi takes the value 0, satisfies a clause if it
satisfies at least one literal of the clause, and satisfies a CNF formula if it satisfies all the
clauses of the formula. An empty clause, denoted by 2, contains no literals and cannot be
satisfied. An assignment for a CNF formula φ is complete if all the variables occurring in
φ have been assigned; otherwise, it is partial.
The Max-SAT problem for a CNF formula φ is the problem of finding an assignment
of values to propositional variables that minimizes the number of unsatisfied clauses (or
equivalently, that maximizes the number of satisfied clauses). Max-SAT is called MaxkSAT when all the clauses have k literals per clause. In the following, we represent a CNF
formula as a multiset of clauses, since duplicated clauses are allowed in a Max-SAT instance.
CNF formulas φ1 and φ2 are equivalent if φ1 and φ2 have the same number of unsatisfied
clauses for every complete assignment of φ1 and φ2 .

3. A Basic Max-SAT Solver
The space of all possible assignments for a CNF formula φ can be represented as a search
tree, where internal nodes represent partial assignments and leaf nodes represent complete
assignments. A basic branch and bound algorithm for Max-SAT explores the search tree in
a depth-first manner. At every node, the algorithm compares the number of clauses unsatisfied by the best complete assignment found so far —called upper bound (U B)— with the
number of clauses unsatisfied by the current partial assignment (#emptyClauses) plus an
underestimation of the minimum number of non-empty clauses that will become unsatisfied
if we extend the current partial assignment into a complete assignment (underestimation).
The sum #emptyClauses + underestimation is a lower bound (LB) of the minimum
number of clauses unsatisfied by any complete assignment extended from the current partial
assignment. Obviously, if LB ≥ U B, a better solution cannot be found from this point in
search. In that case, the algorithm prunes the subtree below the current node and backtracks
to a higher level in the search tree.
If LB < U B, the algorithm tries to find a possible better solution by extending the
current partial assignment by instantiating one more variable; which leads to the creation
of two branches from the current branch: the left branch corresponds to assigning the new
variable to false, and the right branch corresponds to assigning the new variable to true. In
that case, the formula associated with the left (right) branch is obtained from the formula
of the current node by deleting all the clauses containing the literal x̄ (x) and removing all
the occurrences of the literal x (x̄); i.e., the algorithm applies the one-literal rule.
The solution to Max-SAT is the value that U B takes after exploring the entire search
tree.
Figure 1 shows the pseudo-code of a basic solver for Max-SAT. We use the following
notations:
• simplifyFormula(φ) is a procedure that simplifies φ by applying sound inference rules.
• #emptyClauses(φ) is a function that returns the number of empty clauses in φ.
324

New Inference Rules for Max-SAT

Input: max-sat(φ, U B) : A CNF formula φ and an upper bound U B
1: φ ← simplifyFormula(φ);
2: if φ = ∅ or φ only contains empty clauses then
3:
return #emptyClauses(φ);
4: end if
5: LB ← #emptyClauses(φ) + underestimation(φ, U B);
6: if LB ≥ U B then
7:
return U B;
8: end if
9: x ← selectVariable(φ);
10: U B ← min(U B, max-sat(φx̄ , U B));
11: return min(U B, max-sat(φx , U B));
Output: The minimal number of unsatisfied clauses of φ
Figure 1: A basic branch and bound algorithm for Max-SAT
• LB is a lower bound of the minimum number of unsatisfied clauses in φ if the current
partial assignment is extended to a complete assignment. We assume that its initial
value is 0.
• underestimation(φ, U B) is a function that returns an underestimation of the minimum
number of non-empty clauses in φ that will become unsatisfied if the current partial
assignment is extended to a complete assignment.
• U B is an upper bound of the number of unsatisfied clauses in an optimal solution.
We assume that its initial value is the total number of clauses in the input formula.
• selectVariable(φ) is a function that returns a variable of φ following an heuristic.
• φx (φx̄ ) is the formula obtained by applying the one-literal rule to φ using the literal
x (x̄).
State-of-the-art Max-SAT solvers implement the basic algorithm augmented with powerful inference techniques, good quality lower bounds, clever variable selection heuristics,
and efficient data structures.
We have recently defined (Li et al., 2005) a lower bound computation method in which
the underestimation of the lower bound is the number of disjoint inconsistent subsets that
can be detected using unit propagation. The pseudo-code is shown in Figure 2.
Example 1 Let φ be the following CNF formula:
{x1 , x2 , x3 , x4 , x̄1 ∨ x̄2 ∨ x̄3 , x̄4 , x5 , x̄5 ∨ x̄2 , x̄5 ∨ x2 }.
With our approach we are able to establish that the number of disjoint inconsistent
subsets of clauses in φ is at least 3. Therefore, the underestimation of the lower bound is 3.
The steps performed are the following ones:
325

Li, Manyà & Planes

Input: underestimation(φ, U B) : A CNF formula φ and an upper bound U B
1: underestimation ← 0;
2: apply the one-literal rule to the unit clauses of φ (unit propagation) until an empty
clause is derived;
3: if no empty clause can be derived then
4:
return underestimation;
5: end if
6: φ ← φ without the clauses that have been used to derive the empty clause;
7: underestimation := underestimation + 1;
8: if underestimation+#emptyClauses(φ) ≥ U B then
9:
return underestimation;
10: end if
11: go to 2;
Output: the underestimation of the lower bound for φ
Figure 2: Computation of the underestimation using unit propagation
1. φ = {x4 , x̄4 , x5 , x̄5 ∨ x̄2 , x̄5 ∨ x2 }, the first inconsistent subset detected using unit
propagation is {x1 , x2 , x3 , x̄1 ∨ x̄2 ∨ x̄3 }, and underestimation = 1.
2. φ = {x5 , x̄5 ∨x̄2 , x̄5 ∨x2 }, the second inconsistent subset detected using unit propagation
is {x4 , x̄4 }, and underestimation = 2.
3. φ = ∅, the third inconsistent subset detected using unit propagation is {x5 , x̄5 ∨ x̄2 , x̄5 ∨
x2 }, and underestimation = 3. Since φ is empty, the algorithm stops.

4. Inference Rules
We define the set of inference rules considered in the paper. They were inspired by different
unit resolution refinements applied in SAT, and were selected because they could be applied
in a natural and efficient way. Some of them are already known in the literature (Bansal &
Raman, 1999; Niedermeier & Rossmanith, 2000), others are original for Max-SAT.
Before presenting the rules, we define an integer programming transformation of a CNF
formula used to establish the soundness of the rules. The method of proving soundness is
novel in Max-SAT, and provides clear and short proofs.
4.1 Integer Programming Transformation of a CNF Formula
Assume that φ = {c1 , ..., cm } is a CNF formula with m clauses over the variables x1 , ..., xn .
Let ci (1 ≤ i ≤ m) be xi1 ∨ ... ∨ xik ∨ x̄ik+1 ∨ ... ∨ x̄ik+r . Note that we put all positive literals
in ci before the negative ones.
We consider all the variables in ci as integer variables taking values 0 or 1, and define
the integer transformation of ci as
Ei (xi1 , ..., xik , xik+1 , ..., xik+r ) = (1 − xi1 )...(1 − xik )xik+1 ...xik+r
326

New Inference Rules for Max-SAT

Obviously, Ei has value 0 iff at least one of the variables xij ’s (1 ≤ j ≤ k) is instantiated
to 1 or at least one of the variables xis ’s (k + 1 ≤ s ≤ k + r) is instantiated to 0. In other
words, Ei =0 iff ci is satisfied. Otherwise Ei =1.
A literal l corresponds to an integer denoted by l itself for our convenience. The intention
of the correspondence is that the literal l is satisfied if the integer l is 1 and is unsatisfied if
the integer l is 0. So if l is a positive literal x, the corresponding integer l is x, ¯l is 1-x=1-l,
and if l is a negative literal x̄, l is 1-x and ¯l is x=1-(1-x)=1-l. Consequently, ¯l=1-l in any
case.
We now generically write ci as l1 ∨ l2 ∨ ... ∨ lk+r . Its integer programming transformation
is
Ei = (1 − l1 )(1 − l2 )...(1 − lk+r ).
The integer programming transformation of a CNF formula φ = {c1 , ..., cm } over the
variables x1 , ..., xn is defined as
E(x1 , ..., xn ) =

m
X

Ei

(1)

i=1

That integer programming transformation was used (Huang & Jin, 1997; Li & Huang,
2005) to design a local search procedure, and is called pseudo-Boolean formulation by Boros
and Hammer (2002). Here, we extend it to empty clauses: if ci is empty, then Ei =1.
Given an assignment A over the variables x1 , ..., xn , the value of E is the number of
unsatisfied clauses in φ. If A satisfies all clauses in φ, then E = 0. Obviously, the minimum
number of unsatisfied clauses of φ is the minimum value of E.
Let φ1 and φ2 be two CNF formulas, and let E1 and E2 be their integer programming
transformations. It is clear that φ1 and φ2 are equivalent if, and only if, E1 =E2 for every
complete assignment for φ1 and φ2 .
4.2 Inference Rules
We next define the inference rules and prove their soundness using the previous integer
programming transformation. In the rest of the section, φ1 , φ2 and φ′ denote CNF formulas,
and E1 , E2 , and E ′ their integer programming transformations. To prove that φ1 and φ2 are
equivalent, we prove that E1 = E2 .
Rule 1 (Bansal & Raman, 1999) If φ1 ={l1 ∨ l2 ∨ ... ∨ lk , ¯l1 ∨ l2 ∨ ... ∨ lk } ∪ φ′ , then
φ2 ={l2 ∨ ... ∨ lk } ∪ φ′ is equivalent to φ1 .
Proof 1
E1 = (1 − l1 )(1 − l2 )...(1 − lk ) + l1 (1 − l2 )...(1 − lk ) + E ′
= (1 − l2 )...(1 − lk ) + E ′
= E2



General case resolution does not work in Max-SAT (Bansal & Raman, 1999). Rule 1
establishes that resolution works when two clauses give a strictly shorter resolvent.
327

Li, Manyà & Planes

Rule 1 is known in the literature as replacement of almost common clauses. We pay
special attention to the case k=2, where the resolvent is a unit clause, and to the case k=1,
where the resolvent is the empty clause. We describe this latter case in the following rule:
Rule 2 (Niedermeier & Rossmanith, 2000) If φ1 ={l, ¯l}∪φ′ , then φ2 ={2}∪φ′ is equivalent
to φ1 .
Proof 2 E1 =1-l+ l+E ′ =1+ E ′ =E2



Rule 2, which is known as complementary unit clause rule, can be used to replace two
complementary unit clauses with an empty clause. The new empty clause contributes to
the lower bounds of the search space below the current node by incrementing the number
of unsatisfied clauses, but not by incrementing the underestimation, which means that this
contradiction does not have to be redetected again. In practice, that simple rule gives rise
to considerable gains.
The following rule is a more complicated case:
Rule 3 If φ1 ={l1 , ¯l1 ∨ ¯l2 , l2 } ∪ φ′ , then φ2 ={2, l1 ∨ l2 } ∪ φ′ is equivalent to φ1 .
Proof 3
E1 = 1 − l1 + l1 l2 + 1 − l2 + E ′
= 1 + 1 − l1 + l2 (l1 − 1) + E ′
= 1 + 1 − l1 − l2 (1 − l1 ) + E ′
= 1 + (1 − l1 )(1 − l2 ) + E ′
= E2



Rule 3 replaces three clauses with an empty clause, and adds a new binary clause to
keep the equivalence between φ1 and φ2 .
Pattern φ1 was considered to compute underestimations by Alsinet et al. (2004) and Shen
and Zhang (2004); and is also captured by our method of computing underestimations based
on unit propagation (Li et al., 2005). Larrosa and Heras mentioned (2005) that existential
directional arc consistency (de Givry, Zytnicki, Heras, & Larrosa, 2005) can capture this
rule. Note that underestimation computation methods by Alsinet et al. and Shen and
Zhang do not add any additional clause as in our approach, they just detect contradictions.
Let us define a rule that generalizes Rule 2 and Rule 3. Before presenting the rule, we
define a lemma needed to prove its soundness.
Lemma 1 If φ1 ={l1 , ¯l1 ∨ l2 } ∪ φ′ and φ2 ={l2 , ¯l2 ∨ l1 } ∪ φ′ , then φ1 and φ2 are equivalent.
Proof 4
E1 = 1 − l1 + l1 (1 − l2 ) + E ′
= 1 − l1 + l1 − l1 l2 + E ′
= 1 − l2 + l2 − l1 l2 + E ′
= 1 − l2 + (1 − l1 )l2 + E ′
= E2


328

New Inference Rules for Max-SAT

Rule 4 If φ1 ={l1 , ¯l1 ∨l2 , ¯l2 ∨l3 , ..., ¯lk ∨lk+1 , ¯lk+1 }∪φ′ , then φ2 ={2, l1 ∨ ¯l2 , l2 ∨ ¯l3 , ..., lk ∨
¯lk+1 } ∪ φ′ is equivalent to φ1 .
Proof 5 We prove the soundness of the rule by induction on k. When k=1, φ1 = {l1 , ¯l1 ∨
l2 , ¯l2 } ∪ φ′ . By applying Rule 3, we get {2, l1 ∨ ¯l2 } ∪ φ′ , which is φ2 when k = 1. Therefore,
φ1 and φ2 are equivalent.
Assume that Rule 4 is sound for k = n. Let us prove that it is sound for k = n + 1. In
that case:
φ1 = {l1 , ¯l1 ∨ l2 , ¯l2 ∨ l3 , ..., ¯ln ∨ ln+1 , ¯ln+1 ∨ ln+2 , ¯ln+2 } ∪ φ′ .
By applying Lemma 1 to the last two clauses of φ1 (before φ′ ), we get
{l1 , ¯l1 ∨ l2 , ¯l2 ∨ l3 , ..., ¯ln ∨ ln+1 , ¯ln+1 , ln+1 ∨ ¯ln+2 } ∪ φ′ .
By applying the induction hypothesis to the first n + 1 clauses of the previous CNF formula,
we get
{2, l1 ∨ ¯l2 , l2 ∨ ¯l3 , ..., ln ∨ ¯ln+1 , ln+1 ∨ ¯ln+2 } ∪ φ′ ,
which is φ2 when k = n + 1. Therefore, φ1 and φ2 are equivalent and the rule is sound.

Rule 4 is an original inference rule. It captures linear unit resolution refutations in
which clauses and resolvents are used exactly once. The rule simply adds an empty clause,
eliminates two unit clauses and the binary clauses used in the refutation, and adds new
binary clauses that are obtained by negating the literals of the eliminated binary clauses.
So, all the operations involved can be performed efficiently.
Rule 3 and Rule 4 make explicit a contradiction, which does not need to be redetected in
the current subtree. So, the lower bound computation becomes more incremental. Moreover,
the binary clauses added by Rule 3 and Rule 4 may contribute to compute better quality
lower bounds either by acting as premises of an inference rule or by being part of an
inconsistent subset of clauses, as is illustrated in the following example.
Example 2 Let φ={x1 , x̄1 ∨ x̄2 , x3 , x̄3 ∨ x2 , x4 , x̄1 ∨ x̄4 , x̄3 ∨ x̄4 }. Depending on the ordering
in which unit clauses are propagated, unit propagation detects one of the following three
inconsistent subsets of clauses: {x1 , x̄1 ∨ x̄2 , x3 , x̄3 ∨ x2 }, {x1 , x4 , x̄1 ∨ x̄4 }, or {x3 , x4 , x̄3 ∨
x̄4 }. Once an inconsistent subset is detected and removed, the remaining set of clauses is
satisfiable. Without applying Rule 3 and Rule 4, the lower bound computed is 1, because the
underestimation computed using unit propagation is 1.
Note that Rule 4 can be applied to the first inconsistent subset {x1 , x̄1 ∨ x̄2 , x3 , x̄3 ∨ x2 }.
If Rule 4 is applied, a contradiction is made explicit and the clauses x1 ∨ x2 and x3 ∨ x̄2 are
added. So, φ becomes {2, x1 ∨ x2 , x3 ∨ x̄2 , x4 , x̄1 ∨ x̄4 , x̄3 ∨ x̄4 }. It turns out that φ − {2}
is an inconsistent set of clauses detectable by unit propagation. Therefore, the lower bound
computed is 2.
If the inconsistent subset {x1 , x4 , x̄1 ∨ x̄4 } is detected, Rule 3 can be applied. Then, a
contradiction is made explicit and the clause x1 ∨x4 is added. So, φ becomes {2, x1 ∨x4 , x̄1 ∨
x̄2 , x3 , x̄3 ∨ x2 , x̄3 ∨ x̄4 }. It turns out that φ − {2} is an inconsistent set of clauses detectable
by unit propagation. Therefore, the lower bound computed is 2.
329

Li, Manyà & Planes

Similarly, if the inconsistent subset {x3 , x4 , x̄3 ∨ x̄4 } is detected and Rule 3 is applied,
the lower bound computed is 2.
We observe that, in this example, Rule 3 and Rule 4 not only make explicit a contradiction, but also allow to improve the lower bound.
Unit propagation needs at least one unit clause to detect a contradiction. A drawback
of Rule 3 and Rule 4 is that they consume two unit clauses for deriving just one contradiction. A possible situation is that, after branching, those two unit clauses could allow
unit propagation to derive two disjoint inconsistent subsets of clauses, as we show in the
following example.
Example 3 Let φ={x1 , x̄1 ∨x2 , x̄1 ∨x3 , x̄2 ∨ x̄3 ∨x4 , x5 , x̄5 ∨x6 , x̄5 ∨x7 , x̄6 ∨ x̄7 ∨x4 , x̄1 ∨ x̄5 }.
Rule 3 replaces x1 , x5 , and x̄1 ∨ x̄5 with an empty clause and x1 ∨ x5 . After that, if x4
is selected as the next branching variable and is assigned 0, there is no unit clause in φ
and no contradiction can be detected via unit propagation. The lower bound is 1 in this
situation. However, if Rule 3 was not applied before branching, φ has two unit clauses
after branching. In this case, the propagation of x1 allows to detect the inconsistent subset
{x1 , x̄1 ∨ x2 , x̄1 ∨ x3 , x̄2 ∨ x̄3 }, and the propagation of x5 allows to detect the inconsistent
subset {x5 , x̄5 ∨ x6 , x̄5 ∨ x7 , x̄6 ∨ x̄7 }. So, the lower bound computed after branching is 2.
On the one hand, Rule 3 and Rule 4 add clauses that can contribute to detect additional
conflicts. On the other hand, each application of Rule 3 and Rule 4 consumes two unit
clauses, which cannot be used again to detect further conflicts. The final effect of these two
factors will be empirically analyzed in Section 7.
Finally, we present two new rules that capture unit resolution refutations in which
(i) exactly one unit clause is consumed, and (ii) the unit clause is used twice in the linear
derivation of the empty clause.
Rule 5 If φ1 ={l1 , ¯l1 ∨ l2 , ¯l1 ∨ l3 , ¯l2 ∨ ¯l3 } ∪ φ′ , then φ2 ={2, l1 ∨ ¯l2 ∨ ¯l3 , ¯l1 ∨ l2 ∨ l3 } ∪ φ′ is
equivalent to φ1 .
Proof 6
E1 = 1 − l1 + l1 (1 − l2 ) + l1 (1 − l3 ) + l2 l3 + E ′
= 1 − l1 + l1 − l1 l2 + l1 − l1 l3 + l2 l3 + E ′
= 1 + l2 l3 − l1 l2 l3 + l1 − l1 l2 − l1 l3 + l1 l2 l3 + E ′
= 1 + (1 − l1 )l2 l3 + l1 (1 − l2 − l3 + l2 l3 ) + E ′
= 1 + (1 − l1 )l2 l3 + l1 (1 − l2 )(1 − l3 ) + E ′
= E2



We can combine a linear derivation with Rule 5 to obtain Rule 6:
Rule 6 If φ1 ={l1 , ¯l1 ∨ l2 , ¯l2 ∨ l3 , ..., ¯lk ∨ lk+1 , ¯lk+1 ∨ lk+2 , ¯lk+1 ∨ lk+3 , ¯lk+2 ∨ ¯lk+3 } ∪ φ′ ,
then φ2 ={2, l1 ∨ ¯l2 , l2 ∨ ¯l3 , ..., lk ∨ ¯lk+1 , lk+1 ∨ ¯lk+2 ∨ ¯lk+3 , ¯lk+1 ∨ lk+2 ∨ lk+3 } ∪ φ′ is
equivalent to φ1 .
330

New Inference Rules for Max-SAT

Proof 7 We prove the soundness of the rule by induction on k. When k=1,
φ1 = {l1 , ¯l1 ∨ l2 , ¯l2 ∨ l3 , ¯l2 ∨ l4 , ¯l3 ∨ ¯l4 } ∪ φ′ .
By Lemma 1, we get
By Rule 5, we get

{l1 ∨ ¯l2 , l2 , ¯l2 ∨ l3 , ¯l2 ∨ l4 , ¯l3 ∨ ¯l4 } ∪ φ′ .
{l1 ∨ ¯l2 , 2, l2 ∨ ¯l3 ∨ ¯l4 , ¯l2 ∨ l3 ∨ l4 } ∪ φ′ ,

which is φ2 when k = 1. Therefore, φ1 and φ2 are equivalent.
Assume that Rule 6 is sound for k = n. Let us prove that it is sound for k = n + 1. In
that case:
φ1 = {l1 , ¯l1 ∨ l2 , ¯l2 ∨ l3 , ..., ¯ln+1 ∨ ln+2 , ¯ln+2 ∨ ln+3 , ¯ln+2 ∨ ln+4 , ¯ln+3 ∨ ¯ln+4 } ∪ φ′ .
By Lemma 1, we get
{l1 ∨ ¯l2 , l2 , ¯l2 ∨ l3 , ..., ¯ln+1 ∨ ln+2 , ¯ln+2 ∨ ln+3 , ¯ln+2 ∨ ln+4 , ¯ln+3 ∨ ¯ln+4 } ∪ φ′ .
By applying the induction hypothesis, we get
{l1 ∨ ¯l2 , 2, l2 ∨ ¯l3 , ..., ln+1 ∨ ¯ln+2 , ln+2 ∨ ¯ln+3 ∨ ¯ln+4 , ¯ln+2 ∨ ln+3 ∨ ln+4 } ∪ φ′ ,
which is φ2 when k = n + 1. Therefore, φ1 and φ2 are equivalent and the rule is sound.

Similarly to Rule 3 and Rule 4, Rule 5 and Rule 6 make explicit a contradiction, which
does not need to be redetected in subsequent search. Therefore, the lower bound computation becomes more incremental. Moreover, they also add clauses which can improve the
quality of the lower bound, as illustrated in the following example.
Example 4 Let φ={x1 , x̄1 ∨ x2 , x̄1 ∨ x3 , x̄2 ∨ x̄3 , x4 , x1 ∨ x̄4 , x̄2 ∨ x̄4 , x̄3 ∨ x̄4 }. Depending
on the ordering in which unit clauses are propagated, unit propagation can detect one of the
following inconsistent subsets: {x1 , x̄1 ∨ x2 , x̄1 ∨ x3 , x̄2 ∨ x̄3 }, {x4 , x1 ∨ x̄4 , x̄2 ∨ x̄4 , x̄1 ∨ x2 },
{x4 , x1 ∨ x̄4 , x̄3 ∨ x̄4 , x̄1 ∨x3 }, in which Rule 5 is applicable. If Rule 5 is not applied, the lower
bound computed using the underestimation function of Figure 2 is 1, since the remaining
clauses of φ are satisfiable once the inconsistent subset of clauses is removed. Rule 5 allows
to add two ternary clauses contributing to another contradiction. For example, Rule 5
applied to {x1 , x̄1 ∨ x2 , x̄1 ∨ x3 , x̄2 ∨ x̄3 } adds to φ clauses x1 ∨ x̄2 ∨ x̄3 and x̄1 ∨ x2 ∨ x3 ,
which, with the remaining clauses of φ ({x4 , x1 ∨ x̄4 , x̄2 ∨ x̄4 , x̄3 ∨ x̄4 }), give the second
contradiction detectable via unit propagation. So the lower bound computed using Rule 5
is 2.
In contrast to Rule 3 and Rule 4, Rule 5 and Rule 6 consume exactly one unit clause for
deriving an empty clause. Since a unit clause can be used at most once to derive a conflict
via unit propagation, Rule 5 and Rule 6 do not limit the detection of further conflicts via
unit propagation.
331

Li, Manyà & Planes

5. Implementation of Inference Rules
In this section, we describe the implementation of all the inference rules presented in Section 4. We suppose that the CNF formula is loaded and, for every literal ℓ, a list of clauses
containing ℓ is constructed. The application of a rule means that some clauses in φ1 are
removed from the CNF formula, new clauses in φ2 are inserted into the formula, and the
lower bound is increased by 1. Note that in all the inference rules selected in our approach,
φ2 contains fewer literals and fewer clauses than φ1 , so that new clauses of φ2 can be inserted
in the place of the removed clauses of φ1 when an inference rule is applied. Therefore, we
do not need dynamic memory management and the implementation can be faster.
Rule 1 for k=2 and Rule 2 can be applied using a matching algorithm (see, e.g., Cormen,
Leiserson, Rivest, & Stein, 2001, for an efficient implementation) over the lists of clauses.
The first has a time complexity of O(m), where m is the number of clauses in the CNF
formula. The second has a time complexity of O(u), where u is the number of unit clauses
in the CNF formula. These rules are applied at every node, before any lower bound computation or application of other inference rules. Rule 1 (k=2) is applied as many times as
possible to derive unit clauses before applying Rule 2.
The implementation of Rule 3, Rule 4, Rule 5, and Rule 6 is entirely based on unit
propagation. Given a CNF formula φ, unit propagation constructs an implication graph
G (see, e.g., Beame, Kautz, & Sabharwal, 2003), from which the applicability of inference
rules is detected. In this section, we first describe the construction of the implication graph,
and then describe how to determine the applicability of Rule 3, Rule 4, Rule 5, and Rule 6.
Then, we analyze the complexity, termination and (in)completeness of the application of
the rules. Finally we discuss the extension of the inference rules to weighted Max-SAT and
their implementation.
5.1 Implication Graph
Given a CNF formula φ, Figure 3 shows how unit propagation constructs an implication
graph whose nodes are literals.
Note that every node in G corresponds to a different literal, where ℓ and ℓ̄ are considered
as different literals. When the CNF formula contains several copies of a unit clause ℓ, the
algorithm adds just one node with label ℓ.
Example 5 Let φ={x1 , x1 , x̄1 ∨x2 , x̄1 ∨x3 , x̄2 ∨x̄3 ∨x4 , x5 , x̄5 ∨x6 , x̄5 ∨x7 , x̄6 ∨x̄7 ∨x̄4 , x̄5 ∨x8 }.
U nitP ropagation constructs the implication graph of Figure 4, in which we add a special
node 2 to highlight the contradiction.
G is always acyclic because every added edge connects a new node. It is well known
that the time complexity of unit propagation is O(|φ|), where |φ| is the size of φ (see, e.g.,
Freeman, 1995).
We associate clause c=ℓ̄1 ∨ℓ̄2 ∨...∨ℓ̄k−1 ∨ℓk with node ℓk if node ℓk is added into G because
of c. Note that node ℓk does not have any incoming edge if and only if c is unit (k=1), and
the node has only one incoming edge if and only if c is binary (k=2). Once G is constructed,
if G contains both ℓ and ℓ̄ for some literal ℓ (i.e., unit propagation deduces a contradiction),
it is easy to identify all nodes from which there exists a path to ℓ or ℓ̄ in G; i.e., the clauses
332

New Inference Rules for Max-SAT

Input: U nitP ropagation(φ) : φ is a CNF formula not containing the complementary unit
clauses ℓ and ℓ̄ for any literal ℓ
initialize G as the empty graph
add a node labeled with ℓ for every literal ℓ in a unit clause c of φ
repeat
if ℓ1 , ℓ2 , ..., ℓk−1 are nodes of G, c = ℓ̄1 ∨ ℓ̄2 ∨ ... ∨ ℓ̄k−1 ∨ ℓk is a clause of φ, and ℓk is
not a node of G, then
add into G a node labeled with ℓk
add into G a directed edge from node ℓi to ℓk for every i (1 ≤ i < k)
end if
until no more nodes can be added or there is a literal ℓ such that both ℓ and ℓ̄ are nodes
of G
Return G
Output: Implication graph G of φ
Figure 3: Unit propagation for constructing implication graphs

x2
x4

x1
x3
x6
x5

x̄4
x7
x8

Figure 4: Example of implication graph

333

Li, Manyà & Planes

x1
c1
x5
c5

x2
c2
x6
c6

x3
c3
x̄4
c7

x4
c4

Figure 5: Example of implication graph
implying ℓ or ℓ̄. All these clauses constitute an inconsistent subset S of φ. In the above
example, clauses x1 , x̄1 ∨ x2 , x̄1 ∨ x3 and x̄2 ∨ x̄3 ∨ x4 imply x4 , and clauses x5 , x̄5 ∨ x6 , x̄5 ∨ x7
and x̄6 ∨ x̄7 ∨ x̄4 imply x̄4 . Clause x̄5 ∨ x8 does not contribute to the contradiction. The
inconsistent subset S is {x1 , x̄1 ∨ x2 , x̄1 ∨ x3 , x̄2 ∨ x̄3 ∨ x4 , x5 , x̄5 ∨ x6 , x̄5 ∨ x7 , x̄6 ∨ x̄7 ∨ x̄4 }.
5.2 Applicability of Rule 3, Rule 4, Rule 5, and Rule 6
We assume that unit propagation deduces a contradiction and, therefore, the implication
graph G contains both ℓ and ℓ̄ for some literal ℓ. Let Sℓ be the set of all nodes from which
there exists a path to ℓ, let Sℓ̄ be the set of all nodes from which there exists a path to
ℓ̄, and let S=Sℓ ∪ Sℓ̄ . As a clause is associated with each node in G, we also use S, Sℓ ,
and Sℓ̄ to denote the set of clauses associated with the nodes in S, Sℓ , and Sℓ̄ , respectively.
Lemma 2 and Lemma 3 are used to detect the applicability of Rule 3, Rule 4, Rule 5, and
Rule 6.
Lemma 2 Rule 3 and Rule 4 are applicable if
1. in Sℓ (resp. Sℓ̄ ), there is one unit clause and all the other clauses are binary,
2. nodes in Sℓ (resp. Sℓ̄ ) form an implication chain starting at the unit clause and ending
at ℓ (resp. ℓ̄),
3. Sℓ ∩ Sℓ̄ is empty.
Proof 8 Starting from the node corresponding to the unit clause in Sℓ (resp. Sℓ̄ ), and
following in parallel the two implication chains, we have φ1 in Rule 3 or Rule 4 by writing
down the clause corresponding to each node. 
Example 6 Let φ be the following CNF formula containing clauses c1 to c7 : {c1 : x1 , c2 :
x̄1 ∨ x2 , c3 : x̄2 ∨ x3 , c4 : x̄3 ∨ x4 , c5 : x5 , c6 : x̄5 ∨ x6 , c7 : x̄6 ∨ x̄4 }. Unit propagation constructs the implication graph shown in Figure 5, which contains the complementary
literals x4 and x̄4 .
Rule 4 is applicable because ℓ=x4 , Sℓ ={x1 (c1 ), x2 (c2 ), x3 (c3 ), x4 (c4 )}, and
Sℓ̄ ={x5 (c5 ), x6 (c6 ), x̄4 (c7 )}. It is easy to verify that the three conditions of Lemma 2 are
satisfied.
Remark: φ can be rewritten as {c1 : x1 , c2 : x̄1 ∨ x2 , c3 : x̄2 ∨ x3 , c4 : x̄3 ∨ x4 , c7 :
x̄4 ∨ x̄6 , c6 : x6 ∨ x̄5 , c5 : x5 } to be compared with φ1 in Rule 4.
334

New Inference Rules for Max-SAT

x1
c1

x2
c2

x3
c3
x4
c4

x̄4
c5

Figure 6: Example of implication graph
The application of Rule 3 and Rule 4 consists of replacing every binary clause c in S
with a binary clause obtained by negating every literal of c, removing the two unit clauses
of S from φ, and incrementing #emptyClauses(φ) by 1.
Lemma 3 Rule 5 and Rule 6 are applicable if
1. in S=Sℓ ∪ Sℓ̄ , there is one unit clause and all the other clauses are binary; i.e., all
nodes in S, except for the node corresponding to the unit clause, have exactly one
incoming edge in G.
2. Sℓ ∩ Sℓ̄ is non-empty and contains k (k >0) nodes forming an implication chain of
the form ℓ1 → ℓ2 → · · · → ℓk , where ℓk is the last node of the chain.
3. (Sℓ ∪ Sℓ̄ )-(Sℓ ∩ Sℓ̄ ) contains exactly three nodes : ℓ, ℓ̄, and a third one. Let ℓk+1 be
the third literal,
if ℓk+1 ∈ Sℓ , then G contains the following implications
ℓk → ℓk+1 → ℓ
ℓk → ℓ̄
if ℓk+1 ∈ Sℓ̄ , then G contains the following implications
ℓk → ℓ
ℓk → ℓk+1 → ℓ̄
Proof 9 Assume, without loss of generality, that ℓk+1 ∈ Sℓ ; the case ℓk+1 ∈ Sℓ̄ is symmetric. The implication chain formed by the nodes of Sℓ ∩ Sℓ̄ corresponds to the clauses {ℓ1 ,
ℓ̄1 ∨ ℓ2 , . . . , ℓ̄k−1 ∨ ℓk }, which, together with the three clauses {ℓ̄k ∨ ℓk+1 , ℓ̄k+1 ∨ ℓ, ℓ̄k ∨ ℓ̄}
corresponding to ℓk → ℓk+1 → ℓ and ℓk → ℓ̄, give φ1 in Rule 5 or Rule 6. 
Example 7 Let φ be the following CNF formula containing clauses c1 to c5 : {c1 : x1 , c2 :
x̄1 ∨x2 , c3 : x̄2 ∨x3 , c4 : x̄2 ∨x4 , c5 : x̄3 ∨ x̄4 }. Unit propagation constructs the implication
graph shown in Figure 6, which contains the complementary literals x4 and x̄4 .
We have Sx4 ={x1 (c1 ), x2 (c2 ), x4 (c4 )} and Sx̄4 ={x1 (c1 ), x2 (c2 ), x3 (c3 ), x̄4 (c5 )}. The
nodes in Sx4 ∩ Sx̄4 obviously form an implication chain: x1 → x2 . (Sx4 ∪ Sx̄4 )-(Sx4 ∩
Sx̄4 )={x3 (c3 ), x4 (c4 ), x̄4 (c5 )}. G contains x2 → x3 → x̄4 and x2 → x4 . Rule 6 is applicable.
The application of Rule 5 and Rule 6 consists of removing the unit clause of Sℓ ∪ Sℓ̄
from φ, replacing each binary clause c in Sℓ ∩ Sℓ̄ with a binary clause obtained from c by
negating the two literals of c, replacing the three binary clauses in (Sℓ ∪ Sℓ̄ )-(Sℓ ∩ Sℓ̄ ) with
two ternary clauses, and incrementing #emptyClauses(φ) by 1.
335

Li, Manyà & Planes

5.3 Complexity, Termination, and (In)Completeness of Rule Applications
In our branch and bound algorithm for Max-SAT, we combine the application of the inference rules and the computation of the underestimation of the lower bound. Given a CNF
formula φ, function underestimation uses unit propagation to construct an implication
graph G. Once G contains two nodes ℓ and ℓ̄ for some literal ℓ, G is analyzed to determine
whether some inference rule is applicable. If some rule is applicable, it is applied and φ is
transformed into an equivalent Max-SAT instance. Otherwise, all clauses contributing to
the contradiction are removed from φ, and the underestimation is incremented by 1. This
procedure is repeated until unit propagation cannot derive more contradictions. Finally, all
removed clauses, except those removed or replaced due to inference rule applications, are
reinserted into φ. The underestimation, together with the new φ, is returned.
It is well known that unit propagation can be implemented with a time complexity linear
in the size of φ (see, e.g., Freeman, 1995). The complexity of determining the applicability
of the inference rules using Lemma 2 and Lemma 3 is linear in the size of G, bounded by
the number of literals in φ, if we assume that the graph is represented by doubly-linked
lists. The application of an inference rule is obviously linear in the size of G. So, the whole
time complexity of function underestimation with inference rule applications is O(d ∗ |φ|),
where d is the number of contradictions that function underestimation is able to detect
using unit propagation. Observe that the factor d is needed because the application of the
rules inserts new clauses in the place of the removed clauses.
Since every inference rule application reduces the size of φ, function underestimation
with inference rule applications has linear space complexity, and it always terminates. Recall
that new clauses added by the inference rules can be stored in the place of the old ones.
The data structures for loading φ can be statically and efficiently maintained.
We have proved that the inference rules are sound. The following example shows that
the application of the rules is not necessarily complete in our implementation, in the sense
that not all possible applications of the inference rules are necessarily done.
Example 8 Let φ={x1 , x3 , x4 , x̄1 ∨ x̄3 ∨ x̄4 , x̄1 ∨ x̄2 , x2 }. Unit propagation may discover
the inconsistent subset S={x1 , x3 , x4 , x̄1 ∨ x̄3 ∨ x̄4 }. In this case, no inference rule is applicable to S. Then, the underestimation of the lower bound is incremented by 1, and φ
becomes {x̄1 ∨ x̄2 , x2 }. Unit propagation cannot detect more contradictions in φ, and function underestimation stops after reinserting {x1 , x3 , x4 , x̄1 ∨ x̄3 ∨ x̄4 } into φ. The value
1 is returned, together with the unchanged φ. Note that Rule 3 is applicable to the subset
{x1 , x̄1 ∨ x̄2 , x2 } of φ, but is not applied.
Actually, function underestimation only applies Rule 3 if unit propagation detects the
inconsistent subset {x1 , x̄1 ∨ x̄2 , x2 } instead of {x1 , x3 , x4 , x̄1 ∨ x̄3 ∨ x̄4 }. The detection of
an inconsistent subset depends on the ordering in which unit clauses are propagated in unit
propagation. In this example, the inconsistent subset {x1 , x̄1 ∨ x̄2 , x2 } is discovered if unit
clause x2 is propagated before x3 and x4 . Further study is needed to define orderings for
unit clauses that maximize the application of inference rules.
Observe that our algorithm is deterministic, and always computes the same lower bound
if the order of clauses is not changed.
336

New Inference Rules for Max-SAT

5.4 Inference Rules for Weighted Max-SAT
The inference rules presented in this paper can be naturally extended to weighted Max-SAT.
In weighted Max-SAT, every clause is associated with a weight and the problem consists
of finding a truth assignment for which the sum of the weights of unsatisfied clauses is
minimum. For example, the weighted version of Rule 3 could be
Rule 7 If φ1 ={(l1 , w1 ), (¯l1 ∨ ¯l2 , w2 ), (l2 , w3 )} ∪ φ′ , then φ2 ={(2, w), (l1 ∨ l2 , w), (l1 , w1 −
w), (¯l1 ∨ ¯l2 , w2 − w), (l2 , w3 − w)} ∪ φ′ is equivalent to φ1
where w1 , w2 and w3 are positive integers representing the clause weight, and w=min(w1 ,
w2 , w3 ). Mandatory clauses, that have to be satisfied in any optimal solution, are specified
with the weight ∞. Note that if w6=∞, ∞-w=∞ and if w=∞, no optimal solution can be
found and the solver should backtrack. Clauses with weight 0 are removed. Observe that φ1
can be rewritten as φ11 ∪ φ12 , where φ11 ={(l1 , w), (¯l1 ∨ ¯l2 , w), (l2 , w)}, and φ12 ={(l1 , w1 −
w), (¯l1 ∨ ¯l2 , w2 − w), (l2 , w3 − w)} ∪ φ′ . Then, the weighted inference rule is equivalent to
the unweighted version applied w times to the (unweighted) clauses of φ11 .
Similarly, the weighted version of Rule 4 could be
Rule 8 If φ1 ={(l1 , w1 ) (¯l1 ∨ l2 , w2 ), (¯l2 ∨ l3 , w3 ), . . . , (¯lk ∨ lk+1 , wk+1 ), (¯lk+1 , wk+2 )} ∪ φ′ ,
then φ2 ={(2, w), (l1 ∨ ¯l2 , w), (l2 ∨ ¯l3 , w), . . . , (lk ∨ ¯lk+1 , w), (l1 , w1 − w), (¯l1 ∨ l2 , w2 −
w), (¯l2 ∨ l3 , w3 − w), . . . , (¯lk ∨ lk+1 , wk+1 − w), (¯lk+1 , wk+2 − w)} ∪ φ′ is equivalent to φ1
where w=min(w1 , w2 , . . . , wk+2 ). Observe that φ1 can also be rewritten as φ11 ∪ φ12 , with
φ11 ={(l1 , w) (¯l1 ∨ l2 , w), (¯l2 ∨ l3 , w), . . . , (¯lk ∨ lk+1 , w), (¯lk+1 , w)}, The weighted version of
Rule 4 is equivalent to the unweighted Rule 4 applied w times to the (unweighted) clauses
of φ11 .
The current implementation of the inference rules can be naturally extended to weighted
inference rules. If an inconsistent subformula is detected and a rule is applicable (clause
weights are not considered in the detection of the inconsistent subformula and of the applicability of the rule, provided that clauses with weight 0 have been discarded), then φ11
and φ12 are separated after computing the minimal weight w of all clauses in the detected
inconsistent subformula, and the rule is applied to φ11 . The derived clauses and clauses in
φ12 can be used in subsequent reasoning.

6. MaxSatz: a New Max-SAT Solver
We have implemented a new Max-SAT solver, called MaxSatz, that incorporates the lower
bound computation method based on unit propagation defined in Section 3, and applies the
inference rules defined in Section 4. The name of MaxSatz comes from the fact that the
implementation of our algorithm incorporates most of the technology that was developed
for the SAT solver Satz (Li & Anbulagan, 1997b, 1997a).
MaxSatz incorporates the lower bound based on unit propagation, and applies Rule 1,
Rule 2, Rule 3, Rule 4, Rule 5, and Rule 6. In addition, MaxSatz applies the following
techniques:
• Pure literal rule: If a literal only appears with either positive polarity or negative
polarity, we delete the clauses containing that literal.
337

Li, Manyà & Planes

• Empty-Unit clause rule (Alsinet et al., 2003a): Let neg1(x) (pos1(x)) be the number of
unit clauses in which x is negative (positive). If #emptyClauses(φ) + neg1(x) ≥ U B,
then we assign x to false. If #emptyClauses(φ) + pos1(x) ≥ U B, then we assign x
to true.
• Dominating Unit Clause (DUC) rule (Niedermeier & Rossmanith, 2000): If the number of clauses in which a literal x (x̄) appears is not greater than neg1(x) (pos1(x)),
then we assign x to false (true).
• Variable selection heuristic: Let neg2(x) (pos2(x)) be the number of binary clauses
in which x is negative (positive), and let neg3(x) (pos3(x)) be the number of clauses
containing three or more literals in which x is negative (positive). We select the
variable x such that (neg1(x)+4∗neg2(x)+neg3(x))*(pos1(x)+4∗pos2(x)+pos3(x))
is the largest. The fact that binary clauses are counted four times more than other
clauses was determined empirically.
• Value selection heuristic: Let x be the selected branching variable. If neg1(x) + 4 ∗
neg2(x) + neg3(x) < pos1(x) + 4 ∗ pos2(x) + pos3(x), set x to true. Otherwise set x
to false. This heuristics was also determined empirically.
In this paper, in order to compare the inference rules defined, we have used three simplified versions of MaxSatz:
• MaxSat0: does not apply any inference rule defined in Section 4.
• MaxSat12: applies rules 1 and 2, but not rules 3, 4, 5 and 6.
• MaxSat1234: applies rules 1, 2, 3 and 4, but not rules 5 and 6.
Actually, MaxSatz corresponds to MaxSat123456 in our terminology. MaxSat12 corresponds to an improved version of the solver U P (Li et al., 2005), using a special ordering
for propagating unit clauses in unit propagation. MaxSat12 maintains two queues during
unit propagation: Q1 and Q2 . When MaxSat12 starts the search for an inconsistent subformula via unit propagation, Q1 contains all the unit clauses of the CNF formula under
consideration (more recently derived unit clauses are at the end of Q1 ), and Q2 is empty.
The unit clauses derived during the application of unit propagation are stored in Q2 , and
unit propagation does not use any unit clause from Q1 unless Q2 is empty. Intuitively, this
ordering prefers unit clauses which were non-unit clauses before starting the application of
unit propagation. This way, the derived inconsistent subset contains, in general, less unit
clauses. The unit clauses which have not been consumed will contribute to detect further
inconsistent subsets. Our experimental results (Li, Manyà, & Planes, 2006) show that the
search tree size of MaxSat12 is substantially smaller than that of UP, and MaxSat12 is
substantially faster than UP. MaxSat0, Maxsat1234, and MaxSatz use the same ordering
as MaxSat12 for propagating unit clauses in unit propagation.
The source code of MaxSat0, MaxSat12, MaxSat1234, and MaxSatz can be found at
http://web.udl.es/usuaris/m4372594/jair-maxsatz-solvers.zip, and at http://www.laria.upicardie.fr/˜cli/maxsatz.tar.gz.
338

New Inference Rules for Max-SAT

7. Experimental Results
We report on the experimental investigation performed for unweighted Max-SAT in order
to evaluate the inference rules defined in Section 4, and to compare MaxSatz with the
best performing state-of-the-art solvers that were publicly available when this paper was
submitted. The experiments were performed on a Linux Cluster with processors 2 GHz
AMD Opteron with 1 Gb of RAM.
The structure of this section is as follows. We first describe the solvers and benchmarks
that we have considered. Then, we present the experimental evaluation of the inference
rules. Finally, we show the experimental comparison of MaxSatz with other solvers.
7.1 Solvers and Benchmarks
MaxSatz was compared with the following Max-SAT solvers:
• BF2 (Borchers & Furman, 1999): a branch and bound Max-SAT solver which uses
MOMS as dynamic variable selection heuristic and does not consider underestimations
in the computation of the lower bound. It was developed by Borchers and Furman in
1999.
• AGN3 (Alber et al., 2001): a branch and bound Max-2SAT solver. It was developed
by Alber, Gramm and Niedermeier in 1998.
• AMP4 (Alsinet et al., 2003b): a branch and bound Max-SAT solver based on BF that
incorporates a lower bound of better quality and the Jeroslow-Wang variable selection
heuristic (Jeroslow & Wang, 1990). It was developed by Alsinet, Manyà and Planes
and presented at SAT-2003.
• toolbar5 (de Givry et al., 2003; Larrosa & Heras, 2005): a Max-SAT solver whose
inference was inspired in soft arc consistency properties implemented in weighted CSP
solvers. It was developed by de Givry, Larrosa, Meseguer and Schiex and was first
presented at CP-2003. We used version 2.2 with default parameters.
• MaxSolver6 (Xing & Zhang, 2004): a branch and bound Max-SAT solver that applies a
number of efficient inference rules. It was developed by Xing and Zhang and presented
at CP-2004. We used the second release of this solver.
• Lazy7 (Alsinet et al., 2005): a branch and bound Max-SAT solver with lazy data
structures and a static variable selection heuristic. It was developed by Alsinet, Manyà
and Planes and presented at SAT-2005.
2.
3.
4.
5.
6.
7.

Downloaded in October 2004 from http://infohost.nmt.edu/˜borchers/satcodes.tar.gz
Downloaded in October 2005 from http://www-fs.informatik.uni-tuebingen.de/˜gramm/
Available at http://web.udl.es/usuaris/m4372594/software.html
Downloaded in October 2005 from http://carlit.toulouse.inra.fr/cgi-bin/awki.cgi/ToolBarIntro
Downloaded in October 2005 from http://cic.cs.wustl.edu/maxsolver/
Available at http://web.udl.es/usuaris/m4372594/software.html

339

Li, Manyà & Planes

• UP8 (Li et al., 2005): a branch and bound Max-SAT solver with the lower bound
computation method based on unit propagation (cf. Section 3). It was developed by
Li, Manyà and Planes and presented at CP-2005.
We used as benchmarks randomly generated Max-2SAT instances and Max-3SAT instances, graph 3-coloring instances9 , as well as Max-Cut instances10 . We also considered the
unweighted Max-SAT benchmarks submitted to the Max-SAT Evaluation 2006, including
Max-Cut, Max-Ones, Ramsey numbers, and random Max-2SAT and Max-3SAT instances.
We generated Max-2SAT instances and Max-3SAT instances using the generator mwff.c
developed by Bart Selman, which allows for duplicated clauses. For Max-Cut, we first
generated a random graph of m edges in which every edge is randomly selected among
the set of all possible edges. If the graph is not connected, it is discarded. If the graph
is connected, we used the encoding of Shen and Zhang (2005) to encode the Max-Cut
instance into a CNF: we created, for each edge (xi , xj ), exactly two binary clauses (xi ∨ xj )
and (x̄i ∨ x̄j ). If φ is the collection of such binary clauses, then the Max-Cut instance has
a cut of weight k iff the Max-SAT instance has an assignment under which m + k clauses
are satisfied.
For graph 3-coloring, we first used Culberson’s generator to generate a random kcolorable graph of type IID (independent random edge assignment, variability=0) with
k vertices and a fixed edge density. We then used Culberson’s converter to SAT with standard conversion and three colors to generate a Max-SAT instance: for each vertex xi and
for each color j ∈ {1, 2, 3}, a propositional variable xij is defined meaning that vertex i is
colored with color j. For each vertex xi , four clauses are added to encode that the vertex
is colored with exactly one color: xi1 ∨ xi2 ∨ xi3 , x̄i1 ∨ x̄i2 , x̄i1 ∨ x̄i3 , and x̄i2 ∨ x̄i3 ; and, for
each edge (xi , xj ), three clauses are added to encode that vertex xi and vertex xj do not
have the same color: x̄i1 ∨ x̄j1 , x̄i2 ∨ x̄j2 , and x̄i3 ∨ x̄j3 .
In random Max-2SAT and Max-3SAT instances, clauses are entirely independent to each
other and do not have structure. In the graph 3-coloring instances and Max-Cut instances
used in this paper, clauses are not independent and have structure. For example, in a
Max-Cut instance, every time we have a clause xi ∨ xj , we also have the clause x̄i ∨ x̄j ;
the satisfaction of these two clauses means that the corresponding edge is in the cut. In a
graph 3-coloring instance, every time we have a ternary clause xi1 ∨ xi2 ∨ xi3 encoding that
vertex i is colored with at least a color, we also have three binary clauses x̄i1 ∨ x̄i2 , x̄i1 ∨ x̄i3 ,
and x̄i2 ∨ x̄i3 encoding that vertex i cannot be colored with two or more colors. MaxCut instances only contain binary clauses, but graph 3-coloring instances contain a ternary
clause for every vertex in the graph. While we can derive an optimal cut from an optimal
assignment of a Max-SAT encoding of any Max-Cut instance, an optimal assignment of a
Max-SAT encoding of a 3-coloring instance may assign more than one color to some vertices.
8. Available at http://web.udl.es/usuaris/m4372594/software.html
9. Given an undirected graph G = (V, E), where V = {x1 , . . . , xn } is the set of vertices and E is the set
of edges, and a set of three colors, the graph 3-coloring problem is the problem of coloring every vertex
with one of the three colors in such a way that, for each edge (xi , xj ) ∈ E, vertex xi and vertex xj do
not have the same color.
10. Given an undirected graph G = (V, E), let wxi ,xj be the weight associated with each edge (xi , xj ) ∈ E.
P
The weighted Max-Cut problem is to find a subset S of V such that W (S, S) = xi ∈S,xj ∈S wxi ,xj is
maximized, where S = V − S. In this paper, we set weight wxi ,xj = 1 for all edges.

340

New Inference Rules for Max-SAT

The Max-Cut and Ramsey numbers instances from the Max-SAT Evaluation 2006 contain different structures. For example, the underlying graphs in the Max-Cut instances
have different origins such as fault diagnosis problems, coding theory problems, and graph
clique problems. Max-2SAT and Max-3SAT instances from the evaluation do not contain
duplicated clauses.
We computed an initial upper bound with a local search solver for each instance. We
did not provide any parameter to any solver except the instance to be solved and the initial
upper bound. In other words, we used the default values for all the parameters. The
instances from the Max-SAT Evaluation 2006 were solved in the same conditions as in the
evaluation; i.e., no initial upper bound was provided to the solvers, and the maximum time
allowed to solve an instance was 30 minutes.
7.2 Evaluation of the Inference Rules
In the first experiment performed to evaluate the impact of the inference rules of Section 4,
we solved sets of 100 random Max-2SAT instances with 50 and 100 variables; the number
of clauses ranged from 400 to 4500 for 50 variables, and from 400 to 1000 for 100 variables.
The results obtained are shown in Figure 7. Along the horizontal axis is the number of
clauses, and along the vertical axis is the mean time (left plot), in seconds, needed to solve
an instance of a set, and the mean number of branches of the proof tree (right plot). Notice
that we use a log scale to represent both run-time and branches.
We observe that the rules are very powerful for Max-2SAT and the gain increases as
the number of variables and the number of clauses increase. For 50 variables and 1000
clauses (the clause to variable ratio is 20), MaxSatz is 7.6 times faster than MaxSat1234;
and for 100 variables and 1000 clauses (the clause to variable ratio is 10), MaxSatz is 9.2
times faster than MaxSat1234. The search tree of MaxSatz is also substantially smaller
than that of MaxSat1234. Rule 5 and Rule 6 are more powerful than Rule 3 and Rule 4 for
Max-2SAT. The intuitive explanation is that MaxSatz and MaxSat1234 detect many more
inconsistent subsets of clauses containing one unit clause than subsets containing two unit
clauses, so that Rule 5 and Rule 6 can be applied many more times than Rule 3 and Rule 4
in MaxSatz.
Recall that, on the one hand, every application of Rule 3 and Rule 4 consumes two
unit clauses but only produces one empty clause, limiting unit propagation in detecting
more conflicts in subsequent search. On the other hand, Rule 3 and Rule 4 add clauses
which may contribute to detect further conflicts. Depending on the number of clauses (or
more precisely, the clause to variable ratio) in a formula, these two factors have different
importance. When there are relatively few clauses, unit propagation relatively does not
easily derive a contradiction from a unit clause, and the binary clauses added by Rule 3 and
Rule 4 are relatively important for deriving additional conflicts and improving the lower
bound, which explains why the search tree of MaxSat1234 is smaller than the search tree
of MaxSat12 for instances with 100 variables and less than 600 clauses. On the contrary,
when there are many clauses, unit propagation easily derives a contradiction from a unit
clause, so that the two unit clauses consumed by Rule 3 and Rule 4 would probably allow to
derive two disjoint inconsistent subsets of clauses. In addition, the binary clauses added by
Rule 3 and Rule 4 are relatively less important for deriving additional conflicts, considering
341

Li, Manyà & Planes

the large number of clauses in the formula. In this case, the search tree of MaxSat1234
is larger than the search tree of MaxSat12. However, in both cases, MaxSat1234 is faster
that MaxSat12, meaning that the incremental lower bound computation due to Rule 3 and
Rule 4 is very effective, since the redetection of many conflicts is avoided thanks to Rule 3
and Rule 4.

Max-2SAT - 50 variables
1e+07

1000

1e+06

branches (log scale)

time (logscale)

Max-2SAT - 50 variables
10000

100
10
1
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

0.1
0.01
1000

2000

3000

100000
10000
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

1000
100

4000

1000

2000

number of clauses

1000

1e+07

100
10
1

0.01
400

MaxSat0
MaxSat12
MaxSat1234
MaxSatz
500

600
700
800
number of clauses

4000

Max-2SAT - 100 variables
1e+08
branches (log scale)

time (logscale)

Max-2SAT - 100 variables
10000

0.1

3000

number of clauses

900

1e+06
100000
10000
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

1000
100
400

1000

500

600
700
800
number of clauses

900

1000

Figure 7: Comparison among MaxSat12, MaxSat1234 and MaxSatz on random Max-2SAT instances.

Rule 5 and Rule 6 do not limit unit propagation in detecting more conflicts, since their
application produces one empty clause and consumes just one unit clause, which allows to
derive at most one conflict in any case. The added ternary clauses allow to improve the
lower bound, so that the search tree of MaxSatz is substantially smaller than the search
tree of MaxSat1234. The incremental lower bound computation due to Rule 5 and Rule 6
also contributes to the time performance of MaxSatz. For example, while the search tree of
MaxSatz for instances with 50 variables and 2000 clauses is about 11.5 times smaller than
the search tree of MaxSat1234, MaxSatz is 14 times faster than MaxSat1234.
In the second experiment, we solved random Max-3SAT instances instead of random
Max-2SAT instances. We solved instances with 50 and 70 variables; the number of clauses
ranged from 400 to 1200 for 50 variables, and from 500 to 1000 for 70 variables. The results
obtained are shown in Figure 8.
342

New Inference Rules for Max-SAT

Max-3SAT - 50 variables

Max-3SAT - 50 variables
1e+07
branches (log scale)

time (log scale)

1000

100

10
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

1

0.1
400

600

800
1000
number of clauses

1e+06

100000

1000
400

1200

Max-3SAT - 70 variables

600
800
1000
number of clauses

1200

Max-3SAT - 70 variables
1e+08
branches (log scale)

10000

time (logscale)

MaxSat0
MaxSat12
MaxSat1234
MaxSatz

10000

1000

100
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

10

1
500

600

700
800
number of clauses

900

1000

1e+07

1e+06
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

100000

10000
500

600

700
800
number of clauses

900

1000

Figure 8: Comparison among MaxSat12, MaxSat1234 and MaxSatz on random Max-3SAT instances.

Although the rules do not involve ternary clauses, they are also powerful for Max-3SAT.
Similarly to Max-2SAT, Rule 3 and Rule 4 slightly improve the lower bound when there
are relatively few clauses, but do not improve the lower bound when the number of clauses
increases. They improve the time performance thanks to the incremental lower bound
computation they allowed. The gain increases as the number of clauses increases. For
example, for problems with 70 variables, when the number of clauses is 600, MaxSat1234
is 36% faster than MaxSat12 and, when the number of clauses is 1000, the gain is 44%.
Rule 5 and Rule 6 improve both the lower bound and the time performance of MaxSatz.
The gain increases as the number of clauses increases.
In the third experiment we considered the Max-Cut problem for graphs with 50 vertices
and a number of edges ranging from 200 to 800. Figure 9 shows the results of comparing
the inference rules on Max-Cut instances. We observe that the rules allow us to solve
the instances much faster. Similarly to random Max-2SAT, Rule 3 and Rule 4 do not
improve the lower bound when there are many clauses, but improve the time performance
due to the incremental lower bound computation they allowed. Rule 5 and Rule 6 are more
powerful than Rule 3 and Rule 4 for these instances, which only contain binary clauses but
have some structure. In addition, the reduction of the tree size due to Rule 5 and Rule 6
contributes to the time performance of MaxSatz more than the incrementality of the lower
bound computation, as for random Max-2SAT. For example, the search tree of MaxSatz
for instances with 800 edges is 40 times smaller than the search tree of MaxSat1234, and
MaxSatz is 47 times faster.
343

Li, Manyà & Planes

Max-Cut - 50 nodes
1e+09

10000

1e+08
branches (log scale)

time (log scale)

Max-Cut - 50 nodes
100000

1000
100
10
1

MaxSat0
MaxSat12
MaxSat1234
MaxSatz

0.1
0.01
200

300

400
500
600
number of edges

1e+07
1e+06
100000
10000

MaxSat0
MaxSat12
MaxSat1234
MaxSatz

1000
700

100
200

800

300

400
500
600
number of edges

700

800

Figure 9: Experimental results for Max-Cut

In the fourth experiment we considered graph 3-coloring instances with 24 and 60 vertices, and with density of edges ranging from 20% to 90%. Figure 10 shows the results of
comparing the inference rules on graph 3-coloring instances. We observe that Rule 1 and
Rule 2 are not useful for these instances; the tree size of MaxSat0 and MaxSat12 is almost
the same, and MaxSat12 is slower than MaxSat0. On the contrary, other rules are very
useful for these instances, especially because they allow to reduce the search tree size by
deriving better lower bounds.

Graph 3-coloring 24 nodes

Graph 3-coloring 24 nodes
10000
Branches (log scale)

time (log scale)

0.1

0.01

0.001
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

1e-04
20

30

40

50
60
% of edges

70

1000

100
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

10
80

90

20

30

Graph 3-coloring 60 nodes

50
60
% of edges

70

80

90

80

90

Graph 3-coloring 60 nodes
1e+08
Branches (log scale)

10000

time (log scale)

40

1000

100
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

10

1
20

30

40

50
60
% of edges

70

1e+07

1e+06
MaxSat0
MaxSat12
MaxSat1234
MaxSatz

100000
80

90

20

30

40

50
60
% of edges

Figure 10: Experimental results for Graph 3-Coloring
344

70

New Inference Rules for Max-SAT

Note that Rule 3 and Rule 4 have more impact than Rule 5 and Rule 6 on reducing
the cost of solving the instances. This is probably due to the fact that two unit clauses are
needed to detect a contradiction, so that Rule 3 and Rule 4 are applied many more times.
Also note that the instances with 60 vertices become easier to solve when the density of the
graph is high.
In the fifth experiment, we compared different inference rules on the benchmarks submitted to the Max-SAT Evaluation 2006. Solvers ran in the same conditions as in the
evaluation. In Table 1, the first column is the name of the benchmark set, the second
column is the number of instances in the set, and the rest of columns display the average
time, in seconds, needed by each solver to solve an instance (the number of solved instances
in brackets). The maximum time allowed to solve an instance was 30 minutes.
In is clear that MaxSat12 is better than MaxSat0, MaxSat1234 is better than MaxSat12,
and MaxSatz is better than MaxSat1234. For example, MaxSatz solves three MAXCUT
johnson instances within the time limit, while the other solvers only solve two instances.
The average time for MaxSatz to solve one of these three instances is 44.46 seconds, the
third instance needing more time to be solved than the other two instances.
Set Name
MAXCUT brock
MAXCUT c-fat
MAXCUT hamming
MAXCUT johnson
MAXCUT keller
MAXCUT p hat
MAXCUT san
MAXCUT sanr
MAXCUT max cut
MAXCUT SPINGLASS
MAXONE
RAMSEY
MAX2SAT 100VARS
MAX2SAT 140VARS
MAX2SAT 60VARS
MAX2SAT DISCARDED
MAX3SAT 40VARS
MAX3SAT 60VARS

#Instances
12
7
6
4
2
12
11
4
40
5
45
48
50
50
50
180
50
50

MaxSat0
471.01(10)
1.92 (5)
39.42(2)
14.91(2)
512.66(2)
72.16(9)
801.95(7)
323.67(3)
610.28(35)
0.22 (2)
0.03 (45)
8.93 (34)
95.01(50)
153.28(49)
1.35 (50)
126.98(162)
11.52(50)
167.17(50)

MaxSat12
277.12(12)
3.11 (5)
29.43(2)
8.57 (2)
213.64(2)
286.09(12)
305.75(7)
134.74(3)
481.48(40)
0.19 (2)
0.03 (45)
8.42 (34)
11.30(50)
51.76(50)
0.08 (50)
71.85(173)
3.33 (50)
72.72(50)

MaxSat1234
225.11(12)
2.84 (5)
29.48(2)
7.21 (2)
163.26(2)
226.24(12)
245.70(7)
107.76(3)
450.05(40)
0.15 (2)
0.03 (45)
7.80 (34)
8.14 (50)
34.14(50)
0.06 (50)
68.97(175)
2.52 (50)
52.14(50)

MaxSatz
14.01(12)
0.07(5)
171.30(3)
44.46(3)
6.82(2)
16.81(12)
258.65(11)
71.00(4)
7.18(40)
0.14(2)
0.03(45)
7.78(34)
1.25(50)
6.94(50)
0.02(50)
22.72(180)
1.92(50)
40.27(50)

Table 1: Evaluation of the rules with benchmarks from the MAX-SAT Evaluation 2006.
7.3 Comparison of MaxSatz with Other Solvers
In the first experiment, that we performed to compare MaxSatz with other state-of-the-art
Max-SAT solvers, we solved sets of 100 random Max-2SAT instances with 50, 100 and 150
variables; the number of clauses ranged from 400 to 4500 for 50 variables, from 400 to
1000 for 100 variables, and from 300 to 650 for 150 variables. The results of solving such
instances with BF, AGN, AMP, Lazy, toolbar, MaxSolver, UP and MaxSatz are shown in
Figure 11. Along the horizontal axis is the number of clauses, and along the vertical axis is
the mean time, in seconds, needed to solve an instance of a set. When a solver needed too
much time to solve the instances at a point, it was stopped and the corresponding point is
not shown in the figure. That is why for 50 variable instances, BF has only one point in
the figure (for 400 clauses); and for 100 variable instances, BF and AMP also have only one
345

Li, Manyà & Planes

point in the figure (for 400 clauses). The version of MaxSolver we used limits the number
of clauses to 1000 in the instances to be solved. We ran it for instances up to 1000 clauses.
We see dramatic differences on performance between MaxSatz and the rest of solvers
in Figure 11. For the hardest instances, MaxSatz is up to two orders of magnitude faster
than the second best performing solvers (UP). For those instances, MaxSatz needs 1 second
to solve an instance while solvers like MaxSolver and toolbar are not able to solve these
instances after 10,000 seconds.
In the second experiment, we solved random Max-3SAT instances instead of random
Max-2SAT instances. The results obtained are shown in Figure 12. We did not consider
AGN because it can only solve Max-2SAT instances. We solved instances with 50, 70 and
100 variables; the number of clauses ranged from 500 to 1200 for 50 variables, from 500
to 1000 for 70 variables, and from 450 to 800 for 100 variables. For 70 variables, AMP
has only one point in the figure (for 500 clauses) and BF is too slow. For 100 variables,
we compared only the two best solvers. Once again, we observe dramatic differences on
the performance profile of MaxSatz and the rest of solvers. Particularly remarkable are
the differences between MaxSatz and toolbar (the second best performing solver on Max3SAT), where we see that MaxSatz is up to 1,000 times faster than toolbar on the hardest
instances.
In the third experiment, we considered the Max-Cut problem of graphs with 50 vertices
and a number of edges ranging from 200 to 700. Figure 13 shows the results obtained. BF
has only one point in the figure (for 200 edges). MaxSolver solved instances up to 500 edges
(1000 clauses). We observe that MaxSatz is superior to the rest of solvers.
In the fourth experiment, we considered the 3-coloring problem of graphs with 24 and
60 vertices, and a density of edges ranging from 20% to 90%. AGN was not considered
because it can only solve Max-2SAT instances. For 60 vertices, we only compared the three
best solvers, of which MaxSolver is a different version not limiting the number of clauses
of the instance to be solved. Figure 14 shows the comparative results for different solvers.
MaxSatz is the best performing solver, and UP and MaxSolver are substantially better than
the rest of solvers.

Max-Cut - 50 nodes
10000

time (log scale)

1000
100
BF
AMP
AGN
Lazy
toolbar
MaxSolver
UP
MaxSatz

10
1
0.1

0.01
200 300 400 500 600 700
number of edges

Figure 13: Experimental results for Max-Cut
346

New Inference Rules for Max-SAT

Max-2SAT - 50 variables
10000

time (log scale)

1000
100
10

BF
AMP
AGN
Lazy
toolbar
MaxSolver
UP
MaxSatz

1
0.1
0.01
0.001
1000 2000 3000 4000
number of clauses
Max-2SAT - 100 variables
100000

time (log scale)

10000
1000
100

BF
AMP
AGN
Lazy
toolbar
MaxSolver
UP
MaxSatz

10
1
0.1

0.01
400 500 600 700 800 900 1000
number of clauses
Max-2SAT - 150 variables
100000
10000
time (log scale)

1000
100
10

BF
AMP
AGN
Lazy
toolbar
MaxSolver
UP
MaxSatz

1
0.1
0.01
0.001
1e-04
300

400
500
600
number of clauses

Figure 11: Experimental results for 50-variable, 100-variable and 150-variable random Max2SAT instances.

347

Li, Manyà & Planes

Max-3SAT - 50 variables
10000

time (log scale)

1000
100
BF
AMP
Lazy
toolbar
MaxSolver
UP
MaxSatz

10
1
0.1
600
800
1000
number of clauses

1200

Max-3SAT - 70 variables

time (log scale)

10000

1000

100
AMP
Lazy
toolbar
MaxSolver
UP
MaxSatz

10

1
500 600 700 800 900 1000
number of clauses
Max-3SAT - 100 variables
100000

time (log scale)

10000
1000
100
10
1
0.1

toolbar
MaxSatz

0.01
500
600
700
number of clauses

800

Figure 12: Experimental results for 50-variable, 70-variable and 100-variable random Max3SAT instances.

348

New Inference Rules for Max-SAT

Graph 3-coloring 24 nodes
10000

time (log scale)

1000
100
10
1

BF
AMP
Lazy
MaxSolver
toolbar
UP
MaxSatz

0.1
0.01
0.001
1e-04

20 30 40 50 60 70 80 90
% of edges
Graph 3-coloring 60 nodes
100000

time (log scale)

10000
1000
100
10

MaxSolver
UP
MaxSatz

1

20 30 40 50 60 70 80 90
% of edges

Figure 14: Experimental results for Graph 3-Coloring

In the fifth experiment, we compared the Max-SAT solvers on the benchmarks submitted
to the Max-SAT Evaluation 2006. Solvers ran in the same conditions as in the evaluation.
In Table 2, the first column is the name of the benchmark set, the second column is the
number of instances of the set, and the rest of columns display the average time, in seconds,
needed by each solver to solve an instance within a time limit of 30 minutes (the number of
instances solved within the time limit in brackets). A dash means that the corresponding
solver cannot solve the set of instances. It is clear that MaxSatz is the best performing
solver for all the sets.

8. Related Work
The simplest method to compute a lower bound consists of just counting the number of
clauses unsatisfied by the current partial assignment (Borchers & Furman, 1999). One step
forward is to incorporate an underestimation of the number of clauses that will become
unsatisfied if the current partial assignment is extended to a complete assignment. The
most basic method was defined by Wallace and Freuder (1996):
349

#Instances
12
7
6
4
2
12
11
4
40
5
45
48
50
50
50
180
50
50

BF
(0)
6.06 (1)
(0)
(0)
(0)
605.44(2)
(0)
(0)
(0)
0.21 (1)
0.02 (21)
8.53 (30)
0.14 (10)
0.08 (10)
1.92 (3)
357.65(28)
170.49(22)
4.07 (16)

AMP
545.81(1)
1.95 (3)
636.04(1)
394.17(2)
197.15(1)
107.79(8)
563.19(1)
428.18(1)
(0)
0.13 (1)
0.03 (45)
38.44(30)
143.23(11)
91.93(12)
514.02(44)
439.54(76)
202.18(50)
168.00(25)

AGN
856.65(8)
32.70(5)
159.99(1)
92.90(2)
39.36(1)
16.11(8)
72.35(2)
909.32(3)
1742.79(3)
12.70(2)
185.69(30)
126.34(28)
6.34 (50)
99.70(108)
-

toolbar
470.23(12)
42.84(5)
145.84(2)
11.07(2)
255.39(2)
235.60(11)
568.09(7)
234.89(3)
736.34(18)
5.72 (2)
35.35(44)
4.14(27)
244.05(34)
262.30(26)
2.01 (50)
178.23(116)
10.19 (50)
361.95(43)

Lazy
159.28 (12)
13.23 (4)
265.35 (2)
13.50 (2)
348.75 (2)
259.33 (10)
956.54 (5)
410.53 (3)
1027.21 (7)
0.05 (1)
278.58 (26)
10.48 (25)
273.44 (22)
217.12 (17)
26.44 (50)
85.08 (87)
69.72 (50)
242.40 (28)

MaxSolver
380.09(2)
41.58(3)
(0)
1.34 (1)
(0)
14.00(8)
283.34(2)
138.32(1)
(0)
570.68(2)
0.06 (45)
0.20 (20)
532.47(16)
168.42(18)
81.82(50)
308.58(73)
66.34(49)
139.03(22)

UP
629.85(9)
7.19 (5)
294.89(2)
29.42(2)
615.54(2)
140.23(9)
812.47(5)
538.10(3)
623.03(13)
0.86 (2)
0.31 (45)
19.65(25)
192.34(48)
75.57(39)
0.94 (50)
166.29(149)
60.50(50)
166.76(37)

Table 2: Experimental results for benchmarks from the MAX-SAT Evaluation 2006.

MaxSatz
14.01(12)
0.07(5)
171.30(3)
44.46(3)
6.82 (2)
16.81(12)
258.65(11)
71.00(4)
7.18(40)
0.14(2)
0.03 (45)
7.78 (34)
1.25 (50)
6.94 (50)
0.02 (50)
22.72(180)
1.92(50)
40.27(50)

350

Li, Manyà & Planes

Set Name
MAXCUT brock
MAXCUT c-fat
MAXCUT hamming
MAXCUT johnson
MAXCUT keller
MAXCUT DIMACS p hat
MAXCUT san
MAXCUT sanr
MAXCUT max cut
MAXCUT SPINGLASS
MAXONE
RAMSEY ram k
MAX2SAT 100VARS
MAX2SAT 140VARS
MAX2SAT 60VARS
MAX2SAT DISCARDED
MAX3SAT 40VARS
MAX3SAT 60VARS

New Inference Rules for Max-SAT

LB(φ) = #emptyClauses(φ) +
x

X

occurs in

min(ic(x), ic(x̄))
φ

where φ is the CNF formula associated with the current partial assignment, and ic(x) (ic(x̄))
—inconsistency count of x (x̄)— is the number of unit clauses of φ that contain x̄ (x).
The underestimation of the lower bound can be improved by applying to binary clauses
the Directional Arc Consistency (DAC) count defined by Wallace (1995) for Max-CSP. The
DAC count of a value of the variable x in φ is the number of variables which are inconsistent
with that value of x. For example, if φ contains clauses x ∨ y, x ∨ ȳ, and x̄ ∨ y, the value
0 of x is inconsistent with y. Note that value 0 of y is also inconsistent with x. These
two inconsistencies are not disjoint and cannot be summed. Wallace defined a direction
from x to y, so that only the inconsistency for value 0 of x is counted. After defining a
direction between every pair of variables sharing a constraint, one computes the DAC count
for all values of x by checking all variables to which a direction from x is defined. The
underestimation considering the DAC count of Wallace is as follows:

x

X

occurs in

(min(ic(x), ic(x̄)) + min(dac(x), dac(x̄))
φ

where dac(x) (dac(x̄)) is the DAC count of the value 1(0) of x. Wallace statically defined
all directions, so that dac(x) and dac(x̄) can be computed in a preprocessing step for every
x and do not need to be recomputed during search. This is improved by Larrosa, Meseguer
and Schiex (1999) by introducing reversible DAC, which searches for better directions to
obtain a better lower bound at every node of the search tree. An improvement of DAC
counts is the additional incorporation of inconsistencies contributed by disjoint subsets of
variables, based on particular variable partitions (Larrosa & Meseguer, 2002).
Inconsistent and DAC counts deal with unit and binary clauses. Lower bounds dealing
with longer clauses include star rule (Shen & Zhang, 2004; Alsinet et al., 2004) and UP (Li
et al., 2005).
In the star rule, the underestimation of the lower bound is the number of disjoint
inconsistent subformulas of the form {l1 , . . . , lk , ¯l1 ∨ · · · ∨ ¯lk }. The star rule, when k = 1, is
equivalent to the inconsistency counts of Wallace and Freuder.
UP subsumes the inconsistent count method based on unit clauses and the star rule. Its
effectiveness for producing a good lower bound can be illustrated with the following example:
let φ be a CNF formula containing the clauses x1 , x̄1 ∨x2 , x̄1 ∨x3 , x̄2 ∨ x̄3 ∨x4 , x5 , x̄5 ∨x6 , x̄5 ∨
x7 , x̄6 ∨ x̄7 ∨ x̄4 . UP easily detects that inconsistent subset with 8 clauses and 7 variables,
in time linear in the size of the formula. Note that this subset is not detected by any of the
lower bounds described above, except for the variable partition based approach of Larrosa
and Meseguer (2002) in the case that the 7 variables are in the same partition.
We mention two more lower bound computation methods. One is called LB4 and was
defined by Shen and Zhang (2004). It is similar to UP but restricted to Max-2SAT instances
and using a static variable ordering. Another is based on linear programming and was
defined by Xing and Zhang (2005).
Regin et al. (2001) suggested to use arc consistency, instead of unit propagation, to detect
disjoint inconsistent subsets of constraints in weighted constraint networks. However, to the
351

Li, Manyà & Planes

best of our knowledge, this idea has not been incorporated in any lower bound computation
method implemented by the Constraint Programming community.
A good lower bound computation method has a dramatic impact on the performance of
a Max-SAT solver. Another approach to speed up a Max-SAT solver consists of applying
inference rules to transform a Max-SAT instance φ into an equivalent but simpler Max-SAT
instance φ′ . Inference rules that have proven to be useful in practice include: (i) the pure
literal rule (Alsinet et al., 2003b; Xing & Zhang, 2004; Li et al., 2005; Zhang et al., 2003);
(ii) the dominating unit clause rule, first proposed by Niedermeier and Rossmanith (2000),
and later applied in several solvers (Alsinet et al., 2004; Xing & Zhang, 2004; Li et al.,
2005); (iii) the almost common clause rule, first proposed by Bansal and Raman (1999) and
restated as Rule 1 in this paper. The rule was extended to weighted Max-SAT byAlsinet
et al. (2004); was called neighborhood resolution by Larrosa and Heras (2005); and used as a
preprocessing technique by Alsinet et al. (2004), Shen and Zhang (2005), and Li et al. (2005);
(iv) the complementary unit clause rule (Niedermeier & Rossmanith, 2000), restated as Rule
2 in this paper; and (v) the coefficient-determining unit propagation rule (Xing & Zhang,
2005) based on integer programming.
The inference rules presented in this paper simplify a Max-SAT formula φ and allow
to improve the lower bound computation, since they all transform a Max-SAT formula φ
into a simpler and equivalent formula containing more empty clauses. Their soundness
(i.e., the fact that they transform a formula into an equivalent one) can be proved in several
ways, including (i) checking all possible variable assignments, (ii) using integer programming
as done in Section 4, and (iii) using soft local consistency techniques defined for Weighted
Constraint Networks (WCN); Max-SAT can be defined as a subcase of WCN where variables
are Boolean and only unit costs are used.
Soft local consistency techniques for WCN are based on two basic equivalence preserving
transformations called projection and extension (Schiex, 2000; Cooper & Schiex, 2004).
Given a Max-SAT instance, projection replaces two binary clauses x ∨ y and x ∨ ȳ with
the unit clause x, which is Rule 1 for k=2. Extension is the inverse operation of projection
and replaces a unit clause x with two binary clauses x ∨ y and x ∨ ȳ for a selected variable
y. If the projection operation is rather straightforward for a SAT or Max-SAT instance,
the extension operation is very ingenious. To see this, note that Rule 3 can be proved or
applied with an extension followed by a projection:
l1 , ¯l1 ∨ ¯l2 , l2 = l1 ∨ l2 , l1 ∨ ¯l2 , ¯l1 ∨ ¯l2 , l2
= l1 ∨ l2 , ¯l2 , l2
= l1 ∨ l2 , 2
Lemma 1 can also be proved using an extension followed by a projection:
l1 , ¯l1 ∨ l2 = l1 ∨ ¯l2 , l1 ∨ l2 , ¯l1 ∨ l2
= l1 ∨ ¯l2 , l2
The extension operation cannot be used in an unguided way because it may cancel a
previous projection. One way to guide its use is to define an ordering between variables to
352

New Inference Rules for Max-SAT

enforce directional arc consistency (Cooper, 2003; Cooper & Schiex, 2004). Directional arc
consistency allows to concentrate weights on the same variables by shifting weights from
earlier variables to later ones in a given ordering. For example if x1 < x2 in a given variable
ordering, one can extend unit clause x1 to x1 ∨ x2 , x1 ∨ x̄2 , but cannot extend unit clause x2
to x1 ∨ x2 , x̄1 ∨ x2 , allowing unit clauses to be concentrated on variable x2 . Nevertheless,
how to define the variable ordering to efficiently exploit as much as possible the power of
soft arc consistency techniques in the lower bound computation remains an open problem.
The approach with inference rules for Max-SAT presented in this paper does not need
any predefined ordering among variables, since rule applications combining several projection and extension operations are entirely guided by unit propagation.
The projection and extension operations can be extended to constraints involving more
than two variables to achieve high-order consistency in WCN (Cooper, 2005). For a MaxSAT instance, the extended projection and extension operations can be stated using Rule 1
for k>2. For the two formulas φ1 and φ2 in Rule 1, replacing φ1 with φ2 is a projection and
φ2 with φ1 is an extension. Given a unit clause x and three variables x, y, z, the extension
of the unit clause x to the set of three variables can be done as follows : replacing x by
x ∨ y and x ∨ ȳ, and then x ∨ y and x ∨ ȳ by x ∨ y ∨ z, x ∨ y ∨ z̄, x ∨ ȳ ∨ z and x ∨ ȳ ∨ z̄.
Rule 5 can be proved or applied by extending the four clauses of φ1 to ternary clauses on
the three variables of l1 , l2 and l3 , and then applying the projection operation to obtain φ2 .
Larrosa et al. (2007), based on a logical approach, independently and in parallel with
our work, defined and implemented a chain resolution rule and a cycle resolution rule for
weighted Max-SAT. These two rules are extensions of Rules 2-RES and 3-RES presented,
also independently and in parallel with our work (Heras & Larrosa, 2006).
The chain resolution could be stated as follows:


(li , mi − mi+1 )1≤i≤k ,

 

 ¯
 (li ∨ li+1 , ui+1 − mi+1 )1≤i<k ,
 
 (l1 , u1 ),
(li ∨ ¯li+1 , mi+1 )1≤i<k ,
(¯li ∨ li+1 , ui+1 )1≤i<k ,
=
 
 ¯

(¯l , u
− mk+1 ),
(lk , uk+1 )


 k k+1
(2, mk+1 )













where, for 1≤i≤k+1, ui is the weight of the corresponding clause, mi =min(u1 , u2 , . . . , ui ),
and all variables in the literals are different. The weight of a mandatory clause is denoted
by ⊤, and the subtraction − is extended so that ⊤ − ui =⊤. The chain resolution rule is
equivalent to Rule 4 if it is applied to unweighted Max-SAT. The main difference between
the chain resolution rule and the weighted version of Rule 4 presented in Section 5.4 is
that the chain resolution shifts a part of the weight from unit clause (l1 , m1 − mk+1 ), that
is derived in the weighted version of Rule 4, to create unit clauses (li , mi − mi+1 )1<i≤k ,
(l1 , m1 − mk+1 ) itself becoming (l1 , m1 − m2 ).
The cycle resolution rule could be stated as follows:
353

Li, Manyà & Planes



(¯li ∨ li+1 , ui )1≤i<k ,
(¯l1 ∨ ¯lk , uk )



=

















(¯l1 ∨ li , mi−1 − mi )2≤i≤k ,
(¯li ∨ li+1 , ui − mi )2≤i<k ,
(¯l1 ∨ li ∨ ¯li+1 , mi )2≤i<k ,
(l1 ∨ ¯li ∨ li+1 , mi )2≤i<k ,
(¯l1 ∨ ¯lk , uk − mk ),
(¯l1 , mk )

















When a subset of binary clauses have a cyclic structure, the cycle resolution rule allows
to derive a unit clause. Note that the detection of the cyclic structure appears rather timeconsuming if it is applied at every node of a search tree and that 2×(k-2) new ternary
clauses have to be inserted. So, Larrosa et al. apply the cycle resolution rule in practice
only for the case k=3, which is similar to Rule 5, when applied to unweighted Max-SAT.
The cycle resolution rule applied to unweighted Max-SAT for k=3 can replace Rule 5 and
Rule 6 in MaxSatz, but with the following differences compared with Rule 5 and Rule 6:
• the application of Rule 5 and Rule 6 is entirely based on inconsistent subformulas
detected by unit propagation. The detection of the applicability of Rule 5 and Rule 6 is
easy and has very low overhead, since the inconsistent subformulas are always detected
in MaxSatz to compute the lower bound (with or without Rule 5 and Rule 6). Every
application of Rule 5 or Rule 6 allows to increment the lower bound by 1.
• the cycle resolution rule needs an extra detection of the cyclic structure, but allows
to derive a unit clause from the cyclic structure. The derived unit clause could then
be used in a unit propagation, and possibly could allow to detect an inconsistent
subformula and increase the lower bound by 1.
It would be an interesting future research topic to implement the cycle resolution rule in
MaxSat1234 (i.e., MaxSatz without Rule 5 and Rule 6) to evaluate the overhead of detecting
the cyclic structure and the usefulness of the unit clauses and the ternary clauses derived
using the cycle resolution rule, and to compare the implemented solver with MaxSatz. It
would be also interesting to compare the chain resolution rule and the cycle resolution rule
with the weighted inference rules presented in Section 5.4.
A more general Max-SAT resolution rule, where the conclusions were not in clausal
form, was defined by Larrosa and Heras (2005). Independently, Bonet et al. (2006, 2007)
and Heras and Larrosa (2006) defined a version of the rule with the conclusions in clausal
form. Bonet et al. (2006, 2007) also proved that this rule is complete for Max-SAT. Recently,
Ansótegui et al. (2007b, 2007a) have shown that Max-SAT resolution for many-valued CNF
formulas provides a logical framework for the global and local consistency properties defined
for WCN.

9. Conclusions and Future Work
One of the main drawbacks of state-of-the-art Max-SAT solvers is the lack of suitable
inference techniques that allow to detect as much contradictions as possible and to simplify
the formula at each node of the search tree. Existing approaches put the emphasis on
computing underestimations of good quality, but the problem with underestimations is
354

New Inference Rules for Max-SAT

that the same contradictions are computed once and again. Furthermore, it turns out
that U P , one of the currently best performing underestimations consisting of detecting
disjoint inconsistent subsets of clauses in a CNF formula via unit propagation, is still too
conservative. To make the computation of lowers more incremental and to improve the
underestimation, we have defined a number of original inference rules for Max-SAT that,
based on derived contradictions by unit propagation, transform a Max-SAT instance into
an equivalent Max-SAT instance which is easier to solve. The rules were carefully selected
taking into account that they should be applied efficiently. Since all these rules are based
on contradiction detection, they should be particularly useful for hard Max-SAT instances
containing many contradictions.
With the aim of finding out how powerful the inference rules are in practice, we have
developed a new Max-SAT solver, called MaxSatz, which incorporates those rules, and
performed an experimental investigation. The results of comparing MaxSatz with inference
rules and MaxSatz without inference rules provide empirical evidence of the usefulness
of these rules in making lower bound computation more incremental and in improving
the quality of lower bounds. The results of comparing MaxSatz with a large selection of
the solvers available at the time of submitting this paper provide empirical evidence that
MaxSatz, at least for the instances solved, is faster than the other solvers. We observed gains
of several orders of magnitude for the hardest instances. Interestingly, for the benchmarks
used, the second best solver was generally different: UP for Max-2SAT, toolbar for Max3SAT, MaxSolver for Max-Cut, and MaxSolver and UP for graph 3-coloring. So, MaxSatz
is more robust than the rest of solvers. It is worth mentioning that MaxSatz, enhanced with
a lower bound based on failed literal detection (Li et al., 2006), was the best performing
solver for unweighted Max-SAT instances in the Max-SAT Evaluation 2006. The second and
third best performing solvers were, respectively, improved versions of toolbar and Lazy11 .
As future work we plan to study the orderings of unit clauses in unit propagation to
maximize the application of inference rules, and to define new inference rules for ternary
clauses. We are extending the results of this paper to weighted Max-SAT, which is more
suitable for modeling problems such as maximum clique, set covering and combinatorial
auctions, as well as constraint satisfaction problems such as hard instances of Model RB (Xu,
Boussemart, Hemery, & Lecoutre, 2005; Xu & Li, 2006). We are also adapting the results
of this paper to the partial Max-SAT solvers developed by Argelich and Manyà (2005, 2006,
2007).

Acknowledgments
Research partially supported by projects TIN2004-07933-C03-03 and TIN2006-15662-C0202 funded by the Ministerio de Educación y Ciencia. The first author was partially supported by National 973 Program of China under Grant No. 2005CB321900. The second
author was supported by a grant Ramón y Cajal. Finally, we would like to thank the referees
for their detailed comments and suggestions.
11. See http://www.iiia.csic.es/˜maxsat06 for details. Note that the results of the Max-SAT Evaluation
2006 can be compared with the results of this paper because they were obtained with the same cluster
under the same conditions.

355

Li, Manyà & Planes

References
Alber, J., Gramm, J., & Niedermeier, R. (2001). Faster exact algorithms for hard problems:
A parameterized point of view. Discrete Mathematics, 229 (1–3), 3–27.
Alsinet, T., Manyà, F., & Planes, J. (2003a). Improved branch and bound algorithms for
Max-2-SAT and weighted Max-2-SAT. In Proceedings of the Catalonian Conference
on Artificial Intelligence (CCIA-03), P. Mallorca, Spain, Vol. 100 of Frontiers in
Artificial Intelligence and Applications, pp. 435–442. IOS Press.
Alsinet, T., Manyà, F., & Planes, J. (2003b). Improved branch and bound algorithms for
Max-SAT. In Proceedings of the 6th International Conference on the Theory and
Applications of Satisfiability Testing (SAT-03), Portofino, Italy, pp. 408–415.
Alsinet, T., Manyà, F., & Planes, J. (2004). A Max-SAT solver with lazy data structures. In Proceedings of the 9th Ibero-American Conference on Artificial Intelligence
(IBERAMIA-04), Puebla, México, LNCS 3315, pp. 334–342. Springer.
Alsinet, T., Manyà, F., & Planes, J. (2005). Improved exact solver for weighted MaxSAT. In Proceedings of the 8th International Conference on Theory and Applications
of Satisfiability Testing (SAT-05), St. Andrews, Scotland, LNCS 3569, pp. 371–377.
Springer.
Ansótegui, C., Bonet, M. L., Levy, J., & Manyà, F. (2007a). Inference rules for high-order
consistency in weighted CSP. In Proceedings of the 22nd National Conference on
Artificial Intelligence (AAAI-07), Vancouver, Canada, pp. 167–172. AAAI Press.
Ansótegui, C., Bonet, M. L., Levy, J., & Manyà, F. (2007b). The logic behind weighted CSP.
In Proceedings of the 20th International Joint Conference on Artificial Intelligence
(IJCAI-07), Hyderabad, India, pp. 32–37. AAAI Press.
Argelich, J., & Manyà, F. (2005). Solving over-constrained problems with SAT technology.
In Proceedings of the 8th International Conference on Theory and Applications of
Satisfiability Testing (SAT-05), St. Andrews, Scotland, LNCS 3569, pp. 1–15. Springer.
Argelich, J., & Manyà, F. (2006). Exact Max-SAT solvers for over-constrained problems.
Journal of Heuristics, 12 (4–5), 375–392.
Argelich, J., & Manyà, F. (2007). Partial Max-SAT solvers with clause learning. In Proceedings of the 10th International Conference on Theory and Applications of Satisfiability
Testing (SAT-07), Lisbon, Portugal, LNCS 4501, pp. 28–40. Springer.
Bansal, N., & Raman, V. (1999). Upper bounds for MaxSat: Further improved. In Proceedings of 10th International Symposium on Algorithms and Computation (ISAAC-99),
Chennai, India, LNCS 1741, pp. 247–260. Springer.
Beame, P., Kautz, H., & Sabharwal, A. (2003). Understanding the power of clause learning.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence
(IJCAI-03), Acapulco, México, pp. 94–99. Morgan Kaufman.
Bonet, M. L., Levy, J., & Manyà, F. (2006). A complete calculus for Max-SAT. In Proceedings of the 9th International Conference on Theory and Applications of Satisfiability
Testing (SAT-06), Seattle, USA, LNCS 4121, pp. 240–251. Springer.
356

New Inference Rules for Max-SAT

Bonet, M. L., Levy, J., & Manyà, F. (2007). Resolution for Max-SAT. Artificial Intelligence,
171, 606–618.
Borchers, B., & Furman, J. (1999). A two-phase exact algorithm for MAX-SAT and weighted
MAX-SAT problems. Journal of Combinatorial Optimization, 2, 299–306.
Boros, E., & Hammer, P. (2002). Pseudo-Boolean optimization. Discrete Applied Mathematics, 123, 155–225.
Cooper, M. C. (2003). Reduction operations in fuzzy or valued constraint satisfaction. Fuzzy
Sets and Systems, 134, 311–342.
Cooper, M. C. (2005). High-order consistency in valued constraint satisfaction. Constraints,
10, 283–305.
Cooper, M. C., & Schiex, T. (2004). Arc consistency for soft constraints. Artificial Intelligence, 154 (1–2), 199–227.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2001). Introduction to Algorithms
(second edition). MIT Press.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem-proving.
Communications of the ACM, 5, 394–397.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. Journal
of the ACM, 7 (3), 201–215.
de Givry, S., Larrosa, J., Meseguer, P., & Schiex, T. (2003). Solving Max-SAT as weighted
CSP. In Proceedings of 9th International Conference on Principles and Practice
of Constraint Programming (CP-03), Kinsale, Ireland, LNCS 2833, pp. 363–376.
Springer.
de Givry, S., Zytnicki, M., Heras, F., & Larrosa, J. (2005). Existential arc consistency: Getting closer to full arc consistency in weighted csps. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05), Edinburgh, Scotland,
pp. 84–89.
Freeman, J. W. (1995). Improvements to Propositional Satisfiability Search Algorithms.
Ph.D. thesis, Department of Computer and Information Science, University of Pennsylvania, PA, USA.
Goldberg, E., & Novikov, Y. (2001). BerkMin: A fast and robust SAT solver. In Proceedings
of Design, Automation and Test in Europe (DATE-02), Paris, France, pp. 142–149.
IEEE Computer Society.
Heras, F., & Larrosa, J. (2006). New inference rules for efficient Max-SAT solving. In Proceedings of the 21st National Conference on Artificial Intelligence (AAAI-06), Boston,
USA. AAAI Press.
Huang, W. Q., & Jin, R. C. (1997). Solar: A learning from human algorithm for solving
SAT. Science in China (Series E), 27 (2), 179–186.
Jeroslow, R. G., & Wang, J. (1990). Solving propositional satisfiability problems. Annals
of Mathematics and Artificial Intelligence, 1, 167–187.
357

Li, Manyà & Planes

Larrosa, J., & Heras, F. (2005). Resolution in Max-SAT and its relation to local consistency
in weighted CSPs. In Proceedings of the 19th International Joint Conference on Artificial Intelligence (IJCAI-05), Edinburgh, Scotland, pp. 193–198. Morgan Kaufmann.
Larrosa, J., Heras, F., & de Givry, S. (2007). A logical approach to efficient Max-SAT
solving. Artificial Intelligence, (in press).
Larrosa, J., & Meseguer, P. (2002). Partition-based lower bound for Max-CSP. Constraints,
7 (3–4), 407–419.
Larrosa, J., Meseguer, P., & Schiex, T. (1999). Maintaining reversible DAC for Max-CSP.
Artificial Intelligence, 107 (1), 149–163.
Li, C. M. (1999). A constraint-based approach to narrow search trees for satisfiability.
Information Processing Letters, 71, 75–80.
Li, C. M., & Anbulagan (1997a). Heuristics based on unit propagation for satisfiability
problems. In Proceedings of 15th the International Joint Conference on Artificial
Intelligence (IJCAI-97), Nagoya, Japan, pp. 366–371. Morgan Kaufmann.
Li, C. M., & Anbulagan (1997b). Look-ahead versus look-back for satisfiability problems.
In Proceedings of the 3rd International Conference on Principles of Constraint Programming (CP-97), Linz, Austria, LNCS 1330, pp. 341–355. Springer.
Li, C. M., & Huang, W. Q. (2005). Diversification and determinism in local search for
satisfiability. In Proceedings of the 8th International Conference on Theory and Applications of Satisfiability Testing (SAT-05), St. Andrews, Scotland, LNCS 3569, pp.
158–172. Springer.
Li, C. M., Manyà, F., & Planes, J. (2005). Exploiting unit propagation to compute lower
bounds in branch and bound Max-SAT solvers. In Proceedings of the 11th International Conference on Principles and Practice of Constraint Programming (CP-05),
Sitges, Spain, LNCS 3709, pp. 403–414. Springer.
Li, C. M., Manyà, F., & Planes, J. (2006). Detecting disjoint inconsistent subformulas for
computing lower bounds for Max-SAT. In Proceedings of the 21st National Conference
on Artificial Intelligence (AAAI-06), Boston, USA, pp. 86–91. AAAI Press.
Marques-Silva, J. P., & Sakallah, K. A. (1999). GRASP: A search algorithm for propositional
satisfiability. IEEE Transactions on Computers, 48 (5), 506–521.
Niedermeier, R., & Rossmanith, P. (2000). New upper bounds for maximum satisfiability.
Journal of Algorithms, 36, 63–88.
Régin, J. C., Petit, T., Bessière, C., & Puget, J. F. (2001). New lower bounds of constraint
violations for over-constrained problems. In 7th International Conference on Principles and Practice of Constraint Programming (CP-01), Paphos, Cyprus, LNCS 2239,
pp. 332–345. Springer.
Schiex, T. (2000). Arc consistency for soft constraints. In Proceedings of the 6th International Conference on Principles of Constraint Programming (CP-00), Singapore,
LNCS 1894, pp. 411–424. Springer.
358

New Inference Rules for Max-SAT

Shen, H., & Zhang, H. (2004). Study of lower bound functions for max-2-sat. In Proceedings
of the National Conference on Artificial Intelligence (AAAI-04), San Jose, USA, pp.
185–190. AAAI Press.
Shen, H., & Zhang, H. (2005). Improving exact algorithms for max-2-sat. Annals of Mathematics and Artificial Intelligence, 44, 419–436.
Wallace, R. J. (1995). Directed arc consistency preprocessing. In Constraint Processing,
Selected Papers, LNCS 923, pp. 121–137. Springer.
Wallace, R. J., & Freuder, E. (1996). Comparative studies of constraint satisfaction and
Davis-Putnam algorithms for maximum satisfiability problems. In Johnson, D., &
Trick, M. (Eds.), Cliques, Coloring and Satisfiability, Vol. 26, pp. 587–615. American
Mathematical Society.
Xing, Z., & Zhang, W. (2004). Efficient strategies for (weighted) maximum satisfiability. In
Proceedings of the 10th International Conference on Principles and Practice of Constraint Programming (CP-04), Toronto, Canada, LNCS 3258, pp. 690–705. Springer.
Xing, Z., & Zhang, W. (2005). An efficient exact algorithm for (weighted) maximum satisfiability. Artificial Intelligence, 164 (2), 47–80.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2005). A simple model to generate
hard satisfiable instances. In Proceedings of 19th International Joint Conference on
Artificial Intelligence (IJCAI-05), Edinburgh, Scotland, pp. 337–342.
Xu, K., & Li, W. (2006). Many hard examples in exact phase transitions. Theoretical
Computer Science, 355, 291–302.
Zhang, H. (1997). SATO: An efficient propositional prover. In Proceedings in the Conference
on Automated Deduction (CADE-97), pp. 272–275.
Zhang, H., Shen, H., & Manyà, F. (2003). Exact algorithms for MAX-SAT. Electronic
Notes in Theoretical Computer Science, 86 (1).
Zhang, L., Madigan, C., Moskewicz, M., & Malik, S. (2001). Efficient conflict driven learning
in a Boolean satisfiability solver. In International Conference on Computer Aided
Design (ICCAD-01), San Jose, USA, pp. 279–285.

359

Journal of Artificial Intelligence Research 30 (2007) 101-132

Submitted 10/05; published 9/07

The Planning Spectrum — One, Two, Three, Infinity
Marco Pistore

pistore@dit.unitn.it

Department of Information and Communication Technology
University of Trento
Via Sommarive 14, 38050 Povo (Trento), Italy

Moshe Y. Vardi

vardi@cs.rice.edu

Department of Computer Science
Rice University
6100 S. Main Street, Houston, Texas

Abstract
Linear Temporal Logic (LTL) is widely used for defining conditions on the execution
paths of dynamic systems. In the case of dynamic systems that allow for nondeterministic
evolutions, one has to specify, along with an LTL formula ϕ, which are the paths that are
required to satisfy the formula. Two extreme cases are the universal interpretation A.ϕ,
which requires that the formula be satisfied for all execution paths, and the existential
interpretation E.ϕ, which requires that the formula be satisfied for some execution path.
When LTL is applied to the definition of goals in planning problems on nondeterministic
domains, these two extreme cases are too restrictive. It is often impossible to develop plans
that achieve the goal in all the nondeterministic evolutions of a system, and it is too weak
to require that the goal is satisfied by some execution.
In this paper we explore alternative interpretations of an LTL formula that are between
these extreme cases. We define a new language that permits an arbitrary combination of
the A and E quantifiers, thus allowing, for instance, to require that each finite execution
can be extended to an execution satisfying an LTL formula (AE.ϕ), or that there is some
finite execution whose extensions all satisfy an LTL formula (EA.ϕ). We show that only
eight of these combinations of path quantifiers are relevant, corresponding to an alternation
of the quantifiers of length one (A and E), two (AE and EA), three (AEA and EAE), and
infinity ((AE)ω and (EA)ω ). We also present a planning algorithm for the new language
that is based on an automata-theoretic approach, and study its complexity.

1. Introduction
In automated task planning (Fikes & Nilsson, 1971; Penberthy & Weld, 1992; Ghallab, Nau,
& Traverso, 2004), given a description of a dynamic domain and of the basic actions that can
be performed on it, and given a goal that defines a success condition to be achieved, one has
to find a suitable plan, that is, a description of the actions to be executed on the domain in
order to achieve the goal. “Classical” planning concentrates on the so called “reachability”
goals, that is, on goals that define a set of final desired states to be reached. Quite often
practical applications require plans that deal with goals that are more general than sets of
final states. Several planning approaches have been recently proposed, where temporal logic
formulas are used as goal language, thus allowing for goals that define conditions on the
whole plan execution paths, i.e., on the sequences of states resulting from the execution of
plans (Bacchus & Kabanza, 1998, 2000; Calvanese, de Giacomo, & Vardi, 2002; Cerrito &
c
2007
AI Access Foundation. All rights reserved.

Pistore & Vardi

Mayer, 1998; Dal Lago, Pistore, & Traverso, 2002; de Giacomo & Vardi, 1999; Kvarnström
& Doherty, 2001; Pistore & Traverso, 2001). Most of these approaches use Linear Temporal
Logic (LTL) (Emerson, 1990) as the goal language. LTL allows one to express reachability
goals (e.g., F q — reach q), maintainability goals (e.g., G q — maintain q), as well as goals
that combine reachability and maintainability requirements (e.g., F G q — reach a set of
states where q can be maintained), and Boolean combinations of these goals.
In planning in nondeterministic domains (Cimatti, Pistore, Roveri, & Traverso, 2003;
Peot & Smith, 1992; Warren, 1976), actions are allowed to have different outcomes, and it is
not possible to know at planning time which of the different possible outcomes will actually
take place. Nondeterminism in action outcome is necessary for modeling in a realistic way
several practical domains, ranging from robotics to autonomous controllers to two-player
games.1 For instance, in a realistic robotic application one has to take into account that
actions like “pick up object” might result in a failure (e.g., if the object slips out of the
robot’s hand). A consequence of nondeterminism is that the execution of a plan may lead to
more than one possible execution path. Therefore, one has to distinguish whether a given
goal has to be satisfied by all the possible execution paths (in this case we speak of “strong”
planning), or only by some of the possible execution paths (“weak” planning). In the case
of an LTL goal ϕ, strong planning corresponds to interpreting the formula in a universal
way, as A.ϕ, while weak planning corresponds to interpreting it in an existential way, as
E.ϕ.
Weak and strong plans are two extreme ways of satisfying an LTL formula. In nondeterministic planning domains, it might be impossible to achieve goals in a strong way: for
instance, in the robotic application it might be impossible to fulfill a given task if objects
keep slipping from the robot’s hand. On the other hand, weak plans are too unreliable,
since they achieve the goal only under overly optimistic assumptions on the outcomes of
action executions.
In the case of reachability goals, strong cyclic planning (Cimatti et al., 2003; Daniele,
Traverso, & Vardi, 1999) has been shown to provide a viable compromise between weak and
strong planning. Formally, a plan is strong cyclic if each possible partial execution of the
plan can always be extended to an execution that reaches some goal state. Strong cyclic
planning allows for plans that encode iterative trial-and-error strategies, like “pick up an
object until succeed”. The execution of such strategies may loop forever only in the case
the action “pick up object” continuously fails, and a failure in achieving the goal for such
an unfair execution is usually acceptable. Branching-time logics like CTL and CTL* allow
for expressing goals that take into account nondeterminism. Indeed, Daniele et al. (1999)
show how to encode strong cyclic reachability goals as CTL formulas. However, in CTL
and CTL* path quantifiers are interleaved with temporal operators, making it difficult to
extend the encoding of strong cyclic planning proposed by Daniele et al. (1999) to generic
temporal goals.
In this paper we define a new logic that allows for exploring the different degrees in which
an LTL formula ϕ can be satisfied that exist between the strong goal A.ϕ and the weak goal
E.ϕ. We consider logic formulas of the form α.ϕ, where ϕ is an LTL formula and α is a
path quantifier that generalizes the A and E quantifiers used for strong and weak planning.
1. See the work of Ghallab et al. (2004) for a deeper discussion on the fundamental role of nondeterminism
in planning problems and in practical applications.

102

The Planning Spectrum — One, Two, Three, Infinity

A path quantifier is a (finite or infinite) word on alphabet {A, E}. The path quantifier can
be seen as the definition of a two-player game for the selection of the outcome of action
execution. Player A (corresponding to symbol A) chooses the action outcomes in order to
make goal ϕ fail, while player E (corresponding to symbol E) chooses the action outcomes
in order to satisfy the goal ϕ. At each turn, the active player controls the outcome of action
execution for a finite number of actions and then passes the control to the other player.2
We say that a plan satisfies the goal α.ϕ if the player E has a winning strategy, namely if,
for all the possible moves of player A, player E is always able to build an execution path
that satisfies the LTL formula ϕ.
Different path quantifiers define different alternations in the turns of players A and E.
For instance, with goal A.ϕ we require that the formula ϕ is satisfied independently of how
the “hostile” player A chooses the outcomes of actions, that is, we ask for a strong plan.
With goal E.ϕ we require that the formula ϕ is satisfied for some action outcomes chosen
by the “friendly” player E, that is, we ask for a weak plan. With goal AE.ϕ we require that
every plan execution led by player A can be extended by player E to a successful execution
that satisfies the formula ϕ; in the case of a reachability goal, this corresponds to asking
for a strong cyclic solution. With goal EA.ϕ we require that, after an initial set of actions
controlled by player E, we have the guarantee that formula ϕ will be satisfied independently
of how player A will choose the outcome of the following actions. As a final example, with
goal (AE)ω .ϕ = AEAEA · · · .ϕ we require that formula ϕ is satisfied in all those executions
where player E has the possibility of controlling the action outcome an infinite number of
times.
Path quantifiers can define arbitrary combinations of the turns of players A and E, and
hence different degrees in satisfying an LTL goal. We show, however, that, rather surprisingly, only a finite number of alternatives exist between strong and weak planning: only
eight “canonical” path quantifiers give rise to plans of different strength, and every other
path quantifier is equivalent to a canonical one. The canonical path quantifiers correspond
to the games of length one (A and E), two (AE and EA), and three (AEA and EAE), and
to the games defining an infinite alternation between players A and E ((AE)ω and (EA)ω ).
We also show that, in the case of reachability goals ϕ = F q, the canonical path quantifiers
further collapse. Only three different degrees of solution are possible, corresponding to weak
(E. F q), strong (A. F q), and strong cyclic (AE. F q) planning.
Finally, we present a planning algorithm for the new goal language and we study its
complexity. The algorithm is based on an automata-theoretic approach (Emerson & Jutla,
1988; Kupferman, Vardi, & Wolper, 2000): planning domains and goals are represented
as suitable automata, and planning is reduced to the problem of checking whether a given
automaton is nonempty. The proposed algorithm has a time complexity that is doubly
exponential in the size of the goal formula. It is known that the planning problem is
2EXPTIME-complete for goals of the form A.ϕ (Pnueli & Rosner, 1990), and hence the
complexity of our algorithm is optimal.
The structure of the paper is as follows. In Section 2 we present some preliminaries
on automata theory and on temporal logics. In Section 3 we define planning domains and
plans. In Section 4 we define AE-LTL, our new logic of path quantifier, and study its basic
2. If the path quantifier is a finite word, the player that has the last turn chooses the action outcome for
the rest of the infinite execution.

103

Pistore & Vardi

properties. In Section 5 we present a planning algorithm for AE-LTL, while in Section 6
we apply the new logic to the particular cases of reachability and maintainability goals. In
Section 7 we make comparisons with related works and present some concluding remarks.

2. Preliminaries
This section introduces some preliminaries on automata theory and on temporal logics.
2.1 Automata Theory
Given a nonempty alphabet Σ, an infinite word on Σ is an infinite sequence σ0 , σ1 , σ2 , . . . of
symbols from Σ. Finite state automata have been proposed as finite structures that accept
sets of infinite words. In this paper, we are interested in tree automata, namely in finite
state automata that recognize trees on alphabet Σ, rather than words.
Definition 1 (tree) A (leafless) tree τ is a subset of N∗ such that:
•  ∈ τ is the root of the tree;
• if x ∈ τ then there is some i ∈ N such that x · i ∈ τ ;
• if x · i ∈ τ , with x ∈ N∗ and i ∈ N, then also x ∈ τ ;
• if x · (i+1) ∈ τ , with x ∈ N∗ and i ∈ N, then also x · i ∈ τ .
The arity of x ∈ τ is the number of its children, namely arity(x) = |{i : x · i ∈ τ }|. Let
D ⊆ N. Tree τ is a D-tree if arity(x) ∈ D for each x ∈ τ . A Σ-labelled tree is a pair (τ, τ ),
where τ is a tree and τ : τ → Σ. In the following, we will denote Σ-labelled tree (τ, τ ) as
τ , and let τ = dom(τ ).
Let τ be a Σ-labelled tree. A path p of τ is a (possibly infinite) sequence x0 , x1 , . . . of nodes
xi ∈ dom(τ ) such that xk+1 = xk · ik+1 . In the following, we denote with P ∗ (τ ) the set of
finite paths and with P ω (τ ) the set of infinite paths of τ . Given a (finite or infinite) path p,
we denote with τ (p) the string τ (x0 ) · τ (x1 ) · · · , where x0 , x1 , . . . is the sequence of nodes
of path p. We say that a finite (resp. infinite) path p0 is a finite (resp. infinite) extension of
the finite path p if the sequence of nodes of p is a prefix of the sequence of nodes of p0 .
A tree automaton is an automaton that accepts sets of trees. In this paper, we consider
a particular family of tree automata, namely parity tree automata (Emerson & Jutla, 1991).
Definition 2 (parity tree automata) A parity tree automaton with parity index k is a
tuple A = hΣ, D, Q, q0 , δ, βi, where:
• Σ is the finite, nonempty alphabet;
• D ⊆ N is a finite set of arities;
• Q is the finite set of states;
• q0 ∈ Q is the initial state;
104

The Planning Spectrum — One, Two, Three, Infinity

∗

d

• δ : Q × Σ × D → 2Q is the transition function, where δ(q, σ, d) ∈ 2Q ;
• β : Q → {0, . . . , k} is the parity mapping.
A tree automaton accepts a tree if there is an accepting run of the automaton on the tree.
Intuitively, when a parity tree automaton is in state q and it is reading a d-ary node of the
tree that is labeled by σ, it nondeterministically chooses a d-tuple hq1 , . . . , qd i in δ(q, σ, d)
and then makes d copies of itself, one for each child node of the tree, with the state of the
i-th copy updated to qi . A run of the parity tree automaton is accepting if, along every
infinite path, the minimal priority that is visited infinitely often is an even number.
Definition 3 (tree acceptance) The parity tree automaton A = hΣ, D, Q, q0 , δ, βi accepts the Σ-labelled D-tree τ if there exists an accepting run r for τ , namely there exists a
mapping r : τ → Q such that:
• r() = q0 ;
• for each x ∈ τ with arity(x) = d we have hr(x · 0), . . . r(x · (d−1))i ∈ δ(r(x), τ (x), d);
• along every infinite path x0 , x1 , . . . in
for infinitely many nodes xi is even.

τ , the minimal integer h such that β(r(xi )) = h

The tree automaton A is nonempty if there exists some tree

τ

that is accepted by A.

Emerson and Jutla (1991) have shown that the emptiness of a parity tree automaton can
be decided in a time that is exponential in the parity index and polynomial in the number
of states.
Theorem 1 The emptiness of a parity tree automaton with n states and index k can be
determined in time nO(k) .
2.2 Temporal Logics
Formulas of Linear Temporal Logic (LTL) (Emerson, 1990) are built on top of a set Prop
of atomic propositions using the standard Boolean operators, the unary temporal operator
X (next), and the binary temporal operator U (until). In the following we assume to have
a fixed set of atomic propositions Prop, and we define Σ = 2Prop as the set of subsets of
Prop.
Definition 4 (LTL) LTL formulas ϕ on Prop are defined by the following grammar, where
q ∈ Prop:
ϕ ::= q | ¬ϕ | ϕ ∧ ϕ | X ϕ | ϕ U ϕ
We define the following auxiliary operators: F ϕ = > U ϕ (eventually in the future ϕ) and
G ϕ = ¬ F ¬ϕ (always in the future ϕ). LTL formulas are interpreted over infinite words
on Σ. In the following, we write w |=LTL ϕ whenever the infinite word w satisfies the LTL
formula ϕ.
Definition 5 (LTL semantics) Let w = σ0 , σ1 , . . . be an infinite word on Σ and let ϕ be
an LTL formula. We define w, i |=LTL ϕ, with i ∈ N, as follows:
105

Pistore & Vardi

• w, i |=LTL q iff q ∈ σi ;
• w, i |=LTL ¬ϕ iff it does not hold that w, i |=LTL ϕ;
• w, i |=LTL ϕ ∧ ϕ0 iff w, i |=LTL ϕ and w, i |=LTL ϕ0 ;
• w, i |=LTL X ϕ iff w, i+1 |=LTL ϕ;
• w, i |=LTL ϕ U ϕ0 iff there is some j ≥ i such that w, k |=LTL ϕ for all i ≤ k < j and
w, j |=LTL ϕ0 .
We say that w satisfies ϕ, written w |=LTL ϕ, if w, 0 |=LTL ϕ.
CTL* (Emerson, 1990) is an example of “branching-time” logic. Path quantifiers A
(“for all paths”) and E (“for some path”) can prefix arbitrary combinations of linear time
operators.
Definition 6 (CTL*) CTL* formulas ψ on Prop are defined by the following grammar,
where q ∈ Prop:
ψ ::= q | ¬ψ | ψ ∧ ψ | A ϕ | E ϕ
ϕ ::= ψ | ¬ϕ | ϕ ∧ ϕ | X ϕ | ϕ U ϕ
CTL* formulas are interpreted over Σ-labelled trees. In the following, we write
whenever τ satisfies the CTL* formula ψ.

τ

|=CTL* ψ

Definition 7 (CTL* semantics) Let τ be a Σ-labelled tree and let ψ be a CTL* formula.
We define τ , x |=CTL* ψ, with x ∈ τ , as follows:
•

τ , x |=CTL* q iff q ∈ τ (x);

•

τ , x |=CTL* ¬ψ iff it does not hold that τ , x |=CTL* ψ;

•

τ , x |=CTL* ψ ∧ ψ0

•

τ , x |=CTL* A ϕ iff τ , p |=CTL* ϕ holds for all infinite paths p = x0 , x1 , . . . with x0 = x;

•

τ,x

where

iff

|=CTL* E ϕ iff
x0 = x;

τ , x |=CTL* ψ and τ , x |=CTL* ψ0 ;

τ,p

|=CTL* ϕ holds for some infinite path p = x0 , x1 , . . . with

τ , p |=CTL* φ, with p ∈ P ω (τ ), is defined as follows:

•

τ , p |=CTL* ψ iff p = x0 , x1 , . . . and τ , x0 |=CTL* ψ;

•

τ , p |=CTL* ¬ϕ iff it does not hold that τ , p |=CTL* ϕ;

•

τ , p |=CTL* ϕ ∧ ϕ0

•

τ , p |=CTL* X ϕ iff τ , p0 |=CTL* ϕ, where p0 = x1 , x2 , . . . if p = x0 , x1 , x2 , . . .;

•

τ , p |=CTL* ϕ U ϕ0 iff there is some j ≥ 0 such that τ , pk |=CTL* ϕ for all 0 ≤ k < j
and τ , pj |=CTL* ϕ0 , where pi = xi , xi+1 , . . . if p = x0 , x1 , . . ..

iff

τ , p |=CTL* ϕ and τ , p |=CTL* ϕ0 ;

106

The Planning Spectrum — One, Two, Three, Infinity

put_B_on_A
A

B

C

B
A

put_C_on_B
C

C
B
A

Figure 1: A possible scenario in the blocks-world domain.
We say that

τ

satisfies the CTL* formula ψ, written

τ

|=CTL* ψ, if

τ ,  |=CTL* ψ.

The following theorem states that it is possible to build a tree automaton that accepts
all the trees satisfying a CTL* formula. The tree automaton has a number of states that is
doubly exponential and a parity index that is exponential in the length of the formula. A
proof of this theorem has been given by Emerson and Jutla (1988).
Theorem 2 Let ψ be a CTL* formula, and let D ⊆ N∗ be a finite set of arities. One can
build a parity tree automaton AD
ψ that accepts exactly the Σ-labelled D-trees that satisfy ψ.
2
The automaton AD
ψ has 2
formula ψ.

O(|ψ|)

states and parity index 2O(|ψ|) , where |ψ| is the length of

3. Planning Domains and Plans
A (nondeterministic) planning domain (Cimatti et al., 2003) can be expressed in terms of a
set of states, one of which is designated as the initial state, a set of actions, and a transition
function describing how (the execution of) an action leads from one state to possibly many
different states.
Definition 8 (planning domain) A planning domain is a tuple D = hΣ, σ0 , A, Ri where:
• Σ is the finite set of states;
• σ0 ∈ Σ is the initial state;
• A is the finite set of actions;
• R : Σ × A → 2Σ is the transition relation.
We require that for each σ ∈ Σ there is some a ∈ A and some σ 0 ∈ Σ such that σ 0 ∈ R(σ, a).
We assume that states Σ are ordered, and we write R(σ, a) = hσ1 , σ2 , . . . , σn i whenever
R(σ, a) = {σ1 , σ2 , . . . , σn } and σ1 < σ2 < · · · < σn .
Example 1 Consider a blocks-world domain consisting of a set of blocks, which are initially
on a table, and which can be stacked on top of each other in order to build towers (see
Figure 1).
The states Σ of this domain are the possible configurations of the blocks: in the case of
three blocks there are 13 states, corresponding to all the blocks on the table (1 configuration),
a 2-block tower and the remaining block on the table (6 configurations), and a 3-block tower
(6 possible configurations). We assume that initially all blocks are on the table.
107

Pistore & Vardi

The actions in this domain are put X on Y , put X on table, and wait, where X and
Y are two (different) blocks. Actions put X on Y and put X on table are possible only if
there are no blocks on top of X (otherwise we could not pick up X). In addition, action
put X on Y requires that there are no blocks on top of Y (otherwise we could not put X
on top of Y ).
We assume that the outcome of action put X on Y is nondeterministic: indeed, trying
to put a block on top of a tower may fail, in which case the tower is destroyed. Also action
wait is nondeterministic: it is possible that the table is bumped and that all its towers are
destroyed.
A plan guides the evolution of a planning domain by issuing actions to be executed.
In the case of nondeterministic domains, conditional plans (Cimatti et al., 2003; Pistore &
Traverso, 2001) are required, that is, the next action issued by the plan may depend on
the outcome of the previous actions. Here we consider a very general definition of plans: a
plan is a mapping from a sequence of states, representing the past history of the domain
evolution, to an action to be executed.
Definition 9 (plan) A plan is a partial function π : Σ+ * A such that:
• if π(w · σ) = a, then σ 0 ∈ R(σ, a) for some σ 0 ;
• if π(w · σ) = a, then σ 0 ∈ R(σ, a) iff w · σ · σ 0 ∈ dom(π);
• if w · σ ∈ dom(π) with w 6= , then w ∈ dom(π);
• π(σ) is defined iff σ = σ0 is the initial state of the domain.
The conditions in the previous definition ensure that a plan defines an action to be executed
for exactly the finite paths w ∈ Σ+ that can be reached executing the plan from the initial
state of the domain.
Example 2 A possible plan for the blocks-world domain of Example 1 is represented in Figure 2. We remark the importance of having plans in which the action to be executed depends
on the whole sequence of states corresponding to the past history of the evolution. Indeed,
according to the plan if Figure 2, two different actions put C on A and put C on table are
performed in the state with block B on top of A, depending on the past history.
Since we consider nondeterministic planning domains, the execution of an action may
lead to different outcomes. Therefore, the execution of a plan on a planning domain can be
described as a (Σ×A)-labelled tree. Component Σ of the label of the tree corresponds to
a state in the planning domain, while component A describes the action to be executed in
that state.
Definition 10 (execution tree) The execution tree for domain D and plan π is the
(Σ×A)-labelled tree τ defined as follows:
•

τ () = (σ0 , a0 ) where σ0

is the initial state of the domain and a0 = π(σ0 );
108

The Planning Spectrum — One, Two, Three, Infinity

w

π(w)

ABC

put B on A

B
ABC · AC

put C on B

B
ABC · AC

C
B
· A

B
ABC · AC

C
B
· A

B
· AC

B
ABC · AC

C
B
· A

B
· AC

put C on table

any other history

put B on table

· ABC

wait
wait

Figure 2: A plan for the blocks-world domain.
• if p = x0 , . . . , xn ∈ P ∗ (τ ) with τ (p) = (σ0 , a0 ) · (σ1 , a1 ) · · · (σn , an ), and if R(σn , an ) =
0
hσ00 , . . . , σd−1
i, then for every 0 ≤ i < d the following conditions hold: xn · i ∈ dom(τ )
and τ (xn · i) = (σi0 , a0i ) with a0i = π(σ0 · σ1 · · · σn · σi0 ).
A planning problem consists of a planning domain and of a goal g that defines the set
of desired behaviors. In the following, we assume that the goal g defines a set of execution
trees, namely the execution trees that exhibit the behaviors described by the goal (we say
that these execution trees satisfy the goal).
Definition 11 (planning problem) A planning problem is a pair (D, g), where D is a
planning domain and g is a goal. A solution to a planning problem (D, g) is a plan π such
that the execution tree for π satisfies the goal g.

4. A Logic of Path Quantifiers
In this section we define a new logic that is based on LTL and that extends it with the
possibility of defining conditions on the sets of paths that satisfy the LTL property. We
start by motivating why such a logic is necessary for defining planning goals.
Example 3 Consider the blocks-world domain introduced in the previous section. Intuitively, the plan of Example 2 is a solution to the goal of building a tower consisting of
blocks A, B, C and then of destroying it. This goal can be easily formulated as an LTL
109

Pistore & Vardi

formula:
ϕ1 = F ((C on B ∧ B on A ∧ A on table) ∧ F (C on table ∧ B on table ∧ A on table)).
Notice however that, due to the nondeterminism in the outcome of actions, this plan may
fail to satisfy the goal. It is possible, for instance, that action put C on B fails and the
tower is destroyed. In this case, the plan proceeds performing wait actions, and hence the
tower is never finished. Formally, the plan is a solution to the goal which requires that there
is some path in the execution structure that satisfies the LTL formula ϕ1 .
Clearly, there are better ways to achieve the goal of building a tower and then destroying
it: if we fail building the tower, rather than giving up, we can restart building it and keep
trying until we succeed. This strategy allows for achieving the goal in “most of the paths”:
only if we keep destroying the tower when we try to build it we will not achieve the goal. As
we will see, the logic of path quantifiers that we are going to define will allow us to formalize
what we mean by “most of the paths”.
Consider now the following LTL formula:
ϕ2 = F G ((C on B ∧ B on A ∧ A on table).
The formula requires building a tower and maintaining it. In this case we have two possible
ways to fail to achieve the goal. We can fail to build the tower; or, once built, we can fail to
maintain it (remember that a wait action may nondeterministically lead to a destruction of
the tower). Similarly to the case of formula φ1 , a planning goal that requires satisfying the
formula φ2 in all paths of the execution tree is unsatisfiable. On the other hand, a goal that
requires satisfying it on some paths is very weak; our logic allows us to be more demanding
on the paths that satisfy the formula.
Finally, consider the following LTL formula:
ϕ3 = G F ((C on B ∧ B on A ∧ A on table).
It requires that the tower exists infinitely many time, i.e., if the tower gets destroyed, then
we have to rebuild it. Intuitively, this goal admits plans that can achieve it more often, i.e.,
on “more paths”, than ϕ2 . Once again, a path logic is needed to give a formal meaning to
“more paths”.
In order to be able to represent the planning goals discussed in the previous example,
we consider logic formulas of the form α.ϕ, where ϕ is an LTL formula and α is a path
quantifier and defines a set of infinite paths on which the formula ϕ should be checked. Two
extreme cases are the path quantifier A, which is used to denote that ϕ must hold on all the
paths, and the path quantifier E, which is used to denote that ϕ must hold on some paths.
In general, a path quantifier is a (finite or infinite) word on alphabet {A, E} and defines an
alternation in the selection of the two modalities corresponding to E and A. For instance,
by writing AE.ϕ we require that all finite paths have some infinite extension that satisfies
ϕ, while by writing EA.ϕ we require that all the extensions of some finite path satisfy ϕ.
The path quantifier can be seen as the definition of a two-player game for the selection of
the paths that should satisfy the LTL formula. Player A (corresponding to A) tries to build
a path that does not satisfy the LTL formula, while player E (corresponding to E) tries to
110

The Planning Spectrum — One, Two, Three, Infinity

build the path so that the LTL formula holds. Different path quantifiers define different
alternations in the turns of players A and E. The game starts from the path consisting only
of the initial state, and, during their turns, players A and E extend the path by a finite
number of nodes. In the case the path quantifier is a finite word, the player that moves last
in the game extends the finite path built so far to an infinite path. The formula is satisfied
if player E has a winning strategy, namely if, for all the possible moves of the player A, it
is always able to build a path that satisfies the LTL formula.
Example 4 Let us consider the three LTL formulas defined in Example 3, and let us see
how the path quantifiers we just introduced can be applied.
In the case of formula ϕ1 , the plan presented in Example 2 satisfies requirement E.ϕ1 :
there is a path on which the tower is built and then destroyed. It also satisfies the “stronger”
requirement EA.ϕ1 that stresses the fact that, in this case, once the tower has been built and
destroyed, we can safely give the control to player A. Formula ϕ1 can be satisfied in a
stronger way, however. Indeed, the plan that keeps trying to build the tower satisfies the
requirement AE.ϕ1 , as well as the requirement AEA.ϕ1 : player A cannot reach a state where
the satisfaction of the goal is prevented.
Let us now consider the formula ϕ2 . In this case, we can find plans satisfying AE.ϕ2 ,
but no plan can satisfy requirement AEA.ϕ2 . Indeed, player A has a simple strategy to win,
if he gets the control after we built the tower: bump the table. Similar considerations hold
also for formula ϕ3 . Also in this case, we can find plans for requirement AE.ϕ3 , but not for
requirement AEA.ϕ3 . In this case, however, plans exist also for requirement AEAEAE · · · .ϕ3 :
if player E gets the control infinitely often, then it can rebuild the tower if needed.
In the rest of the section we give a formal definition and study the basic properties of
this logic of path quantifiers.
4.1 Finite Games
We start considering only games with a finite number of moves, that is path quantifiers
corresponding to finite words on {A, E}.
Definition 12 (AE-LTL) An AE-LTL formula is a pair g = α.ϕ, where ϕ is an LTL
formula and α ∈ {A, E}+ is a path quantifier.
The following definition describes the games corresponding to the finite path quantifiers.
Definition 13 (semantics of AE-LTL) Let p be a finite path of a Σ-labelled tree
Then:
• p |= Aα.ϕ if for all finite extensions p0 of p it holds that p0 |= α.ϕ.
• p |= Eα.ϕ if for some finite extension p0 of p it holds that p0 |= α.ϕ.
• p |= A.ϕ if for all infinite extensions p0 of p it holds that

τ (p0 ) |=LTL ϕ.

• p |= E.ϕ if for some infinite extension p0 of p it holds that
111

τ (p0 ) |=LTL ϕ.

τ.

Pistore & Vardi

We say that the Σ-labelled tree τ satisfies the AE-LTL formula g, and we write
p0 |= g, where p0 =  is the root of τ .

τ

|= g, if

AE-LTL allows for path quantifiers consisting of an arbitrary combination of As and
Es. Each combination corresponds to a different set of rules for the game between A and
E. In Theorem 4 we show that all this freedom in the definition of the path quantifier is
not needed. Only six path quantifiers are sufficient to capture all the possible games. This
result is based on the concept of equivalent path quantifiers.
Consider formulas A. F p and AE. F p. It is easy to see that the two formulas are equisatisfiable, i.e., if a tree τ satisfies A. F p then it also satisfies AE. F p, and vice-versa. In
this case, path quantifiers A and AE have the same “power”, but this depends on the fact
that we use the path quantifiers in combination with the LTL formula F p. If we combine
the two path quantifiers with different LTL formulas, such as G p, it is possible to find
trees that satisfy the latter path quantifier but not the former. For this reason, we cannot
consider the two path quantifiers equivalent. Indeed, in order for two path quantifiers to
be equivalent, they have to be equi-satisfiable for all the LTL formulas. This intuition is
formalized in the following definition.
Definition 14 (equivalent path quantifiers) Let α and α0 be two path quantifiers. We
say that α implies α0 , written α
α0 , if for all Σ-labelled trees τ and for all LTL formulas
0
ϕ, τ |= α.ϕ implies τ |= α .ϕ. We say that α is equivalent to α0 , written α ∼ α0 , if α
α0
0
and α
α.
The following lemma describes some basic properties of path quantifiers and of the
equivalences among them. We will exploit these results in the proof of Theorem 4.
Lemma 3 Let α, α0 ∈ {A, E}∗ . The following implications and equivalences hold.
1. αAAα0 ∼ αAα0 and αEEα0 ∼ αEα0 .
2. αAα0

αα0 and αα0

αEα0 , if αα0 is not empty.

3. αAα0

αAEAα0 and αEAEα0

αEα0 .

4. αAEAEα0 ∼ αAEα0 and αEAEAα0 ∼ αEAα0 .
Proof. In the proof of this lemma, in order to prove that αα0
αα00 we prove that, given
0
an arbitrary tree τ and an arbitrary LTL formula ϕ, p |= α .ϕ implies p |= α00 .ϕ for every
finite path p of τ . Indeed, if p |= α0 .ϕ implies p |= α00 .ϕ for all finite paths p, then it is easy
to prove, by induction on α, that p |= αα0 .ϕ implies p |= αα00 .ϕ for all finite paths p. In the
following, we will refer to this proof technique as prefix induction.
1. We show that, for every finite path p, p |= AAα0 .ϕ if and only if p |= Aα0 .ϕ: then the
equivalence of αAAα0 and αAα0 follows by prefix induction.
Let us assume that p |= AAα0 .ϕ. We prove that p |= Aα0 .ϕ, that is, that p0 |= α0 .ϕ
for every finite3 extension p0 of p. Since p |= AAα0 .ϕ, by Definition 13 we know that,
3. We assume that α0 is not the empty word. The proof in the case α0 is the empty word is similar.

112

The Planning Spectrum — One, Two, Three, Infinity

for every finite extension p0 of p, p0 |= Aα0 .ϕ. Hence, again by Definition 13, we know
that for every finite extension p00 of p0 , p00 |= α0 .ϕ. Since p0 is a finite extension of p0 ,
we can conclude that p0 |= α0 .ϕ. Therefore, p0 |= α0 .ϕ holds for all finite extensions p0
of p.
Let us now assume that p |= Aα0 .ϕ. We prove that p |= AAα0 .ϕ, that is, for all finite
extensions p0 of p, and for all finite extensions p00 of p0 , p00 |= α0 .ϕ. We remark that
the finite path p00 is also a finite extension of p, and therefore p00 |= α0 .ϕ holds since
p |= Aα0 .ϕ.
This concludes the proof of the equivalence of αAAα0 and αAα0 . The proof of the
equivalence of αEEα0 and αEα0 is similar.
2. Let us assume first that α0 is not an empty word. We distinguish two cases, depending
on the first symbol of α0 . If α0 = Aα00 , then we should prove that αAAα00
αAα00 ,
0
00
which we already did in item 1 of this lemma. If α = Eα , then we show that, for
every finite path p, if p |= AEα00 .ϕ then p |= Eα00 .ϕ: then αAα0
αα0 follows by
00
prefix induction. Let us assume that p |= AEα .ϕ. Then, for all finite extensions p0 of
p there exists some finite4 extension p00 of p0 such that p00 |= α0 .ϕ. Let us take p0 = p.
Then we know that there is some finite extension p00 of p such that p00 |= α0 .ϕ, that is,
according to Definition 13, p |= Eα0 .ϕ.
Let us now assume that α0 is the empty word. By hypothesis, αα0 6= , so α is not
empty. We distinguish two cases, depending on the last symbol of α. If α = α00A, then
we should prove that α00AA
α00A, which we already did in item 1 of this lemma.
If α = α00 E, then we prove that for every finite path p, if p |= EA.ϕ then p |= E.ϕ:
then α00 EA
α00 E follows by prefix induction. Let us assume that p |= EA.ϕ. By
Definition 13, there exists some finite extension p0 of p such that, for every infinite
extension p00 of p0 we have τ (p00 ) |=LTL ϕ. Let p00 be any infinite extension of p0 . We
know that p00 is also an infinite extension of p, and that τ (p00 ) |=LTL ϕ. Then, by
Definition 13 we deduce that p |= E.ϕ.
This concludes the proof that αAα0

αα0 . The proof that αα0

αEα0 is similar.

3. By item 1 of this lemma we know that αAα0
αAAα0 and by item 2 we know that
0
0
αAAα
αAEAα . This concludes the proof that αAα0
αAEAα0 . The proof that
αEAEα0
αEα0 is similar.
4. By item 3 of this lemma we know that (αA)EAEα0
(αA)Eα0 . Moreover, again
0
0
by item 3, we know that αA(Eα )
αAEA(Eα ). Therefore, we deduce αAEα0 ∼
0
0
αAEAEα . The proof that αEAα ∼ αEAEAα0 is similar.

We can now prove the first main result of the paper: each finite path quantifier is
equivalent to a canonical path quantifier of length at most three.
Theorem 4 For each finite path quantifier α there is a canonical finite path quantifier
α0 ∈ {A, E,AE, EA,AEA, EAE}
4. We assume that α00 is not the empty word. The proof in the case where α00 is empty is similar.

113

Pistore & Vardi

such that α ∼ α0 . Moreover, the following implications hold between the canonical finite
path quantifiers:
(1)
A /o /o /o / AEA o/ /o /o / AE
O

O

O

O

O

o
/
o
/
/
/
o
EA
EAE /o /o o/ / E


O

Proof. We first prove that each path quantifier α is equivalent to some canonical path
quantifier α0 . By an iterative application of Lemma 3(1), we obtain from α a path quantifier
α00 such that α ∼ α00 and α00 does not contain two adjacent A or E. Then, by an iterative
application of Lemma 3(4), we can transform α00 into an equivalent path quantifier α0 of
length at most 3. The canonical path quantifiers in (1) are precisely those quantifiers of
length at most 3 that do not contain two adjacent A or E.
For the implications in (1):
•A

AEA and EAE

E come from Lemma 3(3);

• AEA

EA and AE

EAE come from Lemma 3(2);

• AEA

AE and EA

EAE come from Lemma 3(2).



We remark that Lemma 3 and Theorem 4 do not depend on the usage of LTL for formula
ϕ. They depend on the general observation that α
α0 whenever player E can select for
game α0 a set of paths which is a subset of those selected for game α.
4.2 Infinite Games
We now consider infinite games, namely path quantifiers consisting of infinite words on
alphabet {A, E}. We will see that infinite games can express all the finite path quantifiers
that we have studied in the previous subsection, but that there are some infinite games, corresponding to an infinite alternation of the two players A and E, which cannot be expressed
with finite path quantifiers.
In the case of infinite games, we assume that player E moves according to a strategy ξ
that suggests how to extend each finite path. We say that τ |= α.ϕ, where α is an infinite
game, if there is some winning strategy ξ for player E. A strategy ξ is winning if, whenever
p is an infinite path of τ obtained according to α — i.e., by allowing player A to play in an
arbitrary way and by requiring that player E follows strategy ξ — then p satisfies the LTL
formula ϕ.
Definition 15 (strategy) A strategy for a Σ-labelled tree τ is a mapping ξ : P ∗ (τ ) →
P ∗ (τ ) that maps every finite path p to one of its finite extensions ξ(p).
Definition 16 (semantics of AE-LTL) Let α = Π0 Π1 · · · with Πi ∈ {A, E} be an infinite
path quantifier. An infinite path p is a possible outcome of game α with strategy ξ if there
is a generating sequence for it, namely, an infinite sequence p0 , p1 , . . . of finite paths such
that:
• pi are finite prefixes of p;
114

The Planning Spectrum — One, Two, Three, Infinity

• p0 =  is the root of tree

τ;

• if Πi = E then pi+1 = ξ(pi );
• if Πi = A then pi+1 is an (arbitrary) extension of pi .
We denote with Pτ (α, ξ) the set of infinite paths of τ that are possible outcomes of game α
with strategy ξ. The tree τ satisfies the AE-LTL formula g = α.ϕ, written τ |= g, if there
is some strategy ξ such that τ (p) |=LTL ϕ for all paths p ∈ Pτ (α, ξ).
We remark that it is possible that the paths in a generating sequence stop growing, i.e.,
that there is some pi such that pi = pj for all j ≥ i. In this case, according to the previous
definition, all infinite paths p that extend pi are possible outcomes.
In the next lemmas we extend the analysis of equivalence among path quantifiers to
infinite games.5 The first lemma shows that finite path quantifiers are just particular cases
of infinite path quantifiers, namely, they correspond to those infinite path quantifiers that
end with an infinite sequence of A or of E.
Lemma 5 Let α be a finite path quantifier. Then α(A)ω ∼ αA and α(E)ω ∼ αE.
Proof. We prove that α(A)ω ∼ αA. The proof of the other equivalence is similar.
First, we prove that α(A)ω
αA. Let τ be a tree and ϕ be an LTL formula such that
τ |= α(A)ω .ϕ. Moreover, let ξ be any strategy such that all p ∈ Pτ (α(A)ω , ξ) satisfy ϕ. In
order to prove that τ |= αA.ϕ it is sufficient to use the strategy ξ in the moves of player
E, namely, whenever we need to prove that p |= Eα0 .ϕ according to Definition 13, we take
p0 = ξ(p) and we move to prove that p0 |= α0 .ϕ. In this way, the infinite paths selected by
Definition 13 for αA coincide with the possible outcomes of game α(A)ω , and hence satisfy
the LTL formula ϕ.
This concludes the proof that α(A)ω
αA. We now prove that αA
α(A)ω . We distinguish
three cases.
• Case α = (A)n , with n ≥ 0.
In this case, αA ∼ A (Lemma 3(1)) and α(A)ω = (A)ω . Let τ be a tree and ϕ be an
LTL formula. Then τ |= A.ϕ if and only if all the paths of τ satisfy formula ϕ. It is
easy to check that also τ |= (A)ω .ϕ if and only if all the paths of τ satisfy formula ϕ.
This is sufficient to conclude that (A)nA ∼ (A)n (A)ω .
• Case α = Eα0 .
In this case, αA ∼ EA. Indeed, αA is an arbitrary path quantifier that starts with E
and ends with A. By Lemma 3(1), we can collapse adjacent occurrences of A and of
E , thus obtaining αA ∼ (EA)n for some n > 0. Moreover, by Lemma 3(4) we have
(EA)n ∼ EA.
Let τ be a tree and ϕ be an LTL formula. Then τ |= EA.ϕ if and only if there is
some finite path p̄ of τ such that all the infinite extensions of p̄ satisfy ϕ. Now, let
5. The definitions of the implication and equivalence relations (Definition 14) also apply to the case of
infinite path quantifiers.

115

Pistore & Vardi

ξ be any strategy such that ξ() = p̄. Then every infinite path p ∈ Pτ (Eα0 (A)ω , ξ)
satisfies ϕ. Indeed, since player E has the first turn, all the possible outcomes are
infinite extensions of ξ() = p̄.
This concludes the proof that Eα0A

Eα0 (A)ω .

• Case α = (A)n Eα0 , with n > 0.
Reasoning as in the proof of the previous case, it is easy to show that αA ∼ AEA.
Let τ be a tree and ϕ be an LTL formula. Then τ |= AEA.ϕ if and only if for
every finite path p of τ there is some finite extension p0 of p such that all the infinite
extensions of p0 satisfy the formula ϕ. Let ξ be any strategy such that p0 = ξ(p) is a
finite extension of p such that all the infinite extensions of p0 satisfy ϕ. Then every
infinite path p ∈ Pτ ((A)n Eα0 (A)ω , ξ) satisfies ϕ. Indeed, let p0 , p1 , . . . , pn , pn+1 , . . . be
a generating sequence for p. Then pn+1 = ξ(pn ) and p is an infinite extension of pn+1 .
By construction of ξ we know that p satisfies ϕ.
This concludes the proof that (A)n Eα0A

(A)n Eα0 (A)ω .

Every finite path quantifier α falls in one of the three considered cases. Therefore, we can
conclude that αA
α(A)ω for every finite path quantifier α.

The next lemma defines a sufficient condition for proving that α
is useful for the proofs of the forthcoming lemmas.

α0 . This condition

Lemma 6 Let α and α0 be two infinite path quantifiers. Let us assume that for all Σ-labelled
trees and for each strategy ξ there is some strategy ξ 0 such that Pτ (α0 , ξ 0 ) ⊆ Pτ (α, ξ). Then
α
α0 .
Proof. Let us assume that τ |= α.ϕ. Then there is a suitable strategy ξ such that all
p ∈ Pτ (α, ξ) satisfy the LTL formula ϕ. Let ξ 0 be a strategy such that all Pτ (α0 , ξ 0 ) ⊆
Pτ (α, ξ). By hypothesis, all possible outcomes for game α0 and strategy ξ 0 satisfy the LTL
formula ϕ, and hence τ |= α0 .ϕ. This concludes the proof that α
α0 .

In the next lemma we show that all the games where players A and E alternate infinitely
often are equivalent to one of the two games (AE)ω and (EA)ω . That is, we can assume that
each player extends the path only once before the turn passes to the other player.
Lemma 7 Let α be an infinite path quantifier that contains an infinite number of A and
an infinite number of E. Then α ∼ (AE)ω or α ∼ (EA)ω .
Proof. Let α = (A)m1 (E)n1 (A)m2 (E)n2 · · · with mi , ni > 0. We show that α ∼ (AE)ω .
First, we prove that (AE)ω
α. Let ξ be a strategy for the tree τ and let p be an infinite
path of τ . We show that if p ∈ Pτ (α, ξ) then p ∈ Pτ ((AE)ω , ξ). By Lemma 6 this is
sufficient for proving that (AE)ω
α.
Let p0 , p1 , . . . be a generating sequence for p according to α and ξ. Moreover, let p00 = ,
p02i+1 = pm1 +n1 +···+mi−1 +ni−1 +mi and and p02i+2 = pm1 +n1 +···+mi−1 +ni−1 +mi +1 . It is easy to
check that p00 , p01 , p02 , . . . is a valid generating sequence for p according to game (AE)ω and
strategy ξ. Indeed, extensions p00 → p01 , p02 → p03 , p04 → p05 , . . . are moves of player A,
116

The Planning Spectrum — One, Two, Three, Infinity

and hence can be arbitrary. Extensions p01 → p02 , p03 → p04 , . . . correspond to extensions
pm1 → pm1 +1 , pm1 +n1 +m2 → pm1 +n1 +m2 +1 , . . . , which are moves of player E and hence
respect strategy ξ.
We now prove that α
(AE)ω . Let ξ be a strategy for the tree τ . We define a strategy ξ¯
¯ then p ∈ Pτ (α, ξ). By Lemma 6 this is sufficient for proving
such that if p ∈ Pτ ((AE)ω , ξ),
ω
that α
(AE) .
¯ = ξ kp̄ (p̄) with kp̄ = P|p̄| ni . That is, strategy ξ¯ on path
Let p̄ be a finite path. Then ξ(p̄)
i=1
p̄ is obtained by applying kp̄ times strategy ξ. The number of times strategy ξ is applied
depends on the length |p̄| of path p̄.
¯ then p is a possible
We show that, if p is a possible outcome of the game α with strategy ξ,
ω
outcome of the game (AE) with strategy ξ. Let p0 , p1 , . . . be a generating sequence for p
¯ Then
according to (AE)ω and ξ.
p0 , p1 , ..., p1 , ξ(p1 ), ξ 2 (p1 ), ..., ξ n1 (p1 ), p3 , ..., p3 ,
| {z } |
{z
} | {z }
m1 times

m2 times
n1 times
2
n2
ξ(p3 ), ξ (p3 ), ..., ξ (p3 ), p5 , ..., p5 , ...

|

{z

n2 times

} | {z }
m3 times

is a valid generating sequence for p according to α and ξ. The extensions corresponding to
an occurrence of symbol E in α consist of an application of the strategy ξ and are hence valid
for player E. Moreover, extension ξ ni (p2i−1 ) → p2i+1 is a valid move for player A because
p2i+1 is an extension of ξ ni (p2i−1 ). Indeed, ξ ni (p2i−1 ) is a prefix of p2i (and hence of p2i+1 )
P|p2i−1 |
¯ 2i−1 ) = ξ kp2i−1 (p2i−1 ) and kp
since p2i = ξ(p
2i−1 =
x=1 nx ≥ ni , since |p2i−1 | ≥ i. The
other conditions of Definition 16 can be easily checked.
This concludes the proof that α ∼ (AE)ω for α = (A)m1 (E)n1 (A)m2 (E)n2 · · · . The proof that
α ∼ (EA)ω for α = (E)m1 (A)n1 (E)m2 (A)n2 · · · is similar.

The next lemma contains other auxiliary results on path quantifiers.
Lemma 8 Let α be a finite path quantifier and α0 be an infinite path quantifier.
1. αAα0
2. α(A)ω

αα0 and αα0

αEα0 .

αAα0 and αEα0

α(E)ω .

Proof.
1. We prove that αAα0
αα0 . Let ξ be a strategy for tree τ and let p be an infinite
path of τ . We show that if p ∈ Pτ (αα0 , ξ) then p ∈ Pτ (αAα0 , ξ). Let p0 , p1 , . . .
be a generating sequence for p according to αα0 and ξ. Then it is easy to check that
p0 , p1 , . . . , pi−1 , pi , pi , pi+1 , . . ., where i is the length of α, is a valid generating sequence
for p according to αAα0 and ξ. Indeed, the extension pi → pi is a valid move for player
A. This concludes the proof that αAα0
αα0 .
Now we prove that αα0
αEα0 . If α0 = (E)ω , then αEα0 = αE(E)ω = α(E)ω = αα0 ,
and αEα0
αα0 is trivially true. If α0 6= (E)ω , we can assume, without loss of
generality, that α0 = Aα00 . In this case, let ξ be a strategy for tree τ and let p be a
117

Pistore & Vardi

path of τ . We show that if p ∈ Pτ (αEα0 , ξ) then p ∈ Pτ (αα0 , ξ). Let p0 , p1 , . . . be
a generating sequence for p according to αEα0 and ξ. Then it is easy to check that
p0 , p1 , . . . , pi , pi+2 , . . ., where i is the length of α, is a valid generating sequence for p
according to αα0 and ξ. Indeed, extension pi → pi+2 is valid, as it corresponds to the
first symbol of α0 and we have assumed it to be symbol A. This concludes the proof
that αα0
αEα0 .
2. We prove that α(A)ω

αα0 . The proof that αα0

α(E)ω is similar.

Let ξ be a strategy for tree τ and let p be an infinite path of τ . We show that if
p ∈ Pτ (α(A)ω , ξ) then p ∈ Pτ (αα0 , ξ). Let p0 , p1 , . . . be a generating sequence for p
according to αα0 and ξ. Then it is easy to check that p0 , p1 , . . . is a valid generating sequence for p according to α(A)ω and ξ. In fact, α(A)ω defines less restrictive
conditions on generating sequences than αα0 .
This is sufficient to conclude that α(A)ω

αα0 .



We can now complete the picture of Theorem 4: each finite or infinite path quantifier is
equivalent to a canonical path quantifier that defines a game consisting of alternated moves
of players A and E of length one, two, three, or infinity.
Theorem 9 For each finite or infinite path quantifier α there is a canonical path quantifier
α0 ∈ {A, E,AE, EA,AEA, EAE, (AE)ω , (EA)ω }
such that α ∼ α0 . Moreover, the following implications hold between the canonical path
quantifiers:
(2)
A /o /o /o / AEA /o /o /o / (AE)ω /o /o /o / AE
O

O

O


O

EA /o /o /o /

O


O

O

(EA)ω

O


O

/o /o o/ / EAE /o /o o/ / E

Proof. We first prove that each path quantifier is equivalent to a canonical path quantifier.
By Theorem 4, this is true for the finite path quantifiers, so we only consider infinite path
quantifiers.
Let α be an infinite path quantifier. We distinguish three cases:
• α contains an infinite number of A and an infinite number of E: then, by Lemma 7, α
is equivalent to one of the canonical games (AE)ω or (EA)ω .
• α contains a finite number of A: in this case, α ends with an infinite sequence of E,
and, by Lemma 5, α ∼ α00 for some finite path quantifier α00 . By Theorem 4, α00 is
equivalent to some canonical path quantifier, and this concludes the proof for this
case.
• α contains a finite number of E: this case is similar to the previous one.
For the implications in (2):
118

The Planning Spectrum — One, Two, Three, Infinity

• (AE)ω
(AE)ω .

(EA)ω comes from Lemma 8(1), by taking the empty word for α and α0 =

• AEA
(AE)ω , (AE)ω
and 8(2).

AE, EA

(EA)ω , and (EA)ω

EAE come from Lemmas 5

• The other implications come from Theorem 4.



4.3 Strictness of the Implications
We conclude this section by showing that all the arrows in the diagram of Theorem 9
describe strict implications, namely, the eight canonical path quantifiers are all different.
Let us consider the following {i, p, q}-labelled binary tree, where the root is labelled by i
and each node has two children labelled with p and q:
'&%$
!"#
iM
qqq MMMMM
q
q
MMM
q
M&
qqq
()*+
/.-,
()*+
/.-,
p =xq
q
=

 ===
=


==
=

==

=




()*+
/.-,
/.-,
()*+
()*+
/.-,
()*+
/.-,
p.
q.
p.
q.
  ...
  ...
  ...
  ...








()*+
/.-,
/.-,
/.-,
()*+
/.-,
()*+
/.-,
/.-,
/.-,
()*+
/.-,
()*+
p ()*+
p
p ()*+
q
q
p
q
q ()*+

Let us consider the following LTL formulas:
• F p: player E can satisfy this formula if he moves at least once, by visiting a p-labelled
node.
• G F p: player E can satisfy this formula if he can visit an infinite number of p-labelled
nodes, that is, if he has the final move in a finite game, or if he moves infinitely often
in an infinite game.
• F G p: player E can satisfy this formula only if he takes control of the game from a
certain point on, that is, only if he has the final move in a finite game.
• G ¬q: player E can satisfy this formula only if player A never plays, since player A
can immediately visit a q-labelled node.
• X p: player E can satisfy this formula by playing the first turn and moving to the left
child of the root node.
The following graph shows which formulas hold for which path quantifiers:
Fp

GFp

FGp

G ¬q

A /o o/ / AEA /o / (AE)ω /o o/ / AE
O

Xp

O

O


O

O


O

O

O


O

EA /o o/ / (EA)ω /o o/ / EAE /o /o o/ / E
119

Pistore & Vardi

5. A Planning Algorithm for AE-LTL
In this section we present a planning algorithm for AE-LTL goals. We start by showing
how to build a parity tree automaton that accepts all the trees that satisfy a given AE-LTL
formula. Then we show how this tree automaton can be adapted, so that it accepts only
trees that correspond to valid plans for a given planning domain. In this way, the problem
of checking whether there exists some plan for a given domain and for an AE-LTL goal is
reduced to the emptiness problem on tree automata. Finally, we study the complexity of
planning for AE-LTL goals and we prove that this problem is 2EXPTIME-complete.
5.1 Tree Automata and AE-LTL Formulas
Berwanger, Grädel, and Kreutzer (2003) have shown that AE-LTL formulas can be expressed directly as CTL* formulas. The reduction exploits the equivalence of expressive
power of CTL* and monadic path logic (Moller & Rabinovich, 1999). A tree automaton
can be obtained for an AE-LTL formula using this reduction and Theorem 2. However,
the translation proposed by Berwanger et al. (2003) has an upper bound of non-elementary
complexity, and is hence not useful for our complexity analysis. In this paper we describe
a different, more direct reduction that is better suited for our purposes.
A Σ-labelled tree τ satisfies a formula α.ϕ if there is a suitable subset of paths of the
tree that satisfy ϕ. The subset of paths should be chosen according to α. In order to
characterize the suitable subsets of paths, we assume to have a w-marking of the tree τ ,
and we use the labels w to define the selected paths.
Definition 17 (w-marking) A w-marking of the Σ-labelled tree τ is a (Σ×{w, w})-labelled tree τw such that dom(τ ) = dom(τw ) and, whenever τ (x) = σ, then τw (x) = (σ, w)
or τw (x) = (σ, w).
We exploit w-markings as follows. We associate to each AE-LTL formula α.ϕ a CTL*
formula [[α.ϕ]] such that the tree τ satisfies the formula α.ϕ if and only if there is a wmarking of τ that satisfies [[α.ϕ]].
Definition 18 (AE-LTL and CTL*) Let α.ϕ be an AE-LTL formula. The CTL* formula
[[α.ϕ]] is defined as follows:
[[A.ϕ]] = A ϕ
[[E.ϕ]] = E ϕ
[[EA.ϕ]] = EF w ∧ A(F w → ϕ)
[[AEA.ϕ]] = AG EF w ∧ A(F w → ϕ)
[[AE.ϕ]] = AG EXG w ∧ A(F G w → ϕ)
[[EAE.ϕ]] = EF AG EXG w ∧ A(F G w → ϕ)
[[(AE)ω .ϕ]] = AG EF w ∧ A(G F w → ϕ)
[[(EA)ω .ϕ]] = EF AG EF w ∧ A(G F w → ϕ)
In the case of path quantifiers A and E, there is a direct translation into CTL* that does
not exploit the w-marking. In the other cases, the CTL* formula [[α.ϕ]] is the conjunction
120

The Planning Spectrum — One, Two, Three, Infinity

of two sub-formulas. The first one characterizes the good markings according to the path
quantifier α, while the second one guarantees that the paths selected according to the
marking satisfy the LTL formula ϕ. In the case of path quantifiers EA and AEA, we mark
with w the nodes that, once reached, guarantee that the formula ϕ is satisfied. The selected
paths are hence those that contain a node labelled by w (formula F w). In the case of
path quantifiers AE and EAE, we mark with w all the descendants of a node that define an
infinite path that satisfies ϕ. The selected paths are hence those that, from a certain node
on, are continuously labelled by w (formula F G w). In the case of path quantifiers (AE)ω
and (EA)ω , finally, we mark with w all the nodes that player E wants to reach according
to its strategy before passing the turn to player A. The selected paths are hence those that
contain an infinite number of nodes labelled by w (formula G F w), that is, the paths along
which player E moves infinitely often.
Theorem 10 A Σ-labelled tree τ satisfies the AE-LTL formula α.ϕ if and only if there is
some w-marking of τ that satisfies formula [[α.ϕ]].
Proof. In the proof, we consider only the cases of α = AEA, α = AE and α = (AE)ω . The
other cases are similar.
Assume that a tree τ satisfies α.ϕ. Then we show that there exists a w-marking τw of τ
that satisfies [[α.ϕ]].
• Case α = AEA. According to Definition 13, if the tree τ satisfies AEA.ϕ, then every
finite path p of τ can be extended to a finite path p0 such that all the infinite extensions
p00 of p0 satisfy ϕ. Let us mark with w all the nodes of τw that correspond to the
extension p0 of some path p. By construction, the marked tree satisfies AG EF w. It
remains to show that the marked tree satisfies A(F w → ϕ).
Let us consider any path p00 in the tree that satisfies F w, and let us show that p00 also
satisfies ϕ. Since p00 satisfies F w, we know that it contains nodes marked with w. Let
p0 be the finite prefix of path p00 up to the first node marked by w. By construction,
there exists a finite path p such that p0 is a finite extension of p and all the infinite
extensions of p0 satisfy ϕ. As a consequence, also p00 satisfies ϕ.
• Case α = AE. According to Definition 13, if the tree τ satisfies AE.ϕ, then for all the
finite paths p there is some infinite extension of p that satisfies ϕ. Therefore, we can
define a mapping m : P ∗ (τ ) → P ω (τ ) that associates to a finite path p an infinite
extension m(p) that satisfies ϕ. We can assume, without loss of generality, that, if p0
is a finite extension of p and is also a prefix of m(p), then m(p0 ) = m(p). That is, as
far as p0 extends the finite path p along the infinite path m(p) then m associates to
p0 the same infinite path m(p).
For every finite path p, let us mark with w the node of τw that is the child of p
along the infinite path m(p). By construction, the marked tree satisfies AG EXG w.
It remains to show that the marked tree satisfies A(F G w → ϕ).
Let us consider a path p00 in the tree that satisfies F G w, and let us show that p00 also
satisfies ϕ. Since p00 satisfies F G w, we know that there is some path p such that all
the descendants of p along p00 are marked with w. In order to prove that p00 satisfies ϕ
121

Pistore & Vardi

we show that p00 = m(p). Assume by contradiction that m(p) 6= p00 and let p0 be the
longest common prefix of m(p) and p00 . We observe that p is a prefix of p0 , and hence
m(p) = m(p0 ). This implies that the child node of p0 along p00 is not marked with w,
which is absurd, since by definition of p all the descendants of p along p00 are marked
with w.
• Case α = (AE)ω . According to Definition 16, if the tree τ satisfies (AE)ω .ϕ, then
there exists a suitable strategy ξ for player E so that all the possible outcomes of game
α with strategy ξ satisfy ϕ. Let us mark with w all the nodes in τw that correspond
to the extension ξ(p) of some finite path p. That is, we mark with w all the nodes
that are reached after some move of player E according to strategy ξ. The marked
tree satisfies the formula AG EF w, that is, every finite path p can be extended to a
finite path p0 such that the node corresponding to p0 is marked with w. Indeed, by
construction, it is sufficient to take p0 = ξ(p00 ) for some extension p00 of p. It remains
to show that the marked tree satisfies A(G F w → ϕ).
Let us consider a path p in the tree that satisfies G F w, and let us show that p also
satisfies ϕ. To this purpose, we show that p is a possible outcome of game α with
strategy ξ. We remark that, given an arbitrary finite prefix p0 of p it is always possible
to find some finite extension p00 of p0 such that ξ(p00 ) is also a prefix of p. Indeed, the
set of paths P = {p̄ : ξ(p̄) is a finite prefix of p} is infinite, as there are infinite nodes
marked with w in path p.
Now, let p0 , p1 , p2 , . . . be the sequence of finite paths defined as follows: p0 = () is
the root of the three; p2k+1 is the shortest extension of p2k such that ξ(p2k+1 ) is a
prefix of p; and p2k+2 = ξ(p2k+1 ). It is easy to check that p0 , p1 , p2 , . . . is a generating
sequence for p according to (AE)ω and ξ. Hence, by Definition 16, the infinite path p
satisfies the LTL formula ϕ.
This concludes the proof that if τ satisfies α.ϕ, then there exists a w-marking of τ that
satisfies [[α.ϕ]].
Assume now that there is a w-marked tree τw that satisfies [[α.ϕ]]. We show that τ satisfies
α.ϕ.
• Case α = AEA. The marked tree satisfies the formula AG EF w. This means that for
each finite path p (AG) there exists some finite extension p0 such that the final node
of p0 is marked by w (EF w) . Let p00 be any infinite extension of such a finite path p0 .
We show that p00 satisfies the LTL formula ϕ. Clearly, p00 satisfies the formula F w.
Since the tree satisfies the formula A(F w → ϕ), all the infinite paths that satisfy F w
also satisfy ϕ. Therefore, p00 satisfies the LTL formula ϕ.
• Case α = AE. The marked tree satisfies the formula AG EXG w. Then, for each
finite path p (AG) there exists some infinite extension p0 such that, from a certain
node on, all the nodes of p0 are marked with w (EXG w). We show that, if p0 is the
infinite extension of some finite path p, then p0 satisfies the LTL formula ϕ. Clearly,
p0 satisfies the formula F G w. Since the tree satisfies the formula A(F G w → ϕ), all
the infinite paths that satisfy F G w also satisfy ϕ. Therefore, p0 satisfies the LTL
formula ϕ.
122

The Planning Spectrum — One, Two, Three, Infinity

• Case α = (AE)ω . Let ξ be any strategy so that, for every finite path p, the node
corresponding to ξ(p) is marked with w. We remark that it is always possible to define
such a strategy. In fact, the marked tree satisfies the formula AG EF w, and hence,
each finite path p can be extended to a finite path p0 such that the node corresponding
to p0 is marked with w.
Let p be a possible outcome of game α with strategy ξ. We should prove that p satisfies
the LTL formula ϕ. By Definition 16, the infinite path p contains an infinite set of
nodes marked by w: these are all the nodes reached after a move of player E. Hence,
p satisfies the formula G F w. Since the tree satisfies the formula A(G F w → ϕ), all
the infinite paths that satisfy G F w also satisfy ϕ. Therefore, path p satisfies the LTL
formula ϕ.
This concludes the proof that, if there exists a w-marking of tree
then τ |= α.ϕ.

τ

that satisfies [[α.ϕ]],


Kupferman (1999) defines an extension of CTL* with existential quantification over
atomic propositions (EGCTL*) and examines complexity of model checking and satisfiability
for the new logic. We remark that AE-LTL can be seen as a subset of EGCTL*. Indeed,
according to Theorem 10, a Σ-labelled tree satisfies an AE-LTL formula α.ϕ if and only if
it satisfies the EGCTL* formula ∃w.[[α.ϕ]].
In the following definition we show how to transform a parity tree automaton for the
CTL* formula [[α.ϕ]] into a parity tree automaton for the AE-LTL formula α.ϕ. This
transformation is performed by abstracting away the information on the w-marking from
the input alphabet and from the transition relation of the tree automaton.
Definition 19 Let A = hΣ×{w, w}, D, Q, q0 , δ, βi be a parity tree automaton. The parity
tree automaton A∃w = hΣ, D, Q, q0 , δ∃w , βi, obtained from A by abstracting away the wmarking, is defined as follows: δ∃w (q, σ, d) = δ(q, (σ, w), d) ∪ δ(q, (σ, w), d).
Lemma 11 Let A and A∃w be two parity tree automata as in Definition 19. A∃w accepts
exactly the Σ-labelled trees that have some w-marking which is accepted by A.
Proof. Let τw be a (Σ×{w, w})-labelled tree and let τ be the corresponding Σ-labelled
tree, obtained by abstracting away the w-marking. We show that if τw is accepted by A,
then τ is accepted by A∃w . Let r : τ → Q be an accepting run of τw on A. Then r is also
an accepting run of τ on A∃w . Indeed, if x ∈ τ , arity(x) = d, and τw (x) = (σ, m) with
m ∈ {w, w}, then we have hr(x · 0), . . . , r(x · d−1)i ∈ δ(r(x), (σ, m), d). Then τ (x) = σ,
and, by definition of A∃w , we have hr(x · 0), . . . , r(x · d−1)i ∈ δ∃w (r(x), σ, d).
Now we show that, if the Σ-labelled tree τ is accepted by A∃w , then there is a (Σ×{w, w})labelled tree τw that is a w-marking of τ and that is accepted by A. Let r : τ → Q be an
accepting run of τ on A∃w . By definition of run, we know that if x ∈ τ , with arity(x) = d
and τ (x) = σ, then hr(x · 0), . . . , r(x · d−1)i ∈ δ∃w (r(x), σ, d). By definition of δ∃w , we
know that hr(x · 0), . . . , r(x · d−1)i ∈ δ(r(x), (σ, w), d) ∪ δ(r(x), (σ, w), d). Let us define
τw (x) = (σ, w) if hr(x · 0), . . . , r(x · d−1)i ∈ δ(r(x), (σ, w), d), and τw (x) = (σ, w) otherwise.
It is easy to check that r is an accepting run of τw on A.

123

Pistore & Vardi

Now we have all the ingredients for defining the tree automaton that accepts all the
trees that satisfy a given AE-LTL formula.
Definition 20 (tree automaton for AE-LTL) Let D ⊆ N∗ be a finite set of arities, and
let α.ϕ be an AE-LTL formula. The parity tree automaton AD
α.ϕ is obtained by applying the
transformation described in Definition 19 to the parity automaton AD
[[α.ϕ]] built according to
Theorem 2.
Theorem 12 The parity tree automaton AD
α.ϕ accepts exactly the Σ-labelled D-trees that
satisfy the formula α.ϕ.
Proof. By Theorem 2, the parity tree automaton AD
[[α.ϕ]] accepts all the D-trees that satisfy
the CTL* formula [[α.ϕ]]. Therefore, the parity tree automaton AD
α.ϕ accepts all the D-trees
that satisfy the formula α.ϕ by Lemma 11 and Theorem 10.

The parity tree automaton AD
α.ϕ has a parity index that is exponential and a number of
states that is doubly exponential in the length of formula ϕ.
2
Proposition 13 The parity tree automaton AD
α.ϕ has 2

O(|ϕ|)

states and parity index 2O(|ϕ|) .

Proof. The construction of Definition 19 does not change the number of states and the
parity index of the automaton. Therefore, the proposition follows from Theorem 2.

5.2 The Planning Algorithm
We now describe how the automaton AD
α.ϕ can be exploited in order to build a plan for goal
α.ϕ on a given domain.
We start by defining a tree automaton that accepts all the trees that define the valid
plans of a planning domain D = hΣ, σ0 , A, Ri. We recall that, according to Definition 8,
transition relation R maps a state σ ∈ Σ and an action a ∈ A into a tuple of next states
hσ1 , σ2 , . . . , σn i = R(σ, a).
In the following we assume that D is a finite set of arities that is compatible with domain
D, namely, if R(σ, a) = hσ1 , . . . , σd i for some σ ∈ Σ and a ∈ A, then d ∈ D.
Definition 21 (tree automaton for a planning domain) Let D = hΣ, σ0 , A, Ri be a
planning domain and let D be a set of arities that is compatible with domain D. The
D
tree automaton AD
D corresponding to the planning domain is AD = hΣ×A, D, Σ, σ0 , δD , β0 i,
where hσ1 , . . . , σd i ∈ δD (σ, (σ, a), d) if hσ1 , . . . , σd i = R(σ, a) with d > 0, and β0 (σ) = 0 for
all σ ∈ Σ.
According to Definition 10, a (Σ×A)-labelled tree can be obtained from each plan π for
domain D. Now we show that also the converse is true, namely, each (Σ×A)-labelled tree
accepted by the tree automaton AD
D induces a plan.
Definition 22 (plan induced by a tree) Let τ be a (Σ×A)-labelled tree that is accepted by automaton AD
D . The plan π induced by τ on domain D is defined as follows: π(σ0 , σ1 , . . . , σn ) = a if there is some finite path p in τ with τ (p) = (σ0 , a0 ) ·
(σ1 , a1 ) · · · (σn , an ) and a = an .
124

The Planning Spectrum — One, Two, Three, Infinity

The following lemma shows that Definitions 10 and 22 define a one-to-one correspondence between the valid plans for a planning domain D and the trees accepted by automaton
AD
D.
Lemma 14 Let τ be a tree accepted by automaton AD
D and let π be the corresponding
induced plan. Then π is a valid plan for domain D, and τ is the execution tree corresponding
to π. Conversely, let π be a plan for domain D and let τ be the corresponding execution
structure. Then τ is accepted by automaton AD
D and π is the plan induced by τ .
Proof. This lemma is a direct consequence of Definitions 10 and 22.



We now define a parity tree automaton that accepts only the trees that correspond to the
plans for domain D and that satisfy goal g = α.ϕ. This parity tree automaton is obtained
by combining in a suitable way the tree automaton for AE-LTL formula g (Definition 20)
and the tree automaton for domain D (Definition 21).
Definition 23 (instrumented tree automaton) Let D be a set of arities that is compatible with planning domain D. Let also AD
g = hΣ, D, Q, q0 , δ, βi be a parity tree automaton that accepts only the trees that satisfy the AE-LTL formula g. The parity tree
automaton AD
D,g corresponding to planning domain D and goal g is defined as follows:
D
AD,g = hΣ×A, D, Q×Σ, (q0 , σ0 ), δ 0 , β 0 i, where h(q1 , σ1 ), . . . , (qd , σd )i ∈ δ 0 ((q, σ), (σ, a), d) if
hq1 , . . . , qd i ∈ δ(q, σ, d) and hσ1 , . . . , σd i = R(σ, a) with d > 0, and where β 0 (q, σ) = β(q).
The following lemmas show that solutions to planning problem (D, g) are in one-to-one
correspondence with the trees accepted by the tree automaton AD
D,g .
Lemma 15 Let τ be a (Σ×A)-labelled tree that is accepted by automaton AD
D,g , and let π
be the plan induced by τ on domain D. Then the plan π is a solution to planning problem
(D, g).
Proof. According to Definition 11, we have to prove that the execution tree corresponding
to π satisfies the goal g. By Lemma 14, this amounts to proving that the tree τ satisfies g.
By construction, it is easy to check that if a (Σ×A)-labeled tree τ is accepted by AD
D,g , then
D
it is also accepted by Ag . Indeed, if rD,g : τ → Q × Σ is an accepting run of τ on AD
D,g ,
D
then rg : τ → Q is an accepting run of τ on Ag , where rg (x) = q whenever rD,g = (q, σ)
for some σ ∈ Σ.

Lemma 16 Let π be a solution to planning problem (D, g). Then the execution tree of π
is accepted by automaton AD
D,g .
Proof. Let τ be the execution tree of π. By Lemma 14 we know that τ is accepted by AD
D.
Moreover, by definition of solution of a planning problem, we know that τ is accepted also
by AD
g . By construction, it is easy to check that if a (Σ×A)-labeled tree τ is accepted by
D
D
AD and by AD
g , then it is also accepted by AD,g . Indeed, let rD : τ → Σ be an accepting
D
run of τ on AD
D and let rg : τ → Q be an accepting run of τ on Ag . Then rD,g : τ → Q × Σ
D
is an accepting run of τ on AD,g , where rD,g (x) = (q, σ) if rD (x) = σ and rg (x) = q.

125

Pistore & Vardi

As a consequence, checking whether goal g can be satisfied on domain D is reduced to
the problem of checking whether automaton AD
D,g is nonempty.
Theorem 17 Let D be a planning domain and g be an AE-LTL formula. A plan exists for
goal g on domain D if and only if the tree automaton AD
D,g is nonempty.
Proposition 18 The parity tree automaton AD
D,g for domain D = (Σ, σ0 , A, R) and goal
g = α.ϕ has |Σ| · 22

O(|ϕ|)

states and parity index 2O(|ϕ|) .

Proof. This is a consequence of Proposition 13 and of the definition of automaton AD
D,g . 
5.3 Complexity
We now study the time complexity of the planning algorithm defined in Subsection 5.2.
Given a planning domain D, the planning problem for AE-LTL goals g = α.ϕ can
be decided in a time that is doubly exponential in the size of the formula ϕ by applying
Theorem 1 to the tree automaton AD
D,g .
Lemma 19 Let D be a planning domain. The existence of a plan for AE-LTL goal g = α.ϕ
O(|ϕ|)
on domain D can be decided in time 22
.
Proof. By Theorem 17 the existence of a plan for goal g on domain D is reduced to the
emptiness problem on parity tree automaton AD
D,g . By Proposition 18, the parity tree
O(|ϕ|)

2
automaton AD
× |Σ| states and parity index 2O(|ϕ|) . Since we assume that
D,g has 2
domain D is fixed, by Theorem 1, the emptiness of automaton AD
D,g can be decided in time

22

O(|ϕ|)

.



The doubly exponential time bound is tight. Indeed, the realizability problem for an
LTL formula ϕ, which is known to be 2EXPTIME-complete (Pnueli & Rosner, 1990), can
be reduced to a planning problem for the goal A.ϕ. In a realizability problem one assumes
that a program and the environment alternate in the control of the evolution of the system.
More precisely, in an execution σ0 , σ1 , . . . the states σi are decided by the program if i is
even, and by the environment if i is odd. We say that a given formula ϕ is realizable if
there is some program such that all its executions satisfy ϕ independently on the actions of
the environment.
Theorem 20 Let D be a planning domain. The problem of deciding the existence of a plan
for AE-LTL goal g = α.ϕ on domain D is 2EXPTIME-complete.
Proof. The realizability of formula ϕ can be reduced to the problem of checking the exis
tence of a plan for goal A.ϕ on planning domain D = {init} ∪ (Σ × {p, e}), init, Σ ∪ {e}, R ,
with:
R(init, σ 0 ) = {(σ 0 , e)}
0

R(init, e) = ∅

0

R((σ, p), σ ) = {(σ , e)}

R((σ, p), e) = ∅

0

R((σ, e), e) = {(σ 0 , p) : σ 0 ∈ Σ}

R((σ, e), σ ) = ∅
126

The Planning Spectrum — One, Two, Three, Infinity

for all σ, σ 0 ∈ Σ.
States (σ, p) are those where the program controls the evolution through actions σ 0 ∈ Σ.
States (σ, e) are those where the environment controls the evolution; only the nondeterministic action e can be performed in this state. Finally, state init is used to assign the initial
move to the program.
Since the realizability problem is 2EXPTIME-complete in the size of the LTL formula
(Pnueli & Rosner, 1990), the planning problem is 2EXPTIME-hard in the size of the goal
g = α.ϕ. The 2EXPTIME-completeness follows from Lemma 19.

We remark that, in the case of goals of the form E.ϕ, an algorithm with a better
complexity can be defined. In this case, a plan exists for E.ϕ if and only if there is an
infinite sequence σ0 , σ1 , . . . of states that satisfies ϕ and such that σi+1 ∈ R(σi , ai ) for some
action ai . That is, the planning problem can be reduced to a model checking problem
for LTL formula ϕ, and this problem is known to be PSPACE-complete (Sistla & Clarke,
1985). We conjecture that, for all the canonical path quantifiers α except E, the doubly
exponential bound of Theorem 20 is tight.
Some remarks are in order on the complexity of the satisfiability and validity problems
for AE-LTL goals. These problems are PSPACE-complete. Indeed, the AE-LTL formula
α.ϕ is satisfiable if and only if the LTL formula ϕ is satisfiable6 , and the latter problem is
known to be PSPACE-complete (Sistla & Clarke, 1985). A similar argument holds also for
validity.
The complexity of the model checking problem for AE-LTL has been recently addressed
by Kupferman and Vardi (2006). Kupferman and Vardi introduce mCTL*, a variant of
CTL*, where path quantifiers have a “memoryful” interpretation. They show that memoryful quantification can express (with linear cost) the semantics of path quantifiers in our
AE-LTL. For example, the AE-LTL formula AE.ϕ is expressed in mCTL* by the formula
AG E ϕ. Kupferman and Vardi show that the model checking problem for the new logic is
EXPSPACE-complete, and that this result holds also for the subset of mCTL* that corresponds to formulas AE.ϕ. Therefore, the model checking problem for AE-LTL with finite
path quantifiers is also EXPSPACE-complete. To the best of our knowledge the complexity
of model checking AE-LTL formulas (AE)ω .ϕ and (EA)ω .ϕ is still an open problem.

6. Two Specific Cases: Reachability and Maintainability Goals
In this section we consider two basic classes of goals that are particularly relevant in the
field of planning.
6.1 Reachability Goals
The first class of goals are the reachability goals corresponding to the LTL formula F q,
where q is a propositional formula. Most of the literature in planning concentrates on this
class of goals, and there are several works that address the problem of defining plans of
different strength for this kind of goals (see, e.g., Cimatti et al., 2003 and their citations).
6. If a tree satisfies α.ϕ then some of its paths satisfy ϕ, and a path that satisfies ϕ can be seen also as a
tree that satisfies α.ϕ.

127

Pistore & Vardi

In the context of AE-LTL, as soon as player E takes control, it can immediately achieve
the reachability goal if possible at all. The fact that the control is given back to player A
after the goal has been achieved is irrelevant. Therefore, the only significant path quantifiers
for reachability goals are A, E, and AE.
Proposition 21 Let q be a propositional formula on atomic propositions Prop. Then, the
following results hold for every labelled tree τ . τ |= E. F q iff τ |= EA. F q iff τ |= EAE. F q
iff τ |= (EA)ω . F q. Moreover τ |= AE. F q iff τ |= AEA. F q iff τ |= (AE)ω . F q.
Proof. We prove that τ |= AE. F q iff τ |= AEA. F q iff τ |= (AE)ω . F q. The other cases are
similar.
Let us assume that τ |= AE. F q. Moreover, let p be a finite path of τ . We know that p
can be extended to an infinite path p0 such that τ (p0 ) |= F q. According to the semantics of
LTL, τ (p0 ) |= F q means that there is some node x in path p0 such that q ∈ τ (x). Clearly,
all infinite paths of τ that contain node x also satisfy the LTL formula F q. Therefore,
there is a finite extension p00 of p such that all the infinite extensions of p00 satisfy the LTL
formula F q: it is sufficient to take as p00 an finite extension of p that contains node x. Since
this property holds for every finite path p, we conclude that τ |= AEA. F q.
We have proven that τ |= AE. F q implies τ |= AEA. F q. By Theorem 9 we know that
AEA
(AE)ω
AE, and hence τ |= AEA. F q implies τ |= (AE)ω . F q implies τ |= AE. F q.
This concludes the proof.

The following diagram shows the implications among the significant path quantifiers for
reachability goals:
(3)
A /o /o /o / AE /o /o /o / E
We remark that the three goals A. F q, E. F q, and AE. F q correspond, respectively, to the
strong, weak, and strong cyclic planning problems of Cimatti et al. (2003).
6.2 Maintainability Goals
We now consider another particular case, namely the maintainability goals G q, where q is
a propositional formula. Maintainability goals have properties that are complementary to
the properties of reachability goals. In this case, as soon as player A takes control, it can
violate the maintainability goal if possible at all. The fact that player E can take control
after player A is hence irrelevant, and the only interesting path quantifiers are A, E, and
EA.
Proposition 22 Let q be a propositional formula on atomic propositions Prop. Then, the
following results hold for every labelled tree τ . Then τ |= A. G q iff τ |= AE. G q iff τ |=
AEA. G q iff τ |= (AE)ω . G q. Moreover τ |= EA. G q iff τ |= EAE. G q iff τ |= (EA)ω . G q.
Proof. The proof is similar to the proof of Proposition 21.



The following diagram shows the implications among the significant path quantifiers for
maintainability goals:
A /o /o /o / EA /o /o /o / E
128

The Planning Spectrum — One, Two, Three, Infinity

The goals A. G q, E. G q, and EA. G q correspond to maintainability variants of strong, weak,
and strong cyclic planning problems. Indeed, they correspond to requiring that condition q is
maintained for all evolutions despite nondeterminism (A. G q), that condition q is maintained
for some of the evolutions (E. G q), and that it is possible to reach a state where condition
q is always maintained despite nondeterminism (EA. G p).

7. Related Works and Concluding Remarks
In this paper we have defined AE-LTL, a new temporal logic that extends LTL with the
possibility of declaring complex path quantifiers that define the different degrees in which an
LTL formula can be satisfied by a computation tree. We propose to use AE-LTL formulas
for expressing temporally extended goals in nondeterministic planning domains. We have
defined a planning algorithm for AE-LTL goals that is based on an automata-theoretic
framework: the existence of a plan is reduced to checking the emptiness of a suitable parity
tree automaton. We have studied the time complexity of the planning algorithm, proving
that it is 2EXPTIME-complete in the length of the AE-LTL formula.
In the field of planning, several works use temporal logics for defining goals. Most of
these approaches (Bacchus & Kabanza, 1998, 2000; Calvanese et al., 2002; Cerrito & Mayer,
1998; de Giacomo & Vardi, 1999; Kvarnström & Doherty, 2001) use linear temporal logics
as the goal language, and are not able to express conditions on the degree in which the goal
should be satisfied with respect to the nondeterminism in the execution. Notable exceptions
are the works described by Pistore, Bettin, and Traverso (2001), Pistore and Traverso (2001)
and by Dal Lago et al. (2002). Pistore et al. (2001) and Pistore and Traverso (2001) use CTL
as goal language, while Dal Lago et al. (2002) define a new branching time logic that allows
for expressing temporally extended goals that can deal explicitly with failure and recovery
in goal achievement. In these goal languages, however, path quantifiers are interleaved with
the temporal operators, and are hence rather different from AE-LTL.
In the field of temporal logics, the work on alternating temporal logic (ATL) (Alur,
Henzinger, & Kupferman, 2002) is related to our work. In ATL, the path quantifiers in
CTL and CTL* are replaced by game quantifiers. Nevertheless, there is no obvious way to
expressed formulas of the form α.ϕ, where α is a path quantifier and ϕ is an LTL formula
in ATL∗ , which is the most expressive logic studied by Alur et al. (2002). Our conjecture
is that our logic and ATL∗ are of incomparable expressiveness.
Some comments are in order on the practical impact of the 2EXPTIME complexity of
the planning algorithm. First of all, in many planning problems we expect to have very
complex and large domains, but goals that are relatively simple (see, e.g., the experimental
evaluation performed by Pistore et al. (2001) in the case of planning goals expressed as CTL
formulas). In these cases, the doubly exponential complexity of the algorithm in the size of
the formula may not be a bottleneck. For larger AE-LTL goals, a doubly exponential time
complexity may not be feasible, but it should be noted that this is worst-case complexity.
We also note that improved algorithms for plan synthesis is an active research area, including
the analysis of simpler LTL goals (Alur & La Torre, 2004) and the development of improved
automata-theoretic algorithms (Kupferman & Vardi, 2005).
The automata-theoretic framework that we have used in the paper is of wider applicability than AE-LTL goals. An interesting direction for future investigations is the application
129

Pistore & Vardi

of the framework to variants of AE-LTL that allow for nesting of path quantifiers, or for
goals that combine AE-LTL with propositional or temporal operators. This would allow,
for instance, to specify goals which compose requirements of different strength. A simple
example of such goals is (AE. F p)∧(A. G p), which requires to achieve condition p in a strong
cyclic way, maintaining condition q in a strong way. The impossibility to define such kind
of goals is, in our opinion, the strongest limitation of AE-LTL with respect to CTL and
CTL*.
Another direction for future investigations is the extension of the approach proposed in
this paper to the case of planning under partial observability (de Giacomo & Vardi, 1999),
where one assumes that the agent executing the plan can observe only part of the state and
hence its choices on the actions to execute may depend only on that part.
We also plan to explore implementation issues and, in particular, the possibility of
exploiting BDD-based symbolic techniques in a planning algorithm for AE-LTL goals. In
some cases, these techniques have shown to be able to deal effectively with domains and
goals of a significant complexity, despite the exponential worst-case time complexity of the
problems (Bertoli, Cimatti, Pistore, Roveri, & Traverso, 2001; Pistore et al., 2001).

Acknowledgments
A shorter version of this paper, without proofs, has been published by Pistore and Vardi
(2003). The authors would like to thank Erich Grädel for his comments on the reduction
of AE-LTL formulas to CTL* formulas.

References
Alur, R., Henzinger, T., & Kupferman, O. (2002). Alternating-time temporal logic. Journal
of the ACM, 49 (5), 672–713.
Alur, R., & La Torre, S. (2004). Deterministic generators and games for LTL fragments.
ACM Trans. Comput. Log., 5 (1), 1–25.
Bacchus, F., & Kabanza, F. (1998). Planning for temporally extended goals. Ann. of
Mathematics and Artificial Intelligence, 22, 5–27.
Bacchus, F., & Kabanza, F. (2000). Using temporal logic to express search control knowledge
for planning. Artificial Intelligence, 116 (1-2), 123–191.
Bertoli, P., Cimatti, A., Pistore, M., Roveri, M., & Traverso, P. (2001). MBP: A Model
Based Planner. In Proc. of IJCAI’01 workshop on Planning under Uncertainty and
Incomplete Information.
Berwanger, D., Grädel, E., & Kreutzer, S. (2003). Once upon a time in the West - Determinacy, definability, and complexity of path games. In Prof. of 10th Int. Conf on Logic
for Programming, Artificial Intelligence, and Reasoning (LPAR’03), pp. 229–243.
Calvanese, D., de Giacomo, G., & Vardi, M. (2002). Reasoning about actions and planning
in LTL action theories. In Proc. of 8th Int. Conf. on the Principles of Knowledge
Representation and Reasoning (KR’02), pp. 593–602.
130

The Planning Spectrum — One, Two, Three, Infinity

Cerrito, S., & Mayer, M. (1998). Bounded model search in linear temporal logic and its
application to planning. In Proc. of 2nd Int. Conf. on Analytic Tableaux and Related
Methods (TABLEAUX’98), Vol. 1397 of LNAI, pp. 124–140. Springer Verlag.
Cimatti, A., Pistore, M., Roveri, M., & Traverso, P. (2003). Weak, strong, and strong cyclic
planning via symbolic model checking.. Artificial Intelligence, 147 (1-2), 35–84.
Dal Lago, U., Pistore, M., & Traverso, P. (2002). Planning with a language for extended
goals. In Proc. of 18th National Conf. on Artificial Intelligence (AAAI’02). AAAI
Press.
Daniele, M., Traverso, P., & Vardi, M. (1999). Strong cyclic planning revisited. In Proc.
of 5th European Conf. in Planning (ECP’99), Vol. 1809 of LNAI, pp. 35–48. Springer
Verlag.
de Giacomo, G., & Vardi, M. (1999). Automata-theoretic approach to planning with temporally extended goals. In Proc. of 5th European Conf. in Planning (ECP’99), Vol.
1809 of LNAI, pp. 226–238. Springer Verlag.
Emerson, E. A. (1990). Temporal and modal logic. In van Leeuwen, J. (Ed.), Handbook of
Theoretical Computer Science, Volume B: Formal Models and Semantics. Elsevier.
Emerson, E., & Jutla, C. (1988). The complexity of tree automata and logics of programs.
In Proc. of 29th IEEE Symp. on Foundations of Computer Science, pp. 328–337.
Emerson, E., & Jutla, C. (1991). Tree automata, µ-calculus and determinacy. In Proc. of
32nd IEEE Symp. on Foundations of Computer Science, pp. 368–377.
Fikes, R., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2 (3-4), 189–208.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning: Theory and Practice.
Elsevier.
Kupferman, O. (1999). Augmenting branching temporal logics with existential quantification over atomic propositions. Journal of Logic and Computation, 9 (2), 135–147.
Kupferman, O., & Vardi, M. (2005). Safraless decision procedures. In Proc. of 46th IEEE
Symp. on Foundations of Computer Science (FOCS’05), pp. 531–542. IEEE Computer
Society.
Kupferman, O., & Vardi, M. (2006). Memoryful branching-time logic. In Proc. of the
21th IEEE Symposium on Logic in Computer Science (LICS 2006). IEEE Computer
Society.
Kupferman, O., Vardi, M., & Wolper, P. (2000). An automata-theoretic approach to branching time model checking. Journal of the ACM, 47 (2).
Kvarnström, J., & Doherty, P. (2001). TALplanner: A temporal logic based forward chaining
planner. Ann. of Mathematics and Artificial Intelligence, 30, 119–169.
Moller, F., & Rabinovich, A. (1999). On the expressive power of CTL*. In Proc. of 14th
Annual IEEE Symposium on Logic in Computer Science (LICS’99), pp. 360–369.
IEEE Computer Science Press.
131

Pistore & Vardi

Penberthy, J., & Weld, D. (1992). UCPOP: A sound, complete, partial order planner for
ADL. In Proc. of 3rd Int. Conf. on the Principles of Knowledge Representation and
Reasoning (KR’92).
Peot, M., & Smith, D. (1992). Conditional nonlinear planning. In Proc. of 1st Int. Conf.
on AI Planning Systems (AIPS’92), pp. 189–197. Morgan Kaufmann Publisher.
Pistore, M., Bettin, R., & Traverso, P. (2001). Symbolic techniques for planning with
extended goals in non-deterministic domains. In Proc. of 6th European Conf. in
Planning (ECP’01).
Pistore, M., & Traverso, P. (2001). Planning as model checking for extended goals in nondeterministic domains. In Proc. of 17th Int. Joint Conf. on Artificial Intelligence
(IJCAI’01). AAAI Press.
Pistore, M., & Vardi, M. (2003). The planning specturm — one, two, three, infinity. In
Proc. of the 18th IEEE Symposium on Logic in Computer Science (LICS 2003), pp.
234–243.
Pnueli, A., & Rosner, R. (1990). Distributed reactive systems are hard to synthesize. In
Proc. of 31st IEEE Symp. on Foundation of Computer Science, pp. 746–757.
Sistla, A., & Clarke, E. (1985). The complexity of propositional linear temporal logic.
Journal ACM, 32, 733–749.
Warren, D. (1976). Generating conditional plans and programs. In Proc. of the Summer
Conf. on Artificial Intelligence and Simulation of Behaviour (AISB’76), pp. 344–354.

132

Journal of Artificial Intelligence Research 30 (2007) 213-247

Submitted 12/06; published 10/07

Compressed Pattern Databases
Ariel Felner

felner@bgu.ac.il

Department of Information Systems Engineering,
Ben-Gurion University of the Negev
Beer-Sheva, Israel, 85104

Richard E. Korf

korf@cs.ucla.edu

Department of Computer Science
University of California Los Angeles
Los Angeles, CA, 90095

Ram Meshulam

meshulr1@cs.biu.ac.il

Department of Computer Science
Bar-Ilan University
Ramat-Gan, Israel, 52900

Robert Holte

holte@cs.ualberta.ac.ca

Department of Computing Science
University of Alberta
Edmonton, Canada

Abstract
A pattern database (PDB) is a heuristic function implemented as a lookup
table that stores the lengths of optimal solutions for subproblem instances.
Standard PDBs have a distinct entry in the table for each subproblem instance.
In this paper we investigate compressing PDBs by merging several entries into
one, thereby allowing the use of PDBs that exceed available memory in their
uncompressed form. We introduce a number of methods for determining which
entries to merge and discuss their relative merits. These vary from domainindependent approaches that allow any set of entries in the PDB to be merged,
to more intelligent methods that take into account the structure of the problem.
The choice of the best compression method is based on domain-dependent
attributes. We present experimental results on a number of combinatorial
problems, including the four-peg Towers of Hanoi problem, the sliding-tile
puzzles, and the Top-Spin puzzle. For the Towers of Hanoi, we show that
the search time can be reduced by up to three orders of magnitude by using
compressed PDBs compared to uncompressed PDBs of the same size. More
modest improvements were observed for the other domains.

1. Introduction and Overview
Heuristic search algorithms such as A* (Hart, Nilsson, & Raphael, 1968) and IDA* (Korf,
1985) find optimal solutions to state space search problems. They are guided by the cost
function f (n) = g(n) + h(n), where g(n) is the cost of reaching node n from the initial state,
and h(n) is a heuristic function that estimates the cost of reaching a goal state from node
c
2007
AI Access Foundation. All rights reserved.

Felner, Korf, Meshulam, & Holte

n. If h(n) is “admissible”, which means that it never overestimates actual cost, then these
algorithms are guaranteed to find an optimal solution path if one exists.
Pattern databases are admissible heuristic functions implemented as lookup tables stored
in memory (Culberson & Schaeffer, 1998). They are the best known heuristics for a number
of combinatorial problems. In this paper we investigate the idea of compressing pattern
database heuristics in order to improve the accuracy of such heuristics, for a given amount
of memory.
We begin by describing the three different problem domains used in this paper, in order
to ground all of our discussion in concrete examples.
1.1 Problem Domains
1.1.1 The 4-peg Towers of Hanoi

Figure 1: Five-disc four-peg Towers of Hanoi problem
The well-known three-peg Towers of Hanoi problem consists of three pegs and n discs
of all different sizes which are initially stacked in decreasing order of size on one peg. The
task is to transfer all the discs from the initial peg to a goal peg. Only the top disc on any
peg can be moved, and a larger disc can never be placed on top of a smaller disc. For the
three-peg problem, there is a simple recursive algorithm that provably returns an optimal
solution. The idea is to move the n − 1 smallest discs to the intermediate peg, then move
the largest disc from the initial peg to the goal peg, and finally move the n − 1 smallest
discs from the intermediate peg to the goal peg.
The four-peg Towers of Hanoi problem (TOH4), (Hinz, 1997) shown in Figure 1, is
more interesting. The recursive algorithm for the three-peg problem doesn’t yield optimal
solutions, because there are two intermediate pegs, and we don’t know a priori how to
distribute the n−1 smallest discs over these intermediate pegs in an optimal solution. There
exists a deterministic algorithm for finding a solution, and a conjecture that it generates
an optimal solution (Frame, 1941; Stewart, 1941), but the conjecture remains unproven
(Dunkel, 1941). Thus, systematic search is currently the only method guaranteed to find
optimal solutions for problems with a given number of discs.
1.1.2 The Sliding-Tile Puzzles
One of the classic domains in the AI literature is the sliding-tile puzzle. Three common
versions of this puzzle are the 3x3 8-puzzle, the 4x4 15-puzzle and the 5x5 24-puzzle. Each
consists of a square frame containing a set of numbered square tiles, and an empty position
called the blank. The legal operators are to slide any tile that is horizontally or vertically
214

Compressed Pattern Databases

1

2

3

5

1

2

3

4

6

7

8

9

1

2

4

5

6

7

10 11 12 13 14

3

4

5

8

9

10 11

15 16 17 18 19

6

7

8

12 13 14 15

20 21 22 23 24

Figure 2: The 8-, 15- and 24-puzzle goal states
adjacent to the blank into the blank position. The problem is to rearrange the tiles from
some random initial configuration into a particular desired goal configuration. The 8-puzzle
contains 181,440 reachable states, the 15-puzzle contains over 1013 reachable states, and the
24-puzzle contains almost 1025 reachable states. The traditional goal states of these puzzles
are shown in Figure 2.
1.1.3 The Top-Spin Puzzle

Reverse
circle
18

19 20 1

2

3
4

17
16

5

15

6

14

7
13
12 11 10 9

8

Figure 3: (20,4)-Top-Spin Puzzle
The (n,r)-Top-Spin puzzle has n tokens arranged in a ring. The ring of tokens can be
shifted cyclically clockwise or counterclockwise. The tokens pass through the reverse circle
which is fixed in the top of the ring. At any given time r tokens are located inside the reverse
circle. These tokens can be reversed (rotated 180 degrees). The task is to rearrange the
puzzle such that the tokens are sorted in increasing order. The (20,4) version of the puzzle
is shown in figure 3 in its goal position where tokens 19, 20, 1 and 2 are in the reverse circle
and can be reversed. Our encoding of this puzzle has N operators, one for each clockwise
circular shift of length 0 . . . N − 1 of the entire ring followed by a reversal/rotation for the
tokens in the reverse circle. Each operator has a cost of one. This is the same encoding
analyzed by (Chen & Skiena, 1996). Practically, this puzzle can be implemented as a cyclic
215

Felner, Korf, Meshulam, & Holte

buffer where each operator reverses a set of r consecutive tokens1 . Note that there are n!
different possible ways to permute the tokens. However, since the puzzle is cyclic, only the
relative location of the different tokens matters, and thus there are only (n − 1)! different
unique states.
1.2 Pattern Database Heuristics
Heuristics are typically implemented as functions from the states of the domain to a nonnegative number. For example, a well-know heuristic for the sliding-tile puzzles is the
Manhattan distance. It is computed by determining for each tile the minimum distance in
grid units that it must travel to reach its goal location, and summing these values for all
tiles except the blank. It is a lower bound on the optimal solution length, because each tile
must move at least its Manhattan distance, and each move only moves one tile.
Pattern databases (PDBs) are heuristics in the form of lookup tables. A pattern database stores in memory the cost of an optimal solution to each instance of a subproblem
of the original problem. These costs are then used as admissible heuristics for the original
problem. PDBs have been used as lower bounds for combinatorial puzzles (Culberson &
Schaeffer, 1998; Korf, 1997; Korf & Felner, 2002; Felner, Korf, & Hanan, 2004a; Felner,
Meshulam, Holte, & Korf, 2004b), multiple sequence alignment (Zhou & Hansen, 2004; McNoughtton, Lu, Schaeffer, & Szafron, 2002; Schroedl, 2005; Edelkamp & Kissmann, 2007),
vertex cover (Felner et al., 2004a), and planning problems (Edelkamp, 2001).
For example, consider a four-peg Towers of Hanoi problem with more than 15 discs. If
we ignore all but 15 of the discs, there are 415 = 230 different possible configurations of these
15 discs. Now imagine an array which contains an entry for each possible configuration of
these 15 discs, whose value is the exact number of moves required to optimally solve the
corresponding 15-disc problem instance. Note that it doesn’t matter which discs we choose,
since their absolute sizes don’t matter. As long as they all have different sizes only their
relative size matters. If each entry will fit in a byte of memory, this table will occupy exactly
one gigabyte of memory. This is an example of a PDB.
To build this PDB, we execute a complete breadth-first search starting with all 15 discs
on the goal peg. As each configuration is encountered for the first time, we store its search
depth in the corresponding entry in the PDB. A PDB of a given size is only built once, and
can be reused to solve multiple problem instances with the same goal state.
We can use this PDB as a heuristic to solve any four-peg Towers of Hanoi problem with
15 or more discs. For each state encountered in the search, we look up the configuration of
any given subset of 15 discs in the PDB, and use the stored value as a heuristic for the full
state. Since this value is the number of moves needed to get the subset of 15 discs to the
goal peg, it is a lower bound on the total number of moves needed to get all the discs in
the problem to the goal peg.
1.3 Compressed Pattern Databases
The size of a PDB is the number of entries it contains. In general, the larger a PDB is, the
more accurate it is, and the more efficient a search algorithm using that heuristic is. For
1. In the physical puzzle, we first have to rotate the ring such that these tokens will be in the reverse circle.

216

Compressed Pattern Databases

example, for solving a TOH4 problem with 20 or more discs, the values in a PDB based on
20 discs will be more accurate than those based on 15 discs. The drawback of large PDBs,
however, is the amount of memory they consume.
The main idea of this paper is to compress PDBs that are too large to fit into memory
in their uncompressed form into a size that will fit in memory. The compressing is done by
partitioning the original PDB into groups of entries. Each group of entries in the original
PDB is mapped to a single entry in the compressed PDB. In order to preserve admissibility,
the value that is stored in the compressed PDB is the minimum among all the values in the
group in the original PDB.
For example, given a PDB for the four-peg Towers of Hanoi problem based on 20 discs,
we can divide the discs into the 15 largest discs and the 5 smallest discs. We then partition
the PDB entries into 415 groups of entries based on the positions of the 15 largest discs.
Each group will have 45 = 1024 different entries which correspond to the all the different
positions of the 5 smallest discs. In the compressed PDB we only store one entry for each of
these groups. Each entry in the compressed PDB will correspond to a different configuration
of the 15 largest discs, and its value will be the minimum value over all configurations of
the 5 smallest discs from the original PDB, in order to preserve admissibility.
Note that in general, the values in the compressed PDB will be much larger and hence
more accurate than the corresponding values in a simple 15-disc PDB, despite the fact
that the two databases are of the same size. The reason is that each entry of a simple 15disc PDB is just the number of moves needed to solve the corresponding 15-disc problem,
whereas each entry of the compressed PDB is the minimum number of moves needed to
solve any instance of the 20-disc problem in which the 15 largest discs are in one particular
configuration.
1.4 Overview
The primary questions we address in this paper are how to make the best use of a given
amount of memory with compressed PDBs, and how to determine which entries of the PDB
to compress. Specifically we make the following contributions:
• We introduce a number of methods for compressing PDBs. These methods vary from
general methods to more constrained methods. The most general methods often result
in a significant loss of information, while methods that rely on the structure of the
underlying problem space are necessarily domain specific.
• We show that the best compression methods are often domain dependent, and provide
guidelines for choosing the most promising method for a given domain. Experiments
on the 4-peg Towers of Hanoi, the sliding-tile puzzles and the Top-Spin puzzle show
that given the same amount of memory the search effort can many times be reduced
by using compressed PDBs over using uncompressed PDB of the same size.
• We also describe methods for generating PDBs using external memory and compressing them to the size of the available memory.
This paper is organized as follows. We first provide definitions that will be used throughout the paper. We then consider how to build PDBs, and in particular, how to build com217

Felner, Korf, Meshulam, & Holte

pressed PDBs when the original PDB won’t fit in memory. Then we discuss the different
compressing methods. Next, we present our experimental results for the Towers of Hanoi,
the sliding-tile puzzles and the Top-Spin Problem. Finally we offer our conclusions. A
preliminary version of this paper appeared earlier (Felner et al., 2004b).

2. Definitions
We begin by providing an abstract characterization for search spaces of combinatorial problems, with sufficient structure to define PDBs and associated constructs. These definitions
will be used throughout the paper.
2.1 Combinatorial Problems as Vectors of State Variables
A problem space is usually described abstractly as a set of atomic states, and a set of
operators that map states to states. This corresponds to a labeled graph, called the problemspace graph. In addition, a specific problem instance is a problem space together with a
particular initial state and a (set of) goal state(s). The task is to find an optimal path from
the initial state to a goal state.
A state in a combinatorial problem can be described as a vector of state variables, each
of which is assigned a particular value. For the domains studied in this paper, the variables
correspond to the different objects of the problem, and the values correspond to the different
locations that they can occupy. For example, in the Towers of Hanoi problem, there is a
variable for each disc, with a value that indicates which peg the disc is on. For the slidingtile puzzles, there is a variable for each physical tile and one for the blank, with a value that
indicates the position occupied by that tile. For the Top-Spin puzzle, there is a variable for
each token, whose value indicates its position. For convenience, in this paper, we will often
refer to the variables as objects, and the values as locations, but in general these latter
terms are simply convenient synonyms for variables and values, respectively. The size of
a problem space is the number of distinct legal combinations of value assignments to the
variables that are reachable from a given initial state.
A combinatorial problem is a permutation problem if the number of values equals the
number of variables, and each value can only be assigned to one variable. For example,
both Top-Spin and the sliding-tile puzzles are permutation problems because each location
can only be occupied by one object. The Towers of Hanoi is not a permutation problem
because each peg can hold more than one disc.
An operator in this formulation is a partial function from a state vector to a state vector.
An operator changes the values of some of the variables. Note that the number of variables
that have their values changed might be different for different domains and even for different
operators in the same domain. For the Towers of Hanoi, the value of only one variable is
changed by each operator, since only one disc moves at a time. For the sliding-tile puzzles
two variables change values with each operator, because one physical tile and the blank
exchange locations in each move. For Top-Spin, values of four variables are changed since
four tokens change their locations.
The goal is a specified state or set of states. For permutation problems, such as the
sliding-tile puzzles and Top-Spin, the goal state is usually a canonical state where object i
218

Compressed Pattern Databases

is in location i. The standard goal state for the Towers of Hanoi is where all the discs are
on a single goal peg, and thus all the state variables have the same value.
2.2 Pattern Databases
Given a vector of state variables, and a set of operators, a subset of the variables defines a
subproblem where we only assign values to variables in the subset, called pattern variables,
while the values of the other remaining variables are treated as don’t cares. For example,
in the Towers of Hanoi, a subproblem would only include a subset of the discs.
A pattern is a specific assignment of values to the pattern variables. For example, a
pattern might be a particular configuration of a subset of the discs.
A pattern space is the set of all the different reachable patterns of a given subproblem.
For example, the pattern space of a four-peg Towers of Hanoi subproblem with P pattern
variables includes all the different possible assignments of the P discs to the pegs, and is
of size 4P . For permutation problems of n elements with P pattern variables, the pattern
space typically has n × (n − 1) × . . . × (n − P + 1) states.
Each state in the original state space is projected onto a pattern of the pattern space by
only considering the pattern variables, and ignoring the other variables. We refer to the set
of states that project to the same pattern as the states of that pattern.
A goal pattern is the projection of a goal state onto the pattern variables. Multiple goal
states may give rise to multiple goal patterns, or just a single goal pattern.
There is an edge between two different patterns p1 and p2 in the pattern space if and
only if there exist two states s1 and s2 of the original problem, such that p1 is the projection
of s1 , p2 is the projection of s2 , and there is an operator of the original problem space that
connects s1 to s2 .2 The effect of the operator on the patterns is called a pattern move.
The distance between two patterns in the pattern space is the number of edges or pattern
moves on a shortest path between the two patterns.
The distance between two patterns p1 and p2 in the pattern space is therefore a lower
bound on the shortest distance between any pair of states s1 and s2 such that p1 is the
projection of s1 and p2 is the projection of s2 .
A pattern database (PDB) is a lookup table that includes an entry for each pattern of
the pattern space. The value stored for a pattern is the distance of the pattern in the
pattern space from any goal pattern. A PDB value stored for a given pattern is therefore
an admissible heuristic for all the states that project onto that pattern.

3. Building Pattern Databases
In general, a PDB is built by running a breadth-first search in the pattern space backwards
from the goal pattern until the entire pattern space is spanned. However, since the operators
of the problem apply to states of the original problem, and not to the patterns directly,
building the PDB is slightly different for the different domains.
For the Towers of Hanoi problem, we simply ignore all non-pattern discs. In other words,
if there are P discs in the pattern, we simply search a Towers of Hanoi problem space with
P discs. For each state, we keep track of its depth in the breadth-first search.
2. This is sometimes called edge homomorphism.

219

Felner, Korf, Meshulam, & Holte

For the sliding-tile puzzles, we have to keep track of the blank position, even if it is
not included in the pattern, in order to determine whether a move is legal or not. Thus,
a state in this search is uniquely determined by the positions of the pattern tiles and the
blank. This is equivalent to a problem space that includes all the tiles, but in which the
non-pattern non-blank tiles are indistinguishable from each other.
In the original application of PDBs to the sliding-tile puzzles (Culberson & Schaeffer,
1998), all moves were counted in the PDB values. The drawback of this approach is that
with multiple PDBs, the only way to combine their values without sacrificing admissibility is
to take their maximum value. Additive pattern databases (Korf & Felner, 2002) allow us to
sum the values of multiple pattern databases without violating admissibility, and are much
more effective when they are applicable. In order to construct additive pattern databases
for the sliding-tile puzzles, we can only count moves of the pattern tiles, and ignore moves
of non-pattern tiles. We adopt this approach here.
For Top-Spin, the goal is to put the tokens in their correct cyclic order, but our representation is linear rather than cyclic. To reconcile these two different representations, we
always keep token one in location one. If an operator moves token one, this is immediately
followed by a cyclic shift of all the tokens to restore token one to location one, at no additional cost. If our pattern doesn’t include token one, then we must keep track of the position
of token one, in order to correctly implement these shifts. With each state, we store only
the number of moves that involve at least one pattern token, either in the reversal, or in
the subsequent shift to restore token one to position one. For example, if a reversal changes
the position of token one, it will be counted as a pattern move even if the reversal doesn’t
include any pattern tokens, because the pattern tokens will move in the subsequent cyclic
shift of all the tokens.
When each pattern is first generated, the number of pattern moves needed to reach that
state is stored in the corresponding entry in the PDB. The PDB only needs to be built once
for a specific goal state.
3.1 Mapping Patterns to PDB Indices
PDBs are sometimes implemented by sophisticated data structures such as lexicographic
trees (Felner et al., 2004a) or octrees (McNoughtton et al., 2002). In addition, symbolic
pattern databases (Edelkamp, 2002) are based on binary decision diagrams (BDDs) (Dunkel,
1992). Nevertheless, PDBs are most commonly implemented by arrays of entries, each
storing a heuristic value for a specific pattern. Thus, for simplicity, in this paper we assume
that PDBs are implemented by arrays.
To save memory, we want to avoid having to store the patterns themselves along with
their heuristic values. This is done by representing each pattern with a unique index in the
PDB. The particular mapping from patterns to indices is domain specific.
For example, a state of the Towers of Hanoi problem can be uniquely represented by
specifying the peg that each disc is on, since all the discs on any peg must be in sorted
order by size. For the four-peg problem, a peg can be specified by two bits, and a complete
problem state can be uniquely specified by a string of 2n bits, where n is the number of
discs. Furthermore, every such bit string represents a legal state of the problem.
220

Compressed Pattern Databases

For permutation problems, there are two obvious ways to store a PDB with k variables
- a sparse mapping and a compact mapping.
• sparse mapping - The simplest organization is a k-dimensional array, with each
dimension having a range of 0 to n − 1. An individual pattern is mapped into the
table by taking the value of each pattern variable as a separate index into the array.
For example, if we have a pattern of three variables (X,Y,Z), whose values are (2,1,3)
respectively, it would be mapped to the array element A[2][1][3]. The total size of such
an array for all configurations of k elements is nk . This is called a sparse mapping.
The advantage of sparse mapping is that it is simple to implement and the indices
can be efficiently computed. The disadvantage is that it is not efficient in terms of
space, since it wastes those entries with two or more equal indices, since such entries
do not correspond to valid configuration.
• compact mapping - Another method for indexing PDBs for permutation problems
is called a compact mapping. In particular, each permutation of k elements into
n possible locations can be mapped bijectively to a unique index in the range 0
to n × n − 1 × . . . × n − k + 1. One such mapping maps each permutation to its
index in a lexicographic ordering of all such permutations. For example, assuming
k = n = 3, the permutation (2 1 3) is mapped to the third index, since it is preceded
by (1 2 3) and (1 3 2) in lexicographic order. The advantage of compact mapping is
that it does not waste any space. The disadvantage is that computing the indices is
more complex. (Myrvold & Ruskey, 2001) provide linear-time algorithms for bijective
mappings between permutations and integers, and (Korf & Shultze, 2005) provide
linear-time algorithms in which the index of a permutation represents its position in
lexicographic order. At least in our experiments (reported below), even with this
advanced mapping algorithm, the access time of the sparse mapping was faster than
its compact mapping counterpart.
It is worth noting that the memory savings of the compact mapping over the sparse
mapping decreases as the number of variables in the domain increases beyond the number of
elements in the PDB. For example, consider a 6-tile PDB for the 24 Puzzle. Sparse mapping
requires 256 = 244 × 106 entries, while compact mapping uses only 25 × 24 × . . . × 20 =
128 × 106 entries. Indeed, since the memory savings are modest, and sparse mapping is
simpler it has often been used to implement the 6-tile PDBs (Korf & Felner, 2002; Felner
et al., 2004a; Felner, Zahavi, Holte, & Schaeffer, 2005; Zahavi, Felner, Holte, & Schaeffer,
2006). Similarly, sparse mapping was also used to implement the 5-tile PDBs of the 35
puzzle (Felner et al., 2004a).
3.2 Building Large Pattern Databases
Compressed PDBs are used when there isn’t sufficient memory to store the uncompressed
PDBs. This raises the question of how to generate a compressed PDB without exhausting
memory in the first place.
Consider a PDB for the four-peg Towers of Hanoi problem for example. Since a state
can be uniquely represented by a bit string index of length 2n, only the heuristic values are
221

Felner, Korf, Meshulam, & Holte

stored. If we use an unsigned character array, we can store values up to 255 in one byte,
which is sufficient for the maximum number of moves needed to solve any problem with up
to 18-discs (Korf, 2003). Thus, a 15-disc PDB, or a PDB compressed to the size of a 15-disc
PDB, would occupy 415 bytes, or a gigabyte of memory. A 16-disc PDB would need four
gigabytes, however. Given a machine with two gigabytes of memory, for example, how can
we generate a 16-disc PDB compressed to the size of a 15-disc PDB?
To generate such a PDB, we must perform a complete breadth-first search of the 16-disc
problem space, starting from the standard goal state. As each state is generated, the largest
15 discs are used as an index into the PDB, and if the entry is empty, the search depth
is stored. A breadth-first search is normally implemented by a first-in first-out queue of
nodes that have been generated, but not yet expanded. Initially, the goal state is placed
in the queue, and at each step we remove and expand the node at the head of the queue,
and append its children to the tail of the queue. We can keep track of the search depth by
placing a marker in the queue between nodes of successive depths. The maximum size of
this queue is determined by the maximum number of nodes at any depth of the search. For
example, for the 16-disc problem, this number is 162, 989, 898 nodes at depth 134 (Korf,
2003). Since a state of the 16-disc problem can be stored in 32 bits, this queue requires
651, 959, 592 bytes, or 622 megabytes of memory.
In order for this breadth-first search to terminate, however, we have to be able to detect
whether or not we have encountered a particular state before. An efficient way to do this
is to store a bit array, with one bit for each state, initialized to all zeros. Whenever a state
is first encountered, its corresponding bit is set to one. Whenever we generate a node, we
check its bit, and discard the node if its bit is already set to one. For the 16-disc problem
we need 416 bits, or 512 megabytes for this array.
622 megabytes for the queue, plus 512 megabytes for the bit array, and one gigabyte
for the pattern database exceeds the capacity of a two gigabyte machine. We can’t use the
PDB to replace the bit array, because each PDB entry represents four different states of
the 16-disc problem, each of which has to be separately detectable.
In the case of compressing a 16-disc PDB, the solution is simple. Since the breadth-first
search queue is FIFO, all of its accesses are sequential, and it can be efficiently stored on
magnetic disk instead of memory. A simple implementation is to keep two different files,
one for nodes at the current depth, and the other for nodes at the next depth. The first file
is read sequentially, and children are appended to the tail of the second file. When the first
file is exhausted at the end of the current search depth, the second file becomes the new
first file, and another file is created for the children at the next depth. Moving the queue
to disk creates enough space to store both the bit array and the PDB in memory.
Unfortunately, this won’t work for compressing larger PDBs. For example, a complete
breadth-first search of the 17-disc problem will require all two megabytes just for the bit
array. The bit array cannot be stored on disk, because it is accessed randomly, and random
access of a byte on disk requires an average of about 5 milliseconds latency.
Techniques such as delayed duplicate detection have been developed for performing large
breadth-first searches on disk, without storing a bit for each state in memory (Munagala &
Ranade, 1999; Korf, 2003, 2004; Korf & Shultze, 2005). The key idea is to not immediately
check each node to see if it has previously been generated, but delay the duplicate detection
until large numbers of duplicates can be eliminated by sequentially accessing the nodes on
222

Compressed Pattern Databases

disk. Using such techniques, complete breadth-first searches have been performed on the
four-peg Towers of Hanoi problem with up to 22 discs (Korf & Felner, 2007).
Breadth-first search with delayed duplicate detection can be performed using relatively
little memory, allowing a compressed PDB to be built in memory at the same time. However,
the technique is more efficient with more memory. Thus, as an alternative, we can build
the compressed PDB in two phases.
The first phase performs the breadth-first search on disk, without the compressed PDB
in memory. During this phase, as each state is expanded, we write out to another disk file
an ordered pair consisting of the index of the state in the compressed PDB, followed by
its depth in the breadth-first search. This file is simply a linear list of these pairs, and is
written sequentially. Note that the depths in this file will be in sorted order.
In the second phase, we build the compressed PDB in memory, by sequentially reading
this file. For each ordered pair, we look up the index in the PDB. If it is empty, we store
the corresponding depth in the PDB, and if it is full, we simply ignore the ordered pair.
Finally, we write the compressed PDB to disk for use in future searches, and delete the file
of ordered pairs. This scheme allows us to use almost all of our memory for the breadth-first
search, regardless of the size of the compressed PDB.

4. Compressing Pattern Databases
In most previous studies, PDBs have had one entry for each pattern in the pattern space.
In this section, we describe different methods for compressing PDBs by merging a number
of PDB entries into a single entry. In order to preserve admissibility, the merged entry will
store the minimum value of the entries merged, with a consequent loss of information for
those original entries with larger values than the minimum.
The idea to compress large tables into a smaller size by merging several entries together
has been suggested in the past outside of the AI community, e.g., in the form of bit-state
hashing (Bloom, 1970) and in the form of hash compaction (Stern & Dill., 1995).
The ideal compression scheme would group together all entries that have the same value.
This would preserve all the information, and result in a PDB whose size is the number
of distinct values in the original PDB. Of course, this is almost impossible to achieve in
practice.
To compress a PDB by a factor of k, we typically partition the entire pattern space
into M/k sets of k patterns each, where M is the size of the original PDB. The compressed
PDB will contain one entry for each set. The key to making this method effective is that
the values stored in the original PDB for the k entries should be as close to one another as
possible to minimize the resulting loss of information. One of the questions addressed in
this paper is how to identify PDB entries with similar values to be merged.
4.1 Compressing Nearby Patterns
An effective heuristic to identify PDB entries with similar values is to compress entries
that correspond to patterns that are close together in the pattern space. Assume that two
patterns p1 and p2 are close to each other in the pattern space, meaning that the distance
between them is small. Furthermore, assume that all operators are reversible. Then the
distances from p1 to the goal pattern and from p2 to the goal pattern (i.e., their PDB values)
223

Felner, Korf, Meshulam, & Holte

are also similar. More formally:
(d(p1 , p2 ) = c) =⇒ (|P DB(p1 ) − P DB(p2 )| ≤ c).
If these two patterns are compressed into the same entry we are guaranteed that the loss of
information is at most c moves. Therefore, we would like to compress the PDB such that
patterns that are mapped to the same entry are close to each other in the pattern space.
We provide a number of different methods for compressing PDBs. The most general
methods often result in significant loss of information, while methods that rely on the
structure of the underlying problem space are necessarily domain-specific.
4.2 Compression Based on General Mapping Functions

Regular PDB

0
1
2

compressed PDB

Regular PDB

6
6

3

5
4

4
5

3
3

0 5
1 3

Compress by DIV 3

0
1

6
6

2
3

5
4

4
5

3
3

compressed PDB

0 3
1 3

Compress by MOD 3

Figure 4: Compressed pattern database
The easiest way to compress a PDB is to use a general function that maps exactly k
original patterns to each compressed entry. This compresses a PDB with M entries to a
compressed PDB with M/k entries. Two examples of such functions for mapping indices i
in the original PDB to the compressed PDB are:
• Compressed index(i) = i DIV k
• Compressed index(i) = i M OD k
where DIV is integer division by k, and MOD is the remainder after division by k. The
advantage of these mapping functions is that they are general, and thus a PDB of size M
can be compressed to fit into any amount of available memory.
Figure 4 shows these two ways to compress a PDB of size 6 by a factor of 3 into a
compressed PDB of size 2. Note that the values of the specific PDB of the figure are locally
correlated with their indices. Therefore, it is easy to see that in this case the DIV operator
is better than the MOD operator since it compresses more highly correlated values and
therefore the resulting the loss of information is smaller.
The challenge is to build the PDB such that entries with similar indices come from
patterns that are close to each other in the pattern space. Such values are locally correlated.
224

Compressed Pattern Databases

Given locally correlated values, it is better to compress them with the DIV operator, rather
than compressing an arbitrary set of k entries. As will be shown below, for many domains it
is possible to build a PDB such that locally correlated entries correspond to nearby patterns.
4.3 Compression Based on Individual Variables
To compute any PDB, we use a function to map each state to an index in the PDB, which
uniquely represents the corresponding pattern. Many of these functions map the values of
particular variables to particular bits of the index. For example, as described above, the
most natural representation of a state of the n-disc four-peg Towers of Hanoi problem is a
bit string of 2n bits. In this representation, pairs of bits represent the locations of different
discs. The same is true of the sparse representation described above for permutation problems, where different indices of the multi-dimensional array encode the locations of different
tiles or tokens. The situation is a little more complex for the compact representation of
permutation problems, but the same principle applies.
To compress these PDBs, we can map the indices in the original PDB to indices in the
compressed PDB by ignoring certain bits. If these bits correspond to particular variables,
then this has the effect of compressing entries by ignoring the values of those variables. If
the operators of the problem space only change the values of a small number of variables,
then patterns that differ in only a few variables will tend to be close together in the pattern
space, and their distances to the goal will also tend to be similar. Thus, when compressing
these entries the loss of information should be small.
In both the compact and sparse mappings described above we calculate the index in
the PDB according to a predefined order of the variables. Assume that the last variable in
that order has q different possible values. For the sparse mapping of permutation problems
q = n, while for the compact mapping q = n − P + 1, where n is the total number of
variables, and P is the number of pattern variables. If we divide the index by q, then we are
compressing entries that differ only in the value of the last variable. In our problems this
means that in a set of entries that are compressed together, all the pattern objects but the
last one are located in the same location. For example, all patterns which are indexed as
P DB[a][b][c][d] will be compressed to COM P RESSED P DB[a][b][c]. The disadvantage
here is that we are forced to compress by a factor of n in the sparse mapping case, or n−P +1
in the compact mapping case, even if this doesn’t exactly fit the available memory.
The idea of compressing the last variable can be generalized to compressing the last two
or more variables. The compression factor would be n, n2 , n3 etc. for the sparse mapping,
and n − P + 1, (n − P + 1) · (n − P + 2), (n − P + 1) · (n − P + 2) · (n − P + 3) etc. for the
compact mapping. Thus, while this method may achieve high correlation in the values of
compressed entries, it only allows compression by certain values.
4.3.1 Comparing Compressed and Uncompress PDBs of the Same Size
We denote an uncompressed PDB of a set of P variables as P DBP , and similarly an
uncompressed PDB of P − C variables as P DBP −C . We denote a PDB of P variables
compressed by C variables CP DBP/C . We now provide the following two propositions
comparing P DBP −C and CP DBP/C .
1. The size of CP DBP/C is equal to the size of P DBP −C .
225

Felner, Korf, Meshulam, & Holte

Proof: It is easy to see that they are both of size n|P −C| using the sparse mapping,
and (n × (n − 1) × (n − 2) × . . . × (n − |P | + |C| + 1)) using the compact mapping.
2. Assume that P is a set of variables and that C ⊂ P . For each state s of the search
space CP DBP/C (s) ≥ P DBP −C (s).
Proof: Both mappings are based on the same P − C variables. However, while
P DBP −C completely ignores the C variables, CP DBP/C contains the minimum values
of P DBP over all combinations of the C variables.
Thus, for a given memory size M , a PDB larger than M compressed by individual variables to the size of M is at least as accurate and usually more accurate than an uncompressed
PDB of size M for the same variables.
4.4 Compressing Cliques

d
G

d+1
d
Figure 5: Cliques in PDBs

Suppose that a given set of patterns form a clique in the pattern space. This means that
all the patterns in the set are reachable from each other by one move in the pattern space.
Thus, the PDB entries for these nodes will differ from one another by no more than one,
assuming that all operators are reversible. Some will have a value d, and the rest will have
value d + 1 as shown in Figure 5. It is worthwhile to compress such cliques in the pattern
space since the loss of information is guaranteed to be at most one move. If we can identify
a general structure of q entries in the PDB that represent a clique of size q, we can map all
these q patterns to one entry.
The existence of cliques in the pattern space is domain dependent. Furthermore, to
take advantage of them, we need a compression function that will map members of a clique
to the same index in the compressed PDB. These conditions will not exist in all problem
spaces, but do exist in some of the domains we considered.
In combinatorial problems where the operators move only one object at a time, such as
the Towers of Hanoi and the standard sliding-tile puzzles, cliques often represent states that
differ only in the location of a single object. Therefore, compression based on that object (or
variable) amounts to compressing cliques in practice. This coincidence does not necessarily
occur in general. As will be shown below, for Top-Spin, patterns that only differ in the
location of one object are not cliques, and for this domain the two compression methods do
not coincide.
Compressing cliques of size q can be done in the following two ways:
• lossy compression : Store the minimum value of the q entries. The admissibility of
the heuristic is preserved and the loss of information is at most one move.
226

Compressed Pattern Databases

• lossless compression: Store the minimum value for the q entries. Assume this
value is d. Store also q additional bits, one for each entry in the clique, that indicates
whether the entry’s value is d or d + 1. This preserves all the information of the
original PDB, but will usually require less memory.
The idea of compressing cliques can be generalized to a set of z nodes with a diameter
of r. In other words, each pair of nodes within the set have at least one connecting path
consisting of r or fewer edges. A clique is the special case where r = 1. We can compress
this set of nodes into one entry by taking the minimum of their entries and lose at most r
moves. Alternatively, for lossless compression we need an additional z · ⌈log(r + 1)⌉ bits to
indicate the exact value. If the size of an entry is b bits then it will be beneficial in terms
of memory to use lossless compression for sets of nodes with a diameter of r as long as
⌈log(r + 1)⌉ < b.
4.5 Inconsistency of the Compressed Heuristic
An admissible heuristic h is consistent if for any two states, x and y, |h(x)−h(y)| ≤ dist(x, y)
where dist(x, y) is the shortest path between x and y in the problem space. In particular, for
neighboring states the h values of the two states differ by at most one move. An admissible
heuristic h is inconsistent if for at least one pair of nodes x and y, |h(x) − h(y)| > dist(x, y).
Compressed PDBs can be inconsistent since the loss of information can be different for
different states. For example, let x and y be neighboring states (with an edge cost of
1) such that they are respectively mapped to the patterns of lines 2 and 3 from the left
frame of figure 4. While their original heuristics are consistent (5 and 4 respectively)
their compressed heuristics are inconsistent (5 and 3). Pathmax (PMX) is one approach
to correcting inconsistent heuristics (Mero, 1984). It propagates heuristic values from a
parent node p to its child c as follows. h(p) − dist(p, c) is a lower bound on dist(c, Goal)
and therefore can be used instead of h(c) if it is larger. A new approach for further handling
inconsistent heuristics is called bidirectional pathmax (BPMX) (Felner et al., 2005; Zahavi,
Felner, Schaeffer, & Sturtevant, 2007). It generalizes PMX such that with BPMX, heuristic
values are propagated in both directions in order to increase heuristic values of nodes in the
neighborhood. Preliminary results of applying BPMX and compressed PDBs show that in
some cases a small reduction in the number of generated nodes can be achieved.
We now present experimental results obtained with compressed pattern databases on
each of our example domains.

5. The 4-peg Towers of Hanoi Problem (TOH4)
The state space of TOH4 has many small cycles, meaning there are many paths between the
same pair of states. For example, if we move the same disc twice in a row, we can achieve
the same effect with only a single move of the disc. As another example, if we move a disc
from peg A to peg B, and then another disc from peg C to peg D, applying these two moves
in the opposite order will have the same effect. If we forbid moving the same disc twice in a
row, and only apply commutative operators in one order, we get a branching factor of 3.766.
For the 7-disc problem, the optimal solution depth is 25 moves, and a complete search to
this depth will generate 3.76625 nodes. However, there are only 47 = 16, 384 unique states.
227

Felner, Korf, Meshulam, & Holte

Thus, any depth-first search, such as IDA*, will generate enormous numbers of duplicate
nodes, and will be hopelessly inefficient in this domain.
Thus, in order to search in this domain we used Frontier-A* (FA*), a modification of
A* designed to save memory (Korf, Zhang, Thayer, & Hohwald, 2005). FA* saves only
the Open list and deletes nodes from memory once they have been expanded. In order to
keep from regenerating Closed nodes, with each node on the Open list, FA* stores those
operators that lead to Closed nodes, and when expanding a node those operators are not
used.
5.1 Additive Pattern Databases for TOH4
The applicability of PDB heuristics to TOH4 was first shown by (Felner et al., 2004a).
Consider a 16-disc problem. We can build a PDB for the ten largest discs by including
an entry for each of the 410 legal patterns of these discs. The value of each entry is the
minimum number of moves required to move ten discs from their corresponding positions to
the goal peg, assuming there are no other discs in the problem. During the problem-solving
search, given a state of the 16-disc problem, we compute the index corresponding to the
ten largest discs, and look up the value for this configuration in the PDB. This value is
an admissible heuristic for the complete 16-disc problem, because a solution to the 16-disc
problem must move the largest ten discs to the goal peg, in addition to the smallest six
discs
A similar PDB can be built for the six smallest discs. Values from the ten-disc PDB
and the six-disc PDB can be added together to get an admissible heuristic value for the
complete state. The reason is that a complete solution must move all the discs to the
goal peg. Furthermore, each move only moves one disc, and the PDB values for these two
subproblems only count moves of their respective pattern discs. Therefore, the sum of the
PDB values from two disjoint sets of discs is a lower bound on the number of moves needed
to solve the original problem. The idea of additive PDBs was first introduced by (Korf &
Felner, 2002) for the sliding-tile puzzles. A deeper analysis of additive PDBs is provided
by(Felner et al., 2004a).
Note that a PDB based on n discs will contain exactly the same values for the largest n
discs, the smallest n discs, or any other set of n discs. The reason is that all that matters is
that the discs be of different sizes, and not their absolute sizes. Furthermore, a PDB for m
discs also contains a PDB for n discs, if n < m. To look up a pattern of n discs, we simply
assign the m − n remaining discs to the goal peg, and then look up the resulting pattern
in the m-disc PDB. Thus, in practice we only need a single PDB for the largest number of
discs of any group of our partition. In our case, a ten-disc PDB contains both a PDB for
the largest ten discs and a PDB for the smallest six discs. In general, the most effective
heuristic is based on partitioning the discs into groups that maximize the size of the largest
group, subject to the available memory. The largest PDB we can use on a machine with a
gigabyte of memory is for 14 discs. This has 414 entries, and at one byte per entry occupies
256 megabytes. The rest of the memory is used for the Open list of FA*.
Given a PDB of 14 discs, there are two ways to use it for a 16-disc problem. The first
is called static partitioning. In this method, we statically partition the discs into one group
of 14 discs and the remaining 2 discs, and use the same partition for all the nodes of the
228

Compressed Pattern Databases

Heuristic
Static 13-3
Static 14-2
Dynamic 14-2

Path
161
161
161

Avg h
72.17
87.04
93.46

Nodes
134,653,232
36,479,151
12,827,732

Seconds
48.75
14.34
21.56

Table 1: Static vs Dynamic Partitioning for the 16-disc problem
search. The other method is called dynamic partitioning. For each state of the search, we
compute all 16 · 15/2 = 120 different ways of dividing the discs into groups of 14 and 2,
look up the PDB values for each pair, sum the two values, and return the maximum of
these as the overall heuristic value. Here, the exact partitioning into disjoint sets of discs
is dynamically determined for each state of the search. Table 1, taken from (Felner et al.,
2004a), compares static partitioning and dynamic partitioning for solving the standard
initial state of the 16-disc TOH4. Each row corresponds to a different PDB setting. The
static partitions divide the discs into a large group of the largest discs and a smaller group
of the smallest discs. The columns provide the optimal path length, the average heuristic
value over the PDB, the number of generated nodes and amount of time taken to solve the
standard initial state. A 14-2 split is much better than a 13-3 split since the 14-disc PDB
is much more informed than the 13-disc PDB. For the 14-2 split, a dynamically partitioned
heuristic is more accurate, the search generates fewer nodes, and therefore FA* requires less
memory, but takes longer to run due to the multiple heuristic calculations for each state.
Static partitioning is simpler to implement, and consumes much less time per node, but
generates more nodes than dynamic partitioning, thus occupying more memory. Static and
dynamic partitioning apply in any domain where additive PDBs apply.
5.2 Compressed Pattern Databases for TOH4
As explained above, states for TOH4 can be represented by bit strings, where pairs of
adjacent bits represent the positions of particular discs. Therefore, compressing these PDBs
is very easy. For example, if the smallest disc is represented by the least significant two
bits, compression based on the smallest disc can be accomplished by a right shift of two
bits in the bit string. Note that this is logically equivalent to DIV 4. We will refer to this
as “compressing the smallest disc”.
In practice, compressing the smallest disc amounts to compressing cliques in this domain.
For TOH4, the largest cliques are of size four, since the smallest disc can always move among
the four pegs without moving any of the larger discs. Thus, we can store a PDB of P discs
in a table of size 4P −1 , instead of 4P , by compressing the four states of the smallest disc
into one entry.
In the compressed PDB, we will have one entry for each configuration of the P − 1
largest discs. Lossy compression would store the minimum value of the four entries of the
original PDB that correspond to each entry of the compressed PDB, and lose at most one
move for some of the entries. Alternatively, lossless compression would store four additional
bits in each entry of the compressed PDB, to indicate for each location of the smallest disc
whether the value is the minimum value or one greater.
229

Felner, Korf, Meshulam, & Holte

Heuristic
Static 14+2
Static 13+3
Static 12+4
Static 11+5

h(s)
116
102
90
74

Avg h
87.04
72.17
59.01
47.32

Nodes
36,479,151
134,653,232
375,244,455
> 462,093,281

Time
14.34
54.02
184.62
> 243.15

Mem
256M
64M
16M
4M

Table 2: Solving 16 discs without compression
This can be generalized to compressing the smallest two (or more) discs. We fix the
position of largest P − 2 discs, and consider the 16 different configurations of the two
smallest discs. These form a set of nodes of diameter three. Thus, we can compress these
16 entries into one entry, and lose at most three moves for any state. Alternatively, for
lossless compression we can add 2 · 16 = 32 bits to the one byte entry in the compressed
PDB, for a total of five bytes, and store the exact values, compared to 16 bytes for the
uncompressed PDB.
5.3 Experiments on the 16-disc 4-peg Towers of Hanoi
We now provide experimental results for the various compressing methods on the 16-disc
4-peg Towers of Hanoi problem. All the results are for solving the standard version of the
problem, which is moving all discs from one peg to another. The optimal solution to this
problem is 161 moves long. Unless stated otherwise, all the experiments in this paper were
conducted on a 2.4Ghz PC with one gigabyte of main memory.
5.3.1 Uncompressed PDBs of Different Sizes
For comparison purposes, Table 2 shows results for uncompressed PDBs built by static
partitioning of 14 + 2, 13 + 3, 12 + 4 and 11 + 5. The first column shows the heuristic that
was used. In each case, the larger group contained the largest discs. The second column
shows the heuristic value of the initial state. The next column is the average heuristic
value over all entries of the larger PDB (the 14-disc PDB, the 13-disc PDB etc). The next
columns present the number of generated nodes, the amount of time in seconds, and the
amount of memory needed in megabytes for the larger PDB, to optimally solve the 16-disc
problem. We weren’t able to solve the problem with the 11 + 5 heuristic before running out
of memory for the Open list.
5.3.2 Compressing the Largest Discs
As explained above, in this domain compressing cliques is identical to compressing the
smallest disc. A complementary method is to compress the largest discs. In our implementation the smallest discs were represented by the least significant bits and the larger discs
were represents by the most significant bits. In this particular representation, compressing
the largest z discs can be accomplished by masking off the corresponding 2z bits. This is
logically equivalent to taking the representation MOD 4z, if the largest discs are represented
by the most significant bits.
230

Compressed Pattern Databases

Discs
0
.5
1
1.5
2
2.5

h(s)
116
101
100
85
84
69

Avg h
87.04
80.55
72.17
66.46
59.01
53.94

Nodes
36,479,151
70,433,127
285,190,821
410,850,034
791,374,842
1,086,889,788

Time
14.34
28.69
143.78
269.55
543.10
776.22

Mem
256M
128M
64M
32M
16M
4M

Table 3: Solving 16 discs with a 14-2 PDB where the 14-disc PDB was compressed based
on the large discs

Table 3 presents results for solving the 16-disc TOH4 problem by compressing the largest
discs. We statically divided the discs into two groups. The largest fourteen discs define the
14-disc PDB. The smallest two are in their own group and have a separate PDB with
42 = 16 entries. To compute the heuristic for a state, the values from the 2-disc PDB and
the 14-disc PDB are added. The different rows of the table correspond to different degrees
of compression of the 14-disc PDB. The 2-disc PDB only occupies 16 entries and hence
there is no need to compress it.
The first column shows the number of largest discs that were compressed. The first row
represents no compression. The whole numbered rows are based on masking off both bits
used to represent a disc. The fractional numbers are based on masking off one of the two
bits used to represent a disc. For example, the row with 1.5 in the first column is based on
masking off both bits of the largest disc, and the most significant bit of the two bits for the
next largest disc. The rest of the columns are in the same format as Table 2.
The table clearly shows that the running time increases significantly when compressing
the largest discs. The reason is that the locations of the largest discs have a large impact
on the PDB values. Thus, the values that are compressed here are not correlated with each
other, and a great deal of information is lost. This provides evidence for our claim that
values that are uncorrelated should not be compressed.
Comparing Table 3 to Table 2 shows that compressing the largest discs performs worse
than a PDB of similar size without compression. For example, with 64 megabytes of memory
it is better to solve the problem with a simple PDB of 13 discs plus a small PDB of 3 discs
(2nd line of Table 2), than with a 14-disc PDB compressed by the largest disc, plus a small
PDB of 2 discs (3rd line of Table 3). Thus, compressing the largest discs is not beneficial
for TOH4.3
231

Felner, Korf, Meshulam, & Holte

Discs
0
1
2
3
4
5
6
7
8
9
1 lls

h(s)
116
115
113
111
110
103
99
98
96
75
116

Avg h
87.04
86.48
85.67
84.45
82.74
80.85
78.54
74.81
68.34
62.71
87.04

r
0
1
3
5
9
13
17
25
33
41
0

Nodes
36,479,151
37,964,227
40,055,436
44,996,743
45,808,328
61,132,726
76,121,867
97,260,058
164,292,964
315,930,865
36,479,151

Time
14.34
14.69
15.41
16.94
17.36
23.78
33.72
36.63
67.59
155.22
15.87

Mem
256M
64M
16M
4M
1M
256K
64K
16K
4K
1K
96M

Table 4: Solving 16 discs where the 14-disc PDB was compressed by the small discs
5.3.3 Compressing Cliques or the Smallest Discs
Table 4 presents results for a 14-2 static partitioning of the 16 discs, but compressing the
14-disc PDB by the smallest discs. In our representation compression based on the smallest
z discs can be accomplished by masking the left 2z bits. This is logically equivalent to
performing MOD by 4z. The different rows of the table correspond to compressing the
14-disc PDB by different numbers of discs. All the rows represent lossy compression, except
for the last row which represent lossless compression (denoted as lls). The first row of Table
4 is for the complete 14-disc PDB with no compression. The second row corresponds to
compressing the smallest of the 14 largest discs. In that case, the 14-disc PDB only contains
413 entries which correspond to the different possible configurations of the 13 largest discs.
For each of these entries, we store the minimum of the four possibilities for the smallest disc.
This row corresponds to compressing cliques since all four possible positions of the smallest
disc can be reached from one another with a single move. The third row compresses the
two smallest discs of the 14 largest by storing the minimum of the 16 possibilities in a single
entry and so on.
The most important result here is that when compressing the PDB by several orders of
magnitude, most of the information is preserved. For example, compressing the five smallest
of the 14 largest discs reduces the memory by factor of 45 = 1024, but only increased the
search effort by less than a factor of two in both the number of generated nodes and the
time to solve the problem. When the PDB was compressed by a factor of 49 = 262, 144,
the search effort only increased by a about a factor of ten.
Comparing the first four lines of tables 2 and 4 shows that for TOH4, compressing a
larger PDB into a smaller size by the smallest discs is much better then an original PDB of
3. This does not contradict equation 2 of section 4.3.1. That equation dealt with a configuration that both
the compressed and uncompressed PDBs were indexed by the same variables. In our case, however,
we compared a compressed PDB that was indexed by discs 2-14 (where disc 1 was compressed) to an
uncompressed PDB indexed by discs 1-13. The equation will be valid if we compare an uncompressed
PDB of discs 2-14 compared to a our compressed PDB.

232

Compressed Pattern Databases

PDBs

Discs

14-3
14-3
15-2
16-1

0
0
1
2

16-2

2

Type

Avg h
Nodes
17-disc problem
static
90.5 >393,887,912
dynamic
95.7
238,561,590
static
103.7
155,737,832
static
123.8
17,293,603
18-disc problem
static
123.8
380,117,836

Time

Memory

>421
2501
83
7

256M
256M
256M
256M

463

256M

Table 5: Solving the 17 and 18-disc Towers of Hanoi
the same size. For example, comparing the third line of these tables shows that a compressed
PDB with 16 megabytes of memory solved the problem more than ten times faster than a
simple uncompressed PDB of the same size where the indices are based on the same discs
(discs 1-12 in our case). This is empirical evidence for equation 2 of section 4.3.1.
The last row of Table 4 represents lossless compression of the full 14-disc PDB by the
smallest of the 14 discs, where we stored one additional bit for each position of that disc.
This used 8 + 4 = 12 bits per four entries instead of 8 · 4 = 32 bits in the uncompressed
PDB (row 1). While the number of generated nodes is identical to row one, the table clearly
shows that it is not worthwhile using lossless compression for TOH4 since it requires more
time and more memory than lossy compression by the one or two smallest discs (rows 2
and 3).
The Avg h column gives the average heuristic over all entries of the PDB. The difference between the average h value of a compressed PDB, and the average h value of the
uncompressed PDB (87.04), gives the average loss of information due to compression. The
maximum possible loss of information for lossy compression by z discs, the diameter r, is
presented in the fourth column of Table 4. This is the length of an optimal solution for
a problem with z discs if z is less than 15, since for those problems, two states that are
furtherest apart in the problem space are the standard initial and goal states.4 We observe
that the average loss of information is about half the maximum possible information loss d.
To summarize this set of experiments, we conclude that compressing the large discs is not
effective because the values of compressed patterns are not highly correlated. By contrast,
compressing the small discs is extremely efficient because the values of the compressed
entries are highly correlated.
5.4 17 and 18-Disc Problems
We also solved the 17- and 18-disc problems by compressing the smallest discs. The optimal
solutions from the standard initial state are 193 and 225 moves respectively. Results are
presented in Table 5. Static partitioning of the largest 14 discs and the smallest 3 discs
cannot solve the 17-disc problem, since memory is exhausted before reaching the goal after
4. Surprisingly, this is not true with 15 discs, or 20 or more discs (Korf, 2003, 2004).

233

Felner, Korf, Meshulam, & Holte

7 minutes (row 1). With an uncompressed PDB of 14 discs we were only able to solve the
17-disc problem with dynamic partitioning (row 2).
The largest PDB that we could compute entirely in 1 gigabyte of memory was for 16
discs. This database was constructed with a bit-array to detect duplicate nodes, requiring
416 = 4 gigabits, or half a gigabyte. Given the same amount of memory as the full 14-disc
PDB, 256MB, we solved the 17-disc problem in 83 seconds with a 15-disc PDB compressed
by the smallest disc, and in 7 seconds with a 16-disc PDB compressed by the smallest two
discs. This is an improvement of almost two orders of magnitude compared to row 1. The
improvement is 2.5 orders of magnitude compared to the dynamically partitioned heuristic
of the 14-disc PDB of row 2. A PDB of 16 discs compressed by 2 discs consumes exactly
the same amount of memory as an uncompressed PDB of 14 discs but it is much more
informed, as it includes almost all the information about 16 discs. With this PDB we were
also able to solve the 18-disc problem in under 8 minutes.
5.5 Using Symmetry and Disk Storage to Solve up to 31 Discs
The algorithms described above are able to find shortest paths between any legal states of
TOH4. However, we can do much better if we’re only interested in the shortest path from
the standard initial state, where all discs are located on one peg, to the standard goal state,
where they are all located on another peg (Hinz, 1997). To do this we take advantage of
the symmetry between the standard initial and goal states. In particular, we only need
to search half way to the goal, to the first middle state where all discs but the largest are
distributed over the two intermediate pegs. Given such a middle state, we can reach the
goal state by moving the largest disc to the goal peg, and then applying the moves made
to reach the middle state in reverse order, but interchanging the initial and goal pegs.
The challenge is to take advantage of a heuristic function in this half-depth search to
a middle state. The difficulty is that to solve an n disc problem, there are 2n−1 middle
states, one for each way to distribute the n − 1 smallest discs over the two intermediate
pegs. A PDB heuristic provides a solution to this problem. Rather than building the PDB
by a breadth-first search starting with a single goal state, we simply start the search with
all 2n−1 middle states at depth zero, and search breadth-first until the entire pattern space
is generated. The resulting PDB values will be the minimum number of moves required to
reach any of the middle states. We refer to this as a Multiple-Goal Pattern Database, or
MGPDB (Korf & Felner, 2007).
We constructed our heuristic as follows. We first constructed a 22-disc MGPDB compressed to the size of a 15-disc PDB, by compressing the 7 smallest discs. This database
occupied exactly a gigabyte of memory, at one byte per entry, and was constructed in
memory, using magnetic disk storage for the states of the breadth-first search. We also
constructed a separate 8-disc MGPDB, which required very little time or memory. We
then used these MGPDBs to compute heuristics for problems with up to 31 discs. For the
31-disc problem, the goal of the search is a middle state where the 30 smallest discs are
distributed over the two intermediate pegs. Thus, we don’t need to consider the largest disc.
For each state of the search, we statically divided the 30 discs into the largest 22 discs, and
the smallest 8 discs. We looked up the configuration of the 22 largest discs in the 22-disc
MGPDB, we looked up the configuration of the 8 smallest discs in the 8-disc MGPDB, and
234

Compressed Pattern Databases

added the resulting values together. Similarly, we also looked up the configuration of the
22 smallest discs in the same 22-disc MGPDB, and looked up the configuration of the 8
largest discs in the same 8-disc MGPDG, and added theses values together as well. Finally,
we took the maximum of these two sums as our overall heuristic.
The search algorithm we used was a frontier version of breadth-first heuristic search
(BFHS) (Zhou & Hansen, 2006), with the A* cost function of f (s) = g(s) + h(s), where
g(s) is the depth of state s from the initial state, and h(s) is the MGPDB heuristic described
above, which estimates the distance to the closest middle state. Using these techniques, and
a number of others, we were able to verify the presumed optimal solution for all four-peg
Towers of Hanoi problems with up to 31 discs. The 31-disc problem took over 100 CPU-days
to run, and used two terabytes of disk storage. Due to an unrecoverable disk error and the
resulting loss of a single disk block, there is a one in 200 million probability that there is
a shorter solution to the 31-disc problem. We were able to solve all smaller problems with
no errors. This represents the current state-of-the-art for the four-peg Towers of Hanoi
Problem. The interested reader is referred to (Korf & Felner, 2007) for more details on
these experiments.

6. The Sliding-Tile Puzzles
11
00
00
11
00
11
00
11
00
11

7−7−1 partitioning

Figure 6: A 7-7-1 partitioning into disjoint sets of the 15-Puzzle
The best method for solving the sliding-tile puzzles optimally uses disjoint additive
pattern databases (Korf & Felner, 2002). In this case, variables represent tiles and values
represent their locations. The tiles are partitioned into disjoint sets, and a PDB is built for
each set. The PDB stores the cost of moving the tiles in the pattern set from any given
arrangement to their goal positions. Since for each set of pattern tiles we only count moves
of the pattern tiles, and each move only moves one tile, values from different disjoint PDBs
can be added together and the results are still admissible. For a deeper analysis of additive
PDBs, see (Felner et al., 2004a).
An x − y − z partitioning is a partition of the tiles into disjoint sets with cardinalities x,
y and z. Figure 6 shows the 7-7-1 disjoint partitioning of the 15-puzzle used in this paper.
The geometric symmetry of this domain can be used to allow another set of PDB lookups
(Culberson & Schaeffer, 1998; Korf & Felner, 2002; Felner et al., 2004a). For example, if we
reflect the puzzle about the main diagonal, we get another partitioning of this puzzle which
is geometrically symmetric to the original partitioning. Therefore, the same PDB can be
used to retrieve values for both the regular partitioning and for the reflected partitioning.
The maximum of these values can then be taken as an admissible heuristic.
235

Felner, Korf, Meshulam, & Holte

6.1 Combining Functional Heuristics with PDBs
In many domains there exist simple functional heuristics that can be calculated very efficiently. An example is the Manhattan distance (MD) heuristic for the sliding-tile puzzles.
In such domains, a PDB can store just the additional increment (∆) above the functional
heuristic, in order to save memory. We denote such a PDB as a DPDB. During the search
we add values from the DPDB to the value of the functional heuristic.
For the sliding-tile puzzles we can build a DPDB by storing just the additional increment
above MD, which results from conflicts between the tiles in the PDB. These conflicts come
in units of two moves, since if a tile moves away from its Manhattan-distance path, it must
return to that path again for a total of two additional moves. Compressing such a DPDB
for the sliding-tile puzzle can be very effective. Consider a pair of adjacent patterns of
the sliding-tile puzzle whose values are stored in a DPDB. Since they are adjacent, their
Manhattan distances differ by one, but the numbers of additional moves above their MDs
are often the same. Thus, much of the information is preserved when compressing the two
entries and taking their minimum.

3
6 7
1011
a) The goal pattern

0 0 3
0 0 0 7
0 0 1011
0 0 2 2
b) PDB values for tile 6

Figure 7: The goal pattern for tiles {3,6,7,10,11} and values for tile 6
For example, consider the subproblem of the 15-puzzle which includes tiles {3,6,7,10,11}.
The corresponding goal pattern is shown in Figure 7.a. Assume that all these tiles except
tile 6 are in their goal positions, and assume that tile 6 is in location x, which of course
cannot be any of {3,7,10,11}. The values in Figure 7.b written in location x correspond to
the number of moves above their MDs that the pattern tiles must move in order to properly
place tile 6 in its goal location, given that its current location is x. For example, suppose
that tile 6 is placed in the bottom row below where tile 10 belongs. In that case tile 6 is
in a linear conflict (Hansson, Mayer, & Yung, 1992) with tile 10, and one of them must
make at least two horizontal moves more than its MD. Thus we write the number 2 in that
location. For locations where no moves beyond MD are needed we write 0.
Note that most adjacent positions in Figure 7.b have the same value. Thus, if we build
such a DPDB and compress two entries that correspond to such patterns, no information
will be lost in most cases. In fact, for the 7-7-1 partition used in the experiments below,
when compressing two such patterns into one, we have found that more than 80% of the
pairs we compressed stored exactly the same ∆ value as before the compression.
6.2 Compressing PDBs for the Sliding-Tile Puzzles
Since the 15 puzzle is a permutation problem, both the sparse mapping and the compact
mapping defined in section 3.1 are applicable. For the 7-tile PDB of our experiments,
236

Compressed Pattern Databases

sparse mapping uses a multi-dimensional array of size 167 which has 268 × 106 entries.
Alternatively, compact mapping uses an array of size 16 × 15 × . . . × 10 which has 57 × 106
entries. Compact mapping is more complex to implement and turned out to be more time
consuming in our experiments but needs a smaller amount of memory.
6.2.1 Cliques in the Sliding-Tile Puzzles
In the sliding-tile puzzles a full compression of one variable, or tile, is not equivalent to
clique compression. Since every move moves a single tile to an adjacent location, then the
largest clique in the pattern space is of size two, or just a single edge. In such edges, the
locations of all the tiles of the pattern except one are fixed, and the remaining tile is in one
of two adjacent positions. We refer to compressing these two states as edge compression.
For the sparse mapping, there are 16 different entries for each variable which correspond
to the 16 different possible locations of the tile in the 15-puzzle. In order to take advantage
of edge compression, we divided the 16 locations into the following 8 pairs: (0,1), (2,3)
. . . (14,15). Instead of storing 16 entries for the location of the last tile, we store just 8
entries, one for each of these pairs. Thus, the size of the PDB will be 166 × 8, which is half
the size of the original PDB which has 167 different entries. Note that compressing these
particular edges is logically done by applying DIV 2 to the index of the last tile.
Compressing edges (cliques) with the compact mapping is more complicated. If the
PDB is based on P tiles, there are only 16 − P + 1 entries for the last tile. For example,
if P = 7, then out of the 16 different locations, only 10 are legal positions to place the
last tile, since 6 are occupied by the other tiles. If we use the same pairing mechanism
described above for compressing edges then we can compress the 16 locations to 8 entries.
This is only slightly smaller than the 10 entries of the original compact mapping. Edge
compressing will only be effective for the compact mapping if the number of pattern tiles is
considerably smaller than half the size of the puzzle. For example, in the 24-puzzle, it will
be efficient to compress edges for PDBs of 6 tiles even with the compact mapping. Note that
an alternative method would be to compress the 10 entries of the compact mapping into
5 entries (by Div 2). However, as shown below this will not necessarily compress adjacent
locations of the last tile, and therefore will not correspond to edge (clique) compression.
6.3 Results for the 15-Puzzle
Table 6 presents results for different compressing methods on the 15-puzzle for sparse mapping, and Table 7 presents similar results for compact mapping. All the values in the tables
are averages over the 1000 random initial states that were used by (Korf & Felner, 2002).
The Heuristic column defines the partitioning that was used. For example, 1 7-7-1 means
that we used one 7-7-1 partitioning. 2 7-7-1 means that we used two different 7-7-1 partitionings and took their maximum as the heuristic. A + means that we also took the
same partitioning and reflected it about the main diagonal. The next column indicates the
compressing method used. When a subscript ls is shown it means that lossless compression
was used, and otherwise the compression was lossy. The next columns present the number
of nodes generated by IDA*, the average running time in seconds, the amount of memory
in megabytes at one byte per entry, and the average heuristic of the initial states. The time
237

Felner, Korf, Meshulam, & Holte

No

Heuristic

1
2
3
4
5

1
1
1
1
1

6
7
8

1+
1+
1+

9
10

2
2+

Compress
Nodes Time
One PDB lookup
7-7-1 no
464,977 0.058
7-7-1 edge
565,881 0.069
7-7-1 edgels
487,430 0.070
7-7-1 row
1,129,659 0.131
7-7-1 2 tiles
1,312,647 0.152
A PDB lookup and its reflection
7-7-1 no
124,482 0.020
7-7-1 edge
148,213 0.022
7-7-1 row
242,289 0.048
Two different PDBs
7-7-1 edge
147,336 0.021
7-7-1 edge
66,692 0.016

Mem

Av h

524M
262M
262M
131M
131M

43.64
43.02
43.59
42.43
42.21

524M
262M
131M

44.53
43.98
43.39

524M
524M

43.98
44.92

Table 6: 15-puzzle results. Different compressing methods for one or more lookups with
sparse mapping

needed to precompute the PDB is traditionally omitted, since one only needs to precompute
it once, and the same PDB can be used to solve as many problem instances as needed.
6.3.1 Sparse mapping
Row 1 of Table 6 presents the benchmark results for the single 7-7-1 partitioning with no
compression.5 . The next four rows (2-5) present results for different compression methods
for this 7-7-1 PDB. Row 2 gives the results of the 7-7-1 PDB where the two 7-tile PDBs
were compressed by edges. While the size of the PDB was cut in half, the overall effort
was increased by no more than 20% in both the number of generated nodes and in overall
running time. Row 3 presents results of the same 7-7-1 partitioning when we used lossless
compression of edges. While the number of generated nodes decreased by 15% from the
lossy compression, the overall time increased a little. This is due to the additional constant
time for handling the lossless compression.6 Row 4 provides results for the case where the 16
different locations of one tile were compressed into four different entries by saving only the
row of the tile’s position, and not its column. This can be logically done by DIV 4. In both
cases of moving from row 1 to row 2 (in Table 6) and from row 2 to 4, the amount of memory
5. The best results for the 15 puzzle are achieved by using a 7-8 partitioning (Korf & Felner, 2002). This
partitioning cannot be implemented with sparse mapping as it would need 4 Giga bytes of memory.
Thus, we used a 7-7-1 partitioning for this set of experiments.
6. The reason that the number of generated nodes was not identical to the 7-7-1 partitioning without
compression is because in rare cases in our PDBs, the two entries we compressed were not edges and
differed by more than one move. Thus, information was lost even with the lossless compression that we
used. These rare cases are caused by the special way that we treated the location of the blank. A full
technical treatment how the blank is handled is provided by (Felner et al., 2004a).

238

Compressed Pattern Databases

was reduced by a factor of two. However, while the number of generated nodes increased
by 20% by edge compression, the number of generated nodes increased by a factor of 2.5
when going from edge compression to row compression. The reason is again the correlation
among the values of the compressed entries. With edge compression the loss of information
is at most one move, and this does not significantly effect the overall performance. With
row compression, the loss of information is much greater because the four values compressed
can be significantly different from each other. Thus, the effect on overall performance is
much greater. Row 5 represents an alternative way to compress the PDB by a factor of 4.
In this case, the locations of two tiles were compressed into eight locations each. Results
were a little worse than for row compression, because the correlation among the compressed
values was even less in this case.
The next three rows (6-8) show the same tendency when two sets of PDB lookups were
performed on the 7-7-1 PDB, the original set and the same set reflected about the main
diagonal. Row 6 presents the results for the uncompressed versions of these PDBs. Note
that an additional PDB lookup from the same PDB of row 1 improved the results by a
factor of almost four. Again when compressing edges in row 7, the correlation among the
compressed values is high, the loss of information is at most one move, and the running
time is roughly the same compared to no compression. With row compression in row 8, the
correlation of values is worse, and the search effort increases.
Edge compression causes a small loss of information but reduces the memory by half.
Thus, we can use the same amount of memory as the original uncompressed PDB but for
two compressed PDBs. Row 9 presents results when we took two different 7-7-1 PDBs, compressed them by a factor of two using edges, and took their maximum. This configuration
uses the same amount of memory as the benchmark uncompressed single 7-7-1 partitioning
of row 1, but solves the problem almost three times faster. Row 10 also computes the
reflection about the main diagonal of these two different compressed PDBs and takes the
maximum of the 4 different partitionings. This reduced the number of generated nodes by a
factor of 7 and a factor of 2, compared to the uncompressed versions with the same amount
of memory of row 1 and 6, respectively. In terms of running time the speedup was more
modest and was further decreased to 16 milliseconds. This is because we now have 4 PDB
lookups and there is a diminishing return in adding more lookups.7
6.3.2 Compact mapping
Table 7 presents results where the PDB was built by compact mapping. The first line
provides results for the same 7-7-1 partitioning used for the sparse mapping of table 6 but
with compact mapping. The indexing algorithm used for this line is a simple algorithm
where calculating the exact index takes time that is quadratic in the number of objects in
the pattern. Note that the number of generated nodes here is identical to line 1 of table 6
but the amount of memory needed is much smaller. On the other hand, our results show
that the actual CPU time for the compact mapping is worse than the corresponding sparse
mapping. In the rest of the table we used the advanced algorithm for index calculation of
7. See (Holte, Felner, Newton, Meshulam, & Furcy, 2006) for a deeper discussion of this and for advanced
methods to further reduce the constant time per node when a number of PDB lookups are performed.
Furthermore, the exact CPU time measured in all our experiments should be taken with care since it is
greatly influenced by the implementation, compiler and the hardware of the machine used.

239

Felner, Korf, Meshulam, & Holte

No

Heuristic

1

1 7-7-1

2
3
4
5
6

1
1
1
1
1

7-7-1
7-7-1
7-7-1
7-7-1
7-7-1

Compress
Nodes Time
Simple compact mapping
none
464,977 0.232
Advanced compact mapping
none
464,977 0.121
edge
565,881 0.142
edge ls
487,430 0.130
last tile
996,773 0.240
first tile
1,024,972 0.261

Mem

Av h

55M

43.64

55M
44M
44M
27.5M
27.5M

43.64
43.02
43.59
42.87
42.94

Table 7: 15-puzzle results. Compressing the compact mapping
compact mapping that was presented by (Korf & Shultze, 2005). In this advanced algorithm
the time to calculate the exact index is reduced to linear in the number of objects. This is
done with the help of another lookup table which stores values of shift operations needed
by the algorithm. See (Korf & Shultze, 2005) for a deeper discussion and treatment of this
method. Using the advanced algorithm further reduced the running time by a about a factor
of two over the simple quadratic algorithm but this is still slower than the corresponding
sparse mapping reported in table 6.
Lines 3 and 4 provide results for lossy and lossless edge compressing with the advanced
compact mapping. The results report the same number of generated nodes as the corresponding sparse mapping from table 6. Compact mapping only has 10 entries for the last
tile (since 6 locations were occupied by the other tiles). Edge compression (lines 3 and 4)
slightly reduced this to 8 entries offering a small memory reduction of only 20% over the
uncompressed 7-7-1 compact mapping PDB. This is probably not cost effective and it will
not allow another compressed PDB to be stored in the same amount of memory as the
uncompressed PDB. Line 5 presents results where we compressed the index of the last tile
from 10 entries into 5 by taking the maximum of two adjacent entries. This is equivalent
to Div 2. The memory reduction here is a factor of 2. However, since neighboring entries
in the compact mapping do not necessarily correspond to cliques, the number of generated
nodes significantly increased as the loss of information is not guaranteed to be at most 1 as
in edge (clique) compressing. The last line provides similar results where the compressing
was performed on the index of the first tile (from 16 locations to 8). This is equivalent to
MOD 2. Again, the entries that were compressed are not cliques and the loss of data was
rather large.
6.3.3 Summary of the 15 puzzle results
Our results on the 15 puzzle show that only edge (clique) compressing is effective on this
puzzle and most of the information is preserved by this type of compressing. Other compressing techniques cause significant loss of information and are not efficient on this domain.
Edge compressing provides a significant memory reduction of a factor of 2 if sparse mapping
is used. This is encouraging since sparse mapping will probably be the best choice for larger
versions of the puzzle (e.g., 24, 35, etc.). If one chooses to use the compact mapping then
240

Compressed Pattern Databases

memory saved by edge compressing is rather modest and probably not effective. Unlike the
4-peg Towers of Hanoi problem, we could not find a compressing method with a memory
saving factor larger than 2 that proved efficient in the tile puzzle.
The best existing heuristic for the 15 puzzle is a PDB based on a 7-8 partitioning of the
tiles, and its reflection about the main diagonal (Korf & Felner, 2002). We solved the problem by implementing this partitioning with compact mapping which used 562 megabytes.
The average number of generated nodes was 36,710, and the average running time was
.018 seconds for fast compact mapping. This is a reduction of almost a factor of two over
the simple quadratic compact mapping. Since the 7-8 PDBs are implemented by compact
mapping we could not improve these results with compression. It is worth noting that
our best version of the 7-7-1 partitioning (line 10 of Table 6) uses slightly less memory
(524 megabytes) and runs slightly faster(.016 seconds), but generates twice as many nodes.
(66,692).
6.4 Results for the 24-Puzzle

Figure 8: The 6-6-6-6 PDB for 24-Puzzle and its reflection
The best existing heuristic for the 24-puzzle when only one gigabyte of memory is
available is the 6-6-6-6 partitioning and its reflection about the main diagonal (Korf &
Felner, 2002) shown in Figure 8. We compressed the same 6-6-6-6 partitioning and found
that similar to the 15 puzzle lossy compression of edges was effective and only generated
about 20% more nodes. However, by adding another 6-6-6-6 partitioning we could not
achieve any significant reduction in the overall time. Due to geometrical attributes of the
puzzle, the 6-6-6-6 partitioning and its reflection from (Korf & Felner, 2002) are so good
that adding another set of 6-6-6-6 partitioning, even without any compression, only achieves
a small reduction in node generations.
We also tried a 7-7-5-5 partitioning and its refection, which were stored in one gigabyte
of memory by compressing the 7-tile PDBs. Even without compression, the number of
generated nodes was not much better than the 6-6-6-6 partitioning which is probably the
best 4-way partitioning of this puzzle.
One way to obtain a speedup in this domain might be to compress larger PDBs such
as an 8-8-8 partitioning, but that is beyond the scope of this work. An 8-8-8 PDB was
implemented by (Felner & Adler, 2005) using a sophisticated method of instance-dependent
PDBs. This method is based on an idea first presented by (Zhou & Hansen, 2004), in which
only the relevant parts of the PDB are stored in memory given particular initial and goal
states.
241

Felner, Korf, Meshulam, & Holte

7. Top-Spin
We have tried different compression methods on the Top-Spin domain as well. In Top-Spin
more than one object is moved in each move. Therefore, simple disjoint additive PDBs are
not applicable here. The simple way to build a PDB for this domain is to specify a number
of pattern tokens, and for the remaining tokens to be indistinguishable. Here, we only used
the compact mapping.
7.1 Cliques in Top-Spin
Due to the nature of Top-Spin, the largest cliques are of size two, or simply edges, but unlike
the other domains they do not correspond to compression of single variables. For example,
assume a 3-token PDB of tokens (2, 3, 4) for the (9,4) Top-Spin problem. Note that patterns
(∗, 2, 3, 4, ∗, ∗, ∗, ∗, ∗) and (∗, ∗, 4, 3, 2, ∗, ∗, ∗, ∗) are adjacent, and therefore belong to an edge
clique of size two. However, all three tokens move here, and hence all three pattern variables
change their values. Thus, compressing edges (cliques) is not as simple as compressing the
value of a single variable in this problem.
7.2 Compression Based on Individual Variables
In fact, in Top-Spin, compressing by individual variables is the same as ignoring those
variables. In other words, a PDB based on P variables, compressed by C variables, is
actually identical in both memory and values to a PDB based on P − C variables.
To explain this, we first examine compression by a single variable. Consider a threevariable P DB3 based on tokens (1, 2, 3), and a four-variable P DB4 based on tokens (1, 2, 3, 4)
for the (9,4) Top-Spin problem. The non-pattern tokens are represented by ∗. The goal pattern for P DB3 is (1, 2, 3, ∗, ∗, ∗, ∗, ∗, ∗), and the goal pattern for P DB4 is (1, 2, 3, 4, ∗, ∗, ∗, ∗, ∗).
Let p1 = (1, 2, ∗, ∗, ∗, ∗, ∗, ∗, 3) be an example pattern in P DB3 . We can reach p1 from the
goal pattern in two moves, and since the operators in this space are their own inverses,
P DB3 (p1 ) = 2. If we apply these same two moves to the pattern (1, 2, 3, 4, ∗, ∗, ∗, ∗, ∗),
we reach the pattern (1, 2, ∗, ∗, 4, ∗, ∗, ∗, 3), which we’ll call p2 . Thus, P DB4 (p2 ) = 2.
Now consider P DB4−1 which is P DB4 compressed by token or variable 4. By definition,
P DB4−1 (p1 ) is the minimum value over all locations for token 4, of P DB4 (pi ). In fact,
we constructed p2 such that P DB4 (p2 ) = P DB3 (p1 ). Since there can be no smaller value,
P DB4−1 (p1 ) = P DB4 (p2 ) = P DB3 (p1 ). The same argument applies to any other example
pattern. Furthermore, we can apply the same argument to a PDB built by compressing over
any number of individual variables. Thus in the special case of Top-Spin, equation 2 from
section 4.3.1 becomes CP DBP/C (s) = P DBP −C (s). This means that for Top-Spin, compression based on individual variables offers no benefits at all compared to simply ignoring
those variables in a PDB of the same size.
7.3 Experimental Results for Top-Spin
Since compression based on individual variables is of no benefit here, we tried general compression by applying the DIV and M OD operators to the PDB indices. We experimented
with the (17,4)-Top-Spin problem and started with a compact mapping PDB of 9 consecutive tokens, including the 1 token. Note that with a few exceptions noted below, these
242

Compressed Pattern Databases

Comp. Factor
1
2
3
4
5
6
7
8
9
1

Avg. Value

Avg Nodes
9-token PDB
10.52
40,810,940
10.20
59,827,209
10.03
87,517,365
9.88
86,424,249
9.76 127,276,981
9.69 147,626,798
9.61 128,757,535
9.54 159,711,937
9.53 313,375,790
8-token PDB
9.53 313,375,790

Avg Time

Size of PDB

87.36
127.54
183.70
184.19
264.33
307.42
267.73
331.97
650.83

495M
247M
164M
123M
99M
82M
71M
62M
55M

650.83

55M

Table 8: Results for (17,4) Top-Spin with PDBs of 9 and 8 tokens where the 9-token PDB
is compressed by DIV operator

Comp. Factor
1
2
3
4
5
6
7
8
9

Avg. Value
10.52
10.13
9.89
9.82
9.65
9.53
9.38
9.49
9.17

Avg Nodes
40,810,941
50,363,034
57,576,194
76,123,453
87,573,074
87,356,615
85,280,514
152,480,885
89,543,322

Avg Time
87.37
109.88
119.86
158.47
183.05
186.15
205.25
321.23
197.18

Size of PDB
495M
247M
164M
123M
99M
82M
71M
62M
55M

Table 9: Results for(17,4) Top-Spin with a 9-token PDB compressed with MOD

compressions do not preserve the structure of the state variables and there is no correlation
of values between the compressed entries.
Table 8 presents the results of DIV compression performed on our PDB. Each line
represents a different compression factor, which is the second argument to the DIV operator.
As expected, a larger compression factor increased the search effort, but reduced the amount
of memory. Note that the DIV 9 corresponds to compression based on a single variable,
since once 8 variables have been set in a 17-variable permutation problem, there are exactly
9 possible locations remaining for the next variable. Thus, as explained in the previous
section this line is identical to an uncompressed PDB of size 8 (the last line of the table).
243

Number of generated nodes (in Millions)

Felner, Korf, Meshulam, & Holte

350
DIV
MOD

300
250
200
150
100
50
0
1

2

3

4
5
6
7
compression degree

8

9

Figure 9: Nodes generated by DIV and MOD for the 9-token PDB of the (17,4)- Top-Spin
problem

Table 9 shows the results of compressing the same PDB with the MOD operator, in
the same form as Table 8. Figure 9 compares the two methods. Surprisingly, MOD outperformed DIV. This is counterintuitive because DIV compresses entries that tend to have
many state variables in common. Our best explanation for this phenomenon is as follows.
As described above, in Top-Spin the location of several tokens is changed in a single move.
We believe that the distance between two states that are similar, such as differing by a swap
of two tokens for example, is greater than the distance between a randomly-chosen pair of
states.8 Thus, states that are grouped together by the DIV operator will tend to be less
highly correlated than random groupings, or groupings induced by the MOD operator.
Note that the last line of Table 9 (compression by MOD 9) used the same amount of
memory as an uncompressed 8-token PDB (last line of Table 8) but takes only 30% as
long to solve the problems. This shows the benefit of compression in this problem as well.
Compression by the MOD operator is effective and for the given size of memory of the
8-token PDB, it is better to build a larger 9-token PDB and compress it to that size. This
reduces the search effort by a factor of 3.

8. Conclusions
We introduced a new method for improving the performance of PDBs, by compressing
larger PDBs to fit into smaller amounts of memory. We applied the technique to the
four-peg Towers of Hanoi problem, the sliding-tile puzzles, and the Top-Spin puzzle. Our
experiments confirm that given a specific amount of memory M , it is usually better to use
this memory with compressed PDBs than with uncompressed PDBs. This can be practically
achieved either by a larger PDB compressed to size M , or by maximizing over k compressed
PDBs each of size M/k.
8. We performed several experiments to confirm this. Indeed, for the (12,4) - Top-Spin version, a pair of
random states are distanced 9.28 moves away from each other on average while two states that only
differ in two adjacent tokens are 12 moves away. Similarly, for the (16,4) - TopSpin the average distances
were 14.04 and 16 moves, respectively.

244

Compressed Pattern Databases

We introduced a number of different methods for compressing a PDB, ranging from very
general mapping functions of the indices such as DIV and MOD, to methods which take
into account the structure of the problem space. In general, we want to group together PDB
entries that have similar values. This can be achieved by grouping patterns that are close
to each other in the pattern space. In particular, states that form a clique in the problem
space will have PDB values that differ by at most one, and hence are natural candidates for
compression. The exact method for finding these cliques (or nearby patterns) in the PDB
depends on the domain and on the exact way that the PDB is implemented.
For TOH4 and for the sparse mapping of the tile puzzle, a PDB can be constructed very
easily in such a way that cliques reside in nearby entries of the PDB and thus, the PDB
values are locally correlated. Therefore, compressing nearby PDB entries proved useful
for these settings. For Top-Spin, however, values of a PDB stored in the standard way
are not locally correlated and thus compressing nearby PDB entries was not effective. In
this domain it is effective to compress patterns that are far apart in the PDB. Similarly,
neighboring entries for the compact mapping of the tile puzzle do not correspond to cliques.
Thus, compression did not prove useful in this setting and we could not improve the 7-8
partitioning of the 15 puzzle.
In the Towers of Hanoi problem, we achieved dramatic improvements of several orders
of magnitude in running time, compared to uncompressed PDBs of the same size, and used
this technique in verifying the optimal solutions for problems with up to 31 discs, which
is the current state of the art. For the sliding-tile puzzles we showed that compression
can preserve most of the information and our techniques offer some practical improvements
for the sparse mapping but not for the compact mapping. For the Top-Spin, we achieved
improvements with a very naive compression method (the MOD method).
We also described several methods for generating PDBs that are too large to fit into
memory prior to compression, by using auxiliary disk storage.

9. Acknowledgements
This research was supported by the Israel Science Foundation (ISF) grant No. 728/06 to
Ariel Felner. It was also supported by NSF grant No. EIA-0113313 to Richard Korf.

References
Bloom, B. H. (1970). Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(3), 422–426.
Chen, T., & Skiena, S. (1996). Sorting with fixed-length reversals. Discrete Applied Mathematics, 71 (1-3), 269–295.
Culberson, J. C., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence,
14 (3), 318–334.
Dunkel, O. (1941). Editorial note concerning advanced problem 3918. American Mathematical Monthly, 48, 219.
Dunkel, O. (1992). Symbolic boolean manipulation with ordered binary decision diagrams.
ACM Computing Surveys, 24(3), 142–170.
245

Felner, Korf, Meshulam, & Holte

Edelkamp, S. (2001). Planning with pattern databases. In Proceedings of the 6th European
Conference on Planning (ECP-01), pp. 13–34.
Edelkamp, S. (2002). Symbolic pattern databases in heuristic search planning. In Proc.
International Conference on AI Planning and Scheduling (AIPS), pp. 274–293.
Edelkamp, S., & Kissmann, P. (2007). Externalizing the multiple sequence alignment problem with affine gap costs. In German Conference on Artificial Intelligence (KI), LNCS
4467, pp. 444–447.
Felner, A., & Adler, A. (2005). Solving the 24-puzzle with instance dependent pattern
databases. In Proceedings of SARA-05, pp. 248–260, Edinburgh, Scotland.
Felner, A., Korf, R. E., & Hanan, S. (2004a). Additive pattern database heuristics. Journal
of Artificial Intelligence Research (JAIR), 22, 279–318.
Felner, A., Meshulam, R., Holte, R., & Korf, R. (2004b). Compressing pattern databases. In
Proceedings of the Nineteenth National Conference on Artificial Intelligence (AAAI04), pp. 638–643.
Felner, A., Zahavi, U., Holte, R., & Schaeffer, J. (2005). Dual lookups in pattern databases. In Proceedings of the Nineteenth International Joint Conference on Artificial
Intelligence (IJCAI-05), pp. 103–108.
Frame, J. S. (1941). Solution to advanced problem 3918. American Mathematical Monthly,
48, 216–217.
Hansson, O., Mayer, A., & Yung, M. (1992). Criticizing solutions to relaxed models yields
powerful admissible heuristics. Information Sciences, 63(3), 207–227.
Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). A formal basis for the heuristic determination of minimum cost paths. IEEE Transactions on Systems Science and Cybernetics,
SCC-4(2), 100–107.
Hinz, A. M. (1997). The tower of Hanoi. In Algebras and Combinatorics: Proceedings of
ICAC’97, pp. 277–289, Hong Kong. Springer-Verlag.
Holte, R. C., Felner, A., Newton, J., Meshulam, R., & Furcy, D. (2006). Maximizing over
multiple pattern databases speeds up heuristic search. Artificial Intelligence, 170,
1123–1136.
Korf, R. E. (1985). Depth-first iterative-deepening: An optimal admissible tree search.
Artificial Intelligence, 27(1), 97–109.
Korf, R. E. (1997). Finding optimal solutions to Rubik’s Cube using pattern databases. In
Proceedings of the Fourteenth National Conference on Artificial Intelligence (AAAI97), pp. 700–705.
Korf, R. E. (2003). Delayed duplicate detection: Extended abstract. In Proceedings of the
18th International Joint Conference on Artificial Intelligence (IJCAI-03), pp. 1539–
1541, Acapulco, Mexico.
Korf, R. E. (2004). Best-first frontier search with delayed duplicate detection. In Proceedings
of the 19th National Conference on Artificial Intelligence (AAAI-2004), pp. 650–657,
San Jose, CA.
246

Compressed Pattern Databases

Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134, 9–22.
Korf, R. E., & Felner, A. (2007). Recent progress in heuristic search: A case study of
the four-peg towers of hanoi problem. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI-07), pp. 2324–2329.
Korf, R. E., & Shultze, P. (2005). Large-scale, parallel breadth-first search. In Proceedings of
the 20th National Conference on Artificial Intelligence (AAAI-2005), pp. 1380–1385,
Pittsburgh, PA.
Korf, R. E., Zhang, W., Thayer, I., & Hohwald, H. (2005). Frontier search. Journal of the
Association for Computing Machinery (JACM), 52 (5), 715–748.
McNoughtton, M., Lu, P., Schaeffer, J., & Szafron, D. (2002). Memory efficient A* heuristics
for multiple sequence alignment. In Proceedings of the Eighteenth National Conference
on Artificial Intelligence (AAAI-02), pp. 737–743.
Mero, L. (1984). A heuristic search algorithm with modifiable estimate. Artificial Intelligence, 23, 13–27.
Munagala, K., & Ranade, A. (1999). I/o complexity of graph algorithms. In Proceedings of
the 10th Annual Symposium on Discrete Algorithms, pp. 687–694. ACM-SIAM.
Myrvold, W., & Ruskey, F. (2001). Ranking and unranking permutations in linear time.
Information Processing Letters, 79, 281–284.
Schroedl, S. (2005). An improved search algorithm for optimal multiple-sequence alignment.
Journal of Artificial Intelligence Research (JAIR), 23, 587–623.
Stern, U., & Dill., D. L. (1995). Improved probabilistic verification by hash compaction. In
Advanced Research Working Conference on Correct Hardware Design and Verification
Methods, pp. 206–240.
Stewart, B. (1941). Solution to advanced problem 3918. American Mathematical Monthly,
48, 217–219.
Zahavi, U., Felner, A., Holte, R., & Schaeffer, J. (2006). Dual search in permutation state
spaces. In Proceedings of the Twenty First National Conference on Artificial Intelligence (AAAI-06), pp. 1076–1081.
Zahavi, U., Felner, A., Schaeffer, J., & Sturtevant, N. (2007). Inconsistent heurstics. In Proceedings of the Twenty Second National Conference on Artificial Intelligence (AAAI07). To appear.
Zhou, R., & Hansen, E. (2004). Space-efficient memory-based heuristics. In Proceedings of
the Nineteenth National Conference on Artificial Intelligence (AAAI-04), pp. 677–682.
Zhou, R., & Hansen, E. (2006). Breadth-first heuristic search. Artificial Intelligence, 170 (45), 385–408.

247

Journal of Artificial Intelligence Research 30 (2007) 457-500

Submitted 05/07; published 11/07

Using Linguistic Cues for the Automatic Recognition of
Personality in Conversation and Text
François Mairesse

f.mairesse@sheffield.ac.uk

Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Marilyn A. Walker

m.a.walker@sheffield.ac.uk

Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Matthias R. Mehl

mehl@email.arizona.edu

Department of Psychology, University of Arizona
1503 E University Blvd. Building 68, Tucson, AZ 85721, USA

Roger K. Moore

r.k.moore@dcs.shef.ac.uk

Department of Computer Science, University of Sheffield
211 Portobello Street, Sheffield S1 4DP, United Kingdom

Abstract
It is well known that utterances convey a great deal of information about the speaker
in addition to their semantic content. One such type of information consists of cues to the
speaker’s personality traits, the most fundamental dimension of variation between humans.
Recent work explores the automatic detection of other types of pragmatic variation in
text and conversation, such as emotion, deception, speaker charisma, dominance, point
of view, subjectivity, opinion and sentiment. Personality affects these other aspects of
linguistic production, and thus personality recognition may be useful for these tasks, in
addition to many other potential applications. However, to date, there is little work on the
automatic recognition of personality traits. This article reports experimental results for
recognition of all Big Five personality traits, in both conversation and text, utilising both
self and observer ratings of personality. While other work reports classification results, we
experiment with classification, regression and ranking models. For each model, we analyse
the effect of different feature sets on accuracy. Results show that for some traits, any type
of statistical model performs significantly better than the baseline, but ranking models
perform best overall. We also present an experiment suggesting that ranking models are
more accurate than multi-class classifiers for modelling personality. In addition, recognition
models trained on observed personality perform better than models trained using selfreports, and the optimal feature set depends on the personality trait. A qualitative analysis
of the learned models confirms previous findings linking language and personality, while
revealing many new linguistic markers.

1. Introduction
Personality is the complex of all the attributes—behavioural, temperamental, emotional
and mental—that characterise a unique individual.

It is well known that utterances convey a great deal of information about the speaker
in addition to their semantic content. One such type of information consists of cues to the
c
2007
AI Access Foundation. All rights reserved.

Mairesse, Walker, Mehl & Moore

speaker’s personality traits, the most fundamental dimension of variation between humans.
Personality is typically assessed along five dimensions known as the Big Five:
• Extraversion vs. Introversion (sociable, assertive, playful vs. aloof, reserved, shy)
• Emotional stability vs. Neuroticism (calm, unemotional vs. insecure, anxious)
• Agreeableness vs. Disagreeable (friendly, cooperative vs. antagonistic, faultfinding)
• Conscientiousness vs. Unconscientious (self-disciplined, organised vs. inefficient, careless)
• Openness to experience (intellectual, insightful vs. shallow, unimaginative)
These five personality traits have been repeatedly obtained by applying factor analyses
to various lists of trait adjectives used in personality description questionnaires (sample
adjectives above) (Norman, 1963; Peabody & Goldberg, 1989; Goldberg, 1990). The basis
for such factor analyses is the Lexical Hypothesis (Allport & Odbert, 1936), i.e. that the
most relevant individual differences are encoded into the language, and the more important
the difference, the more likely it is to be expressed as a single word. Despite some known
limits (Eysenck, 1991; Paunonen & Jackson, 2000), over the last 50 years the Big Five model
has become a standard in psychology and experiments using the Big Five have shown that
personality traits influence many aspects of task-related individual behaviour. For example,
the success of most interpersonal tasks depends on the personalities of the participants, and
personality traits influence leadership ability (Hogan, Curphy, & Hogan, 1994), general job
performance (Furnham, Jackson, & Miller, 1999), attitude toward machines (Sigurdsson,
1991), sales ability (Furnham et al., 1999), teacher effectiveness (Rushton, Murray, & Erdle,
1987), and academic ability and motivation (Furnham & Mitchell, 1991; Komarraju &
Karau, 2005). However, to date there has been little work on the automatic recognition
of personality traits (Argamon, Dhawle, Koppel, & Pennebaker, 2005; Mairesse & Walker,
2006a, 2006b; Oberlander & Nowson, 2006).
Recent work in AI explores methods for the automatic detection of other types of pragmatic variation in text and conversation, such as emotion (Oudeyer, 2002; Liscombe, Venditti, & Hirschberg, 2003), deception (Newman, Pennebaker, Berry, & Richards, 2003;
Enos, Benus, Cautin, Graciarena, Hirschberg, & Shriberg, 2006; Graciarena, Shriberg,
Stolcke, Enos, Hirschberg, & Kajarekar, 2006; Hirschberg, Benus, Brenier, Enos, Friedman, Gilman, Girand, Graciarena, Kathol, Michaelis, Pellom, Shriberg, & Stolcke, 2005),
speaker charisma (Rosenberg & Hirschberg, 2005), mood (Mishne, 2005), dominance in
meetings (Rienks & Heylen, 2006), point of view or subjectivity (Wilson, Wiebe, & Hwa,
2004; Wiebe, Wilson, Bruce, Bell, & Martin, 2004; Wiebe & Riloff, 2005; Stoyanov, Cardie,
& Wiebe, 2005; Somasundaran, Ruppenhofer, & Wiebe, 2007), and sentiment or opinion
(Turney, 2002; Pang & Lee, 2005; Popescu & Etzioni, 2005; Breck, Choi, & Cardie, 2007).
In contrast with these pragmatic phenomena, which may be relatively contextualised or
short-lived, personality is usually considered to be a longer term, more stable, aspect of
individuals (Scherer, 2003). However, there is evidence that personality interacts with, and
affects, these other aspects of linguistic production. For example, there are strong relations
between the extraversion and conscientiousness traits and the positive affects, and between
458

Recognising Personality in Conversation and Text

neuroticism and disagreeableness and various negative affects (Watson & Clark, 1992). Lying leads to inconsistencies in impressions of the agreeableness personality trait across modes
(visual vs. acoustic), and these inconsistencies are used as cues for deception detection by
human judges (Heinrich & Borkenau, 1998). Outgoing and energetic people (i.e. extravert)
are more successful at deception, while apprehensive (i.e. neurotic) individuals are not as
successful (Riggio, Salinas, & Tucker, 1988), and individuals who score highly on the agreeableness and openness to experience traits are also better at detecting deception (Enos
et al., 2006). Features used to automatically recognise introversion and extraversion in our
studies are also important for automatically identifying deception (Newman et al., 2003).
Speaker charisma has been shown to correlate strongly with extraversion (Bono & Judge,
2004), and individuals who dominate meetings have similar characteristics to extraverts,
such as verbosity (Rienks & Heylen, 2006). Oberlander and Nowson (2006) suggest that
opinion mining could benefit from personality information. Thus this evidence suggests
that incorporating personality models into these other tasks may improve accuracy.
We also hypothesise that computational recognition of user personality could be useful in many other computational applications. Identification of leaders using personality
dimensions could be useful in analysing meetings and the conversations of suspected terrorists (Hogan et al., 1994; Tucker & Whittaker, 2004; Nunn, 2005). Dating websites could
analyse text messages to try to match personalities and increase the chances of a successful
relationship (Donnellan, Conger, & Bryant, 2004). Tutoring systems might be more effective
if they could adapt to the learner’s personality (Komarraju & Karau, 2005). Automatically
identifying the author’s personality in a corpus could also improve language generation,
as individual differences in language affect the way that concepts are expressed (Reiter &
Sripada, 2004). Studies have also shown that users’ evaluation of conversational agents
depends on their own personality (Reeves & Nass, 1996; Cassell & Bickmore, 2003), which
suggests a requirement for such systems to adapt to the user’s personality, like humans do
(Funder & Sneed, 1993; McLarney-Vesotski, Bernieri, & Rempala, 2006).
While in some applications it would be possible to acquire personality information by
asking the user or author directly (John, Donahue, & Kentle, 1991; Costa & McCrae,
1992), here we explore whether it is possible to acquire personality models for the Big Five
personality traits by observation of individual linguistic outputs in text and conversation.
To date, we know of only two studies besides our own on automatic recognition of user
personality (Argamon et al., 2005; Mairesse & Walker, 2006a, 2006b; Oberlander & Nowson,
2006). Other work has applied classification models to the recognition of personality in
texts and blog postings. To our knowledge, the results presented here are the first to
examine the recognition of personality in dialogue (Mairesse & Walker, 2006a, 2006b), and
to apply regression and ranking models that allow us to model personality recognition
using the continuous scales traditional in psychology. We also systematically examine the
use of different feature sets, suggested by psycholinguistic research, and report statistically
significant results.
We start in Section 2 by reviewing the psychology findings linking personality and
language; these findings motivate the features used in the learning experiments described
in Section 3. Section 3 overviews the methods we use to automatically train personality
models, using both conversation and written language samples, and both self-ratings and
observer ratings of personality traits. We explore the use of classification models (Section 4),
459

Mairesse, Walker, Mehl & Moore

regression models (Section 5), and ranking models (Section 6), and the effect of different
feature sets on model accuracy. The results show that for some traits, any type of statistical
model performs significantly better than the baseline, but ranking models perform best
overall. In addition, models trained on observed personality scores perform better than
models trained using self-reports, and the optimal feature set is dependent on the personality
trait. The rules derived and features used in the learned models confirm previous findings
linking language and personality, while revealing many new linguistic markers. We delay
the review of Argamon et al. (2005) and Oberlander and Nowson (2006) to Section 7, when
we can better compare their results with our own, and sum up and discuss future work in
Section 8.

2. Personality Markers in Language
Why do we believe it might be possible to automatically recognise personality from linguistic
cues? Psychologists have documented the existence of such cues by discovering correlations
between a range of linguistic variables and personality traits, across a wide range of linguistic
levels, including acoustic parameters (Smith, Brown, Strong, & Rencher, 1975; Scherer,
1979), lexical categories (Pennebaker & King, 1999; Pennebaker, Mehl, & Niederhoffer,
2003; Mehl, Gosling, & Pennebaker, 2006; Fast & Funder, 2007), n-grams (Oberlander &
Gill, 2006), and speech-act type (Vogel & Vogel, 1986). As the correlations reported in
the literature are generally weak (see Section 3.3), it is not clear whether these features
will improve accuracies of statistical models on unseen subjects. Of all Big Five traits,
extraversion has received the most attention from researchers. However, studies focusing
systematically on all Big Five traits are becoming more common.
2.1 Markers of Extraversion
We summarise various findings linking extraversion and language cues in Table 1, for
different levels of language production such as speech, syntax and content selection. A review by Furnham (1990) describes linguistic features linked to extraversion and other traits,
and Dewaele and Furnham (1999) review studies focusing on the link between extraversion
and both language learning and speech production.
Findings include that there is a higher correlation between extraversion and oral language, especially when the study involves a complex task. Extraverts talk more, louder
and more repetitively, with fewer pauses and hesitations, they have higher speech rates,
shorter silences, a higher verbal output, a lower type/token ratio and a less formal language, while introverts use a broader vocabulary (Scherer, 1979; Furnham, 1990; Gill &
Oberlander, 2002). Extraverts also use more positive emotion words, and show more agreements and compliments than introverts (Pennebaker & King, 1999). Extravert students
learning French as a second language produce more back-channels, and have a more implicit style and a lower lexical richness in formal situations. It seems that the more complex
the task and the higher the level of anxiety, the easier it is to differentiate between introverts
and extraverts (Dewaele & Furnham, 1999).
Heylighen and Dewaele (2002) also note that extraversion is significantly correlated
with contextuality, as opposed to formality. Contextuality can be seen a high reliance
on shared knowledge between conversational partners, leading to the use of many deictic
460

Recognising Personality in Conversation and Text

Level
Conversational
behaviour
Topic
selection

Style
Syntax

Lexicon

Speech

Introvert
Listen
Less back-channel behaviour
Self-focused
Problem talk, dissatisfaction
Strict selection
Single topic
Few semantic errors
Few self-references
Formal
Many hedges (tentative words)
Many nouns, adjectives, prepositions (explicit)
Elaborated constructions
Many words per sentence
Many articles
Many negations
Correct
Rich
High diversity
Many exclusive and inclusive words
Few social words
Few positive emotion words
Many negative emotion words
Received accent
Slow speech rate
Few disfluencies
Many unfilled pauses
Long response latency
Quiet
Low voice quality
Non-nasal voice
Low frequency variability

Extravert
Initiate conversation
More back-channel behaviour
Not self-focused*
Pleasure talk, agreement, compliment
Think out loud*
Many topics
Many semantic errors
Many self-references
Informal
Few hedges (tentative words)
Many verbs, adverbs, pronouns (implicit)
Simple constructions*
Few words per sentence
Few articles
Few negations
Loose*
Poor
Low diversity
Few exclusive and inclusive words
Many social words
Many positive emotion words
Few negative emotion words
Local accent*
High speech rate
Many disfluencies*
Few unfilled pauses
Short response latency
Loud
High voice quality
Nasal voice
High frequency variability

Table 1: Summary of identified language cues for extraversion and various production levels, based on previous studies by Scherer (1979), Furnham (1990), Pennebaker
and King (1999), Dewaele and Furnham (1999), Gill (2003), Mehl et al. (2006).
Asterisks indicate that the cue is only based on a hypothesis, as opposed to study
results.

expressions such as pronouns, verbs, adverbs and interjections, whereas formal language is
less ambiguous and assumes less common knowledge. In order to measure this variation,
Heylighen and Dewaele suggest the use of a metric called formality, defined as:
F = (noun freq + adjective freq + preposition freq + article freq - pronoun freq - verb
freq - adverb freq - interjection freq + 100)/2
They argue that this measure is the most important dimension of variation between
linguistic expressions, as shown in Biber’s factor analysis of various genres (Biber, 1988).
In addition to introversion, the authors also find that formality correlates positively with
the level of education and the femininity of the speaker. Situational variables related to
the use of formal language are the audience size, the time span between dialogues, the
unavailability of feedback, difference of backgrounds and spatial location between speakers,
as well as the preceding amount of conversation.
461

Mairesse, Walker, Mehl & Moore

Scherer (1979) shows that extraverts are perceived as talking louder and with a more
nasal voice, and that American extraverts tend to make fewer pauses, while German extraverts produce more pauses than introverts. Thus personality markers are culture-dependent,
even among western societies.
Oberlander and Gill (2006) use content analysis tools and n-gram language models to
identify markers in extravert and introvert emails. They replicate previous findings and
identify new personality markers such as first person singular pronouns (e.g., I don’t) and
formal greetings (e.g., Hello) for introversion, while less formal phrases such as Take care
and Hi characterise extraverts.

2.2 Markers of Other Big Five Traits
Pennebaker and King (1999) identify many linguistic features associated with each of the
Big Five personality traits. They use their Linguistic Inquiry and Word Count (LIWC)
tool to count word categories of essays written by students whose personality has been
assessed using a questionnaire. The authors find small but significant correlations between
their linguistic dimensions and personality traits. Neurotics use more 1st person singular
pronouns, more negative emotion words and less positive emotion words. On the other hand,
agreeable people express more positive and fewer negative emotions. They also use fewer
articles. Conscientious people avoid negations, negative emotion words and words reflecting
discrepancies (e.g., should and would). Finally, openness to experience is characterised by a
preference for longer words and words expressing tentativity (e.g., perhaps and maybe), as
well as the avoidance of 1st person singular pronouns and present tense forms.
Additionally, Mehl et al. (2006) study markers of personality as perceived by observers.
They find that the use of words related to insight and the avoidance of past tense indicates
openness to experience, and swearing marks disagreeableness. The same authors also show
that some linguistic cues vary greatly across gender. For example, males perceived as
conscientious produce more filler words, while females don’t. Gender differences are also
found in markers of self-assessed personality: the use of 2nd person pronouns indicates a
conscientious male, but an unconscientious female.
Gill and Oberlander (2003) study correlates of emotional stability: they find that neurotics use more concrete and frequent words. However, they also show that observers don’t
use those cues correctly, as observer reports of neuroticism correlate negatively with selfreports.
Concerning prosody, Smith et al. (1975) also show that speech rate is positively correlated with perceived competence (conscientiousness), and that speech rate has an inverted-U
relationship with benevolence (agreeableness), suggesting a need for non-linear models.
Some traits have produced more findings than others. A reason for this might be that
some are more reflected through language, like extraversion. However, it is possible that
this focus is a consequence of extraversion being correlated with linguistic cues that can be
analysed more easily (e.g., verbosity).
462

Recognising Personality in Conversation and Text

3. Experimental Method
We conduct a set of experiments to examine whether automatically trained models can be
used to recognise the personality of unseen subjects. Our approach can be summarised in
five steps:
1. Collect individual corpora;
2. Collect associated personality ratings for each participant;
3. Extract relevant features from the texts;
4. Build statistical models of the personality ratings based on the features;
5. Test the learned models on the linguistic outputs of unseen individuals.
The following sections describe each of these steps in more detail.
3.1 Sources of Language and Personality
Introvert
I’ve been waking up on time so far. What
has it been, 5 days? Dear me, I’ll never
keep it up, being such not a morning
person and all. But maybe I’ll adjust,
or not. I want internet access in my
room, I don’t have it yet, but I will
on Wed??? I think. But that ain’t soon
enough, cause I got calculus homework [...]

Extravert
I have some really random thoughts. I
want the best things out of life.
But I fear that I want too much!
What if I fall flat on my face and
don’t amount to anything. But I
feel like I was born to do BIG things
on this earth. But who knows... There
is this Persian party today.

Neurotic
One of my friends just barged in, and I
jumped in my seat. This is crazy. I
should tell him not to do that again.
I’m not that fastidious actually. But
certain things annoy me. The things
that would annoy me would actually
annoy any normal human being, so I
know I’m not a freak.

Emotionally stable
I should excel in this sport because I
know how to push my body harder than
anyone I know, no matter what the test I
always push my body harder than everyone
else. I want to be the best no matter
what the sport or event. I should also
be good at this because I love to ride
my bike.

Table 2: Extracts from the essays corpus, for participants rated as extremely introvert,
extravert, neurotic, and emotionally stable.

We use the data from Pennebaker and King (1999) and Mehl et al. (2006) in our experiments. The first corpus contains 2,479 essays from psychology students (1.9 million
words), who were told to write whatever comes into their mind for 20 minutes. The data
was collected and analysed by Pennebaker and King (1999); a sample is shown in Table 2.
463

Mairesse, Walker, Mehl & Moore

Introvert
- Yeah you would do kilograms. Yeah I see
what you’re saying.
- On Tuesday I have class. I don’t know.
- I don’t know. A16. Yeah, that is kind of cool.
- I don’t know. I just can’t wait to be with
you and not have to do this every night,
you know?
- Yeah. You don’t know. Is there a bed in
there? Well ok just...

Extravert
- That’s my first yogurt experience here.
Really watery. Why?
- Damn. New game.
- Oh.
- That’s so rude. That.
- Yeah, but he, they like each other.
He likes her.
- They are going to end up breaking up
and he’s going to be like.

Unconscientious
- With the Chinese. Get it together.
- I tried to yell at you through the window.
Oh. xxxx’s fucking a dumb ass. Look at
him. Look at him, dude. Look at him. I
wish we had a camera. He’s fucking brushing
his t-shirt with a tooth brush. Get a kick
of it. Don’t steal nothing.

Conscientious
- I don’t, I don’t know for a fact but
I would imagine that historically women
who have entered prostitution have done
so, not everyone, but for the majority out
of extreme desperation and I think. I don’t
know, i think people understand that
desperation and they don’t don’t see [...]

Table 3: Extracts from the EAR corpus, for participants rated as extremely introvert, extravert, unconscientious, and conscientious. Only the participants’ utterances are
shown.

Personality was assessed by asking each student to fill in the Big Five Inventory questionnaire (John et al., 1991), which asks participants to evaluate on a 5 point scale how well
their personality matches a series of descriptions.
The second source of data consists of conversation extracts recorded using an Electronically Activated Recorder (EAR) (Mehl, Pennebaker, Crow, Dabbs, & Price, 2001), collected
by Mehl et al. (2006). To preserve the participants’ privacy, only random snippets of conversation were recorded. This corpus is much smaller than the essays corpus (96 participants
for a total of 97,468 words and 15,269 utterances). While the essays corpus consists only of
texts, the EAR corpus contains both sound extracts and transcripts. This corpus therefore
allows us to build models of personality recognition from speech. Only the participants’ utterances were transcribed (not those of their conversational partners), making it impossible
to reconstruct whole conversations. Nevertheless, the conversation extracts are less formal
than the essays, and personality may be best observed in the absence of behavioural constraints. Table 4 shows that while the essays corpus is much larger than the EAR corpus,
the amount of data per subject is comparable, i.e. 766 words per subject for the essays and
1,015 for the EAR corpus. Table 3 shows examples of conversations from the EAR corpus
for different personality traits.
For personality ratings, the EAR corpus contains both self-reports and ratings from 18
independent observers. Psychologists use self-reports to facilitate evaluating the personality of a large number of participants, and there are a large number of standard self-report
tests. Observers were asked to make their judgments by rating descriptions of the Big Five
Inventory (John & Srivastava, 1999) on a 7 point scale (from strongly disagree to strongly
464

Recognising Personality in Conversation and Text

Dataset
Source of language
Personality reports
Number of words
Subjects
Words per subject

Essays
Written
Self reports
1.9 million
2,479
766.4

EAR
Spoken
Self and observer
97,468
96
1,015.3

Table 4: Comparison of the essays and EAR corpora.
agree), without knowing the participants. Observers were divided into three groups, each
rating one third of the participants, after listening to each participant’s entire set of sound
files (130 files on average). The personality assessment was based on the audio recordings,
which contain more information than the transcripts (e.g., ambient sounds, including captured conversations). Mehl et al. (2006) report strong inter-observer reliabilities across all
Big Five dimensions (intraclass correlations based on one-way random effect models: mean
r = 0.84, p < .01). The observers’ ratings were averaged for each participant, to produce
the final scores used in our experiments.
Interestingly, the average correlations between frequency counts from psycholinguistic
word categories and the Big Five personality dimensions were considerably larger in the
EAR corpus than with the student essays studied by Pennebaker and King. Moreover,
the correlations reported by Mehl et al. seem to be higher for observer reports than for
self-reports. Based on this observation, we hypothesise that models of observed personality
will outperform models of self-assessed personality.
3.2 Features
The features used in the experiments are motivated by previous psychological findings
about correlations between measurable linguistic factors and personality traits. Features
are divided into subsets depending on their source and described in the subsections below.
The total feature set is summarised in Table 6. The experimental results given in Sections 4,
5, and 6 examine the effect of each feature subset on model accuracy.
3.2.1 Content and Syntax
We extracted a set of linguistic features from each essay and conversation transcript,
starting with frequency counts of 88 word categories from the Linguistic Inquiry and Word
Count (LIWC) utility (Pennebaker et al., 2001). These features include both syntactic (e.g.,
ratio of pronouns) and semantic information (e.g., positive emotion words), which were
validated by expert judges. Some LIWC features are illustrated in Table 5. Pennebaker
and King (1999) previously found significant correlations between these features and each
of the Big Five personality traits. Relevant word categories for extraversion include social
words, emotion words, first person pronouns, and present tense verbs. Mehl et al. (2006)
showed that LIWC features extracted from the EAR corpus were significantly correlated
with both self and observer reports of personality.
We also added 14 additional features from the MRC Psycholinguistic database (Coltheart, 1981), which contains statistics for over 150,000 words, such as estimates of the age
465

Mairesse, Walker, Mehl & Moore

Feature
Anger words
Metaphysical issues
Physical state/function
Inclusive words
Social processes
Family members
Past tense verbs
References to friends
Imagery of words
Syllables per word
Concreteness
Frequency of use

Type
LIWC
LIWC
LIWC
LIWC
LIWC
LIWC
LIWC
LIWC
MRC
MRC
MRC
MRC

Example
hate, kill, pissed
God, heaven, coffin
ache, breast, sleep
with, and, include
talk, us, friend
mom, brother, cousin
walked, were, had
pal, buddy, coworker
Low: future, peace - High: table, car
Low: a - High: uncompromisingly
Low: patience, candor - High: ship
Low: duly, nudity - High: he, the

Table 5: Examples of LIWC word categories and MRC psycholinguistic features (Pennebaker et al., 2001; Coltheart, 1981). MRC features associate each word to a
numerical value.

of acquisition, frequency of use, and familiarity. As introverts take longer to reflect on their
utterances, Heylighen and Dewaele (2002) suggest that their vocabulary is richer and more
precise, implying a lower frequency of use. The MRC feature set was previously used by
Gill and Oberlander (2002), who showed that extraversion is negatively correlated with
concreteness. Concreteness also indicates neuroticism, as well as the use of more frequent
words (Gill & Oberlander, 2003). Table 5 shows examples of MRC scales. Each MRC
feature is computed by averaging the feature value of all the words in the essay or conversational extract. Part-of-Speech tags are computed to identify the correct entry in the
database among a set of homonyms.
3.2.2 Utterance Type
Various facets of personality traits seem to depend on the level of initiative of the speaker
and the type of utterance used (e.g., assertiveness, argumentativeness, inquisitiveness, etc.).
For example, extraverts are more assertive in their emails (Gill & Oberlander, 2002), while
extravert second language learners were shown to produce more back-channel behaviour
(Vogel & Vogel, 1986). We therefore introduced features characterising the types of utterance produced. We automatically tagged each utterance of the EAR corpus with speech
act categories from Walker and Whittaker (1990), using heuristic rules based on each utterance’s parse tree:
• Command: utterance using the imperative form, a command verb (e.g., must and have to) or
a yes/no second person question with a modal auxiliary like can;
• Prompt: single word utterance used for back-channelling (e.g., Yeah, OK, Huh, etc.);
• Question: interrogative utterance which isn’t a command;
• Assertion: any other utterance.
466

Recognising Personality in Conversation and Text

LIWC FEATURES (Pennebaker et al., 2001):
· Standard counts:
- Word count (WC), words per sentence (WPS), type/token ratio (Unique), words captured (Dic), words
longer than 6 letters (Sixltr), negations (Negate), assents (Assent), articles (Article),
prepositions (Preps), numbers (Number)
- Pronouns (Pronoun): 1st person singular (I), 1st person plural (We), total 1st person (Self), total
2nd person (You), total 3rd person (Other)
· Psychological processes:
- Affective or emotional processes (Affect): positive emotions (Posemo), positive feelings (Posfeel), optimism
and energy (Optim), negative emotions (Negemo), anxiety or fear (Anx), anger (Anger),
sadness (Sad)
- Cognitive Processes (Cogmech): causation (Cause), insight (Insight), discrepancy (Discrep), inhibition
(Inhib), tentative (Tentat), certainty (Certain)
- Sensory and perceptual processes (Senses): seeing (See), hearing (Hear), feeling (Feel)
- Social processes (Social): communication (Comm), other references to people (Othref), friends (Friends),
family (Family), humans (Humans)
· Relativity:
- Time (Time), past tense verb (Past), present tense verb (Present), future tense verb (Future)
- Space (Space): up (Up), down (Down), inclusive (Incl), exclusive (Excl)
- Motion (Motion)
· Personal concerns:
- Occupation (Occup): school (School), work and job (Job), achievement (Achieve)
- Leisure activity (Leisure): home (Home), sports (Sports), television and movies (TV), music (Music)
- Money and financial issues (Money)
- Metaphysical issues (Metaph): religion (Relig), death (Death), physical states and functions (Physcal),
body states and symptoms (Body), sexuality (Sexual), eating and drinking (Eating), sleeping
(Sleep), Grooming (Groom)
· Other dimensions:
- Punctuation (Allpct): period (Period), comma (Comma), colon (Colon), semi-colon (Semic), question
(Qmark), exclamation (Exclam), dash (Dash), quote (Quote), apostrophe (Apostro), parenthesis
(Parenth), other (Otherp)
- Swear words (Swear), nonfluencies (Nonfl), fillers (Fillers)
MRC FEATURES (Coltheart, 1981):
Number of letters (Nlet), phonemes (Nphon), syllables (Nsyl), Kucera-Francis written frequency (K-Ffreq), Kucera-Francis number of categories (K-F-ncats), Kucera-Francis number of samples (K-F-nsamp),
Thorndike-Lorge written frequency (T-L-freq), Brown verbal frequency (Brown-freq), familiarity rating
(Fam), concreteness rating (Conc), imageability rating (Imag), meaningfulness Colorado Norms (Meanc),
meaningfulness Paivio Norms (Meanp), age of acquisition (AOA)
UTTERANCE TYPE FEATURES:
Ratio of commands (Command), prompts or back-channels (Prompt), questions (Question), assertions (Assertion)
PROSODIC FEATURES:
Average, minimum, maximum and standard deviation of the voice’s pitch in Hz (Pitch-mean, Pitch-min,
Pitch-max, Pitch-stddev) and intensity in dB (Int-mean, Int-min, Int-max, Int-stddev), voiced time (Voiced)
and speech rate (Word-per-sec)

Table 6: Description of all features, with feature labels in brackets.
We evaluated the automatic tagger by applying it to a set of 100 hand-labelled utterances
randomly selected in the EAR corpus. We obtain 88% of correct labels, which are mostly
assertions. Table 7 summarises the partition and the evaluation results for each speech act
type. For each speech act, the corresponding feature value is the ratio of the number of
occurrences of that speech act to the total number of utterances in each text.
467

Mairesse, Walker, Mehl & Moore

Label
Assertion
Command
Prompt
Question
All

Fraction
73.0%
4.3%
7.0%
15.7%
100%

Labelling accuracy
0.95
0.50
0.57
1.00
0.88

Table 7: Partition of the speech acts automatically extracted from the EAR corpus, and
classification accuracies on a sample of 100 hand-labelled utterances.

3.2.3 Prosody
Personality was also shown to influence speech production. Extraversion is associated with
more variation of the fundamental frequency (Scherer, 1979), with a higher voice quality
and intensity (Mallory & Miller, 1958), and with fewer and shorter silent pauses (Siegman
& Pope, 1965). Smith et al. (1975) showed that speech rate is positively correlated with
perceived competence (conscientiousness). Interestingly, the same authors found that speech
rate has an inverted-U relationship with benevolence (agreeableness), suggesting a need for
non-linear models. See Section 3.4.
We added prosodic features based on the audio data of the EAR conversation extracts.
As the EAR recorded the participants at anytime of the day, it was necessary to automatically remove any non-voiced signal. We used Praat (Boersma, 2001) to compute features
characterising the voice’s pitch and intensity (mean, extremas and standard deviation), and
we added an estimate of the speech rate by dividing the number of words by the voiced time.
As an important aspect of this work is that all features are extracted without any manual
annotation beyond transcription, we didn’t filter out utterances from other speakers that
may have been captured by the EAR even though it utilised a microphone pointing towards
the participant’s head. Although advances in speaker recognition techniques might improve
the accuracy of prosodic features, we make the assumption that the noise introduced by the
surrounding speakers has little effect on our prosodic features, and that it therefore does not
affect the performance of the statistical models. This assumption still remains to be tested,
as the personality similarity-attraction effect (Byrne & Nelson, 1965) might influence the
personality distribution of a participant’s conversational partners.
We included all the features mentioned in this section (117) in the models based on
the EAR corpus. Models computed using the essays corpus contain only LIWC and MRC
features (102), as speech acts are only meaningful in dialogues.
3.3 Correlational Analysis
In order to assess what individual features are important for modelling personality regardless
of the model used, we report previous correlational studies for the LIWC features on the
same data as well as analyses of the new MRC, utterance type and prosodic features. The
LIWC features were already analysed by Mehl et al. (2006) for the EAR dataset, and by
468

Recognising Personality in Conversation and Text

Pennebaker and King (1999) for the essays.1 Tables 8 to 11 show the features correlating
significantly with personality ratings (p < .05, correlations above .05 only), combining
together results from previous studies and new findings that provide insight into the features
likely to influence the personality recognition models in Sections 4.3, 5.3 and 6.3.
The correlation magnitudes in Tables 8 and 9 between LIWC and MRC features and
the essays data set show that although extraversion is very well perceived in conversations,
it isn’t strongly reflected through written language, as the correlation magnitudes for the
essays dataset are noticeably low. Table 10 shows that word count (WC) is a very important feature for modelling extraversion in conversation, both for observer reports and
self-reports. Interestingly, this marker doesn’t hold for written language (see Table 9).
Other markers common to observed and self-reported extraversion include the variation
of intensity (Int-stddev), the mean intensity (Int-mean), word repetitions (Unique), words
with a high concreteness (Conc) and imageability (Imag). See Table 11. On the other
hand, words related to anger, affect, swearing, and positive and negative emotions (Posemo
and Negemo) are perceived as extravert, but they don’t mark self-assessed extraversion in
conversations.
Tables 10 and 11 show that for emotional stability, only a few markers hold for both
self-reports and observer reports: a high word count and a low mean pitch (Pitch-mean).
Surprisingly, observed emotional stability is associated with swearing and anger words, but
not the self-assessed ratings. As reported by Mehl et al. (2006), neurotics are expected to
produce more self-references (Self and I). Pennebaker and King (1999) show that neurotics’
use of self-references is also observed in the essays, as well as the use of words related to
negative emotions and anxiety. Table 11 shows that in conversations, self-assessed neurotics
tend to have a low and constant voice intensity (Int-mean and Int-stddev), while these
markers aren’t used by observers at all.
While emotional stability is expressed differently in various datasets, some markers of
agreeableness are consistent: words related to swearing (Swear) and anger (Anger) indicate both self-assessed and observed disagreeableness, regardless of the source of language.
See Tables 8, 9 and 10. Interestingly, Table 11 shows that agreeable people do more
back-channelling (Prompt), suggesting that they tend to listen more to their conversational
partners. While observers don’t seem to take prosody into account for evaluating agreeableness, Table 11 shows that prosodic cues such as the pitch variation (Pitch-stddev) and
the maximum voice intensity (Max-int) indicate self-assessed disagreeableness.
As far as markers of conscientiousness are concerned, Tables 8 to 10 show that they
are similar to those of agreeableness, as unconscientious participants also use words related
to swearing (Swear), anger (Anger) and negative emotions (Negemo), regardless of the
dataset and assessment method. On the other hand, observed conscientiousness is associated
with words expressing insight, back-channels (Prompt), longer words (Nphon, Nlet, Nsyl
and Sixltr) as well as words that are acquired late by children (AOA), while self-assessed
conscientiousness is mostly expressed through positive feelings (Posfeel) in conversations.
The avoidance of negative language seems to be the main marker of conscientiousness in
essays, as all other features in Table 8 correlate only weakly with the self-reports.
1. Our correlations differ from Pennebaker and King’s study because we use additional student essays
collected during the following years.

469

Mairesse, Walker, Mehl & Moore

Trait
LIWC
Achieve
Affect
AllPct
Anger
Anx
Apostro
Article
Assent
Body
Cause
Certain
Cogmech
Comm
Comma
Death
Dic
Excl
Exclam
Family
Feel
Fillers
Friends
Future
Groom
Hear
Home
Humans
I
Incl
Inhib
Insight
Job
Leisure
Metaph
Motion
Music
Negate
Negemo
Nonfl
Number
Occup
Optim
Other
Othref
Parenth
Period
Physcal
Posemo
Posfeel
Preps
Present
Pronoun
Qmark

Extraversion

.03
.03
-.08**
-.03
-.01
-.08**
-.08**
.01
-.05**
.01
.05*
-.03
-.02
-.02
-.02
.05*
-.01
.00
.05*
-.01
-.04*
.06**
-.02
-.02
-.03
-.01
.04
.05*
.04*
-.03
-.01
.02
-.03
-.01
.03
-.04*
-.08**
-.03
-.03
-.03
.03
.03
.06**
.07**
-.06**
-.05*
-.02
.07**
.07**
.00
.00
.07**
-.06**

Emotional
stability
.01
-.07**
-.04
-.08**
-.14**
-.04
.11**
.02
-.04
-.03
-.01
-.02
.00
.01
-.04
-.09**
.02
-.05*
-.05*
-.09**
.01
-.04*
.01
-.02
.00
-.02
-.02
-.15**
-.01
.02
-.01
.01
.07**
.01
-.01
.06**
-.12**
-.18**
.01
.05*
.05*
.04
-.01
.02
.03
-.03
-.05*
.07**
-.01
.06**
-.12**
-.12**
-.05*

Agreeableness

-.01
-.04
-.01
-.16**
.03
-.02
-.03
.00
-.04*
.00
.03
-.02
-.01
-.02
-.02
.06**
-.02
.06**
.09**
.04
-.01
.02
.02
.01
-.01
.04*
-.03
.05*
.03
-.02
.00
.01
.03
-.01
.05*
-.01
-.11**
-.11**
.01
-.03
.04
.01
.03
.01
-.04*
-.01
-.03
.05*
.03
.04
-.01
.04*
-.04

Conscientiousness

.02
-.06**
-.04
-.14**
.05*
-.06**
.02
-.04
-.04*
-.04
.04*
-.06**
-.05**
-.01
-.06**
.06**
-.01
.00
.04*
.02
-.03
.01
.07**
.01
-.04*
.06**
-.08**
.04
.04*
-.02
-.03
.05**
-.01
-.08**
.03
-.07**
-.07**
-.11**
-.05*
-.02
.09**
.08**
.01
.01
-.01
-.01
-.03
.02
-.02
.08**
-.03
.02
-.06**

Openness
to experience
-.07**
.04*
.10**
.06**
-.04
.05**
.11**
.04*
.02
-.05*
.04
.02
.03
.10**
.05*
-.20**
.07**
-.03
-.07**
-.04*
-.01
-.12**
-.04
-.05**
.04*
-.15**
.04
-.14**
-.03
.04*
.05*
-.05**
-.05**
.08**
-.13**
.10**
.01
.04
.02
-.06**
-.18**
-.07**
.01
.06**
.10**
.04
.01
.02
.08**
-.04
-.09**
-.06**
.08**

Table 8: Pearson’s correlation coefficients between LIWC features and personality ratings
for the essays dataset, based on the analysis from Pennebaker and King (1999)
(* = significant at the p < .05 level, ** = p < .01). Only features that correlate
significantly with at least one trait are shown.

470

Recognising Personality in Conversation and Text

Trait
LIWC (2)
Quote
Relig
Sad
School
See
Self
Semic
Sexual
Sixltr
Sleep
Social
Space
Sports
Swear
Tentat
Time
TV
Unique
Up
WC
We
WPS
You
MRC
AOA
Brown-freq
Conc
Fam
Imag
K-F-freq
K-F-ncats
K-F-nsamp
Meanc
Meanp
Nlet
Nphon
Nsyl
T-L-freq

Extraversion

Emotional
stability

Agreeableness

Conscientiousness

Openness
to experience

-.05*
.00
.00
.03
.00
.07**
-.03
.07**
-.06**
-.01
.08**
-.02
.01
-.01
-.06**
-.02
-.04
-.05**
.03
.03
.06**
-.01
-.01

-.02
.03
-.12**
.05**
.09**
-.14**
.02
-.02
.06**
-.03
.00
.05*
.09**
.00
-.01
.02
.04*
.10**
.06**
-.06**
.07**
.02
.03

-.01
.00
.00
.06**
.00
.06**
.02
.00
-.05*
-.02
.02
.03
.02
-.14**
-.03
.07**
-.02
-.04*
.02
.01
.04*
.02
-.06**

-.03
-.06**
.01
.10**
-.03
.04*
.00
-.04
.02
.03
-.02
.01
.00
-.11**
-.06**
.09**
-.04*
-.05*
-.01
.02
.01
-.02
-.04*

.09**
.07**
-.01
-.20**
.05**
-.14**
.05**
.09**
.10**
-.08**
.02
-.04
-.05**
.08**
.05*
-.15**
.04
.09**
-.06**
.05*
.04
.06**
.11**

-.01
.05*
.02
.08**
.05*
-.01
.06**
.06**
.06**
.02
-.09**
-.08**
-.07**
.01

.05*
-.06**
-.06**
-.05*
-.04*
.10**
-.04*
-.01
-.10**
-.02
.09**
.08**
.07**
.10**

-.04*
.03
.03
.08**
.05*
.00
.08**
.03
.05**
.05*
-.03
-.03
-.02
.01

.06**
.06**
-.01
.05**
.00
.05*
.07**
.05**
-.01
.00
.00
.01
.04
.06**

.11**
-.07**
-.10**
-.17**
-.08**
.07**
-.12**
-.07**
-.11**
-.04*
.15**
.14**
.13**
.05**

Table 9: Continuation of Table 8, i.e. Pearson’s correlation coefficients between LIWC and
MRC features and personality ratings for the essays dataset (* = significant at
the p < .05 level, ** = p < .01). Only features that correlate significantly with at
least one trait are shown.
Tables 8 and 9 show that openness to experience is the trait yielding the highest correlations in the essays corpus: articles, second person pronouns (You) and long words (Sixltr)
indicate openness, while non-open participants tend to talk about their occupations (Occup,
Home and School) and themselves (Self). As far as conversations are concerned, observers
use similar cues for openness as with conscientiousness, such as insight words, longer words,
back-channels and a high age of acquisition (AOA).
This section shows that features are likely to vary depending on the source of language
and the method of assessment of personality. While such analyses can help evaluate the
usefulness of individual features, the question of how such features should be combined to
predict personality accurately is addressed by the statistical models.
471

Mairesse, Walker, Mehl & Moore

Dataset
Trait
LIWC
Affect
Anger
Articles
Assent
Cause
Cogmech
Comm
Dic
Discrep
Eating
Family
Feel
Female
Filler
Friend
Hear
Home
Humans
I
Inhib
Insight
Metaph
Money
Negemo
Nonfl
Other
Othref
Past
Physcal
Posfeel
Pronoun
Relig
Self
Senses
Sexual
Sixltr
Social
Space
Sports
Swear
Tentat
Unique
Up
WC

Extra

Observer reports
Emot
Agree Consc

Open

Extra

Emot

.40**
.37**
.21*
-.29**
-.13
.04
-.18
-.07
.08
.25*
.26*
.21*
.29**
-.01
.14
-.20
-.02
-.01
.03
.19
.04
.30**
-.02
.36**
-.01
.09
.00
-.19
.30**
.28**
-.02
.30**
.09
-.04
.24*
-.04
-.04
.03
.10
.30**
-.04
-.6**
.06
.63**

.13
.30**
.32**
-.02
-.23*
-.01
-.27**
-.16
-.03
.15
-.23*
.06
-.03
-.19
-.01
-.23*
-.19
.21*
-.41**
.01
-.02
.07
.24*
.18
.05
.02
.05
-.07
.24*
.04
-.30**
.06
-.42**
-.12
.21*
-.04
-.06
.18
.28**
.27**
.15
-.18
.04
.28**

.00
-.14
.14
.03
.00
.23*
-.26*
-.08
.23*
-.11
-.04
.05
-.17
.01
-.14
-.29**
.06
-.12
-.17
.00
.32**
-.02
.01
-.11
.06
-.17
-.22*
-.31**
-.17
.05
-.28**
-.07
-.15
-.26*
-.22*
.24*
-.31**
-.07
-.11
-.17
.30**
-.12
-.05
.20

.05
-.02
.03
-.11
.00
.11
-.01
.02
.10
-.03
.14
.08
.24*
-.05
.20*
-.04
.04
.07
.21*
.02
-.06
.20
-.08
.03
-.02
.02
.02
-.10
-.07
.06
.12
.26*
.25*
.03
-.05
-.20
.06
-.10
.03
-.08
-.14
-.32**
.06
.29**

-.13
.07
.00
-.05
-.09
.01
-.13
-.15
-.01
-.02
-.02
.05
.07
-.13
.01
-.08
-.12
-.03
-.16
.02
-.10
.10
.01
-.05
.17
.04
.13
-.18
-.06
-.14
-.07
.15
-.17
-.10
.04
-.15
.04
.09
.21*
.06
.04
-.22*
.07
.22*

-.20
-.49**
.03
.30**
.03
.24*
-.14
-.17
.13
-.31**
-.12
.03
.04
.04
-.08
-.19
.03
-.01
-.21*
-.22*
.34**
-.10
-.13
-.44**
.09
-.07
-.13
-.25*
-.39**
.05
-.23*
-.09
-.25*
-.18
-.49**
.25*
-.17
-.21*
-.15
-.51**
.26*
-.03
-.08
.10

-.24*
-.56**
-.15
.24*
.15
.20*
.00
-.05
.10
-.43**
-.03
-.03
.03
.20*
-.13
-.07
.04
-.23*
-.08
-.14
.29**
-.26*
-.24*
-.49**
.24*
-.09
-.14
-.18
-.47**
.14
-.17
-.27**
-.13
-.15
-.48**
.30**
-.15
-.24*
-.19
-.61**
.15
-.03
-.11
.07

Self-reports
Agree Consc
-.17
-.30**
.04
.19
.07
.08
.20*
.16
.15
-.10
.26**
-.08
.29**
.20
.05
.13
.29**
-.20
.23*
-.18
.03
-.10
-.22*
-.16
-.03
.05
.07
-.05
-.16
-.07
.19
-.06
.18
.12
-.19
-.01
.12
-.18
-.15
-.28**
.05
-.18
-.05
.18

-.19
-.30**
-.09
-.03
-.02
.00
.12
-.01
.09
-.19
.04
.02
.12
.18
.16
.07
-.03
-.06
.01
-.11
.01
-.09
-.06
-.25*
-.02
.05
.01
.05
-.27**
.23*
.05
-.09
.02
.03
-.23*
.19
.06
.01
-.05
-.29**
.14
-.05
.03
.03

Open
.13
.10
-.04
.08
-.23*
-.06
-.17
-.20*
-.09
-.05
-.14
.02
-.22*
-.08
-.11
-.19
-.07
.01
-.08
-.12
.05
.03
-.15
.10
.17
-.28**
-.19
-.26**
.05
.11
-.21*
.04
-.08
-.14
.04
.03
-.21*
.23*
-.03
.06
.05
-.03
.31**
.06

Table 10: Pearson’s correlation coefficients between LIWC features and personality ratings
for the EAR dataset, based on the analysis from Mehl et al. (2006) (* = significant
at the p < .05 level, ** = p < .01). Only features that correlate significantly with
at least one trait are shown.
3.4 Statistical Models
Various systems require different levels of granularity for modelling personality: it might
be more important to cluster users into large groups as correctly as possible, or the system
might need to discriminate between individual users. Depending on the application and the
adaptation capabilities of the target system, it is possible to use different types of personality
models, depending on whether personality modelling is treated as a classification problem, as
472

Recognising Personality in Conversation and Text

Dataset
Trait
Prosody
Int-max
Int-mean
Int-stddev
Pitch-max
Pitch-mean
Pitch-min
Pitch-stddev
Voiced
Word-per-sec
MRC
AOA
Brown-freq
Conc
Fam
Imag
K-F-freq
K-F-ncats
K-F-nsamp
Meanc
Nlet
Nphon
Nsyl
T-L-freq
Utterance
type
Assertion
Command
Prompt
Question

Extra

Observer reports
Emot
Agree Consc

Open

Extra

Emot

Self-reports
Agree Consc

.42**
.32**
.40**
.28**
.17
-.17
-.13
.23*
.07

.12
.20
.03
.10
-.45**
-.23*
.13
.27**
-.14

.07
-.02
-.08
.13
.06
-.02
.07
.06
-.12

-.13
-.06
-.12
.05
.04
.08
.03
.03
-.04

.05
.04
-.08
.23*
-.18
-.04
.11
.21*
-.17

.19
.21*
.36**
-.03
.12
.09
-.28**
-.02
.20*

.10
.22*
.28**
-.11
-.25*
-.08
.01
.07
.07

-.25*
-.05
.00
-.10
.07
.21*
-.34**
-.04
.09

-.01
-.16
-.06
-.03
.03
.04
.03
-.03
.02

.14
.03
.10
.01
-.04
.08
-.03
.03
.04

-.23*
-.26*
.24*
-.17
.33**
-.27**
-.24*
-.24*
.29**
-.14
-.12
-.16
-.24*

.01
-.41**
-.05
-.28**
.00
-.04
-.24*
-.20*
-.10
.17
.09
-.04
-.06

.26**
-.08
-.20*
-.24*
-.23*
.07
-.03
-.03
-.18
.25*
.25*
.23*
.06

.26**
.07
-.33**
-.07
-.33**
.17
.08
.16
-.25*
.31**
.36**
.34**
.16

.21*
-.16
-.32**
-.18
-.35**
.16
.00
.20
-.34**
.25*
.28**
.19
.13

-.12
-.04
.23*
-.03
.25*
-.22*
-.01
-.15
.23*
-.23*
-.16
-.13
-.19

.04
-.15
-.10
-.21*
-.09
-.06
-.06
-.04
-.12
.03
.02
-.02
-.07

.05
.14
.01
.17
.01
-.24*
.17
.03
.08
-.18
-.20
-.06
-.18

-.05
.07
-.12
.01
-.06
.05
.05
.08
-.06
.13
.15
.12
.06

.08
-.12
-.02
-.13
-.03
-.01
-.23*
-.17
-.07
.12
.13
.10
-.08

-.05
.00
-.10
.13

-.21*
.01
.07
.22*

-.03
-.08
.36**
-.16

.01
-.20*
.27**
-.11

-.09
.00
.25*
-.04

-.02
.13
-.05
.01

-.06
.21*
.01
-.01

-.09
-.01
.22*
-.02

.21*
.00
-.05
-.24*

-.14
.16
.02
.10

Open

Table 11: Continuation of Table 10, i.e. Pearson’s correlation coefficients between features
and personality ratings for the EAR dataset (* = significant at the p < .05 level,
** = p < .01). Only features that correlate significantly with at least one trait
are shown.
in previous work by Argamon et al. (2005) and Oberlander and Nowson (2006), or whether
we model personality traits via the scalar values actually generated by the self-reports and
observer methods used in the corpus collection described in Section 3.1.
To support applications in dialogue system adaptation, where the output generation is
limited to a few points at extremes of a personality scale, such as introvert vs. extravert
language or neurotic vs. emotionally stable, we develop classification models by splitting
our subjects into two equal size groups.
However, if we model personality traits as scalar values, we have two choices. We
can treat personality modelling as a regression problem or as a ranking problem. While
regression models can replicate the actual scalar values seen in the personality ratings
data, there is also a good argument for treating personality as a ranking problem because
by definition, personality evaluation assesses relative differences between individuals, e.g.
one person is described as an extravert because the average population is not. Moreover,
Freund, Iyer, Schapire, and Singer (1998) argue that ranking models are a better fit to
learning problems in which scales have arbitrary values (rather than reflecting real world
measures).
473

Mairesse, Walker, Mehl & Moore

For classification and regression models, we use the Weka toolbox (Witten & Frank,
2005) for training and evaluation. In order to evaluate models of personality classification,
we compare six different learning algorithms against a baseline returning the majority class.
The classification algorithms analysed here are C4.5 decision tree learning (J48), Nearest
neighbour (k = 1), Naive Bayes (NB), Ripper (JRip), Adaboost (10 rounds of boosting)
and Support vector machines with linear kernels (SMO).
For regression, we compare five algorithms with a baseline model returning the mean
personality score. We focus on a linear regression model, an M5’ regression tree, an M5’
model tree returning a linear model, a REPTree decision tree, and a model based on Support
vector machines with linear kernels (SMOreg). Parameters of the algorithms are set to
Weka’s default values.
Concerning the ranking problem, we train personality models for each Big Five trait
using RankBoost, a boosting algorithm for ranking (Freund et al., 1998; Schapire, 1999).
Given a personality trait to model, the linguistic features and personality scores are converted into a training set T of ordered pairs of examples x, y:
T = {(x, y)| x, y are language samples from two individuals,
x has a higher score than y for that personality trait}
Each example x is represented by a set of m indicator functions hs (x) for 1 ≤ s ≤ m.
The indicator functions are calculated by thresholding the feature values (counts) described
in Section 3.2. For example, one indicator function is:
(

h100 (x) =

1 if Word-per-sec(x) ≥ 0.73
0 otherwise

So h100 (x) = 1 if x’s average speech rate is above 0.73 words per second. A single parameter αs is associated with each indicator function, and the ranking score for an example
x is calculated as
X
F (x) =
αs hs (x)
s

This score is used to rank various language samples (written text or conversation extracts),
with the goal of duplicating the ranking found in the training data, and the training examples are used to set the parameter values αs . Training is the process of setting the
parameters αs to minimise the following loss function:
Loss =

1 X
eval(F (x) ≤ F (y))
|T | (x,y)∈T

The eval function returns 1 if the ranking scores of the (x, y) pair are misordered, and 0
otherwise. In other words, the ranking loss is the percentage of misordered pairs, for which
the order of the predicted scores doesn’t match the order dictated by the personality scores
from the questionnaire.
Most of the techniques used in this work express the learned models as rules or decision
trees, which support the analysis of differences in the personality models (see Sections 4.3,
5.3 and 6.3).
474

Recognising Personality in Conversation and Text

4. Classification Results
We evaluate binary classification models based on the essays corpus with self-reports of
personality, as well as models based on the EAR corpus with both self and observer reports.
All results are averaged over a 10-fold cross-validation, and all significance tests were done
using a two-tailed paired t-test at the p < .05 level.
4.1 Essays Corpus
Classification results for the essays corpus with self-reports are in Table 12. Interestingly,
openness to experience is the easiest trait to model as five classifiers out of six significantly
outperform the baseline and four of them produce their best performance for that trait,
with accuracies up to 62.1% using support vector machines (SMO). Emotional stability
produces the second best performance for four classifiers out of six, with 57.4% accuracy
for the SMO model. Conscientiousness is the hardest trait to model as only two classifiers
significantly outperform the baseline, however the SMO model performs as well as the best
model for extraversion and agreeableness, with around 55% correct classifications.
We find that support vector machines generally perform the best, with Naive Bayes and
AdaboostM1 in second position. SMO significantly outperforms the majority class baseline
for each trait. A J48 decision tree for recognising extraversion is shown in Figure 1, and the
rule-based JRip model classifying openness to experience with 58.8% accuracy is illustrated
in Table 16.

Trait
Base J48
NN
NB
JRIP
Extraversion
50.04 54.44• 53.27• 53.35• 52.70
Emotional stability
50.08 51.09
51.62
56.42• 55.90 •
Agreeableness
50.36 53.51• 50.16
53.88• 52.63
Conscientiousness
50.57 51.37
52.10
53.80
52.71
Openness to experience 50.32 54.24• 53.07
59.57• 58.85 •
• statistically significant improvement over the majority
baseline (two-tailed paired t-test, p < .05)

ADA
55.00 •
55.98 •
52.71
54.45 •
59.09 •
class

SMO
54.93 •
57.35 •
55.78 •
55.29 •
62.11 •

Table 12: Classification accuracy with two equal size bins on the essays corpus, using selfreports. Models are the majority class baseline (Base); J48 decision tree (J48);
Nearest neighbour (NN); Naive Bayes (NB); JRip rule set (JRIP); AdaboostM1
(ADA); Support vector machines (SMO).

Feature set comparison: In order to evaluate how each feature set contributes to the
final result, we trained binary classifiers using the algorithms producing the best overall
results with each feature set. We only analyse LIWC and MRC features for the essays
corpus, as utterance type and prosodic features don’t apply to written texts. We use the
Naive Bayes, AdaboostM1 and SMO classifiers as they give the best performances with the
full feature set. Results are shown in Table 13.
475

Mairesse, Walker, Mehl & Moore

Articles
≤ 7.23

> 7.23

Sexuality

Introvert

≤ 0.12

> 0.12
Parentheses

Apostrophes
≤ 2.57

> 2.57

≤ 17.91

> 17.91

Up
≤ 0.64

Sadness

Achievement

Words per sentence

Extravert

≤ 1.52
Introvert

> 0.64

≤ 0.64

> 1.52
Extravert

≤ 1.44
Extravert

Introvert
> 1.44
Introvert

> 0.64
Familiarity

Introvert

> 599.7

≤ 599.7

Positive emotions

Introvert

≤ 1.66

> 1.66
Grooming

Introvert
≤ 0.11

> 0.11

Extravert

Introvert

Figure 1: J48 decision tree for binary classification of extraversion, based on the essays
corpus and self-reports.

Remarkably, we can see that the LIWC features outperform the MRC features for every
trait, and the LIWC features on their own always perform slightly better than the full
feature set. This clearly suggests that MRC features aren’t as helpful as the LIWC features
for classifying personality from written text, however Table 13 shows that they can still
outperform the baseline for four traits out of five.
Concerning the algorithms, we find that AdaboostM1 performs the best for extraversion
(56.3% correct classifications), while SMO produces the best models for all other traits. It
suggests that support vector machines are promising for modelling personality in general.
The easiest trait to model is still openness to experience, with 62.5% accuracy using LIWC
features only.
4.2 EAR Corpus
Classification accuracies for the EAR corpus are in Table 14. We find that extraversion is
the easiest trait to model using observer reports, with both Naive Bayes and AdaboostM1
476

Recognising Personality in Conversation and Text

Feature set
None
LIWC features
MRC features
Classifier
Base
NB
ADA
SMO
NB
ADA
SMO
Set size
0
88
88
88
14
14
14
Extraversion
50.04 52.71
56.34• 52.75
52.87• 51.45
53.88
Emotional stability
50.08 56.02• 55.33• 58.20• 52.39
52.06
53.52•
Agreeableness
50.36 54.12• 52.71
56.39• 53.03• 52.06
53.31•
Conscientiousness
50.57 53.92• 54.48• 55.62• 53.03
52.95
53.84
Openness to experience 50.32 58.92• 58.64• 62.52• 55.41• 56.70• 57.47•
• statistically significant improvement over the majority class
baseline (two-tailed paired t-test, p < .05)
Table 13: Classification accuracies with two equal size bins on the essays corpus using the
majority class baseline (Base), Naive Bayes (NB), AdaboostM1 (ADA) and Support Vector Machine (SMO) classifiers, for different feature sets. Best model for
each trait are in bold.

outperforming the baseline with an accuracy of 73.0%. The J48 decision tree for extraversion with a 66.8% accuracy is shown in Figure 2. Emotional stability is modelled with
comparable success using a Naive Bayes classifier, however the improvement over the baseline is lower than with extraversion (22.8% vs. 25.2%) and other classifiers don’t perform as
well. Models of observed conscientiousness also outperform the baseline, with 67.7% accuracy using a Naive Bayes classifier, while the best model for agreeableness produces 61.3%
correct classifications. None of the models for openness to experience significantly outperform the baseline, which suggests that openness to experience is expressed more clearly in
stream of consciousness essays and self-reports than in the EAR dataset. Support vector
machines don’t perform as well as with the essays corpus, probably because of the sparseness of the dataset. Self-reports are much harder to model than observer reports given the
same dataset size, as none of the self-report classifiers significantly outperform the majority
class baseline.
Feature set comparison: For the EAR corpus we investigated the importance of all 4
feature sets: utterance type, LIWC, MRC, and prosodic features. We use the Naive Bayes
models with the observer ratings as they perform the best with all features. Interestingly,
Table 15 shows that the good classification accuracies for extraversion come from a combination of LIWC, MRC and prosodic features, as they all outperform the baseline on their own,
but don’t do as well as the 73.0% accuracy with the full feature set. Moreover, extraversion
is the only trait for which prosody seems to make a difference. LIWC features are the main
indicators of emotional stability, although the model with all features still performs better. MRC features are the most important for classifying conscientiousness (66.8%), while
prosodic features produce the best model of openness to experience with 64.6% accuracy,
improving on the model with all features. Although utterance type features never outperform the baseline on their own, the lack of significance could be the result of the small
1. Although equal size bins were used, the baseline accuracies differ from 50% because of the random
sampling of the cross-validation.

477

Mairesse, Walker, Mehl & Moore

Data
Obs
Obs
Obs
Obs
Obs
Self
Self
Self
Self
Self

Trait Base J48
NN
NB
JRIP ADA
SMO
Extra 47.78 66.78 59.33 73.00• 60.44
73.00 • 65.78
Emot 51.11 62.56 58.22 73.89• 56.22
48.78
60.33
Agree 47.78 48.78 51.89 61.33• 51.89
52.89
56.33
Consc 47.78 57.67 61.56 67.67• 61.56
60.22 • 57.11
Open 47.78 52.22 46.78 57.00
49.67
50.56
55.89
Extra 47.78 48.78 49.67 57.33
50.56
54.44
49.89
Emot 51.11 45.56 46.78 50.44
46.78
41.89
44.33
Agree 52.22 47.89 50.89 58.33
56.89
55.22
52.33
Consc 51.11 33.44 45.56 39.33
43.11
46.11
53.22
Open 51.11 52.00 42.22 61.44
45.00
56.00
47.78
• statistically significant improvement over the majority class
baseline (two-tailed paired t-test, p < .05)

Table 14: Classification accuracy with two equal size bins on the EAR corpus, for observer
ratings (Obs) and self-reports (Self). Models are majority class baseline (Base)1 ;
J48 decision tree (J48); Nearest neighbour (NN); Naive Bayes (NB); JRip rules
set (JRIP); AdaboostM1 (ADA); Support vector machines (SMO).

Feature set
None Type LIWC
MRC
Prosody
Set size
0
4
88
14
11
Extraversion
47.78
45.67
68.89•
68.78•
67.56•
Emotional stability
51.11
60.22
69.89•
60.78
61.78
Agreeableness
47.78
57.56
54.00
58.67
50.44
Conscientiousness
47.78
59.67
60.22
66.78•
52.11
Openness to experience 47.78
53.11
61.11
54.00
64.56•
• statistically significant improvement over the majority class
baseline (two-tailed paired t-test, p < .05)
Table 15: Classification accuracies for the EAR corpus with observer reports using the Naive
Bayes classifier, for different feature sets (None=baseline, Type=utterance type).
Models performing better than with the full feature set are in bold.

dataset size, since Section 3.3 showed that some utterance type features strongly correlate
with several personality traits.
4.3 Qualitative Analysis
Decision trees and rule-based models can be easily understood, and can therefore help to
uncover new linguistic markers of personality. Our models replicate previous findings, such
as the link between verbosity and extraversion (c.f. Word count node of Figure 2), but they
also provide many new markers.
478

Recognising Personality in Conversation and Text

Word count
≤ 1284

> 1284
Extravert

Metaphysical issues
≤ 0.25

> 0.25

Commas
≤ 8.72

Articles
> 8.72

Eating

Extravert

≤ 3.51

> 3.51

Extravert

Space

≤ 0.51

> 0.51

≤ 3.22

Introvert

Sad

Extravert

> 3.22
Frequency of use

≤ 0.15

> 0.15

≤ 6072

Introvert

Extravert

Extravert

> 6072
Introvert

Figure 2: J48 decision tree for binary classification of extraversion, based on the EAR corpus
and observer reports.

#
1
2
3
4
5
6

Ordered rules
(School ≥ 1.47) and (Motion ≥ 1.71) ⇒ Not open
(Occup ≥ 2.49) and (Sixltr ≤ 13.11) and (School ≥ 1.9) and (I ≥ 10.5) ⇒ Not open
(Fam ≥ 600.335106) and (Friends ≥ 0.67) ⇒ Not open
(Nlet ≤ 3.502543) and (Number ≥ 1.13) ⇒ Not open
(School ≥ 0.98) and (You ≤ 0) and (AllPct ≤ 13.4) ⇒ Not open
Any other feature values ⇒ Open

Table 16: JRip rule set for binary classification of openness to experience, based on the
essays corpus.

The model of self-assessed openness to experience detailed in Table 16 shows that students referring a lot to school work tend to have low scores for that trait (Rules 1, 2
and 5). As expected, the avoidance of longer words is also indicative of a lack of cre479

Mairesse, Walker, Mehl & Moore

ativity/conventionality (Rules 4 and 5), as well as the use of high-familiarity words and
references to friends (Rule 3).
The model of observed extraversion in Figure 2 shows that word count is the most important feature for classifying that trait as an observer. The model also suggests that given
low verbosity, extraversion can still manifest itself through the use of words related to metaphysical issues together with few articles, as well as through the use of many commas. The
association between extraversion and the avoidance of articles probably reflects the use of
more pronouns over common nouns and confirms previous findings associating extraversion
with implicit language (Heylighen & Dewaele, 2002).
Interestingly, the decision tree trained on the essays corpus in Figure 1 for self-reported
extraversion differs a lot from the observer model in Figure 2. While word count is the most
important feature for observers, it doesn’t seem to be a marker of self-assessed extraversion
(see Section 3.3), although the number of words per sentence is used to discriminate on a
subset of the data. On the other hand, the self-report model associates introversion with
the use of articles, which was also the case in the observer model. While sexual content
doesn’t affect the observer model, it is the second most important feature for modelling selfreported extraversion. For example, participants using many sex-related words are modelled
as introvert, unless they avoid parentheses and words related to sadness.

5. Regression Results
We also trained regression models using the same corpora. The baseline is a model returning
the mean of all personality scores in the training set. We use the relative absolute error for
evaluation, which is the ratio between the model’s prediction error and the error produced
by the baseline. A low relative error therefore indicates that the model performs better than
the constant mean baseline, while a 100% relative error implies a performance equivalent
to that baseline. All results are averaged over a 10-fold cross-validation, and all significance
tests were done using a two-tailed paired t-test at the p < .05 level.
5.1 Essays Corpus
Regression results with the essays corpus and self-reports are in Table 17. Paired t-tests
show that emotional stability and openness to experience produce models that significantly
improve over the baseline. As with the classification task, openness to experience is the
easiest trait to model using essays: four regression models out of five outperform the baseline. The M5’ model tree produces the best result with a 93.3% relative error for openness
to experience (6.7% error decrease), and a 96.4% relative error for emotional stability.
In terms of correlation between the model predictions and the actual ratings, the model
for emotional stability and openness to experience produce Pearson’s correlation coefficients
of 0.24 and 0.33, respectively. Although the magnitude of the improvement seems relatively
small, one needs to keep in mind the difficulty of the regression task over the binary classification task: it is the most fine-grained personality recognition problem, requiring the
association of an exact scalar value with each individual.
Feature set comparison: Table 18 provides results for a comparison of LIWC with the
MRC feature sets using the linear regression model, the M5’ model tree and the support
480

Recognising Personality in Conversation and Text

Trait
Base
LR
M5R
M5
REP
Extraversion
100.00 99.17
99.31
99.22
99.98
Emotional stability
100.00 96.87•
99.75
96.43•
99.35
Agreeableness
100.00 98.92
99.86
99.22
99.78
Conscientiousness
100.00 98.68
100.62
98.56
100.47
Openness to experience 100.00 93.58•
97.68• 93.27•
99.82
• statistically significant improvement over the mean value
baseline (two-tailed paired t-test, p < .05)

SMO
100.65
98.35
100.28
99.30
94.19•

Table 17: Relative error for regression models trained on the essays corpus with all features.
Models are the mean value baseline (Base), linear regression (LR); M5’ regression tree (M5R), M5’ model tree with linear models (M5), REPTree (REP) and
Support vector machines for regression (SMO).

vector machine algorithm for regression (SMOreg). Overall, LIWC features perform better
than MRC features except for extraversion, for which the linear regression model with
MRC features produces better results than with the full feature set. For all other traits,
LIWC features on their own perform better than the full feature set, and almost always
significantly outperform the baseline. The model for openness to experience produces the
lowest relative error, with 6.50% improvement over the baseline.
Feature set
None
LIWC features
MRC features
Regression model
Base
LR
M5
SMO
LR
M5
SMO
Extraversion
100.00 99.39
99.25• 100.8
98.79• 98.79• 99.13•
Emotional stability
100.00 96.71• 96.42• 98.03
99.49
99.54
99.89
Agreeableness
100.00 98.50• 98.52• 99.52
99.75
99.81
99.31•
Conscientiousness
100.00 98.23• 98.14• 99.46
99.23
99.23
99.16•
Openness to experience 100.00 93.50• 93.70• 94.14• 97.44• 97.44• 97.26•
• statistically significant improvement over the mean value
baseline (two-tailed paired t-test, p < .05)
Table 18: Relative error for regression models trained on the essays corpus with the MRC
and LIWC feature sets only. Models are linear regression (LR); M5’ model tree
(M5); Support vector machines for regression (SMO). Best models are in bold.

5.2 EAR Corpus
Regression results for the EAR corpus are in Table 19. A paired t-test (two-tailed, p < .05)
over the cross-validation folds shows that the error reduction is significant for observed
extraversion (79.9% relative error, i.e. 20.1% error decrease), conscientiousness (14.3% improvement) and emotional stability (13.3% improvement). While extraversion is the easiest
trait to model from observer ratings, models of agreeableness and openness to experience
don’t outperform the baseline.
481

Mairesse, Walker, Mehl & Moore

In terms of correlation between the model predictions and the actual ratings, the models
for extraversion, emotional stability and conscientiousness respectively produce Pearson’s
correlation coefficients of 0.54, 0.47 and 0.44, significantly outperforming the baseline. Such
correlations are relatively high, given that the average correlations between the ratings of
each pair of observers is 0.54 for extraversion, 0.29 for emotional stability and 0.51 for
conscientiousness (18 observers, between 31 and 33 data points for each pair).
Linear regression and support vector machines perform poorly, suggesting that they
require a bigger dataset as in the essays corpus. As in the classification task, self-reports
of the EAR corpus are clearly difficult to model: none of the models show significant
improvement over the baseline.

Data
Obs
Obs
Obs
Obs
Obs
Self
Self
Self
Self
Self

Trait
Base
LR M5R
M5
REP
Extraversion
100.00 179.16
82.16•
80.15
79.94•
Emotional stability
100.00 302.74
92.03•
86.75• 100.51
Agreeableness
100.00 242.68
96.73
111.16
99.37
Conscientiousness
100.00 188.18
82.68•
90.85
98.08
Openness to experience 100.00 333.65 101.64
119.53
102.76
Extraversion
100.00 204.96 104.50
118.44
99.94
Emotional stability
100.00 321.97 104.10
108.39
99.91
Agreeableness
100.00 349.87 106.90
110.84
101.64
Conscientiousness
100.00 177.12 103.39
120.29
107.33
Openness to experience 100.00 413.70 107.12
122.68
126.31
• statistically significant improvement over the mean value
baseline (two-tailed paired t-test, p < .05)

SMO
140.05
162.05
173.97
131.75
213.20
176.51
233.19
201.80
124.91
233.01

Table 19: Relative error for regression models, with observer ratings (Obs) and self-reports
(Self) of the EAR corpus. Models are the mean value baseline (Base); linear
regression (LR); M5’ regression tree (M5R); M5’ model tree with linear models
(M5); REPTree decision tree (REPT); Support vector machines for regression
(SMO). The relative error of the baseline model is 100%.

Feature set comparison: We trained regression models with each individual feature set
using only observer reports, since self-reports didn’t produce any significant result using all
features. We only focus on the three regression tree algorithms as they perform the best
with all features. Table 20 shows that LIWC are good predictors of observed extraversion,
as the REPTree outperforms the same model with all features with a 76.4% relative error
(23.6% improvement over the baseline). LIWC features also produce the best regression
model for conscientiousness (82.1% relative error, 17.9% improvement). Surprisingly, the
best model of emotional stability contains only prosodic features, with a 85.3% relative
error (14.7% improvement). This finding suggests that speech cues are crucial for the
perception of neuroticism, which could explain why Gill and Oberlander (2003) reported a
low correlation between self-assessed and observed emotional stability using text only. As
482

Recognising Personality in Conversation and Text

in the classification task, utterance type features don’t show any significant improvement
on their own.
Set
Model
Extra
Emot
Agree
Consc
Open

Utterance type
LIWC features
MRC features
M5R
M5
REP
M5R
M5
REP
M5R
M5
REP
100.0
103.7
101.8
81.61
77.84•
76.38•
99.23
102.2
99.69
102.5
103.0
102.6
90.79•
109.6
109.6
93.13•
96.08
104.4
102.4
102.7
111.1
98.49
111.7
102.5
104.1
112.5
102.2
100.0
95.04
104.1
82.13•
96.62
93.50
97.00
102.0
91.24•
101.1
99.03
109.9
105.1
129.5
103.7
106.2
111.6
105.5
• statistically significant improvement over the mean value
baseline (two-tailed paired t-test, p < .05)

Prosodic features
M5R
M5
REP
94.07
90.91
88.31•
92.24•
85.32•
97.95
100.0
108.4
108.9
100.0
104.7
101.7
100.1
113.5
99.93

Table 20: Relative error for regression models trained on the EAR corpus with individual
feature sets. Models are M5’ regression tree (M5R); M5’ model tree with linear
models (M5); REPTree regression tree (REP). Best models are in bold.

5.3 Qualitative Analysis
Regression trees for extraversion and conscientiousness are in Figures 3 and 4. As suggested
by the correlations in Section 3.3, the model in Figure 3 shows that the voice’s pitch and
variation of intensity play an important role when modelling extraversion. A high verbal
output is perceived as a sign of extraversion (see Word Count nodes), confirming previous
findings (Scherer, 1979). On the other hand, a low mean pitch combined with a constant
voice intensity characterises high introverts.
Figure 4 suggests that conscientious people use fewer swear words and content related
to sexuality, while preferring longer words. The same figure also shows that conscientious
people use fewer pronouns, i.e. a more explicit style, as well as more words related to
communication (e.g., talk and share).

6. Ranking Results
Results with both corpora and different feature sets are in Tables 21 and 22. The models
are trained over 100 rounds of boosting. The baseline model ranks extracts randomly,
producing a ranking loss of 0.5 on average (lower is better). Results are averaged over a
10-fold cross-validation, and all significance tests were done using a two-tailed paired t-test
at the p < .05 level.
6.1 Essays Corpus
Table 21 shows that openness to experience produces the best ranking model with the
essays corpus, producing a ranking loss of 0.39 (lower is better). Remarkably, this trait
was the easiest to model for all three recognition tasks with that corpus. As it is not the
case with conversational data, it seems that streams of consciousness, or more generally
personal writings, are likely to exhibit cues relative to the author’s openness to experience.
Emotional stability produces the second best model with a ranking loss of 0.42, followed by
conscientiousness and extraversion, while the model for agreeableness produces the highest
483

Mairesse, Walker, Mehl & Moore

Word count
≤ 675

> 675
Word count

Mean pitch
≤ 231

> 231

Intensity variation
≤ 6.39
2.86

3.23

≤ 1299

> 1299

3.83

4.24

> 6.39
3.02

Figure 3: M5’ regression tree for observed extraversion, computed using the EAR corpus.
The target output ranges from 1 to 5.5, where 5.5 means strongly extravert (the
highest value in the means of the observer ratings). The mean pitch value is
expressed in Hertz, and the intensity variation (standard deviation) in decibels.

ranking loss. All models significantly outperform the random ranking baseline, but the
actual improvement is still relatively small.
Feature set
Base All
LIWC MRC
Extraversion
0.50 0.44• 0.44•
0.46•
Emotional stability
0.50 0.42• 0.42•
0.47•
Agreeableness
0.50 0.46• 0.46•
0.48•
Conscientiousness
0.50 0.44• 0.44•
0.47•
Openness to experience 0.50 0.39• 0.39•
0.44•
• statistically significant improvement over
the random ordering baseline
(two-tailed paired t-test, p < .05)
Table 21: Ranking loss for the essays corpus over a 10-fold cross-validation for different
feature sets and the random ordering baseline (Base). Best models are in bold
(lower is better).

Feature set comparison: To evaluate which features contribute to ranking accuracy,
we trained a ranking model with each feature set. Table 21 clearly shows that the LIWC
features are the only contributors to model accuracy, as the inclusion of MRC features
doesn’t reduce the ranking loss for any trait.
484

Recognising Personality in Conversation and Text

Swear words
≤ 0.93

> 0.93
Sexuality words

Pronouns
≤ 16.7
4.01

> 16.7
3.63

≤ 0.62
Comm. words

≤ 1.46
3.15

> 1.46
3.26

> 0.62
Syllables per word
> 1.14

≤ 1.14
2.90

Body states words

≤ 0.59
2.96

> 0.59
2.98

Figure 4: M5’ regression tree for observed conscientiousness, computed using the EAR corpus. The target output ranges from 1 to 7, where 7 means strongly conscientious
(Comm. words is the ratio of words related to communication).

6.2 EAR Corpus
Concerning the EAR corpus, Table 22 reporting experiments using all the features, shows
that models of extraversion, agreeableness, conscientiousness, and openness to experience
are better than the random ranking baseline. Emotional stability is the most difficult trait
to model, while agreeableness and conscientiousness produce the best results, with ranking
losses of 0.31 and 0.33 respectively.
Feature set
None
All
LIWC MRC Type
Extraversion
0.50
0.35• 0.36•
0.45
0.55
Emotional stability
0.50
0.41
0.41
0.39•
0.43
Agreeableness
0.50 0.31• 0.32•
0.44
0.45
Conscientiousness
0.50 0.33• 0.36•
0.41•
0.44
Openness to experience
0.50
0.38• 0.37•
0.41
0.49
• statistically significant improvement over the random
ordering baseline (two-tailed paired t-test, p < .05)

Prosody
0.26•
0.45
0.54
0.55
0.44

Table 22: Ranking loss for the EAR corpus and observer reports1 over a 10-fold crossvalidation for different feature sets (None=baseline, Type=utterance type). Best
models are in bold (lower is better).

485

Mairesse, Walker, Mehl & Moore

Feature set comparison: When looking at individual feature sets, Table 22 shows that
LIWC features perform significantly better than the baseline for all dimensions but emotional stability, while emotional stability is best predicted by MRC features only (0.39
ranking loss). Interestingly, prosodic features are very good predictors of extraversion, with
a lower ranking error than the full feature set (0.26). This model produces the best overall
result, with a 74% chance that the model will detect the most extravert among any two
unseen conversation extracts. As in the previous recognition tasks, utterance type features
on their own never significantly outperform the baseline.
6.3 Qualitative Analysis
The RankBoost rules indicate the impact of each feature on the recognition of a personality
trait by the magnitude of the parameter α associated with that feature. Tables 23 to
25 show the rules with the most impact on each of the best models, with the associated α
values. The feature labels are in Table 6. For example, the model of extraversion in Table 23
confirms previous findings by associating this trait with longer conversations (Rule 5), a
high speech rate (Rules 1 and 4) and a high pitch (Rules 2, 6 and 7) (Nass & Lee, 2001).
But new markers emerge, such as a high pitch variation for introverts (Rules 15, 18 and
20), contradicting previous findings reported by Scherer (1979).
Extraversion model with prosodic features
# Positive rules
α
# Negative rules
1 Word-per-sec ≥ 0.73 1.43 11 Pitch-max ≥ 636.35
2 Pitch-mean ≥ 194.61 0.41 12 Pitch-slope ≥ 312.67
3 Voiced ≥ 647.35
0.41 13 Int-min ≥ 54.30
4 Word-per-sec ≥ 2.22 0.36 14 Word-per-sec ≥ 1.69
5 Voiced ≥ 442.95
0.31 15 Pitch-stddev ≥ 115.49
6 Pitch-max ≥ 599.88
0.30 16 Pitch-max ≥ 637.27
7 Pitch-mean ≥ 238.99 0.26 17 Pitch-slope ≥ 260.51
8 Int-stddev ≥ 6.96
0.24 18 Pitch-stddev ≥ 118.10
9 Int-max ≥ 85.87
0.24 19 Int-stddev ≥ 6.30
10 Voiced ≥ 132.35
0.23 20 Pitch-stddev ≥ 119.73

α
-0.05
-0.06
-0.06
-0.06
-0.06
-0.06
-0.12
-0.15
-0.18
-0.47

Table 23: Subset of the RankBoost model for extraversion with prosodic features only,
based on EAR conversations and observer reports. Rows 1-10 represent the rules
producing the highest score increase, while rows 11-20 indicate evidence for the
other end of the scale, i.e. introversion.

Concerning agreeableness, Rules 1 and 20 in Table 24 suggest that agreeable people
use longer words but shorter sentences, and Rules 2 and 4 show that they express more
tentativity (with words like maybe or perhaps) and positive emotions (e.g., happy and good).
Anger and swear words greatly reduce the agreeableness score (Rules 12, 13, 18 and 19), as
well as the use of negations (Rule 15).
1. We also built models of self-reports of personality based on the EAR corpus, but none of them significantly
outperforms the baseline.

486

Recognising Personality in Conversation and Text

Agreeableness model with all features
# Positive rules
α
# Negative rules
1 Nphon ≥ 2.66
0.56 11 Fam ≥ 601.61
2 Tentat ≥ 2.83
0.50 12 Swear ≥ 0.41
3 Colon ≥ 0.03
0.41 13 Anger ≥ 0.92
4 Posemo ≥ 2.67 0.32 14 Time ≥ 3.71
5 Voiced ≥ 584
0.32 15 Negate ≥ 3.52
6 Relig ≥ 0.43
0.27 16 Fillers ≥ 0.54
7 Insight ≥ 2.09
0.25 17 Time ≥ 3.69
8 Prompt ≥ 0.06 0.25 18 Swear ≥ 0.61
9 Comma ≥ 4.60 0.23 19 Swear ≥ 0.45
10 Money ≥ 0.38
0.20 20 WPS ≥ 6.13

α
-0.16
-0.18
-0.19
-0.20
-0.20
-0.22
-0.23
-0.27
-0.27
-0.45

Table 24: Best RankBoost model based on EAR conversations for agreeableness. Rows
1-10 represent the rules producing the highest score increase, while rows 11-20
indicate evidence for the other end of the scale, i.e. disagreeableness.

Table 25 shows that conscientious people talk a lot about their work (Rule 1), while
unconscientious people swear a lot (Rules 19). Insight words (e.g., think and know) are
also good indicator of conscientiousness, as well as words expressing positive feelings like
happy and love (Rule 2 and 3). Interestingly, conscientious people are modelled as having
a high variation of their voice intensity (Rule 4). On the other hand, Rule 20 shows that
speaking very loud produces the opposite effect, as well as having a high pitch (Rule 13).
Long utterances are also indicative of a low conscientiousness (Rule 12).
The rule sets presented here contain only the most extreme rules of our ranking models,
which contain many additional personality cues that aren’t identified through a typical
correlational analysis. For example, a high speech rate and a high mean pitch tend to
contribute to a high extraversion ranking in Table 23’s model, but they don’t correlate
significantly with observer ratings, as detailed in Table 11. Similarly, positive emotion
words (Posemo) and the avoidance of long utterances (WPS) indicate agreeableness in the
model in Table 24, while these features don’t correlate significantly with agreeableness
ratings.

7. Related Work
To our knowledge, there are only two other studies on the automatic recognition of personality. Both of these studies have focused on the classification of written texts based on
self-reports, rather than using continuous modelling techniques as we do here.
Argamon et al. (2005) use the essays corpus of Pennebaker and King (1999), so their
results are directly comparable to ours. As in our work, they use a top-down approach to
feature definition: their feature set consists of relative frequencies of 675 function words and
word categories based on networks of the theory of Systemic Functional Grammar. However,
they simplify the task by removing the middle third of the dataset, thereby potentially
increasing precision at the cost of reducing recall to a maximum of 67%. They train SMO
models on the top third and lower third of the essays corpus for the two personality traits
487

Mairesse, Walker, Mehl & Moore

Conscientiousness model with
# Positive rules
α
#
1 Occup ≥ 1.21
0.37 11
2 Insight ≥ 2.15
0.36 12
3 Posfeel ≥ 0.30
0.30 13
4 Int-stddev ≥ 7.83 0.29 14
5 Nlet ≥ 3.29
0.27 15
6 Comm ≥ 1.20
0.26 16
7 Nphon ≥ 2.66
0.25 17
8 Nphon ≥ 2.67
0.22 18
9 Nphon ≥ 2.76
0.20 19
10 K-F-nsamp ≥ 329 0.19 20

all features
Negative rules
Swear ≥ 0.20
WPS ≥ 6.25
Pitch-mean ≥ 229
Othref ≥ 7.64
Humans ≥ 0.83
Swear ≥ 0.93
Swear ≥ 0.17
Relig ≥ 0.32
Swear ≥ 0.65
Int-max ≥ 86.84

α
-0.18
-0.19
-0.20
-0.20
-0.21
-0.21
-0.24
-0.27
-0.31
-0.50

Table 25: Best RankBoost model based on EAR conversations for conscientiousness. Rows
1-10 represent the rules producing the highest score increase, while rows 11-20
indicate evidence for the other end of the scale, i.e. unconscientiousness.

of extraversion and emotional stability, achieving accuracies on this subset of the data of
58% for both traits.
We believe it is likely that personality recognition models need to be based on the full
range of values to be useful in any practical application. Nevertheless, in order to do a
direct comparison, we also removed the middle third of the essays dataset and trained an
SMO classifier with the LIWC features. We obtain 57% classification accuracy for extraversion and 60% for emotional stability, whereas when the same algorithm is applied to the
whole corpus, we obtain accuracies of 55% for extraversion and 57% for emotional stability,
significantly outperforming the baseline (see Table 12). Using the EAR conversational data
and observer reports, accuracies of our SMO models remain at 65% for extraversion but
increase to 63% for emotional stability (see Table 14).
These results suggest that our feature set in combination with that of Argamon et al.
could possibly improve performance, as both feature sets perform comparably. Using their
features, Argamon et al. identify that relative frequencies of a set of function words are the
best predictor for extraversion, suggesting that those that refer to norms and certainty are
the most salient. Concerning emotional stability, the feature set characterising appraisal
produces by far the best results. Appraisal features are relative frequencies of positive and
negative words as well as frequencies of each category in the Attitude network (e.g., affect,
appreciation, judgement, etc.). They find that neurotics tend to use more words related to
negative appraisal and affect, but fewer appreciation appraisal words, suggesting that they
focus more on their personal feelings.
Oberlander and Nowson (2006) follow a bottom-up feature discovery method by training Naive Bayes and SMO models for four of the Big Five traits on a corpus of personal
weblogs, using n-gram features extracted from the dataset. In order to be able to compare
with Argamon et al., they report experiments where they remove texts with non-extreme
personality scores from their corpus, but they also report experiments applying classification
algorithms to seven different ways of partitioning the whole corpus into classes, motivated
as approximating a continuous modelling approach. Although, their results aren’t directly
488

Recognising Personality in Conversation and Text

comparable to ours because they are based on different corpora, we report the results that
use all instances of the dataset, as we believe that discarding some of the test data increases
precision at the cost of making recall unacceptably low.
When building Naive Bayes models using the most frequent bi-grams and tri-grams
computed over the full corpus, Oberlander and Nowson (2006) find that the model of agreeableness is the only one outperforming the baseline (54% accuracy, no level of significance
mentioned). When keeping only n-grams that are distinctive of two extreme sets of a given
trait, accuracies range from 65% for extraversion to 72% for emotional stability. Finally,
when applying an automatic feature selection algorithm to the filtered set, accuracies increase to range from 83% for emotional stability to 93% for agreeableness. When testing
whether these models generalise to a different corpus of weblogs, Nowson and Oberlander
(2007) report binary classification accuracies ranging from 55% for extraversion to 65%
for conscientiousness. Interestingly, models trained on the most extreme instances of the
original corpus seem to outperform models trained on the full corpus, although no level of
significance is mentioned. These studies show that careful feature selection greatly improves
classification accuracy, and that n-grams can be appropriate to model self-reports of personality, although, as Oberlander and Nowson point out, such features are likely to overfit.
It would therefore be interesting to test in future work whether the feature sets used here
generalise to another dataset.
Oberlander and Nowson (2006) also report results for 3-way and 5-way classification, in
order to approximate the finer-grained continuous personality ratings used in psychology
(as we do with the scalar models we present here). They obtain a maximum of 44.7%
for extraversion with 5 bins, using raw n-grams (baseline is 33.8%). These results are not
directly comparable to ours because they are on a different corpus, with different feature
sets. Moreover, we have not provided results on such multiple classification experiments,
because such models cannot take into account the fact that the different classes are part
of a total ordering, and thus the resulting models are forced to ignore the importance of
features that correlate with that ordering across all classes. We believe that regression and
ranking models are more appropriate for finer-grained personality recognition (see Sections
5 and 6).
To evaluate this claim, we first mapped the output of the best classifier to a ranking and
compared it with the RankBoost models. We trained a Naive Bayes classifier on the EAR
corpus with observer reports and all features, using 5 equal size bins.2 For each test fold
of a 10-fold cross-validation, we computed the ranking loss produced by the classifier based
on the ordering of the five classes. Results in Table 26 show that RankBoost significantly
outperforms the classifier for four traits out of five (p < .05), with an improvement close to
significance for emotional stability (p = 0.12).
Because RankBoost’s goal is to minimise the ranking loss, this comparison is likely to
favour ranking models. Therefore, we also mapped the output of the RankBoost models
to 5 classification bins to see whether RankBoost could perform as well as a classifier for
the classification task. We divided the output ranking into 5 bins, each containing a 20%
slice of contiguously ranked instances. Results in Table 26 show that the Naive Bayes
classifier never outperforms RankBoost significantly, while the ranking model produces a
2. Oberlander and Nowson use unequal bins defined for each personality trait using standard deviation
from the mean, which may be an easier task than equal size bins.

489

Mairesse, Walker, Mehl & Moore

Task
Ranking
Classification
Model
Base NB Rank Base NB Rank
Extraversion
0.50 0.48 0.35•
20.0 32.3
32.1
Emotional stability
0.50 0.50 0.41
20.0 21.9
21.9
Agreeableness
0.50 0.50 0.31•
20.0 28.4
37.8
Conscientiousness
0.50 0.46 0.33•
20.0 34.7
30.3
Openness to experience 0.50 0.53 0.38•
20.0 19.8
26.8
• statistically significant improvement over the
other model (two-tailed t-test, p < .05)
Table 26: Comparison between ranking (Rank) and classification models (NB) for both personality ranking and classification tasks (5 bins). Evaluation metrics are ranking
loss (lower is better) and classification accuracy (higher is better), respectively.
Results are averaged over a 10-fold cross-validation.

better mean accuracy for agreeableness (38%) and openness to experience (27%), and the
same accuracy for emotional stability (22%). In sum, we find that ranking models perform
as well for classification and better for ranking compared with our best classifier, thus
modelling personality using continuous models is more accurate.

8. Discussion and Future Work
We show that personality can be recognised by computers through language cues.3 While
recent work in AI explores methods for the automatic detection of other types of pragmatic
variation in text and conversation, such as opinion, emotion, and deception, to date, we
know of only two studies besides our own on automatic recognition of user personality (Argamon et al., 2005; Mairesse & Walker, 2006a, 2006b; Oberlander & Nowson, 2006). To our
knowledge, the results presented here are the first to demonstrate statistically significant
results for texts and to recognise personality in conversation (Mairesse & Walker, 2006a,
2006b). We present the first results applying regression and ranking models in order to
model personality recognition using the continuous scales traditional in psychology. We
also systematically examine the use of different feature sets, suggested by previous psycholinguistic research. Although these features have been suggested by the psycholinguistic
literature, reported correlations with personality ratings are generally weak: it was not
obvious that they would improve accuracies of statistical models on unseen subjects.
Computational work on modelling personality has primarily focused on methods for
expressing personality in virtual agents and tutorial systems, and concepts related to personality such as politeness, emotion, or social intelligence (Walker, Cahn, & Whittaker,
1997; André, Klesen, Gebhard, Allen, & Rist, 1999; Lester, Towns, & FitzGerald, 1999;
Wang, Johnson, Mayer, Rizzo, Shaw, & Collins, 2005) inter alia. Studies have shown that
user evaluations of agent personality depend on the user’s own personality (Reeves & Nass,
1996; Cassell & Bickmore, 2003), suggesting that an ability to model the user’s personality
3. An online demo and a personality recognition tool based on the models presented in this paper can be
downloaded from www.dcs.shef.ac.uk/cogsys/recognition.html

490

Recognising Personality in Conversation and Text

is required. Models such as we present here for the automatic recognition of user personality
is one way to acquire such a user model (Chu-Carroll & Carberry, 1994; Thompson, Göker,
& Langley, 2004; Zukerman & Litman, 2001). We plan to test these models as user models
in the context of an adaptive dialogue system.
Table 27 summarises results for all the personality traits and recognition tasks we
analysed. What clearly emerges is that extraversion is the easiest trait to model from
spoken language, followed by emotional stability and conscientiousness. Concerning written language, models of openness to experience produce the best results for all recognition
tasks. We can also see that feature selection is very important, as some of the best models
only contain a small subset of the full feature set. Prosodic features are important for modelling observed extraversion, emotional stability and openness to experience. MRC features
are useful for models of emotional stability, while LIWC features are beneficial for all traits.
We also analysed qualitatively which features had the most influence in specific models, for
all recognition tasks, as well as reporting correlations between each feature and personality
traits in Section 3.3.
Although the parameters of the algorithms have not been optimised, the bottom of
Table 27 seems to indicate that simple models like Naive Bayes or regression trees tend
to outperform more complex ones (e.g., support vector machines), confirming results from
Oberlander and Nowson (2006). However, our experiments on the larger essays corpus
(more than 2,400 texts) show that support vector machines and boosting algorithms produce higher classification accuracies. It is therefore likely that those algorithms would also
perform better on spoken data if they were trained on a much larger corpus than the EAR
dataset, and if their parameters were optimised.
We hypothesised that models of observed personality will outperform models of selfassessed personality. Our results do suggest that observed personality may be easier to
model than self-reports, at least in conversational data. For the EAR corpus, we find many
good results with models of observed personality, while models of self-assessed personality
never outperform the baseline. This may be due to objective observers using similar cues as
our models, while self-reports of personality may be more influenced by factors such as the
desirability of the trait (Edwards, 1953). Hogan (1982) introduced the distinction between
the agent’s and the observer’s perspective in personality assessment. While the agent’s
perspective conceptually taps into a person’s identity (or ‘personality from the inside’), the
observer’s perspective in contrast taps into a person’s reputation (or ‘personality from the
outside’). Both facets of personality have important psychological implications. A person’s
identity shapes the way the person experiences the world. A person’s reputation, however,
is psychologically not less important: it determines whether people get hired or fired (e.g.,
reputation of honesty), get married or divorced, get adored or stigmatised. Because it is
harder to assess, this observer’s perspective has received comparatively little attention in
psychology. Given that in everyday life people act as observers of other people’s behaviours
most of the time, the external perspective naturally has both high theoretical importance
and social relevance (Hogan, 1982).
Recent research exploring this issue in psychology is based on the Brunswikian Lens
model (Brunswik, 1956), which has been used extensively in recent years to explain the
‘kernel of truth’ in the social perception of strangers. Use of the lens model in personality
research reflects the widely shared assumptions that the expression of personality is commu491

Mairesse, Walker, Mehl & Moore

Task
Baseline

Classification
n/a
none
50%

n/a

Regression
none
0%

n/a

Ranking
none

0.50

1%
4%
2%
2%
7%

Rank
Rank
Rank
Rank
Rank

LIWC
LIWC
LIWC
LIWC
LIWC

0.44
0.42
0.46
0.44
0.39

24%
15%
3%
18%
1%

Rank
Rank
Rank
Rank
Rank

prosody
MRC
all
all
LIWC

0.26
0.39
0.31
0.33
0.37

Self-report models trained on written data (essays):
Extraversion
Emotional stability
Agreeableness
Conscientiousness
Openness to experience

ADA
SMO
SMO
SMO
SMO

LIWC
LIWC
LIWC
LIWC
LIWC

56%
58%
56%
56%
63%

LR
M5
LR
M5
M5

MRC
LIWC
LIWC
LIWC
all

Observer report models trained on spoken data (EAR):
Extraversion
Emotional stability
Agreeableness
Conscientiousness
Openness to experience

NB
NB
NB
NB
NB

all
all
all
all
prosody

73%
74%
61%
68%
65%

REP
M5
M5R
M5R
M5

LIWC
prosody
all*
LIWC
type*

Table 27: Comparison of the best models for each trait, for all three recognition tasks. Each
table entry contains the algorithm, the feature set, and the model performance.
See Sections 3.2 and 3.4 for details. Depending on the task, the evaluation metric is either the (1) classification accuracy; (2) percentage of improvement over
the regression baseline; (3) ranking loss. Asterisks indicate results that aren’t
significant at the p < .05 level.

nicatively functional, i.e. that (a) latent attributes of persons are expressed via observable
cues; (b) observers rely on observable cues to infer the latent attributes of others; (c) observers use appropriate cues – that is, their implicit assumptions on the relations between
observable cues and latent attributes are to some extent accurate. The model has also
been useful in identifying observable cues that mediate convergences between judgments of
latent attributes and more direct measures of those attributes (Scherer, 2003; Heinrich &
Borkenau, 1998).
As there are discrepancies between markers of self-assessed and observed personality,
another issue is the identification of the most appropriate model given a specific application.
Such a gold standard can be approximated by either observer or self-reports, however it is
likely that for a specific trait one type of report will be closer to the true personality. A
hypothesis that remains to be tested is that traits with a high visibility (e.g., extraversion)
are more accurately assessed using observer reports, as they tend to yield a higher interjudge agreement (Funder, 1995), while low visibility traits (e.g., emotional stability) are
better assessed by oneself. A personality recogniser aiming to estimate the true personality
would therefore have to switch from observer models to self-report models, depending on
the trait under assessment.
Beyond practical applications of personality recognition models, this work is also an
attempt to explore different ways of looking at the relation between personality and language. We looked at various personality recognition tasks, and applied different learning
492

Recognising Personality in Conversation and Text

methods in Section 3.4. The tasks vary in complexity: a ranking model can be directly
derived from a regression model, while a classification model can be derived from either a
ranking or a regression model. Is any type of model closer to the actual relation between
language, and more generally behaviour, and personality? Does personality vary continuously, or are there clusters of people with similar trait combinations? If the relation is
continuous, classification algorithms will never be able to produce accurate models for more
than two classes, because they don’t take into account any ordering between the classes. As
ranking models outperform classifiers (see Section 7), and given the wide range of individual
differences reflected by the literature on the Big Five (Allport & Odbert, 1936; Norman,
1963; Goldberg, 1990), we believe that personality varies continuously among members of
the population, suggesting that regression or ranking models should be more accurate in
the long run. This hypothesis is supported by recent work in medical research showing that
antisocial personality disorder varies continuously (Marcus, Lilienfeld, Edens, & Poythress,
2006). Regression provides the most detailed model of the output variables, but depending on whether absolute differences between personality scores are meaningful, or if only
relative orderings between people matter, ranking may be more appropriate. Additional
models could also be tried on the ranking task, such as support vector algorithms for ordinal regression (Herbrich, Graepel, & Obermayer, 2000). Moreover, future work should
assess whether optimising the parameters of the learning algorithms improves performance.
In future work, we would like to improve these models and examine how well they
perform across dialogue domains. It is not clear whether the accuracies are high enough
to be useful. Applications involving speech recognition will introduce noise in all features
except for the prosodic features, probably reducing model accuracy, but since the EAR
corpus is relatively small, we expect that more training data would improve performance.
Additionally, we believe that the inclusion of gender as a feature would produce better
models, as the actual language correlates of perceived personality were shown to depend
on the gender of the speaker (Mehl et al., 2006). We also believe that future work should
investigate the combination of individual features in a trait-dependent way. Another issue is
the poor performance of the utterance type features—since there were significant correlation
results for these features in Section 3.3, it is unclear why these features are not useful in
the statistical models. This could possibly arise from the small size of the datasets, or
from the relatively low accuracy of our hand-crafted automatic tagger, compared to other
work using supervised learning methods (Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky,
Taylor, Martin, Ess-Dykema, & Meteer, 2000; Webb, Hepple, & Wilks, 2005).
We have begun to test these models on our spoken language generator (Mairesse &
Walker, 2007). In future work, we plan to compare the utility of models trained on out-ofdomain corpora, such as those here, with other methods for training such models, in terms
of their utility for the automatic adaptation of the output generation of dialogue systems.

Acknowledgments
We would like to thank James Pennebaker for giving us access to the essays data. This
work was partially funded by a Royal Society Wolfson Research Merit Award to Marilyn
Walker, and by a Vice Chancellor’s studentship to François Mairesse.
493

Mairesse, Walker, Mehl & Moore

References
Allport, G. W., & Odbert, H. S. (1936). Trait names: a psycho-lexical study. Psychological
Monographs, 47 (1, Whole No. 211), 171–220.
André, E., Klesen, M., Gebhard, P., Allen, S., & Rist, T. (1999). Integrating models of
personality and emotions into lifelike characters. In Proceedings of the Workshop on
Affect in Interactions - Towards a new Generation of Interfaces, pp. 136–149.
Argamon, S., Dhawle, S., Koppel, M., & Pennebaker, J. (2005). Lexical predictors of
personality type. In Proceedings of the Joint Annual Meeting of the Interface and the
Classification Society of North America.
Biber, D. (1988). Variation across Speech and Writing. Cambridge University Press.
Boersma, P. (2001). Praat, a system for doing phonetics by computer. Glot International,
5 (9/10), 341–345.
Bono, J. E., & Judge, T. A. (2004). Personality and transformational and transactional
leadership: a meta-analysis. Journal of Applied Psychology, 89 (5), 901–910.
Breck, E., Choi, Y., & Cardie, C. (2007). Identifying expressions of opinion in context. In
Twentieth International Joint Conference on Artificial Intelligence (IJCAI).
Brunswik, E. (1956). Perception and the Representative Design of Psychological Experiments. University of California Press, Berkeley, CA.
Byrne, D., & Nelson, D. (1965). Attraction as a linear function of proportion of positive
reinforcements. Journal of Personality and Social Psychology, 1, 659–663.
Cassell, J., & Bickmore, T. (2003). Negotiated collusion: Modeling social language and its
relationship effects in intelligent agents. User Modeling and User-Adapted Interaction,
13, 89–132.
Chu-Carroll, J., & Carberry, S. (1994). A plan-based model for response generation in
collaborative task-oriented dialogue. In Proceedings of the 12th National Conference
on Artificial Intelligence (AAAI), pp. 799–805.
Coltheart, M. (1981). The MRC psycholinguistic database. Quarterly Journal of Experimental Psychology, 33A, 497–505.
Costa, P. T., & McCrae, R. R. (1992). NEO PI-R Professional Manual. Psychological
Assessment Resources, Odessa, FL.
Dewaele, J.-M., & Furnham, A. (1999). Extraversion: the unloved variable in applied
linguistic research. Language Learning, 49 (3), 509–544.
Donnellan, M. B., Conger, R. D., & Bryant, C. M. (2004). The Big Five and enduring
marriages. Journal of Research in Personality, 38, 481–504.
494

Recognising Personality in Conversation and Text

Edwards, A. L. (1953). The relationship between the judged desirability of a trait and the
probability that it will be endorsed. Journal of Applied Psychology, 37, 90–93.
Enos, F., Benus, S., Cautin, R., Graciarena, M., Hirschberg, J., & Shriberg, E. (2006).
Personality factors in human deception detection: Comparing human to machine
performance. In Proceedings of ICSLP.
Eysenck, H. J. (1991). Dimensions of personality: 16, 5 or 3? criteria for a taxonomic
paradigm. Personality and Individual Differences, 12 (8), 773–790.
Fast, L. A., & Funder, D. C. (2007). Personality as manifest in word use: Correlations with
self-report, acquaintance-report, and behavior. Journal of Personality and Social
Psychology, in press.
Freund, Y., Iyer, R., Schapire, R. E., & Singer, Y. (1998). An efficient boosting algorithm
for combining preferences. In Proceedings of the 15th International Conference on
Machine Learning, pp. 170–178.
Funder, D. C. (1995). On the accuracy of personality judgment: A realistic approach.
Psychological Review, 102, 652–670.
Funder, D. C., & Sneed, C. D. (1993). Behavioral manifestations of personality: An ecological approach to judgmental accuracy. Journal of Personality and Social Psychology,
64 (3), 479–490.
Furnham, A. (1990). Language and personality. In Giles, H., & Robinson, W. (Eds.),
Handbook of Language and Social Psychology. Winley.
Furnham, A., Jackson, C. J., & Miller, T. (1999). Personality, learning style and work
performance. Personality and Individual Differences, 27, 1113–1122.
Furnham, A., & Mitchell, J. (1991). Personality, needs, social skills and academic achievement: A longitudinal study. Personality and Individual Differences, 12, 1067–1073.
Gill, A., & Oberlander, J. (2003). Perception of e-mail personality at zero-acquaintance:
Extraversion takes care of itself; neuroticism is a worry. In Proceedings of the 25th
Annual Conference of the Cognitive Science Society, pp. 456–461.
Gill, A. (2003). Personality and Language: The Projection and Perception of Personality
in Computer-Mediated Communication. Ph.D. thesis, University of Edinburgh.
Gill, A. J., & Oberlander, J. (2002). Taking care of the linguistic features of extraversion.
In Proceedings of the 24th Annual Conference of the Cognitive Science Society, pp.
363–368.
Goldberg, L. R. (1990). An alternative “description of personality”: The Big-Five factor
structure. Journal of Personality and Social Psychology, 59, 1216–1229.
Graciarena, M., Shriberg, E., Stolcke, A., Enos, F., Hirschberg, J., & Kajarekar, S. (2006).
Combining prosodic, lexical and cepstral systems for deceptive speech detection. In
Proceedings of IEEE ICASSP.
495

Mairesse, Walker, Mehl & Moore

Heinrich, C. U., & Borkenau, P. (1998). Deception and deception detection: The role of
cross-modal inconsistency. Journal of Personality, 66 (5), 687–712.
Herbrich, R., Graepel, T., & Obermayer, K. (2000). Large margin rank boundaries for
ordinal regression. In Smola, A. J., Bartlett, P., Schölkopf, B., & Schuurmans, D.
(Eds.), Advances in Large Margin Classifiers, pp. 115–132. MIT Press, Cambridge,
MA.
Heylighen, F., & Dewaele, J.-M. (2002). Variation in the contextuality of language: an
empirical measure. Context in Context, Special issue of Foundations of Science, 7 (3),
293–340.
Hirschberg, J., Benus, S., Brenier, J. M., Enos, F., Friedman, S., Gilman, S., Girand,
C., Graciarena, M., Kathol, A., Michaelis, L., Pellom, B., Shriberg, E., & Stolcke,
A. (2005). Distinguishing deceptive from non-deceptive speech. In Proceedings of
Interspeech’2005 - Eurospeech.
Hogan, R. (1982). A socioanalytic theory of personality. Nebraska Symposium of Motivation,
30, 55–89.
Hogan, R., Curphy, G. J., & Hogan, J. (1994). What we know about leadership: Effectiveness and personality. American Psychologist, 49 (6), 493–504.
John, O. P., Donahue, E. M., & Kentle, R. L. (1991). The “Big Five” Inventory: Versions
4a and 5b. Tech. rep., Berkeley: University of California, Institute of Personality and
Social Research.
John, O. P., & Srivastava, S. (1999). The Big Five trait taxonomy: History, measurement,
and theoretical perspectives. In Pervin, L. A., & John, O. P. (Eds.), Handbook of
personality theory and research. New York: Guilford Press.
Komarraju, M., & Karau, S. J. (2005). The relationship between the Big Five personality
traits and academic motivation. Personality and Individual Differences, 39, 557–567.
Lester, J. C., Towns, S. G., & FitzGerald, P. J. (1999). Achieving affective impact: Visual
emotive communication in lifelike pedagogical agents. The International Journal of
Artificial Intelligence in Education, 10 (3-4), 278–291.
Liscombe, J., Venditti, J., & Hirschberg, J. (2003). Classifying subject ratings of emotional
speech using acoustic features. In Proceedings of Interspeech’2003 - Eurospeech.
Mairesse, F., & Walker, M. (2006a). Automatic recognition of personality in conversation.
In Proceedings of HLT-NAACL.
Mairesse, F., & Walker, M. (2006b). Words mark the nerds: Computational models of personality recognition through language. In Proceedings of the 28th Annual Conference
of the Cognitive Science Society, pp. 543–548.
Mairesse, F., & Walker, M. (2007). PERSONAGE: Personality generation for dialogue. In
Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 496–503.
496

Recognising Personality in Conversation and Text

Mallory, P., & Miller, V. (1958). A possible basis for the association of voice characteristics
and personality traits. Speech Monograph, 25, 255–260.
Marcus, D. K., Lilienfeld, S. O., Edens, J. F., & Poythress, N. G. (2006). Is antisocial
personality disorder continuous or categorical? A taxometric analysis. Psychological
Medicine, 36 (11), 1571–1582.
McLarney-Vesotski, A. R., Bernieri, F., & Rempala, D. (2006). Personality perception: A
developmental study. Journal of Research in Personality, 40 (5), 652–674.
Mehl, M. R., Gosling, S. D., & Pennebaker, J. W. (2006). Personality in its natural habitat: Manifestations and implicit folk theories of personality in daily life. Journal of
Personality and Social Psychology, 90, 862–877.
Mehl, M., Pennebaker, J., Crow, M., Dabbs, J., & Price, J. (2001). The Electronically
Activated Recorder (EAR): A device for sampling naturalistic daily activities and
conversations. Behavior Research Methods, Instruments, and Computers, 33, 517–
523.
Mishne, G. (2005). Experiments with mood classification in blog posts. In Proceedings of
ACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access.
Nass, C., & Lee, K. (2001). Does computer-synthesized speech manifest personality? experimental tests of recognition, similarity-attraction, and consistency-attraction. Journal
of Experimental Psychology: Applied, 7 (3), 171–181.
Newman, M. L., Pennebaker, J. W., Berry, D. S., & Richards, J. M. (2003). Lying words:
Predicting deception from linguistic style. Personality and Social Psychology Bulletin,
29, 665–675.
Norman, W. T. (1963). Toward an adequate taxonomy of personality attributes: Replicated
factor structure in peer nomination personality rating. Journal of Abnormal and Social
Psychology, 66, 574–583.
Nowson, S., & Oberlander, J. (2007). Identifying more bloggers: Towards large scale personality classification of personal weblogs. In Proceedings of the International Conference
on Weblogs and Social Media.
Nunn, S. (2005). Preventing the next terrorist attack: The theory and practice of homeland security information systems. Journal of Homeland Security and Emergency
Management, 2 (3).
Oberlander, J., & Gill, A. J. (2006). Language with character: A stratified corpus comparison of individual differences in e-mail communication. Discourse Processes, 42,
239–270.
Oberlander, J., & Nowson, S. (2006). Whose thumb is it anyway? classifying author personality from weblog text. In Proceedings of the 44th Annual Meeting of the Association
for Computational Linguistics (ACL).
497

Mairesse, Walker, Mehl & Moore

Oudeyer, P.-Y. (2002). Novel useful features and algorithms for the recognition of emotions
in speech. In Proceedings of the 1st International Conference on Speech Prosody, pp.
547–550.
Pang, B., & Lee, L. (2005). Seeing stars: Exploiting class relationships for sentiment
categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics (ACL), pp. 115–124.
Paunonen, S. V., & Jackson, D. N. (2000). What is beyond the Big Five? plenty!. Journal
of Personality, 68 (5), 821–836.
Peabody, D., & Goldberg, L. R. (1989). Some determinants of factor structures from
personality-trait descriptor. Journal of Personality and Social Psychology, 57 (3),
552–567.
Pennebaker, J. W., Francis, M. E., & Booth, R. J. (2001). Inquiry and Word Count: LIWC
2001. Lawrence Erlbaum, Mahwah, NJ.
Pennebaker, J. W., & King, L. A. (1999). Linguistic styles: Language use as an individual
difference. Journal of Personality and Social Psychology, 77, 1296–1312.
Pennebaker, J. W., Mehl, M., & Niederhoffer, K. (2003). Psychological aspects of natural
language use: Our words, our selves. Annual Review of Psychology, 54, 547–577.
Popescu, A., & Etzioni, O. (2005). Extracting product features and opinions from reviews.
In Proceedings of HTL-EMNLP.
Reeves, B., & Nass, C. (1996). The Media Equation. University of Chicago Press.
Reiter, E., & Sripada, S. G. (2004). Contextual influences on near-synonym choice. In
Proceedings of the International Natural Language Generation Conference, pp. 161–
170.
Rienks, R., & Heylen, D. (2006). Dominance detection in meetings using easily obtainable
features. In Bourlard, H., & Renals, S. (Eds.), Revised Selected Papers of the 2nd Joint
Workshop on Multimodal Interaction and Related Machine Learning Algorithms, Vol.
3869 of Lecture Notes in Computer Science. Springer Verlag.
Riggio, R. E., Salinas, C., & Tucker, J. (1988). Personality and deception ability. Personality
and Individual Differences, 9 (1), 189–191.
Rosenberg, A., & Hirschberg, J. (2005). Acoustic/prosodic and lexical correlates of charismatic speech. In Proceedings of Interspeech’2005 - Eurospeech.
Rushton, J. P., Murray, H. G., & Erdle, S. (1987). Combining trait consistency and learning specificity approaches to personality, with illustrative data on faculty teaching
performance. Personality and Individual Differences, 8, 59–66.
Schapire, R. (1999). A brief introduction to boosting. Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, 2, 1401–1406.
498

Recognising Personality in Conversation and Text

Scherer, K. R. (1979). Personality markers in speech. In Scherer, K. R., & Giles, H. (Eds.),
Social markers in speech, pp. 147–209. Cambridge University Press.
Scherer, K. R. (2003). Vocal communication of emotion: A review of research paradigms.
Speech Communication, 40 (1-2), 227–256.
Siegman, A., & Pope, B. (1965). Personality variables associated with productivity and
verbal fluency in the initial interview. In Proceedings of the 73rd Annual Conference
of the American Psychological Association.
Sigurdsson, J. F. (1991). Computer experience, attitudes toward computers and personality
characteristics in psychology undergraduates. Personality and Individual Differences,
12 (6), 617–624.
Smith, B. L., Brown, B. L., Strong, W. J., & Rencher, A. C. (1975). Effects of speech rate
on personality perception. Language and Speech, 18, 145–152.
Somasundaran, S., Ruppenhofer, J., & Wiebe, J. (2007). Detecting arguing and sentiment
in meetings. In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue.
Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin,
R., Ess-Dykema, C. V., & Meteer, M. (2000). Dialogue act modeling for automatic
tagging and recognition of conversational speech. Computational Linguistics, 26 (3),
339–371.
Stoyanov, V., Cardie, C., & Wiebe, J. (2005). Multi-perspective question answering using
the OpQA corpus. In Proceedings of HLT-EMNLP.
Thompson, C. A., Göker, M. H., & Langley, P. (2004). A personalized system for conversational recommendations. Journal of Artificial Intelligence Research, 21, 393–428.
Tucker, S., & Whittaker, S. (2004). Accessing multimodal meeting data: Systems, problems and possibilities. Lecture Notes in Computer Science, Machine Learning for
Multimodal Interaction, 3361, 1–11.
Turney, P. D. (2002). Thumbs up or thumbs down? Semantic orientation applied to unspervised classification of reviews. In Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics (ACL), pp. 417–424.
Vogel, K., & Vogel, S. (1986). L’interlangue et la personalité de l’apprenant. International
Journal of Applied Linguistics, 24 (1), 48–68.
Walker, M., Cahn, J. E., & Whittaker, S. J. (1997). Improvising linguistic style: Social
and affective bases for agent personality. In Proceedings of the 1st Conference on
Autonomous Agents, pp. 96–105.
Walker, M., & Whittaker, S. (1990). Mixed initiative in dialogue: an investigation into
discourse segmentation. In Proceedings of the 28th Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 70–78.
499

Mairesse, Walker, Mehl & Moore

Wang, N., Johnson, W. L., Mayer, R. E., Rizzo, P., Shaw, E., & Collins, H. (2005). The
politeness effect: Pedagogical agents and learning gains. Frontiers in Artificial Intelligence and Applications, 125, 686–693.
Watson, D., & Clark, L. A. (1992). On traits and temperament: General and specific
factors of emotional experience and their relation to the five factor model. Journal of
Personality, 60 (2), 441–76.
Webb, N., Hepple, M., & Wilks, Y. (2005). Error analysis of dialogue act classification. In
Proceedings of the 8th International Conference on Text, Speech and Dialogue.
Wiebe, J., & Riloff, E. (2005). Creating subjective and objective sentence classifiers from
unannotated texts. In Proceedings of the 6th International Conference on Intelligent
Text Processing and Computational Linguistics.
Wiebe, J., Wilson, T., Bruce, R., Bell, M., & Martin, M. (2004). Learning subjective
language. Computational Linguistic, 30 (3), 277–308.
Wilson, T., Wiebe, J., & Hwa, R. (2004). Just how mad are you? finding strong and
weak opinion clauses. In Proceedings of the 19th National Conference on Artificial
Intelligence (AAAI), pp. 761–769.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical machine learning tools and
techniques. Morgan Kaufmann, San Francisco, CA.
Zukerman, I., & Litman, D. (2001). Natural language processing and user modeling: Synergies and limitations. User Modeling and User-Adapted Interaction, 11 (1-2), 129–158.

500

Journal of Artiﬁcial Intelligence Research 30 (2007) 273-320

Submitted 02/07; published 10/07

Reasoning with Very Expressive Fuzzy Description Logics
Giorgos Stoilos
Giorgos Stamou

gstoil@image.ece.ntua.gr
gstam@softlab.ece.ntua.gr

Department of Electrical and Computer Engineering,
National and Technical University of Athens,
Zographou 15780, Athens, GR

Jeﬀ Z. Pan

jpan@csd.abdn.ac.uk

Department of Computing Science,
The University of Aberdeen, UK

Vassilis Tzouvaras

tzouvaras@image.ece.ntua.gr

Department of Electrical and Computer Engineering,
National and Technical University of Athens,
Zographou 15780, Athens, GR

Ian Horrocks

horrocks@cs.man.ac.uk

School of Computer Science, The University of Manchester
Manchester, M13 9PL, UK

Abstract
It is widely recognized today that the management of imprecision and vagueness will
yield more intelligent and realistic knowledge-based applications. Description Logics (DLs)
are a family of knowledge representation languages that have gained considerable attention
the last decade, mainly due to their decidability and the existence of empirically high
performance of reasoning algorithms. In this paper, we extend the well known fuzzy ALC
DL to the fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms
(S), inverse roles (I), role hierarchies (H) and number restrictions (N ). We illustrate why
transitive role axioms are diﬃcult to handle in the presence of fuzzy interpretations and
how to handle them properly. Then we extend these results by adding role hierarchies and
ﬁnally number restrictions. The main contributions of the paper are the decidability proof
of the fuzzy DL languages fuzzy-SI and fuzzy-SHIN , as well as decision procedures for
the knowledge base satisﬁability problem of the fuzzy-SI and fuzzy-SHIN .

1. Introduction
Nowadays, many applications and domains use some form of knowledge representation language in order to improve their capabilities. Encoding human knowledge and providing
means to reason with it can beneﬁt applications a lot, by enabling them provide intelligent answers to complex user deﬁned tasks. Examples of modern applications that have
recently adopted knowledge representation languages are the World Wide Web (BernersLee, Hendler, & Lassila, 2001; Baader, Horrocks, & Sattler, 2002b), where knowledge is
used to improve the abilities of agents and the interoperability between disparate systems,
multimedia processing applications (Alejandro, Belle, & Smith, 2003; Benitez, Smith, &
Chang, 2000), which use knowledge in order to bridge the “gap” between human percepc
2007
AI Access Foundation. All rights reserved.

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

tion of the objects that exist within multimedia documents, and computer “perception” of
pixel values, conﬁguration applications (McGuiness, 2003), etc. Unfortunately, there are
occasions where traditional knowledge representation languages fail to accurately represent
the concepts that appear in a domain of interest. For example, this is particularly the case
when domain knowledge is inherently imprecise or vague. Concepts like that of a “near”
destination (Berners-Lee et al., 2001), a “highQuality” audio system (McGuiness, 2003),
“many” children, a “faulty” reactor (Horrocks & Sattler, 1999), “soon” and many more,
require special modelling features. In the past many applications of various research areas, like decision making, image processing, robotics and medical diagnosis have adopted
special mathematical frameworks that are intended for modelling such types of concepts
(Zimmermann, 1987; Larsen & Yager, 1993; Krishnapuram & Keller, 1992). One such a
mathematical framework is fuzzy set theory (Zadeh, 1965). Though fuzzy extensions of various logical formalisms, like propositional, predicate or modal logics have been investigated
in the past (Hajek, 1998), such a framework is is not yet well developed for Description
Logics and much research work needs to be done. More precisely, there is the need for
reasoning in very expressive fuzzy Description Logics.
In order to achieve knowledge reusability and high interoperability, modern applications
often use the concept of an “ontology” (Berners-Lee et al., 2001) to represent the knowledge
that exists within their domain. Ontologies are created by encoding the full knowledge we
possess for a speciﬁc entity of our world using a knowledge representation language. A logical formalism that has gained considerable attention the last decade is Description Logics
(Baader, McGuinness, Nardi, & Patel-Schneider, 2002a). Description Logics (DLs) are a
family of class-based (concept-based) knowledge representation formalisms, equipped with
well deﬁned model-theoretic semantics (Tarski, 1956). They are characterized by the use of
various constructors to build complex concept descriptions from simpler ones, an emphasis
on the decidability of key reasoning problems, and by the provision of sound, complete and
empirically tractable reasoning services. Both the well-deﬁned semantics and the powerful
reasoning tools that exist for Description Logics makes them ideal for encoding knowledge
in many applications like the Semantic Web (Baader et al., 2002b; Pan, 2004), multimedia applications (Meghini, Sebastiani, & Straccia, 2001), medical applications (Rector &
Horrocks, 1997), databases (Calvanese, De Giacomo, Lenzerini, Nardi, & Rosati, 1998) and
many more. Interestingly, the current standard for Semantic Web ontology languages, OWL
(Bechhofer, van Harmelen, Hendler, Horrocks, McGuinness, Patel-Schneider, & eds., 2004),
is based on Description Logics to represent knowledge and support a wide range of reasoning services. More precisely, without regarding annotation properties of OWL, the OWL
Lite species of OWL is equivalent to the SHIF(D+ ) DL, while OWL DL is equivalent to
SHOIN (D+ ) (Horrocks, Patel-Schneider, & van Harmelen, 2003). Although DLs provide
considerable expressive power, they feature limitations regarding their ability to represent
vague (fuzzy) knowledge. As obvious, in order to make applications that use DLs able to
cope with such information we have to extend them with a theory capable of representing
such kind of information. One such important theory is fuzzy set theory. Fuzzy Description
Logics are very interesting logical formalisms as they can be used in numerous domains like
multimedia and information retrieval (Fagin, 1998; Meghini et al., 2001) to provide ranking
degrees, geospatial (Chen, Fellah, & Bishr, 2005) to cope with vague concepts like “near”,
“far” and many more.
274

Reasoning with Very Expressive Fuzzy Description Logics

In order to make the need to handle vagueness knowledge more evident and the application of fuzzy set theory more intuitive, let us consider an example. Suppose that we are
creating a knowledge-based image processing application. In such application the task is
to (semi)automatically detect and recognize image objects. Suppose also that the content
of the images represents humans or animals. For such a domain one can use standard features of Description Logics to encode knowledge. For example, a knowledge base describing
human bodies could contain the following entities
Arm  ∃isPartOf.Body
Body  ∃isPartOf.Human
where  is a subsumption relation and isPartOf is obviously a transitive relation. This
knowledge can be captured with the aid of the S DL (Sattler, 1996). Moreover, one might
want to capture the knowledge that the role hasPart is the inverse of the role isPartOf,
writing hasPart := isPartOf − , thus being able to state that something that is a body and
has a tail is also an animal as,
Body  ∃hasPart.Tail  Animal.
For this new feature one would require the SI DL (Horrocks & Sattler, 1999). The
new axiom gives us the ability to recognize that the concept Arm  Tail is subsumed by
∃isPartOf.Animal. Finally, the SI DL can be further extended with role hierarchies and
number restrictions. Hence, one is able to capture the fact that the role hasDirectPart is a
sub-role of the role hasPart, by writing isDirectPartOf  isPartOf, while we can also provide
a more accurate deﬁnition of the concept Body by giving the axiom,
Body  ∃isDirectPartOf.Human ≤ 2hasArm ≥ 2hasArm
stating that the body is a direct part of a human and it also has exactly two arms.
Up to now we have only used standard Description Logic features. Now suppose that we
run an image analysis algorithm. Such algorithms usually segment the image into regions
and try to annotate them with appropriate semantic labels using low level image features.
This process involves a number of vague concepts since an image region might be red,
blue, circular, small or smooth textured to some degree or two image regions might not
be totally but only to some degree adjacent (since not all of their pixels are adjacent),
one contained within the other, etc. Hence we can only decide about the membership of
a region to a speciﬁc concept only to a certain degree (Athanasiadis, Mylonas, Avrithis,
& Kollias, 2007). For example, in our case we could have that the object o1 isPartOf the
object o2 to a degree of 0.8, that o2 isPartOf o3 to a degree of 0.9, that o1 is an Arm to a
degree of 0.75 and that o2 is a Body to a degree of 0.85. From that fuzzy knowledge one
could deduce that o3 belongs to the concept ∃hasPart.Body  ∃hasPart.Arm to a degree of
0.75. This together with a deﬁnition of the form Human ≡ ∃hasPart.Body  ∃hasPart.Arm,
where ≡ represents equivalence, means that there is a good chance that o3 is a Human.
Observe, that in this deﬁnition, in order for someone to be a human, we do not force a
Body to explicitly have a part that is an Arm. This is a reasonable choice in the present
application, because depending on the level of the segmentation, there might be several
275

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

segmented regions between o2 and o3 . As it is obvious is such applications handling the
inherent vagueness certainly beneﬁts the speciﬁc application.
In this paper we extend the well known fuzzy ALC (f-ALC) DL (Straccia, 2001) to the
fuzzy SHIN DL (f-SHIN ), which extends the f-ALC DL with the inverse role constructor,
transitive role axioms, role hierarchies and the number restrictions constructor. Moreover,
we prove the decidability of the f-SHIN DL by providing a tableaux algorithm for deciding
the standard DL inference problems. In order to provide such an algorithm we proceed in
two steps. First, we focus on the f-SI language studying the properties of fuzzy transitive
roles in value and existential restrictions, as well as the applicability of the techniques used
in the classical SI language to ensure the termination of the algorithm (Horrocks & Sattler,
1999). As we will see there is great diﬃculty on handling such axioms on the context of fuzzy
DLs, but after ﬁnishing our investigation we will see that similar notions as in classical SI
language can be applied. Secondly, we extend these results by adding role hierarchies and
number restrictions. We provide all the necessary extensions to the reasoning algorithm of
f-SI, thus providing a reasoning algorithm for the f-SHIN language. Discarding datatypes,
SHIN is slightly more expressive than SHIF (OWL Lite) and slightly less expressive than
SHOIN (OWL DL). In order to achieve our goal we again extend the techniques used for
the classical SHIN language and which ensure correctness of the algorithm (Horrocks &
Sattler, 1999; Horrocks, Sattler, & Tobies, 2000). Finally, we prove the decidability of the
extended algorithm. There are many beneﬁts on following such an approach. On the one
hand we provide a gradual presentation to the very complex algorithm of f-SHIN , while on
the other hand we provide a reasoning algorithm for a less expressive, but more eﬃcient fuzzy
DL language, f-SI. The classical SI language is known to be Pspace-complete, in contrast
to the Exptime-completeness of SHIN (Tobies, 2001), hence our algorithm for f-SI can
be used for future research and for providing eﬃcient and optimized implementations.
Please note that fuzzy DLs (Straccia, 2001) are complementary to other approaches that
extend DLs, like probabilistic DLs (Koller, Levy, & Pfeﬀer, 1997; Giugno & Lukasiewicz,
2002; Ding & Peng, 2004), or possibilistic DLs (Hollunder, 1994). More precisely, these
theories are meant to be used for capturing diﬀerent types of imperfect information and
knowledge. Fuzziness is purposed for capturing vague (fuzzy) knowledge, i.e. facts that are
certain but which have degrees of truth assigned to them, like for example the degree of
truth of someone being tall. On the other hand, possibilistic and probabilistic logics are
purposed for capturing cases where knowledge is uncertain due to lack of information or
knowledge about a speciﬁc situation or a future event, like for example a sensor reading
or a weather forecast. These facts are assigned degrees of possibility, belief or probability,
rather than truth degrees. Dubois and Prade (2001) provide a comprehensive analysis on
these theories along with their diﬀerent properties.
The rest of the paper is organized as follows. Section 2 brieﬂy introduces the DL SHIN
and provides some preliminaries about the notion of a fuzzy set and how set theoretic and
logical operations have been extended to the fuzzy set framework. Section 3 introduces the
syntax and semantics of the fuzzy SHIN DL, which we call f-SHIN .1 language. Section
4 provides an investigation on the semantics of fuzzy DLs when fuzzy transitive relations
1. In a previous approach to fuzzy DLs the notation fALC is used (Straccia, 2004), but this notation is
not so ﬂexible to represent fuzzy DLs which use diﬀerent norm operations, as we will see later on. In
some other approaches (Tresp & Molitor, 1998; Hölldobler, Khang, & Störr, 2002) the naming ALC F is

276

Reasoning with Very Expressive Fuzzy Description Logics

participate in value and existential restrictions. In section 5 we give a detailed presentation
of the reasoning algorithm for deciding the consistency of a fuzzy-SI ABox and we provide
the proofs for the termination, soundness and completeness of the procedure. Then, in
section 6 we extend the previous results by adding role hierarchies and number restrictions.
More precisely, the results of section 4 are enriched by considering transitive roles and roles
hierarchies in value and existential restrictions. Using this results we extend the algorithm of
section 5 to handle with these new feature and ﬁnally we prove its soundness, completeness
and termination. At last, in section 7 we review some previous work on fuzzy Description
Logics while section 8 concludes the paper.

2. Preliminaries
In the current section we will brieﬂy introduce classical DLs and fuzzy set theory, recalling
some mathematical properties of fuzzy set theoretic operators.
2.1 Description Logics and the SHIN DL
Description Logics (DLs) (Baader et al., 2002a) are a family of logic-based knowledge representation formalisms designed to represent and reason about the knowledge of an application
domain in a structured and well-understood way. They are based on a common family of
languages, called description languages, which provide a set of constructors to build concept (class) and role (property) descriptions. Such descriptions can be used in axioms and
assertions of DL knowledge bases and can be reasoned about with respect to (w.r.t.) DL
knowledge bases by DL systems.
In this section, we will brieﬂy introduce the SHIN DL, which will be extended to the
f-SHIN DL later. A description language consists of an alphabet of distinct concept names
(C), role names (R) and individual (object) names (I); together with a set of constructors
to construct concept and role descriptions.
Now we deﬁne the notions of SHIN -roles and SHIN -concepts.
Deﬁnition 2.1 Let RN ∈ R be a role name and R a SHIN -role. SHIN -role descriptions
(or simply SHIN -roles) are deﬁned by the abstract syntax: S ::= RN | R− . The inverse
relation of roles is symmetric, and to avoid considering roles such as R−− , we deﬁne a
function Inv which returns the inverse of a role, more precisely,

RN − if R = RN ,
Inv(R) :=
RN
if R = RN − .
The set of SHIN -concept descriptions (or simply SHIN -concepts) is the smallest set such
that:
1. every concept name CN ∈ C is a SHIN -concept,
2. if C and D are SHIN -concepts and R is a SHIN -role, then ¬C, C D, C D, ∀R.C
and ∃R.C are also SHIN -concepts, called general negation (or simply negation),
used but this can easily be confused with ALCF (ALC extended with functional restrictions, Horrocks
& Sattler, 1999), when pronounced.

277

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Constructor
top
bottom
concept name
general negation
conjunction
disjunction
exists restriction
value restriction
at-most restriction
at-least restriction

Syntax


⊥
CN
¬C
CD
CD
∃R.C
∀R.C
≤ nR
≥ nR

Semantics
∆I
∅
CNI ⊆ ∆I
∆I \ C I
C I ∩ DI
C I ∪ DI
{x ∈ ∆I | ∃y.x, y ∈ RI ∧ y ∈ C I }
{x ∈ ∆I | ∀y.x, y ∈ RI → y ∈ C I }
{x ∈ ∆I | {y ∈ ∆I | RI (x, y)} ≤ n}
{x ∈ ∆I | {y ∈ ∆I | RI (x, y)} ≥ n}

Table 1: Semantics of SHIN -concepts

disjunction, conjunction, value restrictions and existential restriction, respectively,
and
3. if S a simple2 SHIN -role and n ∈ N, then (≥ nS) and (≤ nS) are also SHIN concepts, called at-most and at-least number restrictions.
By removing point 3 of the above deﬁnition we obtain the set of SI-concepts.
Description Logics have a model-theoretic semantics, which is deﬁned in terms of interpretations. An interpretation (written as I) consists of a domain (written as ∆I ) and an
interpretation function (written as ·I ), where the domain is a nonempty set of objects and
the interpretation function maps each individual name a ∈ I to an element aI ∈ ∆I , each
concept name CN ∈ C to a subset CNI ⊆ ∆I , and each role name RN ∈ R to a binary
relation RN I ⊆ ∆I × ∆I . The interpretation function can be extended to give semantics
to concept and role descriptions. These are given in Table 1.
A SHIN knowledge base (KB) consists of a TBox, an RBox and an ABox. A SHIN
TBox is a ﬁnite set of concept inclusion axioms of the form C  D, or concept equivalence
axioms of the form C ≡ D, where C, D are SHIN -concepts. An interpretation I satisﬁes
C  D if C I ⊆ DI and it satisﬁes C ≡ D if C I = DI . Note that concept inclusion
axioms of the above form are called general concept inclusions (Horrocks & Sattler, 1999;
Baader, 1990). A SHIN RBox is a ﬁnite set of transitive role axioms (Trans(R)), and role
inclusion axioms (R  S). An interpretation I satisﬁes Trans(R) if, for all x, y, z ∈ ∆I ,
{x, y, y, z} ⊆ RI → x, z ∈ RI , and it satisﬁes R  S if RI ⊆ S I . A set of role
* as the
inclusion axioms deﬁnes a role hierarchy. For a role hierarchy we introduce 
transitive-reﬂexive closure of . At last, observe that if R  S, then the semantics of role
inclusion axioms imply that Inv(R)I ⊆ Inv(S)I , while the semantics of inverse roles imply
that Trans(Inv(R)). A SI RBox is obtained by a SHIN RBox if we disallow role inclusion
axioms. A SHIN ABox is a ﬁnite set of individual axioms (or assertions) of the form
.
a : C, called concept assertions, or a, b : R, called role assertions, or of the form a = b.
2. A role is called simple if it is neither transitive nor has any transitive sub-roles. This is crucial in order
to get a decidable logic (Horrocks, Sattler, & Tobies, 1999).

278

Reasoning with Very Expressive Fuzzy Description Logics

An interpretation I satisﬁes a : C if aI ∈ C I , it satisﬁes a, b : R if aI , bI  ∈ RI , and
.
it satisﬁes a = b if aI = bI . A SI ABox is obtained by a SHIN ABox by disallowing
.
inequality axioms a = b. An interpretation I satisﬁes a SHIN knowledge base Σ if it
satisﬁes all the axioms in Σ. Σ is satisﬁable (unsatisﬁable) iﬀ there exists (does not exist)
such an interpretation I that satisﬁes Σ. Let C, D be SHIN -concepts, C is satisﬁable
(unsatisﬁable) w.r.t. Σ iﬀ there exists (does not exist) an interpretation I of Σ s.t. C I = ∅;
C subsumes D w.r.t. Σ iﬀ for every interpretation I of Σ we have C I ⊆ DI . Given a
concept axiom, a role axiom, or an assertion Ψ, Σ entails Ψ, written as Σ |= Ψ, iﬀ for all
models I of Σ we have I satisﬁes Ψ.
2.2 Fuzzy Sets
Fuzzy set theory and fuzzy logic are widely used today for capturing the inherent vagueness
(the lack of distinct boundaries of sets) that exists in real life applications (Klir & Yuan,
1995). The notion of a fuzzy set was ﬁrst introduced by Zadeh (1965). While in classical
set theory an element either belongs to a set or not, in fuzzy set theory elements belong
only to a certain degree. More formally, let X be a collection of elements (called universe
of discourse) i.e X = {x1 , x2 , . . .}. A crisp subset A of X is any collection of elements of
X that can be deﬁned with the aid of its characteristic function χA (x) that assigns any
x ∈ X to a value 1 or 0 if this element belongs to X or not, respectively. On the other
hand, a fuzzy subset A of X, is deﬁned by a membership function µA (x), or simply A(x),
for each x ∈ X. This membership function assigns any x ∈ X to a value between 0 and
1 that represents the degree in which this element belongs to A. Additionally, a fuzzy
binary relation R over two crisp sets X and Y is a function R : X × Y → [0, 1]. For
example, one can say that T om belongs to the set of T all people to a degree of 0.8, writing
T all(T om) = 0.8, or that the object o1 is part of the object o2 to a degree of 0.6, writing
isP artOf (o1 , o2 ) = 0.6. Several properties of fuzzy binary relations have been investigated
in the literature (Klir & Yuan, 1995). For example, a binary fuzzy relation is called sup-min
transitive if R(x, z) ≥ supy∈Y {min(R(x, y), R(y, z))}, while the inverse of a relation R is
deﬁned as R−1 (y, x) = R(x, y) (Klir & Yuan, 1995).
Using the above idea, the most important operations deﬁned on crisp sets and relations,
like the boolean operations (complement, union, and intersection etc.), are extended in order
to cover fuzzy sets and fuzzy relations. Accordingly, a sound and complete mathematical
framework that plays an important role in the management of imprecise and vague information has been deﬁned and used in a wide set of scientiﬁc areas including expert systems and
decision making (Zimmermann, 1987), pattern recognition (Kandel, 1982), image analysis
and computer vision (Krishnapuram & Keller, 1992), medicine (Oguntade & Beaumont,
1982), control (Sugeno, 1985), etc.
2.3 Fuzzy Set Theoretic Operations
In this section, we will explain how to extend boolean operations and logical implications
in the context of fuzzy sets and fuzzy logics. These operations are now performed by
mathematical functions over the unit interval.
The operation of complement is performed by a unary operation, c : [0, 1] → [0, 1], called
fuzzy complement. In order to provide meaningful fuzzy complements, such functions should
279

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

satisfy certain properties. More precisely, they should satisfy the boundary conditions,
c(0) = 1 and c(1) = 0, and be monotonic decreasing, for a ≤ b, c(a) ≥ c(b). In the
current paper we will use the Lukasiewicz negation, c(a) = 1 − a, which additionally is
continuous and involutive, for each a ∈ [0, 1], c(c(a)) = a holds. In the cases of fuzzy
intersection and fuzzy union the mathematical functions used are binary over the unit
interval. These functions are usually called norm operations referred to as t-norms (t), in
the case of fuzzy intersection, and t-conorms (or s-norms) (u), in the case of fuzzy union
(Klement, Mesiar, & Pap, 2004). Again these operations should satisfy certain mathematical
properties. More precisely, a t-norm (t-conorm) satisﬁes the boundary condition, t(a, 1) = a
(u(a, 0) = a), is monotonic increasing, for b ≤ d then t(a, b) ≤ t(a, d) (u(a, b) ≤ u(a, d)),
commutative, t(a, b) = t(b, a) (u(a, b) = u(b, a)), and associative, t(a, t(b, c)) = t(t(a, b), c)
(u(a, u(b, c)) = u(u(a, b), c)). Though there is a wealth of such operations in the literature
(Klir & Yuan, 1995) we restrict our attention to speciﬁc ones. More precisely, we are
using the Gödel t-norm, t(a, b) = min(a, b) and the Gödel t-conorm, u(a, b) = max(a, b).
Additionally to the aforementioned properties, these operations are also idempotent, i.e.
min(a, a) = a and max(a, a) = a, hold. Finally, a fuzzy implication is performed by a
binary operation, of the form J : [0, 1] × [0, 1] → [0, 1]. In the current paper we use the
Kleene-Dienes fuzzy implication which is provided by the equation, J (a, b) = max(c(a), b).
The reason for restricting our attention to these operations would be made clear in section
5.1. We now recall a property of the max norm operation that we are going to use in
the investigation of the properties of transitive relations under the framework of fuzzy set
theory.
Lemma 2.2 (Hajek, 1998) For any a, b ∈ [0, 1], where j takes values from the index set J,
the max operation satisﬁes the following property:
• inf j∈J max(a, bj ) = max(a, inf j∈J bj ).

3. The fKD -SHIN DL
In this section, we introduce a fuzzy extension of the SHIN DL presented in Section 2.1.
Following Stoilos, Stamou, Tzouvaras, Pan, and Horrocks (2005b), since we are using the
Kleene-Dienes (KD) fuzzy implication in our language, we call it fKD -SHIN . This presentation follows the standard syntax and semantics of fuzzy DLs, that has been introduced in
the literature (Straccia, 2001; Hölldobler et al., 2002; Sánchez & Tettamanzi, 2004). More
precisely, fKD -SHIN was ﬁrst presented by Straccia (2005b). For completeness reasons
we will also present the language fKD -SHIN here. Please also note that our presentation
diﬀers from that of Straccia (2005b) in the semantics of concept and role inclusion axioms.
As usual, we consider an alphabet of distinct concept names (C), role names (R) and
individual names (I). The abstract syntax of fKD -SHIN -concepts and fKD -SHIN -roles
(and respectively of fKD -SI-concepts and fKD -SI-roles) is the same as their SHIN counterparts; however, their semantics is based on fuzzy interpretations (see below). Similarly,
fKD -SHIN keeps the same syntax of concept and role axioms as their counterparts in
SHIN . Interestingly, fKD -SHIN extends SHIN individual axioms (assertions) into fuzzy
individual axioms, or fuzzy assertions (following, Straccia, 2001), where membership degrees
can be asserted.
280

Reasoning with Very Expressive Fuzzy Description Logics

Firstly, by using membership functions that range over the interval [0, 1], classical interpretations can be extended to the concept of fuzzy interpretations (Straccia, 2001). Here
we abuse the symbols and deﬁne a fuzzy interpretation as a pair I = (∆I , ·I ),3 where the
domain ∆I is a non-empty set of objects and ·I is a fuzzy interpretation function, which
maps
1. an individual name a ∈ I to an element aI ∈ ∆I ,
2. a concept name A ∈ C to a membership function AI : ∆I → [0, 1],
3. a role name R ∈ R to a membership function RI : ∆I × ∆I → [0, 1].
For example, if o ∈ ∆I then AI (o) gives the degree that the object o belongs to the fuzzy
concept A, e.g. AI (o) = 0.8. By using the fuzzy set theoretic operations deﬁned in section
2.3, the fuzzy interpretation function can be extended to give semantics to fKD -SHIN concepts and fKD -SHIN -roles. For example, since we use the max function for fuzzy
union the membership degree of an object a to the fuzzy concept (C  D)I is equal to
max(C I (a), DI (a)). Moreover since, according to Table 1, a value restriction ∀R.C is an
implication of the form, ∀y(R(x, y) → C(y)), we can interpret ∀ as inf (Hajek, 1998), and
→ as the Kleene-Dienes fuzzy implication and ﬁnally have the equation, inf b∈∆I {max(1 −
RI (a, b), C I (b))}. The complete set of semantics is depicted in Table 2. We have to note
that there are many proposals for semantics of number restrictions in fuzzy DLs (Sánchez &
Tettamanzi, 2004; Straccia, 2005b). We choose to follow the semantics proposed by Straccia
(2005b) since they are based in the First-Order interpretation of number restrictions (Baader
et al., 2002a). Moreover, as it is shown by Stoilos, Stamou, Tzouvaras, Pan, and Horrocks
(2005a) and as we will see in section 6, under these semantics all inference services of
fKD -SHIN stay decidable and reasoning can be reduced to a simple counting problem,
yielding an eﬃcient algorithm. Note that, although most of the above semantics have been
presented elsewhere (Sánchez & Tettamanzi, 2004; Straccia, 2005b), we include them here
simply for the sake of completeness.
An fKD -SHIN knowledge base consists of a TBox, an RBox and an ABox. Let A be a
concept name and C an fKD -SHIN concept. An fKD -SHIN TBox is a ﬁnite set of fuzzy
concept axioms of the form A  C, called fuzzy inclusion introductions, and of the form
A ≡ C, called fuzzy equivalence introductions. A fuzzy interpretation I satisﬁes A  C if
∀o ∈ ∆I , AI (o) ≤ C I (o). A fuzzy interpretation satisﬁes A ≡ C if ∀o ∈ ∆I , AI (o) = C I (o).
A fuzzy interpretation I satisﬁes an fKD -SHIN TBox T iﬀ it satisﬁes all fuzzy concept
axioms in T ; in this case, we say that I is a model of T .
There are two remarks here. Firstly, we give a crisp subsumption of fuzzy concepts here,
which is the usual way subsumption is deﬁned in the context of fuzzy sets (Klir & Yuan,
1995). In contrast, Straccia (2005b) deﬁnes a fuzzy subsumption of fuzzy concepts. As it
was noted by Bobillo, Delgado, and Gómez-Romero (2006) in fKD -DLs fuzzy subsumption is
sometimes counterintuitive. Secondly, as we can see, we are only allowing for simple TBoxes.
A TBox T is called simple if it neither includes cyclic nor general concept inclusions, i.e.
axioms are of the form A  C or A ≡ C, where A is a concept name that is never deﬁned by
3. In the rest of the paper, we use I = (∆I , ·I ) to represent fuzzy interpretations instead of crisp interpretations.

281

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Constructor
top
bottom
general negation
conjunction
disjunction
exists restriction
value restriction
at-most
at-least
inverse role

Syntax


⊥
¬C
CD
CD
∃R.C
∀R.C
≤ pR
≥ pR
R−

Semantics

I (a) = 1
⊥I (a) = 0
(¬C)I (a) = 1 − C I (a)
(C  D)I (a) = min(C I (a), DI (a))
(C  D)I (a) = max(C I (a), DI (a))
(∃R.C)I (a) = supb∈∆I {min(RI (a, b), C I (b))}
(∀R.C)I (a) = inf b∈∆I {max(1 − RI (a, b), C I (b))}
I
inf b1 ,...,bp+1 ∈∆I maxp+1
i=1 {1 − R (a, bi )}
p
supb1 ,...,bp ∈∆I mini=1 {RI (a, bi )}
(R− )I (b, a) = RI (a, b)

Table 2: Semantics of fKD -SHIN -concepts and fKD -SHIN -roles

itself either directly or indirectly. A procedure to deal with cyclic and general TBoxes, in
the context of fuzzy DLs, has been recently developed by Stoilos, Straccia, Stamou, and Pan
(2006), while also in parallel a slightly diﬀerent technique was presented by Li, Xu, Lu, and
Kang (2006a). This process involves additional expansion rules and a preprocessing step
called normalization, which are not aﬀected by the expressivity of the underlying fuzzy DL.
Hence, in order to keep our presentation simple we will not consider general TBoxes in the
following, but we will focus on the decidability and reasoning of fKD -SI and fKD -SHIN ,
which involve many technical details. At the end of section 6 we will comment more on the
issue of handling GCIs in the fKD -SHIN language.
An fKD -SHIN RBox is a ﬁnite set of fuzzy transitive role axioms of the form Trans(R)
and fuzzy role inclusion axioms of the form R  S, where R, S are fKD -SHIN -roles. A fuzzy
interpretation I satisﬁes Trans(R) if ∀a, c ∈ ∆I , RI (a, c) ≥ supb∈∆I {min(RI (a, b), RI (b, c))},
while it satisﬁes R  S if ∀a, b ∈ ∆I , RI (a, b) ≤ S I (a, b). Note that the semantics result
from the deﬁnition of sup-min transitive relations in fuzzy set theory. A fuzzy interpretation
I satisﬁes an fKD -SHIN RBox R iﬀ it satisﬁes all fuzzy transitive role axioms in R; in
this case, we say that I is a model of R. Similarly with the classical SHIN language, the
semantics of inverse roles and role inclusion axioms of fKD -SHIN imply that from Trans(R)
and R  S it holds that Trans(Inv(R)) and Inv(R)−  Inv(S)− .
An fKD -SHIN ABox is a ﬁnite set of fuzzy assertions (Straccia, 2001) of the form
(a : C)n or (a, b : R)n, where  stands for ≥, >, ≤ and <, and n ∈ [0, 1] or of the
.
form a =
 b. Intuitively, a fuzzy assertion of the form (a : C) ≥ n means that the membership
degree of the individual a to the concept C is at least equal to n. We call assertions deﬁned
by ≥, > positive assertions, while those deﬁned by ≤, < negative assertions. Formally, given
a fuzzy interpretation I,
I satisﬁes (a : C) ≥ n
I satisﬁes (a : C) ≤ n
I satisﬁes (a, b : R) ≥ n
I satisﬁes (a, b : R) ≤ n
.
I satisﬁes a = b
282

if
if
if
if
if

C I (aI ) ≥ n,
C I (aI ) ≤ n,
RI (aI , bI ) ≥ n,
RI (aI , bI ) ≤ n,
aI = bI .

Reasoning with Very Expressive Fuzzy Description Logics

The satisﬁability of fuzzy assertions with >, < is deﬁned analogously. Observe that, we can
also simulate assertions of the form (a : C) = n by considering two assertions of the form
(a : C) ≥ n and (a : C) ≤ n (Hölldobler et al., 2002; Straccia, 2001). A fuzzy interpretation
I satisﬁes an fKD -SHIN ABox A iﬀ it satisﬁes all fuzzy assertions in A; in this case, we
say that I is a model of A.
Furthermore, as it was noted by Straccia (2001, 2005b), due to the mathematical properties of the norm operations deﬁned in section 2.3, the following fKD -SHIN -concept equivalences are satisﬁed: ¬
 ≡ ⊥, ¬⊥ ≡ 
, C  
 ≡ C, C  ⊥ ≡ C, C  
 ≡ 
 and C  ⊥ ≡ ⊥.
Furthermore, since the Lukasiewicz complement is involutive it holds that, ¬¬C ≡ C.
Moreover, the De Morgan laws: C1  C2 ≡ ¬(¬C1  ¬C2 ), C1  C2 ≡ ¬(¬C1  ¬C2 ), are
satisﬁed. As a consequence of the satisﬁability of the De Morgan laws and the use of the
Kleene-Dienes fuzzy implication the following concept equivalences also hold.
¬∃R.C

≡

∀R.(¬C),

¬ ≤ p1 R

≡

≥ (p1 + 1)R,

¬∀R.C

≡

¬ ≥ p1 R

≡

∃R.(¬C),

≤ (p1 − 1)R, p1 ∈ N∗
⊥,
p1 = 0

At last note that the classical laws of contradiction (C  ¬C ≡ ⊥) and excluded middle
(C  ¬C ≡ 
), do not hold.
Example 3.1 Let us revisit the fuzzy knowledge base (Σ) that we informally introduced in
section 1. Formally, the knowledge base can be deﬁned as follows: Σ = T , R, A, where
T

=

{Arm  ∃isPartOf.Body,
Body  ∃isPartOf.Human},

A

=

{(o1 , o2  : isPartOf) ≥ 0.8,(o2 , o3  : isPartOf) ≥ 0.9,
(o2 : Body) ≥ 0.85, (o1 : Arm) ≥ 0.75},

R

=

{Trans(isPartOf)}.

Now, in order for some fuzzy interpretation I to be a model of T it should hold that
ArmI (oIi ) ≤ (∃isPartOf.Body)I (oIi ) and BodyI (oIi ) ≤ (∃isPartOf.Body)I (oIi ), ∀oIi ∈ ∆I .
Furthermore, if isPartOf I (oI1 , oI2 ) ≥ 0.8, isPartOf I (oI2 , oI3 ) ≥ 0.9, BodyI (oI2 ) ≥ 0.85 and
ArmI (oI1 ) ≥ 0.75, then I is also a model of A. As a model of the RBox R, I should
also satisfy that isPartOf I (oI1 , oI3 ) ≥ supa∈∆I {min(isPartOf I (oI1 , a), isPartOf I (a, oI3 ))} =
sup{. . . , min(0.8, 0.9), . . .} ≥ 0.8.
Now let us consider the concept ∃hasPart.Body  ∃hasPart.Arm that we mentioned in
Section 1. Due to the semantics of existential restrictions presented in Table 2, we have
that
I

(∃hasPart.Body)I (oI3 )

=

supa∈∆I {min((isPartOf − ) (oI3 , a), BodyI (a))} ≥ 0.85,

(∃hasPart.Arm)I (oI3 )

=

supa∈∆I {min((isPartOf − ) (oI3 , a), ArmI (a))} ≥ 0.75.

I

Hence o3 would belong in the intersection of the two concepts with the minimum membership
degree which is greater or equal than 0.75, as we claimed in Section 1.
♦

283

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

φ≥n
φ>n

φ<m
n≥m
n≥m

φ≤m
n>m
n≥m

Table 3: Conjugated pairs of fuzzy assertions
Following Straccia (2001), we introduce the concept of conjugated pairs of fuzzy assertions to represent pairs of assertions that form a contradiction. The possible conjugated
pairs are deﬁned in Table 3, where φ represents a SI assertion. For example, if φ = a : C,
then the assertions (a : C) ≥ 0.7 and (a : C) < 0.7 conjugate. Furthermore, due to the
presence of inverse roles and role inclusion axioms the deﬁnition should be slightly extended
from that of Straccia (2001) and hence, one should also take under consideration possible
inverse roles or a role hierarchy when checking for conjugation two role assertions. For ex* S, then the assertion (a, b : R) ≥ 0.9, conjugates with (b, a : Inv(S)) ≤ 0.4;
ample, if R 
similarly for the rest of the inequalities.
Now, we will deﬁne the reasoning problems of the fKD -SHIN DL.
A fuzzy interpretation I satisﬁes an fKD -SHIN knowledge base Σ if it satisﬁes all
axioms in Σ; in this case, I is called a model of Σ. An fKD -SHIN knowledge base Σ
is satisﬁable (unsatisﬁable) iﬀ there exists (does not exist) a fuzzy interpretation I which
satisﬁes all axioms in Σ. An fKD -SHIN -concept C is satisﬁable (unsatisﬁable) w.r.t. an
RBox R and a TBox T iﬀ there exists (does not exist) some model I of R and T for
which there is some a ∈ ∆I such that C I (a) = n, and n ∈ (0, 1]. In this case, C is called
n-satisﬁable w.r.t. R and T (Navara, 2000). Let C and D be two fKD -SHIN -concepts.
We say that C is subsumed by D w.r.t. R and T if for every model I of R and T it holds
that, ∀x ∈ ∆I .C I (x) ≤ DI (x). Furthermore, an fKD -SHIN ABox A is consistent w.r.t. R
and T if there exists a model I of R and T that that is also a model of A. Moreover, given
a fuzzy concept axiom or a fuzzy assertion Ψ ∈ {C  D, C ≡ D, φn}, an fKD -SHIN
knowledge base Σ entails Ψ, written Σ |= Ψ, iﬀ all models of Σ also satisfy Ψ.
Furthermore, by studying Table 3, we can conclude that an fKD -SHIN ABox A
can contain a number of positive or negative assertions without forming a contradiction.
Therefore, it is useful to compute lower and upper bounds of truth-values. Given an
fKD -SHIN knowledge base Σ and an assertion φ, the greatest lower bound of φ w.r.t.
Σ is glb(Σ, φ) = sup{n : Σ |= φ ≥ n}, where sup ∅ = 0. Similarly, the least upper bound of
φ w.r.t. Σ is lub(Σ, φ) = inf{n : Σ |= φ ≤ n}, where inf ∅ = 1. A decision procedure to
solve the best truth-value bound was provided by Straccia (2001). In that procedure the
membership degrees that appear in a fKD -SHIN ABox, together with their complemented
values and the degrees 0, 0.5 and 1, were collected in a set of membership degrees N Σ
and subsequently the entailment of a fuzzy assertions φ ≥ n and φ ≤ n, for all n ∈ N Σ
was tested, thus determining glb and lub. Obviously this procedure is independent of the
expressivity of the DL language, and thus also applicable in our context.
Remark 3.2 From Table 2 we see that the semantics of the value and existential restrictions in fuzzy DLs are deﬁned with the aid of an inﬁmum and a supremum operation. This means that we can construct an inﬁnite interpretation I, i.e. an interpretation
284

Reasoning with Very Expressive Fuzzy Description Logics

where ∆I = {b1 , b2 , . . .} contains inﬁnite number of objects, for which ∀R.C is n-satisﬁable
((∀R.C)I (a) = n for some a ∈ ∆I ) but for all bi ∈ ∆I , max(1 − RI (a, bi ), C I (bi )) > n.
This is possible since although the maximum of the membership degrees involved for each
individual object bi is strictly greater than n the limit of the inﬁnite sequence could converge
to n. This fact was ﬁrst noted for fuzzy DLs by Hajek (2005), introducing the notion of
witnessed model for fuzzy DLs. A model is called witnessed if for (∀R.C)I (a) = n there is
some b ∈ ∆I such that either RI (a, bi ) = 1−n or C I (bi ) = n, i.e. there is some b ∈ ∆I that
witnesses the membership degree of a to ∀R.C. Fortunately, there are fuzzy logics that have
an inﬁnite model if and only if they have a witnessed model. More precisely, Hajek proves
this property for the Lukasiewicz fuzzy logic4 . He then concludes that the same proofs can be
modiﬁed to apply to the fuzzy logic deﬁned by the fuzzy operators we are using in the current
paper. That is because these operators are deﬁnable in the Lukasiewicz logic (Mostert &
Shields, 1957). For the rest of the paper, without loss of generality, we are going to consider
only witnessed models.
In this paper, we will provide an algorithm to decide the fuzzy ABox consistency problem
w.r.t. an RBox in very expressive fuzzy DLs. Many other reasoning problems can be reduced
to this problem. Firstly, concept satisﬁability for a fuzzy concept C can be reduced to
consistency checking of the fuzzy ABox {(a : C) > 0}. Secondly, in this paper, we only
consider unfoldable TBoxes, where KB satisﬁability can be reduced to ABox consistency
w.r.t. an RBox. A TBox is unfoldable if it contains no cycles and contains only unique
introductions, i.e., concept axioms with only concept names appearing on the left hand side
and, for each concept name A, there is at most one axiom in T of which A appears on the
left side. A knowledge base with an unfoldable TBox can be transformed into an equivalent
one with an empty TBox by a transformation called unfolding, or expansion (Nebel, 1990):
Concept inclusion introductions A  C are replaced by concept equivalence introductions
A ≡ A  C, where A is a new concept name, which stands for the qualities that distinguish
the elements of A from the other elements of C. Subsequently, if C is a complex concept
expression, which is deﬁned in terms of concept names, deﬁned in the TBox, we replace
their deﬁnitions in C. It can be proved that the initial TBox with the expanded one are
equivalent.
Moreover, the problem of entailment can be reduced to the problem of fuzzy knowledge
base satisﬁability (Straccia, 2001). More precisely, for Σ = T , R, A, Σ |= φ  n iﬀ
Σ = T , R, A ∪ {φ ¬  n} is unsatisﬁable. With ¬ , we denote the “negation” of
inequalities; e.g., if  ≡ ≥ then ¬  ≡ <, while if  ≡ < then ¬  ≡ ≥. Finally, the
subsumption problem of two fuzzy concepts C and D w.r.t. a TBox can also be reduced to
the fuzzy knowledge base satisﬁability problem. More formally, Straccia (2001), proved that
T , ∅, A |= C  D iﬀ T , ∅, {(a : C) ≥ n, (a : D) < n}, for both n ∈ {n1 , n2 }, n1 ∈ (0, 0.5]
and n2 ∈ (0.5, 1], is unsatisﬁable. The above reduction can be extended in order for a fuzzy
knowledge base to also include an RBox. Please note that, in crisp DLs, in order to check
if a concept C is subsumed by a concept D we check for the unsatisﬁability of the concept,
C  ¬D. This reduction to unsatisﬁability is not applicable to fKD -DLs since the fuzzy
operations that we use do not satisfy the laws of contradiction and excluded middle.
4. Lukasiewicz fuzzy logic uses the t-norm t(a, b) = max(0, a + b − 1), the t-conorm u(a, b) = min(1, a + b),
the Lukasiewicz complement and the fuzzy implication J (a, b) = min(1, 1 − a + b)

285

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

We conclude the section with an example.
Example 3.3 Consider again our sample knowledge base (Σ). By applying the transformation of unfolding, deﬁned earlier, one would obtain the following expanded fuzzy TBox:
T

=

{Arm ≡ Arm  ∃isPartOf.Body,
Body ≡ Body  ∃isPartOf.Human}

while the respective fuzzy assertions would be transformed to
A

=

{(o2 : Body  ∃isPartOf.Human) ≥ 0.85,
(o1 : Arm  ∃isPartOf.Body) ≥ 0.75}

Now, let us formally specify the query we introduced in section 1. If Σ = T  , R, A  is
our modiﬁed knowledge base, after unfolding, the query would have the form Σ |= (o3 :
∃hasPart.Body  ∃hasPart.Arm) ≥ 0.75. According to our previous discussion in order to
check for the entailment of such a query one should check for the consistency of the fuzzy
ABox A ∪ {(o3 : ∃hasPart.Body  ∃hasPart.Arm) < 0.75}, w.r.t. the RBox R, since after the
expansion we can remove T  . Our task in the following sections is to provide a procedure
that decides the consistency of a fuzzy ABox w.r.t. an RBox.
♦

4. Transitivity in Fuzzy Description Logics
In classical DLs, a role R is transitive iﬀ for all a, b, c ∈ ∆I , a, b ∈ RI and b, c ∈ RI
imply a, c ∈ RI . Sattler (1996) shows that, for a, b, c1 , . . . , cn ∈ ∆I , if R is transitive, b
is an R-successor of a, c1 , . . . , cn are the R-successors of b, and a ∈ (∀R.C)I , then all Rsuccessors of a should be instances of (∀R.C)I , e.g., b ∈ (∀R.C)I because: (i) a, ci  ∈ RI
(as R is transitive), (ii) ci ∈ C I (as a ∈ (∀R.C)I ) and (iii) b ∈ (∀R.C)I (due to the
semantics of ∀R.C). In other words, this means that the following concept subsumption
holds, ∀R.C  ∀R.(∀R.C).
The above property suggests that value restrictions on transitive relations (∀R.C) are
propagated along the path of individuals. This propagation is crucial for reasoning algorithms in order to retain the tree-model property (Baader et al., 2002a), which is a property
that leads to decidable decision procedures (Vardi, 1997). Our goal in the rest of the section is to investigate this property in the context of fuzzy Description Logics that allow for
transitive role axioms. We have to determine if similar propagation occurs and if it is, to
ﬁnd out the membership degree that the propagation carries to subsequent objects. This is
the ﬁrst time that such an investigation is presented in the literature.
In fuzzy DLs, objects are instances of all possible fuzzy concepts in some degree, ranging
over the interval [0, 1]. As we have shown in Section 3, a fuzzy role R is transitive iﬀ, for all
a, c ∈ ∆I , RI (a, c) ≥ supb∈∆I min(RI (a, b), RI (b, c)). Since this holds for the supremum, for
an arbitrary b ∈ ∆I we have that, RI (a, c) ≥ min(RI (a, b), RI (b, c)) and by applying fuzzy
complement in both sides we get, c(RI (a, c)) ≤ c(min(RI (a, b), RI (b, c))). In what follows,
we will show that not only value restrictions, but also existential restrictions on transitive
286

Reasoning with Very Expressive Fuzzy Description Logics

roles (∃R.C) are being propagated, so as to satisfy inﬁmum and supremum restrictions.
Now we look at the value restrictions. Let a, b ∈ ∆I be objects in ∆I and R a transitive
role. If (∀R.C)I (a) ≥ va , we have (note that c below represents fuzzy complement)
⇒monotonicity
(1) inf d∈∆I max(c(RI (a, d)), C I (d)) ≥ va
(2) inf d∈∆I max(c(min(RI (a, b), RI (b, d))), C I (d)) ≥ va
(3) inf d∈∆I max(max(c(RI (a, b)), c(RI (b, d))), C I (d)) ≥ va

⇒De M organ
⇒associativity

(4) inf d∈∆I max(c(RI (a, b)), max(c(RI (b, d)), C I (d))) ≥ va

⇒Lemma

max(c(RI (a, b)), inf

max(c(RI (b, d)), C I (d)))

(5)
d∈∆I
(6) max(c(RI (a, b)), (∀R.C)I (b)) ≥ va ,

≥ va

2.2

⇒

which means either c(RI (a, b)) ≥ va or (∀R.C)I (b) ≥ va . There are some remarks here.
Firstly, the above b is an arbitrary object in ∆I . In other words, for any object x ∈ ∆I , if
c(RI (a, x)) < va , we have (∀R.C)I (x) ≥ va . Similarly, if (∀R.C)I (a) > va ≥ c(RI (a, x)),
we have (∀R.C)I (x) > va . Hence, the following result is obtained.
Corollary 4.1 If (∀R.C)I (a)  n and Trans(R) then, in an fKD -DL, (∀R.(∀R.C))I (a)  n
holds.
Now, let a, b ∈ ∆I , R a transitive role and consider the case (∃R.C)I (a) ≤ ea . By
applying a fuzzy complement in both sides of the inequation, and since fuzzy complements
are monotonic decreasing, we obtain c((∃R.C)I (a)) ≥ c(ea ). Based on the semantics of
the language this can be rewritten as (¬(∃R.C))I (a) ≥ c(ea ) and by using the concept
equivalences presented in the previous section we have, (∀R.(¬C))I (a) ≥ c(ea ). Hence, by
using the above results for value restrictions we can conclude that for any object x ∈ ∆I , if
c(RI (a, x)) < c(ea ) ⇒ RI (a, x) > ea , then (∀R.(¬C))I (x) ≥ c(ea ) ⇒ (∃R.C)I (x) ≤ ea and
similarly, if (∀R.(¬C))I (a) > c(ea ) ≥ c(RI (a, x)), i.e. (∃R.C)I (a) < c(ea ) ≤ RI (a, x), we
have (∀R.(¬C))I (x) > c(ea ) and thus (∃R.C)I (x) < ea . Hence, again we are able to show
the next result.
Corollary 4.2 If (∃R.C)I (a)  n and Trans(R) then, in an fKD -DL, (∃R.(∃R.C))I (a)  n
holds.
The above results will be used in properties 11 and 12 in Deﬁnition 5.1.

5. Reasoning with Transitive and Inverse Roles in fKD -DLs
In the current section we will show how to reason about transitive and inverse roles in the
context of fuzzy DLs, thus providing a reasoning algorithm for the fKD -SI language. This
algorithm can be used in order to provide eﬃcient implementations for applications that do
not need the expressive power of fKD -SHIN .
In section 3, we have shown that most inference services of fuzzy DLs, like entailment
and subsumption, can be reduced to the problem of ABox consistency checking w.r.t. an
RBox. As other tableaux algorithms, our tableaux algorithm for checking ABox consistency
tries to prove the satisﬁability of an assertion by constructing, for an fKD -SI ABox A, a
fuzzy tableau of A, i.e., an abstraction of a model of A. Given the notion of a fuzzy
tableau, it is quite straightforward to prove the algorithm is a decision procedure for ABox
287

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

consistency. The fuzzy tableau we present here can be seen as an extension of the tableau
presented by Horrocks et al. (2000) to handle with degrees. The ﬁrst such extension was
presented by Stoilos et al. (2005b), but here we will revise that deﬁnition.
Without loss of generality, we assume all concepts C occurring in A to be in negation
normal form (NNF) (Hollunder, Nutt, & Schmidt-Schaus, 1990); i.e., negations occur in
front of concept names only. A fKD -SI-concept can be transformed into an equivalent one
in NNF by pushing negations inwards using a combination of the De Morgan laws (which
are satisﬁed by the operations we deﬁned in section 2.3). Next, for a fuzzy concept D, we
will denote by sub(D) the set that contains D and it is closed under sub-concepts of D
(Horrocks & Sattler, 1999). The set of all sub-concepts of concepts that appear within an
ABox is denoted by sub(A).
In the following, we use the symbols  and  as a placeholder for the inequalities ≥, >
and ≤, < and the symbol  as a placeholder for all types of inequalities. Furthermore, we
use the symbols − , − and − to denote their reﬂections; e.g., the reﬂection of ≤ is ≥
and that of > is <.
Deﬁnition 5.1 If A is an fKD -SI ABox, R an fKD -SI RBox, RA is the set of roles
occurring in A and R together with their inverses and IA is the set of individuals in A, a
fuzzy tableau T for A with respect to R, is deﬁned to be a quadruple (S, L, E, V) such that: S
is a set of elements, L : S×sub(A) → [0, 1] maps each element and concept, that is a member
of sub(A), to the membership degree of that element to the concept, E : RA × S × S → [0, 1]
maps each role of RA and pair of elements to the membership degree of the pair to the
role, and V : IA → S maps individuals occurring in A to elements in S. For all s, t ∈ S,
C, D ∈ sub(A), n ∈ [0, 1] and R ∈ RA , T satisﬁes:
1. L(s, ⊥) = 0 and L(s, 
) = 1 for all s ∈ S,
2. If L(s, ¬A)n, then L(s, A)− 1 − n,
3. If L(s, C  D)  n, then L(s, C)  n and L(s, D)  n,
4. If L(s, C  D)  n, then L(s, C)  n and L(s, D)  n,
5. If L(s, C  D)  n, then L(s, C)  n or L(s, D)  n,
6. If L(s, C  D)  n, then L(s, C)  n or L(s, D)  n,
7. If L(s, ∀R.C)  n, then E(R, s, t) − 1 − n or L(t, C)  n,
8. If L(s, ∃R.C)  n, then E(R, s, t)  n or L(t, C)  n,
9. If L(s, ∃R.C)  n, then there exists t ∈ S such that E(R, s, t)  n and L(t, C)  n,
10. If L(s, ∀R.C)n, then there exists t ∈ S such that E(R, s, t)− 1−n and L(t, C)n,
11. If L(s, ∃R.C)  n and Trans(R), then E(R, s, t)  n or L(t, ∃R.C)  n,
12. If L(s, ∀R.C)  n, Trans(R), then E(R, s, t) − 1 − n or L(t, ∀R.C)  n,
13. E(R, s, t)n iﬀ E(Inv(R), t, s)n,
288

Reasoning with Very Expressive Fuzzy Description Logics

14. If (a : C)n ∈ A, then L(V(a), C)n,
15. If (a, b : R)n ∈ A, then E(R, V(a), V(b))n
There are some remarks regarding Deﬁnition 5.1. First, observe that we use the notation
E(R, s, t) instead of simply E(R, s, t) in order to distinguish between a role R and an
ordered pair of nodes s, t. Moreover, in the above deﬁnition we are based on the semantics
of fuzzy interpretations, presented in Table 2, in order to ﬁnd properties of the fuzzy models
according to what relation holds between a membership degree, a speciﬁc value and an
inequality type. Then, based on these properties we would develop tableaux expansion
rules, which try to construct such an abstracted model. For example, for property 3, due
to the semantics of C  D, we have that if (C  D)I (s) ≥ n, then C I (s) = n1 , DI (s) = n2 ,
with min(n1 , n2 ) = (C  D)I (s) ≥ n. Due to the properties of the min norm we can
conclude that both n1 ≥ n and n2 ≥ n, hold. Furthermore, for property 7, due to the
semantics of ∀R.C, if (∀R.C)I (s) ≥ n we have, max(1 − RI (s, t), C I (s)) ≥ n, hence either
1 − RI (s, t) ≥ n ⇒ RI (s, t) ≤ 1 − n or C I (t) ≥ n. Similarly, we have constructed properties
for all possible relations between a node, an fKD -SI-concept and a value of the unit interval.
Properties 9 and 10 are based on the fact that we assume the existence of only witnessed
models. Otherwise, no such assumption could be made. Hence, intuitively a fuzzy tableau
is an abstraction of the witnessed models of a fuzzy ABox. Finally, property 14 means that
if a fuzzy assertion of the form (a : C) > n exists in a fuzzy ABox, then the membership
degree of the node V(a) to the concept C in the fuzzy tableau, should be strictly greater
than n. Similarly, with the rest of inequalities as well as with property 15.
We now have to prove the lemma connecting ABox consistency and the existence of a
fuzzy tableau for A.
Lemma 5.2 An fKD -SI ABox A is consistent w.r.t. R, iﬀ there exists a fuzzy tableau for
A w.r.t. R.
Proof: For the if direction if T = (S, L, E, V) is a fuzzy tableau for an ABox A w.r.t. R,
we can construct a fuzzy interpretation I =(∆I , ·I ) that is a model of A.
An interpretation can be deﬁned as follows:
∆I

=

S

aI

I (s)

=
=

V(a), a ∈ IA
L(s, 
) for all s ∈ S

⊥I (s)
AI (s)

=
=

RI (s, t)

=

L(s, ⊥) for all s ∈ S
L(s, A) for all s ∈ S and concept names A
 +
RE (s, t) for all s, t ∈ S × S if Trans(R)
RE (s, t) for all s, t ∈ S × S otherwise

where RE (s, t) is a binary fuzzy relation deﬁned as RE (s, t) = E(R, s, t) for all s, t ∈ S×S,
and RE+ represents its sup-min transitive closure (Klir & Yuan, 1995).
To prove that I is a model of A, we show by induction on the structure of concepts
that L(s, C)n implies C I (s)n for any s ∈ S. First, Property 1 ensures that the top
289

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

and bottom concepts are interpreted correctly. Together with properties 14, 15, and the
interpretation of individuals and roles, this implies that I satisﬁes each assertion in A.
Without loss of generality, in the following, we will only show the cases with L(s, C) ≥ n.
The rest of the inequalities can be shown in a similar way.
1. If A is a concept name then by deﬁnition nL(s, A) = AI (s).
2. If L(s, ¬A) ≥ n, then due to property 2 L(s, A) ≤ 1 − n. By deﬁnition of I, AI (s) ≤
1 − n, hence (¬A)I (s) ≥ c(1 − n) = n.
3. If L(s, C  D) ≥ n, then L(s, C) ≥ n and L(s, D) ≥ n. By induction, C I (s) ≥ n,
D I (s) ≥ n, hence (C  D)I (s) = min(C I (s), DI (s)) ≥ n.
4. If L(s, C  D) ≥ n, then L(s, C) ≥ n or L(s, D) ≥ n. By induction either C I (s) ≥ n
or D I (s) ≥ n and (C  D)I (s) = max(C I (s), DI (s)) ≥ n.
5. If L(s, ∃R.C) ≥ n, then there exists t ∈ S such that, E(R, s, t) ≥ n and L(t, C) ≥
n. By deﬁnition RI (s, t) ≥ n and by induction C I (t) ≥ n. Hence, (∃R.C)I (s) =
supt∈∆I min(RI (s, t), C I (t)) ≥ n.
6. If L(s, ∀R.C) ≥ n and RI (s, t) = p, then either
(a) E(R, s, t) = p, or
(b) there exist several paths l ≥ 1 of the form, E(R, s, sl1 ) = pl1 , E(R, sl1 , sl2 ) =
pl2 , . . . , E(R, slm , t) = plm+1 . The membership degree p of the pair s, t to the
transitive closure of R, would be equal to the maximum degree (since we cannot
have inﬁnite number of diﬀerent paths) of all the minimum degrees for each path.
If that degree is such that it is not lower or equal to 1 − n (since ≥− =≤) then
there exists a path, k, where for all degrees:
E(R, ski , ski+1 ) = pki , 0 ≤ i ≤ m, sk0 ≡ s, skm+1 ≡ t,
it holds that pki > 1 − n, because all pki would be greater or equal than the
minimum degree of the path. Hence, due to Property 12 for all ski we have
L(ski , ∀R.C) ≥ n.
In case p ≤ 1 − n we have that max(1 − p, C I (t)) ≥ n. In case p ≤ 1 − n, then
L(t, C) ≥ n, so by induction C I (t) ≥ n and thus also max(1 − RI (s, t), C I (t)) ≥ n.
In both cases we have that (∀R.C)I (s) ≥ n.
For the converse, if I =(∆I , ·I ) is a (witnessed) model of A w.r.t. R, then a fuzzy
tableau T = (S, L, E, V) for A w.r.t. R can be deﬁned as:
S

=

∆I

E(R, s, t)
L(s, C)

=
=

RI (s, t)
C I (s)

V(a)

=

aI

290

Reasoning with Very Expressive Fuzzy Description Logics

1. Property 1 is satisﬁed since I is a fuzzy interpretation.
2. Let L(s, ¬C)  n. The deﬁnition of T implies that (¬C)I (s) = n  n ⇒ C I (s) =
1 − n − 1 − n, so L(s, C) − 1 − n and Property 2 is satisﬁed. Similarly with the
inequalities  ∈ {≤, <}.
3. Let L(s, C  D)  n. The deﬁnition of T implies that (C  D)I (s) = n  n ⇒
min(C I (s), DI (s)) = n  n. By deﬁnition, L(s, C)  n and L(s, D)  n and T satisﬁes
Property 3. Property 4 is proved in a similar way.
4. Let L(s, C  D)  n. The deﬁnition of T implies that (C  D)I (s) = n  n ⇒
max(C I (s), DI (s)) = n  n. By deﬁnition of T , either L(s, C)  n or L(s, D)  n,
and T satisﬁes Property 5. Property 6 is proved in a similar way.
5. Let L(s, ∀R.C)  n. The deﬁnition of T implies that (∀R.C)I (s) = n  n ⇒
inf y∈∆I max(1 − RI (s, y), C I (y)) = n  n. This means that for any t ∈ ∆I either
1 − RI (s, t) = n  n or C I (t) = n  n, and by deﬁnition either E(R, s, t) − 1 − n
or L(t, C) ≥ n. Thus, T satisﬁes Property 7. Property 8 is proved in a similar way.
6. Let L(s, ∃R.C)  n. The deﬁnition of T implies that (∃R.C)I (s) = n  n ⇒
supy∈∆I min(RI (s, y), C I (y)) = n  n. This means that there exists some t ∈ ∆I
with RI (s, t) = n n and C I (t) = n n. By deﬁnition t ∈ S and T satisﬁes Property
9. Property 10 is proved in a similar way.
7. Property 12 of deﬁnition 5.1 is satisﬁed as a result of the semantics of transitive roles
and value restrictions that have been investigated in section 4. Hence, if (∀R.C)I (s) ≥
n, Trans(R) then either RI (s, t) ≤ 1 − n, or (∀R.C)I (t) ≥ n holds, otherwise if
(∀R.C)I (s) > n, Trans(R) then either RI (s, t) < 1 − n or (∀R.C)I (t) > n holds.
By deﬁnition of T if L(s, ∀R.C)  n, Trans(R) then either E(R, s, t) − 1 − n or
L(t, ∀R.C)  n. For similar reasons Property 11, holds.
8. T satisﬁes Property 13 in Deﬁnition 5.1 as a direct consequence of the semantics of
inverse relations.
9. T satisﬁes Properties 14 and 15 in Deﬁnition 5.1 because I is a model of A.




5.1 An Algorithm for Constructing an fKD -SI Fuzzy Tableau
Now we present a tableaux algorithm that tries to construct, given an fKD -SI ABox A and
an fKD -SI RBox R, a fuzzy tableau for A w.r.t. R. We prove that this algorithm construct
a fuzzy tableau for A and R iﬀ there exists a fuzzy tableau for A and R, and thus decides
consistency of fKD -SI ABoxes w.r.t. RBoxes.
Like the tableaux algorithm presented by Horrocks et al. (2000), our algorithm works
on completion-forests rather than on completion-trees, since an ABox might contain several
individuals with arbitrary roles connecting them. Due to the presence of transitive roles,
the termination of the algorithm is ensured by the use of blocking, where an expansion is
291

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

terminated when two individuals on the same path are asserted to belong to the same concepts. As fKD -SI provides both inverse roles and transitive role axioms, our algorithm uses
dynamic blocking (Horrocks & Sattler, 1999); i.e., blocked nodes (and their sub-branches)
can be un-blocked and blocked again later. As it was noted by Horrocks and Sattler (1999)
this un-blocking and re-blocking technique is crucial in the presence of inverse roles since
information might be propagated up the completion-forest and aﬀect other branches. For
example consider the nodes x, y and z, the edges x, y and x, z and suppose that x
blocks y. In the presence of inverse roles it is possible that z adds information to node x,
although z is a successor of x. In that case the block on y must be broken. Finally, even in
cases where a node is blocked and un-blocking does not occur it is necessary to allow some
expansion to be performed. For example node y might contain inverse information that
if allowed to be propagated upwards can render the completion-forest unsatisﬁable. Thus,
dynamic blocking uses the notions of directly and indirectly blocked nodes.
Deﬁnition 5.3 (Completion-Forest) A completion-forest FA for an fKD -SI ABox A
is a collection of trees whose distinguished roots are arbitrarily connected by edges. Each
node x is labelled with a set L(x) = {C, , n}, where C ∈ sub(A),  ∈ {≥, >, ≤, <} and
n ∈ [0, 1]. Each edge x, y is labelled with a set L(x, y) = {R, , n}, where R ∈ RA are
(possibly inverse) roles occurring in A. Intuitively, each triple C, , n (R, , n), called
membership triple, represents the membership degree and the type of assertion of each node
(pair of nodes) to a concept C ∈ sub(A) (role R ∈ RA ).
If nodes x and y are connected by an edge x, y with R, , n ∈ L(x, y), then y is
called an R,n -successor of x and x is called an R,n -predecessor of y. If y is an R,n successor or an Inv(R),n -predecessor of x, then y is called an R,n -neighbour of x. Let
y be an R>,n -neighbour of x, the edge x, y conjugates with triples R, , m if n ≥ m.
Similarly, we can extend it to the cases of R≥,n -, R<,n - and R≤,n -neighbours.
A node x is an R-successor (resp. R-predecessor or R-neighbour) of y if it is an R,n successor (resp. R,n -predecessor or R,n -neighbour) of y for some role R. A node x is
a positive (resp. negative) successor (resp. predecessor or neighbour) of y if  ∈ {>, ≥}
(resp.  ∈ {<, ≤}). As usual, ancestor is the transitive closure of predecessor.
A node x is blocked iﬀ it is not a root node and it is either directly or indirectly blocked.
A node x is directly blocked iﬀ none of its ancestors are blocked, and it has an ancestor
y such that L(x) = L(y). In this case, we say y directly blocks x. A node x is indirectly
blocked iﬀ one of its predecessor is blocked.
A node x is said to contain a clash iﬀ there exist two conjugated triples, or one of the
following triples within L(x):
⊥, ≥, n, 
, ≤, n, for n > 0, n < 1 respectively
⊥, >, n, 
, <, n
C, <, 0, C, >, 1
Moreover, for an edge x, y, L(x, y) is said to contain a clash iﬀ there exist two conjugated
triples in L(x, y), or if L(x, y) ∪ {Inv(R), , n | R, , n ∈ L(y, x)}, and x, y are
root nodes, contains two conjugated triples.
The deﬁnition of a completion-forest is quite intuitive. Since a fuzzy ABox contains fuzzy
assertions of the form (a : C)n and (a, b : R)n, then the nodes and edges of the forest
292

Reasoning with Very Expressive Fuzzy Description Logics

Rule
(¬ )

if 1.
2.
then

Description
¬C, , n ∈ L(x)
and C, − , 1 − n ∈ L(x)
L(x) → L(x) ∪ {C, − , 1 − n}

( )

if 1.
2.
then

C1  C2 , , n ∈ L(x), x is not indirectly blocked, and
{C1 , , n, C2 , , n} ⊆ L(x)
L(x) → L(x) ∪ {C1 , , n, C2 , , n}

(	 )

if 1.
2.
then

C1 	 C2 , , n ∈ L(x), x is not indirectly blocked, and
{C1 , , n, C2 , , n} ⊆ L(x)
L(x) → L(x) ∪ {C1 , , n, C2 , , n}

(	 )

if 1.
2.
then

C1 	 C2 , , n ∈ L(x), x is not indirectly blocked, and
{C1 , , n, C2 , , n} ∩ L(x) = ∅
L(x) → L(x) ∪ {C} for some C ∈ {C1 , , n, C2 , , n}

( )

if 1.
2.
then

C1  C2 , , nL(x), x is not indirectly blocked, and
{C1 , , n, C2 , , n} ∩ L(x) = ∅
L(x) → L(x) ∪ {C} for some C ∈ {C1 , , n, C2 , , n}

(∃ )

if 1.
2.
then

∃R.C, , n ∈ L(x), x is not blocked,
x has no R,n -neighbour y and C, , n ∈ L(y)
create a new node y with L(x, y) = {R, , n}, L(y) = {C, , n}

(∀ )

if 1.
2.
then

∀R.C, , n ∈ L(x), x is not blocked,
x has no R − ,1−n -neighbour y and C, , n ∈ L(y)
create a new node y with L(x, y) = {R, − , 1 − n}, L(y) = {C, , n}

(∀ )

if 1.
2.
3.
then

∀R.C, , n ∈ L(x), x is not indirectly blocked, and
x has an R  ,n1 -neighbour y with C, , n ∈ L(y) and
x, y conjugates with R, − , 1 − n
L(y) → L(y) ∪ {C, , n}

(∃ )

if 1.
2.
3.
then

∃R.C, , n ∈ L(x), x is not indirectly blocked and
x has an R,n1 -neighbour y with C, , n ∈ L(y) and
x, y conjugates with R, , n
L(y) → L(y) ∪ {C, , n}

(∀+ )

if 1.
2.
3.
then

∀R.C, , n ∈ L(x) with Trans(R), x is not indirectly blocked, and
x has an R  ,n1 -neighbour y with ∀R.C, , n ∈ L(y) and
x, y conjugates with R, − , 1 − n
L(y) → L(y) ∪ {∀R.C, , n}

(∃+ )

if 1.
2.
3.
then

∃R.C, , n ∈ L(x) with Trans(R), x is not indirectly blocked and
x has an R,n1 -neighbour y with ∃R.C, , n ∈ L(y) and
x, y conjugates with R, , n
L(y) → L(y) ∪ {∃R.C, , n}

Table 4: fKD -SI completion rules
must contain the information about the concept, the type of inequality and the membership
degree for every individual, which in the forest is represented by a node.
Deﬁnition 5.4 (Tableaux Algorithm) For an fKD -SI ABox A, the algorithm initialises
a forest FA to contain (i) a root node xai , for each individual ai ∈ IA occurring in A, labelled with L(x) such that {Ci , , n} ⊆ L(xai ) for each assertion of the form (ai : Ci )n
293

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

in A, and (ii) an edge xai , xaj , for each assertion (ai , aj  : Ri )n in A, labelled with
L(xai , xaj ) such that {Ri , , n} ⊆ L(xai , xaj ). Moreover, the algorithm expands R by
adding an axiom Trans(Inv(R)) for each Trans(R) ∈ R. FA is then expanded by repeatedly
applying the completion rules from Table 4. The completion forest is complete when, for
some node x, L(x) contains a clash, or none of the completion rules in Table 4 are applicable. The algorithm stops when a clash occurs; it answers ‘A is consistent w.r.t. R’ iﬀ
the completion rules can be applied in such a way that they yield a complete and clash-free
conpletion forest, and ‘A is inconsistent w.r.t. R’ otherwise.
There are some remarks regarding Deﬁnition 5.4. The expansion rules are based on the
properties of the semantics presented in Deﬁnition 5.1. For example, consider the (∀)-rule.
Now, if ∀R.C, ≥, 0.7 ∈ L(x), and R, ≥, 0.6 ∈ L(x, y), this means that the last triple
violates property 7 of Deﬁnition 5.1. This property says that the membership degree of
the edge x, y to the role R should be lower or equal than the degree 1 − 0.7, otherwise
the membership degree of y to C should be greater or equal than 0.7. Interpreted to
membership triples this means that if a triple of the form R, ≥, n exists in L(x, y), then
n ≤ 1 − 0.7, or if the triple is of the form R, >, n, then n < 1 − 0.7. In order to discover if
these restrictions are violated the (∀)-rule compares the triples of the edge x, y with the
artiﬁcial triple R, ≤, 0.3 against conjugation. In the present case conjugation occurs, thus
we should add the triple C, ≥, 0.7 to the label of y. Similar arguments hold for the rest of
the properties. Please note that artiﬁcial triples are not added in the completion-forest but
are only used to perform checks on membership degrees. Secondly, in the above tableaux
algorithm, we see that we are dealing with ﬁnite number of membership degrees. In fact,
from Table 4, we can see that for an arbitrary fuzzy assertion of the form (x : D)n either
value n or its complement c(n) appear in the expansion of a node x where D, , n ∈ L(x).
The ﬁnite property of the membership degrees makes blocking possible in our algorithm.
This property is a consequence of the fuzzy operations used in our context, i.e. the Gödel tnorm and t-conorm, the Lukasiewicz complement and Kleene-Dienes fuzzy implication and
it usually does not hold for other combinations of fuzzy operations. Finding an appropriate
blocking condition when other norms are used in combination with Description Logics that
include transitive relations is an open research issue. Finally, it is worth noting that since
we assume all concepts to be in their negation normal form the (¬ )-rule only applies
to concept names. But, since we employ a rule for handling negated concepts this is not
absolutely necessary in fuzzy DLs. Hence, we are able to not produce the NNF form of
negated concepts and apply the (¬ )-rule directly on them. This might be the base for
optimization, since we might be able to identify clashes earlier, or for generalizations to
other norm operations, since in that case we might not be able to produce the NNF of
negated concepts. In either case, the proof of lemma 5.9 would require a slight modiﬁcation
in order to correctly interpret negated concepts.
Example 5.5 Let us see some examples of applications of expansion rules.
• (∀≥ ): Let ∀R.C, ≥, 0.7 ∈ L(x) and Inv(R), >, 0.3 ∈ L(y, x). According to the
deﬁnition of an R-neighbour, y is an R>,0.3 -neighbour, hence x, y conjugates with
R, ≤, 0.3, and additionally C, ≥, 0.3 ∈ L(y). Thus, we should add C, ≥, 0.3 in
L(y).
294

Reasoning with Very Expressive Fuzzy Description Logics

• (∃≥ ): Let ∃ Inv(R).C, ≥, 0.7 ∈ L(x). Then create a new node y in the forest and set,
Inv(R), ≥, 0.7 ∈ L(x, y), C, ≥, 0.7 ∈ L(y).
• (∃+ ): Let ∃ Inv(R).C, <, 0.5 ∈ L(x), Inv(R), ≥, 0.7 ∈ L(x, y) and Trans(R). y
is an Inv(R)≥,0.7 -neighbour of x, hence x, y conjugates with Inv(R), <, 0.5, and
additionally ∃ Inv(R).C, <, 0.5 ∈ L(y). Hence, ∃ Inv(R).C, <, 0.5 should be added
in L(y).
♦
Now we can revisit example 3.3 to see how the procedure presented in this section can
be used to determine the consistency of the ABox.
Example 5.6 Recall that our fuzzy ABox was A = {(o1 , o2  : isPartOf) ≥ 0.8, (o2 , o3  :
isPartOf) ≥ 0.9, (o2 : Body) ≥ 0.85 and (o1 : Arm) ≥ 0.75}, and that we wanted to test the
consistency of the fuzzy ABox A = A ∪ {(o3 : ∃Inv(isPartOf).Body  ∃Inv(isPartOf).Arm) <
0.75}, w.r.t. R = {Trans(isPartOf)}. According to Deﬁnition 5.4 the algorithm initializes
a completion-forest to contain the following triples (note that we have a node xoi for each
individual oi ):
(1)
(2)
(3)
(4)
(5)

isPartOf, ≥, 0.8 ∈ L(xo1 , xo2 )
isPartOf, ≥, 0.9 ∈ L(xo2 , xo3 )
Body, ≥, 0.85 ∈ L(xo2 )
Arm, ≥, 0.75 ∈ L(xo1 )
∃isPartOf − .Body  ∃isPartOf − .Arm, <, 0.75 ∈ L(xo3 )

Furthermore, the algorithm expands R by adding the axiom Trans(isPartOf − ). Please note
that for simplicity we have not expanded the concepts Arm and Body in the membership
triples. Subsequently, by applying expansion rules from Table 4 we have the following steps:
(6)

∃isPartOf − .Body, <, 0.75 ∈ L(xo3 ) | ∃isPartOf − .Arm, <, 0.75 ∈ L(xo3 )

(< )

Hence at this point we have two possible completion forests. For the ﬁrst one we have,
(61 )
(71 )
(81 )

∃isPartOf − .Body, <, 0.75 ∈ L(xo3 )
Body, <, 0.75 ∈ L(xo2 )
clash (71 ) and (3)

(∃< ) : (61 ), (2)

while for the second possible completion-forest we have.
(62 )
(72 )
(82 )
(92 )
(102 )

∃isPartOf − .Arm), <, 0.75 ∈ L(xo3 )
Arm, <, 0.75 ∈ L(xo2 )
∃isPartOf − .Arm), <, 0.75 ∈ L(xo2 )
Arm, <, 0.75 ∈ L(xo1 )
clash (92 ) and (4)

(∃< ) : (62 ), (2)
(∃+ ) : (62 ), (2)
(∃< ) : (82 ), (1)

Thus, since all possible expansions result to a clash, A is inconsistent the knowledge base
entails the fuzzy assertion.
295

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Example 5.7 Consider the fuzzy knowledge base Σ = T , A, R, with TBox T = {C ≡
∀R− .(∀P − .¬A)}, ABox A = {(a : A) ≥ 0.8, (a, b : P ) ≥ 0.8, (b : C) ≥ 0.8, (b : ∃R.C) ≥
0.8, (b : ∀R.(∃R.C)) ≥ 0.8} and RBox R = {Trans(R)}. First the algorithm expands R by
adding axiom Trans(R− ). Then, in order to check the consistency of A w.r.t. T and R the
algorithm initializes the following completion-forest:
(1)
(2)
(3)
(4)
(5)

A, ≥, 0.8 ∈ L(xa )
C, ≥, 0.8 ∈ L(xb )
∃R.C, ≥, 0.8 ∈ L(xb )
∀R.(∃R.C), ≥, 0.8 ∈ L(xb )
P, ≥, 0.8 ∈ L(xa , xb ).

Then, we get the following application of expansion rules,
(6)
(7)
(8)

C, ≥, 0.8 ∈ L(xo1 ), R, ≥, 0.8 ∈ L(xb , xo1 )
∃R.C, ≥, 0.8 ∈ L(xo1 )
∀R.(∃R.C), ≥, 0.8 ∈ L(xo1 )

(∃≥ ) : (3)
(∀≥ ) : (4)
(∀+ ) : (4)

As we can see L(xb ) = L(xo1 ), hence xo1 is blocked by xb . On the other hand it is not
indirectly blocked. Hence, since ∀R− .(∀P − .¬A), ≥, 0.8 ∈ L(xo1 ) (due to the deﬁnition of
C in the TBox) we have the following application of expansion rules,
(9)
(10)
(11)
(12)

∀P − .¬A, ≥, 0.8 ∈ L(xb )
¬A, ≥, 0.8 ∈ L(xa )
A, ≤, 0.2 ∈ L(xa )
clash (11) and (1)

(∀≥ ) : (6)
(∀≥ ) : (9)
(¬≥ ) : (10)

Please note that adding ∀P − .¬A, ≥, 0.8 to L(xb ) causes the blocking of node xo1 to be
broken since it no longer holds that L(xb ) = L(xo1 ). Hence, the notions of indirectly
blocked nodes and dynamic blocking are crucial in the presence of inverse roles in order to
correctly identify consistent and inconsistent ABoxes. Also note that if the algorithm had
chosen to expand xo1 (since this node is no more blocked) rather than xb , then it would
have created another node, say xo2 , for which L(xo1 ) = L(xo2 ). Then again C, ≥, 0.8
would be added to xo1 , since xo2 would not be indirectly blocked, the block on xo2 would
be broken, but then it would hold that L(xb ) = L(xo1 ). Hence xo1 would be permanently
blocked while xo2 indirectly blocked. Then the algorithm would have no other choice but to
identify the clash in node xa , as it is showed in steps (9) to (12).
5.2 Decidability of fKD -SI
The soundness and completeness of the algorithm will be demonstrated by proving that for
an SI ABox A, it always terminates and that it returns consistent iﬀ A is consistent.
Lemma 5.8 (Termination) For each fKD -SI ABox A and RBox R, the tableaux algorithm terminates, when started for A and R.
Proof: Let m = |sub(A)|, k = |RA | and l be the number of diﬀerent membership degrees appearing in A. Obviously m and l are linear in the length of A. Termination is a
consequence of the following properties of the expansion rules:
296

Reasoning with Very Expressive Fuzzy Description Logics

1. The expansion rules never remove nodes from the forest or concepts from node labels.
2. Only the (∃)- or the (∀)-rule generate new nodes, and each generation is triggered
when ∃R.C, , n or ∀R.C, , n is in a node label where ∃R.C or ∀R.C is in sub(A).
As no nodes can be removed, these rules will not be applied on the same label repeatedly. Since sub(A) contains at most m ∃R.C or m ∀R.C, the out-degree of the forest
is bounded by 2ml.
3. Nodes are labelled with triples of the form C, , n, so there are at most 28ml diﬀerent
possible labellings for a pair of nodes. Thus, if a path p is of length at least 28ml (note
that concepts that cause non-termination interact either with a value n or with it’s
negation, and not with both), then there exist two nodes x, y on p which contain the
same label. Since a path on which nodes are blocked cannot become longer, paths are
of length at most 28ml .


As the previous lemma suggests the tableaux algorithm runs in exponential space. This is
due to a well-known problem inherited from the crisp SI language (Tobies, 2001). Consider
for example the following concepts taken from Tobies (2001),
C ≡ ∃R.D  ∀R.(∃R.D)
D ≡ (A1  B1 )  (A2  B2 )  . . .  (An  Bn )
where R is a transitive role. Now consider that we want to check the consistency of the
fuzzy ABox A = {(a : C) ≥ n}. Concept C causes the generation of R≥,n -successors bi for
which it also holds that (bi : D) ≥ n. Now due to the ≥ -rule, which might choose to add
either (bi : Ai ) ≥ n or (bi : Bi ) ≥ n, there are 2n possible ways of expanding D. Hence, the
algorithm might create a path of exponential depth before blocking applies. Tobies (2001)
presents an optimized blocking technique that leads to a Pspace algorithm for SI. This
technique involves a reﬁned blocking strategy as well as the modiﬁcation of the tableaux
expansion rules. Investigating the applicability of this technique to fKD -SI is an interesting
open problem.
Lemma 5.9 (Soundness) If the expansion rules can be applied to an fKD -SI ABox A
and an RBox R such that they yield a complete and clash-free completion-forest, then A
has a fuzzy tableau w.r.t. R.
Proof: Let FA be a complete and clash-free completion-forest constructed by the tableaux
algorithm for A. The construction of a fuzzy tableau T = (S, L, E, V) is based on the
construction of a fuzzy model, presented by Straccia (2001):
For a set of triples of the form A, ≥, ni , i a positive integer, that might exist within
a set of triples L(x), the maximum value of ni ’s is chosen as a membership degree of x to
the fuzzy set AI , i.e. the degree L(x, A) in our case. If the maximum value participates in
a triple of the form A, >, n a small factor  is added to the maximum. The existence of
such a value is ensured by the clash-freeness of FA . Please also note that without loss of
generality we can force all factors  to be equal. Furthermore, when no triple of the form
C, , ni  ∈ L(x) exists, while only triples C, , ni  ∈ L(x) do, the membership degree is
297

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

set to 0. In cases where a value or existential restriction exists as well as a non conjugated
relation, special care to the choice of  has to be made in order not to choose a high value
that causes a conjugation in the interpretation. At last, in order to interpret concepts of
the form ¬A, where A is a concept name, we ﬁrst compute the maximum degree of a node
to the concept A and then use it to compute ¬A. The function that returns the maximum
degree is denoted by glb (Straccia, 2001). Please note that the labellings L(s, C) refer to
nodes of the fuzzy tableau, while those of L(x) to nodes of the completion-forest. A fuzzy
tableau can be deﬁned as follows:
S
L(x, ⊥)

=
=

{x | x is a node in FA , and x is not blocked},
0, for all x ∈ S,

L(x, 
)
L(x, C)

=
=

1, for all x ∈ S,
glb[C, , ni ], for C, , ni  ∈ L(x) x not blocked,

L(x, ¬A)
E(R, x, y)

=
=

V(ai )

=

1 − L(x, A), for all x in FA not blocked, with ¬A, , n ∈ L(x),
{glb[R∗ , , ni ] | 1. y is an R,ni -neighbour of x or
2.R, , ni  ∈ L(x, z) and y blocks z or
3.Inv(R), , ni  ∈ L(y, z) and x blocks z},
xai , where xai is a root node,

where R∗ represents either R or Inv(R). It can be shown that T is a fuzzy tableau for A
w.r.t. R:
1. Property 1 of Deﬁnition 5.1 is satisﬁed due to the construction of T and because FA
is clash-free.
2. Property 2 of Deﬁnition 5.1 is satisﬁed because the ¬-rule does not apply and we force
all factors  to be equal. Let L(x, ¬A) = n1 ≥ n. The deﬁnition of T implies that
1 − n ≥ 1 − n1 = L(x, A).
3. Properties 3-6 of Deﬁnition 5.1 are satisﬁed because none of  nor  apply to any
x ∈ S. For example, let L(x, C  D) = n1 ≥ n. The deﬁnition of T implies that,
either C  D, ≥, n1  ∈ L(x) or C  D, >, n  ∈ L(x), with n1 = n + . Completeness
of FA implies that either C, ≥, n1  ∈ L(x) and D, ≥, n1  ∈ L(x) or C, >, n  ∈ L(x)
and D, >, n  ∈ L(x). Hence, L(s, C) = glb[C, , ni ] ≥ L(s, C  D) ≥ n, L(s, D) =
glb = [C, , ni ] ≥ L(s, C  D) ≥ n. The rest of properties follow in a similar way.
4. Property 7 in Deﬁnition 5.1 is satisﬁed. Let x ∈ S with L(x, ∀R.C) = n1 ≥ n and
E(R, x, y) − 1 − n. The deﬁnition of T implies that either ∀R.C, ≥, n1  ∈ L(x)
or ∀R.C, >, n  ∈ L(x) with n1 = n + . Moreover, since the glb function does not
create an unnecessary conjugation we have that either:
(a) y is an R,r -neighbour of x
(b) R, , r ∈ L(x, z), y blocks z thus L(y) = L(z), or
(c) Inv(R), , r ∈ L(y, z), x blocks z, thus L(x) = L(z).
298

Reasoning with Very Expressive Fuzzy Description Logics

and R, , r or Inv(R), , r causes conjugation. Hence, in all 3 cases, the ∀-rule
ensures that either C, ≥, n1  ∈ L(y) or C, >, n  ∈ L(y). Thus, either L(y, C) ≥ n1 ≥
n, or L(y, C) ≥ n +  = n1 ≥ n. The case with L(x, ∀R.C) > n and L(x, ∃R.C)  n,
where the latter regards property 8, are shown in a similar way.
5. Property 9 in Deﬁnition 5.1 is satisﬁed. Let x ∈ S with L(x, ∃R.C) = n1 ≥ n. The
deﬁnition of T implies that either ∃R.C, ≥, n1  ∈ L(x) or ∃R.C, >, n  ∈ L(x), with
n1 = n + . Then the ∃-rule ensures that there is either:
(a) a predecessor y such that Inv(R), ≥, n1  ∈ L(y, x) and C, ≥, n1  ∈ L(y) or
dually with > and n . Because y is a predecessor of x it cannot be blocked, so
y ∈ S, E(R, x, y) ≥ n1 ≥ n and L(y, C) ≥ n1 or E(R, x, y) ≥ n +  = n1 ≥ n
and L(y, C) ≥ n +  = n1 ≥ n.
(b) a successor y such that R, ≥, n ∈ L(x, y), C, ≥, n ∈ L(y) or dually with >
and n . If y is not blocked, then y ∈ S and E(R, x, y) ≥ n1 , L(y, C) ≥ n1 or
E(R, x, y) ≥ n + , L(y, C) ≥ n +  = n1 . Otherwise, y is blocked by some
z. Hence, z ∈ S and R, ≥, n1  ∈ L(x, z), C, ≥, n1  ∈ L(z) or R, >, n  ∈
L(x, z), C, >, n ∈ L(z). In both cases L(z, C) ≥ n and E(R, x, z) ≥ n.
Similar proof applies for L(x, ∃R.C) > n and also for Property 10 with L(x, ∀R.C)n.
6. Property 12 in Deﬁnition 5.1 is satisﬁed. Let x ∈ S with L(x, ∀R.C) = n1 ≥ n and
E(R, x, y) ≥− 1 − n. The deﬁnition of T implies that either ∀R.C, ≥, n1  ∈ L(x)
or ∀R.C, >, n  ∈ L(x) with n1 = n + . Moreover, since the glb function does not
create an unnecessary conjugation we have that either:
(a) y is an R,r -neighbour of x
(b) R, , r ∈ L(x, z), y blocks z thus L(y) = L(z), or
(c) Inv(R), , r ∈ L(y, z), x blocks z, thus L(x) = L(z).
and R, , r or Inv(R), , r causes conjugation. Hence, in all 3 cases, the ∀+ rule ensures that either ∀R.C, ≥, n1  ∈ L(y) or ∀R.C, >, n  ∈ L(y). Thus, either
L(y, ∀R.C) ≥ n1 ≥ n, or L(y, ∀R.C) ≥ n + = n1 ≥ n. The case with L(x, ∀R.C) > n
and L(∃R.C, , n, where the latter regards property 11, are shown in a similar way.
7. Property 13 in Deﬁnition 5.1 is satisﬁed because, if E(R, x, y)n, then either:
(a) y is an R,n1 -neighbour of x, so x is an Inv(R),n1 -neighbour of y.
(b) R, , n1  ∈ L(x, z), and y blocks z, so Inv(Inv(R)), , n1  ∈ L(x, z)
(c) Inv(R), , n1  ∈ L(y, z) and x blocks z.
In all 3 cases, E(Inv(R), y, x)n.
8. Properties 14 and 15 are satisﬁed cause of the initialization of the completion-forest
and the fact that the algorithm never blocks root nodes.


299

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Lemma 5.10 (Completeness) Let A be an fKD -SI ABox and R an RBox. If A has
a fuzzy tableau w.r.t. R, then the expansion rules can be applied in such a way that the
tableaux algorithm yields a complete and clash-free completion-forest for A and R.
Proof: Our proof of completeness is based on the proof for crisp DLs presented by Horrocks
and Sattler (1999) and Horrocks et al. (2000).
Let T = (S, L, E, V) be a fuzzy tableau for A. Using T , we trigger the application of
the expansion rules such that they yield a completion-forest FA that is both complete and
clash-free.
Since we know that A has a fuzzy tableau (T ) we can steer the application of rules
such that they yield a complete and clash-free completion-forest. Horrocks and Sattler
(1999) and Horrocks et al. (2000) deﬁne a mapping π which maps nodes of FA to elements
of S, and guide the application of the non-deterministic rules  and . Our method
diﬀers from the one used in crisp DLs (Horrocks & Sattler, 1999) in the following way.
Using the membership degree of a node to a concept, found in the fuzzy tableau, we create
artiﬁcial triples which are tested against conjugation with the candidate triples that the
non-deterministic rules can insert in the completion-forest. The triples that don’t cause a
conjugation can be added. The modiﬁed rules, which are used to guide such an expansion,
are presented in Table 5.
( )

if 1.
2.
then

( )

if 1.
2.
then

C1  C2 , , n ∈ L(x), x is not indirectly blocked, and
{C1 , , n, C2 , , n} ∩ L(x) = ∅
L(x) → L(x) ∪ {C} for some C ∈ {C1 , , n, C2 , , n}
not conjugated with C1 , ≤, L(π(x), C1 ) or C2 , ≤, L(π(x), C2 )
C1  C2 , , n ∈ L(x), x is not indirectly blocked, and
{C1 , , n, C2 , , n} ∩ L(x) = ∅
L(x) → L(x) ∪ {C} for some C ∈ {C1 , , n, C2 , , n}
not conjugated with C1 , ≥, L(π(x), C1 ) or C2 , ≥, L(π(x), C2 )

Table 5: The - and -rules
π ensures that a new fuzzy assertion about the membership degree of a node to a concept,
created by a non-deterministic rule, is not more restrictive than the one already known in
the fuzzy tableau, thus avoiding possible conjugations. This together with the termination
property ensure the completeness of the algorithm.


Theorem 5.11 The tableaux algorithm is a decision procedure for the consistency of fKD -SI
ABoxes and the satisﬁability and subsumption of fKD -SI concepts with respect to simple
terminologies.
Theorem 5.11 is an immediate consequence of lemmas 5.1, 5.9 and 5.10. Moreover, as we
discussed in section 3, subsumption can be reduced to consistency checking for ABoxes.

6. Adding Role Hierarchies and Number Restrictions
In the current section we will provide the necessary extensions of the reasoning algorithm
presented in the previous section, in order to provide reasoning support for the fuzzy DL
300

Reasoning with Very Expressive Fuzzy Description Logics

language fKD -SHIN . To achieve our goal we will extend the results of section 4 by also
considering role hierarchies, while we will also provide an investigation on the number
restrictions constructor.
In classical DLs, the results of transitive roles and value restrictions obtained by Sattler
(1996), were extended by Horrocks and Sattler (1999) to also consider role hierarchies.
* R, then
More precisely, they show that if x ∈ (∀R.C)I , x, y ∈ P I , Trans(P ) and P 
y ∈ (∀P.C)I . In fuzzy DLs that also include role hierarchies we can easily extend the results
obtained in section 4. Let (∀R.C)I (x) ≥ ca , P I (x, y) = p, Trans(P ), and ca , p ∈ [0, 1], and
consider also that P * R. Since P is transitive, then ∀x, y ∈ ∆I and for some arbitrary
z ∈ ∆I it holds that, P I (x, y) ≥ min(P I (x, z), P I (z, y)). Due to the semantics of role
inclusion axioms we have that RI (x, y) ≥ min(P I (x, z), P I (z, y)). Then, if we work in a
similar way as in section 4 we will get that, max(c(P I (a, b)), (∀P.C)I (b)) ≥ va , which means
that either c(P I (a, b)) ≥ va or (∀P.C)I (b) ≥ va . A similar result can be obtained for the
case where (∀R.C)I (a) > n. Hence, we get the following result:
* R, then in a fKD -DL it holds
Corollary 6.1 If (∀R.C)I (a)  n, and Trans(P ) with P 
I
that, (∀P.(∀P.C)) (a)  n.

Finally, for the case of negative assertions and existential restrictions the following is
easily obtained.
* R, then in a fKD -DL it holds
Corollary 6.2 If (∃R.C)I (a)  n, and Trans(P ) with P 
that, (∃P.(∃P.C))I (a)  n.

Now we will investigate fuzzy number restrictions. Although, from in Table 2 it seems
that the semantics of number restrictions are quite complicated, we will see that intuitively,
they are quite similar to their crisp counterparts, as long as we also consider membership
degrees.
Consider for example the at-least restriction (≥ pR)I (a) ≥ n, where a ∈ ∆I . Then
according to Table 2 we have,
p

sup

min{RI (a, bi )} ≥ n.

b1 ,...,bp ∈∆I i=1

This means that there must be at least p pairs a, bi , for which RI (a, bi ) ≥ n, holds. These
semantics are quite intuitive and similar with those of crisp number restrictions. There one
would require at least p pairs for which RI (a, bi ) ≥ 1, which simply means more than p
pairs. Similarly, we can work for (≥ pR)I (a) > n.
Consider now an at-most restriction of the form (≤ pR)I (a) ≥ n. Based on the semantics
we have the inequation,
inf
b1 ,...,bp+1

p+1

∈∆I

max{1 − RI (a, bi )} ≥ n.
i=1

This means that for all p + 1 pairs a, bi , that can be formed, there is at least one pair
for which c(RI (a, bk )) ≥ n, holds. We can also view this equation in a diﬀerent way which
resembles that of crisp number restrictions. From that perspective we can say that, there
301

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

are at most p pairs a, bi  for which c(RI (a, bi )) < n, holds. Similarly, an at-most restriction
of the form (≤ pR)I (a) > n implies that there are at-most p pairs a, bi , for which it holds
that c(RI (a, bi )) ≤ n. Hence reasoning w.r.t. number restrictions can be reduced to counting
how many role assertions (a, bi  : R) ≥ ni satisfy the above inequalities. If we ﬁnd that
more than p assertions satisfy these inequalities, then we have to non-deterministically merge
some of the individual bi , as is the case in the crisp SHIN algorithm (Horrocks et al., 2000).
Now, lets consider the extreme boundaries of 0 and 1, and apply our equation to the classical
at-most restriction, a ∈ (≤ pR)I . The fuzzy equivalent of this assertions is (≤ pR)I (a) ≥ 1,
which implies that there are at most p bi ∈ ∆I such that, c(RI (a, bi )) < 1 ⇒ RI (a, bi ) > 0,
holds. Since we are only considering 0 and 1 the last inequality implies, RI (a, bi ) = 1, i.e.
at-most p successors of a in RI .
Dually, we can also provide such intuitive meaning for the cases which involve negative
inequalities, like for example the cases of (≥ pR)I (a) ≤ n1 or (≤ pR)I (a) ≤ n2 . Applying
negation to the ﬁrst equation we obtain, (¬(≥ pR))I (a) ≥ c(n1 ), where c is a fuzzy complement. Since the min and max operations satisfy the De Morgan laws, this assertion can
be translated to (≤ (p − 1)R)I (a) ≥ 1 − n1 , with p ≥ 1, which is the negation normal form
of the former assertion. Similarly, the equation (≤ pR)I (a) ≤ n2 can be transformed to the
equivalent, (≥ (p + 1)R)I (a) ≥ 1 − n2 .
Using the above results we can proceed in the deﬁnition of an fKD -SHIN fuzzy tableau.
Similarly to 5.1 we consider all concepts to be in NNF. This can be achieved by using the
concept equivalences for number restrictions of section 3. The deﬁnition of a fuzzy tableau
for fKD -SHIN ﬁrst appeared by Stoilos, Stamou, Tzouvaras, Pan, and Horrocks (2005c),
but here we have revised that deﬁnition to better represent the properties of fuzzy models.
Before deﬁning a fuzzy tableau for fKD -SHIN we extend the deﬁnition of sub-concepts of
a concept D and an ABox A.
Deﬁnition 6.3 For a fuzzy concept D and a role hierarchy R we deﬁne sub(D, R) to be
the smallest set of fKD -SHIN -concepts that satisﬁes the following:
• D ∈ sub(D, R),
• sub(D, R) is closed under sub-concepts of D, and
* S, then ∀R.C ∈ sub(D, R)
• if ∀S.C ∈ sub(D, R) and R 
* S, then ∃R.C ∈ sub(D, R)
• if ∃S.C ∈ sub(D, R) and R 

Finally, we deﬁne sub(A, R) =

∪

(a:D)n∈A

sub(D, R).

When R is clear from the context we will simply write sub(A).
Deﬁnition 6.4 If A is an fKD -SHIN ABox, R an fKD -SHIN RBox, RA is the set of
roles occurring in A and R together with their inverses, IA is the set of individuals in A,
then a fuzzy tableau T for A w.r.t. R is deﬁned as in Deﬁnition 5.1 with the additional
properties:
* R, then E(P, s, t)  n or L(t, ∃P.C)  n,
11’. If L(s, ∃R.C)  n, and Trans(P ) with P 

302

Reasoning with Very Expressive Fuzzy Description Logics

12’. If L(s, ∀R.C)n, and Trans(P ) with P 
* R, then E(P, s, t)− 1−n or L(t, ∀P.C)n,
* S, then E(S, s, t)  n,
16. If E(R, s, t)  n and R 

17. If L(s, ≥ pR)  n, then RT (s, , n) ≥ p,
18. If L(s, ≤ pR)  n, then RT (s, − , 1 − n) ≥ p + 1,
T (s, , n) ≤ p − 1,
19. If L(s, ≥ pR)  n, then R¬
T (s, − , 1 − n) ≤ p,
20. If L(s, ≤ pR)  n, then R¬
.
21. If a =
 b ∈ A, then V(a) =
 V(b)

where RT (s, , n) = {t ∈ S | E(R, s, t)n} returns the set of elements t ∈ S that participate in R with some element s with a degree, greater or equal, greater, lower or equal or
T (s, , n) = {t ∈ S | E(R, s, t)  n} returns those elements that don’t
lower than n, and R¬
satisfy the given inequality.
As in Deﬁnition 5.1, we are based on the semantics of the language and the observations made in the beginning of this section about the properties of the value and existential restrictions, when transitive roles and role hierarchies are involved, and the semantic
meaning of at-most and at-least number restrictions. Thus, property 18 should be read
as, if L(s, ≤ pR) ≥ n then there are at-most p t ∈ S such that E(R, s, t)  1 − n, i.e.
E(R, s, t) > 1 − n, and if L(s, ≤ pR) > n, then there are at-most p t ∈ S such that
E(R, s, t) < 1 − n.
Lemma 6.5 An fKD -SHIN ABox A is consistent w.r.t. R, iﬀ there exists a fuzzy tableau
for A w.r.t. R.
Proof: The proof of the lemma is similar to that of lemma 5.2 with some important
technical details. For the “if” direction, if T = (S, L, E, V) is a fuzzy tableau for A w.r.t. R,
then a model I = (∆, ·I ) of A and R is constructed as ∆I = S, aI = V(a), where a ∈ IA ,

I (s) = L(s, 
), ⊥I (s) = L(s, ⊥) for all s ∈ S, and AI (s) = L(s, A), for all s ∈ S and
concept names A, while for roles we have:
 +
RE (s, t),
if Trans(R)
I
R (s, t) =
I
max (RE (s, t), P (s, t)) otherwise
P 
* R,P =R

Observe that the interpretation of non-transitive roles is recursive in order to correctly
interpret those non-transitive roles that have a transitive sub-role. From the deﬁnition of
RI and property 12’, if RI (s, t) = n ∈ (0, 1], then either E(R, s, t) = n, or E(R, s, t) = 0
and there exist several paths l ≥ 1 of the form,
E(P, s, sl1 ) = pl1 , E(P, sl1 , sl2 ) = pl2 , . . . , E(P, slm , t) = plm+1
with Trans(P ), P 
* R and E(R, s, t) = max(0, supl {min(pl1 , . . . , plm+1 )}).
Property 16 of I ensures that ∀s, t ∈ ∆I , P I (s, t) ≤ RI (s, t) for all P * R. Again, by
induction on the structure of concepts we can we show that L(s, C)n implies C I (s)n
for any s ∈ S. Here, we restrict our attention on to the cases that are diﬀerent than lemma
5.2. Similarly to lemma 5.2 we also restrict our attention to the inequalities ≥.
303

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

6’. If L(s, ∀R.C) ≥ n and RI (s, t) = p, then either
(a) E(R, s, t) = p, or
(b) E(R, s, t) = p. Then, there exist several paths l ≥ 1 of the form, E(P, s, sl1 ) =
* R.
pl1 , E(P, sl1 , sl2 ) = pl2 , . . . , E(P, slm , t) = plm+1 , with Trans(P ) and P 
The membership degree p of the pair s, t to (P + )I , would be equal to the maximum degree (since we cannot have inﬁnite number of paths) of all the minimum
degrees for each path. If that degree is such that it is not lower or equal than
1 − n then there exists a path k where all degrees
E(P, ski , ski+1 ) = pki , 0 ≤ i ≤ km , sk0 ≡ s, skm+1 ≡ t
are not lower or equal than 1 − n, because all pki ’s would be greater or equal
than the minimum degree of the path. Hence, due to property 11, we would have
that L(ski , ∀P.C) ≥ n, for all 1 ≤ i ≤ km .
In case p ≤ 1 − n we have then max(1 − p, C I (t)) ≥ n. In case p  1 − n then
L(t, C) ≥ n, so C I (t) ≥ n and thus also max(1 − p, C I (t)) ≥ n. In both cases
(∀R.C)I (s) ≥ n.
7. If L(s, ≥ pR) ≥ n then we have, E(R, s, ti ) ≥ n, 1 ≤ i ≤ p. By deﬁnition RI (s, ti ) ≥
n, and thus
p

n ≤ sup {. . . , min{RI (s, ti )}, . . .} = (≥ pR)I (s).
i=1

ti ∈∆I

8. If ≤ pR, ≥, n there are at most p pairs s, ti  for which, E(R, s, ti ) ≤ 1 − n,
1 ≤ i ≤ p. Thus in all p + 1-tuples that can be formed there would be at least one
pair s, tp+1  for which E(R, s, tp+1 ) ≤ 1 − n (even if E(R, s, tp+1 ) = 0 ≤ 1 − n).
Hence, RI (s, tp+1 ) ≤ 1 − n ⇒ c(RI (s, tp+1 )) ≥ n. Finally, we have that,
p

n ≤ inf {. . . , max(max{c(RI (s, ti ))}, c(RI (s, tp+1 ))), . . .} = (≤ pR)I (s).
ti ∈∆I

i=1

For the converse, if I =(∆I , ·I ) is a model for A w.r.t. R, then a fuzzy tableau T =
(S, L, E, V) for A and R is deﬁned in exactly the same was an in lemma 5.2. Then,
1. Properties 1-10 and 13 of Deﬁnition 5.1 and 16-20 in Deﬁnition 6.4 are satisﬁed as a
direct consequence of the semantics of fKD -SHIN concepts.
2. Property 12’ of Deﬁnition 6.4 is satisﬁed as a consequence of the semantics of transitive roles, role hierarchies and value restrictions that have been investigated in the
* R and Trans(P ) then either
beginning of the section. Hence, if (∀R.C)I (s) ≥ n, P 
P I (s, t) ≤ 1 − n, or (∀P.C)I (t) ≥ n holds, otherwise if (∀R.C)I (s) > n, P * R and
Trans(P ) then either P I (s, t) < 1 − n or (∀P.C)I (t) > n holds. By deﬁnition of T if
* R and Trans(P ) then either E(P, s, t) − 1 − n or L(t, ∀P.C)  n.
L(s, ∀R.C)  n, P 
Similarly, for property 11’ of Deﬁnition 6.4.
3. T satisﬁes Properties 14-15 of Deﬁnition 5.1 and Property 21 in Deﬁnition 6.4 because
I is a model of A.


304

Reasoning with Very Expressive Fuzzy Description Logics

6.1 Constructing an fKD -SHIN Fuzzy Tableau
In this section we will show how the algorithm of fKD -SI, presented in section 5.1, can
be extended to deal with fKD -SHIN ABoxes. There are a number of modiﬁcations that
need to be made, like the deﬁnition of R-neighbours, the (∀+ )- and (∃+ )-rules, the blocking
strategy, the clash deﬁnition and the addition of rules for number restrictions.
The most important modiﬁcation from the algorithm of fKD -SI is the blocking strategy.
As it was noted by Horrocks and Sattler (1999) a DL language that provides inverse roles,
transitive role axioms, and number restrictions lacks the ﬁnite-model property; i.e. there are
fKD -SHIN -concepts that are satisﬁable only in inﬁnite interpretations. This means that
the usual blocking techniques which create a cycle from the predecessor of a blocked node
to the blocking one, might fail to construct a correct tableau and due to lemma 6.5 a correct
model. It is crucial to remark here the diﬀerence between an inﬁnite and a witnessed model,
as presented in remark 3.2. Although there are fKD -SHIN -concept that are satisﬁable in
inﬁnite interpretations, these interpretations can still be witnessed w.r.t. the membership
degrees. The inﬁnite or ﬁnite property of interpretations comes from the constructs of the
language, while the witnessed or non-witnessed property comes from the continuity of the
fuzzy operators (Hajek, 2005).
Consider for example a node x which contains some triple of the form ≤ 1R, ≥, 1.
If a successor of x, say y, is blocked by some ancestor of x, say z, the dynamic blocking
techniques would create a cycle leading from x back to z. But this extra edge x, z might
violate the number restriction on x. To overcome this problem the construction of the tableau from the completion-forest is performed by repeatedly copying the sub-tree underneath
the node that causes blocking, z in our case. Thus, we are able to obtain an inﬁnite out
of the constructed ﬁnite forest. Furthermore, in order for copied nodes to be satisﬁable in
their new locations an extra condition, compared to dynamic blocking has to be employed.
The new blocking technique is called pair-wise blocking (Horrocks & Sattler, 1999); i.e.,
blocking occurs when two nodes belong to the same set of concepts, their predecessors also
belong to the same set of concepts and the edges that connect them are also equal. That
way unravelling is guaranteed.
Deﬁnition 6.6 (fKD -SHIN Completion Forest) First we extend the deﬁnition of Rsuccessors, predecessors and neighbours. If nodes x and y are connected by an edge x, y
* R, then y is called an R,n -successor of x and x is called
with P, , n ∈ L(x, y), and P 
an R,n -predecessor of y. If y is an R,n -successor or an Inv(R),n -predecessor of x, then
y is called an R,n -neighbour of x.
For a role R, a node x in FA , an inequality  and a membership degree n ∈ [0, 1]
FA
(x, , n) = {y | y is an R ,n -neighbour of x, and x, y conjugates with
we deﬁne: RC
R, , n}. Intuitively, this set contains all R-neighbours of x that conjugate with a given
triple.
A node x is blocked iﬀ it is not a root node and it is either directly or indirectly blocked.
A node x is directly blocked iﬀ none of its ancestors is blocked, and it has ancestors x , y
and y  such that:
1. y is not a root node,
2. x is a successor of x and y a successor of y  ,
305

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

3. L(x) = L(y) and L(x ) = L(y  ) and,
4. L(x , x) = L(y  , y).
In this case we say that y blocks x. A node y is indirectly blocked iﬀ one of its ancestors is
blocked, or it is a successor of a node x and L(x, y) = ∅.
For a node x, L(x) is said to contain a clash if it contains an fKD -SI clash, or if it
contains,
• some triple ≤ pR, , n and x has p+1 Ri ,ni -neighbours y0 , . . . , yp , x, yi  conjugates
with R, − , 1 − n and yi = yj , ni , n ∈ [0, 1], for all 0 ≤ i < j ≤ p, or
• some triple ≥ pR, , n and x has p Ri ,ni -neighbours y0 , . . . , yp−1 , x, yi  conjugates
with R, , n and yi = yj , ni , n ∈ [0, 1], for all 0 ≤ i < j ≤ p − 1.
Deﬁnition 6.7 (fKD -SHIN Tableaux Algorithm) The initialisation of a forest (FA )
for an fKD -SHIN ABox A is similar to the initialisation of a forest for an fKD -SI ABox
A, with the diﬀerence, that equalities and inequalities need to be considered. More precisely,
.
.
.
.
 xaj if ai =
 aj ∈ A and the relation = to be empty.
we also initialise the relation = as xai =
The latter is used to keep track the nodes that are merged due to the application of a rule for
number restrictions. Finally, the algorithm expands R by adding axioms Inv(R)  Inv(S)
for each R  S ∈ R. FA is then expanded by repeatedly applying the completion rules from
Tables 4 and 6. Note that in Table 6 we abuse the syntax and use the notation Inv(L(x, y))
to indicate the set of triples obtained from L(x, y) by applying function Inv to the role R
of each triple R, , n ∈ L(x, y).
Example 6.8 Now, let us see some examples of the new expansion rules.
• (∀+ ): Let ∀S.C, >, 0.6 ∈ L(x), Inv(P ), ≥, 0.7 ∈ L(y, x) with Trans(R) and P 
* S, and y is an R≥,0.7 -neighbour of x, since y is
R  S. Then, there is role R, with R 
an Inv(R)≥,0.7 -predecessor of x, x, y conjugates with Inv(R), <, 0.4, and ∀R.C, >
, 0.6 ∈ L(y). Hence, ∀R.C, >, 0.6 should be added in L(y).
• (≤≥ ): Let ≤ 2S, ≥, 0.7 ∈ L(x), S, >, 0.7 ∈ L(x, y1 ), S, >, 0.8 ∈ L(x, y2 ) and
* S. Hence, x has 3 S n -neighbours all conjugated
P, ≥, 0.4 ∈ L(x, y3 ) with P 
with S, ≥− , 1 − 0.7 ≡ S, ≤, 0.3 and none an ancestor of x. Hence we have to nondeterministically merge two of them. If we replace the triple S, >, 0.7 ∈ L(x, y1 )
with S, >, 0.2 the rule is no more applicable. That is because although y1 is an S n neighbour of x, x, y1  does not conjugate anymore with S, ≤, 0.3. Intuitively, this
means that the connection between x and y1 is too weak and thus does not contradict
the at-most restriction on x.
♦
As it is obvious the algorithm can be used in order to perform reasoning for the weaker
language fKD -SHIF (fKD -SHI plus functional number restrictions Horrocks & Sattler,
1999. SHIF is obtained from SHIN by allowing only cardinalities 0 and 1 in at-most
and at-least restriction. It is worth noting that, without counting datatypes, SHIF is the
logical underpinning of the OWL Lite ontology language (Horrocks et al., 2003).
306

Reasoning with Very Expressive Fuzzy Description Logics

Rule
(∀+ )

if 1.
2.
3.
4.
then

Description
∀S.C, , n ∈ L(x), x is not indirectly blocked, and
* S,
there is some R, with Trans(R), and R 
x has a R  ,n -neighbour y with, ∀R.C, , n ∈ L(y), and
x, y conjugates with R, − , 1 − n
L(y) → L(y) ∪ {∀R.C, , n},

(∃+ )

if 1.
2.
3.
4.
then

∃S.C, , n ∈ L(x), x is not indirectly blocked and
* S,
there is some R, with Trans(R), and R 
x has a R,n -neighbour y with, ∃R.C, , n ∈ L(y), and
x, y conjugates with R, , n
L(y) → L(y) ∪ {∃R.C, , n},

(≥ )

if 1.
2.
then

≥ pR, , n ∈ L(x), x is not blocked,
there are no p R,n -neighbours y1 , . . . , yp of x with yi = yj for 1 ≤ i < j ≤ p
create p new nodes y1 , . . . , yp , with L(x, yi ) = {R, , n} and yi = yj for 1 ≤ i < j ≤ p

(≤ )

if 1.
then

≤ pR, , n ∈ L(x), x is not blocked,
apply (≥ )-rule for the triple ≥ (p + 1)R, − , 1 − n

(≤ )

if 1.
2.
3.
then

≤ pR, , n ∈ L(x), x is not indirectly blocked,
.
FA
(x, − , 1 − n) > p, there are two of them y, z, with no y = z and
RC
y is neither a root node nor an ancestor of z
1. L(z) → L(z) ∪ L(y) and
2. if z is an ancestor of x
then L(z, x) −→ L(z, x) ∪ Inv(L(x, y))
else
L(x, z) −→ L(x, z) ∪ L(x, y)
.
.
3. L(x, y) −→ ∅ and set u = z for all u with u = y

(≥ )

if 1.
then

≥ pR, , n ∈ L(x), x is not indirectly blocked,
apply (≤ )-rule for the triple ≤ (p − 1)R, − , 1 − n

(≤r )

if 1.
2.
then

≤ pR, , n ∈ L(x),
.
FA
(x, − , 1 − n) > p, there are two of them y, z, both root nodes, with no y = z and
RC
1. L(z) → L(z) ∪ L(y) and
2. For all edges y, w:
i. if the edge z, w does not exist, create it with L(z, w) = ∅
ii. L(z, w) −→ L(z, w) ∪ L(y, w)
3. For all edges w, y:
i. if the edge w, z does not exist, create it with L(w, z) = ∅
ii. L(w, z) −→ L(w, z) ∪ L(w, y)
4. Set L(y) = ∅ and remove all edges to/from y
.
.
.
5. Set u =
 z for all u with u = y and set y = z

(≥r )

if 1.
then

≥ pR, , n ∈ L(x),
apply (≤r )-rule for the triple ≤ (p − 1)R, − , 1 − n

Table 6: Additional tableaux rules for fKD -SHIN
6.2 Decidability of fKD -SHIN
The proof of termination, soundness and completeness of fKD -SHIN is slightly more involved than that of fKD -SI. This is mainly due to the requirement to apply the unravelling
process on a constructed ﬁnite completion forest.
Lemma 6.9 (Termination) Let A be an fKD -SHIN ABox A and R an RBox. The
tableaux algorithm terminates when started for A and R.
307

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Proof: Let m = |sub(A)|, k = |RA |, pmax = max{p |≥ pR ∈ sub(A)} and l be the number
of diﬀerent membership degrees appearing in A. The termination of our algorithm is a
consequence of the same properties that ensure termination in the case of the crisp SHIN
language (Horrocks et al., 2000). In brief we have the following observations. Firstly, the
only rules that remove nodes or concepts from the node labels are the rules ≤, ≥, ≤r
and ≥r , which either expand them or set them to ∅, which means that nodes will be
blocked and will remain blocked forever. Secondly, the expansion rules (∃, ≥ and the
dual ones for negative inequalities) can only be applied once for each node for the same
reasons as in the SHIN case (Horrocks et al., 2000). Since sub(A) contains at most m
concepts ∃R.C and ∀R.C, the out-degree of the tree is bounded by 2lmpmax . Finally, there
is a ﬁnite number of possible labellings for a pair of nodes and an edge, since concepts are
taken from sub(A) and the number of membership degrees is ﬁnite. Thus, there are at most
28mlk possible labellings for a pair of nodes and an edge. Hence, if a path p is of length at
least 28mlk , the pair-wise blocking condition implies that there are 2 nodes x, y on p such
that y directly blocks x.


Lemma 6.10 (Soundness) If the expansion rules can be applied to an fKD -SHIN ABox
A and RBox R, such that they yield a complete and clash-free completion forest, then A
has a fuzzy tableau w.r.t. R.
Proof: Let FA be a complete and clash-free completion forest constructed by the tableaux
algorithm for A. Since the SHIN language does not have the ﬁnite model property (Horrocks & Sattler, 1999) we have to unravel a possibly blocked tree in order to obtain an
inﬁnite tableau. The constructions of such fuzzy tableau works as follows. An individual in
S corresponds to a path in FA . Moving down to blocked nodes and up to blocking ones we
can deﬁne inﬁnite such paths. More precisely, a path is a sequence of pairs of nodes of FA of
the form p = [ xx0 , . . . , xxn ]. For such a path we deﬁne Tail(p) := xn and Tail (p) := x0 . With
n
0
], we denote the path [ xx0 , . . . , xxn , xxn+1
]. The set Paths(FA ) is deﬁned inductively
[p | xxn+1


n
n+1
0
n+1
as follows:
x

• For root nodes xai of FA , [ xaai ] ∈ Paths(FA ), and
i

• For a path p ∈ Paths(FA ) and a node z in FA :
– if z is a successor of Tail(p) and z is neither blocked not a root node, then
[p | zz ] ∈ Paths(FA ), or
– if for some node y in FA , y is a successor of Tail(p) and z blocks y, then [p | zy ] ∈
Paths(FA )
Please node that since root nodes are never blocked, nor are they blocking other nodes
the only place where they occur in a path is in the ﬁrst place. Moreover, if p ∈ Paths(FA ),
then Tail(p) is not blocked; Tail(p) = Tail (p) iﬀ Tail (p) is not blocked and at last L(Tail(p)) =
L(Tail (p)).
Membership degrees are deﬁned exactly as in the case of fKD -SI. Then, a fuzzy tableau
can be deﬁned as in the case of fKD -SI with the following diﬀerences:
308

Reasoning with Very Expressive Fuzzy Description Logics

E(R, p, [p| xx ])
E(R, [q| xx ], q)
E(R, [ xx ], [ yy ])

S

=
=
=
=

V(ai )

=

Paths(FA ),
glb[R, , n], R, , n ∈ L(Tail(p), x )
glb[Inv(R), , n], Inv(R), , n ∈ L(Tail(q), x )
glb[R∗ , , n], x, y root nodes and y R-neighbour of x,
 xai
[ ] if xai is a root node in FA with L(xai ) = ∅

 xai
xa
[ xaj ] if L(xaj ) = ∅ and xaj is a root node,

j

.
with L(xaj ) = ∅ and xai = xaj

It can be shown that T is a fuzzy tableau for A w.r.t. R:
1. Properties 1-6 and Property 13 of Deﬁnition 5.1 are satisﬁed due to the same reasons
as in the proof of lemma 5.9.
2. For property 7, let p, q ∈ S with L(p, ∀R.C) = n1 ≥ n and E(R, p, q)  1 − n,
i.e. E(R, p, q) > 1 − n. The deﬁnition of T implies that either ∀R.C, ≥, n1  ∈
L(Tail(p)) or ∀R.C, >, n  ∈ L(Tail(p)) with n1 = n + . If q = [p| xx ], then x is
an R-successor of Tail(p) and, since glb does not create unnecessary conjugations we
have that R, , r ∈ L(Tail(p), x ) conjugates with R, ≤, 1 − n. Hence, due to
completeness of FA we have either C, ≥, n1  ∈ L(x ) or C, >, n  ∈ L(x ). From the
deﬁnition of Paths(FA ) we have that L(x ) = L(x) = L(q). If p = [q| xx ], then x is an
Inv(R)-successor of Tail(q) and again, the deﬁnition of glb implies that Inv(R), , r ∈
L(Tail(q), x ) conjugates with Inv(R), ≤, 1 − n. Thus, due to completeness of FA ,
either C, ≥, n1  ∈ L(Tail(q)) = L(q) or C, >, n  ∈ L(Tail(q)) = L(q). If p = [ xx ] and
q = [ yy ] for two root nodes x, y then y is an R-neighbour of x, and since the ∀-rule does
not apply we have that wither C, ≥, n1  ∈ L(y) = L(q) or C, >, n  ∈ L(y) = L(q).
Similar proof holds for L(p, ∀R.C) > n and for property 8, of deﬁnition 5.1 and for
the modiﬁed properties 11’ and 12’ of deﬁnition 6.4.
3. For property 9 of Deﬁnition 5.1, assume that L(p, ∃R.C) = n1 ≥ n and let Tail(p) = x.
The deﬁnition of T implies that either ∃R.C, ≥, n1  ∈ L(x) or ∃R.C, >, n  ∈ L(x),
with n1 = n + . We have to show that there is some q ∈ S such that E(R, p, q) ≥
n1 ≥ n and L(q, C) ≥ n1 ≥ n. Since the ∃-rule is not applicable there is some y in
FA with either C, ≥, n1  ∈ L(y) or C, >, n  ∈ L(y). Now there are two possibilities:
(a) If y is a successor of x, then y can either be a root node or not. In case y is a
root node so is x, since it is a predecessor of y, so p = [ xx ] and q = [ yy ] ∈ S. In
case y is not a root node if y is not blocked, then q = [p| yy ] ∈ S; if y is blocked
by some z then, q = [p| yz ] ∈ S.
(b) x is an Inv(R)-successor of y. Since x is a successor of y we distinguish the cases
of x being a root node or not. If x is a root then so is y, hence q = [ yy ] ∈ S. If
x is not a root node then either p = [q| xx ], with Tail(q) = y, or p = [q| xx ], with
Tail(q) = u = y, x blocks x and u is a predecessor of x . By the deﬁnition of
pair-wise blocking we have that L(y) = L(u) and L(y, x) = L(u, x ).
In any of these cases, E(R, p, q) ≥ n1 ≥ n, L(q, C) ≥ n1 ≥ n. Similar proof applies
for L(p, ∃R.C) > n and for property 10.
309

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

4. Property 16 in deﬁnition 6.4 is satisﬁed due to the deﬁnition of R-successor that takes
into account the role hierarchy.
5. For Property 17 assume that L(p, ≥ mR) = n1 ≥ n. The deﬁnition of T implies
that either ≥ mR, ≥, n1  ∈ L(x) or ≥ mR, >, n  ∈ L(x), with n1 = n + . This
means that there are m individuals y1 , . . . , ym in FA such that each yi is an R≥,n or R>,n -neighbour of x. We have to show that for each of these yi s, there is a path
qi , such that E(R, p, qi ) ≥ n1 , and qi = qj for all 1 ≤ i < j ≤ m. The proof is
similar with the one given by Horrocks et al. (2000). It is based on the fact that in
case where some z blocks several yi s, then the construction of the paths distinguishes
between these yi s be seting qi = [p| yzi ], thus ensuring the existence of diﬀerent paths
in T . Thus, for each yi there is a diﬀerent path qi in S with E(R, p, qi ) ≥ n1 ≥ n, or
E(R, p, qi ) ≥ n +  ≥ n and RT (p, ≥, n) ≥ m. Similarly for L(p, ≥ mR) > n and
for property 18.
6. For Property 19 in deﬁnition 6.4 suppose that there exists p ∈ S with L(p, ≤ mR) =
T (p, ≤, 1 − n) > m. We have to show that this implies RFA (Tail(p), ≤
n1 ≥ n and R¬
C
, 1 − n) > m, in the completion-forest, thus contradicting either clash-freeness or
completeness of FA . More precisely, one has to show that the construction does not
create more conjugated paths for T than those that exist in FA . This can only be
the case if for some node y the construction creates two distinct paths of the form
qi = [p| yyi ]. As shown by Horrocks et al. (2000), the proof relies on the fact that the
function Tail is injective on the paths of T , i.e. for q1 and q2 , Tail (q1 ) = y = Tail (q2 )
implies that q1 = q2 . Hence, such paths cannot be distinct. Similar observations hold
for L(p, ≤ mS) > n and for property 20.
7. Properties 14 and 15 of Deﬁnition 5.1 are satisﬁed cause of the initialization of the
completion-forest and the fact that the algorithm never blocks root nodes. Furthermore, for each root node xai whose label and edges are removed by the ≤r -rule, there
is another root node xaj with xai = xaj and {C, , n|(ai : C)  n ∈ A} ⊆ L(xaj ).
8. Property 21 of Deﬁnition 6.4 is satisﬁed because the ≤r -rule does not identify two
root nodes xai , xaj when xai = xaj holds.


Lemma 6.11 (Completeness) Let A be an fKD -SHIN fuzzy ABox and R an RBox. If
A has a fuzzy tableau w.r.t. R, then the expansion rules can be applied to A and R in such
a way that the tableaux algorithm yields a complete and clash-free completion-forest.
Proof: The proof is quite similar with the proof of lemma 5.10. In the new algorithm we
have some new non-deterministic rules, but again the existence of fuzzy tableau for A w.r.t.
R can help us steer the application of those non-deterministic rules. In the following table
we show the modiﬁed rule ≤. The rest of the non-deterministic rules can be guided by
modifying them in a similar way.


310

Reasoning with Very Expressive Fuzzy Description Logics

(≤ )

if 1.
3.
then

≤ pR, , n ∈ L(x), x is not indirectly blocked,
.
FA
(x, − , 1 − n) > p, there are two of them y, z, with no y =
 z and
RC
y is neither a root node nor an ancestor of z and π(y) = π(z)
1. L(z) → L(z) ∪ L(y) and
2. if z is an ancestor of x
then L(z, x) −→ L(z, x) ∪ Inv(L(x, y))
else
L(x, z) −→ L(x, z) ∪ L(x, y)
.
.
3. L(x, y) −→ ∅ and set u =
 z for all u with u =
 y

Table 7: The ≤-rule
Theorem 6.12 The tableaux algorithm is a decision procedure for the consistency problem
of fKD -SHIN ABoxes and the satisﬁability and subsumption of fKD -SHIN -concepts with
respect to simple terminologies.
We will conclude this section by investigating the complexity of the proposed algorithm.
Lemma 6.13 For an fKD -SHIN ABox A and a role hierarchy R, sub(A, R) = O(|A| ×
|R|).
Proof: The proof is quite similar with the one presented by Tobies (2001). Since sub(A, R)
contains all concepts C such that (a : C)n ∈ A and is closed under sub-concepts of C,
it contains O(|A|) concepts. Additionally, we have to add a concept ∀R.C or ∃R.C to
* S and then close sub(A, R)
sub(A, R) if ∀S.R ∈ sub(A, R) or ∃S.R ∈ sub(A, R) and R 
again under sub-concepts and ∼. This may yield at most two concept for every concept in
sub(A, R) and role in R. Thus, sub(A, R) = O(2|A| × |R|).


Lemma 6.14 The fKD -SHIN -algorithm runs in 2-Nexptime.
Proof: Let A be a fKD -SHIN ABox and R an RBox. Let m = sub(A), k = |RA |,
pmax the maximum number p that occurs in a number restriction and l the number of
diﬀerent membership degrees appearing in A. Following Tobies (2001) we set n = |A| + |R|,
then due to lemma 6.13 it holds that m = O(2|A| · |R|) = O(n2 ), k = O(|A| + |R|),
pmax = O(2|A| ) = O(2n ) and l = O(|A|) = O(n). In the proof of lemma 6.9 we have shown
that paths in a completion-forest for A become no longer than 28mlk and that the out-degree
is bounded by 2lmpmax . Hence, the fKD -SHIN algorithm will construct a forest with no
more than
8mlk

(2lmpmax )2

8n2 ·n·n

= O((2n · n2 · 2n )2

8n4

) = O(2n·2

n5

) = O(22 )

nodes.


Hence, the fKD -SHIN algorithm is of the same theoretical complexity as the SHIN algorithm (Tobies, 2001).
Concluding our presentation on the issue of reasoning with expressive fuzzy DLs we
comment on how to handle GCIs in the fKD -SI and fKD -SHIN languages. As it is noted
by Horrocks and Sattler (1999), SHIN is expressive enough to internalize GCIs into a
single concept, hence reducing reasoning with GCIs to concept satisﬁability. The idea
311

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

behind internalization is that the semantic restrictions imposed by an axioms of the form
C  D can be encoded within a concept of the form ¬C  D. As it was remarked by
Stoilos et al. (2006) this reduction of concept inclusions does not hold for fKD -DLs, since
the semantics of the axiom C  D are diﬀerent than that of the concept ¬C D. Hence, the
internalization method proposed by Horrocks and Sattler (1999) for the SHIN language
cannot be applied in the fKD -SHIN language.
Stoilos et al. (2006) and Li et al. (2006a) propose techniques by which we can handle
GCIs in fKD -DLs. Stoilos et al. (2006) use the DL language fKD -ALC in order to present
their technique, while Li et al. (2006a) use the language fKD -SHI. These procedures can
be applied in the cases of fKD -SI and fKD -SHIN , since they are independent of the
underlying DL formalism. Roughly speaking these techniques are performed in three steps.
In the ﬁrst step the ABox is normalized, by replacing each assertion of the form (a : C) > n
and (a : C) < n by assertions (a : C) ≥ n +  and (a : C) ≤ n − , respectively, where
 is a small number from [0, 1]. Obviously, in a normalized ABox only assertions with
inequalities ≥ and ≤ are present. In the second step the set of relative membership degrees
is constructed: X A = {0, 0.5, 1} ∪ {n, 1 − n | φn}, where obviously  ∈ {≥, ≤}. Finally, a
tableaux expansion rule is employed to transfer the semantic restrictions imposed by each
GCI C  D ∈ T into fuzzy assertions of the ABox. More precisely, for each C  D ∈ T ,
node x in FA and degree n ∈ X A , the algorithm adds either C, ≤, n −  or D, ≥, n to
L(x). We remark here that the rule proposed by Li et al. (2006a) is slightly diﬀerent.
As noted by Stoilos et al. (2006), tableaux algorithms need to be slightly changed in
order to handle GCIs. First, due to the normalization step, degrees are now taken from
the interval [−, 1 + ], thus clash deﬁnitions ⊥, >, n and 
, <, n are removed since no
assertion with > and < exist anymore and the clashes C, <, 0 and C, >, 1 are replaced by
C, ≤, − and C, ≥, 1 + , respectively. The termination of the algorithm is not aﬀected
since again the set of membership degrees is ﬁnite (taken from the set X A ), but the practical
complexity increases dramatically since we have a non-deterministic choice for each axiom
C  D ∈ T and degree n ∈ X A . The proof of soundness is not aﬀected much and as it
was showed by Stoilos et al. (2006) the glb function is replaced by a simple max, due to the
lack of assertions with inequalities > and <, while the non-deterministic rule for handling
subsumptions can also be modiﬁed to be guided, in order to provide us with a proof for
completeness.
Example 6.15 Let the knowledge base Σ = {≥ 1R  C}, {(a, b : R) ≥ 0.6, (a : C) <
0.6}. Intuitively, the concept axioms states that the domain of the role R is concept C.5
Obviously, the knowledge base is unsatisﬁable since the concept axiom suggests that ∀xI ∈
∆I , supbI min1i=1 (RI (xI , bIi )) = RI (xI , cI ) ≤ C I (xI ), for some arbitrary cI ∈ ∆I , but the
1
ABox assertions state that there exists aI ∈ ∆I and bI ∈ ∆I such that RI (aI , bI ) ≥ 0.6 >
C I (aI ). The above concept inclusion axioms is a GCI, hence we have to use a technique
for GCIs.
First, we apply the normalization step in the original ABox, obtaining the normalized
one: {(a, b : R) ≥ 0.6, (a : C) ≤ 0.6 − }. Secondly, we collect the set of relative
membership degrees: X A = {0, 0.5, 1} ∪ {0.4, 0.4 + , 0.6 − , 0.6}.
5. A domain axiom can also be stated as ∃R.  C, but we use the above form in order to show how the
algorithm behaves with number restrictions.

312

Reasoning with Very Expressive Fuzzy Description Logics

Then, the algorithm initializes a completion-forest to contain the following nodes with
the respective triples:
(1)
(2)

R, ≥, 0.6 ∈ L(xa , xb )
C, ≤, 0.6 −  ∈ L(xa )

Then the algorithm expands the completion forest by using the rules from Tables 4 and 6
and with the additional rule presented by Stoilos et al. (2006). This rule applied on the axiom
≥ 1R  C adds either ≥ 1R, ≤, n −  or C, ≥, n in L(xa ), for each n ∈ X A . Hence, at
some point the algorithm chooses 0.6 ∈ X A and adds either ≥ 1R, ≤, 0.6−, or C, ≥, 0.6.
In the former case ≥ 1R, ≤, 0.6− ∈ L(xa ) and xa has 1 R≥,0.6 -neighbour xb , and xa , xb 
conjugated with R, ≤, 0.6 − , while in the latter case {C, ≤, 0.6 − , C, ≥, 0.6} ⊆ L(xa ),
hence L(xa ) contains a pair of conjugated triples and thus a clash. We conclude that all
possible expansions result to a clash, thus the knowledge base is unsatisﬁable.
♦

7. Related Work
There have been many eﬀorts in the past to extend description logics with fuzzy set theory
(Yen, 1991; Tresp & Molitor, 1998; Straccia, 2001; Hölldobler et al., 2002; Sanchez & Tettamanzi, 2006; Straccia, 2005b; Hajek, 2005; Li, Xu, Lu, & Kang, 2006b). The ﬁrst eﬀort was
presented by Yen (1991). In his extension, explicit membership functions over a domain
were used as well as membership manipulators, like “very” or “moreOrLess”, in order to
alter membership functions and deﬁne new concepts from already deﬁned ones. A later
approach was presented by Tresp and Molitor (1998), where membership manipulators also
appear. Regarding reasoning algorithms Yen described a structural subsumption algorithm
for a rather small DL language while Tresp and Molitor a tableaux calculus for ALC F M
(ALC extended with fuzzy set theory and the membership manipulator constructor). The
application of the tableaux rules creates a set of equations and inequations which are later
solved with an optimization method. Moreover, when determining a subsumption or entailment relation between two concepts, with respect to a (KB), the assertions of the KB were
considered of a crisp form (i.e. a belongs to C to a degree of 1). After the application of the
reasoning algorithm and the solution of the equations the minimum value of the solution set
is taken as the degree that the KB entails the crisp assertion or that a concept is subsumed
by another.
A fuzzy extension of the ALC language was also considered by Straccia (2001, 1998).
Reasoning algorithms for the problem of crisp entailment and subsumption were provided,
and were based on tableaux calculus. The algorithm was proved to be PSPACE-complete.
Moreover, complete reasoning algorithms for fuzzy ALC were provided by Hölldobler et al.
(2002), where membership manipulators (linguistic hedges) were also used on primitive
concepts. This approach was later extended by Hölldobler, Nga, and Khang (2005) to allow
linguistic hedges also on complex concepts. The languages presented are called ALC F H
and ALC F LH (ALC plus linguistic hedges and linear linguistic hedges, respectively). In all
these approaches also the min-max norms and the Kleene-Dienes implication were used to
perform the fuzzy set theoretic operations.
313

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Approaches towards more expressive DLs, are presented by Sánchez and Tettamanzi
(2004), Sanchez and Tettamanzi (2006), Straccia (2005b), Straccia (2005a) and Stoilos
et al. (2005c). The language considered by Sánchez and Tettamanzi (2004) is ALCQ (ALC
plus qualiﬁed number restrictions, Tobies, 2001). The authors also include fuzzy quantiﬁers
which is a novel approach to fuzzy DLs, and the norm operations are the same with the ones
used here. Subsequently, Sanchez and Tettamanzi (2006) propose a procedure to calculate
the satisﬁability interval for a fuzzy concept. Due to the presence of fuzzy quantiﬁers it is not
clear how other inference problems, like entailment and subsumption can be solved. Straccia
(2005b) considered the semantics of fuzzy SHOIN (D+ ), which is the DL counterpart of
the OWL DL language. In his approach generalized norm operations were used for the
semantics, while no reasoning algorithms were provided as well as no investigation of the
properties of value and existential restrictions, when transitive relations and role hierarchies
participate in such concepts. Furthermore, the semantics of number restrictions were not
analyzed. The approach by Straccia was used by Stoilos et al. (2005c), in order to provide
the abstract syntax and semantics to concept and role descriptions and axioms of the fuzzy
OWL language. Additionally, Stoilos et al. (2005c) present a method to translate a fuzzy
OWL ontology to a fuzzy SHOIN knowledge base, thus reasoning in fuzzy OWL can be
reduced to reasoning in expressive fuzzy DLs. At last the language considered by Straccia
(2005a) is ALC(D) (ALC plus concrete domains), where additionally a reasoning algorithm
based on an optimization technique was presented. The norm operations used are the ones
we used in the current paper, plus the Lukasiewicz t-norm, t(a, b) = max(0, a + b − 1),
t-conorm, u(a, b) = min(1, a + b) and fuzzy implication J (a, b) = min(1, 1 − a + b). An
approach towards fuzzy DLs with concrete domains has been also presented by Liu, Tian,
and Ma (1994) for modelling the selection of research and development projects.
In all previous approaches reasoning with respect to simple and acyclic TBoxes was
considered. Stoilos et al. (2006) propose a method to perform reasoning w.r.t. general
and/or cyclic TBoxes in the language fKD -ALC. This method applies a preprocessing step
on the ABox, called normalization and then it extends the classical fKD -ALC algorithm
(Straccia, 2001) with an additional rule, in order to deal with general and cyclic axioms.
Moreover, Li et al. (2006a) extend the fuzzy tableau of fKD -SHI proposed by Stoilos et al.
(2005a) with an additional rule to also handle with general and cyclic TBoxes in the language
fKD -SHI. Interestingly, the technique used by Stoilos et al. (2006) is diﬀerent than that
presented by Li et al. (2006a).
It also is worth noting the works of Bonatti and Tettamanzi (2005), where the complexity
of fuzzy DL languages is investigated. Furthermore, Hajek (2005) investigates properties of
the fuzzy ALC language, when arbitrary continuous norm operations are used and provides
interesting results. More precisely, Hajek shows that the problems of concept satisﬁability
and subsumption are decidable for the Lukasiewicz fuzzy ALC (fL -ALC), while in product
fuzzy ALC (fP -ALC) and Gödel fuzzy ALC (fG -ALC) only witnessed satisﬁability and subsumption are decidable. For unrestricted models both fP -ALC and fG -ALC lack the ﬁnite
model property (Hajek, 2005). This is accomplished by reducing these problems to the
problem of propositional satisﬁability of fuzzy propositional logic. These results where further extended to fuzzy DLs with truth constants, i.e. to ABox consistency, again by Hajek
(2006). Moreover, Straccia (2004) present a technique by which an fKD -ALCH knowledge
base can be reduced to a crisp ALCH knowledge base. Hence, reasoning in a fuzzy KB
314

Reasoning with Very Expressive Fuzzy Description Logics

can be performed by using existing and optimized DL systems. Then, Bobillo et al. (2006)
extended this technique to be able to reduce a fKD -SHOIN KB to a crisp SHOIN KB.
At last, Li, Xu, Lu, Kang, and Wang (2005) and Li et al. (2006b), also use the idea of the
reduction in order to annotate the concepts and roles of the crisp languages ALCN and
ALCQ, respectively with degrees denoted as sub-scripts in the syntax of concepts and roles
and provide reasoning for the languages fKD -ALCN and fKD -ALCQ.
In all previous approaches, reasoning algorithms for rather inexpressive fuzzy DLs, i.e.
fuzzy-ALC extended with concept modiﬁers or concrete domains or number restrictions or
qualiﬁed number restrictions or general TBoxes were presented. As far as we know this
is the ﬁrst presentation of a reasoning algorithm for such complex fuzzy DL languages.
In order to achieve our goal we have provided an investigation on the semantics of the
extended language when fuzzy transitive relations and role hierarchies are considered in
value and existential restrictions or of the number restrictions constructor. The aim of
such an investigation is to discover if properties of the classical SI and SHIN languages,
like the propagation of value restrictions or counting of R-neighbours also apply in the
fuzzy case. We have found that apart from value restrictions also existential restrictions
have to be propagated. Additionally, we have shown that the membership degree of these
concepts in their new nodes is the same as that in their source nodes. Moreover, we have
seen that role hierarchies can be smoothly integrated as in the classical case. Additionally,
the analysis of the semantics of number restrictions have shown that despite how complex
their semantics might appear, regarding reasoning they can also be eﬃciently handled,
as in the classical case. Furthermore, we have investigated the applicability of blocking
strategies, like dynamic blocking and pair-wise blocking, which are used in the crisp SI and
SHIN language to ensure the termination of the proposed algorithms. We have seen that
the properties of the norm operations used ensure that such blocking conditions can also
be applied. Based on these investigations, we were able to provide a tableaux reasoning
algorithm, to decide the key inference problems of very expressive fuzzy DLs, and we have
proved their soundness, completeness and termination.

8. Conclusions and Future Work
Making applications capable of coping with vagueness and imprecision will result in the
creation of systems and applications which will provide us with high quality results and
answers to complex user deﬁned tasks. To this direction we have to extend with fuzzy set
theory the underlying logical formalisms that they use in order to represent knowledge and
perform reasoning tasks. DL is a logical formalism that has gained a lot of attention the last
decade, cause of its decidability, the powerful reasoning tools that have been implemented
and the well-deﬁned model-theoretic semantics.
Towards extending DLs with fuzzy set theory we have presented two very expressive
fuzzy DLs, fKD -SI and fKD -SHIN . We have investigated the properties of the semantics
that result by adding fuzziness to very expressive fuzzy DLs, i.e. to fuzzy DLs that allow for
transitive and inverse roles, role hierarchies and number restrictions and we have provided
sound, complete and terminating reasoning algorithms for both of these formalisms. Even
though handling fuzziness in such expressive languages seems quite diﬃcult and reasoning
was not previously known, we show that fKD -SI and fKD -SHIN with the min, max norms
315

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

are still decidable. We have shown that the techniques used in the classical case can also
be applied in the extended frameworks, but this can only happen after closely investigating
the properties of the languages and after proving that these techniques also work in this
new setting. In the current paper we have not addressed nominals (O) (Horrocks & Sattler,
2005). Note that in the fuzzy DL literature, there are proposals for crisp nominals (Stoilos
et al., 2005c) and fuzzy nominals (Bobillo et al., 2006). Thus, the nominal constructor is not
yet a mature notion in fuzzy DLs and more research is needed in order to ﬁnd appropriate
semantics for them, considering also the issue from the application point of view.
As far as future directions are concerned, these will include the extension of the algorithm of fKD -SHIN , in order to provide reasoning support for the fuzzy DL fKD -SHOIQ.
SHOIQ extends SHIN with qualiﬁed number restrictions (Tobies, 2001), which are very
important in real life applications (Rector & Horrocks, 1997), and with nominals. Thus,
we also intend to compare the properties of the diﬀerent proposals for nominals in fuzzy
DLs. Again, although we expect that similar notions as in the classical SHOIQ language
can be applied to fKD -SHOIQ, we need to investigate them in the new setting and prove
that they work. Furthermore, additional research eﬀort can be focused on the investigation
of the reasoning problem for the f-SI and f-SHIN languages, extended with other norm
operations. Regarding f-SHIN , this might be a very diﬃcult problem since counting on
number restrictions might not be possible anymore.

Acknowledgments
This work is supported by the FP6 Network of Excellence EU project Knowledge Web (IST2004-507482). Giorgos Stoilos, Giorgos Stamou and Vassilis Tzouvaras were also partially
funded by the European Commission under FP6 Integrated Project X-Media (FP6-026978).

References
Alejandro, J., Belle, T., & Smith, J. (2003). Modal keywords, ontologies and reasoning for
video understanding. In Proceedings of the International Conference on Image and
Video Retrieval.
Athanasiadis, T., Mylonas, P., Avrithis, Y., & Kollias, S. (2007). Semantic image segmentation and object labeling. IEEE Transactions on Circuits and Systems for Video
Technology, 17 (3), 298–312.
Baader, F. (1990). Augmenting Concept Languages by Transitive Closure of Roles: An
Alternative to Terminological Cycles. Research report RR-90-13. An abridged version
appeaered in Proc. IJCAI-91,pp.446-451.
Baader, F., McGuinness, D., Nardi, D., & Patel-Schneider, P. (2002a). The Description
Logic Handbook: Theory, implementation and applications. Cambridge University
Press.
Baader, F., Horrocks, I., & Sattler, U. (2002b). Description Logics for the Semantic Web.
KI – Künstliche Intelligenz, 16 (4), 57–59.
316

Reasoning with Very Expressive Fuzzy Description Logics

Bechhofer, S., van Harmelen, F., Hendler, J., Horrocks, I., McGuinness, D. L., PatelSchneider, P. F., & eds., L. A. S. (2004). OWL web ontology language reference.
Tech. rep..
Benitez, A. B., Smith, J. R., & Chang, S. (2000). MediaNet: a multimedia information network for knowledge representation. In Proc. SPIE Vol. 4210, p. 1-12, Internet Multimedia Management Systems, John R. Smith; Chinh Le; Sethuraman Panchanathan;
C.-C. J. Kuo; Eds., pp. 1–12.
Berners-Lee, T., Hendler, J., & Lassila, O. (2001). The semantic web. Scientiﬁc American.
Bobillo, F., Delgado, M., & Gómez-Romero, J. (2006). A crisp representation for fuzzy
shoin with fuzzy nominals and general concept inclusions. In Proc. of the 2nd International Workshop on Uncertainty Reasoning for the Semantic Web (URSW 06),
Athens, Georgia.
Bonatti, P., & Tettamanzi, A. (2005). Some complexity results on fuzzy description logics.
In V. Di Gesù, F. Masulli, A. P. (Ed.), WILF 2003 International Workshop on Fuzzy
Logic and Applications, LNCS 2955, Berlin. Springer Verlag.
Calvanese, D., De Giacomo, G., Lenzerini, M., Nardi, D., & Rosati, R. (1998). Description
logic framework for information integration. In Proc. of the 6th Int. Conf. on the
Principles of Knowledge Representation and Reasoning (KR’98), pp. 2–13.
Chen, H., Fellah, S., & Bishr, Y. (2005). Rules for geospatial semantic web applications. In
W3C Workshop on Rule Languages for Interoperability.
Ding, Z., & Peng, Y. (2004). A Probabilistic Extension to Ontology Language OWL. In
Proceedings of the 37th Hawaii International Conference On System Sciences (HICSS37)., p. 10, Big Island, Hawaii.
Dubois, D., & Prade, H. (2001). Possibility theory, probability theory and many-valued
logics: A clariﬁcation. Ann. Math. Artif. Intell., 32 (1-4), 35–66.
Fagin, R. (1998). Fuzzy queries in multimedia database systems. In Proc. Seventeenth ACM
Symp. on Principles of Database Systems, pp. 1–10.
Giugno, R., & Lukasiewicz, T. (2002). P-SHOQ(D): A probabilistic extension of shoq(d)
for probabilistic ontologies in the semantic web. In JELIA ’02: Proceedings of the
European Conference on Logics in Artiﬁcial Intelligence, pp. 86–97, London, UK.
Springer-Verlag.
Hajek, P. (1998). Metamathematics of fuzzy logic. Kluwer.
Hajek, P. (2005). Making fuzzy description logic more general. Fuzzy Sets and Systems,
154 (1), 1–15.
Hajek, P. (2006). Computational complexity of t-norm based propositional fuzzy logics with
rational truth constants. Fuzzy Sets and Systems, 157 (13), 677–682.
Hölldobler, S., Khang, T. D., & Störr, H.-P. (2002). A fuzzy description logic with hedges
as concept modiﬁers. In Proceedings InTech/VJFuzzy’2002, pp. 25–34.
Hölldobler, S., Nga, N. H., & Khang, T. D. (2005). The fuzzy description logic ALC F LH .
In International workshop on Description Logics. CEUR.
317

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Hollunder, B. (1994). An alternative proof method for possibilistic logic and its application
to terminological logics. In Proceedings of the 10th Annual Conference on Uncertainty in Artiﬁcial Intelligence (UAI-94), pp. 327–335, San Francisco, CA. Morgan
Kaufmann Publishers.
Hollunder, B., Nutt, W., & Schmidt-Schaus, M. (1990). Subsumption algorithms for concept
description languages. In European Conference on Artiﬁcial Intelligence, pp. 348–353.
Horrocks, I., Patel-Schneider, P. F., & van Harmelen, F. (2003). From SHIQ and RDF to
OWL: The making of a web ontology language. Web Semantics, 1.
Horrocks, I., & Sattler, U. (1999). A description logic with transitive and inverse roles and
role hierarchies. Journal of Logic and Computation, 9, 385–410.
Horrocks, I., & Sattler, U. (2005). A tableaux decision procedure for SHOIQ. In Proc.
19th Int. Joint Conf. on Artiﬁcial Intelligence (IJCAI 05).
Horrocks, I., Sattler, U., & Tobies, S. (1999). Practical reasoning for expressive description
logics. In Proceedings of the 6th International Conference on Logic for Programming
and Automated Reasoning (LPAR’99), No. 1705 in LNAI, pp. 161–180. SpringerVerlag.
Horrocks, I., Sattler, U., & Tobies, S. (2000). Reasoning with Individuals for the Description
Logic SHIQ . In MacAllester, D. (Ed.), CADE-2000, No. 1831 in LNAI, pp. 482–496.
Springer-Verlag.
Kandel, A. (1982). A Fuzzy Techniques in Pattern Recognition. Wiley.
Klement, E. P., Mesiar, R., & Pap, E. (2004). Triangular norms. position paper I: basic
analytical and algebraic properties. Fuzzy Sets and Systems, 143, 5–26.
Klir, G. J., & Yuan, B. (1995). Fuzzy Sets and Fuzzy Logic: Theory and Applications.
Prentice-Hall.
Koller, D., Levy, A., & Pfeﬀer, A. (1997). P-CLASSIC: A tractable probabilistic Description Logic. In Proceedings of the 14th National Conference on Artiﬁcial Intelligence
(AAAI-97)., pp. 390–397.
Krishnapuram, R., & Keller, J. (1992). Fuzzy set theoretic approach to computer vision:
An overview. In IEEE International Conference on Fuzzy Systems, pp. 135–142.
Larsen, H., & Yager, R. (1993). The use of fuzzy relational thesauri for classiﬁcatory problem
solving in information restrieval and exprert systems. IEEE Trans. in System, Man,
and Cybernetics, 23 (1), 31–41.
Li, Y., Xu, B., Lu, J., & Kang, D. (2006a). Discrete tableau algorithms for FSHI. In
Proceedings of the International Workshop on Description Logics (DL 2006), Lake
District, UK.
Li, Y., Xu, B., Lu, J., & Kang, D. (2006b). Reasoning technique for extended fuzzy ALCQ
. In ICCSA (2), pp. 1179–1188.
Li, Y., Xu, B., Lu, J., Kang, D., & Wang, P. (2005). Extended fuzzy description logic ALCN .
In Proceedings of the 9th International Conference on Knowledge Based Intelligent
Information and EngineeringSystems (KES-05), pp. 896–902.
318

Reasoning with Very Expressive Fuzzy Description Logics

Liu, O., Tian, Q., & Ma, J. (1994). A fuzzy description logic approach to model management in r&d project selection. In Proceedings of the 8th Paciﬁc Asian Conference on
Information Systems (PACIS-04).
McGuiness, D. (2003). Conﬁguration. In Baader, F., Calvanese, D., McGuinness, D.,
Nardi, D., & Patel-Schneider, P. F. (Eds.), The Description Logic Handbook: Theory,
Implementation, and Applications, pp. 388–405. Cambridge University Press.
Meghini, C., Sebastiani, F., & Straccia, U. (2001). A model of multimedia information
retrieval. Journal of the ACM, 48 (5), 909–970.
Mostert, P., & Shields, A. (1957). On the structure of semigroups on a compact manifold
with boundary. The Annals of Mathematics, 65 (1), 117–143.
Navara, M. (2000). Satisﬁability in fuzzy logic. Neural Network World, 10 (5), 845–858.
Nebel, B. (1990). Terminological reasoning is inherently intractable. Journal of Artiﬁcial
Intelligence, 43, 235–249.
Oguntade, O., & Beaumont, P. (1982). Ophthalmological prognosis via fuzzy subsets. Fuzzy
Sets and Systems, 7 (2), 123–179.
Pan, J. Z. (2004). Description Logics: Reasoning Support for the Semantic Web. Ph.D. thesis, School of Computer Science, The University of Manchester, Oxford Rd, Manchester M13 9PL, UK.
Rector, A. L., & Horrocks, I. (1997). Experience building a large, re-usable medical ontology
using a description logic with transitivity and concept inclusions. In Proceedings
Workshop on Ontological Engineering, AAAI Spring Symposium, Stanford CA., pp.
100–107. Hanley and Belfus, Inc., Philadelphia, PA.
Sánchez, D., & Tettamanzi, A. (2004). Generalizing quantiﬁcation in fuzzy description logic.
In Proceedings 8th Fuzzy Days in Dortmund.
Sanchez, D., & Tettamanzi, A. A. (2006). Fuzzy quantiﬁcation in fuzzy description logics.
In Sanchez, E. (Ed.), Capturing Intelligence: Fuzzy Logic and the Semantic Web.
Elsevier.
Sattler, U. (1996). A concept language extended with diﬀerent kinds of transitive roles. In
KI ’96: Proceedings of the 20th Annual German Conference on Artiﬁcial Intelligence,
pp. 333–345. Springer-Verlag.
Stoilos, G., Stamou, G., Tzouvaras, V., Pan, J., & Horrocks, I. (2005a). The fuzzy description logic f-SHIN . In Proceedings of the International Workshop on Uncertainty
Reasoning for the Semantic Web.
Stoilos, G., Stamou, G., Tzouvaras, V., Pan, J., & Horrocks, I. (2005b). A fuzzy description
logic for multimedia knowledge representation. In Proc. of the International Workshop
on Multimedia and the Semantic Web.
Stoilos, G., Stamou, G., Tzouvaras, V., Pan, J., & Horrocks, I. (2005c). Fuzzy OWL:
Uncertainty and the semantic web. In Proc. of the International Workshop on OWL:
Experiences and Directions.
319

Stoilos, Stamou, Pan, Tzouvaras & Horrocks

Stoilos, G., Straccia, U., Stamou, G., & Pan, J. (2006). General concept inclusions in fuzzy
description logics. In Proceedings of the 17th International Conference on Artiﬁcial
Intelligence (ECAI 06), pp. 457–461. IOS Press.
Straccia, U. (2001). Reasoning within fuzzy description logics. Journal of Artiﬁcial Intelligence Research, 14, 137–166.
Straccia, U. (2005a). Description logics with fuzzy concrete domains. In 21st Conf. on
Uncertainty in Artiﬁcial Intelligence (UAI-05), Edinburgh.
Straccia, U. (2005b). Towards a fuzzy description logic for the semantic web. In Proceedings
of the 2nd European Semantic Web Conference.
Straccia, U. (1998). A fuzzy description logic. In AAAI ’98/IAAI ’98: Proceedings of the
ﬁfteenth national/tenth conference on Artiﬁcial intelligence/Innovative applications of
artiﬁcial intelligence, pp. 594–599. American Association for Artiﬁcial Intelligence.
Straccia, U. (2004). Transforming fuzzy description logics into classical description logics.
In Proceedings of the 9th European Conference on Logics in Artiﬁcial Intelligence
(JELIA-04), No. 3229 in Lecture Notes in Computer Science, pp. 385–399, Lisbon,
Portugal. Springer Verlag.
Sugeno, M. (1985). Industrial Applications of Fuzzy Control. North-Holland.
Tarski, A. (1956). Logic, Semantics, Metamathemetics: Papers from 1923 to 1938. Oxford
University Press.
Tobies, S. (2001). Complexity Results and Practical Algorithms for Logics in Knowledge Representation. Ph.D. thesis, Rheinisch-Westfälischen Technischen Hochschule Aachen.
URL http://lat.inf.tu-dresden.de/research/phd/Tobies-PhD-2001.pdf .
Tresp, C., & Molitor, R. (1998). A description logic for vague knowledge. In In proc of the
13th European Conf. on Artiﬁcial Intelligence (ECAI-98).
Vardi, M. Y. (1997). Why is modal logic so robustly decidable?. In DIMACS Series in
Discrete Mathematics and Theoretical Computer Science, pp. 149–184.
Yen, J. (1991). Generalising term subsumption languages to fuzzy logic. In In Proc of the
12th Int. Joint Conf on Artiﬁcial Intelligence (IJCAI-91), pp. 472–477.
Zadeh, L. A. (1965). Fuzzy sets. Information and Control, 8, 338–353.
Zimmermann, H. (1987). Fuzzy Sets, Decision Making, and Expert Systems. Kluwer, Boston.

320

Journal of Artificial Intelligence Research 30 (2007) 133–179

Submitted 3/07; published 9/07

Chain: A Dynamic Double Auction Framework for Matching
Patient Agents
Jonathan Bredin

bredin@acm.org

Dept. of Mathematics and Computer Science, Colorado College
Colorado Springs, CO 80903, USA

David C. Parkes
Quang Duong

parkes@eecs.harvard.edu
qduong@fas.harvard.edu

School of Engineering and Applied Sciences, Harvard University
Cambridge, MA 02138, USA

Abstract
In this paper we present and evaluate a general framework for the design of truthful
auctions for matching agents in a dynamic, two-sided market. A single commodity, such
as a resource or a task, is bought and sold by multiple buyers and sellers that arrive
and depart over time. Our algorithm, Chain, provides the first framework that allows a
truthful dynamic double auction (DA) to be constructed from a truthful, single-period (i.e.
static) double-auction rule. The pricing and matching method of the Chain construction
is unique amongst dynamic-auction rules that adopt the same building block. We examine
experimentally the allocative efficiency of Chain when instantiated on various single-period
rules, including the canonical McAfee double-auction rule. For a baseline we also consider
non-truthful double auctions populated with “zero-intelligence plus”-style learning agents.
Chain-based auctions perform well in comparison with other schemes, especially as arrival
intensity falls and agent valuations become more volatile.

1. Introduction
Electronic markets are increasingly popular as a method to facilitate increased efficiency
in the supply chain, with firms using markets to procure goods and services. Two-sided
markets facilitate trade between many buyers and many sellers and find application to
trading diverse resources, including bandwidth, securities and pollution rights. Recent
years have also brought increased attention to resource allocation in the context of ondemand computing and grid computing. Even within settings of cooperative coordination,
such as those of multiple robots, researchers have turned to auctions as methods for task
allocation and joint exploration (Gerkey & Mataric, 2002; Lagoudakis et al., 2005; Lin &
Zheng, 2005).
In this paper we consider a dynamic two-sided market for a single commodity, for instance a unit of a resource (e.g. time on a computer, some quantity of memory chips) or
a task to perform (e.g. a standard database query to execute, a location to visit). Each
agent, whether buyer or seller, arrives dynamically and needs to be matched within a time
interval. Cast as a task-allocation problem, a seller can perform the task when allocated
within some time interval and incurs a cost when assigned. A buyer has positive value for
the task being assigned (to any seller) within some time interval. The arrival time, acceptable time interval, and value (negative for a seller) for a trade are all private information
c
2007
AI Access Foundation. All rights reserved.

Bredin, Parkes and Duong

to an agent. Agents are self-interested and can choose to misrepresent all and any of this
information to the market in order to obtain a more desirable price.
The matching problem combines elements of online algorithms and sequential decision
making with considerations from mechanism design. Unlike traditional sequential decision
making, a protocol for this problem must provide incentives for agents to report truthful
information to a match-maker. Unlike traditional mechanism design, this is a dynamic
problem with agents that arrive and leave over time. We model this problem as a dynamic
double auction (DA) for identical items. The match-maker becomes the auctioneer. Each
seller brings a task to be performed during a time window and each buyer brings the
capability to perform a single task. The double-auction setting also is of interest in its own
right as a protocol for matching in a dynamic business-to-business exchange.
Uncertainty about the future coupled with the two-sided nature of the market leads to
an interesting mechanism design problem. For example, consider the scenario where the
auctioneer must decide how (and whether) to match a seller with reported cost of $6 at the
end of its time interval with a present and unmatched buyer, one of which has a reported
value of $8 and one a reported value of $9. Should the auctioneer pair the higher bidder
with the seller? What happens if a seller, willing to sell for $4, arrives after the auctioneer
acts upon the matching decision? How should the matching algorithm be designed so that
no agent can benefit from misstating its earliest arrival, latest departure, or value for a
trade?
Chain provides a general framework that allows a truthful dynamic double auction to
be constructed from a truthful, single-period (i.e. static) double-auction rule. The auctions
constructed by Chain are truthful, in the sense that the dominant strategy for an agent,
whatever the future auction dynamics and bids from other agents, is to report its true
value for a trade (negative if selling) and true patience (maximal tolerance for trade delay)
immediately upon arrival into the market. We also allow for randomized mechanisms and,
in this case, require strong truthfulness: the DA should be truthful for all possible random
coin flips of the mechanism. One of the DAs in the class of auctions implied by Chain
is a dynamic generalization of McAfee’s (1992) canonical truthful, no-deficit auction for a
single period. Thus, we provide the first examples of truthful, dynamic DAs that allow for
dynamic price competition between buyers and sellers.1
The main technical challenge presented by dynamic DAs is to provide truthfulness without incurring a budget deficit, while handling uncertainty about future trade opportunities.
Of particular concern is to ensure that an agent does not indirectly affect its price through
the effect of its bid on the prices faced by other agents and thus other supply and demand
in the market. We need to preclude this because the availability of trades depends on the
price faced by other agents. For example, a buyer that is required to pay $4 in the DA to
trade might like to decrease the price that a potentially matching seller will receive from $6
to $3 to allow for trade.
Chain is a modular approach to auction design, which takes as a building block a singleperiod matching rule and provides a method to invoke the rule in each of multiple periods
while also providing for truthfulness. We characterize properties that a well-defined single1. The closest work in the literature is due to Blum et al. (2006), who present a truthful, dynamic DA for
our model that matches bids and asks based on a price sampled from some bid-independent distribution.
We compare the performance of our schemes with this scheme in Section 6.

134

Chain: An Online Double Auction

period matching rule must satisfy in order for Chain to be truthful. We further identify the
technical property of strong no-trade, with which we can isolate agents that fail to trade in
the current period but can nevertheless survive and be eligible to trade in a future period.
An auction designer defines the strong no-trade predicate, in addition to providing a welldefined single-period matching rule. Instances within this class include those constructed
in terms of both “price-based” matching rules and “competition-based” matching rules.
Both can depend on history and be adaptive, but only the competition-based rules use the
active bids and asks to determine the prices in the current period, facilitating a more direct
competitive processes.
In proving that Chain, when combined with a well-defined matching rule and a valid
strong no-trade predicate, is truthful we leverage a recent price-based characterization for
truthful online mechanisms (Hajiaghayi et al., 2005). We also show that the pricing and
matching rules defined by Chain are unique amongst the family of mechanisms that are
constructed with a single-period matching rule as a building block. Throughout our work we
assume that a constant limits every buyer and seller’s patience. To motivate this assumption
we provide a simple environment in which no truthful, no-deficit DA can implement some
constant fraction of the number of the efficient trades, for any constant.
We adopt allocative efficiency as our design objective, which is to say auction protocols
that maximize the expected total value from the sequence of trades. We also consider net
efficiency, wherein any net outflow of payments to the marketmaker is also accounted for in
considering the quality of a design. Experimental results explore the allocative efficiency of
Chain when instantiated to various single-period matching rules and for a range of different
assumptions about market volatility and maximal patience. For a baseline we consider the
efficiency of a standard (non-truthful) open outcry DA populated with simple adaptive
trading agents modeled after “zero-intelligence plus” (ZIP) agents (Cliff & Bruten, 1998;
Preist & van Tol, 1998). We also compare the efficiency of Chain with that of a truthful
online DA due to Blum et al. (2006), which selects a fixed trading price to guarantee
competitiveness in an adversarial model.
From within the truthful mechanisms we find that adaptive, price-based instantiations
of Chain are the most effective for high arrival intensity and low volatility. Even defining
a single, well-chosen price that is optimized for the market conditions can be reasonably
effective in promoting efficient trades in low volatility environments. On the other hand,
for medium to low arrival intensity and medium to high volatility we find that the Chainbased DAs that allow for dynamic price competition, such as the McAfee-based rule, are
most efficient. The same qualitative observations hold whether one is interested in allocative efficiency or net efficiency, although the adaptive, price-based methods have better
performance in terms of net efficiency. The Blum et al. (2006) rule fairs poorly in our
tests, which is perhaps unsurprising given that it is optimized for worst-case performance
in an adversarial setting. When populated with ZIP agents, we find that non-truthful DAs
can provide very good efficiency in low volatility environments but poor performance in
high volatility environments. The good performance of the ZIP-based market occurs when
agents learn to bid approximately truthfully; i.e., when the market operates as if truthful,
but without incurring the stringent cost (e.g., through trading constraints) of imposing
truthfulness explicitly. An equilibrium analysis is available only for the truthful DAs; we
135

Bredin, Parkes and Duong

have no way of knowing how close the ZIP agents are to playing an equilibrium, and note
that the ZIP agents do not even consider performing time-based manipulations.
1.1 Outline
Section 2 introduces the dynamic DA model, including our assumptions, and presents
desiderata for online DAs and a price-based characterization for the design of truthful
dynamic auctions. Section 3 defines the Chain algorithm together with the building block
of a well-defined, single-period matching rule and the strong no-trade predicate. Section 4
gives a number of instantiations to both price-based and competition-based matching rules,
including a general method to define the strong no-trade predicate given a price-based instantiation. Section 5 proves truthfulness, no-deficit and feasibility of the Chain auctions
and also establishes their uniqueness amongst auctions constructed from the same singleperiod matching-rule building block. The importance of the assumption about maximal
agent patience is established. Section 6 presents our empirical analysis, including a description of the simple adaptive agents that we use to populate a non-truthful open-outcry DA
and provide a benchmark. Section 7 gives related work. In Section 8 we conclude with a
discussion about the merits of truthfulness in markets and present possible extensions.

2. Preliminaries: Basic Definitions
Consider a dynamic auction model with discrete, possibly infinite, time periods T =
{1, 2, . . .}, indexed by t. The double auction (DA) provides a market for a single commodity.
Agents are either buyers or sellers interested in trading a single unit of the commodity. An
agent’s type, θi = (ai , di , wi ) ∈ Θi , where Θi is the set of possible types for agent i, defines
an arrival ai , departure di , and value wi ∈ R for trade. If the agent is a buyer, then wi > 0.
If the agent is a seller, then wi ≤ 0. We assume a maximal patience K, so that di ≤ ai + K
for all agents.
The arrival time models the first time at which an agent learns about the market or
learns about its value for a trade. Thus, information about its type is not available before
period ai (not even to agent i) and the agent cannot engage in trade before period ai . The
departure time, di , models the final period in which a buyer has positive value for a trade,
or the final period in which a seller is willing to engage in trade. We model risk-neutral
agents with quasi-linear utility, wi − p when a trade occurs in t ∈ [ai , di ] and payment p
is collected (with p < 0 if the agent is a seller). Agents are rational and self-interested,
and act to maximize expected utility. By assumption, sellers have no utility for payments
received after their true departure period.
Throughout this paper we adopt bid to refer, generically, to a claim that an agent –
either a buyer or a seller – makes to a DA about its type. In addition, when we need to be
specific about the distinction between claims made by buyers and claims made by sellers
we refer to the bid from a buyer and the ask from a seller.
2.1 Example
Consider the following naive generalization of the (static) trade-reduction DA (Lavi & Nisan,
2005; McAfee, 1992) to this dynamic environment. A bid from an agent is a claim about
136

Chain: An Online Double Auction

its type θ̂i = (âi , dˆi , ŵi ), necessarily made in period t = âi . Bids are active while t ∈ [âi , dˆi ]
and no trade has occurred.
Then in each period t, use the trade-reduction DA to determine which (if any) of the
active bids trade and at what price. These trades occur immediately. The trade-reduction
DA (tr-DA) works as follows: Let B denote the set of bids and S denote the set of asks.
Insert a dummy bid with value +∞ into B and a dummy ask with value 0 into S. When
|B| ≥ 2 and |S| ≥ 2 then sort B and S in order of decreasing value. Let ŵb0 ≥ ŵb1 ≥ . . . and
ŵs0 ≥ ŵs1 ≥ . . . denote the bid and ask values with (b0 , s0 ) denoting the dummy bid-ask
pair. Let m ≥ 0 index the last pair of bids and asks to clear in the efficient trade, such that
ŵbm + ŵsm ≥ 0 and ŵbm+1 + ŵsm+1 < 0. When m ≥ 2 then bids {b1 , . . . , bm−1 } and asks
{s1 , . . . , sm−1 } trade and payment ŵbm is collected from each winning buyer and payment
−ŵsm is made to each winning seller.
First consider a static tr-DA with the following bids and asks:
S
B
i ŵi i ŵi
b∗1 15 s∗1 -1
b∗2 10 s∗2 -1
b∗3 4 s∗3 -2
b4 3 s4 -2
s5 -5
The line indicates that bids (1–4) and asks (1–4) could be matched for efficient trade.
By the rules of the tr-DA, bids (1–3) and asks (1–3) trade, with payments $3 collected from
winning buyers and payment $2 made to winning sellers. The auctioneer earns a profit of
$3. The asterisk notation indicates the bids and asks that trade. The tr-DA is truthful, in
the sense that it is a dominant-strategy for every agent to report its true value whatever
the reports of other agents. For intuition, consider the buy-side. The payment made by
winners is independent of their bid price while the losing bidder could only win by bidding
more than $4, at which point his payment would be $4 and more than his true value.
Now consider a dynamic variation with buyer types {(1, 2, 15), (1, 2, 10), (1, 2, 4), (2, 2, 3)}
and seller types {(1, 2, −1), (2, 2, −1), (1, 1, −2), (2, 2, −2), (1, 2, −5)}. When agents are truthful, the dynamic tr-DA plays out as follows:
period 1
period 2
S
B
S
B
i ŵi i ŵi
i ŵi i ŵi
b∗1 15 s∗1 -1
b∗2 10 s∗2 -1
b3 4 s4 -2
b2 10 s3 -2
b3 4 s5 -5
b4 3 s5 -5
In period 1, buyer 1 and seller 1 trade at payments of $10 and $2 respectively. In
period 2, buyer 2 and seller 2 trade at payments of $4 and $2 respectively. But now we can
construct two kinds of manipulation to show that this dynamic DA is not truthful. First,
buyer 1 can do better by delaying his reported arrival until period 2:
137

Bredin, Parkes and Duong

period 1
period 2
S
B
S
B
i ŵi i ŵi
i ŵi i ŵi
b∗1 15 s∗2 -1
b∗2 10 s∗1 -1
b3 4 s4 -2
b3 4 s3 -2
s5 -5
b4 3 s5 -5
Now, buyer 2 trades in period 1 and does not set the price to buyer 1 in period 2. Instead,
buyer 1 now trades in period 2 and makes payment $4.
Second, buyer 3 can do better by increasing his reported value:
period 1
period 2
S
B
S
B
i ŵi i ŵi
i ŵi i ŵi
b∗3 6 s∗2 -1
b∗1 15 s∗1 -1
∗
∗
b4 3 s4 -2
b2 10 s3 -2
b3 6 s5 -5
s5 -5
Now, buyers 1 and 2 both trade in period 1 and this allows buyer 3 to win (at a price
below his true value) in period 2. This is a particularly interesting manipulation because
the agent’s manipulation is by increasing its bid above its true value. By doing so, it allows
more trades to occur and makes the auction less competitive in the next period.
2.2 Dynamic Double Auctions: Desiderata
We consider only direct-revelation, dynamic DAs that restrict the message that an agent
can send to the auctioneer to a single, direct claim about its type. We also consider “closed”
auctions so that an agent receives no feedback before reporting its type and cannot condition
its strategy on the report of another agent.2
Given this, let θ t denote the set of agent types reported in period t, θ = (θ 1 , θ 2 , . . . , θ t , . . . , )
denote a complete type profile (perhaps unbounded), and θ ≤t denote the type profile restricted to agents with (reported) arrival no later than period t. A report θ̂i = (âi , dˆi , ŵi )
represents a commitment to buy (sell) one unit of the commodity in any period t ∈ [âi , dˆi ]
for a payment of at most ŵi . Thus, if a seller reports a departure time dˆi > di , it must
commit to complete a trade that occurs after her true departure and even though a seller
is modeled as having no utility for payments received after her true departure.
A dynamic DA, M = (π, x), defines an allocation policy π = {π t }t∈T and payment
policy x = {xt }t∈T , where πit (θ ≤t ) ∈ {0, 1} indicates whether or not agent i trades in period
t given reports θ ≤t, and xti (θ ≤t ) ∈ R indicates a payment made by agent i, negative if this is
a payment received by the agent. The auction rules can also be stochastic, so that πit (θ ≤t )
and xti (θ ≤t ) are random variables. For a dynamic DA to be well defined, it must hold that
πit (θ ≤t ) = 1 in at most one period t ∈ [ai , di ] and zero otherwise, and the payment collected
from agent i is zero except in periods t ∈ [ai , di ].
In formalizing the desiderata for dynamic DAs, it will be convenient to adopt (π(θ), x(θ))
to denote the complete sequence of allocation decisions given reports θ, with shorthand
2. The restriction to direct-revelation, online mechanisms is without loss of generality when combined with
a simple heart-beat message from an agent to indicate its presence in any period t during its reported
arrival-departure interval. See the work of Pai and Vohra (2006) and Parkes (2007).

138

Chain: An Online Double Auction

πi (θ) ∈ {0, 1} and xi (θ) ∈ R to indicate whether agent i trades during its reported arrivaldeparture interval, and the total payment made by agent i, respectively. By a slight abuse
of notation, we write i ∈ θ ≤t to denote that agent i reported a type no later than period t.
Let B denote the set of buyers and S denote the set of sellers.
We shall require that the dynamic DA satisfies no-deficit, feasibility, individual-rationality
and truthfulness. No-deficit ensures that the auctioneer has a cash surplus in every period:
Definition 1 (no-deficit) A dynamic DA, M = (π, x) is no-deficit if:
X
X
′
′
xti (θ ≤t ) ≥ 0, ∀t, ∀θ

(1)

i∈θ ≤t t′ ∈[ai ,min(t,di )]

Feasibility ensures that the auctioneer does not need to take a short position in the
commodity traded in the market in any period:
Definition 2 (feasible trade) A dynamic DA, M = (π, x) is feasible if:
X
X
X
X
′
′
′
′
πit (θ ≤t ) ≥ 0, ∀t, ∀θ
πit (θ ≤t ) −
i∈θ ≤t ,i∈S t′ ∈[ai ,min(t,di )]

(2)

i∈θ ≤t ,i∈B t′ ∈[ai ,min(t,di )]

This definition of feasible trade assumes that the auctioneer can “hold” an item that
is matched between a seller-buyer pair, for instance only releasing it to the buyer upon
his reported departure. See the remark concluding this section for a discussion of this
assumption.
Let vi (θi , π(θi′ , θ−i )) ∈ R denote the value of an agent with type θi for the allocation
decision made by policy π given report (θi′ , θ−i ), i.e. vi (θi , π(θi′ , θ−i )) = wi if the agent
trades in period t ∈ [ai , di ] and 0 if it trades outside of this interval and is a buyer, or −∞ if
it trades outside of this interval and is a seller. Individual-rationality requires that agent i’s
utility is non-negative when it reports its true type, whatever the reports of other agents:
Definition 3 (individual-rational) A dynamic DA, M = (π, x) is individual-rational
(IR) if vi (θi , π(θ)) − xi (θ) ≥ 0 for all i, all θ.
In order to define truthfulness, we introduce notation C(θi ) ⊆ Θi for θi ∈ Θi to denote
the set of available misreports to an agent with true type θi . In the standard model adopted
in offline mechanism design, it is typical to assume C(θi ) = Θi with all misreports available.
Here, we shall assume no early-arrival misreports, with C(θi ) = {θ̂i = (âi , dˆi , ŵi ) : ai ≤ âi ≤
dˆi }. This assumption of limited misreports is adopted in earlier work on online mechanism
design (Hajiaghayi et al., 2004), and is well-motivated when the arrival time is the first
period in which a buyer first decides to acquire an item or the period in which a seller first
decides to sell an item.
Definition 4 (truthful) Dynamic DA, M = (π, x), is dominant-strategy incentivecompatible, or truthful, given limited misreports C if:
′
′
′
′
vi (θi , π(θi , θ−i
)) − xi (θi , θ−i
) ≥ vi (θi , π(θ̂i , θ−i
)) − xi (θ̂i , θ−i
).
′ ∈ C(θ ), all θ
for all θ̂i ∈ C(θi ), all θi , all θ−i
−i
−i ∈ Θ−i .

139

Bredin, Parkes and Duong

This is a robust equilibrium concept: an agent maximizes its utility by reporting its
true type whatever the reports of other agents. Truthfulness is useful because it simplifies
the decision problem facing bidders: an agent can determine its optimal bidding strategy
without a model of either the auction dynamics or the other agents. In the case that
the allocation and payment policy is stochastic, then we adopt the requirement of strong
truthfulness so that an agent maximizes its utility whatever the random sequence of coin
flips within the auction.
Remark. The flexible definition of feasibility, in which the auctioneer is able to take a
long position in the commodity, allows the auctioneer to time trades by receiving the unit
sold by a seller in one period but only releasing it to a buyer in a later period. This allows
for truthfulness in environments in which bidders can overstate their departure period. In
some settings this is an unreasonable requirement, however, for instance when the commodity represents a task that is performed, or because a physical good is being traded
in an electronic market.3 In these cases, the definition of feasibility strengthened to require exact trade-balance in every period. The tradeoff is that available misreports must
be further restricted, with agents limited to reporting no late-departures in addition to no
early-arrivals (Lavi & Nisan, 2005; Hajiaghayi et al., 2005). For the rest of the paper we
work in the “relaxed feasibility, no early-arrival” model. The Chain framework can be immediately extended to the “strong-feasibility, no early-arrival and no late-departure” model
by executing trades immediately rather than delaying the trade until a buyer’s departure.

3. Chain: A Framework for Truthful Dynamic DAs
Chain provides a general algorithmic framework with which to construct truthful dynamic
DAs from well-defined single-period matching rules, such as the tr-DA rules described in
the earlier section.
Before introducing Chain we need a few more definitions: Bids reported to Chain are
active while t ≤ dˆi (for reported departure period dˆi ), and while the bid is unmatched
and still eligible to be matched. In each period, a single-period matching rule is used to
determine whether any of the active bids will trade and also which (if any) of the bids that
do not match will remain active in the next period.
Now we define the building blocks, well-defined single-period matching rules, and introduce the important concept of a strong no-trade predicate, which is defined for a singleperiod matching rule.
3.1 Building Block: A Single-Period Matching Rule
t
n
In defining a matching rule, it is helpful to adopt bt ∈ Rm
>0 and s ∈ R≤0 to denote the
active bids and active asks in period t, where there are m ≥ 0 and n ≥ 0 bids and asks
respectively. The bids and asks that were active in earlier periods but are no longer active
form the history in period t, denoted H t ∈ Rh where h ≥ 0 is the size of the history.
A single-period matching rule (hereafter a matching rule), Mmr = (πmr , xmr ) defines
an allocation rule πmr (H t , bt , st , ω) ∈ {0, 1}(m+n) and a payment rule xmr (H t , bt , st , ω) ∈

3. Note that if the task is a computational task, then tasks can be handled within this model by requiring
that the seller performs the task when it is matched but with a commitment to hold onto the result until
the matched buyer is ready to depart.

140

Chain: An Online Double Auction

function SimpleMatch(H t ,bt ,st )
matched := ∅
pt := mean(|H t |)
while (bt 6= ∅)&(st 6= ∅) do
i := 0, bi := −ǫ, j := 0, sj := −∞
while (bi < pt )&(bt 6= ∅) do
i := random(bt ), bt := bt \ {i}
end while
while (sj < −pt )&(st 6= ∅) do
j := random(st ), st := st \ {j}
end while
if (i 6= 0)&(j 6= 0) then
matched := matched ∪ {(i, j)}
end if
end while
end function

Figure 1: A well-defined matching rule defined in terms of the mean bid price in the history.
R(m+n) . Here, we include random event ω ∈ Ω to allow explicitly for stochastic matching
and allocation rules.
Definition 5 (well-defined matching rule) A matching rule Mmr = (πmr , xmr ) is welldefined when it is strongly truthful, no-deficit, individual-rational, and strong-feasible.
Here, the properties of truthfulness, no-deficit, and individual-rationality are exactly
the single-period specializations of those defined in the previous section. For instance,
a matching rule is truthful in this sense when the dominant strategy for an agent in a
DA defined with this rule, and in a static environment, is to bid truthfully and for all
possible random events ω. Similarly for individual-rationality. No-deficit requires that the
total payments are always non-negative. Strong-feasibility requires that exactly the same
number of asks are accepted as bids, again for all random events.
One example of a well-defined matching rule is the tr-DA, which is invariant to the
history of bids and asks. For an example of a well-defined, adaptive (history-dependent)
and price-based matching rule, consider procedure SimpleMatch in Figure 1. The SimpleMatch matching rule computes the mean of the absolute value of the bids and asks in
the history H t and adopts this as the clearing price in the current period. It is a stochastic
matching rule because bids and asks are picked from the sets bt and st at random and
offered the price. We can reason about the properties of SimpleMatch as follows:
(a) truthful: the price pt is independent of the bids and the probability that a bid (or
ask) is matched is independent of its bid (or ask) price
(b) no-deficit: payment pt is collected from each matched buyer and made to each
matched seller
(c) individual-rational: only bids bi ≥ pt and asks sj ≥ −pt are accepted.
(d) feasible: bids and asks are introduced to the “matched” set in balanced pairs
141

Bredin, Parkes and Duong

3.2 Reasoning about Trade (Im)Possibility
In addition to defining a matching rule Mmr , we allow a designer to (optionally) designate
a subset of losing bids that satisfy a property of strong no-trade. Bids that satisfy strong
no-trade are losing bids for which trade was not possible at any bid price (c.f. ask price
for asks), and moreover for which additional independence conditions hold between bids
provided with this designation.
We first define the weaker concept of no-trade.
In the following, notation
πmr,i (H t , bt , st , ω|ŵi ) indicates the allocation decision made for bid (or ask) i when its bid
(ask) price is replaced with ŵi :
Definition 6 (no-trade) Given matching rule Mmr = (πmr , xmr ) then the set of agents,
NTt , for which no trade is possible in period t and given random events ω are those for
which πmr,i (H t , bt , st , ω|ŵi ) = 0, for every ŵi ∈ R>0 when i ∈ bt and for every ŵi ∈ R≤0
when i ∈ st .
It can easily happen that no trade is possible, for instance when the agent is a buyer
and there are no sellers on the other side of the market. Let SNTt ⊆ NTt denote the set
of agents designated with the property of strong no-trade. Unlike the no-trade property,
strong no-trade need not be uniquely defined for a matching rule. To be valid, however, the
construction offered by a designer for strong no-trade must satisfy the following:
Definition 7 (strong no-trade) A construction for strong no-trade, SNTt ⊆ NTt , is
valid for a matching rule when:
(a) ∀i ∈ NTt with dˆi > t, whether or not i ∈ SNTt is unchanged for all alternate reports
θi′ = (a′i , d′i , wi′ ) 6= θ̂i while d′i > t,
(b) ∀i ∈ SNTt with dˆi > t, the set {j : j ∈ SNTt , j 6= i, dˆj > t} is unchanged for all
reports θi′ = (a′i , d′i , wi′ ) 6= θ̂i while d′i > t, and independent even of whether or not agent i is
present in the market.
The strong no-trade conditions must be checked only for agents with a reported departure later than the current period. Condition (a) requires that such an agent in NTt cannot
affect whether or not it satisfies the strong no-trade predicate as long as it continues to
report a departure later than the current period. Condition (b) is defined recursively, and
requires that if such an agent is identified as satisfying strong no-trade, then its own report
must not affect the designation of strong no-trade to other agents, with reported departure
later than the current period, while it continues to report a departure later than the current
period – even if it delays its reported arrival until a later period.
Strong no-trade allows for flexibility in determining whether or not a bid is eligible for
matching. Specifically, only those bids that satisfy strong no-trade amongst those that lose
in the current period can remain as a candidate for trade in a future period. The property
is defined to ensure that such a “surviving” agent does not, and could not, affect the set of
other agents against which it competes in future periods.
142

Chain: An Online Double Auction

Example 1 Consider the tr-DA matching rule defined earlier with bids and asks
B
S
ŵi
i ŵi i
∗
∗
b1 10 s1 −4
b2 8 s2 −6
b3 6 s3 −8
Bid 1 and ask 1 trade at price 8 and −6 respectively. NTt = ∅ because bids 2 and 3 could
each trade if they had (unilaterally) submitted a bid price of greater than 10. Similarly for
asks 2 and 3. Now consider the order book
S
B
i ŵi i
ŵi
b1 8 s1 −6
b2 7 s2 −10
b3 2 s3 −12
No trade occurs. In this case, NTt = {b1 , b2 , b3 , s1 }. No trade is possible for any bids, even
bids 2 and 3, because ŵb1 + ŵs2 = 8 − 10 < 0. But, trade is possible for asks 2 and 3, because
ŵb2 + ŵs1 = 7 − 6 ≥ 0 and either ask could trade by submitting a low enough ask price.
Example 2 Consider the tr-DA matching rule and explore possible alternative constructions for strong no-trade.
(i) Dictatorial: in each period t, identify an agent that could be present in the period in
a way that is oblivious to all agent reports. Let i denote the index of this agent. If i ∈ NTt ,
then include SNTt = {i}. Strong no-trade condition (a) is satisfied because whether or not i
is selected as the “dictator” is agent-independent, and given that it is selected, then whether
or not trade is possible is agent-independent. Condition (b) is trivially satisfied because
|SNTt | = 1 and there is no cross-agent coupling to consider.
(ii) SNTt := NTt . Consider the order book
S
B
i ŵi i
ŵi
b1 3 s1 −4
b2 2 s2 −6
b3 1 s3 −8
Suppose all bids and asks remain in the market for at least one more period. Clearly,
NTt = {b1 , b2 , b3 , s1 , s2 , s3 }. Consider the candidate construction SNTt = NTt . Strong notrade condition (a) is satisfied because whether or not i is in set NTt is agent-independent.
Condition (b) is not satisfied, however. Consider bid 2. If bid 2’s report had been 8 instead
of 2 then trade would be possible for bids 1 and 3, and SNTt = NTt = {b2 , s1 , s2 , s3 }. Thus,
whether or not bids 1 and 3 satisfy the strong no-trade predicate depends on the value of bid
2. This is not a valid construction for strong no-trade for the tr-DA matching rule.
(iii) SNTt = NTt if |bt | < 2 or |st | < 2, and SNTt = ∅ otherwise. As above, strong
no-trade condition (a) is immediately satisfied. Moreover, condition (b) is now satisfied
because trade is not possible for any bid or ask irrespective of bid values because there are
simply not enough bids or asks to allow for trade with tr-DA (which needs at least 2 bids
and at least 2 asks).
143

Bredin, Parkes and Duong

Figure 2: The decision process in Chain upon arrival of a new bid. If admitted, then the bid
participates in a sequence of matching events while it remains unmatched and in the
strong no-trade set. The bid matches at the first available opportunity and is priced
immediately.

Example 3 Consider a variant of the SimpleMatch matching rule, defined with fixed
price 9. We can again ask whether SNTt := NTt is a valid construction for strong no-trade.
Throughout this example suppose all bids and asks remain in the market for at least one
more period. First consider a bid with ŵb1 = 8 and two asks with values ŵs1 = −6 and
ŵs2 = −7. Here, NTt = {s1 , s2 } because the asks cannot trade whatever their price since
the bid is not high enough to meet the fixed trading price of 9. Moreover, SNTt = {s1 , s2 }
is a valid construction; strong no-trade condition (a) is satisfied as above and condition (b)
is satisfied because whether or not ask 2 is in NTt (and thus SNTt ) is independent of the
price on ask 1, and vice versa. But consider instead a bid with ŵb1 = 8 and an ask with
ŵs1 = −10. Now, NTt = {b1 , s1 } and SNTt = {b1 , s1 } is our candidate strong no-trade set.
However if bid 1 had declared value 10 instead of 8 then NTt = {b1 } and ask 1 drops out of
SNTt . Thus, strong no-trade condition (b) is not satisfied.
We see from the above examples that it can be quite delicate to provide a valid, nontrivial construction of strong no-trade. Note, however, that SNTt = ∅ is a (trivial) valid
construction for any matching rule. Note also that the strong no-trade conditions (a) and
(b) require information about the reported departure period of a bid. Thus, while the
matching rules do not use temporal information about bids, this information is used in the
construction for strong no-trade.
3.3 Chain: From Matching Rules to Truthful, Dynamic DAs
The control flow in Chain is illustrated in Figure 2. Upon arrival of a new bid, an admission
decision is made and bid i is admitted if its value ŵi is at least its admission price qi . An
admitted bid competes in a sequence of matching events, where a matching event simply
applies the matching rule to the set of active bids and asks. If a bid fails to match in some
period and is not in the strong no-trade set (i ∈
/ SNTt ), then it is priced out and leaves the
market without trading. Otherwise, if it is still before its departure time (t ≤ dˆi ), then it is
available for matching in the next period.
Each bid is always in one of three states: active, matched or priced-out. Bids are active
if they are admitted to the market until t ≤ dˆi , or until they are matched or priced-out. An
144

Chain: An Online Double Auction

active bid becomes matched in the first period (if any) when it trades in the single-period
matching rule. An active bid is marked as priced-out in the first period in which it loses
but is not in the strong no-trade set. As soon as a bid is no longer active, it enters the
history, H t , and the information about its bid price can be used in defining matching rules
for future periods.
Let E t denote the set of bids that will expire in the current period. A well-defined
matching rule, when coupled with a valid strong no-trade construction, must provide Chain
with the following information, given history H t , active bids bt and active asks st , and
expiration set E t in period t:
(a) for each bid or ask, whether it wins or loses
(b) for each winning bid or ask, the payment collected (negative for an ask)
(c) for each losing bid or ask, whether or not it satisfies the strong no-trade condition
Note that the expiration set E t is only used for the strong no-trade construction. This
information is not made available to the matching rule. The following table summarizes the
use of this information within Chain. Note that a winning bid cannot be in set SNTt :
Lose
Win

¬SNTt
priced-out
matched

SNTt
survive
n/a

We describe Chain by defining the events that occur for a bid upon its arrival into the
market, and then in each period in which it remains active:
Upon arrival: Consider all possible earlier arrival periods t′ ∈ [dˆi −K, âi −1] consistent
with the reported type. There are no such periods to consider if the bid is maximally
′
patient. If the bid would lose and not be in SNTt for any one of these arrival periods
t′ , then it is not admitted. Otherwise, the bid would win in all periods t′ for which
′
i∈
/ SNTt , and define the admission price as:
q(âi , dˆi , θ−i , ω) :=

max

t′ ∈[dˆi −K,âi −1],i∈SNT
/

′

t′

[pti , −∞],

(3)

′

where pti is the payment the agent would have made (negative for a seller) in arrival
period t′ (as determined by running the myopic matching rule in that period). When
′
the agent would lose in all earlier arrival periods t′ (and so i ∈ SNTt for all t′ ), or
the bid is maximally patient, then the admission price defaults to −∞ and the bid is
admitted.
While active: Consider period t ∈ [âi , dˆi ]. If the bid is selected to trade by the myopic
matching rule, then mark it as matched and define final payment:
xti (θ ≤t ) = max(q(âi , dˆi , θ−i , ω), pti ),

(4)

where pti is the price (negative for a seller) determined by the myopic matching rule in
the current period. If this is a buyer, then collect the payment but delay transferring
the item until period dˆi . If this is a seller, then collect the item but delay making
the payment until the reported departure period. If the bid loses and is not in SNTt ,
then mark the bid as priced-out.
145

Bredin, Parkes and Duong

We illustrate Chain by instantiating it to various matching rules in the next section.
In Section 5 we prove that Chain is strongly truthful and no-deficit when coupled with a
well-defined matching rule and a valid strong no-trade construction. We will see that the
delay in buyer delivery and seller payment ensures truthful revelation of a trader’s departure
information. For instance, in the absence of this delay, a buyer might be able to do better by
over-reporting departure information, still trading early enough but now for a lower price.
3.4 Comments
We choose not to allow the single-period matching rules to use the reported arrival and
departure associated with active bids and asks. This maintains a clean separation between
non-temporal considerations (in the matching rules) and temporal considerations (in the
wider framework of Chain). This is also for simplicity. The single-period matching rules
can be allowed to depend on the reported arrival-departure interval, as long as the (singleperiod) rules are monotonic in tighter arrival-departure intervals, in the sense that an agent
that wins for some θ̂i = (âi , dˆi , ŵi ) continues to win and for an improved price if it instead
reports (a′i , d′i , ŵi ) with [a′i , d′i ] ⊂ [âi , dˆi ]. However, whether or not trade is possible must
be independent of the reported arrival-departure interval and similarly for strong no-trade.
Determinations such as these would need to be made with respect to the most patient type
(dˆi − K, dˆi , ŵi ) given report θ̂i = (âi , dˆi , ŵi ).

4. Practical Instantiations: Price-Based and Competition-Based Rules
In this section we offer a number of instantiations of the Chain online DA framework. We
present two different classes of well-defined matching rules: those that are price-based and
compute simple price statistics based on the history which are then used for matching, and
those that we refer to as competition-based and leverage the history but also consider direct
competition between the active bids and asks in any period. In each case, we establish that
the matching rules are well-defined and provide a valid strong no-trade construction.
4.1 Price-Based Matching Rules
Each one of these rules constructs a single price, pt , in period t based on the history H t of
earlier bids and asks that are traded or expired. For this purpose we define variations on a
real valued statistic, ξ(H t ), that is used to define this price given the history. Generalizing
the SimpleMatch procedure, as introduced in Section 3.1, the price pt is used to determine
the trades in period t. We also provide a construction for strong no-trade in this context.
The main concern in setting prices is that they may be too volatile, with price updates
driving the admission price higher (via the max operator in the admission rule of Chain)
and having the effect of pricing bids and asks out of the market. We describe various forms
of smoothing and windowing, all designed to provide adaptivity while dampening shortterm variations. In each case, the parameters (e.g. the smoothing factor, or the window
size) can be determined empirically through off-line tuning.
We experiment with five price variants:
History-EWMA: Exponentially-weighted moving average. The bid history, H t , is used
to define price pt in period t, computed as pt := λ ξ(H t ) + (1 − λ)pt−1 , where λ ∈ (0, 1] is a
146

Chain: An Online Double Auction

smoothing constant and ξ(H t ) is a statistic defined for bids and asks that enter the history
in period t. Experimentally we find that the mean statistic, ξ mean (H t ), of the absolute
values of bids and asks that enter the history performs well with λ of 0.05 or lower for most
scenarios that we test. For cases in which ξ(H t ) is not well-defined because of too few (or
zero) new bids or asks, then we set pt := pt−1 .
History-median: Compute price pt from a statistic over a fixed-size window of the most
recent history, pt := ξ(H t , ∆) where ∆ is the window-size, i.e. defining bids introduced
to history H t in periods [t − ∆, . . . , t]. Experimentally, we find that the median statistic,
ξ median (H t , ∆), of the absolute bid and ask values performs well for the scenarios we test,
with the window size depending inversely with the volatility of agents’ valuations. Typically,
we observe optimal window sizes of 20 and 150, depending on volatility. For cases in which
ξ(H t , ∆) is not well-defined because of too few (or zero) new bids or asks, then we set
pt := pt−1 .
History-clearing: Identical to the history-median rule except the statistic ξ(H t , ∆) is
defined as (bm − sm )/2 where bm and sm are the lowest value pair of trades that would be
executed in the efficient (value-maximizing) trade given all bids and asks to enter history
H t in periods [t − ∆, . . . , t]. Empirically, we find similar optimal window sizes for historyclearing as for history-median.
History-McAfee: Define the statistic ξ(H t , ∆) to represent the McAfee price, defined in
Section 4.2, for the bids in H t had they all simultaneously arrived.
Fixed price: This simple rule computes a single fixed price pt := p∗ for all trading periods,
with the price optimized offline to maximize the average-case efficiency of the dynamic DA
given Chain and the associated single-period matching rule that leverages price p∗ as the
candidate trading price.
For each pricing variant, procedure Match (see Figures 3–4) is used to determine which
bids win (at price pt ), which lose, and, of those that lose, which satisfy the strong notrade predicate. The subroutine used to determine the current price is referred to as
determineprice in Match. We provide as input to Match the set E t in addition to
(H t , bt , st ) because Match also constructs the strong no-trade set, and E t is used exclusively for this purpose.
The proof of the following lemma is technical and is postponed until the Appendix.
Lemma 1 Procedure Match defines a valid strong no-trade construction.
Theorem 1 Procedure Match defines a well-defined matching rule and a valid strong notrade construction.
Proof: No-deficit, feasibility, and individual-rationality are immediate by the construction
of Match since bids and asks are added to matched in pairs, with the same payment, and
only if the payment is less than or equal to their value. Truthfulness is also easy to see: the
order with which a bid (or ask) is selected is independent of its bid price, and the price it
faces, when selected, is independent of its bid. If the price is less than or equal to its bid,
then whether or not it trades depends only on its order. The rest of the claim follows from
Lemma 1.

147

Bredin, Parkes and Duong

function Match(H t ,bt ,st ,E t )
matched := ∅, lose := ∅, NTt := ∅, SNTt := ∅
stop := false
pt := determineprice(H t )
while ¬ stop do
i := 0, j := 0, checkedB := ∅, checkedS := ∅
while ((checkedB ⊂ bt )&(i=0)) ∨ ((checkedS ⊂ st )&(j=0)) do
if (i = 0)&(j = 0) then
S
k := random(bt \ checkedB st \ checkedS )
else if (i = 0) then
k := random(bt \ checkedB )
else if (j = 0) then
k := random(st \ checkedS )
end if
if (k ∈ bt ) then
checkedB := checkedB ∪ {k}
if (bk ≥ pt ) then
i := k
end if
else
checkedS := checkedS ∪ {k}
if (sk ≥ −pt ) then
j := k
end if
end if
end while
if (i 6= 0)&(j 6= 0) then S
matched := S
matched
{(i, j)} S
lose := lose
(checkedB \ {i})
(checkedS \ {j})
t
t
t
t
b := b \ checkedB , s := s \ checkedS
else
stop := true
end if
end while
end function
Figure 3: The procedure used for single-period matching in applying Chain to the pricebased rules. The algorithm continues in Figure 4.

148

Chain: An Online Double Auction

function Match (continued)(H t ,bt ,st ,E t )
if (i 6= 0)&(j =S
0) then
lose := lose st , NTt := bt
if (∃k ∈ bt · ((bk ≥ pt )&(dˆk = t))) ∨ (∀k ∈ st · (dˆk = t)) then
SNTt := bt
else
SNTt := bt \ checkedB
end if
else if (j 6= 0)&(i
S = 0) then
lose := lose bt , NTt := st
if (∃k ∈ st · ((sk ≥ −pt )&(dˆk = t))) ∨ (∀k ∈ bt · (dˆk = t)) then
SNTt := st
else
SNTt := st \ checkedS
end if
else if (i = 0)&(j
S t = 0) then
t
t
NT := b s
ˆk = t)) ∨ (∀k ∈ st · (dˆk = t)) then
if (∀k ∈ bt · (dS
t
t
SNT := b st
end if
end if
end function

⊲I

⊲ I-a

⊲ II

⊲ III

Figure 4: Continuing procedure from Figure 3 for single-period matching in applying Chain
to the price-based rules.

Example 4 (i) Bid bt = {8}, ask st = {−6}, indexed {1, 2} and price pt = 9. The outer
while loop in Figure 3 terminates with j = 2 and i = 0 in Case II. The bid is marked as
a loser while NTt = {2}. If the bid will depart immediately, then SNTt = {2}, otherwise
SNTt = ∅.
(ii) Bid bt = {8}, asks st = {−6, −7}, indexed {1, 2, 3}, and price pt = 9. Suppose that
ask 2 is selected before ask 3 in the outer while loop. Then the loop terminates with j = 2
and i = 0 in Case II and NTt = {2, 3}. Suppose the bid and asks leave the market later
than this period. Then SNTt = {3} because checkedS = {2}.
(iii) Bid bt = {8} and ask st = {−10}, indexed {1, 2}, price pt = 9 and both the bid and
the ask is patient. The outer while loop terminates with i = 0 and j = 0 in Case III so
that NTt = {1, 2}. However, SNTt = ∅.

4.2 Competition-Based Matching Rules
Each one of these rules determines which bids match in the current period through price
competition between the active bids. We present three variations: McAfee, WindowedMcAfee and Active-McAfee. The latter two rules are hybrid rules in that they leverage
149

Bredin, Parkes and Duong

history of past offers, in smoothing prices generated by the competition-based matching
rules.
McAfee: Use the static DA protocol due to McAfee as the matching rule. Let B denote
the set of bids and S denote the set of asks. If min(|B|, |S|) < 2, then there is no trade.
Otherwise, first insert two dummy bids with value {∞, 0} and two dummy asks with value
{0, −∞} into the set of bids and asks. Let b0 ≥ b1 ≥ . . . ≥ bm and s0 ≥ s1 ≥ . . . ≥
sn . . . denote the bid and ask values with (b0 , s0 ) denoting dummy pair (∞, 0) and (bm , sn )
denoting dummy pair (0, −∞) and ties otherwise broken at random. Let m ≥ 0 index
the last pair of bids and asks to clear in the efficient trade, such that bm + sm ≥ 0 and
bm+1 + sm+1 < 0. When m ≥ 1, consider the following two cases:
m+1
≤ bm and −pm+1 ≤ sm then the first m bids and
• (Case I) If price pm+1 = bm+1 −s
2
asks trade and payment pm+1 is collected from each winning buyer and made to each
winning seller.

• (Case II) Otherwise, the first m − 1 bids and asks trade and payment bm is collected
from each winning buyer and payment −sm is made to each winning seller.
To define NTt , replace a bid that does not trade with a bid reporting a very large value
and see whether this bid trades. To determine whether trade is possible for an ask that
does not trade: replace the ask with an ask reporting value ǫ > 0, some small ǫ. Say
that there is a quorum if and only if there are at least two bids and at least two asks, i.e.
min(|bt |, |st |) ≥ 2. Define strong no-trade as follows: set SNTt := NTt = bt ∪ st when
there is no quorum and SNTt := ∅ otherwise.
Lemma 2 For any bid bi in the McAfee matching rule, then for any other bid (or ask) j
there is some bid b̂i that will make trade possible for bid (or ask) j when there is a quorum.
Proof: Without loss of generality, suppose there are three bids and three asks. Label the
bids (a, c, e) and the asks (b, d, f ), both ordered from highest to lowest so that (a, b) is the
most competitive bid-ask pair. Proceed by case analysis on bids. The analysis is symmetric
for asks and omitted. Let tp(i) ∈ {0, 1} denote whether or not trade is possible for bid i, so
that i ∈ NTt ⇔ tp(i) = 0. For bid a: when b ≥ −(a − d)/2 then tp(c) = tp(e) = 1 and this
inequality can always be satisfied for a large enough a; when a ≥ (c − d)/2 then tp(b) = 1
and when a ≥ (c − b)/2 then tp(d) = tp(f ) = 1, and both of these inequalities are satisfied
for a large enough a. For bid c: when b ≥ −(c − d)/2 then tp(a) = 1 and when, in addition,
c > a, then tp(e) = 1 and each one of these inequalities are satisfied for a large enough c;
similarly when c ≥ (a − d)/2 then tp(b) = 1 and when c ≥ (a − b)/2 then tp(d) = tp(f ) = 1.
Analysis for bid e follows from that for bid c.

Lemma 3 The construction for strong no-trade is valid and there is no valid strong notrade construction that includes more than one losing bid or ask that will not depart in the
current period for any period in which there is a quorum.
Proof: To see that this is a valid construction, notice that strong no-trade condition (a)
holds since any bid (or ask) is always in both NTt and SNTt . Similarly, condition (b)
trivially holds (with the other bids and asks remaining in SNTt even if any bid is not
150

Chain: An Online Double Auction

present in the market). To see that this definition is essentially maximal, consider now that
min(|bt |, |st |) ≥ 2. For contradiction, suppose that two losing bids {i, j} with departure
after the current period are designated as strong no-trade. But, strong no-trade condition
(b) fails because of Lemma 2 because either bid could have submitted an alternate bid price
that would remove the other bid from NTt and thus necessarily also from SNTt .

The construction offered for SNTt cannot be extended even to include one agent selected
at random from the set i ∈ NTt that will not depart immediately, in the case of a quorum.
Such a construction would fail strong no-trade condition (b) when the set NTt contains
more than one bid (or ask) that does not depart in the current period, because bid i’s
absence from the market would cause some other agent to be (randomly) selected as SNTt .

Windowed-McAfee: This myopic matching rule is parameterized on window size ∆.
Augment the active bids and asks with the bids and asks introduced to the history H t in
periods t′ ∈ {t − ∆ + 1, . . . , t}. Run McAfee with this augmented set of bids and asks and
determine which of these bids and asks would trade. Denote this candidate set C. Some
active agents identified as matching in C may not be able to trade in this period because
C can also contain non-active agents.
Let B ′ and S ′ denote, respectively, the active bids and active asks in set C. WindowedMcAfee then proceeds by picking a random subset of min(|B ′ |, |S ′ |) bids and asks to trade.
When |B ′ | =
6 |S ′ |, then some bids and asks will not trade.
Define strong no-trade for this matching rule as:
(i) if there are no active asks but active bids, then SNTt := bt
(ii) if there are no active bids but active asks, then SNTt := st
(iii) if there are fewer than 2 asks or fewer than 2 bids in the augmented bid set, then
SNTt := bt ∪ st ,
and otherwise set SNTt := ∅. In all cases it should be clear that SNTt ⊆ NTt .
Lemma 4 The strong no-trade construction for windowed-McAfee is valid.
Proof: That this is a valid SNT criteria in case (iii) follows immediately from the validity
of the SNT criteria for the standard McAfee matching rule. Consider case (i). Case (ii)
is symmetric and omitted. For strong no-trade condition (a), we see that all bids i ∈ NTt
and also i ∈ SNTt , and whether or not they are designated strong no-trade is independent
of their own bid price but simply because there are no active asks. Similarly, for strong
no-trade condition (b), we see that all bids (and never any asks) are in SNTt whatever the
bid price of any particular bid (and even whether or not it is present).

Empirically, we find that the efficiency of Windowed-McAfee is sensitive to the size of
t
H , but that frequently the best choice is a small window size that includes only the active
bids.
Active-McAfee: Active-McAfee augments the active bids and asks to include all unexpired but traded or priced-out offers. It proceeds as in Windowed-McAfee given this
augmented bid set.
151

Bredin, Parkes and Duong

4.3 Extended Examples
We next provide two stylized examples to demonstrate the matching performed by Chain
using both a price-based and a competition-based matching rule. For both examples, we
assume a maximal patience of K = 2. Moreover, while we describe when Chain determines
that a bid or an ask trades, remember that a winning buyer is not allocated the good until
its reported departure and a winning seller does not receive payment until its reported
departure.
Example 5 Consider Chain using an adaptive, price-based matching rule. The particular
details of how prices are determined are not relevant. Assume that the prices in periods 1
and 2 are {p1 , p2 } = {8, 7} and the maximal patience is three periods. Now consider period
3 and suppose that the order book is empty at the end of period 2 and that the bids and asks
in Table 1 arrive in period 3.
S
B
ŵi dˆi dˆi −K qi
pi SNT? i
ŵi dˆi dˆi −K qi
pi SNT?
b1 * 15 4
2
7
7
N
s1 -1 4
2
-7 n/a
Y
s2 * -3 5
3
−∞ -6.5
N
b2 * 10 3
1
8
8
N
b3 7 3
1
8 n/a
N
s3 -4 3
1
-7 n/a
Y
s4 * -5 4
2
-7 -6.5
N
b4 6 5
3
−∞ n/a
N
s5 -10 5
3
−∞ n/a
Y
i

Table 1: Bids and asks that arrive in period 3. Bids {b1 , b2 } match with asks {s2 , s4 } (as indicated
with a *). Bid b3 is priced-out upon admission because qb3 > ŵb3 (indicated with a strikethrough). The admission price is qi and the payment made by an agent that trades is
pi . Column ‘SNT?’ indicates whether or not the bid or ask satisfies the strong no-trade
predicate. Asks {s1 , s5 } survive into the next period because they are in SNT and have
di > 3.

Bids {b1 , b2 , b4 } and asks {s1 , .., s5 } are admitted. Bid b3 is priced out because qb3 =
max(p1 , p2 , −∞) = max(8, 7, −∞) = 8 > ŵb3 = 7 by Eq. (3). Note that b4 and s5 are
admitted despite low bids (asks) because they have maximal patience and their admission
prices are −∞. Now, suppose that p3 := 6.5 is defined by the matching rule and consider
applying Match to the admitted bids and asks.
Suppose that the bids are randomly ordered as (b4 , b2 , b1 ) and the asks as
(s4 , s2 , s1 , s3 , s5 ). Bid b4 is picked first but priced-out because ŵb4 = 6 < p3 = 6.5. Bid
b2 is tentatively accepted (ŵb2 = 10 ≥ p3 = 6.5) and then ask s4 is accepted (ws4 = −5 ≥
p3 = −6.5). Bid b2 is matched with ask s4 , with payment max(qb2 , p3 ) = max(8, 6.5) = 8
for b2 by Eq. (4) and payment max(qs4 , p3 ) = max(−∞, −6.5) = −6.5 for s4 . Bid b1 is then
tentatively accepted (15 ≥ 6.5) and then matched with ask s2 , which is accepted because
−3 ≥ −6.5. The payments are max(7, 6.5) = 7 for b1 and max(−∞, −6.5) = −6.5 for s2 .
Ask s3 expires but asks s1 and s5 survive and are marked i ∈ SNT in this period because
they were never offered the chance to match with any bid. These asks will be active in period
4.
Note the role of the admission price in truthfulness. Had bid b1 delayed arrival until
period 4, its admission price would be max(p2 , p3 , −∞) = max(7, 6.5) = 7 and its payment
152

Chain: An Online Double Auction

in period 4 (if it matches) at least 7. Similarly, had ask s4 delayed arrival, then its admission
price would be max(−7, −6.5, −∞) = −6.5 and the maximal payment it can receive in period
4 is 6.5.
Example 6 Consider Chain using the McAfee-based matching rule with K = 3 and with
the same bids and asks arriving in period 3. Suppose that the prices in periods 1 and 2 that
would have been faced by a buyer are {p1b , p2b } = {8, 7} and {p1s , p2s } = {−7, −6} for a seller.
These prices are determined by inserting an additional bid (with value ∞) or an additional
ask (with value 0) into the order books in each of periods 1 and 2. We will illustrate this
for period 3. Consider now the bids and asks in period 3 in Table 2.

i wi di
b1 * 15 4
b2 * 10 3
b3 7 3
b4 6 5

S
B
di −K qi
pi SNT? i
wi di di −K qi
pi SNT?
2
7
7
N
s1 * -1 4
2
-6
-4
N
1
8
8
N
s2 * -3 5
3
−∞ -4
N
1
8 n/a
N
s3 -4 3
1
-6 n/a
N
3
−∞ n/a
N
s4 -5 4
2
-6 n/a
N
s5 -10 5
3
−∞ n/a
N

Table 2: Bids and asks that arrive in period 3. Bids {b1 , b2 } match with asks {s1 , s2 } (as indicated
with a *). Bid b3 is priced-out upon admission because qb3 > ŵb3 . The admission price is
qi and the payment made by an agent that trades is pi . Column ‘SNT?’ indicates whether
or not the bid or ask satisfies the strong no-trade predicate. No asks or bids survive into
the next period.

As before bid b3 is not admitted. The myopic matching rule now runs the (static) McAfee
auction rule on bids {b1 , b2 , b4 } and asks {s1 , .., s5 }. Consider bids and asks in decreasing
order of value, the last efficient trade is indexed m = 3 with ŵb4 + ŵs3 = 6 − 4 ≥ 0. But
pm+1 = (0−(−5))/2 = 2.5 (inserting a dummy bid with value 0 as described in Section 4.2).
Price −pm+1 = −2.5 > s3 = −4 and this trade cannot be executed by McAfee. Instead,
buyers {b1 , b2 } trade and face price pbm = ŵb4 = 6 and sellers {s1 , s2 } trade and face price
psm = ŵs3 = −4. Bids b4 and asks {s3 , s4 , s5 } are priced-out and do not survive into the
next round. Ultimately, payment max(qb1 , pbm ) = max(7, 6) = 7 is collected from buyer b1
and payment max(qb2 , pbm ) = max(8, 6) = 8 is collected from buyer b2 . For sellers, payment
max(−6, −4) = −4 and max(−∞, −4) = −4 for s1 and s2 respectively.
The prices p3b and p3s that are used in Eq. (3) to define the admission price for bids and
asks with arrivals in periods 4 and 5 are determined as follows. For the buy-side price, we
introduce an additional bid with bid-price ∞. With this the bid values considered by McAfee
would be (∞, 15, 10, 6, 0) and the ask values would be (−1, −3, −4, −5, −10), where a dummy
bid with value 0 is included on the buy-side. The last efficient pair to trade is m = 4 with
6 − 5 ≥ 0 and pm+1 = (0 − (−10))/2 = 5, which satisfies this bid-ask pair. Therefore the
buy-side price, p3b := 5. On the sell-side, we introduce an additional ask with ask-price
0 so that the bid values considered by McAfee are (15, 10, 6, 0) (again, with a dummy bid
included) and the ask values are (0, −1, −3, −4, −5, −10). This time m = 3 and the last
efficient pair to trade is 6 − 3 ≥ 0. Now pm+1 = (0 − (−4))/2 = 2 and this price does not
153

Bredin, Parkes and Duong

satisfy s2 , with −pm+1 > s2 and price psm+1 = s2 = −3 is adopted. Therefore the sell-side
price, p3s := −3.
Again, we can see that bidder 1 cannot improve its price by delaying its entry until period
4. The admission price for the bidder would be max(p2b , p3b ) = max(7, p3b ) ≥ 7 and thus its
payment in period 4, if it matches, will be at least 7.4 Similarly for ask s1 , which would face
admission price max{p2s , p3s } = max{−6, −4} = −4 and can receive a payment of at most 4
in period 4. We leave it as an exercise for the reader to verify that p3s = −4 if ask s1 delays
its arrival until period 4 (in comparison, p3s = −3 when ask s1 is truthful).
Because the McAfee-based pricing scheme computes a price and clears the order book
following every period in which there are at least two bids and two asks, the bid activity
periods tend to be short in comparison to the adaptive, price-based rules where orders can
be kept active longer when there is an asymmetry in the number of bids and asks in the
market. In fact, one interesting artifact that occurs with adaptive, price-based matching
rules is that the admission-price and SNT can perpetuate this kind of bid-ask asymmetry.
Once the market has more asks than bids, SNT becomes likely for future asks, but not bids.
Therefore, bids are much more likely than asks to be immediately priced out of the market
by failing to meet the admission price constraint.

5. Theoretical Analysis: Truthfulness, Uniqueness, and Justifying
Bounded-Patience
In this section we prove that Chain combined with a well-defined matching rule and a valid
strong no-trade construction generates a truthful, no-deficit, feasible and individual-rational
dynamic DA. In Section 5.2, we establish that uniqueness of Chain amongst dynamic DAs
that are constructed from single-period matching rules as building blocks. In Section 5.3,
we establish the importance of the existence of a maximal bound on bidder patience by
presenting a simple environment in which no truthful, no-deficit DA can implement even a
single trade despite the number of efficient trades can be increased without bound.
5.1 The Chain Mechanism is Strongly Truthful
It will be helpful to adopt a price-based interpretation of a valid single-period matching rule.
Given rule Mmr , define an agent-independent price, zi (H t , At \ i, ω) ∈ R where At = bt ∪ st ,
such that for all i, all bids bt , all asks st , all history H t , and all random events ω ∈ Ω. We
have:
(A1) ŵi − zi (H t , At \ i, ω) > 0 ⇒ πmr,i (H t , bt , st , ω) = 1, and ŵi − zi (H t , At \ i, ω) <
0 ⇒ πmr,i (H t , bt , st , ω) = 0
(A2) payment xmr,i (H t , bt , st , ω) = zi (H t , At \ i, ω) if πmr,i (H t , bt , st , ω) = 1 and
xmr,i (H t , bt , st , ω) = 0 otherwise
4. We can check that p3b := 6 in this case. Suppose that bidder 1 were not present in period 3. Now consider
introducing an additional bid with value ∞ so that the bids values are {∞, 10, 6, 0} (with a dummy bid)
with ask values {−1, −3, −4, −5, −10}. Then m = 3 and pm+1 = (0 − (−5))/2 = 2.5, which does not
support the trade between bid b4 and ask s3 . Instead, pbm = ŵb4 = 6 is adopted, and we would have
p3b := 6. Of course, this is exactly the price determined by McAfee for bid b1 in period 3 when the bidder
is truthful.

154

Chain: An Online Double Auction

The interpretation is that there is an agent-independent price, zi (H t , At \ i, ω), that is
at least ŵi when the agent loses and no greater than ŵi otherwise. In particular, zi (H t , At \
i, ω) = ∞ when i ∈ NTt . Although an agent’s price is only explicit in a matching rule when
the agent trades, it is well known that such a price exists for any truthful, single-parameter
mechanism; e.g., see works by Archer and Tardos (2001) and Goldberg and Hartline (2003).5
Moving forward we adopt price zi to characterize the matching rule used as a building block
for Chain, and assume without loss of generality properties (A1) and (A2).
Given this, we will now establish the truthfulness of Chain by appeal to a price-based
characterization due to Hajiaghayi et al. (2005) for truthful, dynamic mechanisms. We state
(without proof) a variant on the characterization result that holds for stochastic policies
(π, x) and strong-truthfulness. The theorem that we state is also specialized to our DA
environment. We continue to adopt ω ∈ Ω to capture the realization of stochastic events
internal to the mechanism:
Theorem 2 (Hajiaghayi et al., 2005) A dynamic DA M = (π, x), perhaps stochastic, is
strongly truthful for misreports limited to no early-arrivals if and only if, for every agent i,
all θ̂i , all θ−i , and all random events ω ∈ Ω, there exists a price pi (âi , dˆi , θ−i , ω) such that:
(B1) the price is independent of agent i’s reported value
(B2) the price is monotonic-increasing in tighter [a′i , d′i ] ⊂ [âi , dˆi ]
(B3) trade πi (θ̂i , θ−i ) = 1 whenever pi (âi , dˆi , θ−i , ω) < ŵi and πi (θ̂i , θ−i ) = 0 whenever
pi (âi , dˆi , θ−i , ω) > ŵi , and the trade is performed for a buyer upon its departure period
dˆi .
(B4) the agent’s payment is xi (θ̂i , θ−i ) = pi (âi , dˆi , θ−i , ω) when πi (θ̂i , θ−i ) = 1, with
xi (θ̂i , θ−i ) = 0 otherwise, and the payment is made to a seller upon its departure, dˆi .
where random event ω is independent of the report of agent i in as much as it affects the
price to agent i.
Just as for the single-period, price-based characterization, the price pi (ai , di , θ−i , ω) need
not always be explicit in Chain. Rather, the theorem states that given any truthful dynamic
DA, such as Chain, there exists a well-defined price function with these properties of valueindependence (B1) and arrival-departure monotonicity (B2), and such that they define the
trade (B3) and the payment (B4).
To establish the truthfulness of Chain, we prove that it is well-defined with respect to
the following price function:
pi (âi , dˆi , θ−i , ω) = max(q̌(âi , dˆi , θ−i , ω), p̌i (âi , dˆi , θ−i , ω)),

(5)

where
q̌(âi , dˆi , θ−i , ω) =

max

t
t∈[dˆi −K,âi −1],i∈SNT
/

(zi (H t , At \ i, ω), −∞)

(6)

5. A single-parameter mechanism is one in which the private information of an agent is limited to one
number. This fits the single-period matching problem because the arrival and departure information
is discarded. Moreover, although there are both buyers and sellers, the problem is effectively singleparameter because no buyer can usefully pretend to be a seller and vice versa.

155

Bredin, Parkes and Duong

and
p̌(âi , dˆi , θ−i , ω) =



zi (H t∗ , At∗ \ i, ω) , if decision(i) = 1
+∞
, otherwise

(7)

where decision(i) = 0 indicates that i ∈ SNTt for all t ∈ [âi , dˆi ] and decision(i) = 1
otherwise, and where t∗ ∈ [âi , dˆi ] is the first period in which i ∈
/ SNTt . We refer to this
as the decision period. Term q̌(âi , dˆi , θ−i , ω) denotes the admission price, and is defined on
periods t before the agent arrives for which i ∈
/ SNTt had it arrived in that period. Note
carefully that the rules of Chain are implicit in defining this price function. For instance,
whether or not i ∈ SNTt in some period t depends, for example, on the other bids that
remain active in that period.
We now establish conditions (B1)–(B4). The proofs of the technical lemmas are deferred
until the Appendix. The following lemma is helpful and gets to the heart of the strong notrade concept.
Lemma 5 The set of active agents (other than i) in period t in Chain is independent of
i’s report while agent i remains active, and would be unchanged if i’s arrival is later than
period t.
The following result establishes properties (B1) and (B2).
Lemma 6 The price constructed from admission price q̌ and post-arrival price p̌ is valueindependent and monotonic-increasing when the matching rule in Chain is well-defined,
the strong no-trade construction is valid, and agent patience is bounded by K.
Having established properties (B1) and (B2) for price function pi (âi , dˆi , θ−i , ω), we just
need to establish (B3) and (B4) to show truthfulness. The timing aspect of (B3) and (B4),
which requires that the buyer receives an item and the seller receives its payment upon
reported departure, is already clear from the definition of Chain.
Theorem 3 The online DA Chain is strongly truthful, no-deficit, feasible and individualrational when the matching rule is well-defined, the strong no-trade construction is valid,
and agent patience is bounded by K.
Proof: Properties (B1) and (B2) follow from Lemma 6. The timing aspects of (B3) and
(B4) are immediate. To complete the proof, we first consider (B3). If q̌ > ŵi , then agent
i is priced out at admission by Chain because this reflects that zi (H t , At \ i, ω) > ŵi in
some t ∈ [dˆi − K, âi − 1] with i ∈
/ SNTt , and thus the bid would lose if it arrived in that
period (either because it could trade, but for a payment greater than its reported value, or
because i ∈ NTt ). Also, if there is no decision period, then p̌ = ∞, which is consistent with
Chain, because there is no bid price at which a bid will trade when i ∈ SNTt for all periods
t ∈ [âi , dˆi ]. Suppose now that there is a decision period t∗ and q̌ < ŵi . If p̌ > ŵi , then there
should be no trade. This is the case in Chain, because the price zi (H t∗ , At∗ \ i, ω) in t∗ is
greater than ŵi and thus the agent is priced-out. If p̌ < ŵi then the bid should trade and
indeed it does, again because the price zi in that period satisfies (A1) and (A2) with respect
to the matching rule. Turning to (B4), it is immediate that the payments collected in Chain
156

Chain: An Online Double Auction

are equal to price pi (âi , dˆi , θ−i , ω), because if bid i trades then pi (âi , dˆi , θ−i , ω) ≤ ŵi and
thus q̌ ≤ ŵi and p̌ ≤ ŵi . The admission price q(âi , dˆi , θ−i , ω) = q̌(âi , dˆi , θ−i , ω) when q̌ ≤ ŵi
because price zi is well-defined by properties (A1) and (A2). Similarly, the payment pt∗
defined by the matching rule in Chain in the decision period is equal to p̌.
That Chain is individual-rational and feasible follows from inspection. Chain is nodeficit because the payment collected from every agent (whether a buyer or a seller) is at
least that defined by a valid matching rule in the decision period t∗ (it can be higher when
the admission price is higher than this matching price), the matching rules are themselves
no-deficit, and because the auctioneer delays making a payment to a seller until its reported
departure but collects payment from a buyer immediately upon a match.

We remark that information can be reported to bidders that are not currently participating in the market, for instance to assist in their valuation process. If this information
is delayed by at least the maximal patience of a bidder, so that the bid of a current bidder
cannot influence the other bids and asks that it faces, then this is without any strategic
consequences. Of course, without this constraint, or with bidders that participate in the
market multiple times, the effect of such feedback would require careful analysis and bring
us outside of the private values framework.
5.2 Chain is Unique amongst Dynamic DAs that are constructed from Myopic
Matching Rules
In what follows, we establish that Chain is unique amongst all truthful, dynamic DAs that
adopt well-defined, myopic matching rules as simple building blocks. For this, we define
the class of canonical, dynamic DAs, which take a well-defined single period matching rule
coupled with a valid strong no-trade construction, and satisfy the following requirements:
(i) agents are active until they are matched or priced-out,
(ii) agents participate in the single-period matching rule while active
(iii) agents are matched if and only if they trade in the single-period matching rule.
We think that these restrictions capture the essence of what it means to construct a
dynamic DA from single-period matching rules. Notice that a number of design elements are
left undefined, including the payment collected from matched bids, when to mark an active
bid as priced-out, what rule to use upon admission, and how to use the strong no-trade
information within the dynamic DA. In establishing a uniqueness result, we leverage the
necessary and sufficient price-based characterization in Theorem 2, and exactly determine
the price function pi (âi , dˆi , θ−i , ω) to that defined in Eq. (4) and associated with Chain.
The proofs for the two technical lemmas are deferred until the Appendix.
Lemma 7 A strongly truthful, canonical dynamic DA must define price pi (âi , dˆi , θ−i , ω) ≥
zi (H t∗ , At∗ \ i, ω) where t∗ is the decision period for bid i (if it exists). Moreover, the bid
must be priced-out in period t∗ if it is not matched.
Lemma 8 A strongly truthful, canonical and individual-rational dynamic DA must define
price pi (âi , dˆi , θ−i , ω) ≥ q̌(âi , dˆi , θ−i , ω), and a bid with ŵi < q̌(âi , dˆi , θ−i , ω) must be pricedout upon admission.
157

Bredin, Parkes and Duong

Theorem 4 The dynamic DA algorithm Chain uniquely defines a strongly truthful,
individual-rational auction among canonical dynamic DAs that only designate bids as pricedout when necessary.
Proof: If there is no decision period, then we must have pi (âi , dˆi , θ−i , ω) = ∞, by canonical
(iii) coupled with (B3). Combining this with Lemmas 7 and 8, we have pi (âi , dˆi , θ−i , ω) ≥
max(q̌(âi , dˆi , θ−i , ω), p̌(âi , dˆi , θ−i , ω)). We have also established that a bid must be pricedout if its bid value is less than the admission price, or it fails to match in its decision
period. Left to show is that the price is exactly as in Chain, and that a bid is admitted
when its value ŵi ≥ q̌(âi , dˆi , θ−i , ω) and retained as active when it is in the strong notrade set. The last two control aspects are determined once we choose a rule that “only
designates bids as priced-out when necessary.” We prefer to allow a bid to remain active
when this does not compromise truthfulness or individual-rationality. Finally, suppose
for contradiction that p′ = pi (âi , dˆi , θ−i , ω) > max(q̌(âi , dˆi , θ−i , ω), p̌(âi , dˆi , θ−i , ω)). Then
an agent with max(q̌(âi , dˆi , θ−i , ω), p̌(âi , dˆi , θ−i , ω)) < wi < p′ would prefer to bid ŵi =
q̌(âi , dˆi , θ−i , ω), p̌(âi , dˆi , θ−i , ω)) − ǫ and avoid winning – otherwise its payment would be
greater than its value.

5.3 Bounded Patience Is Required for Reasonable Efficiency
Chain depends on a maximal bound on patience used to calculate the admission price faced
by a bidder on entering the market with Eq. (3). To motivate this assumption about the
existence of a maximal patience, we construct a simple environment in which the number of
trades implemented by a truthful, no-deficit DA can be made an arbitrarily small fraction
of the number of efficient trades with even a small number of bidders having potentially unbounded patience. This illustrates that a bound on bidder patience is required for dynamic
DAs with reasonable performance.
In achieving this negative result, we impose the additional requirement of anonymity,
This anonymity property is already satisfied by Chain, when coupled with matching rules
that satisfy anonymity, as is the case with all the rules presented in Section 4. In defining
anonymity, extend the earlier definition of a dynamic DA, M = (π, x), so that allocation
policy π = {π t }t∈T defines the probability πit (θ ≤t ) ∈ [0, 1] that agent i trades in period t
given reports θ ≤t . Payment, x = {xt }t∈T , continues to define the payment xti (θ ≤t ) by agent
i in period t, and is a random variable when the mechanism is stochastic.
Definition 8 (anonymity) A dynamic DA, M = (π, x) is anonymous if allocation policy
π = {π t }t∈T defines probability of trade πit (θ ≤t ) in each period t that is independent of
identity i and invariant to a permutation of (θ ≤t \ i) and if the payment xti (θ ≤t ), contingent
on trade by agent i, is independent of identity i and invariant to a permutation of (θ ≤t \ i).
We now consider the following simple environment. Informally, there will be a random
number of high-valued phases in which bids and asks have high value and there might be a
single bidder with patience that exceeds that of the other bids and asks in the phase. These
high-valued phases are then followed by some number, perhaps zero, of low-valued phases
with bounded-patience bids and asks. Formally, there are Th ≥ 1 high-valued phases
(a random variable, unknown to the auction), each of duration L ≥ 1 periods, indexed
k ∈ {0, 1, . . . , Th − 1} and each with:
158

Chain: An Online Double Auction

• N or N − 1 bids with type (1 + kL, (k + 1)L, vH ),
• 0 or 1 bids with type (1 + kL, d, αvH ) for some mark-up parameter, α > 1 and some
high-patience parameter, d ∈ T ,
• N asks with type (1 + kL, (k + 1)L, −(vH − ǫ)),
followed by some number (perhaps zero) of low-valued phases, also of duration L, and
indexed k ∈ {Th , . . . , ∞}, with:
• N or N − 1 bids with type (1 + kL, (k + 1)L, vL )
• N asks with type (1 + kL, (k + 1)L, −(vL − ǫ)),
where N ≥ 1, 0 < vL < vH , and bid-spread parameter ǫ > 0. Note that any phase can
be the last phase, with no additional bids or asks arriving in the future.
Definition 9 (reasonable DA) A dynamic DA is reasonable in this simple environment
if there is some parameterization of new bids, N ≥ 1, and periods-per-phase, L ≥ 1, for
which it will execute at least one trade between new bids and new asks in each phase,
for any choice of high value vH , low value vL < vH , bid-spread ǫ > 0, mark-up α > 1, high
patience d.
All of the dynamic DAs presented in Section 4 can be parameterized to make them
reasonable for a suitably large N ≥ 1 and L ≥ 1, and without the possibility of a bid with
an unbounded patience.
Theorem 5 No strongly truthful, individual-rational, no-deficit, feasible, anonymous dynamic DA can be reasonable when a bidder’s patience can be unbounded.
Proof: Fix any N ≥ 1, L ≥ 1, and for the number of high-valued phases, Th ≥ 1, set the
departure of a high-patience agent to d = (Th + 1)L. Keep vH > vL > 0, ǫ > 0, and α > 1
as variables to be set within the proof. Assume a dynamic DA is reasonable, so that it
selects at least one new bid-ask pair to trade in each phase. Consider phase k = 0 and with
N − 1 agents of types (1, L, vH ), N of type (1, L, −(vH − ǫ)) and 1 agent of patient type,
(1, (Th + 1)L, αvH ). If the patient bid deviates to (1, L, vH ), then the bids are all identical,
and with probability at least 1/N the bid would win by anonymity and reasonableness.
Also, by anonymity, individual-rationality and no-deficit we have that the payment made
by any winning bid is the same, and must be p′ ∈ [vH −ǫ, vH ]. (If the payment had been less
than this, the DA would run at a deficit since the sellers require at least this much payment
for individual-rationality.) Condition now on the case that the patient bid would win if it
deviates and reports (1, L, vH ). Suppose the bidder is truthful, reports (1, (Th + 1)L, αvH )
but does not trade in this phase. But, if phase k = 0 is the last phase with new bids and
asks, then the bid will not be able to trade in the future and for strong-truthfulness the
DA would need to make a payment of at least αvH − vH = (α − 1)vH in a later phase to
prevent the bid having a useful deviation to (1, L, vH ) and winning in phase k = 0. But, if:
N ǫ < (α − 1)vH ,
159

(8)

Bredin, Parkes and Duong

then the DA cannot make this payment without failing no-deficit (because N ǫ is an upperbound on the surplus the auctioneer could extract from bidders in this phase without
violating individual-rationality). We will later pick values of α, ǫ and vH , to satisfy Eq. (8).
So, the bid must trade when it reports (1, (Th + 1)L, αvH ), in the event that it would win
with report (1, L, vH ), as “insurance” against this being the last phase with new bids and
asks. Moreover, it should trade for payment, p′ ∈ [vH − ǫ, vH ], to ensure an agent with true
type (1, L, vH ) cannot benefit by reporting (1, (Th + 1)L, αvH ).
Now suppose that this was not the last phase with new bids and asks, and Th > 1.
Now consider what would happen if the patient bid in phase k = 0 deviated and reported
(1 + Th L, (Th + 1)L, vL ). As before, this bid would win with probability at least 1/N by
anonymity and reasonableness, but now with some payment p′′ ∈ [vL − ǫ, vL ]. Condition
now on the case that the patient bid would win, both with a report of (1, L, vH ) and with a
report of (1 + Th L, (Th + 1)L, vL ). When truthful, it trades in phase k = 0 with payment at
least vH − ǫ. If it had reported (1 + Th L, (Th + 1)L, vL ), it would trade in phase k = Th for
payment at most vL . For strong truthfulness, the DA must make an additional payment to
the patient agent of at least (vH − vL ) − (vH − (vH − ǫ)) = vH − vL − ǫ. But, suppose that
the high and low values are such that,
(Th + 1)N ǫ < vH − vL − ǫ.

(9)

Making this payment in this case would violate no-deficit, because (Th +1)N ǫ is an upperbound on the surplus the auctioneer can extract from bidders across all phases, including
the current phase, without violating individual-rationality. But now we can fix any vL > 0,
ǫ < vL and choose vH > (Th + 1)N ǫ + vL + ǫ to satisfy Eq. (9) and α > (N ǫ/vH ) + 1
to satisfy Eq. (8). Thus, we have proved that no truthful dynamic DA can choose a bidask pair to trade in period k = 0. The proof can be readily extended to show a similar
problem with choosing a bid-ask pair in any period k < Th , by considering truthful type of
(1 + kL, (Th + 1)L, αvH ).

To drive home the negative result: notice that the number of efficient trades can be
increased without limit by choosing an arbitrarily large Th , and that no truthful, dynamic DA with these properties will be able to execute even a single trade in each of
these {0, . . . , Th − 1} periods. Moreover, we see that only a vanishingly small fraction of
high-patience agents is required for this negative result. The proof only requires that at
least one patient agent is possible in all of the high-valued phases.

6. Experimental Analysis
In this section, we evaluate in simulation each of the Chain-based DAs introduced in
Section 4. We measure the allocative efficiency (total value from the trades), net efficiency
(total value discounted for the revenue that flows to the auctioneer), and revenue to the
auctioneer. All values are normalized by the total offline value of the optimal matching.
For comparison we also implement several other matching schemes: the truthful, surplusmaximizing matching algorithm presented by Blum et al. (2006), an untruthful greedy
matching algorithm using truthful bids as input to provide an upper-bound on performance,
and an untruthful DA populated with simple adaptive agents that are modeled after the
Zero-intelligence Plus trading algorithm that has been leveraged in the study of static
DAs (Cliff & Bruten, 1998; Preist & van Tol, 1998).
160

Chain: An Online Double Auction

6.1 Experimental Set-up
Traders arrive to the market as a Poisson stream to exchange a single commodity at discrete
moments. This is a standard model of arrival in dynamic systems, economic or otherwise.
Each trader, equally likely to be a buyer or seller, arrives after the previous with an exponentially distributed delay, with probability density function (pdf):
f (x) = λe−λx ,

x ≥ 0,

(10)

where λ > 0 represents the arrival intensity in agents per second. Later we present results
as the interarrival time, λ1 , is varied between 0.05 and 1.5; i.e., as the arrival intensity is
varied between 20 and 23 . A single trial continues until at least 5,000 buyers and 5,000 sellers
have entered the market. In our experiments we vary the maximal patience K between 2
and 10. For the distribution on an agent’s activity period (or patience, di − ai ), we consider
both a uniform distribution with pdf:
f (x) =

1
,
K

x ∈ [0, K],

(11)

and a truncated exponential distribution with pdf:
f (x) = αe−αx ,

x ∈ [0, K],

(12)

where α = − ln(0.05)/K so that 95% of the underlying exponential distribution is less than
the maximal patience. Both arrival time and activity duration are rounded to the nearest
integral time period. A trader who arrives and departs during the same period is assumed
to need an immediate trade and is active for only one period.
Each trader’s valuation represents a sample drawn at its arrival from a uniform distribution with spread 20% about the current mean valuation. (The value is positive for a bid
and negative for an ask.) To simulate market volatility, we run experiments that vary the
average valuation using Brownian motion, a common model for valuation volatility upon
which many option pricing models are based (Copeland & Weston, 1992). At every time
period, the mean valuation randomly increases or decreases by a constant multiplier, e±γ ,
where γ is the approximate volatility and varied between 0 and 0.15 in our experiments.
We plot the mean efficiency of 100 runs for each experiment, with the same sets of bids
and asks used across all double auctions. All parameters of an auction rule are reoptimized
for each market environment; e.g., we can find the optimal fixed price and the optimal
smoothing parameters offline given the ability to sample from the market model.
6.2 Chain Implementation
We implement Chain for the five price-based matching rules (history-clearing, historymedian, history-McAfee, history-EWMA, and fixed-price) and the three competition-based
matching rules (McAfee, active-McAfee, and windowed-McAfee).
The price-based implementations keep a fixed-size set of the most recently expired,
traded, or priced-out offers, H t . Offers priced-out by their admission prices are inserted
into H t prior to computing pt . The history-clearing metric computes a price to maximize the
number of trades to agents represented by H t had they all been contemporary. The historymedian metric chooses the price to be the median of the absolute valuation of the offers
161

Bredin, Parkes and Duong

in H t . The history-McAfee method computes the “McAfee price” for the scenario where
all agents represented by H t are simultaneously present. The EWMA metric computes an
exponentially-weighted average of bids in the order that they expire, trade, or price out.
The simulations initialize the price to the average of the mean buy and sell valuations. If
two bids expire during the same period, they are included in arbitrary order to the moving
average.
None of the metrics require more than one parameter, which is optimized offline with
access to the model of the market environment. Parameter optimization proceeds by uniformly sampling the parameter range, smoothing the result by averaging each result with
its immediate neighbors. The optimization repeats twice more over a narrower range about
the smoothed maximum, returning the parameter that maximizes (expected) allocative efficiency. None of the price-based methods appeared to be sensitive to small (<10%) changes
in the size of H t . With most simulations, the window size was chosen to be about 150 offers. For EWMA, the smoothing factor was usually chosen to be around 0.05 or lower. The
windowed-McAfee matching rule, however, was extremely sensitive to window size for simulations with volatile valuations, and the search process frequently converged to suboptimal
local maxima.
The admission price in the price-based methods is computed by first determining
whether Match would check the value of the bid against bid price if the bid had arrived in
some earlier period t′ . Rather than simulate the entire Match procedure, it is sufficient to
determine the probability ρi of this event. This is determined by checking the construction
′
of the strong no-trade sets in that earlier period. If SNTt contains non-departing buyers
(sellers), then the probability that an additional seller (buyer) would be examined is 1 and
ρi = 1. Otherwise the probability is equal to the ratio of the number of bids (asks) examined not included in SNTt and one more than the total number of bids (asks) present.
′
Finally, with probability ρi the price the agent would have faced in period t′ is defined as pt
′
′
(−pt for sellers), and otherwise it is −∞. Here, pt is the history-dependent price defined
in period t′ .
The competition-based matching rules price out all non-trading bids at the end of each
period in which trade occurs (because of the definition of strong no-trade in that context).
The admission prices are calculated by considering the price that a bid (ask) would have
faced in some period t′ before its reported arrival. In such a period, the price for a bid (ask)
is determined by inserting an additional bid (ask) with valuation ∞ (0) and applying the
competition-based matching rule to that (counterfactual) state. From this we determine
whether the agent would win for its reported value, and if so what price it would face.
6.3 Optimal Offline Matching
We use a commercial integer program solver (CPLEX6 ) to compute the optimal offline
solution, i.e. with complete knowledge about all offers received over time. In determining
the offline solution we enforce the constraint that a trade can only be executed if the activity
periods of both buyer, i, and seller, j, overlap,
(ai ≤ dj ) ∧ (aj ≤ di )
6. www.ilog.com

162

(13)

Chain: An Online Double Auction

An integer-program formulation to maximize total value is:
max

X

xij (wi + wj )

(14)

(i,j)∈overlap

X

s.t. 0 ≤

xij ≤ 1, ∀j ∈ ask

i:(i,j)∈overlap

0≤

X

xij ≤ 1, ∀i ∈ bid

j:(i,j)∈overlap

xij ∈ {0, 1}, ∀i, j,

where (i, j) ∈ overlap is a bid-ask pair that could potentially trade because they have
overlapping arrival and departure intervals satisfying Eq. (13). The decision variable xij ∈
{0, 1} indicates that bid i matches with ask j. This provides the optimal, offline allocative
efficiency.
6.4 Greedy Online Matching
We implement a greedy matching algorithm that immediately matches offers that yield nonnegative budget surplus. This is a non-truthful matching rule but provides an additional
comparison point for the efficiency of the other matching schemes. During each time period,
the greedy matching algorithm orders active bids and asks by their valuations, exactly as
the McAfee mechanism does, and matches offers until pairs no longer generate positive
surplus. The algorithm’s performance allows us to infer the number of offers that the
optimal matching defers before matching and the amount of surplus lost by the McAfee
method due to trade reduction and due to the additional constraint of admission pricing.
6.5 Worst-Case Optimal Matching
Blum et al. (2006) derive a mechanism equivalent to our fixed-price matching mechanism,
except that the price used is chosen from the cumulative distribution
1
ln
D(x) =
rα



x − wmin
(r − 1)wmin



,

(15)

where r is the fixed point to the equation
r = ln



wmax − wmin
(r − 1)wmin



,

(16)

and wmin ≥ 0 and wmax ≥ 0 are the minimum and maximum absolute valuations of all
traders in the market. For our simulations, we give the mechanism the exact knowledge of
the minimum and maximum absolute valuations for each schedule. Blum et al. (2006) show
that this method guarantees an expected competitive ratio of max(2, ln(wmax /wmin )) with
respect to the optimal offline solution in an adversarial setting. We were interested to see
how will this performed in practice in our simulations.
163

Bredin, Parkes and Duong

6.6 Strategic Open-outcry Matching: ZIP Agents
To compare Chain with the existing literature on continuous double auctions, we implement
a DA that in every period sorts all active offers and matches the highest valued bids with
the lowest valued asks so long as the match yields positive net surplus. The DA prices each
trading pair at the mean of the pair’s declared valuations. Since the trade price depends
on a bidder’s declaration, the market does not support truthful bidding strategies. We must
therefore adopt a method to simulate the behavior of bidding agents within this simple open
outcry market.
For this, we randomly assign each bid to one of several “protocol agents” that each use
a modified ZIP trading algorithm, as initially presented by Cliff and Bruten (1998) and
improved upon by Preist and van Tol (1998). The ZIP algorithm is a common benchmark
used to compare learned bidding behavior in a simple double-auction trading environment
in which agents are present at once and adjust their bids in seeking a profitable trade. We
adapt the ZIP algorithm for use in our dynamic environment.
In our experiments we consider five of these protocol agents. New offers are assigned
uniformly at random to a protocol agent, which remains persistent throughout the simulation. Each offer is associated with a patience category, k ∈ {low, medium, high}, defined
to evenly partition the range of possible offer patience. Each protocol agent, j, is defined
with parameters (rj , βj , γj ) and maintains a profit margin, µkj , on each patience category k.
Parameters (βj , γj ) control the adaptivity of the protocol agent in how it adjusts the target
profit margin on an individual offer, with βj ∼ U (0.1, 0.2) defining the offer-level learning
rate and γj ∼ U (0.2, 0.8) defining the offer-level damping factor. Parameter rj ∈ [0, 1] is
the learning rate adopted for updating the profit margins.
The protocol agents are trained over 10 trials and their final performance is measured
in the 11th trial. The learning rate decreases through the training session and depends on
the initial learning rate rj0 and the adjustment rate rj+ . In period t ∈ {1, . . . , tkend } of trial
k ∈ {1, . . . , T + 1}, where T = 10 is the number of trials used for training and tkend is the
number of periods in trial k, the learning rate is defined as:
rj := 1 −

rj0

+ (k −

1)rj+

+



t
tkend

2

rj+

!

(17)

where rj+ = (1 − rj0 )/(T + 1). We define rj0 := 0.7. The effect of this adjustment rule is that
rj is initially 0.3, decreases during training, and trends to 0.0 as t → tend in trial k = 11.
Within a given trial, upon assignment of a new offer i in patience category k, the protocol
agent managing the offer initializes (µi (t), δi (t)) := (µkj , 0), where µi (t) represents the target
profit margin for the offer and δi (t) represents a profit-margin correction term. The target
profit margin and the profit margin correction term are adjusted for offer i in subsequent
periods while the bid remains active.
The target profit margin is used to define a bid price for the offer in each period while
it remains active:
ŵi (t) := wi (1 + µi (t)).
164

(18)

Chain: An Online Double Auction

At the end of a period in which an offer matches or simply expires, the profit margin
µkj for its patience category is updated as:
µkj := (1 − rj )µkj + rj µi (t),

(19)

where the amount of adaptivity depends on the learning rate rj . Because the profit margin
on an offer decays over its lifetime, this update adjusts towards a small profit margin if
the offer expires or took many periods to trade, and a larger profit margin otherwise. The
long-term learning of a protocol agent occurs through the profit margin assigned to each
patience category.
At the start of a period each protocol agent also computes target prices for bids and
asks in each patience category. These are used to drive an adjustment in the target profit
margin for each active bid and ask. Target prices τbk (t) and τsk (t) are computed as:

{ŵi (t − 1)} + ξ , if 0 > max {ŵi (t − 1)} + max {ŵi (t − 1)}
 (1 + η) max
i∈S(t−1)
i∈B k (t−1)
i∈B k (t−1)
k
τb (t) :=
 (1 − η) max {ŵi (t − 1)} − ξ , otherwise
i∈B k (t−1)

(20)

and,
τsk (t)

:=


 (1 + η)

 (1 − η)

max {ŵi (t − 1)} + ξ , if 0 > max {ŵi (t − 1)} + max {ŵi (t − 1)}
i∈B(t−1)

i∈S k (t−1)

i∈S k (t−1)

max {ŵi (t − 1)} − ξ , otherwise

i∈S k (t−1)

(21)

where ξ, η ∼ U (0, 0.05). Here, B(t − 1) and S(t − 1) denote the set of active bids and asks
in the market in period t − 1 (defined before market clearing), and B k (t − 1) and S k (t − 1)
denote the restrictions to patience category k. The target price on a bid in category k
is set to something slightly greater than the most competitive bid in the previous round
when that bid could not trade, and slightly less otherwise. Similarly for the target price on
asks, where these prices are negative, so that increasing the target price makes an ask more
competitive.
Target prices are used to adjust the target profit margin at the start of each period on
all active offers that arrived in some earlier period, where the influence of target prices is
through the profit-margin correction term:
µi (t) :=

(ŵi (t − 1) + δi (t))
− 1,
wi

(22)

and the profit-margin correction term, δi (t), is defined in terms of the target price τik (t)
(equal to τbk (t) if i is a bid and τsk (t) otherwise) as,
δi (t) := γj δi (t − 1) + (1 − γj )βj (τik (t) − ŵi (t − 1)),

(23)

where γj and βj are the offer-level learning rates and damping factor. The value wi and the
“-1” term in Eq. (22) provide normalization. Eq. (23) is the Widrow-Hoff (Hassoun, 1995)
rule, designed to minimize the least mean square error in the profit margin and adopted
here to mimic earlier ZIP designs.
165

Bredin, Parkes and Duong

6.7 Experimental Results
Our experimental results show that market conditions drive DA choice. We compare allocative efficiency, revenue, and net efficiency. All results are averaged over 100 trials.
In experiments we found only minimal qualitative differences between the use of the two
patience distributions. The uniform patience distribution provides a slight increase in efficiency over result using exponential patience, caused by a larger proportion of patient
agents which relaxes somewhat the admission-price constraint in Eq. (3). For this reason
we choose to report only results for the uniform patience distribution.
While the performance of all methods are summarized in Table 3, we omit the performance of some markets from the plots to keep the presentation of results as clear as possible.
We do not plot the price-based results for the median- or clearing-based prices because the
performance was typically around that of the performance of Chain instantiated on the
history-EWMA price. We do not plot the windowed-McAfee results because of inconsistent performance, and in most cases, upon manual inspection, it was optimal to choose the
smallest possible window size, i.e. including only active bids and making it equivalent to
active-McAfee.
Our plots also leave out the performance of the Blum et al. (2006) worst-case optimal
matching scheme because it was dominated by the fixed-price Chain instantiation and in
many cases failed to yield any substantial surplus. We note here that the modeling assumption made by Blum et al. (2006) is quite different than that in our work: they worry about
performance in an adversarial environment while we consider probabilistic environments.
Our fixed-price Chain mechanism operates essentially identically to the surplus-maximizing
scheme of Blum et al. (2006), except that Chain can also use additional statistical information to set the ideal price, rather than drawing the price from a distribution that is used to
guarantee worst-case performance against an adversary. We defer the results for the Blum
et al. (2006) scheme to Table 3.
Figures 5–8 plot results from two sets of experiments, one for high-patience/low-volatility
and one for low-patience/high-volatility, as we vary the inter-arrival time (and thus the
arrival intensity), volatility and maximal patience. All plots are for allocative efficiency
except Figure 6, where we consider net efficiency. Active-McAfee is included on Figure 5,
but not on any other plots because it did not improve upon the McAfee performance in
the other environments. To emphasize: the results for greedy provide an upper-bound on
the best possible performance because this is a non-truthful algorithm, simulated here with
truthful inputs.
In Figure 5 (left) we see that from within the truthful DAs, the McAfee-based DA has
the best efficiency for medium to low arrival intensities. There also is a general decrease
in performance, relative to the optimal offline solution, as the arrival intensity falls. This
trend, also observed with the greedy (non-truthful) DA, occurs because the Chain scheme
is myopic in that it matches as soon as the static DA building block finds a match, while
it is better to be less myopic when arrival intensity is low. The McAfee-based DAs are less
sensitive to this than other methods because they can aggressively update prices using the
active traders. The price-based DAs experience inefficiencies due to the lag in price updates
because they use only expired, traded, and priced-out offers to calculate prices.
166

Chain: An Online Double Auction

(patience=6, volatility=0.01)
greedy
zip
mcafee
ewma
active-mcafee
fixed-price

1.2
1

greedy
mcafee
active-mcafee
ewma
fixed-price
zip

1.4
Allocative Efficiency

1.4
Allocative Efficiency

(patience=2, volatility=0.08)

0.8
0.6
0.4
0.2

1.2
1
0.8
0.6
0.4
0.2

0

0
0

0.2

0.4

0.6
0.8
1
Inter-arrival Time

1.2

1.4

0

0.2

0.4

0.6
0.8
1
Inter-arrival Time

1.2

1.4

Figure 5: Allocative efficiency vs. inter-arrival time (1 / intensity) for several DAs. The left
plot shows high-patience, low-volatility simulations, whereas the right plots results from
low-patience, high-volatility runs. Both sets of experiments use uniform patience distributions.

(patience=6, volatility=0.01)
greedy
zip
mcafee
ewma
active-mcafee
fixed-price

1.2
1

greedy
mcafee
active-mcafee
ewma
fixed-price
zip

1.4
1.2
Net Efficiency

1.4

Net Efficiency

(patience=2, volatility=0.08)

0.8
0.6

1
0.8
0.6

0.4

0.4

0.2

0.2

0

0
0

0.2

0.4

0.6
0.8
1
Inter-arrival Time

1.2

1.4

0

0.2

0.4

0.6
0.8
1
Inter-arrival Time

1.2

1.4

Figure 6: Net efficiency vs. inter-arrival time (1 / intensity) for several DAs. The left plot
shows high-patience, low-volatility simulations, whereas the right plots results from lowpatience, high-volatility runs. Both sets of experiments use uniform patience distributions.

167

Bredin, Parkes and Duong

For very high arrival intensity we see Active-McAfee dominates McAfee. Active-McAfee
smooths the price, which helps to mitigate the impact of fluctuations in cost on the admission
price via Eq. (3) in return for less responsiveness. This is helpful in “well-behaved” markets
with high arrival intensity and low volatility but was not helpful in most environments we
studied, where the additional responsiveness provided by the (vanilla) McAfee scheme paid
off.
The ZIP market also has good performance in this high-patience/low-volatility environment. The reason is simple: this is an easy environment for simple learning agents, and the
agents quickly learn to be truthful. We emphasize that these ZIP market results should be
treated with caution and are certainly optimistic. This is because the ZIP agents are not
programmed to consider timing-based manipulations. The effect in this environment is that
the ZIP market tends to operate as if a truthful market, but without the cost of imposing
truthfulness explicitly via market-clearing rules. By comparison the Chain auctions are
fully strategyproof, to both value and temporal manipulations.
Compare now with Figure 5 (right), which is for low patience and high volatility. Now we
see that McAfee dominates across the range of arrival intensities. Moreover, the performance
of ZIP is now quite poor because the agents do not have enough time to adjust their bids
(patience is low) and high volatility makes this a more difficult environment. With volatile
valuations, the possibility of valuation swings leaves open the possibility of larger profits,
luring agents to set wider profit margins, but only after the market changes. The ZIP agents
also have fewer concurrent competitive offers to use in setting useful price targets during
learning. As we might expect, high volatility also negatively impacts the efficiency of the
fixed-price scheme.
In Figure 6 we see that the net efficiency trends are qualitatively similar except that the
competition-based DAs such as McAfee fare less well in comparison with the price-based
DAs. The auctioneer accrues more revenue for competition-based matching rules such as
McAfee because they often generate buy and sell prices with a spread. Together with
the competition-based schemes being intrinsically more dynamic, this drives an increased
price spread in Chain via the admission price constraints. In Figure 6 (left) we see that
the fixed-price scheme performs well for high arrival intensity while EWMA dominates for
intermediate arrival intensities. The McAfee scheme is still dominant for lower patience and
higher volatility (Figure 6, right).
To reinforce these observations, in Table 3 we present the the net efficiency, allocative
efficiency and (normalized) revenue across all arrival intensities (i.e. inter-arrival time from
0.05 to 1.5) and for both low and high volatility trials. All five price-based methods, all
three competition-based methods, and all three comparison methods are included. We
highlight the best performing competition-based method, price-based method, as well as
the performance of the ZIP market (skipping over the non-truthful, greedy algorithm). We
omit information about the mean standard error for each measurement because in no case
did this error exceed a tenth of a percent of the mean optimal surplus. From within the
truthful DAs, we see that the McAfee-based scheme dominates overall for both allocative
and net efficiency and both low and high volatility, although EWMA competes with McAfee
for net efficiency in low volatility markets. Notice also the good performance of the ZIPbased market (with the aforementioned caveat about the restricted strategy space) at low
volatilities.
168

Chain: An Online Double Auction

scenario
low-volt/high-pat
high-volt/low-pat
net
alloc rev
net
alloc rev
0.33 0.47 0.14 0.40 0.45 0.05
0.24 0.35 0.11 0.32 0.37 0.05
0.24 0.26 0.02 0.21 0.23 0.03
0.33 0.34 0.01 0.17 0.17 0.01
0.33 0.35 0.03 0.19 0.22 0.03
0.23 0.23 0.00 0.04 0.04 0.00
0.33 0.34 0.01 0.15 0.16 0.01
0.33 0.34 0.01 0.17 0.18 0.01
0.10 0.10 0.00 0.02 0.02 0.00
0.86 0.86 0.00 0.87 0.87 0.00
0.82 0.82 0.00 0.23 0.23 0.00

mcafee
active-mcafee
windowed-mcafee
history-clearing
history-ewma
history-fixed
history-mcafee
history-median
blum et al.
greedy
zip

Table 3: Net efficiency, allocative efficiency and auctioneer revenue (all normalized by the optimal
value from trade), averaged across all arrival intensities (0.05–1.5) and for low and high
value volatility. The best performing competition-based, price-based and ‘other’ (ignoring
greedy, which is not truthful) results are highlighted.

(patience=6, inter-arrival=1.0)
1.4

greedy
zip
mcafee
ewma
fixed-price

1

greedy
mcafee
ewma
zip
fixed-price

1.2
Allocative Efficiency

1.2
Allocative Efficiency

(patience=2, inter-arrival=1.0)
1.4

0.8
0.6
0.4
0.2

1
0.8
0.6
0.4
0.2

0

0
0

0.02

0.04

0.06

0.08

0.1

0.12

0.14

0

Volatility

0.02 0.04 0.06 0.08

0.1

0.12 0.14

Volatility

Figure 7: Allocative efficiency vs. volatility for several DAs for a fairly low arrival intensity. The
left plot is for large maximal patience and the right plot is for small maximal patience.
Both sets of experiments use uniform patience distributions.

Figure 7 plots allocative efficiency versus volatility for high patience (left) and low
patience (right) and for fairly low arrival intensity. Higher volatility hurts all methods –
especially the ZIP agents, which struggle to learn appropriate profit and price targets,
probably due to few opportunities to update prices for every individual offer. The McAfee
scheme fairs very well, showing good robustness for both large patience and small patience
environments. The fixed-price scheme has the best performance when there is zero volatility
but its efficiency falls off extremely quickly as volatility increases.
169

Bredin, Parkes and Duong

(inter-arrival=1.0, volatility=0.01)
1.4

greedy
zip
mcafee
ewma
fixed-price

1

greedy
zip
mcafee
ewma
fixed-price

1.2
Allocative Efficiency

1.2
Allocative Efficiency

(inter-arrival=1.0, volatility=0.08)
1.4

0.8
0.6
0.4
0.2

1
0.8
0.6
0.4
0.2

0

0
0

2

4
6
Maximal Patience

8

10

0

2

4
6
Maximal Patience

8

10

Figure 8: Allocative efficiency vs. maximal patience for several DAs and fairly low arrival intensity.
The left plot is for low volatility and the right plot is for high volatility. Both sets of
experiments use uniform patience distributions.

We also consider the effect of varying maximal patience. This is shown in Figure 8,
with low volatility (left) and high volatility (right). Again, the McAfee scheme is the best
of the truthful DAs based on Chain. We also see that the performance of ZIP improves
as patience increases due to more opportunities for learning. Perversely, a larger patience
can negatively affect the truthful DAs. In part this is simply because the performance of
greedy online schemes, relative to the offline optimal, decreases as patience increases and
the offline optimal matching is able to draw more benefit from its lack of myopia.
We also suspected another culprit, however. The possibility of the presence of patient
agents requires the truthful DAs to include additional terms in the max operator in Eq. (3)
to prevent manipulations, leading to higher admission prices and less admitted offers. To
better understand this effect we experimented with delayed market clearing in the McAfee
scheme, where the market matches agents only every τ -th period (the “clearing duration”).
The idea is to make a tradeoff between using fewer admission prices and the possibility that
we will miss the opportunity to match some impatient offers.
Figure 9 shows allocative efficiency when the matching mechanism clears less frequently
and for different maximal patience, K. Figure 9 (left) is for low volatility. There we
see that the best clearing duration is roughly 1, 2, 3 and 4 for maximal patience of K ∈
{4, 6, 8, 10} and that by optimizing the clearing duration the performance of McAfee remains
approximately constant as maximal patience increases. In Figure 9 (right) we consider the
effect in a high volatility environment, with these results averaged over 500 trials because the
performance of the DA has higher variance. We see a qualitatively similar trend, although
higher maximal patience now hurts overall and cannot be fully compensated for by tuning
the clearing duration.

7. Related Work
Static two-sided market problems have been widely studied (Myerson & Satterthwaite, 1983;
Chatterjee & Samuelson, 1987; Satterthwaite & Williams, 1989; Yoon, 2001; Deshmukh
170

Chain: An Online Double Auction

(inter-arrival=1.0 patience=K, volatility=0.01)

(inter-arrival=1.0 patience=K, volatility=0.08)

0.55

0.45
0.4
Allocative Efficiency

Allocative Efficiency

0.5
0.45
0.4
0.35
0.3
K=4
K=6
K=8
K=10

0.25
0.2
0.15
0

0.35
0.3
0.25
K=4
K=6
K=8
K=10

0.2
0.15
2

4
6
Clearing Duration

8

10

0

2

4
6
Clearing Duration

8

10

Figure 9: Allocative efficiency vs. clearing duration in the McAfee-based Chain auction for fairly
low arrival intensity and as maximal patience is varied from 4 to 10. The left plot is
for low volatility and the right plot is for high volatility. Both sets of experiments use
uniform patience distributions.

et al., 2002). In a classic result, Myerson and Satterthwaite proved that it is impossible to
achieve efficiency with voluntary participation and without running a deficit, even relaxing
dominant-strategy equilibrium to a Bayesian-Nash equilibrium. Some truthful DAs are
known for static problems (McAfee, 1992; Huang et al., 2002; Babaioff & Nisan, 2004;
Babaioff & Walsh, 2005). For instance, McAfee introduced a DA that sometimes forfeits
trade in return for achieving truthfulness. McAfee’s auction achieves asymptotic efficiency
as the number of buyers and sellers increases. Huang et al. extend McAfee’s mechanism
to handle agents exchanging multiple units of a single commodity. Babaioff and colleagues
have considered extensions of this work to supply-chain and spatially distributed markets.
Our problem is also similar to a traditional continuous double auction (CDA), where
buyers and sellers may at any time submit offers to a market that pairs an offer as soon
as a matching offer is submitted. Early work considered market efficiency of CDAs with
human experiments in labs (Smith, 1962), while recent work investigates the use of software
agents to execute trades (Rust et al., 1994; Cliff & Bruten, 1998; Gjerstad & Dickhaut,
1998; Tesauro & Bredin, 2002). While these markets have no dominant strategy equilibria,
populations of software trading agents can learn to extract virtually all available surplus,
and even simple automated trading strategies outperform human traders (Das et al., 2001).
However, these studies of CDAs assume that all traders share a known deadline by which
trades must be executed. This is quite different from our setting, in which we have dynamic
arrival and departure.
Truthful one-sided online auctions, in which agents arrive and depart across time, have
received some recent attention (Lavi & Nisan, 2000; Hajiaghayi et al., 2004, 2005; Porter,
2004; Lavi & Nisan, 2005). We adopt and extend the monotonicity-based truthful characterization in the work of Hajiaghayi et al. (2005) in developing our framework for truthful
DAs. Our model of DAs must also address some of the same constraints on timing that
occur in Porter, Hajiaghayi, and Lavi and Nisan’s work. In these previous works, the items
171

Bredin, Parkes and Duong

were reusable or expiring and could only be allocated in particular periods. In our work we
provide limited allowance to the match-maker, allowing it to hold onto a seller’s item until
a matched buyer is ready to depart (perhaps after the seller has departed).
The closest work in the literature is due to Blum et al. (2006), who present online matching algorithms for the same dynamic DA model. The main focus in their paper is on the
design of matching algorithms with good worst-case performance in an adversarial setting,
i.e. within the framework of competitive analysis. Issues related to incentive compatibility
receive less attention. One way in which their work is more general is that they also study
goals of profit and maximizing the number of trades, in addition to the goal of maximizing
social welfare that we consider in our work. However, the only algorithmic result that they
present that is truthful in our model (where agents can misreport arrival and departure) is
for the goal of social welfare. The DA that they describe is an instance of Chain in which
a fixed price is drawn from a distribution at the start of time, and used as the matching
price in every period. Perhaps unsurprisingly, given their worst-case approach, we observe
that their auction performs significantly worse than Chain defined for a fixed price that is
picked to optimize welfare given distributional information about the domain.

8. Conclusions
We presented a general framework to construct algorithms to match buyers with sellers in
online markets where both valuation and activity-period information are private to agents.
These algorithms guarantee truthful dominant strategies by first imposing a minimum admission price for each offer and then pricing and pairing the offer at the first opportunity.
At the heart of the Chain framework lies a pricing algorithm that must for each offer either
determine a price independent of any information describing the offer or choose to discard
the offer. The pricing algorithm should be chosen to match market conditions. We present
several examples of suitable pricing schemes, including fixed-price, moving-average, and
McAfee-based schemes.
More often than not, we find that the competition-based scheme that employs a McAfeebased rule to truthfully price the market delivers the best allocative efficiency. For exceptionally low volatility and high arrival intensity, we find that adaptive price-based schemes
such as an exponentially-weighted moving average (EWMA) and even fixed price schemes
perform well. We see qualitatively similar results for net efficiency, where the revenue that
accrues to the auctioneer is discounted, albeit that the price-based rules such as EWMA
have improved performance because they have no price spread. The observations are rooted
in simulations comparing the market efficiency under each mechanism with the optimal offline solution.
Additionally, we compare the efficiency of our truthful markets with a fixed-price worstcase optimal scheme presented by Blum et al. (2006), a market of strategic agents using
a variant on the ZIP price update algorithm developed by Cliff and Bruten (1998) for
continuous double auctions, and a non-truthful, greedy matching algorithm to provide an
upper-bound on performance. The best of our schemes yield around 33% net efficiency
in low volatility, high patience environments and 40% net efficiency in high volatility, low
patience environments, while the greedy bound suggests that as much as 86% efficiency
is possible with non-strategic agents. We note that the Blum et al.scheme, designed for
adversarial settings, fairs poorly in our simulations (< 10%).
172

Chain: An Online Double Auction

One can argue, we think convincingly, that truthfulness brings benefits in itself in that it
avoids the waste of costly counterspeculation and promotes fairness in markets (Sandholm,
2000; Abdulkadiroǧlu et al., 2006). On the other hand, it is certainly of interest that
the gap between the efficiency of greedy matching with non-truthful matching and that of
our truthful auctions is so large. Here, we observe that the ZIP-populated (non-truthful)
markets achieve around 82% efficiency in low volatility environments but collapse to around
23% efficiency in high volatility environments. Based on this, one might conjecture that
designing for truthfulness is especially important in badly behaved, highly volatile (“thin”)
environments but less important in well behaved, less volatile (“thick”) environments.
Formalizing this tradeoff between providing absolute truthfulness and approximate
truthfulness, and while considering the nature of the environment, is an interesting direction for future work (see paper by Parkes et al., 2001). Given that reporting of market
statistics can be incorporated within our framework (see Section 5.1), and given that markets also play a role in information aggregation and value discovery, future research should
also consider this additional aspect of market design. Perhaps there is an interesting tradeoff
between efficiency, truthful value revelation, and the process of information aggregation.
While the general Chain framework achieves good efficiency, further tuning seems possible. One direction is to adopt a meta-pricing scheme that chooses, or blends, prices from
competing algorithms. Another direction is to consider richer temporal models; e.g., the
value of goods to agents might decay or grow over time to better account for the time
value of assets. A richer temporal model might also consider the possibility of agents or the
match-maker taking short positions (including short-term cash deficits) to increase trade.
It is also interesting to extend our work to markets with non-identical goods and more
complex valuation models such as bundle trades (Chu & Shen, 2007; Babaioff & Walsh,
2005; Gonen et al., 2007), and to dynamic matching problems without prices, such as an
online variation of the classic “marriage” problem (Gusfield & Irving, 1989).

Acknowledgments
An earlier version of this paper appeared in the Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence, 2005. This paper further characterizes necessary conditions
for truthful online trade; truthfully matches offers using a generalized framework based
upon an arbitrary truthful static pricing rule; and compares the efficiency of our truthful
framework to that achieved in non-truthful markets populated with strategic trading agents
and with that of worst-case optimal double auctions.
Parkes is supported in part by NSF grant IIS-0238147 and an Alfred P. Sloan Fellowship
and Bredin would like to thank the Harvard School of Engineering and Applied Sciences
for hosting his sabbatical during which much this work was completed. Thanks also to
the three anonymous reviewers, who provided excellent suggestions in improving an earlier
draft of this paper.

Appendix: Proofs
Lemma 1 Procedure Match defines a valid strong no-trade construction.
173

Bredin, Parkes and Duong

Proof: In all cases, SNTt ⊆ NTt . The set NTt is correctly constructed: equal to all
remaining bids bt when (j = 0) in Case I, all remaining bids st when (i = 0) in Case II,
and all remaining bids and asks otherwise. In each case, no bid (or ask) in NTt could have
traded at any price because there was no available bid or ask on the opposite of the market
given its order.
In verifying strong no-trade (SNT) conditions (a) and (b), we proceed by case analysis.
Case I. (i 6= 0) and (j = 0). NTt := bt .
(I-1) ∀k ∈ st · (dˆk = t) and SNTt := bt . For SNT-a, consider l ∈ NTt with dˆl > t. If
l deviates and i changes but we remain in Case I then NTt is unchanged and still
contains l. If l deviates and i → 0 then, we go to Case III and SNTt := bt ∪ st and still
contains l. For SNT-b, consider l ∈ SNTt that deviates with dˆl > t. Again, either we
remain in this case and SNTt is unchanged or i → 0 and we go to Case III. But now
SNTt still contains all bt and is therefore unchanged for all agents with dˆk > t.
(I-2) Buyer k ∈ bt with dˆk = t and bk ≥ pt and SNTt := bt . For SNT-a, consider l ∈ NTt
with dˆl > t. We remain in this case for any deviation by buyer l because buyer k
will ensure i 6= 0, and so SNTt remains unchanged and still contains l. For SNT-b, if
l ∈ SNTt with dˆl > t deviates we again remain in this case and SNTt is unchanged.
(I-3) Some seller with dˆk > t and no buyer with dˆk′ = t willing to accept the price.
SNTt := bt \ checkedB . For SNT-a, consider l ∈ NTt with dˆl > t. First, suppose
l ∈ checkedB and i 6= l. If l deviates but still has dˆl > t, then even if i := l then we
remain in this case and l does not enter SNTt . Second, suppose l ∈ checkedB and
(i = l). If l deviates but still has dˆl > t, then even if (i = 0) and (j = 0), we go
to Case III and SNTt = ∅ and l does not enter SNTt . Third, suppose l ∈
/ checkedB
ˆ
ˆ
and dl > t. Deviating while dl > t has no effect and we remain in this case and l
remains in SNTt . For SNT-b, consider l ∈ SNTt with dˆl > t, i.e. with l ∈
/ checkedB .
If l deviates but dˆl > t, then this has no effect and we remain in this case and SNTt
remains unchanged.
Case II. (j 6= 0) and (i = 0). NTt := st . Symmetric with Case I.
Case III. (i = 0) and (j = 0). NTt := bt ∪ st .
(III-1) ∀k ∈ bt · (dˆk = t) but ∃k′ ∈ st · (dˆk′ > t) and SNTt := bt ∪ st . For SNT-a, consider
l ∈ NTt with dˆl > t. This must be an ask. If l deviates but we remain in this case,
then l remains in SNTt . If j := l, then we go to Case II and SNTt := st and l remains
in SNTt . For SNT-b, consider l ∈ SNTt with dˆl > t, which must be an ask. If l
deviates but we remain in this case, SNTt is unchanged. If l deviates and j := l, then
we go to Case II, SNTt := st , and buyers bt are removed from SNTt . But this is OK
because all buyers depart in period t anyway.
(III-2) ∀k ∈ st cdot(dˆk = t) but ∃k′ ∈ bt · (dˆk′ > t) and SNTt := bt ∪ st . Symmetric to Case
III-1.
(III-3) ∀k ∈ bt · (dˆk = t) and ∀k ∈ st cdot(dˆk = t). SNTt := bt ∪ st . SNT-a and SNT-b are
trivially met because no bids or asks have departure past the current period.
174

Chain: An Online Double Auction

(III-4) ∃k ∈ bt · (dˆk > t) and ∃k′ ∈ st · (dˆk′ > t) and SNTt := ∅. For SNT-a, consider l ∈ NTt
with dˆl > t. Assume that l is a bid. If l deviates and dˆl > t and i = 0 then we remain
in this case and l is not in SNTt . If l deviates and dˆl > t but i := l, then we go to
Case I and we are necessarily in Sub-case (I-a) because dˆl > t and there can be no
other bid willing to accept the price (else i 6= 0 in the first place). Thus, we would
have SNTt := bt \ checkedB and l would not be in SNTt . For SNT-b, this is trivially
satisfied because there are no agents l ∈ SNTt .

Lemma 5 The set of active agents (other than i) in period t in Chain is independent of
i’s report while agent i remains active, and would be unchanged if i’s arrival is later than
period t.
Proof: Fix some arrival period âi . Show for any âi ≥ ai , the set of active agents in period
t ≥ âi while i is active is the same as At without agent i’s arrival until some a′i > t. Proceed
by induction on the number of periods that t is after âi . For period t = âi this is trivial.
Now consider some period âi + r, for some r ≥ 1 and assume the inductive hypothesis for
′
âi + r − 1. Since i is still active then, i ∈ SNTt for t′ = âi + r − 1, and therefore the other
′
agents in SNTt that survive into this period are independent of agent i’s report by strong
no-trade condition (b). This completes the proof.

Lemma 6 The price constructed from admission price q̌ and post-arrival price p̌ is valueindependent and monotonic-increasing when the matching rule in Chain is well-defined,
the strong no-trade construction is valid, and agent patience is bounded by K.
Proof: First fix âi , dˆi and θ−i . To show value-independence (B1), first note that q̌ is
value-independent, since whether or not i ∈ SNTt in some pre-arrival period t is valueindependent by strong no-trade condition (a) and price zi (H t , At \ i, ω) in such a period
is agent-independent by definition. Term p̌ is also value-independent: the decision period
t∗ to agent i, if any, is independent of ŵi since the other agents that remain active are
independent of agent i while it is active by Lemma 5, and whether or not i ∈ SNTt is
value-independent by strong no-trade (a); and the price in t∗ is value-independent when
the set of other active agents are value-independent.
Now fix θ−i and show the price is monotonically-increasing in a tighter arrival-departure
interval (B2). First note that q̌ is monotonic-increasing in [âi , dˆi ] ⊂ [ai , di ] because an earlier
dˆi and later âi increases the domain t ∈ [dˆi − K, âi − 1] on which q̌ is defined. Fix some
âi ≥ ai . Argue the price increases with earlier d′i ≤ dˆi , for any dˆi > âi . To see this, note that
either dˆi < t∗ and so pi (âi , d′i , θ−i , ω) = ∞ for all d′i ≤ dˆi , or dˆi ≥ t∗ and the price is constant
until dˆi < t∗ at which point it becomes ∞. Fix some dˆi ≥ ai . Argue the price increases with
later a′i ≥ âi , where âi ≥ dˆi − K. First, while a′i ≤ t∗, then p̌ is unchanged by Lemma 5.
The interesting case is when a′i > t∗, especially when q̌(âi , dˆi , θ−i , ω) < p̌(âi , dˆi , θ−i , ω).
By reporting a later arrival, the agent can delay its decision period and perhaps hope to
achieve a lower price. But, note that in this case t∗ ∈ [dˆi − K, a′i − 1] since dˆi − K ≤ âi and
t∗ ∈ [âi , a′i − 1] and so q̌(a′i , dˆi , θ−i , ω) ≥ p̌(H t∗ , At∗ \ i, ω) because q̌ now includes the price
in period t∗ since i ∈
/ SNTt∗ in that pre-arrival period by Lemma 5. Overall, we see that
although p̌ may decrease, max(q̌, p̌) cannot decrease.

175

Bredin, Parkes and Duong

Lemma 7 A strongly truthful, canonical dynamic DA must define price pi (âi , dˆi , θ−i , ω) ≥
zi (H t∗ , At∗ \ i, ω) where t∗ is the decision period for bid i (if it exists). Moreover, the bid
must be priced-out in period t∗ if it is not matched.
Proof: (a) First, suppose zi (H t∗ , At∗ \ i, ω) > ŵi but bid i is not priced-out and instead
survives as an active bid into the next period. But with i ∈
/ SNTt∗ , the set of active bids in
period t ∗ +1 need not be independent of agent i’s bid and the price zi (H t∗+1 , At∗+1 \ i, ω)
need not be agent-independent. Yet, canonical rule (iii) requires that this price be used
to determine whether or not the agent matches, and so the dynamic DA need not be
truthful. (b) Now assume for contradiction that pi (âi , dˆi , θ−i , ω) < zi (H t∗ , At∗ \ i, ω). First,
if zi (H t∗ , At∗ \ i, ω) < ∞, then an agent with value pi (âi , dˆi , θ−i , ω) < wi < zi (H t∗ , At∗ \ i, ω)
will report ŵi = zi (H t∗ , At∗ \ i, ω) + ǫ and trade now for a final payment less than its true
value (whereas it would be priced-out if it reported its true value). If zi (H t∗ , At∗ \i, ω) = ∞,
then pi (âi , dˆi , θ−i , ω) < zi (H t∗ , At∗ \i, ω) implies that some bids will survive this period even
though they are priced-out by the matching rule and not in the strong no-trade set. This
compromises the truthfulness of the dynamic DA, as discussed in part (a).

Lemma 8 A strongly truthful, canonical and individual-rational dynamic DA must define
price pi (âi , dˆi , θ−i , ω) ≥ q̌(âi , dˆi , θ−i , ω), and a bid with ŵi < q̌(âi , dˆi , θ−i , ω) must be pricedout upon admission.
Proof: Suppose dˆi < âi + K so that [dˆi − K, âi − 1] is non-empty. For dˆi = âi + K − 1,
when t = dˆi − K is a decision period (and i ∈
/ SNTt ), we have
pi (âi , dˆi , θ−i , ω) ≥ pi (dˆi − K, dˆi , θ−i , ω) ≥ zi (H t , At \ i, ω),

(24)

where the first inequality is by monotonicity (B2) and the second follows from Lemma 7
since dˆi − K is a decision period, and would remain one with report θi′ = (dˆi − K, dˆi , wi′ )
by Lemma 5. This establishes pi (âi , dˆi , θ−i , ω) ≥ q̌(âi , dˆi , θ−i , ω) for dˆi = âi + K − 1. When
dˆi = âi + K − 2, then we need Eq. (24), and also when t = dˆi − K + 1 is a decision period
(and i ∈
/ SNTt ) we have,

pi (âi , dˆi , θ−i , ω) ≥ pi (dˆi − K + 1, dˆi , θ−i , ω) ≥ zi (H t , At \ i, ω),

(25)

by the same reasoning as above. This generalizes to di = ai + K − r for r ∈ {2, . . . , K}
to establish pi (âi , dˆi , θ−i , ω) ≥ q̌(âi , dˆi , θ−i , ω) for the general case. To see the bid must be
priced-out when ŵi < q̌(âi , dˆi , θ−i , ω), note that if it were to remain active it could match in
the matching rule and by canonical (iii) need to trade, and thus fail individual-rationality
since the payment collected would be more than the value.


References
Abdulkadiroǧlu, A., Pathak, P. A., Roth, A. E., & Sönmez, T. (2006). Changing the Boston
school choice mechanism. Tech. rep., National Bureau of Economic Research Working
Paper No. 11965.
Archer, A., & Tardos, E. (2001). Truthful mechanisms for one-parameter agents. In Proceedings of the 42nd IEEE Symposium on Foundations of Computer Science, pp. 482–491.
176

Chain: An Online Double Auction

Babaioff, M., & Nisan, N. (2004). Concurrent auctions across the supply chain. Journal of
Artificial Intelligence Research, 21, 595–629.
Babaioff, M., Nisan, N., & Pavlov, E. (2001). Mechanisms for a spatially distributed market.
In Proceedings of the 5th ACM Conference on Electronic Commerce, pp. 9–20.
Babaioff, M., & Walsh, W. E. (2005). Incentive-compatible, budget-balanced, yet highly
efficient auctions for supply chain formation. Decision Support Systems, 39, 123–149.
Blum, A., Sandholm, T., & Zinkevich, M. (2006). Online algorithms for market clearing.
Journal of the ACM, 53, 845–879.
Chatterjee, K., & Samuelson, L. (1987). Bargaining with two-sided incomplete information:
An infinite horizon model with alternating offers. Review of Economic Studies, 54,
175–192.
Chu, L. Y., & Shen, Z. M. (2007). Truthful double auction mechanisms for e-marketplace.
Operations Research. To appear.
Cliff, D., & Bruten, J. (1998). Simple bargaining agents for decentralized market-based
control.. In Proceedings of the European Simulation Multiconference – Simulation Past, Present and Future, pp. 478–485, Manchester, UK.
Copeland, T. E., & Weston, J. F. (1992). Financial Theory and Corporate Policy (Third
edition). Addison-Wesley, Reading, MA.
Das, R., Hanson, J. E., Kephart, J. O., & Tesauro, G. (2001). Agent-human interactions
in the continuous double auction. In Proceedings of the 17th International Joint
Conference on Artificial Intelligence, pp. 1169–1187.
Deshmukh, K., Goldberg, A. V., Hartline, J. D., & Karlin, A. R. (2002). Truthful and competitive double auctions. In Proceedings of the European Symposium on Algorithms,
pp. 361–373.
Gerkey, B. P., & Mataric, M. J. (2002). Sold!: Auction methods for multirobot coordination.
IEEE Transactions on Robotics and Automation, 18 (5), 758–768.
Gjerstad, S., & Dickhaut, J. (1998). Price formation in double auctions. Games and
Economic Behavior, 22 (1), 1–29.
Goldberg, A., & Hartline, J. (2003). Envy-free auctions for digital goods. In Proceedings of
the 4th ACM Conference on Electronic Commerce, pp. 29–35.
Gonen, M., Gonen, R., & Pavlov, E. (2007). Generalized trade reduction mechanisms. In
Proceedings of the 8th ACM Conference on Electronic Commerce, pp. 20–29.
Gusfield, D., & Irving, R. W. (1989). The Stable Marriage Problem: Structure and Algorithms. MIT Press, Cambridge, MA.
Hajiaghayi, M. T., Kleinberg, R., Mahdian, M., & Parkes, D. C. (2005). Online auctions with
re-usable goods. In Proceedings of the 6th ACM Conference on Electronic Commerce,
pp. 165–174.
Hajiaghayi, M. T., Kleinberg, R., & Parkes, D. C. (2004). Adaptive limited-supply online
auctions. In Proceedings of the 5th ACM Conference on Electronic Commerce, pp.
71–80.
177

Bredin, Parkes and Duong

Hassoun, M. H. (1995). Fundamentals of Artificial Neural Networks. MIT Press, Cambridge,
MA.
Huang, P., Scheller-Wolf, A., & Sycara, K. (2002). Design of a multi-unit double auction
e-market. Computational Intelligence, 18, 596–617.
Lagoudakis, M., Markakis, V., Kempe, D., Keskinocak, P., Koenig, S., Kleywegt, A., Tovey,
C., Meyerson, A., & Jain, S. (2005). Auction-based multi-robot routing. In Proceedings
of the Robotics Science and Systems Conference, pp. 343–350.
Lavi, R., & Nisan, N. (2005). Online ascending auctions for gradually expiring goods. In
Proceedings of the ACM-SIAM Symposium on Discrete Algorithms, pp. 1146–1155.
Lavi, R., & Nisan, N. (2000). Competitive analysis of incentive compatible on-line auctions.
In Proceedings of the 2nd ACM Conference on Electronic Commerce, pp. 233–241.
Lin, L., & Zheng, Z. (2005). Combinatorial bids based multi-robot task allocation method.
In Proceedings of the 2005 IEEE International Conference on Robotics and Automation, pp. 1145–1150.
McAfee, R. P. (1992). A dominant strategy double auction. Journal of Economic Theory,
56 (2), 434–450.
Myerson, R. B., & Satterthwaite, M. A. (1983). Efficient mechanisms for bilateral trading.
Journal of Economic Theory, 29, 265–281.
Pai, M., & Vohra, R. (2006). Optimal dynamic auctions. Tech. rep., Kellogg School of
Management, Northwestern University.
Parkes, D. C. (2007). Online mechanisms. In Nisan, N., Roughgarden, T., Tardos, E., &
Vazirani, V. (Eds.), Algorithmic Game Theory, chap. 16. Cambridge University Press.
Parkes, D. C., Kalagnanam, J. R., & Eso, M. (2001). Achieving budget-balance with
Vickrey-based payment schemes in exchanges. In Proceedings of the 17th International Joint Conference on Artificial Intelligence, pp. 1161–1168.
Porter, R. (2004). Mechanism design for online real-time scheduling. In Proceedings of the
5th ACM Conference on Electronic Commerce, pp. 61–70.
Preist, C., & van Tol, M. (1998). Adaptive agents in a persistent shout double auction. In In
Proceedings of the First International Conference on Information and Computation
Economies, pp. 11–18.
Rust, J., Miller, J., & Palmer, R. (1994). Characterizing effective trading strategies: Insights
from the computerized double auction tournament. Journal of Economic Dynamics
and Control, 18, 61–96.
Sandholm, T. (2000). Issues in computational Vickrey auctions. International Journal of
Electronic Commerce, 4 (3), 107–129.
Satterthwaite, M. A., & Williams, S. R. (1989). Bilateral trade with the sealed bid k-double
auction: Existence and efficiency. Journal of Economic Theory, 48, 107–133.
Smith, V. L. (1962). An experimental study of competitive market behavior. Journal of
Political Economy, 70, 111–137.
178

Chain: An Online Double Auction

Tesauro, G., & Bredin, J. (2002). Strategic sequential bidding in auctions using dynamic programming. In Proceedings of the First International Joint Conference on Autonomous
Agents and Multiagent Systems, pp. 591–598, Bologna, Italy.
Yoon, K. (2001). The Modified Vickrey Double Auction. Journal of Economic Theory, 101,
572–584.

179

Journal of Artificial Intelligence Research 30 (2007) 51 - 100

Submitted 03/07; published 09/07

Graph Abstraction in Real-time Heuristic Search
Vadim Bulitko
Nathan Sturtevant
Jieshan Lu
Timothy Yau

BULITKO @ UALBERTA . CA
NATHANST @ CS . UALBERTA . CA
JIESHAN @ CS . UALBERTA . CA
THYAU @ UALBERTA . CA

Department of Computing Science, University of Alberta
Edmonton, Alberta, T6G 2E8, CANADA

Abstract
Real-time heuristic search methods are used by situated agents in applications that require the
amount of planning per move to be independent of the problem size. Such agents plan only a
few actions at a time in a local search space and avoid getting trapped in local minima by improving their heuristic function over time. We extend a wide class of real-time search algorithms
with automatically-built state abstraction and prove completeness and convergence of the resulting
family of algorithms. We then analyze the impact of abstraction in an extensive empirical study in
real-time pathfinding. Abstraction is found to improve efficiency by providing better trading offs
between planning time, learning speed and other negatively correlated performance measures.
Keywords: learning real-time heuristic search, state abstraction, goal-directed navigation.

1. Introduction and Motivation
In this paper we study the problem of agent-centered real-time heuristic search (Koenig, 2001).
The distinctive property of such search is that an agent must repeatedly plan and execute actions
within a constant time interval that is independent of the size of the problem being solved. This
restriction severely limits the range of applicable algorithms. For instance, static search algorithms
(e.g., A* of Hart, Nilsson, & Raphael, 1968), re-planning algorithms (e.g., D* of Stenz, 1995),
anytime algorithms (e.g., ARA* of Likhachev, Gordon, & Thrun, 2004) and anytime re-planning
algorithms (e.g., AD* of Likhachev, Ferguson, Gordon, Stentz, & Thrun, 2005) cannot guarantee a
constant bound on planning time per action. LRTA* provides such guarantees by planning only a
few actions at a time and updating its heuristic function, but the solution quality can be poor during
a lengthy convergence process (Korf, 1990; Ishida, 1992).
As a motivating example, consider navigation in gridworld maps in commercial computer
games. In such games, an agent can be tasked to go to any location on the map from its current
location. The agent must react quickly to the user’s command regardless of the map’s size and
complexity. Consequently, game companies impose a time-per-action limit on their pathfinding algorithms. As an example, Bioware Corp., a major game company, limits planning time to 1-3 ms
for all pathfinding units (and there can be many units planning simultaneously).
An additional challenge comes in the form of limited sensing in virtual reality trainers where
Artificial Intelligence controlled characters may not have access to the entire map a priori, in order
to avoid unrealistic behavior (Dini, van Lent, Carpenter, & Iyer, 2006). Such agents have to build
an internal map model based on sensing a limited amount of the map around their position.
An efficient search agent would minimize the delay incurred while planning its actions, explore
and learn the environment quickly, and always discover an optimal path to the goal. Unfortunately,
c
2007
AI Access Foundation. All rights reserved.

B ULITKO , S TURTEVANT, L U , & YAU

these measures are negatively correlated (or antagonistic) in that optimizing performance for one
measure results in worse performance for one of the others. For instance, reducing the amount of
planning done before each action improves the agent’s response time, but leads to slower learning
due to lower-quality actions taken by the agent.
We propose to use graph abstraction to improve efficiency of search agents and make the
following four contributions. First, we introduce a new algorithm, Path Refinement Learning
Real-time Search (PR LRTS)1 , which enhances existing real-time heuristic search algorithms with
automatically-built graph abstraction. PR LRTS learns its heuristic function in an abstract space
thereby substantially accelerating learning. Actions in the abstract space are then refined to actions
in the environment by the A* algorithm. This approach allows agents to generate actions in constant
time, explore the environment quickly, and converge to near-optimal solutions. In this paper we use
the previously published clique abstraction (Sturtevant & Buro, 2005). Our contributions specific
to abstraction are three-fold. First, we introduce the initial clique building and the repair procedure
in more detail than previously published. Second, we prove a worst-case bound on suboptimality
of the path induced by abstraction. Third, we present the first application of state abstraction to
real-time heuristic search.
The standard practice in the heuristic search literature is to promote new algorithms as trading a
“small” amount of one performance measure for a “large” gain in another performance measure. For
instance, state abstraction in non-real time heuristic search has been shown to trade “little” solution
quality for a “substantial” reduction in running time (e.g., Holte, Mkadmi, Zimmer, & MacDonald,
1996; Botea, Müller, & Schaeffer, 2004). Unfortunately, it is not always clear whether the tradeoffs are made optimally. As the second contribution, we demonstrate that PR LRTS outperforms
a number of other algorithms with respect to two antagonistic measures (e.g., learning speed and
amount of planning per action).
As the third contribution, we analyze effects of abstraction on search with respect to commonly
used performance measures: solution suboptimality, amount of planning per action, total travel,
total planning time, and memory footprint. Knowing the effects deepens our understanding of realtime heuristic search methods as well as guides a practitioner in selecting the most appropriate
search algorithm configuration for her application. Fourth, we show theoretically that PR LRTS
unifies and extends several well known existing heuristic search algorithms and satisfies the realtime operation, completeness, and convergence properties. This contribution can be viewed as a
follow-up to previous unification and extension efforts (Bulitko & Lee, 2006).
The rest of the paper is organized as follows. We begin by formulating the problem of real-time
heuristic search in Section 2. The new algorithm, PR LRTS, is described in Section 4. Empirical
results follow in Section 5. Theoretical results are presented in Section 6. We then review existing
agent-centered search algorithms as well as work on automatic graph abstraction in Section 7. The
paper is concluded by a discussion of current limitations and future research.

2. Real-time Heuristic Search
The defining property of real-time heuristic search is that the amount of planning performed by an
agent per action has a constant upper-bound that does not depend on the problem size. Low bounds
are preferred in applications, as they guarantee the agent’s fast response when presented with a new
goal. A real-time search agent plans its next action by considering states in a local search space
1. An early version of this algorithm was published as a conference paper (Bulitko, Sturtevant, & Kazakevich, 2005).

52

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

1 sc ← s0
2 while sc 6= sg do
3
sense the environment, update agent’s model
4
compute partial path p that originates in the current state sc
5
execute p
6
update current state sc
7 end while
Figure 1: Real-time heuristic search (a single trial).
surrounding its current position. A heuristic function (or simply heuristic) estimates the cumulative
cost between a state and the goal, and is used by the agent to rank available actions and select the
most promising one. This process is shown schematically in Figure 1. Agent’s current state sc is
set to the initial state s0 in line 1. As long as the goal sg is not reached (line 2), the agent senses
the environment around it (see Section 3 for details) and updates its model of the search graph it
is operating on in line 3. Then it computes a (partial) path from its current state toward the goal
state in line 4. The real-time property requires that lines 3 and 4 execute in a constant-bounded time
regardless of problem size. This is accomplished by calling a real-time heuristic search algorithm
in line 4. In this paper, we discuss three candidate algorithms: LRTA* in Section 2.1, LRTS in
Section 2.2, and PR LRTS in Section 4. Each of them would be called from line 4. The agent then
executes the path in line 5 and updates its current state in line 6.
A trial is defined as the agent’s problem-solving experience while traveling from its start state
to the goal state. Once the goal state is reached, the agent is teleported to the start state and the next
trial begins. A convergence process is defined as the first sequence of trials until the agent no longer
updates its heuristic function or its model of the search problem. The first trial without such updates
is the final trial and the learning process is said to have converged.
2.1 Learning Real-time A* (LRTA*)
We first review the best known real-time heuristic search algorithm, Learning Real-Time A*
(LRTA*) (Korf, 1990). The algorithm is shown in Figure 2. In line 1, a d-ply breadth-first search
with duplicate detection is used to find frontier states precisely d actions away from the current state
s. The standard path-max (Mero, 1984) technique is used to deal with possible inconsistencies in
the heuristic function when computing g + h-values. The value of each state, ŝ, is the sum of the
cost of a shortest path from sc to ŝ, denoted by g(s, ŝ), and the estimated cost of a shortest path
from ŝ to sg (i.e., the heuristic value h(ŝ, sg )). The state that minimizes the sum is identified as s0
in line 2. The heuristic value of the current state s is updated in line 3. Finally, a path of one action
toward the most promising frontier state s0 is returned in line 4.
path LRTA*(sc , sg , d)
1
2
3
4

generate successor states of sc up to d actions away
find state s0 with the lowest g(sc , s0 ) + h(s0 , sg )
update h(sc , sg ) to g(sc , s0 ) + h(s0 , sg ) if it is greater than the current h
return the first action along an optimal path from sc to s0
Figure 2: The LRTA* algorithm.

53

B ULITKO , S TURTEVANT, L U , & YAU

2.2 Learning Real-time Search (LRTS)
LRTS extends LRTA* in three ways: it puts a weight on the heuristic function, it uses the maxof-min learning rule, and it utilizes backtracking. We review these extensions in more detail in
Section 7.2 and walk through LRTS operation below. LRTS has three control parameters: lookahead
d ∈ N, optimality weight γ ∈ (0, 1], and learning quota T ∈ [0, ∞]. It operates as follows. In the
current state sc , the agent running LRTS conducts a full-width d-ply lookahead search (line 1 in
Figure 3). At each ply, it finds the most promising state (line 2). Assuming that the initial heuristic
h is admissible, it can safely increase h(sc ) to the maximum among the f -values of promising states
for all levels (line 3). If the total learning amount u (updated in line 4) exceeds the learning quota
T , the agent backtracks to the previous state from which it planned (lines 5, 8). Otherwise, it returns
a path of d moves between the current state sc and the most promising state at level d (line 6). The
learning amount u is reset to 0 when the agent is in the start state (i.e., at the beginning of each trial).
path LRTS(sc , sg , d, γ, T )
1
2
3

generate successor states of sc , i actions away, i = 1 . . . d
on level i, find the state si with the lowest f (si ) = γ · g(sc , si ) + h(si , sg )
update h(sc , sg ) to max f (si ) if it is greater than the current h

4
5
6
7
8
9

increase the amount of learning u by ∆h
if u ≤ T then
return a path of d actions from sc to sd
else
return a path of d actions to backtrack to the previous state, set u = T
end if

1≤i≤d

Figure 3: The LRTS algorithm.
LRTS parameters have been previously studied at length (Bulitko & Lee, 2006). Here we summarize the trends. Higher lookahead d reduces convergence travel, convergence memory, and suboptimality. However, it increases the first-move lag. A lower heuristic weight γ leads to less optimal
solutions and, generally speaking, reduces convergence travel and convergence memory. First-move
lag is not influenced by γ. A lower learning quota T causes more backtracking and tends to reduce
convergence travel and convergence memory; T does not affect the first-move lag.
2.3 Notation
Definition 2.1 A search problem is defined as a tuple (G, c, s0 , sg , h0 ) where G = (S, E) is a
directed weighted graph (henceforth search graph). S is a finite set of states (or vertices) and
E ⊂ S × S is a finite set of edges between them. The edge weights are defined by the cost function
c : E → (0, ∞) with c(s1 , s2 ) being the travel cost for the edge e = (s1 , s2 ). s0 ∈ S is the start
state, sg ∈ S is the goal state, and h0 : S → [0, ∞) is the initial heuristic function. We assume
that h0 (sg ) = 0. Out-edges of a state are called moves or actions. The number of out-edges (i.e.,
out-degree of a state) is called the branching factor of a state.
Definition 2.2 A solution to a search problem is a path from the start state s0 to the goal state sg .
The path is denoted by (s0 , s1 , . . . , sg ) where each si is a valid state and there is a valid edge for
each pair of states (si , si+1 ). The travel cost of a path is the sum of travel costs of its edges.
54

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Definition 2.3 At all times, a search agent resides in a single search state sc ∈ S called the current
state. The agent can change its current state only by executing actions and thus incurring travel
cost. Initially, the current state coincides with the start state s0 . An agent is said to succeed when it
makes its current state coincide with the goal state sg .
We assume that the goal state is reachable from any state the agent can get to from its start
state. This is needed for completeness in all real-time heuristic search algorithms. We also follow
the standard practice in real-time heuristic search literature and assume that the environment is
stationary and deterministic. Additionally, to support backtracking (Shue & Zamani, 1993; Shue,
Li, & Zamani, 2001) (i.e., reversing agent’s actions), we require that every action has a reverse
action. This is needed only when backtracking is enabled in our algorithm.
Definition 2.4 The travel cost from state s1 to state s2 denoted by dist(s1 , s2 ) is defined as the cost
of a shortest path from s1 to s2 . Throughout the paper, we will assume that dist satisfies the triangle
inequality: ∀s1 , s2 , s3 ∈ S [dist(s1 , s3 ) ≤ dist(s1 , s2 ) + dist(s2 , s3 )]. Then, for any state, s, h∗ (s)
is defined as the minimal travel cost to the goal: h∗ (s) = dist(s, sg ). A heuristic function, h, is an
approximation of h∗ . It admissible if it does not overestimate h∗ : ∀s ∈ S [h(s) ≤ h∗ (s)]. The value
of h in state s will be referred to as the heuristic value of state s. We assume that for any heuristic
function h(sg ) = 0 which trivially holds for an admissible h.
In our experiments we break all ties between moves in a fixed fashion (e.g., always prefer the
action “north”, then “north east”, then “east”, etc.) which entails that the agent’s behavior will be
identical on all trials after the final trial. It does not necessarily mean that the entire search graph is
explored or the learned heuristic is accurate for all states.
Definition 2.5 Convergence travel is the cumulative cost of all edges traversed by the agent during
the convergence process. Convergence planning is the amount of all planning effort expended by the
agent during the convergence process. The first-move lag is the amount of planning effort expended
by the agent on the first move of its final trial. Convergence memory is measured as the total
number of heuristic values stored during the convergence process. The standard practice in the realtime heuristic search literature (e.g., Korf, 1990; Shimbo & Ishida, 2003) is to store the heuristic
values in a hash table. Hash table misses are handled by a procedurally specified initial heuristic h0
(e.g., the Manhattan distance in grid-based pathfinding). Then convergence memory is the number
of entries in the hash table after convergence. Finally, suboptimality is defined in percentage points
as the final-trial solution cost excess relative to the shortest-path cost. For instance, if the agent
incurred the travel cost of 120 and the shortest-path cost is 100, the suboptimality is 20%.
We measure planning effort in two ways. First, we report the number of states the algorithm
“touched” (i.e., considered) during planning. This measure is called edges traversed (e.g., Holte
et al., 1996, p. 325). Second, we report physical CPU time, measured on a 2.0GHz PowerPC G5
computer with gcc 4.0 under Mac OS 10.4.8. We measure convergence memory in terms of the
number of heuristic values stored. This is meaningful because each heuristic value stored takes the
same fixed amount of memory (i.e., double type of C++) in our implementation of each algorithm.
Definition 2.6 A search algorithm exhibits real-time performance on a heuristic search problem if
its planning effort per move is constant-bounded and the constant is independent of the problem size
(assuming a fixed maximum branching factor).
55

B ULITKO , S TURTEVANT, L U , & YAU

The objectives of a real-time search agent are to be complete (i.e., to arrive at a goal state on
every trial), to converge (i.e., finish the learning process after a finite number of trials), and to
minimize the five performance measures described above. In the rest of the paper we will discuss
how existing and the new algorithms compare in terms of these objectives.
2.4 Application: Goal-directed Navigation
One of the motivating applications of heuristic search is goal-directed navigation, also known as
pathfinding. It is a special case of the heuristic search problem formalized in the previous section
where the search graph (S, E) is defined by a terrain map. Thus, the states/vertices correspond to
geographical positions on a map, the edges describe passability or blocking, and the cost function
represents the difficulty/time of traversing the terrain.
Real-time pathfinding is motivated primarily by time-sensitive robotics (e.g., Koenig & Simmons, 1998; Koenig, 1999; Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjou, & Shimada,
1999; Koenig, Tovey, & Smirnov, 2003) and computer games. The latter include real-time strategy
games (e.g., Blizzard Entertainment, 2002), first-person shooters (e.g., id Software, 1993), and roleplaying games (e.g., BioWare Corp., 1998). In all of these, time plays a critical role since a number
of agents can perform pathfinding simultaneously and gamers would like rapid response and fluid
gameplay. As a result, pathfinding has become a major computational expense: in “Age of Empires
II” (Ensemble Studios, 1999) it takes up to 60-70% of the simulation time (Pottinger, 2000).
In this paper, we follow the footsteps of Furcy and Koenig (2000), Shimbo and Ishida (2003),
Koenig (2004), Botea et al. (2004), Hernández and Meseguer (2005a, 2005b), Sigmundarson and
Björnsson (2006), Koenig and Likhachev (2006) and situate our empirical study in navigation on
two-dimensional grid-based maps. The cells are square and each cell is connected to four cardinally
(i.e., west, north, east, south) and four diagonally neighboring cells. Each cell can be occupied by
an agent (i.e., free) or by a wall (i.e., blocked).
Each free grid cell constitutes a vertex/state in the search space S. If the agent can travel between
any two free neighboring cells, s1 and s2 , an edge (s1 , s2√
) is added to the set of edges E. In this
paper, we set the edge costs to 1 for cardinal moves and to 2 for diagonal moves. The cell initially
occupied by the agent is s0 ; the target cell is sg . An example of converting a grid-based map to
a search problem defined by (G, c, s0 , sg , h0 ) is shown in Figure 4. Note that we do not allow
diagonal moves that “cut corners” and, thus, the state s6 is not connected to states s1 , s5 , s7 , sg .
This is done because a non-zero size agent will not be able to pass through a zero-width bottleneck
formed by two diagonally adjacent blocked cells. In the case when there is only one corner (e.g.,
between states
√ s5 and s6 in Figure 4), allowing to cut it would lead to the actual travel distance
exceeding 2 since a non-zero-width agent will have to walk around the corner.
Video games often feature repeated pathfinding experiences on the same map for two reasons:
(i) there are units that commute between the same source and destination (e.g., resource collectors in
real-time strategy games) and (ii) all ally units can share results of their learning (i.e., the heuristic
function). Since a trial typically improves heuristic values of many states, even a single trial of a
single unit can be of use to other units with different start states as long as they all share a goal
state. This is often the case with state abstraction as an entire region of a map (e.g., a room in a
role-playing game or the player’s home base in a real-time strategy game) can be mapped into a
single abstract state. Thus, single-trial learning experiences of multiple units can be approximated
by multi-trial learning experience of a single unit. The latter is the scenario we study in this paper, in
56

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Goal

s7

s8

sg

s6

Start

s1

s4

s5

s0

s2

s3

Figure 4: A 3×4 grid-based map (left) converted to a 9-state search graph (right). Thinner
√ cardinaldirection edges have the cost of 1, thicker diagonal edges have the cost of 2.
line with Furcy and Koenig (2000), Shimbo and Ishida (2003), Sigmundarson and Björnsson (2006)
and others.

3. Search Graph Discovery
In this paper, we do not require a search agent to know the problem in its entirety. Instead, a portion
of the search problem in the neighborhood of the current state sc is sensed by the agent at each time
step. We assume that an agent can remember parts of the problem it has sensed so far. In other
words, at all times an agent has an internal representation (model) of what the search space is like.
The model is updated as the agent discovers the search graph (line 3 of Figure 1).
Let us illustrate the exploration process in goal-directed navigation. The terrain map is initially
unknown to the agent. As it moves around the environment, grid cells whose coordinates are within
a fixed visibility radius of the agent’s current position are sensed. Formally, the agent situated in cell
(x, y) can check the status (free/blocked) of any cell (x0 , y 0 ) if |x − x0 | ≤ r and |y − y 0 | ≤ r, where
r ∈ N is a visibility radius. Thus, for any two visible cells (x0 , y 0 ) and (x00 , y 00 ) the agent can tell
if there is an edge between them and its cost. This is similar to virtual sensors used by Thalmann,
Noser, and Huang (1997).
One common approach is to assume a regular structure of the unknown part of the search
space (Koenig et al., 2003; Koenig, 2004; Bulitko & Lee, 2006; Koenig & Likhachev, 2006). For
instance, in grid-based pathfinding, the agent can assume that there are no obstacles in the gridworld
until it senses otherwise (this is sometimes called the “free space assumption”). We demonstrate
this in Figure 5, where the agent assumes that the space is obstacle-free (a) and builds its internal
model accordingly (b). Exploration reveals obstacles in the environment (c) which cause the agent
to update its model (d). We impose the restriction that a search agent never needs to add edges to its
model during exploration and the weights of discovered edges never change. In other words, agent’s
initial model is optimistic and contains a superset of edges of the actual search graph. Adding edges
or allowing arbitrary edge weight changes may require the agent to explore the environment explicitly. Combining exploration and exploitation effectively is an active research area (for early work,
refer to Sutton, 1990) and is not addressed in this paper.
Map discovery is natural in robotics where sensors have limited ranges. In software domains,
the agent can theoretically have access to an entire environment. Several types of arguments have
been made to justify restricting an agent’s senses in software domains. First, omniscient virtual
57

B ULITKO , S TURTEVANT, L U , & YAU

Actual search space:

Agent's model:

Explored actual search space:

Updated agent's model:

(a)

(b)

(c)

(d)

Figure 5: (a): initially only the part of the search space shown with solid lines is sensed by an agent
(shown as a stick figure). The agent’s model assumes a regular structure for the unknown
part (b). As the agent moves north-east, it senses an additional part of the search space
(c) and updates its model correspondingly (d).
humans tend to behave unrealistically and, thus, are less suitable for virtual reality trainers (Dini
et al., 2006). Likewise, in commercial games, revealing the entire map to an AI player is viewed
negatively as cheating. Second, it can be computationally expensive to sense (Orkin, 2006) and
reason about an entire environment (Thalmann et al., 1997; Aylett & Luck, 2000). Consequently,
localized sensing is used in large-scale multi-unit systems (Reynolds, 1987).

4. Path Refinement Learning Real-time Search (PR LRTS)
Real-time heuristic search algorithms plan using a small part of the search graph that surrounds
an agent’s current state. In order to avoid getting stuck in infinite loops, they update the heuristic
function over time. This approach guarantees that each action is planned in a constant-bounded
amount of time. The downside is slow convergence.
The central idea of PR LRTS is to address this downside by running real-time search on a
smaller abstract search graph and then refining the produced abstract path into a ground-level path.
The abstract graph is an image of the original graph under an abstraction operator. The operator
maps a region of states in the original graph to a single abstract state in the abstract graph. When
applied multiple times, a hierarchy of abstractions are formed. The hierarchy is a forest (a tree for
each connected component of the search graph) and will be formalized in Section 4.2.
A variety of terminologies have been used in the literature for discussing the relationship between states at different levels of abstraction. In different contexts the abstract states have been
referred to as clusters (Botea et al., 2004), sectors/regions (Sturtevant, 2007), and images (Holte
et al., 1996). Because the abstraction is a forest, and in line with (Bacchus & Yang, 1994; Bulitko
et al., 2005; Sturtevant & Buro, 2005), we sometimes call an image of an abstraction operator parent
and its pre-image children. These terms are not to be confused with successor states in lookahead
search. We first describe PR LRTS at an intuitive level and illustrate it with an example in Section 4.1. Then we give formal details in Section 4.2 and describe the abstraction operator in detail.
4.1 Path Refinement
PR LRTS computes paths at several levels of abstraction. First, a path is found through the most
abstract search space (at level `). Such an abstract path defines a region of the lower-level abstract
58

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

path PR LRTS(sc , sg )
1
2
3
4
5
6
7
8
9
10
11
12
13

if level(sc ) > ` then
return ∅
end if
p =PR LRTS(parent(sc ), parent(sg ))
if p 6= ∅ then
sg = child(end(p))
end if
C = {s | parent(s) ∈ p}
switch(algorithm[level(sc )])
A* : return A*(sc , sg , C)
LRTS : return LRTS(sc , sg , C)
pass-through : return p
end switch

Figure 6: The PR LRTS algorithm.
space that will be searched when refining the abstract path. This refinement proceeds incrementally
until the level-0 (i.e., ground) search space is reached and a ground path is produced. In order to
keep the amount of planning per move constant-bounded regardless of the ground space size, we
need to have a real-time algorithm on the most abstract search graph. In this paper, we use LRTS at
a fixed top level of abstraction (`), and A* for refinement at the lower levels.2 Some abstract levels
can be left as “pass-through” to merely increase the amount of state aggregation; no processing is
carried out at them. This design choice was motivated by experimentation (Section 5).
PR LRTS operates recursively as presented in Figure 6. In line 1 it checks if the states passed
to it are above the top level of abstraction on which pathfinding is to occur. If so, an empty path
is returned (line 2). Otherwise, the function calls itself recursively to compute a path between the
abstract image of sc (denoted by parent(sc ) in line 4) and the abstract image of sg . The returned
path (if non-empty as checked in line 5) is used to derive the new destination in line 6. Specifically,
the new destination sg is a child of the end of the abstract path p.3 In line 8, we compute the
corridor C comprised of pre-images of the states on path p. The corridor C will be empty if the
path p computed in line 4 was empty. Finally, we run the algorithm assigned to our current level of
abstraction (i.e., the level of sc and sg ) in lines 10 and 11. It will be the A* or the LRTS tasked to
find either a full (in the case of A*) or a partial path (in the case of LRTS) from sc to sg limited to
the set of states C. By convention, an empty corridor (C = ∅) allows A*/LRTS to search its entire
graph. Note that no processing happens at a pass-through level (line 12).4
2. Because an agent explores its environment while moving about, we actually use a Local Repair A* instead of A*. It
is described in Section 7.
3. While any child can be used, some choices may lead to better performance. Intuitively, the child chosen by
child(end(p)) should be the “closest representative” of the abstract state end(p) among children(end(p)). In
pathfinding, we implement child(s) to return the element of children(s) that is geographically closest to the average coordinates of states in children(s). Also, if the goal state sg happens to be in the pre-image of end(p) then
we pick it as child(end(p)).
4. Also note that the functions child and parent handle pass-through levels. Specifically, in line 6, the state sg will be
computed by child at the first non-pass-through level below the level at which path p is computed. Likewise, in line
8, the states s forming the corridor C are at the first non-pass-through level (level i) below the level of path p (level
j). Thus, parent(s) will apply the abstraction mapping j − i times so that parent(s) and p are both at level j.

59

B ULITKO , S TURTEVANT, L U , & YAU

Our implementation of A* is standard (Hart et al., 1968) except it is run (line 10) on a subgraph
defined by the corridor C (line 8). Our implementation of LRTS is taken from the literature (Bulitko
& Lee, 2006) and is described in Section 2.2. Like A*, we run LRTS in the corridor C.

Figure 7: The path refinement process. The original graph (level 0) is shown at the bottom. The
abstract graph (level 1) is shown at the top.
For illustration purposes, consider an example in Figure 7. In the example ` = 1, so only one
level of abstraction (shown at the top) is used in addition to the ground level (shown at the bottom).
sc is the current state while sg is the destination state. LRTS is assigned to level 1 while A* is
assigned to level 0. Subfigure (i) shows the ground state space below and one level of abstraction
above. The agent must plan a path from sc to sg located at the ground level. First, the abstract
parents of sc and sg , parent(sc ) = s0c and parent(sg ) = s0g , are located. Then LRTS with d = 3
plans three steps in the abstract space (ii). A corridor C at the ground level comprised of children
of the abstract path is then built (iii). A child representing the end of the abstract path is set as the
new destination sg (iv). Finally, A* is run within the corridor to find a path from sc to the new
destination sg (v).
While an agent is executing a path computed by PR LRTS, new areas of the search graph may
be seen. This causes updates to the abstraction hierarchy that the agent maintains. PR LRTS clears
and recomputes its abstract paths upon discovering new areas of the search graph. Also, if a ground
path proves invalid (e.g., runs into a newly discovered obstacle), the execution stops and PR LRTS
replans from the current state using the updated abstraction hierarchy.
Graph discovery can lead to arbitrary updates to the abstract search graphs an agent maintains.
In our implementation, LRTS operating on an abstract graph resets its heuristic function if its abstract search graph is updated in any way. On the other hand, updates to the ground-level graph are
limited to state and edge removals (Section 3). Consequently, the heuristic learned at the ground
level remains admissible and there is no need to reset it upon updates.
4.2 Automatic Graph Abstraction
We use the term abstraction operator (or abstraction, for short) to mean a graph homomorphism in
line with Holte et al. (1996). Namely, abstraction is a many-to-one function that maps (abstracts)
one or more states to a single abstract state. Adjacent vertices are mapped to adjacent or identical
vertices (Property 5 below). Given such a graph homomorphism function and a model of a search
problem, a PR LRTS agent builds ` additional abstract search graphs, collectively called an abstrac60

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

tion hierarchy, as follows. It first applies the graph homomorphism to the search graph of the model
(called ground-level graph). The result is an abstract search graph at level 1. The process is then
repeated until an abstract search graph at level ` is computed. Any homomorphic abstraction can be
used as long as the resulting hierarchy of abstract graphs satisfies several key properties. In the following we introduce the properties informally and illustrate them with an example. In Appendix B
we formalize them.
Property 1 Every abstract graph is a search graph in the sense of Definition 2.1 in Section 2.
Property 2 Every state has a unique abstract parent (except states at the top level of abstraction).
Property 3 Every state at any abstract level, has at least one child state below.
Property 4 Given a heuristic search problem, the number of children of any abstract state is upperbounded by a constant independent of the number of states in the ground-level graph.
A corollary of this property is that the number of ground-level states that abstract into a single
state at any fixed level of abstraction is also constant-bounded with a constant independent of the
ground-level graph size.
Property 5 (Graph homomorphism) Every edge in the search graph at a level of abstraction has
either a corresponding edge at the level above or the states connected by the edge abstract into a
single abstract state.
Property 6 If an abstract edge exists between two states then there is an edge between at least some
child of one state and some child of the other.
Property 7 Any two children of an abstract state are connected through a path whose states are all
children of the abstract state.
Property 8 The abstraction hierarchy is consistent with agent’s model of its search problem at all
times. That is, properties 1 through 7 are satisfied with respect to the agent’s model.
In this paper, we use a clique-based abstraction mechanism (Sturtevant & Buro, 2005). It operates by finding fully connected components (cliques) in the search graph and mapping each to a
single abstract state. This method of building abstractions is favored in recent analysis by Sturtevant
and Jansen (2007) and earlier analysis by Holte et al. (1996, Section 5.2) who showed that reduction
of search effort due to abstraction is maximized by minimizing edge diameter of the set of children
and maximizing its size. For any clique, its edge diameter (i.e., the maximum number of edges
between any two elements) is one while the number of states in a clique is maximized.
We present the clique-based abstraction mechanism by developing several stages of a handtraceable example. We then illustrate how each of the properties introduced above is satisfied in the
example. A formal introduction of the clique abstraction technique complete with pseudocode is
found in Appendix A. We review other ways of building abstraction in Section 7.3. Note that while
general clique computation is NP-complete, finding cliques in two-dimensional grid-based search
graphs can be done efficiently (Appendix A).
61

B ULITKO , S TURTEVANT, L U , & YAU

A single application of the abstraction procedure is illustrated in Figure 8. Cliques of size four
are first located in the graph, meaning that states s0 , s1 , s2 and s4 are abstracted into s01 . There are
no cliques of size three which are not already abstracted in the first step, so cliques of size two will
be abstracted next. This includes s5 and s3 which are abstracted into s02 , and s7 and s8 which are
abstracted into s03 . Because sg has degree 1, we add it to s03 ; however s6 has degree two, so it is
abstracted into its own parent, s04 . Adding degree 1 states to their neighbors reduces the number of
resulting abstract states but increases the edge diameter of the set of children (it becomes 2 for the
set {s7 , s8 , sg }). This is a minor detail of our abstraction that happens to be effective in grid-based
pathfinding. One can use “pure” clique abstraction as well.
s7

s8

sg

s'3

s6
s1

s4

s'4

s5
s'1

s0

s2

s'2

s3

Level 0 (original graph)

Level 1 (abstract graph)

Figure 8: Clique abstraction: the original search graph from Figure 4 shown on the left is abstracted
into the search graph on the right.

s7

s8

sg

s'3
s''2

s6
s1

s4

s'4

s5
s'1

s0

s2

s'''
1

s'2

s''1

s3

Level 0 (original graph)

Level 1

Level 2

Level 3

Figure 9: Three iterations of the clique abstraction procedure.
The abstraction process can be successively applied until a single abstract state for each connected component of the original graph remains (Figure 9, Level 3). We can now illustrate the
abstraction properties with Figure 9. Property 1 requires that each of the four abstraction levels
in Figure 9 is a search graph in the sense of Definition 2.1 in Section 2. Property 2 requires each
62

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

state at levels 0, 1, and 2 to have a unique parent at the level above. Property 3 requires that each
state at levels 1, 2, and 3 has a non-empty set of children at the level below. Property 4 places an
upper bound on the number of children each abstract state can have. In this example the bound is
4. Properties 5 and 6 require that, for any two abstract states connected by a path, their parents and
any of their children are also connected. Consider, for instance, abstract states s02 and s03 . They are
connected at level 1 as there is an abstract path p = (s02 , s01 , s04 , s03 ) between them. Thus, any child
of s02 is also connected to any child of s03 at level 0. For instance, s3 is connected to s7 . Property 7
requires that all children of node s01 are connected by an internal path within s01 . (s00 , s01 , s02 , s04 ) form
a clique, so this property is satisfied.
The costs of abstract edges (e.g., edge (s01 , s02 ) in Figure 8) can be defined in an arbitrary way
as long as the resulting search graph satisfies properties in Section 2. However, for better performance, a low-cost abstract path should be an abstraction of a low-cost ground path. In this paper we
experiment with grid-based navigation and, correspondingly, define the cost of edge (s01 , s02 ) as the
Euclidean distance between the average coordinates of children(s01 ) and children(s02 ) (Figure 10).

Figure 10: Coordinates and edge costs for all levels of the abstraction hierarchy. At the grid level
(leftmost illustration) vertex
√ coordinates are given through column/row labels. Ground
edges cost 1 (cardinal) 2 (diagonal). Abstract states are labeled with (x, y). Abstract
edges are labeled with their approximate cost.

In practice, Property 8 is satisfied by repairing an agent’s abstraction hierarchy upon updates
to the agent’s model. To illustrate, imagine an agent just discovered that discrepancies between
the terrain elevation in state s4 and s6 (Figure 8) prevent it from being able to traverse the edge
(s4 , s6 ). It will then update its model by removing the edge. Additionally, degree-one state s6 will
join the clique {s7 , s8 }. At this point, an agent’s abstraction hierarchy needs to be repaired. This is
accomplished by replacing abstract states s04 and s03 with a single abstract state s05 . The edges (s01 , s04 )
and (s04 , s03 ) will be removed. If more than one level of abstraction is used then the repair has to be
propagated to higher levels as well. The repair mechanism is presented in detail in Appendix A.2.
We will prove in Section 6 that the PR LRTS algorithm can operate with any abstraction mechanism
that satisfies the properties listed above.
63

B ULITKO , S TURTEVANT, L U , & YAU

Figure 11: Two of the the six maps used in the experiments.

5. Empirical Study
Empirical evaluation of the effects that state abstraction has on learning real-time heuristic search
is presented in four parts. In Section 5.1, we introduce the concept of trading off antagonistic
measures and demonstrate that PR LRTS makes such trade-offs efficiently. This is due to the use
of abstraction and, consequently, we investigate the effects of abstraction independently of LRTS
control parameters in Section 5.2. We then study how PR LRTS performance scales with problem
size (Section 5.3). Finally, we examine the interplay between the effects of abstraction and the LRTS
control parameters. As it is the most domain-specific study we present these details in Appendix F.
The experimental setup is as follows. We used 3000 problems randomly generated over three
maps modeled after environments from a role-playing game (BioWare Corp., 1998) and three maps
modeled after battlefields from a real-time strategy game (Blizzard Entertainment, 2002). The six
maps had 5672, 5852, 6176, 7629, 9749, and 18841 states on grids from 139 × 148 to 193 × 193.
Two maps are in Figure 11. The other four maps are shown in Appendix C. The 3000 problems were
uniformly distributed across five buckets where each bucket represents a range of optimal solution
costs. The first 600 problems had the optimal path cost in the [50, 60) range, the next 600 problems
fell into the [60, 70) bucket and so on until the last 600 problems that were in the [90, 100) bucket.
We experimented with various assignments of algorithms (A*, LRTS, none) to levels of abstraction. Through experimentation, we found that keeping LRTS at the top, A* at the bottom level
and leaving intermediate levels pass-through yielded the best results in our testbed. In the following, we present results of 160 PR LRTS configurations, denoted as LRTS` (d, γ, T ), where ` is the
level of abstraction at which LRTS with control parameters d, γ, T operates, with A* running at the
bottom level. With ` = 0, we run LRTS at the ground level and do not run A* at all. The LRTS
parameter space was as follows: ` ∈ {0, 1, 2, 3, 4}, lookahead depth d ∈ {1, 3, 5, 9}, optimality
weight γ ∈ {0.2, 0.4, 0.8, 1.0}, and learning quota T ∈ {100.0, ∞}. Two visibility radii were used:
10 and 1000. In our analysis, we will focus on the case of visibility radius of 10, in line with the
previous publications in the area (Bulitko et al., 2005; Bulitko & Lee, 2006). Experiments with
the visibility radius of 1000 yielded similar results. As a point of reference, we ran a single nonreal-time algorithm: A*. The algorithms were implemented within the Hierarchical Open Graph
framework (Sturtevant, 2005) in C++ and run on a cluster, with an aggregate of 1.7 years of Intel
Xeon 3.0GHz CPU.
64

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

5.1 Antagonistic Performance Measure Trade-off
From a practitioner’s viewpoint, this section can be viewed as a parameter selection guide. We start
by finding sets of parameters to optimize performance of PR LRTS along a single measure. Then
we will consider optimizing pairs of antagonistic performance measures.
The research on optimizing single performance measures within LRTS was published by Bulitko
and Lee (2006). In Table 1 we extend the results to include A* and PR LRTS. The best algorithms
for a single performance measure are A* and LRTA* and do not use abstraction. The only exception
is convergence planning which is an interplay between planning per move and convergence travel.
Table 1: Optimizing a single performance measure.
Measure
convergence travel
first-move lag (states touched)
conv. memory
suboptimality
conv. planning (states touched)

The best algorithm
A*
LRTA*
A*
A* or LRTA*
LRTS3 (d = 1, γ = 0.2 or 0.4, ∞) or LRTS2 (d = 1, γ = 0.2 or 0.4, ∞)

The power of abstraction comes when we attempt to optimize two negatively correlated (or
antagonistic) performance measures simultaneously. Consider, for instance, convergence travel and
first-move lag. In order to lower convergence travel, the agent needs to select better actions. This
is done by increasing the amount of planning per move which, in turn, increases its first-move
lag. As these measures are negatively correlated, performance along one measure can be traded
for performance along the other. Thus, we are interested in algorithms that make such trade-offs
efficiently. In order to make our analysis more specific, we first introduce the concept of dominance
with respect to a set of parameterized algorithms.
Definition 5.1 Algorithm A is said to dominate algorithm B with respect to performance measures
x and y on a set of problems P if A’s average performance measured in both x and y is not worse
than B’s: avgP x(A) is not worse than avgP x(B) and avgP y(A) is not worse than avgP y(B).
Algorithm C is called dominated in a set of algorithms if the set contains another algorithm that
dominates it.
The definition is illustrated in Figure 12 where non-dominated algorithms are shown as solid circles and dominated algorithms are shown as hollow circles. Intuitively, non-dominated algorithms
make the trade-off between performance measures x and y most efficiently among all algorithms in
a set. They should be considered in practice when one wants to optimize performance in both measures. Dominated algorithms can be safely excluded from consideration regardless of the relative
importance of measures x and y in a particular application.
Non-dominated algorithms for ten pairs of antagonistic measures are summarized in Table 2.
A* and weighted version of Korf’s LRTA* are extreme cases of the performance measures: A*
minimizes convergence travel and uses no heuristic memory; LRTA* minimizes first-move lag. All
non-dominated algorithms between them are PR LRTS with abstraction. In other words, abstraction
and path-refinement improve efficiency of trading off antagonistic performance measures. Figure 13
shows a dominance plot for convergence planning against first-move lag. PR LRTS forms a frontier
65

performance
measure 2

performance
measure 2

worse

worse

B ULITKO , S TURTEVANT, L U , & YAU

B

A
better

better

Nondominated
algorithms

better

performance
measure 1

better

worse

performance
measure 1

worse

Figure 12: Left: algorithm A dominates algorithm B (left). Right: all non-dominated algorithms
are shown as solid circles, dominated algorithms are shown as hollow circles.
of non-dominated algorithms (the rightmost non-dominated point is a weighted LRTA* which has
an extremely low first-move lag). Plots for other combinations are in Appendix D.

First−move Lag (states touched)

Dominated
Non−dominated

3

10

LRTS3(1,0.4,∞)
2

10

LRTS2(1,0.4,∞)

LRTS1(1,0.4,∞)

1

10

LRTS(1,0.4,∞)
4

10

5

10
Convergence Planning (states touched)

6

10

Figure 13: Dominance for convergence planning against first-move lag.
The dominance analysis above is done with respect to performance measures averaged over a
benchmark set of problems. Dominance analysis at the level of individual problems is found in
Appendix E and shows similar trends.
5.2 Effects of Abstraction on Individual Performance Measures
In this section we study effects of abstraction on individual performance measures. We arbitrarily choose three diverse LRTS parameter combinations of lookahead d, optimality weight γ, and
learning quota T : (1, 1.0, ∞), (3, 0.2, 100), (9, 0.4, ∞). The plots are in Figure 14, a qualitative
summary is in Table 3, and an analysis of the trends is below.
Convergence planning decreases with abstraction level. This is because the increase of planning per move at higher abstraction levels is overcompensated for by the decrease in convergence
travel. The exact shape of the curves is due to an interplay between these two measures.
66

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Table 2: Trading off antagonistic performance measures.
Measure 1
first-move lag
(states touched)
first-move lag
(states touched)
first-move lag
(states touched)
first-move lag
(time)
first-move lag
(time)
first-move lag
(time)
suboptimality
suboptimality
suboptimality
suboptimality

Measure 2
conv. travel
conv. memory
conv. plan.
(states touched)
conv. travel
conv. memory
conv. plan.
(time)
conv. plan.
(states touched)
conv. plan.
(time)
conv. travel
conv. memory

Non-dominated algorithms (extreme cases are in italic)
A*, LRTA*(γ = 0.4),
LRTS1...4 (d ∈ {1, 3, 5, 9}, γ ∈ {0.2, 0.4, 0.8}, T ∈ {100, ∞})
A*, LRTA*(γ = 0.2),
LRTS1...4 (d ∈ {1, 3, 5, 9}, γ ∈ {0.2, 0.4}, T ∈ {100, ∞})
LRTS1...3 (d = 1, γ = 0.4, T = ∞),
LRTA*(γ = 0.4)
A*, LRTA*(γ = 0.4),
LRTS1...4 (d ∈ {1, 3, 5, 9}, γ ∈ {0.2, 0.4}, T ∈ {100, ∞})
A*, LRTA*(γ ∈ {0.2, 0.4}),
LRTS1...4 (d ∈ {1, 3, 5}, γ ∈ {0.2, 0.4}, T ∈ {100, ∞})
A*, LRTA*(γ = 0.4),
LRTS1...3 (d ∈ {1, 3}, γ ∈ {0.2, 0.4}, T ∈ {100, ∞})
A*,
LRTS2...3 (d ∈ {1, 3, 5}, γ ∈ {0.2, 0.4, 0.8}, T ∈ {100, ∞})
A*
A*
A*

First-move lag increases with the abstraction level. This is due to the fact that the corridor at
the ground level induced by the abstract path of length d computed by LRTS at the abstract level
increases with the abstraction level. There are two additional factors affecting shape of the curves.
First, the average out-degree of abstract states varies with abstraction level. Second, boundaries of
abstract graphs can often be seen with lookahead of d = 9 at higher abstraction level.
Convergence memory decreases with abstraction level as the learning algorithm (LRTS) operates on smaller abstract maps and incurs smaller travel cost. In practice, the amount of learning in
LRTS tends to correlate tightly with its travel. For instance, for LRTS2 (3, 0.8, ∞) the correlation
between convergence memory and convergence travel is empirically measured at 0.9544 with the
confidence of 99%.
Suboptimality increases with abstraction. The increase is due to the fact that each abstraction
level progressively simplifies the ground graph topology and, while the abstract path is guaranteed to
be refinable into a ground path, it may lead the agent away from the shortest solution. An illustrative
example is given in Appendix B.1 where a refinement of a complete abstract path is 221% longer
than the optimal ground path. Derivation of a theoretical upper bound on suboptimality due to
abstraction is found in Appendix B.2. A second mechanism explains why suboptimality rises faster
with shallower LRTS searches. Specifically, A* at the ground level refines an abstract d-step path
by finding a ground-level solution from the current state to a ground representative of the end of the
abstract path. This solution is guaranteed to be optimal within the corridor and does not necessarily
have to pass geographically closely to intermediate states of the abstract path. Thus, giving A* a
corridor induced by a longer abstract path liberates it from having to plot a path to possibly far-off
intermediate states on the abstract path. This phenomenon is illustrated in Appendix B.1 where
“feeding” A* the abstract path in small fragments results in more suboptimality than giving it the
“big picture” – the abstract path in its entirety. A third factor affecting the suboptimality curves
67

4

6

33 4
x 10

22
B ULITKO
, S TURTEVANT, L U , & YAU

3
2

Convergence Travel

22

4

x 10

3
2
1

00
0

11

22

33

0 Abstraction
1
2 Level3
Abstraction
Level
Abstraction Level

44
4

First!Move Lag

6000

First!Move Lag
First!Move Lag

44

4

2000

00 0

0 000 0
0

4000
4000

4000

30

2000
2000
2000

20

00 0
0 0 0 1 1 1 2 2 2 3 33
Abstraction
Level
Abstraction
Level
Abstraction
Level

4 44

10

1000

2

x 10
6
2x 10

1.5
1.5 x 106

2

11

1.5

0.5
0.5

1

00

0.5

1500
1500
0

00

0

1000
1000
1500

1

30
1500
1500
30
0 1500
0
20
1000
1000
20

6000
6000

3030 30

6 6

11
22
33
Abstraction
Level
Abstraction Level

2000
2000
0.50.5
4000 0.5

11
00

00

x 10
x 10
22
6000
6000
6
0
x 10
02
1
2
3
1.51.5
Abstraction Level
40001.5
4000
6000 1 1

Suboptimality (%)
Convergence Memory
Suboptimality
(%)
Convergence
Memory
Convergence
Memory
Suboptimality
(%)

Convergence Travel
Convergence Travel

33

4

00

1

4 4

x 10
x 10
44

11

Convergence
Memory Memory
Convergence
Convergence
MemoryConvergence
Planning Planning
Convergence
ConvergencePlanning

First!Move Lag
First!Move
Convergence Travel
First!Move Lag
Lag
Convergence Planning
ConvergenceTravel
Travel
Convergence
Convergence
Planning
Convergence
Planning

4

x 10 4
4 x 10
4

4 4

1
2
3
Abstraction Level

4

11
22
3 3
Abstraction
Level
Abstraction
Level

4 4

500
500
1000

222 2
333 3
111 1
1Abstraction
2 Level
3
Level
Abstraction
Abstraction
Level
Abstraction
Level
Abstraction Level

1
2
3
Abstraction Level

444 4
4

4

00

500
0

00

0

10
500
10500
500
0
0 0
00 0
00 00

11
22
3 3
Abstraction
Level
Abstraction
Level

d=1,!=1,T="
d=1,!=1,T="
1
2
3
d=3,!=0.2,T=100
Abstraction Level
d=3,!=0.2,T=100
d=9,!=0.4,T="
d=9,!=0.4,T="

4

d=1,!=1,T="
d=3,!=0.2,T=100
1
2
3
1111
222 2Level333 3
Abstraction
Abstraction
Level
Abstraction
Level
Abstraction
Level
Abstraction
Level

4
444 4

d=9,!=0.4,T="

Suboptimality (%)

Suboptimality (%)
Suboptimality (%)

Figure 14: Effects of abstraction in0 PR LRTS.
Error bars indicate the standard errors and are too
d=1,!=1,T="
d=1,!=1,T="
d=1,!=1,T="
0
1
2
3
4
d=3,!=0.2,T=100
d=3,!=0.2,T=100
d=3,!=0.2,T=100
2020 20
small to see for most data points.Abstraction
Level
d=9,!=0.4,T="
d=9,!=0.4,T="
d=9,!=0.4,T="

1010 10

in the figure is the optimality weight γ. Setting γ to lower values leads to higher suboptimality
0
independently
00
0
1 of abstraction
2
3
4(Bulitko & Lee, 2006).
00
1 1 Abstraction
22
33
44
Level
Convergence
travel
decreases
with abstraction as the bottom level search is constrained within
Abstraction
Level
Abstraction Level
a narrow corridor induced by an abstract path. The decrease is most noticeable for shallow lookahead searches (d = 1 and d = 3). Algorithms using lookahead of d = 9 have low convergence
travel even without abstraction, and the convergence travel is lower bounded by double the optimal
solution cost (one optimal solution for the first trial at which the map is discovered and one for the
final trial with no map discovery or heuristic learning). Consequently, abstraction has diminished
gains for deeper lookahead (d = 9), although this effect would disappear on larger maps.
Table 3: Qualitative effects of abstraction: general trends.
measure / parameter
first-move lag
convergence planning
convergence memory
suboptimality
convergence travel

0→`→∞
↑
↓
↓
↑
↓

Given that higher abstraction reduces convergence travel, one may ask how this compares to
reducing convergence travel of non-abstract algorithms by simply terminating their convergence
process before the final trial. In Figure 15 we compare four algorithms on a single problem: A*,
non-abstract LRTS(3, 0.4, ∞) and two abstract versions: LRTS2 (3, 0.4, ∞) and LRTS4 (3, 0.4, ∞).
The left plot in the figure demonstrates a well-known fact that convergence of learning heuristic
search algorithms is non-monotinic (e.g. Shimbo & Ishida, 2003). The right plot shows the cost of
the shortest solution as a function of cumulative travel. We prefer algorithms that find shorter solutions after traveling as little as possible. In the plot, the abstract algorithms perform better (lower
68

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

A*
LRTS(3,0.4,∞)
LRTS2(3,0.4,∞)

Travel on the trial

180

LRTS4(3,0.4,∞)

160
140
120
100
80

2

4

6
Trial

8

A*
LRTS(3,0.4,∞)
LRTS2(3,0.4,∞)

200
Shortest solution found

200

180

140
120
100
80

10

LRTS4(3,0.4,∞)

160

200

400

600
800
1000
Cumulative travel

1200

1400

Figure 15: Convergence process at the level of individual trials.
curves) and are preferred. In other words, for this single problem, it is better to run abstract algorithms than non-abstract algorithms regardless of how early the convergence process is terminated.
We observe that this is not the case for all problems and for all assignments of d, γ, T we tried. In
certain cases, prematurely terminating convergence process of a non-abstract algorithm can indeed
be beneficial. Future research will investigate to what extent one can automatically select the best
algorithm and a number of trials to run it for.
5.3 Effects of Abstraction: Scaling up with Problem Size
In this section we investigate the effects of abstraction as the problem size increases. We measure the
size of the problem as the cost of a shortest path between the start and the goal position (henceforth
optimal solution cost).
Figures 16 and 17 show five performance measures plotted as bucket averages. For each data
point, we use the middle of the bucket (e.g., 55, 65, . . . ) as the horizontal coordinate. The error bars
indicate standard errors. Overall, the results demonstrate that abstraction enables PR LRTS to be
applied to larger problems by significantly dampening the increase in convergence travel, convergence planning, and convergence memory. These advantages come at the price of suboptimality and
first-move lag. The former clearly increases with abstraction when lookahead is small (Figure 16)
and is virtually bucket-independent. The lookahead of d = 9 (Figure 17) draws the curves together
as deeper lookahead diminishes effects of abstraction on suboptimality (cf. Figure 36).
First-move lag is virtually bucket-independent except in the case of d = 9 and abstraction levels
of 3 and 4 (Figure 17). There, first-move lag is capped for problems in lower buckets as the goal
is seen from the start state at these higher levels of abstraction. Consequently, LRTS computes an
abstract path that is shorter than nine moves. This leads to a smaller corridor and less work for
A* when refining the path. Consequently, the first-move lag is reduced. As the problems become
larger, LRTS has room to compute a full nine-move abstract path and the first-move lag increases.
For abstraction level 3 this phenomenon takes place up to bucket 85 where seeing the goal state from
the start state is not frequent enough to make an impact. This does not happens with abstraction level
4 as proximity of the abstract goal continues to cut the search short even for the largest problems.
Finally, we observe a minute decrease in first-move lag for larger problems. This appears to be
due to the fact that problems in the higher buckets tend to have their start state located in a cluttered
region of the map (so that the optimal solution cost is necessarily higher). Walls reduce the number
of states touched by the agent on its first move and reduce the first-move lag.
69

B ULITKO , S TURTEVANT, L U , & YAU

4

5

x 10

4
Convergence Planning

Convergence Travel

5
4
3
2
1
0

2000

400

3

300

2

1

0

55 65 75 85 95
Optimal Solution Cost

x 10

First−Move Lag

6

55 65 75 85 95
Optimal Solution Cost

200

100

0

55 65 75 85 95
Optimal Solution Cost

30

1500

Suboptimality (%)

Convergence Memory

L=0

25

1000

500

L=1
L=2

20

L=3

15

L=4

10
5

0

0

55 65 75 85 95
Optimal Solution Cost

55 65 75 85 95
Optimal Solution Cost

Figure 16: Scaling up. Each curve shows bucketed means for LRTSL (1, 1.0, ∞). Error bars indicate standard errors and are too small to see for most data points.

6. Theoretical Analysis
PR LRTS subsumes several known algorithms when no abstraction is used. Clearly, LRTS (Bulitko & Lee, 2006) is a special case of PR LRTS when no abstraction is used. LRTS itself subsumes and generalizes several real-time search algorithms including LRTA* (Korf, 1990), weighted
LRTA* (Shimbo & Ishida, 2003), γ-Trap (Bulitko, 2004) and SLA*/SLA*T (Shue & Zamani, 1993;
Shue et al., 2001).
Theorem 6.1 (real-time operation) For any heuristic search problem, LRTS` (d, γ, T ) the amount
of planning per any action is constant-bounded. The constant depends on the constant control
parameters d ∈ N, γ ∈ (0, 1], T ∈ [0, ∞] but is independent of the problem’s number of states.
We first prove an auxiliary lemma.
Lemma 6.1 (downward refinement property) For any abstract path p = (sa , . . . , sb ), any two
children of its ends are connected by a path lying entirely in the corridor induced by p. This means
that any abstract path can be refined within the corridor formed by its children. Formally:
∀1 ≤ k ≤ ` ∀p = (sa , . . . , sb ) [p ⊂ (S(k), E(k)) =⇒
∀s0a ∈ children(s1 ) ∀s0b ∈ children(sm )
∃p0 = (s0a , . . . , s0b ) [p0 ⊂ (S(k − 1), E(k − 1)) & ∀s0 ∈ p0 [s0 ∈ ∪s∈p children(s)]]].
70

(6.1)

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

5

4000

2000

6000
5000

6

4

2

0

0

200

6

55 65 75 85 95
Optimal Solution Cost

x 10

First−Move Lag

6000

8
Convergence Planning

Convergence Travel

8000

4000
3000
2000

55 65 75 85 95
Optimal Solution Cost

1000

55 65 75 85 95
Optimal Solution Cost

L=1

150

100

50

0

55 65 75 85 95
Optimal Solution Cost

Suboptimality (%)

Convergence Memory

L=0

5

L=2
L=3

4

L=4

3

2

55 65 75 85 95
Optimal Solution Cost

Figure 17: Scaling up. Each curve shows bucketed means for LRTSL (9, 0.4, ∞). Error bars indicate standard errors and are too small to see for most data points.
Proof. The proof is by induction on the number of edges in the abstract path. The base case is 0.
This means that any two children of a single abstract state are connected by a path that lies entirely
in the set of children of the abstract state. This holds due to Property 7.
Suppose the statement holds for all abstract paths of length j. We will now show that then it
holds for all abstract paths of length j+1. Consider an arbitrary abstract path p ⊂ (S(k), E(k)), k >
0 that has j + 1 edges. Then we can represent p as (s1 , . . . , sj+1 , sj+2 ). Consider arbitrary children
s01 ∈ children(s1 ) and s0j+2 ∈ children(sj+2 ). We need to show that there is path p0 ⊂ (S(k −
1), E(k − 1)) between them that lies entirely in the union of children of all states on p (let us denote
it by Cp ). Let s0j+1 be an arbitrary child of state sj+1 . Since s1 and sj+1 are only j edges apart, by
inductive supposition, there is a path between s01 and s0j+1 that lies entirely in Cp . All that is left
is to show is that s0j+1 and s0j+2 are connected within Cp . If s0j+1 and s0j+2 have the same parent,
Property 7 guarantees they can be connected. If they have different parents, then Property 6 provides
the same guarantee. Either way, the induction step is completed. 
We can now prove Theorem 6.1.
Proof. At the abstract level `, LRTS(d, γ, T ) considers no more than bd abstract states by the
algorithm design (cf. Section 2.2), here b is maximum degree of any state. As assumed earlier in
the paper, the maximum degree of any state does not depend on the number of states. The resulting
abstract path of no longer than d abstract edges induces a corridor at the ground level. The corridor
consists of all ground-level states that abstract to abstract states on the path. The size of the corridor
71

B ULITKO , S TURTEVANT, L U , & YAU

is upper-bounded by the number of edges in the path (at most d) multiplied by the maximum number
of ground-level children of any abstract state at level `. The latter is upper-bounded by a constant
independent of the number of ground-level states due to Property 4. A* running in a corridor of
constant-bounded size takes a constant-bounded time. Finally, abstraction repair is O(`) and ` is
independent of graph size (Appendix A.2). 
Completeness is defined as the ability to reach the goal state on every trial. We prove completeness for LRTS` (d, γ, T ) based on the following reasoning. Recall that LRTS` (d, γ, T ) uses
the LRTS algorithm to build an abstract path at level `. It then uses a corridor-restricted A* at the
ground level to refine the abstract path into a sequence of ground-level edges. Due to Property 7
of Section 4.2, A* will always be able to find a path between the ground-level states sc and sg that
lie within the corridor C by the time execution gets to line 9 in Figure 6. Due to the exploration
process, the agent’s model of the search graph may be different from what the graph actually is in
reality. Consequently, the path found by A* may contain a ground-level edge that the agent believes
to exist but in reality does not. The following lemma demonstrates that such an execution failure is
possible only a finite number of times for a given search graph:
Lemma 6.2 There are only a finite number of path execution failures on each trial.
Proof. By contradiction: suppose there are an infinite number of such failures. Each failure is due
to a discovery of at least one new blocked edge or vertex in the ground-level graph. Then there will
be infinitely many blocked edges or vertices in a finite graph. 
A direct corollary to this lemma is that for any trial, there will be a moment of time after which
no graph discoveries are made on that trial. Therefore, executing A*’s path will indeed allow the
agent to follow its abstract path on the actual map.
Lemma 6.3 LRTS is complete on an abstract graph.
Proof. First, we show that any abstract graph satisfies the properties under which LRTS is shown to
be complete (Theorem 7.5, Bulitko & Lee, 2006). That is, the abstract graph is finite, each action
is reversible, there are no self-loops, all actions have a positive cost, and the goal state is reachable
from every state. The graph also has to be stationary and deterministically traversible (p.122, Bulitko
& Lee, 2006). Due to abstraction mechanism requirements in Section 4.2, the properties listed above
are satisfied by the clique abstraction mechanism as long as the ground-level graph satisfies these
properties as well (which we require in Section 2). Thus, LRTS running on an abstract graph as if it
were a ground graph is complete.
In PR LRTS, however, LRTS on an abstract graph does not execute its own actions. Instead,
its current (abstract) state is computed as abstract parent of the agent’s current ground-level state.
Therefore, a critical question is whether the agent is able to find a ground-level path from its current
state to the ground-level state corresponding to the end of its abstract path as computed in line 6
of Figure 6. The failure to do so would mean that the corridor computed in line 8 of Figure 6 and
used to refine the path does not contain a ground-level path from sc to sg . Due to the downward
refinement property (Lemma 6.1), this can only be due to graph discovery.
According to Lemma 6.2, after a finite number of failures, the A* algorithm operating at the
ground level is guaranteed to find path to reach the end of the abstract path computed by LRTS.
Thus, LRTS has the effective ability to “execute” its own abstract actions. Putting these results
72

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

together, we conclude that for any valid d, γ, T parameters, LRTS on the abstract graph finds its
goal on every trial. 
The two lemmas lead directly to the following statement.
Theorem 6.2 LRTS` (d, γ, T ) is complete.
Proof. This follows directly from Lemma 6.2 and Lemma 6.3. 
Theorem 6.3 LRTS` (d, γ, T ) with fixed tie-breaking converges to its final solution after a finite
number of trials. On all subsequent trials, it does not update its search graph model or the heuristic
and follows the same path.
Proof. Follows from Lemma 6.2 and Theorem 7.6 of (Bulitko & Lee, 2006) in the same way
Lemma 6.3 and Theorem 6.2 were proved above. 
Theoretical results on suboptimality are found in Appendix B.2

7. Related Research
Existing heuristic search methods for situated methods can be divided into two categories: full
search and real-time search. Full-search algorithms form an entire solution given their current
knowledge of the search graph. In contrast, real-time search plans only a small segment (frequently
just the first action) of their solution and executes it right away. Due to the local nature of planning,
real-time search algorithms need to update the heuristic function to avoid getting stuck in local
minima of their heuristic function.
7.1 Full Search
A common full-search algorithm is a version of A* (Hart et al., 1968) called Local Repair A* (Stout,
1996). In it, a full search is conducted from agent’s current state to the goal state under the free space
assumption. The agent then executes the computed path until either the destination is reached or the
path becomes invalid (e.g., a previously unknown wall blocks the way). In the latter case, the agent
replans from its current position to the goal. Local Repair A* suffers from two problems. First, it
searches for a shortest solution and, for a general search problem, may end up expanding a number
of states exponential in the solution cost due to inaccuracies in the heuristic function (Pearl, 1984).
Second, re-planning episodes do not re-use results of previous search.
The first problem is addressed by suboptimal versions of A* which are frequently implemented
via weighting the heuristic function (Pohl, 1970, 1973). Such a weighted A* (WA*) usually finds
a longer solution in less time. Once a suboptimal solution is found, it can be improved upon by
conducting additional searches. This can be done by re-using the open list between successive
searches (Hansen, Zilberstein, & Danilchenko, 1997; Likhachev et al., 2004; Hansen & Zhou, 2007)
or by re-running A* in a tunnel induced by a suboptimal solution (Furcy, 2006). In the later case,
beam search with backtracking can be used in place of weighted A* (Furcy & Koenig, 2005).
The second problem is addressed by incremental search methods such as D* (Stenz, 1995), D*
Lite (Koenig & Likhachev, 2002a) and LPA* (Koenig & Likhachev, 2002b). These algorithms reuse
some information from the previous search, thus speeding up subsequent replanning episodes.
73

B ULITKO , S TURTEVANT, L U , & YAU

In all of these algorithms, a full path has to be computed before the first move can be executed
by the agent. Consequently, the planning time per move is not constant-bounded and increases with
the problem size. Thus, agent-centered full search is not real-time.
7.2 Learning Real-time Search
Since the seminal work on LRTA* (Korf, 1990), research in the field of learning real-time heuristic
search has flourished resulting in over twenty algorithms with numerous variations. Most of them
can be described by the following four attributes:
The local search space is the set of states whose heuristic values are accessed in the planning
stage. The two common choices are full-width limited-depth lookahead (Korf, 1990; Shimbo &
Ishida, 2003; Shue & Zamani, 1993; Shue et al., 2001; Furcy & Koenig, 2000; Hernández &
Meseguer, 2005a, 2005b; Sigmundarson & Björnsson, 2006; Rayner, Davison, Bulitko, Anderson,
& Lu, 2007) and A*-shaped lookahead (Koenig, 2004; Koenig & Likhachev, 2006). Additional
choices are decision-theoretic based shaping (Russell & Wefald, 1991) and dynamic lookahead
depth-selection (Bulitko, 2004; Luštrek & Bulitko, 2006).
The local learning space is the set of states whose heuristic values are updated. Common
choices are: the current state only (Korf, 1990; Shimbo & Ishida, 2003; Shue & Zamani, 1993; Shue
et al., 2001; Furcy & Koenig, 2000; Bulitko, 2004), all states within the local search space (Koenig,
2004; Koenig & Likhachev, 2006) and previously visited states and their neighbors (Hernández &
Meseguer, 2005a, 2005b; Sigmundarson & Björnsson, 2006; Rayner et al., 2007).
A learning rule is used to update the heuristic values of the states in the learning space. The
common choices are dynamic programming or mini-min (Korf, 1990; Shue & Zamani, 1993; Shue
et al., 2001; Hernández & Meseguer, 2005a, 2005b; Sigmundarson & Björnsson, 2006; Rayner
et al., 2007), their weighted versions (Shimbo & Ishida, 2003), max of mins (Bulitko, 2004), modified Dijkstra’s algorithm (Koenig, 2004), and updates with respect to the shortest path from the
current state to the best-looking state on the frontier of the local search space (Koenig & Likhachev,
2006). Additionally, several algorithms learn more than one heuristic function (Russell & Wefald,
1991; Furcy & Koenig, 2000; Shimbo & Ishida, 2003).
Control strategy decides on the move following the planning and learning phases. Commonly
used strategies include: the first move of an optimal path to the most promising frontier state (Korf,
1990; Furcy & Koenig, 2000; Hernández & Meseguer, 2005a, 2005b), the entire path (Bulitko,
2004), and backtracking moves (Shue & Zamani, 1993; Shue et al., 2001; Bulitko, 2004; Sigmundarson & Björnsson, 2006).
Given the multitude of proposed algorithms, unification efforts have been undertaken. In particular, Bulitko and Lee (2006) suggested a framework, called Learning Real Time Search (LRTS), to
combine and extend LRTA* (Korf, 1990), weighted LRTA* (Shimbo & Ishida, 2003), SLA* (Shue
& Zamani, 1993), SLA*T (Shue et al., 2001), and to a large extent, γ-Trap (Bulitko, 2004). In the
dimensions described above, LRTS operates as follows. It uses a full-width fixed-depth local search
space with transposition tables to prune duplicate states. LRTS uses a max of mins learning rule to
update the heuristic value of the current state (its local learning space). The control strategy moves
the agent to the most promising frontier state if the cumulative volume of heuristic function updates
on a trial is under a user-specified quota or backtracks to its previous state otherwise (Section 2.2).
Within LRTS, the unification of several algorithms was accomplished through implementing
several methods for local search space selection, the learning rule, and the control strategy. Each
74

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

of these methods can be engaged at run-time via user-specified parameters. The resulting parameter space contained all the original algorithms plus numerous new combinations, enabling tuning
performance according to a specific problem and objective function of a particular application. As
a demonstration, Bulitko et al. (2005) tuned LRTS for ten maps from the computer game “Baldur’s
Gate” (BioWare Corp., 1998) and achieved a convergence speed that is two orders of magnitude
faster than LRTA*, while finding paths within 3% of optimal. At the same time, LRTS was about
five times faster on the first move than incremental A*. Despite the improvements, LRTS and other
real-time search algorithms converge more slowly than A* and, visually, may behave unintelligently
by repeatedly revisiting dead-ends and corners.
7.3 State Abstraction
The idea of abstraction has been previously applied to full search methods. In particular, HPA* and
PRA* (Botea et al., 2004; Sturtevant & Buro, 2005) use abstraction to speed up A* search: instead
of running A* on the lowest-level graph, they instead run A* on a smaller abstract graph. PRA*
computes an abstract path and then refines it in a similar manner to PR LRTS. However, PRA*
dynamically chooses which abstract level to use, and computes a path at each intermediate level
(i.e., it does not have pass-through levels). PRA* also widens its corridors to decrease suboptimality
at the cost of lower speed.
HPA* abstracts a map using large regions, and selects connection points (gates) between neighboring regions. For all gates of a region, optimal paths between all gates are pre-computed off-line
using A* and are stored in a table. This means that refining an abstract path (i.e., a sequence of
region gates) can be done simply by concatenating stored optimal paths. Smoothing is applied as a
post-processing step to decrease suboptimality of the resulting path.
Both of these algorithms are based on the ideas presented by Holte et al. (1996), who used an
abstraction mechanism in a similar manner to our use of the clique abstraction. Their method, the
STAR abstraction, can also be described as a radius abstraction. That is, a state is selected, and
is aggregated together with all states in a fixed radius of the original state. Holte et al. (1996)’s
work did not initially gain wide acclaim, because, at the time, there was little interest in problems
which were small enough to fit in memory. Motivating applications, such as pathfinding in computer
games, have resulted in a resurgence of interest in such techniques.
This class of algorithms first plan an abstract path, which is then refined into a traversable path.
Another approach is to build an abstraction which can be directly used for planning in the realworld. This includes methods like framed quad-trees (Yahja, Stentz, Singh, & Brummit, 1998),
which efficiently represent sparse maps. Quad-trees are a multi-resolution representation, as some
areas of the map are represented at high-resolution, and others are represented at lower resolution.
This abstraction differs from abstractions like the clique abstraction in that it can only be applied
once; further applications would not produce lower resolution maps, although the clique abstraction
could be applied to the graph implied by the framed quad-tree representation.
One other common use of abstraction is to provide better heuristics. Holte, Perez, Zimmer, and
MacDonald (1995) used the result of an abstract search to provide a more accurate heuristic for lowlevel search and performed no path refinement. Similarly, pattern databases are abstractions which
are built and solved off-line. The abstract solution costs are stored and then used during search as a
heuristic function (Culberson & Schaeffer, 1996; Felner, Zahavi, Schaeffer, & Holte, 2005).
75

B ULITKO , S TURTEVANT, L U , & YAU

PR LRTS presented in this paper is the first real-time heuristic search algorithm to use
automatically-built state abstraction. Path-refinement algorithms listed above conduct a full-search
and therefore cannot guarantee constant-bounded planning time for all agent’s moves.

8. Limitations and Future Work
The results presented in this paper open several directions for future research. First, PR LRTS is
able to operate with a wide class of homomorphic graph abstraction techniques. Thus, it would
be of interest to investigate the extent to which effects of graph abstraction on real-time search
presented in this paper are specific to the clique abstraction mechanism and the pathfinding domain.
Recent work has shown that the clique abstraction has parameters that are well-tuned to minimize
work done in traditional path planning (Sturtevant & Jansen, 2007). Our experiments in pathfinding
have suggested that the clique abstraction is well-suited to map abstraction because it represents key
properties of the underlying space well. In particular, the branching factor stays roughly constant
at higher levels of abstraction. On an empty map, for instance, the number of nodes at each level
of abstraction will be reduced by a factor of four by the clique abstraction, but the branching factor
of every state will stay the same. (Corner states will have 3 neighbors, edge states will have 5
neighbors, and middle states will have 8 neighbors.) This may not be the case in other domains. For
instance, in the sliding tile puzzle the maximum branching factor of abstract states quickly increases
with abstraction level. As a result, the corridor derived from an abstract path in PR LRTS becomes
excessively wide and does not effectively constrain A* search at the ground level. We conjecture that
algorithms which use homomorphic abstractions will only be effective in a domain if the abstraction
preserves the average, minimum, and maximum branching factor from the original problem at each
level of abstraction. Clique abstraction, then is likely to work well in three-dimensional pathfinding,
while problem-specific mechanisms would be needed for permutation-type puzzles. It is an area of
open research to provide such an abstraction.
Second, PR LRTS uses an abstract solution to restrict its search in the original ground-level
graph. It is interesting to combine this with a complementary approach of using the cost of an
optimal solution to an abstract problem as a heuristic estimate for the original search graph in the
context of real-time search. In particular, we are looking at effective ways of propagating heuristic
values from higher to lower levels of the abstraction hierarchy.
Third, state aggregation is just one way of generalizing learning. Future research will consider
combining it with function approximation for the heuristic function, as is commonly practiced in
large-scale applications of reinforcement learning.
Fourth, we are presently investigating applications of PR LRTS to dynamic environments. In
particular, we are studying the extent to which savings in memory gained by learning at a higher
abstraction level will afford application of PR LRTS to moving target search. An existing algorithm (Ishida & Korf, 1991) requires learning a number of heuristic values quadratic in the size of
the map. This is prohibitive in the case of commercial game maps.
Finally, we are presently extending the graph abstraction method presented in this paper to
stochastic environments formulated as Markov Decision Processes.
76

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

9. Conclusions
Situated agents in real-time environments are expected to act quickly while efficiently learning an
initially unknown environment. Response time and learning speed are antagonistic performance
measures as more planning leads to better actions and, consequently, faster convergence but longer
response time. Full search algorithms, such as local repair A*, converge quickly but do not have
a constant-bounded planning time per move. Real-time heuristic search algorithms have constantbounded planning times per move, but learn slowly.
In this paper, we attempted to combine the best of both approaches and suggest a hybrid algorithm, PR LRTS, that learns a heuristic function in a smaller abstract space and uses corridorrestricted A* to generate a partial ground-level path. In a large-scale empirical study, PR LRTS
was found to dominate virtually all tested algorithms that do not use abstraction with respect to
several performance measure pairs. The combination of learning and planning brings real-time performance to much larger search spaces, substantially benefiting applications such as pathfinding in
robotics and video games.

Acknowledgments
Funding was provided by the National Science and Engineering Research Council of Canada, the
Alberta Ingenuity Centre for Machine Learning, the Informatics Circle of Research Excellence, and
the University of Alberta. We appreciate input from David Thue, Rich Korf, Rob Holte, Csaba
Szepesvári, David Furcy, and Adi Botea. Special thanks go to Jonathan Schaeffer and the anonymous reviewers whose detailed comments greatly improved this paper. This research has been enabled by the use of WestGrid computing resources, which are funded in part by the Canada Foundation for Innovation, Alberta Innovation and Science, BC Advanced Education, and the participating
research institutions. In particular, we would like to acknowledge help from Roman Baranowski,
Masao Fujinaga, and Doug Phillips.

Appendix A. Clique Abstraction
Below we will describe the clique abstraction mechanism in several stages. First, we present the
algorithm for building an initial abstraction hierarchy using the free space assumption. Then we
describe the repair procedure that updates the abstract graphs as an agent explores the environment.
Finally, we consider suboptimality of solution caused by abstraction on examples and derive a worstcase upper bound.
A.1 Building Initial Abstraction Hierarchy
The pseudo-code for building an initial clique abstraction is in Figure 18. The abstract procedure
(lines 5 and 14) takes a set of states at some level i and maps them to a single abstract state at level
i + 1. This involves creating a new abstract state and storing parent and child links. If, in line 20,
a new abstract edge is added where one already exists, we do not add an extra edge but increase a
count associated with the edge. Such counts are used to facilitate abstraction repair as described in
the next section.
In general, clique-finding is an NP-complete problem (Garey & Johnson, 1990). However,
in eight-connected two-dimensional grid-based search graphs the largest possible clique size is 4.
77

B ULITKO , S TURTEVANT, L U , & YAU

graph CliqueAbstraction(graph g)
1
initialize graph g 0 ← ∅
2
for i = 4...2
3
for each unabstracted state s in g
4
if s is part of some i-clique c
5
g 0 ← g 0 ∪ {abstract(c)}
6
end if
7
end for each
8
end for
9
10 for each unabstracted state s in g
11
if degree(s) = 1
12
set parent(s) to parent(neighbor(s))
13
else
14
g 0 ← g 0 ∪ {abstract(n)}
15
end if
16 end for each
17
18 for each edge e = (v1 , v2 )
19
if parent(v1 ) 6= parent(v2 )
20
g 0 ← g 0 ∪ {(parent(v1 ), parent(v2 ))}
21
end if
22 end for each
23 return g 0
Figure 18: Building the initial clique abstraction.
Because the degree of each
state is also constant-bounded (as required in Section 2) the time per

clique constant (i.e., 83 state accesses to check eight neighbors and find 3 which form a 4-clique
together with the current state). Thus, the total running time for doing a single clique abstraction is
O(|S|), where |S| is the number of states in the original search graph. If the abstraction procedure
reduces the graph size by at least a constant factor greater than one, the total cost of abstracting a
graph will also be O(|S|) as the cost of each additional abstraction step is reduced exponentially.
A.2 Repairing Abstraction Hierarchy
As the agent explores its environment, it may find some edges or states blocked. In such cases, it will
remove the corresponding states and edges from its model and will need to propagate the changes
to all abstract graphs in the abstraction hierarchy. We will demonstrate the repair code in Figure 19
with an example that also shows how the repair procedure can have amortized constant-time cost.
In Figure 20 we remove edges from the Level 0 graph at the bottom left of the figure. The right
side of the figure shows the full abstract graph after all edges have been removed. At each level we
show a portion of the abstract graph and assume that there are more states and edges in the graph.
They are shown schematically in Level 0 in gray.
At Level 0 there are four states marked A which form an abstract state at level 1. This is also
true for the states marked A’. A and A’ are joined by an edge at level 1, which is abstracted from
78

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

RemoveEdge(edge e, level l, graph g)
1
decrease edge count of e
2
if child edge count of e 6= 0 return end if
3
e = (v1 , v2 )
4
remove e from g[l]
5
if parent(v1 ) = parent(v2 )
6
AddToRepairQ(parent(v1 ))
7
else
8
RemoveEdge((parent(v1 ), parent(v2 )), l + 1, g)
9
end if
RemoveState(state s, level l, graph g)
10 for each edge e incident to s
11
RemoveEdge(e, l, g)
12 end if
13 remove s from g[l]
14 AddToRepairQ(parent(s))
HandleRepairQ()
15 while RepairQ not empty
16
remove state s at lowest level l of g
17
if abstraction properties do not hold in s
18
AddToRepairQ(parent(s))
19
split state s into s1 . . . sn so that abstraction properties holds in si
20
for i = 1 . . . n either:
21
1. Merge si into existing abstract state
22
2. Extract si into new abstract state
23
end for
24
end if
25 end while
Figure 19: Repairing abstraction hierarchy.
four edges at level 0. When we remove these edges from the level 0 graph using the RemoveEdge()
procedure from Figure 19, the first three removals simply decrement the count associated with the
abstract edge between A and A’ (line 1-2). The fourth removal, however, will result in removing
the edge between A and A’ (line 4). This removal will be recursively propagated (line 8) into the
abstraction hierarchy, but will not change the abstract graph at level 2, because the edge count will
again be decremented.
There are 22 edges which must be removed to perform the full split between the top and bottom
states at level 0 in Figure 20. Removing the first edge between E and E’ at level 2 requires the
removal of 10 underlying edges at level 0 which correspond to 4 edges from level 1 (all edges
between A, B, A’ and B’).
State repair first occurs when we remove the edge from E to E’. In this case E and E’ have the
same parent, so G is added to the repair queue (line 5 and 6). The repair queue is processed after
each set of removal operations. Once the edge from E to E’ is removed, children of G at level 3
no longer form a clique. Thus, G must be split into two states H and H’. Initially these states will
79

B ULITKO , S TURTEVANT, L U , & YAU

Before removal / repair
Level 3

After removal / repair
H'

Level 3

To split

split

G

E'

H
F'

E'

Level 2

Level 1

Level 2

To split

E

F

B'

C'

D'

A

B

C

D

E

Level 1

To split

E

F

F

A'

B'

C'

D'

A

B

C

D

split

E

(10 level 0 edges)

A'

F

A'

Level 0

Level 0

To split

A

split

(4 level 1 edges)

A'

F'

B

C

D

split

A

B

C

D

Figure 20: An example of repairing the clique abstraction.

have an edge between them, but this edge will be removed when the last of the edges from level
0 are removed. The repair code can work with many abstraction mechanisms. Specifically, the
check that an abstract state’s children still form a clique (line 17) can be changed to check for the
corresponding property of a non-clique abstraction.
For this example, the amortized cost of abstraction repair is constant. Imagine an agent traversing the graph at level 0 from left to right and discovering a wall splitting the top and bottom rows of
the states (as shown by “To split” label in the figure). At each step more of the graph is sensed by
the agent and edges will be removed from level 0 graph. Removing the three edges (A, A’), (A, B’),
and (B, A’) at level 1 requires removing six edges at level 0. Similarly, removing the three edges
(E, E’), (E, F’), and (F, E’) requires removing 12 edges at level 0. In general, an agent traveling
at level 0 must move twice as far (or remove twice as many states) before repair is required at an
additional level of abstraction. Thus, the number of abstraction levels repaired when traversing n
ground edges, is:
n
n
n
n
n
1 + 2 + 3 + 4 + · · · + log2 (n) = O(n).
2
4
8
16
n

Consequently, in this example, the amortized repair cost per edge traveled is O(1). In general, the
worst-case complexity of repair is O(`) and, in PR LRTS, ` is a constant independent of graph size.
This is because repairs are propagated only up the abstraction hierarchy (line 18 in Figure 19).
80

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Appendix B. Abstraction Properties
Abstraction properties were informally introduced and illustrated with an example in Section 4.2.
The appendix makes the presentation mathematically precise. In this section, variables i, k, m run
natural numbers including 0.
Property 1. An agent using abstraction maintains a hierarchy of ` abstract search graphs in addition to its model of the environment. Each of the abstract graphs is a search graph in the sense
of Section 2. In the following we denote the abstract search graph at level i, 1 ≤ i ≤ ` by
(G(i), c(i), s0 (i), sg (i), h0 (i)). As before, G(i) = (S(i), E(i)).
Property 2. Each state s in the search graph at level n < ` has a unique “parent” state s0 in level
n + 1 abstract search graph. More formally:


∀s ∈ S(k), k < ` ∃!s0 ∈ S(k + 1) parent(s) = s0 .

(B.1)

Property 3. Each state s in search graph at level m, for 0 < m ≤ `, has at least one “child”
state s0 at level m − 1. The notation children(s) represents the set of children of state s. Thus,
s0 ∈ children(s):


∀s ∈ S(k), k > 0 ∃s0 ∈ S(k − 1) s0 ∈ children(s) .

(B.2)

Property 4. Given a heuristic search problem S, for any instance of that problem, the number of
children of any abstract state is upper-bounded by a constant independent of the number of states:
∀S, S is a search problem ∃m ∀((S, E), c, s0 , sg , h0 ) ∈ S ∀i, 0 < i ≤ ` ∀s ∈ S(i)
[| children(s)| < m] .

(B.3)

Property 5. (Graph homomorphism) Every edge (s1 , s2 ) ∈ E(k), k < n has either a corresponding abstract edge at level k + 1 or s1 and s2 abstract into the same state:
∀s1 , s2 ∈ S(k), k < `
[(s1 , s2 ) ∈ E(k) =⇒ (parent(s1 ), parent(s2 )) ∈ E(k + 1) ∨ parent(s1 ) = parent(s2 ))] .(B.4)

Property 6. If an edge exists between abstract states s1 and s2 then there is an edge between some
child of s1 and some child of s2 :
∀s1 , s2 ∈ S(k), k > 0


0
0
(s1 , s2 ) ∈ E(k) =⇒ ∃s1 ∈ children(s1 ) ∃s2 ∈ children(s2 ) (s01 , s02 ) ∈ E(k − 1) .
For the last property, we need the following definition.
81

(B.5)

B ULITKO , S TURTEVANT, L U , & YAU

Definition B.1 A path p in space (S(k), E(k)), 0 ≤ k ≤ ` is defined as an ordered sequence of
states from S(k) whose any two sequential states constitute a valid edge in E(k). Formally, p is a
path in (S(k), E(k)) if and only if:
∃s1 , . . . , sm ∈ S(k) [p = (s1 , . . . , sm ) & ∀i, 1 ≤ i ≤ m [(si , si+1 ) ∈ E(k)]] .

(B.6)

We use the notation p ⊂ (S(k), E(k)) to indicate that both the vertices and the edges of the path p
are in the sets S(k), E(k) respectively. The notation s ∈ p indicates that state s is on the path p.
Property 7 Any two children of an abstract state are connected through a path whose states are all
children of the abstract state:
∀s ∈ S(k), 0 < k ≤ ` ∀s01 , s02 ∈ children(s) ∃p = (s01 , . . . , s02 ) ⊂ (S(k − 1), E(k − 1)). (B.7)
B.1 Abstraction-induced Suboptimality: Examples
Abstraction can cause suboptimality. In Figure 21 left, we are refining an abstract path. Solid
arrows indicate the abstract path. Ground-level path is shown with thinner dashed arrows. The
agent’s position is shown with “A” and the goal’s position is “G”. The white cells form the corridor
induced by the abstract path. An optimal path is shown on the right.

Figure 21: Abstraction causes suboptimality.
Partial path refinement can increase suboptimality. Refining an entire abstract path (Figure 22,
left) can yield shorter paths than refining a segment of an abstract path (Figure 22, right). Solid
arrows indicate the abstract path. The ground-level path is shown with thinner dashed arrows. The
agent’s position is shown with “A” and the goal’s position is “G”.
B.2 Abstraction-induced Suboptimality: An Upper Bound
There are two factors that contribute to the suboptimality of paths returned by PR LRTS. The first
factor is the parameters chosen for LRTS, which can be weighted to allow suboptimality. This effect
has been analyzed in the literature (Bulitko & Lee, 2006). Here we analyze the suboptimality that
can be introduced by the abstraction. For simplicity of analysis we consider a uniform abstraction
where at each level k states are abstracted into a parent at the next level of abstraction. This assumption simplifies our analysis and also enables application of this analysis to non-clique abstraction
mechanism that maintain this property. Before proving our result, we introduce two simple lemmas:
82

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Figure 22: Partial path refinement increases suboptimality.
Lemma B.1 Suppose all abstract edges have the same cost. If a lowest-cost path p between states
A and B has j edges then a lowest-cost abstract path between their abstract parents A0 and B 0 has
at most j abstract edges.
Proof. We prove this by contradiction. Suppose the lowest-cost abstract path q between A0 and B 0
has m > j edges. Then consider the abstract images of all states on p. They either have an abstract
edge between them or coincide due to Property 5. Thus they form an abstract path p0 between A0
and B 0 and due to Property 2 it has no more than j edges. Since by the assumption of our theorem
all abstract edges have the same cost, the lowest-cost path q between A0 and B 0 must have a higher
cost than path p0 between A0 and B 0 (Figure 23, right). This results in a contradiction. 
Lemma B.2 Any path created by refining an abstract edge at level ` cannot be longer than O(k ` )
at level 0.
Proof. This is demonstrated in the right portion of Figure 24. We assume that every abstract state
has exactly k children. So, at level ` of the abstraction any state A cannot have more than k `
children. Assuming that a path cannot visit a single node more than once, the refined path through
A can therefore have no more than O(k ` ) edges. 
We can now present our main result:
Theorem B.1 Assume that every abstract state has k children and the ground level edge costs are
in [1, e]. At level ` of an abstraction, the cost of a path created by refining an abstract path from
level ` to level 0 (the original space) is at most O(ek ` ) times more costly than the optimal path if all
abstract edges happen to have uniform cost and O(e2 k 2` ) if all abstract edges have costs in [1, ek ` ]
(from Lemma B.2).
Proof. First, we deal with the case where all edges in the abstract graph have uniform cost. Consider
two level-0 states A and B that abstract into level-` states A0 , B 0 (left side of Figure 23). If a lowestcost path p between A and B has j edges then a lowest-cost abstract path between A0 and B 0 has at
most j abstract edges by Lemma B.1.
83

B ULITKO , S TURTEVANT, L U , & YAU

path p', at most j edges

path q, m > j edges

A'

B'

A

B

A'

path p, j edges

B'

path p', at most j edges

Figure 23: Proving that any lowest-cost abstract path will have no more than j edges.
Suppose the agent is in state A and is seeking to go to state B. The agent first computes a
lowest-cost abstract path between A0 and B 0 . In the worst case, the abstract path will have j edges.
Suppose there are two abstract paths between A0 and B 0 : t01 and t02 as shown in Figure 24, left. They
both have j edges and, due to the uniform abstract edge cost assumption, the same cost. In the worst
case scenario, t01 is refined into a lowest-cost path t1 between A and B while t02 is refined into the
highest-cost path t2 between A and B. By analyzing the cost ratio of t1 and t2 we will arrive at the
upper bound of the theorem.
1 edge

path t'2 : j edges

A'

level !

B'

level 0
path t'1 : j edges

k ! edges

Figure 24: Paths t01 , t02 are the same cost yet refine into the shortest and longest paths.
Due to Lemma B.2, one abstract edge at level ` can be refined into at most k ` level-0 edges. In
the worst case, t1 has j edges while t2 has jk ` − 1 edges (the result of abstracting ` levels of k `
states into a single state). Furthermore, all edges on t1 have a cost of 1 leading to the total cost of t1
being j. All edges on t2 have cost e leading to the total cost of t2 being e(jk ` − 1). Thus, the ratio
of t2 and t1 costs is no higher than ek ` which proves the first statement of the theorem.
In the case when abstract edges have non-uniform costs in [1, ek ` ], we again consider two abstract paths t01 and t02 from A0 to B 0 . Now they can both have cost ejk ` , which is the highest possible
cost of a level-` image of a cost-j ground path. On path t01 , the abstract cost might be overestimated
so that there are j abstract edges, each of cost ek ` which refine into the level-0 path t1 of j edges
of cost 1 each. Thus, the total cost of t1 is j which is the lowest possible cost between A and B.
Path t02 has the same cost as t01 but with ejk ` abstract edges, each of cost 1. Since each of these
abstract edges can be refined into at most k ` edges at level 0, path t02 is refined into the path t2
of no more than ejk ` · k ` edges, each of which has cost e. Consequently, the total cost of t2 is
e · ejk ` · k ` = je2 k 2` . Thus, the ratio between the costs of t1 and t2 is e2 k 2` . 
84

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

t'1
t'1
A

A'

B

A

B

B'

w
c

c
t'2

w

w

j

w

w

j

Figure 25: A grid-based example achieving the worst case suboptimality.
The worst-case upper bound is tight, and occurs when we both severely underestimate the cost
of a suboptimal path and overestimate the cost of the optimal path. In Figure 25 we show how this
can happen in practice; the level-0 map is shown on the left. The lowest-cost path (t1 ) between states
A and B is a straight line and has the cost j. All corridors have width 1. The length of the corridors,
w, is chosen so that at level ` of the abstraction, all states in a corridor will abstract together into a
single state. In this map there are only cliques of size two (i.e., k = 2 in Theorem B.1).
The right part of Figure 25 shows level-` abstract graph in thick lines and the original map
in light gray. In this abstraction, the path t02 between A0 and B 0 (the abstract parents of A and
B) goes through the lower part of the map. The path t02 has the abstract cost 2c + j + w but its
abstract edges refine to w ground-level edges each. Thus, the total cost of its refined path t2 is
w × (2c + j) = 2cw + jw. Path t01 is an abstract image of t1 and has the abstract cost w = k ` for
each of its j edges, leading to the total abstract cost of jk ` = jw. It is shown in the right side of the
figure as a highly zigzagged path.
We can now choose c so that t02 costs just as much as t01 . Then the agent can have the bad luck
of choosing to refine t02 . To make this a certainty, we can make the cost of t01 slightly higher than
the cost of t02 . This is accomplished by setting 2c + j + w ≈ jw. From here, 2c ≈ jw − j − w
and c ≈ jw = jk ` = j2` . As a result, the agent chooses to refine t02 into t2 , which has cost
2cw + jw = 2j2` 2` + j2` = O(j22` ). The ratio between this and the cost of t1 is 22` , which
corresponds to the bound of the theorem for k = 2, e = 1.
Our experimental results demonstrate that such large suboptimality does not occur in practice. As an illustration, consider a histogram of suboptimality values for the 3000 problems and
all parametrizations of PR LRTS in Figure 26.
If suboptimality does become a practical concern, one can use ideas from HPA* (Botea
et al., 2004), where optimal path costs within regions were pre-computed and cached. Such precomputation will help prevent the severe over- and under-estimation of abstract path costs which
was assumed by the worst-case analysis in Theorem B.1.

Appendix C. Maps Used in the Empirical Study
The four additional maps are shown in Figure 27.
85

B ULITKO , S TURTEVANT, L U , & YAU

1

Percentage of problems

10

0

10

−1

10

−2

10

−3

10

20

40

60
80
Suboptimality (%)

100

120

Figure 26: Histogram of suboptimality in our experiments.

Figure 27: The four additional maps used in the experiments.

Appendix D. Dominance on Average: Plots
Six plots corresponding to entries in Table 2 are shown in Figures 28, 29.
86

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

A*
LRTS4(9,0.2,∞)
(9,0.2,100)
(9,0.4,∞)
(9,0.4,100)
LRTS
(9,0.2,100)
LRTS33(9,0.2,∞)
(9,0.4,∞)
(9,0.4,100)

Dominated
Non−dominated

LRTS2(9,0.2,∞)

First−move Lag (states touched)

LRTS
(5,0.2,∞)
LRTS
(5,0.4,∞)
(5,0.4,100)
33
3

10

LRTS
LRTS33(3,0.2,∞)
(3,0.4,∞)
LRTS (1,0.4,∞)
4

LRTS3(1,0.4,∞)
2

10

LRTS2(1,0.4,∞)

LRTS2(1,0.8,∞)
LRTS1(1,0.4,∞)

LRTS1(1,0.8,∞)

1

10

LRTS(1,0.4,∞)
3

4

10

10
Convergence Travel

A*

Dominated
Non−dominated

LRTS2(9,0.2,∞)
LRTS2(9,0.2,100)
LRTS (5,0.2,∞)
3
LRTS (5,0.2,100)
3
LRTS
LRTS44(3,0.2,∞)
(3,0.4,∞)

−3

First−move Lag (seconds)

10

LRTSLRTS
(3,0.2,∞)
(5,0.4,∞)
3
2
LRTS4(1,0.4,∞)
LRTS4(1,0.2,∞)
LRTS3(1,0.4,∞)
LRTS (1,0.4,100)
3

−4

LRTS
LRTS
(1,0.4,∞)
(1,0.2,∞) LRTS (1,0.4,100)
2 2
2

10

LRTS
(1,0.4,∞)
LRTS
(1,0.2,∞)
1
1

LRTS(1,0.4,∞)
3

4

10

10
Convergence Travel

A*

Dominated
Non−dominated
−3

First−move Lag (seconds)

10

LRTS (3,0.2,∞)
3

LRTS3(1,0.4,∞)
LRTS (1,0.4,100)
3

−4

10

LRTS
LRTS
(1,0.4,∞)
(1,0.2,∞) LRTS (1,0.4,100)
2 2
2
LRTS
LRTS11(1,0.4,∞)
(1,0.2,∞)

LRTS(1,0.4,∞)
−1

10
Convergence Planning (seconds)

0

10

Figure 28: Dominance for several pairs of performance measures. Part 1.

Appendix E. Dominance on Individual Problems
In Section 5.1 we introduced the concept of dominance and demonstrated that PR LRTS with abstraction dominates all but extreme of the search algorithms that do not use abstraction. This analysis was done using cost values averaged over the 3000 problems. In this section we consider
87

B ULITKO , S TURTEVANT, L U , & YAU

A*
LRTS4(9,0.2,100)
(9,0.2,∞)
LRTS (9,0.2,∞)
(9,0.2,100)
LRTS43(5,0.2,∞)
(5,0.2,100)
(5,0.4,100)
(5,0.4,∞)

Dominated
Non−dominated

4

First−move Lag (states touched)

LRTS3(5,0.2,∞)
(5,0.2,100)
(5,0.4,∞)
(5,0.4,100)
3

10

LRTS3(3,0.2,∞)
(3,0.4,∞)
LRTS4(1,0.2,∞)
(1,0.2,100)
(1,0.4,∞)
(1,0.4,100)
4

LRTS
(1,0.2,100)
LRTS33(1,0.2,∞)
(1,0.4,∞)
2

10

LRTS
LRTS22(1,0.2,∞)
(1,0.4,∞)

LRTS
(1,0.2,∞)
LRTS
(1,0.4,∞)
11

1

10

LRTS(1,0.2,∞)

0

500

1000

1500

Convergence Memory
A*
LRTS (5,0.2,100)
(5,0.4,!)
4

Dominated
Non!dominated

LRTS3(5,0.2,100)
LRTS4(3,0.2,!)
(3,0.2,100)
LRTS
LRTS4 (3,0.4,!)
4

!3

First!move Lag (seconds)

10

LRTS3(3,0.2,!)
LRTS4(1,0.2,!)
(1,0.2,100)
(1,0.4,100)
4
LRTS
(1,0.2,100)
LRTS333(1,0.2,!)
(1,0.4,100)

!4

LRTS
LRTS22(1,0.2,!)
(1,0.4,100)

10

LRTS1(1,0.2,!)

LRTS(1,0.2,!)
LRTS(1,0.4,!)

0

500

1000

1500

Convergence Memory

Dominated
Non−dominated

25
LRTS3(1,0.4,∞)

Suboptimality (%)

20

LRTS2(1,0.4,∞)

15

LRTS3(3,0.2,∞)
LRTS
LRTS
(3,0.4,∞)
(3,0.4,100)
33

10

LRTS3(5,0.2,∞)
LRTS3(5,0.4,∞)
(5,0.4,100)
LRTS
LRTS33(5,0.8,∞)
(5,0.8,100)

5

0

A*
4

10

5

10
Convergence Planning (states touched)

6

10

Figure 29: Dominance for several pairs of performance measures. Part 2.
dominance on individual problems. Due to high variance in the problems and their difficulty, we
report percentages of problems on which dominance is achieved. For every pair of algorithms, we
measure the percentage of problems on which the first algorithm dominates the second. We then
measure the percentage of problems on which the second algorithm dominates the first. The ratio
between these two percentages we call the dominance ratio.
88

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

4

10

3

First−Move Lag

10

2

10

1

10

0

10
2
10

3

4

10

5

10

10

Convergence Travel
4

x 10

Convergence Travel

First−Move Lag
700

14

600

12
LRTS_3(1,1.0,∞)

LRTS_3(1,1.0,∞)

500
10
8
6
4
2

400
300
200
100

5
10
LRTS(5,0.8,∞)

15

200
400
LRTS(5,0.8,∞)

4

x 10

600

Figure 30: Top: LRTS3 (1, 1.0, ∞) is shown as a filled star; LRTS(5, 0.8, ∞) is shown as a hollow
star. Bottom left: convergence travel. Bottom right: first-move lag.
Table 4: Statistics for the two algorithms from Figure 30.
Algorithm

Convergence travel

First-move lag

Both

LRTS3(1,1.0,!)

72.83%

97.27%

70.97%

LRTS(5,0.8,!)

27.17%

2.67%

0.87%

Dominance ratio
81.89

At the top of Figure 30 we see a reproduction of the corresponding plot from Figure 28 where
two particular algorithms are marked with stars. The filled star is LRTS3 (1, 1.0, ∞) that uses three
levels of abstraction. The hollow star is LRTS(5, 0.8, ∞) that operates entirely at the ground level.
Statistics are reported in Table 4. The bottom left of the figure shows advantages of the PR LRTS
with respect to convergence travel. On approximately 73% of the 3000 problems, the PR LRTS
travels less than the LRTS before convergence (points below the 45-degree line). With respect to
the first-move lag, the PR LRTS is superior to the LRTS on 97% of the problems (bottom right in the
figure). Finally, on 71% of the problems the PR LRTS dominates the LRTS (i.e., outperforms it with
89

B ULITKO , S TURTEVANT, L U , & YAU

25

Suboptimality (%)

20
15
10
5
0
3
10

6

4

5

10

10
Convergence Planning

6

Convergence Planning

x 10

7

10

10

Suboptimality (%)

6
80

4

LRTS_3(5,0.8,∞)

LRTS_3(5,0.8,∞)

5

3
2
1

1

2
3
4
LRTS(3,0.2,∞)

5

60

40

20

0

6

0

6

x 10

20

40
60
LRTS(3,0.2,∞)

80

Figure 31: Top: LRTS3 (5, 0.8, ∞) is shown as a filled star; LRTS(3, 0.2, ∞) is shown as a hollow
star. Bottom left: convergence planning. Bottom right: suboptimality.
respect to both measures). On the other hand, the LRTS dominates the PR LRTS on approximately
1% of the problems. This leads to the dominance ratio of 81.89.
Table 5: Statistics for the two algorithms from Figure 31.
Algorithm

Convergence planning

Suboptimality

Both

LRTS3(5,0.8,!)

80.03%

55.80%

48.97%

LRTS(3,0.2,!)

19.93%

40.97%

12.2%

Dominance ratio
4.01

Similarly, Figure 31 compares two algorithms with respect to convergence planning and suboptimality of the final solution. At the top of the figure, we have the corresponding plot from Figure 29
with LRTS3 (5, 0.8, ∞) shown as a filled star and LRTS(3, 0.2, ∞) shown as a hollow star. Percent
points for domination on individual problems are found in Table 5. The plot at the bottom left of
the figure shows that PR LRTS has lower convergence planning cost than the LRTS on 80% of
the problems. The plot at the bottom right shows suboptimality of the solutions these algorithms
90

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

produced. The PR LRTS is more optimal than the LRTS 56% of the time. Finally, the PR LRTS
dominates the LRTS on 49% of the problems. Domination the other way (i.e., the LRTS dominates
the PR LRTS) happens only on 12.2% of the problems. This leads to the dominance ratio of 4.01.
There are several factors that influence the results. First, there is a high variance in the difficulty
of individual problems due to their distribution over five buckets by optimal path distance. Consequently, there is high variance in how the algorithms trade off antagonistic performance measures
on these problems. In the case when there is a large difference between mean values, such as in
Figure 30, dominance on average is supported by dominance on the majority of individual problems. Conversely, a small difference in mean values (e.g., 2.3% in suboptimality for the algorithms
in Figure 31) does not lead to overwhelming dominance at the level of individual problems.
We extended this analysis to all pairs of algorithms displayed in Figures 28, 29. For convergence travel and first-move lag, the dominance ratio varies between 5.48 and ∞ with the values
below infinity averaging 534.32 with the standard error of 123.47. For convergence planning and
suboptimality, the dominance ratio varies between 0.79 and ∞ with the values below infinity averaging 5.16 with the standard error of 1.51. Finally, only a set of 181 algorithms was tested in
our study. Therefore, the results should be viewed as an approximation to the actual dominance
relationship among the algorithms.

Appendix F. Interaction Between Abstraction and LRTS Parameters
In Section 5.2 we observed general trends in influence of abstraction on the five performance measures. As the abstraction level adds another dimension to the parameter space of LRTS, previously
defined by d, γ, T , the natural question is how the four parameters interact. In order to facilitate a
comprehensible visualization in this paper, we will reduce the LRTS parameter space from d, γ, T to
d, γ by setting T = ∞ (i.e., disabling backtracking in LRTS). This is justified for two reasons. First,
recent studies (Bulitko & Lee, 2006; Sigmundarson & Björnsson, 2006) have shown that effects of
backtracking are highly domain-specific.
Table 6 gives an overview of the influence of abstraction on the parameters of LRTS at a qualitative level. A more detailed analysis for each of the five performance measures follows. It is
important to note that these experiments were performed on a set of fixed cost paths and fixed size
maps. Consequently, map boundary effects are observed at higher levels of abstraction. We will
detail their contribution below.
Table 6: Influence of LRTS parameters on the impact of abstraction. Each cell in the table represents the impact of abstraction either amplified (“A”) or diminished (“D”) by increase in d
or γ. Lower-case “a” and “d” indicate minor effect, “-” indicates no effect.
increase in d
d
A
a
D
D

measure / control parameter
convergence travel
first-move lag
convergence planning
convergence memory
suboptimality

increase in γ
A
A
A
a

Convergence travel: increasing the abstraction level generally decreases convergence travel as
LRTS learns on smaller abstract maps. Independently, increasing the lookahead depth in LRTS has a
91

B ULITKO , S TURTEVANT, L U , & YAU

Convergence Travel
x 10

x 10

Level 2
Level 0
Optimality Weight γ

4
3
2
1
0
1
0.8
Optimality Weight γ

4

Difference in Convergence Travel
1

4

2

0.8

1.5

1
0.4

9
0.4
0.2

1

3

0.5

5

0.2

Lookahead Depth d

1

Convergence Travel

3
5
Lookahead Depth d

9

Difference in Convergence Travel
2500

1
Level 4
Level 2
Optimality Weight γ

6000
4000
2000
0
1
0.8
Optimality Weight γ

2000

0.8

1500
1000
0.4

500

9
0.4
0.2

1

3

5

0.2

Lookahead Depth d

1

3
5
Lookahead Depth d

9

Figure 32: Convergence travel: impact of abstraction as a function of d, γ. Top two graphs:
LRTS(d, γ) vs. LRTS2 (d, γ). Bottom two graphs: LRTS2 (d, γ) vs. LRTS4 (d, γ).
similar effect (Bulitko & Lee, 2006). Convergence travel is lower-bounded by the doubled optimal
cost from the start to the goal (as the first trial has to reveal parts of the map and, consequently,
cannot be final). Therefore, decreasing convergence travel via either of two mechanisms diminishes
the gains from the other mechanism. This effect can be seen in Figure 32 where there is a noticeable
gap in convergence travel between abstractions levels 0 and 2. But with a lookahead of 9, there is
only a small difference between using abstraction levels 2 and 4. Thus, increasing the lookahead
slightly diminishes the effect of abstraction (hence the “d” in the table). Increasing γ increases the
convergence travel. The higher the value of γ, the more there is to be gained from using abstraction.
An increase in γ amplifies the advantage of abstraction.
First-move lag generally increases with both the abstraction level and with lookahead depth.
As lookahead depth increases, the size of the corridor used for A* search increases as well. Thus,
increasing d amplifies the first-move lag due to abstraction, because PR LRTS must plan once within
the lookahead space (within LRTS) and once inside the corridor (within A*) (Figure 33).
Deeper lookahead amplifies the impact of abstraction. In the simplified analysis below, we
assume that the map is obstacle free which leads to all levels of abstraction being regular grids
(ignoring boundary effects). The length of a path between two points (expressed in the number of
actions) is, thus, decreased by a factor of two with each abstraction level. Under these assumptions,
the total number of states PR LRTS touches on the first move is Ω(d2 ) at the abstract graph and
92

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

First−Move Lag

Difference in First−Move Lag
−100

1
Level 2
Level 0

−200
Optimality Weight γ

3000
2000
1000
0
1
0.8
Optimality Weight γ

−300

0.8

−400
−500
−600
−700
0.4

−800

9
0.4
0.2

1

3

−900

5

0.2

Lookahead Depth d

1

First−Move Lag

3
5
Lookahead Depth d

9

Difference in First−Move Lag
1

−500

Level 4
Level 2
Optimality Weight γ

4000
3000
2000
1000
0
1
0.8
Optimality Weight γ

0.8
−1000

−1500
0.4
−2000

9
0.4
0.2

1

3

5

0.2

Lookahead Depth d

1

3
5
Lookahead Depth d

9

Figure 33: First-move lag: impact of abstraction as a function of d, γ. Top two graphs: LRTS(d, γ)
vs. LRTS2 (d, γ). Bottom two graphs: LRTS2 (d, γ) vs. LRTS4 (d, γ).
Ω(d · 2` ) at the ground graph. The latter quantity is simply the number of edges in the ground
path computed as the number of edges in the abstract path (d) multiplied by the reduction factor
of 2` . Adding  more abstraction levels increases the first-move lag to Ω(d · 2`+ ). The increase is
a linear function of lookahead depth d. Thus, larger values of d amplify the effect of adding extra
abstraction levels.
There are several points glossed over by our simplified analysis. First, the reduction in the
path length is not always two-fold as we assumed above. In the presence of walls, higher levels of
abstraction are less likely to locate and merge fully-fledged 4-element cliques. Second, boundaries
of the abstract graph can be reached by LRTS in less than d moves at the higher abstraction level.
This effectively decreases the quantity d in the formula above and the size of the corridor can be
reduced from our generous estimate d·2` . Finally, “feeding” A* longer abstract path often improves
its performance as we have analyzed in a previous section (cf. Figure 22). This explains why at
abstraction level 4 deepening lookahead has diminishing returns as seen in Figure 33.
Optimality weight does not affect the number of states touched by LRTS at the abstract level.
On the other hand, it can change the cost of the resulting A* search as a different abstract path may
be computed by LRTS. Overall, however, the effect of γ on the first-move lag and the impact of
abstraction is inconsequential (Figure 33).
93

B ULITKO , S TURTEVANT, L U , & YAU

Convergence Planning

6

Difference in Convergence Planning

x 10

1

6

x 10

Level 2
Level 0

2
Optimality Weight γ

3
2
1
0
1
0.8

0.8
1.5

1
0.4

0.5

9

Optimality Weight γ

0.4
0.2

1

3

5

0.2

Lookahead Depth d

1

Convergence Planning

9

Difference in Convergence Planning
25000

1

4

x 10

Level 4
Level 2

20000
Optimality Weight γ

6
4
2
0
1
0.8
Optimality Weight γ

3
5
Lookahead Depth d

0.8

15000
10000
5000

0.4

0

9
0.4
0.2

1

3

−5000

5

0.2

Lookahead Depth d

1

3
5
Lookahead Depth d

9

Figure 34: Convergence planning: impact of abstraction as a function of d, γ. Top two graphs:
LRTS(d, γ) vs. LRTS2 (d, γ). Bottom two graphs: LRTS2 (d, γ) vs. LRTS4 (d, γ).
Convergence planning: As the abstraction level increases, convergence planning generally
decreases. The effect of d is more complex, because deeper lookahead increases the cost of each
individual planning step, but overall decreases planning costs as convergence is faster. The interplay
of these two trends moderates the overall influence as seen in Figure 34.
The effect of γ on convergence planning is non-trivial. In general, lower values of γ reduce the
convergence planning cost. Note that convergence planning cost is a product of average planning
time per unit of distance and the convergence travel. As we discussed above, optimality weight
amplifies the effects of abstraction convergence travel. At the same time, it does not substantially
affect increase in planning per move as the abstraction goes up. Combining these two influences, we
conclude that optimality weight will amplify effects of abstraction on convergence planning. This
is confirmed empirically in Figure 34.
Convergence memory: Abstraction decreases the amount of memory used at convergence because there are fewer states over which to learn. The effects of d and γ are the same as for convergence travel described above. This is because there is a strong correlation between convergence
travel and convergence memory that we have previously discussed. Visually Figures 32 and 35
display very similar trends.
Suboptimality: Increasing the abstraction level increases suboptimality. For plain LRTS, lookahead depth has no effect on suboptimality of the final solution. However, when we combine deeper
94

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Convergence Memory

Difference in Convergence Memory
1000

1
Level 2
Level 0
Optimality Weight γ

1500
1000
500
0
1
0.8
Optimality Weight γ

800

0.8

600
400
0.4
200

9
0.4
0.2

1

3

5

0.2

Lookahead Depth d

1

Convergence Memory

3
5
Lookahead Depth d

9

Difference in Convergence Memory
90

1
Level 4
Level 2

80
Optimality Weight γ

150
100
50
0
1
0.8
Optimality Weight γ

70

0.8

60
50
40
30
0.4

20

9
0.4
0.2

1

3

10

5

0.2

Lookahead Depth d

1

3
5
Lookahead Depth d

9

Figure 35: Convergence memory: impact of abstraction as a function of d, γ. Top two graphs:
LRTS(d, γ) vs. LRTS2 (d, γ). Bottom two graphs: LRTS2 (d, γ) vs. LRTS4 (d, γ).

lookahead with abstraction the suboptimality arising from abstraction decreases. With deeper lookahead the abstract goal state is seen earlier making PR LRTS a corridor-constrained A*. Additionally,
as we discussed in Section 5.2 and Figure 22, refining shorter paths (computed by LRTS with lower
d) introduces additional suboptimality. As suboptimality is lower bounded by 0%, increasing lookahead diminishes the effects of abstraction on suboptimality (Figure 36) – hence “D” in Table 6.
Increasing γ decreases the amount of suboptimality when no abstraction is used. When combined with abstraction increasing γ has a minor amplification effect on the difference abstraction
makes (Figure 36) for two reasons. First, at abstract levels the graphs are fairly small and γ makes
less difference there. Second, the degree suboptimality of an abstract path does not translate directly
into the degree of suboptimality of the resulting ground path as the A* may still find a reasonable
ground path. Thus, the influence of γ at the abstract level is overshadowed by the suboptimaly
introduced by the process of refinement itself (cf. Figure 21).

95

B ULITKO , S TURTEVANT, L U , & YAU

Suboptimality (%)

Difference in Suboptimality (%)
0

1
Level 2
Level 0

−2
Optimality Weight γ

30
20
10
0
0.2
0.4

5

0.8
1

Optimality Weight γ

9

3

0.8

−4
−6
−8
−10

0.4

−12

1
0.2

Lookahead Depth d

1

Suboptimality (%)

3
5
Lookahead Depth d

9

Difference in Suboptimality (%)
0

1
Level 4
Level 2

−2
Optimality Weight γ

30
20
10
0
0.2
0.4

5

0.8
Optimality Weight γ

1

9

−14

3

0.8
−4
−6
−8

0.4

−10

1
0.2

Lookahead Depth d

1

3
5
Lookahead Depth d

9

Figure 36: Suboptimality: impact of abstraction as a function of d, γ. Top two graphs: LRTS(d, γ)
vs. LRTS2 (d, γ). Bottom two graphs: LRTS2 (d, γ) vs. LRTS4 (d, γ).

96

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

References
Aylett, R., & Luck, M. (2000). Applying artificial intelligence to virtual reality: Intelligent virtual
environments. Applied Artificial Intelligence, 14(1), 3–32.
Bacchus, F., & Yang, Q. (1994). Downward refinement and the efficiency of hierarchical problem
solving.. Artificial Intelligence, 71(1), 43–101.
BioWare Corp. (1998). Baldur’s Gate., Published by Interplay, http://www.bioware.com/bgate/,
November 30, 1998.
Blizzard Entertainment (2002). Warcraft III: Reign of Chaos., Published by Blizzard Entertainment,
http://www.blizzard.com/war3, July 3, 2002.
Botea, A., Müller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding. Journal of
Game Development, 1(1), 7–28.
Bulitko, V. (2004). Learning for adaptive real-time search. Tech. rep. http: // arxiv. org / abs / cs.AI
/ 0407016, Computer Science Research Repository (CoRR).
Bulitko, V., & Lee, G. (2006). Learning in real time search: A unifying framework. Journal of
Artificial Intelligence Research (JAIR), 25, 119 – 157.
Bulitko, V., Sturtevant, N., & Kazakevich, M. (2005). Speeding up learning in real-time search via
automatic state abstraction. In Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 1349 – 1354, Pittsburgh, Pennsylvania.
Culberson, J., & Schaeffer, J. (1996). Searching with pattern databases. In CSCI (Canadian AI
Conference), Advances in Artificial Intelligence, pp. 402–416. Springer-Verlag.
Dini, D. M., van Lent, M., Carpenter, P., & Iyer, K. (2006). Building robust planning and execution
systems for virtual worlds. In Proceedings of the Artificial Intelligence and Interactive Digital
Entertainment conference (AIIDE), pp. 29–35, Marina del Rey, California.
Ensemble Studios (1999). Age of Empires II: Age of Kings., Published by Microsoft Game Studios,
http://www.microsoft.com/games/age2, June 30, 1999.
Felner, A., Zahavi, U., Schaeffer, J., & Holte, R. (2005). Dual lookups in pattern databases. In
Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 103–
108, Edinburgh, United Kingdom.
Furcy, D. (2006). ITSA*: Iterative tunneling search with A*. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), Workshop on Heuristic Search, Memory-Based
Heuristics and Their Applications, Boston, Massachusetts.
Furcy, D., & Koenig, S. (2000). Speeding up the convergence of real-time search. In Proceedings
of the National Conference on Artificial Intelligence (AAAI), pp. 891–897.
Furcy, D., & Koenig, S. (2005). Limited discrepancy beam search. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 125–131.
Garey, M. R., & Johnson, D. S. (1990). Computers and Intractability; A Guide to the Theory of
NP-Completeness. W. H. Freeman & Co., New York, NY, USA.
Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. Journal of Artificial Intelligence
Research (JAIR), 28, 267–297.
97

B ULITKO , S TURTEVANT, L U , & YAU

Hansen, E. A., Zilberstein, S., & Danilchenko, V. A. (1997). Anytime heuristic search: First results.
Tech. rep. CMPSCI 97-50, Computer Science Department, University of Massachusetts.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination of
minimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2), 100–107.
Hernández, C., & Meseguer, P. (2005a). Improving convergence of LRTA*(k). In Proceedings of
the International Joint Conference on Artificial Intelligence (IJCAI), Workshop on Planning
and Learning in A Priori Unknown or Dynamic Domains, pp. 69–75, Edinburgh, UK.
Hernández, C., & Meseguer, P. (2005b). LRTA*(k). In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI), pp. 1238–1243, Edinburgh, UK.
Holte, R., Mkadmi, T., Zimmer, R. M., & MacDonald, A. J. (1996). Speeding up problem solving
by abstraction: A graph oriented approach. Artificial Intelligence, 85(1-2), 321–361.
Holte, R., Perez, M., Zimmer, R., & MacDonald, A. (1995). Hierarchical A*: Searching abstraction
hierarchies efficiently. Tech. rep. tr-95-18, University of Ottawa.
id Software (1993). Doom., Published by id Software, http://en.wikipedia.org/wiki/Doom, December 10, 1993.
Ishida, T. (1992). Moving target search with intelligence. In National Conference on Artificial
Intelligence (AAAI), pp. 525–532.
Ishida, T., & Korf, R. (1991). Moving target search. In Proceedings of the International Joint
Conference on Artificial Intelligence (IJCAI), pp. 204–210.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjou, A., & Shimada, S. (1999).
Robocup rescue: Search and rescue in large-scale disasters as a domain for autonomous agents
research. In Proceedings of the IEEE Conference on Man, Systems, and Cybernetics, Vol. 4,
pp. 739–743.
Koenig, S. (1999). Exploring unknown environments with real-time search or reinforcement learning. In Proceedings of the Neural Information Processing Systems, pp. 1003–1009.
Koenig, S. (2004). A comparison of fast search methods for real-time situated agents. In Proceedings of Int. Joint Conf. on Autonomous Agents and Multiagent Systems, pp. 864 – 871.
Koenig, S., & Likhachev, M. (2002a). D* Lite. In Proceedings of the National Conference on
Artificial Intelligence (AAAI), pp. 476–483.
Koenig, S., & Likhachev, M. (2002b). Incremental A*. In Advances in Neural Information Processing Systems (NIPS), pp. 1539–1546.
Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109–132.
Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. In Proceedings of the International
Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), pp. 281–288.
Koenig, S., & Simmons, R. (1998). Solving robot navigation problems with initial pose uncertainty
using real-time heuristic search. In Proceedings of the International Conference on Artificial
Intelligence Planning Systems, pp. 144 – 153.
Koenig, S., Tovey, C., & Smirnov, Y. (2003). Performance bounds for planning in unknown terrain.
Artificial Intelligence, 147, 253–279.
98

G RAPH A BSTRACTION IN R EAL - TIME H EURISTIC S EARCH

Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189–211.
Likhachev, M., Ferguson, D., Gordon, G., Stentz, A., & Thrun, S. (2005). Anytime dynamic A*: An
anytime, replanning algorithm. In Proceedings of the International Conference on Automated
Planning and Scheduling (ICAPS).
Likhachev, M., Gordon, G. J., & Thrun, S. (2004). Ara*: Anytime a* with provable bounds on suboptimality. In Thrun, S., Saul, L., & Schölkopf, B. (Eds.), Advances in Neural Information
Processing Systems 16. MIT Press, Cambridge, MA.
Luštrek, M., & Bulitko, V. (2006). Lookahead pathology in real-time path-finding. In Proceedings of
the National Conference on Artificial Intelligence (AAAI), Workshop on Learning For Search,
pp. 108–114, Boston, Massachusetts.
Mero, L. (1984). A heuristic search algorithm with modifiable estimate. Artificial Intelligence, 23,
13–27.
Orkin, J. (2006). 3 states & a plan: The AI of F.E.A.R. In Proceedings of the Game Developers
Conference (GDC). http://www.jorkin.com/gdc2006 orkin jeff fear.doc.
Pearl, J. (1984). Heuristics. Addison-Wesley.
Pohl, I. (1970). First results on the effect of error in heuristic search. In Meltzer, B., & Michie, D.
(Eds.), Machine Intelligence, Vol. 5, pp. 219–236. American Elsevier, New York.
Pohl, I. (1973). The avoidance of (relative) catastrophe, heuristic competence, genuine dynamic
weighting and computaional issues in heuristic problem solving. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI), pp. 12–17.
Pottinger, D. C. (2000). Terrain analysis in realtime strategy games. In Proceedings of Computer
Game Developers Conference. www.gamasutra.com/features/gdcarchive/2000/pottinger.doc.
Rayner, D. C., Davison, K., Bulitko, V., Anderson, K., & Lu, J. (2007). Real-time heuristic search
with a priority queue. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI), pp. 2372–2377, Hyderabad, India.
Reynolds, C. W. (1987). Flocks, herds and schools: A distributed behavioral model. In SIGGRAPH
’87: Proceedings of the 14th Annual Conference on Computer Graphics and Interactive Techniques, pp. 25–34, New York, NY, USA. ACM Press.
Russell, S., & Wefald, E. (1991). Do the right thing: Studies in limited rationality. MIT Press.
Shimbo, M., & Ishida, T. (2003). Controlling the learning process of real-time heuristic search.
Artificial Intelligence, 146(1), 1–41.
Shue, L.-Y., Li, S.-T., & Zamani, R. (2001). An intelligent heuristic algorithm for project scheduling
problems. In Proceedings of the 32nd Annual Meeting of the Decision Sciences Institute.
Shue, L.-Y., & Zamani, R. (1993). An admissible heuristic search algorithm. In Proceedings of the
7th International Symposium on Methodologies for Intelligent Systems (ISMIS-93), Vol. 689
of LNAI, pp. 69–75.
Sigmundarson, S., & Björnsson, Y. (2006). Value Back-Propagation vs. Backtracking in RealTime Search. In Proceedings of the National Conference on Artificial Intelligence (AAAI),
Workshop on Learning For Search, Boston, Massachusetts, USA. AAAI Press.
99

B ULITKO , S TURTEVANT, L U , & YAU

Stenz, A. (1995). The focussed D* algorithm for real-time replanning. In Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI), pp. 1652–1659.
Stout, B. (1996). Smart moves: Intelligent pathfinding. Game Developer Magazine, October, 28–35.
Sturtevant, N. (2005). HOG - Hierarchical Open Graph. http://www.cs.ualberta.ca/˜nathanst/hog/.
Sturtevant, N. (2007). Memory-efficient abstractions for pathfinding. In Proceedings of the third
conference on Artificial Intelligence and Interactive Digital Entertainment, pp. 31–36, Stanford, California.
Sturtevant, N., & Buro, M. (2005). Partial pathfinding using map abstraction and refinement. In
Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 1392–1397,
Pittsburgh, Pennsylvania.
Sturtevant, N., & Jansen, R. (2007). An analysis of map-based abstraction and refinement. In
Proceedings of the 7th International Symposium on Abstraction, Reformulation and Approximation, Whistler, British Columbia. (in press).
Sutton, R. (1990). Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the Seventh International Conference on
Machine Learning, pp. 216–224. Morgan Kaufmann.
Thalmann, D., Noser, H., & Huang, Z. (1997). Autonomous virtual actors based on virtual sensors.
In Lecture Notes in Computer Science (LNCS), Creating Personalities for Synthetic Actors,
Towards Autonomous Personality Agents, Vol. 1195, pp. 25–42. Springer-Verlag, London.
Yahja, A., Stentz, A. T., Singh, S., & Brummit, B. (1998). Framed-quadtree path planning for mobile
robots operating in sparse environments. In In Proceedings, IEEE Conference on Robotics
and Automation, (ICRA), Leuven, Belgium.

100

