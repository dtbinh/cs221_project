Journal of Artificial Intelligence Research 24 (2005) 519-579

Submitted 12/04; published 10/05

The Deterministic Part of IPC-4: An Overview
Jörg Hoffmann

hoffmann@mpi-sb.mpg.de

Max-Planck-Institut für Informatik,
Saarbrücken, Germany

Stefan Edelkamp

stefan.edelkamp@cs.uni-dortmund.de

Fachbereich Informatik,
Universität Dortmund, Germany

Abstract
We provide an overview of the organization and results of the deterministic part of
the 4th International Planning Competition, i.e., of the part concerned with evaluating
systems doing deterministic planning. IPC-4 attracted even more competing systems than
its already large predecessors, and the competition event was revised in several important
respects. After giving an introduction to the IPC, we briefly explain the main differences
between the deterministic part of IPC-4 and its predecessors. We then introduce formally
the language used, called PDDL2.2 that extends PDDL2.1 by derived predicates and timed
initial literals. We list the competing systems and overview the results of the competition.
The entire set of data is far too large to be presented in full. We provide a detailed summary;
the complete data is available in an online appendix. We explain how we awarded the
competition prizes.

1. Introduction
In the application of Artificial Intelligence technology to the real-world, time and space
resources are usually limited. This has led to a performance-oriented interpretation of AI
in many of its research branches. Competition events have been established in automated
theorem proving, in satisfiability testing, and, in particular, in AI Planning. A competition
provides a large-scale evaluation platform. Due to the broadness and neutrality of that platform, a competition is far better at assessing the state-of-the-art in a research branch than
the experiments ran by individual authors: more systems are compared, and the benchmarks
are chosen by the competition organizers, rather than by the system authors themselves.
Moreover, a competition can serve to establish a common representation formalism, and a
common core set of benchmarks, marking the edge of current system capabilities.
The International Planning Competition (IPC) is a biennial event, hosted at the international conferences on AI Planning and Scheduling. The IPC began in 1998 when Drew
McDermott and a committee created a common specification language (PDDL) and a collection of problems forming a first benchmark (McDermott et al., 1998). PDDL is a Lisp-like
input language description format that includes planning formalisms like STRIPS (Fikes &
Nilsson, 1971). Five systems participated in the first international planning competition,
IPC-1 for short, hosted at AIPS 1998 in Pittsburgh, Pennsylvania (McDermott, 2000).
In the year 2000, Fahiem Bacchus continued this work, and the IPC-2 event attracted 16
competitors (Bacchus, 2001). The event – hosted at AIPS 2000 in Breckenridge, Colorado
– was extended to include both fully automatic and hand-tailored planning systems. As
c
2005
AI Access Foundation. All rights reserved.

Hoffmann & Edelkamp

hand-tailored planners were allowed to use some additional domain-dependent information
to the PDDL input in order to improve their performance, they participated in an additional,
separate, track. Both STRIPS and ADL (Pednault, 1989) domains were used but no further
extensions were made to the language (Bacchus, 2000).
The 3rd International Planning Competition, IPC-3, was run by Derek Long and Maria
Fox and was hosted at AIPS 2002, Toulouse, France. The competition attracted 14 competitors (Long & Fox, 2003), and focussed on planning in temporal and metric domains.
For that purpose, Fox and Long developed the PDDL2.1 language (Fox & Long, 2003), of
which the first three levels were used in IPC-3. Level 1 was STRIPS and ADL planning as
before, Level 2 added numeric variables, Level 3 added durational constructs.
The 4th International Planning Competition, IPC-4, was hosted at ICAPS-2004,
Whistler, Canada. IPC-4 built on the previous efforts, in particular the language PDDL2.1.
The competition event was extended and revised in several respects. In particular, IPC-4
featured, for the first time, a competition for probabilistic planners, so that the overall
competition was split into a deterministic part – a continuation of the previous events –
as well as a probabilistic part.1 In the latter part, co-organized by Michael Littman and
Håkan Younes, the main objective of the event was to introduce a common representation
language for probabilistic planners, and to establish some first benchmarks and results. For
more information on the probabilistic part of IPC-4 see the work of Younes and Littman
(2005).
Herein, we provide an overview of the organization and results of the deterministic
part of IPC-4. With 19 competing systems (21 when counting different system versions),
the event was even a little larger than its already large predecessors. Several important
revisions were made to the event. We briefly explain the main differences in Section 2.
Afterwards, Section 3 describes the input language used, named PDDL2.2: the first three
levels of PDDL2.1, extended with derived predicates and timed initial literals. Section 4 lists
and briefly explains the competing systems. Section 5 then presents, for each benchmark
domain, a selection of results plots, highlighting the most important points. The entire
set of data points is far too large to be presented in detail. The full data, including plots
for all results, is available in an online appendix. Section 6 explains how we awarded the
competition prizes, Section 7 closes the paper with some concluding remarks. Appendix A
gives a BNF description of PDDL2.2.

2. Main Revisions made in IPC-4
The main revisions we made to the deterministic part of IPC-4, in difference to its predecessors, were the following.
Competition Workshop. We ran an international workshop on the competition one
year before the event itself (Edelkamp & Hoffmann, 2003), providing the involved groups of
people (system developers, organizing committee, AI Planning researchers in general) with
an opportunity to express their views on issues related to the IPC. Discussions, technical
talks, and panels covered all relevant topics ranging from the event’s organizational structure
and its input language to the selection of benchmarks and the evaluation of results. The
1. While IPC-4 was running, the deterministic part was named “classical” part. We re-named it into
“deterministic” part since that wording is less ambiguous.

520

The Deterministic Part of IPC-4: An Overview

workshop was especially useful for us as the organizers, giving us direct feedback on the
event’s organization.
PDDL Extensions. There was large agreement in the community that PDDL2.1 still
posed a significant challenge, and so the IPC-4 language featured only relatively minor
extensions. We added language features for derived predicates and timed initial literals.
The resulting language is called PDDL2.2, and keeps the 3-leveled structure of PDDL2.1.
Both new language features are practically motivated and were put to use in some of the
IPC-4 domains. Derived predicates add a form of domain axioms to PDDL2.1. A typical
use is to formulate indirect consequences of a planner’s actions. Timed initial literals add
to the temporal part of PDDL2.1 (level 3) a way of defining literals that will become true
at a certain time point, independently of the actions taken by the planner. A typical use is
to formulate time windows and/or goal deadlines.
Application-Oriented Benchmarks. Our main effort in the organization of IPC-4
was to devise a range of interesting benchmark domains, oriented at (and as close as possible
to) real-world application domains. We collaborated with a number of people to achieve
this goal. The description of the application domains, and of our PDDL2.2 adaptations, is
long (50+ pages). It is submitted to this same JAIR special track (Edelkamp, Hoffmann,
Englert, Liporace, Thiebaux, & Trüg, 2005). The application domains we modelled were:
• Airport, modelling airport ground traffic control (Hatzack & Nebel, 2001; Trüg, Hoffmann, & Nebel, 2004).
• Pipesworld, modelling oil derivative transportation in pipeline networks (Milidiu, dos
Santos Liporace, & de Lucena, 2003; Milidiú & dos Santos Liporace, 2004).
• Promela, modelling deadlock detection in communication protocols formulated in the
Promela language (Edelkamp, 2003b, 2003a).
• PSR, a deterministic variant of the Power Supply Restoration benchmark (Bertoli,
Cimatti, Slaney, & Thiébaux, 2002; Bonet & Thiebaux, 2003).
• UMTS, modelling the task of scheduling the concurrent application setup in UMTS
mobile devices (Englert, 2003, 2005).
In addition, we re-used the Satellite and Settlers domains from IPC-3. In the case of
Satellite, we added some additional domain versions to model more advanced aspects of the
application (namely, the sending of the data to earth). Each domain is described in a little
more detail in the part (sub-section) of Section 5 that contains the respective competition
results.
Domain Compilations. For the first time in the IPC, we provided domain formulations where problem constraints were compiled from the more complex PDDL subsets
into the less complex subsets. In many of our domains the most natural domain formulation comes with complex precondition formulas and conditional effects, i.e., in ADL. We
compiled these domain formulations to STRIPS. In previous IPCs, for example in the Elevator domain used in IPC-2, the more interesting problem constraints were dropped in
the STRIPS domain versions. By using the compilation approach, we hope and believe to
have created a structurally much more interesting range of STRIPS benchmarks. The ADL
521

Hoffmann & Edelkamp

and STRIPS encodings of the same instances were posed to the competitors in an optional
way, i.e., of every domain version there could be several domain version formulations. The
competitors were allowed to choose the formulation they liked best. The results within a
domain version were evaluated together, in order to keep the number of separation lines in
the data at an acceptable level.
We also employed compilations encoding the new PDDL2.2 language features in terms
of artificial constructs with the old PDDL2.1 features. By this we intended to enable
as wide participation in the domains as possible. The compiled domains were offered as
separate domain versions rather than alternative formulations because, in difference to
the ADL/STRIPS case, we figured that the compiled domains were too different from the
original ones to allow joint evaluation. Most importantly, when compiling derived predicates
or timed initial literals away, the plan length increases. Details about the compilation
methods and about the arrangement of the individual domains are in the paper describing
these domains (Edelkamp et al., 2005).
Optimal vs. Satisficing Planning. We define optimal planners as planners that
prove a guarantee on the quality of the found solution. Opposed to that, satisficing planners are planners that do not prove any guarantee other than the correctness of the solution.2
Previous IPCs did not distinguish between optimal planners and satisficing planners. But
that is not fair since optimal planners are essentially solving a different problem. The theoretical hardness of a domain can differ for the different kinds of planners. In fact, it was
recently proved that, in most benchmark domains, satisficing planning is easy (polynomial)
while optimal planning is hard (NP-complete) (Helmert, 2001, 2003). In practice, i.e., in
most of the commonly used benchmark domains, nowadays there is indeed a huge performance gap between optimal and satisficing planners. In IPC-4, we separated them into
different tracks. The optimal track attracted seven systems; for example, the planning as
satisfiability approach, that had disappeared in IPC-3, resurfaced.
Competition Booklet and Results Posters. At previous competitions, at conference time one could neither access the results obtained in the competition, nor descriptions
of the competing planners. This is clearly a drawback, especially given the growing complexity of the event. For ICAPS 2004, we assembled a booklet containing extended abstracts
describing the core aspects of all competing systems (Edelkamp, Hoffmann, Littman, &
Younes, 2004); the booklet was distributed to all conference participants. The competition
results, i.e., those of the deterministic part, were made available in the form of posters
showing runtime and plan quality plots.
IPC Web-page. An important repository for any large-scale competition event is a web
page containing all the relevant information – benchmark problems, language descriptions,
result files, etc. For IPC-4, we have set this page up at the permanent Internet address
http://ipc.icaps-conference.org. In the long run, this address is intended to provide
an entry point to the IPC event as a whole, thereby avoiding the need to look up the pages
for the different IPC editions at completely separate points in the web.
As a less positive change from IPCs 2 and 3 to IPC-4, the track for hand-tailored planners
disappeared. The reason for this is simply that no such systems registered as competitors.
2. Satisficing planners were referred to as “sub-optimal” when IPC-4 was running. We decided to replace
the term since it is a bit misleading. While not guaranteeing optimal solutions, a satisficing planner may
well produce such solutions in some cases.

522

The Deterministic Part of IPC-4: An Overview

Of course, this is not a coincidence. There is a large agreement in the community that there
are several problems with the hand-tailored track as ran in IPC-2 and IPC-3. The most
important criticism is that, for hand-tailored planners, “performance” is not just runtime
and plan quality results on a set of benchmarks. What’s also important – maybe the most
important aspect of all – is how much effort was spent in achieving these results: how hard
is it to tailor the planner? While the latter obviously is an important question, likewise
obviously there isn’t an easy answer. We discussed the matter with a lot of people (e.g., in
the ICAPS’03 workshop), but no-one could come up with an idea that seemed adequate and
feasible. So basically we offered the hand-tailored planners the opportunity to participate
in a track similar to the previous ones, maybe with some additional ad-hoc measurements
such as how many person hours were spent in tailoring the planner to a domain. Apart
from the evaluation shortcomings of such an approach, another important reason why no
hand-tailored planners registered is that participating in that track is a huge amount of
work – maintaining/developing the planner plus understanding the domains and tailoring
the planner. Understanding the domains would have been particularly hard with the more
complex domains used in IPC-4. It is thus, obviously, difficult to find enough time to
participate in a hand-tailored IPC. Some more on the future of the hand-tailored track is
said in Section 7.

3. PDDL 2.2
As said, the IPC-4 competition language PDDL2.2 is an extension of the first three levels
of PDDL2.1 (Fox & Long, 2003). PDDL2.2 inherits the separation into the levels. The
language features added on top of PDDL2.1 are derived predicates (into levels 1,2, and 3)
and timed initial literals (into level 3 only). We now discuss these two features in that
order, describing their syntax and semantics. A full BNF description of PDDL2.2 is in
Appendix A.
3.1 Derived Predicates
Derived predicates have been implemented in several planning systems in the past, for
example in UCPOP (Penberthy & Weld, 1992). They are predicates that are not affected
by any of the actions available to the planner. Instead, the predicate’s truth values are
derived by a set of rules of the form if φ(x) then P (x). The semantics, roughly, are that
an instance of a derived predicate (a derived predicate whose arguments are instantiated
with constants; a fact, for short) is TRUE if and only if it can be derived using the available
rules (more details below). Under the name “axioms”, derived predicates were a part of
the original PDDL language defined by McDermott (McDermott et al., 1998) for the first
planning competition, but they have never been put to use in a competition benchmark
(we use the name “derived predicates” instead of “axioms” in order to avoid confusion with
safety conditions).
Derived predicates combine several key aspects that made them a useful language extension for IPC-4:
• They are practically motivated: in particular, they provide a concise and convenient
means to express updates on the transitive closure of a relation. Such updates occur
523

Hoffmann & Edelkamp

in domains that include structures such as paths or flows (electricity flows, chemical
flows, etc.); in particular, the PSR domain includes this kind of structure.
• They are also theoretically justified in that compiling them away can be infeasible.
It was recently proved that, in the worst case, compiling derived predicates away
results in an exponential blow up of either the problem description or the plan length
(Thiebaux, Hoffmann, & Nebel, 2003, 2005).
• Derived predicates do not cause a significant implementation overhead in, at least,
the state transition routines used by forward search planners. When the world state –
the truth values of all non-derived, basic, predicates – is known, computing the truth
values of the derived predicates is trivial.3
In the IPC-4 benchmarks, derived predicates were used only in the non-durational context, PDDL2.2 level 1.
3.1.1 Syntax
The BNF definition of derived predicates involves just two small modifications to the BNF
definition of PDDL2.1:
hstructure-defi ::=:derived−predicates hderived-defi

The domain file specifies a list of “structures”. In PDDL2.1 these were either actions or
durational actions. Now we also allow “derived” definitions at these points.
hderived-defi ::= (:derived hatomic formula(term)i hGDi)

The “derived” definitions are the “rules” mentioned above. They simply specify the
predicate P to be derived (with variable vector x), and the formula φ(x) from which instances of P can be concluded to be true. Syntactically, the predicate and variables are
given by the hatomic formula(term)i expression, and the formula is given by hGDi (a “goal
description”, i.e. a formula).
The BNF is more generous than what we actually allow in PDDL2.2, respectively in
IPC-4. We make a number of restrictions to ensure that the definitions make sense and
are easy to treat algorithmically. We call a predicate P derived if there is a rule that has
a predicate P in its head; otherwise we call P basic. The restrictions we make are the
following.
1. The actions available to the planner do not affect the derived predicates: no derived
predicate occurs on any of the effect lists of the domain actions.
2. If a rule defines that P (x) can be derived from φ(x), then the variables in x are
pairwise different (and, as the notation suggests, the free variables of φ(x) are exactly
the variables in x).
3. Note, though, that it may be much less trivial to adapt a heuristic function to handle derived predicates;
this is, for example, discussed by Thiebaux et al. (2003, 2005).

524

The Deterministic Part of IPC-4: An Overview

3. If a rule defines that P (x) can be derived from φ, then the Negation Normal Form
(NNF) of φ(x) does not contain any derived predicates in negated form.
The first restriction ensures that there is a separation between the predicates that the
planner can affect (the basic predicates) and those (the derived predicates) whose truth
values follow from the basic predicates. The second restriction ensures that the rule right
hand sides match the rule left hand sides. Let us explain the third restriction. The NNF of
a formula is obtained by “pushing the W
negations V
downwards”, i.e.
¬∀x : φ into
V transforming
W
∃x : (¬φ), ¬∃x : φ into ∀x : (¬φ), ¬ φi into (¬φi ), and ¬ φi into (¬φi ). Iterating
these transformation steps, one ends up with a formula where negations occur only in front
of atomic formulas – predicates with variable vectors, in our case. The formula contains a
predicate P in negated form if and only if there is an occurrence of P that is negated. By
requiring that the formulas in the rules (that derive predicate values) do not contain any
derived predicates in negated form, we ensure that there can not be any negative interactions
between applications of the rules (see the semantics below).
An example of a derived predicate is the “above” predicate in the Blocksworld, which
is true between blocks x and y whenever x is transitively (possibly with some blocks in
between) on y. Using the derived predicates syntax, this predicate can be defined as follows.
(:derived (above ?x ?y)
(or (on ?x ?y)
(exists (?z) (and (on ?x ?z)
(above ?z ?y)))))

Note that formulating the truth value of “above” in terms of the effects of the normal
Blocksworld actions is very awkward. Since the set of atoms affected by an action depends
on the situation, one either needs artificial actions or complex conditional effects.
3.1.2 Semantics
We now describe the updates that need to be made to the PDDL2.1 semantics definitions
given by Fox and Long (2003). We introduce formal notations to capture the semantics
of derived predicates. We then “hook” these semantics into the PDDL2.1 language by
modifying two of Fox and Long (2003)’s definitions.
Say we are given the truth values of all (instances of the) basic predicates, and want to
compute the truth values of the (instances of the) derived predicates from that. We are in
this situation every time we have applied an action, or parallel action set. In the durational
context, we are in this situation at the “happenings” in our current plan, that is every time
a durative action starts or finishes. Formally, what we want to have is a function D that
maps a set of basic facts (instances of basic predicates) to the same set but enriched with
derived facts (the derivable instances of the derived predicates). Assume we are given the
set R of rules for the derived predicates, where the elements of R have the form (P (x), φ(x))
– if φ(x) then P (x). Then D(s), for a set of basic facts s, is defined as follows.
\
D(s) := {s0 | s ⊆ s0 , ∀(P (x), φ(x)) ∈ R : ∀c, |c| = |x| : (s0 |= φ(c) ⇒ P (c) ∈ s0 )}
(1)
This definition uses the standard notations of the modelling relation |= between states
(represented as sets of facts in our case) and formulas, and of the substitution φ(c) of the
525

Hoffmann & Edelkamp

free variables in formula φ(x) with a constant vector c. In words, D(s) is the intersection
of all supersets of s that are closed under application of the rules R.
Remember that we restrict the rules to not contain any derived predicates in negated
form. This implies that the order in which the rules are applied to a state does not matter
(we can not “lose” any derived facts by deriving other facts first). This, in turn, implies
that D(s) is itself closed under application of the rules R. In other words, D(s) is the least
fixed point over the possible applications of the rules R to the state where all derived facts
are assumed to be FALSE (represented by their not being contained in s).
More constructively, D(s) can be computed by the following simple process.
s0 := s
do
select a rule (P (x), φ(x)) and a vector c of constants,
with |c| = |x|, such that s0 |= φ(c) and P (c) 6∈ s0
0
let s := s0 ∪ {P (c)}
until no such rule and constant vector exist
let D(s) := s0
In words, apply the applicable rules in an arbitrary order until no new facts can be
derived anymore.
We can now specify what an executable plan is in PDDL2.1 with derived predicates.
All we need to do is to hook the function D into Definition 13, “Happening Execution”,
given by Fox and Long (2003). By this definition, Fox and Long (2003) define the state
transitions in a plan. The happenings in a (temporal or non-temporal) plan are all time
points at which at least one action effect occurs. Fox and Long (2003)’s definition is this:
Definition 13 Happening Execution (Fox & Long, 2003)
Given a state, (t, s, x) and a happening, H, the activity for H is the set of grounded actions
AH = {a| the name for a is in H, a is valid and
P rea is satisfied in (t, s, x)}
The result of executing a happening, H, associated with time tH , in a state (t, s, x) is
undefined if |AH | =
6 |H| or if any pair of actions in AH is mutex. Otherwise, it is the state
0
0
(tH , s , x ) where
[
[
s0 = (s \
Dela ) ∪
Adda
(2)
a∈AH

and

x0

a∈AH

is the result of applying the composition of the functions {NPFa | a ∈ AH } to x.

Note that the happenings consist of grounded actions, i.e., all operator parameters are
instantiated with constants. To introduce the semantics of derived predicates, we now
modify the result of executing the happening. (We will also adapt the definition of mutex
actions, see below.) The result of executing the happening is now obtained by applying
the actions to s, then subtracting all derived facts from this, then applying the function D.
That is, in the above definition we replace Equation 2 with the following:
[
[
s0 = D(((s \
Dela ) ∪
Adda ) \ D)
(3)
a∈AH

a∈AH

526

The Deterministic Part of IPC-4: An Overview

where D denotes the set of all derived facts. If there are no derived predicates, D is the
empty set and D is the identity function.
As an example, say we have a Blocksworld instance where A is on B is on C, s =
{clear(A), on(A, B), on(B, C), ontable(C), above(A, B), above(B, C), above(A, C)}, and
our happening applies an action that moves A to the table. Then the happening execution
result will be computed by removing on(A, B) from s, adding clear(B) and ontable(A) into
s, removing all of above(A, B), above(B, C), and above(A, C) from s, and applying D to
this, which will re-introduce (only) above(B, C). So s0 will be s0 = {clear(A), ontable(A),
clear(B), on(B, C), ontable(C), above(B, C) }.
By the definition of happening execution, Fox and Long (2003) define the state transitions in a plan. The definitions of what an executable plan is, and when a plan achieves
the goal, are then standard. The plan is executable if the result of all happenings in the
plan is defined. This means that all action preconditions have to be fulfilled in the state of
execution, and that no two pairs of actions in a happening are mutex. The plan achieves
the goal if the goal holds true in the state that results after the execution of all actions in
the plan.
With our above extension of the definition of happening executions, the definitions of
plan executability and goal achievement need not be changed. We do, however, need to
adapt the definition of when a pair of actions is mutex. This is important if the happenings
can contain more than one action, i.e., if we consider parallel (Graphplan-style) or concurrent (durational) planning. Fox and Long (2003) give a conservative definition that forbids
the actions to interact in any possible way. The definition is the following.
Definition 12 Mutex Actions (Fox & Long, 2003)
Two grounded actions, a and b are non-interfering if
GP rea ∩ (Addb ∪ Delb ) = GP reb ∩ (Adda ∪ Dela ) = ∅,
Adda ∩ Delb = Addb ∩ Dela = ∅,
La ∩ Rb = Ra ∩ Lb = ∅,
La ∩ Lb ⊆ L∗a ∪ L∗b

(4)

If two actions are not non-interfering they are mutex.
Note that the definition talks about grounded actions where all operator parameters
are instantiated with constants. La , Lb , Ra , and Rb refer to the left and right hand side
of a’s and b’s numeric effects. Adda /Addb and Dela /Delb are a’s and b’s positive (add)
respectively negative (delete) effects. GP rea /Gpreb denotes all (ground) facts that occur in
a’s/b’s precondition.
If a preconditionWcontains quantifiers then these are grounded out (∀x
V
transforms to ci , ∃x transforms to ci where the ci are all objects in the given instance),
and GP re is defined over the resulting quantifier-free (and thus variable-free) formula. Note
that this definition of mutex actions is very conservative – if, for example, fact F occurs
only positively in a’s precondition, then it does not matter if F is among the add effects of
b. The conservative definition has the advantage that it makes it algorithmically very easy
to figure out if or if not a and b are mutex.
In the presence of derived predicates, the above definition needs to be extended to exclude possible interactions that can arise indirectly due to derived facts, in the precondition
of the one action, whose truth value depends on the truth value of (basic) facts affected
527

Hoffmann & Edelkamp

by the effects of the other action. In the same spirit in that Fox and Long (2003) forbid
any possibility of direct interaction, we now forbid any possibility of indirect interaction.
Assume we ground out all rules (P (x), φ(x)) for the derived predicates, i.e., we insert all
possible vectors c of constants; we also ground out the quantifiers in the formulas φ(c), ending up with variable free rules. We define a directed graph where the nodes are (ground)
facts, and an edge from fact F to fact F 0 is inserted iff there is a grounded rule (P (c), φ(c))
such that F 0 = P (c), and F occurs in φ(c). Now say we have an action a, where all ground
facts occurring in a’s precondition are, see above, denoted by GP rea . By DP rea we denote
all ground facts that can possibly influence the truth values of the derived facts in GP rea :
DP rea := {F | there is a path from F to an F 0 ∈ GP rea }

(5)

The definition of mutex actions is now updated simply by replacing Equation 4 with:
(DP rea ∪ GP rea ) ∩ (Addb ∪ Delb ) =
(DP reb ∪ GP reb ) ∩ (Adda ∪ Dela ) = ∅,
Adda ∩ Delb = Addb ∩ Dela = ∅,
La ∩ Rb = Ra ∩ Lb = ∅,
La ∩ Lb ⊆ L∗a ∪ L∗b

(6)

Note that the only thing that has changed is the first line, regarding interference of propositional effects and preconditions. As an example, reconsider the Blocksworld and the “above”
predicate. Assume that the action that moves a block A to the table requires as an additional, derived, precondition, that A is above some third block. Then, in principle, two
actions that move two different blocks A and B to the table can be executed in parallel.
Which block A (B) is on can influence the above relations in that B (A) participates; however, this does not matter because if A and B can be both moved then this implies that they
are both clear, which implies that they are on top of different stacks anyway. We observe
that the latter is a statement about the domain semantics that either requires non-trivial
reasoning, or access to the world state in which the actions are executed. In order to avoid
the need to either do non-trivial reasoning about domain semantics, or resort to a forward
search, our definition is the conservative one given above. The definition makes the actions
moving A and B mutex on the grounds that they can possibly influence each other’s derived
preconditions.
The definition adaptations described above suffice to define the semantics of derived
predicates for the whole of PDDL2.2. Fox and Long (2003) reduce the temporal case to the
case of simple plans above, so by adapting the simple-plan definitions we have automatically
adapted the definitions of the more complex cases. In the temporal setting, PDDL2.2 level 3,
the derived predicates semantics are that their values are computed anew at each happening
in the plan where an action effect occurs. As said, in IPC-4 we used derived predicates only
in the non-temporal setting. Some remarks on limitations of the IPC-4 treatment of derived
predicates, and on future prospects, are in Section 7.
3.2 Timed Initial Literals
Timed initial literals are a syntactically very simple way of expressing a certain restricted
form of exogenous events: facts that will become TRUE or FALSE at time points that are
known to the planner in advance, independently of the actions that the planner chooses
528

The Deterministic Part of IPC-4: An Overview

to execute. Timed initial literals are thus deterministic unconditional exogenous events.
Syntactically, we simply allow the initial state to specify – beside the usual facts that are
true at time point 0 – literals that will become true at time points greater than 0.
Timed initial literals are practically very relevant: in the real world, deterministic unconditional exogenous events are very common, typically in the form of time windows –
within which a shop has opened, within which humans work, within which traffic is slow,
within which there is daylight, within which a seminar room is occupied, within which nobody answers their mail because they are all at conferences, etc. The timed initial literals
syntax is just about the simplest way one can think of to communicate such things to a
planner.
Timed initial literals can easily be compiled into artificial constructs (Fox, Long, &
Halsey, 2004), involving an only linear blow-up in the instance representation and in plan
length (i.e., number of actions in it). Still it seems highly likely that handing the timed
literals over to an automated planner explicitly results in far better performance than when
one hands over the artificial (compiled) representation. The results obtained in IPC-4
confirm this, see also Section 5.
3.2.1 Syntax
The BNF notation is:
hiniti ::= (:init hinit-eli∗ )
hinit-eli ::=:timed−initial−literals (at hnumberi hliteral(name)i)

The requirement flag for timed initial literals implies the requirement flag for durational
actions, i.e., as said the language construct is only available in PDDL2.2 level 3. The times
hnumberi at which the timed literals occur are restricted to be greater than 0. If there are
also derived predicates in the domain, then the timed literals are restricted to not influence
any of these, i.e., like action effects they are only allowed to affect the truth values of the
basic (non-derived) predicates (IPC-4 will not use both derived predicates and timed initial
literals within the same domain).
As an illustrative example, consider a planning task where the goal is to have completed
the shopping. There is a single action go-shopping that achieves the goal, and requires the
(single) shop to be open as the precondition. The shop opens at time 9 relative to the initial
state, and closes at time 20. We can express the shop opening times by two timed initial
literals:
(:init
(at 9 (shop-open))
(at 20 (not (shop-open)))
)

3.2.2 Semantics
We now describe the updates that need to be made to the PDDL2.1 semantics definitions
given by Fox and Long (2003). Adapting two of the definitions suffices.
529

Hoffmann & Edelkamp

The first definition we need to adapt is the one that defines what a “simple plan”, and
its happening sequence, is. The original definition by Fox and Long (2003) is this.
Definition 11 Simple Plan (Fox & Long, 2003)
A simple plan, SP , for a planning instance, I, consists of a finite collection of timed simple
actions which are pairs (t, a), where t is a rational-valued time and a is an action name.
The happening sequence, {ti }i=0...k for SP is the ordered sequence of times in the set
of times appearing in the timed simple actions in SP . All ti must be greater than 0. It is
possible for the sequence to be empty (an empty plan).
The happening at time t, Et , where t is in the happening sequence of SP , is the set of
(simple) action names that appear in timed simple actions associated with the time t in SP .
In the STRIPS case, the time stamps are the natural numbers 1, . . . , n when there are n
actions/parallel action sets in the plan. The happenings then are the actions/parallel action
sets at the respective time steps. Fox and Long (2003) reduce the temporal planning case
to the simple plan case defined here by splitting each durational action up into at least two
simple actions – the start action, the end action, and possibly several actions in between
that guard the durational action’s invariants at the points where other action effects occur.
So in the temporal case, the happening sequence is comprised of all time points at which
“something happens”, i.e., at which some action effect occurs.
To introduce our intended semantics of timed initial literals, all we need to do to this
definition is to introduce additional happenings into the temporal plan, namely the time
points at which some timed initial literal occurs. The timed initial literals can be interpreted
as simple actions that are forced into the respective happenings (rather than selected into
them by the planner), whose precondition is true, and whose only effect is the respective
literal. The rest of Fox and Long (2003)’s definitions then carry over directly (except goal
achievement, which involves a little care, see below). The PDDL2.2 definition of simple
plans is this here.
Definition 11 Simple Plan
A simple plan, SP , for a planning instance, I, consists of a finite collection of timed simple
actions which are pairs (t, a), where t is a rational-valued time and a is an action name.
By tend we denote the largest time t in SP , or 0 if SP is empty.
Let T L be the (finite) set of all timed initial literals, given as pairs (t, l) where t is the
rational-valued time of occurrence of the literal l. We identify each timed initial literal (t, l)
in T L with a uniquely named simple action that is associated with time t, whose precondition
is TRUE, and whose only effect is l.
The happening sequence, {ti }i=0...k for SP is the ordered sequence of times in the set
of times appearing in the timed simple actions in SP and T L. All ti must be greater than
0. It is possible for the sequence to be empty (an empty plan).
The happening at time t, Et , where t is in the happening sequence of SP , is the set of
(simple) action names that appear in timed simple actions associated with the time t in SP
or T L.
Thus the happenings in a temporal plan are all points in time where either an action
effect, or a timed literal, occurs. The timed literals are simple actions forced into the
plan. With this construction, Fox and Long (2003)’s Definitions 12 (Mutex Actions) and 13
530

The Deterministic Part of IPC-4: An Overview

(Happening Execution), as described (and adapted to derived predicates) in Section 3.1.2,
can be kept unchanged. They state that no action effect is allowed to interfere with a timed
initial literal, and that the timed initial literals are true in the state that results from the
execution of the happening they are contained in. Fox and Long (2003)’s Definition 14
(Executability of a plan) can also be kept unchanged – the timed initial literals change the
happenings in the plan, but not the conditions under which a happening can be executed.
The only definition we need to reformulate is that of what the makespan of a valid plan
is. In Fox and Long (2003)’s original definition, this is implicit in the definition of valid
plans. The definition is this.
Definition 15 Validity of a Simple Plan (Fox & Long, 2003)
A simple plan (for a planning instance, I) is valid if it is executable and produces a final
state S, such that the goal specification for I is satisfied in S.
The makespan of the valid plan is accessible in PDDL2.1 and PDDL2.2 by the “totaltime” variable that can be used in the optimization expression. Naturally, Fox and Long
(2003) take the makespan to be the end of the plan, the time point of the plan’s final state.
In the presence of timed initial literals, the question of what the plan’s makespan is
becomes a little more subtle. With Fox and Long (2003)’s above original definition, the
makespan would be the end of all happenings in the simple plan, which include all timed
initial literals (see the revised Definition 11 above). So the plan would at least take as long
as it takes until no more timed literals occur. But a plan might be finished long before that
– imagine something that needs to be done while there is daylight; certainly the plan does
not need to wait until sunset. We therefore define the makespan to be the earliest point
in time at which the goal condition becomes (and remains) true. Formally this reads as
follows.
Definition 15 Validity and Makespan of a Simple Plan
A simple plan (for a planning instance, I) is valid if it is executable and produces a final
state S, such that the goal specification for I is satisfied in S. The plan’s makespan is
the smallest t ≥ tend such that, for all happenings at times t0 ≥ t in the plan’s happening
sequence, the goal specification is satisfied after execution of the happening.
Remember that tend denotes the time of the last happening in the plan that contains
an effect caused by the plan’s actions – in simpler terms, tend is the end point of the plan.
What the definition says is that the plan is valid if, at some time point t after the plan’s
end, the goal condition is achieved and remains true until after the last timed literal has
occurred. The plan’s makespan is the first such time point t. Note that the planner can
“use” the events to achieve the goal, by doing nothing until a timed literal occurs that
makes the goal condition true – but then the waiting time until the nearest such timed
literal is counted into the plan’s makespan. (The latter is done to avoid situations where
the planner could prefer to wait millions of years rather than just applying a single action
itself.) Remember that the makespan of the plan, defined as above, is what can be denoted
by total-time in the optimization expression defined with the problem instance.
As with the derived predicates, the definition adaptations above suffice to define the
semantics of timed initial literals for PDDL2.2. Some remarks on limitations of the language
construct, and on future prospects, are in Section 7.
531

Hoffmann & Edelkamp

4. The Competitors
We now provide an overview table listing the systems along with their language capabilities
(subset of PDDL2.2 covered), and we sketch the competing systems. The deterministic
part of IPC-4 attracted 19 competitors (21 when counting different system versions), 12
(14) of which were satisficing and 7 of which were optimal. Each competitor wrote a 23 page extended abstract for inclusion in the IPC-4 booklet at ICAPS 2004 (Edelkamp
et al., 2004). The included sketches are very brief outlines of these abstracts. The reader is
encouraged to read the original work by the system authors.
At this point it is appropriate to say a few words on system development. We allowed
competitors to modify their systems during the competition phase, i.e., while data collection
was running. Our reasons for doing so were twofold. First, some of our domains were quite
unusual – most particularly, those containing ADL constructs compiled to STRIPS – so we
(rightly) expected planner implementations to have parsing etc. trouble with them. Second,
from our own experience with the competitions we knew that people will try to enhance
their planners anyway so we did not see much point in forbidding this. We trusted the
competitors to not do stupid things like hard-coding domain names etc., and we trusted to
have a diverse enough range of domains to make tuning a “domain-independent” task.
An alternative would have been to collect executables before data collection, and then
have all the results collected by the organizers. Anyone who has ever run experiments
with someone else’s planner knows that such an approach is completely infeasible due to
the prototype-nature of the systems, and due to the many tiny details in minor language
changes, programming environments, etc. – if one wants to obtain meaningful results, at
least. One could in principle apply a strict “any failure is counted as unsolved” rule, but
doing so would likely lay way too much emphasis on little programming errors that have
nothing to do with the evaluated algorithms.
The competition phase was, roughly, from February 2004 until middle May 2004. During that time, we released the domains one-by-one, and the competitors worked on their
systems, handing the results over to us when they were ready. At the end of that phase, the
competitors had to submit an executable, and we ran some sampled tests to see that the
executable did really produce the reported results, across all the domains. The abstracts
describing the systems had to be delivered a little earlier, by the end of April, due to timing
constraints for printing the booklet.
We start with the overview table, then sketch the satisficing and optimal competitors.
4.1 Planner Overview
An overview of the participating satisficing planners, and their language capabilities (defined
as what language features they attacked in IPC-4), is given in Table 1.
Observe that most of the planners treat only a small subset of PDDL2.2: from a quick
glance at the table, one sees that there are far more (“-”) entries than (“+”) entries. Note
here that, often, even when a (“+”) sign is given, a planner may treat only a subset of
the specified language feature (as needed for the respective IPC-4 domain). For example, a
planner might treat only a subset of ADL, or only linear arithmetic for numeric variables.
We leave out these details for the sake of readability.
532

The Deterministic Part of IPC-4: An Overview

CRIKEY
FAP
FD, FDD
LPG-TD
Macro-FF
Marvin
Optop
P-MEP
Roadmapper
SGPlan
Tilsapa
YAHSP
BFHSP
CPT
HSP∗a
Optiplan
SATPLAN
SemSyn
TP4

ADL
+
+
+
+
+
+
+
+
-

DP
+
+
+
+
+
-

Numbers
+
+
+
+
+
+
+
+
+

Durations
+
+
+
+
+
+
+
+
+

TL
+
+
+
+
+
-

Table 1: An overview of the planners in IPC-4, and their language capabilities (i.e., the
language features they attacked in IPC-4). satisficing planners are in the top half
of the table, optimal planners are in the bottom half. Each table entry specifies
whether (“+”) or not (“-”) a planner can handle a language feature. “DP” is short
for derived predicates, “TL” is short for timed initial literals. With “Numbers”
and “Durations” we mean numeric fluents and fixed action durations (no duration
inequalities), in the sense of PDDL2.1 level 2 and 3, respectively. The planners
(and their name abbreviations) are explained below.

LPG-TD, Optop, and SGPlan are the only planners that treat the full range of PDDL2.2,
as used in IPC-4. Three satisficing planners (FAP, Roadmapper, and YAHSP), and three
optimal planners (BFHSP, Optiplan, and SATPLAN), treat only pure STRIPS. Derived
predicates are treated by 4 planners, timed initial literals by 5 planners, ADL by 7 planners,
numeric variables by 8 planners, and action durations by 9 planners.
Table 1 shows that there is a significant amount of acceptance of the new (PDDL2.1
and PDDL2.2) language features, in terms of implemented systems participating in the
IPC. Table 1 also shows that the development on the systems side has a tendency to be
slower than the development on the IPC language side: even the most wide-spread language
feature beyond STRIPS, action durations, is dealt with by less than half of the 19 planners.
In our opinion, this should be taken to indicate that further language extensions should be
made slowly. Some more on this is said in Section 7.
533

Hoffmann & Edelkamp

4.2 Satisficing Planners
The satisficing planners – the planners giving no guarantee on the quality of the returned
plan – were the following. We proceed in alphabetical order.
CRIKEY by Keith Halsey, University of Strathclyde, Glasgow, UK. CRIKEY is a
heuristic search forward state space planner. It includes a heuristic for relaxed plans for
temporal and numeric problems and applies a scheduler based on simple temporal networks
to allow posterior plan scheduling.
FAP by Guy Camilleri and Joseph Zalaket, University Paul Sabatier / IRIT CCICSC, France. FAP handles non-temporal non-numeric domains. It is a heuristic planner
using the relaxed planning heuristic in an N -best forward search, with meta-actions (action
sequences) extracted from the relaxed planning graph to perform jumps in the state space.
Fast Downward, FD for short, and Fast Diagonally Downward, FDD for short,
by Malte Helmert and Silvia Richter, University of Freiburg, Germany. FD and FDD can
treat non-temporal non-numeric domains. They apply a new heuristic estimate based on
a polynomial relaxation to the automatically inferred multivariate or SAS+ representation
of the problem space. FDD also applies the traditional FF-style relaxed planning heuristic,
i.e., it applies both heuristics in a hybrid search algorithm.
LPG-TD by Alfonso Gerevini, Alessandro Saetti, Ivan Serina, and Paolo Toninelli,
University of Brescia, Italy. The extensions to the randomized local plan graph search
that was already included in LPG at IPC-3 includes functionality for PDDL2.2 derived
predicates and timed initial literals, as well as various implementation refinements. A version tailored to computation speed, LPG-TD.speed, and a version tailored for plan quality,
LPG-TD.quality, participated. LPG-TD.quality differs from LPG-TD.speed basically in
that it does not stop when the first plan is found but continues until a stopping criterion
is met. The LPG-TD team also ran a third version, called “LPG-TD.bestquality”, which
used, for every instance, the entire half hour CPU time available to produce as good a plan
as possible. This third version is not included in the official data because every team was
allowed to enter at most two system versions.
Macro-FF by Adi Botea, Markus Enzenberger, Martin Müller, Jonathan Schaeffer,
University of Alberta, Canada. As the name suggests Macro-FF extends Hoffmann’s FF
planner with macro operators (and other implementation refinements). Macros are learned
prior to the search and fed into the planner as new data, separated from the operator file
and the problem file. At runtime, both regular actions and macro-actions are used for state
expansion. Heuristic rules for pruning instantiations of macros are added to FF’s original
strategy for search control.
Marvin by Andrew Coles and Amanda Smith, University of Strathclyde, Glasgow,
UK. Marvin extends FF by adding extra features and preprocessing information, such as
plateau-escaping macro actions, to enhance the search algorithm.
Optop by Drew McDermott, Yale University, USA. Optop is an extension of the wellknown UNPOP planner, with the ability to handle a complex input language including,
amongst other things, autonomous processes running in parallel to the actions taken by
the planner. The underlying principle is a forward search with heuristic guidance obtained
from greedy-regression match graphs that are built backwards from the goals.
534

The Deterministic Part of IPC-4: An Overview

P-MEP by Javier Sanchez, Minh Tang, and Amol D. Mali, University of Wisconsin,
USA. P-MEP is short for Parallel More Expressive Planner. Unlike most planners, P-MEP
can treat numeric variables with non-linear action effects. It employs a forward search with
relaxed plan heuristics, enhanced by relevance detection in a pre-process, and by taking
into account exclusion relations during relaxed planning.
Roadmapper by Lin Zhu and Robert Givan, Purdue University, USA. Roadmapper
handles non-temporal non-numeric domains. It is a forward heuristic search planner enhancing the FF heuristic with a reasoning about landmarks. The latter are propositions
that must be true at some point in every legal plan. Roadmapper finds landmarks in a
pre-process, arranges them in a directed road-map graph, and uses that graph to assign
weights to actions in the FF-style relaxed plans.
SGPlan by Yixin Chen, Chih-Wei Hsu and Benjamin W. Wah, University of Illinois,
USA. The planner bootstraps heuristic search planners by applying Lagrange optimization
to combine the solution of planning subproblems. To split the problem, an ordering of the
planning goals is derived. The incremental local search strategy that is applied for Lagrange
optimization on top of the individual planners relies on the theory of extended saddle points
for mixed integer linear programming.
Tilsapa by Bharat Ranjan Kavuluri and Senthil U., AIDB Lab, IIT Madras, India.
The planner extends the SAPA system that handles temporal numeric domains, with the
ability to handle timed initial literals.
YAHSP by Vincent Vidal, University of Artois, France. YAHSP, an acronym for
yet another heuristic search planner, searches forward with a FF-style relaxed planning
heuristic. The main enhancement is a relaxed-plan-analysis phase that replaces actions
in the relaxed plan based on heuristic notions of better suitability in reality, producing
macro-actions in the process. The macro-actions are, basically, as long as possible feasible
sub-sequences of the (modified) relaxed plan.
4.3 Optimal Planners
The participating optimal planners were the following, in alphabetical order.
BFHSP by Rong Zhou and Eric A. Hansen, Mississippi State University, USA. BFHSP,
for breadth-first heuristic search planner, optimizes the number of actions in the plan.
The planner implements the standard max-atom and max-pair heuristics (as well as the
max-triple heuristic, which was however not used in IPC-4). The main difference to other
systems is its search algorithm called breadth-first heuristic search; this improves the memory
requirements of A* search by searching the set of nodes up to a certain threshold value in
breadth-first instead of best-first manner.
CPT by Vincent Vidal, University of Artois, France, and Hector Geffner, University
of Pompeu Fabra, Barcelona, Spain. CPT optimizes makespan. The planner is based
on constraint satisfaction, and transforms the planning problem to a CSP. The branching
scheme that solves the CSP makes use of several constraint propagation techniques related
to POCL, and of the temporal max-atom heuristic.
HSP∗a by Patrik Haslum, Linköping University, Sweden. HSP*a is a derivate of TP4, see
below, with the same expressivity, but with a weakened version of the MaxTriple heuristic
instead of the MaxPair heuristic.
535

Hoffmann & Edelkamp

Optiplan by Menkes van der Briel and Subbarao Kambhampati, Arizona State University, USA. Optiplan is a planner based on integer programming (IP), which itself is a consequent extension to the encoding of the planning graph used in the planning as satisfiability
approach (see below). The compiled planning problem is solved using the CPLEX/ILOG
system. An interesting property of Optiplan is that, due to the power of the underlying IP
solver, it can optimize a great variety of different plan metrics, in fact every plan metric
that can be expressed with a linear function. In that respect, the planner is unique. In
IPC-4, step-optimal plans were computed because that is typically most efficient.
SATPLAN04, SATPLAN for short, by Henry Kautz, David Roznyai, Farhad TeydayeSaheli, Shane Neph, and Michael Lindmark, University of Washington, USA. SATPLAN
treats non-temporal non-numeric domains and optimizes the number of parallel time steps.
The system did not participate in IPC-3 but in IPC-1 and IPC-2, so it was a real comeback.
As the planner compiles a planning problem to a series of satisfiability tests, the performance
of the system relies on the integrated back-end SAT solver. The graph-plan encoding scheme
underwent only minor changes, but the underlying SAT solver is much more powerful than
that of 4 years ago.
SemSyn by Eric Parker, Semsyn Software, Tempe, Arizona. The SemSyn system optimizes the number of actions. Semsyn is linked to a commercial product and applies a
combination of forward and backward chaining. Both searches are variants of the original
algorithms. Forward chaining is goal-directed, while backward chaining is generalized, comprising a partition of the top-level goal. As the product is proprietary, detailed information
on the system is limited.
TP4-04, TP4 for short, by Patrik Haslum, Linköping University, Sweden. The planner
is an extended makespan-optimal scheduling system that does an IDA* search routine with
the max-pair heuristic. It can deal with a restricted form of metric planning problems with
numerical preconditions and effects.

5. Results
The CPU times for IPC-4 were measured on a machine located in Freiburg, running 2
Pentium-4 CPUs at 3 GHz, with 6 GB main memory. Competitors logged in remotely
and ran their planners, producing one separate ASCII result file for each solved instance,
giving the runtime taken, the quality of the found plan, as well as the plan itself. Each
planner run was allowed at most half an hour (CPU, not real) runtime, and 1 GB memory,
i.e., these were the cutoff values for solving a single instance. As a kind of performance
measure, the IPC-3 version of LPG (Gerevini, Saetti, & Serina, 2003) (called “LPG-3”),
i.e. the most successful automatic planner from IPC-3, was also run (by its developers, on
top of the workload with their new planner version LPG-TD). We remark that, for the sake
of simplicity, to save CPU time, and to ensure fairness, randomized planners were allowed
only a single run one ach instance, with a fixed random seed.
IPC-4 featured a lot of domain versions and instances, and a lot of planners. The only
way to fully understand the results of such a complex event is to examine the results in
detail, making sense of them in combination with the descriptions/PDDL encodings of the
domains, and the techniques used in the respective planners. We recommend doing so, at
least to some extent, to everybody who is interested in the results of (the deterministic part
536

The Deterministic Part of IPC-4: An Overview

of) IPC-4. The on-line appendix of this paper includes all the individual solution files. The
appendix also includes all gnuplot graphics we generated (more details below), as well as a
GANNT chart for each of the result files; the GANNT charts can be visualized using the
Vega visualization front-end (Hipke, 2000).
In what follows, we provide an overview of the results, and we highlight (what we think
are) the most important points; we explain how we decided about the IPC-4 competition
awards.
More precisely, Section 5.1 gives an overview over the results, in terms of percentages of
attacked and solved instances per language subset and planner. Section 5.2 describes how
we evaluated the results, particularly what procedure we chose to decide about the awards.
Thereafter, Sections 5.3 to 5.9 in turn consider in some more detail the results in the single
domains.
5.1 Results Overview
One crude way to obtain an overview over such a large data set is to simply count the
number of instances each planner attacked, i.e. tried to solve, and those that it succeeded to
solve. In our case, it also makes sense to distinguish between the different PDDL2.2 subsets
used by the IPC-4 benchmarks. The data is displayed in Table 2.
Table 2 is complicated, and needs some explanation. First, consider the columns in
the table. The leftmost column is, as usual, for table indexing. The rightmost column
contains data for the entire set of instances in IPC-4. The columns in between all refer
to a specific subset of the instances, in such a way that these subsets are disjunct and
exhaustive – i.e., the instance sets associated with columns are pairwise disjunct, and their
union is the entire set of instances. The subsets of instances are defined by the PDDL2.2
subsets they use. The abbreviations in Table 2 are explained in the caption. “X+Y” means
that these instances use language features “X” and “Y”, and none of the other features.
For example, “N+DP” are (only) those instances with uniform durations (i.e., PDDL2.2
level 1) and derived predicates. Note that Table 2 does not have a column for all possible
combinations of language features – we show only those that were used by (a non-empty
subset of) the IPC-4 benchmarks. Note also that we do not distinguish between different
domain formulations, i.e. between ADL and STRIPS. The instance numbers in Table 2 are
counted for domain versions. That is, if there is an ADL and a STRIPS formulation of a
version, then each instance is counted only once.4
The first line in Table 2, indexed “Number”, simply gives the size of the instance set
associated with the column. All other lines correspond, obviously, to planning systems. The
upper half of the table contains, in alphabetical order, the satisficing planners; the lower
half contains the optimal planners. Note that we list only one version of LPG-TD. This is
because the “speed” and “quality” version of this planner differ in terms of plan quality,
but not in terms of the number of attacked/solved instances.
4. As said, each domain featured several domain versions, differing in terms of the application constraints
modelled. Within each domain version, there could be several different domain version formulations,
differing in terms of the precise PDDL subset used to encode the same semantics. In most cases with more
than one formulation, there were exactly two formulations, using/not using ADL constructs, respectively.
Competitors could choose the formulation they liked best and the results across different formulations
were evaluated together.

537

Hoffmann & Edelkamp

Number
CRIKEY
FAP
FD
FDD
LPG-3
LPG-TD
Macro-FF
Marvin
Optop
P-MEP
Roadmapper
SGPlan
Tilsapa
YAHSP
BFHSP
CPT
HSP∗a
Optiplan
SATPLAN
SemSyn
TP4

N
382
42 77
33 64
83 62
92 62
55 62
67 87
57 87
67 62
15 61
28 49
68 100
77
29
33
33
34
46
49
29

87
87
87
38
87
87
48
64

N+DP
196
—
—
84 100
84 100
—
75 74
—
34 100
—
—
—
65 100
—
—
—
—
—
—
—
—
—

N+NV
152

D
D+TL D+NV D+TL+NV
302
116
272
136
47 66
—
98 55
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
—
42 24 45 62
—
56 50
—
61 37 76 62 63 100 96 50 87
100
—
—
—
—
—
—
—
—
—
—
—
8 43
—
—
8 55 24 45 24 43 13 32
—
—
—
—
—
64 100 75 90 78 74 85 100 74
100
10 69
62
63
—
—
—
—
—
—
—
—
—
—
—
22 100
—
—
—
10 62
—
50 50
—
—
—
—
—
—
—
—
—
—
—
11 37
—
—
—
—
17 62
—
52 50
—

56
33
83
88
51
76
57
52
8
17
28
73
38
77
29
28
29
34
46
40
31

All
1,556
61 42
74 16
75 28
75 28
54 38
71 71
100 21
75 28
7 3
43 38
56 12
96 96
13 11
100 21
100 21
100 41
44 30
100 21
100 21
23 15
54 37

Table 2: An overview of the IPC-4 results. Used abbreviations: “N” no durations, “DP”
derived predicates, “NV” numeric variables, “D” durations, “TL” timed initial
literals. Line “Number” gives number of instances, all other lines give percentage
values. In each entry, the right number is the percentage of instances that were
attacked, and the left number is the percentage of these that were solved – i.e.
the left number is the success ratio. In the rightmost column, the middle number
is the percentage of attacked instances relative to all instances that lie within the
language range of the respective planner; the right number is the percentage of
attacked instances over all. A dash indicates that a planner can not handle a
language subset. See the detailed explanation in the text.

Let us consider the table entries in between the leftmost and rightmost columns, i.e.,
the entries concerning a planner “X” and a language/instance subset “Y”. The numbers in
these entries are the success ratio and the attacked ratio. Precisely, they were obtained as
follows. We first counted the number “y” of instances in “Y” that planner “X” tried to solve
– our definition was to take all domain versions (inside “Y”) for which “X” delivered results,
and set “y” to be the total number of instances in these domain versions. We then counted
the number “x” of instances in “Y” that planner “X” succeeded to solve. We obtained the
first number in the table entry – the success ratio – as the ratio (in percent) of “x” divided
by “y”. We obtained the second number in the entry – the attacked ratio – as the ratio (in
percent) of “y” divided by the size of “Y”. For space reasons, we rounded the percentages
538

The Deterministic Part of IPC-4: An Overview

to the values shown in the table. A dash in the table entry means that the planner “X”
can not handle the language subset “Y”. An empty table entry means that the planner can
handle “Y”, but did not attack any instance in “Y”.
In a few cases, namely in the Promela domain, see the discussion in Section 5.5, we
could not formulate the largest instances in STRIPS because these (fully-grounded) representations became too large. So, there, the numbers of test instances are different between
the ADL and STRIPS formulations of the same domain version.5 Table 2 uses the number
of ADL instances, not taking account of this subtlety. This implies a slight disadvantage in
terms of success ratio in columns “N” and “All”, for the planners that attacked the smaller
STRIPS test suites. These planners are the following, with correct column “N” success
ratio in parentheses: CRIKEY (51), FAP (41), LPG-TD (74), SGPlan (80), YAHSP (92),
BFHSP (35), CPT (39), HSP∗a (52), Optiplan (41), SATPLAN (55), and TP4 (37).
The table entries in the rightmost column are obtained similarly as above. They summarize the situation regarding the respective planners across all used language subsets. The
left and right number in each entry give the ratio of the number of solved instances to the
number of attacked instances, and the number of attacked instances to the number of all
instances, respectively. The number in the middle of each entry gives the ratio of the number of attacked instances to the number of all instances that lie within the language range of
the respective planner. We included this number in order to provide a measure of to what
extent each planner/team attacked those instances that they could attack.
Some remarks are in order regarding the latter ratio, and the ratio of attacked instances
in general. First, there was no rule telling the competitors to “attack everything they can”
– competitors were free to choose. In retrospect, we feel that there should have been such a
rule, since it would make data interpretation easier. The way things are, it is not known for
what reason a planner did not attack an instance subset (domain version): Bad results? Not
interested? Overlooked? Second, many planners can handle only subsets of certain language
features, like, of numeric variables (“NV”). This can lead to too low percentages in Table 2,
where for simplicity we do not take account of these details. Third, one detail we did take
account of is that 50 of the 382 instances in column “N” (namely, the “middle-compiled”
version of the PSR domain, see Section 5.6) are formulated in ADL only, and are thus not
accessible to many of the planners. For planners not able to handle ADL, when computing
the middle number in the rightmost table entry, we subtracted these 50 instances from the
“instances within the language range” of the respective planner. In column “N”, however,
we took the usual ratio to the set of all instances – in particular, this is why Macro-FF,
YAHSP, BFHSP, CPT, Optiplan, and SATPLAN all have an 87% attacked-ratio in the “N”
column, but a 100% attacked-ratio as the middle number of the rightmost column.
All in all, the planners all have a pretty good coverage of the language subset they could
attack; except Optop, Tilsapa, and maybe Semsyn. These planners/teams were probably
interested only in a small subset of the instances.6 In the other cases where instances
5. In PSR, it also was impossible to formulate the largest instances in STRIPS. But, there, we split the
domain into different versions regarding the size and formulation language of the instances. We did not
want to introduce more distinction lines in Promela because there we already had 8 different domain
versions.
6. Drew McDermott, the developer of Optop, told us in private conversation that he tried to solve only the
most complicated domain versions.

539

Hoffmann & Edelkamp

were left out, planners/teams typically did not attack the domain versions where derived
predicates or timed initial literals had been compiled away. Note that a lack of coverage
may also be due to language details not accounted for by the rather crude distinctions made
for Table 2.7
What can we conclude from Table 2, in terms of planner performance? Some spotlights
are these:
• The success ratios of optimal planners are generally lower than those of satisficing
planners. The largest overall success ratio of an optimal planner, SATPLAN, is 46
(55 when taking account of the above mentioned subtlety regarding ADL/STRIPS
formulations in Promela); compared to 88 for FDD.
• However, there are also various cases where an optimal planner has a higher success
ratio than a satisficing one.
• FD, FDD, and YAHSP have the best success ratios (as mentioned, the success ratio of
YAHSP in the “N” column is 92 when taking account of ADL/STRIPS in Promela).
FD and FDD also have a pretty good coverage in the “N” and “N+DP” columns, i.e.
within PDDL2.2 level 1 (the left out instances are the domain versions with compiled
derived predicates).
• SGPlan has an extremely high coverage, attacking 96% of all instances. Even so, it has
a very competitive success ratio. (Concretely, SGPlan solved 1,090 of the 1,556 IPC-4
instances; it attacked 1,496. The second place in terms of the number of instances
solved is held by LPG-TD, which solved 843 out of the 1108 instances it could attack.)
• LPG-3, i.e. the IPC-3 version of LPG, ranges somewhere in the middle-ground of
the planners, in terms of success ratio and coverage. This indicates that significant
performance improvements were made in difference to IPC-3. (Note that this holds
for the new LPG version just as for the other top performance planners.)
Naturally, in our evaluation of the results, particularly in the evaluation that formed the
basis of the award decisions, we undertook a more detailed examination of the data set. We
considered, as much as possible, scaling behavior, rather than the simple instance counts
above. In what follows, we first explain our evaluation process in detail, then we consider
each of the domains in turn.
5.2 Results Evaluation
As said, runtime data for satisficing and optimal planners was considered separately. For
plan quality, we distinguished between three kinds of optimization criteria: number of
actions, makespan (equalling the number of time steps in the non-temporal domains), and
metric value. The last of these three was only defined if there was a :metric specification
in the respective task. We compared each planner according to only a single plan quality
criterion. That is, for every domain version the competitors told us what criterion their
7. In private conversation, the LPG-TD team told us that this is the case for LPG-TD, and that they in
fact attacked all domain versions they could at the time IPC-4 was running.

540

The Deterministic Part of IPC-4: An Overview

planner was trying to optimize in that domain version. We evaluated all (and only) those
planners together that tried to optimize the same criterion. Our reason for doing so is that
it does not make sense to evaluate planners based on criteria they don’t actually consider.
Altogether, for every domain version we created up to 5 gnuplot graphics: for satisficing
runtime, for optimal runtime, for number of actions, for makespan, and for metric value. The
plan quality data of optimal planners was put into the same plots as those for satisficing
planners, to enable some comparison. In a few cases, we had to split a runtime figure
(typically, for satisficing planners) into two separate figures because with too many runtime
curves in it the graphic became unreadable.
Based on the gnuplot graphics, we evaluated the data in terms of asymptotic runtime
and solution quality performance. The comparisons between the planners were made by
hand, i.e. by looking at the graphs. While this is simplistic, we believe that it is an adequate
way of evaluating the data, given the goals of the field, and the demands of the event. It
should be agreed that what we are interested in is the (typical) scaling behavior of planners
in the specific domains. This excludes “more formal” primitive comparative performance
measures such as counts of instances solved more efficiently – a planner may scale worse
than another planner, yet be faster in a lot of smaller instances just due to pre-processing
implementation details. The ideal formal measure of performance would be to approximate
the actual scaling functions underlying the planners’ data points. But it is infeasible to
generate enough data, in an event like the IPC, to do such formal approximations. So we
tried to judge the scaling behaviors by hand, laying most emphasis on how efficiently/if at
all the largest instances in a test suite were solved by a planner.8
The rest of the section contains one sub-section for each domain in turn. Each subsection provides the following material. First, we give a brief description of the domain and
its most important features (such as domain versions and formulations used in IPC-4). We
then include a set of gnuplot graphics containing (what we think are) the most important
observations regarding runtime (the total set of gnuplot graphs is too space-consuming). We
discuss plan quality, with data comparing the relative performance of pairs of planners. We
also add some intuitions regarding structural properties of the domains, and their possible
influence on the performance of the planners. Finally, we provide the information – 1st
and 2nd places, see below – underlying the decisions about the awards. This information is
contained in a text paragraph at the end of the sub-section for each domain.
As the basis for deciding about the awards, within every domain version, we identified
a group of planners that scaled best and roughly similar. These planners were counted as
having a 1st place in that domain version. Similarly, we also identified groups of 2nd place
planners. The awarding of prizes was then simply based on the number of 1st and 2nd
places of a planner, see Section 6.9
We consider the domains in alphabetical order. Before we start, there are a few more
remarks to be made. First, some planners are left out of the plots in order to make the
8. Note that the success ratios displayed in Table 2 are also a crude way to access scaling behavior.
9. Of course, many of the decisions about 1st and 2nd places were very close; there were numerous special
cases due to, e.g., what planners participated in what domain versions; summing up the places introduces
a dependency of the results on the number of domain versions in the individual domains. To hand out
awards one has to make decisions, and in these decisions a lot of detail is bound to disappear. One
cannot summarize the results of such a huge event with just a few prizes.

541

Hoffmann & Edelkamp

plots readable. Specifically, we left out LPG-TD.quality as well as LPG-3.quality (the
IPC-3 version of LPG with a preference on quality); these planners are always slower than
their counterparts with a preference on speed. We also left out FD, since in most cases it
showed very similar behavior to FDD. We chose FDD because in a few cases its performance
is superior. Note further that we do not distinguish the runtime performance of optimal
planners optimizing different criteria. Finally, it can make a significant difference whether a
planner is run on an ADL encoding, or on its compilation to STRIPS; this distinction also
gets lost in our evaluation. We emphasize that we applied the simplifications only in order
to improve readability and understandability of the results. We do not wish to imply that
the planners/distinctions that are omitted aren’t important.
5.3 Airport
In the Airport domain, the planner has to control the ground traffic on an airport. The task
is to find a plan that solves a specific traffic situation, specifying inbound and outbound
planes along with their current and goal positions on the airport. The planes must not
endanger each other, i.e. they must not both occupy the same airport “segment” (a smallest
road unit), and if plane x drives behind plane y then between x and y there must be a safety
distance (depending on the size of y). These safety constraints are modelled in terms of
“blocked” and “occupied” predicates, whose value is updated and controlled via complex
ADL preconditions and conditional effects.
There were four different versions of the domain in IPC-4: a non-temporal version,
a temporal version, a temporal version with time windows, and a temporal version with
compiled time windows. The time windows in the latter two versions concern airplanes that
are known to land in the future, and that will thus block certain airport segments (runways)
during certain time windows. The time windows are modelled using timed initial literals,
respectively their compilation. In every domain version, there is an ADL formulation, as
well as a STRIPS formulation obtained by compiling the ADL constructs away (resulting
in a partially grounded encoding).
Instances scale in terms of the size of the underlying airport, as well as the number of
airplanes that must be moved. One remarkable thing about the Airport domain is that
the instances were generated based on a professional airport simulation tool. The largest
instances used in IPC-4 (number 36 to 50) correspond to a real airport, namely Munich
airport (MUC). Instance number 50 encodes a traffic situation with 15 moving airplanes,
which is typical of the situations encountered at MUC in reality. Instances number 1 to 20
come from smaller toy airports, instances number 21 to 35 are based on one half of MUC
airport.
Figure 1 shows the runtime performance in the non-temporal version of the domain. For
readability, the set of satisficing planners is split over two graphs. As will be the case in
all graphs displayed in the subsequent discussions, the x-axis denotes the instance number
(obviously, the higher the number the larger the instance), and the y-axis gives CPU runtime
in seconds on a logarithmic scale. CPU time is total, including parsing and any form of
static pre-processing.
It can be observed in Figure 1 that FDD is the only planner solving all problem instances. LPG-TD and SGPlan both scale relatively well, but fail on the largest instances.
542

The Deterministic Part of IPC-4: An Overview

10000

1000

10000
LPG-TD
MACRO-FF
SGPLAN
CRIKEY
FDD
YAHSP

1000

100

100

10

10

1

1

0.1

0.1

0.01

FAP
MARVIN
PMEP
ROADMAPPER
LPG-3

0.01
5

10

15

20

25

30

35

40

45

50

5

10

15

20

(a)

25

30

35

40

45

50

(b)
10000
BFHSP
TP4
CPT
OPTIPLAN
SATPLAN
SEMSYN

1000

100

10

1

0.1

0.01
5

10

15

20

25

30

35

40

45

50

(c)
Figure 1: Performance in non-temporal Airport, satisficing (a) and (b), optimal (c).

The other planners all behave much more unreliably. Observe also that the IPC-3 version of
LPG lags far behind FDD, LPG-TD, and SGPlan. For the optimal planners, which unsurprisingly behave clearly worse than the satisficing planners, the only clear-cut observation
is a performance advantage for SATPLAN. The other optimal planners all behave relatively
similarly; Semsyn and BFHSP are the only ones out of that group solving a few of the larger
instances.
For plan quality, there are two groups of planners, one trying to minimize the number of
actions, and one trying to minimize makespan, i.e. the number of parallel action steps. In
the former group, the plan quality performance differences are moderate. CRIKEY, LPGTD, SGPlan, and YAHSP sometimes find sub-optimal plans. FDD’s plans are optimal in
all cases where an optimal planner found a solution. As a measure of comparative plan
quality, from now on we provide, given planners A and B, the min, mean, and maximum of
the ratio quality(A) divided by quality(B), for all instances that are solved by both A and
B. We call this data the “ratio A vs B”. For CRIKEY (A) vs FDD (B), the ratio is 0.91
(min), 1.04 (mean), and 1.45 (max), [0.91(1.04)1.45] for short. For LPG-TD.speed vs FDD
543

Hoffmann & Edelkamp

the ratio is [0.91(1.08)1.80]; for LPG-TD.quality vs FDD it is [0.91(1.06)1.80]. For SGPlan
vs FDD it is [0.91(1.07)2.08]; for YAHSP vs FDD it is [0.91(1.07)1.43].
In the group of planners minimizing makespan, (only) Marvin has a tendency to find
very long plans. The comparison Marvin vs the optimal SATPLAN is [1.00(2.46)4.64]. In
the maximum case, SATPLAN finds a plan with 53 steps, and Marvin’s plan is 246 steps
long.
An interesting observation concerns the two scaling parameters of that domain: the
number of airplanes, and the size of the airport. In all the plots in Figure 1, and also in
most plots in Figure 2 below, one can observe that the instances number 26 to 35 become
increasingly hard for the planners – but in the step to instance number 36, the performance
suddenly becomes better again. As said above, the instances 21 to 35 are based on one half of
MUC airport, while the instances 36 to 50 are based on the full MUC airport. But between
instances 26 and 35, the number of airplanes rises from 6 to 12; instance 36 contains only
2 airplanes, instances 37 and 38 contain 3. That is, it is easier for the planners to address
a larger airport with fewer planes. Note here the domain combinatorics: the number of
reachable states is in the order of S n , where S is the number of different airport segments,
and n is the number of airplanes.
Figure 2 shows the runtime performance in the temporal versions of the domain. We
first consider the domain versions without time windows: parts (a) and (b) of the figure.
Of the satisficing planners, LPG-TD and SGPlan both scale just as well as they do in the
non-temporal case; all other satisficing planners scale orders of magnitude worse. Of the
optimal planners, CPT scales best, followed by TP4. The two planners minimizing the
number of actions are SGPlan and CRIKEY; SGPlan behaves somewhat worse, the ratio
is [0.95(1.03)1.54]. Of the planners that minimize makespan, only LPG-TD scales beyond
the smallest instances. The ratio LPG-TD.speed vs the optimal CPT is [1.00(1.15)1.70],
P-MEP vs CPT is [1.00(1.11)1.42]. The ratio LPG-TD.quality vs CPT is [1.00(1.03)1.44].
No optimal planner in the competition could handle timed initial literals, so with explicit time windows no such planner participated. With compiled time windows, only CPT
participated, scaling a little worse than in the temporal domain version without time windows. For the satisficing planners, the results are more interesting. With explicit time
windows, again LPG-TD and SGPlan maintain their good scaling behavior from the simpler domain versions. To the other planners, there is a huge runtime performance gap.
SGPlan is the only planner here that minimizes the number of actions. The makespan ratio
LPG-TD.speed vs LPG-TD.quality is [1.00(1.15)2.01]. With compiled time windows, only
SGPlan and CRIKEY participated. The former consistently solves the smaller instances
up to a certain size, the latter fails on many of the small instances but can solve a few of
the larger ones. Neither of the planners shows reasonable runtime performance compared
to the explicit encoding of the domain. The number of actions ratio CRIKEY vs SGPlan
is [1.00(1.14)1.36].
Our main motivation for including the Airport domain in IPC-4 was that we were able to
model it quite realistically, and to generate quite realistic test instances – the only thing we
could not model was the real optimization criterion (which asks to minimize the summed
up travel time of all airplanes). So the good scaling results of, at least, the satisficing
planners, are encouraging. Note, however, that in reality the optimization criterion is of
crucial importance, and really the only reason for using a computer to control the traffic.
544

The Deterministic Part of IPC-4: An Overview

10000

10000
TP4
CPT
HSPS_A

1000

1000

100

100

10

10

1

1

0.1

0.1

LPG-TD
SGPLAN
CRIKEY
PMEP
LPG-3

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

10

15

20

(a)

25

30

35

40

45

50

(b)

10000

10000
SGPLAN
CRIKEY

1000

1000

100

100

10

10

1

1

0.1

0.1

LPG-TD
SGPLAN
OPTOP
PMEP
TILSAPA

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(c)

10

15

20

25

30

35

40

45

50

(d)

Figure 2: Performance in temporal Airport, satisficing (a) and optimal (b). In temporal
Airport with time windows, satisficing, explicit encoding (c) and compiled encoding (d).

We remark that the domain is not overly difficult from the perspective of relaxed-plan
based heuristic planners (using the wide-spread “ignore deletes” relaxation). Hoffmann
(2005) shows that Airport instances can contain unrecognized dead ends (states from which
the goal can’t be reached, but from which there is a relaxed plan); but such states are
not likely to occur very often. A dead end in Airport can only occur when two planes
block each other, trying to get across the same segment in different directions. Due to the
topology of the airports (with one-way roads), this can happen only in densely populated
parking regions. In case for any two airplanes the respective paths to the goal position are
disjoint, the length of a relaxed plan provides the exact distance to the nearest goal state.
In that sense, the good runtime performance of the satisficing planners didn’t come entirely
unexpected to us, though we did not expect them to behave that well. Note that most of
these planners, particularly FDD, SGPlan, and LPG-TD, do much more than/do things
545

Hoffmann & Edelkamp

different from a standard heuristic search with goal distances estimated by relaxed plan
length. Note also that Hoffmann (2005)’s results are specific to non-temporal domains.
In the non-temporal domain version, we awarded 1st places to FDD (and FD) and
SATPLAN; we awarded 2nd places to LPG-TD, SGPlan, Semsyn, and BFHSP. In the
temporal version, we awarded 1st places to LPG-TD, SGPlan, and CPT; a 2nd place was
awarded to TP4. In the domain version with explicit time windows, 1st places were awarded
to LPG-TD and SGPlan.
5.4 Pipesworld
The Pipesworld domain is a PDDL adaptation of an application domain dealing with complex problems that arise when transporting oil derivative products through a pipeline system. Note that, while there are many planning benchmarks dealing with variants of transportation problems, transporting oil derivatives through a pipeline system has a very different and characteristic kind of structure. The pipelines must be filled with liquid at all times,
and if you push something into the pipe at one end, something possibly completely different
comes out of it at the other end. As a result, the domain exhibits some interesting planning
space characteristics and good plans sometimes require very tricky maneuvers. Additional
difficulties that have to be dealt with are, for example, interface restrictions (different types
of products interfere with each other in a pipe), tankage restrictions in areas (i.e., limited
storage capacity defined for each product in the places that the pipe segments connect),
and deadlines on the arrival time of products.
In the form of the domain used in IPC-4, the product amounts dealt with are discrete in the sense that we assume a smallest product unit, called “batch”. There were six
different domain versions in IPC-4: notankage-nontemporal, notankage-temporal, tankagenontemporal, tankage-temporal, notankage-temporal-deadlines, and notankage-temporaldeadlines-compiled. All versions include interface restrictions. The versions with “tankage”
in their name include tankage restrictions, modelled by a number of “tank slots” in each
place in the network, where each slot can hold one batch (note that the slots introduce
some additional symmetry into the problem; we get back to this below). In the versions
with “temporal” in their name, actions take (different amounts of) time. The versions with
“deadlines” in their name include deadlines on the arrival of the goal batches, modelled
by timed initial literals respectively their compilation to standard PDDL2.1. None of the
encodings uses any ADL constructs, so of each domain version there is just one (STRIPS)
formulation.
The IPC-4 example instances were generated based on five scaling network topologies. The smallest network topology has 3 areas connected by 2 pipes; the largest network
topology has 5 places connected by 5 pipes. For each network we generated 10 instances,
with growing numbers of batches, and of batches with a goal location. So altogether we
had 50 instances per domain version, numbered consecutively. Between domain versions,
the instances were, as much as possible, transferred by adding/removing constructs. E.g.,
the instances in tankage-nontemporal (tankage-temporal) are exactly the same as those in
notankage-nontemporal (notankage-temporal) except that tankage restrictions were added.
We do not include graphs for the domain versions featuring deadlines. In the version with explicit deadlines (modelled by timed initial literals), only LPG-TD and Tilsapa
546

The Deterministic Part of IPC-4: An Overview

10000

10000
LPG-TD
MACRO-FF
SGPLAN
CRIKEY
FDD

1000

FAP
MARVIN
ROADMAPPER
PMEP
YAHSP
LPG-3

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

10

15

20

(a)

25

30

35

40

45

50

(b)
10000
BFHSP
TP4
CPT
OPTIPLAN
SATPLAN
SEMSYN

1000

100

10

1

0.1

0.01
5

10

15

20

25

30

35

40

45

50

(c)
Figure 3: Non-temporal Pipesworld, no tankage constraints, satisficing (a) and (b), optimal
(c).

participated, of which LPG-TD scaled up to middle-size instances; Tilsapa solved only a
few of the smallest instances. In the domain version with compiled deadlines, only CPT
participated, solving a few small instances.
Figure 3 shows the results for the non-temporal domain version without tankage restrictions. Parts (a) and (b) contain the results for satisficing planners in the respective
non-temporal domain version. We observe that YAHSP and SGPlan are the only planners
that can solve all instances. YAHSP is a lot faster in most instances; but it finds excessively
long plans: the ratio YAHSP vs SGPlan is [0.38(1.77)14.04], where in the maximum case
SGPlan needs 72 actions, YAHSP 1,011. For the other planners, plan quality does not
vary much, with the exception of Marvin, whose makespan ratio vs the optimal CPT is
[0.88(1.60)2.19]. LPG-TD and FDD both solve all but a few of the largest instances. Note
that, as in Airport, the IPC-3 version of LPG is outperformed dramatically by the best
IPC-4 planners.
547

Hoffmann & Edelkamp

10000

10000
LPG-TD
SGPLAN
CRIKEY
PMEP
LPG-3

1000

TP4
CPT
HSPS_A
1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(a)

10

15

20

25

30

35

40

45

50

(b)

Figure 4: Temporal Pipesworld without tankage constraints, satisficing (a), optimal (b).

Part (c) of Figure 3 shows the results for the optimal planners. The runtime curves are
extremely similar, nearing indistinguishable. Note that Optiplan is the only optimal planner
that solves an instance with a very large size parameter. However, in Pipesworld, like in
Airport, due to the domain combinatorics, planners are likely to find it easier to solve a large
network with little traffic, than a small network with a lot of traffic.10 The other optimal
planners here may just not have tried to run their planner on larger instances when it already
failed on the smaller ones – we explicitly advised people to save machine workload by not
insisting on spending half an hour runtime on instances that were probably infeasible for
their planners anyway. In cases like this one here, this admittedly is a potentially misleading
guideline.
In parts (a) and (b) of Figure 4, we display the results in the temporal domain version
without tankage restrictions. Part (a) shows a clear win of SGPlan over the other planners.
We find it particularly remarkable that, as before in Airport, SGPlan is just as good in the
temporal domain version as in the nontemporal one. Again, LPG-3 is outperformed by far.
As for the optimal planners in part (b) of the figure, CPT scales best; TP4 and HSP∗a scale
similarly. The only planners minimizing the number of actions here are CRIKEY and SGPlan; the ratio is [0.35(1.24)5.67], showing quite some variance with a slight mean advantage
for SGPlan. Of the planners minimizing makespan, LPG-TD and P-MEP sometimes return
long plans; in one instance (number 2), LPG-TD’s plan is extremely long (makespan 432).
More precisely, the ratio LPG-TD.speed vs the optimal CPT is [1.00(4.55)36.00]; the ratio
P-MEP vs CPT is [1.19(2.25)4.59]; the ratio LPG-TD.quality vs CPT is [0.73(1.05)1.62]
(i.e., in particular the peak at instance 2 disappears for LPG-TD.quality). Note that,
strangely at first sight, sometimes LPG-TD finds better plans here than the optimal CPT.
10. On the other hand, note at this point that the networks in Pipesworld do by far not grow as much as in
Airport, where they grow from microscopic toy airports for the smallest instances to a real-world airport
for the largest instances. As said, the smallest network in Pipesworld has 3 areas connected by 2 pipes,
and the largest network has 5 places connected by 5 pipes.

548

The Deterministic Part of IPC-4: An Overview

10000

10000
LPG-TD
MACRO-FF
SGPLAN
CRIKEY
FDD

1000

FAP
MARVIN
ROADMAPPER
YAHSP
LPG-3

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

10

15

20

25

(a)

30

35

40

45

50

(b)
10000
BFHSP
CPT
OPTIPLAN
SATPLAN
1000

100

10

1

0.1

0.01
5

10

15

20

25

30

35

40

45

50

(c)
10000

10000
LPG-TD
SGPLAN
CRIKEY
LPG-3

TP4
CPT
HSPS_A

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(d)

10

15

20

25

30

35

40

45

50

(e)

Figure 5: Pipesworld with tankage constraints: non-temporal satisficing (a) and (b), optimal (c), temporal satisficing (d), optimal (e).

This is due to the somewhat simpler model of durative actions that CPT uses (Vidal &
Geffner, 2004), making no distinction between the start and end time points of actions.
549

Hoffmann & Edelkamp

In Figure 5, results are displayed for the two domain versions that do feature tankage
restrictions. Very generally, we see that the planners have much more trouble with these
domain versions than with their less constrained counterparts.
In the non-temporal domain version with tankage constraints, for the satisficing planners
in parts (a) and (b) we observe that, as before, YAHSP is most efficient, solving more
instances than any of the other planners. Again, this efficiency is bought at the cost of
overlong plans: the ratio YAHSP vs. SGPlan is [0.66(1.89)6.14], where in the maximum
case SGPlan needs 64 actions, YAHSP 393. For the other planners, plan quality does not
vary as much (e.g. the ratio FDD vs SGPlan is [0.56(0.96)1.38]. Of the optimal planners
displayed in part (c), same non-temporal domain version, SATPLAN and Optiplan scale
a little better than the other planners, solving two and three more instances, respectively.
SATPLAN tends to be faster than Optiplan in the solved cases. Part (d) displays the
results for the satisficing planners in the respective temporal domain version. SGPlan
clearly scales best, once again keeping its performance from the corresponding non-temporal
domain version. LPG-TD is followed relatively closely by CRIKEY but can solve some
more instances. Regarding plan quality, CRIKEY and SGPlan minimize plan length, and
the ratio CRIKEY vs SGPlan is [0.62(1.37)2.54]. For makespan, LPG-TD again has a peak
in instance 2; the ratio LPG-TD.speed vs CPT is [1.00(2.55)7.27], maximum 160 vs 22
units; the ratio LPG-TD.quality vs CPT is [0.86(0.95)1.18] (LPG-TD.quality’s makespan
in instance 2 is 26). Note that, as above, LPG-TD.quality can find better plans than CPT
due to the somewhat simpler model of durative actions used in CPT. In the optimal track,
TP4 performs a little better, in terms of runtime, than the similar-performing CPT and
HSP∗a ; all the planners solve only a few of the smallest instances.
We remark that we were rather surprised by the relatively good scaling behavior of
some planners, particularly of the fastest satisficing planners in the domain versions without
tankage restrictions. Hoffmann (2005) shows that, in Pipesworld, there can be arbitrarily
deep local minima under relaxed plan distances (i.e., the distance from a local minimum
to a better region of the state space may be arbitrarily large). No particularly unusual
constructs are needed to provoke local minima – for example, a cycle of pipe segments,
as also occurs in the network topologies underlying the IPC-4 instances, suffices. Not less
importantly, in the limited experiments we did during testing, FF (Hoffmann & Nebel,
2001) and Mips (Edelkamp, 2003c) generally scaled much worse than, e.g., YAHSP and
SGPlan do as shown above.
As for the domain versions that do feature tankage restrictions, these are harder than
their counterparts. There are two important differences between the domain versions
with/without tankage restrictions. First, the problem is more constrained (remember that,
modulo the tankage constraints, the IPC-4 instances are identical in both respective versions). Second, the tank slots, that model the restrictions, introduce additional symmetry
into the problem: a planner can now choose into which (free) slot to insert a batch. This
choice does not make a difference, but enlarges the state space exponentially in the number
of such choices (to the basis of the number of tank slots). We considered the option to use a
“counter” encoding instead, to avoid the symmetry: one could count the product amounts
in the areas, and impose the tankage restrictions based on the counter values. We decided
against this because symmetry is a challenging aspect of a benchmark.
550

The Deterministic Part of IPC-4: An Overview

In domain version notankage-nontemporal, we awarded 1st places to YAHSP and SGPlan; we awarded 2nd places to LPG-TD and FDD. In version notankage-temporal, we
awarded 1st places to SGPlan and CPT; we awarded 2nd places to LPG-TD, TP4, and
HSP∗a . In version tankage-nontemporal, we awarded 1st places to YAHSP and FDD; we
awarded 2nd places to SGPlan, SATPLAN, and Optiplan. In version tankage-temporal, we
awarded a 1st place to SGPlan; we awarded 2nd places to LPG-TD and TP4. In version
notankage-temporal-deadlines, we awarded a 1st place to LPG-TD.
5.5 Promela
Promela is the input language of the model checker SPIN (Holzmann, 2004), used for
specifying communication protocols. Communication protocols are distributed software
systems, and many implementation bugs can arise, like deadlocks, failed assertions, and
global invariance violations. The model checking problem (Clarke, Grumberg, & Peled,
2000) is to find those errors by returning a counter-example, or to verify correctness by
a complete exploration of the underlying state-space. Edelkamp (2003b) developed an
automatic translation of this problem, i.e., of the Promela language, into (non-temporal)
PDDL, making it possible to apply state-of-the-art planners without any modification.
For IPC-4, two relatively simple communication protocols were selected as benchmarks
– toy problems from the Model-Checking area. One is the well-known Dining-Philosophers
protocol, the other is a larger protocol called Optical-Telegraph. The main point of using
the domain, and these protocols, in IPC-4 was to promote the connection between Planning
and Model-Checking, and to test how state-of-the-art planners scale to the basic problems
in the other area.
The IPC-4 instances exclusively require to find deadlocks in the specified protocols –
states in which no more transitions are possible. The rules that detect whether or not a
given state is a deadlock are most naturally modelled by derived predicates, with complex
ADL formulas in the rule bodies. To enable broad participation, we provided compilations
of derived predicates into additional actions, and of ADL into STRIPS. Edelkamp’s original
translation of Promela into PDDL also makes use of numeric variables. For the non-numeric
planners, the translation was adapted to use propositional variables only – for the relatively
simple Dining-Philosophers and Optical-Telegraph protocols, this was possible.
Precisely, the domain versions and formulations were the following. First, the versions were split over the modelled protocol, Dining-Philosophers or Optical-Telegraph. Second, the versions were split over the used language: with/without numeric variables, and
with/without derived predicates.11 So, all in all, we had 8 domain versions. Each of them
used ADL constructs. In those 4 versions without numeric variables, we also provided
STRIPS formulations, obtained from the ADL formulations with automatic compilation
software based on FF’s pre-processor (Hoffmann & Nebel, 2001), producing fully grounded
encodings. All Promela domain versions are non-temporal.
Here, we show plots only for the results obtained in the non-numeric domain versions.
In the domain versions using both numeric variables and derived predicates, no planner
11. We split with/without numeric variables into different domain versions, rather than formulations, to
encourage the use of numeric planning techniques. We split with/without derived predicates into different
versions since, through the modelling of derived predicates as new actions, the plans become longer.

551

Hoffmann & Edelkamp

10000

10000
LPG-TD
MACRO-FF
SGPLAN
CRIKEY
FAP
PMEP
YAHSP

1000

BFHSP
TP4
CPT
HSPS_A
OPTIPLAN
SATPLAN
SEMSYN

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

5

10

15

20

(a)

25

30

35

40

45

(b)
10000
LPG-TD
SGPLAN
FDD
MARVIN
1000

100

10

1

0.1

0.01
5

10

15

20

25

30

35

40

45

(c)
Figure 6: Performance in Promela/Dining Philosophers. Encoding without derived predicates, satisficing (a), optimal (b). Encoding with derived predicates, satisficing
(c).

participated. In the numeric version of Dining-Philosophers (without derived predicates),
only SGPlan and P-MEP participated. SGPlan solved all instances while P-MEP solved
only a few of the smallest ones. In the numeric version of Optical-Telegraph (without
derived predicates), only SGPlan participated, solving some relatively small instances.
Parts (a) and (b) of Figure 6 show the results for Dining-Philosophers, without derived
predicates, i.e. with additional actions to derive the deadlocks. In the the satisficing
track, YAHSP and SGPlan clearly show the best performance. The other planners all lag
several orders of magnitude behind. In the optimal track, SATPLAN clearly scales best,
followed by Optiplan. Observe that, in difference to what we have seen in Airport and
Pipesworld, the optimal planners are just as efficient as the satisficing ones. This is evident
from the plots. Most particularly, SATPLAN and Optiplan solve just as many instances,
up to 30 philosophers (instance number x features x + 1 philosophers), like YAHSP and
552

The Deterministic Part of IPC-4: An Overview

SGPlan.12 SATPLAN even does so in comparable time. Such a competitivity of optimal
planners with satisficing ones has not been seen in any test suite used in any of the last
three competitions.13 The efficiency of SATPLAN and Optiplan in Dining-Philosophers is
probably due the fact that the needed number of parallel time steps is constantly 11 across
all instances (see below). In a (standard) planning graph (Blum & Furst, 1995, 1997), the
goals are first reached after 7 steps; so only 4 unsuccessful iterations are made before a plan
is found.
Regarding plan quality, there is one group of planners trying to minimize the number
of actions, and one group trying to minimize the makespan (the number of parallel action
steps). The results clearly point to an important difference between this domain and the
other IPC-4 domains. Namely, there is only a single scaling parameter n – the number of
philosophers – and only a single instance per value of n. The optimal number of actions
is a linear function of n, precisely 11n: basically, one blocks all philosophers in sequence,
taking 11 steps for each. The optimal makespan is, as mentioned above, constantly 11:
one can block all philosophers in parallel. The only sub-optimal plan found by any planner
minimizing the number of actions is that of CRIKEY for n = 8, containing 104 instead of
88 actions. Of the planners minimizing makespan, only P-MEP finds sub-optimal plans: it
solves only the smallest four instances, n = 2, 3, 4, 5, with a makespan of 19, 25, 30, and 36,
respectively.
Figure 6 (c) shows the results in the domain version that uses derived predicates to
detect the deadlock situations. No optimal planner could handle derived predicates which
is why there are no results for this. Of the satisficing planners, SGPlan clearly shows the
best performance. FDD and LPG-TD are roughly similar – note that, while LPG-TD is
faster than FDD in most examples, the behavior of FDD in the largest instances indicates
an advantage in scaling. There is a sudden large increase in LPG-TD’s runtime, from
the second largest instance (that is solved in 52 seconds) to the largest instance (that is
solved in 1045 seconds). In difference to that, FDD solves the largest instance in almost
the same time as the second largest one, taking 111 seconds instead of 110 seconds (in fact,
FDD’s runtime performance shows little variance and is pretty much a linear function in
the instance size). We remark that the plans for the largest of these instances are huge:
more than 400 steps long, see below. SGPlan generates these plans in little more than a
single second CPU time.
Regarding plan quality, in this domain version the optimal number of actions is 9n, and
the optimal makespan is constantly 9. All planners except Marvin tried to minimize the
number of actions. FD and SGPlan always find optimal plans. FDD, LPG-TD.speed, and
LPG-TD.quality sometimes find slightly sub-optimal plans. Precisely, the ratio FDD vs
FD is [1.00(1.08)1.44]; LPG-TD.speed vs FD is [1.00(1.10)2.19]; LPG-TD.quality vs FD is
[1.00(1.03)1.24]. As for Marvin, the makespan of its plans is, roughly, linear in n; it always
lies between 7n and 9n.
12. Importantly, these planners handle only STRIPS, and for compilation reasons there were STRIPS instances up to n = 30 only. So these planners solve all their respective test instances. We discuss this in
a little more detail further below.
13. In the first IPC, in 1998, apart from the fact that the “heuristic search” satisficing planners were still
in their infancy, Mystery and Mprime were used as benchmarks. These can’t be solved particularly
efficiently by any planner up to today, except, maybe, FD and FDD (Helmert, 2004).

553

Hoffmann & Edelkamp

At this point, there is an important remark to be made about a detail regarding our
compilation techniques in these domain versions. Observe the huge gap in planner performance between Figure 6 (a) and (b), and Figure 6 (c). As we have seen, the difference in
plan length (makespan) is not very large – 11n (11) compared to 9n (9). Indeed, the apparent performance difference is due to a compilation detail, inherent in the IPC-4 instances,
rather than to planner performance. In the version without derived predicates, we were
able to compile only the instances with up to n = 30 philosophers from ADL into STRIPS.
For larger values of n, the (fully-grounded) STRIPS representations became prohibitively
large. In the version with derived predicates, the blow-up was a lot smaller, and we could
compile all instances, with up to n = 49 philosophers. SGPlan, YAHSP, SATPLAN, and
Optiplan can all handle only STRIPS. So, as mentioned above, in Figure 6 (a) and (b) these
planners actually solve all instances in their respective test suite; they would probably scale
up further when provided with STRIPS representations of instances with larger n values.

10000

10000
LPG-TD
MACRO-FF
SGPLAN
CRIKEY
FAP
PMEP
YAHSP

1000

BFHSP
TP4
CPT
HSPS_A
OPTIPLAN
SATPLAN

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

5

10

15

20

(a)

25

30

35

40

45

(b)
10000
LPG-TD
SGPlan
FDD
MARVIN
1000

100

10

1

0.1

0.01
5

10

15

20

25

30

35

40

45

(c)
Figure 7: Performance in Promela/Optical Telegraph. Encoding without derived predicates, satisficing (a), optimal (b). Encoding with derived predicates, satisficing
(c).

554

The Deterministic Part of IPC-4: An Overview

The Optical-Telegraph protocol is slightly more complex than the Dining-Philosopher
example, i.e. it involves more complicated and indirect interactions between the communicating processes, leading to longer solutions. There is still just one scaling parameter, the
number n of “telegraph station pairs”, and a single instance per value of n. Each telegraph
station pair is a pair of processes that goes through a rather complicated internal communication structure, enabling the exchange of data. The telegraph station pair shares with
the outside world – i.e., with the other telegraph station pairs – two “control channels” that
must be occupied as a prerequisite to the internal exchange of data. Thus, the role of the
control channels is pretty similar to the role of the (shared) forks in the Dining-Philosopher
example. Instance number x in IPC-4 features x + 1 telegraph station pairs.
Figure 7 shows in parts (a) and (b) the performance of the satisficing and optimal
planners, respectively, in the domain version using additional actions to derive deadlocks.
In the group of satisficing planners, Macro-FF clearly performs best. Of the other satisficing
planners, SGPlan is the most efficient. It is important to note that, here, we were able to
compile only the instances up to n = 15 from ADL into STRIPS. So SGPlan and LPG-TD
solve all their respective test instances. Of the optimal planners, SATPLAN and Optiplan
solve the instances up to n = 14, i.e., they failed to solve the largest instance in the STRIPS
test suite they attacked. SATPLAN is much faster than Optiplan. Each of the other optimal
planners could solve only the smallest instance, n = 2. Figure 7 (c) shows that FDD, which
handles the ADL formulation, is by far the most successful satisficing planner in the domain
version encoding Optical-Telegraph with derived predicates for deadlock detection; of the
other planners, SGPlan scales best, solving all instances in the STRIPS set, up to n = 20.
Regarding plan quality, in Optical-Telegraph without derived predicates the optimal
number of actions is 18n, and the optimal makespan is constantly 13: optimal sequential
plans block all telegraph station pairs in sequence, taking 18 steps for each; in parallel
plans, some simultaneous actions are possible within each telegraph station pair. In OpticalTelegraph with derived predicates, the optimal number of actions is 14n, and the optimal
makespan is constantly 11. In the competition results, all planners returned the optimal
plans in these test suites, in all cases – with a single exception. The plans found by Marvin
in the version with derived predicates have makespan 14n in all solved cases.
As we have seen, the results in Promela are, over all, quite different from, e.g., those we
have seen in Airport and Pipesworld. There is only a single scaling parameter and a single
instance per size, and optimal makespan is constant. This leads to rather smooth runtime
and plan quality curves, as well as to an unusual competitivity of optimal planners with
satisficing planners. The scalability of the planners both shows that current planners are
able to efficiently solve the most basic Model-Checking benchmarks (Dining-Philosophers),
and that they are not very efficient in solving more complex Model-Checking benchmarks
(Optical-Telegraph). We remark that Hoffmann (2005) shows that there exist arbitrarily
deep local minima under relaxed plan distances in Optical-Telegraph, but not in DiningPhilosophers, where there exists a large upper bound (31) on the number of actions needed
to escape a local minimum. It is not clear, however, how much these theoretical results have
to do with the planner performance observed above. The worst-cases observed by Hoffmann
all occur in regions of the state space not entered by plans with optimal number of actions
– as found by the IPC-4 participants, in most cases.
555

Hoffmann & Edelkamp

In Dining-Philosophers without derived predicates, we awarded 1st places to YAHSP,
SGPlan, and SATPLAN; we awarded a 2nd place to Optiplan. In Dining-Philosophers with
derived predicates, we awarded a 1st place to SGPlan, and 2nd places to FDD and LPG-TD.
In Optical-Telegraph without derived predicates, we awarded 1st places to Macro-FF and
SATPLAN; we awarded 2nd places to SGPlan and Optiplan. In Optical-Telegraph with
derived predicates, we awarded a 1st place to FDD, and a 2nd place to SGPlan. In the
numeric version of Dining-Philosophers, we awarded a 1st place to SGPlan.
5.6 PSR
PSR is short for Power Supply Restoration. The domain is a PDDL adaptation of an application domain investigated by Sylvie Thiébaux and other researchers (Thiébaux, Cordier,
Jehl, & Krivine, 1996; Thiébaux & Cordier, 2001), which deals with reconfiguring a faulty
power distribution system to resupply customers affected by the faults. In the original PSR
problem, various numerical parameters such as breakdown costs and power margins need
to be optimized, subject to power capacity constraints. Furthermore, the location of the
faults and the current network configuration are only partially observable, which leads to a
tradeoff between acting to resupply lines and acting to reduce uncertainty. In contrast, the
version used for IPC-4 is set up as a pure goal-achievement problem, numerical aspects are
ignored, and total observability is assumed. Temporality is not a significant aspect even in
the original application, and the IPC-4 domain is non-temporal.
We used four domain versions of PSR in IPC-4. Primarily, these versions differ by
the size of the problem instances encoded. The instance size determined in what languages we were able to formulate the domain version. The domain versions are named
1. large, 2. middle, 3. middle-compiled, and 4. small. Version 1 has the single formulation adl-derivedpredicates: in the most natural formulation, the domain comes with derived
predicates to model the flow of electricity through the network, and with ADL formulas to
express the necessary conditions on the status of the network connectors. Version 2 has the
formulations adl-derivedpredicates, simpleadl-derivedpredicates, and strips-derivedpredicates.
Version 3 has the single formulation adl, and version 4 has the single formulation strips.
As indicated, the formulation names simply give the language used. In version 2, ADL
constructs were compiled away to obtain the simpler formulations. In version 3, derived
predicates were compiled away by introducing additional artificial actions; due to the increase in plan length (which we discuss in some detail further below), we turned this into
a separate domain version, rather than a formulation. As for the strips domain version, to
enable encoding of reasonably-sized instances for this we adopted a different fully-grounded
encoding inspired by the work of Bertoli et al. (2002). The encoding is generated from a description of the problem instance by a tool performing some of the reasoning devoted to the
planner under the other domain versions. Still we were only able to formulate comparatively
small instances in pure STRIPS.
Starting with the performance in the smallest instances, we depict the performance of
satisficing and optimal planners in the PSR STRIPS domain in Figure 8. Both result graphs
are divided into two because they are completely unreadable otherwise. Most planners show
a lot of variance in this domain, blurring the performance differences (observable) between
the individual systems. Still it is possible to identify systems that behave better than others.
556

The Deterministic Part of IPC-4: An Overview

10000

10000
LPG-TD
SGPLAN
CRIKEY

FDD
MARVIN
YAHSP
LPG-3

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

10

15

20

(a)

25

30

35

40

45

50

(b)

10000

10000
BFHSP
TP4
CPT
HSPS_A

OPTIPLAN
SATPLAN
SEMSYN

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(c)

10

15

20

25

30

35

40

45

50

(d)

Figure 8: PSR small, satisficing (a) and (b), optimal (c) and (d).

Of the satisficing systems, only FDD can solve all the 50 instances; it does so consistently
fast (the results for FD are almost identical). Of the other planners, LPG-TD, SGPlan,
and YAHSP have the best success ratio: they solve 49, 47, and 48 instances, respectively;
CRIKEY solves 29 instances, Marvin 41. As for the optimal systems, here the only system
solving the entire test suite is SATPLAN. BFHSP solved 48 instances, CPT 44, HSP∗a 44,
Optiplan 29, Semsyn 40, TP4 38.
For plan quality, once again there are groups of planners trying to minimize plan
length (number of actions), and makespan. In the former group, we take as a performance measure the plans found by BFHSP, which are optimal in that sense, and which,
as said above, we have for all but 2 of the 50 instances. The ratio CRIKEY vs BFHSP is
[1.00(1.72)3.44]; the ratio FDD vs BFHSP is [1.00(1.02)1.52]; the ratio LPG-TD.speed vs
BFHSP is [1.00(5.52)12.70]; the ratio LPG-TD.quality vs BFHSP is [1.00(1.82)8.32]; the
ratio SGPlan vs BFHSP is [1.00(1.01)1.24]; the ratio YAHSP vs BFHSP is [1.00(1.00)1.05].
In the planner group minimizing makespan, all planners except Marvin are optimal. The
ratio Marvin vs SATPLAN is [1.00(1.28)2.07].
557

Hoffmann & Edelkamp

10000

10000
LPG-TD
SGPLAN
FDD
MARVIN

SGPLAN
FDD
MARVIN

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(a)

10

15

20

25

30

35

40

45

50

(b)

Figure 9: PSR, middle (a) and large (b), satisficing planners.

Apart from the observations to be made within the groups of satisficing respectively
optimal planners, there is something to be said here on the relationship between these
two groups. Like in Dining-Philosophers, we have the rather unusual situation that the
optimal planners are just as efficient as the satisficing ones. Indeed, by solving all instances,
SATPLAN is superior to most of the satisficing planners. We remark at this point that
Hoffmann (2005) shows the existence of arbitrarily deep local minima in PSR, i.e., regions
where it takes arbitrarily many step to escape a local minimum under relaxed plan distances.
For example, local minima arise naturally because a relaxed plan is able to supply and not
supply a line at the same time, and thereby does not have to make the crucial distinction
between faulty and non-faulty lines. Now, these observations hold for the original domain
formulation, with derived predicates and ADL constructs. As said, the IPC-4 STRIPS
formulation of PSR is obtained from that by a combination of complicated pre-processing
machines. These are not likely to make the real structure of the domain more amenable to
relaxed plan distances. While the satisficing planners participating in PSR small are by no
means exclusively dependent on relaxed plan distances, for most of them these distances do
form an important part of the search heuristics.
Figure 9 gives the results for the PSR domain versions, middle and large, with larger
instances, encoded with ADL constructs and/or derived predicates. No optimal planner
could handle these language constructs, so unfortunately the above comparison between
the two groups can not be continued. In the middle sized instances, FDD, SGPlan, and
LPG-TD all scale through the entire test suite. FDD indicates better scaling behavior in the
largest instances. In the domain version with large instances (available only in a formulation
using ADL), FDD indeed shows that it outperforms the other planners (at least, SGPlan,
that also participates here) by far. The data for FD is almost identical to that for FDD.
Regarding plan quality, the only participating planner in each of middle and large that
tries to minimize makespan is Marvin, so we have no basis for a comparison. Of the
other planners, SGPlan generally finds the plans with the smallest number of actions. Precisely, in the middle version the plan quality ratios are as follows. The ratio FDD vs
558

The Deterministic Part of IPC-4: An Overview

SGPlan is [0.67(1.23)2.33] (FD: [0.67(1.25)2.85]). The ratio LPG-TD.speed vs SGPlan is
[0.67(1.82)7.27]; the maximum case is instance number 49, where LPG-TD.speed takes 80
steps and SGPlan 11. The ratio LPG-TD.quality vs SGPlan is [0.60(1.08)7.27], with the
same maximum case. In the large version, only FD and FDD solve a considerable number
of instances; the ratio FD vs FDD is [0.60(1.01)1.30].
In the PSR domain version with middle-size instances and derived predicates compiled
into ADL, i.e., in domain version middle-compiled, only Macro-FF and SGPlan participated.
Macro-FF scaled relatively well, solving 32 instances up to instance number 48. SGPlan
solved only 14 instances up to instance number 19. Both planners try to minimize the
number of actions, and the ratio SGPlan vs Macro-FF is [0.51(1.28)1.91].
It is interesting to observe that, in PSR, when compiling derived predicates away,
the plan length increases much more than what we have seen in Section 5.5 for DiningPhilosophers and Optical-Telegraph. In the latter, the derived predicates just replace two
action applications per process, detecting that the respective process is blocked. In PSR,
however, as said the derived predicates model the flow of electricity through the network,
i.e., they encode the transitive closure of the underlying graph. The number of additional
actions needed to simulate the latter grows, of course, with the size of the graph. This
phenomenon can be observed in the IPC-4 plan length data. The PSR middle-compiled
instances are identical to the middle instances, except for the compilation of derived predicates. Comparing the plan quality of Macro-FF in middle-compiled with that of SGPlan
in middle, we obtain the remarkably high ratio values [7.17(12.53)25.71]. The maximum
case is in instance number 48 – the largest instance solved by Macro-FF – where Macro-FF
takes 180 steps to solve the compiled instance, while SGPlan takes only 7 steps to solve the
original instance.14
In domain version small, we awarded 1st places to FDD (and FD), and SATPLAN; we
awarded 2nd places to LPG-TD, SGPlan, YAHSP, and BFHSP. In domain version middle,
we awarded a 1st place to FDD (and FD); we awarded 2nd places to LPG-TD and SGPlan.
In domain version middle-compiled, we awarded a 1st place to Macro-FF. In domain version
large, we awarded a 1st place to FDD (and FD).
5.7 Satellite
The Satellite domain was introduced in IPC-3 by Long and Fox (2003). It is motivated
by a NASA space application: a number of satellites has to take images of a number of
spatial phenomena, obeying constraints such as data storage space and fuel usage. In IPC-3,
there were the domain versions Strips, Numeric, Time (action durations are expressions in
static variables), and Complex (durations and numerics, i.e. the “union” of Numeric and
Time). The numeric variables transport the more complex problem constraints, regarding
data capacity and fuel usage.
For IPC-4, the domain was made a little more realistic by additionally introducing time
windows for the sending of the image data to earth, i.e. to antennas that are visible for
satellites only during certain periods of time. We added the new domain versions Time14. While Macro-FF is not an optimal planner, and the optimal plan lengths for the middle-compiled instances are not known, it seems highly unlikely that these observations are only due to overlong plans
found by Macro-FF.

559

Hoffmann & Edelkamp

timewindows, Time-timewindows-compiled, Complex-timewindows, and Complex-timewindowscompiled, by introducing time windows, explicit and compiled, into the IPC-3 Time and
Complex versions, respectively.15 None of the domain versions uses ADL constructs, so of all
versions there was only a single (STRIPS) formulation. The instances were, as much as possible, taken from the original IPC-3 instance suites. Precisely, the Strips, Numeric, Time,
and Complex versions each contained the 20 instances posed in IPC-3 to the fully-automatic
planners, plus the 16 instances posed in IPC-3 to the hand-tailored planners. For Timetimewindows and Time-timewindows-compiled, we extended the 36 instances from Time
with the time windows. Similarly, for Complex-timewindows and Complex-timewindowscompiled, we extended the 36 instances from Complex with the time windows. That is, in
the newly added domain versions the sole difference to the previous instances lies in the
time windows.
Let us first consider the results in the simpler versions of the Satellite domain. Figure 10
shows the results for the Strips and Time versions. To many of the satisficing planners, the
Strips version does not pose a serious problem, see Figure 10 (a). Macro-FF and YAHSP
show the best runtime behavior, followed closely by LPG-TD and FDD (as well as FD). As
for the optimal planners in Figure 10 (b), none of them scales up very far. SATPLAN is
most efficient, solving 11 instances; CPT and Semsyn each solve 9 instances, Optiplan solves
8, BFHSP solves only 6. In the Time version, LPG-TD and SGPlan behave similarly up
to instance number 30, but LPG-TD solves two more, larger, instances. The Time optimal
planners, see Figure 10 (d), are clearly headed by CPT.
Regarding the efficiency of the satisficing planners in the Strips test suite, we remark
that these instances were solved quite efficiently at IPC-3 already, e.g. by FF and LPG-3.
Further, Hoffmann (2005) shows that, in this domain, under relaxed plan distance, from
any non-goal state, one can reach a state with strictly smaller heuristic value within at most
5 steps. In that sense, the results depicted in Figure 10 (a) didn’t come as a surprise to us.
Let us consider plan quality in Strips and Time. In both domain versions, as before the
addressed optimization criteria are plan length, and makespan. In Strips, the plan quality
behavior of all the planners trying to minimize plan length is rather similar. The overall
shortest plans are found by LPG-TD.quality. Precisely, the ratio FDD vs LPG-TD.quality
is [0.96(1.19)1.48]; the ratio FD vs LPG-TD.quality is [0.96(1.20)1.53]; the ratio LPGTD.speed vs LPG-TD.quality is [1.00(1.12)1.31]; the ratio Macro-FF vs LPG-TD.quality
is [0.95(1.03)1.17]; the ratio Roadmapper vs LPG-TD.quality is [1.00(1.35)1.71]; the ratio
SGPlan vs LPG-TD.quality is [0.99(1.07)1.70]; the ratio YAHSP vs LPG-TD.quality is
[0.97(1.22)1.93]. Of the planners trying to optimize makespan in the Strips version, only
Marvin and P-MEP are satisficing; Marvin is the only planner that scales to the larger
instances. The ratio P-MEP vs SATPLAN (which solves a superset of the instances solved
by P-MEP) is [1.01(2.22)3.03]. The ratio Marvin vs SATPLAN is [1.08(2.38)4.00].
In the Time version, the only planner minimizing plan length (i.e., the number of actions) is SGPlan, so there is no basis for comparison. The only satisficing planners trying
15. In the new domain versions derived from Complex, we also introduced utilities for the time window
inside which an image is sent to earth. For each image, the utility is either the same for all windows, or
it decreases monotonically with the start time of the window, or it is random within a certain interval.
Each image was put randomly into one of these classes, and the optimization requirement is to minimize
a linear combination of makespan, fuel usage, and summed up negated image utility.

560

The Deterministic Part of IPC-4: An Overview

10000

10000

1000

1000

100

100

10

10

1

BFHSP
CPT
OPTIPLAN
SATPLAN
SEMSYN

1
LPG-TD
MACRO-FF
SGPLAN
FDD
MARVIN
PMEP
ROADMAPPER
YAHSP
LPG-3

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

5

10

15

(a)

20

25

30

35

(b)

10000

10000
TP4
CPT
HSPS_A

1000

1000

100

100

10

10

1

1

0.1

0.1
LPG-TD
SGPLAN
PMEP
LPG-3

0.01

0.01
5

10

15

20

25

30

35

5

(c)

10

15

20

25

30

35

(d)

Figure 10: Satellite; Strips satisficing (a), optimal (b), and Time satisficing (c), optimal
(d).

to minimize makespan are P-MEP and LPG-TD. In the small instances solved by CPT (actually, a superset of those solved by P-MEP), the ratio P-MEP vs CPT is [1.10(3.71)6.49].
The ratio LPG-TD.speed vs CPT is [1.33(3.37)5.90]; the ratio LPG-TD.quality vs CPT is
[0.85(1.24)1.86]. Note that, like we have seen in Pipesworld before, sometimes LPG-TD
finds better plans here than the optimal CPT. As said, this is due to the somewhat simpler
model of durative actions that CPT uses (Vidal & Geffner, 2004), making no distinction
between the start and end time points of actions.
Figure 11 (a) shows the results in Satellite Numeric, together for all participating planners – the only optimal planner that participated here was Semsyn. SGPlan and LPG-TD
scale best; they solve the same number of instances, but SGPlan solves some larger ones
and is at least an order of magnitude faster in those instances solved by both. Regarding
plan quality, SGPlan was the only planner here trying to minimize plan length. The other
planners tried to minimize the metric value of the plan, i.e., the quality metric specified in
the instance files, which is fuel usage. The best plans in this respect are found by LPG561

Hoffmann & Edelkamp

10000

10000
LPG-TD
SGPLAN
PMEP
SEMSYN
LPG-3

1000

LPG-TD
SGPLAN

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

5

10

15

(a)

20

25

30

35

(b)

10000

10000
LPG-TD
SGPLAN
TILSAPA

1000

1000

100

100

10

10

1

1

0.1

0.1
LPG-TD
SGPLAN
PMEP
LPG-3

0.01

0.01
5

10

15

20

25

30

35

5

(c)

10

15

20

25

30

35

(d)

Figure 11: Satellite; Numeric (a), Time-timewindows (b), satisficing Complex (c),
Complex-timewindows (d).

TD.quality; it is unclear to us why Semsyn was marked to optimize the metric value here,
and why it does not find the optimal plans. The ratio LPG-TD.speed vs LPG-TD.quality is
[1.00(2.26)4.69]; the ratio P-MEP vs LPG-TD.quality is [1.01(2.16)3.03]; the ratio Semsyn
vs LPG-TD.quality is [1.00(1.28)1.58].
In Time-timewindows, no optimal planner could participate due to the timed initial
literals. Of the satisficing planners, see Figure 11 (b), only SGPlan and LPG-TD participated, which both scaled relatively well, with a clear advantage for SGPlan. Regarding
plan length, SGPlan minimizes the number of actions, and LPG-TD the makespan. The ratio LPG-TD.speed vs LPG-TD.quality is [1.00(1.22)1.51]. In Time-timewindows-compiled,
only SGPlan and CPT participated. SGPlan largely maintained its performance from the
Time-timewindows version, CPT could solve only 3 of the smallest instances. Plan quality can’t be compared due to the different criteria (plan length and makespan) that are
minimized.
562

The Deterministic Part of IPC-4: An Overview

Figure 11 (c) shows the performance of the satisficing planners in Satellite Complex.
SGPlan and LPG-TD scale well, in particular much better than the only other competitor,
P-MEP. In the largest three instances, SGPlan shows a clear runtime advantage over LPGTD. In the optimal track, only TP4 and HSP∗a competed here. Both solved the same four
very small instances, numbers 1, 2, 3, and 5, in almost the same runtime. SGPlan is the only
planner minimizing the number of actions. Of the other planners, which all try to minimize
makespan, only LPG-TD scales up to large instances. The ratio LPG-TD.speed vs LPGTD.quality is [1.01(2.76)4.71]. The ratio LPG-TD.speed vs TP4 is [1.81(2.09)2.49]; the ratio
LPG-TD.quality vs TP4 is [0.93(1.07)1.19]; the ratio P-MEP vs TP4 is [1.27(2.25)3.32]. As
above for CPT, the better plan (in one case) found by LPG-TD.quality is due to the
somewhat simpler action model used in TP4 (Haslum & Geffner, 2001).
Figure 11 (d) shows the performance of all planners in Complex-timewindows – as
above, due to the timed initial literals no optimal planner could compete here. SGPlan
scales clearly best, followed by LPG-TD; Tilsapa solves only the 3 smallest instances. Of
these 3 participating planners, each one minimizes a different quality criterion: number of
actions for SGPlan, makespan for Tilsapa, and metric value – the aforementioned linear
combination of makespan, fuel usage, and summed up negated image utility – for LPG-TD.
So the only useful comparison is that between LPG-TD.speed and LPG-TD.quality, where
the ratio is [1.00(2.33)7.05].16 In Complex-timewindows-compiled, the only participating
planner was SGPlan. It maintained its good scalability from the Complex domain version
with explicit time windows.
In domain version Strips, we awarded 1st places to Macro-FF, YAHSP, and SATPLAN;
we awarded 2nd places to FDD (and FD), LPG-TD, CPT, Optiplan, and Semsyn. In
domain version Time, we awarded 1st places to LPG-TD and CPT; we awarded a 2nd place
to SGPlan. In each of the domain versions Numeric, Time-timewindows, Complex, and
Complex-timewindows, we awarded a 1st place to SGPlan and a 2nd place to LPG-TD.
5.8 Settlers
The Settlers domain was also introduced in IPC-3 (Long & Fox, 2003). It features just
a single domain version, which makes extensive use of numeric variables. These variables
carry most of the domain semantics, which is about building up an infrastructure in an
unsettled area, involving the building of houses, railway tracks, sawmills, etc. In IPC-3,
no planner was able to deal with the domain in an efficient way – the best IPC-3 planner
in Settlers, Metric-FF (Hoffmann, 2003), solved only the smallest six instances of the test
suite. For these reasons, we included the domain into IPC-4 as a challenge for the numeric
planners. We used the exact same domain file and example instances as in IPC-3, except
that we compiled away some universally quantified preconditions to improve accessibility for
planners. The quantifiers were not nested, and ranged over a fixed set of domain constants,
so they could easily be replaced by conjunctions of atoms.
16. Due to the negated image utility, the quality values here can be negative. For LPG-TD.speed and LPGTD.quality here, this happened in 5 instances. We skipped these when computing the given ratio values,
since putting positive and negative values together doesn’t make much sense – with negative quality,
the number with larger absolute value represents the better plan. It would probably have been better
to define, in this domain version, an image penalty instead of an image utility, and thus obtain strictly
positive action “costs”.

563

Hoffmann & Edelkamp

10000

1000

100

10

1

0.1
LPG-TD
SGPLAN
SEMSYN
0.01
2

4

6

8

10

12

14

16

18

20

Figure 12: Performance of all planners in Settlers.
Figure 12 shows the runtime results obtained in IPC-4. The efficiency increase compared
to IPC-3 is, obviously, dramatic: SGPlan solves every instance within less than 15 seconds
(instance number 8 is unsolvable).17 LPG-TD solves 13 instances out of the set, Semsyn
solves 3 (numbers 1,2, and 5). We remark that the solutions for these tasks, as returned
by SGPlan, are huge, up to more than 800 actions in the largest tasks. On the one hand,
this once again demonstrates SGPlan’s capability to find extremely long plans extremely
quickly. On the other hand, SGPlan’s plans in Settlers might be unnecessarily long, to some
extent. In the largest instance solved by Semsyn, number 5, Semsyn finds a plan with 94
actions, while the plan found by SGPlan has 264 actions. In the largest instance solved by
LPG-TD, number 17, LPG-TD.quality takes 473 actions while SGPlan takes 552. LPG-TD
minimizes the metric value of the plans, a linear combination of the invested labor, the
resource use, and the caused pollution. The ratio LPG-TD.speed vs LPG-TD.quality is
[1.00(1.21)3.50].
We awarded a 1st place to SGPlan and a 2nd place to LPG-TD.
5.9 UMTS
UMTS applications require comparatively much time to be started on a hand-held device,
since they have to communicate with the network several times. If there is more than one
application that has been called, this yields a true bottleneck for the user. Therefore, the
applications set-up is divided into several parts to allow different set-up modules to work
concurrently. The task in the domain is to provide a good schedule – minimizing the used
time – for setting up timed applications, respecting the dependencies among them.
In IPC-4, there were the six domain versions UMTS, UMTS-timewindows, UMTStimewindows-compiled, UMTS-flaw, UMTS-flaw-timewindows, and UMTS-flaw-timewindowscompiled. All of these domain versions are temporal, and make use of numeric variables to
model the properties of the applications to be set up. ADL constructs are not used. UMTS
17. We remark that instance 8 is trivially unsolvable. One of the goals can only be achieved by actions
having a static precondition that is false in the initial state. This can be detected by simple reachability
analyses like, e.g., planning graphs, even without taking account of delete lists.

564

The Deterministic Part of IPC-4: An Overview

is our standard model of the domain. In UMTS-timewindows there are additional time
windows regarding the executability of set up actions, encoded with timed initial literals.
In UMTS-timewindows-compiled the same time windows are compiled into artificial constructs. The remaining three domain versions result, as their names suggest, from adding
a flaw construction to their respective counterparts. The flaw construction is practically
motivated. It consists of an extra action that has an important sub-goal as its add effect,
but that deletes another fact that can’t be re-achieved. So the flaw action can’t be used
in a plan, but it can be used in relaxed plans, i.e. when ignoring the delete effects. In
particular, adding an important sub-goal, the flawed action provides a kind of “short-cut”
for relaxed plans, where the short-cut does not work in reality. This can lead to overly optimistic heuristic values. Thus the flaw may confuse the heuristic functions of relaxed-plan
based heuristic planners. We used the flawed domain versions in IPC-4 to see whether the
latter would be the case.
In the IPC-4 test suites, all instances, irrespective of their number/size, contain 10
applications. The main scaling parameter is the number of applications that must actually
be set up. For IPC-4 instances number 1 . . . 5, a single application must be set up; for
instances number 6 . . . 10, two applications must be set up, and so on, i.e., the number of
needed applications is bx/5c where x is the index of the instance.
Figure 13 (a) and (b) shows the IPC-4 performance in the basic domain version. Obviously, SGPlan and LPG-TD have no difficulty at all with the domain – they solve every
instance within split seconds. CRIKEY takes more time, but also scales up nicely. As for
the optimal planners, only TP4 and HSP∗a were able to handle the domain syntax, due to
the combination of numeric variables and action durations. As Figure 13 (b) shows they
scaled relatively similarly, with a slight advantage for HSP∗a . Note that the two planners
scaled better than P-MEP and LPG-3 in the satisficing track.
We remark at this point that UMTS is mainly intended as a benchmark for optimal
planners minimizing makespan. The domain is a pure scheduling problem by nature. As
in most scheduling problems, it is trivial to find some plan – for example, one can simply
schedule all applications in sequence.18 The only point of the domain, and, indeed, of using
a computer to solve it, is to provide good schedules, that is, schedules with the smallest
possible execution time, corresponding to the makespan of the plan.
That said, we observe that the satisficing planners in IPC-4 were quite good at finding
near-optimal plans in UMTS. In fact, as we will see in detail in the following, the only
planner finding highly non-optimal plans was LPG-3. Let’s consider the basic domain
version treated in Figure 13 (a) and (b). Two of the participating planners, CRIKEY
and SGPlan, try to minimize the number of actions (i.e., the wrong optimization criterion).
Their data is identical, i.e. the plan “lengths” are the same for all instances. Precisely, both
planners find, for each instance number x, a plan with bx/5c∗8 actions in it. This is, in fact,
the optimal (smallest possible) number of actions – remember what we said above about the
scaling in the IPC-4 test suites. The participating planners trying to minimize makespan
are LPG-3, LPG-TD, P-MEP, HSP∗a , and TP4. P-MEP solves only the smallest 5 instances,
finding the optimal plans. The ratio LPG-TD.speed vs HSP∗a is [1.00(1.02)1.11]. The ratio
18. Note that this is not possible in the Airport domain – that can also be viewed as a type of scheduling
problem (Hatzack & Nebel, 2001) – due to the restricted space on the airport. In that sense, the Airport
domain incorporates more planning aspects than UMTS.

565

Hoffmann & Edelkamp

10000

10000
LPG-TD
SGPLAN
CRIKEY
PMEP
LPG-3

1000

TP4
HSPS_A

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

10

15

20

(a)

25

30

35

40

45

50

(b)

10000

10000
LPG-TD
SGPLAN
CRIKEY
LPG-3

TP4
HSPS_A

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(c)

10

15

20

25

30

35

40

45

50

(d)

Figure 13: UMTS, satisficing (a) optimal (b); UMTS-flaw, satisficing (c) optimal (d).
LPG-TD.quality vs HSP∗a is [1.00(1.00)1.03]. The ratio LPG-3 vs HSP∗a is [1.00(1.53)2.27];
we remark that the plan quality data used here is for “LPG-3.bestquality”, which takes the
entire available half hour time trying to optimize the found plan. Looking at the plots, one
sees that the LPG-3 makespan curve is much steeper than those of the other planners.
In Figure 13 (c) and (d), we see the performance of the IPC-4 planners in the same
basic domain version, but with the flaw construct. LPG-TD remains unaffected, but SGPlan and CRIKEY become a lot worse. Particularly, CRIKEY now takes several minutes
to solve even the smallest instances. We take this to confirm our intuition that the flaw can
(but does not necessarily) confuse the heuristic functions of relaxed-plan based heuristic
planners. Whether or not the heuristic function becomes confused probably depends on
details in the particular way the relaxed plans are constructed (such a construction may,
e.g., choose between different actions based on an estimate of how harmful they are to
already selected actions). In the optimal track, when introducing the flaw construct into
the domain, the performance of TP4 and HSP∗a becomes slightly worse, and nearly indistinguishable. Regarding plan quality, there now is a quality difference in terms of the number
of actions needed by CRIKEY and SGPlan. SGPlan’s plans still all have the smallest pos566

The Deterministic Part of IPC-4: An Overview

10000

10000
LPG-TD
SGPLAN
TILSAPA

LPG-TD
SGPLAN

1000

1000

100

100

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20

25

30

35

40

45

50

5

(a)

10

15

20

25

30

35

40

45

50

(b)

Figure 14: UMTS-timewindows (a); UMTS-flaw-timewindows (b).
sible “length”, bx/5c ∗ 8. Those of CRIKEY have a lot of variance, and are longer; the ratio
CRIKEY vs SGPlan is [1.07(1.72)3.88]. In the group minimizing makespan, the observations are very similar to the unflawed domain version above. The ratio LPG-TD.speed vs
HSP∗a is [1.00(1.01)1.04], and the ratio LPG-TD.quality vs HSP∗a is [1.00(1.00)1.01]. Although it solves only the smaller half of the instances, the ratio LPG-3.bestquality vs HSP∗a
is [1.00(1.48)2.27].
Figure 14 shows the results in the two domain versions with explicitly encoded time
windows. No optimal planners could participate since none of them could handle timed
initial literals. In the version without the flaw, again LPG-TD and SGPlan need only
split seconds. The only other competitor, Tilsapa, needs more runtime but also scales up
well. When introducing the flaw, the only competitors are SGPlan and LPG-TD. SGPlan’s
runtime performance becomes a lot worse, while LPG-TD remains completely unaffected.
Regarding plan quality, the number of actions in SGPlan’s plans is still bx/5c∗8 in all cases.
LPG-TD.speed and LPG-TD.quality return plans with identical makespan in all cases. In
the non-flawed version, UMTS-timewindows, the makespan ratio Tilsapa vs LPG-TD is
[1.00(1.20)1.37].
We do not show runtime graphs for the domain versions with compiled time windows.
In UMTS-timewindows-compiled, SGPlan and CRIKEY participated. Both scale up well
and solve all instances, but SGPlan needs only split seconds, while CRIKEY needs up to
more than 100 seconds per instance in the larger cases. Both planners try to minimize the
number of actions, and both need exactly bx/5c ∗ 8 + 5 actions for each instance number
x – namely, the optimal number bx/5c ∗ 8 of actions as before, plus 5 artificial actions
encoding the time windows. In UMTS-flaw-timewindows-compiled, the sole participating
planner was SGPlan. It solved only the smaller half of the instances, finding plans of length
bx/5c ∗ 8 + 6 – i.e., using one unnecessary action in each instance (the action concerns an
application that does not need to be set up).
In domain version UMTS, we awarded 1st places to SGPlan, LPG-TD, and HSP∗a ; we
awarded 2nd places to CRIKEY and TP4. In domain version UMTS-flaw, we awarded
567

Hoffmann & Edelkamp

a 1st place to LPG-TD, and 2nd places to SGPlan, HSP∗a , and TP4. In version UMTStimewindows, we awarded 1st places to SGPlan and LPG-TD, and a 2nd place to Tilsapa.
In UMTS-flaw-timewindows, we awarded a 1st place to LPG-TD. In UMTS-timewindowscompiled, we awarded a 1st place to SGPlan.

6. IPC-4 Awards
The numbers of 1st and 2nd places achieved by the planners are shown in Tables 3 and 4;
planners that never came in 1st or 2nd place are left out of the tables. Since many of the
planners (6 of the satisficing planners, and 4 of the optimal planners) only dealt with the
purely propositional domain versions (i.e., STRIPS or ADL), we counted the performance
in these domains separately.
SGPlan LPG-TD FD FDD Macro-FF YAHSP CRIKEY Tilsapa
Propositional
3/6
1/6 5/2 6/3
3/0
4/1
0/0
0/0
Temp/Metric 13 / 2
7/7
0/1
0/1
Total Count
16 / 8
8 / 13 5 / 2 6 / 3
3/0
4/1
0/1
0/1
Table 3: Summary of results: satisficing planners, number of 1st places / number of 2nd
places.

CPT
Propositional 0 / 1
Temp/Metric 3 / 0
Total Count 3 / 1

TP-4
0/0
0/5
0/5

HSP∗a SATPLAN Optiplan Semsyn BFHSP
0/0
5/1
0/4
0/2
0/2
1/2
1/2
5/1
0/4
0/2
0/2

Table 4: Summary of results: optimal planners, number of 1st places / number of 2nd
places.
For the satisficing planners, based on our observations we decided to award separate
prizes for performance in the pure STRIPS and ADL domains. For the optimal planners
this seemed not appropriate due to, first, the small number of planners competing in the
temporal/metric domains, and, second, the smaller overall number of competing systems –
giving 4 prizes to 7 systems seemed too much. Overall, the awards made in the deterministic
part of IPC-4 are the following:
• 1st Prize, Satisficing Propositional Track – Fast (Diagonally) Downward, by Malte
Helmert and Silvia Richter
• 2nd Prize, Satisficing Propositional Track – YAHSP, by Vincent Vidal
• 2nd Prize, Satisficing Propositional Track – SGPlan, by Yixin Chen, Chih-Wei Hsu
and Benjamin W. Wah
• 1st Prize, Satisficing Metric Temporal Track – SGPlan, by Yixin Chen, Chih-Wei Hsu
and Benjamin W. Wah
568

The Deterministic Part of IPC-4: An Overview

• 2nd Prize, Satisficing Metric Temporal Track – LPG-TD, by Alfonso Gerevini, Alessandro Saetti, Ivan Serina, and Paolo Toninelli
• 1st Prize, Optimal Track – SATPLAN, by Henry Kautz, David Roznyai, Farhad
Teydaye-Saheli, Shane Neth, and Michael Lindmark
• 2nd Prize, Optimal Track – CPT, by Vincent Vidal and Hector Geffner
We would like to re-iterate that the awarding of prizes is, and has to be, a very sketchy
“summary” of the results of a complex event such as IPC-4. A few bits of information are
just not sufficient to summarize thousands of data points. Many of the decisions we took in
the awarding of the prizes, i.e. in the judgement of scaling behavior, were very close. This
holds especially true for most of the runtime graphs concerning optimal planners, and for
some of the runtime graphs concerning satisficing propositional planners. What we think is
best, and what we encourage everybody to do, is to have a closer look at the results plots
for themselves. As said before, the full plots are available in an online appendix.

7. Conclusion
All in all, our feeling as the organizers is that the deterministic part of IPC-4 was a great
success, and made several valuable contributions to the field. To mention the, from our
perspective, two most prominent points: the event provided the community with a set
of interesting new benchmarks, and made visible yet another major step forward in the
scalability of satisficing planning systems. The latter was made possible by novel heuristics
and domain analysis techniques.
There is a wide variety of questions to be addressed in the context of the future of the
(deterministic part of) the IPC. Let us discuss just a few that we feel are important. Regarding benchmark domains, as said we invested significant effort into the IPC-4 benchmarks,
and we would definitely recommend to re-use some of them in future IPC editions. While
some domain versions were solved relatively easily by certain groups of planners, many of
them still constitute major challenges. Examples are Pipesworld with tankage restrictions,
Optical-Telegraph, large PSR instances, and UMTS for optimal planners. That said, most
of the IPC-3 domains are also still challenging and should be re-used. It is probably more
useful for the community to consolidate performance on the existing set of benchmarks,
rather than to increase the benchmark database at large pace. For one thing, to measure
progress between IPC editions, re-used benchmark domains (and instances), like Satellite
and Settlers in the case of IPC-4, are more useful.19 More generally, the benchmark set is
already large; if there is a too large set, then a situation may arise where authors select
rather disjoint subsets in their individual experiments.
19. In the field addressing the SAT problem, particularly in the respective competition events, progress
is also often measured simply in terms of the size (number of variables and clauses) of the formulas
that could be tackled successfully, making just a few distinctions about the origin of the formulas (like,
randomly generated or from an application). In the context of the IPC, one can do similar things by
measuring parameters such as, e.g., the number of ground actions tackled successfully. However, given
the large differences between the individual domains used in the IPC, more distinctions must be made.
A detailed investigation of the effect of several parameters, in the IPC-4 domains, is given by Edelkamp
et al. (2005).

569

Hoffmann & Edelkamp

Regarding PDDL extensions, the formalisms for derived predicates and timed initial
literals that we introduced are only first steps into the respective directions. The PDDL2.2
derived predicates formalism is restrictive in that it allows no negative interactions between
derived predicates; one could easily allow negative interactions that do not lead to cycles
and thus to ambiguous semantics (Thiebaux et al., 2003, 2005). One could also imagine
derivation rules for values of more general data types than predicates/Booleans, particularly for numeric variables. As for timed initial literals, obviously they encode only a very
restrictive subset of the wide variety of forms that exogenous events can take. Apart from
exogenous events on numeric values, such events may in reality be continuous processes
conditioned on the world state, rather than finitely many discrete time instants known beforehand. The PDDL2.2 action model itself is, of course, still restrictive in its postulation of
discrete variable value updates. Still we believe that the IPC should not let go off the very
simple PDDL subsets such as STRIPS, to support accessibility of the competition. As noted
in Section 4.1, most systems are still not able to handle all the language features introduced
with IPC-3 and IPC-4. Also, we believe that a STRIPS track, and more generally domain
versions formulated in simple language subsets, is important to encourage basic algorithms
research. A new idea is easier to try in a simple language. To avoid misunderstandings: the
language features introduced with PDDL2.1 and PDDL2.2 already have a significant basis
of acceptance in implemented systems, and should definitely be kept for future editions of
the IPC.
In the context of basic research, it should be noted that the satisficing track of IPC-4
was almost entirely populated by planners of the relaxed-plan based heuristic search type.
This demonstrates the danger of the competition to concentrate research too much around
a few successful methods. Still, two of the most remarkable planners in that track, FastDownward and SGPlan, use a significantly different heuristic and new domain analysis
techniques as the basis of their success, respectively.
Putting the optimal planners in a separate track serves to maintain this, very different,
type of planning algorithms. We recommend to keep this distinction in the future IPC
events.
About the hand-tailored track, we have to say that it seems unclear if and how that
could be brought back into the focus. Maybe a more suitable form of such an event would
be an online (at the hosting conference) programming (i.e., planner tailoring) competition.
This would, of course, imply a much smaller format for the event than the format the IPC
for automated planners has grown to already. But an online hand-tailored competition
would have the advantage of better visibility of programming efforts, and of not taking up
prohibitively much time of the system developers.
In the context of competition size, last not least there are a few words to be said about
the role and responsibilities of the organizers. The IPC has grown too large to be handled,
in all its aspects, by just two persons. It may be worth thinking about distributing the
organization workload among a larger group of people. One approach that might make
sense would be to let different people organize the tracks concerning the different PDDL
subsets. Another approach would be to let different people handle the language definition,
the benchmark preparation, and the results collection, respectively. It would probably be
a good idea to establish an IPC council that, unlike the organizing committees in the past,
570

The Deterministic Part of IPC-4: An Overview

would persist across individual competitions, and whose role would be to actively set up
and support the organizing teams.

Acknowledgments
We would like to thank the IPC-4 organizing committee, namely Fahiem Bacchus, Drew
McDermott, Maria Fox, Derek Long, Jussi Rintanen, David Smith, Sylvie Thiebaux, and
Daniel Weld for their help in taking the decision about the language for the deterministic
part of IPC-4, and in ironing out the details about syntax and semantics. We especially
thank Maria Fox and Derek Long for giving us the latex sources of their PDDL2.1 article,
and for discussing the modifications of this document needed to introduce the semantics of
derived predicates and timed initial literals. We are indebted to Roman Englert, Frederico
Liporace, Sylvie Thiebaux, and Sebastian Trüg, who helped in the creation of the benchmark domains. We wish to say a big thank you to all the participating teams for their
efforts. There is significant bravery in the submission of a planning system to a competition, where the choice and design of the benchmark problems is up to the competition
organizers, not to the individuals. We thank the LPG team for investing the extra effort of
running the IPC-3 LPG version on the IPC-4 benchmarks, and we thank Shahid Jabbar for
proofreading this text. We thank Subbarao Kambhampati for pointing out that the name
“classical part” is ambiguous, and suggesting to use “deterministic part” instead. Last not
least, we thank the anonymous reviewers, and Maria Fox in her role as the responsible JAIR
editor, for their comments; they helped to improve the paper. At the time of organizing
the competition, Jörg Hoffmann was supported by the DFG (Deutsche Forschungsgemeinschaft), in the research project “HEUPLAN II”. Stefan Edelkamp was and is supported by
the DFG in the research project “Heuristic Search”.

Appendix A. BNF Description of PDDL2.2
This appendix contains a complete BNF specification of the PDDL2.2 language. For readability, we mark with (∗ ∗ ∗) the points in the BNF where, in comparison to PDDL2.1, the
new language constructs of PDDL2.2 are inserted.
A.1 Domains
Domains are defined exactly as in PDDL2.2, except that we now also allow to define rules
for derived predicates at the points where operators (actions) are allowed.
hdomaini

hrequire-defi
hrequire-keyi

::= (define (domain hnamei)
[hrequire-defi]
[htypes-defi]:typing
[hconstants-defi]
[hpredicates-defi]
[hfunctions-defi]:fluents
hstructure-defi∗ )
::= (:requirements hrequire-keyi+)
::= See Section A.6
571

Hoffmann & Edelkamp

htypes-defi
::= (:types htyped list (name)i)
hconstants-defi
::= (:constants htyped list (name)i)
hpredicates-defi
::= (:predicates hatomic formula skeletoni+ )
hatomic formula skeletoni
::= (hpredicatei htyped list (variable)i)
hpredicatei
::= hnamei
hvariablei
::= ?hnamei
hatomic function skeletoni
::= (hfunction-symboli htyped list (variable)i)
hfunction-symboli
::= hnamei
hfunctions-defi
::=:fluents (:functions hfunction typed list
(atomic function skeleton)i)
hstructure-defi
::= haction-defi
hstructure-defi
::=:durative−actions hdurative-action-defi
(∗ ∗ ∗) hstructure-defi
::=:derived−predicates hderived-defi
∗
htyped list (x)i ::= x
htyped list (x)i ::=:typing x+ - htypei htyped list(x)i
hprimitive-typei::= hnamei
htypei
::= (either hprimitive-typei+ )
htypei
::= hprimitive-typei
hfunction typed list (x)i ::= x∗
hfunction typed list (x)i ::=:typing x+ - hfunction typei
hfunction typed list(x)i
hfunction typei
::= number

A.2 Actions
The BNF for an action definition is the same as in PDDL2.2.
haction-defi

::= (:action haction-symboli
:parameters ( htyped list (variable)i )
haction-def bodyi)
haction-symboli ::= hnamei
haction-def bodyi ::= [:precondition hGDi]
[:effect heffecti]
hGDi
::= ()
hGDi
::= hatomic formula(term)i
hGDi
::=:negative−preconditions hliteral(term)i
hGDi
::= (and hGDi∗ )
hGDi
::=:disjunctive−preconditions (or hGDi∗ )
hGDi
::=:disjunctive−preconditions (not hGDi)
hGDi
::=:disjunctive−preconditions (imply hGDi hGDi)
hGDi
::=:existential−preconditions
(exists (htyped list(variable)i∗ ) hGDi )
:universal−preconditions
hGDi
::=
(forall (htyped list(variable)i∗ ) hGDi )
:fluents
hGDi
::=
hf-compi
hf-compi
::= (hbinary-compi hf-expi hf-expi)
hliteral(t)i
::= hatomic formula(t)i
hliteral(t)i
::= (not hatomic formula(t)i)
572

The Deterministic Part of IPC-4: An Overview

hatomic formula(t)i
htermi
htermi
hf-expi
hf-expi
hf-expi
hf-expi
hf-headi
hf-headi
hbinary-opi
hbinary-opi
hbinary-opi
hbinary-opi
hbinary-compi
hbinary-compi
hbinary-compi
hbinary-compi
hbinary-compi
hnumberi
heffecti
heffecti
heffecti
hc-effecti
hc-effecti
hc-effecti
hp-effecti
hp-effecti
hp-effecti
hp-effecti
hcond-effecti
hcond-effecti
hassign-opi
hassign-opi
hassign-opi
hassign-opi
hassign-opi

::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=
::=

(hpredicatei t∗ )
hnamei
hvariablei
hnumberi
(hbinary-opi hf-expi hf-expi)
(- hf-expi)
hf-headi
(hfunction-symboli htermi∗ )
hfunction-symboli
+
−
∗
/
>
<
=
>=
<=
Any numeric literal
(integers and floats of form n.n).

::= ()
::= (and hc-effecti∗ )
::= hc-effecti
::=:conditional−effects (forall (hvariablei∗ ) heffecti)
::=:conditional−effects (when hGDi hcond-effecti)
::= hp-effecti
::= (hassign-opi hf-headi hf-expi)
::= (not hatomic formula(term)i)
::= hatomic formula(term)i
::=:fluents (hassign-opi hf-headi hf-expi)
::= (and hp-effecti∗ )
::= hp-effecti
::= assign
::= scale-up
::= scale-down
::= increase
::= decrease

A.3 Durative Actions
Durative actions are the same as in PDDL2.2, except that we restrict ourselves to level 3
actions, where the duration is given as the fixed value of a numeric expression (rather than
as the possible values defined by a set of constraints). This slightly simplifies the BNF.
hdurative-action-defi ::= (:durative-action hda-symboli
:parameters ( htyped list (variable)i )
hda-def bodyi)
hda-symboli
::= hnamei
hda-def bodyi
::= :duration (= ?duration hf-expi)
:condition hda-GDi
573

Hoffmann & Edelkamp

:effect hda-effecti
::= ()
::= htimed-GDi
::= (and htimed-GDi+ )
::= (at htime-specifieri hGDi)
::= (over hintervali hGDi)
::= start
::= end
::= all

hda-GDi
hda-GDi
hda-GDi
htimed-GDi
htimed-GDi
htime-specifieri
htime-specifieri
hintervali

A.4 Derived predicates
As said, rules for derived predicates can be given in the domain description at the points
where actions are allowed. The BNF is:
(∗ ∗ ∗) hderived-defi

::= (:derived htyped list (variable)i hGDi)

Note that we allow the specification of types with the derived predicate arguments. This
might seem redundant as the predicate types are already declared in the :predicates field.
Allowing to specify types with the predicate (rule) “parameters” serves to give the language
a more unified look-and-feel, and one might use the option to make the parameter ranges
more restrictive. (Remember that the specification of types is optional, not mandatory.)
Repeating what has been said in Section 3.1.1, this BNF is more generous than what is
considered a well-formed domain description in PDDL2.2. We call a predicate P derived if
there is a rule that has a predicate P in its head; otherwise we call P basic. The restrictions
we apply are:
1. The actions available to the planner do not affect the derived predicates: no derived
predicate occurs on any of the effect lists of the domain actions.
2. If a rule defines that P (x) can be derived from φ(x), then the variables in x are
pairwise different (and, as the notation suggests, the free variables of φ(x) are exactly
the variables in x).
3. If a rule defines that P (x) can be derived from φ, then the Negation Normal Form
(NNF) of φ(x) does not contain any derived predicates in negated form.
A.5 Problems
The only change made to PDDL2.1 in the problem description is that we allow the specification of timed initial literals.
hproblemi

::= (define (problem hnamei)
(:domain hnamei)
[hrequire-defi]
[hobject declarationi ]
hiniti
574

The Deterministic Part of IPC-4: An Overview

hgoali
[hmetric-speci]
[hlength-speci ])
hobject declarationi ::= (:objects htyped list (name)i)
hiniti
::= (:init hinit-eli∗ )
hinit-eli
::= hliteral(name)i
hinit-eli
::=:fluents (= hf-headi hnumberi)
(∗ ∗ ∗) hinit-eli
::=:timed−initial−literals (at hnumberi hliteral(name)i)
hgoali
::= (:goal hGDi)
hmetric-speci
::= (:metric hoptimizationi hground-f-expi)
hoptimizationi
::= minimize
hoptimizationi
::= maximize
hground-f-expi
::= (hbinary-opi hground-f-expi hground-f-expi)
hground-f-expi
::= (- hground-f-expi)
hground-f-expi
::= hnumberi
hground-f-expi
::= (hfunction-symboli hnamei∗ )
hground-f-expi
::= total-time
hground-f-expi
::= hfunction-symboli

Repeating what has been said in Section 3.1.1, the requirement flag for timed initial
literals implies the requirement flag for durational actions (see also Section A.6), i.e. the
language construct is only available in PDDL2.2 level 3. Also, the above BNF is more
generous than what is considered a well-formed problem description in PDDL2.2. The
times hnumberi at which the timed literals occur are restricted to be greater than 0. If
there are also derived predicates in the domain, then the timed literals are restricted to not
influence any of these, i.e., like action effects they are only allowed to affect the truth values
of the basic (non-derived) predicates (IPC-4 will not use both derived predicates and timed
initial literals within the same domain).

A.6 Requirements

Here is a table of all requirements in PDDL2.2. Some requirements imply others; some are
abbreviations for common sets of requirements. If a domain stipulates no requirements, it
is assumed to declare a requirement for :strips.
575

Hoffmann & Edelkamp

Requirement
:strips
:typing
:negative-preconditions
:disjunctive-preconditions
:equality
:existential-preconditions
:universal-preconditions
:quantified-preconditions
:conditional-effects
:fluents
:adl

:durative-actions
:derived-predicates
:timed-initial-literals

Description
Basic STRIPS-style adds and deletes
Allow type names in declarations of variables
Allow not in goal descriptions
Allow or in goal descriptions
Support = as built-in predicate
Allow exists in goal descriptions
Allow forall in goal descriptions
= :existential-preconditions
+ :universal-preconditions
Allow when in action effects
Allow function definitions and use of effects using
assignment operators and arithmetic preconditions.
= :strips + :typing
+ :negative-preconditions
+ :disjunctive-preconditions
+ :equality
+ :quantified-preconditions
+ :conditional-effects
Allows durative actions.
Note that this does not imply :fluents.
Allows predicates whose truth value is
defined by a formula
Allows the initial state to specify literals
that will become true at a specified time point
implies durative actions (i.e. applicable only
in PDDL2.2 level 3)

References
Bacchus, F. (2000). Subset of PDDL for the AIPS2000 Planning Competition. The AIPS-00
Planning Competition Committee.
Bacchus, F. (2001). The AIPS’00 planning competition. The AI Magazine, 22 (3), 47–56.
Bertoli, P., Cimatti, A., Slaney, J., & Thiébaux, S. (2002). Solving power supply restoration
problems with planning via symbolic model-checking. In Harmelen, F. V. (Ed.),
Proceedings of the 15th European Conference on Artificial Intelligence (ECAI-02),
pp. 576–580, Lyon, France. Wiley.
Blum, A. L., & Furst, M. L. (1995). Fast planning through planning graph analysis. In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI95), pp. 1636–1642, Montreal, Canada. Morgan Kaufmann.
Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90 (1-2), 279–298.
Bonet, B., & Thiebaux, S. (2003). GPT meets PSR. In Proceedings of the 13th International
Conference on Automated Planning and Scheduling (ICAPS-03), pp. 102–111, Trento,
Italy. Morgan Kaufmann.
Cesta, A., & Borrajo, D. (Eds.). (2001). Recent Advances in AI Planning. 6th European
Conference on Planning (ECP’01), Toledo, Spain. Springer-Verlag.
576

The Deterministic Part of IPC-4: An Overview

Clarke, E. M., Grumberg, O., & Peled, D. (2000). Model Checking. MIT Press.
Edelkamp, S. (2003a). Limits and possibilities of PDDL for model checking software.. In
Edelkamp, & Hoffmann (Edelkamp & Hoffmann, 2003).
Edelkamp, S. (2003b). Promela planning. In Proceedings of Model Checking Software
(SPIN), pp. 197–212.
Edelkamp, S. (2003c). Taming numbers and durations in the model checking integrated
planning system. Journal of Artificial Intelligence Research, 20, 195–238.
Edelkamp, S., & Hoffmann, J. (Eds.). (2003). Proceedings of Workshop on the Competition:
Impact, Organization, Evaluation, Benchmarks, at ICAPS’03. AAAI Press.
Edelkamp, S., Hoffmann, J., Englert, R., Liporace, F., Thiebaux, S., & Trüg, S. (2005).
Engineering benchmarks for planning: the domains used in the deterministic part of
IPC-4. Journal of Artificial Intelligence Research. Submitted.
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.). (2004). Proceedings of the
4th International Planning Competition. JPL.
Englert, R. (2003). Re-scheduling with temporal and operational resources for the mobile
execution of dynamic UMTS applications. In KI-Workshop AI in Planning, Scheduling, Configuration and Design (PUK).
Englert, R. (2005). Planning to optimize the UMTS call set-up for the execution of mobile
agents. Journal of Applied Artificial Intelligence (AAI), 19 (2), 99–117.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence, 2, 189–208.
Fox, M., Long, D., & Halsey, K. (2004). An investigation into the expressive power of
PDDL2.1. In Saitta, L. (Ed.), Proceedings of the 16th European Conference on Artificial Intelligence (ECAI-04), Valencia, Spain. Wiley.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. Journal of Artificial Intelligence Research, 20, 61–124.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239–290.
Haslum, P., & Geffner, H. (2001). Heuristic planning with time and resources.. In Cesta,
& Borrajo (Cesta & Borrajo, 2001), pp. 121–132.
Hatzack, W., & Nebel, B. (2001). The operational traffic control problem: Computational
complexity and solutions.. In Cesta, & Borrajo (Cesta & Borrajo, 2001), pp. 49–60.
Helmert, M. (2001). On the complexity of planning in transportation domains.. In Cesta,
& Borrajo (Cesta & Borrajo, 2001), pp. 349–360.
Helmert, M. (2003). Complexity results for standard benchmark domains in planning.
Artificial Intelligence, 143, 219–262.
Helmert, M. (2004). A planning heuristic based on causal graph analysis. In Koenig, S.,
Zilberstein, S., & Koehler, J. (Eds.), Proceedings of the 14th International Conference
on Automated Planning and Scheduling (ICAPS-04), pp. 161–170, Whistler, Canada.
Morgan Kaufmann.
577

Hoffmann & Edelkamp

Hipke, C. A. (2000). Distributed Visualization of Geometric Algorithms. Ph.D. thesis,
University of Freiburg.
Hoffmann, J. (2003). The Metric-FF planning system: Translating “ignoring delete lists”
to numeric state variables. Journal of Artificial Intelligence Research, 20, 291–341.
Hoffmann, J. (2005). Where ignoring delete lists works: Local search topology in planning
benchmarks. Journal of Artificial Intelligence Research. To appear.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Holzmann, G. J. (2004). The SPIN model checker: Primer and reference manual. Addison
Wesley.
Long, D., & Fox, M. (2003). The 3rd international planning competition: Results and
analysis. Journal of Artificial Intelligence Research, 20, 1–59.
McDermott, D. (2000). The 1998 AI planning systems competition. The AI Magazine,
21 (2), 35–55.
McDermott, D., et al. (1998). The PDDL Planning Domain Definition Language. The
AIPS-98 Planning Competition Committee.
Milidiú, R. L., & dos Santos Liporace, F. (2004). Plumber, a pipeline transportation planner.
In Proceedings of the International Workshop on Harbour and Maritime Simulation,
HMS, pp. 99–106, Rio de Janeiro, Brazil.
Milidiu, R. L., dos Santos Liporace, F., & de Lucena, C. J. (2003). Pipesworld: Planning pipeline transportation of petroleum derivatives.. In Edelkamp, & Hoffmann
(Edelkamp & Hoffmann, 2003).
Pednault, E. P. (1989). ADL: Exploring the middle ground between STRIPS and the situation calculus. In Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), Principles of
Knowledge Representation and Reasoning: Proceedings of the 1st International Conference (KR-89), pp. 324–331, Toronto, ON. Morgan Kaufmann.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner
for ADL. In Nebel, B., Swartout, W., & Rich, C. (Eds.), Principles of Knowledge
Representation and Reasoning: Proceedings of the 3rd International Conference (KR92), pp. 103–114, Cambridge, MA. Morgan Kaufmann.
Thiébaux, S., & Cordier, M.-O. (2001). Supply restoration in power distribution systems —
a benchmark for planning under uncertainty.. In Cesta, & Borrajo (Cesta & Borrajo,
2001), pp. 85–95.
Thiébaux, S., Cordier, M.-O., Jehl, O., & Krivine, J.-P. (1996). Supply restoration in power
distribution systems — a case study in integrating model-based diagnosis and repair
planning. In Horvitz, E., & Jensen, F. (Eds.), Proceedings of the 12th International
Conference on Uncertainty in AI (UAI-96), pp. 525–532, Portland, OR, USA. Morgan
Kaufmann.
Thiebaux, S., Hoffmann, J., & Nebel, B. (2003). In defence of PDDL axioms. In Gottlob, G.
(Ed.), Proceedings of the 18th International Joint Conference on Artificial Intelligence
(IJCAI-03), pp. 961–966, Acapulco, Mexico. Morgan Kaufmann.
578

The Deterministic Part of IPC-4: An Overview

Thiebaux, S., Hoffmann, J., & Nebel, B. (2005). In defence of PDDL axioms. Artificial
Intelligence. To appear.
Trüg, S., Hoffmann, J., & Nebel, B. (2004). Applying automatic planning systems to airport
ground-traffic control — a feasibility study. In Biundo, S., Frühwirth, T., & Palm,
G. (Eds.), KI-04: Advances in Artificial Intelligence, pp. 183–197, Ulm, Germany.
Springer-Verlag.
Vidal, V., & Geffner, H. (2004). Branching and pruning: An optimal temporal POCL planner based on constraint programming. In Proceedings of the 19th National Conference
of the American Association for Artificial Intelligence (AAAI-04), pp. 570–577, San
Jose, CA. MIT Press.
Younes, H., & Littman, M. (2005). Probabilistic part of the 4th international planning
competition. Journal of Artificial Intelligence Research. Submitted.

579

Journal of Artificial Intelligence Research 24 (2005) 641-684

Submitted 04/05; published 11/05

Binary Encodings of Non-binary Constraint Satisfaction
Problems: Algorithms and Experimental Results
Nikolaos Samaras

samaras@uom.gr

Department of Applied Informatics
University of Macedonia, Greece

Kostas Stergiou

konsterg@aegean.gr

Department of Information and Communication Systems Engineering
University of the Aegean, Greece

Abstract
A non-binary Constraint Satisfaction Problem (CSP) can be solved directly using extended versions of binary techniques. Alternatively, the non-binary problem can be translated into an equivalent binary one. In this case, it is generally accepted that the translated
problem can be solved by applying well-established techniques for binary CSPs. In this
paper we evaluate the applicability of the latter approach. We demonstrate that the use of
standard techniques for binary CSPs in the encodings of non-binary problems is problematic
and results in models that are very rarely competitive with the non-binary representation.
To overcome this, we propose specialized arc consistency and search algorithms for binary encodings, and we evaluate them theoretically and empirically. We consider three
binary representations; the hidden variable encoding, the dual encoding, and the double
encoding. Theoretical and empirical results show that, for certain classes of non-binary
constraints, binary encodings are a competitive option, and in many cases, a better one
than the non-binary representation.

1. Introduction
Constraint Satisfaction Problems (CSPs) appear in many real-life applications such as
scheduling, resource allocation, timetabling, vehicle routing, frequency allocation, etc. Most
CSPs can be naturally and eﬃciently modelled using non-binary (or n-ary) constraints that
may involve an arbitrary number of variables. It is well known that any non-binary CSP
can be converted into an equivalent binary one. The most well-known translations are the
dual encoding (Dechter & Pearl, 1989) and the hidden variable encoding (Rossi, Petrie, &
Dhar, 1990). The ability to translate any non-binary CSP into binary has been often used
in the past as a justiﬁcation for restricting attention to binary CSPs. Implicitly, the assumption had been that when faced with a non-binary CSP we can simply convert it into a
binary one, and then apply well-known generic techniques for solving the binary equivalent.
In this paper we will show that this assumption is ﬂawed because generic techniques for
binary CSPs are not suitable for binary encodings of non-binary problems.
In the past few years, there have been theoretical and empirical studies on the eﬃciency
of binary encodings and comparisons between binary encodings and the non-binary representation (Bacchus & van Beek, 1998; Stergiou & Walsh, 1999; Mamoulis & Stergiou, 2001;
Smith, 2002; Bacchus, Chen, van Beek, & Walsh, 2002). Theoretical results have showed
c
2005
AI Access Foundation. All rights reserved.

Samaras & Stergiou

that converting non-binary CSPs into binary equivalents is a potentially eﬃcient way to
solve certain classes of non-binary problems. However, in the (limited) empirical studies
there are very few cases where this appears to be true, with Conway’s “game of Life” (Smith,
2002) being the most notable exception. There are various reasons for this. In many cases,
the extensive space requirements of the binary encodings make them infeasible. Also, in
many non-binary problems we can utilize eﬃcient specialized propagators for certain constraints, such as the algorithm developed by Régin (1994) for the all-diﬀerent constraint.
Converting such constraints into binary is clearly impractical. Another reason, which has
been overlooked, is that most (if not all) experimental studies use well-known generic local
consistency and search algorithms in the encodings. In this way they fail to exploit the
structure of the constraints in the encodings, ending up with ineﬃcient algorithms. To
make the binary encodings a realistic choice of modelling and solving non-binary CSPs, we
need algorithms that can utilize their structural properties. Finally, it is important to point
out that the use of a binary encoding does not necessarily mean that we have to convert
all the non-binary constraints in a problem into binary, as it is commonly perceived. If we
are selective in the constraints we encode, based on properties such as arity and tightness,
then we can get eﬃcient hybrid models.
To address these issues, we show that the use of specialized arc consistency and search
algorithms for binary encodings of non-binary CSPs can lead to eﬃcient models. We consider three encodings; the dual, the hidden variable, and the double encoding. The latter,
which is basically the conjunction of the other two encodings, has received little attention
but may well turn out to be the most signiﬁcant in practice. The aim of this study is
twofold. First, to present eﬃcient algorithms for the binary encodings and analyze them
theoretically and experimentally. Second, and more importantly, to investigate if and when
the use of such algorithms can help solve non-binary problems eﬃciently. Towards these
aims, we make the following contributions:
• We describe a simple algorithm that enforces arc consistency on the hidden variable
encoding of an arbitrary non-binary CSP with O(ekdk ) time complexity, where e
is the number of constraints, k the maximum arity of the constraints, and d the
maximum domain size. This gives an O(d) improvement compared to the asymptotic
complexity of a generic arc consistency algorithm. The improved complexity is now
the same as the complexity of an optimal generalized arc consistency algorithm in
the non-binary representation of the problem. We also identify a property of the arc
consistency algorithm for the hidden variable encoding that can make it run faster,
on arc inconsistent problems, than the generalized arc consistency algorithm.
• We then consider search algorithms that maintain local consistencies during search
in the hidden variable encoding. We show that, like maintaining arc consistency,
all the generalizations of forward checking to non-binary CSPs can be emulated by
corresponding forward checking algorithms that run in the hidden variable encoding
and only instantiate original variables (i.e. the variables of the initial non-binary
problem). We show that each such algorithm and its corresponding algorithm for nonbinary constraints have the following relationships: 1) they visit the same number of
search tree nodes, and 2) the asymptotic cost of each of them is within a polynomial
bound of the other.
642

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

• We describe a specialized algorithm for the dual encoding that achieves arc consistency
with O(e3 dk ) worst-case time complexity. This is signiﬁcantly lower than the O(e2 d2k )
complexity of a generic arc consistency algorithm. The improvement in the complexity
bound stems from the observation that constraints in the dual encoding have a speciﬁc
structure; namely they are piecewise functional (Van Hentenryck, Deville, & Teng,
1992). Apart from applying arc consistency in the dual encoding of a non-binary
CSP, this algorithm can also be used as a specialized ﬁltering algorithm for certain
classes of non-binary constraints.
• We adapt various search algorithms to run in the double encoding and compare them
theoretically to similar algorithms for the hidden variable encoding and the non-binary
representation. Search algorithms that operate in the double encoding can exploit the
advantages of both the hidden variable and the dual encoding. For example, we show
that, under certain conditions, the asymptotic cost of the maintaining arc consistency
algorithm in the double encoding can only be polynomially worse than the asymptotic
cost of the corresponding algorithm in the non-binary representation (and the hidden
variable encoding), while it can be exponentially better.
• Finally, we make an extensive empirical study on various domains. We consider
random problems as well as structured ones, like crossword puzzle generation, conﬁguration, and frequency assignment. This study consists of two parts. In the ﬁrst
part, we give experimental results that demonstrate the advantages of the specialized
algorithms for binary encodings compared to generic algorithms. For example, the
specialized arc consistency algorithm for the dual encoding can be orders of magnitude faster than a generic arc consistency algorithm. In the second part we show that
the use of binary encodings can oﬀer signiﬁcant beneﬁts when solving certain classes
of non-binary CSPs. For example, solving the dual encoding of some conﬁguration
problems can be orders of magnitudes more eﬃcient than solving the non-binary
representation. Also, empirical results from frequency assignment - like problems
demonstrate that a binary encoding can be beneﬁcial even for non-binary constraints
that are intentionally speciﬁed.

This paper is structured as follows. In Section 2 we give the necessary deﬁnitions and
background. In Section 3 we describe a specialized arc consistency algorithm for the hidden
variable encoding. We also demonstrate that all the extensions of forward checking to
non-binary CSPs can be emulated by binary forward checking algorithms that run in the
hidden variable encoding. In Section 4 we explain how the complexity of arc consistency
in the dual encoding can be improved and describe a specialized arc consistency algorithm.
Section 5 discusses algorithms for the double encoding. In Section 6 we present experimental
results on random and structured problems that demonstrate the usefulness of the proposed
algorithms. We also draw some conclusions regarding the applicability of the encodings,
based on theoretical and experimental results. Section 7 discusses related work. Finally, in
Section 8 we conclude.
643

Samaras & Stergiou

2. Background
In this section we give some necessary deﬁnitions on CSPs, and describe the hidden variable,
dual, and double encodings of non-binary CSPs.
2.1 Basic Definitions
A Constraint Satisfaction Problem (CSP), P , is deﬁned as a tuple (X, D, C), where:
• X = {x1 , . . . , xn } is a ﬁnite set of n variables.
• D = {Din (x1 ), . . . , Din (xn )} is a set of initial domains. For each variable xi ∈ X,
Din (xi ) is the initial ﬁnite domain of its possible values. CSP algorithms remove
values from the domains of variables through value assignments and propagation. For
any variable xi , we denote by D(xi ) the current domain of xi that at any time consists
of values that have not been removed from Din (xi ). We assume that for every xi ∈ X,
a total ordering <d can be deﬁned on Din (xi ).
• C = {c1 , . . . , ce } is a set of e constraints. Each constraint ci
a pair (vars(ci ), rel(ci )), where 1) vars(ci ) = {xj1 , . . . , xjk } is
of X called the constraint scheme, 2) rel(ci ) is a subset of the
Din (xj1 )x . . . xDin (xjk ) that speciﬁes the allowed combinations of
ables in vars(ci ).

∈ C is deﬁned as
an ordered subset
Cartesian product
values for the vari-

The size of vars(ci ) is called the arity of the constraint ci . Constraints of arity 2 are called
binary. Constraints of arity greater than 2 are called non-binary (or n-ary). Each tuple
τ ∈ rel(ci ) is an ordered list of values (a1 , . . . , ak ) such that aj ∈ Din (xj ),j = 1, . . . , k. A
tuple τ = (a1 , . . . , ak ) is valid if ∀ aj , j ∈ 1, . . . , k, aj ∈ D(xj ). That is, a tuple is valid if
all the values in the tuple are present in the domains of the corresponding variables. The
process which veriﬁes whether a given tuple is allowed by a constraint ci or not is called a
consistency check. A constraint can be either deﬁned extensionally by the set of allowed (or
disallowed) tuples or intensionally by a predicate or arithmetic function. A binary CSP can
be represented by a graph (called constraint graph) where nodes correspond to variables
and edges correspond to constraints. A non-binary CSP can be represented by a constraint
hyper-graph where the constraints correspond to hyper-edges connecting two or more nodes.
The assignment of value a to variable xi will be denoted by (xi , a). Any tuple τ =
(a1 , . . . , ak ) can be viewed as a set of value to variable assignments {(x1 , a1 ), . . . , (xk , ak )}.
The set of variables over which a tuple τ is deﬁned will be denoted by vars(τ ). For any
subset vars of vars(τ ), τ [vars ] denotes the sub-tuple of τ that includes only assignments to
the variables in vars . Any two tuples τ and τ  of rel(ci ) can be ordered by the lexicographic
ordering <lex . In this ordering, τ <lex τ  iﬀ there a exists a subset {x1 , . . . , xj } of ci such
that τ [x1 , . . . , xj ] = τ  [x1 , . . . , xj ] and τ [xj+1 ] <lex τ  [xj+1 ]. An assignment τ is consistent,
if for all constraints ci , where vars(ci ) ⊆ vars(τ ), τ [vars(ci )] ∈ rel(ci ). A solution to a CSP
(X, D, C) is a consistent assignment to all variables in X. If there exists a solution for a
given CSP, we say that the CSP is soluble. Otherwise, it is insoluble.
A basic way of solving CSPs is by using backtracking search. This can be seen as a
traversal of a search tree which comprises of the possible assignments of values to variables.
644

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

Each level of the tree corresponds to a variable. A node in the search tree corresponds to
a tuple (i.e. an assignment of values to variables). The root of the tree corresponds to
the empty tuple, the ﬁrst level nodes correspond to 1-tuples (an assignment of a value to
one variable), the second level nodes correspond to 2-tuples (assignment of values to two
variables generated by extending the ﬁrst level 1-tuples) etc. At each stage of the search
tree traversal, the variables that have been already assigned are called past variables. The
most recently assigned variable is called current variable. The variables that have not been
assigned yet are called future variables.
In the rest of this paper we will use the notation n for the number of variables in a
CSP, e for the number of constraints in the problem, d for the maximum domain size of the
variables, and k for the maximum arity of the constraints.
2.1.1 Arc Consistency
An important concept in CSPs is the concept of local consistency. Local consistencies are
properties that can be applied in a CSP, using (typically) polynomial algorithms, to remove
inconsistent values either prior to or during search. Arc consistency is the most commonly
used local consistency property in the existing constraint programming engines. We now
give a deﬁnition of arc consistency.
Definition 2.1 A value a ∈ D(xj ) is consistent with a constraint ci , where xj ∈ vars(ci ) if
∃τ ∈ rel(ci ) such that τ [xj ] = a and τ is valid. In this case we say that τ is a support of a in
ci . A constraint ci is Arc Consistent (AC) iﬀ for each variable xj ∈ vars(ci ), ∀ a ∈ D(xj ),
there exists a support for a in ci . A CSP (X, D, C) is arc consistent iﬀ there is no empty
domain in D and all the constraints in C are arc consistent.
Arc consistency can be enforced on a CSP by removing all the unsupported values from
the domains of variables. By enforcing arc consistency (or some local consistency property
A in general) on a CSP P , we mean applying an algorithm that yields a new CSP that is
arc consistent (or has the property A) and has the same set of solutions as P . The above
deﬁnition of arc consistency applies to constraints of any arity. To distinguish between
the binary and non-binary cases, we will use the term arc consistency (AC) to refer to the
property of arc consistency for binary constraints only. For non-binary constraints we will
use the term Generalized Arc Consistency (GAC).
The usefulness of AC processing was recognized early, and as a result, various AC
algorithms for binary constraints have been proposed in the literature (e.g. AC-3 in Mackworth, 1977, AC-4 in Mohr & Henderson, 1986, AC-5 in Van Hentenryck et al., 1992, AC-7
in Bessière et al., 1995, AC-2001 in Bessière & Régin, 2001, AC3.1 in Zhang & Yap, 2001).
Some of them have been extended to the non-binary case (e.g. GAC-4 in Mohr & Masini,
1988, GAC-Schema in Bessière & Régin, 1996a, GAC-2001 in Bessière & Régin, 2001).
AC can be enforced on a binary CSP with O(ed2 ) optimal worst-case time complexity. The
worst-case complexity of enforcing GAC on a non-binary CSP is O(ekdk ) (Bessière & Régin,
1996a).
In this paper we use algorithms AC-2001 and GAC-2001 for theoretical and empirical
comparisons with specialized algorithms for the encodings. This is not restrictive, in the
sense that any generic AC (and GAC) algorithm can be used instead.
645

Samaras & Stergiou

Following Debruyne & Bessiére (2001), we call a local consistency property A stronger
than B iﬀ for any problem enforcing A deletes at least the same values as B, and strictly
stronger iﬀ it is stronger and there is at least one problem where A deletes more values than
B. We call A equivalent to B iﬀ they delete the same values for all problems. Similarly, we
call a search algorithm A stronger than a search algorithm B iﬀ for every problem A visits
at most the same search tree nodes as B, and strictly stronger iﬀ it is stronger and there
is at least one problem where A visits less nodes than B. A is equivalent to B iﬀ they visit
the same nodes for all problems.
Following Bacchus et al. (2002), the asymptotic cost (or just cost hereafter) of a search
algorithm A is determined by the worst-case number of nodes that the algorithm has to
visit to solve the CSP, and the worst-case time complexity of the algorithm at each node1 .
As in the paper by Bacchus et al. (2002), we use this measure to set asymptotic bounds
in the relative performance of various algorithms. For example, if two algorithms A and
B always visit the same nodes and A enforces a property at each node with exponentially
higher complexity than the property enforced by B, then we say that algorithm A can have
an exponentially greater cost than algorithm B.
2.1.2 Functional and Piecewise Functional Constraints
The specialized AC algorithms for the hidden variable and the dual encoding that we will
describe in Sections 3 and 4 exploit structural properties of the encodings. As we will
explain in detail later, the binary constraints in the hidden variable encoding are one-way
functional, while the binary constraints in the dual encoding are piecewise functional. We
now deﬁne these concepts.
Definition 2.2 A binary constraint c, where vars(c) = {xi , xj }, is functional with respect
to D(xi ) and D(xj ) iﬀ for all a ∈ D(xi ) (resp. b ∈ D(xj )) there exists at most one value
b ∈ D(xj ) (resp. a ∈ D(xi )) such that b is a support of a in c (resp. a is a support of b).
An example of a functional constraint is xi = xj . A binary constraint is one-way functional
if the functionality property holds with respect to only one of the variables involved in the
constraint.
Informally, a piecewise functional constraint over variables xi , xj is a constraint where
the domains of xi and xj can be partitioned into groups such that each group of D(xi ) is
supported by at most one group of D(xj ), and vice versa. To give a formal deﬁnition, we
ﬁrst deﬁne the concept of a piecewise decomposition.
Definition 2.3 (Van Hentenryck et al., 1992) Let c be a binary constraint with vars(c) =
{xi , xj }. The partitions S = {s1 , . . . , sm } of D(xi ) and S  = {s1 , . . . , sm } of D(xj ) are a
piecewise decomposition of D(xi ) and D(xj ) with respect to c iﬀ for all sl ∈ S,sl ∈ S  , the
following property holds: either for all a ∈ sl , b ∈ sl , (a, b) ∈ rel(c), or for all a ∈ sl , b ∈ sl ,
(a, b) ∈
/ rel(c).
1. In the paper by Bacchus et al. (2002) the cost of applying a variable ordering heuristic at each node is
also taken into account. When we theoretically compare search algorithms in this paper we assume that
they use the same variable ordering, so we do not take this cost into account.

646

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

Definition 2.4 (Van Hentenryck et al., 1992) A binary constraint c, where vars(c) =
{xi , xj }, is piecewise functional with respect to D(xi ) and D(xj ) iﬀ there exists a piecewise
decomposition S = {s1 , . . . , sm } of D(xi ) and S  = {s1 , . . . , sm } of D(xj ) with respect to
c such that for all sl ∈ S (resp. sl ∈ S  ), there exists at most one sl ∈ S  (resp. sl ∈ S),
such that for all a ∈ sl , b ∈ sl (a, b) ∈ rel(c).
Example of piecewise functional constraints are the modulo (x2 MOD x3 = a) and integer
division (x2 DIV x3 = a) constraints.
2.2 Binary Encodings
There are two well-known methods for transforming a non-binary CSP into a binary one;
the dual graph encoding and the hidden variable encoding. Both encode the non-binary
constraints to variables that have as domains the valid tuples of the constraints. That is, by
building a binary encoding of a non-binary constraint we store the extensional representation
of the constraint (the set of allowed tuples). A third method is the double encoding which
combines the other two.
2.2.1 Dual Encoding
The dual encoding (originally called dual graph encoding) was inspired by work in relational
databases. In the dual encoding (DE) (Dechter & Pearl, 1989) the variables are swapped
with constraints and vice versa. Each constraint c of the original non-binary CSP is represented by a variable which we call a dual variable and denote by vc . We refer to the variables
of the original non-binary CSP as original variables. The domain of each dual variable vc
consists of the set of allowed tuples in the original constraint c. Binary constraints between
two dual variables vc and vc exist iﬀ vars(c) ∩ vars(c ) = ∅. That is, iﬀ the constraints c
and c share one or more original variables. If common vars is the set of original variables
common to c and c then a tuple τ ∈ D(vc ) is supported in the constraint between vc and
vc iﬀ there exists a tuple τ  ∈ D(vc ) such that τ [common vars] = τ  [common vars].

v c1

v c4

(0,0,1) (0,1,0)

(0,0,0) (0,1,1)

(1,0,0)

(1,0,1)

(0,0,1) (1,0,0)

(0,1,0) (1,0,0)

(1,1,1)

(1,1,0) (1,1,1)
v c3

v c2

Figure 1: Dual encoding of a non-binary CSP.
647

Samaras & Stergiou

Consider the following example with six variables with 0-1 domains, and four constraints:
c1 : x1 + x2 + x6 = 1, c2 : x1 − x3 + x4 = 1, c3 : x4 + x5 − x6 ≥ 1, and c4 : x2 + x5 −
x6 = 0. The DE represents this problem with 4 dual variables, one for each constraint.
The domains of these dual variables are the tuples that satisfy the respective constraint.
For example, the dual variable vc3 associated with the third constraint has the domain
{(0, 1, 0), (1, 0, 0), (1, 1, 0), (1, 1, 1)} as these are the tuples of values for (x4 , x5 , x6 ) which
satisfy x4 + x5 − x6 ≥ 1. As a second example, the dual variable vc4 associated with the
last constraint has the domain {(0, 0, 0), (0, 1, 1), (1, 0, 1)}. Between vc3 and vc4 there is a
compatibility constraint to ensure that the two original variables in common, x5 and x6 ,
have the same values. This constraint allows only those pairs of tuples which agree on the
second and third elements (i.e. (1, 0, 0) for vc3 and (0, 0, 0) for vc4 , or (1, 1, 1) for vc3 and
(0, 1, 1) for vc4 ). The DE of the problem is shown in Figure 1.
In the rest of this paper, we will sometimes denote by cvi the non-binary constraint
that is encoded by dual variable vi . For an original variable xj ∈ vars(cvi ), pos(xj , cvi ) will
denote the position of xj in cvi . For instance, given a constraint cvi on variables x1 , x2 , x3 ,
pos(x2 , cvi ) = 2.
2.2.2 Hidden Variable Encoding
The hidden variable encoding (HVE) was inspired by the work of philosopher Peirce (1933).
According to Rossi et al. (1990), Peirce ﬁrst showed that binary relations have the same
expressive power as non-binary relations.
In the HVE (Rossi et al., 1990), the set of variables consists of all the original variables
of the non-binary CSP plus the set of dual variables. As in the dual encoding, each dual
variable vc corresponds to a constraint c of the original problem. The domain of each dual
variable consists of the tuples that satisfy the original constraint. For every dual variable
vc , there is a binary constraint between vc and each of the original variables xi such that
xi ∈ vars(c). A tuple τ ∈ D(vc ) is supported in the constraint between vc and xi iﬀ there
exists a value a ∈ D(xi ) such that τ [xi ] = a.
Consider the previous example with six variables with 0-1 domains, and four constraints:
c1 : x1 + x2 + x6 = 1, c2 : x1 − x3 + x4 = 1, c3 : x4 + x5 − x6 ≥ 1, and c4 : x2 + x5 − x6 = 0.
In the HVE there are, in addition to the original six variables, four dual variables. As in
the DE, the domains of these variables are the tuples that satisfy the respective constraint.
There are now compatibility constraints between a dual variable vc and the original variables
contained in constraint c. For example, there are constraints between vc3 and x4 , between
vc3 and x5 and between vc3 and x6 , as these are the variables involved in constraint c3 . The
compatibility constraint between cv3 and x4 is the relation that is true iﬀ the ﬁrst element
in the tuple assigned to cv3 equals the value of x4 . The HVE is shown in Figure 2.
2.2.3 Double Encoding
The double encoding (Stergiou & Walsh, 1999) combines the hidden variable and the dual
encoding. As in the HVE, the set of variables in the double encoding consists of all the
variables of the original non-binary CSP plus the dual variables. For every dual variable
vc , there is a binary constraint between vc and each of the original variables xi involved in
the corresponding non-binary constraint c. As in the DE, there are also binary constraints
648

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

v c1

v c4

(0,0,1) (0,1,0)

(0,0,0) (0,1,1)

(1,0,0)

(1,0,1)

x1 0 1

x2 0 1

x3 0 1

x4 0 1

(0,0,1) (1,0,0)

x5

01

x6

01

(0,1,0) (1,0,0)

(1,1,1)

(1,1,0) (1,1,1)
v c3

v c2

Figure 2: Hidden variable encoding of a non-binary CSP.
between two dual variables vc and vc if the non-binary constraints c and c share one or
more original variables.

3. Algorithms for the Hidden Variable Encoding
In this section we discuss specialized algorithms for the HVE. We ﬁrst describe a simple
AC algorithm for the HVE that has the same worst-case time complexity as an optimal
GAC algorithm for the non-binary representation. In Appendix A, we also show that for
any arc consistent CSP the proposed AC algorithm performs exactly the same number of
consistency checks as the corresponding GAC algorithm. For arc inconsistent problems we
show that the AC algorithm for the HVE can detect the inconsistency earlier and thus
perform fewer consistency checks than the GAC algorithm.
We also consider search algorithms for the HVE that maintain local consistencies during
search. We show that, like maintaining arc consistency, the generalizations of forward
checking to non-binary CSPs can be emulated by corresponding binary forward checking
algorithms in the HVE that only instantiate original variables.
3.1 Arc Consistency
It has been proved that AC in the HVE is equivalent to GAC in the non-binary problem
(Stergiou & Walsh, 1999). Since the HVE is a binary CSP, one obvious way to apply AC
is by using a generic AC algorithm. However, this results in redundant processing and an
asymptotic time complexity worse than O(ekdk ). To be precise, in the HVE of a problem
with k−ary constraints we have ek binary constraints between dual and original variables.
On such a constraint, AC can be enforced with O(ddk ) worst-case time complexity. For the
whole problem the complexity is O(ekdk+1 ).
Instead, we will now describe a simple AC algorithm that operates in the HVE and
achieves the same worst-case time complexity as an optimal GAC algorithm applied in the
non-binary representation. We can achieve this by slightly modifying the GAC algorithm
649

Samaras & Stergiou

of Bessiére and Régin (2001) (GAC-2001). In Figure 3 we sketch the AC algorithm for the
HVE, which we call HAC (Hidden AC).
function HAC
1:
Q←∅
2:
for each dual variable vj
3:
for each variable xi where xi ∈ vars(cvj )
4:
if Revise(xi , vj ) = T RU E
5:
if D(xi ) is empty return INCONSISTENCY
6:
put in Q each dual variable vl such that xi ∈ vars(cvl )
7:
return P ropagation
function P ropagation
8:
while Q is not empty
9:
pop dual variable vj from Q
10:
for each unassigned variable xi where xi ∈ vars(cvj )
11:
if Revise(xi , vj ) = T RU E
12:
if D(xi ) is empty return INCONSISTENCY
13:
put in Q each dual variable vl such that xi ∈ vars(cvl )
14: return CONSISTENCY
function Revise(xi , vj )
15: DELETION ← FALSE
16:
for each value a ∈ D(xi )
17:
if currentSupportxi,a,vj is not valid
18:
if ∃ τ (∈ D(vj )) >lex currentSupportxi,a,vj , τ [xi ] = a and τ is valid
19:
currentSupportxi,a,vj ← τ
20:
else
21:
remove a from D(xi )
22:
for each vl such that xi ∈ vars(cvl )
23:
remove from D(vl ) each tuple τ  such that τ  [xi ] = a
24:
if D(vl ) is empty return INCONSISTENCY
25:
DELETION ← TRUE
26: return DELETION
Figure 3: HAC: an AC algorithm for the hidden variable encoding.
The HAC algorithm uses a stack (or queue) of dual variables to propagate value deletions, and works as follows. In an initialization phase it iterates over each dual variable
vj (line 2). For every original variable xi constrained with vj the algorithm revises the
constraint between vj and xi . This is done by calling function Revise (line 4). During each
revision, for each value a of D(xi ) we look for a tuple in the domain of vj that supports
it. As in AC-2001, we store currentSupportxi,a,vj : the most recent tuple we have found in
D(vj ) that supports value a of variable xi 2 . If this tuple has not been deleted from D(vj )
2. We assume, without loss of generality, that the algorithm looks for supports by checking the tuples in
lexicographic order.

650

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

then we know that a is supported. Otherwise, we look for a new supporting tuple starting
from the tuple immediately after currentSupportxi ,a,vj . If no such tuple is found then a is
removed from D(xi ) (line 21). In that case, all tuples that include that value are removed
from the domains of the dual variables that are constrained with xi (lines 22–23). If these
dual variables are not already in the stack they are added to it3 . Then, dual variables are
removed from the stack sequentially. For each dual variable vj that is removed from the
stack, the algorithm revises the constraint between vj and each original variable xi constrained with vj . The algorithm terminates if all the values in a domain are deleted, in
which case the problem is not arc consistent, or if the stack becomes empty, in which case
the problem is arc consistent.
The main diﬀerence between HAC and GAC-2001 is that GAC-2001 does not include
lines 22–24. That is, even if the non-binary constraints are given in extension, GAC2001 does not remove tuples that become invalid from the lists of allowed tuples. As a
result, the two algorithms check for the validity of a tuple (in lines 17 and 18) in diﬀerent
ways. Later on in this section we will explain this in detail. Apart from this diﬀerence,
which is important because it aﬀects their run times, the two algorithms are essentially the
same. We can move from HAC to GAC-2001 by removing lines 22–24 and substituting any
references to dual variables by references to the corresponding constraints. For example,
currentSupportxi,a,vj corresponds to currentSupportxi,a,cvj in GAC-2001, i.e. the last tuple
in constraint cvj that supports value a of variable xi . Note that in such an implementation
of GAC-2001, propagation is constraint-based. That is, the algorithm utilizes a stack of
constraints to perform the propagation of value deletions.
3.1.1 Complexities
We now give a upper bound on the number of consistency checks performed by HAC in the
worst-case. Function Revise(xi , vj ) can be called at most kd times for each dual variable
vj , once for every deletion of a value from the domain of xi , where xi is one of the k original
variables constrained with vj . In each call to Revise(xi , vj ) the algorithm performs at most
d checks (one for each value a ∈ D(xi )) to see if currentSupportxi,a,vj is valid (line 17). If
currentSupportxi,a,vj is not valid, HAC tries to ﬁnd a new supporting tuple for a in D(vj ).
To check if a tuple τ that contains the assignment (xi , a) supports a we need to check if τ
is valid. If a tuple is not valid then one of its values has been removed from the domain
of the corresponding variable. This means that the tuple has also been removed from the
domain of the dual variable. Therefore, checking the validity of a tuple can be done in
constant time by looking in the domain of the dual variable. The algorithm only needs
to check for support the dk−1 , at maximum, tuples that contain the assignment (xi , a).
Since HAC stores currentSupportxi,a,vj , at each call of Revise(xi , vj ) and for each value
a ∈ D(xi ), it only checks tuples that have not been checked before. In other words, we can
check each of the dk−1 tuples at most once for each value of xi . So overall, in the worst
case, we have dk−1 checks plus the d checks to test the validity of the current support. For
kd values the upper bound in checks performed by HAC to make one dual variable AC is
3. Note that dual variables that are already in the stack are never added to it. In this sense, the stack is
implemented as a set.

651

Samaras & Stergiou

O(kd(d + dk−1 ))=O(kdk ). For e dual variables the worst-case complexity bound is O(ekdk ),
which is the same as the complexity of GAC in the non-binary representation.
The asymptotic space complexity of the HAC algorithm is dominated by the O(edk )
space needed to store the domains of the dual variables. The algorithm also requires O(nde)
space to store the current supports. Since the space required grows exponentially with the
arity of the constraints, it is reasonable to assume that the HVE (and the other binary
encodings) cannot be practical for constraints of large arity, unless the constraints are very
tight.
As mentioned, a consistency check in the non-binary representation is done in a diﬀerent
way than in the HVE. Assume that GAC-2001 looks for a support for value ai ∈ D(xi ) in
constraint c, where vars(c) = {x1 , . . . , xk } and xi ∈ vars(c). A tuple τ = (a1 , . . . , ak )
supports ai if τ [xi ] = ai and τ is valid. To check if τ is valid, GAC-2001 has to check if
values a1 , . . . , ak (except ai ) are still in the domains of variables x1 , . . . , xk . Therefore, in
the worst case, a consistency check by GAC-2001 involves k − 1 operations. In contrast,
HAC checks for the validity of a tuple in constant time by looking in the domain of the
corresponding dual variable to see if the tuple is still there. However, this means that the
algorithm has to update the (usually) large domains of the dual variables after a value
deletion from an original variable. This aﬀects the run times of the algorithms in diﬀerent
problems settings.
In Appendix A we show that HAC does not only have the same complexity, but it also
performs exactly the same number of consistency checks as GAC-2001 in arc consistent
problems. We also show that in arc inconsistent problems there can be a diﬀerence in the
number of checks in favor of the HVE.
3.2 Search Algorithms
Search algorithms that maintain local consistencies are widely used for CSP solving. Some of
them have been extended to the non-binary case. For example, maintaining arc consistency
(MAC) and forward checking (FC). It has been shown that the non-binary version of MAC
(MGAC) applied in a non-binary CSP is equivalent to MAC applied in the HVE of the
CSP when only original variables are instantiated and the same variable orderings are used
(Stergiou & Walsh, 1999). We show that, like MGAC, non-binary extensions of FC can be
emulated by equivalent algorithms that run in the HVE.
FC (Haralick & Elliot, 1980) was ﬁrst generalized to handle non-binary constraints by
Van Hentenryck (1989). According to the deﬁnition of Van Hentenryck (1989), forward
checking is performed after the k-1 variables of an k-ary constraint have been assigned and
the remaining variable is unassigned. This algorithm is called nFC0 in the paper by Bessiére,
Meseguer, Freuder, & Larrosa (2002) where more, and stronger, generalizations of FC to
non-binary constraints were introduced. These generalizations diﬀer between them in the
extent of look-ahead they perform after each variable instantiation. Algorithm nFC1 applies
one pass of GAC on each constraint or constraint projection involving the current variable
and exactly one future variable4 . Algorithm nFC2 applies GAC on the set of constraints
involving the current variable and at least one future variable, in one pass. Algorithm nFC3
applies GAC to the set of constraints involving the current variable and at least one future
4. “One pass” means that each constraint is processed only once.

652

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

variable. Algorithm nFC4 applies GAC on the set of constraints involving at least one
past variable and at least one future variable, in one pass. Algorithm nFC5, which is the
strongest version, applies GAC to the set of constraints involving at least one past variable
and at least one future variable. All the generalizations reduce to simple FC when applied
to binary constraints.
We will show that the various versions of nFC are equivalent, in terms of visited nodes,
to binary versions of FC that run in the HVE of the problem. This holds under the
assumption that the binary algorithms only assign original variables and they use the same
variable and value ordering heuristics, static or dynamic, as their non-binary counterparts.
Note that if such an algorithm ﬁnds a consistent assignment for all original variables, and
these assignments are propagated to the dual variables, then all the domains of the dual
variables will be reduced to singletons. That is, the domain of each dual variable vc will
only contain the single tuple that is consistent with the assignments of the original variables
constrained with vc . Therefore, the algorithm can proceed to assign the dual variables in a
backtrack-free manner.
The equivalence between nFC1 and a version of FC for the HVE, called FC+ (Bacchus
& van Beek, 1998), has been proved by Bessiére et al. (2002). FC+ is a specialized forward
checking algorithm for the HVE. It operates like standard binary FC except that when the
domain of a dual variable is pruned, FC+ removes from adjacent original variables any
value which is no longer supported.
Algorithms nFC2-nFC5 also have equivalent algorithms that operate in the HVE. We
call these algorithms hFC2–hFC5. For example, hFC5 will enforce AC on the set of dual
variables, and original variables connected to them, such that each dual variable is connected
to at least one past original variable and at least one future original variable. Note that
nFC0 has no natural equivalent algorithm in the HVE. If we emulate it in the HVE we will
get an ineﬃcient and awkward algorithm. In the following, hFC0 will refer to the standard
binary FC algorithm and hFC1 will refer to FC+.

Proposition 3.1 In any non-binary CSP, under a ﬁxed variable and value ordering, algorithm nFCi, i= 2, . . . 5, is equivalent to algorithm hFCi that operates in the hidden variable
encoding of the problem.
Proof: We prove this for nFC5, the strongest among the generalized FC algorithms.
The proof for the other versions is similar. We only need to prove that at each node of
the search tree algorithms nFC5 and hFC5 will delete exactly the same values from the
domains of original variables. Assume that at some node, after instantiating the current
variable, nFC5 deletes value a from a future variable xi . Then there exists some constraint
c including xi and at least one past variable, and value a of xi has no supporting tuple in c.
In the HVE, when hFC5 tries to make vc (the dual variable corresponding to c) AC it will
remove all tuples that assign a to xi . Hence, hFC5 will delete a from the domain of xi . Now
in the opposite case, if hFC5 deletes value a from an original variable xi it means that all
tuples including that assignment will be removed from the domains of dual variables that
include xi and at least one past variable. In the non-binary representation of the problem,
the assignment of a to xi will not have any supporting tuples in constraints that involve xi
and at least one past variable. Therefore, nFC5 will delete a from the domain of xi . 2
653

Samaras & Stergiou

Algorithms nFC2–nFC5 are not only equivalent in node visits with the corresponding algorithms hFC2–hFC5, but they also have the same asymptotic cost. This holds under the
condition that the non-binary algorithms use GAC-2001 (or some other optimal algorithm)
to enforce GAC, and the HVE versions use algorithm HAC.
Proposition 3.2 In any non-binary CSP, under a ﬁxed variable and value ordering, algorithm nFCi, i= 2, . . . 5, has the same asymptotic cost as algorithm hFCi that operates in
the hidden variable encoding of the problem.
Proof: In Section 3.1 we showed that we can enforce AC on the HVE of a non-binary
CSP with the same worst-case complexity as GAC in the non-binary representation of the
problem. Since algorithm nFCi enforces GAC on the same part of the problem on which
algorithm hFCi enforces AC, and they visit the same nodes of the search tree, it follows
that the two algorithm have the same asymptotic cost. 2
In the paper by Bessiére et al. (2002), a detailed discussion on the complexities of algorithms nFC0–nFC5 is made. The worst-case complexity of nFC2 and nFC3 in one node is
O(|Cc,f |(k − 1)dk−1 ), where |Cc,f | is the number of constraints involving the current variable and at least one future variable. This is also the complexity of hFC3 and hFC4. The
worst-case complexity of nFC4 and nFC5 in one node is O(|Cp,f |(k − 1)dk−1 ), where |Cp,f |
is the number of constraints involving at least one past variable and at least one future
variable. This is also the complexity of hFC4 and hFC5.
Assuming that nodes(algi ) is the set of search tree nodes visited by search algorithm
algi then the following holds.
Corollary 3.1 Given the hidden variable encoding of a CSP with ﬁxed variable and value
ordering schemes, the following relations hold:
1. nodes(hFC1)⊆ nodes(hFC0)
2. nodes(hFC2)⊆ nodes(hFC1)
3. nodes(hFC5)⊆ nodes(hFC3)⊆ nodes(hFC2)
4. nodes(hFC5)⊆ nodes(hFC4)⊆ nodes(hFC2)
5. nodes(MAC)⊆ nodes(hFC5)
Proof: The proof of 1 is straightforward, see the paper by Bacchus & van Beek (1998).
Proof of 2-4 is a straightforward consequence of Proposition 3.1 and Corollary 2 from the
paper by Bessiére et al. (2002) where the hierarchy of algorithms nFC0-nFC5 in node visits
is given. It is easy to see that 5 holds since hFC5 applies AC in only a part of the CSP,
while MAC applies it in the whole problem. Therefore, MAC will prune at least as many
values as hFC5 at any given node of the search tree. Since the same variable and value
ordering heuristics are used, this means that MAC will visit at most the same number of
nodes as hFC5. 2
Note that in the paper by Bacchus & van Beek (1998) experimental results show differences between FC in the HVE and FC in the non-binary representation. However, the
654

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

algorithms compared there were FC+ and nFC0, which are not equivalent. Also, it has
been proved that hFC0 can have an exponentially greater cost than nFC0, and vice versa
(Bacchus et al., 2002). However, these algorithms are not equivalent. As proved in Proposition 3.2, the result of Bacchus et al. (2002) does not hold when comparing equivalent
algorithms.
So far we have showed that solving a non-binary CSP directly is in many ways equivalent
to solving it using the HVE, assuming that only original variables are instantiated. A
natural question is whether there are any search techniques which are inapplicable in the
non-binary case, but can be applied in the encoding. The answer is the ability of a search
algorithm that operates on the encoding to instantiate dual variables. In the equivalent
non-binary representation this would imply instantiating values of more than one variables
simultaneously. To implement such an algorithm we would have to modify standard search
algorithms and heuristics or devise new ones. On the other hand, in the HVE an algorithm
that instantiates dual variables can be easily implemented.

4. Algorithms for the Dual Encoding
In this section we turn our attention to the DE and describe a specialized AC algorithm
with signiﬁcantly lower complexity than a generic algorithm.
4.1 Arc Consistency
We know that AC in the DE is strictly stronger than GAC in the non-binary representation
and AC in the HVE (Stergiou & Walsh, 1999). Since the DE is a binary CSP, one obvious
way to apply AC is using a generic AC algorithm. The domain size of a dual variable
corresponding to a k−ary constraint is dk in the worst case. Therefore, if we apply an
optimal AC algorithm then we can enforce AC on one dual constraint with O(d2k ) worstcase complexity. In the DE of a CSP with e constraints of maximum arity k there are
at most e(e − 1)/2 binary constraints (when all pairs of dual variables share one or more
original variables). Therefore, we can enforce AC in the DE of the CSP with O(e2 d2k ) worstcase complexity. This is signiﬁcantly more expensive compared to the O(ekdk ) complexity
bound of GAC in the non-binary representation and AC in the HVE. Because of the very
high complexity bound, AC processing in the DE is considered to be impractical, except
perhaps for very tight constraints.
However, we will now show that AC can be applied in the DE much more eﬃciently. To
be precise we can enforce AC on the DE of a non-binary CSP with O(e3 dk ) worst-case time
complexity. The improvement in the asymptotic complexity can be achieved by exploiting
the structure of the DE; namely, the fact that the constraints in the DE are piecewise
functional.
Consider a binary constraint between dual variables vi and vj . We can create a piecewise
decomposition of the tuples in the domain of either dual variable into groups such that all
tuples in a group are supported by the same group of tuples in the other variable. If the
non-binary constraints corresponding to the two dual variables share f original variables
x1 , . . . , xf of domain size d, then we can partition the tuples of vi and vj into df groups.
Each tuple in a group s includes the same sub-tuple of the form (a1 , . . . , af ), where a1 ∈
D(x1 ), . . . , af ∈ D(xf ). Each tuple τ in s will be supported by all tuples in a group s of
655

Samaras & Stergiou

the other variable, where each tuple in s also includes the sub-tuple (a1 , . . . , af ).The tuples
belonging to s will be the only supports of tuple τ since any other tuple does not contain
the sub-tuple (a1 , . . . , af ). In other words, a group of tuples s in variable vi will only be
supported by a corresponding group s in variable vj where the tuples in both groups have
the same values for the original variables that are common to the two encoded non-binary
constraints. Therefore, the constraints in the DE are piecewise functional.
Example 4.1 Assume that we have two dual variables v1 and v2 . v1 encodes constraint
(x1 , x2 , x3 ), and v2 encodes constraint (x1 , x4 , x5 ), where the original variables
x1 , . . . , x5 have the domain {0, 1, 2}. We can partition the tuples in each dual variable
into 3 groups. The ﬁrst group will include tuples of the form (0, ∗, ∗), the second will include tuples of the form (1, ∗, ∗), and the third will include tuples of the form (2, ∗, ∗). A
star (∗) means that the corresponding original variable can take any value. Each group is
supported only by the corresponding group in the other variable. Note that the tuples of a
variable vi are partitioned in diﬀerent groups according to each constraint that involves vi .
For instance, if there is another dual variable v3 encoding constraint (x6 , x7 , x3 ) then the
partition of tuples in D(v1 ) according to the constraint between v1 and v3 is into groups of
the form (∗, ∗, 0), (∗, ∗, 1), (∗, ∗, 2).
Van Hentenryck, Deville & Teng (1992) have shown that AC can be achieved in a
set of binary piecewise functional constraints with O(ed) worst-case time complexity, an
improvement of O(d) compared to the O(ed2 ) complexity of arbitrary binary constraints
(Van Hentenryck et al., 1992). Since we showed that the constraints in the DE are piecewise
functional, the result of Van Hentenryck et al. (1992) means that we can improve on the
O(e2 d2k ) complexity of AC in the DE.
In Figure 4 we sketch an AC-3 like AC algorithm speciﬁcally designed for the DE,
which we call PW-AC (PieceWise Arc Consistency). As we will show, this algorithm has
a worst-case time complexity of O(e3 dk ). The same complexity bound can be achieved
by the AC-5 algorithm of Van Hentenryck et al. (1992), in its specialization to piecewise
functional constraints, with the necessary adaptations to operate in the DE. As do most
AC algorithms, PW-AC uses a stack (or queue) to propagate deletions from the domains
of variables. This stack processes groups of piecewise decompositions, instead of variables
or constraints as is usual in AC algorithms. We use the following notation:
• S(vi , vj ) = {s1 (vi , vj ), . . . , sm (vi , vj )} denotes the piecewise decomposition of D(vi )
with respect to the constraint between vi and vj . Each sl (vi , vj ), l = 1, . . . , m, is a
group of the partition.
• sup(sl (vi , vj )) denotes the group of S(vj , vi ) that can support group sl (vi , vj ) of
S(vi , vj ). As discussed, this group is unique.
• counter(sl (vi , vj )) holds the number of valid tuples that belong to group sl (vi , vj ) of
decomposition S(vi , vj ). That is, at any time the value of counter(sl (vi , vj )) gives the
current cardinality of the group.
• GroupOf (S(vi , vj ), τ ) is a function that returns the group of S(vi , vj ) where tuple τ
belongs. To implement this function, for each constraint between dual variables vi
656

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

function P W − AC
1:
Q←∅
2:
initialize all group counters to 0
3:
for each variable vi
4:
for each variable vj constrained with vi
5:
for each tuple τ ∈ D(vi )
6:
counter(GroupOf (S(vi , vj ), τ )) ← counter(GroupOf (S(vi , vj ), τ )) + 1
7:
for each variable vi
8:
for each variable vj constrained with vi
9:
for each group sl (vi , vj )
10:
if counter(sl (vi , vj )) = 0
11:
put sl (vi , vj ) in Q
12: return P ropagation
function P ropagation
13: while Q is not empty
14:
pop group sl (vi , vj ) from Q
15:
δ←∅
16:
δ ← Revise(vi , vj , sl (vi , vj ))
17:
if D(vj ) is empty return INCONSISTENCY
18:
for each group sl (vj , vk ) in δ put sl (vj , vk ) in Q
19: return CONSISTENCY
function Revise(vi , vj , sl (vi , vj ))
20: for each tuple τ ∈ D(vj ) where τ ∈ sup(sl (vi , vj ))
21:
remove τ from D(vj )
22:
for each group sl (vj , vk ) that includes τ
23:
counter(sl (vj , vk )) ← counter(sl (vj , vk )) − 1
24:
if counter(sl (vj , vk )) = 0
25:
add sl (vj , vk ) to δ
26: return δ
Figure 4: PW-AC. An AC algorithm for the dual encoding.

and vj we store the original variables shared by the non-binary constraints cvi and
cvj . Also, for each such original variable xl we store pos(xl , cvi ) and pos(xl , cvj ). In
this way the GroupOf function takes constant time.
• The set δ contains the groups that have their counter reduced to 0 after a call to
function Revise. That is, groups such that all tuples belonging to them have been
deleted.
The algorithm works as follows. In an initialization phase, for each group we count
the number of tuples it contains (lines 3–6). Then, for each variable vi we iterate over the
657

Samaras & Stergiou

variables vj that are constrained with vi . For each group sl (vi , vj ) of S(vi , vj ), we check if
sl (vi , vj ) is empty or not (line 10). If it is empty, it is added to the stack for propagation.
In the next phase, function P ropagation is called to delete unsupported tuples and
propagate the deletions (line 12). Once the previous phase has ﬁnished, the stack will
contain a number of groups with 0 cardinality. For each such group sl (vi , vj ) we must
remove all tuples belonging to group sup(sl (vi , vj )) since they have lost their support. This
is done by successively removing a group sl (vi , vj ) from the stack and calling function
Revise. Since group sup(sl (vi , vj )) has lost its support, each tuple τ ∈ D(xj ) that belongs
to sup(sl (vi , vj )) is deleted (lines 20–21). Apart from sup(sl (vi , vj )), tuple τ may also belong
to other groups that D(vj ) is partitioned in with respect to constraints between vj and other
variables. Since τ is deleted, the counters of these groups must be updated (i.e. reduced
by one). This is done in lines 22–23. In the implementation we use function GroupOf
to access the relevant groups. If the counter of such a group becomes 0 then the group is
added to the stack for propagation (lines 24–25 and 18). The process stops when either the
stack or the domain of a variable becomes empty. In the former case, the DE is AC, while
in the latter it is not.
The following example illustrates the advantage of algorithm PW-AC over both a generic
AC algorithm employed in the DE, and AC in the HVE (or GAC in the non-binary representation).
Example 4.2 Consider three constraints c1 , c2 , c3 as part of a CSP, where vars(c1 ) =
{x0 , x1 , x3 }, vars(c2 ) = {x2 , x3 , x4 }, vars(c3 ) = {x2 , x4 , x5 }. Assume that at some point
the domains of the variables in the DE of the problem are as shown in Figure 5 (disregarding
the original variables depicted with dashed lines). Assume that we try to enforce AC in the

x 2x 3x 4

x 0x 1x 3

vc1

0,0,0
0,1,0
0,1,3
1,0,1
1,0,2

vc2

0,0,0
0,1,1
0,2,1
0,3,1
1,1,0
1,2,0
1,3,0

x2

x 2x 4x 5

0,1
1,0,0
0,1,0

vc3

0,1
x4

Figure 5: Dual encoding of a non-binary CSP.
DE using algorithm AC-20015 . The algorithm will discover that the ﬁrst tuple in D(vc2 )
has no support in D(vc3 ) (there is no tuple with x2 = 0 and x4 = 0) and will delete it.
Because of this deletion, the ﬁrst two tuples in D(vc1 ) lose their support in D(vc2 ) and
AC-2001 must therefore look for new supports. For each of the two tuples of D(vc1 ) the
algorithm will check all the 6 remaining tuples in D(vc2 ) before discovering that there is
no support. As a result the two tuples will be deleted. Algorithm PW-AC, on the other
hand, will set the counter of the group where the ﬁrst tuple of D(vc2 ) belongs (according
to partition S(vc2 , vc3 )) to 0 once it deletes the tuple. This will result in a call to function
5. Note that we can construct similar examples for any generic AC algorithm.

658

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

Revise and an automatic deletion of the ﬁrst two tuples of D(vc1 ), saving a total of 2 × 6
checks.
Now consider the HVE of the problem. Applying AC on the HVE will have no eﬀect
because values 0 and 1 of x2 and x4 are both supported in D(vc2 ) and D(vc3 ). Therefore
there is no propagation through these variables, and as a result the two tuples of D(vc1 ) will
not be deleted. Similarly, there will be no propagation if we apply GAC in the non-binary
representation of the problem.
Note that the theoretical results regarding the DE presented in the rest of the paper
hold if the AC-5 algorithm of Van Hentenryck et al. (1992) was adapted and used the DE
instead of PW-AC. The two algorithms have some similarities (e.g. they both use a function
to access the group of a decomposition that a certain tuple belongs to, though implemented
diﬀerently), but their basic operation is diﬀerent. The algorithm of Van Hentenryck et al.
(1992), being an instantiation of AC-5, handles a queue of triples (vi , vj , a) to implement
constraint propagation, where vi and vj are two variables involved in a constraint and
a is a value that has been removed from D(vj ). PW-AC utilizes a queue of piecewise
decompositions. Also the data structures used by the algorithms are diﬀerent. PW-AC
checks and updates counters to perform the propagation which, as we explain below, requires
space exponential in the number of common variables in the non-binary constraints. The
algorithm of Van Hentenryck et al. (1992) utilizes a more complicated data structure which
requires space exponential in the arity of the non-binary constraints. It has to be noted,
however, that PW-AC is speciﬁcally designed for the DE. That is, its operation, data
structures, and the way it checks for consistency are based on the fact that the domains of
the dual variables consist of the tuples of the original constraints extensionally stored. On
the other hand, the algorithm of Van Hentenryck et al. (1992) is generic, in the sense that
it can be adapted to operate on any piecewise functional constraint.
4.1.1 Complexities
The PW-AC algorithm consists of two phases. In the initialization phase we set up the group
counters, and in the main phase we delete unsupported tuples and propagate the deletions.
We now analyze the time complexity of PW-AC. Note that in our complexity analysis we
measure operations, such as incrementing or decrementing a counter, since PW-AC does
not perform consistency checks in the usual sense.
Proposition 4.1 The worst-case time complexity of algorithm PW-AC is O(e3 dk ).
Proof: We assume that for any constraint in the dual encoding, the non-binary constraints corresponding to the two dual variables vi and vj share at most f original variables
x1 , . . . , xf of domain size d. This means that each piecewise decomposition consists of at
most df groups. Obviously, f is equal to k − 1, where k is the maximum arity of the
constraints. In the initialization phase of lines 3–6 we iterate over all constraints, and for
each constraint between variables vi and vj , we iterate over all the tuples in D(vi ). This
is done with O(e2 dk ) asymptotic time complexity. Then, all empty groups are inserted in
Q (lines 7–11). This requires e2 df operations in the worst case. After the initialization,
function P ropagation is called. A group is inserted to Q (and later removed) only when
it becomes empty. This means that the while loop of P ropagation is executed at most
659

Samaras & Stergiou

df times for each constraint, and at most e2 df times in total. This is also the maximum
number of times function Revise is called (once in every iteration of the loop). The cost
of function Revise is computed as follows: Assuming Revise is called for a group sl (vi , vj ),
we iterate over the (at most) dk−f tuples of group sup(sl (vi , vj )) (line 20). In each iteration
we remove a tuple τ (line 21) and we update the counters of the groups where τ belongs
(lines 22–23). There are at most e such groups (in case vj is constrained with all other dual
variables). Therefore, each iteration costs O(e), and as a result, each call to Revise costs
O(edk−f ). Since Revise is called at most e2 df times, the complexity of PW-AC, including
the initialization step, is O(e2 dk + e2 df + e2 df edk−f )=O(e3 dk ). 2
Note that PW-AC can be easily used incrementally during search. In this case, the
initialization phase will only be executed once. The asymptotic space complexity of PWAC, and any AC algorithm on a binary encoding, is dominated by the O(edk ) space need
to store the allowed tuples of the non-binary constraints. Algorithm PW-AC also requires
O(e2 df ) space to store the counters for all the groups, O(e2 df ) space for the stack, and
O(f e2 ) space for the fast implementation of function GroupOf .

5. Algorithms for the Double Encoding
The double encoding has rarely been used in experiments with binary encodings, although
it combines features of the HVE and the DE, and therefore may exploit the advantages of
both worlds. To be precise, the double encoding oﬀers the following interesting potential:
search algorithms can deploy dynamic variable ordering heuristics to assign values to the
original variables, while constraint propagation can be implemented through the constraints
between dual variables to achieve higher pruning. In this section we ﬁrst brieﬂy discuss how
AC can be applied in the double encoding. We then show how various search algorithms
can be adapted to operate in the double encoding.
5.1 Arc Consistency
AC can be enforced on the double encoding using algorithm PW-AC with the addition
that each time a value a of an original variable xi loses all its supports in an adjacent dual
variable, it is deleted from D(xi ). Alternatively, we can use any generic AC algorithm, such
as AC-2001. Note that an AC algorithm applied in the double encoding can enforce various
levels of consistency depending on which constraints it uses for propagation between dual
variables. That is, propagation can be either done directly through the constraints between
dual variables, or indirectly through the constraints between dual and original variables.
For example, if we only use the constraints between dual and original variables then we get
the same level of consistency as AC in the HVE. If propagation between dual variables is
performed using the constraints of the DE then we get the same level of consistency as AC
in the DE, for the dual variables, and we also prune the domains of the original variables.
In between, we have the option to use diﬀerent constraints for propagation in diﬀerent parts
of the problem. As the next example shows, AC in the double encoding can achieve a very
high level of consistency compared to the non-binary representation. In Sections 6.2 and 6.3
we will show that this can have a profound eﬀect in practice.
660

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

Example 5.1 Consider the problem of Figure 6. Applying AC through the constraint
between the two dual variables will determine that the problem is insoluble. However, the
problem in its non-binary representation is not only GAC, but also singleton (generalized)
arc consistent (SGAC), which is a very high level of consistency. A CSP is SGAC if after
applying GAC in the problem induced by any instantiation of a single variable, there is no
domain wipeout (Debruyne & Bessière, 2001; Prosser, Stergiou, & Walsh, 2000).

x1
x 0x 1x 2x 3

0
x0

0,1

0
x4

x 1x 2x 3x 4

x2
vc1

0, 0, 0, 0
0, 0, 1, 1
0, 1, 0, 1
0, 1, 1, 0

0,1

0, 0, 1, 0
0, 1, 0, 0
1, 0, 0, 0
1, 1, 1, 0

vc2

0,1
x3

Figure 6: Double encoding of a problem that is not AC in the double encoding but is SGAC
in the non-binary representation.

5.2 Search Algorithms
Various search algorithms for the double encoding can be deﬁned, depending on the variables
that are instantiated and the constraints that are used for propagation. Here we will restrict
ourselves to algorithms that only instantiate original variables and perform propagation
using the constraints between dual variables. Intuitively this is the most interesting class
of algorithms because they combine nice features from the non-binary representation and
the HVE (small domain sizes), and from the DE (strong propagation).
We ﬁrst show that the FC versions for the HVE discussed in Section 3.2 can be adapted
to yield algorithms that run on the double encoding. We call these algorithms dFC0–dFC5.
Each algorithm dFCi (i = 0, . . . , 5) instantiates only original variables and enforces AC on
exactly the same set of variables of the double encoding as the corresponding algorithm hFCi
does in the HVE. For example, dFC5 will enforce AC on the set of dual variables, and original
variables connected to them, such that each dual variable is connected to at least one past
original variable and at least one future original variable. The diﬀerence between algorithm
dFCi (i = 2, . . . , 5) and hFCi is that the former can exploit the constraints between dual
variables to enforce a higher level of consistency than the latter. Not surprisingly, this
results in stronger algorithms.
Proposition 5.1 In any non-binary CSP, under a ﬁxed variable and value ordering, algorithm dFCi (i= 2, . . . , 5) is strictly stronger than the respective algorithm hFCi.
Proof: It is easy to see that if a value is pruned by hFCi in the HVE then it is also
pruned by dFCi in the double encoding. This is a straightforward consequence of the
fact that 1) the double encoding subsumes the HVE, and 2) algorithms dFCi and hFCi
enforce AC on the same set of variables. Algorithm dFCi is strictly stronger than hFCi
661

Samaras & Stergiou

because, by exploiting the constraints between dual variables, it can prune more values than
hFCi. Consider, for instance, a problem with two constraints c1 and c2 , where vars(c1 ) =
{x1 , x2 , x3 , x4 } and vars(c2 ) = {x1 , x2 , x3 , x5 }. All variables xi , i = 1, . . . , 5, have domains
{0, 1}. The allowed tuples of the constraints are rel(c1 ) = {(0, 0, 1, 0), (0, 1, 0, 1), (1, 1, 0, 1)}
and rel(c2 ) = {(0, 0, 0, 0), (0, 1, 1, 1), (1, 0, 0, 0)}. If x1 is given value 0 in the HVE then
algorithms hFC2–hFC5 will prune tuples (1, 1, 0, 1) and (1, 0, 0, 0) from the domains of dual
variables vc1 and vc2 respectively. No other pruning will be performed. In the double
encoding, the same variable assignment, by any of the algorithms dFC2–dFC5, will cause
the domain wipe-out of the two dual variables. 2
Corollary 5.1 In any non-binary CSP, under a ﬁxed variable and value ordering, algorithm
dFCi (i= 2 . . . 5) is strictly stronger than the respective algorithm nFCi (i=2 . . . 5).
Proof: Straightforward consequence of Propositions 5.1 and 3.1. 2
It is easy to see that algorithm hFC0 (i.e. simple binary FC) is equivalent to dFC0. The
same holds for algorithms hFC1 and dFC1. As with the various versions of FC, the MAC
algorithm can be adapted to run in the the double encoding so that only original variables
are instantiated, and propagation is implemented through the constraints between dual
variables. It is easy to see that this algorithm is strictly stronger than the corresponding
algorithm for the HVE (the proof is similar to the proof of Proposition 5.1). More interestingly, we show that this MAC algorithm for the double encoding can, at most, have a
polynomially greater cost than the corresponding MAC algorithm for the HVE, while, on
the other hand, it can be exponentially better.
Proposition 5.2 In any non-binary CSP, under a ﬁxed variable and value ordering, the
MAC algorithm for the hidden variable encoding that only instantiates original variables
can have exponentially greater cost than the corresponding MAC algorithm for the double
encoding.
Proof: To prove this, we can use Example 14 from the paper by Bacchus et al. (2002).
In this example we have a CSP with 4n + 2 variables, x1 , . . . , x4n+2 , each with domain
{1, . . . , n}, and 2n + 1 constraints:
c1 : (x1 + x2 mod 2) = (x3 + x4 mod 2)
c2 : (x3 + x4 mod 2) = (x5 + x6 mod 2)
...
c2n : (x4n−1 + x4n mod 2) = (x4n+1 + x4n+2 mod 2)
c2n+1 : (x4n+1 + x4n+2 mod 2) = (x1 + x2 mod 2)
Assume that the variables are assigned in lexicographic order in the double encoding. If x1
and x2 are assigned values such that (x1 + x2 mod 2) = 0 then enforcing AC will prune any
tuples from D(vc1 ) such that (x3 + x4 mod 2) = 0. This in turn will prune from D(vc2 ) any
tuples such that (x5 + x6 mod 2) = 1. Continuing this way, AC propagation will prune from
D(vc2n+1 ) any values such that (x1 + x2 mod 2) = 0. When these deletions are propagated
to vc1 , D(vc1 ) will become empty. In a similar way, enforcing AC after assignments to
662

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

x1 and x2 , such that (x1 + x2 mod 2) = 1, leaves D(vc1 ) empty. Therefore, the CSP is
insoluble. MAC in the double encoding needs to instantiate two variables to discover this,
and visit O(n2 ) nodes. On the other hand, as explained by Bacchus et al. (2002), MAC in
the HVE needs to visit O(nlog(n) ) nodes to conclude that the problem is insoluble. Finally,
note that, for each node, the asymptotic costs of MAC in the double encoding (using PWAC) and MAC in the HVE are polynomially related. Therefore, MAC in the HVE can be
exponentially worse than MAC in the double encoding. 2
A corollary of Proposition 5.2 is that MAC in the double encoding can have have exponentially smaller cost than MGAC in the non-binary representation.
Proposition 5.3 In any non-binary CSP, under a ﬁxed variable and value ordering, the
MAC algorithm for the double encoding that only instantiates original variables can have
at most polynomially greater cost than the corresponding MAC algorithm for the hidden
variable encoding.
Proof: To prove this we need to show two things: 1) The number of node visits made by
MAC in the double encoding is at most by a polynomial factor greater than the number of
node visits made by MAC in the HVE, 2) At each node, the worst-case cost of MAC in the
double encoding is at most by a polynomial factor greater than the worst-case cost of AC
in the HVE. The former is true since MAC in the double encoding is strictly stronger than
MAC in the HVE. The latter can be established by considering the worst case complexities
of the algorithms at each node. MAC in the HVE costs O(ekdk ) at each node, while MAC
in the double encoding can use PW-AC to enforce AC, which costs O(e3 dk ). Therefore,
there is only a polynomial diﬀerence. 2
In a similar way, we can prove that the relationship of Proposition 5.3 holds between each
algorithm dFCi (i= 2 . . . 5) and the corresponding algorithm hFCi. A corollary of Proposition 5.3 is that MAC in the double encoding can have at most polynomially greater cost
than MGAC in the non-binary representation. It is important to note that Proposition 5.3
holds only if algorithm PW-AC is used to enforce AC in the double encoding. If we use a
generic algorithm, like AC-2001, then we can get exponential diﬀerences in favor of MAC
in the HVE. Finally, regarding the relationship in node visits among the algorithms for the
double encoding, we have the following.
Proposition 5.4 Given the double encoding of a CSP with ﬁxed variable and value ordering schemes, the following relations hold:
1. nodes(dFC1)⊆ nodes(dFC0)
2. nodes(dFC2)⊆ nodes(dFC1)
3. nodes(dFC5)⊆ nodes(dFC3)⊆ nodes(dFC2)
4. nodes(dFC5)⊆ nodes(dFC4)⊆ nodes(dFC2)
5. nodes(dMAC)⊆ nodes(dFC5)
Proof: The proof is very simple and it is based on comparing the size of the subsets of
the problem where each algorithm enforces AC. 2
663

Samaras & Stergiou

6. Experimental Results
In this section we make an empirical study of algorithms for binary encodings. The empirical
study is organized in two parts:
• In the ﬁrst part (Subsections 6.1 and 6.2) we evaluate the improvements oﬀered by
specialized algorithms compared to generic ones. At the same time we compare the
eﬃciency of algorithms that run in the binary encodings to their non-binary counterparts. This comparison can give us a better understanding of when encoding a
non-binary problem into a binary one pays oﬀ, and which encoding is preferable. For
this empirical investigation we use randomly generated problems, random problems
with added structure, and benchmark crossword puzzle generation problems. Random
problems allow us to relate the performance of the algorithms to certain parameters,
such as tightness, constraint graph density, and domain size. Crossword puzzles are
standard benchmarks for comparing binary to non-binary constraint models, and allow us to to evaluate the performance of the algorithms on problems that include
constraints of high arity.
• In the second part (Subsection 6.3) , we investigate the usefulness of binary encodings
in more realistic problem settings. For this study we use problems from the domains
of conﬁguration and frequency assignment and we compare the performance of MAC
algorithms that run in the encodings to an MGAC algorithm in the non-binary representation.
All algorithms were implemented in C. All experiments were run on a PC with a 3.06
GHz Pentium 4 processor and 1 GB RAM. In all experiments, all algorithms use the
dom/deg heuristic (Bessière & Régin, 1996b) for dynamic variable ordering and lexicographic value ordering.
6.0.1 Random Problems
Random instances were generated using the extended model B as it is described by Bessiére
et al. (2002). To summarize this generation method, a random non-binary CSP is deﬁned
by the following ﬁve input parameters:
n - number of variables
d - uniform domain size
k - uniform arity of the constraints
p - density (%) percentage of the generated graph, i.e. the ratio between existing constraints
and the number of possible sets of k variables
q - uniform looseness (%) percentage of the constraints, i.e. the ratio between allowed
tuples and the dk total tuples of a constraint
The constraints and the allowed tuples were generated following a uniform distribution. We
made sure that the generated graphs were connected. In the following, a class of non-binary
664

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

CSPs will be denoted by a tuple of the form < n, d, k, p, q >. We use a star (∗) for the case
where one of the parameters is varied. For example, the tuple < 50, 20, 3, 10, ∗ > stands
for the class of problems with 50 variables, domain size 20, arity 3, graph density 10%, and
varying constraint looseness.
6.0.2 Crossword Puzzles
Crossword puzzle generation problems have been used for the evaluation of algorithms and
heuristics for CSPs (Ginsberg, Frank, Halpin, & Torrance, 1990; Beacham, Chen, Sillito, &
van Beek, 2001) and binary encodings of non-binary problems (Bacchus & van Beek, 1998;
Stergiou & Walsh, 1999). In crossword puzzle generation we try to construct puzzles for a
given number of words and a given grid which has to be ﬁlled in with words. This problem
can be represented as either a non-binary or a binary CSP in a straightforward way. In
the non-binary representation there is a variable for each letter to be ﬁlled in and a nonbinary constraint for each set of k variables that form a word in the puzzle. The domain
of each variable consists of the low case letters of the English alphabet giving a domain
size of 26. The allowed tuples of such a constraint are all the words with k letters in the
dictionary used. These are very few compared to the 26k possible combinations of letters,
which means that the constraints are very tight. In the DE there is a variable for each word
of length k in the puzzle and the possible values of such a variable are all the words with k
letters in the dictionary. This gives variables with large domains (up to 4072 values for the
Unix dictionary that we used in the experiments). There are binary constraints between
variables that intersect (i.e. they have a common letter). In the HVE there are all the
original variables as well a set of dual variables, one for each non-binary constraint.
6.1 Hidden Variable Encoding
In our ﬁrst empirical study we investigated the performance of two MAC algorithms that
operate in the HVE, and compared them to MGAC-2001, their counterpart in the nonbinary representation. The two MAC algorithms for the HVE are MHAC-2001, which
stands for MAC in the HVE that only instantiates original variables, and MHAC-2001-f ull
which is a MAC algorithm that may instantiate any variable (dual or original) according to
the heuristic choice. As stated by their names, all three algorithms use AC-2001 (GAC-2001)
to enforce AC. Although we also run experiments with the various versions of FC, we do not
include any results since these algorithms are ineﬃcient on hard problems (especially on hard
crossword puzzles). However, the qualitative comparison between FC-based algorithms for
the HVE and the non-binary representation is similar to the comparison regarding MACbased algorithms.
6.1.1 Random Problems
Table 1 shows the performance, measured in cpu time, of the algorithms on ﬁve classes of
randomly generated CSPs. All classes are from the hard phase transition region. Classes 1,
2, 3, and 4 are sparse, while 5 is more dense. We do not include results on MHAC-2001-f ull
because experiments showed that this algorithm has very similar behavior to MHAC-2001.
The reason is that, because of the nature of the constraints, the dom/deg heuristic almost
665

Samaras & Stergiou

always selects original variables for instantiation. In the rare cases where the heuristic
selected dual variables, this resulted in a large increase in cpu time.
class
MGAC-2001 MHAC-2001
1: < 30, 6, 3, 1.847, 50 >
2.08
1.90
2: < 75, 5, 3, 0.177, 41 >
4.09
3.41
3: < 50, 20, 3, 0.3, 5 >
64.15
28.10
4: < 50, 10, 5, 0.001, 0.5 >
74.72
22.53
5: < 20, 10, 3, 5, 40 >
5.75
8.15
Table 1: Comparison of algorithms MGAC-2001 and MHAC-2001 on random classes of
problems. Classes 1 and 2 taken from the paper by Bessiére et al. (2002). We give
the average run times (in seconds) for 100 instances at each class. The “winning”
time for each instance is given in bold. We follow this in the rest of the paper.

From Table 1 we can see that MHAC-2001 performs better than MGAC-2001 on the
sparse problems. In general, for all the 3-ary classes we tried with density less than 3% − 4%
the relative run time performance of MHAC-2001 compared to MGAC-2001 ranged from
being equal to being around 2-3 times faster. In the very sparse class 4, which includes
problems with 5-ary constraints, MHAC-2001 is considerably more eﬃcient than MGAC2001. This is due to the fact that for sparse problems with relatively large domain sizes
the hard region is located at low constraint looseness (i.e. small domains for dual variables)
where only a few operations are required for the revision of dual variables. Another factor
contributing to the dominance of the binary algorithm in class 4 is the larger arity of the
constraints. The non-binary algorithm requires more operations to check the validity of
tuples when the tuples are of large arity, as explained in Section 3.1.
When the density of the graph increases (class 5), the overhead of revising the domains
of dual variables and restoring them after failed instantiations slows down MHAC-2001, and
as a result it is outperformed by MGAC-2001. For denser classes than the ones reported,
the phase transition region is at a point where more than half of the tuples are allowed, and
in such cases the non-binary algorithm performs even better.
6.1.2 Crossword Puzzles
Table 2 demonstrates the performance of search algorithms on various crossword puzzles.
We used benchmark puzzles from the papers by Ginsberg et al. (1990) and Beacham et al.
(2001). Four puzzles (15.06, 15.10, 19.03, 19.04) could not be solved by any of the algorithms
within 2 hours of cpu time. Also, two puzzles (19.05 and 19.10) were arc inconsistent. In
both cases GAC discovered the inconsistency slower than AC in HVE (around 3:1 time
diﬀerence in 19.05 and 10:1 in 19.10) because the latter method discovered early the domain
wipe-out of a dual variable.
In the rest of the puzzles we can observe that MHAC-2001 performs better than MGAC2001 on the hard instances. For the hard insoluble puzzles MHAC-2001 is up to 3 times
faster than MGAC-2001. This is mainly due to the large arity of the constraints in these
666

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

puzzle
15.02
15.04*
15.07
19.02
19.08
6×6
7×7*
8×8*
10×10*

n
80
76
74
118
134
12
14
16
20

e MGAC-2001 MHAC-2001 MHAC-2001-f ull
191
3.70
3.58
—
193
40.84
38.88
29.75
193
94.65
46.36
44m
296
34.62
35.08
—
291
—
—
5.45
36
9.39
5.07
5.49
49
14m
8m
12m
64
6m
2m
5m
100
11.81
11.85
15.39

Table 2: Comparison (in cpu time) between algorithms for the HVE and algorithms for the
non-binary representation of crossword puzzles. n is the number of words and e
is the number of blanks. All times are in seconds except those followed by “m”
(minutes). A dash (—) is placed wherever the algorithm did not manage to ﬁnd
a solution within 2 hours of cpu time. Problems marked by (*) are insoluble. We
only include problems that were reasonably hard for at least one algorithm and at
the same time were solvable within 2 hours by at least one algorithm.

classes6 . Another interesting observation is that there can be signiﬁcant diﬀerences between
the performance of methods that may instantiate dual variables and those which instantiate
only original ones. In many cases MAC-2001-f ull managed to ﬁnd a (diﬀerent) solution
than MHAC-2001 and MGAC-2001 earlier. On the other hand, MAC-2001-f ull was subject
to thrashing in some instances where other methods terminate. The fact that in all insoluble
puzzles MAC-2001-f ull did not do better than MHAC-2001 shows that its performance is
largely dependent on the variable ordering scheme. In many cases MAC-2001-f ull visited
less nodes than MHAC-2001. However, this was not reﬂected to similar time performance
diﬀerence because when a dual variable is instantiated MAC-2001-f ull does more work
than when an original one is instantiated. It has to instantiate automatically each original
variable xi constrained with the dual variable and propagate these changes to other dual
variables containing xi .
6.2 Dual and Double Encodings
In this empirical study we investigated the performance of algorithms for the DE and double
encoding. We tried to answer the following three questions: 1) How eﬃcient are specialized
algorithms compared to generic algorithms? 2) Does the use of a specialized algorithm
make the DE an eﬀective option for solving non-binary CSPs? 3) Can we take advantage
of the theoretical properties of the double encoding in practice? To answer these questions,
we run experiments with random and structured problems to evaluate the beneﬁts oﬀered
by the specialized algorithm PW-AC when maintaining AC during search. We compared
6. Puzzles 6×6-10×10 correspond to square grids with no blank squares.

667

Samaras & Stergiou

the performance of two MAC algorithms; one that uses AC-2001 to enforce AC (MAC2001), and another that uses PW-AC to enforce AC (MAC-PW-AC). We also compared
these algorithms to an algorithm that maintains GAC in the non-binary representation
using GAC-2001 (MGAC-2001), and to MAC algorithms that maintain AC in the double
encoding using PW-AC (algorithm MAC-PW-ACd) and AC-2001 (algorithm MAC-2001d).
6.2.1 Random Problems
We ﬁrst give some indicative results of a comparison between the various algorithms using
random problems. Figure 7 compares the time that is required to enforce AC in the DE
and GAC in the non-binary representation, while Figures 8-10 compare algorithms that
maintain these consistencies.
Figure 7 shows the average cpu times (in msecs) that PW-AC and AC-2001 take to
enforce AC in the DE of 100 random CSPs with 50 variables with domain size 30, ternary
constraints, and 0.3 graph density (58 constraints). We also include the average time GAC2001 takes to enforce GAC on the non-binary representation of the generated instances. The
looseness of the constraints is varied starting from a point where all instances are not GAC
up until GAC and AC in the DE do not delete any values. There is signiﬁcant diﬀerence in
the performance of PW-AC compared to AC-2001 which constantly rises as the looseness of
the constraints becomes higher. This is expected, since as the number of allowed tuples in a
constraint grows, AC-2001 takes more and more time to ﬁnd supports. GAC-2001 is faster
than PW-AC (up to one order of magnitude) when the looseness in low, but the diﬀerence
becomes smaller as the looseness grows.
Figure 8 shows cpu times for a relatively sparse class of problems with 30 variables and
40 ternary constraints (p = 1). Figure 9 shows cpu times and node visits for a denser class
with 30 variables and 203 ternary constraints (p = 5). Along the x-axis we vary the domain
size of the variables. All data points show average cpu times (in secs) over 100 instances
taken from the hard phase transition region.
We can make the following observations: 1) MAC-PW-AC and MAC-PW-ACd are signiﬁcantly faster (one order of magnitude) than MAC-2001 and MAC-2001d, respectively, in
both classes of problems. 2) For both these classes, the non-binary representation is preferable to the DE (MGAC-2001 is two orders of magnitude faster in the denser class). In the
sparser class, MAC in the double encoding (i.e. algorithm MAC-PW-ACd) is competitive
with MGAC-2001 for small domain sizes, and considerably faster for larger domain sizes.
The eﬀect of the domain size in the relative performance of the algorithms is mainly due to
a run time advantage of PW-AC compared to GAC-2001, and not to the higher consistency
level achieved in the double encoding7 . The run time advantage of PW-AC can be explained
considering that, as the domain size increases, GAC-2001 has to check an increasing number
of tuples for supports; an operation more costly than the counter updates of PW-AC. In the
denser class MGAC-2001 is constantly faster than all the other algorithms for all domain
sizes. This is not surprising considering the O(e3 dk ) and O(ekdk ) complexities of PW-AC
and GAC-2001 (i.e. factor e becomes more signiﬁcant).
In Figure 10 we compare algorithms MGAC-2001 and MAC-PW-ACd (the faster among
the algorithms for the encodings) in a class of problems with 20 variables and 48 4-ary
7. This was verified by looking at the node visits of the two algorithms.

668

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

10000

10000
cpu time (secs)

1000
cpu time (msecs)

100000

AC-2001
PW-AC
GAC-2001

100

10

1

1000
100
10
1

0.1

0.1
0

0.005

0.01

0.015
q

0.02

0.025

0.03

0

100000

10

15

20

25

Figure 8: < 30, ∗, 3, 1, ∗ > CSPs.
100000

MAC-2001
MAC-AC2001d
MAC-PW-AC
MAC-PW-ACd
MGAC-2001

MGAC-2001
MAC-PW-ACd

10000
cpu time (secs)

10000

5

d

Figure 7: < 50, 30, 3, 0.3, ∗ > CSPs.

cpu time (secs)

MAC-2001
MAC-2001d
MAC-PW-AC
MGAC-2001
MAC-PW-ACd

1000
100
10

1000

100

10

1
0.1

1
0

5

10
d

15

20

0

5

10

15

20

25

d

Figure 9: < 30, ∗, 3, 5, ∗ > CSPs.

Figure 10: < 20, ∗, 4, 1, ∗ > CSPs.

constraints. The other algorithms for the encodings were not competitive in this class of
problems. We can see that MGAC-2001 is more eﬃcient for small domain sizes, while for
larger domain sizes MAC-PW-ACd can be up to one order of magnitude faster. However,
for denser classes of problems these results are reversed.
From our experiments with random problems we conjecture that the double encoding
should be the preferred model for sparse problems, provided that an eﬃcient algorithm like
PW-AC is used for propagation. For CSPs with medium and high density the non-binary
representation is preferable to the encodings.
Random Problems with Added Structure In the experiments with ternary CSPs
we did not detect an advantage of the DE compared to the non-binary representation
(and consequently the HVE). MAC in the DE is rarely better than MGAC-2001 (only in
some cases of very tight constraints and large domain sizes), despite the use of PW-AC
for propagation. Also, while MAC-PW-ACd is competitive and often faster than MGAC2001 in sparse random problems, this result is reversed as the density increases. A basic
reason for these results is that in randomly generated problems (especially ones with ternary
constraints) we get many pairs of non-binary constraints that share at most one original
variable. It is known (see Bacchus et al., 2002 for example) that for any such pair of
constraints, the ﬁltering achieved by AC in the DE is the same as the ﬁltering achieved
by GAC in the non-binary representation (and AC in the HVE). Therefore, AC in the DE
“looses” much of its ﬁltering power.
669

Samaras & Stergiou

To validate this conjecture, we experimented with a generation model where structure is
added to purely random problems. To be precise, we experimented with problems where a
clique of variables is embedded in a randomly generated instance. For ternary problems any
two constraints in the clique may share one or two variables. This is decided at random. For
4-ary problems any two constraints in the clique may share one, or two, or three variables.
Again, this is decided at random. Table 3 compares the performance of various MAC
algorithms on < 30, 10, 3, 5, ∗ > and < 20, 10, 4, 1, ∗ > problems of this type. For the second
class we only include results for MAC-PW-ACd, which is by far the best algorithm for such
problems, and MGAC-2001.
arity clique size MGAC-2001 MAC-2001 MAC-PW-AC MAC-2001d MAC-PW-ACd
3
0
633.50
45303.76
9205.95
7549.61
1362.31
3
10
874.07
21007.45
5349.55
3078.45
421.07
3
20
1121.39
1932.49
389.22
392.08
65.81
3
30
1598.87
374.03
48.22
102.30
5.02
4
0
90.11
106.04
4
10
247.18
61.12
4
20
8348.92
322.86
Table 3: Average cpu times of MAC algorithms for the DE and the double encoding and
MGAC for the non-binary representation of random problems with embedded
cliques. All times are in seconds. Each number gives the average of 50 instances
from around the phase transition region.
As we can see, the comparative results of the algorithms vary according to the size of
the embedded clique. When there is no clique embedded (clique size=0) MGAC-2001 is
faster than the algorithms for the binary encodings. As the clique size grows, the binary
encodings, and especially the double, become more eﬃcient. The double encoding is more
eﬀective than the DE for all clique sizes. For a large clique that covers all variables, MAC
in the double encoding is many orders of magnitude faster than MGAC-2001. This huge
diﬀerence is caused by the presence of many constraints that share more than one variable in
the non-binary representation. In such cases ﬁltering through the constraints between dual
variables is very strong. However, much of this advantage is lost when generic algorithms
are used in the encodings. Similar results occur in denser problems when this generation
model is used.
6.2.2 Crossword Puzzles
Table 4 compares the cpu times of the two MAC algorithms in the DE and MGAC in the
non-binary representation using various benchmark crossword puzzles. We do not include
results for MAC in the double encoding since this particular representation of crossword
puzzle generation problems is impractical. The reason for this is that for each pair of dual
variables involved in a constraint, the two variables have at most one original variable in
common (i.e. the letter on which the two words intersect). As explained previously, this
670

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

degrades the ﬁltering achieved by the constraints between dual variables. Such constraints
in the double encoding are redundant since the same ﬁltering can be achieved through the
constraints between dual and original variables.

puzzle
15.02
15.04*
15.07
15.09
19.01
19.02
19.08
21.02
21.03
21.08
21.09
6×6
7×7*
8×8*
9×9*
10×10*

n
80
76
74
82
128
118
134
130
130
150
144
12
14
16
18
20

e MGAC-2001 MAC-2001 MAC-PW-AC
191
3.98
164.04
13.70
193
40.84
895.29
140.65
193
94.65
—
—
187
0.43
—
—
301
1.50
—
—
296
34.62
—
1028.17
291
—
33.01
7.76
295
2.29
77.51
11.82
295
345.96
273.87
40.57
365
—
—
9.96
366
—
—
8.87
36
9.39
98.49
6.04
49
14m
—
12m
64
6m
—
9m
81
—
—
128.92
100
11.81
474.54
22.78

Table 4: Comparison (in cpu time) between MAC algorithms for the DE and MGAC for the
non-binary representation of crossword puzzles. All times are in seconds except
those followed by “m” (minutes). The cpu limit was 2 hours. Problems marked by
(*) are insoluble. We only include problems that were reasonably hard for either
MGAC-2001 or MAC-PW-AC and at the same time were solvable within 2 hours
by at least one algorithm.

From the data in Table 4 we can clearly see that MAC-PW-AC is signiﬁcantly faster
than MAC-2001 on all instances. The speedup oﬀered by the use of PW-AC makes MAC
in the DE competitive with MGAC in many cases where using a generic algorithm in the
DE results in a clear advantage in favor of MGAC. Also, in some instances (e.g. puzzles
21.03, 21.08, 21.09), the use of PW-AC makes MAC in the DE considerably faster than
MGAC. However, there are still some instances where MGAC (and consequently MHAC)
ﬁnds a solution (or proves insolubility) fast, while MAC in the DE thrashes, and vice versa.
Note, that only 4 of the 10 very hard 21×21 puzzles that we tried were solved by any
algorithm within the time limit of two hours. MAC-PW-AC managed to solve these 4
instances relatively fast, while the other two algorithms solved only 2 of them within the
cpu limit.
671

Samaras & Stergiou

6.3 Experiments with Realistic Problems
In the next sections we present experimental results from conﬁguration and frequency assignment problems. The aim of these experiments was to investigate the usefulness of binary
encodings in realistic structured domains. We focus on the dual and double encodings which
are the most promising binary encodings because of the strong propagation they can oﬀer.
6.3.1 Configuration
Conﬁguration is an area where CSP technology has been particularly eﬀective. A conﬁguration problem can be viewed as trying to specify a product deﬁned by a set of attributes,
where attribute values can only be combined in predeﬁned ways. Such problems can be
modelled as CSPs, where variables correspond to attributes, the domains of the variables
correspond to the possible values of the attributes, and constraints specify the predeﬁned
ways in which values can be combined. In many conﬁguration problems the constraints
are expressed extensionally through lists of allowed (or disallowed) combinations of values. Alternatively, constraints are expressed as rules which can easily be transformed into
an extensional representation. Consider the following example adapted from the paper by
Subbarayan, Jensen, Hadzic, Andersen, Hulgaard & Moller (2004).
Example 6.1 The conﬁguration of a T-shirt requires that we specify the size (small,
medium, or large), the print (“Men in Black” - MIB or “Save the Whales” - STW), and the
color (black, white, or red). There are the following constraints: 1) If the small size is chosen
then the STW print cannot be selected. 2) If the MIB print is chosen then the black color has
to be chosen as well, and if the STW print is chosen then the black color cannot be selected.
This conﬁguration problem can be modelled as a CSP with three variables {x1 , x2 , x3 } representing size, print, and color respectively. The domains of the variables are D(x1 ) =
{small, medium, large}, D(x2 ) = {M IB, ST W }, and D(x3 ) = {black, white, red}. The
ﬁrst constraint is a binary constraint between variables x1 and x2 with the following allowed
tuples: {< small, M IB >, < medium, M IB >, < medium, ST W >, < large, M IB >, <
large, ST W >}. The second constraint is a binary constraint between variables x2 and x3
with the following allowed tuples: {< M IB, black >, < ST W, white >, < ST W, red >}.
In practice, many solvers for conﬁguration problems are able to interact with the user
so that, apart from meeting the given speciﬁcations, the user’s choices of values for certain
attributes are also satisﬁed. In this study we use conﬁguration instances to compare the
non-binary representation to binary encodings on structured realistic problems. Although
it would be interesting to investigate the applicability of binary encodings in an interactive
conﬁgurator, such work is outside the scope of this paper.
We run experiments on ﬁve problems taken from CLib, a library of benchmark conﬁguration problems (CLib, 2005). The ﬁrst thing we noticed after encoding the ﬁve problems
as binary and non-binary CSPs is that they are trivially solvable by all algorithms without
backtracking. A closer look at the structure of CLibs’s problems revealed the reason; their
constraint graphs consist of various unconnected components. Each component consists of
very few or, in some cases, a single variable. As a result, the problems are split into independent subproblems that are trivially solved by all algorithms. In order to obtain diﬃcult
instances for benchmarking, we made the graphs connected by adding some randomness.
672

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

Each of the ﬁve problems was extended by adding to it 6 variables and 8-10 constraints
so that the graph became connected8 . Table 5 shows the total number of variables and
constraints in the modiﬁed problems. The added constraints were of arity 2, 3, or 4 (chosen
at random) and the variables on which they were posted were selected at random, making
sure that the resulting graph was connected. The looseness of each added constraint was
also set at random, and ﬁnally, the allowed tuples of each constraint were chosen at random
according to its looseness.

problem n

e arity dom

machine
fx
fs
esvs
bike

22
21
18
20
28

30
24
29
33
43

4
5
6
5
6

9
44
51
61
37

MGAC-2001
MAC-PW-AC MAC-PW-ACd
nodes - time
nodes - time
nodes - time
535874 - 13.83
813 - 0.37
3367 - 1.64
193372 - 4.10
92 - 0.01
70 - 0.01
618654 - 30.43
41 - 0.03
193 - 0.05
9960160 - 332.52
7384 - 3.09
64263 - 29.86
21098334 - 501.85 16890 - 12.23 112957 - 87.77

Table 5: Comparison of algorithms on conﬁguration problems. arity and dom are the
maximum constraint arity and maximum domain size in the problem. Run times
are given in seconds.

Table 5 gives the average run times and node visits for algorithms MGAC-2001 in the
non-binary representation, MAC-PW-AC in the DE, and MAC-PW-ACd in the double
encoding. For each of the ﬁve benchmarks we repeatedly generated instances using the
model described above. Each generated instance was solved by all three algorithms and
was stored if the instance was hard for at least one algorithm. Otherwise, it was discarded.
An instance was considered hard if at least one algorithm took more than one second to
solve it. Table 5 reports averages over the ﬁrst 50 hard instances generated from each
benchmark. That is, we run 250 hard instances in total. Note that in the binary encodings
all constraints of the original problem (even binary ones) were encoded as dual variables.
The experimental results of Table 5 show a very signiﬁcant advantage in favor of the
binary encodings compared to the non-binary representation, both in node visits and run
times. The DE is clearly the most eﬃcient model. MAC-PW-AC in the DE can be up
to three orders of magnitude faster than MGAC-2001 in the non-binary representation.
There was not a single instance among the 250 instances where MGAC-2001 was faster
than MAC-PW-AC. The double encoding is also much more eﬃcient than the non-binary
representation. The main factor contributing to the performance of the encodings is the
strong propagation that is achieved through the constraints between dual variables, which
is reﬂected on the numbers of node visits. There is a number of reasons, related to the
structure of these conﬁguration problems, that can justify the strong performance of the
encodings:
8. Experiments showed that these are the minimum additions that need to be made in order to get hard
problems without altering the structure of the problems too much.

673

Samaras & Stergiou

• The constraint graphs are very sparse. This is typical of conﬁguration problems since,
usually, an attribute of the product speciﬁcation has dependencies with only a few of
the other attributes.
• The constraints of high arity are very tight. Moreover, each value of the variables with
large domain sizes has very few (typically one) supporting tuple in the constraints such
variables participate.
• There are intersecting non-binary constraints with more than one original variable in
common. As explained, and demonstrated empirically in Section 6.2, this can have a
signiﬁcant impact on the propagation power of AC in the dual and double encodings.
Note that the “proﬁle” of conﬁguration problems, as analyzed above, agrees with the conjectures we made based on results from random problems. That is, the dual and double
encodings are suitable for sparse problems with tight constraints, where intersecting constraints may share more than one variable.
6.3.2 Frequency Assignment
Frequency assignment is an important problem in the radiocommunication industry. In
such a problem there is a radio communications network in a given region consisting of a
set of transmitters. Each transmitter has a position in the region, a frequency spectrum,
a certain power, and a directional distribution. The aim is to assign values to some or all
of the properties of the transmitters so that certain criteria are satisﬁed. There are various
types of frequency assignment problems. In this study we consider a version of the radio
link frequency assignment problem (RLFA). In such a problem we are given a set of links
{L1 , . . . , Ln }, each consisting of a transmitter and a receiver. Each link must be assigned a
frequency from a given set F . At the same time the total interference at the receivers must
be reduced below an acceptable level using as few frequencies as possible. These problems
are typically optimization problems but for the purposes of this study we treat them as
satisfaction problems.
A RLFA problem can be modelled as a CSP where each transmitter corresponds to
a variable. The domain of each variable consists of the frequencies that can be assigned
to the corresponding transmitter. The interferences between transmitters are modelled as
binary constraints of the form |xi − xj | > s, where xi and xj are variables and s ≥ 0 is
a required frequency separation. Such a constraint restricts the frequencies that the two
transmitters can simultaneously be assigned, and in that way the interference between them
is minimized. This is under the realistic assumption that the closer two frequencies are the
greater is the interference between them. This binary model has been used extensively to
represent RLFA problems, and numerous solution methods (CSP-based or other) have been
proposed. Also, RLFA has been widely used as a benchmark to test new algorithms for
binary constraints (mainly AC algorithms).
It has been argued that the standard binary model of frequency assignment problems
fails to capture some important aspects of real problems, such as multiple interferences,
resulting in non-optimal frequency assignments (Jeavons, Dunkin, & Bater, 1998; Watkins,
Hurley, & Smith, 1998; Bater, 2000; Hodge, Hurley, & Smith, 2002). As a consequence,
there have been some eﬀorts to introduce more expressive methods that utilize non-binary
674

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

constraints in frequency assignment (e.g. Bater, 2000; Hodge et al., 2002). There are many
types of non-binary constraints that can be considered. The following ones have received
attention:
co-channel constraints - e.g., the frequencies assigned to n transmitters are not all equal.
adjacent-channel constraints - e.g., the frequencies assigned to n transmitters are at
least one frequency apart.
separation constraints - e.g., the frequencies assigned to n transmitters are at least s
frequencies apart.
Obviously, separation constraints generalize the adjacent-channel constraints. The ﬁrst
two types of constraints are typically very loose while the third can be tight. Separation
constraints are used in densely constrained areas (representing conurbations in a region)
where there is large number of links closely situated. In such cases, large separations in
the frequencies of the transmitters must be imposed, resulting in tight constraints. We
also consider a richer type of separation constraints: The frequencies assigned to a set of n
transmitters are at least s frequencies apart and n transmitters among them are at least
s (> s) frequencies apart from all the others. Note that some of the non-binary constraints
can be equivalently decomposed into a clique of binary constraints (without introducing
dual variables) resulting however in weaker propagation. An example is adjacent-channel
constraints. Others cannot be equivalently expressed as a set of binary constraints unless
a binary encoding is used. For example, co-channel constraints. As noted by Hodge et al.
(2002), only non-binary constraints of low arity can be utilized in practice. It has been
shown that in many cases such constraints are suﬃcient to achieve very low interferences.
Constraints of higher arity may oﬀer improvements in the quality of solutions, but they tend
to slow down the solution process to an extend that solving large real problems becomes
infeasible.
In the empirical study presented here we are interested in comparing models of RLFAtype problems with non-binary constraints to the corresponding binary encodings and not
in devising new eﬃcient methods for solving RLFA problems. Since the available RLFA
benchmarks follow the standard binary approach, to test the algorithms we generated nonbinary problems by placing variables, corresponding to links, on a grid following the typical
RLFA structure. That is, the problems consist of several groups of closely situated variables
plus a few constraints that connect these groups. For example, such structures are depicted
in Figure 11. This corresponds to the constraint graph of a binary RLFA problem which
typically consists of a set of cliques (or near-cliques) of binary constraints and a small number
of constraints connecting the various cliques (e.g. the benchmarks of Cabon, De Givry,
Lobjois, Schiex, & Warners, 1999). From the binary encodings we only considered the
double since dual variables can have very large domains, which makes the DE ineﬃcient.
Indicative results of the experiments we run are depicted in Table 6. In these experiments
we posted only low-arity (i.e. 3-ary to 5-ary) separation constraints, as shown in Figure 11,
and compared the performance of algorithm MGAC-2001 on the non-binary model of the
problems to the performance of MAC-PW-ACd on the double encoding of the problems. We
tried two implementations of MGAC-2001; one that utilizes specialized propagators for the
675

Samaras & Stergiou

a) prob1

b) prob2

c) prob3

Figure 11: Examples of RLFA problems with non-binary separation constraints.

separation constraints (written as functions), and another that operates on the extensional
representation of the constraints. The ﬁrst implementation was generally faster, so all the
results of MGAC-2001 presented below refer to the intentional implementation. The double
encoding was built by translating the separation constraints into lists of allowed tuples in
a preprocessing step.

problem
prob1
prob1
prob1
prob2
prob2
prob2
prob3
prob3
prob3
prob4
prob4
prob4
prob5
prob5
prob5

(easiest)
(median)
(hardest)
(easiest)
(median)
(hardest)
(easiest)
(median)
(hardest)
(easiest)
(median)
(hardest)
(easiest)
(median)
(hardest)

n
48
48
48
44
44
44
54
54
54
68
68
68
100
100
100

e arity
25
25
25
21
21
21
24
24
24
38
38
38
58
58
58

4
4
4
4
4
4
5
5
5
5
5
5
5
5
5

MGAC-2001
MAC-PW-ACd
nodes - time
nodes - time
583 - 1.51
48 - 0.36
18161268 - 39518.71
50 - 0.34
—
—
56474 - 26.46
172071 - 219.06
304676 - 169.01
305588 - 504.06
53654514 - 4220.21 68771412 - 24133.45
103 - 13.45
0 - 6.42
2134 - 14.14
2569 - 53.18
4194 - 115.12
0 - 5.64
70 - 1.21
72 - 7.24
—
90 - 8.34
—
—
294 - 5.29
0 - 2.42
96106 - 522.64
99 - 2.81
—
104 - 13.45

Table 6: Comparison of algorithms on RLFA problems with separation constraints. arity
is the maximum constraint arity. Run times are given in seconds. A dash (—) is
placed wherever the algorithm did not ﬁnish its run within 12 hours of cpu time.

676

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

Table 6 reports results from a total of 50 instances created using ﬁve diﬀerent constraint
graph topologies. All variables have domains of 20 or 25 values. The number of allowed
tuples for the constraints varied from around 50 for very tight constraints to several thousands for looser ones, according to the frequency separation imposed by parameters s and
s of the separation constraints. These parameters were set at random for each constraint,
making sure that very loose constraints were not generated. For example, for a 4-ary basic
separation constraint on variables with domain size 20, s was at least 3 (giving 7920 allowed
tuples) and at most 5 (giving 120 allowed tuples).
prob1, prob2, and prob3 refer to problems having the topologies shown in Figure 11.
prob4 consists of three groups of variables, similar to the ones of prob3, arranged in a
chain-like structure. Finally, instances of prob5 consist of randomly generated groups of
variables; each one having 8-10 variables and 3-5 3-ary to 5-ary constraints. These groups
are interconnected according to their topological distance (i.e. constraints are posted on
variables of nearby groups). All instances of prob1-prob4 have ﬁxed topology. For each
topology a set of instances was created by changing the type of the constraints. For example,
two instances having the topology of prob1 may diﬀer in the type of separation constraints
(basic or richer) they include. Also, the frequency separations s and s imposed by a
constraint may diﬀer. Instances of prob5 may also diﬀer in their constraint graph topology.
We report node visits and run times of the easiest, median, and hardest instance for each
topology, with respect to the performance of MGAC9 . The hardest instances were the same
for both the encoding and the non-binary representation (except for prob3), while the easiest
and median instances were sometimes diﬀerent.
In Table 6 we can see that there can be very substantial diﬀerences in favor of the
double encoding. Many instances were solvable in the double encoding with no or very
little backtracking while MGAC-2001 thrashed. This is mainly due to the large number
of interleaved constraints sharing more than one variable, which boosts propagation in the
double encoding. The performance of the algorithms seems to be heavily dependent on the
topology of the problems. For example, on instances of prob2 the non-binary representation
was much more eﬃcient than the double encoding. It seems that in this particular class of
problems heuristic choices were misled by the propagation achieved in the double encoding.
We have not been able to come up with a satisfactory explanation as to why this occurred
in this particular topology.
Finally, to investigate the eﬀect that the presence of loose constraints of higher arity
has, we run experiments where 8-ary adjacent-channel constraints were posted between
variables further apart in the graph, in addition to the separation constraints. In this
case using the double encoding to model all the constraints in the problems was infeasible
due to the spatial requirements. For example, trying to generate the allowed tuples of a
single 8-ary adjacent-channel constraint consumed all the memory of the system. Therefore,
we compared algorithm MGAC-2001 on the non-binary model to a MAC algorithm that
runs on a hybrid model where the tight separation constraints are modelled using the
double encoding and the loose adjacent-channel constraints are kept in the intentional nonbinary representation. Table 7 reports results from a total of 30 instances created using the
9. To create the instances we varied the type of the constraints and the values of parameters s and s until
non-trivial problems were generated. We consider as trivial problems that are arc inconsistent or solvable
with no backtracking.

677

Samaras & Stergiou

problem
prob1
prob1
prob1
prob2
prob2
prob2
prob5
prob5
prob5

(easiest)
(median)
(hardest)
(easiest)
(median)
(hardest)
(easiest)
(median)
(hardest)

n
48
48
48
44
44
44
100
100
100

e arity
25
25
25
21
21
21
58
58
58

8
8
8
8
8
8
8
8
8

MGAC-2001
MAC-hybrid
nodes - time
nodes - time
106 - 20.88
50 - 47.60
5078 - 1201.98
84 - 195.43
—
—
647 - 192.64
1019 - 308.84
80245 - 17690.12
—
—
—
76 - 45.92
0 - 22.19
22785 - 3230.47
99 - 78.41
—
18447 - 4233.50

Table 7: Comparison of algorithms on RLFA problems with separation and adjacentchannel constraints. MAC-hybrid corresponds to a MAC algorithm that runs
on the hybrid model.

graph topologies of prob1, prob2 and prob5 with the addition of four 8-ary adjacent-channel
constraints in each instance. The hybrid model is more eﬃcient on instances of prob1 and
prob5 because of the strong propagation achieved through the binary encoding of the tight
constraints. The non-binary model is better on instances of prob2 where it seems that
propagation through the binary encoding results in bad heuristic choices.
6.4 Discussion
In this section we summarize the results of our experimental studies and draw some conclusions regarding the applicability of the encodings, based on our theoretical and experimental
analysis.
Hidden Variable Encoding As theoretical results suggested, and empirical results conﬁrmed, solving problems in the HVE using algorithms that only instantiate original variables
is essentially analogous to solving the non-binary representation directly. All the commonly
used algorithms for non-binary problems can be applied, with adjustments, to the HVE, and
vice versa. When such algorithms are used, the HVE oﬀers some (moderate) computational
savings compared to the non-binary representation, especially in sparse problems. These
savings are due to the ability of the AC algorithm in the HVE to detect inconsistencies earlier than the corresponding GAC algorithm in the non-binary representation. Therefore, we
conjecture that the HVE is applicable in sparse non-binary problems where the constraints
are extensionally speciﬁed. In other cases, the HVE is either less eﬃcient in run times
than the non-binary representation (e.g. dense problems), or building the HVE adds space
overheads that are not justiﬁed by the marginal gains in search eﬀort. Additionally, there
is not enough empirical evidence to suggest that the essential diﬀerence between search
algorithms for the HVE and the non-binary representation, i.e. the ability of the former
to branch on dual variables, can make the HVE signiﬁcantly more eﬃcient in some class
678

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

of problems. This, coupled with the fact that any beneﬁts gained by instantiating dual
variables can be maximized if the double encoding is used instead of the HVE, limits the
applicability of such algorithms.
Dual and Double Encodings The DE and the double encoding have the advantage
of strong ﬁltering through the constraints between dual variables. We showed that this
advantage can be exploited by a low cost specialized algorithm, such as PW-AC, to make
the DE competitive and often signiﬁcantly better than the non-binary representation in
several sparse CSPs, such as crossword puzzle generation and conﬁguration problems. For
dense CSPs, the DE does not pay oﬀ because either the spatial requirements make its use
infeasible, or if this is not the case, the advantages oﬀered are outweighed by the overhead
of updating the domains of dual variables. The same holds for CSPs containing constraints
of large arity unless they are very tight (as in crossword puzzles).
Algorithms for the double encoding demonstrate especially promising performance.
When many non-binary constraints that share more than one variable are present in a
problem then MAC in the double encoding can exploit the beneﬁts of both the variable
ordering heuristic, borrowed from the non-binary representation, and the stronger ﬁltering,
borrowed from the DE, to outperform the other representations. This was demonstrated in
problems with such structure (random and also frequency assignment - like). This is also
the case in the “still-life” problem, which explains the success of the double encoding10 . In
addition, the double encoding oﬀers the interesting potential of hybrid models where certain
constraints are encoded into binary and others are kept in the non-binary representation
based on certain properties of the constraints. To be precise we can beneﬁt by encoding
constraints that are either naturally speciﬁed in extension, or have relatively low arity and
are tight. This was demonstrated in various domains. Most notably, in the frequency assignment problems where the double encoding (or a hybrid one) payed oﬀ in most cases,
although the constraints in such problems are naturally deﬁned intentionally.

7. Related Work
Although, the DE was proposed in 1989 (Dechter & Pearl, 1989) and the HVE in 1990 (Rossi
et al., 1990), the ﬁrst substantial eﬀort towards evaluating their eﬃciency was carried out
in 1998 (Bacchus & van Beek, 1998). In that work, Bacchus and van Beek compared theoretically and empirically the FC algorithm in the two encodings against FC for non-binary
CSPs. Also, they introduced FC+, a specialized algorithm for the HVE. The algorithms
compared by Bacchus and van Beek were the simplest versions of FC; hFC0 and hFC1 (i.e.
FC+) for the HVE, and nFC0 for the non-binary representation. We extend that work by
studying various recent and more advanced versions of FC.
Following Bacchus and van Beek (1998), Stergiou and Walsh made a theoretical and
empirical study of AC in the encodings (Stergiou & Walsh, 1999). It was proved that AC
in the HVE is equivalent to GAC in the non-binary representation, while AC in the DE is
stronger. In the small experimental study included in the paper by Stergiou & Walsh (1999),
MAC for the HVE, DE, and double encoding was compared to MGAC in the non-binary
10. Although the title of Smith’s paper (2002) refers to the DE, the model of the “still-life” problem used is
based on the double encoding.

679

Samaras & Stergiou

representation of some crossword puzzles and Golomb rulers problems. Results showed an
advantage for the non-binary representation and the HVE, but it is important to note that
all the MAC algorithms used generic ineﬃcient algorithms to enforce AC.
Smith, Stergiou & Walsh (2000) performed a more extensive experimental comparison
of MAC in the HVE and the double encoding, and MGAC in a non-binary model of the
Golomb rulers problem. However, again the MAC algorithms in the encodings used a generic
algorithm to enforce AC. As a result they were outperformed by MGAC in the non-binary
model.
Beacham et al. (2001) compared the performances of diﬀerent models, heuristics, and
algorithms for CSPs using crossword puzzle generation problems as benchmarks. Among
the models that were compared was the HVE, the DE and the non-binary representation.
Once again, the algorithms that were applied in the encodings were generic algorithms.
For example, two of the implemented algorithms for the DE were MAC that uses AC-3 for
propagation and MAC that uses AC-7. Both these algorithms suﬀer from the very high complexity of AC propagation. As demonstrated, the use of algorithm PW-AC for propagation
can signiﬁcantly enhance the performance of MAC in crossword puzzle problems.
Bacchus et al. (2002) presented an extensive theoretical study of the DE and the HVE.
Among other results, polynomial bounds were placed on the relative performance of FC
and MAC in the two encodings and the non-binary representation, or it was shown that no
polynomial bound exists. For example, it was shown that FC in the HVE (i.e. hFC0 in the
terminology we use) is never more than a polynomial factor worse than FC in the DE, but
FC in the DE can be exponentially worse than FC in the HVE. Also, FC in the non-binary
representation (i.e. nFC0 in the terminology we use) can be exponentially worse than FC in
the HVE, and vice versa. We add to these results by analyzing the performance of various
more advanced algorithms for the HVE and the double encoding.
Smith modelled the problem of ﬁnding a maximum density stable pattern “still-life”
in Conway’s game of Life using MAC in the double encoding with remarkable success,
compared to other constraint programming and integer programming approaches (Smith,
2002). The MAC algorithm was implemented using the Table constraint of ILOG Solver.
This constraint implements the generic AC algorithm of Bessiére & Régin (1996a), which is
very expensive when used in the DE because of its high time complexity. We believe that
the results presented by Smith can be further improved if MAC-PW-AC is used instead.

8. Conclusion
In this paper we studied three binary translations of non-binary CSPs; the hidden variable
encoding, the dual encoding, and the double encoding. We showed that the common perception that standard algorithms for binary CSPs can be used in the encodings of non-binary
CSPs suﬀers from ﬂaws. Namely, standard algorithms do not exploit the structure of the encodings, and end up being ineﬃcient. To address this problem, we proposed specialized arc
consistency and search algorithms for the encodings, and we evaluated them theoretically
and empirically. We showed how arc consistency can be enforced on the hidden variable
encoding of a non-binary CSP with the same worst-case time complexity as generalized arc
consistency on the non-binary representation. We showed that the structure of constraints
in the dual encoding can be exploited to achieve a much lower time complexity than a
680

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

generic algorithm. Empirical results demonstrated that the use of a specialized algorithm
makes the dual encoding signiﬁcantly more eﬃcient. We showed that generalized search
algorithms for non-binary CSPs can be relatively easily adjusted to operate in the hidden
variable encoding. We also showed how various algorithms for the double encoding can be
designed. These algorithms can exploit the properties of the double encoding (strong ﬁltering and branching on original variables) to achieve very good results in certain problems.
Empirical results in random and structured problems showed that, for certain classes of
non-binary constraints, using binary encodings is a competitive option, and in many cases,
a better one than solving the non-binary representation.

Acknowledgements
We would like to thank Panagiotis Karagiannis, Nikos Mamoulis, and Toby Walsh for their
help in various stages of this work. We would also like to thank the anonymous reviewers
of an earlier version of this paper for their very useful comments and suggestions.

Appendix A
As explained, the main diﬀerence between the AC algorithm for the HVE and the corresponding GAC algorithm is the fact that the AC algorithm has to update the domains of
the dual variables as well as the original ones. This incurs a time overhead, but as we will
show, deleting values from dual variables can help propagation discover domain wipe-outs
in arc inconsistent problems faster.
Proposition 8.1 Let P be a non-binary CSP. Assume that after generalized arc consistency is applied in P , there is no domain wipeout in the resulting problem. Enforcing arc
consistency in the hidden variable encoding of P using HAC requires the same number of
consistency checks as enforcing generalized arc consistency in P using GAC-2001, assuming the two algorithms follow the same ordering of variables and values when looking for
supports and propagating deletions.
Proof: First, consider that if no domain wipeout in any variable (original or dual) occurs
then the two algorithms will add constraints (dual variables) to the stack and remove
them for revision in exactly the same order. Therefore, we only need to show that if a
value is deleted from a variable during the revision of a constraint or ﬁnds a new support
in the constraint then these operations will require the same number of checks in both
representations. Assume that in the non-binary version of the algorithm value a is deleted
from the domain of variable xi because it has no support in constraint c. If |T | is the
number of allowed tuples in c then determining the lack of support will require |T | −
currentSupportxi,a,c checks, one for each of the tuples in c that have not been checked yet.
If the value is not deleted but ﬁnds a new support τ , with τ > currentSupportxi,a,c , then
τ − currentSupportxi,a,c checks will be performed. In the HVE, xi will be processed in the
same order as in the non-binary version and we will require |T | − currentSupportxi,a,vc or
τ − currentSupportxi,a,vc checks depending on the case. Obviously, currentSupportxi ,a,c
is the same as currentSupportxi ,a,vc since a tuple in c corresponds to a value in vc , and
therefore, the same number of checks will be performed in both representations. 2
681

Samaras & Stergiou

Proposition 8.2 Let P be a non-binary CSP. Assume that the application of generalized
arc consistency in P results in a domain wipeout. Algorithm HAC applied in the hidden
variable encoding of P discovers the domain wipeout with at most the same number of
consistency checks as algorithm GAC-2001 in the non-binary representation, assuming the
two algorithms follow the same ordering of variables and values when looking for supports
and propagating deletions.
Proof: In any CSP, arc inconsistency in detected when the domain of a variable is wiped
out while applying AC. In the HVE of a non-binary CSP, arc inconsistency is detected when
the domain of an original variable is wiped out or (crucially) when the domain of a dual
variable is wiped out. The second possibility can make an AC algorithm that operates in
the HVE more eﬃcient than the corresponding GAC algorithm. To prove this consider
an arc inconsistent non-binary problem. Assume that the domain of original variable xi
is wiped out during the processing of constraint c which is encoded as a dual variable vc
in the HVE. Until the point where function Revise is called with xi and c as arguments,
there is no inconsistency and according to Proposition 8.1 the GAC algorithm and the
AC algorithm on the HVE will perform the same number of consistency checks. Assume
that there are j values left in D(xi ) before the call to Revise. In function Revise we will
unsuccessfully look for a support for each of the j values. If |T | is the number of allowed
tuples in c then, for each value a ∈ D(xi ), this will require |T | − currentSupportxi,a,c checks
for the GAC algorithm and |T | − currentSupportxi ,a,vc checks for the AC algorithm. Since
|T | − currentSupportxi ,a,c = |T | − currentSupportxi,a,vc , the two algorithms will perform
the same number of consistency checks to detect the domain wipeout.
The following example demonstrates that HAC may discover the inconsistency with less
checks. Consider a problem with variables x1 , x2 , x3 , x4 which have domains {0, 1}, {0, 1},
{0, . . . , 9}, and {0, 1}, respectively. There are two constraints, c1 and c2 , with vars(c1 ) =
{x1 , x2 , x3 } and vars(c2 ) = {x1 , x2 , x4 } respectively. Value 0 of x2 is supported in c1 by
tuples that include the assignment (x1 , 1). Value 0 of x1 is supported in c2 by tuples that
include the assignment (x2 , 0). Constraint c2 allows only tuples that include the assignment
(x2 , 0). Values 0, . . . , 9 of x3 are supported in c1 by tuples that include (x2 , 0) and by tuples
that include (x2 , 1). Now assume that variable x1 is instantiated to 0, which means that
the deletion of 1 from D(x1 ) must be propagated. In the HVE, we will ﬁrst delete all
tuples that include the value (x1 , 1) from dual variables vc1 and vc2 . Then, we will add dual
variables vc1 and vc2 to the stack, remove them, and revise all original variables connected
to them. Assuming that vc1 is removed ﬁrst, value 0 of x2 will have no support in vc1 so
it will be deleted. As a result, we will delete all tuples from dual variable vc2 that include
the pair (x2 , 0). This means that the domain of vc2 will be wiped out. In the non-binary
representation, we will proceed in a similar way and perform the same number of checks
until 0 is deleted from x2 . After that deletion the algorithm will look for supports in c1 for
value 1 of x2 and all values of x3 . This will involve checks that are avoided in the HVE.
The inconsistency will be discovered later when we process constraint c2 and ﬁnd out that
value 1 of x2 has no support in c2 resulting in the domain wipeout of x2 . 2

682

Binary Encodings of Non-binary CSPs: Algorithms & Experimental Results

References
Bacchus, F., Chen, X., van Beek, P., & Walsh, T. (2002). Binary vs. Non-binary CSPs.
Artificial Intelligence, 140, 1–37.
Bacchus, F., & van Beek, P. (1998). On the Conversion between Non-binary and Binary
Constraint Satisfaction Problems. In Proceedings of AAAI’98, pp. 310–318.
Bater, J. (2000). Non-binary (Higher-Order) Modelling and Solution Techniques for Frequency Assignment in Mobile Communications Networks. Ph.D. thesis, University of
London.
Beacham, A., Chen, X., Sillito, J., & van Beek, P. (2001). Constraint programming lessons
learned from crossword puzzles. In Proceedings of the 14th Canadian Conference in
AI.
Bessière, C., Freuder, E., & Régin, J. (1995). Using Inference to Reduce Arc Consistency
Computation. In Proceedings of IJCAI’95, pp. 592–599.
Bessière, C., Meseguer, P., Freuder, E., & Larrosa, J. (2002). On Forward Checking for
Non-binary Constraint Satisfaction. Artificial Intelligence, 141, 205–224.
Bessière, C., & Régin, J. (1996a). Arc Consistency for General Constraint Networks: Preliminary Results. In Proceedings of IJCAI’97, pp. 398–404.
Bessière, C., & Régin, J. (1996b). MAC and Combined Heuristics: Two Reasons to Forsake
FC (and CBJ?) on Hard Problems. In Proceedings of CP’96, pp. 61–75.
Bessière, C., & Régin, J. (2001). Reﬁning the Basic Constraint Propagation Algorithm. In
Proceedings of IJCAI’2001, pp. 309–315.
Cabon, B., De Givry, S., Lobjois, L., Schiex, T., & Warners, J. (1999). Radio Link Frequency
Assignment. Constraints, 4, 79–89.
CLib (2005). Conﬁguration Benchmarks Library (http://www.itu.dk/doi/VeCoS/clib/),
Maintained by VeCoS group, IT-University of Copenhagen.
Debruyne, R., & Bessière, C. (2001). Domain Filtering Consistencies. Journal of Artificial
Intelligence Research, 14, 205–230.
Dechter, R., & Pearl, J. (1989). Tree Clustering for Constraint Networks. Artificial Intelligence, 38, 353–366.
Ginsberg, M., Frank, M., Halpin, M., & Torrance, M. (1990). Search Lessons Learned from
Crossword Puzzles. In Proceedings of AAAI-90, pp. 210–215.
Haralick, R., & Elliot, G. (1980). Increasing Tree Search Eﬃciency for Constraint Satisfaction Problems. Artificial Intelligence, 14, 263–313.
Hodge, L., Hurley, S., & Smith, D. (2002). Higher-Order Constraint Techniques for the
Frequency Assignment Problem. Tech. rep., University of Cardiﬀ.
Jeavons, P., Dunkin, N., & Bater, J. (1998). Why Higher Order Constraints are Necessary
to Model Frequency Assignment Problems. In ECAI’98 Workshop on Non-binary
constraints.
Mackworth, A. (1977). Consistency in Networks of Relations. Artificial Intelligence, 99–118.
683

Samaras & Stergiou

Mamoulis, N., & Stergiou, K. (2001). Solving Non-Binary CSPs using the Hidden Variable
Encoding. In Proceedings of CP-2001, pp. 168–182.
Mohr, R., & Henderson, T. (1986). Arc and Path Consistency Revisited. Artificial Intelligence, 28, 225–233.
Mohr, R., & Masini, G. (1988). Good Old Discrete Relaxation. In Proceedings of ECAI-88,
pp. 651–656.
Peirce, C. (1933) Collected Papers Vol. III. Cited in F. Rossi, C. Petrie, and V. Dhar 1989.
Prosser, P., Stergiou, K., & Walsh, T. (2000). Singleton Consistencies. In Proceedings of
CP-2000, pp. 353–368.
Régin, J. (1994). A Filtering Algorithm for Constraints of Diﬀerence in CSPs. In Proceedings
of AAAI-94, pp. 362–367.
Rossi, F., Petrie, C., & Dhar, V. (1990). On the Equivalence of Constraint Satisfaction
Problems. In Proceedings of ECAI-90, pp. 550–556.
Smith, B. (2002). A Dual Graph Translation of a Problem in ‘Life’. In Proceedings of
CP-02, pp. 402–414.
Smith, B., Stergiou, K., & Walsh, T. (2000). Using Auxiliary Variables and Implied Constraints to Model Non-binary Problems. In Proceedings of AAAI’2000, pp. 182–187.
Stergiou, K., & Walsh, T. (1999). Encodings of Non-Binary Constraint Satisfaction Problems. In Proceedings of AAAI’99, pp. 163–168.
Subbarayan, S., Jensen, R., Hadzic, T., Andersen, H., Hulgaard, H., & Moller, J. (2004).
Comparing two Implementations of a Complete and Backtrack-Free Interactive Conﬁgurator. In Proceedings of the CP-04 Workshop on CSP Techniques with Immediate
Application, pp. 97–111.
Van Hentenryck, P. (Ed.). (1989). Constraint Satisfaction in Logic Programming. MIT
Press.
Van Hentenryck, P., Deville, Y., & Teng, C. (1992). A Generic Arc Consistency Algorithm
and its Specializations. Artificial Intelligence, 57, 291–321.
Watkins, W., Hurley, S., & Smith, D. (1998). Area Coverage Frequency Assignment: Evaluation of Models for Area Coverage. Tech. rep., University of Glamorgan. Also presented
in INFORMS-98.
Zhang, Y., & Yap, R. (2001). Making AC-3 an Optimal Algorithm. In Proceedings of
IJCAI’2001, pp. 316–321.

684

Journal of Artificial Intelligence Research 24 (2005) 157-194

Submitted 09/04; published 07/05

Learning Content Selection Rules for Generating Object
Descriptions in Dialogue
Pamela W. Jordan

pjordan@pitt.edu

Learning Research and Development Center & Intelligent Systems Program
University of Pittsburgh, LRDC Rm 744
Pittsburgh, PA 15260

Marilyn A. Walker

M.A.Walker@sheffield.ac.uk

Department of Computer Science, University of Sheffield
Regent Court, 211 Portobello Street
Sheffield S1 4DP, U.K.

Abstract
A fundamental requirement of any task-oriented dialogue system is the ability to generate object descriptions that refer to objects in the task domain. The subproblem of
content selection for object descriptions in task-oriented dialogue has been the focus of
much previous work and a large number of models have been proposed. In this paper, we
use the annotated coconut corpus of task-oriented design dialogues to develop feature
sets based on Dale and Reiter’s (1995) incremental model, Brennan and Clark’s (1996)
conceptual pact model, and Jordan’s (2000b) intentional influences model, and use these
feature sets in a machine learning experiment to automatically learn a model of content
selection for object descriptions. Since Dale and Reiter’s model requires a representation
of discourse structure, the corpus annotations are used to derive a representation based on
Grosz and Sidner’s (1986) theory of the intentional structure of discourse, as well as two
very simple representations of discourse structure based purely on recency. We then apply
the rule-induction program ripper to train and test the content selection component of an
object description generator on a set of 393 object descriptions from the corpus. To our
knowledge, this is the first reported experiment of a trainable content selection component
for object description generation in dialogue. Three separate content selection models that
are based on the three theoretical models, all independently achieve accuracies significantly
above the majority class baseline (17%) on unseen test data, with the intentional influences model (42.4%) performing significantly better than either the incremental model
(30.4%) or the conceptual pact model (28.9%). But the best performing models combine
all the feature sets, achieving accuracies near 60%. Surprisingly, a simple recency-based
representation of discourse structure does as well as one based on intentional structure. To
our knowledge, this is also the first empirical comparison of a representation of Grosz and
Sidner’s model of discourse structure with a simpler model for any generation task.

1. Introduction
A fundamental requirement of any task-oriented dialogue system is the ability to generate
object descriptions that refer to objects in the task domain. For example, consider the
excerpt of a task-oriented dialogue from the coconut corpus in Figure 1 (Di Eugenio,
Jordan, Thomason, & Moore, 2000). The conversants in this dialogue are attempting to
c
2005
AI Access Foundation. All rights reserved.

Jordan & Walker

(Partial solution to problem already agreed upon in prior dialogue: 3 green chairs for 50, 1 green
chair for 100 and a green table in the dining room, and a blue sofa in the living room. The numbers
in brackets indicate the utterance breaks for analysis of the dialogue.)
G: [36] That leaves us with 250 dollars. [37] I have a yellow rug for 150 dollars. [38] Do you have
any other furniture left that matches for 100 dollars?”
S: [39] No, I have no furniture left that costs $100. [40] I guess you can buy the yellow rug for $150.
G: [41] Okay. [42] I’ll buy the rug for 150 dollars. [43] I have a green chair [44] that I can buy for
100 dollars [45] that should leave us with no money.
S: [46] That sounds good. [47] Go ahead and buy the yellow rug and the green chair.
G: [48] I’ll buy the green 100 dollar chair. [49] Design Complete?
S: [50] Sounds good, [51] do you want the green chair in the dining room with the other chairs? [52]
I put the yellow rug in the living room. [53] Then the design is complete.
G: [54] Sounds good. [55] Hit the design complete

Figure 1: Excerpt of a coconut dialogue illustrating variable selection of attributes for
object descriptions that refer to furniture items

collaboratively construct a solution for furnishing a two room house. Each conversant starts
the task with a separate set of furniture items that can be used in the solution. In the process
of negotiating the solution, they generate object descriptions (shown in italics) for items of
furniture.
Each furniture type in the coconut task domain has four associated attributes: color,
price, owner and quantity. As a first step, an object description generator must decide which
of these four attributes to include in an utterance, while subsequent surface generation steps
decide where in the utterance the attributes will be expressed. For example, the task domain
objects under discussion in the dialogue in Figure 1 are a $150 yellow rug owned by Garrett
(G) and a $100 dollar green chair owned by Steve (S). In the dialogue excerpt in Figure
1, the yellow rug is first referenced in utterance 36 as a yellow rug for 150 dollars and
then subsequently as the yellow rug for 150 dollars, the rug for 150 dollars, the yellow rug,
where the owner attribute is sometimes realized in a separate noun phrase within the same
utterance. It could also have been described by any of the following: the rug, my rug, my
yellow rug, my $150 yellow rug, the $150 rug. The content of these object descriptions
varies depending on which attributes are included. How does the speaker decide which
attributes to include?
The problem of content selection for subsequent reference has been the focus of much
previous work and a large number of overlapping models have been proposed that seek to
explain different aspects of referring expression content selection (Clark & Wilkes-Gibbs,
1986; Brennan & Clark, 1996; Dale & Reiter, 1995; Passonneau, 1995; Jordan, 2000b) inter
alia. The factors that these models use include the discourse structure, the attributes and
attribute values used in the previous mention, the recency of last mention, the frequency of
mention, the task structure, the inferential complexity of the task, and ways of determining
salient objects and the salient attributes of an object. In this paper, we use a set of factors
considered important for three of these models, and empirically compare the utility of these
158

Learning Content Selection Rules for Generating Object Descriptions

factors as predictors in a machine learning experiment in order to first establish whether the
selected factors, as we represent them, can make effective contributions to the larger task of
content selection for initial as well as subsequent reference. The factor sets we utilize are:
• contrast set factors, inspired by the incremental model of Dale and Reiter
(1995);
• conceptual pact factors, inspired by the models of Clark and colleagues (Clark &
Wilkes-Gibbs, 1986; Brennan & Clark, 1996);
• intentional influences factors, inspired by the model of Jordan (2000b).
We develop features representing these factors, then use the features to represent examples of object descriptions and the context in which they occur for the purpose of learning
a model of content selection for object descriptions.
Dale and Reiter’s incremental model focuses on the production of near-minimal subsequent references that allow the hearer to reliably distinguish the task object from similar
task objects. Following Grosz and Sidner (1986), Dale and Reiter’s algorithm utilizes discourse structure as an important factor in determining which objects the current object
must be distinguished from. The model of Clark, Brennan and Wilkes-Gibbs is based on
the notion of a conceptual pact, i.e. the conversants attempt to coordinate with one
another by establishing a conceptual pact for describing an object. Jordan’s intentional
influences model is based on the assumption that the underlying communicative and
task-related inferences are important factors in accounting for non-minimal descriptions.
We describe these models in more detail in Section 3 and explain why we expect these
models to work well in combination.
Many aspects of the underlying content selection models are not well-defined from an
implementation point of view, so it may be necessary to experiment with different definitions
and related parameter settings to determine which will produce the best performance for a
model, as was done with the parameter setting experiments carried out by Jordan (2000b).1
However, in the experiments we describe in this paper, we strive for feature representations
that will allow the machine learner to take on more of the task of finding optimal settings
and otherwise use the results reported by Jordan (2000b) for guidance. The only variation
we test here is the representation of discourse structure for those models that require it.
Otherwise, explicit tests of different interpretations of the models are left to future work.
We report on a set of experiments designed to establish the predictive power of the factors emphasized in the three models by using machine learning to train and test the content
selection component of an object description generator on a set of 393 object descriptions
from the corpus of coconut dialogues. The generator goes beyond each of the models’
accounts for anaphoric expressions to address the more general problem of generating both
initial and subsequent expressions. We provide the machine learner with distinct sets of
features motivated by these models, in addition to discourse features motivated by assumed
1. Determining optimal parameter settings for a machine learning algorithm is a similar issue (Daelemans
& Hoste, 2002) but at a different level. We use the same machine learner and parameter settings for all
our experiments although searching for optimal machine learner parameter settings may be of value in
further improving performance.

159

Jordan & Walker

familiarity distinctions (Prince, 1981) (i.e. new vs. evoked vs. inferable discourse entities),
and dialogue specific features such as the speaker of the object description, its absolute
location in the discourse, and the problem that the conversants are currently trying to
solve. We evaluate the object description generator by comparing its predictions against
what humans said at the same point in the dialogue and only counting as correct those that
exactly match the content of the human generated object descriptions (Oberlander, 1998).2
This provides a rigorous test of the object description generator since in all likelihood there
are other object descriptions that would have achieved the speaker’s communicative goals.
We also quantify the contribution of each feature set to the performance of the object
description generator. The results indicate that the intentional influences features, the
incremental features and the conceptual pact features are all independently significantly better than the majority class baseline for this task, with the intentional influences model (42.4%) performing significantly better than either the incremental model
(30.4%) or the conceptual pact model (28.9%). However, the best performing models
combine features from all the models, achieving accuracies at matching human performance
near 60.0%, a large improvement over the majority class baseline of 17% in which the generator simply guesses the most frequent attribute combination. Surprisingly, our results
in experimenting with different discourse structure parameter settings show that features
derived from a simple recency-based model of discourse structure contribute as much to this
particular task as one based on intentional structure.
The coconut dataset is small compared to those used in most machine learning experiments. Smaller datasets run a higher risk of overfitting and thus specific performance
results should be interpreted with caution. In addition the coconut corpus represents only
one type of dialogue; typed, collaborative, problem solving dialogues about constraint satisfaction problems. While the models and suggested features focus on general communicative
issues, we expect variations in the task involved and in the communication setting to impact the predictive power of the feature sets. For example, the conceptual pact model
was developed using dialogues that focus on identifying novel, abstract figures. Because
the figures are abstract it is not clear at the start of a series of exercises what description
will best help the dialogue partner identify the target figure. Thus the need to negotiate
a description for the figures is more prominent than in other tasks. Likewise we expect
constraint satisfaction problems and the need for joint agreement on a solution to cause the
intentional influences model to be more prominent for the coconut dialogues. But
the fact that the conceptual pact features show predictive power that is significantly
better than the baseline suggests that while the prominence of each model inspired feature
set may vary across tasks and communication settings, we expect each to have a significant
contribution to make to a content selection model.
Clearly, for those of us whose ultimate goal is a general model of content selection
for dialogue, we need to carry out experiments on a wide range of dialogue types. But
for those of us whose ultimate goal is a dialogue application, one smaller corpus that is
representative of the anticipated dialogues is probably preferable. Despite the two notes of
2. Note that the more attributes a discourse entity has, the harder it is to achieve an exact match to a
human description, i.e. for this problem the object description generator must correctly choose among
16 possibilities represented by the power set of the four attributes.

160

Learning Content Selection Rules for Generating Object Descriptions

caution we expect our feature representations to suggest a starting point for both larger
endeavors.
Previous research has applied machine learning to several problems in natural language
generation, such as cue word selection (Di Eugenio, Moore, & Paolucci, 1997), accent placement (Hirschberg, 1993), determining the form of an object description (Poesio, 2000),
content ordering (Malouf, 2000; Mellish, Knott, Oberlander, & O’Donnell, 1998; Duboue
& McKeown, 2001; Ratnaparkhi, 2002), sentence planning (Walker, Rambow, & Rogati,
2002), re-use of textual descriptions in automatic summarization (Radev, 1998), and surface realization (Langkilde & Knight, 1998; Bangalore & Rambow, 2000; Varges & Mellish,
2001).
The only other machine learning approaches for content selection are those of Oh and
Rudnicky (2002) and of Roy (2002). Oh and Rudnicky report results for automatically
training a module for the CMU Communicator system that selects the attributes that the
system should express when implicitly confirming flight information in an ongoing dialogue.
For example, if the caller said I want to go to Denver on Sunday, the implicit confirmation
by the system might be Flying to Denver on Sunday. They experimentally compared a
statistical approach based on bigram models with a strategy that only confirms information
that the system has just heard for the first time, and found that the two systems performed
equally well. Roy reports results for a spoken language generator that is trained to generate
visual descriptions of geometric objects when provided with features of visual scenes. Roy’s
results show that the understandability of the automatically generated descriptions is only
8.5% lower than human-generated descriptions. Unlike our approach, neither of these consider the effects of ongoing dialogue with a dialogue partner, or the effect of the dialogue
context on the generated descriptions. Our work, and the theoretical models it is based
on, explicitly focus on the processes involved in generating descriptions and redescriptions
of objects in interactive dialogue that allow the dialogue partners to remain aligned as the
dialogue progresses (Pickering & Garrod, 2004).
The most relevant prior work is that of Jordan (2000b). Jordan implemented Dale and
Reiter’s incremental model and developed and implemented the intentional influences model, which incorporates the incremental model, and tested them both against
the coconut corpus. Jordan also experimented with different parameter settings for vague
parts of the models. The results of this work are not directly comparable because Jordan
only tested rules for subsequent reference, while here we attempt to learn rules for generating both initial and subsequent references. However, using a purely rule-based approach,
the best accuracy that Jordan reported was 69.6% using a non-stringent scoring criterion
(not an exact match) and 24.7% using the same stringent exact match scoring used here.
In this paper, using features derived from Jordan’s corpus annotations, and applying rule
induction to induce rules from training data, we achieve an exact match accuracy of nearly
47% when comparing to the most similar model and an accuracy of nearly 60% when comparing to the best overall model. These results appear to be an improvement over those
reported by Jordan (2000b), given both the increased accuracy and the ability to generate
initial as well as subsequent references.
Section 2 describes the coconut corpus, definitions of discourse entities and object
descriptions for the coconut domain, and the annotations on the corpus that we use
to derive the feature sets. Section 3 presents the theoretical models of content selection
161

Jordan & Walker

Opal 1

YOUR INVENTORY
1
0
1
1
1
2
0
0

TABLE-HIGH YELLOW $400
SOFA GREEN $350
SOFA YELLOW $400
RUG RED $200
LAMP-FLOOR BLUE $50
CHAIR BLUE $75
CHAIR GREEN $100
CHAIR RED $100

End of Turn

Design Complete

PARTNER’S INVENTORY
TABLE-LOW
TABLE-HIGH
RUG
SOFA
LAMP-TABLE
LAMP-FLOOR
CHAIR
ARMCHAIR
DESK

> change the chairs,
I have two red ones for the same price.
As much as I like green
it looks ugly with red.

LIVING-ROOM
> we bought the green sofa 350,
the green table 400,
and 2 green chairs 100 each.

DINING-ROOM

100
400
350

100

100

100

400

Your budget is: $400

Figure 2: A snapshot of the interface for the coconut task
for object descriptions in more detail and describes the features inspired by the models.
Section 4 describes the experimental design and Section 5 presents the quantitative results
of testing the learned rules against the corpus, discusses the features that the machine
learner identifies as important, and provides examples of the rules that are learned. Section
6 summarizes the results and discusses future work.

2. The Coconut Corpus
The coconut corpus is a set of 24 computer-mediated dialogues consisting of a total of
1102 utterances. The dialogues were collected in an experiment where two human subjects
collaborated on a simple design task, that of buying furniture for two rooms of a house
(Di Eugenio et al., 2000). Their collaboration was carried out through a typed dialogue
in a workspace where each action and utterance was automatically logged. An excerpt
of a coconut dialogue is in Figure 1. A snapshot of the workspace for the coconut
experiments is in Figure 2.
In the experimental dialogues, the participants’ main goal is to negotiate the purchases;
the items of highest priority are a sofa for the living room and a table and four chairs for the
dining room. The participants also have specific secondary goals which further constrain
the problem solving task. Participants are instructed to try to meet as many of these
goals as possible, and are motivated to do so by rewards associated with satisfied goals.
162

Learning Content Selection Rules for Generating Object Descriptions

The secondary goals are: 1) match colors within a room, 2) buy as much furniture as you
can, 3) spend all your money. The participants are told which rewards are associated with
achieving each goal.
Each participant is given a separate budget (as shown in the mid-bottom section of
Figure 2) and an inventory of furniture (as shown in the upper-left section of Figure 2).
Furniture types include sofas, chairs, rugs and lamps, and the possible colors are red, green,
yellow or blue. Neither participant knows what is in the other’s inventory or how much
money the other has. By sharing information during the conversation, they can combine
their budgets and select furniture from each other’s inventories. Note that since a participant
does not know what furniture his partner has available until told, there is a menu (see the
mid-right section of Figure 2) that allows the participant to create furniture items based on
his partner’s description of the items available. The participants are equals and purchasing
decisions are joint. In the experiment, each set of participants solved one to three scenarios
with varying inventories and budgets. The problem scenarios varied task complexity by
ranging from tasks where items are inexpensive and the budget is relatively large, to tasks
where the items are expensive and the budget relatively small.
2.1 Discourse Entities and Object Descriptions in the Corpus
A discourse model is used to keep track of the objects discussed in a discourse. As an object
is described, the conversants relate the information about the object in the utterance to the
appropriate mental representation of the object in the discourse model (Karttunen, 1976;
Webber, 1978; Heim, 1983; Kamp & Reyle, 1993; Passonneau, 1996). The model contains
discourse entities, attributes and links between entities (Prince, 1981). A discourse entity
is a variable or placeholder that indexes the information about an object described in a
particular linguistic description to an appropriate mental representation of the object. The
discourse model changes as the discourse progresses. When an object is first described, a
discourse entity such as ei is added to the discourse model. As new utterances are produced,
additional discourse entities may be added to the model when new objects are described,
and new attributes may get associated with ei whenever it is redescribed. Attributes are not
always supplied by a noun phrase (NP). They may arise from other parts of the utterance
or from discourse inference relations that link to other discourse entities.
To illustrate the discourse inference relations relevant to coconut, in (1b) the green set
is an example of a new discourse entity which has a set/subset discourse inference relation
to the three distinct discourse entities for 2 $25 green chairs, 2 $100 green chairs and 1
$200 green table.
(1) a. : I have [2 $25 green chairs] and [a $200 green table].
b. : I have [2 $100 green chairs]. Let’s get [the green set].
A class inference relation exists when the referent of a discourse entity has a subsumption
relationship with a previous discourse entity. For example, in (2) the table and your green
one have a subsumption relationship.
(2)

Let’s decide on [the table] for the dining room. How about [your green one]?
163

Jordan & Walker

A common noun anaphora inference relation occurs in the cases of one anaphora and
null anaphora. For example, in (3) each of the marked NPs in the last part of the utterance
has a null anaphora relation to the marked NP in the first part. Note that this example
also has a class inference relation as well.
(3)

I have [a variety of high tables] ,[green], [red] and [yellow] for 400, 300, and 200.

Discourse entities can also be related by predicative relationships such as is. For example, in (4) the entities defined by my cheapest table and a blue one for $200 are not the
same discourse entities but the information about one provides more information about the
other. Note that this example also includes common noun anaphora and class inference
relations.
(4)

[My cheapest table] is [a blue one for $200].

An object description is any linguistic expression (usually an NP) that initiates the creation or update of a discourse entity for a furniture item, along with any explicit attributes
expressed within the utterance. We consider the attributes that are explicitly expressed
outside of an NP to be part of the object description since they can be realized either as
part of the noun phrase that triggers the discourse entity or elsewhere in the utterance.
Attributes that are inferred (e.g. quantity from “a” or “the”) help populate the discourse
entity but are not considered part of an object description since inferred attributes may
or may not reflect an explicit choice. The inferred attribute could be a side-effect of the
surface structure selected for realizing the object description.3
2.2 Corpus Annotations
After the corpus was collected, it was annotated by human coders for three types of
features: problem-solving utterance level features as shown in Figure 3, discourse
utterance level features as illustrated in Figure 4 and discourse entity level features as illustrated in Figure 5. Some additional features are shown in Figure 6. Each of
the feature encodings shown are for the dialogue excerpt in Figure 1.
All of the features were hand-labelled on the corpus because it is a human-human corpus
but, as we will discuss further at the end of this section, many of these features would need
to be established by a system for its collaborative problem solving component to function
properly.
Looking first at Figure 6, it is the explicit attributes (as described in the previous
section) that are to be predicted by the models we are building and testing. The remaining
features are available as context for making the predictions.
The problem-solving utterance level features in Figure 3 capture the problem
solving state in terms of the goals and actions that are being discussed by the conversants,
constraint changes that are implicitly assumed, or explicitly stated by the conversants, and
the size of the solution set for the current constraint equations. The solution set size for
3. While the same is true of some of the attributes that are explicitly expressed (e.g. “I” in subject position
expresses the ownership attribute), most of the attribute types of interest in the corpus are adjuncts
(e.g. “Let’s buy the chair [for $100].”).

164

Learning Content Selection Rules for Generating Object Descriptions

Utterance

37
38
39
40
42
43
44
46
47
48
51
52

Goal/
Action
Label
SelectOptionalItemLR
SelectOptionalItem
SelectOptionalItem
SelectOptionalItemLR
SelectOptionalItemLR
SelectOptionalItemDR
SelectOptionalItemDR
SelectOptionalItemDR
SelectOptionalItemDR
SelectOptionalItemLR
SelectOptionalItemDR
SelectOptionalItemDR,
SelectChairs
SelectOptionalItemLR

Introduce
or
continue
introduce
introduce
continue
continue
continue
continue
continue
continue
continue
continue
continue
continue
introduce
continue

Goal/
Action
Identifier
act4
act5
act5
act4
act4
act5
act5
act5
act4
act5
act5
act5,
act3
act4

Change
in
Constraints
drop color match
color,price limit
none
none
none
none
none
none
none

Solution
Size

none
none

determinate
determinate

none

determinate

indeterminate
indeterminate
indeterminate
determinate
determinate
indeterminate
determinate
determinate
determinate

Figure 3: Problem solving utterance level annotations for utterances relevant to problem
solving goals and actions for the dialogue excerpt in Figure 1

Utterance

Influence
on Listener

Influence
on Speaker

37
40
42
43
44
46
47
48
49
51
52

ActionDirective
ActionDirective
ActionDirective
OpenOption
ActionDirective
ActionDirective
ActionDirective
ActionDirective
ActionDirective
ActionDirective
ActionDirective

Offer
Commit
Commit
nil
Offer
Commit
Commit
Commit
Offer
Offer
Commit

Figure 4: Discourse utterance level annotations for utterances relevant to establishing joint
agreements for the dialogue excerpt in Figure 1

a constraint equation is characterized as being determinate if the set of values is closed
and represents that the conversants have shared relevant values with one another. An
indeterminate size means that the set of values in still open and so a solution cannot yet
be determined. The problem-solving features capture some of the situational or problemsolving influences that may effect descriptions and indicate the task structure from which
the discourse structure can be derived (Terken, 1985; Grosz & Sidner, 1986). Each domain
165

Jordan & Walker

Utterance

37
38
39
40
42
43
44
47
47
48
51
51
52

Reference
and
Coreference
initial ref-1
initial ref-2
initial ref-3
corefers ref-1
corefers ref-1
initial ref-4
corefers ref-4
corefers ref-1
corefers ref-4
corefers ref-4
corefers ref-4
initial ref-5
corefers ref-1

Discourse
Inference
Relations
nil
nil
class to ref-20
nil
nil
nil
CNAnaphora ref-4
nil
nil
nil
nil
set of ref-12,ref-16

Attribute
Values
my,1,yellow,rug,150
your,furniture,100
my,furniture,100
your,1,yellow,rug,150
my,1,rug,150
my,1,green,chair
my,100
your,1,yellow,rug
your,1,green,chair
my,1,green,chair,100
1,green,chair
chair
1,yellow rug

Argument
for Goal/Action
Identifier
act4
act5
act5
act4
act4
act5
act5
act4
act5
act5
act5
act3
act4

Figure 5: Discourse entity level annotations for utterances referring to furniture items in
Figure 1

Utterance

Speaker

37
38
39
40
42
43
44
47
47
48
51
51
52

G
G
S
S
G
G
G
S
S
G
S
S
S

Explicit
Attributes
type,color,price,owner
type,color,price,owner
type,price,owner
type,color,price,owner
type,price,owner
type,color,owner
price,owner
type,color
type,color
type,color,price,owner
type,color
type
type,color

Inferred
Attributes
quantity

quantity
quantity
quantity
owner,quantity
owner,quantity
quantity
quantity
quantity
quantity

Description
a yellow rug for 150 dollars
furniture ... for 100 dollars
furniture ... 100 dollars
the yellow rug for $150
the rug for 150 dollars
a green chair
[0] for 100 dollars
the yellow rug
the green chair
the green 100 dollar chair
the green chair
the other chairs
the yellow rug

Figure 6: Additional features for the dialogue excerpt in Figure 1

goal provides a discourse segment purpose so that each utterance that relates to a different
domain goal or set of domain goals defines a new segment.
The discourse utterance level features in Figure 4 encode the influence the utterance is expected to have on the speaker and the listener as defined by the DAMSL
scheme (Allen & Core, 1997). These annotations also help capture some of the situational
influences that may effect descriptions. The possible influences on listeners include open
options, action directives and information requests. The possible influences on speakers are
offers and commitments. Open options are options that a speaker presents for the hearer’s
future actions, whereas with an action directive a speaker is trying to put a hearer under
166

Learning Content Selection Rules for Generating Object Descriptions

an obligation to act. There is no intent to put the hearer under obligation to act with
an open option because the speaker may not have given the hearer enough information to
act or the speaker may have clearly indicated that he does not endorse the action. Offers
and commitments are both needed to arrive at a joint commitment to a proposed action.
With an offer the speaker is conditionally committing to the action whereas with a commit
the speaker is unconditionally committing. With a commit, the hearer may have already
conditionally committed to the action under discussion, or the speaker may not care if the
hearer is also committed to the action he intends to do.
The discourse entity level features in Figure 5 define the discourse entities that
are in the discourse model. Discourse entities, links to earlier discourse entities and the
attributes expressed previously for a discourse entity at the NP-level and utterance level
are inputs for an object description generator. Part of what is used to define the discourse
entities is discourse reference relations which include initial, coreference and discourse inference relations between different entities such as the links we described earlier; set/subset,
class, common noun anaphora and predicative. In addition, in order to link the expression
to appropriate problem solving actions, the action for which the entity is an argument is
also annotated. In order to test whether an acceptable object description is generated by
a model for a discourse entity in context, the explicit attributes used to describe the entity
are also annotated (recall Figure 6).
Which action an entity is related to helps associate entities with the correct parts of
the discourse structure and helps determine which problem-solving situations are relevant
to a particular entity. From the other discourse entity level annotations, initial representations of discourse entities and updates to them can be derived. For example, the initial
representation for “I have a yellow rug. It costs $150.” would include type, quantity, color
and owner following the first utterance. Only the quantity attribute is inferred. After the
second utterance the entity would be updated to include price.
The encoded features all have good inter-coder reliability as shown by the kappa values
given in Table 1 (Di Eugenio et al., 2000; Jordan, 2000b; Krippendorf, 1980). These values
are all statistically significant for the size of the labelled data set, as shown by the p-values
in the table.
Discourse
Entity
Level

Problem
Solving
Utterance
Level
Discourse
Utterance
Level

Reference
and
Coreference
.863
(z=19, p<.01)
Introduce
Goal/Action
.897
(z=8, p<.01)
Influence
on Listener
.72
(z=19, p<.01)

Discourse
Inference
Relations
.819
(z=14, p<.01)
Continue
Goal/Action
.857
(z=27, p<.01)
Influence
on Speaker
.72
(z=13, p<.01)

Argument
for Goal/
Action
.857
(z=16, p<.01)
Change in
Constraints
.881
(z=11, p<.01)

Attributes

.861
(z=53, p<.01)
Solution
Size
.8
(z=6, p<.01)

Table 1: Kappa values for the annotation scheme
167

Goal/Action
.74
(z=12, p<.01)

Jordan & Walker

While the availability of some of this annotated information in a dialogue system is
currently an ongoing challenge for today’s systems, a system that is to be a successful
dialogue partner in a collaborative problem solving dialogue, where all the options are not
known a priori, will have to model and update discourse entities, understand the current
problem solving state and what has been agreed upon, and be able to make, accept or reject
proposed solutions. Certainly, not all dialogue system domains and communicative settings
will need all of this information and likewise some of the information that is essential for
other domains and settings will not be necessary to engage in a coconut dialogue.
The experimental data consists of 393 non-pronominal object descriptions from 13 dialogues of the coconut corpus as well as features constructed from the annotations described
above. The next section explains in more detail how the annotations are used to construct
the features used in training the models.

3. Representing Models of Content Selection for Object Descriptions as
Features
In Section 1, we described how we would use the annotations on the coconut corpus to
construct feature sets motivated by theories of content selection for object descriptions.
Here we describe these theories in more detail, and present, with each theory, the feature
sets that are inspired by the theory. In Section 4 we explain how these features are used to
automatically learn a model of content selection for object descriptions. In order to be used
in this way, all of the features must be represented by continuous (numeric), set-valued, or
symbolic (categorial) values.
Models of content selection for object descriptions attempt to explain what motivates a
speaker to use a particular set of attributes to describe an object, both on the first mention
of an object as well as in subsequent mentions. In an extended discourse, speakers often
redescribe objects that were introduced earlier in order to say something more about the
object or the event in which it participates. We will test in part an assumption that many
of the factors relevant for redescriptions will also be relevant for initial descriptions.
All of the models described below have previously had rule-based implementations of
them tested on the coconut corpus and were all found to be nearly equally good at explaining the redescriptions in the corpus (Jordan, 2000b). All of them share a basic assumption
about the speaker’s goal when redescribing a discourse entity already introduced into the
discourse model in prior conversation. The speaker’s primary goal is identification, i.e. to
generate a linguistic expression that will efficiently and effectively re-evoke the appropriate
discourse entity in the hearer’s mind. A redescription must be adequate for re-evoking the
entity unambiguously, and it must do so in an efficient way (Dale & Reiter, 1995). One
factor that has a major effect on the adequacy of a redescription is the fact that a discourse
entity to be described must be distinguished from other discourse entities in the discourse
model that are currently salient. These other discourse entities are called distractors. Characteristics of the discourse entities evoked by the dialogue such as recency and frequency
of mention, relationship to the task goals, and position relative to the structure of the
discourse are hypothesized as means of determining which entities are mutually salient for
both conversants.
168

Learning Content Selection Rules for Generating Object Descriptions

• what is mutually known: type-mk, color-mk, owner-mk, price-mk, quantity-mk
• reference-relation: one of initial, coref, set, class, cnanaphora, predicative

Figure 7: Assumed Familiarity Feature Set.

We begin the encoding of features for the object description generator with features
representing the fundamental aspects of a discourse entity in a discourse model. We divide
these features into two sets: the assumed familiarity feature set and the inherent
feature set. The assumed familiarity features in Figure 7 encode all the information
about a discourse entity that is already represented in the discourse model at the point
in the discourse at which the entity is to be described. These attributes are assumed to
be mutually known by the conversational participants and are represented by five boolean
features: type-mk, color-mk, owner-mk, price-mk, quantity-mk. For example, if type-mk has
the value of yes, this represents that the type attribute of the entity to be described is
mutually known.
Figure 7 also enumerates a reference-relation feature as described in Section 2 to encode
whether the entity is new (initial), evoked (coref) or inferred relative to the discourse
context. The types of inferences supported by the annotation are set/subset, class, common
noun anaphora (e.g. one and null anaphora), and predicative (Jordan, 2000b), which are
represented by the values (set,class,cnanaphora,predicative). These reference relations are relevant to both initial and subsequent descriptions.
• utterance-number, speaker-pair, speaker, problem-number
• attribute values:
– type: one of sofa, chair, table, rug, lamp, superordinate
– color: one of red, blue, green, yellow
– owner: one of self, other, ours
– price: range from $50 to $600
– quantity: range from 0 to 4.

Figure 8: Inherent Feature Set: Task, Speaker and Discourse Entity Specific features.

The inherent features in Figure 8 are a specific encoding of particulars about the
discourse situation, such as the speaker, the task, and the actual values of the entity’s known
attributes (type, color, owner, price, quantity). We supply the values for the attributes in
case there are preferences associated with particular values. For example, there may be a
preference to include quantity, when describing a set of chairs, or price, when it is high.
The inherent features allow us to examine whether there are individual differences in
selection models (speaker, speaker-pair), or whether specifics about the attributes of the
169

Jordan & Walker

object, the location within the dialogue (utterance-number), and the problem difficulty
(problem-number) play significant roles in selecting attributes. The attribute values for an
entity are derived from annotated attribute features and the reference relations.
We don’t expect rules involving this feature set to generalize well to other dialogue
situations. Instead we expect them to lead to a situation specific model. Whenever
these features are used there is overfitting regardless of the training set size. Consider that
a particular speaker, speaker-pair or utterance number are specific to particular dialogues
and are unlikely to occur in another dialogue, even a new coconut dialogue. These feature
representations would have to be abstracted to be of value in a generator.
3.1 Dale and Reiter’s Incremental Model
Most computational work on generating object descriptions for subsequent reference (Appelt, 1985a; Kronfeld, 1986; Reiter, 1990; Dale, 1992; Heeman & Hirst, 1995; Lochbaum,
1995; Passonneau, 1996; van Deemter, 2002; Gardent, 2002; Krahmer, van Erk, & Verleg,
2003) concentrates on how to produce a minimally complex expression that singles out
the discourse entity from a set of distractors. The set of contextually salient distractors is
identified via a model of discourse structure as mentioned above. Dale and Reiter’s incremental model is the basis of much of the current work that relies on discourse structure
to determine the content of object descriptions for subsequent reference.
The most commonly used account of discourse structure for task-oriented dialogues is
Grosz and Sidner’s (1986) theory of the attentional and intentional structure of discourse.
In this theory, a data structure called a focus space keeps track of the discourse entities
that are salient in a particular context, and a stack of focus spaces is used to store the focus
spaces for the discourse as a whole. The content of a focus space and operations on the
stack of focus spaces is determined by the structure of the task. A change in task or topic
indicates the start of a new discourse segment and a corresponding focus space. All of the
discourse entities described in a discourse segment are classified as salient for the dialogue
participants while the corresponding focus space is on the focus stack. Approaches that use
a notion of discourse structure take advantage of this representation to produce descriptors
that are minimally complex given the current focus space, i.e. the description does not have
to be unambiguous with respect to the global discourse.
According to Dale and Reiter’s model, a descriptor containing information that is not
needed to identify the referent given the current focus space would not be minimally complex
but a small number of overspecifications that appear relative to the identification goal are
expected and can be explained as artifacts of cognitive processing limits. Trying to produce
a minimally complex description can be seen as an implementation of the two parts of
Grice’s Maxim of Quantity, according to which an utterance should both say as much as
is required, and no more than is required (Grice, 1975). Given an entity to describe and a
distractor set defined by the entities in the current focus space, the incremental model
incrementally builds a description by checking a static ordering of attribute types and
selecting an attribute to include in the description if and only if it eliminates some of the
remaining distractors. As distractors are ruled out, they no longer influence the selection
process.
170

Learning Content Selection Rules for Generating Object Descriptions

• Distractor Frequencies: type-distractors, color-distractors, owner-distractors, price-distractors, quantity-distractors
• Attribute Saliency: majority-type, majority-type-freq, majority-color, majority-color-freq,
majority-price, majority-price-freq, majority-owner, majority-owner-freq, majority-quantity,
majority-quantity-freq

Figure 9: contrast set Feature Sets

A set of features called contrast set features are used to represent aspects of Dale
and Reiter’s model. See Figure 9. The goal of the encoding is to represent whether there
are distractors present in the focus space which might motivate the inclusion of a particular
attribute. First, the distractor frequencies encode how many distractors have an attribute
value that is different from that of the entity to be described.
The incremental model also utilizes a preferred salience ordering for attributes and
eliminates distractors as attributes are added to a description. For example, adding the
attribute type when the object is a chair, eliminates any distractors that aren’t chairs. A
feature based encoding cannot easily represent a distractor set that changes as attribute
choices are made. To compensate, our encoding treats attributes instead of objects as
distractors so that the attribute saliency features encode which attribute values are most
salient for each attribute type, and a count of the number of distractors with this attribute
value. For example, if 5 of 8 distractors are red then majority-color is red and the majoritycolor-freq is 5. Taking the view of attributes as distractors has the advantage that the
preferred ordering of attributes can adjust according to the focus space. This interpretation
of Dale and Reiter’s model was shown to be statistically similar to the strict model but
with a higher mean match to the corpus (Jordan, 2000b). Thus our goal in adding these
additional features is to try to obtain the best possible performance for the incremental
model.
Finally, an open issue with deriving the distractors is how to define a focus space (Walker,
1996a). As described above, Grosz and Sidner’s theory of discourse creates a data structure
called a focus space for each discourse segment, where discourse segments are based on the
intentions underlying the dialogue. However Grosz and Sidner provide no clear criterion
for assigning the segmentation structure. In order to explore what definition variations will
work best, we experiment with three focus space definitions, two very simple focus space
definitions based on recency, and the other based on intentional structure as described
below. To train and test for the three focus space definitions, we create separate datasets
for each of the three. To our knowledge, this is the first empirical comparison of Grosz and
Sidner’s model with a simpler model for any discourse-related task.
For intentional structure, we utilize the problem solving utterance features hand-labelled
on the coconut corpus with high reliability as discussed above in Section 2. The annotated
task goals are used to derive an intentional structure for the discourse, which provides a
segmentation of the discourse, as described by Grosz and Sidner (1986). The current focus
space as defined by the annotated task goals is used to define segment distractors. This
dataset we label as segment. For recency, one extremely simple focus space definition
171

Jordan & Walker

uses only the discourse entities from the most recent utterance as possible distractors. This
dataset we label as one utterance. The second extremely simple focus space definition
only considers the discourse entities from the last five utterances as possible distractors.
This dataset we label as five utterance. For each dataset, the features in Figure 9 are
computed relative to the distractors determined by its focus space definition.
3.2 Jordan’s Intentional Influences Model
Jordan (2000b) proposed a model to select attributes for object descriptions for subsequent reference called the intentional influences model. This model posits that along
with the identification goal, task-related inferences and the agreement process for task negotiation are important factors in selecting attributes. Attributes that are not necessary
for identification purposes may be intentional redundancies with a communicative purpose
(Walker, 1996b) and not always just due to cognitive processing limits on finding minimally
complex descriptions (Jordan, 2000b).
A goal-directed view of sentence generation suggests that speakers can attempt to satisfy
multiple goals with each utterance (Appelt, 1985b). It suggests that this strategy also
applies to lower-level forms within the utterance (Stone & Webber, 1998). That is, the same
form can opportunistically contribute to the satisfaction of multiple goals. This many-one
mapping of goals to linguistic forms is more generally referred to as overloading intentions
(Pollack, 1991). Subsequent work has shown that this overloading can involve trade-offs
across linguistic levels. That is, an intention which is achieved by complicating a form at
one level may allow the speaker to simplify another level by omitting important information.
For example, a choice of clausal connectives at the pragmatic level can simplify the syntactic
level (Di Eugenio & Webber, 1996), and there are trade-offs in word choice at the syntax
and semantics levels (Stone & Webber, 1998).
The intentional influences model incorporates multiple communicative and problem solving goals in addition to the main identification goal in which the speaker intends
the hearer to re-evoke a particular discourse entity. The contribution of this model is that it
overloads multiple, general communicative and problem solving goals when generating a description. When the model was tested on the coconut corpus, inferences about changes in
the problem solving constraints, about conditional and unconditional commitments to proposals, and about the closing of goals were all shown to be relevant influences on attribute
selection (Jordan, 2000a, 2002) while goals to verify understanding and infer informational
relations were not (Jordan, 2000b).4
The features used to approximate Jordan’s model are in Figure 10. These features cover
all of the general communicative and problem solving goals hypothesized by the model
except for the identification goal and the information relation goal. Because of the difficulty
of modelling an information relation goal with features, its representation is left to future
work.5
4. A different subset of the general goals covered by the model are expected to be influential for other
domains and communication settings, therefore a general object description generator would need to be
trained on a wide range of corpora.
5. Information relation goals may relate two arbitrarily distant utterances and additional details beyond
distance are expected to be important. Because this goal previously did not appear relevant for the
coconut corpus (Jordan, 2000b), we gave it a low priority for implementation.

172

Learning Content Selection Rules for Generating Object Descriptions

• task situation: goal, colormatch, colormatch-constraintpresence, pricelimit, pricelimit-constraintpresence, priceevaluator, priceevaluator-constraintpresence, colorlimit, colorlimit-constraintpresence, priceupperlimit, priceupperlimit-constraintpresence
• agreement state: influence-on-listener, commit-speaker, solution-size, prev-influence-on-listener, prev-commit-speaker, prev-solution-size, distance-of-last-state-in-utterances, distanceof-last-state-in-turns, ref-made-in-prev-action-state, speaker-of-last-state, prev-ref-state
• previous agreement state description: prev-state-type-expressed, prev-state-color-expressed,
prev-state-owner-expressed, prev-state-price-expressed, prev-state-quantity-expressed
• solution interactions: color-contrast, price-contrast

Figure 10: Intentional Influences Feature Set.

The task situation features encode inferable changes in the task situation that are related
to item attributes, where colormatch is a boolean feature that indicates whether there has
been a change in the color match constraint. The pricelimit, colorlimit and priceupperlimit
features are also boolean features representing that there has been a constraint change
related to setting limits on values for the price and color attributes. The features with
constraintpresence appended to a constraint feature name are symbolic features that indicate
whether the constraint change was implicit or explicit. For example, if there is an agreed
upon constraint to try to select items with the same color value for a room, and a speaker
wants to relax that constraint then the feature colormatch would have the value yes. If the
speaker communicated this explicitly by saying “Let’s forget trying to match colors.” then
the constraintpresence feature would have the value explicit and otherwise it would have
the value implicit. If the constraint change is not explicitly communicated and the speaker
decides to include a color attribute when it is not necessary for identification purposes, it
may be to help the hearer infer that he means to drop the constraint
The agreement state features in Figure 10 encode critical points of agreement during
problem solving. Critical agreement states are (Di Eugenio et al., 2000):
• propose: the speaker offers the entity and this conditional commitment results in a
determinate solution size.
• partner decidable option: the speaker offers the entity and this conditional commitment results in an indeterminate solution size.
• unconditional commit: the speaker commits to an entity.
• unendorsed option: the speaker offers the entity but does not show any commitment
to using it when the solution size is already determinate.
For example, if a dialogue participant is unconditionally committing in response to a
proposal, she may want to verify that she has the same item and the same entity description as her partner by repeating back the previous description. The features that
encode these critical agreement states include some DAMSL features (influence-on-listener,
173

Jordan & Walker

commit-speaker, prev-influence-on-listener, prev-commit-speaker), progress toward a solution (solution-size, prev-solution-size, ref-made-in-prev-action-state), and features inherent
to an agreement state (speaker-of-last-state, distance-of-last-state-in-utterances, distanceof-last-state-in-turns). The features that make reference to a state are derived from the
agreement state features and a more extensive discourse history than can be encoded within
the feature representation. In addition, since the current agreement state depends in part
on the previous agreement state, we added the derived agreement state. The previous
agreement state description features in Figure 10 are booleans that capture dependencies
of the model on the content of the description from a previous state. For example, if the
previous agreement state for an entity expressed only type and color attributes then this
would be encoded yes for prev-state-type-expressed and prev-state-color-expressed and no
for the rest.
The solution interactions features in Figure 10 represent situations where multiple proposals are under consideration which may contrast with one another in terms of solving
color-matching goals (color-contrast) or price related goals (price-contrast). When the
boolean feature color-contrast is true, it means that the entity’s color matches with the
partial solution that has already been agreed upon and contrasts with the alternatives that
have been proposed. In this situation, there may be grounds for endorsing this entity relative to the alternatives. For example, in response to S’s utterance [37] in Figure 1, in a
context where G earlier introduced one blue rug for $175, G could have said “Let’s use my
blue rug.” in response. In this case the blue rug would have a true value for color-contrast
because it has a different color than the alternative, and it matches the blue sofa that had
already been selected.
The boolean feature price-contrast describes two different situations. When the feature
price-contrast is true, it either means that the entity has the best price relative to the
alternatives, or when the problem is nearly complete, that the entity is more expensive
than the alternatives. In the first case, the grounds for endorsement are that the item is
cheaper. In the second case, it may be that the item will spend out the remaining budget
which will result in a higher score for the problem solution.
Note that although the solution interaction features depend upon the agreement states,
in that it is necessary to recognize proposals and commitments in order to identify alternatives and track agreed upon solutions, it is difficult to encode such extensive historical
information directly in a feature representation. Therefore the solution interaction features
are derived, and the derivation includes heuristics that use agreement state features for
estimating partial solutions. A sample encoding for the dialogue excerpt in Figure 1 for its
problem solving utterance level annotations and agreement states were given in Figures 3
and 4.
3.3 Brennan and Clark’s Conceptual Pact Model
Brennan and Clark’s conceptual pact model focuses on the bidirectional adaptation of
each conversational partner to the linguistic choices of the other conversational participant.
The conceptual pact model suggests that dialogue participants negotiate a description
that both find adequate for describing an object (Clark & Wilkes-Gibbs, 1986; Brennan
& Clark, 1996). The speaker generates trial descriptions that the hearer modifies based
174

Learning Content Selection Rules for Generating Object Descriptions

on which object he thinks he is suppose to identify. The negotiation continues until the
participants are confident that the hearer has correctly identified the intended object.
Brennan and Clark (1996) further point out that lexical availability, perceptual salience
and a tendency for people to reuse the same terms when describing the same object in a
conversation, all significantly shape the descriptions that people generate. These factors
may then override the informativeness constraints imposed by Grice’s Quantity Maxim.
Lexical availability depends on how an object is best conceptualized and the label associated
with that conceptualization (e.g. is the referent “an item of furniture” or “a sofa”). With
perceptual salience, speakers may include a highly salient attribute rather than just the
attributes that distinguish it from its distractors, e.g. “the $50 red sofa” when “the $50
sofa” may be informative enough. Adaptation to one’s conversational partner should lead
to a tendency to reuse a previous description.
The tendency to reuse a description derives from a combination of the most recent,
successfully understood description of the object, and how often the description has been
used in a particular conversation. However, this tendency is moderated by the need to
adapt a description to changing problem-solving circumstances and to make those repeated
descriptions even more efficient as their precedents become more established for a particular
pairing of conversational partners. Recency and frequency effects on reuse are reflections
of a coordination process between conversational partners in which they are negotiating a
shared way of labelling or conceptualizing the referent. Different descriptions may be tried
until the participants agree on a conceptualization. A change in the problem situation may
cause the conceptualization to be embellished with additional attributes or may instigate
the negotiation of a new conceptualization for the same referent.
The additional features suggested by this model include the previous description since
that is a candidate conceptual pact, how long ago the description was made, and how
frequently it was referenced. If the description was used further back in the dialogue or was
referenced frequently, that could indicate that the negotiation process had been completed.
Furthermore, the model suggests that, once a pact has been reached, that the dialogue
participants will continue to use the description that they previously negotiated unless the
problem situation changes. The continued usage aspect of the model is also similar to
Passonneau’s lexical focus model (Passonneau, 1995).
• interactions with other discourse entities: distance-last-ref, distance-last-ref-in-turns, numberprev-mentions, speaker-of-last-ref, distance-last-related
• previous description: color-in-last-exp, type-in-last-exp, owner-in-last-exp, price-in-last-exp,
quantity-in-last-exp, type-in-last-turn, color-in-last-turn, owner-in-last-turn, price-in-lastturn, quantity-in-last-turn, initial-in-last-turn
• frequency of attributes: freq-type-expressed, freq-color-expressed, freq-price-expressed, freqowner-expressed, freq-quantity-expressed
• stability history: cp-given-last-2, cp-given-last-3

Figure 11: conceptual pact Feature Set.

175

Jordan & Walker

The conceptual pact features in Figure 11 encode how the current description relates
to previous descriptions of the same entity. We encode recency information: when the entity
was last described in terms of number of utterances and turns (distance-last-ref, distancelast-in-turns), when the last related description (e.g. set, class) was (distance-last-related),
how frequently it was described (number-prev-mentions), who last described it (speaker-oflast-ref), and how it was last described in terms of turn and expression since the description
may have been broken into several utterances (color-in-last-exp, type-in-last-exp, owner-inlast-exp, price-in-last-exp, quantity-in-last-exp, type-in-last-turn, color-in-last-turn, ownerin-last-turn, price-in-last-turn, quantity-in-last-turn, initial-in-last-turn). We also encode
frequency information: the frequency with which attributes were expressed in previous
descriptions of it (freq-type-expressed, freq-color-expressed, freq-price-expressed, freq-ownerexpressed, freq-quantity-expressed), and a history of possible conceptual pacts that may
have been formed; the attribute types used to describe it in the last two and last three
descriptions of it if they were consistent across usages (cp-given-last-2, cp-given-last-3).

4. Experimental Method
The experiments utilize the rule learning program ripper (Cohen, 1996) to learn the content
selection component of an object description generator from the object descriptions in the
coconut corpus. Although any categorization algorithm could be applied to this problem
given the current formulation, ripper is a good match for this particular setup because
the if-then rules that are used to express the learned model can be easily compared with
the theoretical models of content selection described above. One drawback is that ripper
does not automatically take context into account during training so the discourse context
must be represented via features as well. Although it might seem desirable to use ripper’s
own previous predictions as additional context during training, since it will consider them
in practice, it is unnecessary and irrelevant to do so. The learned model will consist of
generation rules that are relative to what is in the discourse as encoded features (i.e. what
was actually said in the corpus) and any corrections it learns are only good for improving
performance on a static corpus.
Like other learning programs, ripper takes as input the names of a set of classes to be
learned, the names and ranges of values of a fixed set of features, and training data specifying
the class and feature values for each example in a training set. Its output is a classification
model for predicting the class of future examples. In ripper, the classification model is
learned using greedy search guided by an information gain metric, and is expressed as an
ordered set of if-then rules. By default ripper corrects for noisy data. In the experiments
reported here, unlike those reported by Jordan and Walker (2000), corrections for noisy
data have been suppressed since the reliability of the annotated features is high.
Thus to apply ripper, the object descriptions in the corpus are encoded in terms of a set
of classes (the output classification), and a set of input features that are used as predictors
for the classes. As mentioned above, the goal is to learn which of a set of content attributes
should be included in an object description. Below we describe how a class is assigned to
each object description, summarize the features extracted from the dialogue in which each
expression occurs, and the method applied to learn to predict the class of object description
from the features.
176

Learning Content Selection Rules for Generating Object Descriptions

Class Name
CPQ
CPO
CPOQ
T
CP
O
CO
C
CQ
COQ
OQ
PO
Q
P
PQ
POQ

N in
Corpus
64
56
46
42
41
32
31
18
14
13
12
11
5
4
2
2

Explicit attributes in
object description
Color, Price, Quantity
Color, Price, Owner
Color, Price, Owner, Quantity
None (type only)
Color, Price
Owner
Color, Owner
Color
Color, Quantity
Color, Owner, Quantity
Owner, Quantity
Price, Owner
Quantity
Price
Price, Quantity
Price, Owner, Quantity

Figure 12: Encoding of attributes to be included in terms of ML Classes, ordered by frequency

4.1 Class Assignment
The corpus of object descriptions is used to construct the machine learning classes as
follows. The learning task is to determine the subset of the four attributes, color, price,
owner, quantity, to include in an object description. Thus one method for representing
the class that each object description belongs to is to encode each object description as
a member of the category represented by the set of attributes expressed by the object
description. This results in 16 classes representing the power set of the four attributes as
shown in Figure 12. The frequency of each class is also shown in Figure 12. Note that these
classes are encodings of the hand annotated explicit attributes that were shown in Figure 6
but exclude the type attribute since we are not attempting to model pronominal selections.
4.2 Feature Extraction
The corpus is used to construct the machine learning features as follows. In ripper, feature
values are continuous (numeric), set-valued, or symbolic. We encoded each discourse entity
for a furniture item in terms of the set of 82 total features described in Section 3 as inspired
by theories of content selection for subsequent reference. These features were either directly
annotated by humans as described in Section 2, derived from annotated features, or inherent
to the dialogue (Di Eugenio et al., 2000; Jordan, 2000b). The dialogue context in which
each description occurs is directly represented in the encodings. In a dialogue system, the
dialogue manager would have access to all these features, which are needed by the problem
solving component, and would provide them to the language generator. The entire feature
set is summarized in Figure 13.
177

Jordan & Walker

• Assumed Familiarity Features
– mutually known attributes: type-mk, color-mk, owner-mk, price-mk, quantity-mk
– reference-relation: one of initial, coref, set, class, cnanaphora, predicative
• Inherent Features
– utterance-number, speaker-pair, speaker, problem-number
– attribute values:
∗
∗
∗
∗
∗

type: one of sofa, chair, table, rug, lamp, superordinate
color: one of red, blue, green, yellow
owner: one of self, other, ours
price: range from $50 to $600
quantity: range from 0 to 4.

• Conceptual Pact Features
– interactions with other discourse entities: distance-last-ref, distance-last-ref-in-turns, number-prev-mentions, speaker-of-last-ref, distance-last-related
– previous description: color-in-last-exp, type-in-last-exp, owner-in-last-exp, price-in-last-exp, quantityin-last-exp, type-in-last-turn, color-in-last-turn, owner-in-last-turn, price-in-last-turn, quantity-in-lastturn, initial-in-last-turn
– frequency of attributes: freq-type-expressed, freq-color-expressed, freq-price-expressed, freq-ownerexpressed, freq-quantity-expressed
– stability history: cp-given-last-2, cp-given-last-3
• Contrast Set Features
– distractor frequencies: type-distractors, color-distractors, owner-distractors, price-distractors, quantitydistractors
– Attribute Saliency: majority-type, majority-type-freq, majority-color, majority-color-freq, majorityprice, majority-price-freq, majority-owner, majority-owner-freq, majority-quantity, majority-quantityfreq
• Intentional Influences Features
– task situation: goal, colormatch, colormatch-constraintpresence, pricelimit, pricelimit-constraintpresence, priceevaluator, priceevaluator-constraintpresence, colorlimit, colorlimit-constraintpresence, priceupperlimit, priceupperlimit-constraintpresence
– agreement state: influence-on-listener, commit-speaker, solution-size, prev-influence-on-listener, prevcommit-speaker, prev-solution-size, distance-of-last-state-in-utterances, distance-of-last-state-in-turns,
ref-made-in-prev-action-state, speaker-of-last-state, prev-ref-state
– previous agreement state description: prev-state-type-expressed, prev-state-color-expressed, prev-stateowner-expressed, prev-state-price-expressed, prev-state-quantity-expressed
– solution interactions: color-contrast, price-contrast

Figure 13: Full Feature Set for Representing Basis for Object Description Content Selection
in Task Oriented Dialogues.

4.3 Learning Experiments
The final input for learning is training data, i.e., a representation of a set of discourse
entities, their discourse context and their object descriptions in terms of feature and class
178

Learning Content Selection Rules for Generating Object Descriptions

values. In order to induce rules from a variety of feature representations, the training data
is represented differently in different experiments.
The goal of these experiments is to test the contribution of the features suggested by the
three models of object description content selection described in Section 3. Our prediction
is that the incremental and the intentional influences models will work best in
combination for predicting object descriptions for both initial and subsequent reference.
This is because: (1) the intentional influences features capture nothing relevant to
the reference identification goal, which is the focus of the incremental model, and (2) we
hypothesize that the problem solving state will be relevant for selecting attributes for initial
descriptions, and the incremental model features capture nothing directly about the
problem solving state, but this is the focus of the intentional influences model. Finally
we expect the conceptual pact model to work best in conjunction with the incremental
and the intentional influences models since it is overriding informativeness constraints,
and since, after establishing a pact, it may need to adapt the description to make it more
efficient or re-negotiate the pact as the problem-solving situation changes.
Therefore, examples are first represented using only the assumed familiarity features
in Figure 7 to establish a performance baseline for assumed familiarity information. We
then add individual feature sets to the assumed familiarity feature set to examine the
contribution of each feature set on its own. Thus, examples are represented using only the
features specific to a particular model, i.e. the conceptual pact features in Figure 11, the
contrast set features in Figure 9 or the intentional influences features in Figure 10.
Remember that there are three different versions of the contrast set features, derived
from three different models of what is currently “in focus”. One model (segment) is based
on intentional structure (Grosz & Sidner, 1986). The other two are simple recency based
models where the active focus space either contains only discourse entities from the most
recent utterance or the most recent five utterances (one utterance, five utterance).
In addition to the theoretically-inspired feature sets, we include the task and dialogue
specific inherent features in Figure 8. These particular features are unlikely to produce
rules that generalize to other domains, including new coconut dialogues, because each
domain and pair of speakers will instantiate these values uniquely for a particular domain.
Thus, these features may indicate aspects of individual differences, and the role of the
specific situation in models of content selection for object descriptions.
Next, examples are represented using combinations of the features from the different
models to examine interactions between feature sets.
Finally, to determine whether particular feature types have a large impact (e.g. frequency features), we report results from a set of experiments using singleton feature sets,
where those features that varied by attribute alone are clustered into sets while the rest
contain just one feature. For example, the distractor frequency attributes in Figure 9 form
a cluster for a singleton feature set whereas utterance-number is the only member of its
feature set. We experimented with singleton feature sets in order to determine if any are
making a large impact on the performance of the model feature set to which they belong.
The output of each machine learning experiment is a model for object description generation for this domain and task, learned from the training data. To evaluate these models,
the error rates of the learned models are estimated using 25-fold cross-validation, i.e. the total set of examples is randomly divided into 25 disjoint test sets, and 25 runs of the learning
179

Jordan & Walker

program are performed. Thus, each run uses the examples not in the test set for training
and the remaining examples for testing. An estimated error rate is obtained by averaging
the error rate on the test portion of the data from each of the 25 runs. For sample sizes in
the hundreds (the coconut corpus provides 393 examples), cross-validation often provides
a better performance estimate than holding out a single test set (Weiss & Kulikowski, 1991).
The major advantage is that in cross-validation all examples are eventually used for testing,
and almost all examples are used in any given training run.

5. Experimental Results
Table 2 summarizes the experimental results. For each feature set, and combination of
feature sets, we report accuracy rates and standard errors resulting from 25-fold crossvalidation. We test differences in the resulting accuracies using paired t-tests. The table
is divided into regions grouping results using similar feature sets. Row 1 provides the
accuracy for the majority class baseline of 16.9%; this is the standard baseline that
corresponds to the accuracy achieved from simply choosing the description type that occurs
most frequently in the corpus, which in this case means that the object description generator
would always use the color, price and quantity attributes to describe a domain entity. Row
2 provides a second baseline, namely that for using the assumed familiarity feature
set. This result shows that providing the learner with information about whether the
values of the attributes for a discourse entity are mutually known does significantly improve
performance over the majority class baseline (t=2.4, p< .03). Examination of the rest
of the table shows clearly that the accuracy of the learned object description generator
depends on the features that the learner has available.
Rows 3 to 8 provide the accuracies of object description generators trained and tested
using one of the additional feature sets in addition to the familiarity feature set. Overall,
the results here show that compared to the familiarity baseline, the features for intentional influences (familiarity,iinf t=10.0, p<.01), contrast set (familiarity,seg
t=6.1, p< .01; familiarity,1utt t=4.7, p< .01; familiarity,5utt t=4.2, p< .01), and
conceptual pact (familiarity,cp t=6.2, p< .01) taken independently significantly improve performance. The accuracies for the intentional influences features (Row 7) are
significantly better than for conceptual pact (t=5.2, p<.01) and the three parameterizations of the incremental model (familiarity,seg t=6, p<.01; familiarity,1utt t=4.3,
p<.01; familiarity,5utt t=4.2, p<.01), perhaps indicating the importance of a direct
representation of the problem solving state for this task.
In addition, interestingly, Rows 3, 4 and 5 show that features for the incremental
model that are based on the three different models of discourse structure all perform equally
well, i.e. there are no statistically significant differences between the distractors predicted by
the model of discourse structure based on intention (seg) and the two recency based models
(1utt, 5utt), even though the raw accuracies for distractors predicted by the intentionbased model are typically higher.6 The remainder of the table shows that the intention
based model only performs better than a recency based model when it is combined with all
features (Row 15 seg vs. Row 16 1utt t=2.1, p<.05).
6. This is consistent with the findings reported by Jordan (2000b) which used a smaller dataset to measure
which discourse structure model best explained the data for the incremental model.

180

Learning Content Selection Rules for Generating Object Descriptions

Row
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20

Model Tested
baseline
baseline
incremental
incremental
incremental
conceptual pact
intentional influences
situation specific
intentional influences,
incremental
intentional influences,
incremental
intentional influences,
incremental
all theory features combined
all theory features combined
all theory features combined
all theories
& situation specific
all theories
& situation specific
all theories
& situation specific
best singletons
from all models combined
best singletons
from all models combined
best singletons
from all models combined

Feature Sets Used
majority class
familiarity
familiarity,seg
familiarity,1utt
familiarity,5utt
familiarity,cp
familiarity,iinf
familiarity,inh

Accuracy (SE)
16.9% (2.1)
18.1% (2.1)
29.0% (2.2)
29.0% (2.5)
30.4% (2.6)
28.9% (2.1)
42.4% (2.7)
54.5% (2.3)

familiarity,iinf,seg

46.6% (2.2)

familiarity,iinf,1utt

42.7% (2.2)

familiarity,iinf,5utt
familiarity,iinf,cp,seg
familiarity,iinf,cp,1utt
familiarity,iinf,cp,5utt

44.4%
43.2%
40.9%
41.9%

(2.6)
(2.8)
(2.6)
(3.2)

familiarity,iinf,inh,cp,seg

59.9% (2.4)

familiarity,iinf,inh,cp,1utt

55.4% (2.2)

familiarity,iinf,inh,cp,5utt
familiarity,iinf,inh,cp,seg

57.6% (3.0)
52.9% (2.9)

familiarity,iinf,inh,cp,1utt

47.8% (2.4)

familiarity,iinf,inh,cp,5utt

50.3% (2.8)

Table 2: Accuracy rates for the content selection component of a object description generator using different feature sets, SE = Standard Error. cp = the conceptual pact
features. iinf = the intentional influences features. inh = the inherent features. seg = the contrast-set, segment focus space features. 1utt = the
contrast set, one utterance focus space features, 5utt = the contrast
set, five utterance focus space features.

Finally, the situation specific model based on the inherent feature set (Row 8)
which is domain, speaker and task specific performs significantly better than the familiarity baseline (t=16.6, p< .01). It is also significantly better than any of the models
utilizing theoretically motivated features. It is significantly better than the intentional
influences model (t=5, p<.01), and the conceptual pact model (t=9.9, p<.01), as
well as the three parameterizations of the incremental model (seg t=10, p<.01; 1utt
t=10.4, p<.01; 5utt t=8.8, p<.01).
181

Jordan & Walker

Say POQ if priceupperlimit-constraintpresence = IMPLICIT ∧ reference-relation = class
Say COQ if goal = SELECTCHAIRS ∧ colormatch-constraintpresence = IMPLICIT ∧ prev-solution-size =
DETERMINATE ∧ reference-relation = coref
Say COQ if goal = SELECTCHAIRS ∧ distance-of-last-state-in-utterances >= 3 ∧ speaker-of-last-state = SELF ∧
reference-relation = initial
Say COQ if goal = SELECTCHAIRS ∧ prev-ref-state = STATEMENT ∧ influence-on-listener = action-directive ∧
prev-solution-size = DETERMINATE
Say C if prev-commit-speaker = commit ∧ influence-on-listener = action-directive ∧ color-contrast = no ∧
speaker-of-last-state = SELF
Say C if color-contrast = yes ∧ goal = SELECTTABLE ∧ prev-influence-on-listener = action-directive ∧ influenceon-listener = na
Say C if solution-size = DETERMINATE ∧ prev-influence-on-listener = na ∧ prev-state-color-expressed = yes ∧
prev-state-price-expressed = na ∧ prev-solution-size = DETERMINATE
Say CO if colorlimit = yes
Say CO if price-mk = yes ∧ prev-solution-size = INDETERMINATE ∧ price-contrast = yes ∧ commit-speaker = na
Say CO if price-mk = yes ∧ prev-ref-state = PARTNER-DECIDABLE-OPTION ∧ distance-of-last-state-inutterances <= 1 ∧ prev-state-type-expressed = yes
Say O if prev-influence-on-listener = open-option ∧ reference-relation = coref
Say O if influence-on-listener = info-request ∧ distance-of-last-state-in-turns <= 0
Say CP if solution-size = INDETERMINATE ∧ price-contrast = yes ∧ distance-of-last-state-in-turns >= 2
Say CP if distance-of-last-state-in-utterances <= 1 ∧ goal = SELECTSOFA ∧ influence-on-listener = na ∧
reference-relation = class
Say T if prev-solution-size = DETERMINATE ∧ distance-of-last-state-in-turns <= 0 ∧ prev-state-type-expressed
= yes ∧ ref-made-in-prev-action-state = yes
Say T if prev-solution-size = DETERMINATE ∧ colormatch-constraintpresence = EXPLICIT
Say T if prev-solution-size = DETERMINATE ∧ goal = SELECTSOFA ∧ prev-state-owner-expressed = na ∧
color-contrast = no
Say CPOQ if goal = SELECTCHAIRS ∧ prev-solution-size = INDETERMINATE ∧ price-contrast = no ∧ type-mk
= no
Say CPOQ if distance-of-last-state-in-utterances >= 5 ∧ type-mk = no
Say CPOQ if goal = SELECTCHAIRS ∧ influence-on-listener = action-directive ∧ distance-of-last-state-inutterances >= 2
Say CPO if influence-on-listener = action-directive ∧ distance-of-last-state-in-utterances >= 2 ∧ commit-speaker =
offer
Say CPO if goal = SELECTSOFA ∧ distance-of-last-state-in-utterances >= 1
default Say CPQ

Figure 14: A Sampling of Rules Learned Using assumed familiarity and intentional
influences Features. The classes encode the four attributes, e.g CPOQ =
Color,Price,Owner and Quantity, T = Type only

In Section 4.3, we hypothesized that the incremental and intentional influences
models would work best in combination. Rows 9, 10 and 11 show the results of this combination for each underlying model of discourse structure. Each of these combinations
provides some increase in accuracy, however the improvements in accuracy over the object
description generator based on the intentional influences features alone (Row 7) are
not statistically significant.
Figure 14 shows the rules that the object description generator learns given the assumed familiarity and intentional influences features. The rules make use of both
types of assumed familiarity features and all four types of intentional influences
features. The features representing mutually known attributes and those representing the
attributes expressed in a previous agreement state can be thought of as overlapping with
182

Learning Content Selection Rules for Generating Object Descriptions

the conceptual pact model, while features representing problem-solving structure and
agreement state may overlap with the incremental model by indicating what is in focus.
One of the rules from the rule set in Figure 14 is:
Say T if prev-solution-size = DETERMINATE ∧ colormatch-constraintpresence
= EXPLICIT .
An example of a dialogue excerpt that matches this rule is shown in Figure 15. The
rule captures a particular style of problem solving in the dialogue in which the conversants
talk explicitly about the points involved in matching colors (we only get 650 points without
rug and bluematch in living room) to argue for including a particular item (rug). In this
case, because a solution had been proposed, the feature prev-solution-size has the value
determinate. So the rule describes those contexts in which a solution has been counterproposed, and support for the counter-proposal is to be presented.
D: I suggest that we buy my blue sofa 300, your 1 table high green 200, your 2 chairs red 50, my 2
chairs red 50 and you can decide the rest. What do you think
J: your 3 chair green my high table green 200 and my 1 chair green 100. your sofa blue 300 rug blue
250. we get 700 point. 200 for sofa in livingroom plus rug 10. 20 points for match. 50 points for
match in dining room plus 20 for spending all. red chairs plus red table costs 600. we only get 650
points without rug and bluematch in living room. add it up and tell me what you think.

Figure 15: Example of a discourse excerpt that matches a rule in the intentional influences and assumed familiarity rule set

Rows 12, 13 and 14 in Table 2 contain the results of combining the conceptual pact
features with the intentional influences features and the contrast set features.
These results can be directly compared with those in Rows 9, 10 and 11. Because ripper uses a heuristic search, the additional features have the effect of making the accuracies
for the resulting models lower. However, none of these differences are statistically significant. Taken together, the results in Rows 9-14 indicate that the best accuracies obtainable
without using situation specific features (the inherent feature set), is the combination of
the intentional influences and contrast set features, with a best overall accuracy
of 46.6% as shown in Row 9.
Rows 15, 16 and 17 contain the results for combining all the features, including the
inherent feature set, for each underlying model of discourse structure. This time there is
one significant difference between the underlying discourse models in which the intentionbased model, segment, is significantly better than the one utterance recency model
(t=2.1, p<.05) but not the five utterance recency model. Of the models in this group
only the segment model is significantly better than the models that use a subset of the
features (vs. inherent t=2.4, p<.03). Figure 16 shows the generation rules learned with
the best performing feature set shown in Row 15. Many task, entity and speaker specific
features from the inherent feature set are used in these rules. This rule set performs
at 59.9% accuracy, as opposed to 46.6% accuracy for the more general feature set (shown
in Row 9). In this final rule set, no conceptual pact features are used and removing
183

Jordan & Walker

Say Q if type=CHAIR ∧ price>=200 ∧ reference-relation=set ∧ quantity>=2.
Say Q if speaker=GARRETT ∧ color-distractors<=0 ∧ type=CHAIR.
Say PO if color=unk ∧ speaker-pair=GARRETT-STEVE ∧ reference-relation=initial ∧ color-contrast=no.
Say PO if majority-color-freq>=6 ∧ reference-relation=set.
Say PO if utterance-number>=39 ∧ type-distractors<=0 ∧ owner=SELF ∧ price>=100.
Say OQ if color=unk ∧ quantity>=2 ∧ majority-price-freq<=5.
Say OQ if prev-state-quantity-expressed=yes ∧ speaker=JULIE ∧ color=RED.
Say COQ if goal=SELECTCHAIRS ∧ price-distractors<=3 ∧ owner=SELF ∧ distance-of-last-state-inutterances>=3 ∧ majority-price<=200.
Say COQ if quantity>=2 ∧ price<=-1 ∧ ref-made-in-prev-action-state=no.
Say COQ if quantity>=2 ∧ price-distractors<=3 ∧ quantity-distractors>=4 ∧ influence-on-listener=action-directive.
Say CQ if speaker-pair=DAVE-GREG ∧ utterance-number>=22 ∧ utterance-number<=27 ∧ problem<=1.
Say CQ if problem>=2 ∧ quantity>=2 ∧ price<=-1.
Say CQ if color=YELLOW ∧ quantity>=3 ∧ influence-on-listener=action-directive ∧ type=CHAIR.
Say C if price-mk=yes ∧ majority-type=SUPERORDINATE ∧ quantity-distractors>=3.
Say C if price-mk=yes ∧ utterance-number<=21 ∧ utterance-number>=18 ∧ prev-state-price-expressed=na ∧
majority-price>=200 ∧ color-distractors>=2.
Say CO if utterance-number>=16 ∧ price<=-1 ∧ type=CHAIR.
Say CO if price-mk=yes ∧ speaker-pair=JILL-PENNY.
Say CO if majority-price<=75 ∧ distance-of-last-state-in-utterances>=4 ∧ prev-state-type-expressed=na.
Say O if color=unk ∧ speaker-pair=GARRETT-STEVE.
Say O if color=unk ∧ owner=OTHER ∧ price<=300.
Say O if prev-influence-on-listener=open-option ∧ utterance-number>=22.
Say CP if problem>=2 ∧ quantity<=1 ∧ type=CHAIR.
Say CP if price>=325 ∧ reference-relation=class ∧ distance-of-last-state-in-utterances<=0.
Say CP if speaker-pair=JON-JULIE ∧ type-distractors<=1.
Say CP if reference-relation=set ∧ owner=OTHER ∧ owner-distractors<=0.
Say T if prev-solution-size=DETERMINATE ∧ price>=250 ∧ color-distractors<=5 ∧ owner-distractors>=2 ∧
utterance-number>=15.
Say T if color=unk.
Say T if prev-state-type-expressed=yes ∧ distance-of-last-state-in-turns<=0 ∧ owner-distractors<=4.
Say CPOQ if goal=SELECTCHAIRS ∧ prev-solution-size=INDETERMINATE.
Say CPOQ if speaker-pair=KATHY-MARK ∧ prev-solution-size=INDETERMINATE ∧ owner-distractors<=5.
Say CPOQ if goal=SELECTCHAIRS ∧ influence-on-listener=action-directive ∧ utterance-number<=22.
Say CPO if utterance-number>=11 ∧ quantity<=1 ∧ owner-distractors>=1.
Say CPO if influence-on-listener=action-directive ∧ price>=150.
Say CPO if reference-relation=class ∧ problem<=1.
default Say CPQ

Figure 16: A Sampling of the Best Performing Rule Set. Learned using the assumed familiarity, inherent, intentional influences and contrast set feature
sets. The classes encode the four attributes, e.g., CPOQ = Color,Price,Owner
and Quantity, T = Type only.

these features during training had no effect on accuracy. All of the types of features in
the assumed familiarity, inherent, and contrast set are used. Of the intentional
influences features, mainly the agreement state and previous agreement state descriptions
are used. Some possible explanations are that the agreement state is a stronger influence
than the task situation or that the task situation is not modelled well.
Why does the use of the inherent feature set contribute so much to overall accuracy
and why are so many inherent features used in the rule set in Figure 16? It may be that
the inherent features of objects would be important in any domain because there is a lot
of domain specific reasoning in the task of object description content selection. However,
these features are most likely to support rules that overfit to the current data set; as we
184

Learning Content Selection Rules for Generating Object Descriptions

have said before, rules based on the inherent feature set are unlikely to generalize to new
situations. However, there might be more general or abstract versions of these features
that could generalize to new situations. For example, the attribute values for the discourse
entity may be capturing aspects of the problem solving (e.g. near the end of the problem,
the price of expensive items is highly relevant). Second, the use of utterance-numbers can
be characterized as rules about the beginning, middle and end of a dialogue and may again
reflect problem solving progress. Third, the rules involving problem-number suggest that
the behavior for the first problem is different from the others and may reflect that the
dialogue partners have reached an agreement on their problem solving strategy. Finally,
the use of speaker-pair features in the rules included all but two of the possible speakerpairs, which may reflect differences in the agreements reached on how to collaborate. One
of the rules from this rule set is shown below:
Say CP if price >= 325 ∧ reference-relation = class ∧ distance-of-last-statein-utterances <= 0.
This rule applies to discourse entities in the dialogues of one speaker pair. An example
dialogue excerpt that matches this rule is in Figure 17. The rule reflects a particular style
of describing the items that are available to use in the problem solving, in which the speaker
first describes the class of the items that are about to be listed. This style of description
allows the speaker to efficiently list what he has available. The distance-of-last-state-inutterances feature captures that this style of description occurs before any proposals have
been made.
M: I have $550, my inventory consists of 2 Yellow hi-tables for $325 each. Sofas, yellow for $400
and green for $350.

Figure 17: Example of a dialogue excerpt that matches a rule in the best performing rule
set

As described above, we also created singleton feature sets, in addition to our theoretically inspired feature sets, to determine if any singleton features are, by themselves, making
a large impact on the performance of the model it belongs to. The singleton features shown
in Table 3 resulted in learned models that were significantly above the majority class baseline. The last column of Table 3 also shows that, except for the assumed familiarity and
incremental 5utt models, the theory model to which a particular singleton feature belongs is significantly better, indicating that no singleton alone is a better predictor than the
combined features in these theoretical models. The assumed familiarity and incremental 5utt models perform similarly to their corresponding single feature models indicating
that these single features are the most highly useful features for these two models.
Finally, we combined all of the singleton features in Table 3 to learn three additional
models shown in Rows 18, 19 and 20 of Table 2. These three models are not significantly
different from one another. The best performing model in Row 15, which combines all
185

Jordan & Walker

Source
Model
assumed familiarity
conceptual
pact

Features in Set

type-mk, color-mk, owner-mk,
price-mk, quantity-mk
freq-type-expressed, freq-colorexpressed, freq-price-expressed,
freq-owner-expressed,
freqquantity-expressed
cp-given-last-2
type-in-last-exp, color-in-lastexp, price-in-last-exp, owner-inlast-exp, quantity-in-last-exp
type-in-last-turn,
color-inlast-turn,
price-in-last-turn,
owner-in-last-turn, quantity-inlast-turn
incremental type-distractors,
colorseg
distractors, price-distractors,
owner-distractors,
quantitydistractors
majority-type, majority-color,
majority-price, majority-owner,
majority-quantity
incremental type-distractors,
color1utt
distractors, price-distractors,
owner-distractors,
quantitydistractors
incremental type-distractors,
color5utt
distractors, price-distractors,
owner-distractors,
quantitydistractors
intentional distance-of-last-state-ininfluences
utterances
distance-of-last-state-in-turns
colormatch
prev-state-type-expressed,
prev-state-color-expressed,
prev-state-owner-expressed,
prev-state-price-expressed,
prev-state-quantity-expressed
situation
type, color, price, owner, quanspecific
tity
utterance-number

Accuracy
(SE)
18.1% (2.1)

Better than
baseline
t=2.4, p<.03

Source Model
Better
identical

22.1% (1.8)

t=3.7, p<.01

t=5.7, p<.01

20.9% (2.1)
18.9% (1.9)

t=3.9, p<.01
t=2.8, p<.02

t=4.3, p<.01
t=5.7, p<.01

18.1% (2.0)

t=3.4, p<.02

t=6.4, p<.01

21.4% (2.5)

t=3.2, p<.01

t=3.6, p<.01

19.9% (2.3)

t=2.5, p<.02

t=4.8, p<.01

20.8% (2.4)

t=3.2, p<.01

t=3.2, p<.01

25.7% (2.7)

t=4.4, p<.01

t=1.5, NS

21.3% (2.0)

t=3.7, p<.01

t=11, p<.01

20.0% (2.1)
19.3% (2.2)
19.2% (1.9)

t=3.6, p<.01
t=3.7, p<.01
t=3.6, p<.01

t=10.2, p<.01
t=10.3, p<.01
t=8.8, p<.01

24.3% (2.5)

t=4.1, p<.01

t=12.5, p<.01

20.5% (2.3)

t=3.3, p<.01

t=16.2, p<.01

Table 3: Performance using singleton feature sets, SE = Standard Error

features, is significantly better than 1utt (t=4.2, p<.01) and 5utt (t=2.8, p<.01) in Rows
19 and 20, but is not significantly different from seg (t=2.0, NS) in Row 18. The combined
singletons seg model (Row 18) is also not significantly different from the inherent model
186

Learning Content Selection Rules for Generating Object Descriptions

(Row 8). The combined singletons seg model has the advantage that it requires just two
situation specific features and a smaller set of theoretical features.
Class
CPQ
CPO
CPOQ
T
CP
O
CO
C
CQ
COQ
PO
OQ
Q
POQ
PQ

recall
100.00
66.67
100.00
50.00
100.00
100.00
66.67
0.00
0.00
100.00
50.00
66.67
0.00
0.00
0.00

precision
63.64
100.00
100.00
100.00
100.00
60.00
100.00
0.00
100.00
100.00
100.00
50.00
0.00
100.00
100.00

fallout
12.12
0.00
0.00
0.00
0.00
5.41
0.00
5.13
0.00
0.00
0.00
5.41
2.50
0.00
0.00

F (1.00)
0.78
0.80
1.00
0.67
1.00
0.75
0.80
0.00
0.00
1.00
0.67
0.57
0.00
0.00
0.00

Table 4: Recall and Precision values for each class; the rows are ordered from most frequent
to least frequent class

Class
CPQ
O
COQ
C
CPO
CO
PO
T
OQ
POQ
CPOQ
Q
CP
PQ
CQ

CPQ
7
0
0
1
0
1
0
0
0
0
0
0
0
1
1

O
0
3
0
0
1
0
1
0
0
0
0
0
0
0
0

COQ
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0

C
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0

CPO
0
0
0
0
4
0
0
0
0
0
0
0
0
0
0

CO
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0

PO
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0

T
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0

OQ
0
0
0
0
0
0
0
0
2
1
0
0
0
1
0

POQ
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

CPOQ
0
0
0
0
0
0
0
0
0
0
6
0
0
0
0

Q
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0

CP
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0

PQ
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

CQ
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Table 5: Confusion matrix for a held-out test set; The row label indicates the class, while
the column indicates how the token was classified automatically.

Tables 4 and 5 show the recall and precision for each class and a sample confusion matrix
for one run of the best performing model with a held-out test-set consisting of 40 examples.
Table 4 shows that the overall tendency is for both recall and precision to be higher for
classes that are more frequent, and lower for the less frequent classes as one would expect.
Table 5 shows there aren’t any significant sources of confusion as the errors are spread out
across different classes.
187

Jordan & Walker

6. Discussion and Future Work
This article reports experimental results for training a generator to learn which attributes
of a discourse entity to include in an object description. To our knowledge, this is the
first reported experiment of a trainable content selection component for object description
generation in dialogue. A unique feature of this study is the use of theoretical work in
cognitive science on how speakers select the content of an object description. The theories we
used to inspire the development of features for the machine learner were based on Brennan
and Clark’s (1996) model, Dale and Reiter’s (1995) model and Jordan’s (2000b) model.
Because Dale and Reiter’s model relies on a model of discourse structure, we developed
features to represent Grosz and Sidner’s (1986) model of discourse structure, as well as
features representing two simple recency based models of discourse structure. The object
description generators are trained on the coconut corpus of task-oriented dialogues. The
results show that:
• The best performing learned object description generator achieves a 60% match to
human performance as opposed to a 17% majority class baseline;
• The assumed familiarity feature set improves performance over the baseline;
• Features specific to the task, speaker and discourse entity (the inherent feature set)
provide significant performance improvements;
• The conceptual pact feature set developed to approximate Brennan and Clark’s
model of object description generation significantly improves performance over both
the baseline and assumed familiarity;
• The contrast set features developed to approximate Dale and Reiter’s model significantly improve performance over both the baseline and assumed familiarity;
• The intentional influences features developed to approximate Jordan’s model
are the best performing theoretically-inspired feature set when taken alone, and the
combination of the intentional influences features with the contrast set features is the best performing of the theoretically-based models. This combined model
achieves an accuracy of 46.6% as an exact match to human performance and holds
more promise of being general across domains and tasks than those that include the
inherent features.
• Tests using singleton feature sets from each model showed that frequency features and
the attributes last used have the most impact in the conceptual pact model, the
distractor set features are the most important for the incremental models, and features related to state have the biggest impact in the intentional influences model.
But none of these singleton features perform as well as the feature combinations in
the related model.
• A model consisting of a combination of the best singleton features from each of the
other models was not significantly different from the best learned object description
generator and achieved a 53% match to human performance with the advantage of
fewer situation specific features.
188

Learning Content Selection Rules for Generating Object Descriptions

Thus the choice to use theoretically inspired features is validated, in the sense that every
set of cognitive features improves performance over the baseline.
In previous work, we presented results from a similar set of experiments, but the best
model for object description generation only achieved an accuracy of 50% (Jordan & Walker,
2000). The accuracy improvements reported here are due to a number of new features that
we derived from the corpus, as well as a modification of the machine learning algorithm to
respect the fact that the training data for these experiments is not noisy.
It is hard to say how good our best-performing accuracy of 60% actually is as this is
one of the first studies of this kind. There are several issues to consider. First, the object
descriptions in the corpus may represent just one way to describe the entity at that point in
the dialogue, so that using human performance as a standard against which to evaluate the
learned object description generators provides an overly rigorous test (Oberlander, 1998;
Reiter, 2002). Furthermore, we do not know whether humans would produce identical object
descriptions given the same discourse situation. A previous study of anaphor generation
in Chinese showed that rates of match for human speakers averaged 74% for that problem
(Yeh & Mellish, 1997), and our results are comparable to that. Furthermore, the results
show that including features specific to speaker and attribute values improves performance
significantly. Our conclusion is that it may be important to quantify the best performance
that a human could achieve at matching the object descriptions in the corpus, given the
complete discourse context and the identity of the referent. In addition, the difficulty of
this problem depends on the number of attributes available for describing an object in the
domain; the object description generator has to correctly make four different decisions to
achieve an exact match to human performance. Since the coconut corpus is publicly
available, we hope that other researchers will improve on our results.
Another issue that must be considered is the extent to which these experiments can be
taken as a test of the theories that inspired the feature sets. There are several reasons to
be cautious in making such interpretations. First, the models were developed to explain
subsequent reference and not initial reference. Second, the feature sets cannot be claimed in
any way to be complete. It is possible that other features could be developed that provide
a better representation of the theories. Finally, ripper is a propositional learner, and
the models of object description generation may not be representable by a propositional
theory. For example, models of object description generation rely on a representation of
the discourse context in the form of some type of discourse model. The features utilized
here represent the discourse context and capture aspects of the discourse history, but these
representations are not as rich as those used by a rule-based implementation. However it
is interesting to note that whatever limitations these models may have, the automatically
trained models tested here perform better than the rule-based implementations of these
theoretical models, reported by Jordan (2000b).
Another issue is the extent to which these findings might generalize across domains.
While this is always an issue for empirical work, one potential limitation of this study is
that Jordan’s model was explicitly developed to capture features specific to negotiation
dialogues such as those in the coconut corpus. Thus, it is possible that the features
inspired by that theory are a better fit to this data. Just as conceptual pact features are
less prominent for the coconut data and that data thus inspired a new model, we expect to
find that other types of dialogue will inspire additional features and feature representations.
189

Jordan & Walker

Finally, a unique contribution of this work is the experimental comparison of different
representations of discourse structure for the task of object description generation. We
tested three representations of discourse structure, one represented by features derived
from Grosz and Sidner’s model, and two recency based representations. One of the most
surprising results of this work is the finding that features based on Grosz and Sidner’s
model do not improve performance over extremely simple models based on recency. This
could be due to issues discussed by Walker (1996a), namely that human working memory
and processing limitations play a much larger role in referring expression generation and
interpretation than would be suggested by the operations of Grosz and Sidner’s focus space
model. However it could also be due to much more mundane reasons, namely that it
is possible (again) that the feature sets are not adequate representations of the discourse
structure model differences, or that the differences we found would be statistically significant
if the corpus were much larger. However, again the results on the discourse structure model
differences reported here confirm the findings reported by Jordan (2000b), i.e. it was also
true that the focus space model did not perform better than the simple recency models in
Jordan’s rule-based implementations.
In future work, we plan to perform similar experiments on different corpora with different communications settings and problem types (e.g. planning, scheduling, designing) to
determine whether our findings are specific to the genre of dialogues that we examine here,
or whether the most general models can be applied directly to a new domain. Related to
this question of generality, we have created a binary attribute inclusion model using domain
independent feature sets but do not yet have a new annotated corpus upon which to test it.

Acknowledgments
Thanks to William Cohen for helpful discussions on the use of ripper for this problem,
and to the three anonymous reviewers who provided many helpful suggestions for improving
the paper.

References
Allen, J., & Core, M. (1997). Draft of DAMSL: Dialog act markup in several layers. Coding scheme developed by the MultiParty group, 1st Discourse Tagging Workshop,
University of Pennsylvania, March 1996.
Appelt, D. (1985a). Planning English Sentences. Studies in Natural Language Processing.
Cambridge University Press.
Appelt, D. E. (1985b). Some pragmatic issues in the planning of definite and indefinite
noun phrases. In Proceedings of 23rd ACL, pp. 198–203.
Bangalore, S., & Rambow, O. (2000). Exploiting a probabilistic hierarchical model for
Generation. In COLING, pp. 42–48, Saarbucken, Germany.
Brennan, S. E., & Clark, H. H. (1996). Lexical choice and conceptual pacts in conversation.
Journal of Experimental Psychology: Learning, Memory And Cognition, 22, 1482–
1493.
190

Learning Content Selection Rules for Generating Object Descriptions

Clark, H. H., & Wilkes-Gibbs, D. (1986). Referring as a collaborative process. Cognition,
22, 1–39.
Cohen, W. (1996). Learning trees and rules with set-valued features. In Fourteenth Conference of the American Association for Artificial Intelligence, pp. 709–716.
Daelemans, W., & Hoste, V. (2002). Evaluation of machine learning methods for natural language processing tasks. In Proceedings of LREC-2002, the 3rd International
Language Resources and Evaluation Conference, pp. 755–760.
Dale, R. (1992). Generating Referring Expressions. ACL-MIT Series in Natural Language
Processing. The MIT Press.
Dale, R., & Reiter, E. (1995). Computational Interpretations of the Gricean Maxims in the
Generation of Referring Expressions. Cognitive Science, 19 (2), 233–263.
Di Eugenio, B., Jordan, P. W., Thomason, R. H., & Moore, J. D. (2000). The agreement
process: An empirical investigation of human-human computer-mediated collaborative
dialogues. International Journal of Human-Computer Studies, 53 (6), 1017–1076.
Di Eugenio, B., Moore, J. D., & Paolucci, M. (1997). Learning features that predict cue
usage. In Proceedings of the 35th Annual Meeting of the Association for Computational
Linguistics, ACL/EACL 97, pp. 80–87.
Di Eugenio, B., & Webber, B. (1996). Pragmatic overloading in natural language instructions. International Journal of Expert Systems, Special Issue on Knowledge Representation and Reasoning for Natural Language Processing, 9 (1), 53–84.
Duboue, P. A., & McKeown, K. R. (2001). Empirically estimating order constraints for
content planning in generation. In Proceedings of the 39rd Annual Meeting of the
Association for Computational Linguistics (ACL/EACL-2001).
Gardent, C. (2002). Generating minimal definite descriptions. In Proceedings of Association
for Computational Linguistics 2002, pp. 96–103.
Grice, H. (1975). Logic and conversation. In Cole, P., & Morgan, J. (Eds.), Syntax and
Semantics III - Speech Acts, pp. 41–58. Academic Press, New York.
Grosz, B. J., & Sidner, C. L. (1986). Attention, intentions and the structure of discourse.
Computational Linguistics, 12, 175–204.
Heeman, P. A., & Hirst, G. (1995). Collaborating on referring expressions. Computational
Linguistics, 21 (3), 351–383.
Heim, I. (1983). File change semantics and the familiarity theory of definiteness. In Bauerle,
R., Schwarze, C., & von Stechow, A. (Eds.), Meaning, use, and the interpretation of
language, pp. 164–189. Walter de Gruyter, Berlin.
Hirschberg, J. B. (1993). Pitch accent in context: predicting intonational prominence from
text. Artificial Intelligence Journal, 63, 305–340.
Jordan, P., & Walker, M. A. (2000). Learning attribute selections for non-pronominal
expressions. In In Proceedings of the 38th Annual Meeting of the Association for
Computational Linguistics (ACL-00), Hong Kong, pp. 181–190.
191

Jordan & Walker

Jordan, P. W. (2000a). Influences on attribute selection in redescriptions: A corpus study.
In Proceedings of CogSci2000, pp. 250–255.
Jordan, P. W. (2000b). Intentional Influences on Object Redescriptions in Dialogue: Evidence from an Empirical Study. Ph.D. thesis, Intelligent Systems Program, University
of Pittsburgh.
Jordan, P. W. (2002). Contextual influences on attribute selection for repeated descriptions.
In van Deemter, K., & Kibble, R. (Eds.), Information Sharing: Reference and Presupposition in Language Generation and Interpretation, pp. 295–328. CSLI Publications.
Kamp, H., & Reyle, U. (1993). From Discourse to Logic; Introduction to Modeltheoretic
Semantics of Natural Language, Formal Logic and Discourse Representation Theory.
Kluwer Academic Publishers, Dordrecht Holland.
Karttunen, L. (1976). Discourse referents. In McCawley, J. (Ed.), Syntax and Semantics,
Vol. 7, pp. 363–385. Academic Press.
Krahmer, E., van Erk, S., & Verleg, A. (2003). Graph-Based generation of referring expressions. Computational Linguistics, 29 (1), 53–72.
Krippendorf, K. (1980). Content Analysis: An Introduction to its Methodology. Sage Publications, Beverly Hills, Ca.
Kronfeld, A. (1986). Donnellan’s distinction and a computational model of reference. In
Proceedings of 24th ACL, pp. 186–191.
Langkilde, I., & Knight, K. (1998). Generation that exploits corpus-based statistical knowledge. In Proceedings of COLING-ACL, pp. 704–710.
Lochbaum, K. (1995). The use of knowledge preconditions in language processing. In
IJCAI95, pp. 1260–1266.
Malouf, R. (2000). The order of prenominal adjectives in natural language generation.
In Proceedings of the Meeting of the Association for Computational Lingustics, ACL
2000, pp. 85–92.
Mellish, C., Knott, A., Oberlander, J., & O’Donnell, M. (1998). Experiments using stochastic search for text planning. In Proceedings of International Conference on Natural
Language Generation, pp. 97–108.
Oberlander, J. (1998). Do the right thing...but expect the unexpected. Computational
Linguistics, 24 (3), 501–508.
Oh, A. H., & Rudnicky, A. I. (2002). Stochastic natural language generation for spoken
dialog systems. Computer Speech and Language: Special Issue on Spoken Language
Generation, 16 (3-4), 387–407.
Passonneau, R. J. (1995). Integrating Gricean and Attentional Constraints. In Proceedings
of IJCAI 95, pp. 1267–1273.
Passonneau, R. J. (1996). Using Centering to Relax Gricean Informational Constraints on
Discourse Anaphoric Noun Phrases. Language and Speech, 32 (2,3), 229–264.
Pickering, M., & Garrod, S. (2004). Toward a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27 (2), 169–226.
192

Learning Content Selection Rules for Generating Object Descriptions

Poesio, M. (2000). Annotating a corpus to develop and evaluate discourse entity realization algorithms: issues and preliminary results. In Proc. Language Resources and
Evaluation Conference, LREC-2000, pp. 211–218.
Pollack, M. E. (1991). Overloading intentions for efficient practical reasoning. Noûs, 25,
513 – 536.
Prince, E. F. (1981). Toward a taxonomy of given-new information. In Radical Pragmatics,
pp. 223–255. Academic Press.
Radev, D. R. (1998). Learning correlations between linguistic indicators and semantic
constraints: Reuse of context-dependent decsriptions of entities. In COLING-ACL,
pp. 1072–1078.
Ratnaparkhi, A. (2002). Trainable approaches to surface natural language generation and
their application to conversational dialog systems. Computer Speech and Language:
Special Issue on Spoken Language Generation, 16 (3-4), 435–455.
Reiter, E. (1990). Generating appropriate natural language object descriptions. Tech. rep.
TR-10-90, Department of Computer Science, Harvard University. Dissertation.
Reiter, E. (2002). Should corpora be gold standards for NLG?. In Proceedings of the 11th
International Workshop on Natural Language Generation, pp. 97–104.
Roy, D. K. (2002). Learning visually grounded words and syntax for a scene description
task. Computer Speech and Language: Special Issue on Spoken Language Generation,
16 (3-4), 353–385.
Stone, M., & Webber, B. (1998). Textual economy through close coupling of syntax and
semantics. In Proceedings of 1998 International Workshop on Natural Language Generation, pp. 178–187, Niagra-on-the-Lake, Canada.
Terken, J. M. B. (1985). Use and Function of Accentuation: Some Experiments. Ph.D.
thesis, Institute for Perception Research, Eindhoven, The Netherlands.
van Deemter, K. (2002). Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28 (1), 37–52.
Varges, S., & Mellish, C. (2001). Instance-based natural language generation. In Proceedings
of the North American Meeting of the Association for Computational Linguistics, pp.
1–8.
Walker, M., Rambow, O., & Rogati, M. (2002). Training a sentence planner for spoken
dialogue using boosting. Computer Speech and Language: Special Issue on Spoken
Language Generation, 16 (3-4), 409–433.
Walker, M. A. (1996a). Limited attention and discourse structure. Computational Linguistics, 22-2, 255–264.
Walker, M. A. (1996b). The Effect of Resource Limits and Task Complexity on Collaborative
Planning in Dialogue. Artificial Intelligence Journal, 85 (1–2), 181–243.
Webber, B. L. (1978). A Formal Approach to Discourse Anaphora. Ph.D. thesis, Harvard
University. New York:Garland Press.
193

Jordan & Walker

Weiss, S. M., & Kulikowski, C. (1991). Computer Systems That Learn: Classification and
Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. San Mateo, CA: Morgan Kaufmann.
Yeh, C.-L., & Mellish, C. (1997). An empirical study on the generation of anaphora in
chinese. Computational Linguistics, 23-1, 169–190.

194

Journal of Artiﬁcial Intelligence Research 24 (2005) 357-406

Submitted 12/04; published 09/05

Pure Nash Equilibria: Hard and Easy Games
Georg Gottlob

gottlob@dbai.tuwien.ac.at

Information Systems Department,
Technische Universität Wien,
A-1040 Wien, Austria

Gianluigi Greco

ggreco@mat.unical.it

Dipartimento di Matematica,
Università della Calabria,
I-87030 Rende, Italy

Francesco Scarcello

scarcello@deis.unical.it

DEIS,
Università della Calabria,
I-87030 Rende, Italy

Abstract
We investigate complexity issues related to pure Nash equilibria of strategic games. We
show that, even in very restrictive settings, determining whether a game has a pure Nash
Equilibrium is NP-hard, while deciding whether a game has a strong Nash equilibrium is
ΣP
2 -complete. We then study practically relevant restrictions that lower the complexity.
In particular, we are interested in quantitative and qualitative restrictions of the way each
player’s payoﬀ depends on moves of other players. We say that a game has small neighborhood if the utility function for each player depends only on (the actions of) a logarithmically
small number of other players. The dependency structure of a game G can be expressed
by a graph G(G) or by a hypergraph H(G). By relating Nash equilibrium problems to
constraint satisfaction problems (CSPs), we show that if G has small neighborhood and if
H(G) has bounded hypertree width (or if G(G) has bounded treewidth), then ﬁnding pure
Nash and Pareto equilibria is feasible in polynomial time. If the game is graphical, then
these problems are LOGCFL-complete and thus in the class NC2 of highly parallelizable
problems.

1. Introduction and Overview of Results
The theory of strategic games and Nash equilibria has important applications in economics
and decision making (Nash, 1951; Aumann, 1985). Determining whether Nash equilibria exist, and eﬀectively computing them, are relevant problems that have attracted much
research in computer science (e.g. Deng, Papadimitriou, & Safra, 2002; McKelvey & McLennan, 1996; Koller, Megiddo, & von Stengel, 1996). Most work has been dedicated to complexity issues related to mixed equilibria of games with mixed strategies, where the player’s
choices are not deterministic and are regulated by probability distributions. In that context, the existence of a Nash equilibrium is guaranteed by Nash’s famous theorem (Nash,
1951), but it is currently open whether such an equilibrium can be computed in polynomial
time (cf., Papadimitriou, 2001). First results on the computational complexity for a twoperson game have been presented by Gilboa and Zemel (1989), while extensions to more
general types of games have been provided by Megiddo and Papadimitriou (1991), and by
c
2005
AI Access Foundation. All rights reserved.

Gottlob, Greco, and Scarcello

Papadimitriou (1994b). A recent paper of Conitzer and Sandholm (2003b) also proved the
NP-hardness of determining whether Nash equilibria with certain natural properties exist.
In the present paper, we are not dealing with mixed strategies, but rather investigate the
complexity of deciding whether there exists a Nash equilibrium in the case of pure strategies,
where each player chooses to play an action in a deterministic, non-aleatory manner. Nash
equilibria for pure strategies are brieﬂy referred to as pure Nash equilibria. Note that in
the setting of pure strategies, a pure Nash equilibrium is not guaranteed to exist (see,
for instance, Osborne & Rubinstein, 1994). Particular classes of games having pure Nash
equilibria have been studied by Rosenthal (1973), Monderer and Shapley (1993), and by
Fotakis et al. (2002). Recently, Fabrikant at el. (2004) renewed the interest in the class
of games deﬁned Rosenthal (1973), called congestion games, by showing that a pure Nash
equilibrium can be computed in polynomial time in the symmetric network case, while the
problem is PLS-complete (Johnson, Papadimitriou, & Yannakakis, 1998) in general.
Our goal is to study fundamental questions such as the existence of pure Nash, Pareto,
and strong Nash equilibria, the computation of such equilibria, and to ﬁnd arguably realistic
restrictions under which these problems become tractable. Throughout the paper, Pareto
and strong Nash equilibria are considered only in the setting of pure strategies.
While pure strategies are conceptually simpler than mixed strategies, the associated
computational problems appear to be harder. In fact, we show that even if severe restrictions are imposed on the set of allowed strategies, determining whether a game has a pure
Nash or Pareto Equilibrium is NP-complete, while deciding whether a game has a strong
Nash equilibrium is even ΣP2 -complete. However, by jointly applying suitable pairs of more
realistic restrictions, we obtain settings of practical interest in which the complexity of the
above problems is drastically reduced. In particular, determining the existence of a pure
Nash equilibrium and computing such an equilibrium will be feasible in polynomial time
and we will show that, in certain cases, these problems are even complete for the very low
complexity class LOGCFL, which means that these problems are essentially as easy as the
membership problem for context-free languages, and are thus highly parallelizable (in NC2 ).
In the setting of pure strategies, to which we will restrict our attention in the rest of
this paper, a ﬁnite strategic game is one in which each player has a ﬁnite set of possible
actions, from which she chooses an action once and for all, independently of the actual
choices of the other players. The choices of all players can thus be thought to be made
simultaneously. The choice of an action by a player is referred to as the player’s strategy.
It is assumed that each player has perfect knowledge over all possible actions and over
the possible strategies of all players. A global strategy, also called profile in the literature,
consists of a tuple containing a strategy for each player. Each player has a polynomial-time
computable real valued utility function, which allows her to assess her subjective utility of
each possible global strategy (global strategies with higher utility are better). A pure Nash
equilibrium (Nash, 1951) is a global strategy in which no player can improve her utility by
changing her action (while the actions of all other players remain unchanged). A strong
Nash equilibrium (Aumann, 1959) is a pure Nash equilibrium where no change of strategies
of whatever coalition (i.e., group of players) can simultaneously increase the utility for all
players in the coalition. A pure Nash equilibrium is Pareto optimal (e.g. Maskin, 1985) if
the game admits no other pure Nash equilibrium for which each player has a strictly higher
utility. A Pareto-optimal Nash equilibrium is also called a Pareto Nash Equilibrium.
358

Pure Nash Equilibria: Hard and Easy Games

Before describing our complexity results, let us discuss various parameters and features
that will lead to restricted versions of strategic games. We consider restrictions of strategic
games which impose quantitative and/or qualitative limitations on how the payoﬀs of an
agent (and hence her decisions) may be inﬂuenced by the other agents.
The set of neighbors Neigh(p) of a player is the set of other players who potentially
matter w.r.t. p’s utility function. Thus, whenever a player q = p is not in Neigh(p) then p’s
utility function does not directly depend on the actions of q. We assume that each game
is equipped with a polynomial-time computable function Neigh with the above property.1
The player neighborhood relationship, typically represented as a graph (or a hypergraph),
is the central notion in graphical games (Koller & Milch, 2001; Kearns, Littman, & Singh,
2001b), as we will see in more detail in the next section.
A ﬁrst idea towards the identiﬁcation of tractable classes of games is to restrict the
cardinality of Neigh(p) for all players p. For instance, consider a set of companies in a
market. Each company has usually a limited number of other market players on which it
bases its strategic decisions. These relevant players are usually known and constitute the
neighbors of the company in our setting. However, note that even in this case the game
outcome still depends on the interaction of all players, though possibly in an indirect way.
Indeed, the choice of a company inﬂuences the choice of its competitors, and hence, in turn,
the choice of competitors of its competitors, and so on. In this more general setting, a
number of real-world cases can be modeled in a very natural way. We can thus deﬁne the
following notion of limited neighborhood:
Bounded Neighborhood: Let k > 0 be a ﬁxed constant. A strategic game with associated neighborhood function Neigh has k-bounded neighborhood if, for each player p,
|Neigh(p)| ≤ k .
While in some setting the bounded neighborhood assumption is realistic, in other settings the constant bound appears to be too harsh an imposition. It is much more realistic
and appealing to relax this constraint and consider a logarithmic bound rather than a constant bound on the number of neighbors.
Small Neighborhood: For a game G denote by P (G) the set of its players, by Act(p)
the set of possible actions of a player p, and by ||G|| the total size of the description of a
game G (i.e., the input size n). Furthermore, let maxNeigh(G) = maxp∈P (G) |Neigh(p)| and
maxAct (G) = maxp∈P (G) |Act(p)|.
A class of strategic games has small neighborhood if, for each game G in this class,
maxNeigh(G) = O (

log ||G||
)
log maxAct (G)

Note the denominator log maxAct(G) in the above bound. Intuitively, we use this term
to avoid “cheating” by trading actions for neighbors. Indeed, roughly speaking, player
interactions may be reduced signiﬁcantly by adding an exponential amount of additional
actions. For any player, these fresh actions may encode all possible action conﬁgurations of
1. Note that each game can be trivially represented in this setting, possibly setting Neigh(p) to be the
set of all players, for each player p. In most cases, however, one will be able to provide a much better
neighborhood function.

359

Gottlob, Greco, and Scarcello

Figure 1: Dependency hypergraph and dependency graph for the game G.
some of her neighbors, yielding an equivalent game with less interaction, and possibly with
fewer players, too. The denominator takes this into account.
In other terms, a class of games has small neighborhood if there is a constant c such
||G||
that for all but ﬁnitely many pairs (G, p) of games and players, |Neigh(p)| < c × ( loglog
|Act(p)| ).
The related notion i(G) of intricacy of a game is deﬁned by:
i(G) =

maxNeigh(G) × log maxAct (G)
.
log ||G||

It is clear that a class of games has small neighborhood if and only if the intricacy of all
games in it is bounded by some constant.
Obviously, bounded neighborhood implies small neighborhood, but not vice-versa. We
believe that a very large number of important (classes of) games in economics have the
small neighborhood property.
In addition to the quantitative aspect of the size of the neighborhood (and of the neighborhood actions), we are also interested in qualitative aspects of mutual strategic inﬂuence.
Following Kearns et al. (2001b), for a game G with a set P of players, we deﬁne the strategic dependency graph as the undirected graph G(G) having P as its set of vertices and
{{p, q} | q ∈ P ∧ p ∈ Neigh(q)} as its set of edges. Moreover, we deﬁne the strategic dependency hypergraph H(G), whose vertices are the players P and whose set of hyperedges is
{{p} ∪ Neigh(p) | p ∈ P }. For instance, consider a game G over players A, B, C, and D such
that Neigh(A) = {B, C }, Neigh(B) = {A, C }, Neigh(C ) = {A, B}, and Neigh(D ) = {C }.
Figure 1 shows the dependency graph and the dependency hypergraph associated with G.
We consider the following classes of structurally restricted games:
Acyclic-Graph Games:

Games G for which G(G) is acyclic.

Acyclic-Hypergraph Games: Games G for which H(G) is acyclic. Note that there are
several deﬁnitions of hypergraph acyclicity (Fagin, 1983). Here we refer to the broadest
(i.e., the most general) one, also known as α-acyclicity (Fagin, 1983; Beeri, Fagin, Maier,
& Yannakakis, 1983) (see Section 2).
Each acyclic-graph game is also an acyclic-hypergraph game, but not vice-versa. As an
extreme example, let G be a game with player set P in which the utility of each action for
each player depends on all other players. Then G(G) is a clique of size |P | while H(G) is
the trivially acyclic hypergraph having the only hyperedge {P }.
For strategic games, both the acyclic graph and the acyclic hypergraph assumptions are
very severe restrictions, which are rather unlikely to apply in practical contexts. However,
360

Pure Nash Equilibria: Hard and Easy Games

there are important generalizations that appear to be much more realistic for practical
applications. These concepts are bounded treewidth (Robertson & Seymour, 1986) and
bounded hypertree width (Gottlob, Leone, & Scarcello, 2002b) (see also Section 5), which
are suitable measures of the degree of cyclicity of a graph and of a hypergraph, respectively.
In particular, each acyclic graph (hypergraph) has treewidth (hypertree width) ≤ 1. It was
argued that an impressive number of “real-life” graphs have a very low treewidth (Downey
& Fellows, 1995). Hypertree width in turn was fruitfully applied in the context of database
queries (Gottlob et al., 2002b) and constraint satisfaction problems (Gottlob, Leone, &
Scarcello, 2000). Formal deﬁnitions are given in Section 5. Note that both computing
the treewidth of a graph and the hypertree width of a hypergraph are NP-hard problems.
However, for each (ﬁxed) constant k, it can be checked in polynomial time whether a
graph has treewidth k (Bodlaender, 1997) and whether a hypergraph has hypertree width
k (Gottlob et al., 2002b). We have, for each constant k, the following restricted classes of
games:
Games of treewidth bounded by k:
is ≤ k.

The games G such that the treewidth of G(G)

Games of hypertree width bounded by k: The games G such that the hypertree
width of H(G) is ≤ k.
In the context of complexity and eﬃciency studies, it is very important to make clear
how an input (in our case, a multiplayer game) is represented. We say that a game is
in general form if the sets of players and actions are given in extensional form and if
the neighborhood and utility functions are polynomially computable functions. Unless
otherwise stated, we always assume that games are given in general form. For classes of
games having particular properties, some alternative representations have been used by
various authors. For instance, in game theory literature, the set of utility functions is often
represented through a single table (or matrix) having an entry for each combination of
players’ actions containing, for each player p, the evaluation of her utility function for that
particular combination. This representation is said to be in standard normal form (SNF)
(see, for instance, Osborne & Rubinstein, 1994; Owen, 1982). Note that, if there are many
players, this representation may be very space consuming, particularly if some players are
not interested in all other players, but only in some subset of them. Moreover, in this case,
the monolithic utility table in SNF obscures much of the structure that is present in realworld games (Koller & Milch, 2001). In fact, in the context of games with restricted players
interactions, the most used representation is the graphical normal form (GNF). In GNF
games, also known as graphical games (Kearns, Littman, & Singh, 2001a; Kearns et al.,
2001b; Kearns & Mansour, 2002; Vickrey, 2002), the utility function for each player p is
given by a table that displays p’s utility as a function of all possible combined strategies of
p and p’s neighbors, but not of other players irrelevant to p. Therefore, for large population
games (modeling for instance agent interactions over the internet), the SNF is practically
unfeasible, while the more succinct graphical normal form works very well, and is actually
a more natural representation.
Main results.

The main results of this paper are summarized as follows:
361

Gottlob, Greco, and Scarcello

• Determining whether a strategic game has a pure Nash equilibrium is NP-complete
and remains NP-complete even for following two restricted cases:
– Games in graphical normal form (GNF) having bounded neighborhood (Theorem 3.1).
– Acyclic-graph games, and acyclic-hypergraph games (Theorem 3.2).
The same results hold for Pareto Nash equilibria for pure strategies.
• Determining whether a strategic game has a strong Nash equilibrium is Σp2 -complete
and thus at the second level of the Polynomial Hierarchy (Theorem 3.7 and Theorem 3.8). The proof of this theorem gives us a fresh game-theoretic view of the class
ΣP2 as the class of problems whose positive instances are characterized by a coalition
of players who cooperate to provide an equilibrium, and win against any other disjoint coalition, which fails in trying to improve the utility for all of its players. E.g.,
in the case of Σ2 quantiﬁed Boolean formulas, the former coalition consists of the
existentially quantiﬁed variables, and the latter of the universally quantiﬁed ones.
• The pure Nash, Pareto and strong equilibrium existence and computations problems
are feasible in logarithmic space for games in standard normal form (Theorem 4.1).
• The pure Nash equilibrium existence and computation problems are tractable for
games (in whatever representation) that simultaneously have small neighborhood and
bounded hypertree width (Theorem 5.3). Observe that each of the two joint restrictions, small neighborhood and bounded hypertree width, is weaker than the restrictions of bounded neighborhood and acyclicity, respectively, of which each by itself does
not guarantee tractability. Thus, in order to obtain tractability, instead of strengthening a single restriction, we combined two weaker restrictions. While we think that
each of the two strong restrictions is unrealistic, we believe that for many natural
games the combined weaker restrictions do apply. In order to prove the tractability
result, we establish a relationship between strategic games and the well-known ﬁnite
domain constraint satisfaction problem (CSP), much studied in the AI and OR literature (e.g. Vardi, 2000; Gottlob et al., 2000). Let us point out that also Vickrey and
Koller (2002) recently exploited a mapping to CSP for the diﬀerent problem of ﬁnding
approximate mixed equilibria in graphical games. We show that each (general, not
necessarily GNF) strategic game G can be translated into a CSP instance having the
same hypertree width as G, and whose feasible solutions exactly correspond to the
Nash equilibria of the game. Then, we are able to prove that G is equivalent to an
acyclic constraint satisfaction problem of size ||G||O(i(G)×hw(G)) , where i(G) is the intricacy of G and hw(G) is the hypertree width of its strategic dependency hypergraph.
Acyclic CSPs, in turn, are well-known to be solvable in polynomial time.
• Exploiting the same relationship with CSPs, we prove that the Nash-equilibrium existence and computation problems are tractable for games in graphical normal form
(GNF) having bounded hypertree width (Theorem 5.3), regardless of the game intricacy, i.e., even for unbounded neighborhood.
362

Pure Nash Equilibria: Hard and Easy Games

• We show that if a strategic game has bounded treewidth, then it also has bounded
hypertree width (Theorem 5.7). Note that this is a novel result on the relationship
between these two measures of the degree of cyclicity, since earlier works on similar
subjects dealt with either the primal or the dual graph of a given hypergraph, rather
than with a dependency graph, as we do in the present paper, focused on games. Combined with the two previous points, this entails that the Nash-equilibrium existence
and computation problems are tractable for games that simultaneously have small
neighborhood and bounded treewidth, and for GNF games having bounded treewidth
(Corollary 5.9).
• In all above cases where a pure Nash Equilibrium can be computed in polynomial time,
also a Pareto Nash equilibrium can be computed in polynomial time (Theorem 4.6
and Corollary 5.4).
• These tractability results partially extend to strong Nash equilibria. Indeed, the
checking problem becomes feasible in polynomial time for acyclic-hypergraph games
in GNF. However, even in such simple cases, deciding whether a game has strong
Nash equilibria is NP-complete, and thus still untractable (Theorem 4.8).
• We go a bit further, by determining the precise complexity of games with acyclic
(or even bounded width) interactions among players: In case a game is given in
GNF, the problem of determining a pure Nash equilibrium of a game of bounded
hypertree-width (or bounded treewidth) is LOGCFL-complete and thus in the parallel complexity class NC2 (Theorem 6.1). Membership in LOGCFL follows from
the membership of bounded hypertree-width CSPs in LOGCFL (Gottlob, Leone, &
Scarcello, 2001). Hardness for LOGCFL is shown by transforming (logspace uniform
families of) semi-unbounded circuits of logarithmic depth together with their inputs
into strategic games, such that the game admits a Nash equilibrium if and only if the
circuit outputs one on the given input.
Figure 2 summarizes our results on the existence of pure Nash equilibria.
While various authors have dealt with the complexity of Nash equilibria (e.g. Gilboa &
Zemel, 1989; Papadimitriou, 1994b; Koller & Megiddo, 1992, 1996; Conitzer & Sandholm,
2003b), most investigations were dedicated to mixed equilibria and — to the best of our
knowledge — all complexity results in the present paper are novel. We are not aware of any
other work considering the quantitative and structural restrictions on pure games studied
here. Note that tree-structured games were ﬁrst considered by Kearns et al. (2001b) in
the context of mixed equilibria. It turned out that, for such games, suitable approximation
of (mixed) Nash equilibria can be computed in polynomial time. In our future work, we
would like to extend our tractability results even to this setting. We are not aware of
any work by others on the parallel complexity of equilibria problems. We believe that
our present work contributes to the understanding of pure Nash equilibria and proposes
appealing and realistic restrictions under which the main computation problems associated
with such equilibria are tractable.
The rest of the paper is organized as follows. In Section 2 we introduce the basic notions
of games and Nash equilibria that are studied in the paper, and we describe how games
363

Gottlob, Greco, and Scarcello

Figure 2: Complexity of deciding existence of pure Nash equilibria for games in GNF —
numbers indicate theorems where the corresponding results are proved.

may be represented. In Section 3 we thoroughly study the computational complexity of
deciding the existence of pure Nash, Pareto and strong equilibria. In Section 4 we identify
tractable classes of games, and in Section 5 we extend our tractability results to larger
class of games, where the interaction among players has a bounded degree of cyclicity. In
Section 6 we improve the results on the polynomial tractability of easy games, providing the
precise computational complexity for games with acyclic (or bounded width) interactions
among players. Finally, in Section 7, we draw our conclusions, and we discuss possible
further research and related works.
364

Pure Nash Equilibria: Hard and Easy Games

2. Games and Nash Equilibria
A game G is a tuple P, Neigh, Act, U , where P is a non-empty set of distinct players and
Neigh : P −→ 2 P is a function such that for each p ∈ P , Neigh(p) ⊆ P − {p} contains
all neighbors of p, Act : P −→ A is a function returning for each player p a set of possible
actions Act(p), and U associates a utility function up : Act(p) ×j ∈Neigh(p) Act(j ) → 
 to
each player p.
Note that, in general, the players interests are not symmetric. Thus, it may happen
that, for a pair of players p1 , p2 ∈ P , p1 ∈ Neigh(p2 ) but p2 ∈ Neigh(p1 ).
For a player p, pa denotes her choice to play the action a ∈ Act(p). Each possible pa is
called a strategy for p, and the set of all strategies for p is denoted by St(p).2
For a non-empty set of players P  ⊆ P , a combined strategy for P  is a set containing
exactly one strategy for each player in P  . St(P  ) denotes the set of all combined strategies
for the players in P  .
A combined strategy (also, profile) x is called global if all players contribute to it, that
is, if P  = P . The global strategies are the possible outcomes of the game.
A set of players K ⊆ P is often called a coalition. Let x be a global strategy, K a
coalition, and y a combined strategy for K. Then, we denote by x−K [y] the global strategy
where, for each player p ∈ K, her individual strategy pa ∈ x is replaced by her individual
strategy pb ∈ y. If K is a singleton {p}, we will simply write x−p [y].
Let x be a global strategy, p a player, and up the utility function of p. Then, with a
small abuse of notation, up (x) will denote the output of up on the projection of x to the
domain of up , i.e., the output of the function up applied to the actions played by p and her
neighbors according to the strategy x.
In the context of complexity and eﬃciency studies it is very important to make clear
how an input (in our case, a multiplayer game) is represented.
General Form: A game is in general form if the sets of players and actions are given in
extensional form, while the neighborhood and utility functions are given intentionally, e.g.,
through encodings of deterministic Turing transducers. More precisely, we are interested in
classes of games such that the computation time of the neighborhood and utility functions
is globally bounded by some polynomial. Let us denote by Ck the class of all games G
in general form whose neighborhood and utility functions are computable in time O(nk ),
where n = |G|.
For the sake of presentation, we assume hereafter k̄ ≥ 1 to be any such a ﬁxed global
bound. Moreover, unless otherwise stated, when we speak of a “general game G” (or we
omit any speciﬁcation at all) we mean a game G ∈ Ck̄ .
The following more restrictive classes of (representations of) games have been used by
many authors.
Standard Normal Form (SNF): A game with set P of players is in standard normal
form (SNF) if its utility functions are explicitly represented by a single table or matrix
having an entry (or cell) for each global strategy x, displaying a list containing for each player
2. Note the technical distinction between actions and strategies: an action is an element of the form a,
while a strategy is an element of the form pa , i.e., it is an action chosen by a player. This helps in
technical proofs, since a strategy singles out both a player and her choice.

365

Gottlob, Greco, and Scarcello

p, p’s payoﬀ up (x) w.r.t. x. (Equivalently, we may describe the utilities by |P | such tables,
where the i-th table describes just the payoﬀ of player i.) This is a representation of utility
functions often assumed in the literature (see, for instance, Osborne & Rubinstein, 1994;
Owen, 1982). Observe that, in the general case, even if an utility function is polynomially
computable, writing it down in form of a table may require exponential space.
Graphical Normal Form (GNF): A game with a set P of players is in graphical normal
form (GNF) if the utility function for each player p is represented by a separate table
containing a cell for each combined strategy x ∈ St(Neigh(p) ∪ {p}) of the p’s set of
neighbors Neigh(p)∪ {p}, displaying p’s payoﬀ up (x) w.r.t. x. A game in GNF is illustrated
in Example 2.1. The GNF representation has been adopted in several recent papers that
study games with a large number of players, where the utility function of each player depends
directly only on the strategies of those (possibly few) players she is interested in (e.g. Kearns
et al., 2001a, 2001b; Kearns & Mansour, 2002; Vickrey, 2002). Note that GNF may lead to
an exponentially more succinct game representation than SNF. Notwithstanding, the SNF
is often used in the literature, mostly because, historically, the ﬁrst investigations focused
on two-player games. Moreover, games in GNF are often referred to as graphical games.
We prefer to use the phrasing games in graphical normal form, because this makes clear
that we are addressing representational issues.
The following example, to which we will refer throughout the paper, should sound
familiar to everyone, as it is a generalization of the well known two-person game “battle of
sexes”.
Example 2.1 (FRIENDS) Let us consider the game FRIENDS, that is played by a group
of persons that have to plan their evening happenings. The players are George (short: G),
Pauline (P ), Frank (F ), Robert (R), and Mary (M ). Each of them have to decide to go
either to see a movie (m) or to see an opera (o). However, preferences concern not only the
particular option (m or o) to be chosen, but usually also the persons to join for the evening
(possibly, depending on the movie or opera choice). For instance, we assume that Frank
is interested in joining Pauline and Robert. He would like to join both of them. However,
Pauline is an expert of movies and Robert is an expert of operas. Thus, if it is not possible
to go out all together, he prefers to go to the cinema with Pauline and to the opera with
Robert. Pauline would like to stay with Frank, and she prefers the movies. Robert does
not like Frank because he speaks too much and, as we know, he prefers the opera. Mary,
too, likes operas and would like to go to the opera with Robert. Finally, George is the
matchmaker of the group: He has no personal preferences but would like that Frank and
Pauline stay together for the evening, best if they go to the cinema. All the utility functions
associated with this game are shown in Figure 3, where we denote the fact that a player X
chooses an action a by Xa , e.g., Fm denotes the strategy where Frank chooses to play the
action m.
2

Let us now formally deﬁne the main concepts of equilibria to be further studied in this
paper.
Definition 2.2 Let G = P, Neigh, A, U  be a game. Then,
366

Pure Nash Equilibria: Hard and Easy Games

F
m
o

Pm Rm
2
0

Pm Ro
2
2
R
m
o

Fm
0
2

Po Rm
1
1

Po Ro
0
2

Fo
1
0

P
m
o

Fm
2
0

G
m
o

Pm Fm
2
2

Fo
0
1

M
m
o

Pm Fo
0
0
Rm
1
0

Po Fm
0
0

Po Fo
1
1

Ro
0
2

Figure 3: Utility functions for FRIENDS in GNF
• a global strategy x is a pure Nash Equilibrium for G if, for every player p ∈ P ,
 ∃pa ∈ St(p) such that up (x) < up (x−p [pa ]);
• a global strategy x is a pure strong Nash Equilibrium for G if, ∀K ⊆ P, ∀y ∈
St(K ), ∃p ∈ K such that up (x) ≥ up (x−K [y]) or, equivalently, if ∀K ⊆ P ,  ∃y ∈
St(K ) such that, ∀p ∈ K, up (x) < up (x−K [y]);
• a pure Nash equilibrium x is a pure Pareto Nash Equilibrium for G if there does not
exist a pure Nash equilibrium y for G such that, ∀p ∈ P, up (x) < up (y).3
The sets of pure Nash, strong Nash, and Pareto Nash equilibria of G are denoted by
NE(G), SNE(G), and PNE(G), respectively. It is easy to see and well known that the
following relationships hold among these notions of Nash equilibria: SNE(G) ⊆ PNE(G) ⊆
NE(G). Moreover, the existence of a Nash equilibrium does not imply the existence of a
strong Nash equilibrium. However, if there exists a Nash equilibrium, then there exists also
a Pareto Nash equilibrium.
{Fm , Pm , Ro , Go , Mo },
Example 2.3 The
strategies
{Fm , Pm , Ro , Gm , Mo },
{Fo , Po , Rm , Gm , Mm } and {Fo , Po , Rm , Go , Mm } are the Nash equilibria of the FRIENDS
game. For instance, consider the latter strategy, where all players get payoﬀ 1. In this
case, since P plays opera and R plays movie, F cannot improve his payoﬀ by changing from
opera to movie. The same holds for G, while R, P , and M would get the lower payoﬀ 0, if
they change their choices.
Moreover, note that the ﬁrst two strategies above, namely {Fm , Pm , Ro , Gm , Mo } and
{Fm , Pm , Ro , Go , Mo }, are the only Pareto Nash equilibria, as well as the strong Nash equilibria. Indeed, for these global strategies all players get their maximum payoﬀ 2, and thus
there is no way to improve their utilities.
2

The interaction among players of G can be more generally represented by a hypergraph
H(G) whose vertices coincide with the players of G and whose set of (hyper)edges contains
for each player p a (hyper)edge H(p) = {p} ∪ Neigh(p), referred-to as the characteristic edge
of p. Intuitively, characteristic edges correspond to utility functions.
3. Note that only pure strategies do matter in this definition, as there is no requirement with regard to
how pure candidate equilibria compare to possible mixed equilibria.

367

Gottlob, Greco, and Scarcello

A fundamental structural property of hypergraphs is acyclicity. Acyclic hypergraphs
have been deeply investigated and have many equivalent characterizations (e.g. Beeri et al.,
1983). We recall here that a hypergraph H is acyclic if and only if there is a join tree for H,
that is, there is a tree JT whose vertices are the edges of H and, whenever the same player
p occurs in two vertices v1 and v2 , then v1 and v2 are connected in JT , and p occurs in each
vertex on the unique path linking v1 and v2 (see Figure 5 for a join tree of H(FRIENDS)).
In other words, the set of vertices in which p occurs induces a (connected) subtree of JT .
We will refer to this condition as the Connectedness Condition of join trees (also known as
running intersection property).
Another representation of the interaction among players is through the (undirected)
dependency graph G(G) = (P, E), whose vertices coincide with the players of G, and {p, q} ∈
E if p is a neighbor of q (or vice versa). For completeness we observe that, even if most works
on graphical games use this dependency graph, another natural choice is representing the
game structure by a directed graph (also called inﬂuence graph), which takes into account
the fact that payoﬀs of a player p may depend on payoﬀs of a player q and not vice versa, in
general. Following Kearns et al. (2001b), in the present paper we use the undirected version
because we are interested in identifying game structures that possibly allow us to compute
eﬃciently Nash equilibria, and directed graphs do not help very much for this purpose. Let
us give a hint of why this is the case, by thinking of a group of players X̄ = {X1 , . . . , Xn },
each one having only one neighbor C, whose payoﬀs do not depend on any player in X̄.
Player C has one neighbor D, who has C has its only neighbor. Figure 4 shows a directed
graph representing these player interactions. It is easy to design a game with these players

Figure 4: A directed graph representation of player interactions.
such that, for some combined strategy x of the X̄ players, there is no Nash equilibrium,
while for some other combined strategy x of these players, there is a combined strategy y
of C and D such that the union of x and y is a Nash equilibrium for the game. Therefore,
as far as the possibility of reaching a Nash equilibrium is concerned, the choices of players
in X̄ depend on each other, on the way of playing of C, and transitively on player D, too.
Observe that the undirected dependency graph represents in a succinct way, i.e., through
their direct connections, such a mutual relationship among players. However, the direct
graph does not model this kind of inﬂuence, as looking at this graph it seems that players
in X̄ should not worry about any other player in the game. In fact, exploiting the gadgets
and the constructions described in this paper, it is easy to see that even simple games whose
directed inﬂuence graphs are quasi-acyclic (i.e., they are acyclic, but for some trivial cycles
like the one shown in Figure 4), are hard to deal with. Thus, apart from the well known
368

Pure Nash Equilibria: Hard and Easy Games

G

G

F
P

R

P

M
H(FRIENDS)

G

F
R

F
P

M
G(FRIENDS)

HF(F,P,R)

R
HP(P,F)

HR(R,F)

HG(G,F,P)

HM(M,R)

M
PG of H(FRIENDS)

Figure 5: Hypergraph, dependency graph, and primal graph of the FRIENDS game. On
the right, a join tree for H(FRIENDS).

easy acyclic cases, any reasonable generalization of the notion of direct acyclicity does not
appear to be useful for identifying further tractable classes of games.
Observe that the dependency graph G(G) is diﬀerent from the so called primal graph PG
of H(G), which contains an edge for all pairs of vertices that jointly occur in some hyperedge
of H(G). In general, G(G) is much simpler than PG. For instance, consider a game G with
a player p that depends on all other players q1 , . . . , qn , while these players are independent
of each other (but possibly depend on p). Then, G(G) is a tree. However, the primal graph
of H(G) is a clique with n + 1 vertices.
Example 2.4 The hypergraph H(FRIENDS), the graph G(FRIENDS) and the primal
graph of H(FRIENDS) are shown in Figure 5. Note that the dependency graph associated with the FRIENDS game is not acyclic, even though the associated hypergraph is
acyclic (on the right, we also report a join tree for it). Moreover, note that the dependency
graph diﬀers from the primal graph, as player P is not a neighbor of R and viceversa. 2

3. Hard Games
In this section, we precisely characterize the complexity of deciding the existence of the
diﬀerent kinds of pure Nash equilibria (regular, Pareto, and strong). This way, we are able
to identify the sources of complexity of such problems, in order to single out, in the following
sections, some natural and practically relevant tractable cases.
Every game considered in this section is assumed to be either in general or in graphical
normal form. Indeed, as we shall discuss in details in Section 4, for games in standard
normal form, computing pure Nash equilibria is a tractable problem, since one can easily
explore the (big) table representing the utility functions of all players, in order to detect
the strategy of interest. Since this table is given in input, such a computation is trivially
feasible in polynomial time.
We start by showing that deciding the existence of a pure Nash equilibrium is a diﬃcult
problem, even in a very restricted setting.

369

Gottlob, Greco, and Scarcello

Figure 6: Schema of the reduction in the proof of Theorem 3.1.
Theorem 3.1 Deciding whether a game G has a pure Nash equilibrium is NP-complete.
Hardness holds even if G is in GNF and has 3-bounded neighborhood, and the number of
actions is fixed.
Proof. Membership. We can decide that NE(G) = ∅ by guessing a global strategy x and
verifying that x is a Nash equilibrium for G. The latter task can be done in polynomial
time. Indeed, for each player p and for each action a ∈ Act(p), we only have to check that
choosing the strategy pa does not lead to an increment of up , and each of these tests is
feasible in polynomial time.
Hardness. Recall that deciding whether a Boolean formula in conjunctive normal form
Φ = c1 ∧ . . . ∧ cm over the variables X1 , . . . , Xn is satisﬁable, i.e., deciding whether there
exists truth assignments to the variables making each clause cj true, is the well known
NP-complete problem (CNF) SAT. Hardness holds even for its 3SAT restriction where each
clause contains at most three distinct (possibly negated) variables, and each variable occurs
in at most three clauses (Garey & Johnson, 1979). W.l.o.g, assume Φ contains at least one
clause and one variable.
We deﬁne a GNF game G such that: The players are partitioned into two sets Pv and Pc ,
corresponding to the variables and to the clauses of Φ, respectively; for each player c ∈ Pc ,
Neigh(c) is the set of the players corresponding to the variables in the clause c, and for
each player v ∈ Pv , Neigh(v ) is the set of the players corresponding to the clauses in which
v occurs; {t, f, u} is the set of possible actions for all the players, in which t and f can be
interpreted as the truth values true and false for variables and clauses. Figure 6 shows the
graph G(G) for the game G associated with the formula (X1 ∨ X3 ∨ X4 ) ∧ (¬X2 ∨ X4 ) ∧
(X4 ∨ X5 ∨ X6 ).
Let x be a global strategy, then the utility functions are deﬁned as follows. For each
player c ∈ Pc , her utility function uc is such that
(i) uc (x) = 3 if c plays t, and all of her neighbors play an action in {t, f } in such a way
that at least one of them makes the corresponding clause true;
(ii) uc (x) = 2 if c plays u, and all of her neighbors play an action in {t, f } in such a way
that none of them makes the corresponding clause true;
(iii) uc (x) = 2 if c plays f and there exists v ∈ Neigh(c) such that v plays u;
(iv) uc (x) = 1 in all the other cases.
370

Pure Nash Equilibria: Hard and Easy Games

For each player v ∈ Pv , her utility function uv is such that
(v) uv (x) = 3 if v plays an action in {t, f } and all of her neighbors play an action in {t, f };
(vi) uv (x) = 2 if v plays u and there exists c ∈ Neigh(v ) such that c plays u;
(vii) uv (x) = 1 in all the other cases.
We claim: Φ is satisﬁable ⇔ G admits a Nash equilibrium.
(⇒) Assume Φ is satisﬁable, and take one of such satisfying truth assignments, say σ.
Consider the global strategy x for G where each player in Pv chooses the action according
to σ, and where each player in Pc plays t. Note that, in this case, all players get payoﬀ 3
according to the rules (i) and (v) above, and since 3 is the maximum payoﬀ, x is a Nash
equilibrium for G.
(⇐) Let us show that each Nash equilibrium x for G corresponds to a satisfying truth
assignment of G. We ﬁrst state the following properties of the strategies for G.
P1 : A strategy x in which a player v ∈ Pv plays u cannot be a Nash equilibrium. Indeed,
assume by contradiction that x is a Nash equilibrium. Then, all c ∈ Neigh(v ) must
play f and have payoﬀ 2, from rule (iii); otherwise they would get payoﬀ 1, by rule (iv).
However, in this case player v gets payoﬀ 1 from rule (vii), and thus, from rule (v),
she can easily increase her payoﬀ to 3 by playing an action in {t, f }. Contradiction.
P2 : A strategy x in which a player c ∈ Pc plays u cannot be a Nash equilibrium. Indeed,
if there is such a player c that chooses u, then from rule (vi) each variable player
v ∈ Neigh(c) must play u, in order to get her maximum possible payoﬀ 2 for the case
at hand. Therefore, x cannot be a Nash equilibrium, by property P1 above.
P3 : A strategy x in which all players play an action in {t, f } and the corresponding truth
assignment makes a clause c false cannot be a Nash equilibrium. In fact, in this case,
from rule (ii), c should play u in order to get its maximum possible payoﬀ 2, and
hence x is not a Nash equilibrium, by property P2 .
P4 : A strategy x in which all players play an action in {t, f } and there exists a player c ∈ Pc
that plays f cannot be a Nash equilibrium. Indeed, if x is a Nash equilibrium and all
players play an action in {t, f }, by property P 3 the truth assignment corresponding to
this strategy satisﬁes each clause c. It follows that a player c that plays f contradicts
the assumption that x is a Nash equilibrium, because c could change her choice to t,
improving her payoﬀ to 3.
From these properties, it follows that every Nash equilibrium of G should be a strategy
where all players play either t or f and all players corresponding to clauses must play t and
get payoﬀ 3, as they are all made true by the truth assignment of their neighbors.
Combined with the “⇒”-part, this entails that there is a one-to-one correspondence
between satisfying truth assignments to the variables of Φ and Nash equilibria of the game G.
Finally, observe that the tables (matrices) representing the entries of the utility functions (rules (i)–(vii) above) can be built in polynomial time from Φ. Moreover, from the
assumptions that we made on the structure of Φ, each player depends on 3 other players
at most.
2

371

Gottlob, Greco, and Scarcello

It is worthwhile noting that, though quite limited, in the above proof the interaction
among players is rather complicated. In particular, it is easy to see that the dependency
graph of the game described in the hardness part of the proof is cyclic. Thus, one may
wonder if the problem is any easier for games with simple structured interactions. We next
show that, in the general setting, even if the structure of player interactions is very simple,
the problem of deciding whether pure Nash equilibria exist remains hard.
Theorem 3.2 Deciding whether a game G in general form has a pure Nash equilibrium is
NP-complete, even if both its dependency graph and its associated hypergraph are acyclic,
and the number of actions is fixed.
Proof. Membership follows from the previous theorem. We next prove that this problem is
NP-hard, via a reduction from SAT. Given a Boolean formula Φ over variables X1 , ..., Xm ,
we deﬁne a game G with m players X1 , ..., Xm corresponding to the variables of Φ, and two
additional players T and H. Any player Xi , 1 ≤ i ≤ m, has only two available actions, t and
f , corresponding to truth assignments to the corresponding variable of Φ. Moreover, the
utility function of each player Xi is a constant, say 1. Hence, the choice of Xi is independent
of any other player.
The actions of player T are s and u, which can be read “satisﬁed” and “unsatisﬁed,”
while the actions of player H are g and b, and can be read “good” and “bad,” respectively.
The role of T is to check whether the actions chosen by X1 , . . . , Xm encode a satisfying truth
assignment for Φ. Indeed, their behaviors – described below – ensure that only strategies
where T plays s may be Nash equilibria, because of her interaction with player H, whose
role is to discard bad strategies. Given a combined strategy x for the “variable players”
X1 , ..., Xm , we denote by Φ(x) the evaluation of Φ on the truth values determined by the
strategy x.
Player T depends on the players in {X1 , . . . , Xm , H}, and her utility function is deﬁned
as follows. For any combined strategy y = x1 ∪ x2 for X1 , . . . , Xm , H, T , where x1 is
a combined strategy for X1 , ..., Xm and x2 for T and H:
• uT (y) = 1 if Φ(x1 ) is true and T plays s, or Φ(x1 ) is false and x2 = {Tu , Hg }, or
Φ(x1 ) is false and x2 = {Ts , Hb };
• uT (y) = 0, otherwise.
Player H depends only on T and, for any combined strategy x for H and T , her utility
function is the following:
• uH (x) = 1 if x is either {Ts , Hg } or {Tu , Hb };
• uH (x) = 0, otherwise.
We claim there is a one-to-one correspondence between Nash equilibria of this game
and satisfying assignments for Φ. Indeed, let φ be a satisfying assignment for Φ and xφ the
combined strategy for X1 , . . . , Xm where these players choose their actions according to φ.
Then, xφ ∪ {Ts , Hg } is a Nash equilibrium for G, because all players get their maximum
payoﬀ.
372

Pure Nash Equilibria: Hard and Easy Games

On the other hand, if Φ is unsatisﬁable, for any combined strategy x1 for X1 , . . . , Xm ,
Φ(x1 ) is false. In this case, T and H have opposite interests and it is easy to check that, for
each combined strategy x2 ∈ St({H , T }), x1 ∪ x2 is not a Nash equilibrium for G, because
either H or T can improve its payoﬀ.
Finally, observe that the dependency graph G(G) is a tree, and that the hypergraph
H(G) is acyclic.
2
As shown in Figure 2, the above NP-hardness result immediately extends to all generalizations of acyclicity. The case of acyclic-hypergraph games in GNF will be dealt with in
Section 4.
Let us now draw our attention to Pareto equilibria. By Deﬁnition 2.2, a Pareto Nash
equilibrium exists if and only if a Nash equilibrium exists. Therefore, from Theorems 3.1
and 3.2, we get the following corollary.
Corollary 3.3 Deciding whether a game G has a Pareto Nash equilibrium is NP-complete.
Hardness holds even if G has a fixed number of actions and if either G is in graphical normal
form and has k-bounded neighborhood, for any fixed constant k ≥ 3, or if both G(G) and
H(G) are acyclic.
However, while checking whether a global strategy x is a pure Nash equilibrium is
tractable, it turns out that checking whether x is a Pareto Nash equilibrium is a computationally hard task. In fact, we next show that this problem is as diﬃcult as checking
whether x is a strong Nash equilibrium. However, we will see that deciding the existence of
a strong Nash equilibrium is much harder, and in fact complete for the second level of the
Polynomial Hierarchy. To this end, in the following proofs, we associate quantiﬁed Boolean
Formulas having two quantiﬁer alternations (2QBFs) with games.
Quantified Boolean Formulas (QBFs) and games.
Ξ

=

∃α1 , . . . αn

∀β1 , . . . βq

Let
Φ

be a quantiﬁed Boolean formula in disjunctive normal form, i.e., Φ is a Boolean formula of
the form d1 ∨. . .∨dm over the variables α1 , . . . αn , β1 , . . . βq , where each di is a conjunction of
literals. Deciding the validity of such formulas is a well-known ΣP2 -complete problem – (e.g.
Stockmeyer & Meyer, 1973), and it is easy to see that hardness result holds even if each
disjunct dj in Ξ contains three literals at most and each variable occurs in three disjuncts
at most. Moreover, without loss of generality, we assume that the number m of disjuncts is
a power of 2, say m = 2 , for some integer  ≥ 2. Note that, if 2−1 < m < 2 , then we can
build in polynomial time a new QBF Ξ having 2l − m more disjuncts, each one containing
both a fresh existentially quantiﬁed variable and its negation. Clearly, such disjuncts cannot
be made true by any assignment, and hence Ξ is equivalent to Ξ. Hereafter, we will consider
quantiﬁed Boolean formulas of this form, that we call R2QBF. For each such formula Ξ, a
truth value assignment σ to the existentially quantiﬁed variables α1 , . . . , αn such that the
formula ∀ β1 , . . . βq σ(Φ) is valid is called a witness of validity for Ξ.
As a running example for this section, we consider the following QBF Ξr :
∃α1 α2 α3 ∀β1 β2 β3 β4 β5 (α1 ∧ α2 ) ∨ (α1 ∧ α3 ) ∨ (α1 ∧ ¬β1 ) ∨ (β1 ) ∨ (¬β2 ∧ ¬β3 ) ∨ (β1 ∧ β3 ) ∨
(β3 ∧ β4 ) ∨ (β5 ).
373

Gottlob, Greco, and Scarcello

Figure 7: On the left: the dependency graph of the game GΞr . On the right: a truth-value
assignment for Ξr corresponding to a strong Nash equilibrium of GΞr .

We deﬁne a GNF game GΞ associated with a R2QBF Ξ as follows. The players of GΞ
are partitioned in ﬁve sets Pe , Pu , Pd , Pt , and the singleton {C}.
Players in Pe , Pu , and Pd correspond to the existential variables α1 , . . . αn , to the universal variables β1 , . . . βl , and to the m disjuncts of Φ, respectively. Each “variable” player v in
Pe ∪ Pu may play a “truth value” action in {T, F } (read: {true, false}), and her neighbors
are the (at most three) players in Pd corresponding to the disjuncts of Φ where v occurs.
Each “disjunct” player p may play an action in {T, F, w}, and her neighbors are the (at
most three) players corresponding to her variables, plus one player belonging to the set
Pt , as described below. As shown in Figure 7, these disjunct players are the leaves of a
complete binary tree comprising all players in Pt as intermediate vertices, and the player
C as its root. In fact, the “tree” players Pt act as logical-or gates of a circuit. For the sake
of a simpler presentation, for each tree player p, children(p) denotes the set of two players
that are children of p in this tree, while parent(p) denotes her parent. As for disjunct players, the set of available actions for players in Pt is {T, F, w}. Finally, the player C, called
“challenger,” may play actions in {T, w}. As shown in Figure 7, the neighbors of C are the
two top level tree-players.
Let x be a global strategy. The utility functions for the players in GΞ are deﬁned as
follows.
Existential-variables players. For each α ∈ Pe ,
(E-i) uα (x) = 1, no matter of what other players do;
Universal-variables players. For each β ∈ Pu ,
(U-i) uβ (x) = 2, if there exists a (disjunct) neighbor playing w;
(U-ii) uβ (x) = 1 in all other cases.
Disjuncts players. For each d ∈ Pd ,
374

Pure Nash Equilibria: Hard and Easy Games

(D-i) ud (x) = 2 if d and her parent (i.e., a node from Pt ) both play w, and the
disjunct of Φ corresponding to d is made false by the truth-value actions in x of
her variable players, i.e., by the players in Neigh(d) ∩ (Pe ∪ Pu );
(D-ii) ud (x) = 1 if d plays T and the disjunct of Φ corresponding to d is made true
by the truth-value actions in x of her variable players;
(D-iii) ud (x) = 1 if d plays F , the disjunct of Φ corresponding to d is made false
by the truth-value actions of her variable players, and her parent node (a tree
player) does not play w;
(D-iv) ud (x) = 0, in all other cases.
Tree players. For each p ∈ Pt ,
(TREE-i) up (x) = 2 if both p and all of her neighbors play w;
(TREE-ii) up (x) = 1 if p plays T , none of her neighbors plays w, and some player
in children(p) plays T ;
(TREE-iii) up (x) = 1 if p plays F , none of her neighbors plays w, and all players in
children(p) plays F ;
(TREE-iv) up (x) = 1 if p plays an action in {T, F }, and some of her neighbors plays
w, but not all of them;
(TREE-v) up (x) = 0 in all other cases.
Challenger. For player C,
(CHALL-i) uC (x) = 2 if C plays w, and either both of her neighbors play F , or at
least one of them plays w;
(CHALL-ii) uC (x) = 1 if C plays T , some of her neighbors plays T , and none plays
w;
(CHALL-iii) uC (x) = 0 in all other cases.
Intuitively, universal-variable players corresponding to the variables β1 , . . . βq choose
their actions trying to falsify the formula, since their maximum payoﬀ 2 can be obtained
only if Φ is not satisﬁed. The strategies of variable players are suitably “evaluated” by
players in Pd ∪ Pt , and eventually by C.
It is worthwhile noting that GΞ can be built in polynomial time (actually, in LOGSPACE)
from Ξ, and that each player in GΞ may play at most three actions and has a bounded
number of neighbors. More precisely, for the sake of presentation, in the above construction
each player has at most four neighbors. However, we will show later in this section how this
bound can be reduced easily to three.
Let x be a global strategy for GΞ . We denote by σ(x) the truth-value assignment for
Φ determined by the actions chosen by the variable players (i.e., those in Pe ∪ Pu ) in the
strategy x. Moreover, we denote by σe (x) and σu (x) its restriction to the existential and
the universal variables, respectively.
Lemma 3.4 There is a one-to-one correspondence among the satisfying truth-value assignments for Φ and the Nash equilibria of GΞ where no player plays w.
375

Gottlob, Greco, and Scarcello

Proof. Assume Φ is satisﬁable, and let σ be a satisfying truth-value assignment for it.
Consider the following global strategy xσ for GΞ : each variable player in Pe ∪ Pu chooses her
truth-value action according to σ; each disjunct player in Pd plays either T or F , depending
on the logical evaluation of the disjunct associated with her; each tree player p in Pt plays
either T or F , acting as an OR gate having as its input the values played by children(p);
the player C plays T . Note that no player chooses w in xσ .
Figure 7 shows on the right the strategy for GΞ associated with the truth assignment
α1 = α2 = α3 = β1 = T and β2 = β3 = β4 = β5 = F .
Since σ is a satisfying assignment, at least one disjunct of Φ will be evaluated to true
and thus at least one of the tree-player neighbors of C plays T . Therefore, it is easy to
check that, according to the utility functions of GΞ , all players get payoﬀ 1 with respect
to xσ . In particular, this follows from rule (D-ii) or (D-iii) for disjunct players, from rule
(TREE-ii) or (TREE-iii) for tree players, and from rule (CHALL-ii) for the player C.
Moreover, the only rules that may increase some payoﬀ from 1 to 2 are (TREE-i),
(CHALL-i) and (D-i). However, no single player can increase her payoﬀ by changing her
action, because all these rules may be applied only if there is some neighbor that is playing
w, which is not the case in xσ . It follows that the global strategy xσ is a Nash equilibrium
for GΞ .
We next prove the converse, that is, we show that each Nash equilibrium x for GΞ where
no player chooses w corresponds to a satisfying truth-value assignment for Φ. This proof is
based on the following properties of x:
P1 : At least one player in Neigh(C ) does not play F . Otherwise, C would play w in order
to get payoﬀ 2, from rule (CHALL-i), contradicting the hypothesis on x.
P2 : At least one player in Pd plays T . Otherwise, since no player chooses w in x, the only
possible choice for all disjunct players would be F . However, in this case, the best
available choice for all tree-players depending on disjunct players in Pd is to play F ,
according to (TREE-iii). It follows by induction that all players in Pt would play F
and, in particular, all neighbors of C. However, this contradicts property P1 of x.
P3 : Φ is satisfied by the truth-assignment σ(x). Let p ∈ Pd be a disjunct player that plays
T , and whose existence is guaranteed by property P2 . From the hypothesis that no
player chooses w, rule (D-i) is not applicable to p. It follows that the disjunction d
of Φ corresponding to player p is true with respect to σ(x). Otherwise, x would not
be a Nash equilibrium, because p would get payoﬀ 0 from (D-iv) and could improve
it by playing the correct evaluation F , after rule (D-iii).
Therefore, in case no player chooses w, all global strategies that are Nash equilibria
correspond to satisfying assignments for Φ.
2
Lemma 3.5 A Nash equilibrium x for GΞ where no player chooses w is strong if and only
if σe (x) is a witness of validity for Ξ.
Proof. (⇒) Assume x is a strong Nash equilibrium for GΞ where no player chooses w.
Then, σ(x) satisﬁes Φ, from the previous lemma. Assume by contradiction that σe (x) is
not a witness of validity for Ξ. Then, there is an assignment σu for the universal variables
376

Pure Nash Equilibria: Hard and Easy Games

such that Φ is not satisﬁed with respect to σe (x) ∪ σu . Let K be the coalition comprising
all players in GΞ but the existential players in Pe , and let y be the combined strategy for K
such that all universal players in Pu choose their truth-value action according to σu , and all
the other players in K play w. By the choice of σu , all the disjuncts are made false via the
truth-value actions chosen by players in Pu . Then, from rule (D-i), all disjunct players get
payoﬀ 2 according to x−K [y]. Similarly, from (Tree-i) and (Chall-i), all tree-players and the
player C get payoﬀ 2 in x−K [y]. However, this means that all players in the coalition are
improving their payoﬀ, and this contradicts the fact that x is a strong Nash equilibrium.
(⇐) Assume that Ξ is valid and let σ(x) be a satisfying truth-value assignment for Φ
such that σe (x) is a witness of validity for Ξ, for some Nash equilibrium x for GΞ . Indeed,
from Lemma 3.4 we know such an equilibrium (where no player plays w) exists for every
satisfying assignment, and that no player chooses w in x. Moreover, it is easy to check that
all players get payoﬀ 1, according to this global strategy. Assume by contradiction that x is
not a strong Nash equilibrium for GΞ . Then, there is a coalition K and a combined strategy
y for K such that all players in the coalition may improve their payoﬀ in the global strategy
x−K [y], and hence they get payoﬀ 2 in this strategy. Note that no existential player may
belong to K and thus change her action, because there is no way for her to improve her
payoﬀ. Then, from rule (Tree-i), the only way for players in Pt to improve their payoﬀ to 2
is that all of them change their actions to w, because all these rules require that, for each
player, all of her neighbors play w. Thus, if one of them belongs to K, then all of them
belong to this coalition, as well as player C and all the disjunct players in Pd , that are their
neighbors and should change her choices to w in order to get 2, too. On the other hand,
for all disjunct players p in Pd , this improvement to 2 depends also on the variable players
occurring in the disjunct of Φ associated with p (D-i). In particular, besides playing w, this
disjunct should also be made false, because of some change in the choices of the universal
players p depends on. Note that such players may in turn improve their payoﬀ to 2, if p plays
w. It follows any coalition K that shows x is not a Nash equilibrium contains a number of
universal players that are able to let all the disjunct players to be unsatisﬁed and hence to
change their actions to w, thus getting payoﬀ 2. However, the truth-values corresponding
to the actions of players in Pu ∩ K determine an assignment σu that contradicts the validity
of Ξ.
2

Theorem 3.6 Given a game G and a global strategy x, deciding whether x ∈ SNE(G)
(resp., x ∈ PNE(G)) is co-NP-complete. Hardness holds even if the given strategy x is a
pure Nash equilibrium, G is in graphical normal form and has 3-bounded neighborhood, and
the number of actions is fixed.
Proof. Membership. Deciding whether x ∈ PNE(G) is in NP: (i) check in polynomial
time whether x ∈ NE(G); (ii) if this is not the case, guess a global strategy y and check
in polynomial time whether y ∈ NE(G) and y dominates x. Similarly, deciding whether
x ∈ SNE(G) is in NP: (i) check in polynomial time whether x ∈ NE(G); (ii) if this is not
the case, guess a coalition of players K and a combined strategy y for the players in K,
and check in polynomial time whether all the players in K increase their payoﬀ by playing
their actions according to the new strategy y.
377

Gottlob, Greco, and Scarcello

Figure 8: Transformation of a disjunct containing exactly three literals.
Hardness. It is well-known that checking whether a satisﬁable formula Φ in 3DNF is tautologically valid is co-NP complete. We reduce this problem to the problem of checking
whether a given Nash equilibrium is strong (Pareto). Let Φ be a satisﬁable formula in
3DNF. Find a satisfying truth value assignment σ for Φ. This is obviously possible in
polynomial time, given that Φ is satisﬁable and in DNF. Deﬁne Ξ = ∀β1 , . . . βq Φ. Ξ can
be considered a (degenerate) R2QBF without existentially quantiﬁed variables. Let GΞ be
the game associated with Ξ. Obviously, GΞ can be constructed in polynomial time from
Φ. Let x be the Nash equilibrium for GΞ determined by this truth-value assignment where
no player chooses w, as described in Lemma 3.4, and note that even this equilibrium can
be computed in polynomial time from σ. Then, by Lemma 3.5, x is strong if only if Φ is
valid (i.e., the vacuous assignment σe is a witness of validity). This settles the hardness of
checking whether a given Nash Equilibrium is strong. In addition, it is not hard to see that
in the above construction x is a Pareto equilibrium for GΞ if and only if Φ is a tautology.
Indeed, note that, there is a truth-value assignment σ  that falsiﬁes Φ, if and only if there
is a global strategy y where (universal) variable players play according to σ  and all other
players choose w, such that y dominates the Nash equilibrium x. Thus checking whether a
given Nash equilibrium is Pareto is co-NP complete, too.
We conclude the proof by observing that, in our reduction, each player in Pe ∪ Pu ∪ Pt ∪
{C} depends on three other players at most, but some player d in Pd may depend on four
other players, if the corresponding disjunct contains exactly three literals. In this case, we
may introduce an additional player d whose set of actions is {T, F, w} and whose neighbors
are the ﬁrst two literals plus d. Moreover, her utility function is such that d acts as an
AND-gate on the inputs of the literals, preferring w if the two literals are evaluated false, or
if d plays w. In this new encoding, d depends only on d and on the third literal occurring
in the disjunct. As for the basic construction previously deﬁned, her payoﬀ is 2 if she plays
w, her parent plays w, and the third literal is false, or if d plays w. Figure 8 shows this
transformation. Note that this construction preserves all the properties proved so far, and
each player depends on three other players at most.
2
Deciding whether a game has a strong Nash equilibrium turns out to be much more
diﬃcult than deciding the existence of pure (Pareto) Nash equilibria. Indeed, this problem
is located at the second level of the polynomial hierarchy.
Theorem 3.7 Given a game G, deciding whether G has a strong Nash equilibrium is ΣP2 complete. Hardness holds even if G is in graphical normal form and has 3-bounded neighborhood, and the number of actions is fixed.
378

Pure Nash Equilibria: Hard and Easy Games

Proof. Membership. We can decide that SNE(G) = ∅ by guessing a global strategy x and
verifying that x is a strong Nash equilibrium for G. From Theorem 3.6, the latter task is
feasible in co-NP. It follows that the problem belongs to ΣP2 .
Hardness. Let Ξ = ∃α1 , . . . αn ∀β1 , . . . βq Φ be a R2QBF. Recall that deciding the validity
of such formulas is a ΣP2 -complete problem – see the deﬁnition of R2QBF above, in the
current section.
We deﬁne a game GΞ associated with Ξ, obtained from GΞ with the addition of one
more gadget. In GΞ there is a fresh player D depending on the player C only. It is called
“duplicator,” because D gets her maximum payoﬀ if she plays the same action as the
challenger player C. On the other hand, also C depends on this new player D, besides her
other tree-players neighbors (recall the construction shown in Figure 7). Both C and D
play actions in {T, w, u}. Everything else is the same as in GΞ , but for the utility functions
of players C and D:
Challenger. For player C,
(CHALL-i) uC (x) = 2 if C and D play diﬀerent actions in {w, u} and either all of
her tree-players neighbors (i.e., the players in Neigh(C ) − {D }) play F , or at
least one of them plays w;
(CHALL-ii) uC (x) = 1 if C plays T , one player in Neigh(C ) − {D } plays T , and
none plays w;
(CHALL-iii) uC (x) = 0 in all other cases.
Duplicator. For player D,
(DUPL-i) uD (x) = 1 if D plays the same action as C;
(DUPL-ii) uD (x) = 0 in all other cases.
Recall that, in order to maximize their payoﬀs, the universal-variable players try to
make Φ false, and that the strategies of variable players are suitably “evaluated” by players
in Pd ∪ Pt , acting as a Boolean circuit. Moreover, the challenger and the duplicator are
designed in such a way that strategies where some player chooses w, which do not correspond
to satisfying assignments for Φ, cannot lead to Nash equilibria. Formally, for any Nash
equilibrium of GΞ , the following properties hold.
P1 : At least one player in Neigh(C )−{D } does not play F and no player in Neigh(C )−{D }
plays w. Otherwise, C would play an action in {w, u} diﬀerent from the one played
by D in x, in order to get payoﬀ 2, from rule (CHALL-i). However, such a strategy
cannot be an equilibrium, because D could improve her payoﬀ by choosing the same
action as C in x.
P2 : If a player in Pd ∪ Pt plays w then all players in Pd ∪ Pt play w. Indeed, let p be
a player in Pd ∪ Pt playing w and assume by contradiction that there is a player q
in Pd ∪ Pt that does not play w. W.l.o.g., we can assume that q ∈ Neigh(p) and
viceversa. Then, p gets payoﬀ 0, but could increase her payoﬀ by playing an action in
{T, F } according to (D-ii) or (D-iii) for players in Pd , and to (TREE-ii), (TREE-iii)
or (TREE-iv) for players in Pt . This contradicts the fact that x is a Nash equilibrium.
379

Gottlob, Greco, and Scarcello

P3 : No player in Pd ∪ Pt plays w. Otherwise, from P2 , all players in Pd ∪ Pt play w and,
in particular, the players in Neigh(C ) − {D }. However, this is not possible, after
property P1 .
Therefore, rule (Chall-i) is not applicable for C with respect to x. However, from the
above properties at least one of her tree-players neighbors should play T and C can play T
in her turn and get payoﬀ 1, from rule (Chall-ii). Then, D may play T and get payoﬀ 1, as
well. Thus, it is no longer possible to have a Nash equilibrium with some player choosing
the action w. Therefore, after Lemma 3.5 (as the presence of the duplicator does not aﬀect
that proof), we get the following fundamental result: a global strategy x is a strong Nash
equilibrium for GΞ if and only if the truth-value assignment σe (x) is a witness of validity
for Ξ. In particular, SNE(GΞ ) = ∅ if and only if Ξ is valid.
Finally, observe that even the game GΞ can be modiﬁed as shown in Figure 8, in order
to get a 3-bounded neighborhood game.
2
We next show that, if the game is given in general form, the above hardness result holds
even if the structure of player interactions is very simple.
Theorem 3.8 Deciding whether a game G in general form has a strong Nash equilibrium
is ΣP2 -complete, even if both its dependency graph and its associated hypergraph are acyclic,
and the number of actions is fixed.
Proof. The proof of membership in ΣP2 is the same as the membership proof in the previous
theorem. We next prove that this problem is hard for ΣP2 .
Let Ξ = ∃α1 , . . . αn ∀β1 , . . . βq Φ be a quantiﬁed Boolean formula in disjunctive
normal form. We deﬁne a game G¯Ξ with m players α1 , . . . αn , β1 , . . . βq corresponding to
the existentially and universally quantiﬁed variables of Ξ, and two additional players T and
H. The game is based on a combination of the game techniques exploited in the proofs of
Theorem 3.2 and Theorem 3.7. Any player associated with a variable has only two available
actions, t and f , which represent truth assignments to the corresponding variable of Φ; the
actions of player T are s and u, which can be read “satisﬁed” and “unsatisﬁed,” while the
actions of player H are g and b, and can be read “good” and “bad,” respectively.
Given a combined strategy x, we denote by Φ(x) the evaluation of Φ on the truth values
determined by the strategy x. Then, the utility functions are deﬁned as follows. Any player
αi (1 ≤ i ≤ n) gets always payoﬀ 1. Any player βj (1 ≤ j ≤ q) depends on T and gets
payoﬀ 1 if T plays s in x, and payoﬀ 2 if T plays u in x. Player H depends only on T and
gets payoﬀ 1 if x contains either {Ts , Hg } or {Tu , Hb }, and 0, otherwise. Finally, player T
depends on players in {α1 , . . . αn , β1 , . . . βq , H}, and her utility function is deﬁned as follows:
• uT (x) = 2, if Φ(x) is false, T plays u, and H plays g;
• uT (x) = 1, if Φ(x) is true and T plays s, or if Φ(x) is false, T plays s, and H plays b;
• uT (x) = 0, otherwise.
First, observe that both the dependency graph and the dependency hypergraph of G¯Ξ
are acyclic. Moreover, after the proof of Theorem 3.2, it is easy to see that there is a oneto-one correspondence between Nash equilibria of this game and satisfying assignments for
380

Pure Nash Equilibria: Hard and Easy Games

Φ. Then, for any Nash equilibrium x for G¯Ξ , we denote by σ x its corresponding truth-value
assignment, and by σex the restriction of this assignment to the existential variables of Ξ.
Note that, as shown in the above mentioned proof, at any Nash equilibrium, player T plays
s and all players in the game get payoﬀ 1.
We next prove that any witness of validity for Ξ corresponds to a strong Nash equilibrium for G¯Ξ . Let x be a Nash equilibrium and consider a coalition K of players that
deviate from x, leading to a new proﬁle x . From the deﬁnition of the game, the only way
for the coalition to get a payoﬀ higher than 1 is that T changes her choice to u. In this
case, if Φ(x ) is false (and H plays g), T will get payoﬀ 2. Since Φ is true with respect to
x, it follows that some variable players have to change their choices, which means that they
should belong to the coalition and improve their payoﬀs. Therefore, all variable players
in K should correspond to universally quantiﬁed variables, as only these players are able
to improve their payoﬀs from 1 to 2. Thus, such a coalition K exists if and only if the
universally quantiﬁed variables can make the formula false, that is, σex is not a witness of
validity for Ξ. Equivalently, it follows that x is a strong Nash equilibrium if and only if Ξ
is valid.
2
Remark 3.9 Recall that we assumed any general game G to be taken from the class Ck̄ .
It is worthwhile noting that, for games without restriction on player interactions, our
hardness results hold for games where the utility functions are computable in constant time,
too. Namely, consider Theorems 3.1, 3.6, and 3.7. In these constructions, each player has
at most three neighbors and a fixed number of actions. Therefore, the utility function of
each player is computable in constant time.

4. Easy Games
Before we deal with tractable games in graphical normal form (GNF), let us recall that
all computational problems dealt with in this paper are tractable for games in standard
normal form (SNF), even for arbitrary interactions among players. Actually, we next point
out that they can be carried out in logarithmic space and are thus in a very low complexity
class that contains highly parallelizable problems only. This is not very surprising, because
in fact the size of SNF games may be exponentially larger than the size of the same games
encoded in GNF.
Theorem 4.1 Given a game in standard normal form, the following tasks are all feasible
in logarithmic space: Determining the existence of a pure Nash equilibrium, a pure Pareto
equilibrium, or a strong Nash equilibrium, and computing all such equilibria.
Proof. Let P , as usual, denote the set of players. We assume w.l.o.g. that each player has
at least two possible actions (in fact, a player with a single action can be eliminated from
the game by a simple logspace transformation, yielding an equivalent game). The size of
the input matrix is thus at least 2|P | .
Given that all possible global strategies are explicitly represented, each corresponding
to a table cell (which can be indexed in logarithmic space in the size of the input, which
corresponds to polynomial space in |P | × |A|, where P and A are the sets of players and the
381

Gottlob, Greco, and Scarcello

set of all possible actions, respectively), the Nash equilibria are easily identiﬁed by scanning
(i.e., enumerating) all global strategies x keeping a logspace index of x on the worktape,
and checking in logarithmic space (in the size of the input) whether no player can improve
her utility by choosing another action. Given that all Nash Equilibria can be generated in
logarithmic space, they also can be generated in polynomial time.
The Pareto equilibria can be identiﬁed by successively enumerating all Nash equilibria
x, and by an additional loop for each x, enumerating all Nash equilibria y (indexed in
logarithmic space as above) and outputting x if there is no y such that, ∀p ∈ P , up (y) >
up (x). The latter condition can be tested by means of a simple scan of the players.
Strong Nash equilibria can be identiﬁed by enumerating all Nash equilibria and by scanning all possible coalitions of the players (which can be indexed in logarithmic space, as
their number is 2|P | ) in order to discard those equilibria x for which there exists a coalition
K ⊆ P and a combined strategy y for K, such that for each p ∈ K, up (x) < up (x−K [y]).
Finally, note that, for a ﬁxed coalition K, the enumeration of all the combined strategies y
for K can be carried out by means of an additional nested loop, requiring logarithmic space
for indexing each such strategy.
2
4.1 Constraint Satisfaction Problems and Games in Graphical Normal Form
Let us now consider games in GNF. We ﬁrst establish an interesting connection between
constraint satisfaction problems and games. An instance of a constraint satisfaction problem
(CSP) (also constraint network) is a triple I = (Var , U, C), where Var is a ﬁnite set of variables, U is a ﬁnite domain of values, and C = {C1 , C2 , . . . , Cq } is a ﬁnite set of constraints.
Each constraint Ci is a pair (Si , ri ), where Si is a list of variables of length mi called the
constraint scope, and ri is an mi -ary relation over U , called the constraint relation. (The
tuples of ri indicate the allowed combinations of simultaneous values for the variables Si ).
A solution of a CSP instance is a substitution θ : Var −→ U , such that for each 1 ≤ i ≤ q,
Si θ ∈ ri . The problem of deciding whether a CSP instance has any solution is called constraint satisfiability (CS). Since we are interested in CSPs associated with games, where
variables are players of games, we will use interchangeably the terms variable and player,
whenever no confusion arises.
Let G = P, Neigh, A, U  be a game and p ∈ P a player. Deﬁne the Nash constraint
NC (p) = (Sp , rp ) as follows: The scope Sp consists of the players in {p} ∪ Neigh(p), and the
relation rp contains precisely all combined strategies x for {p} ∪ Neigh(p) such that there
is no yp ∈ St(p) such that up (x) < up (x−p [yp ]). Thus note that, for each Nash equilibrium
x of G, x ∩ St(Sp ) is in rp .
The constraint satisfaction problem associated with G, denoted by CSP (G), is the triple
(Var , U, C), where Var = P , the domain U contains all the possible actions of all players,
and C = {NC (p) | p ∈ P }, i.e., it is the set of Nash constraints for the players in G.
Example 4.2 The constraint satisfaction problem associated with FRIENDS game is
({F, G, R, P, M }, {m, o}, C), where the set of constraints contains exactly the following Nash
constraints: NC (F ) = ({F , P , R}, rF ), NC (G) = ({G, P , F }, rG ), NC (R) = ({R, F }, rR ),
NC (P ) = ({P , F }, rP ), and NC (M ) = ({M , R}, rM ), where the constraint scopes are shown
in Figure 9.
2

382

Pure Nash Equilibria: Hard and Easy Games

rF :

F
m
m
o
m
o
o

P
m
m
m
o
o
o

R
m
o
o
m
m
o

rG :

G
m
o
m
o
m
o
m
0

P
m
m
m
m
o
o
o
o

F
m
m
o
o
m
m
o
o

rR :

R
o
m

F
m
o

rM :

P
m
o

rP :

M
m
o

F
m
o

R
m
o

Figure 9: Constraint relations of the game FRIENDS in Example 2.1.
The structure of a constraint satisfaction problem I = (Var , U, C) is represented by the
hypergraph H(I) = (V, H), where V = Var and H = {var (S) | C = (S, r) ∈ C}, and var (S)
denotes the set of variables in the scope S of the constraint C. Therefore, by deﬁnition
of CSP (G), the hypergraph of any game G coincides with the hypergraph of its associated
constraint satisfaction problem, and thus they have the same structural properties.
The following theorem establishes a fundamental relationship between games and CSPs.
Theorem 4.3 A strategy x is a pure Nash equilibrium for a game G if and only if it is a
solution of CSP (G).
Proof. Let x be a Nash equilibrium for G and let p be any player. Then, for each strategy
pa ∈ St(p), up (x) ≥ up (x−p [pa ]). Since up depends only on the players in {p} ∪ Neigh(p),
their combined strategy x ⊆ x is a tuple of NC (p), by construction. It follows that the
substitution assigning to each player p its individual strategy pa ∈ x is a solution of CSP (G).
On the other hand, consider any solution θ of CSP (G), and let p be any player. Let

P = {p} ∪ Neigh(p) and x the combined strategy {θ(q) | q ∈ P  }. Then, x is a tuple
of NC (p), because θ is a solution of CSP (G). Thus, for each p, by deﬁnition of NC (p),
there is no individual strategy for p that can increase her utility, given the strategies of the
other players. It follows that the global strategy containing θ(p) for each player p is a Nash
equilibrium for G.
2
The following theorem states the feasibility of the computation of CSP (G).
Theorem 4.4 Let G be a game having small neighborhood or in graphical normal form.
Then, computing CSP (G) is feasible in polynomial time.
Proof. Let G = P, Neigh, A, U  be a game having small neighborhood. We show that
NC (p) = (Sp , rp ) can be computed in polynomial time. We initialize rp with all the combined strategies for {p} ∪ Neigh(p). The number of these combined strategies is bounded
by
|Neigh (p)| )
≤ 2 i(G)×log(||G||) = ||G||i(G) ,
maxAct (G)|Neigh(p)| = 2 log(maxAct (G)
where the intricacy i (G) of G is given by
maxNeigh(G) × log maxAct(G)
.
log ||G||
383

Gottlob, Greco, and Scarcello

Function NashEvaluation(JT : join tree of H(G)): Boolean
begin
Bottom-Up(JT );
let v = (Sv , rv ) be the root of JT ;
if rv = ∅ then return false;
else
Top-Down(JT );
Select-equilibrium(JT );
output JT ;
return true;
end;
Procedure Bottom-Up(var T : tree)
begin
Done := the set of all the leaves of T ;
while ∃v ∈ T such that
(i) v ∈ Done, and
(ii) {c | c is child of v} ⊆ Done do
for each c = (Sc , rc ) child of v do
rv := rv  rc ;
Done := Done ∪ {v};
end while
end;

Procedure Top-Down(var T : tree)
begin
let v = (Sv , rv ) be the root of T ;
for each c = (Sc , rc ) child of v do
rc := rc  rv ;
let Tc be the subtree of T rooted at c;
Top-Down(Tc );
end for
end;

Figure 10: Evaluation of an acyclic game.
Since G has small neighborhood, i(G) is bounded by some constant, and thus the set of all
combined strategies for p is polynomially bounded. The initialization process computes for
each p all corresponding combined strategies (via a simple enumeration), and thus takes
polynomial time (in the size of G).
Now, for each tuple x in rp we have to check whether it should be kept in rp or not. Let
m = up (x). For each action a ∈ Act(p), compute in polynomial time m = up (x−p [pa ]) and
delete x if m > m. It follows that CSP (G) can be computed in polynomial time from G.
A similar line of reasoning applies if G is in GNF. In this case, the utility functions are
explicitly given in input in a tabular form, and thus the computation of Nash constraints
is yet easier. In fact, this task is feasible in logspace for GNF games.
2
After Theorem 4.3, an acyclic-hypergraph game G having small neighborhood or in
graphical normal form can be solved in polynomial time. Indeed, G and CSP (G) have the
same hypergraph and, as shown by Gottlob et al. (2000), a solution of an acyclic constraint
satisfaction problem can be computed by (a slight adaptation of) the well known Yannakakis’s algorithm for evaluating acyclic conjunctive queries (Yannakakis, 1981), or by the
LOGCFL algorithm proposed by Gottlob et al. (2000), which shows this problem is highly
parallelizable — see Section 6, for more information on the complexity class LOGCFL. For
the sake of completeness, in Figure 10, we report an algorithm for deciding the existence of
a Nash equilibrium and for computing Nash equilibria of acyclic-hypergraph games, based
on these results. We assume the reader is familiar with typical database operations like
semi-joins (for more details, see, e.g., Maier, 1986).
The algorithm takes in input a join tree JT of H(G). With a small abuse of notation,
each vertex of JT , which is formally a hyperedge Hv associated with a player v, is also
used to denote the player herself as well as the Nash constraint (Sv , rv ) associated with v.
384

Pure Nash Equilibria: Hard and Easy Games

Procedure Select-equilibrium(var T : tree)
begin
let v = (Sv , rv ) be the root of T ;
select any combined strategy tv ∈ rv s.t. ∀tv ∈ rv , uv (tv ) ≤ uv (tv );
delete all tuples in rv , but tv ;
for each c = (Sc , rc ) child of v do
rc := rc  rv ;
let Tc be the subtree of T rooted at c;
Select-equilibrium(Tc );
end for
end;

Figure 11: Selection of a (Pareto) Nash equilibrium of an acyclic game.
The algorithm consists of two phases. In the ﬁrst bottom-up phase, the constraint relation
rv of each node v = (Sv , rv ) of JT is ﬁltered by means of a semijoin with the constraint
relation rc (denoted by rv  rc ) of each of her children c in JT . This semijoin eliminates
all tuples from rv corresponding to combined strategies of the players in P  = (Neigh(c) ∪
{c}) ∩ (Neigh(v ) ∪ {v })) that are not available (or no longer available) in rc .
This way, all tuples corresponding to strategies that do not match and hence cannot lead
to Nash equilibria are deleted, starting from the leaves. Finally, either the root is empty and
hence G does not have equilibria, or the tuples remaining in the root p encode the strategies
that p may choose in Nash equilibria of the game. In the top-down phase, this property
of the root is propagated down the tree by taking the semi-join of every vertex with all its
children. At the end, we get a tree such that all tuples encode strategies belonging to Nash
equilibria and, vice versa, all Nash equilibria are made from strategies in the relations stored
in JT . Then, by standard techniques developed for acyclic database queries and CSPs, we
can compute from JT all Nash equilibria of G in a backtrack-free way, and thus in time
polynomial in the combined size of the input game and of the equilibria in output. Note
that this is the best we can do, as a game may have an exponential number of equilibria.
For completeness, Figure 11 shows Procedure Select-equilibrium, that selects from JT
one Nash equilibrium. It is very similar to Procedure Top-Down, but for the selection step
before semi-joins: for any vertex, Select-equilibrium ﬁrst picks a combined strategy tv ∈ rv ,
deletes all other tuples in rv , and then performs the semi-joins with its children and calls
itself recursively, to propagate the choice of tv towards the leaves of the tree. Note that the
selection of tv may be arbitrary, after the previous bottom-up and top-down steps. However,
in Figure 11 we select strategies giving the best payoﬀs, in order to get a Nash equilibrium
that cannot be dominated by any other Nash equilibrium.
Theorem 4.5 Deciding the existence of pure Nash equilibria, as well as computing a Nash
equilibrium is feasible in polynomial time for all classes C of acyclic-hypergraph games such
that every game G ∈ C has small neighborhood or is in graphical normal form.
From the above discussion, it immediately follows that this tractability result can be
extended to the problem of computing a Pareto Nash equilibrium.
Theorem 4.6 Deciding the existence of Pareto Nash equilibria, as well as computing a
Pareto Nash equilibrium for pure strategies is feasible in polynomial time for all classes C
385

Gottlob, Greco, and Scarcello

Function InCoalitionx,J T (Hp : vertex, st: combined strategy): boolean
begin
let (Sp , rp ) be the constraint associated with player p and N + = {p} ∪ Neigh(p);
guess a tuple st ∈ rp such that st matches st on the players they have in common;
if p’s strategy in st is diﬀerent from her strategy in x and up (x−N + [st ]) ≤ up (x) then
return false;
else
let Kp be the set of children of Hp in the join tree JT ;
if Kp = ∅ then
return true;
else

return H  ∈Kp InCoalitionx,J T (Hp ,st );
p

end if
end.

Figure 12: Algorithm for deciding the existence of a coalition improving x.
of acyclic-hypergraph games such that every game G ∈ C has small neighborhood or is in
graphical normal form.
Proof. Recall that Procedure Select-equilibrium in Figure 11, at each vertex v encountered
during the visit of JT , select a combined strategy that guarantees to the player corresponding to v the maximum payoﬀ over the available choices (that, at this point, are all and
only those strategies that may lead to Nash equilibria). In particular, the payoﬀ of the ﬁrst
player to be evaluated, say the root p, cannot be worse than the payoﬀ of p in any other
available strategy. Thus, the tuples left in JT after this procedure encode a Pareto Nash
equilibrium of G, as it cannot be strictly dominated by any other Nash equilibrium.
For the sake of completeness, we point out that, diﬀerently from the previous case of
plain Nash equilibria, from JT we cannot compute easily all Pareto Nash equilibria of the
game in input-output polynomial time.
2
One may thus wonder whether the above result holds for strong Nash equilibria, too.
Unfortunately, we next show that computing a Strong Nash equilibrium is a diﬃcult problem
even in the case of acyclic interactions among players. However, the complexity is reduced
by one level with respect to the arbitrary interaction case, because checking whether a given
equilibrium is strong is feasible in polynomial time in the acyclic case.
Lemma 4.7 Let G be an acyclic-hypergraph game that has small neighborhood or is in
graphical normal form, and let x be a global strategy. Then, deciding whether x ∈ SNE(G)
is feasible in polynomial time.
Proof. Since G has small neighborhood or is in graphical normal form, from Theorem
4.4 we can build in polynomial time the constraints associated with each player. Moreover,
its hypergraph H(G) is acyclic and thus it has a join tree. Let JT be a join tree of H(G).
We show how to use JT for deciding in polynomial time whether the strategy x is not in
SNE(G), i.e., that there exists a coalition C of players getting an incentive to deviate all
together from x. Then, the result follows because PTIME is closed under complementation.
Speciﬁcally, we next show an implementation of this task by an alternating Turing machine
M with a logarithmic-space working tape. Therefore, the problem is in ALOGSPACE,
which is equal to PTIME (Chandra, Kozen, & Stockmeyer, 1981).
386

Pure Nash Equilibria: Hard and Easy Games

The machine M for deciding whether x is not a strong Nash equilibrium works as follows:
• guess a player q;
• guess a strategy stq for q that is diﬀerent from her choice in x;
• root the tree JT at the vertex corresponding to the characteristic edge Hq of player q;
• check that InCoalition x,JT (Hq , stq ) returns true, where InCoalition x,JT is the Boolean
function shown in Figure 12.
Intuitively, the non-deterministic Turing machine ﬁrst chooses a player q belonging to
a possible coalition C disproving x. Thus, q should improve her payoﬀ, and in general –
unless x is not a strong Nash equilibrium, getting this improvement may require that some
of her neighbors Kq deviate from x and hence belong to C. However, in this case, all players
in Kq should be able to improve their payoﬀs. Again, to do that, they can involve other
players in the coalition, and so on.
Whether or not this process is successful is checked by the recursive Boolean function
InCoalition x,JT , which takes in input a vertex Hp of the join tree JT and a combined
strategy st. Recall that each vertex of the join tree is an (hyper)edge of the hypergraph,
corresponding to a player of G. In particular, Hp is the characteristic edge of player p.
InCoalition x,JT has to check whether all players deviating from the given global strategy x
are able to improve their payoﬀs. At the ﬁrst call of this function, the ﬁrst parameter Hq
is the root of JT and identiﬁes the ﬁrst player q chosen for the coalition C. The second
parameter st is the strategy chosen by q, which is diﬀerent from q’s corresponding choice
in x. At a generic recursive call, the ﬁrst parameter Hp identiﬁes a player p to be checked,
and the second parameter st encodes a combined strategy for the player w associated with
the parent Hw of Hp in JT and for w’s neighbors. Now, the function has to check that
either p does not change her choice with respect to x, or she changes and improves her
payoﬀ. To this end, the function guesses a tuple st ∈ rp , where rp is the constraint relation
associated with p. Then, st encodes a combined strategy for p and her neighbors. This
strategy has to match the parameter st on the players they have in common, because st
contains the actions already chosen by the algorithm when the parent Hw of Hp has been
evaluated. Then, if p’s choice in st is diﬀerent from p’s choice in x, it means that p has been
non-deterministically chosen as a member of the coalition C. Thus, p should improve her
payoﬀ, or she immediately causes the fail of this computation branch of the nondeterministic
Turing machine. Otherwise, that is, if p plays the same action as in x, then she does not
belong to C and the function has to check, recursively, that in the rest of the join tree all
deviating players improve their payoﬀs. This is done by propagating the current combined
strategy st to the children of Hp in JT . Observe that this propagation is necessary even if
p does not belong to C, because connected coalitions do not necessarily induce connected
subtrees in JT . Indeed, it may happen that some player z belonging to the coalition is a
neighbor of both p and w, but her characteristic edge Hz occurs far from Hw in the join
tree, possibly in the subtree of JT rooted at p. (For the sake of completeness, note that in
this case z should be a neighbor of all players occurring in the path from Hz to Hw in JT ,
from the connectedness property of join trees.)
387

Gottlob, Greco, and Scarcello

Figure 13: On the left: the dependency graph of the game G(Φs ). On the right: a coalition
witnessing that c5 and c7 are playing in a conﬂicting way.

Finally, let us consider brieﬂy the low-level implementation of the alternating Turing
machine M . Existential states correspond to guesses, while universal states correspond to
the recursive calls to InCoalition x,JT , plus some further machinery for auxiliary computations. At each step, we have to encode on the worktape the two parameters p and st and the
local variables, while x, JT , the game G and the pre-computed constraint relations are on
the input tape. Note that p may be encoded by the logspace pointer to her position in the
input tape, as well as any combined strategy stw may be encoded by a logspace pointer to
its corresponding entry in the constraint relation associated with w. Similar considerations
apply to the other local variables, e.g., the guessed combined strategy st . Moreover, it is
easy to check that all the computations performed by the function are feasible in logspace.
Therefore M is a logspace ATM, and the overall computation is in PTIME. (For a detailed
description of such logspace ATM computations, we refer the interested reader to Gottlob
et al., 2001; Gottlob, Leone, & Scarcello, 2002a).
2

Theorem 4.8 Let G be an acyclic-hypergraph game that has small neighborhood or is in
graphical normal form. Then, deciding whether G has strong Nash equilibria, i.e., SNE(G) =
∅ is NP-complete. Hardness holds even if G is in graphical normal form and has 3-bounded
neighborhood.
Proof. Membership. Given the game G, we can guess a global strategy x and verify in
polynomial time, by Lemma 4.7, that x is in fact a strong Nash equilibrium.
Hardness. The reduction is from SAT. Consider a Boolean formula in conjunctive normal
form Φ = c1 ∧ . . . ∧ cm over variables X1 , . . . , Xn and assume, w.l.o.g, that m = 2 , for some
 > 0. As a running example, consider the formula Φs = (X1 ∨ X2 ) ∧ (X1 ∨ X3 ) ∧ (X1 ∨
¬X4 ∨ X8 ) ∧ (X4 ) ∧ (¬X5 ∨ ¬X6 ) ∧ (X1 ∨ X4 ∨ X6 ) ∧ (X6 ∨ X7 ) ∧ (X8 ).
From Φ, we build the following GNF game G(Φ). The players are partitioned in two
sets Pc and Pt . The set Pc contains exactly one player for each clause of Φ, while players
in Pt are such that G(G) is a complete binary tree, whose leaves are the players in Pc , as
shown in Figure 13 for Φs .
388

Pure Nash Equilibria: Hard and Easy Games

Players in G(Φ) play actions corresponding to variables in Φ plus some further special
actions. Intuitively, each player c ∈ Pc may play a literal occurring in the clause she
represents, while players in Pt have to check that no pair of players ci , cj ∈ Pc plays in
a conﬂicting way, that is, plays complementary literals. To this end, the game rules are
designed in such a way that players in Pt may improve their payoﬀs if they are able to
form a coalition proving that some pair of players are playing complementary literals. It is
worthwhile noting that, in general, this situation cannot be detected by any single player,
since the conﬂicting clauses may be very far from each other. For instance, in Figure 13, c5
and c7 play x¯6 and x6 , respectively, which is detected by a coalition involving their lowest
common ancestor, say p, and the players of Pt occurring in the two paths from c5 and c7 to
p. We show that a global strategy is a strong Nash equilibrium of this game if and only if
there is no such a disproving coalition. Indeed, in this case, there are no conﬂicting clauses
and thus the formula Φ is satisﬁable by setting to true all literals played by the clause
players.
Formally, each player c ∈ Pc may play either a special action B (read: bad) or an action
xi (resp. x̄i ) called literal action, provided that Xi is a variable occurring positively (resp.
negatively) in the corresponding clause of Φ. Each player t ∈ Pt may play an action in
{vi , wi , w̄i | Xi is a variable in Φ} ∪ {T }, where T can be read as “okay with me!”
We next describe the utility functions, given any global strategy x.
A player c ∈ Pc gets payoﬀ 1 if she plays a literal action and her unique neighbor (i.e.,
her parent) plays T , or if she plays B and her neighbor does not play T (C-i); otherwise,
she gets payoﬀ 0 (C-ii).
For a player t ∈ Pt , the utility function ut is such that:
(T-i) ut (x) = 2 if t plays wi , her parent (if any) plays wi or vi , none of her children plays
B, and one of her children plays either wi or xi (depending on whether she is a leaf
or not);
(T-ii) ut (x) = 2 if t plays w̄i , her parent (if any) plays w̄i or vi , none of her children plays
B, and one of her children plays either w̄i or x̄i ;
(T-iii) ut (x) = 2 if t plays vi , her parent (if any) plays T , and her children play either xi
and x̄i or wi and w̄i ;
(T-iv) ut (x) = 1 if t plays T ;
(T-v) ut (x) = 0 in all the other cases.
Then, G(Φ) has the following properties.
P1 : Let x be a global strategy for G(Φ). Then, x is a Nash equilibrium if and only if all
players in Pt play T in x and there is no player in Pc playing B.
If players in Pt play T and players in Pc do not play B, then they get payoﬀ 1 due
to rules (T-iv) and (C-i). In this case, no player has an incentive to deviate, since by
changing strategy she would get payoﬀ 0 due to rules (T-v) and (C-ii).
For the other direction of the proof, let x be a Nash equilibrium and assume, by
contradiction, that there is a player c ∈ Pc choosing B. From (C-i) and (C-ii), it
389

Gottlob, Greco, and Scarcello

follows that some neighbor of c, say t, does not play T . However, this is impossible,
because t would get payoﬀ 0 (from T-v) and could improve her payoﬀ by playing T ,
contradicting the fact that x is a Nash equilibrium. Next, assume there exists a player
in Pt that does not play T , and let t ∈ Pt be a player at the lowest possible level of
the tree satisfying this assumption. It follows that the children of t are clause players,
for otherwise, by the choice of t, both of them should play T , and thus t would get
payoﬀ 0 (T-V) and could improve to 1 by playing T . Therefore, Neigh(t) ∩ Pc = ∅.
Then, the only way for t to get payoﬀ greater than 0 comes from rule (T-iii), which
means that her clause children play in a conﬂicting way, say xi and x̄i . However, since
t does not play T , they both get payoﬀ 0 and thus could deviate from x by playing B
and getting payoﬀ 1. Contradiction.
P2 : Let x be a Nash equilibrium for G(Φ). Then, a coalition of players getting an incentive
to deviate from x exists if and only if there are two clauses playing in x in a conflicting
way.
(If part.) Since x is a Nash equilibrium, from Property P1 , all players in Pt play T and
get payoﬀ 1. If there are two clauses, say c1 and c2 , playing xi and x̄i , respectively,
we may identify an improving coalition as follows: let t be ﬁrst common ancestor of
c1 and c2 , and let P1 and P2 be the sets of vertices (players) occurring in the paths
from t to c1 and c2 , respectively. Then, let t change to vi , and all the players in P1
(resp. P2 ) change to wi (resp. w̄i ) in x – see Figure 13. Then, from the game rules
above, all players in the coalition K = P1 ∪ P2 ∪ {t} get payoﬀ 2, improving the payoﬀ
1 that they get in x.
(Only-if part.) Let K be a coalition of players improving the Nash equilibrium x.
From property P1 , all players in Pt ∩ K should get payoﬀ 2, as they get 1 in x. Let
t ∈ K by the player in Pt at the highest (close to the root) level in the tree, i.e., such
that parent(t), if any, does not belong to K. From (T-iii), the children of t must play
either xi and x̄i or wi and w̄i , depending on whether they are or are not leaves of
the tree. In the former case, we have identiﬁed two conﬂicting players and thus the
property is immediately proved. Hence, let us investigate the latter one. Let t and t
be the children of t playing wi and w̄i , respectively. Since they do not play T , both t
and t belong to K and have to improve their payoﬀ to 2. Therefore, a child of t must
play wi and a child of t must play w̄i , according to (T-i) and (T-ii). Therefore, these
players belong to K, too, and the same happen for some of their children. Eventually,
a leaf descendant of t plays xi and a leaf descendant of t plays x̄i , qed.
The NP hardness of deciding the existence of a SNE follows from the following claim: Φ
is satisﬁable ⇔ G(Φ) admits a strong Nash equilibrium.
(⇒) Assume Φ is satisﬁable and take a satisfying assignment σ. Let xσ be a global
strategy such that: each player c ∈ Pc plays any literal occurring in the clause c that is true
with respect to σ; and each player t ∈ Pt plays T . From P1 , xσ is a Nash equilibrium for
G(Φ). Moreover, by construction no pair of players choose conﬂicting actions in xσ . Hence,
due to P2 , there exists no coalition of players getting an incentive by deviating from xσ ,
and thus xσ is strong. (⇐) Let x be a strong Nash equilibrium for G(Φ). Then, there is
no coalition of players getting an incentive to deviate from x. Due to P1 , no player in Pc
390

Pure Nash Equilibria: Hard and Easy Games

plays B, and due to P2 no pair of players play in a conﬂicting way. Hence, σ x witnesses
that Φ is satisﬁable. More precisely, it encodes an implicant of Φ, that can be extended to
a satisfying assignment choosing any truth value for all Boolean variables occurring in Φ
not chosen by any player in x.
2

5. Further Structurally Tractable Classes of Games
For strategic games, both the acyclic graph and the acyclic hypergraph assumptions are
very severe restrictions, which are rather unlikely to apply in practical contexts. In this
section, we prove that even more general and structurally complicated classes of games
can be dealt with in an eﬃcient way. We consider the notions of treewidth (Robertson &
Seymour, 1986) and hypertree width (Gottlob et al., 2002b), which are the broadest known
generalizations of graph and hypergraph acyclicity, respectively (Gottlob et al., 2000). We
show that tractability results for acyclic games hold for these generalizations, too, and study
the relationship between the two notions.
5.1 Hypertree Decompositions of Games
Let H = (V, E) be a hypergraph. Denote by vert(H) and edges(H) thesets V and E,
respectively. Moreover, for any set of edges E  ⊆ edges(H), let vert(E  ) = h∈E  h.
A hypertree for a hypergraph H is a triple T, χ, λ, where T = (N, E) is a rooted
tree, and χ and λ are labeling functions which associate with each vertex p ∈ N two
sets χ(p)⊆ vert(H) and λ(p) ⊆ edges(H). If T  = (N  , E  ) is a subtree of T , we deﬁne
χ(T  ) = v∈N  χ(v). We denote the root of T by root(T ). Moreover, for any p ∈ N , Tp
denotes the subtree of T rooted at p.
Definition 5.1 (Gottlob et al., 2002b) A hypertree decomposition of a hypergraph H
is a hypertree HD = T, χ, λ for H, where T = (N, E), which satisﬁes all the following
conditions:
1. for each edge h ∈ edges(H), there exists p ∈ N such that vert(h) ⊆ χ(p) (we say that
p covers h);
2. for each vertex Y ∈ vert(H), the set {p ∈ N | Y ∈ χ(p)} induces a (connected) subtree
of T ;
3. for each p ∈ N , χ(p) ⊆ vert(λ(p));
4. for each p ∈ N , vert(λ(p)) ∩ χ(Tp ) ⊆ χ(p).
An edge h ∈ edges(H) is strongly covered in HD if there exists p ∈ N such that vert(h) ⊆
χ(p) and h ∈ λ(p). In this case, we say that p strongly covers h. A hypertree decomposition
HD of hypergraph H is a complete decomposition of H if every edge of H is strongly covered
in HD. The width of a hypertree decomposition T, χ, λ is maxp∈vertices(T ) |λ(p)|. The
hypertree width hw(H) of H is the minimum width over all its hypertree decompositions.
Note that for any constant k checking whether a hypergraph has hypertree-width at
most k is feasible in polynomial time (Gottlob et al., 2002b).
391

Gottlob, Greco, and Scarcello

G

{F,L,G,M,P,R} {HF, HL}

F
P

R

{F,G,P} {HG}

{F,M,P,R} {HM, HR}

L

L

M

{F,P} {HP}

Figure 14: H(FRIENDS’) and a hypertree decomposition for it.
Let k > 0 be a ﬁxed constant. Then, we say that a game G has k-bounded hypertree
width if the hypertree width of its associated hypergraph H(G) is at most k. A hypertree
decomposition of width at most k (if any) can be computed in polynomial time.
Recall that the notion of bounded hypertree-width generalizes the notion of (hypergraph) acyclicity. In particular, the class of acyclic-hypergraph games is precisely the class
of games G whose hypergraph H(G) has hypertree width 1.
Example 5.2 Consider again the game FRIENDS in Example 2.1. Figure 5 shows on the
left its associated hypergraph, and on the right a join tree for it. In fact, this join tree is a
hypertree decomposition of width 1 for the hypergraph, where, for each vertex p, λ(p) is the
set of hyperedges reported in p and χ(p) is the set of players occurring in these hyperedges.
For a more involved example, consider the extension FRIENDS of FRIENDS where a
new player Laura (short: L) joins the group. Laura would like to go with George to the
cinema, and with Pauline and Mary to the opera. Figure 14 shows on the left the hypergraph H(FRIENDS’). This hypergraph is not acyclic, but with a low degree of cyclicity.
Indeed, its hypertree width is 2, as witnessed by the hypertree decomposition of width 2
shown on the right, in Figure 14. Here, for each vertex p of the decomposition tree, the two
sets denote its labels χ(p) and λ(p), respectively.
2
A class of games C is said to have bounded hypertree-width if there is a ﬁnite k such
that, for each game G ∈ C, G has k-bounded hypertree width. We next show that all the
tractability results that hold for acyclic-hypergraph games holds for bounded hypertreewidth games, as well.
Theorem 5.3 Deciding the existence of pure Nash equilibria, as well as computing a Nash
equilibrium is feasible in polynomial time for all classes C of games having bounded hypertreewidth and such that every game G ∈ C has small neighborhood or is in graphical normal
form.
Proof. Let C be a class of games such that each game G ∈ C has hypertree-width at most
k, for some k > 0, and has small neighborhood or is in graphical normal form. Then, we
can build the constraint satisfaction problem CSP (G) in polynomial time, by Theorem 4.4.
392

Pure Nash Equilibria: Hard and Easy Games

Moreover, the hypertree width of G is at most k, and its hypergraph H(G) is the same as
the hypergraph H associated with CSP (G). From results by Gottlob et al. (2001), it follows
that CSP (G) can be solved in polynomial time, which is equivalent to deciding the existence
of Nash equilibria in polynomial time, by Theorem 4.3.
Constructively, we can compute in polynomial time a hypertree decomposition of H having width at most k, exploit this decomposition for building an equivalent acyclic problem
(by putting together the constraints of players occurring in the same vertex of the decomposition tree), and ﬁnally solve this problem by using the algorithm shown in Figure 10. 2
As for acyclic-hypergraph games, this result can be immediately extended to the problem
of computing a Pareto Nash equilibrium.
Corollary 5.4 Deciding the existence of Pareto Nash equilibria, as well as computing a
Pareto Nash equilibrium for pure strategies is feasible in polynomial time for all classes
C of games having bounded hypertree-width and such that every game G ∈ C has small
neighborhood or is in graphical normal form.
5.2 Treewidth and Hypertree Width of Games
We next consider the treewidth of game structures. Recall that any game may be represented either by the primal graph or by the dual graph, as shown in Figure 5 for the game
FRIENDS. Therefore, a ﬁrst question is which graph is better as far as the identiﬁcation of
tractable classes of games is concerned. From the results by Gottlob et al. (2000), we know
that the notion of bounded treewidth for the primal graph is generalized by the notion of
bounded hypertree width, that is, looking at the hypertree width of the game hypergraph
we may identify wider classes of tractable games. Moreover, from the results by Greco and
Scarcello (2003), it follows that looking at the treewidth of the dependency graph is better
than looking at the treewidth of the primal graph.4
We thus know that bounded treewidth for the primal graph is suﬃcient for ensuring
game tractability. However, two questions are still to be answered, and will be the subject
of this section:
1. Do tractability results for bounded treewidth for the primal graph extend to the wider
class of games having bounded treewidth for the dependency graph?
2. What is the relationship between bounded treewidth for the dependency graph and
bounded hypertree width of the game hypergraph?
Definition 5.5 (Robertson & Seymour, 1986) A tree decomposition of a graph G =
(V, E) is a pair T, χ, where T = (N, F ) is a tree, and χ is a labeling function assigning
to each vertex p ∈ N a set of vertices χ(p) ⊆ V , such that the following conditions are
satisﬁed:
(1) for each vertex b of G, there exists p ∈ N such that b ∈ χ(p);
4. In fact, this result is on relationship between primal graph and incidence graph. However, it is easy to
see that, for games, the treewidth of the incidence graph is the same as the treewidth of the dependency
graph.

393

Gottlob, Greco, and Scarcello

G

F
P

{F,L,P,R}

R
{L,P,R,M}

L

{F,G,L,P}

M

Figure 15: G(FRIENDS’) and a tree decomposition for it.
(2) for each edge {b, d} ∈ E, there exists p ∈ N such that {b, d} ⊆ χ(p);
(3) for each vertex b of G, the set {p ∈ N | b ∈ χ(p)} induces a connected subtree of T .
Note that Condition 1 is subsumed by Condition 2 for graphs without isolated vertices.
The width of the tree decomposition T, χ is maxp∈N |χ(p) − 1|. The treewidth of G is the
minimum width over all its tree decompositions. This notion generalizes graph acyclicity,
as the acyclic graphs are precisely those graphs having treewidth 1. 5
Example 5.6 Consider again the game FRIENDS introduced in the Example 5.2. Figure 15 shows on the left the cyclic dependency graph G(FRIENDS’), and on the right a
tree decomposition of width 3 of this graph.
2
Let k > 0 be a ﬁxed integer, and let G be a game. We say that a game G has k-bounded
treewidth if the treewidth of its dependency graph G(G) is at most k. Recall that, given
a graph G, computing a tree-decomposition of width at most k of G (if any) is feasible in
polynomial (actually, linear) time (Bodlaender, 1997).
We next prove an interesting graph-theoretic result to shed some light on the diﬀerent
possible representations of game structures: every class of games having bounded treewidth
has bounded hypertree width, too. We remark that the previous results on the relationship
between treewidth and hypertree width described in the literature (e.g. Gottlob et al.,
2000) cannot be used here. Indeed, they deal with the primal graph and the dual graph
representations, while we are now interested in the dependency graph, which is more eﬀective
than the primal graph, and somehow incomparable with the (optimal) dual graph. A
detailed comparison of these two latter notions is reported by Greco and Scarcello (2003).
Theorem 5.7 For each game G, hypertreewidth (H(G)) ≤ treewidth(G(G)) + 1.
Proof. Let TD be a tree decomposition of G(G) and let k − 1 be its width, that is,
the largest label of the vertices of TD contains k players. Then, we show that there is a
hypertree decomposition of H(G) having width k. Recall that H(G) contains, for each player
5. Observe that the “−1” in the definition of treewidth has been introduced in order to get this correspondence with acyclic graphs, as 2 is the minimum cardinality for the largest label in any tree decomposition.

394

Pure Nash Equilibria: Hard and Easy Games

p, the characteristic edge H(p) = {p} ∪ Neigh(p). Let HD = T, χ, λ be a hypertree such
that:
• the tree T has the same form as the decomposition tree TD, i.e., there is a tree
isomorphism δ : vert(T ) −→ vert(T D) between T and TD;
• for each vertex v ∈ T , λ(v) = {H(p) | p ∈ δ(v)}, i.e., λ(v) contains the characteristic
edge of each player occurring in the vertex of the tree decomposition corresponding
to v;
• χ(v) is the set of all vertices occurring in the edges in λ(v), i.e., contains all players
in δ(v) and their neighbors.
Note that the width of HD is k, as it is determined by the largest λ label, which contains
the same number of elements as the largest label in TD.
We claim that HD is a hypertree decomposition of H(G). Consider the four conditions
in Deﬁnition 5.1: Conditions 3 and 4 are trivially satisﬁed because, for each vertex v,
χ(v) = vert(λ(v)), by construction. Condition 1 is guaranteed by the fact that TD satisﬁes
its corresponding Conditions 1 and 2. We next show that Condition 2, i.e., the connectedness
condition, holds, too.
Let v1 and v2 be two vertices of T such that there exists p ∈ χ(v1 )∩χ(v2 ). Let v1 = δ(v1 )
and v2 = δ(v2 ) be the sets of vertices in the tree decomposition TD corresponding to v1 and
v2 , respectively. Since p ∈ χ(v1 ) and p ∈ χ(v2 ), there are two players p1 and p2 such that (i)
H(p1 ) ∈ λ(v1 ) and p ∈ H(p1 ), and (ii) H(p2 ) ∈ λ(v2 ) and p ∈ H(p2 ). Then, by construction,
p1 ∈ v1 and p2 ∈ v2 (see Figure 16).
We claim that, for each vertex v in the unique path connecting v1 and v2 in T (denoted
by v1  v2 ), λ(v) contains a player from the set {p1 , p2 , p}, which entails that p ∈ χ(v)
and hence that Condition 2 is satisﬁed by HD. This is equivalent to claim, on the tree
decomposition TD, that each vertex v  in the path v1  v2 contains a player in {p1 , p2 , p}.
If both v1 and v2 contain p, then the claim trivially holds because all the vertices in the
path v1  v2 must contain p, from Condition 3 of tree decompositions (the connectedness
condition).
Hence, let us assume that v1 does not contain p. Since p ∈ H(p1 ), this means that p is
a neighbor of p1 and thus there exists a vertex of TD, say v3 = v1 , whose labeling contains
both p and p1 . Assume now that v2 contains p. Figure 16.1 shows the path comprising
vertices v1 , v3 , and v2 — notice that v1 cannot be in the path v3  v2 , otherwise it should
contain p as well. The result follows by observing that, again from Condition 3 of tree
decompositions, all vertices in the path v1  v3 must contain p1 , and all vertices in the
path v3  v2 must contain p. Similarly, assume that v2 does not contain p. In this case,
since p is a neighbor of p2 (recall the above discussion for p1 ), there is a vertex v4 in TD
whose labeling contains both p2 and p. Figure 16.2 shows how these vertices should look
like in the tree decomposition TD. Then, the result follows by observing that all vertices in
the path v1  v3 contains p1 , all vertices in the path v3  v4 contains p, and all vertices
2
in the path v4  v2 contains p2 .
We next show that the converse does not hold, that is, there are classes of games having
bounded hypertree width, but unbounded treewidth. That is, the technique based on the
395

Gottlob, Greco, and Scarcello

Figure 16: Schema of the reduction in the proof of Theorem 5.7.
hypertree width of the game hypergraph is more eﬀective than the corresponding technique
based on the treewidth of the dependency graph, because it allows us to identify strictly
broader classes of tractable games.
Theorem 5.8 There are classes C of games having hypertree width 1 but unbounded
treewidth, i.e., such that, for any finite k > 0, there is a game G ∈ C such that the treewidth
of G is not bounded by k.
Proof. Take the class of all games where every player depends on all other players. For
every such game G, H(G) is acyclic and thus its hypertree width is 1, while G(G) is a clique
containing all players and its treewidth is the number of players minus 1.
2

396

Pure Nash Equilibria: Hard and Easy Games

From Theorem 5.3, Corollary 5.4, and Theorem 5.7, we immediately get the following
tractability results for bounded treewidth games.
Corollary 5.9 Deciding the existence of pure (Pareto) Nash equilibria, as well as computing a pure (Pareto) Nash equilibrium is feasible in polynomial time for all classes C of
games having bounded treewidth and such that every game G ∈ C has small neighborhood or
is in graphical normal form. Moreover, all Nash equilibria of such games can be computed
in time polynomial in the combined size of input and output.

6. Parallel Complexity of Easy Games
In this section, we show that dealing with Nash equilibria for games with good structural
properties is not only tractable but also parallelizable. More precisely, we show that deciding
the existence of Nash equilibria for graphical games where the player interactions has a low
degree of cyclicity is complete for the class LOGCFL. Also, we show that computing such
an equilibrium belongs to the functional version of LOGCFL.
The complexity class LOGCFL consists of all decision problems that are logspace reducible to a context-free language. In order to prove the following theorem, we exploit a
characterization of LOGCFL in terms of circuits.
We recall that a Boolean circuit Gn with n inputs is a ﬁnite directed acyclic graph whose
nodes are called gates and are labeled as follows. Gates of fan-in (indegree) zero are called
circuit input gates and are labeled from the set {false, true, z1 , z2 , . . . , zn , ¬z1 , ¬z2 , . . . , ¬zn }.
All other gates are labeled either AND, OR, or NOT. The fan-in of gates labeled NOT must
be one. The unique node with fan-out (outdegree) zero is called output gate. The evaluation
of Gn on input string w of length n is deﬁned in the standard way. In particular, any input
gate g labeled by zi (resp. ¬zi ) gets value true (resp. false) if the ith bit of w is 1 (resp. 0);
otherwise, g gets value false (resp. true).
A Boolean circuit is thus given as a triple (N, A, label), where N is the set of nodes
(gates), A is the set of arcs, and label is the labeling of the nodes as described.
The depth of a Boolean circuit G is the length of a longest path in G from a circuit
input gate to the output gate of G. The size S(G) of G is the number of gates (including
input-gates) in G.
A family G of Boolean circuits is a sequence (G0 , G1 , G2 , . . .), where the nth circuit Gn
has n inputs. Such a family is logspace-uniform if there exists a logspace Turing machine
which, on the input string containing n bits 1, outputs the circuit Gn . Note that the size
of the nth circuit Gn of a logspace-uniform family G is polynomial in n. Intuitively, this
uniformity condition is crucial in characterizations of low parallel complexity classes in terms
of circuits, because hidden inherent sequentialities in the circuit construction process must
be avoided. In fact, the cicuits which serve as parallel devices for evaluating input strings of
length n, and which must be constructed for each n separately, should be constructible in
parallel themselves. This is assured by requiring logspace uniformity, because LOGSPACE
is a highly parallelizable complexity class contained in LOGCFL.

The language L accepted by a family G of circuits is deﬁned as follows: L = n≥0 Ln ,
where Ln is the set of input strings accepted by the nth member Gn of the family. An input
string w of length n is accepted by the circuit Gn if Gn evaluates to true on input w.
397

Gottlob, Greco, and Scarcello

A family G of Boolean circuits has bounded fan-in if there exists a constant c such that
each gate of each member Gn of G has its fan-in bounded by c.
A family G of Boolean circuits is semi-unbounded if the following two conditions are met:
• All circuits of G involve as non-leaves only AND and OR gates, but no NOT gates
(negation may thus only occur at the circuit input gates); and
• there is a constant c such that each AND gate of any member Gn of G has its fan-in
bounded by c (the OR gates may have unbounded fan-in).
For i ≥ 1, ACi denotes the class of all languages recognized by logspace-uniform families
of Boolean circuits of depth O(logi n).
For i ≥ 1, NCi denotes the class of all languages recognized by logspace-uniform families
of Boolean circuits of depth O(logi n) having bounded fan-in.
For i ≥ 1, SACi denotes the class of all languages recognized by semi-unbounded
logspace-uniform families of Boolean circuits of depth O(logi n).
Venkateswaran (1991) proved the following important relationship between LOGCFL
and the semi-unbounded circuits:
LOGCFL = SAC1 .
Since LOGCFL = SAC1 ⊆ AC1 ⊆ NC2 , the problems in LOGCFL are all highly
parallelizable. In fact, each problem in LOGCFL is solvable in logarithmic time by a
concurrent-read concurrent-write parallel random access machine (CRCW PRAM) with
a polynomial number of processors, or in log2 -time by an exclusive-read exclusive-write
PRAM (EREW PRAM) with a polynomial number of processors (Johnson, 1990).
We next show that the evaluation problem of SAC1 circuits can be transformed in
logspace into the considered Nash equilibrium existence problems.
g19

AND g19
OR g17

OR g18

AND g13 AND g14 AND g15 AND g16
OR g8

g17

g16

g13

g16

OR g9 OR g10 OR g11 OR g12

g8

g9

g11

g12

qx1 g1 x2 g2 qx2 g3 x4 g4 x5 g5 qx6 g6 x7 g7

g1

g3

g6

g6

A)

B)

C)

Figure 17: (A) A normalized circuit, (B)its skeleton tree, (C) a labeling corresponding to a
proof tree.

Theorem 6.1 The existence problem for pure Nash equilibria is LOGCFL-complete for the
following classes of strategic games in graphical normal form: acyclic-graph games, acyclichypergraph games, games of bounded treewidth, and games of bounded hypertree-width.
398

Pure Nash Equilibria: Hard and Easy Games

Proof. It is suﬃcient to show membership for bounded hypertree width (the largest of the
4 classes) and hardness for acyclic-graph games (the smallest one).
Membership. The Nash equilibrium existence problem for NF games of bounded hypertree width is in LOGCFL because, as shown in Section 4, this problem can be transformed
in logspace into a CSP of bounded hypertree width, and, as shown by Gottlob et al. (2001),
checking satisﬁability of the latter is in LOGCFL. (Recall that LOGCFL is closed under
logspace reductions.)
Hardness. We assume that a logspace-uniform family C = {G1 , G2 , . . .} of SAC1 circuits
is given, and we prove that the problem of checking whether a binary string w is accepted
by C can be translated in logspace to an acyclic Nash equilibrium problem in NF.
On input w, compute in logspace the appropriate circuit C = G|w| . As shown by Gottlob
et al. (2001), this circuit can be transformed in logspace into an equivalent normalized circuit
C  which is stratiﬁed and strictly alternating (see Figure 17 (A)), and a tree-shaped proof
skeleton SKEL (see Figure 17 (B)) which encompasses the common structure of all possible
proof trees for (C  , w). A proof tree is a subtree of C  of gates having value 1 witnessing
that C  accepts w. Each proof tree corresponds to an appropriate labeling of SKEL with
gates from C  (for example, the labeling shown in Figure 17 (C)). A labeling is correct if
the root of SKEL is labeled with the output gate of C  , each AND gate labeled g has two
children that are labeled with the input gates to g, each OR node of SKEL labeled g has
one child labeled with some input gate to g, and each leaf of T is labeled with an input gate
to C  whose output to the next higher level is 1. C  accepts w if and only if there exists a
proof tree for (C  , w), and thus if there exists a correct labeling of SKEL.
Build a strategic game G from (C  , w) and SKEL as follows. The set of players consists
of all vertices V of SKEL plus two special players α and β. The possible actions for the
players in V are pairs (g, t), where g is a gate and t is a truth value in {true, false}. The
utilities for players in V are given as follows.
1. The utility of each leaf u of SKEL only depends on its action and is 1 if it plays an
input gate g of C  and if g is associated with the constant true, or g is a ¬ gate and
corresponds to an input bit 0 of the string w, or g is not a ¬ gate and corresponds to
an input bit 1 of w. Otherwise, the utility of u is 0.
2. Each non leaf vertex p ∈ V gets payoﬀ 1, if it plays an action (g, true), and if either
p is an OR vertex and the unique child of p in SKEL takes action (g , true), and g
is a child of the gate g in C  , or p is an AND vertex and the unique children of p
in SKEL take actions (g , true), and (g , true), respectively, where g and g are the
children of the gate g in C  .
3. Each non leaf vertex p ∈ V gets payoﬀ 1, if it plays an action an action (g, false), and
if either p is an OR vertex and the unique child of p in SKEL takes action (g , false),
and g is a child of g in C  , or p is an AND vertex and the unique children of p in
SKEL take actions (g , t ), and (g , t ), respectively, where g and g are the children
of g in C  and t ∧ t = false.
4. In all other cases, all actions of a non leaf vertex p ∈ V have utility −1.
According to what we have deﬁned so far, it is easy to see that every Nash equilibrium
of the game corresponds to a labeling of SKEL by assigning each player of V the gate g
399

Gottlob, Greco, and Scarcello

of its respective action (g, t). In particular, the root node r is forced to be labeled with the
output gate g∗ of C  , and the action played by r is (g∗ , true) if and only if the particular
labeling is a proof tree and (g∗ , false) otherwise.
It remains to deﬁne the actions and utilities for the special players α and β. Their
intuitive role is to “kill” all those equilibria which do not correspond to a proof three i.e.,
those where the root vertex plays (g∗ , false)). The possible actions are {ok, head, tail} for
α, and {head, tail} for β. The strategies where α plays ok have utility 1 for α if the root
vertex r of SKEL plays (g∗ , true)) and utility 0 otherwise. Strategies where α plays head
(resp., tail) have utility 1 for α if r plays (g∗ , false) and if β plays tail (resp., head), and
0 otherwise. Thus, in case r plays (g∗ , false), player α tries to play the opposite of player
β. The strategies where β plays head (resp., tail) have utility 1 for player β if α plays the
same action, and 0 otherwise.
Therefore, in case C  outputs 0 on input w, r plays (g∗ , false), and thus α tries to play
opposite to β while β tries to mimic α. This is a classical non-equilibrium situation. In
summary, each Nash equilibrium of G corresponds to a proof tree for (C  , w). Note also
that G(G) is acyclic, and that the construction of G from (C  , w) can be done in logspace. 2
Note that, by Deﬁnition 2.2, a Pareto Nash equilibrium exists if and only if a Nash
equilibrium exists.
Corollary 6.2 The existence problem for pure Pareto Nash equilibria is LOGCFL-complete
for the following classes of strategic games in graphical normal form: acyclic-graph games,
acyclic-hypergraph games, games of bounded treewidth, and games of bounded hypertreewidth.
Finally, as far as computation of Nash equilibria is concerned, the following corollary
follows from the above result and from a result by Gottlob at al. (2002a), stating that
witnesses (i.e., proof trees) of LOGCFL decision problems can be computed in functional
LOGCFL (i.e., in logspace with an oracle in LOGCFL, or, equivalently, by using SAC1
circuits.
Corollary 6.3 For the classes of games mentioned in Theorem 6.1, the computation of
a single pure Nash equilibria can be done in functional LOGCFL, and is therefore in the
parallel complexity class N C2 .

7. Conclusion
In this paper we have determined the precise complexity of pure Nash equilibria in strategic
games. As depicted in Figure 2, our study proceeded along three directions: representation issues, structural properties of player interactions, and diﬀerent notions of equilibria.
Indeed, besides “plain” Nash equilibria, we considered Pareto and Strong Nash equilibria,
where we look for Nash equilibria that are not dominated by any other Nash equilibrium, or
for proﬁles where no possible coalition of players may improve the payoﬀs of all its members,
respectively.
It turns out that, apart from the simple case of standard normal form, deciding the
existence of Nash equilibria is an intractable problem (unless PTIME = NP), if there is no
400

Pure Nash Equilibria: Hard and Easy Games

restriction on the relationships among players. Interestingly, for Strong Nash Equilibria,
this problem is located at the second level of the polynomial hierarchy, and gives us a fresh
game-theoretic view of the class ΣP2 , as the class of problems whose positive instances are
characterized by a coalition of players who cooperate to provide an equilibrium, and win
against any other disjoint coalition, which fails in trying to improve the utility for all of its
players.
However, this paper is not just a collection of bad news. Rather, a central goal was
to single out large classes of strategic games where detecting Nash equilibria is a tractable
problem. In particular, while early studies in game theory mainly focused on games with
a small number of players (e.g., the traditional two-player framework), we are interested
here in large population games, too. In such cases, adopting the standard normal form is
clearly impractical as, for each player, one should specify her payoﬀs for any combination
of choices of all players in the game. We thus considered a diﬀerent representation for these
games, known in literature as graphical games (Kearns et al., 2001b), where the payoﬀs
of each player p are functions of p’s neighbors only, that is, p’s utility function depends
only on those players p is directly interested in. These relationships among players may
be represented as a graph or, more faithfully, as a hypergraph. We showed that, if utility
functions are represented as tables (graphical normal form) and the game structure is acyclic
or has a low degree of cyclicity (i.e., it has bounded hypertree width), then deciding the
existence of a Nash equilibrium and possibly computing it is feasible in polynomial time.
These results complement those obtained for graphical games in the mixed Nash equilibria
framework (e.g. Kearns et al., 2001b; Kearns & Mansour, 2002). Moreover, in the case of
quasi-acyclic structures, we were also able to extend tractability to classes of games where
utility functions are given implicitly (as in the general form), provided that each player has
a small number of neighbors with not too many available actions.
This paper sheds light on the sources of complexity of ﬁnding pure Nash equilibria
in strategic games, and, in particular, on the roles played by game representations and
game structures. It is worthwhile noting that these aspects of game theory have received
a renewed deal of attention recently. For instance, see Papadimitriou (2004) for a recent
work on the complexity of pure Nash equilibria in some particular classes of games, and
the various contributions on diﬀerent kinds of concise game representations (e.g. Koller &
Milch, 2001; Vickrey, 2002; Kearns et al., 2001b; Leyton-Brown & Tennenholtz, 2003; Gal
& Pfeﬀer, 2004; Kearns & Mansour, 2002).
We recall that a preliminary version of the present work has been presented at the
9th ACM Conference on Theoretical Aspects of Rationality and Knowledge (TARK’03).
Since then, our results have been extended along diﬀerent directions. In particular, Alvarez
et al. (2005) considered a further version of general form games, called games in implicit
form, where also payoﬀ values are given in a succinct way. They showed that, for such
games, the complexity of deciding the existence of pure Nash equilibria increases from the
ﬁrst level to the second level of the polynomial hierarchy. We point out that our general
form is slightly diﬀerent from the general form adopted in the above mentioned paper, and
some confusion may arise by reading their citation of the results presented in our TARK’03
paper (whose full version is the present paper). In their terminology, our Turing-machine
encoding of payoﬀ functions in general form games should be classiﬁed as non-uniform,
with a uniform time-bound. However, apart from such subtle technical issues, some of their
401

Gottlob, Greco, and Scarcello

results on general form games with non implicit actions are very similar to ours, but their
contributions focus on games with a large number of actions, while our hardness results hold
even for games with a ﬁxed number of actions and payoﬀ levels. Moreover, while we show
that hardness holds even for acyclic games, they did not consider any restriction on player
interactions. Observe that their results may be immediately strengthened, given that from
our proofs about GNF games with arbitrary player interactions it follows that NP-hardness
holds even for constant-time utility functions (as discussed in Remark 3.9).
Another line of research studies games where the computation of any Nash equilibrium
is not satisfactory, and one is rather interested in equilibria that satisfy some additional requirements (e.g., the best social welfare). Greco and Scarcello (2004) proved that deciding
the existence of such pure Nash equilibria, called constrained Nash equilibria, is intractable
even for very simple requirements. However, they were also able to identify some restrictions
(for player interactions and requirements) making both the existence and the computation
problems easy. Recent contributions on this subject (on both pure and mixed Nash equilibria) have been done by Schoenebeck at al. (2005) and Greco and Scarcello (2005).
Finally, we observe that there is an interesting connection among strong Nash equilibria
and some equilibria studied in cooperative/coalitional game theory (e.g. Mas-Colell, Whinston, & Green, 1995). In this framework, for each subset K of the players, we are given
the utility that players in K may get, if they cooperate together. The core of a game is
the set of proﬁles x such that there is no subset of players that may improve their utilities by forming their own coalition, deviating from x (Gillies, 1953). Recently, Conitzer
and Sandholm (2003a) proposed a concise representation for coalition utilities, and showed
that determining whether the core of such a game is nonempty is NP-hard. An interesting
future work may concern a detailed study of the complexity of these coalitional games, possibly exploiting suitable notions of quasi-acyclic structures for identifying relevant tractable
classes.

Acknowledgments
Part of this work has been published in preliminary form in the Proceedings of the 9th
ACM Conference on Theoretical Aspects of Rationality and Knowledge (TARK’03).
Georg Gottlob’s work was supported by the Austrian Science Fund (FWF) under
project Nr. P17222-N04 Complementary Approaches to Constraint Satisfaction, and by
the GAMES Network of Excellence of the EU.
We thank the anonymous referees and Tuomas Sandholm for their very useful comments.

References
Alvarez, C., Gabarro, J., & Serna, M. (2005). Pure Nash equilibria in games with a
large number of actions. Electronic Colloquium on Computational Complexity, Report TR05-031.
Aumann, R. (1959). Accetable points in general cooperative n-person games. Contribution
to the Theory of Games, IV.
402

Pure Nash Equilibria: Hard and Easy Games

Aumann, R. (1985). What is game theory trying to accomplish?. Frontiers of Economics,
28–76.
Beeri, C., Fagin, R., Maier, D., & Yannakakis, M. (1983). On the desirability of acyclic
database schemes. Journal of the ACM, 30(3), 479–513.
Bodlaender, H. (1997). Treewidth: Algorithmic techniques and results. In Proc. of the
22nd International Symposium on Mathematical Foundations of Computer Science
(MFCS’97), pp. 19–36, Bratislava, Slovakia.
Chandra, A., Kozen, D., & Stockmeyer, L. (1981). Alternation. Journal of the ACM, 28(1),
114–133.
Conitzer, V., & Sandholm, T. (2003a). Complexity of determining nonemptiness of the
core. In Proc. of the 18th International Joint Conference on Artificial Intelligence
(IJCAI’03), pp. 613–618, Acapulco, Mexico.
Conitzer, V., & Sandholm, T. (2003b). Complexity results about nash equilibria. In Proc.
of the 18th International Joint Conference on Artificial Intelligence (IJCAI’03), pp.
765–771, Acapulco, Mexico.
Deng, X., Papadimitriou, C., & Safra, S. (2002). On the complexity of equilibria. In Proc.
of the 34th Annual ACM Symposium on Theory of Computing (STOC’02), pp. 67–71,
Montreal, Canada.
Downey, R., & Fellows, M. (1995). Fixed-parameter tractability and completeness i: Basic
results. SIAM Journal on Computing, 24(4), 873–921.
Fabrikant, A., Papadimitriou, C., & Talwar, K. (2004). The complexity of pure nash
equilibria. In Proc. of the 36th Annual ACM Symposium on Theory of Computing
(STOC’04), pp. 604–612, Chicago, IL, USA.
Fagin, R. (1983). Degrees of acyclicity for hypergraphs and relational database schemes.
Journal of the ACM, 30(3), 514–550.
Fotakis, D., Kontogiannis, S., Koutsoupias, E., Mavronicolas, M., & Spirakis, P. (2002).
The structure and complexity of nash equilibria for a selﬁsh routing game. In Proc.
of the 29th International Colloquium on Automata, Languages and Programming
(ICALP’02), pp. 123–134, Malaga, Spain.
Gal, Y., & Pfeﬀer, A. (2004). Reasoning about rationality and beliefs. In Proc. of the
3rd International Joint Conference on Autonomous Agents and Multiagent Systems
(AAMAS’04), pp. 774–781, New York, NY, USA.
Garey, M., & Johnson, D. (1979). Computers and Intractability. A Guide to the Theory of
NP-completeness. Freeman and Comp., NY, USA.
Gilboa, I., & Zemel, E. (1989). Nash and correlated equilibria: Some complexity considerations. Games and Economic Behaviour, 1, 80–93.
403

Gottlob, Greco, and Scarcello

Gillies, D. (1953). Some theorems on n-person games. PhD thesis, Princeton, Dept. of
Mathematics.
Gottlob, G., Leone, N., & Scarcello, S. (2000). A comparison of structural csp decomposition
methods. Artificial Intelligence, 124(2), 243–282.
Gottlob, G., Leone, N., & Scarcello, S. (2001). The complexity of acyclic conjunctive queries.
Journal of the ACM, 48(3), 431–498.
Gottlob, G., Leone, N., & Scarcello, S. (2002a). Computing logcﬂ certiﬁcates. Theoretical
Computer Science, 270(1-2), 761–777.
Gottlob, G., Leone, N., & Scarcello, S. (2002b). Hypertree decompositions and tractable
queries. Journal of Computer and System Sciences, 63(3), 579–627.
Greco, G., & Scarcello, S. (2003). Non-binary constraints and optimal dual-graph representations. In Proc. of the 18th International Joint Conference on Artificial Intelligence
(IJCAI’03), pp. 227–232, Acapulco, Mexico.
Greco, G., & Scarcello, S. (2004). Constrained Pure Nash Equilibria in Graphical Games.
In Proc. of the 16th Eureopean Conference on Artificial Intelligence (ECAI’04), pp.
181–185, Valencia, Spain.
Greco, G., & Scarcello, S. (2005). Bounding the Uncertainty of Graphical Games: The
Complexity of Simple Requirements, Pareto and Strong Nash Equilibria. to appear
In Proc. of the 21st Conference in Uncertainty in Artificial Intelligence (UAI’05),
Edinburgh, Scotland.
Johnson, D. (1990). A catalog of complexity classes. Handbook of Theoretical Computer
Science, Volume A: Algorithms and Complexity, 67–161.
Johnson, D., Papadimitriou, C., & Yannakakis, M. (1998). How easy is local search?.
Journal of Computer and System Sciences, 37, 79–100.
Kearns, M., Littman, M., & Singh, S. (2001a). An eﬃcient exact algorithm for singly connected graphical games. In Proc. of the 14th International Conference on Neural Information Processing Systems (NIPS’01), pp. 817–823, Vancouver, British Columbia,
Canada.
Kearns, M., Littman, M., & Singh, S. (2001b). Graphical models for game theory. In Proc.
of the 17th International Conference on Uncertainty in AI (UAI’01), pp. 253–260,
Seattle, Washington, USA.
Kearns, M., & Mansour, Y. (2002). Eﬃcient nash computation in large population games
with bounded inﬂuence. In Proc. of the 18th International Conference on Uncertainty
in AI (UAI’02), pp. 259–266, Edmonton, Alberta, Canada.
Koller, D., & Megiddo, N. (1992). The complexity of two-person zero-sum games in extensive
form. Games and Economic Behavior, 2, 528–552.
404

Pure Nash Equilibria: Hard and Easy Games

Koller, D., & Megiddo, N. (1996). Finding mixed strategies with small supports in extensive
form games. International Journal of Game Theory, 14, 73–92.
Koller, D., Megiddo, N., & von Stengel, B. (1996). Eﬃcient computation of equilibria for
extensive two-person games. Games and Economic Behavior, 14, 220–246.
Koller, D., & Milch, B. (2001). Multi-agent inﬂuence diagrams for representing and solving
games. In Proc. of the 7th International Joint Conference on Artificial Intelligence
(IJCAI’01), pp. 1027–1034, Seattle, Washington, USA.
Leyton-Brown, K., & Tennenholtz, M. (2003). Local-eﬀect games. In Proc. of the 18th
International Joint Conference on Artificial Intelligence (IJCAI’03), pp. 772–780,
Acapulco, Mexico.
Maier, D. (1986). The Theory of Relational Databases, Rochville, Md, Computer Science
Press.
Mas-Colell, A., Whinston, M., & Green, J. (1995). Microeconomic Theor. Oxford University
Press.
Maskin, E. (1985). The theory of implementation in nash equilibrium. Social Goals and
Organization: Essays in memory of Elisha Pazner, 173–204.
McKelvey, R., & McLennan, A. (1996). Computation of equilibria in ﬁnite games. Handbook
of Computational Economics, 87–142.
Megiddo, N., & Papadimitriou, C. (1991). On total functions, existence theorems, and
computational complexity. Theoretical Computer Science, 81(2), 317–324.
Monderer, D., & Shapley, L. (1993). Potential games. Games and Economic Behavior.
Nash, J. (1951). Non-cooperative games. Annals of Mathematics, 54(2), 286–295.
Osborne, M., & Rubinstein, A. (1994). A Course in Game Theory. MIT Press.
Owen, G. (1982). Game Theory. Academic Press, New York.
Papadimitriou, C. (1994a). Computational Complexity. AAddison-Wesley, Reading, Mass.
Papadimitriou, C. (1994b). On the complexity of the parity argument and other ineﬃcient
proofs of existence. Journal of Computer and System Sciences, 48(3), 498–532.
Papadimitriou, C. (2001). Algorithms, games, and the internet. In Proc. of the 28th
International Colloqium on Automata, Languages and Programming (ICALP’01), pp.
1–3, Crete, Greece.
Robertson, N., & Seymour, P. (1986). Graph minors ii. algorithmic aspects of tree width.
Journal of Algorithms, 7, 309–322.
Rosenthal, R. (1973). A class of games possessing pure-strategy nash equilibria. International Journal of Game Theory, 2, 65–67.
405

Gottlob, Greco, and Scarcello

Schoenebeck, G.R., & Vadhan, S.P. (2005). The Computational Complexity of Nash Equilibria in Concisely Represented Games. Electronic Colloquium on Computational Complexity, Report TR05-052.
Stockmeyer, L., & Meyer, A. (1973). Word problems requiring exponential time: Preliminary report. In Proc. of the 5th Annual ACM Symposium on Theory of Computing
(STOC’73), pp. 1–9.
Vardi, M. (2000). Constraint satisfaction and database theory: a tutorial. In Proc. of
the 19th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database
Systems, pp. 76–85, Dallas, Texas, USA.
Venkateswaran, H. (1991). Properties that characterize logcﬂ. Journal of Computer and
System Sciences, 43(2), 380–404.
Vickrey, D. amd Koller, D. (2002). Multi-agent algortihms for solving graphical games. In
Proc. of the 18th National Conference on Artificial Intelligence (AAAI’02), p. 345251
Edmonton, Alberta, Canada.
Yannakakis, M. (1981). Algorithms for acyclic database schemes. In Proc. of the 7th International Conference on Very Large Data Bases (VLDB81), p. 8294 Cannes, France.

406

Journal of Artificial Intelligence Research 24 (2005) 581-621

Submitted 01/05; published 10/05

Macro-FF: Improving AI Planning with Automatically
Learned Macro-Operators
Adi Botea
Markus Enzenberger
Martin Müller
Jonathan Schaeffer

adib@cs.ualberta.ca
emarkus@cs.ualberta.ca
mmueller@cs.ualberta.ca
jonathan@cs.ualberta.ca

Department of Computing Science, University of Alberta
Edmonton, AB Canada T6G 2E8

Abstract
Despite recent progress in AI planning, many benchmarks remain challenging for current planners. In many domains, the performance of a planner can greatly be improved by
discovering and exploiting information about the domain structure that is not explicitly
encoded in the initial PDDL formulation. In this paper we present and compare two automated methods that learn relevant information from previous experience in a domain and
use it to solve new problem instances. Our methods share a common four-step strategy.
First, a domain is analyzed and structural information is extracted, then macro-operators
are generated based on the previously discovered structure. A filtering and ranking procedure selects the most useful macro-operators. Finally, the selected macros are used to
speed up future searches.
We have successfully used such an approach in the fourth international planning competition IPC-4. Our system, Macro-FF, extends Hoffmann’s state-of-the-art planner FF
2.3 with support for two kinds of macro-operators, and with engineering enhancements. We
demonstrate the effectiveness of our ideas on benchmarks from international planning competitions. Our results indicate a large reduction in search effort in those complex domains
where structural information can be inferred.

1. Introduction
AI planning has recently made great advances. The evolution of the international planning
competition over its four editions (Bacchus, 2001; Hoffmann, Edelkamp, Englert, Liporace,
Thiébaux, & Trüg, 2004; Long & Fox, 2003; McDermott, 2000) accurately reflects this.
Successive editions introduced more and more complex and realistic benchmarks, or harder
problem instances in the same domain. The top performers could successfully solve a large
percentage of the problems each time. However, many hard domains, including benchmarks
used in IPC-4, still pose great challenges for current automated planning systems.
The main claim of this paper is that in many domains, the performance of a planner
can be improved by inferring and exploiting information about the domain structure that
is not explicitly encoded in the initial PDDL formulation. The implicit structural information that a domain encodes is, arguably, proportional to how complex the domain is,
and how realistically this models the world. For example, consider driving a truck between
two locations. This operation is composed of many subtasks in the real world. To name
just a few, the truck should be fueled and have a driver assigned. In a detailed planning
c
2005
AI Access Foundation. All rights reserved.

Botea, Enzenberger, Müller, & Schaeffer

Figure 1: CA-ED – Integrating component abstraction and macro-operators into a standard
planning framework.

formulation, we would define several operators such as fuel, assign-driver, and drive.
This representation already contains implicit information about the domain structure. It is
quite obvious for a human that driving a truck between two remote locations would be a
macro-action where we first fuel the truck and assign a driver (with no ordering constraints
between these two actions) and next we apply the drive operator. In a simpler formulation,
we can remove the operators fuel and assign-driver and consider that, in our model, a
truck needs neither fuel nor a driver. Now driving a truck is modeled as a single action,
and the details described above are removed from the model.
In this article we present and evaluate two automated methods that learn such implicit
domain knowledge and use it to simplify planning for new problem instances. The learning
uses several training problems from a domain. Our methods share a common four-step
pattern:
1. Analysis – Extract new information about the domain structure.
2. Generation – Build macro-operators based on the previously acquired information.
3. Filtering – Select the most promising macro-operators.
4. Planning – Use the selected macro-operators to improve planning in future problems.
1.1 Component Abstraction – Enhanced Domain
The first method produces a small set of macro-operators from the PDDL formulations of a
domain and several training problems. The macro-operators are added to the initial domain
formulation, resulting in an enhanced domain expressed in the same description language.
The definitions of the enhanced domain and new problem instances can be given as input
to any planner, with no need to implement additional support for macro-operators (Botea,
Müller, & Schaeffer, 2004b). We call this approach CA-ED for Component Abstraction –
Enhanced Domain.
Figure 1 shows the general architecture of CA-ED. The box Abstraction in the figure
includes steps 1 – 3 above. Step 1 uses component abstraction, a technique that exploits
permanent relationships between low-level features of a problem. Low-level features (i.e.,
constant symbols) linked by static facts (i.e., facts that remain true during planning) form
a more complex unit called an abstract component.
582

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Figure 2: The general architecture of SOL-EP. Enhanced Planner means a planner with
capabilities to handle macros.

At step 2, local analysis of abstract components builds macros that can speed up planning. CA-ED generates macros using a forward search process in the space of macro operators. A macro operator is built as an ordered sequence of operators linked through a
mapping of the operators’ variables. Applying a macro operator is semantically equivalent to applying all contained operators in the given order, respecting the macro’s variable
mapping and the interactions between preconditions and effects.
At step 3 (filtering), a set of heuristic rules is used to prune the search space and generate
only macros that are likely to be useful. Macros are further filtered dynamically, based on
their performance in solving training problems, and only the most effective ones are kept
for future use.
The best macro operators that this method generates are added as new operators to the
initial PDDL domain formulation, enhancing the initial set of operators. Hence, we need
complete macro-operator definitions, including precondition and effect formulas. Expressing these formulas starting from the contained operators is easy in STRIPS, but hard in
larger PDDL subsets such as ADL, where the preconditions and the effects of the contained
operators can interact in complex ways. See Section 3.1 for a detailed explanation. In
CA-ED no work is required to implement step 4, since the planner makes no distinction
between a macro operator and a normal operator. Once the enhanced domain formulation
is available in standard STRIPS, any planner can be used to solve problem instances.
The architecture of CA-ED has two main limitations. First, component abstraction can
currently be applied only to domains with static facts in their formulation. Second, adding
macros to the original domain definition is limited to STRIPS domains.
1.2 Solution – Enhanced Planner
The second abstraction method presented in this article does not suffer from the above
limitations, and is applicable to a larger class of problems. We call this approach SOL-EP,
which stands for Solution – Enhanced Planner. SOL-EP extracts macros from solutions
of training problems and uses them in a planner enhanced with capabilities to handle
macros. The general architecture of this approach is shown in Figure 2. As before, the
module Abstraction implements steps 1 – 3. Instead of using static facts and component
583

Botea, Enzenberger, Müller, & Schaeffer

abstraction as in CA-ED, step 1 in SOL-EP processes the solutions of training problems.
To extend applicability from STRIPS to ADL domains, a different macro representation is
used as compared to CA-ED. A SOL-EP macro is represented as a sequence of operators
and a mapping of the operators’ variables rather than a compilation into a single operator
with complete definition of its precondition and effects. As shown in Section 3.1, for ADL
domains, it is impractical to use macros with complete definition.
For this reason, SOL-EP macros cannot be added to the original domain formulation as
new operators anymore. They are distinct input data for the planner, and for step 4 the
planner is enhanced with code to handle macro operators. Since SOL-EP is more general,
we used this approach in the fourth planning competition IPC-4.
We implemented the ideas presented in this article in Macro-FF, an adaptive planning system developed on top of FF version 2.3 (Hoffmann & Nebel, 2001). FF 2.3 is a
state-of-the-art fully automatic planner that uses a heuristic search approach. The solving
mechanism of FF has two main phases: preprocessing and search. The preprocessing phase
builds data structures needed at search time. All operators are instantiated into ground
actions, and all predicates are instantiated into facts. For each action, pointers are stored
to all precondition facts, all add-effect facts, and all delete-effect facts. Similarly, for each
fact f , pointers are stored to all actions where f is a precondition, to all actions where f
is an add effect, and to all actions where f is a delete effect. This information is instantly
available at run-time, when states are evaluated with the relaxed graphplan heuristic.
Macro-FF adds the ability to automatically learn and use macro-actions, with the goal
of improving search. Macro-FF also includes engineering enhancements that can reduce
space and CPU time requirements that were performance bottlenecks in some of the test
problems. The engineering enhancements affect neither the number of expanded nodes nor
the quality of found plans.
The contributions of this article include a detailed presentation of Macro-FF. We
present and compare two methods that automatically create and use macro-operators in
domain-independent AI planning. Experimental evaluation is focused on several main directions. First, the impact of the engineering enhancements is analyzed. Then we evaluate
how SOL-EP macros implemented in the competition system can improve planning. These
experiments use as testbeds domains used in IPC-4. Finally, we compare the two abstraction methods on test instances where both techniques are applicable, and evaluate them
against planning with no macros.
The rest of the paper is structured as follows: The next two sections describe CA-ED
and SOL-EP respectively. Section 4 summarizes the implementation enhancements that we
added to FF. We present experimental results and evaluate our methods in Section 5. In
Section 6 we briefly review related work and discuss the similarities and differences with
our work. The last section contains conclusions and ideas for future work.

2. Enhancing a Domain with Macros based on Component Abstraction
The first part of this section introduces component abstraction. The topic of the second
part is CA-ED macro-operators.
584

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Figure 3: Static graph of a Rovers problem.

2.1 Component Abstraction
Component abstraction is a technique that groups related low-level constants of a planning
problem into more abstract entities called abstract components or, shorter, components. The
idea is similar to how humans can group features connected through static relationships into
one more abstract unit. For example, a robot that carries a hammer could be considered a
single component, which has mobility as well as maintenance skills. Such a component can
become a permanent object in the representation of the world, provided that no action can
invalidate the static relation between the robot and the hammer.
Component abstraction is a two-step procedure:
1. Build the problem’s static graph, which models permanent relationships between constant symbols of a problem.
2. Build abstract components with a clustering procedure. Formally, an abstract component is a connected subgraph of the static graph.
2.1.1 Building the Static Graph of a Problem
A static graph models static relationships between constant symbols of a problem. Nodes are
constant symbols, and edges correspond to static facts in the problem definition. Following
standard terminology, a fact is an instantiation of a domain predicate, i.e., a predicate
whose parameters have all been instantiated with constant symbols. A fact f is static for
a problem p if f is part of the initial state of p and no operator can delete it.
Each constant that is an argument of at least one static fact defines a node in the static
graph. All constants in a fact are linked pairwise. All edges in the graph are labeled with
the name of the corresponding predicate.
We use a Rovers problem as an example of how component abstraction works. In this
domain, rovers can be equipped with cameras and stores where rock and soil samples can
be collected and analyzed. Rovers have to gather pictures and data about rock and soil
samples, and report them to their base. For more information about the Rovers domain, see
the work of Long and Fox (2003). Figure 3 shows the static graph of the sample problem.
The nodes include two stores (store0 and store1), two rovers (rover0 and rover1),
585

Botea, Enzenberger, Müller, & Schaeffer

two photo cameras (cam0 and cam1), two objectives (obj0 and obj1), two camera modes
(colour and high-res), and four waypoints (point0,... point3). The edges correspond
to the static predicates (store-of ?s - store ?r - rover), (on-board ?c - camera ?r
- rover), (supports ?c - camera ?m - mode), (calibration-target ?c - camera
?o - objective), and (visible-from ?o - objective ?w - waypoint).
The two marked clusters in the left are examples of abstract components generated by
this method. Each component is a rover equipped with a camera and a store. A more
detailed and formal explanation is provided in the following paragraphs.
To identify static facts necessary to build the static graph, the set of domain operators
O is used to partition the predicate set P into two disjoint sets, P = PF ∪PS , corresponding
to fluent and static predicates. An operator o ∈ O is represented as a structure
o = (V (o), P (o), A(o), D(o)),
where V (o) is the variable set, P (o) is the precondition set, A(o) is the set of add effects,
and D(o) is the set of delete effects. A predicate p is fluent if p is part of an operator’s
effects (either positive or negative):
p ∈ PF ⇔ ∃o ∈ O : p ∈ A(o) ∪ D(o).
Otherwise, p is static, denoted by p ∈ PS .
In a domain with hierarchical types, instances of the same predicate can be both static
and fluent. Consider the Depots domain, a combination of Logistics and Blocksworld, which
was used in the third international planning competition (Long & Fox, 2003). This domain
has such a type hierarchy. Type locatable has four atomic sub-types: pallet, hoist,
truck, and crate. Type place has two atomic sub-types: depot and distributor.
Predicate (at ?l - locatable ?p - place), which indicates that object ?l is located at
place ?p, corresponds to eight specialized predicates at the atomic type level. Predicate (at
?p - pallet ?d - depot) is static, since there is no operator that adds, deletes, or moves
a pallet. However, predicate (at ?c - crate ?d - depot) is fluent. For instance, the lift
operator deletes a fact of this type.
To address the issue of hierarchical types, we use a domain formulation where all types
are expressed at the lowest level in the hierarchy. We expand each predicate into a set
of low-level predicates whose arguments have low-level types. Similarly, low-level operators
have variable types from the lowest hierarchy level. Component abstraction and macro
generation are done at the lowest level. After building the macros, we restore the type
hierarchy of the domain. When possible, we replace a set of two or more macro operators
that have low-level types with one equivalent macro operator with hierarchical types. In
this way, macros respect the same definition style (with respect to hierarchical types) as
the rest of the domain operators. For planners that pre-instantiate all operators, such as
FF, the existence of hierarchical types is not relevant. Before searching for a solution, all
operators are instantiated into ground actions whose arguments have low-level types.
Facts corresponding to static predicates are called static facts. In our current implementation we ignore static predicates that are unary 1 or contain two or more variables of
the same type. The latter kind of facts are often used to model topological relationships,
and can lead to large components.
1. In fact, in many current domains, unary static facts have been replaced by types associated with variables.

586

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Step
1
2

3

4

5

Current
Predicate

Used.
Pred.
NO

component0
Consts
Facts
cam0
cam0

component1
Consts
Facts
cam1
cam1

(supports
?c - camera
?m - mode)
(calibr-target
?c - camera
?o - objective)
(on-board
?c - camera
?r - rover)
(store-of
?s - store
?r - rover)

NO

cam0

cam1

YES

cam0
rover0

(on-board
cam0 rover0)

cam1
rover1

(on-board
cam1 rover1)

YES

cam0
rover0
store0

(on-board
cam0 rover0)
(store-of
store0 rover0)

cam1
rover1
store1

(on-board
cam1 rover1)
(store-of
store1 rover1)

Table 1: Building abstract components for the Rovers example.
2.1.2 Building Abstract Components
Abstract components are built as connected subgraphs of the static graph of a problem.
Clustering starts with abstract components of size 1, containing one node each, that are
generated based on a domain type t, called the seed type. For each node with type t
in the static graph, a new abstract component is created. Abstract components are then
iteratively extended with a greedy approach.
Next we detail how the clustering procedure works on the example, and then provide
a more formal description, including pseudo-code. As said before, Figure 3 shows the two
abstract components built by this procedure. The steps of the clustering are summarized
in Table 1, and correspond to the following actions:
1. Choose a seed type (camera in this example), and create one abstract component
for each constant of type camera: component0 contains cam0, and component1
contains cam1. Next, iteratively extend the components created at this step. One
extension step uses a static predicate that has at least one variable type already
encoded into the components.
2. Choose the predicate (supports ?c - camera ?m - mode), which has a variable of
type camera. To avoid ending up with one large component containing the whole
graph, merging two existing components is not allowed. Hence a check is performed
whether the static facts based on this predicate keep the existing components separated. These static facts are (supports cam0 colour), (supports cam0 highres), (supports cam1 colour), and (supports cam1 high-res). The test fails,
since constants colour and high-res would be part of both components. Therefore,
this predicate is not used for component extension (see the third column of Table 1).
3. Similarly, the predicate (calibration-target ?c - camera ?o - objective), which
would add the constant obj1 to both components, is not used for extension.
587

Botea, Enzenberger, Müller, & Schaeffer

4. The predicate (on-board ?c - camera ?r - rover) is tried. It merges no components, so it is used for component extension. The components are expanded as shown
in Table 1, Step 4.
5. The predicate (store-of ?s - store ?r - rover), whose type rover has previously been encoded into the components, is considered. This predicate extends the
components as presented in Table 1, Step 5.
After Step 5 is completed, no further component extension can be performed. There are
no other static predicates using at least one of the component types to be tried for further
extension. At this moment the quality of the decomposition is evaluated. In this example it is satisfactory (see discussion below), and the process terminates. Otherwise, the
decomposition process restarts with another domain type.
The quality of a decomposition is evaluated according to the size of the built components,
where size is defined as the number of low-level types in a component. In our experiments, we
limited size to values between 2 and 4. The lower limit is trivial, since an abstract component
should combine at least two low-level types. The upper limit was set heuristically, to prevent
the abstraction from building just one large component. These relatively small values are
also consistent with the goal of limiting the size and number of macro operators. We discuss
this issue in more detail in Section 2.2.
Figure 4 shows pseudo-code for component abstraction. Types(g) contains all types of
the constant symbols used as nodes in g. Given a type t, Preds(t) is the set of all static
predicates that have a parameter of type t. Given a static predicate p, Types(p) includes
the types of its parameters. Facts(p) are all facts instantiated from p.
Each iteration of the main loop tries to build components starting from a seed type
t ∈ Types(g). The sets Open, Closed, T ried, and AC are initialized to ∅. Each graph node
of type t becomes the seed of an abstract component (method createComponent). The components are greedily extended by adding new facts and constants, such that no constant is
part of any two distinct components. The method predConnectsComponents(p, AC) verifies
if any fact f ∈ Facts(p) merges two distinct abstract components in AC.
Method extendComponents(p, AC) extends the existing components using all static facts
f ∈ Facts(p). For simplicity, assume that a fact f is binary and has constants c1 and c2 as
arguments. Given a component ac, let N odes(ac) be its set of constants (subgraph nodes)
and F acts(ac) its set of static facts (subgraph edges). In the most general case, four possible
relationships can exist between the abstract components and elements f , c1 , and c2 :
1. Both c1 and c2 already belong to the same abstract component ac:
∃(ac ∈ AC) : c1 ∈ Nodes(ac) ∧ c2 ∈ Nodes(ac).
In this case, f is added to ac as a new edge.
2. Constant c1 is already part of an abstract component ac (i.e., c1 ∈ Nodes(ac)) and c2
is not assigned to a component yet. Now ac is extended with c2 as a new node and f
as a new edge between c1 and c2 .
3. If neither c1 nor c2 are part of a previously built component, a new component containing f , c1 and c2 is created and added to AC.
588

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

componentAbstraction(Graph g) {
for (each t ∈ T ypes(g) chosen in random order) {
resetAllStructures();
Open ← t;
for (each ci ∈ N odes(g) with type t)
AC ← createComponent(ci );
while (Open 6= ∅) {
t1 ← Open;
Closed ← t1 ;
for (each p ∈ P reds(t1 ) \ T ried)
T ried ← p;
if ¬(predConnectsComponents(p, AC)) {
extendComponents(p, AC);
for (each t2 ∈ T ypes(p))
if (t2 ∈
/ Open ∪ Closed)
Open ← t2 ;
}
}
if (evaluateDecomposition() = OK)
return AC;
}
return ∅;
}
Figure 4: Component abstraction in pseudo-code.

4. Constants c1 and c2 belong to two distinct abstract components:
∃(ac1 , ac2 ) : c1 ∈ Nodes(ac1 ) ∧ c2 ∈ Nodes(ac2 ) ∧ ac1 6= ac2 .
While possible in general, this last alternative never occurs at the point where the
method extendComponents is called. This is ensured by the previous test with the
method predConnectsComponents.
Consider the case when a static graph has two disconnected (i.e., with no edge between
them) subgraphs sg1 and sg2 such that Types(sg1 ) ∩ Types(sg2 ) = ∅. In such a case, the
algorithm shown in Figure 4 finds abstract components only in the subgraph that contains
the seed type. To perform clustering on the whole graph, the algorithm has to be run on
each subgraph separately.
Following the standard of typed planning domains, abstract components are assigned
abstract types. Figure 5 shows the abstract type assigned to the components of our example.
As shown in this figure, the abstract type of an abstract component is a graph obtained
from the component graph by changing the node labels. The constant symbols used as node
labels have been replaced with their low-level types (e.g., constant cam0 has been replaced
by its type camera).
589

Botea, Enzenberger, Müller, & Schaeffer

Figure 5: Abstract type in Rovers.

Figure 6: Example of a macro in Depots.

Our example also shows that components with identical structure have the same abstract
type. Identical structure is a strong form of graph isomorphism, which preserves the edge
labels as well as the types of constants used as node labels. A fact f = f (c1 , ..., ck ) ∈
F acts(ac) is a predicate whose variables have been instantiated to constants c i ∈ N odes(ac).
Two abstract components ac1 and ac2 have identical structure if:
1. |N odes(ac1 )| = |N odes(ac2 )|; and
2. |F acts(ac1 )| = |F acts(ac2 )|; and
3. there is a bijective mapping p : N odes(ac1 ) → N odes(ac2 ) such that
• ∀c ∈ N odes(ac1 ) : Type(c) = Type(p(c));
• ∀f (c11 , ..., ck1 ) ∈ F acts(ac1 ) : f (p(c11 ), ..., p(ck1 )) ∈ F acts(ac2 );
• ∀f (c12 , ..., ck2 ) ∈ F acts(ac2 ) : f (p−1 (c12 ), ..., p−1 (ck2 )) ∈ F acts(ac1 );
2.2 Creating Macro-Operators
A macro-operator m in CA-ED is formally equivalent to a normal operator: it has a set of
variables V (m), a set of preconditions P (m), a set of add effects A(m), and a set of delete
effects D(m). Figure 6 shows an example of a macro in Depots. Figure 7 shows complete
STRIPS definitions for this macro and the operators that it contains.
Macro operators are obtained in two steps, which are presented in detail in the remaining part of this section. First, an extended set of macros is built and next the macros are
590

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

(:action UNLOAD—DROP
:parameters
(?h - hoist ?c - crate ?t - truck ?p - place ?s - surface)
:precondition
(and (at ?h ?p) (in ?c ?t) (available ?h)
(at ?t ?p) (clear ?s) (at ?s ?p))
:effect
(and (not (in ?c ?t)) (not (clear ?s))
(at ?c ?p) (clear ?c) (on ?c ?s))
)
(:action UNLOAD
:parameters
(?x - hoist ?y - crate ?t - truck ?p - place)
:precondition
(and (in ?y ?t) (available ?x) (at ?t ?p) (at ?x ?p))
:effect
(and (not (in ?y ?t)) (not (available ?x)) (lifting ?x ?y))
)
(:action DROP
:parameters
(?x - hoist ?y - crate ?s - surface ?p - place)
:precondition
(and (lifting ?x ?y) (clear ?s) (at ?s ?p) (at ?x ?p))
:effect
(and (available ?x) (not (lifting ?x ?y)) (at ?y ?p)
(not (clear ?s)) (clear ?y) (on ?y ?s))
)
Figure 7: STRIPS definitions of macro unload—drop and the operators that it contains.

filtered in a quick training process. Since empirical evidence indicates that the extra information added to a domain definition should be quite small, the methods described next
tend to minimize the number of macros and their size, measured by the number of variables,
preconditions and effects. Static macro generation uses many constraints for pruning the
space of macro operators, and discards large macros. Finally, dynamic filtering keeps only
a few top performing macros for solving future problems.
2.2.1 Macro Generation
For an abstract type at, macros are generated by performing a forward search in the space of
macro operators. Macros perform local processing within a component of type at, according
to the locality rule detailed below.
The root state of the search represents an empty macro (i.e., empty sets of operators,
variables, preconditions, and effects). Each search step appends an operator to the current
591

Botea, Enzenberger, Müller, & Schaeffer

void addOperatorToMacro(operator o, macro m, variable-mapping vm) {
for (each precondition p ∈ P (o)) {
if (p ∈
/ A(m) ∪ P (m))
P (m) = P (m) ∪ {p};
}
for (each delete effect d ∈ D(o)) {
if (d ∈ A(m))
A(m) = A(m) − {d};
D(m) = D(m) ∪ {d};
}
for (each add effect a ∈ A(o)) {
if (a ∈ D(m))
D(m) = D(m) − {a};
A(m) = A(m) ∪ {a};
}
}
Figure 8: Adding operators to a macro.

macro, and fixes the variable mapping between the new operator and the macro. Adding
a new operator o to a macro m modifies P (m), A(m), and D(m) as shown in Figure 8.
Even if not explicitely shown in the figure, the variable mapping vm in the procedure is
used to check the identity between operator’s predicates and macro’s predicates (e.g., in
p∈
/ A(m) ∪ P (m)). Two predicates are considered identical if they have the same name and
the same set of parameters. The variable mapping vm tells what variables (parameters) are
common in both the macro and the new operator.
The search is selective: it includes a set of rules for pruning the search tree and for
validating a built macro operator. Validated macros are goal states in this search space.
The search enumerates all valid macro operators. The following pruning rules are used for
static filtering:
• The negated precondition rule prunes operators with a precondition that matches one
of the current delete effects of the macro operator. This rule avoids building incorrect
macros where a predicate should be both true and false.
• The repetition rule prunes operators that generate cycles. A macro containing a cycle
is either useless, producing an empty effect set, or it can be written in a shorter form
by eliminating the cycle. A cycle in a macro is detected when the effects of the first
k1 operators are the same as for the first k2 operators, with k1 < k2 . In particular, if
k1 = 0 then the first k2 operators have no effect.
• The chaining rule requires that for consecutive operators o1 and o2 in a macro, the
preconditions of o2 must include at least one positive effect of o1 . This rule is motivated
by the idea that the action sequence of a macro should have a coherent meaning.
592

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

(:action TAKE-IMAGE
:parameters
(?r - rover ?p - waypoint ?o - objective ?i - camera ?m - mode)
:precondition
(and (calibrated ?i ?r) (on-board ?i ?r) (equipped-for-imaging ?r)
(supports ?i ?m) (visible-from ?o ?p) (at ?r ?p))
:effect
(and (have-image ?r ?o ?m) (not (calibrated ?i ?r)))
)
(:action TAKE-IMAGE—TAKE-IMAGE
:parameters
(?r0 - rover ?p - waypoint ?o - objective ?i0 - camera ?m - mode
?r1 - rover ?i1 - camera)
:precondition
(and (calibrated ?i0 ?r0) (on-board ?i0 ?r0) (equipped-for-imaging ?r0)
(calibrated ?i1 ?r1) (on-board ?i1 ?r1) (equipped-for-imaging ?r1)
(supports ?i0 ?m) (visible-from ?o ?p) (at ?r0 ?p)
(supports ?i1 ?m) (at ?r1 ?p))
:effect
(and (have-image ?r0 ?o ?m) (not (calibrated ?i0 ?r0))
(have-image ?r1 ?o ?m) (not (calibrated ?i1 ?r1)))
)
Figure 9: Operator take-image and macro-operator take-image—take-image in
Rovers. This macro is rejected by the locality rule.

• We limit the size of a macro by imposing a maximal length and a maximal number
of preconditions. Similar constraints could be added for the number of variables or
effects, but we found this unnecessary. Limiting the number of preconditions indirectly
limits the number of variables and effects. Large macros are generally undesirable, as
they can significantly increase the preprocessing costs and the cost per node in the
planner’s search.
• The locality rule is meant to prune macros that change two or more abstract components at the same time. All local static preconditions of an acceptable macro are
part of the same abstract component. Given an abstract type at and a macro m,
let the local static preconditions be the static predicates that are part of both m’s
preconditions and at’s edges. Local static preconditions and their parameters in m’s
definition define a graph structure (different variable bindings for the operators that
compose m can create different graph structures). To implement the idea of locality
we require that this graph is isomorphic with a subgraph of at.
As an example of the locality rule, consider the Rovers abstract type at in Figure 5
and the macro m take-image—take-image shown in Figure 9 (this figure also shows
593

Botea, Enzenberger, Müller, & Schaeffer

Figure 10: Local static preconditions of macro take-image—take-image with respect to
the abstract type in Figure 5. As the picture shows, these correspond to a graph
with 4 nodes and 2 edges.

the definition of the take-image operator). Intuitively, m involves two components, since
two distinct cameras and two distinct rovers are part of the macro’s variables. We show
that this macro is rejected by the locality rule. The graph corresponding to the local static
preconditions of m and at is shown in Figure 10. Obviously, this is not a subgraph of at’s
graph shown in Figure 5, so m is rejected.
2.2.2 Macro Ranking and Filtering
The goal of ranking and filtering is to reduce the number of macros and use only the most
efficient ones for solving problems. The overhead of poor macros can outweight their benefit.
This is known under the name of the utility problem (Minton, 1988). In CA-ED, adding
more operators to a domain increases the preprocessing costs and the cost per node in the
planner’s search.
We used a simple but efficient and practical approach to dynamic macro filtering to
select a small set of macro operators. We count how often a macro operator is instantiated
as an action in the problem solutions found by the planner. The more often a macro has
been used in the past, the greater the chance that the macro will be useful in the future.
For ranking, each macro operator is assigned a weight that estimates its efficiency. All
weights are initialized to 0. Each time a macro is present in a plan, its weight is increased
by the number of occurrences of the macro in the plan (occurrence points), plus 10 bonus
points. No effort was spent on tuning parameters such as the bonus. For common macros
that are part of all solutions of training problems, any bonus value v ≥ 0 will produce the
same ranking among these common macros. No matter what the value v is, each common
macro will receive v × T bonus points, where T is the number of training problems. Hence
the occurrence points decide the relative ranking of common macros.
We use the simplest problems in a domain for training. For these simple problems,
we use all macro operators, giving each macro a chance to participate in a solution plan
and increase its weight. After the training phase, the best macro operators are selected to
become part of the enhanced domain definition. In experiments, 2 macros, each containing
two steps, were added as new operators to the initial sets of 9 operators in Rovers, and
5 operators in Depots and Satellite. In these domains, such a small amount of extra594

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

information was observed to be a good tradeoff between the benefits and the additional
pre-processing and run-time costs. In more difficult domains, possibly with larger initial
sets of operators, using more macros would probably be beneficial.

3. Using Macros from Solutions in an Enhanced Planner
In this section we introduce SOL-EP, the macro system that we used in the fourth international planning competition. We start with a motivation in Section 3.1, and then describe
the method in the following two sections. SOL-EP follows the same four-step pattern as
before, but can be applied to more general classes of problems. Section 3.2 describes steps
1 – 3, and Section 3.3 shows step 4. Section 3.4 concludes this section with a discussion.
3.1 Motivation
SOL-EP was designed with the goal of eliminating the main limitations of CA-ED. Specifically, we wanted to extend the applicability of CA-ED to larger classes of domains. Since
CA-ED generates macros based on component abstraction, its applicability is limited to
domains with static predicates in their definition. SOL-EP generates macros from solutions
of sample problems, with no restrictions caused by the nature of a domain’s predicates.
Furthermore, CA-ED is limited to relatively simple subsets of PDDL such as STRIPS.
Since CA-ED adds macros as new operators to the original domain, complete definitions of
macros, including precondition and effect formulas, are required. These formulas are easy
to obtain in STRIPS, as shown in Figures 7 and 8. However, adding macros to an ADL
domain file becomes unfeasible in practice for two main reasons. First, the precondition
and effect formulas of a macro are hard to infer from the formulas of contained operators.
Second, even if the previous issue is solved and a macro with complete definition is added
to a domain, the costs for pre-instantiating it into ground macro-actions can be large.
To illustrate how challenging the formula inference is in ADL, consider the example
in Figure 11, which shows operator move from the ADL Airport domain used in IPC-4.
The preconditions and the effects of this operator are quite complex formulas that include
quantifiers, implications and conditional effects. Assume we want to compose a macro
that applies two move actions in a row with a given parameter mapping. To achieve a
complete definition of macro move move, its precondition and effect formulas would have
to be automatically composed by analyzing how the preconditions and effects of the two
contained operators interact. We could not find a straight-forward way to generate a macro’s
formulas, so we decided to move towards an alternative solution that is presented later in
this subsection.
Even if the above issue is solved and macros can be added as new domain operators, preinstantiating a macro into ground actions can be costly. Many top-level planners, including
FF, pre-instantiate the domain operators into all possible ground actions that might be
applied in the problem instance at hand. The cost of instantiating one operator is exponential in the total number of parameters and quantifier variables. Macros tend to have larger
numbers of parameters and quantifiers and therefore their instantiation can significantly
increase the total preprocessing costs. ADL Airport is a good illustration of how important
this effect can be. As shown in Section 5.2, the preprocessing is so costly as compared to the
595

Botea, Enzenberger, Müller, & Schaeffer

(:action move
:parameters
(?a - airplane ?t - airplanetype ?d1 - direction ?s1 ?s2 - segment ?d2 - direction)
:precondition
(and (has-type ?a ?t) (is-moving ?a)
(not (= ?s1 ?s2))
(facing ?a ?d1) (can-move ?s1 ?s2 ?d1)
(move-dir ?s1 ?s2 ?d2) (at-segment ?a ?s1)
(not
(exists (?a1 - airplane)
(and (not (= ?a1 ?a)) (blocked ?s2 ?a1))))
(forall (?s - segment)
(imply (and (is-blocked ?s ?t ?s2 ?d2)
(not (= ?s ?s1)))
(not (occupied ?s))))
)
:effect
(and (occupied ?s2) (blocked ?s2 ?a)
(not (occupied ?s1))
(when (not (is-blocked ?s1 ?t ?s2 ?d2))
(not (blocked ?s1 ?a)))
(when (not (= ?d1 ?d2))
(not (facing ?a ?d1)))
(not (at-segment ?a ?s1))
(forall (?s - segment)
(when (is-blocked ?s ?t ?s2 ?d2)
(blocked ?s ?a)))
(forall (?s - segment)
(when (and (is-blocked ?s ?t ?s1 ?d1)
(not (= ?s ?s2))
(not (is-blocked ?s ?t ?s2 ?d2)))
(not (blocked ?s ?a))))
(at-segment ?a ?s2)
(when (not (= ?d1 ?d2))
(facing ?a ?d2))
)
)
Figure 11: Operator move in ADL Airport.

596

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Figure 12: The solution steps of problem 1 in the Satellite benchmark.

main search that it dominates the total cost of solving a problem in this domain. Further
increasing the preprocessing costs with new operators is not desirable in such domains.
Our solution for ADL macros is to represent a SOL-EP macro as a list of atomic actions.
Precondition and effect formulas are not explicitly provided. Rather, they are determined
at run-time, when a macro is dynamically instantiated by applying its action sequence.
The benchmarks used in IPC-4 emphasize the need to address the issues described
above. Many competition domains were provided in both STRIPS and ADL formulations.
The “main” definition was in ADL and, for planners that could not take ADL domains as
input, STRIPS compilations of each ADL domain were provided. We could only run our
system on ADL domains. The reason is that in STRIPS compilations of ADL domains,
a distinct domain file was generated for each problem instance. However, our learning
approach requires several training problems for each domain.
3.2 Generating Macros
As a running example, we will use the solution plan for problem 1 in the Satellite domain
shown in Figure 12. For each step, the figure shows the order in the linear plan, the action
name, the argument list, the preconditions, and the effects. To keep the picture simple,
we ignore static preconditions of actions. Static facts never occur as action effects, and
therefore do not affect the interactions between preconditions and effects of actions.
In SOL-EP, macro-operators are extracted from the solutions of the training problems.
Each training problem is first solved with no macros in use. The found plan can be represented as a solution graph, where each node represents a plan step (action), and edges
model interactions between solution steps. Building the solution graph is step 1 (analysis)
in our general four-step pattern. In IPC-4 we used a first implementation of the solution
graph, that considers interactions only between two consecutive actions of a plan. Here an
interaction is defined if the two actions have at least one common argument, or at least one
597

Botea, Enzenberger, Müller, & Schaeffer

action has no arguments at all. Hence the implementation described in this article extracts
only such two-action sequences as possible macros.
The macro-actions extracted from a solution are translated into macro-operators by
replacing their instantiated arguments with generic variables. This operation preserves
the relative mapping between the arguments of the contained actions. Macro-actions with
different sets of arguments can result in the same macro-operator. For the Satellite solution
in Figure 12, the sequence turn-to followed by take-image occurs three times. After
replacing the constant arguments with generic variables, all occurrences yield the same
macro-operator.
There are many pairs of actions in a solution, and a decision must be made as to
which ones are going to beneficial as macro-operators in a search. Macros are statically
filtered according to the rules of Section 2.2.1 excluding the limitation of the number of
preconditions, which is not critical in this algorithm, and the locality rule. Also, as said
before, we use a different version of the chaining rule. We request that the operators of a
macro have common variables, unless an operator has 0 parameters.
Macro-operators are stored in a global list ordered by their weight, with smaller being
better. Weights are initialized to 1.0 and updated in a dynamic ranking process using a
gradient-descent method.
For each macro-operator m extracted from the solution of a training problem, we resolve the problem with m in use. Let L be the solution length when no macros are used, N
the number of nodes expanded to solve the problem with no macros, and Nm the number
of expanded nodes when macro m is used. Then we use the difference N − Nm to update
wm , the weight of macro m. Since N − Nm can take arbitrarily large values, we map it to
a new value in the interval (−1, 1) by
δm = σ(

N − Nm
)
N

where σ is the sigmoid function
σ(x) =

2
− 1.
1 + e−x

Function σ generates the curve shown in Figure 13. This particular definition of σ was
chosen because it is symmetric in (0, 0) (i.e., σ(x) = −σ(−x)) and bounded within the
interval (−1, 1). In particular, the symmetry property ensures that, if Nm = N , than the
weight update of m at the current training step is 0. The size of the boundary interval has no
effect on the ranking procedure, it only scales all weight updates by a constant multiplicative
factor. We used a sigmoid function bounded to (−1, 1) as a canonical representation, which
limits the absolute value of δm between 0 and 1.
The update formula also contains a factor that measures the difficulty of the training
instance. The harder the problem, the larger the weight update should be. We use as the
difficulty factor the solution length L rather than N , since the former has a smaller variance
over a training problem set. The formula for updating wm is
wm = wm − αδm L
where α is a small constant (0.001 in our implementation). The value of α does not affect
the ranking of macros. It was used only to keep macro weights within the vicinity of 1. See
598

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

1

0

-1
-10

-5

0

5

10

Figure 13: Sigmoid function.

the second part of Section 3.4 for a comparison between CA-ED’s frequency-based ranking
and SOL-EP’s gradient-descent ranking.
In CA-ED, only two macros are kept for future use, given the large extra-costs associated
with this type of macros. In SOL-EP we allow an arbitrary (but still small) number of
macros to be used in search, given the smaller extra-costs involved. SOL-EP macros have
no preprocessing costs, and the cost per node in the search can be much smaller than in
the case of CA-ED macros (see Table 4).
To decide the number of selected macros in a domain, a weight threshold wim is defined. This threshold can be seen as the weight of an imaginary macro im with “constant
performance” in all training instances. By “constant performance” we mean that, for each
training instance,
N − Nim
= c,
N
where c > 0 is a constant parameter. The threshold wim is updated following the same
procedure as for regular macros: The initial value of wim is set to 1. For each training
problem, the weight update of im is
wim = wim − αδim L = wim − ασ(c)L
After all training problems have been processed, macros with a weight smaller than w im
are selected for future use. In experiments we set c to 0.01. Given the competition tight
deadline, we invested limited time in studying this method and tuning its parameters. How
to best determine the number of selected macros is still an open problem for us, which
clearly needs more thourough study and evaluation.
3.3 Using Macros at Run-Time
The purpose of learned macros is to speed up search in new problem instances. A classical
search algorithm expands a node by considering low-level actions that can be applied to the
current state. We add successor states that can be reached by applying the whole sequence
of actions in a macro. We order these macro successors before the regular successors of a
599

Botea, Enzenberger, Müller, & Schaeffer

state. Macros affects neither the completeness nor the correctness of the original algorithm.
The completeness of an original search algorithm is preserved since SOL-EP removes no
regular successors of a state. Correctness is guaranteed by the following way of applying
a macro to a state. Given a state s0 and a sequence of actions m = a1 a2 ...ak (k = 2 in
our competition system), we say that m is applicable to s0 if ai can be applied to si−1 ,
i = 1, ..., k, where si = γ(si−1 , ai ) and γ(s, a) is the state obtained by applying a to s.
When a given state is expanded at runtime, many instantiations of a macro could be applicable but only a few would actually be shortcuts towards a goal state. If all instantiations
are considered, the branching factor can significantly increase and the induced overhead can
be larger than the potential savings achieved by the useful instantiations. Therefore, the
challenge is to select for state expansion only a small number of good macro instantiations.
To determine what a “good” instantiation of a macro is, we use a heuristic method called
helpful macro pruning. Helpful macro pruning is based on the relaxed graphplan computation that FF (Hoffmann & Nebel, 2001) performs for each evaluated state s. Given a
state s, FF solves a relaxed problem, where the initial state is the currently evaluated state,
the goal conditions are the same as in the real problem, and the actions are relaxed by ignoring their delete effects. This computation produces a relaxed plan RP (s). In FF, the
relaxed plan is used to heuristically evaluate problem states and prune low-level actions in
the search space (helpful action pruning).
In addition, we use the relaxed plan to prune the set of macro-instantiations that will
be used for node expansion. Since actions from the relaxed plan are often useful in the real
world, we request that a selected macro and the relaxed plan match i.e., each action of the
macro is part of the relaxed plan.
3.4 Discussion
The first part of this section summarizes properties of CA-ED macros and SOL-EP macros.
Then comments on macro-ranking are provided, including a brief comparison of frequencybased ranking and gradient-descent ranking.
3.4.1 CA-ED Macros vs SOL-EP Macros
When treated as single moves, macro-actions have the potential to influence the planning
process in two important ways. First, macros can change the search space (the embedding
effect), adding to a node successor list states that would normally be achieved in several
steps. Intermediate states in the macro sequence do not have to be evaluated, reducing the
search costs considerably. In effect, the maximal depth of a search could be reduced for the
price of increasing the branching factor.
Second, macros can improve the heuristic evaluation of states (the evaluation effect). As
shown before, FF computes this heuristic by solving a relaxed planning problem (i.e., the
delete effects of actions are ignored) in a graphplan framework. To illustrate the benefits of
macros in relaxed graphplan, consider the example in Figures 6 and 7. Operator unload
has one add effect (lifting) and one delete effect (available) that update the status of
a hoist from available (free) to lifting (busy). Similarly, operator drop updates the hoist
status with two such effects. However, when macro unload—drop is used, the status of
the hoist does not change: it was available (free) before, it will be available after. No effects
600

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

are necessary to express changes in the hoist status. Hence two delete effects (one for each
operator) are safely eliminated from the real problem before relaxation is performed. The
relaxed problem is more similar to the real problem and the information loss is less drastic.
See Section 5.4 for an empirical evaluation of how macros added to a domain affect the
heuristic state evaluation with relaxed graphplan.
When macros can be added to the original domain formulation, both the evaluation
effect and the embedding effect are present, with no need to extend the original planning
engine. The disadvantages of this alternative include the limitation to STRIPS domains
and, often, a significant increase of the preprocessing costs, memory requirements, or cost
per node in the search (as shown in Section 5). When SOL-EP macros are used, each of the
two effects needs a special extension of the planning engine. The current implementation of
the enhanced planner handles the embedding effects but does not affect the computation of
the heuristic state evaluation. Improving the heuristic state evaluation with macros is an
important topic for future work.
3.4.2 Comments on Ranking
The frequency-based ranking method used with CA-ED is simple, fast and was shown to
produce useful macros. Part of its success is due to the combination with static pruning
rules. In particular, limiting macro length to only two actions simplifies the problem of
macro ranking and filtering.
However, in the general case, the savings that a macro can achieve depend not only
on how often it occurs as part of a solution, but also on several other factors, which can
interact. Examples of such factors include the number of search nodes that the application
of a macro would save, and the ratio of useful instantiations of a macro (providing shortcuts
towards a goal state) versus instantiations that guide the search into a wrong direction. See
the work of McCluskey and Porteous (1997) for more details on factors that determine the
performance of macro-operators in AI planning.
The reason why we have extended our approach from frequency-based ranking to gradientdescent ranking is that integrating such factors as above into a ranking method is expected
to produce more accurate results. Compared to frequency-based ranking, gradient-descent
ranking measures the search performance of a macro more directly. To illustrate this,
consider the solution plan in Figure 12. Table 2 shows the 5 distinct macro-operators extracted from this solution plan. For each macro, both the gradient-descent weight and the
frequency-based weight are shown. In the latter case, the bonus points are ignored, since
they do not affect the ranking (all macros will receive the same amount of bonus points
for being part of this solution plan). Each method produces a different ranking. For example, macro take-image turn-to is ranked fourth with the gradient-descent method
and second with the frequency-based method. The reason is that a macro such as turn-to
calibrate (or switch-on turn-to) saves more search nodes than take-image turn-to,
even though it appears less frequently in the solution.
When compared to the simple and fast frequency-based method, gradient-descent ranking is more expensive. Each training problem has to be solved several times; once with no
macros in use and once for each macro. As shown in Table 3 in Section 5, the training
time can become an issue in domains such as PSR and Pipesworld Non-Temporal Tankage.
601

Botea, Enzenberger, Müller, & Schaeffer

Macro
turn-to take-image
turn-to calibrate
switch-on turn-to
take-image turn-to
calibrate turn-to

Weight
0.999103
0.999103
0.999103
0.999401
0.999700

Occurrences
3
1
1
2
1

Table 2: Macros generated in the Satellite example.

Both ranking techniques ignore elements such as the interactions of several macros when
used simultaneously, or the effects of macros on the quality of plans. See Section 5 for an
evaluation of the latter.
Macro ranking is a difficult problem. The training data is often limited. In addition,
factors such as frequency, number of search nodes that a macro would save, effects on
solution quality, etc. have to be combined into a total ordering of a macro set. There is no
clear best solution for this problem. For example, should we select a macro that speeds up
planning but increases the solution length?

4. Implementation Enhancements in Macro-FF
This section describes the implementation enhancements added to FF with the goal of
improving CPU and memory requirements. FF version 2.3 is highly optimized with respect
to relaxed graphplan generation, which was assumed to be the performance bottleneck by
the original system designers. We found that in several domains of the planning competition
this assumption does not hold and the planner spends a significant portion of its time in
other parts of the program. We applied two implementation enhancements to FF to reduce
the CPU time requirements.
Another problem was that the memory requirements for some data structures built
during the pre-processing stage grew exponentially with problem size and therefore did not
scale. We replaced one of these data structures and were able to solve a few more problems
in several domains within the memory limit used in the planning competition.
The enhancements described in this section affect neither the number of expanded nodes
nor the quality of plans found by FF.
4.1 Open Queue
FF tries to find a solution using an enhanced hill climbing method and, if no solution is
found, switches to a best-first search algorithm. Profiling runs showed that in the Pipesworld
domains of the planning competition up to 90% of the CPU time is spent inserting nodes
into the open queue. The open queue was implemented as a single linked list. We changed
the implementation to use a linked list of buckets, one bucket for each heuristic value. The
buckets are implemented as linked lists and need constant time for insertion, since they no
longer have to be sorted.
602

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

4.2 State Hashing
The original FF already used state hashing to help identify previously visited states, with
a full comparison of states in case of a collision. Each fact of a planning problem is assigned
a unique 32-bit random number, and the hash code of a problem state is the sum of all
random numbers associated to the facts that characterize the given state. Profiling runs
showed that in some domains up to 35% of the CPU time is spent in the comparison of
states. These are in particular domains with large states and small graphplan structures
such as PSR and Philosophers. We replaced the original hash key by a 64-bit Zobrist hash
key implementation, a standard technique in game-tree search (Marsland, 1986). Each fact
is assigned 64-bit random number, and the hash key of a state is obtained applying the
XOR operator to the random numbers corresponding to all facts true in the state.
When checking if two states are identical, only their hash codes are compared. If the
hash codes are different, than the states are guaranteed to be different too. If the two
compared states have the same hash code, we assume that the states are identical. This
choice gives up completeness of a search algorithm: two different states with the same hash
code can exist. However, this is so unlikely to occur that fast state comparison based on
64-bit Zobrist hashing is a common standard in high-performance game-playing programs.
The large size of the hash key and the better randomization makes the occurrence of hash
collisions much less probable than random hardware errors.
4.3 Memory Requirements
Some of the optimizations in FF require the creation of large lookup tables built during the
preprocessing stage. One of them is a lookup table storing the facts of the initial state. This
table is sparsely populated but the required memory is equal to the number of constants
to the power of the arity of each predicate summed over all predicates in the domain. This
caused the planner to run out of memory in some large domains given the 1 GB memory
limit used in the planning competition. We replaced the lookup table by a balanced binary
tree with minimal memory requirement and a lookup time proportional to the logarithm of
the number of facts in the initial state.

5. Experimental Results
This section summarizes our experiments and analysis of results. We evaluate our ideas with
several experiments, described in the next subsections. Section 5.1 evaluates the impact of
the implementation enhancements on the planner’s performance. Section 5.2 focuses on the
effect of macro-operators in the system used in the competition. In these two subsections,
the benchmarks that we competed in as part of IPC-4 are used for experimental evaluation: Promela Dining Philosophers – ADL (containing a total of 48 problems), Promela
Optical Telegraph – ADL (48 problems), Satellite – STRIPS (36 problems), PSR Middle
Compiled – ADL (50 problems), Pipesworld Notankage Nontemporal – STRIPS (50 problems), Pipesworld Tankage Nontemporal – STRIPS (50 problems), and Airport – ADL
(50 problems). Macro-FF took the first place in Promela Optical Telegraph, PSR, and
Satellite.
603

Botea, Enzenberger, Müller, & Schaeffer

Section 5.3 compares the two abstraction techniques discussed in this article using
STRIPS domains with static facts. We provide more details later in this section. Section 5.4
contains an empirical analysis of how CA-ED macros affect heuristic state evaluation and
depth of goal states. All experiments reported in this article were run on a AMD Athlon 2
GHz machine, with the limits of 30 minutes and 1 GB of memory for each problem.
5.1 Enhanced FF
The new open queue implementation shows a significant speed-up in the Pipesworld domains. Figure 14 shows the difference in CPU time for the two different Pipesworld domains
(note the logarithmic time scale). The simplest problems at the left of these charts are solved
so quickly that no data bar is drawn. The speedup depends on the problem instance with
maximum gains reaching a factor of 10. As a result two more problems were solved in
the Pipesworld Tankage Non-Temporal domain and one more problem in the Pipesworld
No-Tankage Non-Temporal domain.
The new 64-bit state hashing is especially effective in the PSR and Promela Dining
Philosophers domains. Figure 15 shows a speed-up of up to a factor of 2.5. This resulted in
3 more problems solved in PSR, contributing to the success of Macro-FF in this domain.
The reduced memory requirement is important in Promela Optical Telegraph. Figure 16
shows the memory requirement of the original FF for the initial facts lookup table. As a
result of the replacement of the lookup table, 3 more problems were solved in this domain.
Pipesworld Tankage Non-Temporal - CPU Time (seconds)
1e+04
FF open queue
New open queue
1e+03

Pipesworld No-Tankage Non-Temporal - CPU Time (seconds)
1e+04
FF open queue
New open queue
1e+03

1e+02

1e+02

1e+01

1e+01

1e+00

1e+00

1e-01

1e-01

1e-02

5

10

15

20

25

30

1e-02

35

Problem

5

10

15

20

25

30

35

Problem

Figure 14: Comparison of old and new open queue implementation in Pipesworld Tankage Non-Temporal (left) and Pipesworld No-Tankage Non-Temporal (right). Results are shown for sets of 50 problems in each domain.

5.2 Evaluating Macros in the Competition System
In this subsection we evaluate how SOL-EP macros can improve performance in the competition system. We compare the planner with implementation enhancements against the
planner with both implementation enhancements and SOL-EP macros.
604

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

PSR - CPU Time (seconds)
1e+04
1e+03

Philosophers - CPU Time (seconds)
1e+04

FF hashing
New hashing

1e+02

1e+02

1e+01

1e+01

1e+00

1e+00

1e-01

1e-01

1e-02

5

10

15

FF hashing
New hashing

1e+03

20

25

30

35

40

1e-02

45

2

4

6

Problem

8

10

12

Problem

Figure 15: Comparison of the two implementations of state hashing in PSR (left) and
Promela Dining Philosophers (right). Results are shown for 50 problems in
PSR and 48 problems in Promela Dining Philosophers.

Optical
1e+11

Old
New

1e+10

Bytes

1e+09
1e+08
1e+07
1e+06
100000

0

5

10

15

20

25

30

35

40

45

50

Problem

Figure 16: Size of the data structures for initial facts in the old implementation (lookup table) and the new implementation (balanced tree) in Promela Optical Telegraph.

For each of the seven test domains, we show the number of expanded nodes and the
total CPU time, again, on a logarithmic scale. A CPU time chart shows no distinction
between a problem solved very quickly (within a time close to 0) and a problem that could
not be solved. To determine what the case is, check the corresponding node chart, where
the absence of a data point always means no solution.
Figure 17 summarizes the results in Satellite, Promela Optical Telegraph, and Promela
Dining Philosophers. In Satellite and Promela Optical Telegraph, macros greatly improve
performance over the whole problem sets, allowing Macro-FF to win these domain formu605

Botea, Enzenberger, Müller, & Schaeffer

lations in the competition. In Promela Optical Telegraph macros led to solving 12 additional
problems. The savings in Promela Dining Philosophers are limited, resulting in one more
problem solved.
Figure 18 shows the results in the ADL version of Airport. The savings in terms of
expanded nodes are significant, but they have little effect on the total running time. In this
domain, the preprocessing costs dominate the total running time.
The complexity of preprocessing in Airport also limits the number of solved problems
to 21. The planner can solve more problems when the STRIPS version of Airport is used,
but no macros could be generated for this domain version. STRIPS Airport contains one
domain definition for each problem instance, whereas our learning method requires several
training problems for a domain definition.
Figure 19 contains the results in Pipesworld Non-Temporal No-Tankage, Pipesworld
Non-Temporal Tankage, and PSR. In Pipesworld Non-Temporal No-Tankage, macros often
lead to significant speed-up. As a result, the system solves four new problems. On the other
hand, the system with macros fails in three previously solved problems. The contribution
of macros is less significant in Pipesworld Non-Temporal Tankage. The system with macros
solves two new problems and fails in one previously solved instance. Out of all seven
benchmarks, PSR is the domain where macros have the smallest impact. Both systems
solve 29 problems using similar amounts of resources. In the competition official run,
Macro-FF solved 32 problems in this domain formulation.
Table 3 shows the number of training problems, the total training time, and the selected
macros in each domain. The training phase uses 10 problems for each of Airport, Satellite,
Pipesworld Non-Temporal No-Tankage, and PSR. We reduced the training set to 5 problems
for Promela Optical Telegraph, 6 problems for Promela Dining Philosophers, and 5 problems
for Pipesworld Non-Temporal Tankage. In Promela Optical Telegraph, the planner with
no macros solves 13 problems, and using most of them for training would leave little room
for evaluating the learned macros. The situation is similar in Promela Dining Philosophers;
the planner with no macros solves 12 problems. In Pipesworld Non-Temporal Tankage,
the smaller number of training problems is caused by both the long training time and the
structure of the competition problem set. The first 10 problems use only a part of the
domain operators, so we did not include these into the training set. Out of the remaining
problems, the planner with no macros solves 11 instances. The large training times in
Pipesworld Non-Temporal Tankage and PSR are caused by the increased difficulty of the
training problems.
5.3 Evaluating our Abstraction Techniques
To evaluate the performance of our two abstraction methods, we compare four setups of
Macro-FF. In all four setups, the planner includes the implementation enhancements
described in Section 4. Setup 1 is the planner with no macros. Setup 2 includes CA-ED,
the method described in Section 2. Setup 3 uses SOL-EP, the method described in Section
3. Setup 4 is a combination of 2 and 3. Since both methods have benefits and limitations,
it is interesting to analyze how they perform when applied together. In setup 4, we first
run CA-ED, obtaining an enhanced domain. Next we treat this as a regular domain, and
606

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Domain
Airport

TP
10

TT
365

Promela
Optical
Telegraph

5

70

Promela
Dining
Philosophers
Satellite

6

10

10

8

Pipesworld
Non-Temporal
No-Tankage
Pipesworld
Non-Temporal
Tankage

10

250

5

4,206

PSR

10

1,592

Macros
MOVE MOVE
PUSHBACK MOVE
PUSHBACK PUSHBACK
MOVE TAKEOFF
QUEUE-WRITE ADVANCE-EMPTY-QUEUE-TAIL
ACTIVATE-TRANS QUEUE-WRITE
ACTIVATE-TRANS ACTIVATE-TRANS
PERFORM-TRANS ACTIVATE-TRANS
ACTIVATE-TRANS QUEUE-READ
ACTIVATE-TRANS ACTIVATE-TRANS
QUEUE-READ ADVANCE-QUEUE-HEAD
TURN-TO SWITCH-ON
SWITCH-ON TURN-TO
SWITCH-ON CALIBRATE
TURN-TO TAKE-IMAGE
TURN-TO CALIBRATE
TAKE-IMAGE TURN-TO
POP-START POP-END
PUSH-START PUSH-END
PUSH-START POP-START
PUSH-START PUSH-END
PUSH-START POP-END
PUSH-END POP-START
POP-END PUSH-START
PUSH-END PUSH-START
PUSH-START POP-START
POP-START PUSH-START
AXIOM AXIOM
CLOSE AXIOM

Table 3: Summary of training in each domain. TP is the number of training problems
and TT is the total training time in seconds. The last column shows the macros
selected for each domain. For simplicity, we do not show the variable mapping of
each macro.

607

Botea, Enzenberger, Müller, & Schaeffer

Satellite - Nodes
1e+08
1e+07

Satellite - CPU Time (seconds)
1e+04

FF enhanced
With macros

1e+03

1e+06

1e+02

1e+05
1e+04

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
1e+00

FF enhanced
With macros

5

10

15

20

25

30

1e-02

35

5

10

15

Problem
Optical - Nodes
1e+08
1e+07

FF enhanced
With macros

1e+03

1e+04

35

FF enhanced
With macros

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
1e+00

5

10

15
Problem

20

1e-02

25

5

Philosophers - Nodes

10

15
Problem

20

25

Philosophers - CPU Time (seconds)
1e+04

FF enhanced
With macros

1e+03

1e+06

FF enhanced
With macros

1e+02

1e+05
1e+04

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
1e+00

30

1e+02

1e+05

1e+07

25

Optical - CPU Time (seconds)
1e+04

1e+06

1e+08

20
Problem

2

4

6

8
Problem

10

12

1e-02

14

2

4

6

8
Problem

10

12

14

Figure 17: Comparison of our enhanced version of FF with and without macros in Satellite
(36 problems), Promela Optical Telegraph (48 problems) and Promela Dining
Philosophers (48 problems).

apply SOL-EP to generate a list of SOL-EP macros. Finally, the enhanced planner uses as
input the enhanced domain, the list of SOL-EP macros, and regular problem instances.
Since component abstraction can currently be applied only to STRIPS domains with
static facts in their formulation, we used as testbeds domains that satisfy these constraints.
608

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Airport - Nodes
1e+08
1e+07

Airport - CPU Time (seconds)
1e+04

FF enhanced
With macros

1e+03

1e+06

1e+02

1e+05
1e+04

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
1e+00

FF enhanced
With macros

5

10

15

1e-02

20

Problem

5

10

15

20

Problem

Figure 18: Comparison of our enhanced version of FF with and without macros in Airport
(50 problems in total).

We ran this experiment on Rovers (20 problems), Depots (22 problems), and Satellite (36
problems). These domains were used in the third international planning competition IPC-3,
and Satellite was re-used in IPC-4 with an extended problem set. In our experiments, the
Rovers and Depots problems sets are the same as in IPC-3, and the Satellite problem set
is the same as in IPC-4.
Figures 20 – 23 and Table 4 summarize our results. The performance consistently
improves when macros are used. Interestingly, combining CA-ED and SOL-EP often leads
to better performance than each abstraction method taken separately. In Rovers, all three
abstraction setups produce quite similar results, with a slight plus for the combined setup. In
Depots, CA-ED is more effective than SOL-EP in terms of expanded nodes. The differences
in CPU time become smaller, since adding new operators to the original domain significantly
increases the cost per node in Depots (see the discussion below). Again, the overall winner
in this domain is the combined setup. In Satellite, adding macros to the domain reduces
the number of expanded nodes, but has significant impact in cost per node (see Table 4
later in this section) and memory requirements. Setups 2 and 4, which add macros to the
original domain, fail to solve three problems (32, 33, and 36) because of the large memory
requirements in FF’s preprocessing phase.
Table 4 evaluates how macros can affect the cost per node in the search. The cost per
node is defined as the total search time divided by the number of evaluated states. A value
in the table is the cost per node in the corresponding setup (i.e., CA-ED or SOL-EP) divided
by the cost per node in the setup with no macros. For each of the two methods we show the
minimum, the maximum, and the average value. When macros are added to the original
domain (i.e., the domain is enhanced), the increase in cost per node can be significant. The
average rate is 7.70 in Satellite, and 6.06 in Depots. It is interesting to notice that this cost
is less than 1 in Rovers. This is an effect of solving a problem with less nodes expanded.
Operations such as managing the open list and the closed list have costs that increase with
the size of the lists at a rate that can be higher than linear. The right part of the table
shows much better values for the cost rate when macros are used in an enhanced planner.
609

Botea, Enzenberger, Müller, & Schaeffer

Pipesworld No-Tankage Non-Temporal - Nodes
1e+08
1e+07

Pipesworld No-Tankage Non-Temporal - CPU Time (seconds)
1e+04
FF enhanced
With macros
1e+03

FF enhanced
With macros

1e+06

1e+02

1e+05
1e+04

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
1e+00

10

20

30

40

1e-02

50

10

20

Problem
Pipesworld Tankage Non-Temporal - Nodes
1e+08
1e+07

FF enhanced
With macros

1e+03

1e+04

FF enhanced
With macros

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
5

10

15

20
25
Problem

30

35

1e-02

40

5

10

PSR - Nodes
1e+07

15

20
25
Problem

30

35

40

40

45

PSR - CPU Time (seconds)
1e+04

FF enhanced
With macros

1e+03

1e+06

FF enhanced
With macros

1e+02

1e+05
1e+04

1e+01

1e+03

1e+00

1e+02

1e-01

1e+01
1e+00

50

1e+02

1e+05

1e+08

40

Pipesworld Tankage Non-Temporal - CPU Time (seconds)
1e+04

1e+06

1e+00

30
Problem

5

10

15

20 25 30
Problem

35

40

1e-02

45

5

10

15

20 25 30
Problem

35

Figure 19: Comparison of our enhanced version of FF with and without macros in
Pipesworld No-Tankage Non-Temporal, Pipesworld Tankage Non-Temporal and
PSR (50 problems for each domain).

It is important to analyze why macros added as new operators generate such an increase
in cost per node in Satellite and Depots. The overhead is mostly present in the relaxed
graphplan algorithm that computes the heuristic value of a state. The complexity of this
algorithm depends upon the total number of actions that have been instantiated during
610

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

preprocessing for a given problem. Adding new operators to a domain increases the number of pre-instantiated actions. Since macros tend to have more variables than a regular
operator, the corresponding number of instantiations can be significantly larger. Let the
action instantiation rate be the number of actions instantiated for a problem when macros
are used divided by the number of actions instantiated in the original domain formulation.
Our statistics show that the average action instantiation rate is 6.03 in Satellite, 3.20 in
Depots, and 1.04 in Rovers.
The results show no significant impact of macro-operators on the solution quality. When
macros are used, the length of a plan slightly varies in both directions, with an average close
to the value of the original FF system.
Domain
Depots
Rovers
Satellite

Min
3.27
0.70
0.98

CA-ED
Max
8.56
0.90
14.38

Avg
6.06
0.83
7.70

SOL-EP
Min Max Avg
0.93 1.14 1.04
0.85 1.14 1.00
0.92 1.48 1.11

Table 4: Relative cost per node.

5.4 Evaluating the Effects of CA-ED Macros on Heuristic State Evaluation
As shown in Section 3.4, macros added to a domain as new operators affect both the
structure of the search space (the embedding effect) and the heuristic evaluation of states
with relaxed graphplan (the evaluation effect). This section presents an empirical analysis
of these.
Figure 24 shows results for Depots, Rovers and Satellite. For each domain, the chart
on the left shows data for the original domain formulation, and the chart on the right
shows data for the macro-enhanced domain formulation. For each domain formulation,
the data points are extracted from solution plans as follows. Each state along a solution
plan generates one data point. The coordinates of the data point are the state’s heuristic
evaluation on the vertical axis, and the number of steps left until the goal state is reached
on the horizontal axis. The number of steps to a goal state may be larger than the distance
(i.e., length of shortest path) to a goal state. The reason why states along solution plans
were used to generate data is that for such states, both the heuristic evaluation, and the
number of steps to a goal state are available after solving a problem.
The first conclusion from Figure 24 is that macros added to a domain improve the
accuracy of heuristic state evaluation of relaxed graphplan. The closer a data point is to
the diagonal, the more accurate the heuristic evaluation of the corresponding state.
Secondly, data clouds are shorter in macro-enhanced domains. This is a direct result of
the embedding effect, which reduces the depth of goal states.
611

Botea, Enzenberger, Müller, & Schaeffer

Rovers - Nodes
10000

(1) No Macros
(2) CA-ED
(3) SOL-EP
(4) 2 + 3

1000
100
10
1

0

2

4

6

8

10 12 14 16 18 20 22
Problem

Rovers - CPU Time (seconds)
10

(1) No Macros
(2) CA-ED
(3) SOL-EP
(4) 2 + 3

1

0.1

0.01

0

2

4

6

8

10 12 14
Problem

16

18

20

22

Figure 20: Evaluating abstraction techniques in Rovers. We show the number of expanded
nodes (top), and the CPU time (bottom).
612

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Depots - Nodes
1e+06

(1) No Macros
(2) CA-ED
(3) SOL-EP
(4) 2 + 3

100000
10000
1000
100
10
1

5

10
15
Problem

20

Depots - CPU Time (seconds)
1000
100
10
1
0.1
0.01

5

10

15

20

Problem
Figure 21: Evaluating abstraction techniques in Depots. We show the number of expanded
nodes (top), and the CPU time (bottom).
613

10
5
1

10

100

1000

10000

(1) No Macros
(2) CA-ED
(3) SOL-EP
(4) 2 + 3

15

20
Problem

Satellite - Nodes

25

30

35

Botea, Enzenberger, Müller, & Schaeffer

Figure 22: Evaluating abstraction techniques in Satellite. We show the number of expanded
nodes.
614

30
25
20
Problem
15
10
5
0.01

0.1

1

10

100

1000

(1) No Macros
(2) CA-ED
(3) SOL-EP
(4) 2 + 3
10000

Satellite - CPU Time (seconds)

35

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Figure 23: Evaluating abstraction techniques in Satellite (continued). We show the CPU
time.
615

Botea, Enzenberger, Müller, & Schaeffer

Depots + CA-ED Macros
120

100

100
Heuristic evaluation

Heuristic evaluation

Original Depots
120

80
60
40
20
0

0

80
60
40
20
0

20 40 60 80 100 120
Number of steps to goal

0

Rovers + CA-ED Macros

160

160

140

140

120

120

Heuristic evaluation

Heuristic evaluation

Original Rovers

100
80
60
40
20
0

100
80
60
40
20

0

0

20 40 60 80 100 120 140 160
Number of steps to goal

0

Original Satellite

Satellite + CA-ED Macros
350

300

300
Heuristic evaluation

Heuristic evaluation

20 40 60 80 100 120 140 160
Number of steps to goal

350

250
200
150
100
50
0

20 40 60 80 100 120
Number of steps to goal

250
200
150
100
50

0

0

50 100 150 200 250 300 350
Number of steps to goal

0

50 100 150 200 250 300 350
Number of steps to goal

Figure 24: Effects of CA-ED macros on heuristic state evaluation and depth of goal states.
616

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

6. Related Work
The related work described in this section falls into two categories. We first review approaches that make use of the domain structure to reduce the complexity of planning, and
next consider previous work on macro-operators.
An automatic method that discovers and exploits domain structure has been explored by
Knoblock (1994). In this work, a hierarchy of abstractions is built starting from the initial
low-level problem description. A new abstract level is obtained by dropping literals from
the problem definition at the previous abstraction level. Planning first produces an abstract
solution and then iteratively refines it to a low-level representation. The hierarchy is built
in such a way that, if a refinement of an abstract solution exists, no backtracking across
abstraction levels is necessary during the refinement process. Backtracking is performed
only when an abstract plan has no refinement. Such situations can be arbitrarily frequent,
with negative effects on the system’s performance.
Bacchus and Yang (1994) define a theoretical probabilistic framework to planning in
hierarchical models. Abstract solutions of a problem at different abstraction levels are
hierarchically represented as nodes in a tree structure. A tree edge indicates that the
target node is a refinement of the start node. An abstract solution can be refined to
the previous level with a given probability. Hierarchical search in this model is analytically
evaluated. The analytical model is further used to enhace Knoblock’s abstraction algorithm.
The enhancement refers to using estimations of the refinement probabilities for abstract
solutions.
More recently, implicit causal structure of a domain has been used to design a domainindependent heuristic for state evaluation (Helmert, 2004). These methods either statically
infer information about the structure of a domain, or dynamically discover the structure
for each problem instance. In contrast, we propose an adaptive technique that learns from
previous experience in a domain.
Two successful approaches that use hand-crafted information about the domain structure are hierarchical task networks and temporal logic control rules. Hierarchical task
networks (HTNs) guide and restrict planning by using a hierarchical representation of a domain. Human experts design hierarchies of tasks that show how the initial problem can be
broken down to the level of regular actions. The idea was introduced by Sacerdoti (1975)
and Tate (1977), and has widely been used in real-life planning applications (Wilkins &
desJardins, 2001). SHOP2 (Nau, Au, Ilghami, Kuter, Murdock, Wu, & Yaman, 2003) is a
well-known heuristic search planner where search is guided by HTNs.
In planning with temporal logic control rules, a formula is associated with each state in
the problem space. The formula of the initial state is provided with the domain description.
The formula of any other state is obtained based on its successor’s formula. When the
formula associated with a state can be proven false, that state’s subtree is pruned. The
best known planners of this kind are TLPlan (Bacchus & Kabanza, 2000) and TALPlanner
(Kvarnström & Doherty, 2001). While efficient, these approaches also rely heavily on human
knowledge, which might be expensive or impossible to obtain.
Early contributions to macro-operators in AI planning includes the work of Fikes and
Nilsson (1971). Macros are extracted after a problem was solved and the solution became
available. Minton (1985) advances this work by introducing techniques that filter the set
617

Botea, Enzenberger, Müller, & Schaeffer

of learned macro-operators. In his approach, two types of macro-operators are preferred:
S-macros, which occur with high frequency in problem solutions, and T-macros, which can
be useful but have low priority in the original search algorithm. Iba (1989) introduces
the so-called peak-to-peak heuristic to generate macro-operators at run-time. A macro
is a move sequence between two peaks of the heuristic state evaluation. Such a macro
traverses a “valley” in the search space, and using this later can correct errors in the heuristic
evaluation. Mooney (1988) considers whole plans as macros and introduces partial ordering
of operators based on their causal interactions.
Veloso and Carbonell (1993) and Kambhampati (1993) explore how planning can reuse
solutions of previously solved problems. Solutions annotated with additional relevant information are stored for later use. This additional information contains either explanations of
successful or failed search decisions (Veloso & Carbonell, 1993), or the causal structure of
solution plans (Kambhampati, 1993). Several similarity metrics for planning problems are
introduced. When a new problem is fed to the planner, the annotated solutions of similar
problems are used to guide the current planning process.
McCluskey and Porteous (1997) focus on constructing planning domains starting from
a natural language description. The approach combines human expertise and automatic
tools, and addresses both correctness and efficiency of the obtained formulation. Using
macro-operators is a major technique that the authors propose for efficiency improvement.
In this work, a state in a domain is composed of local states of several variables called
dynamic objects. Macros model transitions between the local states of a variable.
The planner Marvin (Coles & Smith, 2004) generates macros both online (as plateauescaping sequences) and offline (from a reduced version of the problem to be solved). No
macros are cached from one problem instance to another.
Macro-moves were successfully used in single-agent search problems such as puzzles or
path-finding in commercial computer games, usually in a domain-specific implementation.
The sliding-tile puzzle was among the first testbeds for this idea (Korf, 1985; Iba, 1989).
Two of the most effective concepts used in the Sokoban solver Rolling Stone, tunnel and
goal macros, are applications of this idea (Junghanns & Schaeffer, 2001). More recent work
in Sokoban includes an approach that decomposes a maze into a set of rooms connected
by tunnels (Botea, Müller, & Schaeffer, 2002). Search is performed at the higher level
of abstract move sequences that rearrange the stones inside a room so that a stone can
be transferred from one room to another. Using macro-moves for solving Rubik’s Cube
puzzles is proposed by Hernádvölgyi (2001). A method proposed by Botea, Müller, and
Schaeffer (2004a) automatically decomposes a navigation map into a set of clusters, possibly
on several abstraction levels. For each cluster, an internal optimal path is pre-computed
between any two entrances of that cluster. Path-finding is performed at an abstract level,
where a macro-move crosses a cluster from one entrance to another in one step.
Methods that exploit at search time the relaxed graphplan associated with a problem
state (Hoffmann & Nebel, 2001) include helpful action pruning (Hoffmann & Nebel, 2001)
and look-ahead policies (Vidal, 2004). Helpful action pruning considers for node expansion
only actions that occur in the relaxed plan and can be applied to the current state. Helpful
macro pruning applies the same pruning idea for the macro-actions applicable to a state,
with the noticeable difference that helpful macro pruning does not give up completeness
of the search algorithm. A lookahead policy executes parts of the relaxed plan in the real
618

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

world, as this often provides a path towards a goal state with no search and few states
evaluated. The actions in the relaxed plan are iteratively applied as long as this is possible
in a heuristically computed order. When the lookahead procedure cannot be continued with
actions from the relaxed plan, a plan-repair method selects a new action to be applied.

7. Conclusion and Future Work
Despite the great progress that AI planning has recently achieved, many benchmarks remain
challenging for current planners. In this paper we presented techniques that automatically
learn macro-operators from previous experience in a domain, and use them to speed up the
search in future problems. We evaluated our methods on standard benchmarks from international planning competitions, showing significant improvement for domains where structure
information can be inferred. We implemented our ideas in Macro-FF, an extension of
FF version 2.3. Macro-FF participated in the classical part of the fourth international
planning competition, competing in seven domains and taking first place in three of them.
Exploring our method more deeply and improving the performance in more classes of
problems are major directions for future work. We also plan to extend our approach in
several directions. Our learning method can be generalized from macro-operators to more
complex structures such as HTNs. Little research focusing on learning HTNs has been
conducted, even though the problem is of great importance.
We plan to explore how a heuristic evaluation based on the relaxed graphplan can be
improved with macro-operators. As shown in this article, a macro added to an original
domain formulation as a regular operator influences the results of the heuristic function.
This is convenient (no changes are necessary in the planning engine), but limited only to
STRIPS domains. For other subsets of PDDL, the relaxed graphplan algorithm can be
extended with special capabilities to handle macros when no enhanced domain definition is
provided.
The long-term goal of component abstraction is automatic reformulation of planning
domains and problems. When a real-world problem is abstracted into a planning model,
the corresponding formulation is expressed at an abstraction level that a human designer
considers appropriate. However, choosing a good abstraction level could be a hard and
expensive problem for humans. Hence methods that automatically update the formulation
of a problem based on its structure would be helpful.

Acknowledgments
This research was supported by the Natural Sciences and Engineering Research Council of
Canada (NSERC) and Alberta’s Informatics Circle of Research Excellence (iCORE). We
thank Jörg Hoffmann for making the source code of FF available, and for kindly answering
many technical questions related to FF. We also thank the organizers of IPC-4, the reviewers
of this article, and Maria Fox, who led the reviewing process.

References
Bacchus, F. (2001). AIPS’00 Planning Competition. AI Magazine, 22 (3), 47–56.
619

Botea, Enzenberger, Müller, & Schaeffer

Bacchus, F., & Kabanza, F. (2000). Using Temporal Logics to Express Search Control
Knowledge for Planning. Artificial Intelligence, 16, 123–191.
Bacchus, F., & Yang, Q. (1994). Downward Refinement and the Efficiency of Hierarchical
Problem Solving. Artificial Intelligence, 71 (1), 43–100.
Botea, A., Müller, M., & Schaeffer, J. (2002). Using Abstraction for Planning in Sokoban.
In Schaeffer, J., Müller, M., & Björnsson, Y. (Eds.), 3rd International Conference on
Computers and Games (CG’2002), Vol. 2883 of Lecture Notes in Artificial Intelligence,
pp. 360–375, Edmonton, Canada. Springer.
Botea, A., Müller, M., & Schaeffer, J. (2004a). Near Optimal Hierarchical Path-Finding.
Journal of Game Development, 1 (1), 7–28.
Botea, A., Müller, M., & Schaeffer, J. (2004b). Using Component Abstraction for Automatic
Generation of Macro-Actions. In Fourteenth International Conference on Automated
Planning and Scheduling ICAPS-04, pp. 181–190, Whistler, Canada. AAAI Press.
Coles, A., & Smith, A. (2004). Marvin: Macro Actions from Reduced Versions of the
Instance. In Booklet of the Fourth International Planning Competition, pp. 24–26.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: A New Approach to the Application of Theorem
Proving to Problem Solving. Artificial Intelligence, 5 (2), 189–208.
Helmert, M. (2004). A Planning Heuristic Based on Causal Graph Analysis. In Fourteenth
International Conference on Automated Planning and Scheduling ICAPS-04, pp. 161–
170, Whistler, Canada.
Hernádvölgyi, I. (2001). Searching for Macro-operators with Automatically Generated
Heuristics. In Fourteenth Canadian Conference on Artificial Intelligence, pp. 194–
203.
Hoffmann, J., Edelkamp, S., Englert, R., Liporace, F., Thiébaux, S., & Trüg, S. (2004).
Towards Realistic Benchmarks for Planning: the Domains Used in the Classical Part
of IPC-4. In Booklet of the Fourth International Planning Competition, pp. 7–14.
Hoffmann, J., & Nebel, B. (2001). The FF Planning System: Fast Plan Generation Through
Heuristic Search. Journal of Artificial Intelligence Research, 14, 253–302.
Iba, G. A. (1989). A Heuristic Approach to the Discovery of Macro-Operators. Machine
Learning, 3 (4), 285–317.
Junghanns, A., & Schaeffer, J. (2001). Sokoban: Enhancing Single-Agent Search Using
Domain Knowledge. Artificial Intelligence, 129 (1–2), 219–251.
Kambhampati, S. (1993). Machine Learning Methods for Planning, chap. Supporting Flexible Plan Reuse, pp. 397–434. Morgan Kaufmann.
Knoblock, C. A. (1994). Automatically Generating Abstractions for Planning. Artificial
Intelligence, 68 (2), 243–302.
Korf, R. (1985). Macro-Operators: A Weak Method for Learning. Artificial Intelligence,
26(1), 35–77.
Kvarnström, J., & Doherty, P. (2001). TALplanner: Temporal Logic Based Forward Chaining Planner. Annals of Mathematics and Artificial Intelligence, 30, 119–169.
620

Macro-FF: Improving AI Planning with Automatically Learned Macro-Operators

Long, D., & Fox, M. (2003). The 3rd International Planning Competition: Results and
Analysis. Journal of Artificial Intelligence Research, 20, 1–59. Special Issue on the
3rd International Planning Competition.
Marsland, T. A. (1986). A Review of Game-Tree Pruning. International Computer Chess
Association Journal, 9 (1), 3–19.
McCluskey, T. L., & Porteous, J. M. (1997). Engineering and Compiling Planning Domain
Models to Promote Validity and Efficiency. Artificial Intelligence, 95, 1–65.
McDermott, D. (2000). The 1998 AI Planning Systems Competition. AI Magazine, 21 (2),
35–55.
Minton, S. (1985). Selectively Generalizing Plans for Problem-Solving. In IJCAI-85, pp.
596–599.
Minton, S. (1988). Learning Search Control Knowledge: An Explanation-Based Approach..
Hingham, MA. Kluwer Academic Publishers.
Mooney, R. (1988). Generalizing the Order of Operators in Macro-Operators. In Fifth
International Conference on Machine Learning ICML-88, pp. 270–283.
Nau, D., Au, T., Ilghami, O., Kuter, U., Murdock, J., Wu, D., & Yaman, F. (2003). SHOP2:
An HTN Planning System. Journal of Artificial Intelligence Research, 20, 379–404.
Sacerdoti, E. (1975). The Nonlinear Nature of Plans. In Proceedings IJCAI-75, pp. 206–214.
Tate, A. (1977). Generating Project Networks. In Proceedings of IJCAI-77, pp. 888–893.
Veloso, M., & Carbonell, J. (1993). Machine Learning Methods for Planning, chap. Toward
Scaling Up Machine Learning: A Case Study with Derivational Analogy, pp. 233–272.
Morgan Kaufmann.
Vidal, V. (2004). A Lookahead Strategy for Heuristic Search Planning. In Fourteenth
International Conference on Automated Planning and Scheduling ICAPS-04, pp. 150–
159, Whistler, Canada.
Wilkins, D., & desJardins, M. (2001). A Call for Knowledge-Based Planning. AI Magazine,
22 (1), 99–115.

621

Journal of Artificial Intelligence Research 24 (2005) 341-356

Submitted 11/04; published 09/05

Efficiency versus Convergence of Boolean Kernels for
On-Line Learning Algorithms
Roni Khardon

roni@cs.tufts.edu

Department of Computer Science, Tufts University
Medford, MA 02155

Dan Roth

danr@cs.uiuc.edu

Department of Computer Science, University of Illinois
Urbana, IL 61801 USA

Rocco A. Servedio

rocco@cs.columbia.edu

Department of Computer Science, Columbia University
New York, NY 10025

Abstract
The paper studies machine learning problems where each example is described using a
set of Boolean features and where hypotheses are represented by linear threshold elements.
One method of increasing the expressiveness of learned hypotheses in this context is to
expand the feature set to include conjunctions of basic features. This can be done explicitly
or where possible by using a kernel function. Focusing on the well known Perceptron
and Winnow algorithms, the paper demonstrates a tradeoff between the computational
efficiency with which the algorithm can be run over the expanded feature space and the
generalization ability of the corresponding learning algorithm.
We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run
the Perceptron algorithm over a feature space of exponentially many conjunctions; however we also show that using such kernels, the Perceptron algorithm can provably make an
exponential number of mistakes even when learning simple functions.
We then consider the question of whether kernel functions can analogously be used
to run the multiplicative-update Winnow algorithm over an expanded feature space of
exponentially many conjunctions. Known upper bounds imply that the Winnow algorithm
can learn Disjunctive Normal Form (DNF) formulae with a polynomial mistake bound in
this setting. However, we prove that it is computationally hard to simulate Winnow’s
behavior for learning DNF over such a feature set. This implies that the kernel functions
which correspond to running Winnow for this problem are not efficiently computable, and
that there is no general construction that can run Winnow with kernels.

1. Introduction
The problem of classifying objects into one of two classes being “positive” and “negative”
examples of a concept is often studied in machine learning. The task in machine learning
is to extract such a classifier from given pre-classified examples - the problem of learning
from data. When each example is represented by a set of n numerical features, an example
c
°2005
AI Access Foundation. All rights reserved.

Khardon, Roth, & Servedio

can be seen as a point in Euclidean space <n . A common representation for classifiers in
this case is a hyperplane of dimension (n − 1) which splits the domain of examples into
two areas of positive and negative examples. Such a representation is known as a linear
threshold function, and many learning algorithms that output a hypothesis represented in
this manner have been developed, analyzed, implemented, and applied in practice. Of
particular interest in this paper are the well known Perceptron (Rosenblatt, 1958; Block,
1962; Novikoff, 1963) and Winnow (Littlestone, 1988) algorithms that have been intensively
studied in the literature.
It is also well known that the expressiveness of linear threshold functions is quite limited (Minsky & Papert, 1968). Despite this fact, both Perceptron and Winnow have been
applied successfully in recent years to several large scale real world classification problems.
As one example, the SNoW system (Roth, 1998; Carlson, Cumby, Rosen, & Roth, 1999) has
successfully applied variations of Perceptron and Winnow to problems in natural language
processing. The SNoW system extracts basic Boolean features x1 , . . . , xn from labeled pieces
of text data in order to represent the examples, thus the features have numerical values restricted to {0, 1}. There are several ways to enhance the set of basic features x 1 , . . . , xn
for Perceptron or Winnow. One idea is to expand the set of basic features x 1 , . . . , xn using
conjunctions such as (x1 ∧ x3 ∧ x4 ) and use these expanded higher-dimensional examples, in
which each conjunction plays the role of a basic feature, as the examples for Perceptron or
Winnow. This is in fact the approach which the SNoW system takes running Perceptron or
Winnow over a space of restricted conjunctions of these basic features. This idea is closely
related to the use of kernel methods, see e.g. the book of Cristianini and Shawe-Taylor
(2000), where a feature expansion is done implicitly through the kernel function. The approach clearly leads to an increase in expressiveness and thus may improve performance.
However, it also dramatically increases the number of features (from n to 3 n if all conjunctions are used), and thus may adversely affect both the computation time and convergence
rate of learning. The paper provides a theoretical study of the performance of Perceptron
and Winnow when run over expanded feature spaces such as these.
1.1 Background: On-Line Learning with Perceptron and Winnow
Before describing our results, we recall some necessary background on the on-line learning
model (Littlestone, 1988) and the Perceptron and Winnow algorithms.
Given an instance space X of possible examples, a concept is a mapping of instances into
one of two (or more) classes. A concept class C ⊆ 2X is simply a set of concepts. In on-line
learning a concept class C is fixed in advance and an adversary can pick a concept c ∈ C.
The learning is then modeled as a repeated game where in each iteration the adversary
picks an example x ∈ X, the learner gives a guess for the value of c(x) and is then told the
correct value. We count one mistake for each iteration in which the value is not predicted
correctly. A learning algorithm learns a concept class C with mistake bound M if for any
choice of c ∈ C and any (arbitrarily long) sequence of examples, the learner is guaranteed
to make at most M mistakes.
In this paper we consider the case where the examples are given by Boolean features,
that is X = {0, 1}n , and we have two class labels denoted by −1 and 1. Thus for x ∈ {0, 1}n ,
a labeled example hx, 1i is a positive example, and a labeled example hx, −1i is a negative
342

Efficiency versus Convergence of Boolean Kernels

example. The concepts we consider are built using logical combinations of the n base
features and we are interested in mistake bounds that are polynomial in n.
1.1.1 Perceptron
Throughout its execution Perceptron maintains a weight vector w ∈ < N which is initially
(0, . . . , 0). Upon receiving an example x ∈ <N the algorithm predicts according to the
linear threshold function w · x ≥ 0. If the prediction is 1 and the label is −1 (false positive
prediction) then the vector w is set to w − x, while if the prediction is −1 and the label is 1
(false negative) then w is set to w + x. No change is made to w if the prediction is correct.
Many variants of this basic algorithm have been proposed and studied and in particular one
can add a non zero threshold as well as a learning rate that controls the size of update to
w. Some of these are discussed further in Section 3.
The famous Perceptron Convergence Theorem (Rosenblatt, 1958; Block, 1962; Novikoff,
1963) bounds the number of mistakes which the Perceptron algorithm can make:
Theorem 1 Let hx1 , y1 i, . . . , hxt , yt i be a sequence of labeled examples with xi ∈ <N , kxi k ≤
R and yi ∈ {−1, 1} for all i. Let u ∈ <N , ξ > 0 be such that yi (u · xi ) ≥ ξ for all i. Then
2
2
mistakes on this example sequence.
Perceptron makes at most R ξkuk
2
1.1.2 Winnow
The Winnow algorithm (Littlestone, 1988) has a very similar structure. Winnow maintains
a hypothesis vector w ∈ <N which is initially w = (1, . . . , 1). Winnow is parameterized by
a promotion factor α > 1 and a threshold θ > 0; upon receiving an example x ∈ {0, 1} N
Winnow predicts according to the threshold function w · x ≥ θ. If the prediction is 1 and the
label is −1 then for all i such that xi = 1 the value of wi is set to wi /α; this is a demotion
step. If the prediction is −1 and the label is 1 then for all i such that xi = 1 the value of wi
is set to αwi ; this is a promotion step. No change is made to w if the prediction is correct.
For our purposes the following mistake bound, implicit in Littlestone’s work (1988), is
of interest:
Theorem 2 Let the target function be a k-literal monotone disjunction f (x 1 , . . . , xN ) =
xi1 ∨ · · · ∨ xik . For any sequence of examples in {0, 1}N labeled according to f the number
α
of prediction mistakes made by Winnow(α, θ) is at most α−1
· Nθ + k(α + 1)(1 + logα θ).
1.2 Our Results
We are interested in the computational efficiency and convergence of the Perceptron and
Winnow algorithms when run over expanded feature spaces of conjunctions. Specifically,
we study the use of kernel functions to expand the feature space and thus enhance the
learning abilities of Perceptron and Winnow; we refer to these enhanced algorithms as
kernel Perceptron and kernel Winnow.
Our first result (cf. also the papers of Sadohara, 1991; Watkins, 1999; and Kowalczyk
et al., 2001) uses kernel functions to show that it is possible to efficiently run the kernel
Perceptron algorithm over an exponential number of conjunctive features.
343

Khardon, Roth, & Servedio

Result 1: (see Theorem 3) There is an algorithm that simulates Perceptron over the 3 n dimensional feature space of all conjunctions of n basic features. Given a sequence of t
labeled examples in {0, 1}n the prediction and update for each example take poly(n, t) time
steps. We also prove variants of this result in which the expanded feature space consists of
all monotone conjunctions or all conjunctions of some bounded size.
This result is closely related to one of the main open problems in learning theory:
efficient learnability of disjunctions of conjunctions, or DNF (Disjunctive Normal Form)
expressions.1 Since linear threshold elements can represent disjunctions (e.g. x1 ∨ x2 ∨ x3
is true iff x1 + x2 + x3 ≥ 1), Theorem 1 and Result 1 imply that kernel Perceptron can be
used to learn DNF. However, in this framework the values of N and R in Theorem 1 can be
exponentially large (note that we have N = 3n and R = 2n/2 if all conjunctions are used),
and hence the mistake bound given by Theorem 1 is exponential rather than polynomial
in n. The question thus arises whether the exponential upper bound implied by Theorem
1 is essentially tight for the kernel Perceptron algorithm in the context of DNF learning.
We give an affirmative answer, thus showing that kernel Perceptron cannot efficiently learn
DNF.
Result 2: There is a monotone DNF f over x1 , . . . , xn and a sequence of examples labeled
according to f which causes the kernel Perceptron algorithm to make 2 Ω(n) mistakes. This
result holds for generalized versions of the Perceptron algorithm where a fixed or updated
threshold and a learning rate are used. We also give a variant of this result showing
that kernel Perceptron fails in the Probably Approximately Correct (PAC) learning model
(Valiant, 1984) as well.
Turning to Winnow, an attractive feature of Theorem 2 is that for suitable α, θ the bound
is logarithmic in the total number of features N (e.g. α = 2 and θ = N ). Therefore, as
noted by several researchers (Maass & Warmuth, 1998), if a Winnow analogue of Theorem 3
could be obtained this would imply that DNF can be learned by a computationally efficient
algorithm with a poly(n)-mistake bound. However, we give strong evidence that no such
Winnow analogue of Theorem 3 can exist.
Result 3: There is no polynomial time algorithm which simulates Winnow over exponentially many monotone conjunctive features for learning monotone DNF unless every problem
in the complexity class #P can be solved in polynomial time. This result holds for a wide
range of parameter settings in the Winnow algorithm.
We observe that, in contrast to this negative result, Maass and Warmuth have shown
that the Winnow algorithm can be simulated efficiently over exponentially many conjunctive
features for learning some simple geometric concept classes (Maass & Warmuth, 1998).
Our results thus indicate a tradeoff between computational efficiency and convergence
of kernel algorithms for rich classes of Boolean functions such as DNF formulas; the kernel
1. Angluin (1990) proved that DNF expressions cannot be learned efficiently using equivalence queries
whose hypotheses are themselves DNF expressions. Since the model of exact learning from equivalence
queries only is equivalent to the mistake bound model which we consider in this paper, her result implies
that no online algorithm which uses DNF formulas as hypotheses can efficiently learn DNF. However,
this result does not preclude the efficient learnability of DNF using a different class of hypotheses. The
kernel Perceptron algorithm generates hypotheses which are thresholds of conjunctions rather than DNF
formulas, and thus Angluin’s negative results do not apply here.

344

Efficiency versus Convergence of Boolean Kernels

Perceptron algorithm is computationally efficient to run but has exponentially slow convergence, whereas kernel Winnow has rapid convergence but seems to require exponential
runtime.

2. Kernel Perceptron with Many Features
It is well known that the hypothesis w of the Perceptron algorithm is a linear combination
of the previous examples on which mistakes were made (Cristianini & Shaw-Taylor, 2000).
More precisely, if we let L(v) ∈ {−1, 1} denote the label of example v, then we have that
P
w = v∈M L(v)v where M is the set of examples on which the algorithm made a mistake.
P
P
Thus the prediction of Perceptron on x is 1 iff w·x = ( v∈M L(v)v)·x = v∈M L(v)(v ·x) ≥
0.
For an example x ∈ {0, 1}n let φ(x) denote its transformation into an enhanced feature
space such as the space of all conjunctions. To run the Perceptron algorithm over the
enhanced space we must predict 1 iff w φ · φ(x) ≥ 0 where w φ is the weight vector in
P
the enhanced space; from the above discussion this holds iff v∈M L(v)(φ(v) · φ(x)) ≥ 0.
P
Denoting K(v, x) = φ(v) · φ(x) this holds iff v∈M L(v)K(v, x) ≥ 0.
Thus we never need to construct the enhanced feature space explicitly; in order to run
Perceptron we need only be able to compute the kernel function K(v, x) efficiently. This is
the idea behind all so-called kernel methods, which can be applied to any algorithm (such
as support vector machines) whose prediction is a function of inner products of examples.
A more detailed discussion is given in the book of Cristianini and Shawe-Taylor (2000).
Thus the next theorem is simply obtained by presenting a kernel function capturing all
conjunctions.
Theorem 3 There is an algorithm that simulates Perceptron over the feature spaces of
(1) all conjunctions, (2) all monotone conjunctions, (3) conjunctions of size ≤ k, and (4)
monotone conjunctions of size ≤ k. Given a sequence of t labeled examples in {0, 1} n the
prediction and update for each example take poly(n, t) time steps.
Proof: For case (1) φ(·) includes all 3n conjunctions (with positive and negative literals) and
K(x, y) must compute the number of conjunctions which are true in both x and y. Clearly,
any literal in such a conjunction must satisfy both x and y and thus the corresponding bit
in x, y must have the same value. Thus each conjunction true in both x and y corresponds
to a subset of such bits. Counting all these conjunctions gives K(x, y) = 2same(x,y) where
same(x, y) is the number of original features that have the same value in x and y, i.e. the
number of bit positions i which have xi = yi . This kernel has been obtained independently
by Sadohara (2001).
To express all monotone monomials as in (2) we take K(x, y) = 2|x∩y| where |x ∩ y| is
the number of active features common to both x and y, i.e. the number of bit positions
which have xi = yi = 1.
Similarly, for case (3) the number of conjunctions that satisfy both x and y is K(x, y) =
Pk ¡same(x,y)¢
. This kernel is reported also by Watkins (1999). For case (4) we have
l=0
l
¡
¢
P
K(x, y) = kl=0 |x∩y|
.
2
l
345

Khardon, Roth, & Servedio

3. Kernel Perceptron with Many Mistakes
In this section we describe a simple monotone DNF target function and a sequence of
labeled examples which causes the monotone monomials kernel Perceptron algorithm to
make exponentially many mistakes.
For x, y ∈ {0, 1}n we write |x| to denote the number of 1’s in x and, as described above,
|x∩y| to denote the number of bit positions i which have xi = yi = 1. We need the following
well-known tail bound on sums of independent random variables which can be found in,
e.g., Section 9.3 of the book by Kearns and Vazirani (1994):
Fact 4 Let X1 , . . . , Xm be a sequence of m independent 0/1-valued random variables, each
P
of which has E[Xi ] = p. Let X denote m
i=1 Xi , so E[X] = pm. Then for 0 ≤ γ ≤ 1, we
have
Pr[X > (1 + γ)pm] ≤ e−mpγ

2 /3

and

Pr[X < (1 − γ)pm] ≤ e−mpγ

2 /2

.

We also use the following combinatorial property:
Lemma 5 There is a set S of n-bit strings S = {x1 , . . . , xt } ⊂ {0, 1}n with t = en/9600
such that |xi | = n/20 for 1 ≤ i ≤ t and |xi ∩ xj | ≤ n/80 for 1 ≤ i < j ≤ t.
Proof: We use the probabilistic method. For each i = 1, . . . , t let xi ∈ {0, 1}n be chosen
by independently setting each bit to 1 with probability 1/10. For any i it is clear that
E[|xi |] = n/10. Applying Fact 4, we have that Pr[|xi | < n/20] ≤ e−n/80 , and thus the
probability that any xi satisfies |xi | < n/20 is at most te−n/80 . Similarly, for any i 6= j we
have E[|xi ∩ xj |] = n/100. Applying Fact 4 we have that Pr[|xi ∩ xj | > n/80] ≤ e−n/4800 ,
and thus the probability that any xi , xj¡ with
i 6= j satisfies |xi ∩ xj | > n/80 is at most
¡ t ¢ −n/4800
t ¢ −n/4800
n/9600
+ te−n/80 is less than 1. Thus for some
. For t = e
the value of 2 e
2 e
choice of x1 , . . . , xt we have each |xi | ≥ n/20 and |xi ∩ xj | ≤ n/80. For any xi which has
|xi | > n/20 we can set |xi | − n/20 of the 1s to 0s, and the lemma is proved.
2
Now using the previous lemma we can construct a difficult data set for kernel Perceptron:
Theorem 6 There is a monotone DNF f over x1 , . . . , xn and a sequence of examples labeled
according to f which causes the kernel Perceptron algorithm to make 2Ω(n) mistakes.
Proof: The target DNF with which we will use is very simple: it is the single conjunction
x1 x2 . . . xn . While the original Perceptron algorithm over the n features x1 , . . . , xn is easily
seen to make at most poly(n) mistakes for this target function, we now show that the
monotone kernel Perceptron algorithm which runs over a feature space of all 2 n monotone
monomials can make 2 + en/9600 mistakes.
Recall that at the beginning of the Perceptron algorithm’s execution all 2 n coordinates
of wφ are 0. The first example is the negative example 0n . The only monomial true in this
example is the empty monomial which is true in every example. Since w φ · φ(x) = 0 Perceptron incorrectly predicts 1 on this example. The resulting update causes the coefficient
w∅φ corresponding to the empty monomial to become −1 but all 2n − 1 other coordinates
of wφ remain 0. The next example is the positive example 1n . For this example we have
wφ · φ(x) = −1 so Perceptron incorrectly predicts −1. Since all 2n monotone conjunctions
346

Efficiency versus Convergence of Boolean Kernels

are satisfied by this example the resulting update causes w∅φ to become 0 and all 2n − 1
other coordinates of w φ to become 1. The next en/9600 examples are the vectors x1 , . . . , xt
described in Lemma 5. Since each such example has |xi | = n/20 each example is negative;
however as we now show the Perceptron algorithm will predict 1 on each of these examples.
Fix any value 1 ≤ i ≤ en/9600 and consider the hypothesis vector w φ just before example
i
x is received. Since |xi | = n/20 the value of w φ · φ(xi ) is a sum of the 2n/20 different
coordinates wTφ which correspond to the monomials satisfied by xi . More precisely we have
P
P
wφ · φ(xi ) = T ∈Ai wTφ + T ∈Bi wTφ where Ai contains the monomials which are satisfied
by xi and xj for some j 6= i and Bi contains the monomials which are satisfied by xi but
no xj with j 6= i. We lower bound the two sums separately.
Let T be any monomial in Ai . By Lemma 5 any T ∈ Ai contains at most n/80 variables
¢
Pn/80 ¡
monomials in Ai . Using the well known bound
and thus there can be at most r=0 n/20
r
¢
¡
Pα` `
(H(α)+o(1))`
where 0 < α ≤ 1/2 and H(p) = −p log p − (1 − p) log(1 − p) is
j=0 j = 2
the binary entropy function, which can be found e.g. as Theorem 1.4.5 of the book by
Van Lint (1992), there can be at most 20.8113·(n/20)+o(n) < 20.041n terms in Ai . Moreover
the value of each wTφ must be at least −en/9600 since wTφ decreases by at most 1 for each
P
example, and hence T ∈Ai wTφ ≥ −en/9600 20.041n > −20.042n . On the other hand, any T ∈ Bi
is false in all other examples and therefore wTφ has not been demoted and wTφ = 1. By
Lemma 5 for any r > n/80 every r-variable monomial satisfied by xi must belong to Bi ,
¢
¡
P
Pn/20
and hence T ∈Bi wTφ ≥ r=n/80+1 n/20
> 20.049n . Combining these inequalities we have
r
w · xi ≥ −20.042n + 20.049n > 0 and hence the Perceptron prediction on xi is 1.
2
Remark 7 At first sight it might seem that the result is limited to a simple special case of
the perceptron algorithm. Several variations exist that use: an added feature with a fixed
value that enables the algorithm to update the threshold indirectly (via a weight ŵ), a non
zero fixed (initial) threshold θ, and a learning rate α, and in particular all these three can
be used simultaneously. The generalized algorithm predicts according to the hypothesis
w · x + ŵ ≥ θ and updates w ← w + αx and ŵ ← ŵ + α for promotions and similarly
for demotions. We show here that exponential lower bounds on the number of mistakes
can be derived for the more general algorithm as well. First, note that since our kernel
includes a feature for the empty monomial which is always true, the first parameter is
already accounted for. For the other two parameters note that there is a degree of freedom
between the learning rate α and fixed threshold θ since multiplying both by the same factor
does not change the hypothesis and therefore it suffices to consider the threshold only. We
consider several cases for the value of the threshold. If θ satisfies 0 ≤ θ ≤ 2 0.047 then we
use the same sequence of examples. After the first two examples the algorithm makes a
promotion on 1n (it may or may not update on 0n but that is not important). For the
P
P
examples in the sequence the bounds on T ∈Ai wTφ and T ∈Bi wTφ are still valid so the
final inequality in the proof becomes w · xi ≥ −20.042n + 20.049n > 20.047n which is true for
sufficiently large n. If θ > 20.047n then we can construct the following scenario. We use the
function f = x1 ∨ x2 ∨ . . . ∨ xn , and the sequence of examples includes 2θ − 1 repetitions of
the same example x where the first bit is 1 and all other bits are 0. The example x satisfies
exactly 2 monomials and therefore the algorithm will make mistakes on all the examples in
the sequence. If θ < 0 then the initial hypothesis misclassifies 0n . We start the example
347

Khardon, Roth, & Servedio

sequence by repeating the example 0n until it is classified correctly, that is d−θe times.
If the threshold is large in absolute value e.g. θ < −20.042n we are done. Otherwise we
continue with the example 1n . Since all weights except for the empty monomial are zero at
this stage the examples 0n and 1n are classified in the same way so 1n is misclassified and
therefore the algorithm makes a promotion. The argument for the rest of the sequence is as
above (except for adding a term for the empty monomial) and the final inequality becomes
w · xi ≥ −20.042n − 20.042n + 20.049n > −20.042n so each of the examples is misclassified. Thus
in all cases kernel Perceptron may make an exponential number of mistakes.
3.1 A Negative Result for the PAC Model
The proof above can be adapted to give a negative result for kernel Perceptron in the PAC
learning model (Valiant, 1984). In this model each example x is independently drawn from
a fixed probability distribution D and with high probability the learner must construct a
hypothesis h which has high accuracy relative to the target concept c under distribution D.
See the Kearns-Vazirani text (1994) for a detailed discussion of the PAC learning model.
Let D be the probability distribution over {0, 1}n which assigns weight 1/4 to the ex1
to each of the en/9600 examples
ample 0n , weight 1/4 to the example 1n , and weight 21 en/9600
x1 , . . . , xt .
Theorem 8 If kernel Perceptron is run using a sample of polynomial size p(n) then with
probability at least 1/16 the error of its final hypothesis is at least 0.49.
Proof: With probability 1/16, the first two examples received from D will be 0n and then
1n . Thus, with probability 1/16, after two examples (as in the proof above) the Perceptron
algorithm will have w∅φ = 0 and all other coefficients of w φ equal to 1.
Consider the sequence of examples following these two examples. First note that in any
trial, any occurrence of an example other than 1n (i.e. any occurrence either of some xi or of
P
the 0n example) can decrease T ⊆[n] wTθ by at most 2n/20 . Since after the first two examples
P
we have w φ · φ(1n ) = T ⊆[n] wTθ = 2n − 1, it follows that at least 219n/20 − 1 more examples
must occur before the 1n example will be incorrectly classified as a negative example. Since
we will only consider the performance of the algorithm for p(n) < 219n/20 − 1 steps, we
may ignore all subsequent occurrences of 1n since they will not change the algorithm’s
hypothesis.
Now observe that on the first example which is not 1n the algorithm will perform a
demotion resulting in w∅φ = −1 (possibly changing other coefficients as well). Since no
promotions will be performed on the rest of the sample, we get w∅φ ≤ −1 for the rest of
the learning process. It follows that all future occurrences of the example 0 n are correctly
classified and thus we may ignore them as well.
Considering examples xi from the sequence constructed above, we may ignore any example that is correctly classified since no update is made on it. It follows that when the
perceptron algorithm has gone over all examples, its hypothesis is formed by demotions on
examples in the sequence of xi ’s. The only difference from the scenario above is that the
algorithm may make several demotions on the same example if it occurs multiple times in
the sample. However, an inspection of the proof above shows that for any x i that has not
P
P
been seen by the algorithm, the bounds on T ∈Ai wTφ and T ∈Bi wTφ are still valid and
348

Efficiency versus Convergence of Boolean Kernels

therefore xi will be misclassified. Since the sample is of size p(n) and the sequence is of
size en/9600 the probability weight of examples in the sample is at most 0.01 for sufficiently
large n so the error of the hypothesis is at least 0.49.
2

4. Computational Hardness of Kernel Winnow
In this section, for x ∈ {0, 1}n we let φ(x) denote the (2n − 1)-element vector whose coordinates are all nonempty monomials (monotone conjunctions) over x1 , . . . , xn . We say that
a sequence of labeled examples hx1 , b1 i, . . . , hxt , bt i is monotone consistent if it is consistent
with some monotone function, i.e. xik ≤ xjk for all k = 1, . . . , n implies bi ≤ bj . If S is
monotone consistent and has t labeled examples then clearly there is a monotone DNF
formula consistent with S which contains at most t conjunctions. We consider the following
problem:
KERNEL WINNOW PREDICTION(α, θ) (KWP)
Instance: Monotone consistent sequence S = hx1 , b1 i, . . . , hxt , bt i of labeled examples with
each xi ∈ {0, 1}m and each bi ∈ {−1, 1}; unlabeled example z ∈ {0, 1}m .
Question: Is w φ · φ(z) ≥ θ, where w φ is the N = (2m − 1)-dimensional hypothesis vector
generated by running Winnow(α, θ) on the example sequence hφ(x1 ), b1 i, . . . hφ(xt ), bt i?
In order to run Winnow over all 2m − 1 nonempty monomials to learn monotone DNF,
one must be able to solve KWP efficiently. Our main result in this section is a proof
that KWP is computationally hard for a wide range of parameter settings which yield a
polynomial mistake bound for Winnow via Theorem 2.
Recall that #P is the class of all counting problems associated with N P decision problems; it is well known that if every function in #P is computable in polynomial time then
P = N P. See the book of Papadimitriou (1994) or the paper of Valiant (1979) for details
on #P. The following problem is #P-hard (Valiant, 1979):
MONOTONE 2-SAT (M2SAT)
Instance: Monotone 2-CNF Boolean formula F = c1 ∧ c2 ∧ . . . ∧ cr with ci = (yi1 ∨ yi2 )
and each yij ∈ {y1 , . . . , yn }; integer K such that 1 ≤ K ≤ 2n .
Question: Is |F −1 (1)| ≥ K, i.e. does F have at least K satisfying assignments in {0, 1}n ?
Theorem 9 Fix any ² > 0. Let N = 2m − 1, let α ≥ 1 + 1/m1−² , and let θ ≥ 1 be such
α
that max( α−1
· Nθ , (α + 1)(1 + logα θ)) = poly(m). If there is a polynomial time algorithm
for KWP(α, θ), then every function in #P is computable in polynomial time.
Proof: For N, α and θ as described in the theorem a routine calculation shows that
1 + 1/m1−² ≤ α ≤ poly(m)

and

2m
≤ θ ≤ 2poly(m) .
poly(m)

(1)

The proof is a reduction from the problem M2SAT. The high level idea of the proof is
simple: let (F, K) be an instance of M2SAT where F is defined over variables y1 , . . . , yn . The
Winnow algorithm maintains a weight wTφ for each monomial T over variables x1 , . . . , xn . We
define a 1-1 correspondence between these monomials T and truth assignments y T ∈ {0, 1}n
349

Khardon, Roth, & Servedio

for F, and we give a sequence of examples for Winnow which causes wTφ ≈ 0 if F (y T ) = 0
and wTφ = 1 if F (y T ) = 1. The value of w φ · φ(z) is thus related to |F −1 (1)|. Note that
if we could control θ as well this would be sufficient since we could use θ = K and the
result will follow. However θ is a parameter of the algorithm. We therefore have to make
additional updates so that w φ · φ(z) ≈ θ + (|F −1 (1)| − K) so that w φ · φ(z) ≥ θ if and
only if |F −1 (1)| ≥ K. The details are somewhat involved since we must track the resolution
of approximations of the different values so that the final inner product will indeed give a
correct result with respect to the threshold.
General setup of the construction. In more detail, let
• U = n + 1 + d(dlogα 4e + 1) log αe,
n+1
• V = d log
α e + 1,
U +2
• W = d log
αe + 1

and let m be defined as
m = n + U + 6V n2 + 6U W + 3.

(2)

Since α ≥ 1 + 1/m1−² , using the fact that log(1 + x) ≥ x/2 for 0 < x < 1 we have that
log α ≥ 1/(2m1−² ), and from this it easily follows that m as specified above is polynomial in
n. We describe a polynomial time transformation which maps an n-variable instance (F, K)
of M2SAT to an m-variable instance (S, z) of KWP(α, θ) where S = hx 1 , b1 i, . . . , hxt , bt i
is monotone consistent, each xi and z belong to {0, 1}m , and wφ · φ(z) ≥ θ if and only if
|F −1 (1)| ≥ K.
The Winnow variables x1 , . . . , xm are divided into three sets A, B and C where A =
{x1 , . . . , xn }, B = {xn+1 , . . . , xn+U } and C = {xn+U +1 , . . . , xm }. The unlabeled example z
is 1n+U 0m−n−U , i.e. all variables in A and B are set to 1 and all variables in C are set to 0.
P
P
We thus have w φ ·φ(z) = MA +MB +MAB where MA = ∅6=T ⊆A wTφ , MB = ∅6=T ⊆B wTφ and
P
MAB = T ⊆A∪B,T ∩A6=∅,T ∩B6=∅ wTφ . We refer to monomials ∅ 6= T ⊆ A as type-A monomials,
monomials ∅ 6= T ⊆ B as type-B monomials, and monomials T ⊆ A∪B, T ∩A 6= ∅, T ∩B 6= ∅
as type-AB monomials.
The example sequence S is divided into four stages. Stage 1 results in MA ≈ |F −1 (1)|;
as described below the n variables in A correspond to the n variables in the CNF formula
F. Stage 2 results in MA ≈ αq |F −1 (1)| for some positive integer q which we specify later.
Stages 3 and 4 together result in MB + MAB ≈ θ − αq K. Thus the final value of w φ · φ(z) is
approximately θ + αq (|F −1 (1)| − K), so we have w φ · φ(z) ≥ θ if and only if |F −1 (1)| ≥ K.
Since all variables in C are 0 in z, if T includes a variable in C then the value of wTφ
does not affect w φ · φ(z). The variables in C are “slack variables” which (i) make Winnow
perform the correct promotions/demotions and (ii) ensure that S is monotone consistent.
Stage 1: Setting MA ≈ |F −1(1)|. We define the following correspondence between
truth assignments y T ∈ {0, 1}n and monomials T ⊆ A : yiT = 0 if and only if xi is not
present in T. For each clause yi1 ∨ yi2 in F, Stage 1 contains V negative examples such that
xi1 = xi2 = 0 and xi = 1 for all other xi ∈ A. We show below that (1) Winnow makes a
false positive prediction on each of these examples and (2) in Stage 1 Winnow never does a
350

Efficiency versus Convergence of Boolean Kernels

promotion on any example which has any variable in A set to 1. Consider any y T such that
F (y T ) = 0. Since our examples include an example y S such that y T ≤ y S the monomial T
is demoted at least V times. As a result after Stage 1 we will have that for all T , w Tφ = 1
if F (y T ) = 1 and 0 < wTφ ≤ α−V if F (y T ) = 0. Thus we will have MA = |F −1 (1)| + γ1 for
some 0 < γ1 < 2n α−V < 21 .
We now show how the Stage 1 examples cause Winnow to make a false positive prediction
on negative examples which have xi1 = xi2 = 0 and xi = 1 for all other i in A as described
above. For each such negative example in Stage 1 six new slack variables xβ+1 , . . . , xβ+6 ∈ C
are used as follows: Stage 1 has dlog α (θ/3)e repeated instances of the positive example which
has xβ+1 = xβ+2 = 1 and all other bits 0. These examples cause promotions which result
in θ ≤ wxφβ+1 + wxφβ+2 + wxφβ+1 xβ+2 < αθ and hence wxφβ+1 ≥ θ/3. Two other groups of
similar examples (the first with xβ+3 = xβ+4 = 1, the second with xβ+5 = xβ+6 = 1) cause
wxφβ+3 ≥ θ/3 and wxφβ+5 ≥ θ/3. The next example in S is the negative example which has
xi1 = xi2 = 0, xi = 1 for all other xi in A, xβ+1 = xβ+3 = xβ+5 = 1 and all other bits 0.
For this example w φ · φ(x) > wxφβ+1 + wxφβ+3 + wxφβ+5 ≥ θ so Winnow makes a false positive
prediction.
Since F has at most n2 clauses and there are V negative examples per clause, this
construction can be carried out using 6V n2 slack variables xn+U +1 , . . . , xn+U +6V n2 . We
thus have (1) and (2) as claimed above.
Stage 2: Setting MA ≈ αq |F −1(1)|. The first Stage 2 example is a positive example
with xi = 1 for all xi ∈ A, xn+U +6V n2 +1 = 1 and all other bits 0. Since each of the 2n
monomials which contain xn+U +6V n2 +1 and are satisfied by this example have wTφ = 1,
we have w φ · φ(x) = 2n + |F −1 (1)| + γ1 < 2n+1 . Since θ > 2m /poly(m) > 2n+1 (recall
from equation (2) that m > 6n2 ), after the resulting promotion we have w φ · φ(x) =
α(2n + |F −1 (1)| + γ1 ) < α2n+1 . Let
q = dlogα (θ/2n+1 )e − 1
so that
αq 2n+1 < θ ≤ αq+1 2n+1 .

(3)

Stage 2 consists of q repeated instances of the positive example described above. After
these promotions we have w φ · φ(x) = αq (2n + |F −1 (1)| + γ1 ) < αq 2n+1 < θ. Since 1 <
|F −1 (1)| + γ1 < 2n we also have
αq < MA = αq (|F −1 (1)| + γ1 ) < αq 2n < θ/2.

(4)

Equation (4) gives the value which MA will have throughout the rest of the argument.
Some Calculations for Stages 3 and 4. At the start of Stage 3 each type-B and typeAB monomial T has wTφ = 1. There are n variables in A and U variables in B so at the
start of Stage 3 we have MB = 2U − 1 and MAB = (2n − 1)(2U − 1). Since no example in
Stages 3 or 4 satisfies any xi in A, at the end of Stage 4 MA will still be αq (|F −1 (1)| + γ1 )
and MAB will still be (2n − 1)(2U − 1). Therefore at the end of Stage 4 we have
wφ · φ(z) = MB + αq (|F −1 (1)| + γ1 ) + (2n − 1)(2U − 1).
351

Khardon, Roth, & Servedio

To simplify notation let
D = θ − (2n − 1)(2U − 1) − αq K.
Ideally at the end of Stage 4 the value of MB would be D − αq γ1 since this would imply that
wφ · φ(z) = θ + αq (|F −1 (1)| − K) which is at least θ if and only if |F −1 (1)| ≥ K. However it
is not necessary for MB to assume this exact value, since |F −1 (1)| must be an integer and
0 < γ1 < 21 . As long as
1
(5)
D ≤ MB < D + α q
2
we get that
1
θ + αq (|F −1 (1)| − K + γ1 ) < wφ · φ(z) < θ + αq (|F −1 (1)| − K + γ1 + ).
2
Now if |F −1 (1)| ≥ K we clearly have w φ · φ(z) ≥ θ. On the other hand if |F −1 (1)| < K
then since |F −1 (1)| is an integer value |F −1 (1)| ≤ K − 1 and we get w φ · φ(z) < θ. Therefore
all that remains is to construct the examples in Stages 3 and 4 so that that M B satisfies
Equation (5).
We next calculate an appropriate granularity for D. Note that K ≤ 2 n , so by Equation (3) we have that θ − αq K > θ/2. Now recall from Equations (2) and (1) that m >
2
n + U + 6n2 and θ > 2m /poly(m), so θ/2 ≥ 2n+U +6n /poly(m) À 2n 2U . Consequently we
certainly have that D > θ/4, and from Equation (3) we have that D > θ/4 > α q 2n−1 > 14 αq .
Let
c = dlogα 4e,
so that we have
1
αq−c ≤ αq < D.
4

(6)

There is a unique smallest positive integer p > 1 which satisfies D ≤ pαq−c < D + 41 αq . The
Stage 3 examples will result in MB satisfying p < MB < p + 14 . We now have that:
1
αq−c < D ≤ pαq−c < D + αq
4
3 q
≤ θ− α
4
≤ αq+1 2n+1 − 3αq−c
= α

q−c

· (α

c+1 n+1

2

− 3).

(7)
(8)
(9)

Here (7) holds since K ≥ 1, and thus (by definition of D) we have D + αq ≤ θ which is
equivalent to Equation (7). Inequality (8) follows from Equations (6) and (3).
Hence we have that
1 < p ≤ αc+1 2n+1 − 3 ≤ 2n+1+d(c+1) log αe − 3 = 2U − 3,

(10)

where the second inequality in the above chain follows from Equation (9). We now use the
following lemma:
352

Efficiency versus Convergence of Boolean Kernels

Lemma 10 For all ` ≥ 1, for all 1 ≤ p ≤ 2` − 1, there is a monotone CNF F`,p over
` Boolean variables which has at most ` clauses, has exactly p satisfying assignments in
{0, 1}` , and can be constructed from ` and p in poly(`) time.
Proof: The proof is by induction on `. For the base case ` = 1 we have p = 1 and F `,p = x1 .
Assuming the lemma is true for ` = 1, . . . , k we now prove it for ` = k + 1 :
If 1 ≤ p ≤ 2k − 1 then the desired CNF is Fk+1,p = xk+1 ∧ Fk,p . Since Fk,p has at most k
clauses Fk+1,p has at most k + 1 clauses. If 2k + 1 ≤ p ≤ 2k+1 − 1 then the desired CNF is
Fk+1,p = xk+1 ∨ Fk,p−2k . By distributing xk over each clause of Fk,p−2k we can write Fk+1,p
as a CNF with at most k clauses. If p = 2k then Fk,p = x1 .
2
Stage 3: Setting MB ≈ p. Let FU,p be an r-clause monotone CNF formula over the
U variables in B which has p satisfying assignments. Similar to Stage 1, for each clause
of FU,p , Stage 3 has W negative examples corresponding to that clause, and as in Stage
1 slack variables in C are used to ensure that Winnow makes a false positive prediction
on each such negative example. Thus the examples in Stage 3 cause MB = p + γ2 where
0 < γ2 < 2U α−W < 41 . Since six slack variables in C are used for each negative example
and there are rW ≤ U W negative examples, the slack variables xn+U +6V n2 +2 , . . . , xm−2 are
sufficient for Stage 3.
Stage 4: Setting MB + MAB ≈ θ − αq K. All that remains is to perform q − c
promotions on examples which have each xi in B set to 1. This will cause MB to equal
(p + γ2 )αq−c . By the inequalities established above, this will give us
1
1
D ≤ pαq−c < (p + γ2 )αq−c = MB < D + αq + γ2 αq−c < D + αq
4
2
which is as desired.
In order to guarantee q − c promotions we use two sequences of examples of length
−n
U −n
q − dU
log α e and d log α e − c respectively. We first show that these are positive numbers. It
follows directly from the definitions U = n + 1 + d(dlog α 4e + 1) log αe and c = dlogα 4e
−n
6n2 (by definition of m and Equation (1)) and α is bounded
that U
log α ≥ c. Since θ > 2
by a polynomial in m, we clearly have that log(θ/2n+1 ) > U − n + log(α). Now since
n+1 )
U −n
−n
q = dlogα (θ/2n+1 )e − 1 this implies that q > log(θ/2
−1 > dU
log α e, so that q − d log α e > 0.
log(α)
−n
The first q − d U
log α e examples in Stage 4 are all the same positive example which has
each xi in B set to 1 and xm−1 = 1. The first time this example is received, we have
2
wφ · φ(x) = 2U + p + γ2 < 2U +1 . Since θ > 26n , by inspection of U we have 2U +1 < θ, so
−n
Winnow performs a promotion. Similarly, after q − d U
log α e occurrences of this example, we
have
q−d U −n e
q−d U −n e
wφ · φ(x) = α log α (2U + p + γ2 ) < α log α 2U +1 ≤ αq 2n+1 < θ

so promotions are indeed performed at each occurrence, and
MB = α

−n
q−d U
e
log α

(p + γ2 ).

−n
The remaining examples in Stage 4 are d U
log α e − c repetitions of the positive example x
which has each xi in B set to 1 and xm = 1. If promotions occurred on each repetition of

353

Khardon, Roth, & Servedio

this example then we would have w φ · φ(x) = α

−n
dU
e−c
log α

(2U + α

−n
q−d U
e
log α

(p + γ2 )), so we need

only show that this quantity is less than θ. We reexpress this quantity as α
αq−c (p + γ2 ). We have

−n
e−c U
dU
log α
2

1
αq−c (p + γ2 ) < pαq−c + αq−c
4
3 q
1
≤ θ − α + αq
4
16
1 q
< θ− α
2

+

(11)

d U −n e−c

where (11) follows from (7) and the definition of c. Finally, we have that α log α 2U ≤
1
θ
α · 22U −n−c log α < α · 22U −n−2 < 2α
< 21 αq , where the last inequality is by Equation (3)
2n+1
and the previous inequality is by inspection of the values of α, θ and U . Combining the two
bounds above we see that indeed w φ · φ(x) < θ.
Finally, we observe that by construction the example sequence S is monotone consistent.
Since m = poly(n) and S contains poly(n) examples the transformation from M2SAT to
KWP(α, θ) is polynomial-time computable and the theorem is proved.
2(Theorem 9)

5. Conclusion
Linear threshold functions are a weak representation language for which we have interesting learning algorithms. Therefore, if linear learning algorithms are to learn expressive
functions, it is necessary to expand the feature space over which they are applied. This
work explores the tradeoff between computational efficiency and convergence when using
expanded feature spaces that capture conjunctions of base features.
We have shown that while each iteration of the kernel Perceptron algorithm can be
executed efficiently, the algorithm can provably require exponentially many updates even
when learning a function as simple as f (x) = x1 x2 . . . xn . On the other hand, the kernel
Winnow algorithm has a polynomial mistake bound for learning polynomial-size monotone
DNF, but our results show that under a widely accepted computational hardness assumption
it is impossible to efficiently simulate the execution of kernel Winnow. The latter also implies
that there is no general construction that will run Winnow using kernel functions.
Our results indicate that additive and multiplicative update algorithms lie on opposite
extremes of the tradeoff between computational efficiency and convergence; we believe that
this fact could have significant practical implications. By demonstrating the provable limitations of using kernel functions which correspond to high-degree feature expansions, our
results also lend theoretical justification to the common practice of using a small degree in
similar feature expansions such as the well-known polynomial kernel.2
Since the publication of the initial conference version of this work (Khardon, Roth, &
Servedio, 2002), several authors have explored closely related ideas. One can show that our
construction for the negative results for Perceptron does not extend (either in the PAC or
2. Our Boolean kernels are different than standard polynomial kernels in that all the conjunctions are
weighted equally, and also in that we allow negations.

354

Efficiency versus Convergence of Boolean Kernels

online setting) to related algorithms such as Support Vector Machines which work by constructing a maximum margin hypothesis consistent with the examples. The paper (Khardon
& Servedio, 2003) gives an analysis of the PAC learning performance of maximum margin
algorithms with the monotone monomials kernel, and derives several negative results thus
giving further negative evidence for the monomial kernel. In the paper (Cumby & Roth,
2003) a kernel for expressions in description logic (generalizing the monomials kernel) is
developed and successfully applied for natural language and molecular problems. Takimoto and Warmuth (2003) study the use of multiplicative update algorithms other than
Winnow (such as weighted majority) and obtain some positive results by restricting the
type of loss function used to be additive over base features. Chawla et al. (2004) have
studied Monte Carlo estimation approaches to approximately simulate the Winnow algorithm’s performance when run over a space of exponentially many features. The use of
kernel methods for logic learning and developing alternative methods for feature expansion
with multiplicative update algorithms remain interesting and challenging problems to be
investigated.

Acknowledgments
This work was partly done while Khardon was at the University of Edinburgh and partly
while Servedio was at Harvard University. The authors gratefully acknowledge financial
support for this work by EPSRC grant GR/N03167, NSF grant IIS-0099446 and a Research Semester Fellowship Award from Tufts University (Khardon), NSF grants ITR-IIS00-85836, ITR-IIS-0085980 and IIS-9984168 (Roth), and NSF grant CCR-98-77049 and NSF
Mathematical Sciences Postdoctoral Fellowship (Servedio).

References
Angluin, D. (1990). Negative results for equivalence queries. Machine Learning, 2, 121–150.
Block, H. (1962). The perceptron: a model for brain functioning. Reviews of Modern
Physics, 34, 123–135.
Carlson, A., Cumby, C., Rosen, J., & Roth, D. (1999). The SNoW learning architecture.
Tech. rep. UIUCDCS-R-99-2101, UIUC Computer Science Department.
Chawla, D., Li, L., & Scott., S. (2004). On approximating weighted sums with exponentially
many terms. Journal of Computer and System Sciences, 69, 196–234.
Cristianini, N., & Shaw-Taylor, J. (2000). An Introduction to Support Vector Machines.
Cambridge Press.
Cumby, C., & Roth, D. (2003). On kernel methods for relational learning. In Proc. of the
International Conference on Machine Learning.
Kearns, M., & Vazirani, U. (1994). An Introduction to Computational Learning Theory.
MIT Press, Cambridge, MA.
355

Khardon, Roth, & Servedio

Khardon, R., Roth, D., & Servedio, R. (2002). Efficiency versus convergence of Boolean
kernels for on-line learning algorithms. In Dietterich, T. G., Becker, S., & Ghahramani,
Z. (Eds.), Advances in Neural Information Processing Systems 14, Cambridge, MA.
MIT Press.
Khardon, R., & Servedio, R. (2003). Maximum margin algorithms with Boolean kernels. In
Proceedings of the Sixteenth Annual Conference on Computational Learning Theory,
pp. 87–101.
Lint, J. V. (1992). Introduction to Coding Theory. Springer-Verlag.
Littlestone, N. (1988). Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm. Machine Learning, 2, 285–318.
Maass, W., & Warmuth, M. K. (1998). Efficient learning with virtual threshold gates.
Information and Computation, 141 (1), 378–386.
Minsky, M., & Papert, S. (1968). Perceptrons: an introduction to computational geometry.
MIT Press, Cambridge, MA.
Novikoff, A. (1963). On convergence proofs for perceptrons. In Proceeding of the Symposium
on the Mathematical Theory of Automata, Vol. 12, pp. 615–622.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Rosenblatt, F. (1958). The Perceptron: a probabilistic model for information storage and
organization in the brain. Psychological Review, 65, 386–407.
Roth, D. (1998). Learning to resolve natural language ambiguities: A unified approach. In
Proc. of the American Association of Artificial Intelligence, pp. 806–813.
Sadohara, K. (2001). Learning of Boolean functions using support vector machines. In Proc.
of the Conference on Algorithmic Learning Theory, pp. 106–118. Springer. LNAI 2225.
Takimoto, E., & Warmuth, M. (2003). Path kernels and multiplicative updates. Journal of
Machine Learning Research, 4, 773–818.
Valiant, L. G. (1979). The complexity of enumeration and reliability problems. SIAM
Journal of Computing, 8, 410–421.
Valiant, L. G. (1984). A theory of the learnable. Communications of the ACM, 27 (11),
1134–1142.
Watkins, C. (1999). Kernels from matching operations. Tech. rep. CSD-TR-98-07, Computer
Science Department, Royal Holloway, University of London.

356


	
 
			 ! #"$ % 
'&)(
*)+,(--.0/1((!2'34(5!2

64789  :)-;0<
-
*=?>7	%&:@-A0<
-.

BDC4EGFHCEGIKJHLNMPORQTSUJWVGMPQ9LXJZY[O]\GQ^Y[\GORL`_bac\dEHegfhC4ijLlknmWEGMoipC0Q9qr_sMoEGt
uvORwyxdz0L{i

k|C}pQN\dzYgm~v

JZY[LVfwMPORtkLilmWq$YRC0mWC4EGIfMoxd\
JHLNMPORQ#S

RR1^?
«­¬?®g¯?°¬y±y¬²°³®g¬?´gµ¬·¶
³¸¹¬?²'³?¸°º»
¼9½¾N½1¿ ³ÀGÁ?ÂÄÃ·ÃÅTÆG«Ç·ÇÇ·Ã
È ´©¶ÉËÊ0ÉËº¸¹ÊÉËºÅ1±PÆÌÂËÍ!Ç·ÂÁÎ ÇÇ·ÇÃÐÏ1« È

) $¡4¢£¤¡1£$¥1¦,#§©¨¢gª

ÑyÒrÓÔ1ÕÕÄg'{×Ö#ØÙ

ÛÚ$¦,ÜÄÝÞR¤ß­¡§©ß­¢1Ü¢¡0Ä Ý$§àÝ¥á

âã ­äåÒræDgçG

Ú1¢gÛÝg¤ß­¡§©ß­¢1Ü¢¡0Ä Ý$§àÝ¥á

è]³éêgÉ ²ëº0¸@«ì0°º®gì
º)íyºëêË¬¸²éîº®Ë²
è]³´©³?¸¹¬·¯³H«²ë¬²ëºGÏ®°ïº¸
»°²ð
ñ ³¸²)èR³?´´°®»
Åè ¾

ÂÃÁòÄóDÏ« È

ôöõ)÷·ø ùúRûgø
üý?þËÿ!ý
	
	4
 ÿ 	! 	!"#$% ý 	&'	&%(*Ä) ÿËý$	,+- $ ÿ% 	." 
0/%	oý 
ý1+3245(6Ëý/7yþ	ý8  	,ý$9:%	
	;4ý8	<:·þ$=?>@? :	ABBCD	!/%:4ý/
¹ÿE?FG
ý?þÿH!ýJIK !ý-Ëý1sýL:MWÿE/%?ýE/%	&NOZýM+QP?+R  	!ý$
ý:?S5 	!^ý$&	 TIU	E$©ÿ./%	E&B
%/%$!5 	&%(V$?$.¹ÿE%(W	
G/%+MËý
	!TýE/=	!/%U	A(
X 4 ý8	0Y '!ý0 Ë: ýG#4 ý8 ÿ?Z  ý8	%Ë[ ÿ?ESO:E  þ$?\/%	]F0 ÿ$,+>5^_'SE 	/%?K 
)Äÿ,	E	<
SUS`M5 a,· þ(V :b  / ÿ$	E&:· þ$dcWeUf%4Zg­I ý0/Z ý	<CD? B ý?þËÿ
 ýo ý$&	A=Ëý1T þ.?h 8CDi þ3Z ýP ý?þ$+	>Y43 ?M	 ÿE$+ICZ/%?-E 
 ý8 /
4
Ë ý8 -
4 ýb/%	ýESy þ.,C?<
 ý./%j$%4 ý$9:% 	Z ýHýE/T Eh! ý?: 	î ý$k $ ÿ%	7	
	&E$+lS ,$ ý8?/bCD	AN:· þ$m/%	A]F ÿ$A,+b# ýTC?$$(VPUE8CDb
 ý?þÿN 4 ý ý$& 	 En#B 
eMf%4L	U M/ ÿESy/ þMî+ üý	$,$ ý/>Yo*
	!3:Ëý:.??IC/%	0ÿ. "ý 	· ÿE3S-	&U"p E	3-! ý¹ÿ
ýE/b/?$
: ýq?Cr
%/%$3:E  þ$?s/%	]F0 ÿ$,+7  ý8BS ?S E? 
/ X 	?ES	??>
^bF 8C
Ë ý8] üý	$,$ ý/t ý$&	Au ý` þK-%/%?$/BCD	 
	& X /%?$	,`+ ýýG 	
:$K ý ,	 ýUv­ ýG 
 ý	&U (
# rC ý/<
 ýE/nrC ý$PpÛ> üK= ýE/%n^C ý$Pw-%/%?T$ ý?S ÿU#'E4 ý$s
+ ý$$5Z = ý ,	 ý?þ	$	A,+
	R7SU,- ) ÿE	?/N _$% ý å
 þ. Q:%	Z ýr$ ýE/Q ÿþ(6:% 	Z ý$K©$ ÿ%	Eh _ ýE/%xeUf%4I
ýE/_: 8M	!/%?`ýlS`%:,$ ýË
 ý 	_#h/%	Ap.S?'	l/%	]F0 ÿ$,+bD ýE/%s¹ÿEO,¹ ÿ.¹ ÿE /
eMf%43?>Yyk	Ë ý$$+ICZD/%	0ÿ. [ý./B?-:E		4 ý$$+'¹ÿþEýU ,	 ý8 ,CZ8$M:?/%	!	Ek ?& ý/%	& ý?þËÿ
 ýH ý$&	A
4
 þ.?Ë ý1M	?>"yk	?IU EG
 %/#DSE,¹ ÿ. 	&h i		 ,	 ý$J $ ÿ% 	0	Z	&$+
ÿE$	P?$+- -	-
: ý G: #î ýES
 ý?þÿq!ýJ>kfMSE/IU ý·þÿF ÿ · ÿ$!Z/ þG $?S /
d
 þ.H ý' î ý$#$ ýO:	 þ$=CD	$ 	` ÿ$ ý ÿE$b
+ ý1	!/%	E&T 4 ýlý&Ë ý8	Jzk ý©$ ÿE?O$,ý&?
Ë ýTS?ý+=$! ý/ =	&E	 X 4 ýU{/%& ý/ ý8	EZ	T: #Z ýE>
ý

"| }7~U ø ùD ûgø, 
RD%Y9S%9J#w8J 0J%'1p8#w1JS8M9iS1.M.SQ,i8<%5. F%8W¡
¢%Y£n¤Zp%Z¥¦%¦p¥M§"O#MS¢kJ¨%¢kv©-.8kª%1«E .S%v¤7kv£¬­%#S"3¥¦%¦%®§3¯p°%8Y©-.8
£x¯p<%#*5±M²%²%²E³?´<¤'<< 2 M<S°QR%819EF6%=S7MM#M9#NµM¶1
%98%w6%iB%O7%S-%°%%S9<{lF·h#'%J°%h%59S%9nJS%J1G%J_J%
p#
¸¹JºT»¼,»½!»¼5¾,¿iÀO¿Á»ÂÄÃ5¿?½Å¼,¿ÆUÂÄ»ÀuÁÇÉÈOÊËUÂÉ¾WÌ-ÍÃÎÏÐÑkÒOÏÓ8ÔÕ Ð¾,Ö¼,¿Ë×Ö¿Ë¾"¾,ÖÇÄÃ3ÅUÍ?Å%»¼¹
Ø

YÙ

UÚ

WÛ UÚ

(--.^g %% !: ­		Ä&0%1% 0:

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

S°9	98%.h#J°E-#.bSTSMSJSJ#Rv8·G8 MµS9%17J1S%k9S%9 Jw89%
%JN%#°%%SJ vJ%´
«S9#3YMk8U·GO#.%S°MO1D7p#%JSM9qMT6%KS
%¡%7S9J##°BJS%¡
  .¯ i³?´¤'J
 .¯  #q%
 {¡JM 19JM%#%%JS#<	 MS#¶J%9 %9¶J%qv17

%ZS=S%JJMl%JL<i·B#<SSJJ_JS%Y<G#lS9J##°0SM?"´¤ZM9lM?
%°%%#SJ<MBS8°MJw%G%7°qSB7G v1S%-M9JS%J6%°%J8MS#°qJ°p¡ J%# 
#JSJG7S .¯   %#N£R88%"3¥¦%¦%¦³'%JL8SESwS8JS.OS=SM1¡,%W¡,S1¡MS-.
7176%SSM9FwMS°#l%8hSF8S'17v8SS­ h·B#¢£s¯pqSJ#¢*±M²%² § i# 8 8·h# %
-<S?¢%%v
£ ZS"K¥¦%¦%®³?´'¬ J#=MJ8BJ%q%?J8%Q1JS#J8M9F%M%J1-#Lv8¡
6%<%J1_S#J1<¤Z%###M?!
 JJ7JM¶S< v1S%Sq%iSM9HM %°%%#SJ<=6%
S ¯ u#µ¥#¦ "%¦k179MMS%#_##Sh9S%°%SS'9%Ov8L<%=·iMNJ8%%9#°7%N9J8¡
S%J9#°q%· S=%°%%#SJ<G·%¢YJ*´É%´J?JM%18$ #°7S=JJJ8p#°J#°p¡8%vM?
pJ%<#8JJ8SS%JJ#°B·BEqS'JJ%w#8KMS'
 k1S#%BTS% ¯ ZM%97JJ8#J°-·
S09%<#8O<°.OvF7pJ	 J_<p#lVSS8'7JS%7ESB#lv8SW%?<%J1%´
«h#B·!¡,¢J·BLS9MhSqJ1SJSF%DSqMQ9%1T# &YJ1OJ%9xJ#w8J b6%
SM9HM %J¶%S80p8%SM %°%%S9<w ''88%'¥¦%#¦ "³?´ªGJ( .S%Zª<#9%E
MJJ%b08%%Y#°1i7p#{{0ES#WqShWMS%"S-M?bY%1BSJMiMS
J°9N1%SS#M ·hS M¶18´¬µ<J%7v8SW%?7Q1J%7%J%p
%STSMSp¡
SJ#wv8·G8lUM?JiM?L9%1
WMSJSi%J_JS%9mJb8J76%O¤%##M* )ÄSM9lM?
%°%%#SJ 6%S+
 .¯  6¬­MS", ¢Y. -h·G%{£d¬ JS8%D±M²%²¥M{±M²%² /³?1
´ 0=2 9J9#°
·G8S
#MS°%#_8°MS#%# 3G<%.lWMSJS'SJMhMSF·B#Jwv#8%l_# &9JJ1
JS%9d9w8J w6%
p8%M?7MS%.#qV%18J0·M¢pq1%SS#M7·hSqJS%9jJb8J%´K¨MS{SJ?7%DS
Jv8D%v%JS#w%J#S#J',i#MS¢q8%,´Y¥¦%¦%®³?pS'9%¢.vhS	 8,¯p#%J8T£ ¬­%#S"p±M²%²¥³?
%JwSB7%bJ#S%J1Bk8 ·8b%JJnp8%%JS#<b*RM6#" i8S·BS"£  =%J68k¥¦%¦%¦³
%81J.h6%h#SOSJ%ª<SJ?l%ZSJ%S%UMM9## b#RMª1-W%-¤%##M* )ÄO%°%%?SJL´
« 1.%8i·B#°ªW SM?  JS%9 Jb8J¶W%7p8%iMr%J 5
 46B¡¯ 4D¤
,¯p#J°%8=8F%*´±M²%²%²E³?Z·<6JJQSJM=SJ<7%­J#SS%J17v8·G8µ%J #8%K%JS<L%J
S_MSF%9S#<%S#S"3·BJ#?µ·_% 78 9:(;=<>9:(;*K#9°JQ1%SS#M¶·BS­9S%9
Jb8J%v%81J.S#°l6%FM
#%
± ?@/_%Sq%S%DUM?#M9###L#QMµ1S76¬­MSS­8=%*´
±M²%² /³?´N¬µb6S879MHSJM 7A8 9:B;C<D9:(;%81J.SqW%q0J?¶%iSbUM?#M9###ªµS
1D%vp8MS#°=S¡,%9S#<%#S9=SJE ¯ Z%%JTW%{9= k8J1D#qSiS#MS%'9w8J 
%G FH JMS( I7%8JJ
 FS1S%°9#MK IL¯ 8´
h8%8SS8S 7A8 9:B;C<D9:(;O1<7p9%78%8%OS%S1<#°8´­¨K?8{S_1vJS_%
179JS#° 7A8 9M:(;=<>9:(;K##<R=%J%ph_#MS%R<%#59S%9 #JS%918Y?%#S#°b1p¡
18J-S8°M?J#°_S8%#M9#lL7%SqS%##SS#8%#lS	 8QJS%Y #9S%J18´F¯1J"kSSJJ%#
JJJ8qSJ 78 9:(;=<>9:(;O7pOMSN#M°%_6%wµJ0k8T%hJS%9 #9S%J18G%J SN7pi#
%F%88JM<6%=S<7=Jb8J-JS%9 #9S%J18´q¤'J5Yv8%J7SJ 7N8 9:(;=<>9:(;D7p
JShJ_J#S1-JS°.h#._STJp¡,S#<vJp%-%¤Z%###M?* )ÄB%°%%?SJLk·78SES#
<%hJJ8SS%JP
 ORQTS 78 9:(;=<>9:(;5#B<J°J#b1%SSML·BSLM?R1S8´
¬µ=#.SpJJ1Bq%v1i7pJS9MG1%S1SiW%GShM6%S7.SlU Y8J8%5S
78 9:(;=<>9M:(;p1SK<,´¤'J7p*M·B9#09%TT
8S%#q%J%pS#%9SJG?Jp¡,S#7vJp%
%¤Z%##M* )Ä
%°%%S9L"#
S<MS¢MM9ª%88M%%819ES#J°LW%=%80#¦ V %GSTUMM9## 
#RS01h%{p8MS#°wv%SR%JS#<%%JRSJ¡,%JS#<%3#S9O_w·B#=?%°%0%D9S%9
#JSS%J18´R%Sv8	 98%%p·FSSM9##SlSJ=W##·B#J°qSJSU 3
WBWBW

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

¥M´-¯pM<7¤%##M*)ÄZ%°%%S9MJvMK=vGv1S%TSS#1T=SJ¡9%18 9:(;
%7SJlSJMª1ES%#9Lv%S p8%
%JS#< %J\#S9lSJMRMS­%8 8#µ
VSk8$ 98%#%"¥¡ ±<<%-·iL6Sb³Gp8%"%JS#wp´
±´B¤Z%##M* )ÄF%°%%?SJ 8%Hvw7p¶·BS­<MS¢MM9J
 9J# %qRMM#%.%'ªS#T¡
Y<1¡J7JS9%?%J ·i%¢ %8qS
  8 9:B; S¡Y%1%´_¤'J<·G%#¢ª1J9#S=·G
J%SM9TW%?<
%Y#%
#QS7%JS#SR9S%9M9#S8´F¨KS8kSJ7JS%9M9#L%GM?
<p#°08#8l66MS86Sb³DShJMS%9S#<%9S#Sb#{J%k%SJ%5V#.%8
9S%v%SSJ%W³<uS­8JSS.NJ#S%J1µ6S S­JMSN%JS<%=#S#"´d¯p1J"
SMb1p99SK77.SJ 3D{{7%SB¢%07%-88l66MS8{6Sb³ZShM¡
S0%JS<%S¶#iSM JS8pJQ<% 88T 66MSSJ806Sb³=SbM
%9S#<%5S"´{R%S8%8kSF?%Jd·i%¢l7J"%81JESh6%BMB%-¦%® V %KS
MM#MY## w#lS<%NM?R1-%1SSBT?%°%F%KB#9S%J18´
/´B¤Bi%Jj·i%¢07pp#Z( J%#%88JM'6%D%JL%·G%S¢ &J·E%J &J·hS%L
 .¯ 8´
-h·G8%8< 
%J= v8SJ1{1p#S{#TSBJ0k8D%vSMG#TSO·i%¢k.*´É%´.S'<U#<%
vS9=J#SS%J1Fv8·G8Rb#SL%9NSJMSB%9S#<%5S"´ 
=v8SJ1h#
SJb<U#<%J#S%91FVJ#Q%81JEq6%·G#!¡,¢p·B¶J= v8SJ1µSb9w8J ª%
9S%9d#9S%J1'?·Bl6SdSSFUM?J'SJ¡8#%S´
´B¤B7%88%1Q%S7%9 ·G%¢Q7pZ%J68-NlSM9 M?­%°%%S9 Y%
­S7v·G8S69 H7%b%k8?M%5·BJ #=7%S<8#QS#M RSM1¡,%W¡,S1¡MS
SMYNM?R%°%%#SJ<O6%'S
 ¯ uSJ%R¤%##M* )Äi%°%%#SJL´
´B¤B<%JJ ·G%#¢ <1%SS1S JS9#1S=SJMq#JS#MS°_¤Z%##M* )Ä=%°%%#SJ 6S
9°p¡ J%# ­SMSS#°µSJ0·B##JH#7JS%lk8W%w%J1_#'SL#JSJTMS
%8l8#F<SFJMS'%JS<%5#JS"´
®´B¤BB%9n·i%¢77p91%S1S7JSJ1S{SJMG%E7SM9bShMS°%8SJ%bS-<#J!¡
qJ( .J#ST%<M?wSM°JMS#w#{¢%0#J1S%OSJOW%1S#7%vS'M?
S9%1T19#%SRR¤Z%###M?* )ÄB%°%%?SJLv%JQ%=b1JS( .91qp#Q_8
  "!#w#
9S%9 9w8J %´<«W%?<%#%5SJw8#<ES%JMSS7%'#M°%<SM9µS0#F%6
1p9#%¶S79ª.µ%9S8S°NSJMT#MS°%b.0#<9%10M? SS°¶Sb#S%
&91pY## w%JlSFSJS#J°q#9M9## <w8MS8VJ##w1p9%S
SS9%1F%K°.k%?#°TM¡
SJ8´ 0
DSJSKJSOF7%S'8S%7%J<1J18B%81J.%kSJ#K9J7
lS1.1h%ZSJ .¯ Z´
¤'JRS<%J8<%FS9#<9Mv8_#<%S°%9	 8 %_6#·B8´ ¬µQk8°r# ¯1SJ_±¶%J /
·BSªw1JSN%DS ¯ Z9¤%##M* )ÄB%°%%SJN9%JNSJJS%Y #9S%J1B9R#N
%J%pS#´T¤B<Ev%SS#=9J8p#°b%J%#S=#F8S%##µ#¶¯1S$
 ´w¬µbSJw<M	 8
%J 1#S$ .J_9%7SM?  9S%9 J#w8J ­W%wSM9uM?r%°%%?SJ<T6%<S5
 ¯ 
# ¯1S ´µ¯1S#Jq®RSSJ° ¦NW% Sl1%S_%'S_9Mv8Z­·BJ­·Gl8%% %J
M%##JMQl%J ·i%¢r7ph%0¤Z%###M?* )Äw%°%%?SJL´ « ¯p1S ¥8² · 1p9%SQS
MJ9#8M9###_%{Sq%J ·G%#¢R7p3L7%q?J1SSªJS%9# #JSS%J18´F¯1S ¥%¥
1p9%S7SJRMJ9#8M9## ­%
SN%9 ·i%¢ 7JOHµSMYrM?r%°%%?SJ SJM_#
7%SL8JS.SMS%N%
SM1¡,%W¡,S1¡MSl%°%%SJwTW%<SJ5
 .¯ S9%u¤%##M* )ÄT%°%%?SJL´
¯1Sª¥±J8S%##D ·TJ%kSJB%JJn·i%¢77JJ#<JSJ#1S#%B8M9%8 %´{¬µ-198#J
qJS8JSS°hSJB#79#8MSJZ%v{SJSD%J<9S1SJK6%{VSSOMb#w¯1Sª(¥ /´
WBW%

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

K} ù] õ ú  
	1 ÷ø ~U ÷ø ú  û1÷
¬µ¶1J#8LS­·G#!¡,¢p·B SMS#MF88?<#J#SS#P¯x# ·hJ# 
%9LqJNv
JSp1S_1%1Sw91-_%?L% <%J=G8 8·B#
8O%*´"¥¦%¦%®³?´%? 
%B¥
³#GSlS°_%N%S <%JO#_TJS1¡U 9Jw%J8·B8S!.³iJ%
S"%S w%J#J ¥#
$%ª³0#HSlSS°ª%?8T% 
%&´¶¤'_JSp1SS#J°R% 
%=
<%?J#'  !³D#%)( +* %Jw#8%#<%w%k8?MS"´ 4hw%v8MS"( +* qJkO9S1
 <%?J#!.³06%7% #E8°%?%G9MS&,-+*#. ²p
´ 0
J1L#J#S#M"KJSp1SS#J°ª8%J%7v
JS1¡,<Jµ%J 1J8SJ1Rµ#9Jp#JJ%5w%J#J-=%=%#·G"5*´É%´3S7<%J=MS
JJ#¡8M9%8 S?18´s¨%R±/012µ(3+* 8%JJ%_v8°# JSp1SS° 9ES#(  + *54 2¹/ J%
179#8H9S1S#°´µ¤'_SJJJ##J°R% 
1S%L#qµ<#J#w	 87SL<M¢%9%76 Ò98;: *´É%´
ST1798S#RS#<%{ST#%-%v8MSª%%. 
%3´-ªM¢%9%p¡<J#<	 MS#N6%-SJ ¯ 
#9<>=
¡JMNW%?. ±7%J@A. /L,©=MS8% JJ"9£ ¯8SJ*5¥¦CBM®³?´
4-L#JS%91-%ZSDE) ¯ u#iJ9$ .J<U 9_wSFS8O%FG %v8MSLJJMSJ
,-+*q%JH5
 
%ªSJS#°w%?8´h¬µ7U 9Jq   JICKML ¯  %
%QJS%J10°%8M H¥³
S%7Y##°iSN,-O*#J8vJJES-%JFJ9W%?<B6S %#.8SM%P+QSRUTWVRYX.%Jb*±³51J91S#°
SD  W ?%Jmv80JSMSJG%3S=#.8°%8'( J1Z\[¥MT^]^]^]_T;µ´RO%WHQFR$[ ¥
%J`VRa[ ¦%¦rV¤Z%##M"h¥¦%#¦ /§ -<S¢%,iRESpO£?b %F¥¦%#¦ "³?´ ¤BR< 
%  %2 ¯ 
vJ9<MS¢bJS%J18Y#J8#J9#°0<OW9JL#lS 0G'dc3#JMS ( JMF%J ¯ ´
hp¡,%JJ .¯ =8%µv<1J91µ. #7vS°_SJ1SS7­S8=Se,-+*M3Se
%
v%S"´-¤lJM%"SM?8
J%7JN1JS8SRJS%J1-·BSQ91SS>v%#S°
i#G%°.W%·GM_T%JMJi1#S#°q78SpJW%i°%8MS°qJp¡,%Jf,-+*-W% &J· S%
S?JJ#°RJ%9<l6¬­MS" iMS9J8"K¬ J#S8%{£ -h·G%O±M²%²±³q S .¯ Z.´ 0
LM¡
JS%?  °%8MS#J°uJ1Sg  #.%%l9MSSJ#J°HSµ8L%\ <%JL#. OWh
1.S°J8( J%#.¡S	 8bS98S8%# OK jilk9@K O#m  on pn pK  !?´G¨%i1p%7Y%.·h OWhY[ ±pS
8
%Y <%9#
#h9MSSSJª#._·GNS98S-1.S%#J#°_ST<%9#T¥qSS°qqrM±
%JEqrM±s ¥TSS°Eµ"SSk1S#%%´0«µSJ?µl·GM¡,9MSSS#µS<%38%8SL
 
%­qJ=v
JSp1S  %#G<%?J#7#HS+
 JS09MSSHk86%SlJSp18J#J°R­%.H<%?J#l# S
1Jb9MSS#S"R´ BT1J%ESGMS-9#%1<bS 
%bSS#J°%8?{·BS9#7%_9MSS"´
¬µTS868h .¯ h·BS OWh[ ±bS#79#_% OK jilk9@K O.¯ 8´c5S-1<7 MSkt#K O !KQuKjm .¯ 8
·B8 OWhv[gµY*´É%´J%#"%ZS 
%9Op#SiSFw%J#J'#lS%7FJS1¡J88<#_%J8´
¬ J#GSiJSSJ1i%Y91SSi%W7wM¢%{SJJJ##J°BJS%Y<%S8DF%F6¬­MS
80%,´K±M²%²±³?{S9#F#0%FS_8%w6% ¯ F·BS­?J1SSw´N©=% pA %JAµZS
%8?M°%<Jw8J#_%{JS%9#x#JS%919xN%h<%SSª.LSJ01-S( JSRbSJ8hp8M
%_%JS#<%vSb%GTJ%=S-%JS#<%<%qS#S"xw#<9#8%TJS%v%SSJ%9
OWh´=«Q%S8
·G%J8v%JJ ¯ -MST°%8%LST%S=#9S%J18v·BJ2
 &J·BS% .¯ 
MSqS77
Jw898´U#JJ16%
SJ#h%98SMMS <-6S w·BMM8 N%J18´
¨%71%79%i¯%S8787%*´'¥¦%¦%±³T#.SpJJ1 8ST% M²y ¥8²Q%J %J ·G%S¢ &9· .¯ 
#H¥¦%¦%±§"S%9 ¯ B·8 J#S¢pL%ª_%JS#<%##%9·hJ#=Sq%JS#w%3<M¢%Y%J
%B%#90b%BSb·G%S¢ &J· ¯ 0MS_8JSS.S­J¢p·h"´Q¯p#w##M%3S_<q9w8J
¥8²'_¥8²FvJ?J<MS¢qJS%Y<8¨K#SJ8%Jw¤B79 )ÄD#V%7J'¥8²Ub¥8²0#JS%J1'%9L
 4hJ91¡
°M7%JQi.%¢ )Äq¥¦%¦p¥³z|{|}NJS%J18kMS0%% FM# Iw·%S¢ &J·j% &J·BS%!
 .¯ 8Y#NSJM
S7S( J7E
SJMF 
%µvTJSp1S µ%#D<%?J#=ªJ7·%S¢ &J·n9MSSS#ªv86%S
JSp18J#J°qb%EL<%?J#F#lSJ1pO·%S¢ &J·\9MSS#SL#OS##°ESbS#Up"´
~¹9Ö8¾*¾,Å o;GGD¹ Æ¼,Ë»Â¹ Í?Ê¹ Ë3Á»Å¾,ÃÀ'Í-¼,»Ã*»ÍS¼,Ê ÖMV»Æ¿¼,ÂÄÇAÆM6¿ÆUÃ*Ö¿ÅÇ½!¿¹ Ö8¾,ÀOÂ
WBW^

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

4-88MQ1w7p#T%
p8%'M?r%°%%#SJ<7MSN°%J8%#H6J91SJT%-SR8<%
°%9%b%9S#<%5SJO_<J%9d#JSS%J1b6¬­MSQ8h%,´k±M²%²/§3¯p#°%8B8h%*´v±M²%²%²E³?
%J¶Sb1q7p#=·Gb8%#% 8<6JSS8F7YJ%S	 8<S9#8vJJJ1%
´ 
_#µ9MF
S-179SMS9%91G%".978MS".i%J%pS#{##MS°%TSS#1b08S%3®' T%J
®Dw®0%9L·%S¢ &J·F%JL
 &9·BSJ% ¯ ´ %l8O1.S%#J
¥MÄ²%²%²7#JS%91GMY1%p·BS
S, +* S%<9T6S S'#.8SM%P!¥MT¦%¦3X,´¤3F%SDS'8%#M9###%k{17J#8%·G'%#
J'08{%D¥8²%²w¥8²'w¥8²F?%J
 ¯ 8%·h8S'S,-+*BMSBJJ#W%wS%7Y76SSBE8SM%
P!¥MT¦%¦3X,´L« ¯1S ¥8²pK·w19S#8S8S%=¥8²@ ¥8²N·G%S¢ &J· %J1
 &9·BSJ%
 .¯ =°%8?M
# %u%9%%°%Jww%J8´u¨%l179M?MS%N9kST·BS SR8MSS%·ª%JJS9%#
S8v%SiSS9SGW%OMMJ
¥8²  ¥8²<#JSS%J1GWJ9L#_S
 0 'c3#JMS%´ 4-S°lS7¥8²  ¥8²
0 '/c3J?MSw#JSS%J1OMSF<J°%8B1JS#J8Sl9MSS#89#M<?J%#°°pS8_9%1%
S°9	 98%.GJ%#8%kMES#R%Jl8S%=7M%##9MhSJSG%JS%#L9S#°0O·Bµ¥8² Q¥8²
JS%Y S88´ ¨J%7% #JSS%J1_¶%? %'SLMW%7ES# JS%9 8S8QMM#%.0%
GS¢N%Jª¨ )ÄT*±M²%²%²E³B19%#.¡JS1ªSJJJ##J°T%#°%%SJ ·i%h9Lw17Yv%S
SF%9S#<%"<M¢%Y%R%JlS08'%Z%JS#<%5S#SJ´
ô 5TùEëø 	úõG
	 $ú[ùËû ú   
ø Y	 
K} 	Y #

-J78SJSM9lM?L%°%%S9<J%=v8b8%#%kb6%GSJ ¯  %#L£jN88%55¥¦%¦%¦³?´

¨%==%J%#S8Y·G71=% %°%%S9 #.SpJJ1ª.ª¤%##MRH¥¦%¦"´0¬µT797.
 MM#%.q%-¤%##M* )Äq%°%%#SJL{·B9# ·GN%M%J %S##¶S8JJ91¶SJS
1JSEl·B#S SJµS8v%S . ¤%##M"´ # JKnl SM1¡,%W¡,S1¡MSªSM9 M?
%°%%#SJ\6%DSE .¯ B§SJG%°%%?SJ<Z%N h·BS¢p%JT¯0S9#S¢pk*±M²%²³?ZUU8##
%JTN8##
*±M²%²%²E³?%9P
 iMJ=%J­iJ%0k8?q¥¦%#¦ ³=ª°%8
%8%#Kv8S6%<%91%2´ 4h#Z%{SJ
%°%%#SJ<ZvSSD=1%SBSM97M?<79%J#S SJM{K%8Sq#<##MZ=SJM{6JJT#
 
9J= v8#wSh?#1-%57%-%v8M%pSh<8SbJb°%8?M
JS#%SJ8%J
S9=%Z#°M¡,8 <7%Sl7J%9#S<OS9N%BS#.JS	 98MS#"´
0
J-?#1T%
 b#hJM°wMS#MG´ 86%STS%S¢p##J°_7%q17Y1k"SM1¡,%W¡,S1¡MS0%°%M¡
S9<8%·G% JS{8%%b1S7p#K%"F#MS%TS#7Yi9DS8JESMS#%B%8S#7%vSM9
M? %J­S p<MS#8%­%SS0Sl# &9JJ17%B7%S_1791¶%#°%%SJ<7WMSJS
l1O7p#%3S-9%S#-%°%%#SJL´GJ( .S%pO#79#7ESMS#w% T8p#M
6Sx¤Z%###M )Ä%?°#J%"%°%%?SJd#lS8FSv1S8´{¨KS8·1<9SL<M¢%Y%J
1%1SR#JS%R%9S#°wb179SMS#J%#L8w8#E-S#<MS#ªS?7%´=¯1J"k·Tl%
JQ6S( .JJ1E¡,Y% 77%Sk§¤Z%###Mj¥¦%"¦ 
5´-¥8²%²E³L#JJ#8MbSJMlS vU J_%SJ?
77%T#MS°%#q#1wq#9S%J1{( .J##°=F%8<#MS°%hJ0v8hV*´É%´ \¥Bw###k³5%
8MS#J8´{¤'J#"·F#9S#M-#%#i%! q6S %9dp8%v%JS#<NVJ#°T7S<1¡
S1#kTv·-³K#9%7%vS'SJS#J°-6S ¤%##M* )ÄD88?<#J#SS#i1JJ1S#<78Sp"´
4-FJ#S89SQ#¶¯1S ¥±´#¥MZS8S<#
°L8J17SJM=S7 .v7%GS7#JS%S
J%'w8°#°Y-79%1'LSJv8L·BSl·hJ#"
 <p8M'%9S#<%5SJ8p·B9#l·G
SM¢%0%BS=J<MSw% 
1S%0#lB%J%#S8´

#¹$%'&(hÇÄÃÇÁ» 8¾,ÇÄÊÍ?Â9¾,¿
¾,ÖU»BÍ?ÂÄ×¿¼,ÇÉ¾,ÖUÀ Á» ¿?¾,»Á)$%*'+-, 8 8 +-.0/hÇ+7¿Ë¼»ÍS¼,ÂAÇÄ»¼GÅMÍÅ%»¼1!ºTÍ¾,Ã*¿j7»¾iÍÂ¹32 ~54646#6798Y¾,Ö»
U »lEU ¿¾Í¾,ÇÄ¿o" ÍÃKÊÖU¿Ã*»  ¾,¿'Æ%»¾*¾,»¼ZÊ¿o;:1» Ì
¾,ÖU»{½ÍÊ¾K¾,ÖMÍS¾Z¾,Ö»GÍÂA×?¿?¼,ÇÉ¾,ÖUÀ Á»<:ÇAÍ¾,»Ã3½¼,¿?À>=pÍ?ÇÄÂÄÂAÍ¼Á? ÃD¿¼,ÇÄ×Ç+MÍÂ
Í?ÂÄ×?¿?¼,ÇÉ¾,ÖUÀuÇ+- Ã*»<: »¼Í?ÂE¼,»Ã*Å%»Ê¾,Ã3ÍjU ÁB¾,¿i»ÀOÅUÖMÍÃ*ÇA@»Z¾,ÖU»K¼,»ÂÍS¾,ÇA:1»DÇÄÀOÅ%¿?¼*¾ÍjÊ»K¿½¾,ÖU»KÀO¿B:1»K¿?Å%»¼Í¾,¿¼¹
WBW'C

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

ª9FM%wc%M%H8T%*´	)Äl¥¦%¦%±³·#¡,¢·B  7%b%v8M%Z·hJ#H·iM9
% 
S%1EL%v8MS#J_ 1?S#8%h9pS¢p_rSµS?JJ#%´ * « L#79#7ESMS#"'%J #
1.%l w%Eup8%-M %°%%?SJ<wW%_S1.¯ B·B#¢=£ ¯p0JSJ#S¢p*=¥¦%¦%®³? 
9%?=%O% 
%1.T1S#8%D%v8MS#JMSb1JS#8­%J¶J%E
SJ=Sb¶ªS#°<1?S#8%
9MS"G´ 4i
%¶8MSª% pvS7<M¢%9%Q%%?µ°.v%h%{SJT8JSS.-S
#'1<9R%9NSJp¡,SM9RJ°Ev%	
 ZU³O·BSLS0S<%#SB<M¢%9%ª#'#1
6%=S71F#8MS"§5S#=MSTJS%¢% %JJ<%´ c38wp(3+*CT (W³hJ%7ST9%
%G%@ 
%1.
1S8%v%k8?MSJOSJMhMS·iMJvL#<°%8MW9SJ?LSJM'(+*FMJvMiv86%S(5#
SbJSp1SS#°R%8%'w%J#J !.³?´R«¶SbSJ9( Ej#8MSJ8! RJ8%ES0%
#Mv#'%
 FSM9A I_%.N7%TSJM
#.%8SS'Sq%v8MSR9%qp( T (+*³?´B¤B#pkwMM#%.B%
6S( .JJ1E¡,Y%¶7<%S%ZFR9S8%E0S1.S­·iMJvµ9%F%i1?S#8%{%v8MS#J=6S
v#°_S1¡,SM9#S"´0¤'7S8%#M #-¢·Bµ%
S7SM9 S<%9Q=JJ6%<LS%<9
8%8S_¥M]A± + 8?MSJWS  pp¡,·B#S0#.8SU%GP  , T + X,§EJTpJ%<#SM9q.JS
8%R%#N·G#!¡,¢p·BL18#FM?NvJ%?O%Sp8#MN·B#S pLSM9LS,©=%8=£
c3M°9JpY¥¦%¦CB%³?´c38 "!#$ ;pJ%OS'v{S7p8M<J?#°=%ET8MS#7%vS'8SS.
J¶%T#%{% p´Q¬ J6 Ò98;:   ³&%`6 Ò98 : "!'#($ ;³?DSbSMYHSMS90%)  #q8°M"§G#
%S8=·G%J8  _79
lS#<90%S9MS 8%K1#8"´=«QMS78%85SJTw#J#<%
°.v%#°b<M¢%9% <Nv0%98% .Rk%SQp¡,SMYª%JªSM9p¡,Y¡%9R7%85#
·BJl8%F7p¡,SMYL7%F#i%·ip'%818J5´«%#v7%'# *KM³MS=SM9"77%#
%818JbW%S'8SS.{8MS#"´K¬µB%98%'SJM{#7SJ'M9J1'%kSM9<<%JS8%.S#°
#79S%7.w%=SJª8SE_#S )Ä<<M¢%9%5 H%1S_%_¶#79L°%8uS1.
JSp1J%´ R%S v8	 98%##%G_#w8#MbSJMbSª1%QM? 9#%w1pJ#9  ¶#
88v¡1E-8%3M59SJ?LSJM'S8#OS°9	 98%.G9SSS-·GM?ªp8%v%JS#wp´
¤ZM90.G8%qJ%O-< 
%K79%130v8S6%<%J1%´ G%S44035 q7Y#8%4 %5S3 S8¤%##M
U9J+  ,, [ ²] "-d%J.  + [ ¥M]A±-HU·h8S/-?[ FsqrM±³10,2 .76 s<erM±80,2 .76 V¤%##M"
¥¦%"¦ .³?§ H%Jy MS0Sv1S%lSJ0v8B% 
%YB%Jª<%JB#RSFJ%9x#9S%J1
%J < [gGµ´K«<J##<#9MS=1v87ESMS#"E·GB%9S8S%wS9MS'SJS#J°=.BM%#
6%'h® w%JR® R®7JS%9dS8S06Sv1S%#EP /uKT 3XK%J P WT®3X6³iV%#lbJS8%.B1p8##°
%FSM°JMS# k9p%´
«9%"v·78"P   , T  + XK( J%KAPÉ®uT¥ MXW%=%##%h#.%p#°
S_#JS%91%J¶S1¡S%79#<SwSMY­S<8%8S (¥ N8MS#J8´L¨%l¥8²@ ¥8²R#JSS%J1
·Gq8 P   ,, T  + X( J%EP "uT¥ MX{6%
%#5%#B%JªM°%#QS1¡S%7YFSqSM9ª.8%8S
(¥ b8MSJ§9Sqv8	 9
M%#BM0SM¢%Q6S ¤Z%###M?* )ÄOSM?"k·BJ#?N%#_°%SRS
M6%S7.SQJW%
#%h#.%p#°R¥8² H¥8²l#9S%J1TV¤%##M"Z¥¦%"¦ .³?´F¤%##M* )ÄO?J
MSNJS 9J7pJ	 JH# %#G#%#T#.%p#° #MS°%87JS%Y #JS%J1{%´É°´SJN%J% 8
v· #R¯1S ´
9 Ò:<;Ws1 ã Ó Ø1>Ë= @? ·ÕÄØ'= 
b-J#¢%B<%S'v1S%2.¯H7%-%v8M%GSJb% µh·B#¢v£ ¯0S9#S¢p*9¥¦%¦%®³?S *
%v8M%#9JJ1DM?wY%1DSJMMS  K    5n  IMp#TSJM{{#D%#·GkS9G7%'6S
%LMS9#MSbS#S_<T°%9%v%9S#09L´GJ( .S%'#GvSSYBw19J1'7p8%
M?%°%%?SJ 9%S= 0SJM3#"9S%9M9##S#8%##iM9JS#<M#-17Y8h  4B'³{ -h8
¥¦%#¦ "³?YJLS9MB%l%JS#<%"S#Sl·B#98%.SJ%##bv=p8ML°%NSJw8.Sb#M°%=Jp¡
S#7R´ 0
31v8#<ES%ESSJS3S°%°%SJM B#  4B
S 
15-S%9M98S°56%
A ¹CBZË¼¿?¾ÍS¾,ÇA¿j-½!¿¼3ÀO5¿ :»K¿Å%»¼ÍS¾,¿?¼,Ã3ÇÄÃ5¾Í 1» B½¼,¿ÀEDYÂÍG@F » 5ÇÄ<Ê @ 1V¸IH1HKJ 7 ¹
WBWL

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

SiSM9qS%§E°#%TKJW%{S1S#°   ,, %J  + 
#%.% hV%#q=p8M
%Q%JS#<%Z#JSRL%.N%{STJ%9 JS%J1
S1?vR#µ¯1Sµ±´
«ª9MSS8J#M
S<SMYµ.JSwqJ=v<#MS°%<J°µ6%)
 LNS8Mvwp8%D%JS<p§Z9S#°LS%=SM9
S8"-#hS%°.W%S·iMª_19J1
1%79h·B8S w·B#"v170v8w%ES#
MJv7#qS'M?%1%9%#q%k=S°i8%p%9S#09L´Z¤=vOJSUM9#
  4B JJJ8Z°%J8%
9M%<88TS8S#°8DS   %°%%#SJ ·GJ#H#¢% S( Jb7pJ	 98MS#J=JM9#°R0
%818JB%RMS9Mw7%qM'%E_°%N#8MS"J%##·B#J°qM?N<%#·GBJS%°%SSi·GM?
b°%9%5%9S#09L§ -h.7¥¦%#¦ "³hJ#8JShS#<#MOS( .9S7.ShLST1E1p
%.  4B p8%
M?r%°%%?SJ<T6%w5
 46h¡¯ 4D¤=´{¬µQJ%RJ%79SJ Ju<J$ 98MSJqv8%JLS8
°%{J%1S#8%.8w891F#SS3%Sp8#Mq·BS=v%39M%<88ZM%#1S"%%Jv8%J
'#O9J8MB· SJJJ1_%JJ<Si·GJ#L#<9%1OS1%SSM9NM?RpJ%<88´
¤'JF79?#8%*  4B JS%v8S lJM9'9ObJMSJ%#_U 9FSJ1B( .J#Slb#%0
°%¶JS%9 #JS%J176%0$
 !    b#%D%  N%SbJ0k8=%O8?MSJF( .J#S 
p8M=0°#%9%#T%JS#w%kS#S"´Z«b°%J8%*pML1S'JJ8 T#Gq%J UM?#M9
·BSª%ªM9JS#<M#_1pv.S#%3J9JS"J%h·TJ#S89S'# ¯1S ¦´-GJ( .S%
·GBU 9OSJBM?w1W%F°%<J%9#JSS%J1'%{SJ8S'79#%7%<%<.9v8
%O8MS9=S( JS R#8M_%µ%JS#<%#S5§3S<v1S%L
 J%ES#SFMSwJ%

 7%J U´ ¬µQS#<Mªk%S
 	7%J LJS#J°u¥MÄ²%²%² J8vJ.T#%#8´ 
RHS
1pkJES#%"9MSS
%SJFJJ8##J°J#?9S5E7#MS°%FJ0k8i%KS%79#O#GS( .9S_
%98%S%SJM9_%88MSS#<M'%Kv%SNSMSS#88´
0
JG%J%#SSlSJ-%S#b%S
J#S%J1
 
  2 T  ( ³{v8 ·8l ·7#JSJ  2 %J
 ( O·hJ#u·ªSM¢% %wSR·G#!¡,¢p·BrJ 
9J1S%L°%M9 J#S%91H*RM6# 8b%,´F¥¦%¦%¦³?´
c58 K  TCT T U³0J%bN9SJ#8M<S9M088w#F·h8S8E 
%>QMJvMFk86%S2 
%
 µ#
S<9S1S#°L%J8F%O<%JB%O#JS E´w¤'<J 
9J1S%T°%M9HJ#S%J1
 
  2 T  ( ³
v8·G8 2 %J  ( OSNU YL%
6 3|4 2 3

  2 T  ( ³v[   *    W *  K  TCT T  2 ³ K  TT T  ( ³
2
2
2
·B8=SFpvj%BS2G.#% 60 ' %k8?M%´D«JW%w%#%pSFJ#
J91S%h°%?M9
J#SS%J1_#79µ8MJSJSqS_J8°%S8l%B88S%°%# %98S% #HSl<%Jb9S1S#°
( J1h%D·GL#S98´ 4J%SM99S%v8SL%DSqJ# 
SJJ1S#%=°%M9R9#S%J1q#'SJM-
8S%%L·8FvJ9"v·BJ#?µ#
79?#8%#LS°.83 SJw.9v8
% H7%0S( JS
7%JSW%  2 #E& ( ´{¤'J#i#G¢%8l#_'%J%pS#8p%'17YSMSl%ZS
1p%1BJ0v8G%
 7%
S( JLw%9W%?  2 #.  ( #%
  D¡9M  {MJ8D¥¦%#¦ ³?´B«NSJ0Sw%#J8
%3S9#G9Mv8.·G=J
S
8< FJS%J1( I7%J FJ# 
J91S%h°%?M9lJ#S%J1( ITp.<JS%´
¨K#J%%M·OU 9Ji+
 F%9 8%p%9S#09J
 Ih%D=S#S0SSJ#S#°B6SjS'MJY##8MS
%O88v¡S1.0#8%DM?¶9J8=S
 ¶%k8?M%RL?%J !#^L 5n  bS"´
4sS<!¡%1S%7#JSªhU Y %-_6%S9#0#JS V,´É%´5#%S¢p#°L1p8##0%8#J°b8vp¡
J8#?³
#µ·hJ#¶%#D%v8MSJMSwJ1SS­MS#FM#SFvSS9#TSSMSS#°NS7%´b¤
1J?J1'%JJx<!¡%1S#%0#JSJ8·GJFwJSp1JS=#.SpJJ1LLRM6# ¥¦%¦%®
¯1SQ±´A±³?´G¤BF88v¡S1.-JSp1JS=7YpO%JJdS1¡,JSM¢p#°<LSFJJ1
%D09S9
( J%#_°%N%#8JMS%-%JL8w#JM'910<S 0#'p8MQSJNSJM
   
 *ZU³? 6 Òt8;: U³9 6 Òt8;:   ³?´
WBW



 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

{} 	Yµ  	blg ú{,]ùQ	
  dRøúø   õ{÷$ù[úø,  ÷
¨%=b°%R9S%9 #JS%J1%v1JS8BSq9%1q%{6%S90#S9 r%JRSJ0SJ¡9%1
8 9:(; r1ES%J#°w%3p8%5%JS<p´ 
0bS0SS°wY#%'·GM p8%5%9S#<wJJJ1
FSO1%SO88v¡S1.?M8°%%E·GG1pv1   
W( ..S%79G#S9# 8 9:(;
J?#°=M?".´ -B·8%8Sh8°%8B·hJ#w#S9{# 8 9:(;kMSB%1SJ%##  m  !##n n  
%
#JSJDp#S<. #FVJJ1S7%"v%SwSh°%S<%5p8%J%9S#<FM%1%OY%S#J
#µSJ
 .¯ \%J¶Swv8	 98F%OSwS%¡,8 <7%Sµ79%J#SL´_«µ9MSS#89#MSS°
M%1%
9%SJi·9#LS( JS <w<%VMh·GN6Sxp8%"%JS#w<#l%8hw%
M?7SM°JMS5R´ 'B1ES#%%¬­MSR*±M²%² /³ZS·TS9MSiM%1%{Y%S#J%9p8%E%9S#<
#TS% ¯ ­MS'SJS#°-·GM¢TT°%8%%J<8%<kiS8Mv7·hSTJ°TJS%YM9## FS#7Y
Q¥³i%818JS°<TS%Si%JJd( .JJ1<V*´É%´J%#°%Sµ¥-%h±07.S?³%Z<%J#8%##
·G%J#J° <%Q%Jn*±³_S1¡#9S#MS#° °%S8 S1.8´x« %S8R·G%JhSMS% <%#
v8SSSYMSJOMSqSw8.'_7%Tp8%3M?RJ'%DS0M?%1%-Y%S#L%D<°#%ªp8%
%JS#qJ #0S .¯ Z´U¬µO%98S%iSJMKSOM6%S7.SqJSp1JJp#Z%T%k8?MSJ%
U 99ST%5M%1%i9%S<S°%S5.%´É°´pv8	 YOS8MvBJ%9M9###°#%w5
 F·G%9#° I
%JJ ( J1ª%
#°%Su¢k'#u1.%_H7%ª#6%<%B%SJ<J %wJMS·BJS8
·B#JS"p%BJ#%788´
i%w<SB%98SMMSJ8E·B.k%SJS	 8OSJMGMb# F#D v1S%TS?#1
QS_¡9%1 8 9:(;  8 9:(;1.S%#J°ªv%S p8%%JS<R%J #JSJFS9MqMSl%8S
8_µp8%%JS<R#¶8<q%BJ 
9J1S%<°%MYHJS%J1_%K( .J#U%.S%KSl.9v8
%  7%8´h¤3bBSJB..v%SS8J·G07J#%BS I  !   nI  !-n   7%D#S9OS
 w9#°7M?ªR<?%°%%K%JJ .¯ OSM¢%Q6SdS 0 '/cJM%´¬µTU 9
SN1Ew9#S%J1l%
µ8%JJ#JMNS#S ª%7SNJ# 
J91S%b°%?M9 J#S%91 
Q|T #³
v8·G8EL%J  p8%%9S#09 	{°%8Mu.¶M99p#°ªS88kS¡S1.<JJ8qS *
%v8M%N E´m« S%%=S1.NJ#SS%J1­#lpJ%S­J  S­JSµ%0%9 S1¡
JSM¢p#°lJ?#°l88v¡S1.8´T¬µw%#­1p%1?JM%18$ MSµ%iS1.FJ#S%J1T6%
%.79MSS#8JM -%Jw#9%w17Yh1EGJ#S%J1-SMS#S8{%8O·B#O%J°%B%>E´K¨J%
%QJ%9x#9S%J1%9·Gq18
 <6%hq<##l8MSJY179JS#°<SqS1.
J#SS%J1w%'SJ_8SET#JS¶Mq% 8?MS %J¶1%J#°ªSwSJS#J°NS#71¡8?8§
#%K%  FM'8<#9M7J1h%<%JS#<%JS#S7#{J1J.8Sw%JwS1¡SMSwWs
%JJ8%J%JS#qJL´{¯8%8%SSM8?J%B#.S9J177%SDSJMMSh1J18JSJ%##
S#MN<S1.'J#S%J1%YOMSF#l1E?%'9%lRJ= v8SJ1O#l#JS
 JSS8´{¨J%
1%79%pM?EI lm n Qµ -h 
8¢Y5¥#¦ "#"³GU 9Jb%GS-<#J#w%J#J1S%S-#L
 JSS
V%SS9<#°
<#9#<	 MSk³iSJMqJ=v7%818JH# %8FL8MkbL8%K%9S#09 M%1%0Y%S#"´
¯p#w##M%K¯pJw%J0%9 ¯JS8r*±M²%²¥³qU 9_M? J8JSH%qSlJ= v8SJ1bv8·G8
S2 9SSi%D<#JS%9lSJM'%D7°%9%v%9S#09  ´
¯p9<<MS<SMSS#8i6%GSJ-SJS#J°0S1.'J#SS%J1iMS
S8v%Sl#b¤ZM97¥M§6%O1T¡
9MMS#%9JSv8·Gi%JJS9%#B8k%3SG7%0JS1EKJ#SS%J1%9S8S%qW%Zw###
%JJ w!¡%1S%l#S98´ª¤'Jb7% %J 7J#%HS1.7J#S%J1_SMS#S8qJJ#8M
SJM  1JS#.SHS<%#JqJ# ¥jx± 7%w·i 6S p8%G%JS#<p JI  m   I  nT%
JS%Y S	 8%´ 4h#S°7M?w#Kp88%S9%#0J%q%8ST6MD6Sn#8%p%JS<pESJ?78%.S
MShMSO¡K%1%S%k%?MwTSh· S%9JMwJ8#MS#J8´ 79##8%p8p#J1B°%°%SSJM
#MS°%1¡9#S%J1F8%.S
MS%hJF<SJF1pJ1%Dp8%"%JS<7·BSN%8SL88ªM%1%
9%SJ8J9JBMS?MS8-%QMSSV%1B% )Ä-S%S¡,8? 7<%SN79%J#SL´O¤'JSJS
WBW

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

» Ã*Ê» 8¾ ÇÄÃV¾ÍjÊ»
 MÁ» ¼ $%'&(
-»Íj
»Ã*Ê» 8¾ ÇÄÃV¾Í UÊ»
 ÇA@»
 ¾Á¹ »<:%¹
 ÃV¾ÍjUÊ»
-»ÁÇAÍ 
-»Í 

Í¹
½!¿¼ÍjUÁ¿?À  ¿?ÂAË¾,ÇÄ¿oUÃ
¸-4
	¸-4 
¸ ¸?¹  A
¸?¹ 
~54
¸;~¹ A #
¸-4
	¸-4 
¸ ¸?¹ H1J
¸?¹ 1J
¸7J
¸<#¹ 46#
¸-4
	¸-4 
~ ~8¹ 41J
¸?¹ H A
¸
¸<#¹ A ~
¸-4
	¸-4 
~ ~8¹A¸ 
¸?¹ H A
¸
¸<#¹ HKH
¸-4
	¸-4 
~ ~8¹ ~j~
¸?¹ H A
~54
¸<#¹ 464
¸-4
	¸-4  "!$#
~ ~8¹A¸7J
¸?¹ H%
¸7J
¸<#¹ HKJ
¸-4
	¸-4  "!$
~ ~8¹A¸;~
¸?¹ 1H
¸7J
¸;~¹ H&
¸'(	¸' )*+
¸ ¸?¹ H%
¸?¹ H%
¸
~ A ¹ #%
~ 4
	¸' ) 
¸ ¸?¹ 8¸
¸?¹  #
~8¸
# 4¹ 4&
~ 4
	~ 4 ),
~ ~8¹ ~ 
~8¹ ~ A
~A
# A ¹Ä¸-4
#64
	¸' )-,
¸ ~8¹ 4 4
~8¹ ~54
~54
#1H¹ ~%
A J¹Ä¸IH
#64
	~ 4 ) .
¸ ¸?¹ /J 
~8¹ 4o~
#1J
A ¹Ä¸¸
 4
	¸'
)#,
¸ ¸?¹ 1J
~8¹ #%
~
 4
	~ 4
),
~ ~8¹ /~ 
~8¹  J
J4
/¹  4
¸-4 40	~ 4 ),
¸ ~8¹ 54
#¹A¸7J A 4
8¸¹ ~1J
¤ZM9<¥@3%-S1.BJ#SS%J1=SMS#S#8i6% 0LS1O%J ¯G6SmS 0 ' c3#JMS%´
¯SMSS#8M<SM¢%¶%8TS#71¡8#=%O°%S­Jw<## 8MSJ´ ''JS=6%
%9x#JSJOMSF17YRJ#°q<##b%JxS<!¡%1S%S#SJ´
S9k%{iEv%SS#SJMGSML# 0#G#MS°%7#1b0SJ-SJ¡9%1h%5W%9
#JSJ=1.S%#J#°Rk%S¶8%{%JS#<N%9­#S#J=SJMMw%8Sµ8QV#µ8<%OJ#¡

SJJ1S%-°%M9lJ#SS%J1³T#8%k%JS#wp´K¤'·TV%1%iJM9-SJ#GvJ%B 3ZSJ-°T9#%
·iMª8%v%9S#<TSJMBO#JJJ1_bSJ1%S88v¡JS1E-M8°%l% T%9LS
S#MS#%·GM¢h%M%1%=9%S#9B#RS ¯ Z´Y¤'JV%1%
%#_JM9)
   _#°%S
SYS%ES%{JS%v%SS#J=%'S_M? S9%1%{%SJb7% S1.TJ#S%J1_JJ8)
 Q#
J_76%1SL%KSF1E-J#S%J1=%Z%JJx<!¡%1S#%#S#J8´
¤'JLJ#<M­9Svl%
S%S¡,8? 77% # SMYuM?r#T­9M9LS8MvL6S
p8%%JS#w<,©
%8O£ cM°JJpY¥¦%¦CB%³?´¬µBS8%#JSJM
 F{%{7YT%E7J°M¡,8
77%<7J%9#SL´©
#%Q¥³S-M9J1-%3%_1Y##8{J#°p¡8%kMl?M8°%b%J *±³
SF#%¢b%Z%E$
 Dm   K  i8#JJ1
<S°%°%OSJM 7#G9%l·iMNv8	 YM%´É°´YJ°p¡
J%# %S8°J%  8 9:B;.·BJS%vOS'6#·B#°=..v%S#U 3   ! !  LUm  SUm # h^K oL    
 JICKML O i"K   n Q   8 9:B;  !%132 !m  1´D«"%JS%9 Jw897S9#<vB1%S#M_·BS
4 8 9:(; 4 5S<	 8T%S  8 9:B;rSJ¡9%1%´T«79##8#h# SJ-%8SS­hSJ<%SSJ79SQSJM
S=1J1S# b# 8 9:(;¶#iSw8.S7S8°9#MOSJlS9MG%9S#<%k#S9GM
J%G#S#M
S#MS#%'FS'{%kSBM?wY%1%.%´É°´JqS%9M9'S°7=68· J°9#79S%9M9
9MS.·G´F¨ZJ%#%k·q%98%qS9M-SJ0JJ1q%09S9%JS#<%Z#S#JBJJ1-S
JS%v%SS#N% 8 9:(; uSJM-qJ-v1p9%S5Y %8?M°%%5v8W%Sqw°#%9%#L%JS<%3S
#TJ19E8S5´ G9( .JES%·L6p8J7#J%  SJ

 65  _n  N	 8l% 8 9:(; ·hJ# ·G
%- 4 8 9M:(;  4 V´{¨%i·F.S9#7%Sh{J<%_M9?%1Sb9<T8MJSShShJ%S
SJMOJ%9mJw8J#w#iqVJJ1S_% 4  8 9:B; 4 %9bSJ
J0v8i%9LJ##9S7%°%9%##
%JS#w%S#SJ·BS9#S9MK¡9%1%´¯v8	 Y{S#wMDW% 4 8 9:(; 4 pMSO1JS#J8Sqk#·
#R¯1S#J% TS°qB.´
WBW7

 $¡4¢£

 DYJ ÷bô 	Dhú[ù
K} 	^øúø ûbT÷ø d

Ú$¦,ÜÄÝÞ¢­ZÝ

ú

 1÷$ú[ùËû 
 ùEëøK­	  Eù ,Tù E

¬  STSO118JS7%kSM?<T99%O%JS#SJK#77%<JS%J1OJb8J_,i#MS¢08%*´
¥¦%¦%®³?J%#1#S°F1i7J#D%"p8%9M?b%#°%%SJ<MSB9%S<wSJ1S%Y%J%p{%
S'J98p#° JSSD#%9JS8Mv8´3«6%<%##% JSSD#%9JS8MvO#D
%8S1¡,·°.<°%?M9
#w·BJ#?7SB%8S#8S8JS.8%9J#JMBS#SJSB%81w·#°ESGS8JES% 9SS
%B·G%SSR%{#S98J%JLS°%h8M9SSSN% 
S%1J1LSMSJSJ#9i#JJJ1N.L
°.v%_%B<%%v8M%TVM´É ´ ''88%8Z¥¦%#¦ "§"¯S%98J±M²%²±³?´
S8#JS%·F%9% 8_S
SMSJSJ#bv8·G8lMMJ JSO#%JJS8MvhWMS'%J
JS%Y Jw89 W% \6¬­MSj8R%,´=±M²%²¥M=±M²%² /³?´ ¬µHJ S8°%SSS# 78SpJ
u1J?J1LSMS#S#8%=<wSMS#°  %N7%Su6%JS9%#u%9W%?7Y³w#%JJS8Mv
6MSS8D%´É°´S_#%°MSJ %'Sl.9v8q%'%JS#<%G#JSJ8Z Sb?%J6%7 1
!K  2 -  ³
S( .9S Rp8Mw%9S#<%D#JSJ
Q®) R%JH® ­®L%J ¯ 8´ 8%9
S8NMS09%ªªSSMS#MYS71¡#.UM%E'6MSSh%DS JSS-#%JJ8Mk%9·0S8W8-wSJ
7p#T% !-n n  N1w<8´ ¤BL%88%1 %-µSSMS#L1w7pO8% v J%ES$ J %
SbS8°%SSS#	
  ( Z*´É%´ZSbJS%v%SSµ%iSb%S%MM#MY## ª#HM 1q%81JE 6%
­Sl7J*´QRT%'Sl7p#0·l1JS#J8S¶·G8S_9%  S#79#w##MqS8°%SS
%8iJJ#SMS06MSS.SJ?7SJMS
  ( 8MJSS<UMM9## 0J98DSB%SJ7JST%"0##M
VJJ1SJ%BS#MS9SJuv8 ·8 #%J9S8MvNWMSJSl%J SM 18´j¬µ WJ9uSJM_S
%88%1l%KSMS#F1SB7p#G9%LN·#¡,¢·Bl#%JJ8Mk
6MSS'JN%'SF.9v8
%O%JS#w%{#S#Jw,i#M¢Q8q%*´i¥¦%¦%®³?{SwY%S¢kJwS	 8 ,¯p#%8­£d¬­%S"D±M²%²¥³?%J
S7%8?M°%wJ#S%910v8·G8 %JJ p8%Z%JS#< *ªMW# 8F%,´K¥¦%¦%¦³h·i%
JR·M¢.¡
M¡7p8M%{·hS
  ( ?%°#°ªWS ²]A±%±ª ²] "´ 4hSJ°¶%qS8v%S"D·Gl%JJS9%#
#.%S°MQ7%S'SJ?R% 9SS¡JS%J1F1%SSMSH G%Y F%9°Y£nªJJJ55¥¦%"¦ .³
%Jb#%JJS8MvB1%S#MS_°%SQ,¯S%J8±M²%²±³vxT7%SSJMGMhqJ_7%S-1<7J
#.%S°Mª#l%v8MSJOMª%JL8%#JSJMSl179S°7SJ%N# 4h«x_%JL%9S%#
8%R#·G8  ( U%#J8´
-·B#°F6SMbb 46B¡¯ 4D¤ ,¯p#J°%8{8%*´p±M²%²%²E³?·'SJw79M<S9M
SMS#h17pY%<wSB7%bJ#S%J1Ov8 ·8b%Js8%J%JS#<F%97SBM
%JS#w%9#JS"·B9#7·G
% 7A8 9M:(;=<>9:(;*p#S#°J	 98%.ST7%SB%88?M%J#°  ( M%#
%²] "#/T%9_²]A#® 7W%'® 7%JL® _®0%J .¯ 8pSv1S%#%R´ 4-OS·Bl#_S=86GSh%
¨K°S=¥MSJi%1S9%  W%®=®B?%J .¯ K#Z9#8%#·BSJ#q-6%1%%Z¥8²<V*´É%´.=7%S
SJ%u¥8²RS#7F%9­R=SJ%uB¥ ?¥8²E³F%iS<JS9#1  	 %#S°­#¶LW8·s1p18JS#J%
8%wSN%98%uJ= v8SJ17118r­6%1%b%T¥8²%²p´ 4h9JSJ%%D·ªS·GuS9M<S
78 9:(;=<>9M:(;Y7pJ%81J.SGW%i7%vSBMM#M9#q#<SJh1S( .9S7q8M
 !/1 2 Kjm n  L 
#JSJKFSSBS%7'9S%9#9S%J1{%JT9Sp#{%<19%JMS76%J= v8SJ1#TS
S#MS#%FJw8J#w%G FH JMS( Iª Fr3
  ¥³G%8S9J
 FS1S%°JMK Iª Fr3
  ¥³i%J ¯ ´
¨%lSQMM#%.b%=S 78 9:(;=<>9M:(;-7%Sª%8M ·hS  46B¡¯ 4D¤FO¯p°%8Q*±M²%²%²p-5´
®CB%³
v8JMhS9MF#JS%J1
·hS #MS°% 78 9:(;=<>9M:(;KM<7%S<Jb8Jh9T F1´´´q#9S#%Z#MS°%
J#SS%J1 WS SJ$P %9S#<% XF#JSJ_%LS 19S%SN%S ´´´ MS # ·BJ S
P %JS#w% X5#S9G%.%h719JMS_%SJ=6%1%?8´ I iJ#J#°FNSJ#G%98SMMS"p·G
p8· 7N8 9:B;C<D9:(;5%hw1J1S80<%SSF% 4  8 9:B;  4 V9v8	 Y8%#wv8%JS 78 9:(;=<>9:B;3S#qJS%8J
%81J.S-W% 2 Kn QRSJ0S	 8% 8 9M:(;  %JNSJJ#?9S_%D%JS<%5#JSJO·B#SJ# 8 9:(;´
W%

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

10000

1e+07

1000

1e+06
Search cost

1e+08

Search cost

100000

100

100000

10

10000

1

1000
0

5

10

15

20

25

30

35

Mean distance to nearest optimal solution

40

45

50

55

60

65

70

75

80

85

90

95

Mean distance to nearest optimal solution

¨K°S<¥@3
¯p8M8¡,9%SG% 7N8 9:(;=<>9:(;Y%8SJ 	{W%O®w®_V8W J°³{%JQ¥8²DN¥8²L6°. 9 °S³
%J ¯ §SJ%¡HJMS J'##OMSv8¡#<kS"´
«R#J°wk·q%SSJ7SJMh?%J #8%3%JS#<bMSS89S.SMS%%{SJh# 8 9:(;
·BJL#'ESJ#S% 
SJS	 YM9-°%LS#·\7%ªM?R8JSl%Y8S%R9J8 p´ .
« SJv%S<%0_8· 1J18?J#° SQ¢%8rSQ% 4  8 9:B;  4 -#r9S%9 Jw89%O·G
%98%-S9MOS
%S8'SMS#
1'7p#%  T1JS8Sb.b¬­MSN%JlJ#i1##M°
*±M²%²¥MG±M²%² /³7MSbY%  %JJS8Mvw6MSSL6Sl9%¢.vlS	 8%S_J0v80%'%9S#<%
#JSJ8{%7SJN%8M°%QJ#S%J1Lv8 ·8u#8%i%JS<E³SJML
 J%ES#W­#S87SNS	 8L%
8 9:(;u%-SJTJ0v8 ?UJ#?9Sl%%JS#<%K#JSJ8Y9
%
k%S5´-«ª%S8=·%?J8vS
JJJ8p#°0<%SSOV%#v_8MJSS=%S·Gw¢%8LJ#79SJ% 4 8 9M:(; 4 W´
-9#i#SK19%JM%SFv·G8·GiJS8pJS#F#.S	 Jq8%8%JU Y8J8%YSJ 7A8 9M:(;=<>9:(;
1<O6¬­MSH8F%*´±M²%² /³?´<¨K?85S<7pK#=%SF%88M<6%=S<7F9w8J
JS%Yx#JSS%J1B·hSJ#LL
 ¡S	 8°%5´B¯1J"YS0<36%#Ol%81JE-6%-bp¡
p#%KJ%k%Sr  ¥_@r /³F%iS<MM#M9#Q#µJS%Y Jb8JªW%T®­®l%JJ .¯ 8´
¤'J#"p7J"%88%1LV%##i<%9W8Bw7%SF?J1SSL·G%S¢ &J· .¯ 8´

: â  â #1Ù]·Øs+=1rØ'Ø!Ù

=v8SJ1w SR%88%1 %
S 78 9:(;=<>9:(;B<i ®y ¶%Jr®qH®­%J ¯w%#
%#SR1J18JbS8°M9#°¶S8%M9## H #MS°%8i<%SRS%#S#8%## S$8uJS%Y #JSS%J18´
<9#8%%E·GJ%F%98S%RSJMOS<%N.9v8O%Z%JS#<%"SJO#l?%J .¯
°%S·BOM9J7·BSl#91S%O#_JS%Y S	 8%´D¬ L19_·B#SbSJFJw8J#<% FH .9MS(I
#JSS%J1D·BS    ¥8²p.SJOSSJS°=1{%"179S°=v%S 78 9:(;=<>9:B;%9
  	 JS8pJ
S?#1FK%J%pS#5-®F -%J0®FO®O%9 ¯ 8´ -B·8%8·BSFJ8·8K<#1S%9S1%8
·GMS·\M9=b%SSBSF%88%1L%ZS 7A8 9:B;C<D9:(;51h<vR#MS°%8'%9 .¯ 8´
¬µ 179 7A8 9:(;=<>9:B;'W%bSQ¦%±­%-Jª¥8²%² ¥8²q ¥8²­?%J ¯ w·BS M²¶w###
%JS#w%"#S98§pSF179SMS#N#OJ9%1S#8%kW%BS=S<%#9#° "7#JS%918´YS<M

¹ ÃZÁÇÄÃ*ÊËÃ*Ã*»ÁÇ+  »Ê ¾,ÇA¿j ;J 2E»ÀOÅUÇÉ¼,ÇÄÊÍ?Â9ÁÍ¾Í'¿Æ¾ÍÇ+U»ÁÁË¼,Ç×'¿Ë¼KÃ*»Í¼,ÊÖ½¿?¼{ÀO¿¼,»GÍÊÊË¼ÍS¾,»iÊ¿?ÃV¾DÀO¿Á»ÂÄÃK¿½
$%'&(iËUÂÉ¾,ÇÄÀ'Í¾,»ÂÄÌ=½¿?¼,Ê»ÃËÃZ¾,¿'¼,»¾*¼Í?Ê-¾ 2.¿?¼KÀO¿?¼,»GÅ¼,»ÊÇÄÃ*»ÂÉÌFÀO¿ÁÇÉ½Ì 2E¾,ÖÇÄÃDÍ?Ã*Ã*ËÀOÅ¾,ÇÄ¿oE
¹ 	Z¿;v<» :»-¼ 2.¼,»ÃV¾*¼,ÇAÊ ¾,ÇA¿jUÃ
¿oB¾,ÖU
»  8 9:(; Ã*ËUÆ Ã*ÅUÍ?Ê»KÃV¾,ÇÄÂÄÂÅÂAÍÌhÍGÊ» 8¾*¼Í?Â¼,¿ÂÄ»{Ç-Í?ÂÄÂÃ*ËÆUÃ*» 8Ë» 8¾"Ê¿ÃV¾ÀO¿Á»ÂAÃ¹


Ò

W%

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

% 78 9:(;=<>9M:(;hMSª9%  Ä²%²%² %J p8%'%JS#wp´ ¬µQSJ· ¶S8M8¡,Y%b% 7A8 9M:(;=<>9:(;
% 8S9 	=W%qS_JS%9# #JS%910#¶Sw?°EqS#w%'¨Z#°Sª¥M´ª¤'  ( M%#w%'S
1%SSSk9J#°F8°%SSSl7pY#i²] E®·BJ#?wS89S.SG /#/Vm1S%S
b7pk%88%1
S#MS#%-TSF®\b®qJ%9m88´{¤'J#SS9GJ7JMiSJ-V%##JSB%S 78 9:(;=<>9:(;"7p
FS8%iF#M°%8 ¯ ´¬µO%9S8S%'S#<#M%9K#0%88J%106%{SMS#O1S{7J#3Y%T
S'J0k8D%k%JS#w%#S98MSO9%S¢v'S	 8%.%J7S'7%wJ#S%J1iv8·G8<?%J
p8%"%JS#wR6¬­MS"5±M²%² /³?´b-W%SJJM%J·G08%9%h8SES#_%SSSB#M°%8BS1S%°9#M
#JSS%J1'J=wS=U%SB.9v8FV*´É%´9J'%Z9##J?³D%Z%9S#<%"#JSJ8´





K} ô ûûp3  ø%   T ù	 $ú[ùËû  L ú[÷bô RNúR÷M    úG ¹ûb#÷·ø dDY
'¤ 
U98#J8%5S 78 9:(;=<>9:(;v1O7pYJJ#8M-SJMGS8F¥³ 78 9:(;=<>9:(;v#G%O%b.SS#
%88Mb7%SS7% 4 8 9:(; 4 Z%b*±³-=%9 ·G%#¢Q..v%S#
#=#J1%S183*´É%´ 4  8 9:B;  4 
#q%T1798H#JJ#8MS#%<%'JS%Y 9w8J %´R¬µL·m6p8J S
 JS0%89MS%%
4
4
·BS SJ7°%%D%GJ8%%9#°LL7%<%88Mb<%SS7% 8 9:(;  S9% 78 9:(;=<>9:B;V´T«J%µ%
%JJj#8%.%JS#<p%·GO#J%T1J#8ZSO8K%Y#JSJ# *I 1    M?"´
¬µ8W8'wSFSS9S#°71B<"%h $1 "! ;@I S ML  1Sh7p*´{¤'J F .J%!¡pJ%<#U I
7pJ	 J88%WS Sh6%1GSJM%#S°_%°%%SJ pJ%<#8{MhSM¢%l#.0%81J.8%
1p9##8O7pv%ZJp¡,S7
k9p%B#O%h1JJ15´
¬µq8%%N .J%!¡pJ%<#
1B<v% 7_%J%# #J°7SFJ#S%91Ov8·G8
#JSJDp#SwJ#J°
SMl%JwSB1%SSSk9J#°FM%JS<%9SJ8´ c58 7 9:B; M³
%wS<J#S%917k8 ·8HN#JS <%J­S<MS%JS#<%DS"3,´É%´ 7 9M:(; U³[
L   
  ST M³·B8S  b%qSL8T%B%9S#<%G#S98´#c58+
- ; + ! _%lSl8
%SJB#N
 bJ?#°<%Q19R?JN b°%R9S%9 #JS%J1%v%JQ8
- . 8 9:(;3%q<8'%K%9xp8%5%JS<p´¬µ0SNU 9J 7A; + ! <D9:(; 7A8 9:(;=<>9M:(;W³i%'S07%
J#SS%J12 7 9:B; M³Gv8·G8RS#SJ*
 +
- ; + ! 7
 - . 8 9:(;6³G%JLSJMSO%9S#<%5S"´
¨K°b±NSJ·B798%DJ##9SJ-% 7 9:(; U³
W%qS&- . 8 9:(;%9 +
- ; + ! T%i · ¥8²@
¥8² %JJ ¯ !
´ G%S  .vq%h9#YS­MSL°%J8%#¶S<<8#_%Ju©
%JSS%p¡#¢%%
%SJ°<·GB#6S( ES#%98%h¢%8·Gb9#YSJv%S<·BSw%J7·BSJ{p8¡,S9%p¡
©
%JS#%lS%##.´ h8p#MS9OW Sq©
%JS#%L#%"MF7%S=JS8M%.'#lSFSw%#8'®  
%JT® '®OJS%Y 8S8´K«%#E%ZZJS%J18 7N; + ! <>9:(;+% 7N8 9M:(;=<>9:(;*%*´É%´ B1JS#SES
p#SS'S#SJ'SJM'Q%8M°%7MS8#8h_%N%JS<%5#JSLSJ%R%J<#b°%8?M
p8%%9S#<p´­¯p#<#MF%98SMMSJT­6%TS#S $1   n SM{S9 SJMT#S90# +
- ;+ !
1JSES#bvS'·G8B<M¢%S9%J'S9%N#S#JO# - . 8 9:(;*´
¤'JJ#%°%?%<BSJ·Bª#N¨K°Sq±<8S%q%B##JMS#%=1p%7Y'%K·G_kh%DM?
9#%1J9<.
 p´{¨K8pM?l#°79#%Sw·GMlSJSJMGM-% F%8S¡
M°%( I=J#SS%J1v8 ·8TSiMSSZ%9S#<%#Sq%JT#JSJ5S9MKMiwUp#w%#=J#S%.
6S SbMS0%JS#<%{S#S"´L¯1J"Z%JJ p8%{%JS#wNMS JKCnF1SSM?#QS8¡
S.SMS%l%OS_80%'S#SJS­JJ#°NM?"{1.%J#1S°RS_%SSJ<JSµ·G
SM79S8#JSF#w¯p1SJ
 ´ 4hSJ°TMb#
    !O#MS°%0S#1T
  8 9:B;ES8S
v%ES%#F1#D#MS°%Ov%SS#JZ% 8 9:(; yxFW%S%JK·G'8SS.Sq=%D69#FJJJ8S%J x
SJM   #iJJ¢%<p#S8´D¨J%#S
<%81J.'6%'SFJJ19#%S_S8°Ji·B#YJ1SSM##
p#¶198SMMS%lS#<MT% 4  8 9:B;  4  ´Q¬µ_%Y8S%lSJMqSlSSJ#S %T1E%9#1
G%Js·G%#¢<Ev%SS#8§.MS8·G-S#J%S8SJM 0#k8W%w#°F0%Js·G%¢
%8
Tv%ES%#_S#1RS¡8O% 8 9:(;´




	
 








W% W







Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

3000

4500
Random Local Optima
Solutions Visited During Search

Random Local Optima
Solutions Visited During Search
4000

2500
3500

3000
Frequency

Frequency

2000

1500

1000

2500

2000

1500

1000
500
500

0

0
0

20

40

60

80

100

120

140

0

Distance to nearest optimal solution

20

40

60

80

100

120

Distance to nearest optimal solution

¨K°S± 3-h#S%°%%<G%3S
J#SS%J1hTS
MSSG%9S#<%k#Sb6%Gv%Sb%J p8%
%JS#<
%J7SJZp#ST)
W%D ·1p%7YF¥8²t7¥8²
%J ¯O6%
J°SF1%SvJJiw<9J$ h9S%9d#9S%J1³?´
¬µGv#8%S9M3SGU 98J8#5%JS 78 9:(;=<>9:B;.7pEMSGJG#F#MS°%G9MSBSGV%##SD%
S-JJ8##J°
<%SSh0%88M#w89#1ShSJ¡9%1h%5#S9#¢%#0qv'1Y%S
 ´F«µ1E%S85S 7; + ! <>9:(;K7%SSTªU YJSQ%81J.S=W%=SJ<8
%GSJ
#¢%#  vQSu p´ GJS( ..S%'·GµEv%SS	 8QSJML
 .J%!¡pJ%<#ª1
7pp9%<TS 7; + ! <>9:(;7%S'SJ#qp#7S#°J	 98%.D#7JS%7.S{#T%88%17%8
´ 4h8#JJ1<6%SJ#Ev%SS#8·b%98S%bSJM0%#S°
SwSSMS# 7N8 9M:(;=<>9:(;1q7p*+
J#1S89%J8qv8·G8 SNJ9JSJ=% 7 9:B; M³W%7?%J p8%i%JS#< %J SJ
p#S0.)
   ·G8SO<#J#w%E#0{®v =9S%9j8S8E°J	 98%.9= k8J1Z·8Si%9S8S%
#¶Sb#M°%8T®@­®ª%J ¥8²y ¥8²QJS%9 8S xQSlS%7l#JS%91F6%0·B9#¶S 7A8 9M:(;=<>9:(;
7pZ#h#%-%88JM%´F¤3lVSS8
##JMFSJq<M°9SJ0%{S7J= v8SJ18k·G0%9S8S%
SJMGW%GS E±F%3-¥8²L¥8²q%JJ ¯ ·BSHj¥8²%²pÄ²%²%²T%JS#w%YSJ8 7; + ! <D9:(;Y#
%8?M°% /C#B V ·G8BS9% 78 9:(;=<>9:B;V´G¨%BSJS%7JS%J189S#JSJO# +
- ; + ! =N%8M°%
vSSB7<M¢%9%H(¥ /Vd·8hSJ%LS%ZSJO# - . 8 9:(;*´
¬µF· .J%.S67S=%88%1b%3S 7; + ! <D9:(; J%S¡9%<#'1'<YL®D J® w®
%JL¥8²NT¥8²
%J ¯ 8´Z¨%%.0°#%7#JS%J1%E·O1J?J1/+
- ; + ! iJS#J°-SJS
 b%8wMM#MYqJ0v8h%J8vJ.'#%8G´ 4#%Z#
#JSMN6S b?%J
p8%J%JS0J %Jb8<#JMwJ1
F°%9%T%JS#<%Y#Sb##8M"´{¤B'8<JMS
18?w##7v7v8%JSBS8Sh1p°%Y%#T%JS#<%Y#S#JD6Ss·hJ#w07%OMS
vSSYJJJ8S <7%O%v8M%B B·B#¢J£ ¯pqSJ#¢,p¥¦%¦%®³?´¬µO8<#JMGSi.SS
JSp1S8J8#JJ°OSO8SS.K#%*MJ1 4 +
- ; + ! 4 [7¥8²%²pÄ²%²%²p´¤'GSS9S#°+
- ; + ! MGSJ7J
01<9 7N; + ! <>9:B;6§SB#M°%B.9v8{%"S%7Y{#D( .J#S70%?J8%hS%JM9#q%88?M
S#wM'%KSJ#OSMSS#M´
¯p8M8S¡,9%SO% 7A; + ! <D9:(;Y%8?SJ 	W%OSJ=®  <%JL® L®0JS%Y 8SOMS=Sv1S%#
S·B¶ Sb9k8F#8W%9¶Jv8=#°E#=%O¨K°S
 /
´ 'B8°%SSSH7p#=% 7N; + ! <>9:(;
(
%8S9!K  2 -  S³G##LSv1S%  U%#J'%{²] ""l%JR²] @B ""1%SSSk9J#°T
  Vd%JQ±MT² V
#J1%F# %88%1 SMS%7NS 7A8 9:B;C<D9:(;{17p*´T¤'7%1SJ% 	h9#8%##ª8p#M
6S SªJSJ1 	Tu¶<%SRSJ% ­V%1%l%G J%ª%9u·GR%98%R68·8l%J S













 







W% %

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

10000

10000

1000

1000
Search cost

100000

Search cost

100000

100

100

10

10

1

1
0

5

10

15

20

25

4

6

8

Mean distance to nearest optimal solution

10

12

14

16

18

20

22

24

Mean distance to nearest optimal solution

1e+07

LA19
ABZ5
LA18

Search cost

1e+06

ABZ6

100000

10000

LA20

1000
25



30

35

40
45
50
Mean distance to nearest optimal solution

55

60

65

¨K°S/ 
3 ¯p8M8¡,9%SB% 7A; + ! <>9:B;v%8?SJOM?R1  W%h®  RVJJv8'8W J°JS³?Y® R®
VJv8=°.G J°S³?Z%Ju¥8² ¥8²HV·8 J°S³h?%J ¯8§Sw%S¡H.9MS
J'##OMSv8¡#<kS"´
1pS7hMS°%1¡,SS#9J%#JS%91DSJ%<J98{S 7A8 9M:(;=<>9:(;97p56J#qSJS8'{%"±Ä²%²%²qJMS
v#.SOJ=v8O.L7%S=S9%N7V%1%B%O¥8²p´³
¨%-SS8B% E±N¥8²"H¥8²7%JJ .¯ O·B#S#s¥8²%²pÄ²%²%²_%JS#<%3#S98 5 <S8°%SS
7p
% 7; + ! <>9M:(;-%8?SJ !K  2 -  S³_##Jl%  ( M%#µ%T²]A®%® V8¶S­·G8Nv%SS %
¨K°S!
 /³?§FS 179SMS# % 7; + ! <>9M:(;-#_J9%1S#8%'6%lSQS<%#J°¶#JSS%J18´ ¤'
SS9S#°  ( # JB¥ Vn°%SM8OSJ%bSJM%9S8S%_W%S 7A8 9M:(;=<>9:(;Y<9wSJ-S%<h#JSS%J18´
¨JS8'S %1SJ% 	w#w .Y#8%# ·B#SJ#uH6%1%L% ­%FSJªJJ#1  <%J # 
8%q#OS9#S1S89%91_#M°%8BSJ%Nw6%1%-%O¥8²p´¬µqJ%T%#b%J%SMªSS8M8¡,Y%
S·B_#wSJ-·8iv%SSb%3¨K° /·BS_JMS0W%iS J%
%5S
8%µ¥8²R¥8²0?%J
.¯ 0JS.q¶S 0 '`cJMS ·B#S  ¥8²%²TS²%²%²µ%JS<%S#SJ´Q¤'J
 M}Q%J
 	

J¹CBZË¼KÃ*»ÂÄ»Ê¾,ÇÄ¿o7Ê¼,ÇÉ¾,»¼,ÇÄ¿j7Á¿8»Ã MÏ?ÑKÂÄ»Í?Á0¾,¿
Í-ÊÂA»Íj7ÁÇAÃV¾,Ç+Ê¾,ÇÄ¿oTÆ%»¾ v»» q»ÍÃVÌqÍ MÁÖMÍ¼ÁÅ¼,¿?ÆUÂÄ»À Ç+UÃV¾Í UÊ»<Ã 8
¾,ÖU»{ÖUÍ¼Á»ÃV¾G-¸ 40	q<¸ 4BÇ+ÃV¾ÍjUÊ»ÖUÍ?ÃKÍÅUÅ¼,'¿ ÇÄÀ'Í¾,»ÂÄÌ0¸  OÀOÇAÂÄÂÄÇÄ¿oF¿Å¾,ÇÄÀ'Í?ÂpÃ*¿?ÂAË¾,ÇÄ¿oUÃ¹ 	Z¿;v» :1»-¼ 2%Ç+UÃV¾Í UÊ»Ã 5ÇÉ¾,Ö
 -¸ 4 4'2 464 4B¿?Å¾,ÇÄÀ'Í?ÂÃ*¿?ÂAË¾,ÇÄ¿oUÃÍ¼,»{×?» U»¼ÍÂÄÂÄÌhÀO¿?¼,»{ÁÇÉÈOÊËUÂÉ-¾ 2_5ÇÉ¾,Ö=ÍGÀO»ÁÇÍ    ¿½ &J ;2 8-¸ 4'2 :1»¼,Ã*ËUÃD<¸ #'2 ~1H¸½¿?¼
Ç+UÃV¾Í UÊ»ÃG5ÇÉ¾,Ö
ÀO¿¼,»Z¾,ÖMÍ 0-¸ 4 4'2 464 4B¿?Å¾,ÇÄÀ'Í?ÂÃ*¿?ÂAË¾,ÇÄ¿oUÃ¹





W%5

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

#JSS%J1iJ%=v8bWJ9w7vBSJ
<OJw8J##bSJ#i8F%#l£ N88?%"5¥¦%¦%¦³?J·BJ
#O1J#.'·BSlS%98S%LM%#'% 7A; + ! <>9:B;6´
«w1J8JS" #{J#°JTJJ#¢%#q#{#MS°%hS8°J%"ShM?_9%1B6%i<%E
JS%Y #JS%918´ 4hFL19( .JJ1%Z<%SS=% 4 8 9:(; 4  9%µµYSN?%J p8%
%JS#w0MSF¢%7Tv
198SMMS%=%Jl#J%88M%9Sp#J#J°0q9MSS#%Y1p9#%JMS_6%OS
V%##Sv%pS 78 9:(;=<>9:(;E<,´5«1E%S8%S 7; + ! <>9:(;7%SJS{
JU 9JS
%81J.SW%SJ#
9J7"p#9#°=7%B%88M
7%SB% 4 8 9:(; 4  %J<07%Sh%88JM-1S7p*´
-h·G8%8EYSiS°9	 98%.3#7J%7.SK#%88?%1%S 7N8 9:(;=<>9:(;%J 7A; + ! <>9:B;.hJMS
·GLVJJJ%7.S%ZU 98J8#U 3B%88%1QS##V%##hl8%0NMS°%8
JS%9 #JSS%J18"%J
S=7p#JS#Jh7J1G#9S°.#.0SJ-#MSJS9wv8·G8lS=JJJ8p#°F7%SJS
%JN%#°%%SJ<h?Jp¡,S#7=pJ%<88´









} ô   úG  û b# ÷·ø dDY
'¤  7A8 9M:(;=<>9:(;3%J 7N; + ! <>9M:(;v1Sh7p#iJp#=S°w8#JJ1=SJM<#iv1S%Lv8¡
6%<#°hB%9\·i%¢F%8DBv%.S#%#=SS#1qS98%JS 8 9:(; NSJ¡9%1%´R-B·8%8
·GFJ%=%8O7J%kS-%._Sk8$ 9hJ8S%##8%´É°´JSF8i%ZSMB#_S=7pk%OSGJ%#!¡
SMS%B9MSS'%kSO%JSTJS%YM9##S#8´3¤''JJ%w#Gk9p%D%"%E)L -L"K  S  ! !-p8%
M?R%°%%?SJL%´É°´k8MR#8%5SM pc5S91YRMS#"Y£s¯p   %9±M²%² /³B%JNS#qp¡
#Mb%J%##J°7V S¢9M#¢Y©
#M8p£ D8J*v¥#¦ "#/³?8%"pM%G#TJ#989%Mv'7p
JS°¶RM¢% ?J%#J( 3RSQ8b%
6%S9#R#S#J<#w¢·B"iSª%JSS# JS%9MY##S
v8·G8r#°Ev%#J°QSJ78% vN17Y"i%J SRªMS¢% 9S%v8S #TJSS8S%"´
c5p8%vM?_%#°%%SJ<G%°7.l·BSb77%S%p%´É°´SM9lM?"8%l%#qvh7p_%
ªMS¢%¶?J%#JF.QvJ9#°lSw1.ESq%O77% ENSJwSMbJU 9JS55SJ­SJM
S<ªMS¢%QJS%v8S R#-JS8%"2´ 4h#S°ª1p%18S0SJS#J°b7p#h°%8%##NS( JS
MF%wVJ8k9J#°_ S<179#1p ª%S<77%³-%µ1pv.S#%K.J0v8=%GSSMx
 *± 6    ³ ³{#<S .¯Ax<%J<S88W%ShJS#Jh#S#'#JS°.#.SJJ%#SMS%-JMSSh%"S
M? 9%<#88´B¤B0?J%#°%T#h_J8%%QM°%°%8°Mw7p#BR·B9#ª#MS°%T.J0v8'%
SM-MS=°%SvNE<78SU¡SSM8Yp#9#°T7%S=%1SM9#%JN1JS( ..SlJJ8?S%Jp¡
M9ªMS¢%NJ%#98´



:ÓÔ
	r^ØØ
¤<7JvSF#79%1O%KS%¡,8x77%blS=vJ#%O%  p·GGJ'%J%# 8F·
M?LJS%°%SOSJ8O·iML%B·G_6SdSJ
JMS'%JS<%v#S5´Z«L¨K°S ·G
S·jbS#71¡S8h%DSq9#S%J10bSTMSSh%JS<%3SN6%-k%SQb%J ·G%¢
JJJ8{S l7%h%v8M%i%J
 F_F .Y#8%3¥8²l¥8²%9 ¯ Z´E¬µB%9S%#b#<##M
SS9SDwS%79#°=%v%Js®' 0%Jb®'q2® ¯ E#<%J9ST00.9v8{%"91SS
JS%Yd#JS%918´¤'J=%Jd·i%¢l1J9SOw#J#<%vSJ%S¡,8xJJ#°TvJ%·BS
M?­7#J°N·i W %88=N%µ%9S#<%K#S#ª·hSQS°Jª( .9%KJS%9M9#%´
«L1E%S89·GF%98S%0S°7S8°9#MSi#lSJFvJp%O%! ´{¤B=S#71¡8?'S·B
#QST°.
S#q%¨K°
 N<JM
SJM   #
M9qN<%#.S%# M? °%%J.S
6%1pJ_v8pJD%5S7%´D¤'J#%98SMMS_%J0S-6#·h#°..v%S#U 3ZS-%S¡
8 77%S­<J%JS % R%1Sqª19S#.Sµ9#%0M?¶SJ8·iM %T·i
6SdSFM'%JS#w%5#S#"´
Ò

W%

C

Ú$¦,ÜÄÝÞ¢­ZÝ

80

80

70

70
Distance to the nearest optimal solution

Distance to the nearest optimal solution

 $¡4¢£

60

50

40

30

60

50

40

30

2000

2050

2100

2150

2200

2250
Iteration #

2300

2350

2400

2450

2500

2000

2050

2100

2150

2200

2250
Iteration #

2300

2350

2400

2450

2500

¨K°S 3h¤'#71¡8?=%iSwJS%J1<ªSwMSS%JS#w%{#S#µW%q#S#J
S
.<0%J ·i%¢RV86,J°JS³D%J _6#°E J°S³DW%i_¥8²DN¥8²?%J .¯Z´



# lRMS¢%u7pO% ­%w
9%BS8JS.S#°7v%S¶¥³OS8h%D#S9OJ#S%J1 6SxS0MS'%9S#<%5S
%J *±³FSJb8SEqMH°%%J#E 3´N¬µl%_SbJ78?#7M%#  
 P ¥MTS²T¥oXO·BS
S7pv#

5%J
vSv1S%#%´=«Qv185·7MS77p#°wS7#<9%1h%
i%ruSJ#wEv%SS#8·ªU9N­SSM





Î ÕÄÏ?ÐÔ Ô38Õ 
	 8 ÑUÔ
S%¡,8 77%SR%
lS#<9S8%M
%9R0vJJ#°wSJ#-S8%#M=#.bS7SM7UYJS"´
h18v·GqJ%ST<U#09xvSSYFJ#S%916S %QMS9#MSL#S#N_S0M
%JS#w%ZS#Sª. 
  + ´F¨K#J%%"8hSJT19JSJ%JS%YM9## y=w  *  4   ³BJ%7S
JS%YM9## ª%-S#qJS%89S %8#°QSlM?H°%?%J.06S   %J 7#J°R6S
Q#S¶J#S%J1 
WS S_MSq%JS#<%#S#­ R#JS¶JS%J1 Q6S S
MSSF%JS#<%#S5´<¤'<< 
% Q%iSS<JS%9MY##S-( J%{²pKv8	 98%##NW%q%E
9%0%'SM  *  %9   L·BJ8S 4   4  ¥b%0·BJ¶S#qJS%J8J?J%°%T#­v%SHS
°%%J#E%97Sh9#S%J1O0SBMSS%JS#w%J#S#7MSh%°8%#07vSS9#%U%´É°´6S
SM    8 9I$# . 7SM    2  8 9I$# . ´D¨%O%?)S"¥$ 
  + .SJ8Sh1#iMi7GSB6#·B#°
J#J
JpM¡ 88S<%9SSlJ%9M9###SU 3
 =b   4 2  8 9I$# . 4    8 9I$# . ³?=w   #  + 8 4    8 9$ # . ³?9%Jy=b    2  +-. ;K# . 4    8 9I$# . ³
 =b   4 2  8 9I$# . 4   #  + 8É³?=w   #  + 8 4   #  + 8A³?9%J@=b    2  +-. ;K# . 4   #  + 8É³
 =b   4 2  8 9I$# . 4    +-. ;K# . ³?J=w   #  + 8 4    +-. ;K# . ³?9%J@=w    2  +-. ;K# . 4    +-. ;K# . ³
¤'iJ%9M9###SS=w  *  4   E³MS'%#S
 
1Z=SJOW##·B#J°h%S%!¡,9S%9M9#19%#.SU 3
 =b   4 2  8 9I$# . 4    8 9I$# . ³s&=w   #  + 8 4    8 9$ # . ³s=w    2  +-. ;K# . 4    8 9I$# . ³v[ ¥
 =b   4 2  8 9I$# . 4   #  + 8É³s=b   #  + 8 4   #  + 8É³s=b    2  +-. ;K# . 4   #  + 8Ä³v[¥
 =b   4 2  8 9I$# . 4    +-. ;K# . ³Gs$=w   #  + 8 4    +-. ;K# . ³s=w    2  +-. ;K# . 4    +-. ;K# . ³Y[¥



 











 


















W%	L

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

¤<1798=SJRMS¢%L7JY%!p·F1SM0TSU&J1S#°T9M8OM [ 
  + %J
%<M9S%S9#°
SM-Mv [ ²
0v1S%07vS#°-S'1J?%#ES=w  -  8 9I$# . 4  -  8 9I$# . ³Y[ ¥
%Jq=w  4 2  8 9$ # . 4   +-. ;K# . ³s=w  # + 8 4   +B. ; # . ³[n¥M´O¤'F1JS%#.SBp#
SS8b##M ­SSMU 3  -#   + 8  -  +B. ; # . K%J     89I$# . ´LGJ( .SªSJbRM¢%­7p
1JSSO%Z1p%1S5
 /)0 
  + SM8´
¬µ<1J8JªJ%S#°_SJM=%QM°%°%8°M­%JJ ·i%¢ª<%
 µ6%F%Eª%S8
p8%OM?r%°%%#SJb³·B## JKn78M9SSLSLVJ#G8S%#O%-SNJ98p#°QMuJSp1S8´
«µ9MSS#89#M"S79MSSµ#J9J1 . M°%°%S8°MS#JP
° .¯ \#JSJ
9%­µSFJS%J1
_S0MB%JS<%3SR'%  1LUm +2  RV =7.R£m¯pJ#*5¥¦%®M²E³?§5J#SS#J1BSJ
MB#JES#8%"9#S%J1OwSMSS'%JS#<%"S#SLJ%0J	 k8S.i%JS#SlJS%9MY##S
6%B7p#°b88'_%JLVMSS8'6SdS0MS'%9S#<%"#JS"JJ=wk%SH¥³OJJ> .
Jv8B%Jª9#YSJG%{#6%SY=°.k%?B%J *±³'J9$ .JFJ##9SJG%{#°Ev%
<M¢%Y%J8´0¤B.J"S .SSQ·<MSTvS#J°_#-·BJ8S8=S8ST1#=Sw8#E
S8°JMS
#LSJF%JS#SLJS%YM9##S#6%hS#SJi·hSJ#Lw°%N9MSSNS9NS9MBB#'kS9
w8S_M9JS#<MS=vJp%'%ZSFVJ#"ªMS¢%N?J%#NJ#°7TSJJ1¡,%8B?J%#"´





Ò
	$ÕÄ

·gÕbäîØ

 

Ø

¬µ
S#<M
S=ªMS¢%l7p9YM%788 
  + %JbS
8i%S=w  *  4   ³.<%79##J°0
SY8O%D#S9i#L ´¨J%h7°%RJS%9#d#JS%J1%9·F%9S%#NMh#%   ,,
%J­M07   + I  !on 5n=SJ
M%?¶JS%J1B6S SJwMS%JS#<%DS"
 µ%Ju®y ®ªJS%9 8S·GN8   , [ M²
·B8L±     n  7A8 9:(;=<>9:B;³?´  ¨%<SN®H

%J   + [ #± M²p§{6%FSQ¥8²@ ¥8²ª88K·b8   ,, [ M²R%9   + [ M²%²p´N¤B7M%#F%
  ,, %J   + MSF#MS°%=J°L7JSJS=SJM'MSS	 Y8#%#w#MLSMhMS%O°%8?M
JbQ% #9Sw8.J0k80%BS%7Y8´N«JJp#J9%K#%#%
   MS_18J JES
°%9% %JS#<%G#S#¶#08M5{Mq·hJ#¶v#EqªJ8· %#q#JSM"´N¤'bJSp1S
S8vMS{J.S#M#%   ,, S%79#DMS'%JS%#J7W%%w9#S%J1956SSJ'MS{%9S#<%
#JS"9±"7N  n 7A8 9M:(;=<>9:(;W³?YMh·BJ#?Lv#.OS8SEB%#°%%SJ<
#%5#B#<7JM
8<JM"´
¤'JRJv8<vJJ   + #b#7v  <S°MªSª79%1<%FSJ7S9MbMSQSU¡
S#S8%#­JJ¢%  k_p#SH. ªJ?#°ª%. JI   *I 1 B?#%*D9TMSl8S#S
J1J.8Sl·B#S_Jp¡8°#°#9'J%9M9###T·B_1p8S#°TS=#MS°%=J0k8G%3?#%#SJM
MSbS( .9S ª%?J8%_SwS%<9##°l8w#JMS­1?8"´b«JW%w%#%   + %#·B9=
JSJSTSJMFJªJRS8JS.SMS%wS#SJ
MS<J8#J #QS<%79788´bi%9J#JM
#JSJKMSOJq1JS#J8SqW%J8#JS#F8%8Sl¥8²%²F8MS#JKW%S'S<%##8D®N =%J<®NF®
JS%Y 8S8{%9­8%8SH±M²%²Q8MS#JF6%0SJb#MS°%8l¥8²@ ¥8²ªJS%9 88´Q¯pJ?¶v8#J
S%7Y##°hJSSKS9MDSO1#1<S%<9KMS'JJ1%S#M"§SJiSk8$ 9iS%79#°h#.8SM%#
MS09%ªNS#wMh%DSJ0#%J9S8Mv1%SS#MS#R°%S *RM6#Q8h%,´Z¥¦%¦%¦³?"*´É%´kS
1pk1_Jv8{%"8?MSJ{%"%Js·G%¢7M68·B9#<S#S 9SS{JJ1%S#M"´
i%J9#JMSJMSG%81897#F%J8%9M9kM?%J1%%*´É%´%S 9   , J#SS#J13SJ
J1J.8S M=w°#%ªJS%J1 iMq%·ip-S8S%#5k%JQMSTJ#S8M?NJ1qST.9v8
%9%h%79
M=J#S%91\i118J
   + ´F¨J%=%­S%<9Q#SQM=J#S%91\i6S
S=MSSG%9S#<%v#S#_%9_SMl°%%9E ·G=%¢bS=JS%J19<%J_°%%J#E 
6%'SSL#lSF9( .'8MS"´
¹ =vÖU»½!ËÊ¾,ÇÄ¿o  0Ñ 17kÇAÃ3Á» M»ÁhÍÃ  Ñ 17 4  625ÖÇÄÊ Ö-¼,¿?ËUÁÃv¾,¿G¾,Ö»vU»ÍS¼,»ÃV¾3Ç8¾,»×?»¼¹



W%





 $¡4¢£



Ú$¦,ÜÄÝÞ¢­ZÝ

 

c38L   E³i%JN  *  4   %³Gv1S%l%SJ=%S%J0k8O%K%9S8S%RS%<9
#  SM   %J Sµ%S%FJ0k8l%0%98S% %9SSJ_6S  SM   Hu SM
 *  ´HS#<M0%'Sb%JSS#­JS%YM9##S#-M_17YH9S#°NSJw%96%qJ#%8
%´É°´9=w   4 2  8 9I$# . 4    8 9I$# . ³e[L   4 2  8 9I$# . 4    8 9I$# . ³;rL    8 9I$# . ³?´u¬µR6S( ES#¶%98%RM
%   , S%79#DW%GJ#S%91v   n 78 9:(;=<>9M:(;W³?´{¤3FSS#<M 
  + E·G JS88<OS
<#9#<%-dSJ?qSJMSJJv8Z%JS%79#MKJ#S%91 -x#SSJ%   , M,´É%´USJGSw%#
J#SS%J1wMF·B9#­%79FMw%1J#.S %98S%"´b¬µwSHU 9
 
  + [ -  ¥M§
<#Sµ%'SM   N·hS#  - J%q8°#°Y7#79%10H7p{%88J%1%´ª¨K#J%%3·G
%98%'SJMJ{S#<M%vk%S<S'=w  *  4   E³K%J 
  + MSBMS°%7#J9SS%Gv%S
ST#JS%3#JSR%JQST( J1T%SJhSR9#°wSTUM#JB?#%#8v*´É%´vS
SMS#SS#8'MJvM'<vF#S%S%9#M´
¤'JRMW%7ES#r9S1w#wJ##N SJM_Sª17Yu9M?%788bSS#<MlMS
9%lLS#SJi%1SJ%wp#S_ ´D«%#%p9M%<88'S#<MB1J#_v
J8%
#JJ8k9ES#q%3S=%°%%SJ J98G19S#8MS#".6%i1p%<9-#q%L%J%pS#%3?%J
p8%k%JS#<p.´ -h·G8%89 ·7V%1%'199S-TJS8%.'SJ?L%LMJJS%?L#_S2 .¯ Z´p¨Z#8
%7S·B # ¯1Su®{%9 8%G%JS<ªM_ .Y#8%#­%TS89S.SMS%L%hSJ
p#SH. QJ°RM?"%J ·G_8JSS.S­ %T6JQ9J8S%9¶S_S%q8%9
%DSJ#B9<HV%SJ°R9S##<JMSw8p#910#J9#8MBh#BJJ0#ª#MS°%YMSB_S
J#S9S#H%=#JW%S#9L#S#J7·BS9# SN6%S9#R9%1³?´\¯1J5i#w#<J98Mb·
_S%##S8%#lS%790ST1E.S
%{S%S¡,8? <7%S%´
iJ( ES#%Y·GqMq8SES#
6%1r J>
 ­ °%8M%Bp#H RE¶iM?M¡#¢%N9S18'¶8JS.SMS% 8b%
S%7Y8´'¨JS8k·q%0S9MhS0%W 88?<#J#SS#
vJp%B%   VJ#S1J.S#°wS
#wSh8%-%3qJSYi( .J%T°%.pbJp¡,SM9_7%i%Jb%Jw	 MS<%"SJhSM9w.JS³
°%8%NJS8%.S
JS1
9M%18	 MS­%{SyI  !on  62$1n  K H%{%JS#SR9S%9M9#SO6%
%. !     ¶S%7Y%-%LlkS9ªW%N#8%=M? %°%%?SJ<_·BS  °%8RpJ%SS#
17v.8J%´É°´Y#8MNp8%5M?N%hR8S%v##BS%79#°l6¬­MSS"v±M²%² /³?´
«r¨K° i·QS· SRSS#<Mu9S%9M9#Sq%F7p#°H88w V86L
 J°³q%
VMSS8
W 6#°E J°S³'STMS
%JS<%S#SªW%=b9#8%i¥8²)¶¥8²_%J ¯ '§
SiJS%YM9## =%Y<%ES%#9#°h% Ô 8?Õ SMT°%%9ED#K8°°9
%u²]#¥³?#J8vJ.3%
Si8SS.KJ#S%J1
SGJMSK%JS#<%#JS"´5¬µO%9S8S% J%##SMS%FS#w##M5SJS
6%i%9%5JO® J®Db®%JQ¥8² N¥8²q%9 .¯ 8.%SJ°_·
%
SJMiSSJ#S6%O7
#JSS%J1Z°%8%#kSK7%Si#=V*´É%´E<%#!¡S8%#S8°J#MS#?³kSJ%TSSG%Y8S%T#
¨K°S ´¤'=SS9SO#JJ8M-S9M'SFJ%9M9###<%K1ES.J°7w7%088Bª66MS8
6Sb³GSqMSh%JS#<%3#SR#O9#8%b9S%v%SSJ%{V#.%8#_9S%v%SSJ%W³bS
8SE<JS%J1L6S SNM<%JS#qJL´ 4- 118JS p88<·B ¥8²­%9 S
°%%J#EO# ÎÕÄÏ?ÐÔ  ·B8S
S=9S%9M9#7%Z1ES#J#J°q<<%8S8Ow%l%JS#<%vS
%1SJ%##7#G%N ²p´D¬µ=8SES<9%=q19%JMSbW%iSJ#9J7"%S°_
MJvMOwk=JJF#l9MSObSF88v¡S1.BY#%O1J9_   ´
¤'JªJ%9M9###ST%07°H8S8_ ?VMSS8L6S S MSl%JS<%hS#S MS%
#ª°%8%*"°JNp<78?#0MJJ 
  + rM±3JQSJM=M?µ#   #h9#%Sª·GM?
#JSJZSJM{MSO%7%8?M°%hJ#SS%J1iWS SJiJMSD%JS#w%S#S"´K¤'J?JM%18#SS#
JST%H1p9#%9MS W%7SR©
%9SS#%p¡¢%bJ9JSJ=% 7 9:(; %98% W%<SJ
p#Su9#°µM?"O%´É°´'%_S·Br# ¨K°JSª±´ ¤'R79%1<%=S%¡,8 <7%S #
%# 8p#.8
%LSµJS%YM9##  %qw%#ES%J#° S­8SELM? °%%9EL#L9° %J
1JSES#µ1p18JT²] ª#¶%{%OSbJS%9 #JS%91·Gw1%<#5Z·hSµS_1p189S







 

W%



Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

1

1
Probability of moving closer given grad=closer
Probability of moving closer given grad=equal
Probability of moving closer given grad=farther

Probability of moving farther given grad=closer
Probability of moving farther given grad=equal
Probability of moving farther given grad=farther

0.6

0.6
Probability

0.8

Probability

0.8

0.4

0.4

0.2

0.2

0

0
0

20

40
60
80
Distance to the nearest optimal solution

100

120

0

20

40
60
80
Distance to the nearest optimal solution

100

120

¨K°S 3h¤'G%JSS#J%9M9###S"6%K<p#°=88K<V86 9°S³5%D6MSSJ8Z6Sd6°.
J°S³3S'MD%JS#<%S#STJJJ8!
6%
9#8%"¥8²t7¥8²
%JJ .¯Z´
%'p88%SJ%i98OS%90µµ·G8TSJ% ²] µMq1pS<%O9#S%J1 S{*´É%´  ²ª% 

  + ´7¤'J7JS%9M9#L%i#.%8SS#°lS<8JSS.=°%%J.F#=%#LLVJJ1S#ª%GS<JS%J1
­SNJMS<%JS<%O#S# %J SN8°%8N%-?J%°%%´r¨J%w1%79%GSLJS%YM9## 
%i·BS?J#°l°%%J#ES
6S Ô 8?Õ  ÎÕÄÏÐÔ  #=J#°8=SJ% S7J%9M9###N%i·BS?J#°_6S
	 8 Ñ UÔ   ÎÕÄÏÐÔ  ´G9S#.O·BS_SFSJSGJSSENMv%#R¯1SN®Z¥³GSFJ#SS%J1D
M-·BJ#?H=w   4 2  8 9$ # . 4    8 9I$# . ³N[ =b    2  +-. ;K# . 4    +-. ;K# . ³'#hMJJSp<ML( J%3 7N; + ! <>9:(;
%J *±³ 7N8 9:B;C<D9:(;5°%8%l6%#'%E·B8#LS0%°%P 
  + rM±uT 
  + X,´i¨K#9%#%J·G0%0S
S09#%91Nv8·G8rSª%JS#S JS%YM9##S#q _ªMS¢%r7p'%JrSª S
·G#!¡,¢p·B>JS6h7pW9JR#ªST#8MSJSQJS%YM9## lS8%SHV¨8D¥¦%#® ""5´
/CBB%³?§p#v%ST7p#8MSi%9j·G%¢qpJ%<#8Z8%Tvp8·G7%D-#79J= "JSJSp1S
·BSLw1E%3S%°TW%1%´



9 1'Ø ã Ø

¤RM%##JM7Sw%J ·G%¢ 7p*·w1<9MSwSJw%1SJ%G7%¶M?¶1S 7%9S8S%
JJJ8   ·BS S­1%SSSk9J#°­M%# JJ#1 .rS­7p*´¬µµS 1JSJ1
  K  2 - ¡!K  2 - #M08°%SSS 7p%BS_JS9#1 %8SJT%1SJ% b%J .J%.S6¶7p
%88%1 %=STSJS#°  ( ´ G8%Jw=#-9%S  ST%J ·i%¢ª7pZ% p"·G
S8680ªSwSJS#J°N##M0S8°%SSS#¶7p{%Tw#I S" ML  l1T7p*´ 
wQS_8
S#MS#JSJ v8 ·8 SR?%J ·i%¢u%JuJJ%w#N1_7J#8G·QJRSJN·GH8?<
#.89%°%M9_·BNES$ 98MSl%K<7%SFv8	 Y=1E1ph#O9J1SSM%´
¤ª17YwSwJSJ#1 7W%qN°%¶JS%9# #JS%91%·G<S8vMJµS0JM<S
1%SSSk9J#°G%J ·G%¢
7p%U 9J=.-S{YM%788 
  + SJ8"%SM   .M%J
ShS#<M_%JS7J%9M9###Sv=w  *  4   ³?´Y%?bS0JMS<%J#G#JS#M<WSm
SM  6 3J·h8St [ 7 9M:(; U³36%{
#%!¡v8	 Y{%JJj#8%p%JS0J B%J b( J%# ÎÕÄÏ?ÐÔ 
% 	 8 Ñ UÔ  ·BSQ( .9%KJS%9M9#k§"S8%#KS9M=S7JS%9MY## L%G<%ES%#9#°l% Ô 38Õ M?
°%%J#E8°##°9%´<¬µl179J" 1%1SrV%0%Jvµª#79QJS#J$
°  n 7A8 9:B;C<D9:(;W³³
Ò

 
W%7



 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

#Q%8
N1.SK6%=kS90 k1SF%S7J#S9S#N%, 7 9:(; W%=?%J p8%%9S#<p
·BJ­9¶Qvb7%S_SS8°9#MbV*´É%´{¡©
%JSS%Y³-6%T<%#DJ%9 JS%J18§{#8S#°
% V<³-1%w#µ7pD%88?%1%´w¬µ<S¶U9
 [  n, 7A8 9:(;=<>9M:(;W³hSSJS=µLS#°.w7
S=JJ#1LM?R1 -%BSF7%NJ0v8O%ZS0JML8MSJ'S( .9S_w%98%
SMY%S9#J°qSM  -  8 9I$# . §9SMS#S8'MSFSM¢%ª%8q¥8²pÄ²%²%²b#98vJ.%#8´
¬µ, J51J#85SJSv%JS%#J=W%®G h%J0®G®i?%J ¯ 8´K¯p8M8¡,9%S%pS
JS9#1%8?SJK%1SJ% {6%KSi·G
JS%9 8SDMSiS·BT#FSi%Tv%SS0%9¨K°Si®´
¤'  ( M%#l6%Tk%S %hSL1%SSvJ9#° !K  2 - ¡ K  2 - S8°%SS 7p#q#T S<MS¢MM9
²]A¦%®´¨J%'%#k9O±p¥=%J ¥%¥
%3S
Sv1S%7¥MÄ²%²%²<®\ 7%JL®\_®q#9S%J18pS=%1SJ% -#
·BS9#R_V%1%=%G±b%STJJ#1 ´F¨J%FSTS<%#9#°w#9S%J18vS7%1SJ% q8p#M
6S SJ0JJ#1ªU%#JNb<U#qJ 6%1%=% W] _%J!
 /u] "Sv1S%%´h«Q1.%

S 7A8 9:(;=<>9M:(;k%J 7A; + ! <>9:(;v1O7p#8S8S
#G08p#91-%%_E%8S-1%S#MS_v8·G8
JS%Y Jb8J
%J07JE%88%1k§Ep%..SJ°MS7pE3%Z%88MGW%S  "!   !-n
JS%Y #9S%J18"%
S·BªRSTJv8-8W
S#J%¨K°q®G´ 4n8S%##R1%<#JMS#ª%
SQJ°p¡,S#JJ%O#9S%J1_#JJ#8MwSJMwSJQJ1R%=SQJS9#1S 8S%_#w°%8%##
S_6%1TSJM Q#Sv8	 YwSY8S%-#S#JFSJM7MSl8lQ%JS#<%iSJ
·BS ¶JJS%v%SS9MHJ#°u6S( .JJ1%'SJ?rSJMbSR9#<MS %SJ7JSrJJ8##J°
RªMS¢% 7p*-*´É%-J79M9#%'b°%SSup#M5´ 4-LS·h k#·Fh·µJ%¶%
%8q%98S%HSJ#=vJ#%0#­S8S%'#M°%8%JJ .¯ 8Z%#S°LS<vSS#9## NSJMqS
9J7J7#O#S#M"´
h18v·Gq%S-STS8%#M9#l%{SqpJ%<#01-<3.ª1JS#8?#°<S
 E±b%D
¥8²yu¥8²Q%9 .¯ ·B#S  ¥8²%²pÄ²%²%²µ%JS<%S#SJP
´ 4x8M8¡,9%T%BSJbJJ#1
%8S9=%1SJ% qW%=SS<#JS%J1
#=·B #QS<#·G8=k%SQ%¨K°S<®§S  ( M%#
%SJT1%SvJJ#J° K  2 - ¡!K  2 - S8°%SS 7pK#-²]A¦CB.´0¨%F8W8S91%"·7#J8JFSJS
V#MvY³06%wSJ¶¥8²E ¥8²¶%J .¯ 7W S!
 0G'ac3#JMS ·BS  ¥8²%²pÄ²%²%²H%9S#<%
#JSJ8´-¤'T%1SJ% #-%·ip
·BSJ#RbV%1%=%±´#¥q%{SqJS9#1 Mv%JQS8ST#-
8p#J1-%%.w1%SSMS_k8 ·8R%88%1b%9b9S%9mJw89%´DN%F#7v%SS%.S%E·G
%98%<8°%%9MSR#L%88J%1lS#MS%FwSFS<%##8OJS%9dS8S8´
¬µ J%Q%#­1p9%u¶J0k8w%
S1JJMS 18?#µ6%wM%##JMS %=SJRpJ%<#
1O7p*´Z«b9MSS8J#M.·-%98S%
<#9#<%J= v8SJ1v8·G8lSB9SJ#1_%J_%1SJ%
SMS#SS#8%FI  !on   2&1Wn pK  !B%vv%SR¥³KShJ#S%J1FSBJMS{%JS<%J#JS<%JR*±³DS
SJw°%SJ8.*´É%´ES'J0v8{%k#8MSJDS9M1JS#SESMb°%%J.S{MSh<%#.S%#"´
4-JJS9%#%5·Gb1J#809= k8J1#¶Sb9#YSQ%OJSJ1­%8S9%1SJ%GM?
1Shv· #Q¯1SR¦´G¨ZJ%#%·G%FS9MBSJJ%w#
1h7p"#'( .J%b%88?M
  ( .\²]A¦%®³
ª%81J.S#°L6%
S71=%#8MS#J°_SJ¡,%JS#<%Z#JSJhNMS9MR®e 
%Jl®<®=%JJ .¯ 8%·G#Y%v8#%q19J1_8SG%"%8SwJ#w8J® 0%9_®<®
%JJ .¯ 8´ G%SNJS%Y v'MSFVJ#b8S%#L_¬­MS *±M²%² /³F´



=
¤'hSSJ#S{JSSEl#wSJ#S1S_JSp#hS°J18p#J1hW%GiEv%SS#SJM
M?wJ98   %1S%=UM?#%ED%"FS%°.W%S·iM7J1¡J#7JJ%p%JJ ·G%¢T%8
S 8 9M:(;  S¡S9%1%
´ -h·G8%8{SJ<%JS#SµJS%YM9##S#
k8 ·8 SM0%OSw?%J
·i%¢bMSFJp¡JJ6%L.SU &J1S#J°qSJ=JSJ1
%Z·Gwv8	 9h9#%SGbSJFMR9%<#88´
¨K8SM7#Z9%·GM?<#S#J3S9MKMiM9JS#<M#F( .9!¡J#S%.ZWS SJGM
ÒÌÓ

Ø Ë#?·Ø

W^



Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

10000

100000

10000

Actual search cost

Actual search cost

1000

100

1000

100

10
10

1

1
1

10

100

1000

10000

1

10

Predicted search cost

100

1000

10000

100000

Predicted search cost

1e+007

ABZ5
LA19

Actual search cost

1e+006

LA18

100000

ABZ6

10000

LA20

1000
1000

10000

100000
Predicted search cost

1e+006

1e+007

¨K°S® 3
¯p8M8¡,9%Sw%hSJLJSJ#1 %8J<%1SJ%'SMu1S _W%b®H
rV9k8786
J°S³?3® Q® V9k8-°. J°S³?"%J ¥8²) ¥8²­V·G8GJ°S³B%9 .¯8§"S
%¡H JMS% J'##iMS0Sv8¡#7v5´
%JS#w%.S%90SJSJMKMSGwUp#w%#=J#S%.ZWS SiMSZ%JS#w%.S"´
GJS( ..S%v#N8?<B%%J ·G%¢LSJ8%S%vS0?Jp¡,S#709%<#8B8%ªvp8·Gª%

J= "JS#J1SK·hST=1E%JS%#J°-6%1'·GM?<#S#JZSJM{MS'%<%8M°%hJS%J1
6S SNMSS7%JS#<%i#JS"´ ¯p1J" )Ä<SJ%S¡,8 7<%SH8%JS<M? 
1JSES#wJS%°%SOS8O·GM?L%B·il6SdS=M'%JS#w%kS#Sl6%O19
S#7qv8pJ8§vSJ?ª°lSJJ#J°wvJp%
J%
%
v8R%9S8S%µ#ª%J ·G%#¢-%=#
%S8F7<%S#S
8%DSMµ%°%%?SJ<-6%=SJ
 .¯ n6¬­MS5Z±M²%² /³?´ hS97S=1.%
S # SM9 M5hl%J%pS#_#J9#8MwS9M8hSJ#S°%%S¡,8 7<%Sr#_%
%·iplvU 98#%*´«M? #_JS%°%SS#° ·iM % %JS<%
#S5'S S%¡,8
77%u·B##B#J1S%QSQJS%9MY## HSJMlM? ·B##'JSp18 8% 8S8´ « 1E%S8
·B M? #l7#J° ·G 6S % %JS#w%-S#S"hS%S¡,8 <7%S  &9M_S
JS%YM9## 0S9MM?_·B##1.S#Bqvhb%%´¨ZJ%#%·G
%BSJMi#¢% 7A8 9:(;=<>9:B;Y%J
7; + ! <>9:(;* 
  + B_1J1S877%S% 4 8 9:(; 4  §k%5SS8T7%SJSh9S1S J%ES#W%Y·BS



W^ 

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

MMS°L8°%S8=%G%88%1%ZS+OpIn Qµ%GS7M?­9%171p9%S  ´q¬µ76JSS8
J#8JSiSF#¢UM°%=v8 ·8RS7%SOv· #R¯1S5
 "´



=
-h.w*±M²%²±³FJªMS¢% <=S#<#MhLSS<JS.µ8S7ª%J% 87S<17%
v8	9bSS8°9#MS%98S% #HS_Jp¡#°%SH9#YSJ<V8ª¯p1S ¦³F%h7LM¡
8%3SMQ%°%%S9<O6%
¯ 4D¤=´8%9S9MSS#8JMB%°%%S9<O#.%S°Mª. -h.-MS
77%8ZSSM7#­S_1%SSvJ9#°ªªMS¢% J%¶7p#79QS8JEqS_80%
#JSJ=J#SS%J1 QW S<MSSF%JS#<%'6%q7%S<MJ9S%J#M#ª# S<8%Sw%'¯ 4D¤=
SMS#SWp#°.³#S#"´Z¤B
%JSS#wJ%9M9###SD6%'7p#°qS4 8O88 OT%OVM4 SS8i6S
%T%JS#w%#STMSEJT=SO4v1S%O1JS%.{U%  %J [ ¥   E#J8vp¡
.{% kR´ GTMMS°=SOM%#D%   -h.7JSMDSJM{SJ'SSJ#S#°=RMS¢%w9%#J
1J9DSJhS%7hv%"9p¡°%S<9#YSJK%·#¡,¢·B<p8%9SMl%°%%#SJ<D6%
¯ 4D¤=v#J8JJ#°b©
¬ ¯ 4D¤%9ª¬­%#¢¯ 4D¤F´ {JJ-%{SJ#-7pKMST%JJS9%#LJª
%J% 8SM°9MSNvJ%OS9MB#Op88%SJ%w1J9#l._S%7F%°%%S9<8´
0
JFSM?H9= k8?F6S SJM0%E
 -B0­8%8%Sv1S8ZSb<%#JF%i·BJ
#
S71p9#8=7J##°_% )ÄF%S¡,8 <7%S 7J%9#SL´<R%Sw#7v%SS%.S%"·G
8#%hS<MO%3v%S_S
%JSbJS%9M9#S{%9lS=.J0v8%ZSM'J#S1S76S
#JSS%J11¡v8	 9OJMSp´D¬µhS7SBM9#q%vS'SJS#°=7pq8MJSSBS'vJ.¡
%{%   7SBSk8$ 9GJS%9JS%J1%´Z«w1E%S8 -h.vSSD=9MS#8J#MD91SS
wSJF%JS#SlJS%9MY##S  m  K  6´O¤'"lUM#J°79M%788-U%'SJ?R%  4 %J
S'J0k8D%"7pSM -h.7J?M{SJMDSBSSJS°-<D8MJSSOSB%°%
%DJ¡°%SNJ9JSJG1p99LNp8%3M?Q%°%%SJw'6%
¯ 4D¤=§k%88%1N#MS%
w#9Jp#JJ%Y#JS%J1B#O%B%SSS"´
4-JJS#J%#%%·G'6JJ<8#JJ1'SJM{SJ'%JS#S7JS%YM9##S#ZTS ¯ ¶MSh#J1¡
vJ.{%"SB8JSS.J#SS%J1'SJhMSS%JS#w%9#JS"´{©
%bSJM-¥³{SBS
S8JESMS#HJ98p#°   %J¶w%E %S8Tp8%M?H%#°%%SJ<W%0S ¯  
i#JMS ..v88vl%J\*±³<°.k%?7JJ87S
 *r%v8M%bMSR. U 9J#S -h%w<#°
J#SS%J1¶¥N6S SR8SEwS#S"i1JS%._%JSS# JS%9MY##Sq·GJ# vN.SS#
J1pv1<6SsS8%S8S#8%kS%JJkEh6¬­MSS"J±M²%² /³?´¨K#J%##%·-J%
8%%vb%J%!¡
%°%JipJ%<#B1G<DW%iq.J0v8{%37<%S#Sp8%9SMl%°%%#SJ<{6%GS ¯ 
9%ªQS µ7%T%k8?M%"#J8#J9#°7wYSF%9 ·i%¢kk#8M 8%ZM?""%J
R8S%v##OS%7Y##°N6¬­MS"v±M²%² /³?´
¨K#J%%ES8-MS=S<##M#SKv8·G8lGJ%Sw% k1S#%
SMLY%1-S$ 8T 4 8 9:(; 4 ³
%J_SF1J189O%  on 1 ZM?N9%1FS$ 8%.´ -BF¥¦%#¦ "³i%9S8S%iSJM'p8%vMR%°%M¡
S9<D1J9S°
1v.S#%#TJ#S97SM_1S=6·BJ#?<#J8J p%GJ#8JS
#b¯p1S_¦³DvJ%-#w0<%J8G#.S#8%Y##JT°S#°<SJ¡9%1'%5S#SJ1p¡
S%#J°lk%S °#%9%#N%9S#<%D%J­¡,%JS#w%ZS#SJ´ b-J8=SJ=#E8JS8SMS"7%S
v1S%<p8%ZSM­%#°%%SJ<-MS7M9qLS?#1hSJq%S%.J0v8h%GS¡,%9S#<%Z#p¡
SJJJ8{19S#8MS#".*´É%´.S8T%v8Mh#wS<%8{pSSJ%M?_9%1%,´ 0
%S<%
v1S%qSMª9%1S$ 88MJSSh<S#w##M'#.SJS5Y'#'#L1.%-°%SJ9LJS1S#
#µ8?<F%OM?HS9%1w%J%pS#§Z#­ k18K·G<JS#J<%¶%J·G8Q5
 S­kSµ.
-h.89·B7#JJ8MJ
 F#%##%·G=·GJ#L#¢%=wk=M9#=w#.S6wJ1SJ%vWMSJSO%KS
%°J%YSMNS9%1=·BJ#?l8%lk
S·Bl7vhS#°ES_1%SS#Ml·hS_SJ%vMLS9%1
Ò 

g ã

g?g$Õ ÄÖ

W^ W

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

S	 8( I ¥¦%¦#"O5´'¥(/#/³?´ ¨JJSS8G·R79J%S$8LSLSL%
SNJ0k8  JIuJ9JS¶%
°%9% %JS#<%%JS#<%G#S#JF·B#SJ#µS_S¡S9%1w%BS#SJ0JJ8q1JS#J8MS"
%JLJ#S1S<SMFJp¡,S7-JJ%w#8=V%O%JvlwM?N1hJ##9SJ³Z7k1S#%
M?RY%1S	 8wV*´É%´9SS° 
  + ³?´



	 Y
 B  $ ø)  	 $ú[ùËû
	úRû	#ø ùE{ûgøNùC ú   µ   	_g   úG ¹û÷
K} 
« %JSS#J#°HWS SSMS# upJ%<#Q1N7p#8hL68JLJ6 6S %°%%?SJT¡
#JJ8k9EBWMSh%S JSS
#%J9S8Mvl1Y##8B<B%%#°%%SJ Jp¡,S7v1¡
J%´ GL8%8M°#J°b#J1%S#°_8S%#R#6%<MS"·G·G8SMYFw%9S%#R7J%J#
#79S%7.Sw 1w7pO%88%1%´ 
#S1J.S#°µSJNJw8J#ST%Sp8#Mu·B#S #p¡
S6°­SQMJJS%J?#MR .vb%#6%<MS5i¶vSS%Q1%SS#MS v8·G8 #6%<MS
J%ES# %9 1Sl<
%88?%1 #_1J18JS9%#rJ9SSJ?#S#°´ ¯SMS# 1L7p#lMS
%°%%#SJT¡#JJ8k9Ex¶ J8VJWMSlH18SS% 1.1pS x¶%J ·L%.S#89MlQ·M¢
Jv8=vJJ HSwMY#w%88J%1µ%OS_7J#8+
´ 0=
 J%S!¡pJ%< 7A; + ! <>9:B;1
7p{#F9%­¶SwS%<bSJw<MS SMS#S#w%SbSMS# 7A8 9M:(;=<>9:(;1q7p*§ZJ S
S%7Yb8S7E%% # 179SMS#H%BS_SSMS#S#_MSLJ= v8S.8´ª«¶#S8q8%S%S_S1¡
SJ#S#°F1G7J#MS-SS9#S#J°F%88M%pv8#%##0°%wSh#79#80%"ShSMSS#M´
«ª1.%83_179M?MS%R%8S·B<#°_#J1S%T#ªST%7J.-%#6%<MSQMJvM
­vLS( JS V%wvpJ # SNJJ%w#L1w7pW³ %J8%R6S8<#J1S%S<#
%88%1%´
Z8JMY07%_#.8SS°RSJ%HS_1%SS#MS v8·G8 #6%<MS .J%.S ¶%J 1
7pD%88%1 #
S<9MSST%SJ7S#MSJJQk8 ·8­S<#6%<MS J98p#°_1
7p#LMRJ81SS% F8%#H IpF*´É%´
v8·G8\SSMS#­%J J%S¡9%<#Q7J#8-% J%S!¡
pJ%<#B%JbJJ%w#'7p#8´{¯v8	 98%%·G-MS°JhSJMGSh9M%788i%SS8#M_·B#Sw
9MSS8J#Mh1S-7p3S#<Mq¢%8N9M%<88-%DSq1S-7pMhSTSY( .JEB9°8
8%*1pvS#°T%LJ1pv1JwSS°<%JLS79-##¢7v8·G8 JSJSO#%JJ8Mk
91SS
#lS .¯ u%9LS=J¡,S#7=pJ%<#8i%   ´
'B8%#56S ¯1S>B.´A±wSJMhSS<MR?%JSSNJS%9M9#Si#RSpJ%<#1
7pMS% J%##SMS%0ES8%%1SS{SJ'%°%O%vJS%Y#JS%91{%JTSJM<< 
%
J= v8SJ1TMSN9bµMM#MY## ­#
 
  + SL<U#<%G%9S8S% J#S%91bµSLM
%JS#w%5#S#"´¨JS8J·G%9S8S%SJMBS5
 F88 Iw%J F6MSSJ8K I7%JSLJ%9M9##¡
SMS'S°9qS<<8#'MS9J
 
  + rM±´iJ( ES#%.M?b9J8 #{J1SSM##
9#%SN·iM #JSJ'S9M-MSqMJJSp<MNJ#SS%J1 
  + rM±<6S S0Mh%9S#<%
#JS"´{N%=JS8##%·GF%S=7%LJS9#1lJ#S%J1=7SJFMSO%JS<%vM¡
#S#7. 7 /
	I; +  ,  <>9:B;6§S#qJ#MS#<1 Jw{SJM 7 /
	I; +  ,  <>9M:(;  
  + rM±p·B8h%.<8MSJ
MS_Jw9#<MRªS_%S<<8#7b#­%JSS#­JS%YM9## ª%   ²N6%q°%%J.S
(J%9 ÎÕÄÏ?ÐÔ  R´ i 7; + ! <>9:(;9%#Sq7%SSh7%_J#SS%J1Bv8·G8l#S#JDp#SbJ¡
#°lM?­%J S7M=%JS#<%KS""*´É%´ 7A; + ! <>9:B;  7 /
	I; +  ,  <>9:(;*´0¤B.Jk·G7v#8%
STSJ81S
%S 7; + ! <>9M:(;Z7pZ#-Jq_SJqV%1
SJM<¥³BST%JSS#NJS%YM9##S#'#
SwJJ%w#T107p{ML
 .9%#SMS%Q#.S#8%{%1SJ= v8S.=JS%9# #JS%J10%J
*±³ 7; + ! <>9M:(;#J9S1SQMJJSp#wMSb¢%8­9M%<88 
  + %OS_9%<#<1T7p*
p# SNSMS 7; + ! <>9:(; 7 /	I; +  ,  <D9:(;  
  + rM±´ 
#S1S89%98<# Sª%88%1 %
S
pJ%<#L%9 J%S!¡pJ%<L1_<<MSR1pv1"'%_HS°N7%SJSR#b#¢% 
8MJS=SF#79%1i%KSJS#
#SS8°J#M?SG#_S=%JS#SlJS%9MY##S´¤'
v·G8'%ZS













W^ %

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ



78 9:(;=<>9M:(;J7JD#qS7JiFSJGV%1{SJM 7N8 9:(;=<>9:(;  7A; + ! <>9:B;|x9JZ96%{S<%#p9S%9
#JSS%J18´l¨%qMS°%8J%9 #9S%J18 78 9:(;=<>9:B;1JS#.S %8S¡,S#<M 7A; + ! <>9:(;*K%J
1J( .S 
  + K 6%##°L J#S1J.FSSwS8°9F%OS_M Y%1wSJMR#



JJ¢%­H19#%S%´r¤¶1J8#JJ%SRUM#J71b<7MSNS#Mr. SN1pJSS
78 9:(;=<>9M:(;  7A; + ! <>9:(;  7 /
	I; +  ,  <>9:B;  
  + rM±´
¤'#¢MM°%v8·G8QS07p#h#B9_S
V%1SJMZSi7p#%#EM798·BSUMSp#°h8°%S8Z%J%88%1% J%ES#W=S k1S#%
S	 8 4  8 9:B; 4  %ZSSJ¡9%1=%D#JSJi#¢%b<v=Sl  7J?#°TM5´







	 Y   úG  û bT ÷ø dDYJy ú   µ    Rø  ÷ø ùE õ9ø  ÷
K} 
'¤ 
1'7p#G8%%vL#L¯1S9 _xB7%81JEO6%iUM?#M9###<#_1E?%vJJ1_7U¡
S=%9S%9 J#w8J %3*´É%´ 7%9 	´<«µS%##%SMH10#=N%J MM#MYe6T´
GJS( ..S%F1<9SJ#7%#0k%SL
 .J%SMS%q%JL
 J%.SSMS%78MJS'S
VJ#I  !-n   2&1Wn  K H%96T2´ G8%J7SJ8ªMST9%  S79qSJ<<MNSMS#S#85--9w8J
<#<M°=· SMSF%J+
 .9%S!¡pJ%<#-1B<Gw°Eiv
19L<%81J.'6%U6T´
«R1.%8v7JSJ1q6nO%S#l%JS%L6SxS09%<#=1
7p*§J%-J#S8JL#
¯1S)B.´ /=9SJ#1e6 #D°%8Mb#T%8179 O%J<#DSJ9( ES#0J#8M"´
¬µO·u%9% 8GSOJMS%JSJGVJ#6rJSJ#10FSOpJ%<#1D7J.%JT88?<#
·B8SJ8B'%88M#_8JS.SOS%1S9%5J#?9Sb%DM?R1ShJJ8 p´
¬µ<6#· -Bw¥¦%#¦ "³F# S868S#°_NS<9#YS>6 W%Fl°%­JS%9 #9S%J1
%0S %1      n Q I  !on  62$1n  K   'Uc F³?´3«­·hJM6#·B83·Gb19S#80·Gªv8	 9L
 .SSJ
S#MS°wlS ''c --%{?%J .¯ h9J8 30¥³h¨ ·hJMhV%<##L%J#?9S9
MSqS ''c -
?·B q%9 *±³ 4hS0SJ0JJ#1Q%J %1SJ%R ''c 

#.S#8%##l9#Y 
G%S1
 .JSJ
8%Qvq%JS·8Sµ9S#°wSS%JJM SMS#SS#8%Z°%.pJS¡,%M¡ J
S
´ 4h#S°
S 'Uc 
O6% <MS9#S1S8%·GMJJ#<MFS0%1SJ%5J#S9S#J9S#°71.S#.JJ
%JJUM?#M98§.SBMJ9S#<MSb#DS°.9OSh·B#O?%°%'%5M?b1SS%9S8S%
%1SS#JJ##9J%%#%8%#·B#°=JK%#<#SKS#M<FS'v8	 98MS#T%YSOY#TS	 8
#TS'SS%JJM
  ( °%pJS¡,%WM¡ JDW%{JS1S8O%JJ UMM9'6%´É°´pSJ7%{S'v8¡
6%7q. -B?³?´K«9%"%·GGJSSi ·M¡S%<9O =#7%°%%.¡¯p<JbV 0¯9³"°%9S¡,%WM¡ J
85·hJ# %SJ7-ST1pJ17%%797V#QST6% %8JqJ#MS%79SR6J91SJ
%w 
¨K?³W%7J#SS#J11.S#Jq%J MM#M9´Q¤'JbJ#.k%SJS#0W%TSl 0¯­
SM=S9M=S<J#?9S9'JJ8?°<v%SµS%79#-M<#.S#8%#NJ9J"´=¤'7 0¯
{SMSS# .9%ES	 9SJiwUp#w%9#S%J1iv8·G8TSi·Gq 
¨KD%JqSO.9#..k%SJS#
#S 
1L"S+
 FJ#S%J1( I0v8·G8lSm#w8.S<#MS°%7,¯p? k8B£ ªi#%%5¥¦%¦M²E³?´
¬µ J1JS#8SBV%<#q%5J#S9S#JZWSs·BJ#?7S-#JJp#J9% 'Uc 
Mh·B"´
¤Z%###M ¥¦%"¦ D5´{¥%¥®³F#JJ8M=SJMFSb.J0v8=%O8MSJFS( .9S ª#8M<%9S#<%
#JSJiJS#J°qJG%#°%%SJd#iMJ9S#<Mb1v.S#%##wJ#?9"´ -B·8%8Y=J#
S8v%S J%##SMS%=SSJ#SiW%BwS#°7¥8² ­¥8²7JS%Yd#JS%91%´bh#°TB8B%i¥8²e­¥8²
%JJ .¯ 8.·GF· v8SW%?mT7%F17JS9S%=%J%pS#8´D¨%B%RJS%J1%p·GF1T¡
9-S-·GM¡S%79#
 q¯7OSMSS#hW%GS
.J9..v%SSSJMGSG ''c \#1pkJES#%
J#S95´¤' J?=S%7971J#S
%GS<%1SJ%{M?¶1SS
 q%98S%­%8b¥MÄ²%²%²ª#p¡
8vJ.7#%8´r¤'ª1Jr%79N1J#S7%<¥MÄ²%²%²pÄ²%²%² %JJ S%79#<·B 6S
%u1pv.S#%'J#S9S#¶·B#Su­7% N179uWS S J?<S%79#%§BJS1bS%T¡
9#°N6S SlS8%S8S#8%B 
¨ #qS( .9S­¶SJb9MS#8J#MTSMS#S8%GS%W ·GMSLY%S¢MM°%
·GF79%R#NB%J%pS#´{¬µ0SJ·\SJJ#?9Sb%KSF¡,M%#B%Sp8#MN·B#SLS
W^_

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

16

Empirical CDF
1

14

0.9

0.8

12

0.7

0.6

8

F(x)

Frequency

10

0.5

0.4

6

Exponential
0.3

4

Actual
0.2

2
0.1

0

0

0

0.1

0.2

0.3

0.4

0.5
P Value

0.6

0.7

0.8

0.9

1

0

0.5

1

1.5

2

2.5
x

3

3.5

4

4.5

5

¨K°S\B 3Uc586T¨K°JS#3L
##9S¶%'Sl¡,M%#qW%TS 
1S#°QSLJ#.k%SJS#0SJM
S5''c
7%7¥8²q ¥8²µ%JJ .¯<MSN1pv.S#%# J##9 JJ8   ´
'B°.¨K°S# 3K¤'h%1SJ%9%Jw1v.S#%A 'U
c 
{6%SJ¥8²'_¥8²#9S%J1'·B#S7S
S<%#SO¡,U%%´
SS9S#°  0¯HwSSMS#S#8<HSJN8W<#l%-¨K°JSHB.´ 4i   ²]Ä²¥MG·GLS 
1<SNJ#
..v%SS6%N¥%¥_%BSµ¥8²%²µ#JSS%J18{*´É%´SM 1<9J8 #JKn01pkJES#%
J#S9ª#ªS°JH¥8T² V %S7#JSS%J18´=«ªSJq?°E=S%G¨Z#°SeB."·7SJ·  
¨K
%Dv%SNS0%1SJ% 'Uc n%9NSq1%SSvJJ°71v.S#% 'Uc 6%BSJ0#JSS%J1F·BSRS
S<%q¡,U%%K%{( JM%ES#%DS_MS°%78p#MS v8·G8 Sb ·HE -¨K8´­¨J%7SJ#
#JSS%J1%%J %#%S8<#JSS%J1q·BSd ²]Ä²¥MSl ·­J9JSJFJ	 k8qJ#wM# #
S786qS%##8´µ« 9MSS#8JMK·L%9S8S%l6MT68·8<·O¡179JS9% WJ9H# Q9
1pkJES#%"9#YS"´
0
J_SJS_S#JW%1 ¤Z%###M )Äw%9S8SUMS# SJMLS1
 ''c -L9J8"
  MS­M9JS¡
#<M#u1v.S#%##uJ##9"´ Dpv.S#% 'Uc 
L%# MQ SJ 1.1pN%0p8%
M?µ%°%%S9<B6%
%S8D<q==¡JMQJS%9#<8´h¨J%=1p%7Y%* -B<¥¦%#¦ "55´K¥%(¥ "³hS8v%SS
J%#SMS%#-S<##M"SJS"6%ZB%°%{%J#8%.Mq%°%%#SJ<6%´É°´%¬­%¢.¡¯ 4D¤h³56%Z 4%6h¡
¯ 4D¤=´ -Bh%9JSJ%b<JMRSJMBSq8p#MSL6SxSJ1pkJES#%%
 F#%$ I<B
VJJ1S %iJS%Y Jw89 3G''c 
=%OJM?8b6%S81³-JS%9# JS%J1FMw7%SªV#S?³
%88M# <# . % 1pv.S#%'J#S9S#"
´ 4 S<##MTS#MS#JSJ JTW%bS
''
c 
J98 p´D«b¨Z#°S "·-S· 0S8M8¡,9%G%"S-7%wM?l1 '%8SJS
M%#
%3S= 0¯bS'SMS#S#-6%F¥8²\ ¥8²T%9 .¯ 8´D¤'=JMS7#J9#8MhS9MOS
U%#J-%
S< 0¯ SMS#SS#<#=#.%8#RJS%v%SS#J%ZR#JS%91<Jw8J#%´wR%Swv8	 98%%"S
''
c 
OJJ8 qMFMJJSp#wM<1pkJES#%k6%'7p8M1¡,M¡Jb8JO#JS%918p·BJ#
Sh1v.S#%9M9JS#<MS#<8°%%6%%S#8#JSS%J18E%´É°´%GS·Bw#wSB?°ES#
%¨K°JSDB.´©=%LS°J$ 98%EGJ= v8SJ1Gv8·G8R 4%6h¡¯ 4D¤ %J_S2 .¯ iSS9?%#
SbkS9#L%'N<%SbJ9%8S%K9J7"´l¨K#J%##%5·_%wS9M -h.q%#RJ7p¡
M­SJM=S ''c -
%G% #JS%91-M7·#¡MJJSp#wMªªl¬µ#9J#ZJ#?9S5
l°%8%	 MSµ%GS<1pvES%DJ##9S"´ 4hSJ°µ%FS8v%SµJ8S%5SJ# YJJ#°
%#7?%JS#MB<S2 ''c 
'%ZS#9S%J1'SJ·BNL¨Z#°S
 "T·B#S qu²]Ä² ´
W^

C

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

0.16
p=0.05
p=0.01
0.14

KS Test Statistic

0.12

0.1

0.08

0.06

0.04

0.02

0
100

1000

10000

100000

1e+006

1e+007

1e+008

¨K°S" 3
¯p8M8¡,9%O%Z7%LM?L1 ³%8SJGS
M%#-%3S= =7%°%%S.¡¯p<#
0SMSS#76%017YM#°NSJw%1SJ%SM 109#YSQ·BSµS9MF%OS
1%SSvJJ°=1v.S#%kJ#?9S5´Sc3MS°%-M%#{%3SBiSMS#SS#hJJ#8M
7%S°J	 98%.5J= v8SJ1´¤'%	 8.S%E#5#J9#8M{.9#..v%SS"S 
1S
SSSJOM qu²]Ä²¥0%J qu²]Ä² ´
Search cost

h18Y·q%J% 8F·B8SJ8BS'Uc
'9SJ#1LlSpJ%<=1h7p58%ª%81J.
6%GSJ-%1SJ%*'Uc
G%98S%l6% ´{¨J%O%L%5J
¥8²\N¥8²T%JJ ¯8·G
1<9
S' ·M¡%79- 0¯TGSMS#SOW%SBJ#..v%S#DSJMS'JS9#1<%J<%1S9%'Uc

%°JMTWS S7S%<<JJ8##J°<J##9S"´
¤' JS
%79T1JS#SSh%S7%1SJ% 
%98%R%8<¥MÄ²%²%²_J8vJ.O#%#B%   ´h¤'0S1Jª%7919S#Sh%DSb¥8²pÄ²%²%²
#J9JJ%Z1S wJ­ªS#<MbSw9SJ#1 NV8L¯p1S&B.´ /³?´R¤BwJ#1S89%J1 #
S
·G7S%79#-S$ 8GS<iW SJ-1'%8Ml·BSb%JS%#J#J°0#J9JJ%S%7Y8´K¬µ
JHS8v%STSSJ#S06%<S E± JS%J176%7·BJ S#wMS %hSJN9%<#_1w7p
9M%<887#7179JSMSJ%#¶%1SM9%´ ¨%<%#i9JT®Q%hS E± #JS%J1N  (¥ V<³?·G
S 
1BS0.J"Ev%SS#OS9M'SF ·lJ#?9S9MS0#.S#8%5M Er²]Ä²¥M§k·WJ9N
8p#J17%i%E 1%SSMS v8·G8 H%J J%9 Jw89%´q« %SJ8F·%?J83S9TS
SJ81B%{SJ0pJ%<#1-<#R%819ES#J°b6% UvB°%J8%#lV%##'l%81J.
W%-S
VJ# ''c q´{88D9#wSMS#S8%# S°J	 Y8%EFJ	 k8S918SJ<JSJ1¶%J­%1S9%, 'Uc 

MSF°%J8%#+
 .9%#SMS%_#.S#8%*´{¨%h1p%<9%91J#8OSF9SJ#1L%JR%1SJ% 'Uc 

6%=S7·GR#JSS%J1
##J#°_S7S<%=%Jµ#MS°%S=¡,U%85%FSJ·Bµ#µ¨K°JS<¦´q«
SLv78%%iSL ·¶9#YSJ0MSL k1S#% #.S#8%*´ « S1
 OK  !-n78%%iSL·G
J#S9S#JM9kMhwv2 J%#SMS%#_#JES#8%,9SJ?NSJM'SJ0%1SJ% ''c 8%ªvF8
MJJ#<ML_J6S#°TS=JS9#1 ''c %°7S=¡U#8´
«­VSS8qSJv%S=%OS9#=%98UMS5Z·Gb<%Sb8MS8VJ##Q1J#8SJ
 /%®ª#JS%J10#
·BJbS-9= k8J1v8 ·8lS
%1SJ%"%JbJS9#1 'Uc 
iMS
SMS#S#8%<S°J$ 98%EGM
qu²]Ä²¥M´¨%%?bJS%J1%.·-179hSB 0¯7SMSS#O6%SJBJ= v8SJ1'v8 ·8bS
JS9#1 'Uc x%JH%¶1pv.S#%{J#S9S# ·BS­7%H( J%{QSw9SJ#1 ´N¨J%
%#DJ#w8J=#JSS%J183v8	 Y8%#RSJ<·BSµNJSJ#1 ). ¥8²TS²%²%²pD·w6%#KRS 
10S
J#9.k%SJS#SJMiS-9J8p#°F9#YSJDMS=#JES#8%kM qu²]Ä²¥M´GiJ( ES#%p#
STpJ%<#F1S
<3·8qM9#0l%88MRJSJ#1 UkSJJS9#1ª%9ª%1SJ%R 'Uc 

W^

L

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

Empirical CDF
1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

F(x)

F(x)

Empirical CDF
1

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
2.5

3

3.5

4

4.5
x

5

5.5

6

0
2.5

6.5

3

3.5

4

4.5
x

5

5.5

6

6.5

¨K°S¦ 3
 
¨Z_%=SQJS9#1r%J %1SJ% ''c-_6%b · ¥8²E ¥8²¶%9 4  ¯´ ¤'
¡,U%O6%'SF 0¯_SBSMS#S=MSFSv1S%#_²] B@/w%J5/u] e­¥8²  A ´
·GJ# vRSMSS#8%# #9J#S#J°J#SJMY%´ ¨%lSNS<%J#°µ%u%9r7JJ #JSS%J18
·GN %98S%NSMSS#8%# S°J$98%ETJ=v8SJ17M  x²]Ä²¥Lv8·G8 SlJS9#1 %J
1%SSSk9J#°<1pv.S#%R''c
8´2-h·G8%8Z%ER9=k8J1
MS7##MQlS786hS%#h%
v%SNJ#S9S#J8ES9LSJMBSFJS9#1 ''c -BJJ8¡8JGSJ1%SSvJ9#°q1v.S#%
''
c 
8D%´É°´%7SJ·B #HSl°.0S#Jb%B¨K°JS@B.´ « %S8T·%98DS ''c -qJJ#1
­SLpJ%<#b177J8MJSJSbSJL8MSJ0WS Sl1pvES%6% SJM7MS
%98%N#LS2 'Uc 
O% 7L%L%JN<J#J Jb8J_#JS%918´
©
#% ¥³S-·G#!¡,¢p·B_Jb8JT%3%88JMwSS#<MS#°0S
7%_%3%_1v.S#%
%<1pvES%!¡#¢%LJ#S9S#H%J *±³TSLV%1<SJMw%. F#J7vA I 7pi% µ·B##
1SM#ª6%#ZR8M9SS7S7VJ#Z8S%#K%GS7<9J8p#°lRM¢%µJ%"5=SJS
JSh°T8#JJ1-bSJJv%S{%3SJ-..v%S#S9Mi%.<%9S8S%LJ= v8SJ1v8·G8
S{9SJ#1F%9F%1SJ% ''c 
MS%3JJ#8MS%D%%.=< 
%ZJ1SJ%# &9·H#=SJpJ%<#
1B<,´
|D}

  Y      	 	 E  C9 	 
ø

û

ëø

#ø ù ûgø ù

÷

¨%R p_%JµMSG7%09w8J 
%N.¯Q#JS%91#3¢J·B0h#91S%%ZSG.9v83%
·G%S¢ &9· 9MSSS#J.OWh5#MMF6Sm¥-V1%SvJJ#J°'h?%J .¯?³k V1%SvJJ#°

 &J·B%P¯?³?k*´É%´v%h7%091SS0B#.SpJJ1"´O¨%-1p%<9%kS07% 	O9J8
µ%98S%uW%<Jb®y ® %JN·G%S¢ &9·Fi%J &J·BS% .¯ 7MSR#± "M²p% /#(¥ /CB.'%J
¥±#¥±CB.vSv1S%%´G¤'0J= v8SJ1OS89S.B%N%J8¡,%¡wM°JSJ0#J1S%0#L%8M°%
Jb8J %P
 OWh<#<MMrW ¥ª qrM±H%JrM°%# 6S qrM±¶ µ´ ¯p#<#M<J= v8SJ1
MS_%98S% 6%L¥8²yu¥8²ª%JLD·G%S¢ &J·{%J &9·BSJ% ¯ 8K·BJ8SwSl7%
  	 MS
A
Sv1S%#
 /p(¥  J(¥ / W] /#µ¥8²  J%JR±u]A®%± ¥8² ´{¤B=%WN1S<Jw89<%K91SS
.¯ G#VSSJ8G##JMw.wShV%1iSJMiS
7OJw89B¥8² R¥8² &9·BSJ% ¯  S( JS
%N%8M°%0%{¦M²%²@L '  K ­8MS9i%! 7w#8M%N%JS#<%5S#S"´
W^



 $¡4¢£

{%9

®



®R®

Ú$¦,ÜÄÝÞ¢­ZÝ

¯p$8 p¯ J1SS
'h%J
¬µ%¢ &J·
¨K·BS%
'h%J
¬µ%¢ &J·
¨K·BS%



GhRJ
7; + ! <>9:(;
²p´ ""
²p´A®%±
²p´ BM®
²p´ BU²
²p´ BM±
²p´A"® 
²p´ @B "
²p´ /M²
²p´ #
²p´ J¥
²p´ #
7A8 9M:(;=<>9:(;
²p´ M" ²



¤ZM9± B3 ¤'J  ( U%#JO%DS 7A8 9:B;C<D9:(;3%J 7; + ! <>9M:(;31h<O%   %JS%#L6%-®"
%Jª®N®T%JJL·%¢ &J·F9%J &JB· %5.¯8´


¬µN9S8#JS­8k% SJM7SR%88J%1 %hS 78 9:(;=<>9:(;'1b7Ji6%#Tµ%J68
6S ?%J ¯ª·G%S¢ &J· .¯l6¬­MS 80%*´{±M²%²/³?§i%JJS#J%D1pk8?#7.Sp#
S#w##MBSJShW% &J·BS% ¯8´=¤ZM97±_S·hhSJ  ( U%#Jh%S 78 9:(;=<>9:B;Z%J 7N; + ! <>9:(;
1w<06%b®@ ¶%Ju®HH®Q%JJL{·G%S¢ &9·Fi%J &J·BS% ¯8´ ¤'lSS9S7#p¡
J#8MwSJMFS 7N8 9:(;=<>9:(;{7p{=7%Sw%88?Mw­?%J ¯=S9%µ­SJ8F·%¢ &J·
%J+
 &J·BS%5
 ¯ p%#S°L%88%1L#<JS%'·B_%JS#SJ#°q6S ·%¢ &J· J
 &J·O¡
S%1
 .¯ 8´-¤' 7A; + ! <>9:B;7pB7%ST%88M7S9%RS 78 9:(;=<>9M:(;Z7p3ªv%Sªvh%
91SS .¯ Z´ -B·8%899#hS
M9=#7JS%7.S#MS%
TS 78 9:(;=<>9:(;"7p*
%88%1ª%{S 7; + ! <>9:(;<Z1S%S
·BSQ#J1S%-# OWh1G´ 0-%8?%#*kS 7N; + ! <>9M:(;7p
%81J.S
W%
S##°ESl%8FJ%D%DSqUM?#M9###L#N9S%9 Jw8J#l%98S% #RST7%S
Jb8JhSJ1SSµ® Q+
® ¯ 8´0¤'TS°J	 Y8%E
J= v8SJ1T# %88%1   /MT² V<³h#MS%T
%JJ ¯ B%#hSvS9###wS9MhSqpJ%<#01-<<RkqJJM9#F_1%S1
6%'SU Y8J8O%KS 7N; + ! <>9M:(;57J*´D«N9MSS#8JMp6%1%h%S8hSJ% 4 8 9:(; 4  <Lv
1.9S°NQS_9w8J ª%hJ1S .¯ 8K%T0<¶%qv<vSS9#7 <S
vJp%O%   N91SSN#9S%J1O%BwS#79#h%9d·i%¢Y´
G9S#8OS=?%JSSl9S%9M9#SGJJ8OSFJJ%w#
1B7p"6%h®  w%9N®R®
·G%S¢ &9· %9!
 &J·B% .¯ 85%9S%#­J#°lS<S%7Y##°L78Sp%°% S1?vQ¶¯¡
S B.´A±´<¨K°N¥8²LS·h=S7JS%YM9##S#'% l1.S#J#°lL%9SS 88NS
MSS-%9S#<%Z#Sª6%- ·N® QL
® &J·BS%µ#JSS%J18´=¨%=ST#JS%91q1%SvJJ#°
wSJ086¡J%J5
 J°S%YSF%JSN9S%9M9#SiMS°J_JS%v%SS9%"wSq8SS.
J#SS%J1w6S S_JMSq%JS#<%#S5·B9#H#019S#.0·hS­S_SS9SF%9S8S%
6%q?%J .¯ 8D%´É°´%7·B M87#H¨K°JS+
 ´¶« 1.%8GW%TSL#JSS%J1_1%S1¡
vJJ°=TSh?°E¡9%JL
 9°S%Sh9S%9M9#q% q1.S#J#°T<%=88iTS
MSS%JS#w%{#S#µ#= v1S%­1JS%.0M m²]A®´L¤'<1%79###JM7L¢%8
v#.BS8°M9#°_S0vJ#%h%    J1SP
 ¯ U 3BS0?%JSSªJS%YM9##S#'MS
S°9	 98%.S77%ShJ88S%°%8JiSJ%_SSh%9S8S%w6%i%J .¯ 8%W_8p#MS#°qS°M¡
J	 Y8%EST6S S FJ%%9#8%$ IlV*´É%´p<78#BMSJJ 
  + rM±³{W%? # 2 Kn Q.J%SMS%
%J!
 J%ES#SMS%<%v1S8´q©
#%µSJ #M°%TJ8#MS#J8"
#
JJSJ#S°TSJM=1F7p#
9%­¶S#<9TSJ<<MQSMSS#88Zv8	 98%# 7A; + ! <D9:(;V3V%#Kª%81J.6%N9S%.S#%
JS%v%SS#l%SUMM9## w#lSF9w8J w%D91SS .¯ 8´












W^

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

1

1
Probability of moving closer given grad=closer
Probability of moving closer given grad=equal
Probability of moving closer given grad=farther

Probability of moving closer given grad=closer
Probability of moving closer given grad=equal
Probability of moving closer given grad=farther

0.6

0.6
Probability

0.8

Probability

0.8

0.4

0.4

0.2

0.2

0

0
0

10

20

30

40

50

60

70

Distance to the nearest optimal solution

0

10

20

30

40

50

60

70

Distance to the nearest optimal solution

¨K°S<¥8²3h¤'l%JS#S JS%9M9#SW%<7°µ88<µSLM7%JS#<%iS
JJ8 76%BJ#SJ1'®R® &J·BS% ¯´

-9#J=v 8SJ1O#NS=%JS#SlJS%9MY##SS#MS#%F<S%98S%RW%'?%J
.¯8J·Gq%98S%qb#<9%1BRS%8%#K%88%1N%DSJ0pJ%<#=1S-7p*´O¨%F®e 
%J ® ­®L·%S¢ J& ·  ¯ 83S  ( M%#F1%SvJJ#°lQ !K  2 -  !K  2 - S8°%SSS#µ%OS
JS9#1Q%8SJ=%1SJ% TMSTSSk1S#%ª²]A¦CB_%Jµ²]A¦# ´T¤'7%J%#%°%J  ( 6%=v%Sµ®e 
%J ®­® J& ·BS% . ¯ #²]A¦%®´ª¨%7%#9RW8·m1p189SJ%JS%J18KS_%1SJ% <#
%·ipi·BSJwq6%1%O%R0
/ %3S-JSJ1 U§#lT8%FGS=J=v 8SJ1B118NqV%1%
% ´<¯#<##M-SS9S=MS7%JS%#J ­LS<%#K8=%G%JJ<N°%8?Mr¥8² ¥8²L·%¢ J& ·
%J 9& ·BSJ%5. ¯ OW%B·BJ#?l%JSS#lJS%9M9#wS#<MSR#O179SMS#J%#bW%S#9%´
'¤ JwpJ%<#<1S07p%88JM­8MJSSbJp¡,S7<pJ%<#8=% N 2 KCn Q
%JJ %J0J1SJS ¯U%SJ°SJ%JSF9S%9M9#S"MS .9%#SMS%=J	k8S.
#0Si·G=vK%JJS%9#L´ -h88S%°% 7#SJG?%JSS0JS%YM9##S#5%9?J1SS .¯ 
%JJ#SJ%#Q#JJ8MFSJM0UMM9## QµSlJw8J#Q%OSbJS%J1q#9J#¢%ªªv
8MJST.TS#7YGJ<<MSqSMS#S88p#J#J°BSJ91SJD#qS'%88?%1q%k{SMS'%J
J%S!¡pJ%<B1'7p#´K¨K#9%#%·=%9S8S%=SJMOO#iS##YvSSYBSJMiSF%88J%1b%
S<pJ%<q1S7pK<µ8°%%J<S°J	 Y8%ESªJJ8
VJJJ%7.S%#ªJ= v8SE= .v=%
91SS7SJ%µSJw1JS8S­8%3%´É°´Z·BSµSJ°R1%SS#MS#µv8·G8¶SY8S=%iS
%v8MSN9MSJ9,-+*M´
ªMWl8'%*´5¥¦%¦%¦³i#JJ8MhSJMOJ	 k8S91O#wSJ
9w8J 7%3%9 %Jl·%¢ &J·
.¯ _MSµJQ 9= k8J1b SJQS$ 8ª%0SQSM Y%18-%l7%S .rSµ7%
J#SS%J1Rv8·G8 %J p8%'%JS<p´ ¤'JR%98% 7% <U#<%hJ#S%91 
  + 
µMSS7%JS#<%OS #J1S%ST·B 7#J°Q6S %9 .¯ Tµ·%¢ &J· .¯ 8
%J M°%# 6S ·%¢ &J· .¯ h+
 &J·BS% ¯ 8§vSJS-W%F®  L%Jµ® Q®bJ%9 8S
MSw·BH#µ¤ZM9 /´NGJ( .S%3JFSSJ#SF8S%bR8#M6µRM6#­8q%*	´ )ÄF%?°#J%
%S8S* 3O°%QSJM 
  + #h_<%SSq% 4 8 9:(;  4 Vv9= k8J1hNSTJw89L%{?%J
%JNSJ1SS ¯ OMS#7YwJ=bJ= v8SJ1'#lSJ= k1S%qS	 8=%KSSMª9%1%´
W^7

 $¡4¢£

{S%9#

® 



® R®

Ú$¦,ÜÄÝÞ¢­ZÝ

¯?J1SSF¤.v
¬µ%S¢ &9· ¨K·BS%
/CB.´Ä²¥
 ´ "M²
B.´ "M²
#® "´ "%®

¯p	8 'B%JJ
±p¥M´ E®
±%®´A®CB

¤ZM9/ B3 ¤'Jq7% <U#<%ZJ#SS%J1lSTMS
%JS<%S#S  
  + ³'%98S%Q6%
® <%JR®R®7%JLp·G%S¢ &J·FY%J &J·BS% ¯´
5| |"} x {  g	- ú[ùE b#÷·ø dDY¹÷N	 	#ø úøM  	  ø  ôöù øÐô#5]ù.ëø÷
4-3J#8JS=#q¯1S / '#5BS#MS%#-?%°E6%S·iM797.SMS=%pSM90M?
6%GSJG ¯ Z´.«b9MSS#8JM##%¢i6MSSOS9l%i%U%91L7%=%v8M%i%JlJ°M¡,8
77%u7J%9#S<bSJMlJ% v8r<JM  #7JS%RSQv8S6%<%J1ª%=SM9
M?L%°%%S9<GW%OS2 ¯ Z´©
#%L%_%88JM1O7pk% pS=1pi#%°#8%v8
#iwSwMS#8%#_%SSBSF#79%1O%ZSF%#°%%SJ<h6MSS'N7p"J1SJSF%J
%88%1%´b-S#wM%9S°%%#Bb#91S7.S%#l7%0k%SRSSM°%8-%°%%?SJx%JRS
%Sp8#ML1O7pk·iMlS
SM1¡,%W¡,S1¡MS8k%´É°´J%O8SS.S7S89S.l.L
 h·h#S¢p
%J ¯p0JSJ#S¢p )Äw*±M²%² ³\,¡¤h¯ 4 n%°%%S9L´l¬µb· SM¢%L%¶JS#%D8 ·iM SJ#F°%%
 J7JMS°LSJML¢%8 v8S6%<%J11¡,9J%J8#°N1<kJExªSJ<v·G8S69  H7%
%v8M%9xwV%##ib#79%1OSJ8'S?J1SS=%B%88J%1L%SJpJ%<#
1B<,´
'B8%#pWs¯1SL
 /-SJMDSJ'°.k%?.p0%k=S 'J98KS *b7%'%v8M%
1JSST%-%O#S9T%JS%#  E%8SS° SL%87%
µ9%T%
% 
S%1Eb%k8?MSJ
 S­%7¶1S8%
9pS¢k´ c58   
 ZU³_J%­S­#JS %JS%# . #.%8SS#°
Sµ%J8N%0 · % 
%1.ª1?S#8%-%v8MSJy(+*µ%Jd(=# E´ « qv%S (O*¶%J (FMS
1.S%# ES#S ·BSJuH1?S#8%'Y¢Y',´É%´BS8b%v8MS MJvML
 9w%l#%_#
ST9pS¢kvS 6 Ò98 : 	³D. 6 Ò98 : M³q*RM6#"¥¦%¦%®³?´q«R%S8=·G%J5<%Eª7%F9J8
_JSMM978%JJ%p#<#w7J#MB7JS%7ESG#7SJh<M¢%Y%w%"S-8SS.S
%JTS88W%SOSJJ#q%DvG1J#8STJ?#°hSM"´ iJ#J#°'TS9#K%98SMMS" h·h#S¢p
%Jµ¯pqSJ#¢¥¦%¦%®³-ESpJJ10bJ#°J_S#1ªMM#%.B%DS * <%T%v8M%k·BS
S=°%%3%Z%818?MS#°wp8%"M?LbS9J8#°TS=%S%1'%K°.k%?.pb8U%#9MS"´
¤'J3%v8M%E·BJ#?0·GO% 5.1E#9DS°9	 98%.S==Siv8SW%?<%J1G%9v%STS
·G#!¡,¢p·BR¤h¯ 4  %#°%%SJ  B·B#S¢p£m¯pqSJ#¢*"¥¦%¦%®³B%JNSJ8SS.BSM1¡,%W¡,S1¡MS
%°%%#SJ 6%FS ¯ Z*¡¤h¯ 4 x h·B#¢G£ ¯pqSJS¢p*3±M²%² ³?´ -B·8%8ZSJ<v·G8%iS
r%v8M%_17<·hS µJ1# 3LSR#J9J1 M 9%1RHI  !  K    _n  IM'J S9Mw
#0%q%#·GqvSSYTQ7%lWS % MS9M S#S¶QR°%9%# %JS<%S"´
GJS( ..S%pp8%9SMl%°%%#SJn9%Sw1ST l8%wv  4Br#7SJhS8%8S#8%
JS%´K¨9SS8%D9#S8JSSv·F%79?#8%T  4B vJ%K6%K9%S#GSM9TM?<%°%%#SJ<
9%NLS ª%k8?M%B#'%OvS9-W%B%J ¯ ´
¤'Jh#%¢<%5S  4BrJ%k8<S°9	 98%.S7179##8MS
8%%9<EG%31i7p#8
%==#=J98M=· J%.SWªM¶1F%"( JM%.S%"JS%9# J#w8J %´¤'J#-V%18
# #MS°%_9MS8{%l78#S#¶ 9%_TSM?  S   %°%%S9L1
´ ''1.S%
·G<7J?M SJM=W%=?%J .¯ -S  ¶%v8M%8%µ#9JJ1TS<%# FM9K Il%F#M¡
#MN¡17v.SO%ZSM?RY%1=WSx·BJ#?lS8Mv#O#7vS9T6¬­MS""±M²%² /³?´
¨%SS9JM%KS<?M9MSw%S# 81 %J¶8%¶k78Mk¶NNS%F%J ·G%¢
W'C



Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

1e+08

1e+07

1e+06
1e+06
Actual search cost

Mean search cost under the N1 operator

1e+07

100000

100000

10000
10000
1000

100
100

1000

10000

100000

1e+06

1e+07

1e+08

1000
1000

10000

100000

1e+06

1e+07

¨K°S<¥%¥@3Uc586K¨K°JS#3K¯p8M8S¡,9%D%9SJi%1S9%M71S J98SJ -%8SJ! 
%°%%SJw8§5S7##J
 H[ H#
Sv8¡#7v5´ 'h°.F¨Z#°S#3
¯8M8¡,9%=%GS
JSJ#1R%8SJh%1SJ%3SMQ1 
W%q¥8²e¶¥8²w%J .¯ 'JS#J° §YS
%¡H JMS% J'##=#Ov8¡#7v"´
Mean search cost under the N5 operator

Predicted search cost



JJJ8hST7%Sq°%8%'V,´É%´v191Y³ µ7%7%v8M%´F©=%QST%98SMMSJY·G
#.SpJJ1¶Q%  J¡,9% SM9 M? %#°%%SJ W%TS .¯\SJM7#079##8%#P 4B
u%#G%-w®y G®HH®i%J ¥8²H ¥8²­8S7%-%J ¯8´ ¤'L%°%%?SJL{·hJ# ·G
% pJ#i8S%##bw¬­MSSH*±M²%² /³?,´ -h·G8%8Y·BS_S
1p189Sl%v%S_S=7%
%v8M%w%J SJLMr81S ?S8MvR79%J#S< µ%J µMSN#.S#8%*´ ¬µ
VSS8%98S%-SJMGM9GMSh#JWS( .STJ1J.8S 69#8%#<Mi7GJ1-8%8S M  %
7%S=8?MSJ?³?%Jl·G=J=#.S#8%v8S#°G6%'%#k9M%788?iWJ9_bv%Sl%°%%#SJ<8
*´É%´>  , %9   + ´7GJ( .S%"·<M<·nM9#qN?JM%18$ 8<S7#79%1
%S  
7%B%v8M%{7SJ' k1S#%S{%kSM9<SMwW%{SJ% .¯ HJJ8D1.S#71pv8#7.S%
1JJ#SJ8´
G9S#8=Sb7%¶M? 1 7J98Fv%S   %J
   ¶w¥8²@ ¥8²N?%J
.¯ 8§pSF1%SvJJ#°TS8M8¡,Y%B#OSJ·BL#lSJF8W'#-%Z¨K°<¥%¥M´{¬µF%98%Jp¡
1pk19<· 1%SSMSwv8 ·8_JS%YsJ#w8J T9J8SJB·G7%°%%S9<8§pJ= v8SJ1
%OLV%1%0%=¥8²RMw1<7H%JµS%?HM?ªR6%1%0%
¥8²%²ª#µSw·%S8%%+
´ -J<
Sw#·n6S( J1Q%p88SS91%N<J#<%ZJS%v%SS#ª%iSw%98S%­9= k8J1F8%­v
MYl<S=Mª81SN%Jl8Mk07J%9#S<8´D¤'JF#79#8MSbiSJMOSF7%
%v8M%i8%b?%<MS#8%#T%8GSh1SS( J7.TSM9_M?wqp8Mh%JS<%9SJ
q%J .¯ 8´«_<%E<8%S8.SJh v1G%1SJ%##HI ^n   L #n 3#<SJMSJh7%_.9v8
%98?MSJZS( JS0JJ8! -8%qvG°J	 98%.S=#MS°%8KSJ%qSJMKS( J0J98 ´
-h·G8%8{S_.J0v8%'#°Ev%0JJ8FSJ  %v8M%q1w7Jµ118JqSJM09J8
S L%v8M%'bqV%1%'%¥8²T%'7%%v8#%#7L#M°%8G9S%9 #JSS%J18<%¢p#°
%.w8#<ES%k v1SO#wSJhM%O< 
%w%38%8´ 4-iSJ8 q1J#.S<p8M
%JS#w%"#S9O#L·G8B%8%# %1   n  L  !Fª%8M°%qSJ%
   ´
cMS°%7J= v8SJ1-#NSq%98% 0JJ8 b%J>
 bMS71SM#L#J9#8MS%%
J= v8SJ1h#LS0JJ8##J°q?Jp¡,S#7FJJ%w#88´¬µT· 1JS8'·B8SJ8BSqJ= v8SJ1
MSL9 J%#SMS#%L%w78S
 .9%ESSMS#%%´u« %SJ8<·%?J8i#TSL%JJ ·G%¢ 7p
W'C 

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

JS%vR#Q¯1S#>B7Jb°%8-MJ9#8M9%%
8%NSqJ=v8SJ1'vF1p9#%N#L8?<B%
9%°%T#­7p{YM%788qSJ?¶% 
  + %9­Sb%JSS#µJS%9MY##SD=b  *  4   ³ 
¤<%J·G8OSJ# .JS"p·GG JO179J-SS#<MO%ZS=pJ%<#-1'7pv9M%<88
JS°TSFS%79#°T78Sp%°%lS1vl#R¯1S#qB.´A±9·B#SbJF1p189S* 3JJ-wS
S#MS#%FM %9·G0b%BM7Jhb8MJSJSFS0%Jd·i%¢l8%.S-%8MN·BSNM
S8Mv%´¤TJSS
%1SMY## %·=1JS8iJwSJ E±q%F¥8² ª¥8²T?%J .¯ G·BSE
¥8²%²pÄ²%²%²b%JS#<%5S#SJ´
¤'JSS9S#°'?%JSSFJS%9M9#S"MSi7%SiS8°J#M5SJ%0SG%98%q9J8 
qSOS%7iJS%Y JS%J18wSS%°'SGSSJ#S%9S%#qW%D?J1SS .¯ ZS1v
#R¯1S#­¥8²p,´ 4-JJS#J%#%·WS( .Sw%Y8S%#M°%J#S189%J8i# 
  + JJ8  
%J p·BJ#?"w9M8J%81JES'W%iS
%98%LJ#S1S8Y%J8G# M´Yb-S#J°0S
SJS#°
9M%<88S<M8p·-179hSB9SJ#1 B%Jb1<9MSBSJBSS9SD·BSwSh%1S9% 
%98%bJ#° p§.S-1%SSvJ9#°FS8M8¡,Y%G{S·Bw#<SB#°E#'%5¨K°¥%¥M´
¤'  ( M%#%DSJq1%SvJJ#°  K  2 - ¡ K  2 - S8°%SSª<#-²]A¦%®5%JªR%#8%
S
%1SJ% 
8p#MO6S S
JJ#1 -._<7%S=S9%LT6%1%'% ´ 0-%8?%#*pS=SJS
79M<SJM0S
  7%b%k8?M%0J%FJ8°#°9#q v1¶S<M9w%88%1µ%
SJJ%w#=1h<,§9%B#   YMQ#
   S79_%1Sh%h<9#%SL%Jx·G%¢
%8qS 8 9M:(;  S¡Y%1%´LG9( .JES%SwpJ%<7107p{F%¶MJ9S%J#M7Y%S#
6%
l8S%ª%9%#B%*¡¤h¯ 4  %=#M %°%%SJw8Y·hJ#QJ= v8
W  bJ#<M?#
#HSLJ_%B°M¡,8? 77%S¶<J%JS<qSJ %TSEJ	 98MS %J 9MS S##J¢#J°
 h·B#¢Z£s¯pqSJ#¢,±M²%² ³?´

 



 ú {ú[õ ë ø   ZøY   úG  û _T ÷ø dDY
| K} ] ùE   ø  ùC9¹ ûgø S
¤'Jh6Mv
J?#<MSL°%%KJ%
v8ª_1p9#%RST?1q%STUMM9## L#RSJq1=%
p8MS#°F%9S#<%JSJD0%J .¯ {JS°   §.S-pJ%<#O1G7p9#.SpJJ1
# ¯1S7Bª#MS°%#¶%J#8%TSJ#q% 
1S%%´ -9lSJqJ81S8J·G8%8·GLJ%LJ#
##JMHSl1Y#%JM%S v·G87%BSN7p*´¶«%%{S8.S	 9l7p#TMSLJJ#1S%%
# SJM_S8r%  8· 1 
1SJSl1J18J#J° SJ 
1bvJp%l%J MSQ19( .JES
V%#S	 9MY%´"¬µ1p5JDSJpJ%<#K1Z7JM'JS%v{%JF798%#'1 J? ·GB%
1 
1SSS8°MJ#J°LS7vJp%F% l­%JJ .¯ 8´ 0
F%J%#S=7J?M
SJM
STS###l%1=7p#h8% 19ªv8%JQM68¡,S1¡,V%1F1Y#%JMSJh%%°%%?SJ
vJp%´
: 	RÒ:WÖ# 1ÕÄØrG­T
	#   â ·gÕÄ#Ø	
?^ØÄØØ$Ø ? Ö# ã 
<9#8%8p#J1'°%°%S{S9MJ°p¡J%##0JS#%p#JSJ   l7JS%OS'v8S6%<%J1
%BSMY M %#°%%SJ<q6%7S .¯ x E%#" 'h%°%·i%%G£ R88%"i±M²%²%²E³?´ {88v%S
S71%1F1JJS#J
JJ8
·BJ #7J%7.SF8% v7%J8%¶%J ST1v1­J8°%S8
%7JS%7EOMS
v%<JJJ8"´Z¬µF· 19#%S-TYMSS#8J#Mi%v1i%3SJJ%8
#SbH19S#8°ªSJ+
 S#* 3b¬ JM<#<9%17µJ= v8SE7#9S#%#	 MS#¶78SpJ7J%
ªS1-S( JSL"
   _#8M0%JS#<%#S9Ow%9 ¯  µ¤'FJ1J#°
%J%pi%! TM-Y%lLS=%SS97JS_SJM'M?N#OJS#MbWdT%Jdp8%
%JS#qJL´ -h8S%·GR#JS% 1JS#8TSLvJ#%7JSJ#1 HSJN9%<#_1w7p
·BLSMª#'#J#S#M_WS #S9i%S8BSJ%N%Jdp8%"%JS<p´
W'CBW

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

c38 |=%RSLJSJ1 7%rM?r1S<S( JS ¶#8MR%u%JS#<%OS
J JJ8-SJwpJ%<#q17pK·BJµMH#=#J#S#MQWS #S9hS9MMS<J#SS%J1
6S SFMSSO%JS#<%"S"´ 4-'#N¯1SqB.9·F%SJ7
SF#J#S#%9°%?%J. R(J%#
ÎÕÄÏ?ÐÔ  % 	 8  Ñ UÔ  ·BSw( .J%YJS%YM9## %´«°%°Fv%ES%Y%p<78?<S
J#?9ST%
7 9M:(; U³h6%=%J p8%K%JS#w E"%98%7SJM=S7JS9#1 q-M9JS#<M#R( J%K
-·BJ8S
  [  n 7N8 9M:(;=<>9:(;W³?Y,´É%´   #BJS#Ml6S bp8%5%JS#qJdS9Mh#'%ª%8M°%
J#SS%J1 7N8 9M:(;=<>9:(;36S SqMS
%JS#w%3#JS"´B¬µT%JSBSJq#S%S0#<9%1h%
%89MS%7#JS#%Z#JSJh STk8W%w%J1T% bª%9% °lS7JMSST% 6%

[  ´K«7¨Z#°SF¥±E·G'S·u9%SK%kSOJJ#171S |k%8
·h#i%°%O% 56%v8	 9
®<®wV86 J°S³K%9ª¥8²L¥8²l6°. J°³K%JJ ¯ 8§.SSJ#S{6% ±·h8S u	 ¥8²%²p
MST<Q6%
9SvB%p#SJ%#$ MS"´h¨J%=ST®  ®w#9S%J1%"M?µ1
#hM9#9
v8·G8AU[ /L%JA[ ¥8²R%J­1.S#.J=N°%%J9%#ªJ1S%<%   
  + ´<«µ1E%S8
M?µ1
W%
Sl¥8²)H¥8²l#JSS%J10?#hM9#J#bv8·G8EN[±b%J>  (¥ b9Jh#hS°J#
 K  !on  JnVZ7pJJ#bS<%79##J°_%5J1" m(¥ ´"{%µ·hE'[ /3S<pJ%<#T7p
JS9#1SGSJMOML1S'#iS##kS°J$ 98%EU 3K5SJ
JS#%YMl°%?%J.iGJ% ÎÕÄÏ?ÐÔ  M?
#M9#9=%0·iM7#JSJSJMDMSOJ#S%.6S SOMSK%JS<%.S#Sq%JT%E
vU JZ%YSJGV%%?M9'#J#S#%vSS#0#D#8´¬µO%Y8S% J%#SMS%##.S#8%vJp%D#
wMS°%S%7YF%KB?%J ¯ JMSp#°<MhS6#·B#°T°%8%%98UMS 36%B%
VJMk³
#JS%918SJbMJJ%H·iM %¶%p7J%S#wU%<%
 M'%   
  + #F°%?%JJ%
6M9Y³?´0GJ( .S%"·G<Ev%SS	 8qSJMF_9MSS#89#M-JS#%#$ MSR78SJQ·B#M
v
J%7<#J<%Y79%1OLS=v8S6%<%91-%!  1    ! !FS=SJS#°T#JSJiMSF%8S
8BFSBM{%JS#w%#S5´ 4-JJS#J%#%M·G'%98S%BSJM{S'pJ%<O17p
JS9#1SS9MS
J#SS%J1hTS
MSS%JS<%YS#S"%Jl%i#SL
 JSSJ1SM
S<vU 9=%iN°%¶#9S#%#	 MS#Q<8S5´b¤'<9#S#J1S#µ#=v8#%##R¢%8­# S
 .¯ Z
·B8 JSS¡9#S%J1<1%S#MS­#
¢p·BµRv7179M?MS% ·M¢kZ%´É°´Z#µ1.%0
S¤%##°b¯p%#S<% {%9 *ªMW#5¥¦%¦%®³?´
¤GSJ#Ev%SS#·
%J% 8BSJBv8S6%<%J1'% 9S#°F0UM?87%5J#S#
%Jb%Jm78SpJ0°%J8M=#JS#%9#S98´Z¨J#·h#° E%#<8O%*´v*±M²%²%²E³?J·G-1J#8
SbW##·B#J°R8%hJ°p¡ J%# ª9% Q9#9MS?¶9w  'h?³F9¶#H1 
SJJ1S¶·BS
©
 w8B%JL¤'<9* )Ä¥¦%®M²E³OJSp1JS=6%'°%8?MS#°<%1S%q#S9 A 3
 QV¨K¡i7%9¨K¡¯p8S%³?
Q  pc3°% ''ª%#J#°T·G%S¢J³?
 
  ''w%#J#°.³?%J
 *RB¬µ%?
 {1S#°<¤'#<³?´
   ,¯p%%
4-JJS9%#%p·q1JS#J8Bv%SR%1S%T%JNJp¡#N#S9,©
 <8
£n¤'J79"¥¦%®M²E³
°%8MrJS#°Q%JJ , 'B8Sv1S%¶J%
   +  ;  %J!
  ; / 8 	 ´ ¨ZJ%#%D·G
#J8JD¤%##M* )Ä{¥¦%"¦ .³1#1%°%MYJ#{1JSJ1SF<8S5UJ% Q "$#=M%JS#98SS
JSp1Jq#.SpJJ1QR¬µ880%J ¬ ¢#8T¥¦%#¦ ³?K·BJQ·7J%
 L§3SJTM8#
L%-SLv<1J?J1S%N?#S#8TM%##MYlW%<SJL%J ¯   % 8<%*´O±M²%²%²E³?´
'B%9x<¡%1S%0SJO8%%B7Y%##=%JRMS%%
   $#  , ´¤'SJ
¹ =»ÊÖÇÄÊÍ?ÂÄÂÉÌ 2¾,ÖU»¼,»ÇAÃ5ÍD½!¿¼,À'Í?ÂEÁ'Ç &»¼,» UÊ»Æ%»¾ k»» 
Í{Ã*¿ÂÄË¾,ÇA¿j
Í MÁBÍÃ*ÊÖU»ÁËUÂÄ»ZÇ+'¾,ÖU)
» ( +* ¹ 	Z¿;v<» :»-¼ 2Æ%»ÊÍËUÃ*»
v»{ÍS¼,»{Í?Ã*Ã*ËÀOÇ×G»ÍS¼,ÂÄÇA»ÃV¾KÃV¾Í¼*¾ #¾,ÇÄÀO»KÃ*ÊÖU»ÁËUÂÄÇ+U×¿?½9Í?ÂÄÂ.¿Å%»¼ÍS¾,ÇA¿jU<Ã 23v»DËUÃ*»K¾,Ö»Z¾ k¿G¾,»¼,ÀOÃ"Ç+8¾,»¼,ÊÖMÍj×»ÍÆUÂÉÌ8¹



W'C%

Ú$¦,ÜÄÝÞ¢­ZÝ

3400

320000

3200

300000

3000

280000

2800

260000
Predicted search cost

Predicted search cost

 $¡4¢£

2600

2400

2200

240000

220000

200000

2000

180000

1800

160000

1600

140000
0

5

10

15
20
25
30
Distance to the nearest optimal solution

35

40

45

0

10

20

30

40
50
60
70
Distance to the nearest optimal solution

80

90

100

¨K°S<¥± 3%SO%ZS=J#SS%J1DDv8·G8R%L#J#S#%YS#Sl%JlSFJMSO%JS#w%"#p¡
Sb%9wS-JS9#1wM?l1 |56%iq®<®_V8W J°³D%J_w¥8²L¥8²N6°.
J°S³-?%J ¯ Z´3¤'
 [   Jn, 7A8 9:(;=<>9:B;³
W%0S< · #JS%91FMS_±%¦R%J
#® 9Sv1S%#%´
SS9S#°76S %378SpJhMS%9W%?7R#.b#8%3%JS#<7NMJ9##J°TS88kS¡S1.
JJJ8OS
 *ª%v8M%´
¬µqM°%#N1JS8'JbS E±L¥8² ­¥8²7%9 .¯i·BS#s¥8²%²pÄ²%²%²b%9S#<%"#JSJ8§
·G S1 Sµ#MS°%8_¡<%_%Jv  ®E  %R®>u®¶¡qJ%9 8LJQ SQ9°8
8°%S8T%{1pv1µJw89%´
¨%
%­JS#%#$ MSN78Sp"k·717Y 7N8 9:(;=<>9:(;ZW%
%?
JS%Y #9S%J1ª9S#° 6·hSrSJR118JS % Q " #='·hJ# #wJ88<#JS#³ Ä²%²%² p8%
%JS#wN°%8M ­MJ9p#°L88v¡JS1E7M? ªSJb#JSJ=SJS#°L6S S
°% 78Sp"´
¨%
   $#  , k·GT%JS%#Qb<% 7N8 9:(;=<>9:(;*v%8M°%­%8%# E±b#JS%J1k%
MJJ#<M BU²]A¦%±´¬µi· SO7% 7A8 9M:(;=<>9:(;W%D%?7S<%#9#°hJS#%#$ MSF78SpT#
¤ZM9 §E¡,U%36%KSSSMS#S#8%pS°9	 98%J1{%9SG<%q9= k8J1# 7A8 9:(;=<>9M:(;Ev8·G8TS
MMJO78SJJi%J  $#  , J17YLJS#J°TT¬ ##1LJp¡,9M%78?#M9%S¡S%79
S°J %¢ 8BMR%#HJSp#5´r¬ SuSª1p189Su%   i·ª%98S%QS°J	 Y8%E
J= v8SJ1Z# 78 9:(;=<>9:B;.v8·G8TSG9%##J   $#  , SJ%JqSGSSJS°'6Sj%S8
#J#S#%#	 MS <8S98´ «JS#%##%SSQJMS S°%°%SbSJMl_<uvNvS9L #7J%
Sµv8S6%<%J1 %
  JS#° #J#S#%#	 MS 78SpJl·B#S · 7N8 9M:(;=<>9:(;*´ -B·8%8FS
·q7%­M%#F% 7A8 9M:(;=<>9:(;*Z%JS%#­9S#°lS)Q "$# %J ! 78SpJ8ZMSwS#D#MS°%
#µM9S#<8w8´_GJS( ..S%°%­N109#JMS#µ%iF·G%S¢°NEv%SS#=%9µS
%8?M°%T9w8J w%'¥8² ­¥8²w#9S%J106%´É°´v80SF°.BS=%{¨K°S<¥±³?vB8wB#¢%#
SJM{8%<SBSJKMS'S#.FVM{6S S'M{%JS#w%#Sq79%1D%8?%#
M?R18´
¨%
%? JS%9 #JS%91%Y·GqJ1-179qS   JJJ8B%µJS#%#$ MSL78Sp"§
SMS#SS#8FMS<SM¢%¶%8l¥MÄ²%²%²R#98vJ.h%#
% ´<¤'J7k8?1EFJ	 k8S91F# 	
6%w% #9S#%#	 MS# <8SuS#MS%ª¶SJMb%JS%#u9J8<S   $#  , 9%#NMS
S8v%S #Q¤MY ´F¤'J OK  !on   !#b8p#MSQ#h#S-SJ%1
 /V %9ªSJ0v
#7J%7.8
%JS%#JªJ98%
 !L"#B9R±´ BM¦ V_´F¨JS8"%#%98S% J#1S89%J8-8%QvqM#9
S%<9##°
8S%#7179JSMS<% 	K%JwFJ= v8SJ1D·G8SBSSMS#S#8%##qS#°J	 98%.K8%
W'C^

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ



/  A H
½
!

¿
s
¼
O
À

»
j
Í

4
 464¸
)
ÁÇ'&»¼,» Ê»sÇ
% ! &' &'
¼,»ÂAÍ¾,( AÇ :1»  ¾,¿
*,+ .
ÀO»Íj ¸ ¹ 1J
Á'Ç &»¼,» Ê»sÇ
0/1 ¼,»ÂAÍS¾,3Ç :»
¾,2¿ *,+ ) ¿?½ ÀO»Íj 4  4& H
Á'Ç &»¼,» Ê»sÇ
ÕÄ"Ï 35476 1  /1 7
¼,»ÂAÍ¾,AÇ :1» ¾,¿

% ! &'  ( &' 

%



	


~¹ #-~

~¹ H A

H&  A ¸ &H   H A
4  464¸ 4  464¸

4  4% A 4  4&5#



UÇÉ¾,ÇAÍ?ÂÄÇA@Í¾,ÇÄ¿j -»¾,ÖU¿Á
%  $


 "! # $
J A  %H  A H  ~/ J A  J%    %
5# Ä¸-4
4 Ä¸ ~ J 4  464¸ 4  4 4¸
4  4 4¸
4  4 4¸
¸¹ 4%

4¹ 41J

 ~¹ 

4  4&/ 4  ¸<# 4  / #

4  /1J

4  # 4KH

¸¹ %

¸¹ AKA

H

*,+ -

¤ZM9 3B¤'JGJ	k8S91#0v%SSJi<%q9#S%J1GhSJiJMSZ%JS#<%#S#_ 7N8 9M:(;=<>9:(;W³
%JQM?ª1S7 ³'6%hMMJh#JS%#	MSL78SJJBH¥8² H¥8²w%J .¯8§
J	 k8S91'MS7%SlS#MS#%F<%JJx<!¡%1S#%#S#JF   $#  , ³?´
M 7s²]Ä²´l¤B<JMSRJv%S
Sb.k%SJS#
JS9#1µQSJwpJ%<#71q7p3F6%
J b8J-JS%9w8"M%##M97#9S#%#	MS#R78SpJ
6%FSJ.¯ J% JKNS#°J	98%.-79%1
lS-v8SW%?<%J1-% ´D¬µ=1J8#J-w%98#J°0SJMiiSJSGSl%SJ#J°0Mv
SR1SwS( JS 6% µ 8Mª%JS#<%B#S9T¶%.¡,M¡7p8M #JS%J1w%
SJ¡,%JS#<%
#JSJ_  %°%µ%qJS%Y #JS%J1´ « 9MSS#89#MB·Gµ%98%µSJM
%89MS%F#JS%#	 MS_78SpJ'<_7JS%=k8W%w%J1=#lSSJMSJp9
<S
°%%J9%9#J1%h%  %Sp8#M_·BSbSJw8J#{JS%9#n#JS%918´¯p#<#M%%8?JMS%
#J#S#%#	 MS¶78SpJT<HkJU JSM9 SM %°%%?SJ<0SJMT79­S1¡#.JS$ 98MS"
SJ? %bS 8%%v . h·h#S¢ph%9 ¯pqSJS¢p*´ ¬µ 9% %_E%S#°M ·h8S8
S#w##MFSJSq#H SJ1SS .¯ 8DJ?#J89%Rv8%J_%'Sl#J1% Jw8J#µ#
179JS#°Tv%S 	i%J 7A8 9M:(;=<>9:(;"6%'SJS%J18´

: 	RÒ
	 WÖ# 98N8=ËØ 	 =ËØv  )^ )gR#Õ
<9#8%%Siv8S6%<%J1G%9SMYqSM<J8k9JJp#v0SO1G%YSM9q.JS%´
4-S°P
 F=S#°#J#J%Zv80JS°q
##%q v1S%O.JSiW%D%#8#%SZ%YJS%¡
J
 Ib,©
%8'£dc3M°JJpk¥¦%¦CB.p5´ B%³?#{°%8%#TS1%°9	 8<SJMGS<%#JSM9<S%
<M?NSM°JMS#"J*´É%´SF#9M9## T7S8MvF8%v%9S#<pp·BJ##-#M°%-SMYl.'8%
p#TS°J$ 98%EK88#%MS7#TM?7 v1S%S8.´ G8%JTSJ'i%98SMMSJ8EJ%¡
SSJ8iJ%S-°J#J%J1=#l#1S#°TSM9LS8J%9bSJ8SF#i7S8%S8S8% 
9S	 98U¡
S0W%JS868S#J°B%E=YMSS#8J#M3U%#J3·B#SJ#=h%°%%YMJ9MS.S
S%JMYvSS#9##S#8´
«
 9<S#J1¡, k1'%DSJ%S¡,8 <7%SL#ib19S#.S_9#%'M?N#S8'·iMª%
·i_6SmS-JMSG%JS#<%k#S#"´K«.SJS%#%.·Gh·GJ#b1pk1GS=<M°J#SJB%3SJ#
9#%FNv<JS%v%SS9%ªS<SM9¶.JS '§K°%80.JSSJJ#QW%?1bM?­Q<M¢%
W'C'C

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

7%S
MY#w9S%°%SSO·G_6SmJS8pJ7SbS8°Ji%S=MR9%1%´GG9S#8GS
S1JM?#T·B9# F#%J#qJS%°%SS#°=·G<WnSBJMS{%JS<%J#JS"SJ?
SJMqSl8SS.q9#S%J1bQSJ_M0%9S#<%G#SH#0°% . -  7; + ! <>9:(;V´Q¨J%
%. pB w·h##"8%.SJ%#ª#E%8BSq°%%J.
%Jª<%7M?ª·GM? S0M
%JS#w%#JS"´ -h·G8%8{S_#MS°%8qSwM%#b%'KSb<%SbJS%E R#q#¢%Q
7%<W S<MS=%9S#<%K#S# k86%S7#.%8SQp888´q« 8<
%GFpJ%<#
7pk% pSJ#GS°%°%S'SJMiS=<Up<%v#¢%w9#S%J1 
  + 7S=MSSG%9S#<%
#JSN%98%R"
 w#OJ%k%SJ%v B´¬µTJ%0S·BNS9MBJS%YxJb8J_#
S .¯  #G#MS°%706J91S7%5Sh k1S#%
SM_9%1
S	 8 4 8 9M:(; 4  %5·BJ
 
  + #
7%SJS%´GJ( .S%E·G
Ev%SS	 8hSJMG%.<#J1%h#wSBSMYwS-%JSM#.
°%S·'SR# 4  8 9:B; 4 "%JlL#68SJ1
JS%YdJw89%´
¤F{S9#{..v%SS8%·% JD1%<#OSJ 
  + %JS%<JS°
S%7Y##°
78Spp¡
%°%ª%8F_%°%q%{SM9QS8´
¬µ<1JS8q¥8²) ¥8²b?%J .¯ 8kSk8$ 98%#lS E±
#JSS%J1B·hSA ¥8²%²TS²%²%²l%JS#<%#S#J8´O« 9ST7%ªSM9NS #B9#1SM
NS7#.8SU%tP  ,, T + X,G´ {89S%k·78P  ,, T + X [`P "uT¥ MX,§SJ#h9MSS#8JMBM%#
·i%w7Y#8%##¶88<  ¤Z%##M ¶p# °%rv8SW%?<%J1NrS
   ¥8²q ¥8²
vJ9<MS¢ #JS%J1%´_¬µl· S_#79%1%Gv%SHS<%#80%J­#MS°%80SM9­S_#p¡
8SM%#'RSFv8S6%<%J1=% ,´ i%ªL1pJS%01v87ESMS#"·0%98S%SJM
P uT¥8²X'MJJSp<MSJbS<%=SbE8SM%{6%q·BJ#? R#F<9#8%  4B-K*´É%´
%Jqk1w#°NM9kH#­#S8q#8%%JS#<R%0#1HS8°J0%'S_M? S9%1%´
0
_%8M°%%9S 
  + %9S%#lJJ8SP "uT¥ MX3#E8U%vMS-S°J#7® V °%M8OSJ%wSJ
%JS%#JbJ98SP uT¥8²X3#.8SM%*.·hJ#'SJ 
  + %JS%#J_9J8q#MS°%8=VMS9#M#7?M¡
Y³ P!¥8²T(¥ "3Xi#.8SU%DMT SQS°J# V °%SM80SJ% S7%9S8S%­JJJ8-SJHP "uT¥ MX
#.8SM%*§·0M9Sp¡9J6%m°%S·'SRMF_J= v8SJ1'#lSJ7%LSM9R.JS
JJJ8'S P uT¥8²X{%J&P "uT¥ MXK#.8SM%#'%8?SJ'SJ P "uT¥ MX{%JwP!¥8²T¥®3X{#.8SM%#8´ 0-%8%#*SJ
SS9SD1 JjS'ESJ#SqSJM{#MS°%8{SMYT.{%T#91S% 4 8 9:(; 4 6E%7%S
 
  + §J·%98S% J%##SMS%bES8%"J%°%h# 7A; + ! <>9:(;V´
¤_1A JxS9Mh9%°%
# 
  + p#N1%SvJJ#°<?J%°%
#RJS%9 Jw8J#%J·G
179J0S0%98S% 9J8 <6%-% JS%Yx#JSS%J1%8=SS80SM9ªS
#.8SM%#8´ -B8S%h·µ1J#8l%#0¥8²%² #9S%J1l#uJ ¥8²# ¥8² JS%Y 88§SQ79##8#
%SS97JS #_SJMNS<##MlJ%J°%N# 
  +  6%N#JSS%J1l·BS  ¥8²%²pÄ²%²%²u%9S#<%
#JSJ8´-¯8M8¡,9%S
%{SJSS9S#° W%"P uT¥8²X{%8J P "uT¥ MX%9&P "uT¥ MX{%8J P!¥8²T(¥ "3X
S
#E8U%#GMS
Sv1S%#<S·h_<S
86G%J_#°EGS#%3¨K°ST(¥ /´¤'J B9J8
S77JJ #E8U%9P "uT¥ MXMS7°JR#¦ V MS°%8=SJ% ST%9S8S%­JJJ8hS7Sw%#8
P uT¥8²X"#E8U%*E·BJ#iS i%9S%#<J98{S P!¥8²T¥®3X5#.8SM%JMS'SJ°Jq®MT² V°%SM8GSJ%
SO%9S%#7JJJ8ZSJ P "uT¥ MXk#.8SM%*§EM°%#"E·'M#9OSOp¡9J6%j°%S·'S<MO
J#1S89%J8OLSJ	 k8S91F#N7%RSM9NS%´¬µ0%9S8S%S#w##MO7%J=°%S·BS
#qJS%YJw89F6%{F#<qS%79i%k8%w#M°%8D.'#E8U%#´ 0-%8?%#*%SOSJS
S9k%O-.k%SJS#'S9Mh#MS°%8-SM9ª.JShJ1S%0JS%9 Jw89%9Sk8$ 98%#_.
# &YMS#° 4 8 9:(;  4 6,´ 4hSJ°L%'S8v%SLJ8S%J·G%JJS#J%#7%9S8S%S#w##MiSSJ#SiR
S<%"S%79
%{®R®q·G%S¢ &J·j%J &J·BS%5
 ¯ ´
0
J1pk8?#7.S#J9#8M'SJMSBSM9bS 6%   J#7vB?$
 "! !jL ' "!
m"K ! !  2 0·BJ##hS#qJS%J8JSw%#J#J°TSMNSSM°JMS"´{«L%JJ#S_7JSp#J°0SJG J
S8%S8S8%#G 
SJS$ Jw°9###JB6%'1S#°7qSM9lS%SJ%Y8SMMSl79J%	 8iS
v%ES%#R8?#7.S%DJMSS7%iS%S¡,8? 77%S%´T« 9MSS#8JM"S7SS9S-JS.





W'CL

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

1e+08

1e+08
y=x

1e+07

1e+07

1e+06

1e+06
[10,18] tabu tenure

[8,14] tabu tenure

y=x

100000

100000

10000

10000

1000

1000

100
100

1000

10000

100000

1e+06

1e+07

100
100

1e+08

1000

10000

100000

1e+06

1e+07

1e+08

¨K°S<¥(/ 
3 ¯p8M8¡,9%S-%KSF#MS%01S =S(Jlbp8M0%R%JS#<%#SL9J8
<W%-S<%#3%8SJh7p8MqSM9NSTV86%J°³O%Jª7J8M%8J
#MS°%FSM9N.6°. J°³?W%¥8²e­¥8²<%9 .¯ 8´
[5,10] tabu tenure

[8,14] tabu tenure

Mv%<SJ°%°%FSJM  ASQ%7J.=%G%S¡,8 7<%Sª#Q11S=%GSJM
·BJ#? #
S(JS
qS8MvhS
M%1%O9%#J{%8%9%JS#<q##¢%TT8°%%-S-k8W%w%J1B% ´
 _  ûW  ÷M,  ÷
| K}7~  ¹ û­úø%  ÷ ú  
0

SJS-JS#JqlS°9	98%.J?-S8µ·iM­8%%Y#°L% JJ8SS%JJ#°b%S7.¡
J%<87JJ8##J°RSM9 M?"´ ¬µR9%R#.SpJJ1 µ%9 ·G%#¢H7pi%
¤%##M*)Ä
SM9LM?L%°%%#SJmW%OS2 ¯  SJM89S9
SiS#79#8%%81J.S'6%'Mw%Y%S
MM#M9#N#QS71S( .9SªN8M7%9S#<%K#S#J-L%J .¯ 8
´ 4-JJ#SJ%#%
Sq7J3%81J.S-W%-S#<#Mb9°LJS%v%SS#JO%DS0UMM9## _LS01ShS( JL
p8MOv%S7S¡,%JS<%p#S9%J .¯ D%JT%JS<%#S#JZF7%S'91SS
.¯ 8J
´ 0

SS9S=#JJ8MqS9MFM¶#µ¤%##M* )Ä-%°%%#SJ 8%µvT8·G­%FlMM#%.
%Dw?%°E6%S·iMNJ1¡J#7JJ%v%JJd·G%#¢_S9MB1J9Si ·b¢%8Lv'%D9#%( 3h¥³
H9#%<·GM? S#SJ<SJM_MªSJ°J ( .J¡J#S%.w6S SJQMw%JS<%BS
%JQ#S#JOSJM
MST<Up<%#lJ#S%.B6S SqJMS-%JS#<%#SR%J *±³Bb9#%
SJM7J9T­<%#.S%# 1JS#SETJS%°%S7S8T·GMr%<·i 6S SLMSS7%JS!¡
<%#S5´G«R1.%-b1S-7p#'%DJS%9 Jw8J#l9%RªSMSM?ªS9%1
6MSS8DS_%JJ ·i%¢¶7pG#qS8%#M9_%JHJSp#qJ10#J°Eq#.RSlpJ%T¡
#8-%SJTSMµ9S182´ 4h9JSJ%%9·G<#.S	 Jª%µJ1pk19RSJ°l#¢lv8·G8
STJ¡,S#7T9%<#8-%SM9µSMµ%Jµ#790WMSJS
%S7J98p#°wSM­S9%1%
·BJu9Sp#_% 19%JMSu6%lSªJS#%'J81Sl%J JS#<Mª6%#SN%bM?#8
78 9:(;=<>9M:(;i<G%hJS%9 J#w8J %´ 4hSJ° ·LJ%N%7VJ#µ1p9%S SlJJ#1S%
8M9M9#S
%OSw%J ·i%¢­7p*3·GQ%vJ#%=JS9#1µQSJb7p{J%
v8¶1 97­SJS°µ1pv8#7.SMS­H%JJ .¯ U 3SJwV%##S7%'JS#%DSJ
H°J	 98%.S #79%1<%°%%?SJ v8S6%<%J1ª%J SNv%.S#%# 87ES%BJMSSN%
S%¡,8 7<%S%´
W'C



 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

-9#wS_SJ81S0%iSw%9 ·i%¢­7p*Z8%8%G#SSFS<%#HJS#%"´_¨J%
1%79%Z=#FJJ8#M=·BE N#=JJ¢%NN1p9%7k%.S#%##ª#MS°%wS8°J=% 8 9:(;
*´É%´%·B.F%9 8%.%JS#whMSO%D1SM#=8JS.SMS%i%YSJ3#  
J?#°
M5´¯p#<#M%MF8%JS%1p9#%9MSTW%SO9#%K·GM?bSJKSJM{M'S°J#
(J!¡J#SS%E{6S %JS#<%Y#S#J{%J_#S#J{<U#<%##qJS%EWs%JS<%9SJ
#
#%¢#J°5%S° J##<#9MSl8#JJ1qJJ#8M
ST9%--#79N9T_SJ<#17%
S8JESMS#"9*´É%´S9#JMS_Ev88v%´
Z8JMYhSJq7S
7v%SS%E-1E?9Sª%{Sq%JJ ·i%¢R<Z#hSqWJ9JMS
=Jp#=6%VSS7M5´L¯SM1¡,%W¡,S1¡MSTSM9HM?¶%°%%#SJ<=6%S
 .¯  <M¢%
1pJS%R9N%=°M¡,8 77%S  h·B#¢-£ ¯pqSJS¢p*G±M²%² ³?B%9u<#<9J8Mw·
SJ?<77%q·h##p#79%1DSJBJ1SJSO%vSO%Jn·G%¢77p*´DRp#°Fv8%JTS .¯ Z
S8S'KS .JST%v°%8%#$ MS* 3-#<##MZSS9SD#T·BJ71JS#8?#°-SM97M?
%°%%#SJ<F6%q%S8 <>=
¡JM?¶J%9<8%´É°´DS+
 .J%JMS#<%S°J7.0%JHv8qSMS
&J·O¡S% S9J##°L9S%9<  4hSJ°­S_7p{%JH%Sp8#MH<8SJ%°%­8%¶v
%#°E6%S·iMJ­MJY#­ %S879S%9<Z8JS.SMSJ8{%9H8% #8%GM? %°%M¡
S9<8v--9J8M  m  pK  h·B8S8
·GT8% 1pv1=Sw8.-S8°J#M#ShRSTSJS#°
%JS<9S%9M9#SZ0p#b%88M-JSJ1SJ8´K¤'J##Sk8%#qB6%GJS%9w{#
·BJLJ°9b?J1SSLvJ?J<MS¢p'MS07%SFJ8U%.8J%´É°´k#ª¯ 4D¤=´J¨K#9%#%SF?%J
·i%¢N7p3#YM
%h8JSS.S%kMS°%l%{J

 \mW"K !on #  K  hJ%´B« -#BJ98Mh·SJQ
7pv<°.Ov
#8%8M°%N#_%8Ob8%%N#79S%lSM9LSMN%#°%%SJ<´{¨%'1%T¡
9%M%SJ°338MZSJMZS9%"·iMT#S#J"SJMKMS9#S%.5W\%JS<%ESJ
S9# v<<#J#w	 8"5F#F6M0WS %.pJ· SJ#F8%¶k<%?J8%"´N¯p#w##M%5%J%S8
v%ES%kM99##8MSb%3SJ-?%Jm·G%#¢b7pk#E%#%iJSJ#1S°J%9mJw8J#k§·
SJMTSL<J%EqV%1%<# &YJ8#°RJS%9 Jb8J­HSJ ¯ MSlv1<#°Qv88
JJJ8"%¶%#JF1p08 #FQ%J% 8w·BJ8S8FvSSYTª%98%b%88?M
S#wM'%KS2 J%.SSO·B#SL<#J#w%Y%h7J8M179JSMSJ%" v%S8´
¤'J% 
1S%O%YKSM?T·G%K F0SSW I-SivJp%Z%9SM97M?7%°%%#SJ<8
JS° S5
 .¯  %w v"´r« S9#T°%%*i·N9%ªJ818"´ 0=7%J ·G%¢ 7p
8MJS5SJp¡,S#<KJJ%w#8"%SM9FSM0W%S ¯ R%819ESW%S9#<MShvJp%
%v#E8U 3SJ'1{S( JSq#8MO%JS<%#S#JZFJS%9#jJS%J18´D¤'JGv·8{%
Si7pE#VSS8Z###JSM=FSMY## -=%81J.ZW%D%JJ#SJ%%vJ%?%E9J7J
%J 1%SS1S#uJSJ1_%-k9p%8´¤'° 8M86J-7p##J°¶%J %J%pS#b·µJ%
79M7SJM9OSJ{ v1S%S8.SM9wSMb%°%%SJwKW%{SJ .¯ ¶MSB#TV%1
JS79F#NSB%v8MS#"´O¤'F%9x·i%¢L7p3SJJ#L8S%T%-wJ8VJ"Y%S#O6%
1p9%#J°-S<##MK#SSJZ0SJ'1E1p%Yk%S7<%S'%M%J1TSM9<M?<%°%%?SJ<Z6%{S
.¯ u%JLSM9RMª%°%%S9<O6%'%S8'<>=
¡JMR19JM%#%v%JS<	 MSL9S%9<´
ô

û

D  q9 F  

ø÷

¯p%J9#b=L0J#S9Svq#Mv%M%SQ%v8M .­¯p%J9#NG%Sv%MS"5@c3¢J8p¡ RMS#
G7Y%E%UW%3SYbh90¯SM h8YMSS7.5%W8S°%
JJJ8v1.%1RD¡4Bi² M¡ ¦"4c"#M²%²%²p´
¤'JB·G%S¢R·i%=vJ%QR9M-RSJ 4-=¨%1 0
w17%i¯p8#ES	 Y
 ''SM"4h=¨%?1
ªM8Zi<<%J"Jb
¯ 4-¨GYJ98O°%%.hJv8B¨ E¦%®%±M²U¡² /¡¥¡²#± /#/p´=¤'J b´¯v´k©-%8?J7.
#K%S%?	 8T=S8JSpJJ1O%JTJ9JS8J?#ESZ6%©-%8?J7.S%J9kS% ·BSJSS%Jp¡
W'C

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

#°7%._1%°.iJ%SMSLS8S85´¤'=%S%?i·J_¢%
7SJ%¢LRMS8· ¯S888'6%
v#.S#°Fi &9· ·BSb%_M#8G%8Sb%5i78Sp%°%wW%GS#<MS°S-%JS
JS%YM9##S##lSJpJ%<#
1-7J*´D¨ZJ%#%·GF°%M8VJ#_%S¢p·h°%SJ=1p18JS#p¡
%#LSJ°EVJZ%JQ8S%#ªW8J9%S¢ªWS ST%.<J-S8#8·8Y·B9#ªJM9QJh
S°9	 98%.Sb#<JS%=J'%°#9%Y9S.SMS"´

E   C  
$ù

û 1÷

4hJ98°M%

q´O£ G%¢YG¬j´=¥¦%¦p¥³?´ 4 179SMS9%hS9 %FSL
%J¡S% S9J##°
9 S%9L´ K1   K   KMLUm 1Wn    K*±³?¥ E¦_xY¥(%®´
iM8 J´£ iJ%0v88.´'¥¦%¦#³?´ ¯##J°ªS 
%uS%uJ9##°QJS%9# ·hS SM9
SM"´	
 "    !#5n  K  !Y#± CB5x±%®#/´
GS¢k
 ´Y£ ¨v5­´K*±M²%²%²E³?´ -pJ%<#=9S%9 J1S%J%pS#'%-<Y%S#'6%h19%#.¡
9S1NS?JJ#°T#SS#88´ -n     n '     1> v¥³? /p¥jx "p¥M´
i#
 8 8·B# % ´ hwS¢%%k¬j´k£ " '´Z¥¦%¦%®³?´h¤' 
%ªSJ%ªS?JJ#°TJS%9 3YGp¡
%.SJ%O%9 J8· S#S 9J$ 8´ "
1 ;Kjm   W*K 1  'K hFm  n  K    !  QE
 Z¥³?K¥jx /#/´
G.S%' 7´O F%J°E
 40´O£ ªJJJ5O¯k´=¥¦%"¦ .³?´ 4 8· %JMJS#% qJS¡SMSlJ9$ .Jª6%
109#9M%#%"°%Y%v%JS#<	 MS#J8´Fm # n  K  !   !  Q)Q ^n n # ! *±³?Z¥8²¥jxY¥%(¥ /´
i8S<%" Z´3 F%J86¢% -´£ ¤%5¬ ´G¥¦%¦p¥³?´_¬ 8SwS   ' SµJM? JS%9w=MS%´
«   K    I   !K hUn Q  O  h-n Q n   n  K WK n  K 5h      DK  on     n '     
      9J5N´ /#/p¥jx /#/CB.´
i#M¢Y q´J¨%¢kN ´9©-.8J«´9R%1«.S%'´J¤7YN 0´9£j¬­%#S5p¤F´3¥¦%¦%®³?´c5p8%"M?
%9<SBJ0v8{%"#S98´9«<¨S98B´-´Yp"´³?   K    I    !K hUn Q    jK JI n # 
 n pK   K 5#h ## K    #m   ! J
I   _n    !UK h  K  !on  n  ;K  ML L         U
95´5¥%¥¦_xY(¥ /#/´3¯J#J°%8¡ D8#M°´
-<S¢%,S'´KN.SpK¯k´K£`b % 'F´G¥¦%#¦ "³?´ 9J<M¢0W%qS%¶J9##°lJS%Y<8´
"01  Kjm    W*K 1   K;h!Fm  n  K    !#   UQE #"  ¥³?K(¥ /CB5xY¥ J¥M´
¨##8Y¬j´¥¦%#® "³?´ $ n ;K5*I 1 5n  K $n
K   +K 2 +2 '  n S Q  K  S   I%on !NmCmC   n  K  !ªV¤BJªJ!¡
S#Y³?´ D*´3¥M´ %Jl¬ #8l%Jª¯98´
©
MS8%-­´% JJ"E q´B£ ¯8SJ, '´¥¦CBM®³?´x¤'µ1791 r%
 &9·BSJ% %J 
%YS%
J9##°´ 
Cn Q -L n   ! K;h&Fm  n  K  !   !  QE  '¥%¥_B5xY¥±%¦´
©
 w8 -´9£ ¤'J79"J©q´Wc{´5¥¦%®M²E³?´ 4-°%%#SJ<W%'##J°0JJ91SbJ9##°0JS%¡
#<8´Fm # n  K  !   !  Q.)(K .³? T"CB5x M² /´
©
%8O¨G´O£ c3M°9Jpi¶´F¥¦%¦CB%³?
´ 2&1    Q.´n F#.·8+
 4h8%J<#P
 9#S8. S"
ª%SS%?.98S8´
-- 
8¢k,
 -´-¥#¦ "#"³?´ G.#°­S?JJ<W%b%JS#<%h%J%##J°´ 
Cn Q -L n   !>K;h*Fm  n  K  !
 !#  QE/+K*±³? /p¥%¥jx /%±%¦´
-h.8
 -0´{*±M²%²±³?´ 4s<!pSS1¡<6%=S7vJp-%i¯cK¯ª%°%%SJwh6%0¯ 4D¤=´«   K 
   I    ! K;h n Q "   Qn   n Q  n  K   K 5#h ## eK $ -n   # , Jn '   #   - 
".'
95´9®%®p¥jx®%®CB.*´ 44 4B« {S ?U¤'R«¤ {S8´
W'C7

 $¡4¢£

Ú$¦,ÜÄÝÞ¢­ZÝ

JnK Q  !on  )QSK      Q 5n QuK5"I ! K5I   ! tmCm    n  K  !?´ "´ q´SJS#8

MwS%'b-J%8_%D¤3?J%°%%´
%"#4´.£\N88%5p¯k´9¥¦%¦%¦³?´*-88<#9#S#
%¡S%<J9##°3#K%8JE8.%J7VSS%´
"01  Kjm    W*K 1   K;h!Fm  n  K    !#   UQE G+Z*±³?*
 /%¦M²3x T/"´
%"
 4´ 'B%°%·i%0% -´£ R88%"K¯k´*±M²%²%²E³?
´ h8· %9 I%S°%8 I 
%J¡S%¶#°Ev%¡
J.pJU 34 Wp8JD<S'78Sp7% B·B#S¢pJ%J_¯p0JSJ#S¢p¥¦%¦%®³?´)K 1   K h  &1   !-n  #!?
 .³? TCB5x T"M²p´
 =7.% J´Y£s¯p* J´3¥¦%®M²E³?´   pn  
 jiMK   Q    !_V¨KOJ#SY³?.´ q´ {%5
 h%9"´
 F¢.9M?#S¢k¯k´G©-#M8O-´£ D8?J*i­´h¥#¦ "#/³?´ 0-9S#<	 MS  S#qJ#M %JJ%##°´
 #   1  "Uk®CB¥jx#® "M²p´
c5JSJ1. -0´RMSS" 0q´{£ ¯   %D¤=´B*±M²%² /³?´ «8Mrp8%iM?"´H«r©
%8{¨G´£
 F?Ev8S°%8k©q´pJ8´³?  J+I 2 K5KiK;h  5n  Q &1   !-n  #!?´9 #E·G8 4-8%<#2 9#S88´
ªMW#5N q´¥¦%¦%®³?
´ " K 1Wn pK   S    Q  JIn Q  +K 2 AQuKjmY´ .S8U¡ D8#M°N -h#v8S°´
ªMW#5 q´* i8S·B#SS"k-´"£m =%J68 -´D¥¦%¦%¦³?
´ 4nM?µ9%17%J%#S'%SJ 
%µS%
J9##°TJS%YL´   !\K h!Fm  n pK  !  !#   UQE (#U  J¥jx T#/´
h·B#¢,3'´M£ ¯pqSJS¢p*U-´.¥¦%¦%®³?´ 4 V%3SMvhM?q%°%%SJ\W%S* 
%0%J%9L´
    -L  J
n  # 8 D*®³?GBM¦CB5x "p(¥ /´
h·B#¢,F'´K£ ¯0S9#S¢p*Z-´G*±M²%² ³?´  ^n  Q $1   !on   Fm n  L  n pK 
	    ^L"K  S$ JI " K 
 1Wn pK J39M5´Z¯7J
 h8·n«%F#µ¤-¯ªW% %H¯p%H¯p?JJ#°kJ5´D¥#® _xY¥¦M²p´_ #E·G8
4-8%<2
 Y##S8?8´
ZU U8##pk¨´k£sR8S##*WB´*±M²%²%²E³?E
´ 4 SMYRM?R78SJL°9#L_J6S#°Tv%SS¢l6%
SJ 
%NSJ%NS9J##°0JS%9#L´ "01  Kjm    W*K 1   SK h!Fm # n  K 	   !#   UQE + "3*±³?
±%¦CB5x /p¥8²p´
''88%Z-´{¥¦%#¦ "³?´"c3%J9S8Mv8Y%v8M%?F%J S#qM5´   !K h Fm  n pK  !   
!  Q.)(#M @B /_x E¦M²p´
¯p? k8N 'F´Y£sRi%% J´3¥¦%¦M²E³?
´   K 2 +2#  n S  JI n Cn  !-n  #!h-K "        !b /UL9SY³?´
¬ ¯¡ =E
 9#SJ#J°qG79%.%N G"´
¯p?.JJ<%J8, 0´O£ ¯S8%G¨´=*±M²%²¥³?´ c5p8%hSM 9M%18#S8w%J1798 ¯ 4D¤
9S19S8´  -n   #   n       1 +  'K¥±p¥jxY(¥ M²p´
¯p#J°%8R J´'*±M²%²%²E³?´ +QTS!-K 1Wn  K  !    2  Q  ;I&nK   I   h  Cn 1 'n Q  K  SwK h  K !onYh^K    K 
  !#  Q   K   n QL K    JICKML ! n  !  +2 ' pn S
 !on    !?´ 5´ 0´DSJS#8vb-J%8S#¶%
J#.9S°5´
¯p#J°%8 J´9©-E8«´9£¯p<%#* 40´5*±M²%²%²E³?´ i%S¢kJ=WM°#b%JlS=p8%kSMN1SOkM¢k´
W*K 1   K;h -n      n '   #  !#  QE/ Y#± /#_x±CBU²p´
¯p#%J8% J´"£m¬­%S""¤=´D*±M²%²¥³?´ i%S¢vF#ª%JS#w	 MS %J MJJSp<MS"´
«P
 h8v*
-´Kp"´³?	  ;K    I   !K h\n Q     n   n Q% n #  n pK  K  n  K _#h ## K   on    
 Jn '   #      
" J95´J#± "x#± %¦´"R%S°%ª F%V<%J"´
¯S%J#8 Z´O*±M²%²±³?´­¨Z#SSq#%JJS8Mv8´L«&c3%SS#°K¶´K£ {%#8?#%J* 40´OpJ8´³?YR pK K    
" K 1Wn pK $ J
I n n  !on   	  QTS"!  #!?JJ3´5(¥ "CB5x±M²|B.´5¯pJ#°%8S¡ D8#M° G8##"´
-h.8R-0´O¥¦%¦#"³?´

WL

Ý9Þ¡¦Þ¦£1¨#á
	gÝ gßgÚ

¯%S8'´-´p¬­"Y¯k´q´9£ {%88M*N'´"¥¦%¦%±³?´.h8· M?N9%1i6%'(J8#J°0J%9<
·hSLMJ9#8MSl 
 %RS%RJ9##°´$
    ^L #n #    1 (Z¥8²E³?K¥ E¦#_xY¥(M²¦´
¤Z%###M5'´5¥¦%#¦ /³?´ GJ?J<MS¢pi6%O9%S#
S?JJ#°J%9<8´)"
1  Kom    WK*1  SK h!Fm # 
Cn pK  	
  !#  QE  G*±³?"±C@B "_x#± "#´
¤Z%###M5'´D¥¦%"¦ .³?2´ KM%#SMv.lM?ª?JJ$ hW%
S% 
%QSJ%QS?JJ#°<J%9L´
   K 1   K   KMLUm 1Wn     *±³?¥8² "_xY¥%¥_B.´
{MS98 '´v¥¦%#¦ ³?´    #     I!W+K 2 AQuKjm  Q  *I 1      KMLUm   n S   I\QK      QE´ "´ q´
SJS#8#JJ%@bh9%8S b%K¤J#%°%%´
M%gc%M%" Z´ 4-MSS89B´O£2c3JpE ´F¥¦%¦%±³?´ % S% S9J##°H. S#qJ#M
%9%##°´Fm  n pK  !   !#   UQE "3¥³?K¥%(¥ /_xY¥#± ´
¬­MS" ´J*±M²%² /³?	´ "9LUm      AK5I      JI   S"!  !UK hQSK       UQ )  K   n Q|L !h^K 'n Q  W+K 2 
AQuKjm Q  *I 1     ;+K 2 -LTG´ 5´ 0´vS#8 -89MS7E
%OG798F¯p891%"G#%%
¯pSM\b-J%8S#%´
¬­MS" ´ iMS9J8"c{´¬ J#S8%Sc{´K£ -h·G% 40´G*±M²%²±³?´ GE?%S#°RSJ1SSH%J
?%Jmv80JSMS
 &J·O¡%LS9J##°JS%9wU 3p¯M?L9%1
%v%°%_%Jl%°%M¡
?SJ v8S6%<%J1%´      W*K 1  FK   KL'm 1n    i*±³?v#¦ "_xY¥#± /´
¬­MS"* ´ ¢Y* ´ -B·% 4´v£¬ JS8%Jc{´*±M²%²¥³?´'¤·iMQ<S1#JS%F7p"%{p8%
SMµ1=# 
%J¡S% S9J##°´h« HK ji ! QuKjm7K nK Q "!-n      Q    K  pn Q|L ! n
n Q !   #n  #n Q% n   n  K  WK  Jn  K 5h       K   on      Jn '   #     -  "U´
¬­MS" ´ GS¢k ´ -h·G%N 4´9£j¬ JS#8%c´5*±M²%² /³?´ {S%Y Jw8J#<6%OSM9lM?N#

%¡SJ%RSJJJ##J°´  on   #   n        1  K*±³?Z(¥ "%¦_x±p¥_B.´
¬µ88J¨´J£j¬ #¢p8 4´3¥¦%#¦ ³?´D«J8SS#b?JJ$ i6%iS=#SS#BS_%3S, 
%
%NJS%YL´   !## ^n  Nm|m    I  n Q ^L Cn  #!?  (%¥¦p¥jx±p¥%¥M´

WL

Journal of Artificial Intelligence Research 24 (2005) 465-518

Submitted 09/04; published 10/05

Reasoning about Action:
An Argumentation-Theoretic Approach
Quoc Bao Vo

vqbao@cs.rmit.edu.au

School of Computer Science and Information Technology
RMIT University
GPO Box 2476V, Melbourne, VIC 3001, Australia

Norman Y. Foo

norman@cse.unsw.edu.au

Knowledge Systems Group
Artificial Intelligence Laboratory
School of Computer Science and Engineering
University of New South Wales, Sydney, NSW 2052, Australia

Abstract
We present a uniform non-monotonic solution to the problems of reasoning about action
on the basis of an argumentation-theoretic approach. Our theory is provably correct relative
to a sensible minimisation policy introduced on top of a temporal propositional logic.
Sophisticated problem domains can be formalised in our framework. As much attention of
researchers in the field has been paid to the traditional and basic problems in reasoning
about actions such as the frame, the qualification and the ramification problems, approaches
to these problems within our formalisation lie at heart of the expositions presented in this
paper.

1. Motivation and Introduction
The need for a good reasoning about action formalism is apparent for research in artificial
intelligence (AI). Alongside the logicist point of view to artificial intelligence, more recently,
there emerges the cognitivist and situated action-based approaches(see Kushmerick, 1996
and the references therein). The latter approaches provide some immediate and practical
answers to certain issues of AI. The current problem domains for (Soccer) Robot Cup
seem to be an area where these approaches promise to gain fruitful results. On the other
hand, the logicist approach aims at long term solutions for the general problems of AI.
From a logicist approach, formalising dynamic domains for reasoning about action can
be realised within a logical knowledge representation. The general idea is that intelligent
agents should be able to represent all kinds of knowledge in a uniform way such that some
general problem solver can fully employ and find a solution based on their knowledge. As
it turns out, there are difficulties with such a general approach to AI. Consider the task of
formalising dynamic domains in some logical language. To formalise the dynamics of an
action (or event) in a language with n fluents 1 , one will need to axiomatise not only about
the fluents that are effected by the action but also about those that are not. Essentially, it
requires that n axioms be asserted. Such a formalisation can hardly be considered a good
1. fluent is a technical term referring to functions or predicates whose values can be varied relative to time.
c
2005
AI Access Foundation. All rights reserved.

Vo & Foo

representation. Hence, there is the need to solve this problem in logic-based reasoning about
action formalisms. This is the well known frame problem as introduced by McCarthy and
Hayes (1969). Moreover, there is still a problem in axiomatising the effects of an action,
called the effect axioms. A logical axiomatisation requires that the conditions under which
the effects will take place after executing the action be precisely specified. However, there
are potentially infinitely many such conditions, some of which the reasoner may never have
thought about. No realistic formalisation would ever be able to exhaustively enumerate all
of those conditions. Nonetheless, to start a car, most people only worry about whether they
have the key to that car. They never bother checking whether there is something blocking
the tailpipe or checking all electric circuits to make sure that they are all well connected.
Such a story has long been well-known within the community of commonsense reasoning,
in particular reasoning about action. This is known as the qualification problem and was
introduced by McCarthy (1977).
While there have been a number of solutions to the frame problem (e.g., Shanahan, 1997;
Reiter, 1991; Castilho, Gasquet, & Herzig, 1999), the qualification problem has largely been
ignored with the notable exception of Thielscher’s (2001) solution within the Fluent Calculus and Doherty and Kvarnström’s (1998) circumscription-based solution using fluent
dependency constraints. Some people argue that the frame problem is already very challenging and it would be a good approach to thoroughly solve the frame problem before
complicating a formalism with the qualification problem. We argue that there is a danger
of approaching these problems from that point of view for (at least) two reasons:
1. It may be very hard to come up with a uniform solution for all problems: while many
existing solutions for the frame problem are monotonic (e.g., Reiter, 1991; Castilho
et al., 1999), the qualification problem inherently requires a non-monotonic solution.
This is the case with the original qualification problem as stated by McCarthy (1977)
for the dynamics of actions/events need to be finitely axiomatisable and when an
unexpected qualification for an action arises, the agent must necessarily retract his
initial expectation that the effects caused by the action would take place, making the
underlying reasoning machinery non-monotonic (see section 1.2 for a discussion on
the qualification problem).
2. Many solutions to the frame problem can only succeed under some precise assumptions.
For instance,
• Actions always succeed. This is the action omniscience assumption. More precisely, this assumption dictates that the qualification problem is skipped. This
is the case with all monotonic solutions to the frame problem. 2
• Fluents change if and only if the reasoner knows that there exists an action
that possibly changes its value. This can be termed as domain omniscience
2. The argument that any solution to the frame problem which works with nondeterministic action is not
subject to this assumption does not stand. The quick fix of allowing such an approach to a fortiori express
actions that may fail by representing failure as a possible effect is an invalid one. It is because we can
no longer infer that, in the absence of evidences that suggest otherwise, actions would normally succeed.
It’s also worth noting that Lin’s (1996) extension of Reiter’s (1991) solution to the frame problem in the
Situation Calculus to deal with nondeterministic action is based on circumscription.

466

Reasoning about Action: An Argumentation-Theoretic Approach

assumption. It assumes that the reasoner has complete (ontological) knowledge
of the domain about which he is reasoning.
The above two reasons are of course closely related as the former arises due to the
underlying assumptions in the latter which no longer holds once the qualification problem
is taken into consideration.
In the remainder of this section, we review several works on this topic before introducing
the reader to our approach.
1.1 The Frame Problem
In the late 1960s, the frame problem had been recognised as a major obstacle to formalising
dynamic domains (see the discussions and exposition by McCarthy & Hayes, 1969; Green,
1969). Several alternative responses to the frame problem have been proposed along way.
To respond to the explosive number of axioms required for theorem proving-based planners
as proposed by Green (1969), Fikes and Nilsson (1971) introduce procedures that operate
on special data structures used to represent dynamic domains. However, for complex and
sophisticated problem domains, e.g. those with domain constraints, concurrent actions,
observations at different time points, etc. STRIPS quite often fails to express the domain
knowledge. In fact, the expressivity of STRIPS is quite limited as has been pointed out
by Lifschitz (1987). Another response attributing the frame problem as an artefact of
the situation calculus has proved to be ungrounded. There are attempts to distinguish
a logical or epistemological aspect of the frame problem from the computational aspect
(e.g., McDermott, 1987; Kowalski, 1992). While the computational inefficiency associated
with a representation of dynamic domains in the situation calculus can be attributed to
the explosive number of global situations required by the situation calculus as argued by
Kowalski and Sergot (1986), the logical aspect of the frame problem is inherent to any
logic-based representation of dynamic domains. It is thus essential that a logical approach
to AI and knowledge representation have a decent solution to the frame problem.
Later, with the introduction of the qualification problem by McCarthy (1977), it is
reckoned that formalising dynamic domains not only is about solving the frame problem
but would require systematic studies to fundamental issues of knowledge representation. In
the early 1980s, the frame and the qualification problem were considered to be instances
of commonsense reasoning problems. In particular, many believed that a non-monotonic
reasoning framework would solve the frame problem. It was argued that the ‘principle of
inertia’3 which is considered to be the key to the frame problem can be formalised in terms
of default rules or default axioms in default reasoning. Moreover, it was also argued that to
solve the qualification problem, the following common sense law should be rendered: “an
action, by default, would qualify to succeed and bring about the intended effects unless there
is known reason for it not to,” in formalisations of dynamic domains.
With the introduction of several non-monotonic reasoning formalisms, e.g. truth maintenance systems (TMSs) by Doyle (1979), default logics by Reiter (1980), circumscriptive
approaches by McCarthy (1980), modal non-monotonic logic by McDermott and Doyle
3. The principle of inertia or the common sense law of inertia basically states that “By default a fluent
is assumed to persist over time unless there is evidence to believe otherwise.” The reader is referred to
Shanahan’s (1997) book for more details on this principle and the issues around it.

467

Vo & Foo

(1980), autoepistemic logic by Moore (1985), etc., it was believed that these problems were
solved. The solutions to these problems were illustrated as examples for the proposed nonmonotonic reasoning frameworks. For instance, McCarthy (1986) showed how circumscription is used to solve the frame problem relative to the blocks world domain. Unfortunately,
Hanks and McDermott (1987) show that these formalisations do not work correctly in a
simple dynamic domain known as the Yale Shooting Problem (YSP). We will now review
successful attempts to solve the frame problem:
1. Baker (1989) successfully modifies the original (and incorrect) circumscription policy
proposed by McCarthy (1986) to deal with the Yale Shooting Problem. In the traditional circumscriptive policy, the predicate Abnormal is minimised with the predicate
Holds allowed to vary. Baker suggests that, instead of allowing Holds to vary, the
function Result should be the one to be varied. While this does not solve the frame
problem in its full generality, this initiates the line of research which brings many
fruitful results to reasoning about action community. For a more detailed discussion
about these solutions and the follow up works, the reader is referred to Shanahan’s
(1997) book. Furthermore, Foo, Zhang, Vo, and Peppas (2001) present an exposition on the issue from an automata and system theory point of view. In order for
Baker’s (1989) solution to work correctly, additional axioms need to be introduced,
e.g. domain closure axioms, axioms about the existence of situations, etc. This emphasises that: (i) the circumscriptive approach to reasoning about action only works
under careful designation and considerations of the domain; and more importantly,
(ii) circumscription is domain dependent. That is, a domain dependent circumscriptive policy is required to correctly render the common sense of a particular problem
domain.
2. A number of researchers argue that in many cases, a monotonic solution to the frame
problem will be sufficient. Pednault (1989) assumes that the effect of actions on fluents
are specified by effect axioms of the following forms:
x, ~y , s) ⊃ F (~x, do(A(~y ), s)),
ε+
F (~
x, ~y , s)
ε−
F (~

⊃ ¬F (~x, do(A(~y ), s)),

(1)
(2)

Here, A(~y ) and F (~x, s) are the parameterised action and fluent, respectively; ε +
x, ~y , s)
F (~
and ε−
(~
x
,
~
y
,
s)
are
first
order
formulas
whose
free
variables
are
among
~
x
,
~
y
,
s.
Pednault
F
(1989) makes the following Causal Completeness Assumption:
The axioms (1) and (2) specify all the causal laws relating the action A
and the fluent F .
Note that the Causal Completeness Assumption is a stronger form of the domain
omniscience assumption presented above. Under this assumption, the following frame
axioms can be introduced:
F (~x, s) ∧ ¬ε−
x, ~y , s) ⊃ F (~x, do(A(~y ), s)).
F (~
468

Reasoning about Action: An Argumentation-Theoretic Approach

and
¬F (~x, s) ∧ ¬ε+
x, ~y , s) ⊃ ¬F (~x, do(A(~y ), s)).
F (~
Schubert (1990), elaborating on a proposal of Haas (1987), employs the so-called
Explanation Closure Axioms of the following forms:

F (~x, s) ∧ ¬F (~x, do(A, s)) ⊃ αF (~x, A, s),

(3)

¬F (~x, s) ∧ F (~x, do(A, s)) ⊃ βF (~x, A, s),

(4)

Or, equivalently, we can rewrite the above two axioms as follows:

F (~x, s) ∧ ¬αF (~x, A, s) ⊃ F (~x, do(A, s)).
and
¬F (~x, s) ∧ ¬βF (~x, A, s) ⊃ ¬F (~x, do(A, s)).
Schubert’s proposal is correct under the following assumption, called the Explanation Closure Assumption:
αF completely characterises all those actions A that can cause the fluent
F ’s truth value to change from true to false; similarly for β F .
Reiter (1991) then combines the merits of the above two proposals by systematically
generating the frame axioms as proposed by Pednault (1989) with the quantifiers over
the set of actions as proposed by Haas (1987) and Schubert (1990).
Other researchers who also propose monotonic solution to the frame problem include
Castilho et al. (1999), and Zhang and Foo (2002).
3. Attempts to solve the frame problem using default logic (Reiter, 1980) also encounter
some problematic issues. Hanks and McDermott (1987) show that a natural formulation of the Yale Shooting Problem in default logic suffers the same problem as that
with circumscriptive approaches, viz. the existence of anomalous extensions. Morris
(1988) proposes a slight modification on Hanks and McDermott’s original formulation
to the Yale Shooting Problem in an attempt to avoid the anomalous extensions. As
pointed out by Turner (1997), Morris’ formulation is complete, and thus eliminates the
anomalous extensions in the Yale Shooting Problem, but unsound. More importantly,
from Morris’ formulation it is not clear how dynamic domains should be formulated
in general. Turner (1997) himself then proposes a way to formalise dynamic domains
using default logic (and also logic programming). His solution is based on the following observation: In the Yale Shooting domain and similar dynamic domains, the
anomalous extensions arise because undesired effects of an action can be derived by
reasoning “backward in time.” For instance, in the Yale Shooting domain, by making
469

Vo & Foo

the counterintuitive supposition that the victim of the shooting is somehow still alive
after the shooting, an anomalous extension come up in the following way. First, it
allows the default saying that the victim persists to be alive regarding the shooting
action to be applicable. As a consequence, the gun must not be loaded before the
shooting action. Therefore, it blocks the application of the default saying that the
gun persists to be loaded regarding the waiting action. In other words, the loaded
gun would get unloaded (magically) during the waiting action which is an undesirable conclusion. To block these lines of “backward” reasoning, Turner appeals to the
non-contrapositivity of inference rules and replaces the implications by inference rules.
To guarantee that this formulation work correctly, additional techniques are required
such as a fact ϕ will be formulated as an inference rule:
¬ϕ
false
and enforcing the completeness of the initial situation by adding the following rules:
: Holds(f, S0 )
Holds(f, S0 )

: ¬Holds(f, S0 )
¬Holds(f, S0 )

for every fluent f .
However, Turner’s (1997) formulation is still fairly ad hoc as different techniques are
added to fix the known issues. For example, the inference rules are used in place of
implications to block the application of the undesirable “backward” reasoning, the
rules completing the initial situations are added to overcome the unsoundness issue in
Morris’ (1988) formulation, etc. This also shows the problematic side of default logic as
a uniform formalisation to various problems of common sense reasoning. This becomes
a serious issue if one proceeds with the question of how the qualification problem, a
typical problem of default reasoning, is solved in Turner’s (1997) formalisation of
dynamic domains. This is the case, for example, when instead of asserting that the
victim dies whenever it is shot by a loaded gun the reasoner can only maintain that as
a default proposition as there may be many hidden possible conditions under which
the victim may not die. Thus the reasoner is able to deal with ‘surprising’ situations
in which the victim is observed to be still alive after the shooting action (of a loaded
gun). Another, symmetric, case is with ‘surprises’ regarding the persistence of fluents.
For instance, after a waiting action, which is not supposed to unload a gun, the gun,
which was loaded before the wait action, is observed to be unloaded. Such a scenario
was first introduced by Kautz (1986) in a scenario called the Stolen Car Problem.
In both cases, reasoning “backward in time” is necessary. It is not clear how these
will be rendered in Turner’s formulation which explicitly intends to block “backward”
reasoning. These scenarios will be analysed in the solution we present later in this
paper.
1.2 The Qualification Problem
While there have been several solutions to the qualification problem, none of these addressed
the original qualification problem introduced by McCarthy (1977) and later formalised by
Ginsberg and Smith (1988).
470

Reasoning about Action: An Argumentation-Theoretic Approach

1. Lin and Reiter (1994) propose a formalisation for action theories in the situation calculus (SC). Their formalism is an extension of Reiter’s (1991) solution to the frame
problem (sometimes) by incorporating state constraints. They discover that there are
at least two different kinds of state constraints which they call ramification and qualification constraints. They then go further to claim a solution to the qualification (and
the ramification) problem. The basic idea behind their solution to the qualification
problem is that certain state constraints imply implicit preconditions of some actions.
Thus an action may not be qualified even though it appears (from the explicit action
description) to be. While this is of course a special case of the qualification problem, the classical qualification problem as introduced by McCarthy (1977) has a much
broader extent. In this setting, the qualification problem is more a pragmatic issue
than a technical issue. Similar to the frame problem, it is impractical, and sometimes
impossible, to axiomatise all the possible preconditions of an action. For example, in
addition to the requirement that the gun be loaded, to guarantee that performing the
action shoot would kill the victim, many preconditions must also be included such as:
the gun is not malfunctioning, the shooter does not miss the victim, the victim does
not wear a bullet-proof jacket, etc. among which some may be very improbable such
as: “no alien interferes with the bullet.” The reasoner simply does not want to consider these conditions by assuming that they are not the case unless there are explicit
evidences stating otherwise. In other words, the qualification problem in its original
form requires that the reasoner be able to tolerate the mistaken conclusions possibly
jumped to by previous inferences and to correct them appropriately. Henceforth, we
will always refer to the qualification problem in this original form. It is this similarity
to the frame problem that led John McCarthy to conjecture that:
The frame problem may be a sub-case of what we call the qualification
problem, and a good solution of the qualification problem may solve the
frame problem also.
(McCarthy, 1977, p. 1040, italics are original.)
Roughly 10 years after his introduction of the qualification problem, McCarthy (1986)
presented his solution to the problem using his non-monotonic formalism of circumscription. However, this solution suffers an almost identical flaw as its counterpart
regarding the frame problem: simple minimisation of abnormalities sanctions anomalous models (e.g., Thielscher, 2001).
2. McCain and Turner (1995) propose a solution to the problem described by Lin and
Reiter (1994), viz. the problem of deriving the implicit preconditions from state
constraints. McCain and Turner’s solution is posed in a model-based representation
of action theories.
3. Similar to McCain and Turner’s (1995) result, Baral (1995) offers a solution to the
problem defined by Lin and Reiter using a state-based representation. Baral extends
the language of disjunctive logic programs for state specification and as an action
description language.
471

Vo & Foo

4. Doherty and Kvarnström (1998) make a careful investigation to the qualification problem. They are aware of the shortcomings present in the definition of the “qualification
problem” introduced by Lin and Reiter (1994). They proceed one step further to distinguish between the weak and strong forms of the qualification problem. To deal
comprehensively with the qualification problem in its full extent, Doherty and Kvarnström apply circumscription on a predicate which plays a similar role to the predicate
P oss used by Lin and Reiter (1994).
Even though Doherty and Kvarnström’s (1998) solution is closest in spirit to the
original form of the qualification problem, there is still a serious problem with their
approach. The intended designation on predicate P oss and its variants is actionoriented. That is, it would qualify on the executability condition for the action under
consideration, not towards the effects that action is supposed to cause. In other
words, only circumscribing P oss does not guarantee to capture the full extent of
the qualification problem. For example, a single action of shooting a gun may cause
several effects: killing the victim, making a loud noise, emptying the cartridge, etc.
The conditions for such an action to be executable is: having the gun, the gun is not
broken, the gun is loadable, etc. Once the action is executable, it is not necessary
that all effects will take place. It may be the case that there is a loud noise and the
cartridge is emptied but the victim is still alive since the victim was wearing a bulletproof jacket. Assuming that the reasoner is somehow aware about this possibility,
should he include the requirement that the victim not wear a bullet-proof jacket as a
qualification for the action shoot? Perhaps he should not for he would still expect to
hear a loud noise and the cartridge to be emptied after the shoot action.
These are not the end of all the troubles though. The presence of both qualification
and ramification constraints causes several complications. Firstly, they are not syntactically distinguishable. Secondly, as has been mentioned by Doherty and Kvarnström
(1998), qualification constraints may cause indirect effects to arise and vice versa, i.e.
ramification constraints may reveal implicit action preconditions.
Remark: The terms ramification constraints and qualification constraints were first
introduced by Lin and Reiter (1994) when a careful examination to state constraints
was taken. As discussed by Doherty and Kvarnström (1998), these two kinds of constraints might interact in several ways. Consider the example introduced by Doherty
and Kvarnström (1998): the only preconditions of the action board a plane are havingticket and at-gate. However, to a passenger who places a gun into his pocket at home
before travelling to the airport and proceeding to the gate, a new qualification for
the action board materialises. Because one ramification of putting an object in your
pocket is that it will stay with you as you travel from location to location (i.e. result of
a ramification constraint), a reasoner could easily conjecture that our passenger fails
to board the plane. On the other hand, the fact that a passenger who possesses a gun
when trying to board the plane must fail to board the plane is a result of a qualification
constraint. Now, not only the fact that a passenger possesses a gun disqualifies the
action of boarding a plane but it also brings about an indirect effect when the action
of boarding a plane is executed: the passenger being put under arrest. Note that for
this indirect effect to take place, both requirements must be present: the action of
472

Reasoning about Action: An Argumentation-Theoretic Approach

boarding a plane being executed, and the above qualification constraint is present. In
other words, a qualification constraint might also bring about indirect effects.
We believe that all these problems are too sophisticated for any circumscription policy to successfully address in most situations. Furthermore, such a policy would be
extremely hard to understand and error-prone. Recall the failure of non-monotonic
reasoning formalisms regarding the frame problem (in its simplest form viz. without
the ramification and the qualification problems). Researchers had failed to point out
this bug for several years before Hanks and McDermott discovered it in their award
winning paper (1987).
5. More recently, Thielscher (2001) gives an exposition on the qualification problem.
Thielscher discusses the problem sustained by McCarthy’s (1986) simplistic circumscription policy, viz. the anomalous models. He then introduces a default logic based
formalisation for the qualification problem in the Fluent Calculus and shows that
his formalisation does not suffer the problem of anomalous models. Note that in
Thielscher’s formalisation, circumscription is still required to generate the initial theories of the default theories (in addition to the set of default rules). Nevertheless,
Thielscher’s solution still suffers the following drawbacks: (i) Thielscher’s use of the
predicate P oss is in the same way as has been formulated by Doherty and Kvarnström
(1998). Thus, qualifications are taken over the executability conditions for the actions
rather than over different effects of the actions; and (ii) while Thielscher shows that
the problem of anomalous models sustained by McCarthy’s (1986) circumscription
policy is overcome in his formalisation, it’s not entirely clear whether Thielscher’s
formalisation which is based on both circumscription and default logic will not suffer
from other anomalies.
1.3 The Ramification Problem
In the context of reasoning about action, the ramification problem is mainly related to
indirect effects. Finding a solution to this problem may not be easy as indirect effects
indicate exceptions to frame assumptions and require special treatment. While there have
been several formalisms dealing with the ramification problem, e.g., see (Lin, 1995; McCain
& Turner, 1995; Thielscher, 1997), there are still several issues that need a more careful
consideration. We consider three examples to motivate our discussion.
Example 1 Consider Thielscher’s (1997) circuit:
This example is interesting because it gives a counterexample for the minimalistic approaches e.g. in the work of McCain and Turner (1995). In this domain, the intended
relationship between relay and sw2 is that when relay is on, it would make sw 2 jump off.
Thus, when sw1 and sw3 are both closed, sw2 can not be also closed as that is prevented
by relay. However, there is certainly a duration (no matter how short it is) before sw 2
is forced to jump off by relay. In the state given in Figure 1, after performing the action
of closing switch sw1 , two next states are equally possible: one in which detect is on, in
another it is off.4 Only the latter is sanctioned in a minimalistic account. Through this
4. The reason for nondeterminism in this case is due to insufficiency of domain information: depending on
the sensitivity of relay, light and detect, when light could get lit quickly and detect is very sensitive to

473

Vo & Foo

-sw1

sw2
-light

-detect

-relay
sw3

Figure 1: Thielscher’s circuit
example, Thielscher pointed out the need for keeping track of the chains of applications of
indirect effects.
Thielscher (1997) proposes a way to remedy this problem by keeping track of the applications of the domain constraints which are re-expressed in terms of causal relationships.
Thus, given the above example, his formalism is able to arrive at a next state in which the
detect is on. Following such chains of causal relationships, the dynamic system undergoes
several intermediate states before arriving at the next state.
2
In this paper, we proceed one step further from Thielscher’s (1997) position by formally
representing the intermediate states as possible states of the world. 5 We believe that an
intelligent agent should be able to reason about these intermediate states even though
they may not satisfy all domain constraints. This capability is especially important if the
reasoner needs to explain certain observations about the world in a systematic way. We
note that given an observation, there may be several chains of causal relationships that
bring about that observation.6 Unless intermediate states are explicitly represented and
reasoned about, there is no way for an agent to have a full insight to the system in hand
and certain information would be missing.
Example 2 Consider Lin’s (1995) spring-loaded suitcase with two latches. Let’s assume
that the latches can be toggled if the suitcase is closed. The following state constraint is
supposed to apply in this domain: up(Latch 1 ) ∧ up(Latch2 ) ⊃ open(Suitcase).
The question is: how does a robot close the above suitcase back after opening it? McCain
and Turner (1997) also consider this problem and their answer is:
detect any glimpse of light and relay is not sensitive enough to make the switch sw2 jump off quickly
enough then detect will be on; otherwise it will stay off.
5. Note that this point of view also corresponds to the traditional definition of states as snapshots of the
world.
6. For example, given detect is not on in the next state, it can be that either the light has never been bright
or the light may have been bright but the detect is not sufficiently sensitive to detect its momentary
brightness.

474

Reasoning about Action: An Argumentation-Theoretic Approach

In general, when both latches are up, it is impossible to perform only the action
of closing the suitcase; one must also concurrently toggle at least one of the
latches.
(McCain & Turner, 1997, p. 464, italic is original.)
The problem now is how to represent the action of holding the suitcase closed such
that it would overcome the above indirect effect caused by the loaded spring. This also
suggests another kind of actions whose direct effects are to keep the world unchanged.
These actions have usually been formalised by other researchers as fluents, e.g. holding.
Our main objection to this approach is that agents also need to reason about these actions
since they may also require certain preconditions such as the agent is strong enough to
hold the object. Moreover, under certain (abnormal) circumstances, agents may also fail to
perform such actions. That is, these actions are also subject to the qualification problem
discussed in the previous subsection.
2
Example 3 Consider the circuit in Figure 2: 7

-relay1

-sw1
sw2

-relay2
Figure 2: A dynamic domain with a (potentially) infinite sequence of indirect effects
It is quite obvious that after performing the action f lip 1 whose direct effect is having sw1
closed, the following circular sequence of indirect effects will take place: {relay 1 , ¬relay2 } →
¬sw2 → {¬relay1 , relay2 } → sw2 → {relay1 , ¬relay2 }. This sequence of course would
potentially carry on the above sequence of indirect effects indefinitely unless sw 1 is flipped
open or some device stopped functioning correctly, e.g. when the battery is out of charge.
In other words, this action domain requires some action to be inserted in between a series
of on going indirect effects which can not be captured by the above representation. Note
also that none of the causation-based representations proposed by Lin (1995), McCain and
Turner (1995) or Thielscher (1997) is able to deal with the above action domain.
2
1.4 Towards a Solution
To address the problems discussed in the previous sections, we argue that in order to find a
uniform solution to these problems one should avoid cryptic formalisms whose consequences
7. This example is an instance of the so-called “vicious cycles” scenarios, e.g., see (Shanahan, 1999).

475

Vo & Foo

can not be seen clearly from the formalisation of the problem domains. As a consequence,
we propose a uniform non-monotonic solution to the main problems of reasoning about action. Essentially, when performing commonsense reasoning, the reasoner relies on a number
of plausible assumptions, e.g., assuming that an instance of birds flies, or assuming that
shooting a turkey with a loaded gun causes it to die, etc. In traditional default reasoning
formalisms such as circumscriptive approaches or default logic, these assumptions are made
implicit. For example, these are the instances of predicates which are minimised away by
circumscription or the implicitly asserted justifications in default rules when they are still
consistent with the extension under consideration in default logic. The proposed representation formalism aims at making these assumptions explicit so that an automated reasoner
is conscious (at least) about what assumptions it relies on when performing reasoning.
Then the reasoner can always manipulate these assumptions independently of each other.
It is also the basic idea of assumption-based frameworks which are at heart of Bondarenko,
Dung, Kowalski, and Toni’s (1997) argumentation-theoretic approach.
We then proceed to consider the ramification problems and domain theories with concurrent and non-deterministic events. Among the major results, we show that our framework
captures the essence of the causation-based approaches regarding the ramification problem.
Moreover, we also show the expressiveness of our formalism through two examples in which
indirect effects also need qualifications and infinite sequence of indirect effects. To the best
of our knowledge, none of the existing formalisms are able to cope with these scenarios.
Based on the basic idea of assumption-based frameworks, our approach comprises the
following major aspects of representation:
1. We introduce different types of assumptions to render various laws of common sense
in dynamic domains. For instance, frame assumptions are introduced to capture
the common sense law of inertia whilst (two types of) qualification assumptions are
introduced to overcome the qualification problem.
2. We introduce a special class of (system-generated) dummy actions to allow the explanation problem, i.e. when some actions or events occur outside of the reasoner’s
knowledge, to be dealt with in a uniform manner.
3. Being based on Bondarenko et al.’s (1997) argumentation-theoretic framework, our
approach makes use of the inference rules to represent domain knowledge.
4. Lying at heart of our approach is an argumentation-theoretic semantics, called plausibility semantics, which is argued to best render common sense knowledge in dynamic
domains. This semantics consists in a particular policy of resolving conflicting assumptions when computing the argumentation to be accepted.
To summarise, in this paper we formalise an expressive representation scheme in order
to cope with sophisticated action domains. We believe that such a formalisation sometimes
requires certain advanced knowledge to be encoded in a precise and well-engineered way.
The representation of action theories proposed in this paper can be considered as the intermediate level between commonsense and scientific knowledge. The expressiveness of the
formalism is improved through several independent steps by adding further assumptions
476

Reasoning about Action: An Argumentation-Theoretic Approach

into the domain descriptions. This also shows one advantage of our solution: a simple representation can be achieved by simply removing the involved assumptions. This is arguably
a desirable feature as the reasoner has the option of either increasing the expressibility of the
representation formalism or improving the simplicity and, as a consequence, the efficiency
of the reasoning system.
The rest of the paper is organised as follows: Section 2 summarises relevant features of
the abstract argumentation framework proposed by Bondarenko et al. (1997), its semantics and concrete instances. In Section 3 we present the syntax and semantics of the basic
temporal logic and the extension for reasoning about action. In Section 4 we present our
formalisation for reasoning about action based on the argumentation-theoretic approach
introduced by Bondarenko et al. (1997). Our approach to reasoning about action, in particular a uniform solution to the frame and the qualification problems, as well as the main
results of the paper are presented in Section 5. In Section 6, we show how the proposed
formalism is extended to deal with more complex dynamic domains, including those with
concurrent and non-deterministic events, and indirect effects. Related work and future
research directions are discussed in Section 7. We will defer most proofs of the results
presented in the paper to the Appendix.
This paper is an extended version of two earlier conference papers (Vo & Foo, 2001,
2002). The main differences are that in this version all proofs are included, lemmas that are
used in the proofs of the theorems are introduced to help the reader more easily comprehend
these results, and the presentation has been improved and extended with more examples of
the various constructions.

2. Defeasible Reasoning by Argumentation
Let a deductive system hL, Ri be given, where L is some formal language with countably
many sentences and R is a set of inference rules. Given a theory Γ ⊆ L and a sentence
α ∈ L, we write Γ `hL,Ri α if there is a deduction from Γ whose last element is α. T h hL,Ri (Γ)
denotes the set {α ∈ L | Γ `hL,Ri α}. Since the language L is generally kept fixed whereas
the set of inference rules R is likely to vary depending on the description of the domain,
when there is no possible confusion we will abbreviate ` hL,Ri and T hhL,Ri as `R and T hR ,
respectively. Thus the classical inference relation ` can also be written as ` RC where RC is
the set of inference rules of classical propositional logic. Note also that every set of inference
rules considered in this paper will be a super set of R C .
Given a deductive system hL, Ri, an assumption-based framework with respect to hL, Ri
consists of a theory Γ representing the current knowledge of the reasoner about the domain,
an assumption base AB and a contrariness operator − , i.e. given an assumption δ ∈ AB, δ
denotes the contrary of δ.
Remark: The notion of the contrary of an assumption is intended to generalise the classical
negation ¬δ. Note that in general assumptions may be constructed by special operators
(e.g. negation-as-failure in the case of logic programming, or the modal operator L in the
case of autoepistemic logic, Moore, 1985), thus the contrariness operator must also be sufficiently general.

477

Vo & Foo

The hardest part in reasoning with assumption-based frameworks is computing the set
of assumptions to augment the given theory Γ. In an argumentation-theoretic approach,
this is realised by the attack relation. To determine which assumptions to be accepted,
assumptions are put together to form arguments. The assumptions behind the best arguments are considered to be acceptable. Several semantics for best arguments are presented
by Bondarenko et al. (1997) based on the notions of attack: Given an assumption-based
framework hΓ, AB,− i and an assumption set ∆ ⊆ AB:
• ∆ attacks an assumption δ ∈ AB iff δ ∈ T h(Γ ∪ ∆).
• ∆ attacks an assumption set ∆0 ∈ AB iff ∆ attacks some assumption δ ∈ ∆ 0 .
• ∆ is closed iff ∆ = AB ∩ T h(Γ ∪ ∆).
• ∆ is conflict-free iff there does not exist any δ ∈ AB such that Γ ∪ ∆ ` R δ, δ.
Assumption-based frameworks in which assumption sets are always closed are referred
to as flat. In a flat assumption-based framework, the conflict-free property of a set of
assumptions ∆ is equivalent to the property that ∆ does not attack itself. The major
argumentation-theoretic semantics defined by Bondarenko et al. (1997) for assumptionbased frameworks include:
• Stability semantics: an assumption set ∆ ⊆ AB is stable iff
1. ∆ is closed,
2. ∆ does not attack itself, and
3. ∆ attacks each assumption δ ∈
/ ∆.
Bondarenko et al. (1997) show that the above stability semantics corresponds to the
standard semantics of extensions of Theorist (Poole, 1988), minimal models of (many
cases of) circumscription (McCarthy, 1980, 1986), extensions of Default Logic (Reiter,
1980), stable expansions of Autoepistemic Logic (Moore, 1985), and stable models of
logic programming. In other words, from a complexity-theoretic perspective, any
approach based on the existing formalisms to default reasoning can be rendered in a
corresponding assumption-based argumentation framework with no loss in terms of
computational complexity.
• Admissibility and Preferability semantics: Bondarenko et al. (1997) go further to
extend these existing formalisms by generalising the semantics of admissible and preferred arguments which were originally proposed for logic programming only. The
new semantics are defined in terms of “admissible” and “preferred” sets of assumptions/extensions. An assumption set ∆ ⊆ AB is admissible iff
1. ∆ is closed,
2. ∆ does not attack itself, and
3. for all closed sets of assumptions ∆ 0 ⊆ AB if ∆0 attacks ∆ then ∆ attacks ∆0 .
Maximal (with respect to set inclusion) admissible assumption sets are called preferred.
478

Reasoning about Action: An Argumentation-Theoretic Approach

Throughout this paper assumptions are expressed in terms of usual propositions. Thus,
we will replace the notion of contrariness − in Bondarenko et al.’s (1997) system with the
classical negation ¬ and omit it from the specification of assumption-based frameworks.
That is, an assumption-based framework hΓ, ABi consists of a theory Γ ⊆ L, and the
assumption base AB which contains the assumptions to be used in the reasoning.

3. Domain Descriptions
We introduce a propositional action description language based on a more comprehensive
representation formalism proposed by Sandewall (1994). In particular, we extend Drakengren and Bjä reland’s (1999) language so that it is possible to describe narratives in our
framework.
3.1 Syntax
Following Sandewall (1994), the underlying representation of time is a (discrete) time structure T = hT, <, +, −i consisting of
• a time domain T whose members are called time points which are integers in this
paper (except in a later part of the paper where the distinction will be made explicit);
• <, +, − are as usual for integers.
Given a time structure T = hT, <, +, −i, a signature σ with respect to T is a tuple
= hT , F, Ai, where T is a set of countably infinitely many time-point variables, F is a set
of propositional fluent names, and A is a set of action names. Since the time structure T is
fixed in the rest of this paper, T will be taken implicitly whenever a signature is introduced.
We assume that all sets in σ are countable. We denote F ∗ = {[¬]f | f ∈ F}.8 A member
of F ∗ is a fluent literal. Moreover, A = A0 ∪ DA, where A0 is the set of domain dependent
action names, called basic actions, e.g. load, shoot, etc. and DA = {da l | l ∈ F ∗ } is the set
of dummy actions. As will be explained later in this paper, our solution to the problems
of reasoning about action is based on the basic guideline of attributing changes to events.
Given the reasoner’s ignorance about certain events that bring about changes in the world,
the dummy actions are to be used to make up for these gaps in the reasoner’s belief state.
That is why we need to associate the dummy actions with the fluent literals from F ∗ .
For each fluent literal l ∈ F ∗ , we introduce the following two symbols: AQ l , and F Al :
• AQl is associated with the assumed qualifications upon the preconditions of an action
regarding the fluent literal l. Essentially, AQ l when used in the description of the
dynamics of an action α allows the reasoner to describe only the main preconditions
of α (with regards to the fluent literal l) while leaving other possible (but less probable)
qualifications to be rendered by a single assumption AQ l .
• F Al is associated with the frame assumptions regarding l. F A l , when coupled with
a particular frame inference rule, allows the reasoner to infer that the fluent literal l
continues to hold in future time points unless there is a reason that defeats F A l .
8. The notation [¬] means that the formula following it may, or may not, be negated.

479

Vo & Foo

def

def

Given a set of fluent literals Γ ⊆ F ∗ , we denote F AΓ = {F Al | l ∈ Γ} and AQΓ =
{AQl | l ∈ Γ}.
A time-point expression is one of the following:
• a member of T,
• a time-point variable in T ,

• an expression formed from time-point expressions using + and −. For convenience,
we will also write τ + and τ − instead of τ + 1 and τ − 1, respectively.
We denote the set of time-point expressions by T E.
Definition 3.1 Let a signature σ = hT , F, Ai be given and τ, υ ∈ T E, f ∈ F, α ∈ A,
R ∈ {=, <}, ⊗ ∈ {∧, ∨, →, ↔}. Define the basic (domain description) language Λ over σ
by:
Λ0 ::= true | false | f | τ Rυ | ¬Λ0 | Λ0 ⊗ Λ0 | [τ ]Λ0 ,
Λ ::= Λ0 | [τ, υ]α | ¬Λ | Λ ⊗ Λ
and the assumption base AB by:
AB = ABAQ ∪ ABF A , where
ABAQ = {[τ, υ]AQl | τ, υ ∈ T E and l ∈ F ∗ }, and
ABF A = {[τ ]F Al | τ ∈ T E and l ∈ F ∗ }.
The domain description language LD (over σ) is defined: LD = Λ ∪ AB.9
[τ, υ]α means the action α has a duration corresponding to the interval [τ, υ]. [τ, υ]AQ l
means the fluent literal l is assumed to be qualified to hold by the end of the interval [τ, υ].
[τ ]F Al means the fluent literal l is assumed by default to persist from the time point τ
to the next, i.e. the principle of inertia. Notice the difference between [τ ]F A l1 and [τ ]l2
for some fluent literals l1 , l2 ∈ F ∗ . [τ ]l2 indicates that the fluent literal l2 holds at τ while
[τ ]F Al1 indicates that the fact that the fluent literal l 1 persists during the interval [τ, τ + ]
is true.
For example, in the blocks world domain, to say that block A is on block B at the time
point 2, we write: [2]on(A, B); or, to say that an action pickup the block A occurs between
time points t1 + 3 and t2 − 1 and that the relation < holds between t 1 + 5 and t2 , we write
[t1 + 3, t2 − 1]pickup(A) ∧ (t1 + 5 < t2 ).
A formula that does not contain any connectives (i.e. ∧, ∨, →, ↔, ¬, and [.]) is atomic.
If γ is atomic and τ ∈ T E, then the formulas γ, [τ ]γ, ¬γ, ¬[τ ]γ, and [τ ]¬γ are literals.
Let γ be a formula. A fluent f ∈ F occurs free in γ iff it does not occur within the
scope of a [τ ] expression in γ. τ ∈ T E binds f in γ if a formula [τ ]ψ occurs as a subformula
of γ, and f is free in ψ. If no fluent occurs free in γ, γ is closed. If γ does not contain any
occurrence of [τ ] for any τ ∈ T E, then γ is propositional.
9. It would be more precise to denote the domain description language over σ by Lσ . However, as the
signature is usually clear from the context and in order to avoid the mention of σ every time we have to
formalise something with the domain description language, we choose to denote it by LD .

480

Reasoning about Action: An Argumentation-Theoretic Approach

3.2 Semantics
Definition 3.2 Let σ = hT , F, Ai be a signature. A state over σ is a function from F to
the set {true, false} of truth values. A history over σ is a function h from T to the set of
states. A valuation is a function φ from T E to T. A narrative assignment is a function η from
T×A×T to the set {true, false}. In addition, we define ε q : T×AQF ∗ ×T → {true, false}
and εf : T×FAF ∗ → {true, false}. An interpretation over σ is a tuple hh, φ, η, ε q , εf i where
h is a history, φ is a valuation, η is a narrative assignment and ε q , εf are defined as above.
Example 4 Consider Hanks and McDermott’s (1987) Yale Shooting Problem (YSP) :
There are three possible actions: load (the gun), wait, and shoot (the victim with the
gun). Normally, waiting does not cause any change in the world, but shooting leads to the
victim’s death, provided that, of course, the gun is loaded. Assume that all three actions
are performed, in the given order.
We define the signature σysp to be a tuple h{t, t1 , t2 , . . . , u, u1 , u2 , . . .}, {loaded, alive}, {load,
wait, shoot}i. Then the Yale Shooting problem can be formulated in the domain description
language Lysp as the following theory: Γysp,0 = {[0]alive, [0, 1]load, [1, 2]wait, [2, 3]shoot}.
The following two histories h1 and h2 are corresponding to the well-known models in
the literature of reasoning about action: h 1 to the intended model and h2 to the anomalous
model most frameworks would produce.

h1

h2

A

A

A

A

L

L

L

L

0

1

2

3

A

A

A

A

L

L

L

L

0

1

2

3

Figure 3: The two histories for the YSP action description.
Each oval in Figure 3 represents a state over σ ysp . A narrative assignment complying
with the above action description would map the three tuples (0, load, 1), (1, wait, 2), and
(2, shoot, 3) to true and other tuples to false (relative to the assumption that ‘normally,
given any action and any time point, there is no instance of that action at that time point
unless specified otherwise’ ).
Definition 3.3 Let γ, δ ∈ Λ and I = hh, φ, η, ε q , εf i an interpretation. Assume τ, υ ∈ T E,
f ∈ F, A ∈ A, R ∈ {=, <}, l ∈ F ∗ , ⊗ ∈ {∧, ∨, →, ↔}, and χ ∈ {true, false}. Define the
truth value of γ in I for a time point t ∈ T, denoted I(γ, t) as follows:

481

Vo & Foo

I(χ, t) = χ
I([τ, υ]A, t) = η(φ(τ ), A, φ(υ))
I([τ ]F Al , t) = εf (φ(τ ), F Al )
I(¬γ, t) = ¬I(γ, t)
I([τ ]γ, t) = I(γ, φ(τ ))

I(f, t) = h(t)(f )
I([τ, υ]AQ l , t) = εq (φ(τ ), AQl , φ(υ))
I(τ Rυ, t) = φ(τ )Rφ(υ)
I(γ ⊗ δ, t) = I(γ, t) ⊗ I(δ, t)

Two formulas γ and δ are equivalent iff I(γ, t) = I(δ, t) for all I and t. An interpretation
I is a model of a set Γ ⊆ Λ of formulas, denoted I |= Γ, iff I(γ, t) = true for every t ∈ T
and γ ∈ Γ. A formula γ ∈ Λ is entailed by a set Γ ⊆ Λ of formulas, denoted Γ |= γ, iff γ is
true in all models of Γ.
Definition 3.4 Let I = hh, φ, η, εq , εf i be an interpretation.
1. The set OccI = {(t, A, u) ∈ T × A × T | η(t, A, u) = true} is called action occurrence
denotation of I.
2. The set F AI = {(t, F Al ) ∈ T × FAF ∗ | εf (t, F Al ) = true} is called F A-denotation of
I.
3. The set AQI = {(t, AQl , u) ∈ T × AQF ∗ × T | εq (t, AQl , u) = true} is called AQdenotation of I.

4. Representing Dynamic Domains in the Argumentation-Theoretic
Approach
We now proceed to showing an assumption-based framework for representing dynamic domains. We subsequently introduce a uniform framework for solving the frame and the
qualification problems based on the argumentation-theoretic approach. General solutions
for the frame and the qualification problems can be obtained by computing plausible sets
of assumptions which guarantee that extensions computed from these sets of assumptions
will be consistent when the given theory is consistent. We now introduce some additional
notations: Given an inference rule r ∈ R, we denote by prem(r) and cons(r) the premise
and the consequence of rule r, respectively.
Definition
S 4.1 A deductive system hL D , Ri is well-defined iff for each subset S ⊆ R, if
the set r∈S prem(r) is consistent then the set CON S(S) = {cons(r) | r ∈ S} is also
consistent.
Henceforth, we will assume that deductive systems are well-defined. Being formalised
in terms of the argumentation-theoretic approach, the representation requires an extended
notion of consistency.
Definition 4.2 Let hLD , Ri be a deductive system,
(i) a set of sentences Γ ⊆ LD is R-consistent iff Γ 6`R false;
(ii) an assumption-based framework hΓ, ABi with respect to hL D , Ri is consistent iff Γ is
R-consistent.
482

Reasoning about Action: An Argumentation-Theoretic Approach

Remark: Observe that even when hLD , Ri is a well-defined deductive system, consistency
b
is not equivalent to R-consistency. For instance, let R = { }, the (logically) consistent
¬a
theory Γ = {a, b} is not R-consistent.
Example 4 (continued) Returning to the Yale Shooting problem, the following inference
rules describe the actions of this domain:
[τ, υ]load
[υ]loaded ∧ ¬[τ ]F A¬loaded

(5)

[τ, υ]shoot, [τ ]loaded
¬[υ]alive ∧ ¬[τ ]F Aalive

(6)

[τ ]loaded, [τ ]F Aloaded
[τ + ]loaded

(7)

[τ ]alive, [τ ]F Aalive
[τ + ]alive

(8)

¬[τ ]loaded, [τ ]F A¬loaded
¬[τ + ]loaded

(9)

¬[τ ]alive, [τ ]F A¬alive
¬[τ + ]alive

(10)

Rules (5) and (6) represent the descriptions of the actions load and shoot, respectively.
Action wait does not cause any effect to the world, so there is no need to describe it. Other
rules render the common sense law of inertia: “at any time point, a fluent literal presumably
persists to the next time point.”
Most argumentation-theoretic semantics, e.g. stability, admissibility, preferability, complete, well-founded semantics, etc. (Bondarenko et al., 1997) are based on the notion of
attack. However, to reason about problem domains with incomplete information, especially
action domains, this notion alone may not be sufficient as we may not always be able to
construct explicit arguments to defeat unsound assumptions. For example, consider the
Yale Shooting Problem: By observing that a turkey is shot with a loaded gun at time point
1, the reasoner infers plausibly that the turkey is dead at time point 2 using the assumption
that the action shoot is qualified to bring about the effect of killing the victim. However,
at time point 2, the reasoner could observe that the turkey is still alive. Existing solutions
to the frame problem, e.g. Reiter’s (1991), Thielscher’s (1997), Castilho et al.’s (1999), etc.
fail to deal with such a surprise since they allow a contradiction to be derived. Observe that
the reasoner does not have any explicit reason to defeat the above qualification assumption,
i.e. she is not aware of any cause that prevents the application of this qualification assumption. She only knows that it is not acceptable in this case by common sense. To formalise
such phenomena, we introduce the notion of rejected assumptions.
483

Vo & Foo

Definition 4.3 Given an assumption-based framework hΓ, ABi, a set of assumptions ∆ ⊆
AB rejects an assumption δ ∈ AB iff
(a) ∆ is conflict-free, and
(b) ∆ ∪ {δ} attacks itself.
For instance, in example 4, the set of assumptions ∆ 1 = {[0]F Aalive , [1]F Aalive , [1]F Aloaded }
attacks the assumption [2]F Aalive .10 Moreover, relative to the given action description, any
set of assumptions attacks the assumption [0]F A ¬loaded . On the other hand, the set of assumptions ∆2 = {[0]F Aalive , [1]F Aalive , [2]F Aalive } rejects the assumption [1]F Aloaded but
∆2 does not attack it.
Observation 1 Given an assumption-based framework hΓ, ABi and a conflict-free set of
assumptions ∆ ⊆ AB, if ∆ attacks an assumption δ ∈
/ ∆ then ∆ rejects δ.
Then, why do we not generalise the contrariness notion of an assumption so that it
would be general enough to account for all rejected assumptions? The reason is because we
want to isolate the set of assumptions that are rejected but are not attacked as part of our
solution to the frame problem.
Definition 4.4 Given an assumption-based framework hΓ, ABi, a set of assumptions ∆ ⊆
AB leniently rejects an assumption δ ∈ AB iff
(a) ∆ rejects δ, and
(b) ∆ does not attack δ.
def

We denote Lr(∆) = {δ ∈ AB | δ is leniently rejected by ∆}.
To show that our solution provides an intuitive account for problems of reasoning about
action, several scenarios should be considered. These include the projection problem, the
most basic form of the frame problem, whose typical example is the infamous YSP. Another
scenario concerns with the explanation problem which are usually discussed with the Stolen
Car Problem (Kautz, 1986) and the Stanford Murder Mystery (Baker, 1989). We will first
provide an informal discussion of our approach through these examples.
In the present solution, the frame assumptions are the essence of the principle of inertia,and their role in the argumentation approach is illustrated below by the Yale Shooting
Problem. In this formulation we intentionally ignore the qualification problem (it is addressed in the next section) to highlight how the frame problem is solved. We now reconsider the well-worn example YSP to motivate our approach to the frame problem.
Example 4 (continued) Given the theory Γ ysp , the argumentation-theoretic approach will
yield the following preferred set of assumptions (Bondarenko et al., 1997):
{[t]F Al | t ∈ T and l ∈ {loaded, alive, ¬loaded, ¬alive}} \ {[0]F A ¬loaded , [2]F Aalive },
which corresponds to the intended model of this scenario in which the gun remains loaded
at time point 2 and the victim is not alive at time point 3.
This extension is also the stable extension and well-founded semantics (Bondarenko
et al., 1997) of the given theory under the argumentation-theoretic approach. Note that in
10. In fact, any set of assumptions containing the assumption [1]F Aloaded would attack [2]F Aalive .

484

Reasoning about Action: An Argumentation-Theoretic Approach

case one would like to be uncertain about whether the gun is still loaded after the shooting
action, one just simply needs to add an axiom: [τ, υ]shoot → ¬[τ ]F A loaded to dictate that
the persistence of the fluent loaded after the action shooting is not guaranteed. In that
case, we can still derive that [τ ]loaded for τ = 1, 2, but we can no longer give a definite
assertion about [τ ]loaded for τ ≥ 3.
As the above formalisation of YSP resembles that using default logic, it may be surprising that the problem of unintended models pointed out by Hanks and McDermott
(1987) for circumscription, default logic, autoepistemic logic does not happen here. The
principal reason is the interaction of the inference rules and the notion of attack in the
argumentation-theoretic framework, which invalidates undesired assumptions. Notice that
even if ¬[2]loaded can be (magically) derived, it cannot lead to ¬[1]F A loaded . Therefore, the
set of assumptions corresponding to this case does not satisfy the conditions of preferred
set of assumptions, thus ruling out this unintended model. This shows one of the important
features of assumption-based frameworks on its capability of making explicit the assumptions used by the reasoner during the course of inference. Recall that defaults’ justifications
are accepted as long as they are consistent with some extension for credulous semantics
or all extensions for skeptical semantics (thus the name consistency-based approach.) In
light of the inertia principle, it’s considered to be abnormal if a fluent does not persist
from a state to the next state. To minimise the abnormality, (normal) default rules are
introduced to express the fact that if it’s consistent to believe that there is no abnormality
with respect to a fluent f and an action a in a situation s then assert that. But then we
would fail to distinguish between the abnormalities brought about by reasonable causes and
those unintuitively generated to make them consistent with some possible extension. The
latter is of course corresponding to the anomalous models. By using explicit assumptions,
not only consistency is maintained (by preventing the accepted assumptions from attacking
themselves) but each rejection of assumptions must also be justified by the known facts
from the given theory.
Discussion:
1. Turner (1997) showed that an alternative representation of the YSP in default logic
can help solve the issue of anomalous models introduced by Hanks and McDermott’s
(1987) representation. Turner formulates the Yale Shooting scenario as follows:
¬Holds(Alive, S0 )
False

(11)

True
Holds(Loaded, Result(Load, s))

(12)

Holds(Loaded, s)
¬Holds(Alive, Result(Shoot, s))

(13)

: Holds(f, S0 )
Holds(f, S0 )

(14)

485

Vo & Foo

: ¬Holds(f, S0 )
¬Holds(f, S0 )

(15)

Holds(f, s) : Holds(f, Result(a, s))
Holds(f, Result(a, s))

(16)

¬Holds(f, s) : ¬Holds(f, Result(a, s))
¬Holds(f, Result(a, s))

(17)

Notice that Turner also uses the inference rules to block the “backward” reasoning
that generates the anomalous models of the Yale Shooting scenario. However, this also
means that all kinds of useful backward reasoning will also be blocked. In other words,
Turner’s formulation fails to deal with “surprising” observations about states at later
time points. As a consequence, Turner’s formulation only works when the domain is
restricted to qualification-free. As soon as the action descriptions, e.g. the one for
the shoot action in YSP, need to rely on some default justifications, e.g. qualification
assumptions, Turner’s formulation would also encounter the problem of undesirable
extensions. Our approach offers solutions to both of the above issues.
2. After Hanks and McDermott’s (1987) seminal paper in which early approaches to
the frame problems were exposed, besides new attempts to solve the frame problem,
Sandewall (1994) should be accredited as the first who tried to approach the problems of reasoning about action in a systematic way. As part of this effort, he also
examines the reason behind the failure of early approaches to the frame problem. As
discussed by Sandewall, early approaches to reasoning about action while attempting
to formulate the inertia principle have made the common mistake of making changes
the abnormality regarding this principle but failing to distinguish between normal
changes triggered by actions and anomalous changes. This important insight turns
out to be a consequence of a much more general law for reasoning about dynamic
domains discovered by researchers in the community in pursuit of solutions to various
problems of reasoning about actions: “action dynamics are causality-based.” It is this
principle that underpins most solutions to the problems of reasoning about action.
The anomalous models that arise in early approaches to the frame problem discovered by Hanks and McDermott (1987) or to the qualification problem as discussed
by Thielscher (2001) are those in which the causes of abnormalities are not present.
On the other hand, regarding the ramification problem, given a domain constraint
involving a number of fluents, it’s important to know which of these fluents are the
causes influencing the other fluents, i.e. the causality direction between the involved
fluents.
In light of the above analysis, Turner’s (1997) approach appears to be rather ad hoc.
Note that Turner’s solution to the problem of anomalous models is to block “backward reasoning” by the use of inference rules without any motivation on why backward
reasoning is a bad thing. While his approach appears to share with solutions based
on chronological ignorance (which will be discussed in more details in Section 7) the
486

Reasoning about Action: An Argumentation-Theoretic Approach

notion of directedness: By minimizing chronologically or blocking backward reasoning, one tends to minimize causes rather than effects. However, a more systematic
approach to various problems of reasoning about action is still very much desired.
Nevertheless, while the preferability semantics copes successfully with the YSP, it can
not properly account for the explanation problem, e.g. the Stanford Murder Mystery (Baker,
1989), the Stolen Car Problem (Kautz, 1986). The subtlety lies in the derivation of the
contrary of the frame assumptions. The contrary of a frame assumption is derived only
when both the occurrence of the event that brings about the change (absent in the Stolen
Car Problem) and the preconditions required to be satisfied for the change to actually take
place (absent in the Stanford Murder Mystery) are explicitly derivable. This is where the
notion of (leniently) rejected assumptions is called into service.
Definition 4.5 Given an assumption-based framework F A =hΓ, ABi, a set of assumptions
∆ ⊆ AB is presumable wrt F A iff
(a) ∆ = {δ ∈ AB | Γ ∪ ∆ `R δ} (in the terms given in Bondarenko et al., 1997, ∆ is
closed),
(b) ∆ does not attack itself, and
(c) for each assumption δ 6∈ ∆, δ is rejected by ∆.
Definition 4.6 Given an assumption-based framework F A =hΓ, ABi, a set of assumptions
∆ ⊆ AB is plausible wrt F A iff
(a) ∆ is presumable, and
(b) there exists no ∆0 ⊆ AB such that ∆0 is presumable and Lr(∆0 ) ⊂ Lr(∆).
We now proceed to formalising action theories in our framework.
Definition 4.7 Let σ = hT , F, Ai be a signature. Assume τ, υ ∈ T E, α ∈ A, Φ ⊆ Λ, and
l ∈ F ∗ . A domain description D (over σ) is a tuple hL D , R, AB, Γi, where:
1. LD is the domain description language and AB is an assumption base over σ;
2. R = RC ∪ RF ∪ RA ∪ RQ , where
(a) RC is the set of inference rules of (classical) propositional logic;
[τ ]l, [τ ]F Al
(b) RF is the set of frame-based inference rules of the form:
, i.e. those
[τ + ]l
that represent the frame axioms in terms of inference rules;
Φ, [τ, υ]α, [τ, υ]AQl
,
(c) RA is the set of action descriptions which are inference rules of the form:
[υ]l ∧ ¬[τ ]F A¬l
i.e. those that represent the conditions for the action α to bring about l; and
Φ
(d) RQ is the set of qualification-based inference rules of the form:
, i.e.
¬[τ, υ]AQl
those that represent the qualifications regarding the fluent literal l.
3. The theory Γ ⊆ Λ.
Given a set of assumptions ∆, we denote ∆ F A = ∆ ∩ ABF A and ∆AQ = ∆ ∩ ABAQ .
Observation 2 Let D = hLD , R, AB, Γi be a domain description, for each set of assumptions ∆ ⊆ AB, either ∆ is closed or ∆ attacks itself.
487

Vo & Foo

5. Reasoning about Action: The Frame and the Qualification Problems
In general, we adopt the following guidelines in seeking a uniform solution to the problems
of reasoning about action:
• The derived pieces of information do not conflict with the given facts;
• Occurrences of events are minimised; and
• The inertia of fluents is maximised though the minimality of the event occurrences
will be of higher priority.
Aside from the trivial case of occurrences of actions causing the frame assumptions to
be rejected, two aspects of actions can be distinguished:
1. An action happens but the change it is supposed to cause does not take place. We
call this expectation failure and this is more or less the qualification problem; and
2. No actions that are known to have happened and caused a change but the change
did take place. We call this surprise and this is usually known as the explanation
problem.
The following assumption represents our underlying intuition behind reasoning about
action formalisms.
Assumption 1 Intuitive models contain minimal (with respect to set inclusion) sets of
surprises.
Now we introduce some model-theoretic counterpart notions of the assumption-based
notions presented above.
Definition 5.1 Let σ = hT , F, Ai be a signature and D = hL D , R, AB, Γi a domain description over σ. An interpretation I = hh, φ, η, ε q , εf i is a model of D iff
1. I is a model of Γ;
2. for each r ∈ R, if I |= prem(r) then I |= cons(r).
The following definition captures one of several aspects of the (model-theoretic) solution
of the frame problem. This aspect is known as the action-oriented frame problem in Lin
and Shoham’s (1995) terms. The proposed minimisation policy formalises the intuition that
change does not happen by itself but is caused by some kind of event. Thus, for each fluent,
if its value is changed between two timepoints τ and υ, (at least) an occurrence of some
event must end at υ that brings about that change.
Definition 5.2 Let D = hLD , R, AB, Γi be a domain description and I a model of D. I is
a coherent model of D iff
1. for each basic action α ∈ A0 and τ, υ ∈ T E, if I |= [τ, υ]α then Γ |= [τ, υ]α ; and
488

Reasoning about Action: An Argumentation-Theoretic Approach

2. for each l ∈ F ∗ and t ∈ T, if I |= [t]l ∧ ¬[t+ ]l then either
(a) there are α ∈ A0 and s ∈ T such that r =
I |= prem(r)[τ1 /s, τ2 /t+ ],11 or
(b) I |= [t, t+ ]da¬l

Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈ R and
¬[τ2 ]l ∧ ¬[τ1 ]F Al

Thus, in a coherent model: (i) all satisfiable basic actions must follow from the given
theory, and (ii) all changes are attributable to events of one kind or another.
Given an interpretation I, we want to extract the sets of assumptions satisfiable in I.
Definition 5.3 Let σ = hT , F, Ai be a signature and I an interpretation over σ. The set
of frame assumptions satisfiable in I, denoted ∆ IF A , is defined as follows:
∆IF A = {[t]F Al | (t, F Al ) ∈ F AI }
and the set of qualification assumptions satisfiable in I, denoted ∆ IAQ , is :
∆IAQ = {[t1 , t2 ]AQl | (t1 , AQl , t2 ) ∈ AQI }
We also write ∆IQF = ∆IAQ ∪ ∆IF A .
Conversely, given a theory Γ and a set of assumptions ∆, a reasoner can also construct
his models about the domain of interest.
Definition 5.4 Let D = hLD , R, AB, Γi be a domain description and ∆ ⊆ AB. A model
I = hh, φ, η, εq , εf i of D is ∆-relativised iff
1. for each δ ∈ AB, I |= δ iff δ ∈ ∆; and
2. OccI = OAD ∪ DAS(∆), where:
(a) OAD = {(φ(τ1 ), α, φ(τ2 )) ∈ T × A0 × T | |= [τ1 , τ2 ]α}, and
(b) DAS(∆) = {(t, da¬l , t+ ) ∈ T × DA × T | [≈]FAl ∈
/  and there do not exist any
action
Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈ R and I |=
α ∈ A0 and s ∈ T such that r =
¬[τ2 ]l ∧ ¬[τ1 ]F Al
prem(r)[τ1 /s, τ2 /t+ ]}.
∆-relativised models are one of the central notions of our framework. Essentially, assumptions underpin our machinery to conjecture information based on common sense knowledge. As such, we will try to accept as many assumption as possible unless there is a good
reason not to. Therefore, given a set of assumptions, we will attribute every missing frame
assumption to a possible change in the domain the agent is reasoning about which is caused
by either a known action/event or some unknown action, called dummy actions in this
paper.
The following observation is immediate from condition (1.) in the above definition.
Observation 3 Let a domain description D = hL D , R, AB, Γi and a set of assumptions
∆ ⊆ AB be given. If the model I of D is ∆-relativised then ∆ IQF = I(∆, t) for every t ∈ T.
11. The notation ϕ[v1 /t1 , . . . , vn /tn ] is standard in logic and meant to be the instantiation of the formula ϕ
with the variables v1 , . . . , vn being replaced by the terms t1 , . . . , tn , respectively.

489

Vo & Foo

5.1 The Frame Problem
First we will address the frame problem in a simple setting viz. without qualification
assumptions, but we will lift the restrictions later.
Definition 5.5 Let D = hLD , R, AB, Γi be a domain description. D is a simple domain
description, or S-domain, iff RQ = ∅ and AQ does not occur anywhere in R or Γ.
Definition 5.6 Let D = hLD , R, AB, Γi a domain description. An interpretation I =
hh, φ, η, εq , εf i is a simple model, or S-model, of D iff
1. I is a model of D; and
2. εq (t, AQl , u) = true for every (t, AQl , u) ∈ T × AQF ∗ × T.
This effectively isolates the frame problem from the qualification problem. Note also
that if I is an S-model then ∆IAQ = ABAQ . A coherent S-model is an S-model which is
coherent.
Example 4 (continued.) The following is part of one of the coherent models of D ysp :
{[0, 1]load, ¬[0]loaded, [1]loaded, [0]alive, [1]alive,
[1, 2]wait, [1, 2]da¬loaded , ¬[2]loaded, [2]alive,
[2, 3]shoot, ¬[3]loaded, [3]alive},
which corresponds to one of the anomalous models of this scenario (the one pointed out by
Hanks and McDermott).
But it is not desirable to admit the occurrence of an event when there is no evidence
for it. Thus we need to minimise the set of action occurrences in a given action theory.
Definition 5.7 Let D be an S-domain. A coherent S-model I of D is a prioritised minimal
model (or simply PMM) of D iff there does not exist any coherent S-model I 0 of D such
0
that OccI ⊂ OccI .
Note that the above model-theoretic minimisation policy is not based on the frame
assumptions. This solution to the frame problem is thus amenable to well-known techniques
such as circumscription12 , but we believe an argumentation-theoretic approach is not only
more direct but also has wider applicability. In order to provide the connection between
the above (model-theoretic) minimisation policy and the (argumentation-theoretic) notion
of plausible sets of assumptions we need to maximise the set of assumptions satisfiable in a
PMM.
Definition 5.8 Let D be an S-domain. A PMM I of D is a canonical prioritised minimal
model (or simply CPMM) of D iff there does not exist any PMM I 0 of D such that
0
F AI ⊂ F A I .
We now want to see how the account of plausible sets of assumptions connects to this
account of minimality.
12. in combination with the introduction of occurrences of dummy actions.

490

Reasoning about Action: An Argumentation-Theoretic Approach

Lemma 1 Let D = hLD , R, AB, Γi be an S-domain. If I is a CPMM of D then for each
assumption δ ∈ AB: δ ∈
/ ∆IQF iff δ is rejected by ∆IQF .
Proof. (⇒) Suppose by way of contradiction that δ is not rejected by ∆ IQF , then ∆IQF ∪{δ} is
R-consistent, i.e. Γ∪ ∆IQF ∪ {δ} 6`R false. Since D is an S-domain, AB AQ ⊆ ∆IQF . Assume
that δ = [τ ]F Al for some τ ∈ T E and l ∈ F ∗ , we can construct an interpretation I 0 in such a
0
way that I 0 interprets everything except F A the same as I and F A I = F AI ∪{(φ(τ ), F Al )}.
Since I is a PMM of D, from the above construction, I 0 is also a PMM of D. But
0
F AI ⊂ F AI . Contradiction.
(⇐) Obvious.

2

Lemma 2 Let D = hLD , R, AB, Γi be an S-domain and I a CPMM of D. If there are
t ∈ T and l ∈ F ∗ such that [t]F Al ∈ Lr(∆IQF ) then I |= [t, t+ ]da¬l .
Proof. Let δ denote the assumption [t]F A l . First we observe that δ ∈ Lr(∆IQF ) implies
δ 6∈ ∆IQF since δ is rejected by ∆IQF and ∆IQF does not attack itself. This in turn implies
that I |= {[t]l, ¬[t+ ]l} as the denotation of F A in I is required to be maximal since I is a
CPMM of D. From the condition that I is coherent, either (i) there are α ∈ A 0 and s ∈ T
such that
Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈R
r=
¬[τ2 ]l ∧ ¬[τ1 ]F Al
and I |= prem(r)[τ1 /s, τ2 /t+ ], or (ii) I |= [t, t+ ]da¬l . Condition (i) guarantees that δ is
attacked by ∆IQF and thus δ can not be a member of Lr(∆IQF ). Therefore, (ii) must be the
case.
2
The converse of Lemma 2 does not hold. There are cases in which I |= [t, t + ]da¬l and
the assumption [t]F Al is attacked by ∆IQF as a basic action occurs that changes the fluent
literal l.
Theorem 1 Let D = hLD , R, AB, Γi be an S-domain. If I is a CPMM of D then ∆ IQF is
plausible.
We now prove that not only can we derive a plausible set of assumptions from a given
CPMM but we can also construct CPMMs from a plausible set of assumptions of a given
S-domain.
The set of ∆-relativised models of an S-domain D is denoted as M od S∆ (D).
Observation 4 Let D be an S-domain and ∆ a set of assumptions of D. For each I ∈
M odS∆ (D), ∆ = ∆IQF .
Proof. From the construction of ∆-relativised models:
For each I ∈ M odS∆ (D), δ = [τ ]F Al ∈ ∆ iff I |= [τ ]F Al iff δ ∈ ∆IF A . (More precisely,
we have the assumption [φ(τ )]F Al is in ∆IF A , where φ is the valuation defined in I. But
relative to I, it is identical to δ.)
2
Therefore, ∆ = ∆IQF .
491

Vo & Foo

Theorem 2 Let D = hLD , R, AB, Γi be an S-domain and ∆ ⊆ AB. ∆ is plausible wrt D
iff M odS∆ (D) 6= ∅ and for each I ∈ M odS∆ (D), I is a CPMM of D.
Theorem 3 Let D = hLD , R, AB, Γi be an S-domain. Furthermore, suppose that CP M M (D)
is the set of CPMMsSof D and P laus(D) is the set of plausible sets of assumptions of D,
then CP M M (D) = ∆∈P laus(D) M odS∆ (D).
5.1.1 Discussion:
So, how does our account of the frame problem relate to the existing approaches to the frame
problem? While there has been a long line of development behind monotonic approaches
to the frame problem starting with Haas’s (1987) and Schubert’s (1990) early attempts and
resulting in Reiter’s (1991) monotonic solution to the frame problem together with other
solutions proposed by others such as Castilho et al. (1999) and Zhang and Foo (2002) or
Thielscher’s (1999) Fluent Calculus-based monotonic solution to the frame problem, with
the notable exception of Thielscher’s (2001) attempt to address the qualification problem,
few have tried to tackle the qualification within the framework they use to address the frame
problem.
On the other hand, in the action languages (Gelfond & Lifschitz, 1998) and related
approaches such as those proposed by McCain and Turner (1995, 1997), Giunchiglia, Kartha,
and Lifschitz (1997), and Giunchiglia and Lifschitz (1998), state transition systems are
employed as the underlying computation machinery which essentially provides the reasoner
with all possible complete states of the world. Furthermore, as domain decsriptions can
be uniquely translated to state transition systems, the reasoner could safely derive the
successor state(s) based on the current together with the transition function.
5.2 Solving the Qualification Problem (in the Presence of the Frame Problem)
The results reported in the previous section are established in a simple setting. If we add
the following observation to the theory in example 4: [3]alive, i.e. after the shoot action,
the victim is still alive, then like most existing formalisms, the above account of plausibility
would come up with a contradiction. In fact, it would be more reasonable that such a
failure is explained as an occurrence of some qualification. In this section, we remove certain
restrictions on the qualifications of actions in order to achieve a more general framework.
There are some subtleties in the way action theories are represented in our proposed
assumption-based framework. Note first that there is a potential difficulty if frame assumptions and qualification assumptions are treated equally, which can be illustrated by a version
of the YSP. Consider the following action description:
{

[τ ]alive, [τ ]F Aalive [τ ]loaded, [τ, υ]shoot, [τ, υ]AQ¬alive
,
} ⊆ R,
[τ + ]alive
¬[υ]alive ∧ ¬[τ ]F Aalive

{[0]loaded, [0]alive, [0, 1]shoot} ⊆ Γ.
From this, we have (at least) two stable sets of assumptions: one contains the frame
assumption [0]F Aalive which rejects the qualification assumption [0, 1]AQ ¬alive and another
contains [0, 1]AQ¬alive which attacks [0]F Aalive . Only the latter is intuitive in this case but
we do not have any explicit criterion to prefer one over another. The following assumption
492

Reasoning about Action: An Argumentation-Theoretic Approach

asserts that in our solution to the frame problem in the presence of the qualification problem,
an action is presumed to bring about its effects unless there is an explicit justification for
its disqualification.
Assumption 2 When there is a direct conflict between a frame assumption and a qualification assumption (over a fluent literal), the qualification assumption takes precedence.
Given the presence of several kinds of assumptions, i.e. frame and qualification, we
will adopt the following convention: we will write Lr P (∆) instead of (Lr(∆))P for P ∈
{F A, AQ}. Since we no longer exclude qualification assumptions from our assumptionbased domain descriptions, we will simply refer to assumption-based domain descriptions
as Q-domains.
Definition 5.9 Let D = hLD , R, AB, Γi be a Q-domain. A presumable set of assumptions
∆ ⊆ AB is semi-Q-plausible wrt D iff Lr F A (∆) is minimal (with respect to set inclusion).
Definition 5.10 Let D = hLD , R, AB, Γi be a Q-domain. A set of assumptions ∆ ⊆ AB
is Q-plausible wrt D iff
1. ∆ is semi-Q-plausible wrt D,
2. ∆AQ is maximal, i.e. there does not exist any ∆ 0 ⊆ AB such that ∆0 is semi-Qplausible (wrt D) and ∆AQ ⊂ ∆0AQ , and
3. ∆F A is maximal relative to the above two conditions, i.e. there does not exist any
∆0 ⊆ AB such that ∆0 satisfies the above two conditions and ∆ F A ⊂ ∆0F A .
We will now refer to models of a Q-domain as Q-models. A coherent Q-model is a Qmodel which is coherent. We minimise the set of action occurrences in coherent Q-models
of a given action theory.
Definition 5.11 Let D be a Q-domain. A coherent Q-model I of D is a prioritised minimal
Q-model (or simply PMQM) of D iff there does not exist any coherent Q-model I 0 of D
0
such that OccI ⊂ OccI .
Definition 5.12 Let D be an S-domain. A PMQM I of D is a canonical prioritised minimal
Q-model (or simply CPMQM) of D iff
0

1. there does not exist any PMQM I 0 of D such that AQI ⊂ AQI , and
0

2. there does not exist any PMM I 0 of D such that F AI ⊂ F AI .
Now we can proceed to obtaining the main results for CPMQMs regarding Q-plausible
sets of assumptions which are similar to those for CPMMs regarding plausible sets of assumptions. The following lemma, which is a straightforward extension of Lemma 1 and
Lemma 2 proved in the previous section, is introduced to assist in the proof of Theorem 4 .
Lemma 3 Let D = hLD , R, AB, Γi be an Q-domain and I a CPMM of D,
493

Vo & Foo

1. For each assumption δ ∈ AB: δ ∈
/ ∆IQF iff δ is rejected by ∆IQF .
2. If δ = [τ ]F Al ∈ Lr(∆IQF ) then I |= [τ, τ + ]da¬l .
Theorem 4 Let D be a Q-domain. If I is a CPMQM of D then ∆ IQF is Q-plausible wrt
D.
Similar to the previous section, we now prove that not only can we derive a plausible set
of assumptions from a given CPMQM but we can also construct CPMQMs from a plausible
set of assumptions of a given domain description. The set of ∆-relativised models of a
Q-domain D is denoted as M odQ
∆ (D).
The following observation is obvious:
Observation 5 Let D be a Q-domain and ∆ a set of assumptions of D. For each I ∈
I
M odQ
∆ (D), ∆ = ∆QF .
Theorem 5 Let D = hLD , R, AB, Γi be a Q-domain and ∆ ⊆ AB. ∆ is Q-plausible wrt D
Q
iff M odQ
∆ (D) 6= ∅ and for each I ∈ M od∆ (D), I is a CPMQM of D.
Theorem 6 Let D be a Q-domain. Furthermore, suppose that CP M QM (D) is the set of
Q
CPMQMs of D and
S P laus (D) is Qthe set of Q-plausible sets of assumptions of D, then
CP M QM (D) = ∆∈P lausQ (D) M od∆ (D).
Q-plausible sets of assumptions allow one to overcome scenarios in which expectation
failures (or, qualification surprises) arise, e.g. shooting a turkey with a loaded gun and
observing that the turkey is still alive. When such surprises arise, the reasoner knows
who’s to blame: qualification assumptions. She can then accordingly remove the “guilty”
assumptions. Just as the anomalous models forming obstacle to early approaches to the
frame problem, similar anomalous models can also arise in solutions to the qualification
problem. This important issue related to the qualification problem has been thoroughly
discussed by Thielscher (2001) and a solution was presented within the framework of the
Fluent Calculus. To give the reader a flavour of this problem within our framework, we
invite the reader to consider the following classical example:
Example 5 Consider the problem of starting a car whose tail pipe could possibly be
blocked by a potato, formalised in our formalism as follows.
1. The set of inference rules R is:13
[τ ]BlockedT P
,
¬[τ, υ]AQGetStarted
[τ ]HasP otato, [τ, υ]InsertP otato, [τ, υ]AQ BlockedT P
,
[υ]BlockedT P ∧ ¬[τ ]F A¬BlockedT P
[τ ]HasKey, [τ, υ]T urnOnIgnition, [τ, υ]AQ GetStarted
.
[υ]GetStarted ∧ ¬[τ ]F A¬GetStarted
13. Of course we also have the frame-based inference rules for HasKey, BlockedT P , etc. but we omit them
from this representation for sake of readability.

494

Reasoning about Action: An Argumentation-Theoretic Approach

2. The theory Γ is: {[0]HasP otato, [0]HasKey, ¬[0]BlockedT P, ¬[0]GetStarted,
[0, 1]InsertP otato, [1, 2]T urnOnIgnition}.
Of course, we have designed this example to try to avoid any possible troubles with
the frame problem. We have to consider two conflicting qualification assumptions in this
case which are [0, 1]AQBlockedT P and [1, 2]AQGetStarted . Given the above action theory, any
Q-plausible set of assumptions would contain exactly one of them. Thus, there will be at
least two extensions, one in which the car can not get started since the tailpipe is blocked
and it’s no longer consistent to assume [1, 2]AQ GetStarted . The other extension disqualifies
the action of inserting the potato into the tailpipe and thus the action of starting the car
becomes successful. Only the former is intuitive in this case and the account of Q-plausible
sets of assumptions fails to deliver this desired solution.
However, in exactly the same way the problem of anomalous models arising in the solution to the frame problem is tackled, the above problem is easily addressed within our framework. The key insight is of course also underlined by the notion of causality: [1]BlockT P
was caused (by the action [0, 1]InsertP otato) which in turn allows ¬[1, 2]AQ GetStarted to be
derived. On the other hand, there was no cause that allows ¬[0, 1]AQ BlockT P to be derived.
How is the above insight realised in our framework? The answer turns out to be rather simple: Just as the distinction between leniently rejected frame assumptions and non-leniently
rejected frame assumptions allows us to distinguish between normal changes (i.e. actiontriggered) and anomalous changes, a distinction between leniently rejected qualification assumptions and non-leniently rejected qualification assumptions will allow us to distinguish
between normal disqualifications (i.e. those underlined by a cause) and anomalous disqualifications. Thus, facing a collection of Q-plausible sets of assumptions, a reasoner simply
selects the set of assumptions that contains the smallest (with respect to set inclusion) set
of leniently rejected qualification assumptions.
The ability to introduce different argumentation-theoretic semantics for assumptionbased frameworks is arguably the biggest advantage of our approach. The following example
illustrates this critical point:
Example 6 We modify an example presented by Lin and Shoham (1995) which is in turn
a modification of Kautz’s (1986) Stolen Car Problem. A spy possessed a microfilm of a top
secret evidence which an organisation, A, tried to steal. For some reason, another organisation, B, wanted to murder this spy. The microfilm was in the safe at the spy’s home at
time 0. The spy was not at home at time 0. A tried to steal the evidence between the
time points 0 and 1. The spy might return home at any time between 0 and 1. B tried to
murder the spy at any time between 0 and 1. return cancels the effects of steal, murder
cancels the effects of return and steal cancels the effects of murder. Any of the three
actions steal, return, and murder takes only one time step. The domain is formalised as
follows: Γ = {¬[0]EvStolen, [0]Alive, ¬[0]AtHome}; and R contains
{

[τ, υ]return, [τ, υ]AQAtHome [τ, υ]murder, [τ, υ]AQ¬Alive
[τ, υ]steal, [τ, υ]AQEvStolen
,
,
,
[υ]EvStolen ∧ ¬[τ ]F A¬EvStolen [υ]AtHome ∧ ¬[τ ]F A¬AtHome ¬[υ]Alive ∧ ¬[τ ]F AAlive

[τ ]AtHome ∧ τ < υ ¬[τ ]Alive ∧ τ < υ [τ ]EvStolen ∧ τ < υ
,
,
}.
¬[τ, υ]AQEvStolen
¬[τ, υ]AQAtHome
¬[τ, υ]AQ¬Alive
495

Vo & Foo

Given the above formalisation of the problem, traditional accounts of non-monotonic
reasoning (i.e. Default Logic, circumscription, Autoepistemic Logic, etc.) can not provide
one with a solution since these formalisms can not produce any extension under their standard semantics. However, the argumentation-theoretic approach does gives several semantics for this problem including preferability semantics. Note, however, that all admissible
sets of assumptions and preferred sets of assumptions for this domain contain none of the
assumptions: [0, 1]AQEvStolen , [0, 1]AQAtHome , and [0, 1]AQ¬Alive . This essentially means
that, under the admissibility and the preferability semantics, the reasoner could only infer
that none of the above actions would succeed On the other hand, our proposed plausible
semantics gives an alternative solution for this problem in which each consistent set of assumptions can contain at most one of the following three assumptions: [0, 1]AQ EvStolen ,
[0, 1]AQAtHome , and [0, 1]AQ¬Alive . Moreover, each plausible set of assumptions must contain exactly one of them. Among the other two assumptions which do not belong to a
plausible set of assumptions, one is attacked and the other is leniently rejected.
We believe that the above examples have underlined the major advantages of our approach in which a reasoner is aware of the (defeasible) assumptions used in her reasoning as
well as being able to explicitly reason about these assumptions. The flexibility of allowing
a reasoner to introduce different argumentation-theoretic semantics for assumption-based
framework by simply varying the notion of acceptability of sets of assumptions is certainly
another advantage in favour of our approach.

6. More Complex Dynamic Domains and Indirect Effects
So far we haven’t taken into consideration the issues of concurrent actions and indirect
effects. To ensure that the formalisation introduced is expressive enough to deal with
complex domains, we will show how these issues are coped with by our approach. Firstly,
we motivate our formalisation with an informal discussion.
6.1 Concurrent and Non-Deterministic Events
Given our temporal representation, formulating concurrent events is not a difficult issue in
our framework. However, there are some subtleties which need to be carefully considered.
Firstly, the use of assumptions. As presented earlier in this paper, qualification assumptions
are fluent oriented, i.e. we qualify over the effects of action rather than over the action itself.
Whilst this manifests the capability of formulating actions with multiple effects, and thus
each effect should be qualified independently, it may fail to formalise concurrent events
with the same effects. For example, both actions hit the vase with a hammer and shoot
it with a loaded gun bring about the effect that the vase is broken. In other words, it
is essential that qualification assumptions be dependent on the actions that bring about
the effect under consideration. Thus, given n actions and m fluents whose values can
be changed by those actions, we have to potentially introduce 2 × m × n qualifications
assumptions.14 Therefore, instead of subscripting the assumption symbols AQ with the
fluent literals from F ∗ , we extend the set of subscripts of AQ, denoted as AF, to contain
14. In fact, as we will see later, there are potentially 2 × m × (n + 1) qualification assumptions in this case
since there is one special event corresponding to all (natural) events that bring about the indirect effects.

496

Reasoning about Action: An Argumentation-Theoretic Approach

both the action and the corresponding fluent literals. 15 For example, given the above two
actions hit and shoot and an additional action repair whose effect is change a broken vase
to being non-broken, viz. ¬broken, we will need to introduce the following qualification
assumptions: AQHit-Broken , AQShoot-Broken and AQRepair-¬Broken . Therefore, syntactically
we also extend the set AB AQ = {[τ, υ]AQϕ | τ, υ ∈ T E and ϕ ∈ AF} and semantically
the function εq : T × AQAF × T → {true, false}. In the definition of an interpretation
I = hh, φ, η, εq , εf i, we also have I([τ, υ]AQϕ , t) = εq (φ(τ ), AQϕ , φ(υ)), where τ, υ ∈ T E
and ϕ ∈ AF.
Though this does increase the complexity of the framework, it is the price we have to
pay for the expressiveness of the resulting system. To the best of our knowledge, none of
the existing formalisms possesses such an expressiveness.
How about non-deterministic actions? The solution turns out to be quite simple: we can
treat action non-determinism as a special kind of action qualification. For example, to formulate the Russian shooting scenario (Sandewall, 1994) in which a gun non-deterministically
gets loaded or not after spinning its revolver, the following action descriptions can be asserted:
[τ, υ]spin, [τ, υ]AQSpin-Loaded
[τ, υ]spin, [τ, υ]AQSpin-¬Loaded
adr1 =
and adr2 =
[υ]loaded ∧ ¬[τ ]F A¬loaded
¬[υ]loaded ∧ ¬[τ ]F Aloaded
together with the following two qualification rules:
qr1 =

[υ]loaded
¬[υ]loaded
and qr2 =
.
¬[τ, υ]AQSpin-¬Loaded
¬[τ, υ]AQSpin-Loaded

The above guarantee that either ¬[υ]loaded or [υ]loaded will follow from [τ, υ]spin (remark that the set of qualification assumptions ∆ AQ is maximised), but not both. As a
consequence, two possible extensions will arise given the above non-deterministic action.
Dealing with non-determinism could be more complicated when the information used to
encode the action description is disjunctive. As we only restrict the conclusion of an action
description rule to contain only fluent literals, such disjunctive effect is not straightforwardly
dealt with in our framework. However, note that the restriction is similar to that imposed
on action language A (Gelfond & Lifschitz, 1998). An extension to action descriptions
which is similar to the way the action language A is extended into the action language
C can be done to allow a complex expression in the conclusion of action description rule.
However, for the sake of presentation, we choose to use a simpler language to introduce our
framework to the reader.
6.2 Formalisation
From the analysis in the introductory section regarding indirect effects, we observe that
the ordering in which domain constraints are applied plays an essential role in a technically
sound framework. Moreover, it’s no longer guaranteed that domain constraints would be
strictly satisfied at every time point. 16 To help the reader better understand this technical
subtlety, it’s useful to think that changes are attributable to events. However, events
15. The reader is referred to section 3 for the general formalisation.
16. Of course, it’s not necessary that the state of the world at every time point is observable to a reasoner.
However, it is important that she be aware of such states and able to reason about them.

497

Vo & Foo

are further divided into two categories: external and internal. Basically, external events
correspond to the direct effects of actions performed by some agents (including the reasoner.)
On the other hand, internal events correspond to the indirect effects when certain conditions
about the world are met and can be attributed to Nature. 17
Like external events, internal events also happen in a certain order. Although it is not
straightforward to observe this order 18 , it is important that a reasoner be able to reason
about them. Hence we propose to add one more dimension into the set of assumptions
of a given domain description. Since these assumptions play essentially the same role as
that of qualification assumptions, 19 we can subsume them under the set of qualification
assumptions AQAF . As they are not associated with any specific action, we can ignore the
initial of the action name from the subscripts of AQ. For example we will write AQ broken
in case we want to qualify over the fluent broken as an indirect effect, i.e. independently of
any action. Moreover, to avoid the axiomatiser from the confusion of determining the time
span taken by indirect effects, we will assume that it is atomic. It means all indirect effects
always take place between one time point and the next one. This does not mean that all
indirect effects have the same real-time duration, it simply means that relative to the given
time structure T they take place between two consecutive time points. This allows us to
avoid the granularity problem as it is irrelevant from the viewpoint of the problems we are
trying to solve.
6.2.1 A Representation Issue and Some Notations:
One important remark is in place here. Until now, we have used integers and the corresponding expressions to denote time points. There are two reasons for this pratice: (i) It
significantly simplifies the presentation, and (ii) Without the complication of the ramification problem, all time points which are reasoned about are also observable to the reasoner.
However, as will be discussed thoroughly in the next section, in the presence of ramifications
a time point at which some change takes place maight not be observable to the reasoner as
such a change could be one of the indirect effects after the execution of some action. Such
representation issues make the integer-based time structure employed so far in this paper
inappropriate. We do need a richer representation of time structure. Following Sandewall’s
(1994) basic formulation, the only basic constant to be included in the representation of
the time structure T is denoted by the symbol Θ and is referred to as the origin of T.
The standard algebraic operations + and − are still employed to obtain time expressions
and bear their usual meanings. The relation symbol < will also be used to compare time
expressions. However, we can no longer allow expressions such as Θ + 1 for integers are no
longer elements of the time structure.
Given a time expression τ :
17. We avoid the use of the terms exogenous/endogenous events to label these two categories because
throughout the literature of reasoning about action, exogenous events are used to refer to actions that
carried out by agents that are different from the reasoner or outside events whose occurrences are beyond
the reasoner’s control.
18. Unless indirect effects are somehow delayed and become observable to the reasoner, they usually take
place immediately after direct effects.
19. The only difference is that these assumptions are qualified over the indirect effects which can be considered
to be the effects of the actions of Nature.

498

Reasoning about Action: An Argumentation-Theoretic Approach

• We will continue to denote the next time point of τ by τ + .
• We define τ 1+ to be τ + and, let n > 1, τ n+ to be (τ (n−1)+ )+ .
Furthermore, as we will assume throughout that an indirect effect takes place between
two consecutive time points, we also simplify the presentation by using the following syntactical sugar: instead of writing [τ, τ + ]AQl (for some τ ∈ T E and l ∈ F ∗ ), we will write
[τ ]AQl . This also provides a simple way to distinguish between qualification assumptions
for ramificational effects and those caused directly by an action or event.
How are domain descriptions affected from this augmentation? The only change is for
the set RA of action descriptions which consists of rules either of the form
Φ, [τ, υ]α, [τ, υ]AQα- l
[υ]l ∧ ¬[τ ]F A¬l
or of the form

Φ, [τ ]AQl
.
∧ ¬[τ ]F A¬l

[τ + ]l

For convenience, we will refer to the set of inference rules having the latter form as R I ,
Φ, [τ ]AQl
for some fluent literal l ∈ F ∗ }. Now, regarding
i.e. RI = {r ∈ RA | r = +
[τ ]l ∧ ¬[τ ]F A¬l
the ramification problem, the basic idea is that the state obtained by updating the previous
state may not necessarily be stable due to the presence of indirect effects. By representing
indirect effects as causal rules (using inference rules in our framework) we can reason about
which causal rules have fired and (relatively) when. Moreover, since these causal rules
may not take the same amount of time to fire, we should be able to reason about different
possible orders in which they fire. This will be achieved by a distinction between stable and
unstable states.
Definition 6.1 A time-point expression θ ∈ T E is stable wrt a given domain description
D = hLD , R, AB, Γi and ∆ ⊆ AB iff there does not exist any fluent literal l ∈ F ∗ such that
1.

Φ, [τ ]AQl
∈ RI and Γ ∪ ∆ `R Φ, [θ]AQl , and
[τ + ]l ∧ ¬[τ ]F A¬l

2. Γ ∪ ∆ 6`R [θ]l.
Definition 6.2 Let D = hLD , R, AB, Γi be a domain description. A set of assumptions ∆
is said to be ramification-compliant iff
1. there does not exist any δ ∈ AB AQ such that T hR (∆ ∪ {δ}) 6= T hR (∆) ∪ {δ}.
2. there does not exist any unstable time-point expression τ ∈ T E such that [τ ]F A l ∈ ∆
for every l ∈ F ∗ .
We note that by using unstable time points, we have conceptually isolated the ramification problem from the task of reasoning about the (explicit) actions.
499

Vo & Foo

Definition 6.3 Let D = hLD , R, AB, Γi be a domain description. A set of assumptions ∆
is generally plausible for action domains, or simply AD-plausible, iff
• ∆ is ramification-compliant, and
• ∆ is Q-plausible relative to the above condition.
Remarks:
1. From the above definition, it can be seen that a solution for the ramification is (in
a sense) independent from the frame and the qualification problems regarding action
occurrences.
2. Given the above temporal ontology, it is worth noting that the traditional notion
of state constraints may no longer hold in this representation regarding time points.
More precisely, only states associated with stable time points are subject to these
constraints.
6.3 Connection to Causation-Based Formalisms
The question is, of course, whether the above computational mechanism gives satisfactory
conclusions for problems in reasoning about action. While many formalisms have been
proposed, a general criterion for reasoning about action formalisms still seems to be missing.
The major stream of research towards a solution to the ramification problem is based on the
notion of causality (e.g., Lin, 1995; McCain & Turner, 1995; Thielscher, 1997). As has been
discussed above, none of these approaches can deal with domains in which (potentially)
infinite sequences of indirect effects are present. Most of these approaches (including the
above three references), however, produce a successor state after the execution of an action
to capture changes that have taken place either as direct or as indirect effects of that action.
In this sub-section, we show how our formalism captures the notion of successor states in
the absence of the infinite sequences of indirect effects.
Definition 6.4 A domain description D = hL D , R, AB, Γi is non-stratified iff there exist
two sets Φ ⊆ Λ and {l1 , . . . , ln } ⊆ F ∗ such that
1. for each 1 ≤ i ≤ n, Φ ∪ {[τ ]li } is R-consistent, for every τ ∈ T E; and
2. for each 1 ≤ i ≤ n, if k = (i mod n) + 1, then there exists a set Ψ k ⊆ Λ such that
Φ ∪ {[τ ]li } `R Ψk , and
Ψk , [τ ]AQlk
∈ RI .
[τ + ]lk ∧ ¬[τ ]F A¬lk
Of course, a domain description is stratified if and only if it is not non-stratified. Given a
def

domain description D = hLD , R, AB, Γi, we’ll denote ED (∆) = T hR (Γ ∪ ∆), the extension
of an action theory D according to ∆. We will also write E D instead of ED (∅) for brevity.
Definition 6.5 Let σ = hT , F, Ai be a signature.
500

Reasoning about Action: An Argumentation-Theoretic Approach

1. A set S ⊆ F ∗ is an instantwise state iff for every l ∈ F ∗ , either l ∈ S or ¬l ∈ S. IS
will be used to denote the set of instantwise states of Λ,
def

def

2. Let Γ ⊆ F ∗ and τ ∈ T E , we denote Γ = {¬l | l ∈ Γ} and [τ ]Γ = {[τ ]l | l ∈ Γ},
3. Let D = hLD , R, AB, Γi be a domain description. D is simplistic iff Γ = ∅.
The motivation behind the introduction of instantwise states is to allow the reasoner
to reason about the intermediate states which may not be practically observable to her.
That is, when indirect effects (following an occurrence of an action or event) take place,
the world might transit through a number of intermediate states before becoming stabilised in a final state in which all domain constraints necessarily hold. The ability to
explicitly reason about such (unstable) intermediate states is one of the advantages offered by our approach. For instance, let’s consider the scenario in Example 1 which
was originally introduced by Thielscher (1997). Thielscher’s (1997) approach allows two
possible successor states as the results of closing the switch sw 1 in the initial state S =
{¬sw1 , sw2 , sw3 , ¬relay, ¬light, ¬detect}. These are T 1 = {sw1 , ¬sw2 , sw3 , relay, ¬light, ¬detect}
and T2 = {sw1 , ¬sw2 , sw3 , relay, ¬light, detect}. However, to an outside observer who has
no idea about the tricky internal mechanism of this circuit, it could be quite difficult to
explain why T2 should be one of the possible outcomes of the action of closing the switch
sw1 : starting from an initial state in which detect and light are both off, after performing
an action toggle(sw1 ), light remains off but somehow detect becomes on. In other words,
Thielscher’s (1997) framework fails to render the intermediate (and unstable) state in which
light was on, albeit for only a short instant, before it was turned off again. On the other
hand, such states are explicitly represented and reasoned about as instantwise states in our
framework. As a consequence, domain constraints do not necessarily hold in an instantwise
state.
In the following, we abbreviate simplistic and stratified domain descriptions as SSDs.
Let Ω ⊆ RI , we denote:
def

CON SR (Ω) = {l ∈ F ∗ |

[τ ]Ψ, [τ ]AQl
∈ Ω}.
[τ + ]l ∧ ¬[τ ]F A¬l

Similarly, let Ω ⊆ RA , we denote
def

CON SA (Ω) = {l ∈ F ∗ |

[τ1 ]Ψ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQα-l
∈ Ω}.
[τ2 ]l ∧ ¬[τ1 ]F A¬l

[τ1 ]Ψ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQα-l
∈ RA is applicable in S iff Ψ ⊆ S.
[τ2 ]l ∧ ¬[τ1 ]F A¬l
We use ΥSα to denote the set of action descriptions that is applicable (in S) regarding the
action α. Ω ⊆ ΥSα is a possible application of α in S iff CON S A (Ω) is consistent and there
does not exist any Ω0 ⊆ ΥSα such that Ω ⊂ Ω0 and CON SA (Ω0 ) is consistent.
The action description

Definition 6.6 Let σ = hT , F, Ai be a signature and D = hL D , R, AB, ∅i a SSD. Suppose
S ∈ IS and α ∈ A. We formalise the direct effects of an action α using a relation Res D :
for all S, S 0 ∈ IS, (S, α, S 0 ) ∈ ResD iff there is a possible application Ω of α in S such that:
501

Vo & Foo

(i) CON SA (Ω) ⊆ S 0 , and
(ii) there does not exist any instantwise state S 00 such that S 00 satisfies (i) and S 00 \ S ⊂
S 0 \ S.
Definition 6.7 Let σ = hT , F, Ai be a signature and D = hL D , R, AB, ∅i a SSD. The
causation relation according to D, denoted as Causes D , is defined as follows: for all S, S 0 ∈
IS, CausesD (S, S 0 ) iff there exists a non-empty set Ω ⊆ R I of ramification inference rules
such that
(i) for each

[τ ]Ψ, [τ ]AQl
∈ Ω, Ψ ⊆ S and
∧ ¬[τ ]F A¬l

[τ + ]l

(ii) CON SR (Ω) ⊆ S 0 , and
(iii) there does not exist any instantwise state S 00 such that S 00 satisfies (i) and (ii) and
S 00 \ S ⊂ S 0 \ S.
Given a domain description D, a state S ∈ IS is stable regarding D iff there does not
exist any S 0 ∈ IS such that CausesD (S, S 0 ).
Definition 6.8 Let σ = hT , F, Ai be a signature and D a SSD. Suppose w ∈ IS and α ∈ A.
The state transition from w in D according to α, denoted T rans αD (w), is the transitive closure of CausesD regarding the state ω1 ∈ IS satisfying (w, α, ω1 ) ∈ ResD . Formally,
T ransαD (w) = {$ ∈ IS | there exists a sequence ω 1 , . . . , ωn ∈ IS such that (w, α, ω1 ) ∈
ResD and $ = ωn and (ωi , ωi+1 ) ∈ CausesD for each 1 ≤ i < n and $ is stable regarding D}.
Let I be an interpretation for LD and t ∈ T,
def

• we use [I]t to denote the instantwise state specified by I at time point t: [I] t = {l ∈
F ∗ | I(l, t) = true}. If [I]t is stable then t is said to be a stable time point in I.
• we use NI to denote a function that maps a time point t ∈ T to the next stable time
point in I: NI (t) ∈ T such that [I]NI (t) is stable and for every u ∈ T, if t < u < N I (t)
then [I]u is not stable.
Theorem 7 Let σ = hT , F, Ai be a signature and D 0 = hLD , R, AB, ∅i a SSD and α ∈ A0 .
Suppose that w ∈ IS. Define a domain description D = hL D , R, AB, Γi, where Γ = {[Θ]ϕ |
ϕ ∈ w}∪{[Θ, Θ+ ]α}. A set ∆ ⊆ AB of assumptions is AD-plausible wrt D iff for each model
M of ED (∆), [M ]NM (Θ) ∈ T ransαD ([M ]Θ ), i.e. [M ]NM (Θ) belongs to the state transition
from [M ]Θ in D according to α.
We now re-consider the motivating examples introduced in Section 1.3.
Example 1 (continued.) We re-formulate the action theory for this example in terms of
our formalism:

502

Reasoning about Action: An Argumentation-Theoretic Approach

[τ ]sw1 , [τ ]sw3 , [τ ]AQrelay
,
[τ + ]relay ∧ ¬[τ ]F A¬relay
¬[τ ]swi , [τ ]AQ¬relay
(i = 1, 3),
¬[τ + ]relay ∧ ¬[τ ]F Arelay
[τ ]light, [τ ]AQdetect
,
+
[τ ]detect ∧ ¬[τ ]F A¬detect

[τ ]sw1 , [τ ]sw2 , [τ ]AQlight
,
[τ + ]light ∧ ¬[τ ]F A¬light
¬[τ ]swi , [τ ]AQ¬light
(i = 1, 2),
¬[τ + ]light ∧ ¬[τ ]F Alight
[τ ]relay, [τ ]AQ¬sw2
,
¬[τ + ]sw2 ∧ ¬[τ ]F Asw2
¬[τ ]swi , [τ, υ]togglei
(i = 1, 2, 3),
[υ]swi ∧ ¬[τ ]F A¬swi
[τ ]ζ, [τ ]F Aζ
, where ζ ∈ F ∗ .
[τ + ]ζ

[τ ]swi , [τ, υ]togglei
(i = 1, 2, 3),
¬[υ]swi ∧ ¬[τ ]F Aswi

The theory is described as follows:
Γ = {¬[Θ]sw1 , [Θ]sw2 , [Θ]sw3 , ¬[Θ]relay, ¬[Θ]light, ¬[Θ]detect} ∪ {[Θ, N (Θ)]toggle 1 }.
Consider ∆ such that ∆AQ = ABAQ \{[Θ2+ ]AQdetect , [Θ3+ ]AQdetect } and ∆F A = ABF A \
+
2+
3+
¬light , [Θ ]F A¬relay , [Θ ]F Asw2 , [Θ ]F Alight }. ∆ is AD-plausible resulting in
the next stable state being ω = {sw1 , ¬sw2 , sw3 , relay, ¬light, ¬detect} at [Θ4+ ].

{[Θ+ ]F A

In addition, the set ∆0 ⊆ AB, where ∆0AQ = ABAQ \ {[Θ2+ ]AQdetect } and ∆0F A =
ABF A \ {[Θ+ ]F A¬light , [Θ+ ]F A¬relay , [Θ2+ ]F Asw2 , [Θ3+ ]F Alight , [Θ3+ ]F A¬detect }, is also
AD-plausible which results to the next stable state ω 0 = {sw1 , ¬sw2 , sw3 , relay, ¬light, detect}
at [Θ4+ ].
Moreover, the set ∆00 ⊆ AB, where ∆00AQ = ABAQ and ∆00F A = ABF A \{[N (Θ)]F A¬light ,
[Θ+ ]F A¬relay , [Θ2+ ]F Asw2 , [Θ2+ ]F A¬detect , [Θ3+ ]F Alight }, is also AD-plausible which also
results to the next stable state ω 0 at [Θ4+ ]. In other words, the model implied by ∆ 0 reflects
a domain in which it takes the same amount of time for the detect to be on and the switch
sw2 to jump off. On the other hand, the model implied by ∆ 00 reflects a domain in which
the amount of time for the detect to be on is (approximately) equal to the amount of time
for switch sw2 to jump off and cause the light to be off as well. The detect implied by ∆ is
so insensitive that even though the light and the relay are on at the same time (Θ 2+ ), the
relay causes switch sw2 to jump off and then leads to the light to be off as well but the
detect is yet on.

Example 2 (continued.) We re-formulate the action theory for this example in terms of
our formalism:

503

Vo & Foo

[τ ]upL1 , [τ ]upL2 , [τ ]AQopen
,
[τ + ]open ∧ ¬[τ ]F A¬open
[τ, υ]f lipi , ¬[τ ]upLi
(i = 1, 2),
[υ]upLi ∧ ¬[τ ]F A¬upLi
[τ1 , τ2 ]hold closed, ¬[τ1 ]open, τ1 ≤ τ ≤ τ2
,
[τ ]held closed ∧ ¬[τ ]F A¬held closed
[τ ]ζ, [τ ]F Aζ
, where ζ ∈ F ∗ .
[τ + ]ζ

[τ, υ]f lipi , [τ ]upLi
(i = 1, 2),
¬[υ]upLi ∧ ¬[τ ]F AupLi
[τ, υ]close, [τ ]upL1 , [τ ]upL2
,
¬[υ]open ∧ ¬[τ ]F Aopen
[τ ]held closed
,
¬[τ ]AQopen

The theory is described as follows:
Γ = {[Θ]upL1 , [Θ]upL2 , [Θ]open, [Θ]¬held closed}∪{[c1 , c2 ]close, [c2 , c3 ]hold closed, [c4 , c5 ]f lip1 }∪
{Θ ≤ c1 < c2 ≤ c4 < c5 ≤ c3 }.
Consider ∆ ⊆ AB such that ∆AQ = ABAQ \ {[c]AQopen | c2 ≤ c ≤ c3 } and ∆F A =
ABF A \ ({[c1 ]F Aopen } ∪ {[c]F A¬held closed | c2 ≤ c ≤ c3 } ∪ {[c4 ]F AupL1 }). ∆ is AD-plausible
and resulting in the following:
[c1 ]{upL1 , upL2 , open, ¬held closed},
[c2 ]{upL1 , upL2 , ¬open, held closed},
[c4 ]{upL1 , upL2 , ¬open, held closed},
[c5 ]{¬upL1 , upL2 , ¬open, held closed},
[c3 ]{¬upL1 , upL2 , ¬open, held closed}.
Example 3 (continued.) The domain description:
[τ ]sw1 , [τ ]sw2 , [τ ]AQrelay1
,
[τ + ]relay1 ∧ ¬[τ ]F A¬relay1
[τ ]relay1 , [τ ]AQ¬sw2
,
¬[τ + ]sw2 ∧ ¬[τ ]F Asw2
¬[τ ]swi , [τ, υ]togglei
(i = 1, 2),
[υ]swi ∧ ¬[τ ]F A¬swi
[τ ]ζ, [τ ]F Aζ
, where ζ ∈ F ∗ .
[τ + ]ζ

[τ ]sw1 , ¬[τ ]sw2 , [τ ]AQrelay2
,
[τ + ]relay2 ∧ ¬[τ ]F A¬relay2
[τ ]relay2 , [τ ]AQsw2
,
[τ + ]sw2 ∧ ¬[τ ]F A¬sw2
[τ ]swi , [τ, υ]togglei
(i = 1, 2),
¬[υ]swi ∧ ¬[τ ]F Aswi

The state of the circuit in Figure 2 is captured by the following action theory:
Γ = {¬[Θ]sw1 , [Θ]sw2 , ¬[Θ]relay1 , ¬[Θ]relay2 }∪{[c1 , c2 ]toggle1 , [c3 , c4 ]toggle1 }∪ {Θ ≤ c1 <
c2 < c 3 < c 4 }
Consider ∆ ⊆ AB such that ∆AQ = ABAQ and ∆F A = AB F A \ ({[c1 ]F A¬sw1 } ∪
{[c3 ]F Asw1 } ∪ {[c]F A¬relay1 | c2 ≤ c ≤ c3 and c = (c2 )k+ where k = 4i and i = 0, 1, 2, . . .}∪
{[c]F Asw2 , [c]F Arelay1 | c2 ≤ c ≤ c3 and c = (c2 )k+ where k = 4i + 1 and i = 0, 1, 2, . . .}∪
{[c]F A¬relay2 | c2 ≤ c ≤ c3 and c = (c2 )k+ where k = 4i + 2 and i = 0, 1, 2, . . .}∪
{[c]F A¬sw2 , [c]F Arelay2 | c2 ≤ c ≤ c3 and c = (c2 )k+ where k = 4i + 3 and i = 0, 1, 2, . . .}).
∆ is AD-plausible resulting to several possible models for this domain depending on when
switch sw1 is toggled off:
[c1 ]{¬sw1 , sw2 , ¬relay1 , ¬relay2 },
504

Reasoning about Action: An Argumentation-Theoretic Approach

[c2 ]{sw1 , sw2 , ¬relay1 , ¬relay2 },
[c3 ]{sw1 , ±sw2 , ∓relay1 , ±relay2 },
[c4 ]{¬sw1 , ±sw2 , ∓relay1 , ±relay2 }.
Remark:20
Here, it is also important to raise the question whether our solution to the ramification
problem conflicts with the formulation of concurrent actions. When two actions α 1 and α2
are concurrently performed: α1 causes an indirect effect E and α2 triggers a non-terminating
chains of effects. If the direct and indirect effects caused by α 2 don’t interfere with α1 ’s
production of E then the reasoner can reason about time points after the execution of α 1
where E holds for being an indirect effect of α 1 . These time points are of course unstable
due to the non-terminating chains of indirect effects caused by α 2 . On the other hand,
if the direct and indirect effects caused by α 2 have the potential to prevent the former
from producing E, the reasoning could be more complex. If it is observable that the direct
effects of both actions take place at the same time then the concurrent occurrences of the two
actions can be viewed as one single occurrence of one complex actions whose direct effects are
a combination of the direct effects of the two actions. Further reasoning about ramifications
will then be carried out as usual. On the other hand, when the reasoner herself is unsure
about the temporal correlation between the direct effects of the two actions, every possible
order of changes must be taken into consideration leading to a highly nondeterministic
outcome about what fluent will hold in the future time points. However, such a situation
can still be handled nicely by our formalisation. The only representation issue which is
not addressed by our framework is about indirect effects with duration. As asserted above,
given the state of the world at a time point τ , we assume that all subsequent indirect
effects will take place at the next time point τ + . This certainly fails to deal properly with
indirect effects whose durations are different. A simple solution to this problem can be to
associate one next time point operator for each possible indirect effect. This, however, will
significantly complicate the representation.

7. Discussion and Future Work
We developed a uniform framework for reasoning about action using an argumentationtheoretic approach (more precisely, assumption-based approach). We have also presented
how our framework copes with the frame, the qualification and the ramification problems in
several sophisticated settings. We have shown how our framework can be naturally extended
to become more expressive.
We also explored a new abstraction level which we believe to be an intermediate layer
between the common sense knowledge and the scientific knowledge. Sophisticated domain
knowledge as well as representation are, we argue, required to achieve an adequate underlying representation and reasoning process. Among the merits of this approach, we emphasise
the following:
• Non-monotonicity is handled by assumptions and argumentation-theoretic approach.
20. The authors would like to acknowledge an anonymous referee for pointing out this very subtle issue.

505

Vo & Foo

• The flexibility of working with different kinds of information representation since there
is no restriction on the syntax of the system.
• Expressivity: temporal information is explicitly represented. Thus the system is capable of capturing many important features of temporal reasoning.
As reviewed in the introductory section of this paper, numerous approaches to reasoning
about action have been proposed. As such, much research related to these approaches and
frameworks has evolved around the central problems addressed in this paper, namely the
frame, qualification and the ramification problems. However, as non-monotonic reasoningbased formalisms for reasoning about actions were shown to be flawed by Hanks and McDermott (1987), many existing solutions of the frame problem are based on other approaches
which are usually monotonic. Since one of the inherent properties of the argumentationtheoretic approach is monotonicity, few have attempted to solve the problems of reasoning
about actions using this approach.
A framework for reasoning about actions under the argumentation-theoretic approach is
independently proposed by Kakas, Miller, and Toni (1999, 2000, 2001). In their approach,
the admissibility semantics is also employed to resolve conflicts between adversary arguments. To represent common sense knowledge, e.g. the common sense law of inertia, an
order is imposed on the set of inference rules of the assumption-based framework. The computation of the arguments and their competing is performed on top of the so-called proof
trees. Nodes on the proof trees are arguments which are sets of propositions. 21 Construction of proof trees is on the same level of hardness as known frameworks for argumentationtheoretic computation. While their framework allows the persistence of (inertial) fluents to
be captured and dealt with, it is not very clear whether their formalisation can be extended
to deal with other problems of reasoning about actions such as the qualification and the
ramification problems.
More recently, Dimopoulos, Kakas, and Michael (2004), and Bracciali and Kakas (2004)
extended Kakas et al.’s framework to deal with the ramification and the qualification problems. Essentially, in their solution to the ramification problem, the so-called r-propositions
of the following form:22
L whenever C
are added to the domain description. A resulting model for a given domain description will
then be computed by first computing all possible indirect effects as the fixed point of repeated application of r-propositions, and then completing the model by allowing unaffected
fluent literals to persist over time. It should be clear that the above representation of indirect effects is quite restrictive as it doesn’t allow more complex expressions for the conditions
of an indirect effects and the indirect effects themselves. These restriction is essentially due
to their use of Answer Set Programming (ASP) as the underlying computation mechanism
of their framework. On the other hand, their solution to the qualification problem lies in the
use of default rules for representing the effects of actions. This approach is fairly similar to
21. In Kakas et al.’s (1999, 2000) terminologies, each node is a set of arguments which is equivalent to the
notion of arguments in our exposition.
22. L is a fluent literal and C is a set of fluent literals which is essentially equivalent to a conjunction of
fluent literals.

506

Reasoning about Action: An Argumentation-Theoretic Approach

Thielscher’s (2001) solution to the qualification problem discussed above. As their approach
is essentially based on stability semantics, it’s not clear how the argumentation-theoretic
approach will bring about any benefit to their framework.
As discussed earlier in the paper, the key insight behind the solutions implemented in
our framework is the exploitation of causality to drive the inference. It is causality that has
helped throughout in our solutions to the frame and the qualification problems to provide
the mechanisms to eliminate the anomalous models while retaining the intuitive models
during the process of reasoning. This insight is certainly a more general version of the conclusions derived by Sandewall (1994) when investigating the reason behind the production
of anomalous models in early approaches to the frame problem, namely the failure to distinguish between normal changes which are triggered by actions and abnomalous changes.
Sandewall’s insight has certainly originated the occlusion-based solution to the frame problem (Sandewall, 1994; Doherty, 1994; Gustafsson & Doherty, 1996). Roughly speaking, in
this approach each action type is associated with a subset of fluents that are influenced
by the action. Also, a predicate Occlude is introduced to allow this subset of fluents to
be specified. When reasoning about dynamic domains, changes are minimised with the
exception of the fluents specified by Occlude. In other words, Occlude distinguishes the
normal changes (associated with the action types) from the anomalous changes and thus
avoids unintended models from arising.
Regarding the qualification problem, Thielscher’s (2001) solution to the problem of
anomalous models in relation to the qualification problem shares the key insight of causality
with our approach. However, Thielscher’s framework is based on the standard semantics
of Default Logic (with the initial theories are generated using circumscription). Thus, his
approach does not deal with problem domains where standard semantics of Default Logic
does not produce an extension. Furthermore, as discussed in the introductory section of our
paper, in Thielscher’s framework qualifications are taken over the executability conditions
for the actions instead of over different effects of the actions.
In parallel to the causality-based insight to dynamic domains, other mechanisms for
solving various problems of reasoning about action exist. For instance, a number of approaches employ the concept of chronological ignorance (Shoham, 1987, 1988) to tackle
the problem of anomalous models. In general, causality-based frameworks and approaches
based on chronological ignorance share the notion of directedness: when changes are minimised chronologically, causes are minimised instead of effects as causes are likely to precede
effects. However, as a consequence, backward reasoning is blocked which prevents chronological ignorance-based approaches from dealing with surprises and expectation failures.
Furthermore, non-deterministic actions or incomplete state knowledge are known to cause
difficulty to chronological minimization. For a more detailed comparison between causalitybased approaches and their counterparts that are based on chronological ignorance, the
reader is referred to an article by Thielscher (2001). On the other hand, approaches that
are based on Motivated Action Theory (MAT) (Amsterdam, 1991; Stein & Morgenstern,
1994) can be considered as a special case of the causality-based paradigm. MAT frameworks also advocate for the insight that an appropriate notion of causality is necessary when
assuming away abnormalities. MAT frameworks, however, don’t cope very well with the
explanation problem and the ramification problem as pointed out by Jr. (1999) who also
introduce an approach to improve MAT and overcome these problems.
507

Vo & Foo

Nonetheless, several issues still remain within our framework and will need further treatment to achieve an optimal solution. Firstly, this formalism may not be very suitable for
the large-scale problems as too many assumptions will need to be taken into account. To
address this problem, a localisation procedure is invented to guarantee that only an adequate sub-language will be used to capture the circumstance the agent is reasoning about.
As a consequence, the set of assumptions will be restricted to those which are necessary to
infer the conclusions the agent is interested. This idea is under development. Furthermore,
while a uniform solution to all major problems of reasoning about action may be quite
attractive especially regarding the toy scenarios such as those considered in this paper as
well as in the literature, such a solution may not be pragmatic. By considering the locality
account of reasoning, in particular of the assumptions used in default reasoning, promising
solution can be achieved from both computational and representational points of view. We
are undertaking further investigation towards this research direction.
A major limitation of our framework is the ability to deal with disjunctive axiomatisation of action occurrences. For instance, when the domain axiomatisation constains a
disjunction about the action occurrences such as [t 1 , t2 ]α1 ∨ [t1 , t2 ]α2 , then no plausible sets
of assumptions, and accordingly, no CPMMs (resp. CPMQMs) will be produced to account
for the effects of these actions. While an initial formalisation of our framework allowed
complex expressions for action occurrences in the premises of inference rules which overcame this problem, the formalisation appeared to be too complex and some of the technical
results could not be established. Further investigation, therefore, is needed to overcome
this problem while avoiding to produce an overly complex formalisation.
Provided that our formalism has been designed to provide a general framework for reasoning about action, the following comes as a natural question: To what extent the proposed
approach can be used to formalise dynamic domains? Sandewall (1994) suggests that one
should systematise a framework for reasoning about action against some standard criteria
to provide the formal indications about the expressiveness and capacity of a formalism.
This is in contrast to the more traditional approach that had prevailed for many years in
reasoning about action in which one tries to come up with some examples to show that no
existing approaches are able to deal with such a scenario and claims that one’s approach is
better than others in which it can solve the proposed scenario.
Relative to Sandewall’s standard criteria, our approach enjoys the following properties:
1. Our approach is non-inertia as it allows observations about the later state to correct the system’s predictions about those states using some explanation mechanism
through the use of assumptions.
2. Our formalism is able to deal with non-deterministic and concurrent events.
Another issue is the abduction problem which is also known in the literature as the
explanation problem. For example, in the Stolen car scenario, by observing that the car
disappeared from the parking lot where the reasoner had left it, an expectation failure arises.
Most formalisms would try to accommodate this problem by introducing a stealing action
as part of the vocabulary and try to bind the disappearance of the car to this action. This
arguably is not very intuitive as there is no good reason why we should include this action
in the vocabulary in the first place but not others such as the car being towed away by the
508

Reasoning about Action: An Argumentation-Theoretic Approach

police or a fairy turning the car into a pumpkin, etc. From a pragmatic point of view, some
reasoners may just simply acquire more information (perhaps from the police) instead of
confusing themselves with all kinds of explanations towards possible but uncertain causes.
In other words, we effectively isolate the issues of deducing the new conclusions from the
existing knowledge base from abducing the possible causes of some observations. This is
of course closely related to Shanahan’s approach in his IJCAI’95 paper (Shanahan, 1989)
whose title clearly indicated that “Prediction is Deduction but Explanation is Abduction”.
We are also working on an assumption-based framework to solve the abduction problem.
Acknowledgements
This work was performed when the first author was at the School of Computer Science and
Engineering, University of New South Wales. The authors wish to thank other members
of the Knowledge Systems Group, in paricular Dongmo Zhang and Maurice Pagnucco, and
the anonymous reviewers of an earlier version of this paper for very helpful comments and
suggestions that have significantly improve the quality as well as the readability of this
paper. The first author was partially supported by an International Postgraduate Research
Scholarship (IPRS) sponsored by the Australian government. The first author is presently
supported by a by a DEST IAP grant (2004-2006, grant CG040014).

Appendix A
Theorem 1 Let D = hLD , R, AB, Γi be an S-domain. If I is a CPMM of D then ∆ IQF is
plausible.
Proof: Suppose that I is a CPMM of D,
(i) we prove that ∆IQF is presumable:
∆IQF is R-consistent since I is a model of D. From Observation 2, we have ∆ IQF is
closed and does not attack itself. From Lemma 1, for each assumption δ ∈ AB, if δ ∈
/ ∆ IQF
then δ is rejected by ∆IQF .
(ii) we prove that Lr(∆IQF ) is minimal:
Suppose by way of contradiction that there exists a presumable set of assumptions ∆
(wrt D) such that Lr(∆) ⊂ Lr(∆IQF ). Let I∆ be the ∆-relativised model of D. It is obvious
that I∆ is coherent. We will derive a contradiction by proving that Occ I∆ ⊂ OccI :
(ii.1) OAD ⊆ OccI : obvious as OAD = {(φ(τ1 ), α, φ(τ2 )) ∈ T × A0 × T | Γ |= [τ1 , τ2 ]α},
and I is a model of D.
(ii.2) DAS(∆) ⊆ OccI : Let (t, da¬l , t+ ) ∈ DAS(∆), then there do not exist any action
α ∈ A0 and s ∈ T such that
r=

Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈R
¬[τ2 ]l ∧ ¬[τ1 ]F Al

and I |= prem(r)[τ1 /s, τ2 /t+ ]}. Thus, the assumption δ = [t]F Al ∈ Lr(∆). Thus, δ ∈
Lr(∆IQF ) (from the hypothesis.) From Lemma 2, I |= [t, t + ]da¬l . Thus (t, da¬l , t+ ) ∈ OccI .
(ii.3) OccI 6⊆ OccI∆ : Let δ = [t]F Al ∈ Lr(∆IQF ) \ Lr(∆) for some t ∈ T and l ∈ F ∗ .
Since I is a CPMM of D, from Lemma 2, I |= [t, t + ]da¬l . Thus (t, da¬l , t+ ) ∈ OccI .
509

Vo & Foo

Moreover, δ ∈
/ Lr(∆) iff either (a) δ ∈ ∆, or (b) there is α ∈ A 0 such that
Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈ R and Γ ∪ ∆ `R Φ[τ1 /t, τ2 /t+ ], [t, t+ ]α, [t, t+ ]AQ¬l
r=
¬[τ2 ]l ∧ ¬[τ1 ]F Al
iff, following the construction of ∆-relativised models, I ∆ 6|= [t, t+ ]da¬l . Thus (t, da¬l , t+ ) ∈
/
OccI∆ . Therefore, OccI∆ ⊂ OccI .
Hence, Lr(∆IQF ) is minimal and ∆IQF is plausible.
2
Theorem 2 Let D = hLD , R, AB, Γi be an S-domain and ∆ ⊆ AB. ∆ is plausible wrt D
iff M odS∆ (D) 6= ∅ and for each I ∈ M odS∆ (D), I is a CPMM of D.
Proof:
(⇒) Suppose that ∆ ⊆ AB is plausible wrt D. Then Γ∪∆ is R-consistent, i.e. Γ∪∆ 6` R
false. From the construction of ∆-relativised models, M od S∆ (D) 6= ∅.
For each I = hh, φ, η, εq , εf i ∈ M odS∆ (D), we prove that I is a CPMM of D.
(i) I is a coherent S-model of D: obvious from the definition of ∆-relativised models.
(ii) OccI is minimal:
Assume the contrary, i.e. there is a non-empty set M I = {J | J is a coherent S-model
of D and OccJ ⊂ OccI }. Let H ∈ MI such that there does not exist any model J ∈ M I
and F AH ⊂ F AJ .
Consider the set of assumptions ∆H
QF :
H
(ii.a) ∆QF is presumable wrt D:
• ∆H
QF is closed and does not attack itself (since H is a model of D);
∗
• Let δ ∈ AB, if δ = [τ ]F Al 6∈ ∆H
QF (for some τ ∈ T E and l ∈ F ) and δ isn’t rejected by
∆H
QF then we can easily construct a model J that interprets everything except F A the
same as H and F AJ = F AH ∪ {(φ(τ ), F Al )}. Obviously, J ∈ MI and F AJ ⊂ F AH
which is a contradiction. Thus δ is rejected by ∆ H
QF .

Therefore, ∆H
QF is presumable.
H
(ii.b) Lr(∆QF ) ⊂ Lr(∆):
OccH ⊂ OccI since H ∈ MI .
H
Let δ ∈ Lr(∆H
QF ). Since AB AQ ⊆ ∆QF , δ = [t]F Al ∈ AB F A for some t ∈ T and
∗
l ∈ F . From the definition of leniently rejected assumptions, Γ ∪ ∆ H
QF 6`R false but
H
Γ ∪ ∆QF ∪ {δ} `R false. But then, from the definition of coherent models, H |= [t, t + ]da¬l
(since H is a coherent model of D). Thus, I |= [t, t + ]da¬l .
From the construction of ∆-relativised models, [t]F A l 6∈ ∆. Thus, δ is rejected by ∆
(since ∆ is plausible wrt D). Also from the construction of ∆-relativised models, Γ ∪ ∆ 6` R
I
¬δ. Therefore, δ ∈ Lr(∆), or Lr(∆H
QF ) ⊆ Lr(∆). Now, Occ = OAD ∪ DAS(∆). OAD ⊆
OccH since H is a model of D. Thus, OccI \ OccH = DAS(∆) \ OccH . Let (t, da¬l , t+ ) ∈
DAS(∆) \ OccH . From the construction of ∆-relativised models, δ = [t]F A l ∈ Lr(∆).
H
Suppose further that δ ∈ Lr(∆H
/ ∆H
QF ). Then, δ ∈
QF (as ∆QF is presumable). We
construct a model J in such a way that J interprets everything except F A the same as H.
510

Reasoning about Action: An Argumentation-Theoretic Approach

From the definition of coherent models, (t, da ¬l , t+ ) ∈
/ OccH iff either (1) H 6|= [t]l ∧ ¬[t+ ]l;
or (2) there is α ∈ A0 , and
r=

Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈R
¬[τ2 ]l ∧ ¬[τ1 ]F Al

and Γ ∪ ∆ |= prem(r)[τ1 /t, τ2 /t+ ], i.e., Γ ∪ ∆ |= Φ[τ1 /t, τ2 /t+ ], [t, t+ ]α, [t, t+ ]AQ¬l . Since
δ = [t]F Al ∈ Lr(∆), condition (2) can not be satisfied.
Thus, (t, da¬l , t+ ) ∈
/ OccH iff H 6|= [t]l ∧ ¬[t+ ]l. In other words, it is consistent to add
the assumption [t]F Al to the set of assumptions ∆H
QF . Or, from a model-theoretic point of
view, to augment the denotation of F A in H with (t, F A l ) and we can still obtain a coherent
S-model J such that F AH ⊂ F AJ . But this is a contradiction. Hence, δ ∈
/ Lr(∆ H
QF ).
H
We have shown that Lr(∆QF ) ⊂ Lr(∆).
From (ii.a) and (ii.b) we are led to the conclusion that Occ I is minimal. As a consequence, I is a CPMM of D (from (i) and (ii)).
(⇐) Suppose that M odS∆ (D) 6= ∅ and for each I ∈ M odS∆ (D), I is a CPMM of D.
We prove that ∆ is plausible wrt D. Take an arbitrary model I ∈ M od S∆ (D). Following
Observation 4, ∆ = ∆IQF . From Theorem 1 and the hypothesis that I is a CPMM of D,
we conclude that ∆ is plausible wrt D.
2
Theorem 3 Let D = hLD , R, AB, Γi be an S-domain. Furthermore, suppose that CP M M (D)
is the set of CPMMsSof D and P laus(D) is the set of plausible sets of assumptions of D,
then CP M M (D) = ∆∈P laus(D) M odS∆ (D).
Proof:
(⊇)
then M odS∆ (D) ⊆ CP M M (D) (Following Theorem 2). ThereS Let ∆ ∈ P laus(D),
S
fore, ∆∈P laus(D) M od∆ (D) ⊆ CP M M (D).
(⊆) Let I ∈ CP M M (D), then ∆IQF ∈ P laus(D) (Following Theorem 1).
We prove that I ∈ M od∆I (D) based on Definition 5.4:
QF

1) I is an S-model of D,
2) for each δ = [τ ]F Al ∈ ABF A (for some τ ∈ T E and l ∈ F ∗ ), δ ∈ ∆IQF iff (φ(τ ), F Al ) ∈
F AI (Following the definition of ∆IQF - Definition 5.3)
3) We prove that OccI = OAD ∪ DAS(∆IQF ):
(3.⊇)
• OAD = {(φ(τ1 ), α, φ(τ2 )) ∈ T × A0 × T | Γ |= [τ1 , τ2 ]α} ⊆ OccI (as I is a model of D),
• (t, da¬l , t+ ) ∈ DAS(∆IQF ).
From Definition 5.4,
(i) δ = [t]F Al ∈
/ ∆IQF , and
(ii) there exists no action α ∈ A0 such that:
r=

Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈R
¬[τ2 ]l ∧ ¬[τ1 ]F Al
511

Vo & Foo

and Γ ∪ ∆ |= Φ[τ1 /t, τ2 /t+ ], [t, t+ ]α, [t, t+ ]AQ¬l .
From Lemma 1, δ ∈
/ ∆IQF iff δ is rejected by ∆IQF iff — as I is a coherent model —
either (a) there are α ∈ A0 and s ∈ T such that
r=

Φ, [τ1 , τ2 ]α, [τ1 , τ2 ]AQ¬l
∈R
¬[τ2 ]l ∧ ¬[τ1 ]F Al

and I |= prem(r)[τ1 /s, τ2 /t+ ],
or (b) I |= [t, t+ ]da¬l .
Since (a) violates condition (ii) above, (b) must be the case. Thus (t, da ¬l , t+ ) ∈ OccI .
⇒ DAS(∆IQF ) ⊆ OccI .
⇒ OccI ⊇ OAD ∪ DAS(∆IQF ).
(3.⊆) Suppose that OccI 6⊆ OAD ∪ DAS(∆IQF ). From (3.⊇), we have OccI ⊃ OAD ∪
DAS(∆IQF ). Since ∆IQF is plausible, there exists a model J ∈ M od S∆I (D) such that J
QF

is a CPMM of D. Following Definition 5.4, Occ J = OAD ∪ DAS(∆IQF ) which is a proper
subset of OccI . Contradiction! Therefore, OccI ⊆ OAD ∪ DAS(∆IQF ).
Thus we have shown that I ∈ M odS∆I (D),
QF
S
⇒ M ∈ ∆∈P laus(D) M odS∆ (D) (Since ∆IQF ∈ P laus(D))
S
⇒ CP M M (D) ⊆ ∆∈P laus(D) M odS∆ (D).
S
2
Therefore, CP M M (D) = ∆∈P laus(D) M odS∆ (D).
Theorem 4 Let D be a Q-domain. If I is a CPMQM of D then ∆ IQF is Q-plausible wrt
D.
Proof:
Suppose that I is a CPMQM of D,
(i) it’s easy to verify that ∆IQF is semi–Q-plausible wrt D: similar to the proof of theorem
1, but using Lemma 3 instead of Lemma 1 and Lemma 2.
(ii) ∆IAQ is maximal:
Assume the contrary, i.e. there exists a set of assumptions ∆ such that ∆ is semi–Qplausible wrt D and ∆IAQ ⊂ ∆AQ . Since ∆ is presumable, Γ ∪ ∆ is R-consistent. Let I ∆ be
the ∆-relativised model of D. Obviously, I ∆ is a coherent Q-model of D. It’s easy to verify
that OccI∆ is minimal because otherwise a coherent Q-model J (of D) can be constructed
such that OccJ ⊂ OccI∆ . We have OccI∆ = OAD ∪ DAS(∆) and OAD ⊆ OccJ since J is a
model of D. Thus there exists (t, da¬ϕ , t+ ) ∈ DAS(∆) \ OccJ . But then ∆JQF is presumable
wrt D and Lr(∆JQF ) ⊂ Lr(∆) which is a contradiction. Thus, Occ I∆ is minimal. But then
the model I∆ is a PMQM of D and AQI∆ ⊂ AQI which contradicts with the fact that I is
a CPMQM of D. Therefore, ∆IAQ is maximal.
(iii) ∆IF A is maximal (relative to (i) and (ii)):
Assume the contrary, i.e. there exists a set of assumptions ∆ such that ∆ is semi–Qplausible wrt D and ∆AQ is maximal and ∆IF A ⊂ ∆F A . Let I∆ be the ∆-relativised model
512

Reasoning about Action: An Argumentation-Theoretic Approach

of D. Similar to the proof in part (ii), we can easily verify that I ∆ is a PMQM of D. As the
fact that ∆AQ is maximal and ∆IF A ⊂ ∆F A contradicts with the given hypothesis that I is
a CPMQM of D, we conclude that ∆IF A is maximal. Therefore, ∆IQF is Q-plausible wrt D. 2
Theorem 5 Let D = hLD , R, AB, Γi be a Q-domain and ∆ ⊆ AB. ∆ is Q-plausible wrt D
Q
iff M odQ
∆ (D) 6= ∅ and for each I ∈ M od∆ (D), I is a CPMQM of D.
Proof:
(⇒) Suppose that ∆ is Q-plausible wrt D.
As ∆ is presumable, Γ ∪ ∆ is R-consistent. Following the construction of ∆-relativised
models, M odQ
∆ (D) 6= ∅.
Let I ∈ M odQ
∆ (D):
(i) it’s easy to verify that I is coherent from the construction of ∆-relativised models.
(ii) OccI is minimal:
Assume the contrary, i.e. the set ΣI = {σ | σ is a coherent Q-model of D and Occ σ ⊂
OccI } is non-empty.
Let J ∈ ΣI such that the following conditions are satisfied:
1. there does not exist any model σ ∈ Σ I such that Occσ ⊂ OccJ .
2. AQJ is maximal relative to 1.
3. F AJ is maximal relative to 1. and 2.
Consider the set of assumptions ∆JQF : Obviously, ∆JQF is closed and does not attach
itself. For each δ ∈ AB, if δ ∈
/ ∆JQF then δ is rejected by ∆JQF , otherwise it would violate
the maximality of ∆JAQ and ∆JF A . Thus, ∆JQF is presumable.
Remark that OccJ ⊂ OccI . Besides, OccI = OAD ∪ DAS(∆). But OAD ⊆ OccJ as
J is a model of D. Thus there exists (t, da ¬ϕ , t+ ) ∈ DAS(∆) \ OccJ . But then ∆JQF is
presumable wrt D and Lr(∆JQF ) ⊂ Lr(∆) which is a contradiction. Thus, Occ I is minimal.
As a consequence, I is a PMQM of D.
(iii) AQI is maximal (relative to (i) and (ii)):
Assume the contrary, i.e. the set ΣI = {σ | σ is a PMQM of D and AQI ⊂ AQσ } is
non-empty.
Let J ∈ ΣI such that the following conditions are satisfied:
1. F AJ is maximal; and
2. AQJ is maximal relative to 1.
We can prove that that ∆JQF is semi-Q-plausible (i.e. presumable wrt D and Lr F A (∆JQF )
is minimal):
That ∆JQF is presumable wrt D is easy to verify.
It’s also easy to verify that LrF A (∆JQF ) is minimal as J is coherent and OccJ is minimal.
This would guarantee that the set of occurrences of dummy actions in J is minimised and as
a consequence the set of leniently rejected frame assumptions is also minimised. Formally,
if a presumable set of assumptions Π is such that Lr F A (Π) ⊂ LrF A (∆JQF ) then the ∆relativised model IΠ is a coherent Q-model and OccIΠ ⊂ OccJ , which is a contradiction
with the fact that J ∈ ΣI and thus J is a PMQM of D.
Thus, ∆JQF is semi-Q-plausible wrt D. But, ∆AQ ⊂ ∆JAQ which is a contradiction.
Therefore, we have shown that AQI is maximal (relative to (i) and (ii)).
513

Vo & Foo

(iv) Now, we can prove that F AI is maximal (relative to (i), (ii) and (iii)):
Assume the contrary, i.e. there exists a PMQM J of D such that F A I ⊂ F AJ .
Among those models satisfying the above condition, we choose a model K such that
0
there does not exist any PMQM K 0 of D such that F AK ⊂ F AK . Thus, K is a CPMQM
of D.
Following Theorem 4, we have the set of assumptions ∆ K
QF is Q-plausible and ∆F A ⊂
I
K
K
∆F A since F A ⊂ F A . Contradiction with the hypothesis that ∆ is Q-plausible.
From (i), (ii), (iii) and (iv) we are led to the conclusion that I is a canonical prioritised
minimal Q-model of D.
(⇐) Suppose that M odQ
∆ (D) 6= ∅ and I is a canonical prioritised minimal model of D
Q
for each I ∈ M od∆ (D), we prove that ∆ is plausible.
I
Take an arbitrary model I ∈ M odQ
∆ (D). Following Observation 5, ∆ = ∆ QF . From
Theorem 4 and the hypothesis that I is a canonical prioritised minimal Q-model of D, we
conclude that ∆ is Q-plausible.
2
Theorem 6 Let D be a Q-domain. Furthermore, suppose that CP M QM (D) is the set of
Q
CPMQMs of D and
S P laus (D) is Qthe set of Q-plausible sets of assumptions of D, then
CP M QM (D) = ∆∈P lausQ (D) M od∆ (D).
Proof: Similar to the proof of Theorem 3.

2

Theorem 7 Let σ = hT , F, Ai be a signature and D 0 = hLD , R, AB, ∅i a SSD and α ∈ A0 .
Suppose that w ∈ IS. Define a domain description D = hL D , R, AB, Γi, where Γ = {[Θ]ϕ |
ϕ ∈ w}∪{[Θ, Θ+ ]α}. A set ∆ ⊆ AB of assumptions is AD-plausible wrt D iff for each model
M of ED (∆), [M ]NM (Θ) ∈ T ransαD ([M ]Θ ), i.e. [M ]NM (Θ) belongs to the state transition
from [M ]Θ in D according to α.
Proof:
(⇒) Suppose that ∆ ⊆ AB is AD-plausible wrt D, we prove that for each model M of
ED (∆), [M ]NM (Θ) ∈ T ransαD ([M ]Θ ).
Let M ∈ M od(ED (∆)), as M |= Γ, we have M |= [Θ]ϕ iff ϕ ∈ w. Thus w = [M ] Θ .
We prove that [M ]NM (Θ) ∈ T ransαD (w), i.e. there exists a sequence ω1 , . . . , ωn ∈ IS such
that (w, α, ω1 ) ∈ ResD and [M ]NM (Θ) = ωn and (ωi , ωi+1 ) ∈ CausesD for each 1 ≤ i < n.
If Υw
α = ∅: to the reasoner’s knowledge, α is not applicable in the instantwise state w
due to either non-executability of α in w, or α does not bring about any effects concerned
+
+
to the reasoner. Thus, [M ]Θ = w. Of course, (w, α, [M ]Θ ) ∈ ResD .
If Υw
α 6= ∅: let Ω = {r ∈ RA | M |= prem(r) and M |= cons(r)}, we prove that Ω is a
possible application of α in w.
0
Apparently, Ω ⊆ Υw
α . Ω is maximal since otherwise we can construct a model M such
that M 0 satisfies the qualification assumption of the additional action description rule. But
0
it means M 0 is a PMQM model of D such that AQM ⊂ AQM . Thus M is not a CPMQM
of D. Following Theorem 5, ∆ is not Q-plausible wrt D. Contradiction.
We now prove that there does not exists any instantwise state S such that CON S A (Ω) ⊆
+
S and [M ]Θ \ w ⊂ S \ w.
514

Reasoning about Action: An Argumentation-Theoretic Approach

Assume the contrary, i.e. there exists a fluent literal ϕ ∈ F ∗ such that ϕ ∈ (S \ w) \
+
([M ]Θ \ w).
Then M 6|= [Θ]F Aϕ since M is a model of D.
Construct a model M 0 in such a way that M 0 interprets everything the same as M except
0
0
F A, and F AM = F AM ∪ {(Θ, F Aϕ )}. Obviously, M 0 is a PMQM of D and AQM = AQM ,
0
but F AM ⊂ F AM . Thus M is not a CPMQM of D. Following Theorem 5, ∆ is not
+
Q-plausible wrt D. Contradiction. Therefore, (w, α, [M ] Θ ) ∈ ResD .
(⇐) Suppose that for each model M of E D (∆), [M ]NM (Θ) ∈ T ransαD ([M ]Θ ), we prove
that ∆ ⊆ AB is AD-plausible wrt D which is obvious.
2

References
Amsterdam, J. B. (1991). Temporal reasoning and narrative conventions. In Allen, J. F.,
Fikes, R., & Sandewall, E. (Eds.), KR’91: Principles of Knowledge Representation
and Reasoning, pp. 15–21, Cambridge, MA. Morgan Kaufmann.
Baker, A. B. (1989). A simple solution to the Yale Shooting problem. In Brachman, R. J.,
Levesque, H. J., & Reiter, R. (Eds.), KR’89: Principles of Knowledge Representation
and Reasoning, pp. 11–20, San Mateo, California. Morgan Kaufmann.
Baral, C. (1995). Reasoning about actions: Non-deterministic effects, constraints, and qualification. In International Joint Conference on Artificial Intelligence.
Bondarenko, A., Dung, P. M., Kowalski, R. A., & Toni, F. (1997). An abstract,
argumentation-theoretic approach to default reasoning. Artificial Intelligence Journal,
93, 63–101.
Bracciali, A., & Kakas, A. C. (2004). Frame consistency: computing with causal explanations. In 10th International Workshop on Non-Monotonic Reasoning (NMR 2004),
pp. 79–87.
Castilho, M. A., Gasquet, O., & Herzig, A. (1999). Formalizing action and change in modal
logic I: The frame problem. Journal of Logic and Computation, 9(5), 701–735.
Dimopoulos, Y., Kakas, A. C., & Michael, L. (2004). Reasoning about actions and change
in answer set programming. In International Conference on Logic Programming and
Nonmonotonic Reasoning - LPNMR’ 04, pp. 61–73.
Doherty, P. (1994). Reasoning about action and change using occlusion. In European
Conference on Artificial Intelligence, pp. 401–405.
Doherty, P., & Kvarnström, J. (1998). Tackling the qualification problem using fluent dependency constraints: Preliminary report. In 5th Workshop on Temporal Representation
and Reasoning - TIME, pp. 97–104.
Doyle, J. (1979). A truth maintenance system. Artificial Intelligence Journal, 12(3), 231–
272.
Drakengren, T., & Bjäreland, M. (1999). Reasoning about action in polynomial time.
Artificial Intelligence Journal, 115, 1–24.
Fikes, R., & Nilsson, N. J. (1971). STRIPS: A new approach to the application of theorem
proving to problem solving. Artificial Intelligence Journal, 2(3/4), 189–208.
515

Vo & Foo

Foo, N. Y., Zhang, D., Vo, Q. B., & Peppas, P. (2001). Circumscriptive models and automata. In Thielscher, M., & Williams, M.-A. (Eds.), Workshop on Non-monotonic
Reasoning, Action and Change - colocated with IJCAI-01.
Gelfond, M., & Lifschitz, V. (1998). Action languages. Electronic Transactions on AI,
3(16), 193–210.
Ginsberg, M. L., & Smith, D. E. (1988). Reasoning about action I: A possible worlds
approach. Artificial Intelligence Journal, 35(2), 165–196.
Giunchiglia, E., Kartha, G. N., & Lifschitz, V. (1997). Representing action: Indeterminacy
and ramifications. Artificial Intelligence Journal, 95(2), 409–438.
Giunchiglia, E., & Lifschitz, V. (1998). An action language based on causal explanation:
Preliminary report. In National Conference on Artificial Intelligence, pp. 623–630.
Green, C. (1969). Application of theorem proving to problem solving. In International Joint
Conference on Artificial Intelligence, pp. 219–240.
Gustafsson, J., & Doherty, P. (1996). Embracing occlusion in specifying the indirect effects
of actions. In Principles of Knowledge Representation and Reasoning, pp. 87–98.
Haas, A. R. (1987). The case for domain-specific frame axioms. In The frame problem in
artificial intelligence: proc. of 1987 workshop. Morgan Kaufmann.
Hanks, S., & McDermott, D. (1987). Nonmonotonic logic and temporal projection. Artificial
Intelligence Journal, 33(3), 379–412.
Jr., C. L. O. (1999). Explanatory update theory: Applications of counterfactual reasoning
to causation. Artificial Intelligence Journal, 108(1-2), 125–178.
Kakas, A. C., Miller, R., & Toni, F. (1999). An argumentation framework of reasoning
about actions and change. In International Conference on Logic Programming and
Nonmonotonic Reasoning - LPNMR’ 99, pp. 78–91.
Kakas, A. C., Miller, R., & Toni, F. (2000). E-res - a system for reasoning about actions, events and observations. In Baral, C., & Truszczynski, M. (Eds.), International
Workshop on Non-Monotonic Reasoning, Special Session on System Descriptions and
Demonstration - NMR’2000.
Kakas, A. C., Miller, R., & Toni, F. (2001). E-res: Reasoning about actions, events and
observations. In International Conference on Logic Programming and Nonmonotonic
Reasoning - LPNMR’ 01, pp. 254–266.
Kautz, H. (1986). The logic of persistence. In National Conference on Artificial Intelligence.
Kowalski, R., & Sergot, M. J. (1986). A logic-based calculus of events. New Generation
Computing, 4, 67–95.
Kowalski, R. (1992). Database updates in the event calculus. Journal of Logic Programming,
12, 121–146.
Kushmerick, N. (1996). Cognitivism and situated action: two views on intelligent agency.
Computers and Artificial Intelligence, 15(5).
Lifschitz, V. (1987). On the semantics of STRIPS. In Georgeff, & Lansky (Eds.), Reasoning
about Actions and Plans. Morgan Kauffman, Los Altos.
516

Reasoning about Action: An Argumentation-Theoretic Approach

Lin, F. (1995). Embracing causality in specifying the indirect effects of actions. In International Joint Conference on Artificial Intelligence.
Lin, F. (1996). Embracing causality in specifying the indeterminate effects of actions. In
National Conference on Artificial Intelligence, pp. 670–676.
Lin, F., & Reiter, R. (1994). State constraints revisited. Journal of Logic and Computation,
4(5), 655–678.
Lin, F., & Shoham, Y. (1995). Provably correct theories of action. Journal of the ACM,
42(2), 293–320.
McCain, N., & Turner, H. (1995). A causal theory of ramifications and qualifications. In
International Joint Conference on Artificial Intelligence.
McCain, N., & Turner, H. (1997). Causal theories of action and change. In National
Conference on Artificial Intelligence, pp. 460–465.
McCarthy, J. (1977). Epistemological problems of artificial intelligence. In International
Joint Conference on Artificial Intelligence, pp. 555–562.
McCarthy, J. (1980). Circumscription - a form of non-monotonic reasoning. Artificial
Intelligence Journal, 13(1-2), 27–39.
McCarthy, J. (1986). Applications of circumscription to formalizing common sense knowledge. Artificial Intelligence Journal, 26(3), 89–116.
McCarthy, J., & Hayes, P. (1969). Some philosophical problems from the standpoint of
artificial intelligence. In Michie, D., & Meltzer, B. (Eds.), Machine Intelligence 4.
Edinburgh University Press.
McDermott, D. V. (1987). We’ve been framed: Or why ai is innocent of the frame problem. In Pylyshyn, Z. (Ed.), The Robots Dilemma: The Frame Problem in Artificial
Intelligence, pp. 113–122. Ablex.
McDermott, D. V., & Doyle, J. (1980). Non-monotonic logic I. Artificial Intelligence
Journal, 13(1-2), 41–72.
Moore, R. C. (1985). Semantical considerations on nonmonotonic logic. Artificial Intelligence Journal, 25(1), 75–94.
Morris, P. H. (1988). The anomalous extension problem in default reasoning. Artificial
Intelligence Journal, 35(3), 383–399.
Pednault, E. (1989). ADL: Exploring the middle ground between STRIPS and the situation calculus. In KR’89: First International Conference on Principles of Knowledge
Representation and Reasoning, pp. 324–332. Morgan Kaufmann.
Poole, D. (1988). A logical framework for default reasoning. Artificial Intelligence Journal,
36(1), 27–47.
Reiter, R. (1980). A logic for default reasoning. Artificial Intelligence Journal, 13, 81–132.
Reiter, R. (1991). The frame problem in the situation calculus: A simple solution (sometimes) and a completeness result for goal regression. In Lifschitz, V. (Ed.), AI and
Mathematical Theory of Computation: Papers in Honor of John McCarthy, pp. 418–
420. Academic Press.
517

Vo & Foo

Sandewall, E. (1994). Features and Fluents. Oxford University Press, Oxford.
Schubert, L. (1990). Monotonic solution of the frame problem in the situation calculus; an
efficient method for worlds with fully specified actions. In Kyburg, H., Loui, R., &
Carlson, G. (Eds.), Knowledge Representation and Defeasible Reasoning, pp. 23–67.
Kluwer Academic Publishers, Dordrecht.
Shanahan, M. (1989). Prediction is deduction but explanation is abduction. In International
Joint Conference on Artificial Intelligence, pp. 1055–1060.
Shanahan, M. (1997). Solving the Frame Problem: A Mathematical Investigation of the
Common Sense Law of Inertia. MIT Press, Cambridge, Massachussets.
Shanahan, M. (1999). The ramification problem in the event calculus. In International
Joint Conference on Artificial Intelligence, pp. 140–146.
Shoham, Y. (1987). Reasoning about Change. MIT Press, Cambridge, MA.
Shoham, Y. (1988). Chronological ignorance: Experiments in nonmonotonic temporal reasoning. Artificial Intelligence Journal, 36, 279–331.
Stein, L. A., & Morgenstern, L. (1994). Motivated action theory: a formal theory of causal
reasoning. Artificial Intelligence Journal, 71(1), 1–42.
Thielscher, M. (1997). Ramification and causality. Artificial Intelligence Journal, 89, 317–
364.
Thielscher, M. (1999). From situation calculus to fluent calculus: State update axioms as
a solution to the inferential frame problem. Artificial Intelligence Journal, 111(1-2),
277–299.
Thielscher, M. (2001). The qualification problem: A solution to the problem of anomalous
models. Artificial Intelligence Journal, 131(1-2), 1–37.
Turner, H. (1997). Representing actions in logic programs and default theories: A situation
calculus approach. Journal of Logic Programming, 31(1-3), 245–298.
Vo, Q. B., & Foo, N. Y. (2001). Solving the qualification problem. In Australian Joint
Conference on Artificial Intelligence, pp. 519–531.
Vo, Q. B., & Foo, N. Y. (2002). Solving the ramification problem: Causal propagation in
an argumentation-theoretic approach. In 7th Pacific Rim International Conference
on Artificial Intelligence - PRICAI2002, pp. 49–59.
Zhang, D., & Foo, N. Y. (2002). Interpolation properties of action logic: Lazy-formalization
to the frame problem. In Flesca, S., Greco, S., Leone, N., & Ianni, G. (Eds.), Logics
in Artificial Intelligence, European Conference, JELIA 2002, pp. 357–368.

518

Journal of Artificial Intelligence Research 24 (2005) 1-48

Submitted 11/04; published 07/05

CIXL2: A Crossover Operator for Evolutionary Algorithms
Based on Population Features
Domingo Ortiz-Boyer
César Hervás-Martı́nez
Nicolás Garcı́a-Pedrajas

dortiz@uco.es
chervas@uco.es
npedrajas@uco.es

Department of Computing and Numerical Analysis
University of Córdoba, Spain

Abstract
In this paper we propose a crossover operator for evolutionary algorithms with real
values that is based on the statistical theory of population distributions. The operator is
based on the theoretical distribution of the values of the genes of the best individuals in
the population. The proposed operator takes into account the localization and dispersion
features of the best individuals of the population with the objective that these features
would be inherited by the offspring. Our aim is the optimization of the balance between
exploration and exploitation in the search process.
In order to test the efficiency and robustness of this crossover, we have used a set of
functions to be optimized with regard to different criteria, such as, multimodality, separability, regularity and epistasis. With this set of functions we can extract conclusions
in function of the problem at hand. We analyze the results using ANOVA and multiple
comparison statistical tests.
As an example of how our crossover can be used to solve artificial intelligence problems,
we have applied the proposed model to the problem of obtaining the weight of each network
in a ensemble of neural networks. The results obtained are above the performance of
standard methods.

1. Introduction
Evolutionary algorithms (EAs) are general purpose searching methods. The selection process and the crossover and mutation operators establish a balance between the exploration
and exploitation of the search space which is very adequate for a wide variety of problems
whose solution presents difficulties that are insolvable using classical methods. Most of
these problems are defined in continuous domains, so the evolutionary algorithms applied
use real values, namely, evolution strategies (EPs), real-coded genetic algorithms (RCGAs),
and evolutionary programming (EP). For these paradigms the precision of the solution does
not depend on the coding system, as in binary coded genetic algorithms, but on the precision
of the computer system where the algorithms are run.
The selection process drives the searching towards the regions of the best individuals.
The mutation operator randomly modifies, with a given probability, one or more genes of a
chromosome, thus increasing the structural diversity of the population. As we can see, it is
clearly an exploration operator, that helps to recover the genetic diversity lost during the
selection phase and to explore new solutions avoiding premature convergence. In this way,
the probability of reaching a given point in the search space is never zero. This operator,
c
°2005
AI Access Foundation. All rights reserved.

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Exploration
2

β2

2

Ex

pl
oi
ta

1

tio
n

Hβ , β

β12

Exploration

Exploitation
Sβ , β

1
1

β

2
1

β

ai

(a)

1

2

i

i

Exploration

2

1
i

β

βi

bi

(b)

Figure 1: (a) Hypercube defined by the first two genes of the parents; (b) Representation
of the segment defined by the ith genes of two chromosomes.

in fact, implements a random search whose well-studied features are useful in the field of
evolutionary computation.
The crossover operator combines the genes of two or more parents to generate better
offspring. It is based on the idea that the exchange of information between good chromosomes will generate even better offspring. The effect of the crossover operator can be
studied from two different points of view: at chromosome level and at gene level. The effect
of the crossover operator at chromosome level can be considered in a geometric way. Given
two parents β 1 = {β11 , β21 } and β 2 = {β12 , β22 } with two genes, we denote by Hβ 1 β 2 the
hypercube defined by their genes (Figure 1a). At gene level the representation would be
linear, defining in this case a segment or interval Sβ 1 ,β 2 for each pair of genes (Figure 1b).
i
i
Most crossover operators generate individuals in the exploitation zones, S β 1 ,β 2 or Hβ 1 β 2 .
i
i
In this way, the crossover operator implements a depth search or exploitation, leaving the
breadth search or exploration for the mutation operator.
This policy, intuitively very natural, makes the population converge to values within
the hypercubes defined by their parents, producing a rapid decrease in the population
diversity which could end up in a premature convergence to a non-optimal solution. Recent
studies on BLX-α crossover (Eshelman & Schaffer, 1993), the crossover based on fuzzy
connectives (Herrera, Herrera-Viedma, Lozano, & Verdegay, 1994), and fuzzy recombination
(Voigt, Mühlenbein, & Cvetkovic, 1995), have confirmed the good performance of those
crossover operators that also generate individuals in the exploration zone. These operators
avoid the loss of diversity and the premature convergence to inner points of the search
space, but also the generation of new individuals in the exploration zone could slow the
search process. For this reason, the crossover operator should establish an adequate balance
between exploration (or interpolation) and exploitation (or extrapolation), and generate
offspring in the exploration and exploitation zones in the correct proportion.
Establishing a balance between exploration and exploitation is important, but it is also
important that such a balance is self-adaptive (Kita, 2001; Beyer & Deb, 2001; Deb &
Beyer, 2001), that is, it must guarantee that the dispersion of the offspring depends on
2

CIXL2: A Crossover Operator for Evolutionary Algorithms

the dispersion of the parents. So, two close parents must generate close offspring, and two
distant parents must generate distant offspring. The control of dispersion in the crossover
based on fuzzy connectives is based on the generation of offspring using the fuzzy connectives t-norms, t-conorms, average functions, and a generalized operator of compensation
(Mizumoto, 1989). In fuzzy recombination the offspring is generated using two triangular
distributions whose averages derive from each of the genes of the two parents. In BLX-α
we have the same probability of generating an offspring between the parents, and in an area
close to the parents whose amplitude is modulated by the α parameter.
Ono and Kobayashi (1997) have proposed a Unimodal Normally Distributed Crossover
(UNDX), where three parents are used to generate two or more children. The children are
obtained using an ellipsoidal distribution where one axis is the segment that joins the two
parents and the extent of the orthogonal direction is decided by the perpendicular distance
of the third parent from the axis. The authors claim that this operator should preserve the
statistics of the population. This crossover is also self-adaptive, but it differs from BLX-α
by the fact that it is more probable to generate offspring near the average of the first two
parents.
Another self-adaptive crossover is the Simulated Binary Crossover (SBX) (Deb & Agrawal,
1995). Based on the search features of the single-point crossover used in binary-coded genetic algorithms, this operator respects the interval schemata processing, in the sense that
common interval schemata of the parents are preserved in the offspring. The SBX crossover
puts the stress on generating offspring near the parents. So, the crossover guarantees that
the extent of the children is proportional to the extent of the parents, and also favors
that near parent individuals are monotonically more likely to be chosen as children than
individuals distant from the parents.
The main goal of this paper is to propose a crossover operator that avoids the loss
of diversity of the population of individuals, and, at the same time, favors the speed of
convergence of the algorithm. These two goals are, at first, conflicting; their adequate
balance is controlled by two of the basic features of the crossover operator: i) the balance
between exploration and exploitation and, ii) the self-adaptive component. These two
features make the evolutionary algorithms avoid premature convergence and favor local
fine-tuning. Both attributes are highly appreciated in any search algorithm.
In most current crossover operators, the features of the offspring depend on the features
of just a few parents. These crossovers do not take into account population features such
as localization and dispersion of the individuals. The use of these statistical features of the
population may help the convergence of the population towards the global optimum.
The crossover operator implements basically a depth or exploitative search, just like
other methods such as steepest gradient descent, local search or simulated annealing, but
in these three search methods the algorithm takes the quality of the solutions into account.
So, it is reasonable to think that it is also convenient for the crossover operator to consider
the performance on the individuals involved in the crossover operation. This idea is already
implemented by some heuristic crossovers (Wright, 1991).
Nevertheless, following the previous line of argument, it seems rather poor to use just
two parents, and not to consider the most promising directions towards which it would
be advisable to drive the search. That is, instead of using a local heuristic that uses two
3

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

individuals, involving the whole population or an adequate subset in the determination of
the direction of the search whose features would be specially suitable.
Motivated by this line of argument, in this paper we propose a crossover operator, which
will be called Confidence Interval Based Crossover using L2 Norm (CIXL2). On the one
hand, it takes advantage of the selective component that is derived from the extraction of
the features of the best n individuals of the population and that indicates the direction of
the search, and on the other hand, it makes a self-adaptive sampling around those features
whose width depends on the number of best individuals, dispersion of those best individuals,
confidence coefficient, and localization of the individuals that participate in the crossover.
Now, the exploitation region is not the area between the two parents that are involved
in the crossover, but the area defined by the confidence interval built from the n best
individuals of the population; and the exploratory region is the rest of the search domain.
To the previous concepts of exploration and exploitation, merely geometrical, is added a
probabilistic component that depends on the population features of the best individuals.
Estimation of Distribution Algorithms (EDAs) or Probabilistic Model-Building Evolutionary Algorithms (Mühlenbein & Paaβ, 1998; Mühlenbein, Mahnig, & Rodriguez, 1999)
are based on a, seemingly, similar idea. These algorithms do not have mutation and crossover
operators. After every generation the population distribution of the selected individuals is
estimated and the new individuals are obtained sampling this estimated distribution. However, the underlying idea behind our crossover is the extraction of population features, mean
and standard deviation, in order to detect the regions where there is a higher probability
of getting the best individuals. In order to perform the crossover, we create three virtual
parents that represent the localization estimator mean, and the bounds of the confidence
interval from which, with a certain confidence degree, this localization estimator takes the
values. In this way, the children generated from these three parents will inherit the features
of the best individuals of the population.
The rest of the paper is organized as follows: Section 2 explains the definition of CIXL2
and its features; Section 3 discusses the problem of the selection of the test sets, and
justifies the use of a test set based on the one proposed by Eiben and Bäck (1997a); Section
4 describes the experimental setup of evolutionary algorithm (RCGA) used in the tests;
Section 5 studies the optimal values of the parameters of CIXL2; Section 6 compares the
performance of CIXL2 against other crossovers; Section 7 compares CIXL2 with EDAs;
Section 8 describes the application of RCGAs with CIXL2 to neural network ensembles;
and, finally, Section 9 states the conclusions of our paper and the future research lines.

2. CIXL2 Operator
In this section we will explain the theoretical base that supports the defined crossover
operator, and then we will define the crossover. We will use an example to explain the
dynamics of a population subject to this crossover operator.
2.1 Theoretical Foundation
In this section we will study the distribution of the i-th gene and the construction of a
confidence interval for to the localization parameter associated with that distribution.
4

CIXL2: A Crossover Operator for Evolutionary Algorithms

Let β be the set of N individuals with p genes that make up the population and β ∗ ⊂
β the set of the best n individuals. If we assume that the genes βi∗ of the individuals
belonging to β ∗ are independent random variables with a continuous distribution H(βi∗ )
with a localization parameter µβi∗ , we can define the model
βi∗ = µβi∗ + ei ,

for i = 1, ..., p,

(1)

being ei a random variable. If we suppose that, for each gene i, the best n individuals form
∗ , β ∗ , ..., β ∗ } of the distribution of β ∗ , then the model takes the form
a random sample {βi,1
i,2
i,n
i
∗
βij
= µβi∗ + eij ,

for i = 1, ..., p and j = 1, ..., n.

(2)

Using this model, we analyze an estimator of the localization parameter for the i-th
gene based on the minimization of the dispersion function induced by the L2 norm. The L2
norm is defined as
n
X
(eij )2 ,
(3)
kei k22 =
j=1

hence the associated dispersion induced by the L2 norm in the model 2 is
D2 (µβi∗ ) =

n
X
j=1

∗
(βij
− µβi∗ )2 ,

(4)

and the estimator of the localization parameter µβi∗ is:
µ̂

βi∗

= arg min D2 (µ ) = arg min
βi∗

n
X
j=1

∗
(βij
− µβi∗ )2 .

(5)

Using for minimization the steepest gradient descent method,
S2 (µβi∗ ) = −
we obtain
S2 (µβi∗ ) = 2

∂D2 (µβi∗ )
,
∂µβi∗

n
X
j=1

∗
(βij
− µβi∗ ),

(6)

(7)

and making (7) equal to 0 yields
µ̂βi∗ =

Pn

∗
j=1 βij

n

= β̄i∗ .

(8)

So, the estimator of the localization parameter for the i-th gene based on the minimization of the dispersion function induced by the L2 norm is the mean of the distribution of
βi∗ (Kendall & Stuart, 1977), that is, µ̂βi∗ = β̄i∗ .
5

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

The sample mean estimator is a linear estimator1 , so it has the properties of unbiasedness2 and consistency3 , and it follows a normal distribution N (µβi∗ , σβ2 ∗ /n) when the
i
distribution of the genes H(βi∗ ) is normal. Under this hypothesis, we construct a bilateral
confidence interval for the localization of the genes of the best n individuals, using the
studentization method, the mean as the localization parameter,and the standard deviation
Sβi∗ as the dispersion parameter:
¸
·
Sβi∗
Sβi∗ ∗
CI
∗
(9)
I = β̄i − tn−1,α/2 √ ; β̄i + tn−1,α/2 √
n
n
where tn−1,α/2 is the value of Student’s t distribution with n − 1 degrees of freedom, and
1 − α is the confidence coefficient, that is, the probability that the interval contains the true
value of the population mean.
2.2 CIXL2 Definition
From this definition of the confidence interval, we define three intervals to create three “virtual” parents, formed by the lower limits of the confidence interval of each gene, CILL 4 , the
upper limits, CIU L5 , and the means CIM 6 . These parents have the statistical information
of the localization features and dispersion of the best individuals of the population, that is,
the genetic information the fittest individuals share. Their definition is:
CILL = (CILL1 , . . . , CILLi , . . . CILLp )

(10)

CIU L = (CIU L1 , . . . , CIU Li , . . . CIU Lp )
CIM

= (CIM1 , . . . , CIMi , . . . CIMp ),

where
Sβ ∗
CILLi = β̄i∗ − tn−1,α/2 √ i
n
S
β∗
CIU Li = β̄i∗ + tn−1,α/2 √ i
n
CIMi = β̄i .

(11)

The CILL and CIU L individuals divide the domain of each gene into three subintervals:
Di ≡ IiL ∪ IiCI ∪ IiU , where IiL ≡ [ai , CILLi ); IiCI ≡ [CILLi , CIU Li ]; IiU ≡ (CIU Li , bi ];
being ai and bi the bounds of the domain (see Figure 2).
The crossover operator creates one offspring β s , from an individual of the population
β f ∈ β, randomly selected, and one of the individuals CILL, CIU L or CIM , depending
on the localization of β f , as follows:
1. It is a linear combination of the sample values.
2. An estimator θ̂ is an unbiased estimator of θ if the expected value of the estimator is the parameter to
be estimate: E[θ̂] = θ.
3. A consistent estimator is an estimator that converges in probability to the quantity being estimated as
the sample size grows.
4. Confidence Interval Lower Limit.
5. Confidence Interval Upper Limit.
6. Confidence Interval Mean.

6

CIXL2: A Crossover Operator for Evolutionary Algorithms

Di
I

ai

CI

L
i

Ii
C I LL i

C I Mi

U

β

s
i

f

Ii

βi
C I U Li

bi

Figure 2: An example of confidence interval based crossover
• βif ∈ IiL : if the fitness of β f is higher than CILL, then βis = r(βif − CILLi ) + βif , else
βis = r(CILLi − βif ) + CILLi .
• βif ∈ IiCI : if the fitness of β f is higher than CIM, then βis = r(βif − CIMi ) + βif , else
βis = r(CIMi − βif ) + CIMi .
• βif ∈ IiU : if the fitness of β f is higher than CIUL, then βis = r(βif − CIU Li ) + βif , else
βis = r(CIU Li − βif ) + CIU Li (this case can be seen in Figure 2).
where r is a random number in the interval [0, 1].
With this definition, the offspring always takes values in the direction of the best of the
two parents but never between them. If the virtual individual is one of the bounds of the
confidence interval and is better than the other parent, the offspring is generated in the
direction of the confidence interval where it is more likely to generate better individuals.
If the virtual individual is worse than the other parent, the offspring is generated near the
other parent in the opposite direction of the confidence interval. On the other hand, if a
parent selected from the population is within the confidence interval, the offspring can be
outside the interval – but always in its neighborhood – if the fitness of the center of the
confidence interval is worse. This formulation tries to avoid a shifting of the population
towards the confidence interval, unless this shifting means a real improvement of the fitness
in the population.
If β f is distant from the other parent, the offspring will probably undergo a marked
change, and if both parents are close, the change will be small. The first circumstance will
be likely to occur in the first stages of the evolutionary process, and the second one in the
final stages.
The width of the interval I CI depends on the confidence coefficient, 1 − α, the number
of best individuals, n, and the dispersion of the best individuals. In the first stages of the
evolution, the dispersion will be large, specially for multimodal functions, and will decrease
together with the convergence of the genetic algorithm. These features allow the balance
between exploitation and exploration to adjust itself dynamically. The crossover will be
more exploratory at the beginning of the evolution, avoiding a premature convergence, and
more exploitative at the end, allowing a fine tuning. The parameters n and 1−α regulate the
dynamics of the balance favoring a higher or lower degree of exploitation. That suggests the
CIXL2 establishes a self-adaptive equilibrium between exploration and exploitation based
on the features that share, with a certain confidence degree 1 − α, the best n individuals
7

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

25

Best individuals distribution
Population distribution
Population distribution after crossover

CIP

D
of population
DCI of the best individuals
of population after crossover
Individuals
Individuals proyected on axis x1x2
Best individuals
Best individuals proyected on axis x1x2
CIP

D

f(x)

20

Individuals number

4000

0

15

10

2
1.5
1
-2

5

0.5
-1.5

-1

-0.5
x1

0
-0.5
0

0.5

1

1.5

-1
-1.5
2

x2

I2L

I2CI

I2U

-2
0

(a)

-2

-1

CILL2

0
x2

CIM2

CIUL2

1

2

(b)

Figure 3: Effect of the CIXL2 crossover over a population used for the minimization of the
Rosenbrock function with two variables

of the population. A preliminary theoretical study of this aspect is carried out by HervásMartı́nez and Ortiz-Boyer (2005).
2.3 Crossover Dynamics
Figure 3 shows a simulation of the behavior of the crossover for the optimization of Rosenbrock function (Eiben & Bäck, 1997b) with two variables. On Figure 3a, we observe how
most of the individuals are within the domain D CIP ; while the best n are within the confidence domain D CI ≡ I1CI ×I2CI . DCI is shifted towards the minimum of the function placed
in (1, 1), the domain D CIP of the new population, generated after applying CIXL2, will be
shifted to the optimum. This displacement will be higher in the first stages of evolution,
and will decrease during evolution. It may be modulated by the parameters n and 1 − α.
Figure 3a shows how the population, after applying the crossover operator, is distributed
in a region nearer the optimum whose diversity depends on the parameters of the operator.
Figure 3b shows how the whole population and the n best individuals are distributed. As
we can see, the distribution of the best n individuals keeps the features of the distribution of
the population, but it is shifted to the optimum. The shifting towards the optimum will be
more marked if the value of n is small. The tails of the distribution of the best individuals
will be larger if the dispersion of the best individuals is also large, and smaller if they are
concentrated in a narrow region. The size of these tails also depends on the features of the
problem, the stage of the evolution, and the particular gene considered. The effect of the
crossover on the distribution of the population is to shift the distribution towards the best
n individuals and to stretch the distribution modulately depending on the amplitude of the
confidence interval. The parameters n and 1 − α are responsible for the displacement and
the stretching of the region where the new individuals will be generated.
If n is small, the population will move to the most promising individuals quickly. This
may be convenient for increasing the convergence speed in unimodal functions. Nevertheless,
it can produce a premature convergence to suboptimal values in multimodal functions. If
n is large, both the shifting and the speed of convergence will be smaller. However, the
8

CIXL2: A Crossover Operator for Evolutionary Algorithms

evolutionary process will be more robust, this feature being perfectly adequate for the
optimization of multimodal, non-separable, highly epistatic functions.
The parameter n is responsible for the selectiveness of the crossover, as it determines
the region where the search will be directed. The selection is regulated by the parameter
1 − α. This parameter bounds the error margin of the crossover operator in order to obtain
a search direction from the feature that shares the best individuals of the population.

3. Benchmark Problems
In the field of evolutionary computation, it is common to compare different algorithms using
a large test set, especially when the test involves function optimization (Gordon & Whitley,
1993). However, the effectiveness of an algorithm against another algorithm cannot be
measured by the number of problems that it solves better. The “no free lunch” theorem
(Wolpert & Macready, 1995) shows that, if we compare two searching algorithms with all
possible functions, the performance of any two algorithms will be , on average, the same
. As a result, attempting to design a perfect test set where all the functions are present
in order to determine whether an algorithm is better than another for every function, is a
fruitless task.
That is the reason why, when an algorithm is evaluated, we must look for the kind
of problems where its performance is good, in order to characterize the type of problems
for which the algorithm is suitable. In this way, we have made a previous study of the
functions to be optimized for constructing a test set with fewer functions and a better
selection (Whitley, Mathias, Rana, & Dzubera, 1995; Salomon, 1996). This allows us to
obtain conclusions of the performance of the algorithm depending on the type of function.
Taking into account this reasoning, the test set designed by Eiben and Bäck (1997b) is
very adequate. The test set has several well characterized functions that will allow us to
obtain and generalize, as far as possible, the results regarding the kind of function involved.
Nevertheless, we have added two functions to the test set with the aim of balancing the
number of functions of each kind. These two new functions are the function of Rosenbrock
(Rosenbrock, 1960) extended to p dimensions and the function of Schwefel (Schwefel, 1981);
both of them have been widely used in evolutive optimization literature. Table 1 shows the
expression of each function and a summary of its features: separability, multimodality, and
regularity.
A function is multimodal if it has two or more local optima. A function of p variables is
separable if it can be rewritten as a sum of p functions of just one variable (Hadley, 1964).
The separability is closely related to the concept of epistasis or interrelation among the
variables of the function. In the field of evolutionary computation, the epistasis measures
how much the contribution of a gene to the fitness of the individual depends on the values
of other genes.
Non separable functions are more difficult to optimize as the accurate search direction
depends on two or more genes. On the other hand, separable functions can be optimized for
each variable in turn. The problem is even more difficult if the function is also multimodal.
The search process must be able to avoid the regions around local minima in order to
approximate, as far as possible, the global optimum. The most complex case appears when
the local optima are randomly distributed in the search space.
9

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Function
Sphere

Schwefel’s
double sum
Rosenbrock

Rastrigin

Schwefel

Ackley

Griewangk

Fletcher
Powell

Langerman

Definition
Pp
2
fSph (x) =
i=1 xi
xi ∈ [−5.12, 5.12]
x∗ = (0, 0, . . . , 0); fSph (x∗ ) = 0
³P
´2
Pp
i
fSchDS (x) =
j=1 xj
i=1
xi ∈ [−65.536, 65.536]
x∗ = (0, 0, . . . , 0); fSchDS (x∗ ) = 0
Pp−1
2 2
2
fRos (x) =
i=1 [100(xi+1 − xi ) + (xi − 1) ]
xi ∈ [−2.048, 2.048]
x∗ = (1, 1, . . . , 1); fRos (x∗ ) = 0
Pp
fRas (x) = 10p + i=1 (x2
i − 10 cos(2πxi ))
xi ∈ [−5.12, 5.12]
∗
x = (0, 0, . . . , 0); fRas (x∗ ) = 0
³p
´
Pp
fSch (x) = 418.9829 · p + i=1 xi sin
|xi |
xi ∈ [−512.03, 511.97]
x∗ = (−420.9687, . . . , −420.9687);
(x∗ ) ´
=0
³
q fSch
1 Pp
2
fAck (x) = 20 + e − 20exp −0.2 p
i=1 xi −
³ P
´
p
1
exp p
i=1 cos(2πxi )
xi ∈ [−30, 30]
∗
x = (0, 0, . . . , 0); fAck (x∗ ) = 0
´
³
Qp
Pp
x2
x
i −
√i
fGri (x) = 1 + i=1 4000
i=1 cos
i

xi ∈ [−600, 600]
x∗ (0, 0, . . . , 0); fGri (x∗ ) = 0
Pp
(A − Bi )2
fF le (x) =
i=1
Pp i
(aij sinαj + bij cosαj )
Ai =
j=1
Pp
Bi =
j=1 (aij sinxj + bij cosxj )
xi , αi ∈ [−π, π]; aij , bij ∈ [−100, 100]
x∗ = α; fF le (x∗ ) = 0
³
´
P
1 Pp
fLan (x) = − m
c · exp − π
(x − aij )2 ·
³ Pi=1 i
´ j=1 j
p
cos π j=1 (xj − aij )2
xi ∈ [0, 10]; m = p
x∗ = random; fLan (x∗ ) = random

Multimodal?
no

Separable?
yes

Regular?
n/a

no

no

n/a

no

no

n/a

yes

yes

n/a

yes

yes

n/a

yes

no

yes

yes

no

yes

yes

no

no

yes

no

no

Table 1: Definition of each function together with its features

The dimensionality of the search space is another important factor in the complexity
of the problem. A study of the dimensionality problem and its features was carried out by
Friedman (1994). In order to establish the same degree of difficulty in all the problems, we
have chosen a search space of dimensionality p = 30 for all the functions.
Sphere function has been used in the development of the theory of evolutionary strategies
(Rechenberg, 1973), and in the evaluation of genetic algorithms as part of the test set
proposed by De Jong (1975). Sphere, or De Jong’s function F1, is a simple and strongly
convex function. Schwefel’s double sum function was proposed by Schwefel (1995). Its main
difficulty is that its gradient is not oriented along their axis due to the epistasis among their
variables; in this way, the algorithms that use the gradient converge very slowly. Rosenbrock
function (Rosenbrock, 1960), or De Jong’s function F2, is a two dimensional function with
a deep valley with the shape of a parabola of the form x21 = x2 that leads to the global
minimum. Due to the non-linearity of the valley, many algorithms converge slowly because
they change the direction of the search repeatedly. The extended version of this function
was proposed by Spedicato (1975). Other versions have been proposed (Oren, 1974; Dixon,
1974). It is considered by many authors as a challenge for any optimization algorithm
(Schlierkamp-Voosen, 1994). Its difficulty is mainly due to the non-linear interaction among
its variables.
Rastrigin function (Rastrigin, 1974) was constructed from Sphere adding a modulator
term α · cos(2πxi ). Its contour is made up of a large number of local minima whose value
increases with the distance to the global minimum. The surface of Schwefel function (Schwefel, 1981) is composed of a great number of peaks and valleys. The function has a second
10

CIXL2: A Crossover Operator for Evolutionary Algorithms

best minimum far from the global minimum where many search algorithms are trapped.
Moreover, the global minimum is near the bounds of the domain.
Ackley, originally proposed by Ackley (1987) and generalized by Bäck (1993), has an
exponential term that covers its surface with numerous local minima. The complexity of
this function is moderated. An algorithm that only uses the gradient steepest descent
will be trapped in a local optima, but any search strategy that analyzes a wider region
will be able to cross the valley among the optima and achieve better results. In order to
obtain good results for this function, the search strategy must combine the exploratory and
exploitative components efficiently. Griewangk function (Bäck, Fogel, & Michalewicz, 1997)
has a product term that introduces interdependence among the variables. The aim is the
failure of the techniques that optimize each variable independently. As in Ackley function,
the optima of Griewangk function are regularly distributed.
The functions of Fletcher-Powell (Fletcher & Powell, 1963) and Langerman (Bersini,
Dorigo, Langerman, Seront, & Gambardella, 1996) are highly multimodal, as Ackley and
Griewangk, but they are non-symmetrical and their local optima are randomly distributed.
In this way, the objective function has no implicit symmetry advantages that might simplify
optimization for certain algorithms. Fletcher-Powel function achieves the random distribution of the optima choosing the values of the matrixes a and b, and of the vector α at
random. We have used the values provided by Bäck (1996). For Langerman function, we
have used the values of a and c referenced by Eiben and Bäck (1997b).

4. Evolutionary Algorithm
The most suitable evolutionary algorithms to solve optimization problems in continuous
domains are evolutionary strategies (Schwefel, 1981; Rechenberg, 1973), genetic algorithms
(Holland, 1975; Goldberg, 1989a) with real coding (Goldberg, 1991) and evolutionary programming (Fogel, Owens, & Walsh, 1966; Fogel, 1995). For evaluating CIXL2 we have
chosen real coded genetic algorithms, because they are search algorithms of general purpose where the crossover operator plays a central role. The general structure of the genetic
algorithm is shown in Figure 4.
Nevertheless, CIXL2 could be applied to any evolutionary algorithms with a crossover
or similar operator. On the other hand, the real codification is the most natural one in
continuous domains, each gene representing a variable of the function. In this way, the
precision of the solution only depends on the data type used to store the variables.
Our objective is the comparison of the behavior of the proposed crossover against other
crossovers. This comparison must be made in a common evolutionary framework that is
defined by the features of the genetic algorithm. For the definition of such features, we have
taken into account the previous studies on the matter. In the following paragraphs we will
describe in depth the different components of our genetic algorithm.
4.1 Structure of the Individual and Population Size
Each individual is made up of p = 30 genes, the dimensionality of the functions to optimize.
The size of the population is one of the critical parameters for many applications. If
the size of the population is too small, the algorithm could converge quickly towards suboptimal solutions; if it is too large, too much time and resources could be wasted. It is also
11

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Genetic algorithm
begin
t←0
initialize β(t)
evaluate β(t)
while (not stop-criterion) do
begin
t←t+1
select β(t) from β(t − 1)
crossover β(t)
mutate β(t)
evaluate β(t)
end
end
Figure 4: Structure of the genetic algorithm, t is the current generation.
obvious that the size of the population, together with the selective pressure, influences the
diversity of the population.
Several researches have studied these problems from different points of view. Grefenstette (1986) used a meta-genetic algorithm for controlling the parameters of another genetic
algorithm, such as population size and the selection method. Goldberg (1989b) made a theoretical analysis of the optimum population size. A study of the influence of the parameters
on the search process was carried out by Schaffer, Caruana, Eshelman and Das (1989).
Smith (1993) proposed an algorithm that adjusts the size of the population with respect to
the error probability of the selection . Another method consists of changing the size of the
population (Arabas, Michalewicz, & Mulawka, 1994) dynamically.
The size of the population is usually chosen in an interval between 50 and 500 individuals,
depending on the difficulty of the problem. As a general practice, in function optimization,
the size is in the interval [50, 100] for unimodal functions, and in the interval [100, 500]
for multimodal functions. However, several papers use a compromise size of 100 for all the
functions in order to homogenize the comparison environment. We will also use a population
size of 100 individuals like other comparative studies (Zhang & Kim, 2000; Takahashi, Kita,
& Kobayashi, 1999).
4.2 Selection
Zhang and Kim (2000) a comparative study was carried out of the performance of four
selection methods: proportional, ranking, tournament and Genitor. In contrast to other
studies that are based on an asymptotic study under more or less ideal conditions, this
paper is devoted to a practical case, the problem of machine layout. The paper analyzes
the quality of the solutions obtained in a reasonable amount of time and using mutation
and crossover operators. The study concludes that the methods of ranking and tournament
selection obtain better results than the methods of proportional and Genitor selection.
12

CIXL2: A Crossover Operator for Evolutionary Algorithms

We have chosen the binary tournament selection, against the ranking selection, used by
Zhang and Kim (2000) for two reasons:
• The complexity of the tournament selection is lower than the complexity of the ranking
selection (Bäck, 1996).
• The selective pressure is higher. This feature allows us to measure whether each
crossover is able to keep the population diversity (Goldberg & Deb, 1991).
Tournament selection runs a tournament between two individuals and selects the winner.
In order to assure that the best individuals always survive to the next generation, we use
elitism, the best individual of the population in generation t is always included in the
population in generation t + 1. It has been proved, both theoretically (Rudolph, 1994) and
empirically (Bäck, 1996; Michalewicz, 1992; Zhang & Kim, 2000), the convenience of the
use of elitism.
4.3 Population Update Model
There are different techniques for updating the population, among the most important are
the generational model and the steady-state model. In the generational model in each
generation a complete set of N new offspring individuals is created from N parents selected
from the population. In most such generational models, the tournament selection is used to
choose two parent individuals, and a crossover with pc probability and a mutation operator
con pm probability are applied to the parents.
This contrasts with the steady-state model, where one member of the population is
replaced at a time. The steady-state model selects an individual to be mutated and the
mutated individual replaces another individual of the population. For the crossover two
individuals are selected and one of the offspring replaces one individual of the population.
There are a number of different replacement strategies: replace-worst, replace a randomly
chosen member, select replacement using negative fitness.
The model that extrapolates between generational and steady-state is said to have a
generation gap G (De Jong, 1975; Jong & Sarma, 1993). Thus for a generational model,
G = 1; while for a steady-state model, G = 1/N . One of the most widely used variants of
the steady-stated genetic algorithm is the Minimal Generation Gap (MGG) model (Satoh,
Yamamura, & Kobayashi, 1996). This model takes two parents randomly from the population and generates λ children. Two individuals are selected from the parents and the
offspring: the best individual, and another individual chosen by roulette selection. These
two individuals substitute the parents in the population.
The generational model is the most frequently used in the comparative studies that use
BLX, SBX, logical crossover and fuzzy recombination. This is the reason why it will be the
model used in this paper. However, for UNDX crossover we have used the MGG model,
because UNDX and MGG are commonly used together and the generational model can
have a negative influence on the performance of UNDX.
For the parameters of the two models we have used the most commonly used in the
literature. For the generational model, we use a probability of crossover of p c = 0.6 (De
Jong, 1975; Herrera, Lozano, & Verdegay, 1998). For the MGG model we have used λ = 200,
13

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

as this is a value commonly used in papers about UNDX (Ono & Kobayashi, 1997; Ono,
Kita, & Kobayashi, 1999; Ono, Kobayashi, & Yoshida, 2000). For the mutation probability,
values in the interval pm ∈ [0.001, 0.1] are usual (De Jong, 1975; Herrera et al., 1998;
Michalewicz, 1992; Bäck, 1996). We have chosen a value of pm = 0.05 for both models.
4.4 Initialization
In a search algorithm, the initialization method is very important. In many cases the
initialization determines the success or failure of the search process. We have opted, as in
other papers (Herrera et al., 1998; De Jong, 1975; Beyer & Deb, 2001; Herrera, Lozano,
& Sánchez, 2003), for initializing the values of the genes by means of a uniform random
distribution within the domain of each variable.
4.5 Mutation
As mutation operator we have chosen the non-uniform mutation with parameter b = 5
(Michalewicz, 1992) as its dynamical nature makes it very suitable for a wide variety of
problems (Herrera & Lozano, 2000).
The individuals β m generated by this mutation are obtained as follows:
βim

=

½

βi + 4(t, bi − βi ) si τ = 0
βi − 4(t, βi − ai ) si τ = 1

(12)

being
4(t, y) = y(1 − r

(1− g

t
)b
max

)

(13)

where t is the generation, gmax is the maximum number of generations, τ is a random value,
τ ∈ {0, 1}, r is a random number in the interval [0, 1] and b is a parameter that determines
the degree of dependence of the mutation with regards to the number of iterations. Equation
13 gives values in the interval [0, y]. The probability of obtaining a value near 0 increases
as the algorithm progresses. This operator performs a uniform search in the initial stages
of the evolution, and a very localized search in the final stages.
4.6 Stop Criterion
The part of the genetic algorithm that takes up most of the time is the evaluation of the
fitness function. The number of evaluations of the fitness in each generation depends on the
operators used and the population update model. Different operators and update models
can lead to very different numbers of evaluations per generation. That is the reason why
it is common to use the number of evaluations as the stop criterion instead of the number
of generations. We have used a limit of 300,000 evaluations (Eiben, van der Hauw, & van
Hemert, 1998; De Jong & Kosters, 1998) as stop criterion. The precision of the solutions
is bounded by the precision of the data type used in the implementation of the genetic
algorithm. We have used a double precision data type of 64 bits following the specification
ANSI/IEEE STD 754-1985 (IEEE Standard for Binary Floating-Point Arithmetic). This
data type has a precision of 15 - 17 digits.
14

CIXL2: A Crossover Operator for Evolutionary Algorithms

5. Analysis of CIXL2
In this section we will perform an analysis of the crossover, and will obtain for every test
function the following information:
1. The optimal value for the confidence coefficient 1 − α of the confidence interval. The
values used are 1 − α = {0.70, 0.90, 0.95, 0.99}.
2. The optimal number of best individuals used by the crossover to calculate the confidence intervals of the mean. The values used are n = {5, 10, 30, 60, 90}.
These two factors are not independent, so we will perform an analysis using all the
possible pairs (1 − α, n) of the Cartesian product of the two sets. For each pair we will
perform 30 runs of the genetic algorithm with different random seeds. Table 2 shows the
average value and standard deviation of the 30 runs for each experiment.
The study of the results has been made by means of an analysis of variance ANOVA
II (Dunn & Clark, 1974; Miller, 1981; Snedecor & Cochran, 1980), with the fitness of the
best individuals, A, as test variable. This fitness is obtained independently in 30 runs and
depending on two fixed factors and their interaction. The fixed factors are: the confidence
coefficient C with four levels and the number of best individuals B with five levels. The
linear model has the form:

Aij = µ + Ci + Bj + CBij + eij

(14)

i = 1, 2, 3, 4; and j = 1, 2, 3, 4, 5
where:
• Ci is the effect over A of the i-th level of factor C, where C1 represents a confidence
coefficient of 0.70, C2 of 0.90, C3 of 0.95 and C4 of 0.99.
• Bj is the effect over A of the j-th level of factor B, where B1 represents a value of
n = 5, B2 of n = 10, B3 of n = 30, B4 of n = 60 and B5 of n = 90.
• CBij represents the effect of the interaction between the confidence coefficient C and
the number of best individuals B.
• µ is the global mean of the model. The variation of the experimental results from µ
is explained by the effects of the different levels of the factors of the model and their
interaction.
• eij are error variables.
The hypothesis tests try to determine the effect of each term over the fitness of the best
individuals, A. We have carried out tests for every factor and for the interaction among
the factors. This and subsequent tests are performed with a confidence level of 95%. The
coefficient R2 of the linear model tells us the percentage of variance of A that is explained
by the model.
15

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Function

n 1−α

Mean

St. Dev.

1−α

Mean

Mean

D. Tip.

1−α

Mean

St. Dev.

Sphere
fSph

5
10
30
60
90

0.70

6.365e-16
5.736e-15
3.728e-12
6.082e-10
3.838e-09

2.456e-16
2.495e-15
1.623e-12
2.499e-10
2.326e-09

0.90

4.885e-16
2.554e-15
1.446e-11
2.867e-08
4.383e-08

St. Dev. 1 − α
1.969e-16
8.934e-16
7.062e-12
1.642e-08
3.068e-08

0.95

3.553e-16
2.642e-15
2.279e-11
1.557e-07
6.840e-08

1.710e-16
1.258e-15
1.256e-11
9.911e-08
5.894e-08

0.99

1.998e-16
1.480e-15
1.248e-10
5.494e-07
1.061e-07

6.775e-17
1.032e-15
5.914e-11
6.029e-07
8.401e-08

Schwefel’s
5
double sum 10
fSchDS
30
60
90

0.70

1.995e-03
2.232e-02
8.464e-02
1.376e-01
8.048e-01

2.280e-03
2.859e-02
1.168e-01
1.202e-01
5.403e-01

0.90

8.403e-03 7.748e-03
5.407e-02 3.792e-02
3.190e-01 2.798e-01
4.059e-01 2.395e-01
2.257e+00 1.490e+00

0.95

7.662e-03
4.168e-02
2.644e-01
2.223e-01
7.048e-01

9.693e-03
4.383e-02
2.569e-01
1.384e-01
7.689e-01

0.99

1.305e-02
1.462e-02
1.223e-01
2.134e-01
2.799e-01

1.303e-02
1.422e-02
9.018e-02
1.464e-01
2.322e-01

Rosenbrock 5
fRos
10
30
60
90

0.70

2.494e+01
2.579e+01
2.611e+01
2.576e+01
2.562e+01

1.283e+00
2.044e-01
1.471e-01
1.988e-01
2.827e-01

0.90

2.506e+01
2.591e+01
2.632e+01
2.593e+01
2.570e+01

3.050e-01
1.324e-01
1.745e-01
2.292e-01
2.974e-01

0.95

2.497e+01
2.589e+01
2.642e+01
2.600e+01
2.579e+01

4.663e-01
9.426e-02
1.377e-01
4.045e-01
2.629e-01

0.99

Rastrigin
fRas

5
10
30
60
90

0.70

2.919e+00 1.809e+00
6.799e+00 2.480e+00
9.452e+00 2.434e+00
1.413e+01 4.126e+00
1.771e+01 5.063e+00

0.90

6.036e+00
1.068e+01
1.270e+01
1.837e+01
2.438e+01

2.023e+00
3.786e+00
3.522e+00
6.070e+00
7.688e+00

0.95

7.893e+00
1.297e+01
1.327e+01
1.499e+01
1.987e+01

2.450e+00
3.844e+00
4.770e+00
4.434e+00
5.637e+00

0.99

7.164e+00
1.675e+01
1.552e+01
1.691e+01
2.249e+01

2.579e+00
6.554e+00
3.664e+00
4.123e+00
6.058e+00

Schwefel
fSch

5
10
30
60
90

0.70

6.410e+02 2.544e+02
1.793e+03 4.172e+02
2.675e+03 2.592e+02
2.700e+03 1.471e+02
2.738e+03 1.476e+02

0.90

1.145e+03
1.325e+03
2.264e+03
2.513e+03
2.704e+03

5.422e+02
2.340e+02
2.758e+02
1.927e+02
1.516e+02

0.95

1.424e+03
1.486e+03
2.061e+03
2.496e+03
2.672e+03

6.837e+02
2.607e+02
2.369e+02
2.146e+02
1.349e+02

0.99

2.844e+03
2.525e+03
1.986e+03
2.169e+03
2.529e+03

4.168e+02
3.069e+02
2.424e+02
2.434e+02
1.837e+02

Ackley
fAck

5
10
30
60
90

0.70

1.378e-08
2.074e-07
8.328e-06
1.019e-04
2.518e-04

5.677e-09
9.033e-08
1.403e-06
2.396e-05
7.167e-05

0.90

6.320e-09
9.544e-08
1.483e-05
8.292e-04
7.544e-04

2.966e-09
3.422e-08
3.956e-06
2.097e-04
2.668e-04

0.95

4.677e-09
9.396e-08
2.246e-05
1.897e-03
9.571e-02

1.960e-09
3.513e-08
4.957e-06
9.190e-04
3.609e-01

0.99

5.188e-09
5.806e-08
4.976e-05
3.204e-03
1.741e-01

2.883e-09
2.683e-08
1.298e-05
1.373e-03
5.290e-01

Griewangk
fGri

5
10
30
60
90

0.70

1.525e-02
1.647e-02
2.012e-02
7.884e-03
7.391e-03

1.387e-02
1.951e-02
2.372e-02
1.061e-02
7.617e-03

0.90

2.463e-02
2.695e-02
1.819e-02
2.808e-02
5.248e-03

2.570e-02
2.713e-02
1.664e-02
9.686e-02
6.741e-03

0.95

1.574e-02
2.195e-02
2.321e-02
7.410e-03
8.938e-03

1.411e-02
2.248e-02
3.842e-02
1.321e-02
1.196e-02

0.99

1.285e-02
3.194e-02
2.254e-02
1.582e-02
1.230e-02

1.801e-02
3.680e-02
1.877e-02
2.727e-02
2.356e-02

Fletcher
fF le

5
10
30
60
90

0.70

1.523e+04
1.966e+04
2.145e+04
2.133e+04
2.432e+04

1.506e+04
1.585e+04
1.631e+04
2.110e+04
2.273e+04

0.90

2.293e+04
2.248e+04
2.129e+04
2.124e+04
2.898e+04

1.882e+04
2.300e+04
1.310e+04
1.213e+04
3.131e+04

0.95

1.286e+04 1.317e+04
1.633e+04 1.344e+04
3.049e+04 2.306e+04
2.935e+04 2.155e+04
2.918e+04 2.418e+04

0.99

1.527e+04
1.891e+04
2.492e+04
2.374e+04
3.453e+04

1.362e+04
1.612e+04
1.967e+04
1.479e+04
2.498e+04

Langerman 5
fLan
10
30
60
90

0.70

-2.064e-01
-2.339e-01
-2.124e-01
-1.975e-01
-1.599e-01

9.346e-02
1.280e-01
1.038e-01
1.405e-01
9.057e-02

0.90

-2.544e-01
-2.582e-01
-2.191e-01
-1.752e-01
-1.336e-01

1.401e-01
1.574e-01
1.100e-01
7.145e-02
6.042e-02

0.95

-3.545e-01
-2.663e-01
-1.908e-01
-1.762e-01
-1.656e-01

0.99

-2.803e-01
-2.830e-01
-2.382e-01
-1.949e-01
-1.796e-01

1.350e-01
1.645e-01
1.572e-01
9.500e-02
8.453e-02

1.802e-01
1.247e-01
9.776e-02
8.929e-02
8.336e-02

2.463e+01 1.330e+00
2.579e+01
1.609e-01
2.668e+01
9.999e-02
2.617e+01
4.787e-01
2.585e+01
3.654e-01

Table 2: Average value and standard deviation of the 30 runs for each experiment

16

CIXL2: A Crossover Operator for Evolutionary Algorithms

For determining whether there are significant differences among the various levels of
a factor, we perform a multiple comparison test of the average fitness obtained with the
different levels of each factor. First, we carry out a Levene test (Miller, 1996; Levene,
1960) for evaluating the equality of variances. If the hypothesis that the variances are
equal is accepted, we perform a Bonferroni test (Miller, 1996) for ranking the means of
each level of the factor. Our aim is to find the level of each factor whose average fitness
is significantly better than the average fitness of the rest of the levels of the factor. If
the test of Levene results in rejecting the equality of covariance matrixes, we perform a
Tamhane test (Tamhane & Dunlop, 2000) instead of a Bonferroni test. Tables 9, 12, and
13 in Appendix A show the results obtained following the above methodology.
For Sphere function, the significant levels α∗ of each term of the linear model on Table 9
show that none of the factors of the linear model has a significant effect on the model built
to explain the variance of the fitness A. This effect is due to the fact that fSph is easy to
optimize and the fitness behaves as a singular random variable with sample variance near
0. We can see in Table 2 that the best results are obtained with the pair (0.99, 5). The
multiple comparison test of Table 12 confirms that the means obtained with the value n = 5
are significatively better than the means obtained with other values. In the same way, the
average fitness for 1 − α = 0.70 is significantly the best one. The results show that, for any
value of n, the best value of 1 − α, in general, is 1 − α = 0.70. Due to the simple form of
fSph , the best parameters of the crossover show a high exploitative component with a fast
shifting towards the region of the best individuals.
For the unimodal and non-separable functions fSchDS and fRos , both factors and their
interaction are significant in the linear model that explains the sample variance of A with
a determination coefficient around 0.5. Table 2 shows that the best results are obtained
with n = 5; the Tamhane test shows that the means obtained with this value of n are
significatively better than the means obtained with other values. The results for the value
of the confidence coefficient are less conclusive. In fact, for fRos there are no significant
differences among the different values of 1 − α, although the best results are obtained with
1 − α = 0.7. For fSchDS the average fitness for µ0.99 is the best one, but without significant
differences with µ0.70 . µ0.70 together with n = 5 is the one that shows the best results. We
can conclude that the feature of non-separability of the functions does not imply a notable
change in the parameters of the crossover with respect to the parameters used for f Sph .
For fRas and fSch , which are separable and multimodal, the most adequate pair of
parameters is (0.70, 5). For fRas , the test shows that the performance of this pair is significantly better. However, for fSch , the best mean is obtained with µ5 with results that are
significantly better than these obtained with other values, with the exception of µ 10 . There
are no significant differences among µ0.70 , µ0.95 and µ90 . The three factors of the linear
model are significant with quite large determination coefficients of 0.617 for f Ras and 0.805
forfSch . This means that the factors and their interaction explain a high percentage of the
variance of the fitness A.
For fAck , the best results are obtained with the pair (0.95, 5). The Tamhane test confirms
that n = 5 is the most suitable value, while there are no significant differences among µ 0.70 ,
µ0.95 and µ0.99 . For fGri the best results are obtained with the pair (0.90, 90). The test
shows that large values of n are the most suitable for the optimization of this function.
There are no significant differences among the performance of the different values of 1 − α.
17

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

For both functions the determination coefficient of the linear model is low, showing that
the linear model does not explain the variance of the fitness. The lack of a linear relation
among n, 1 − α and the fitness makes it more difficult to determine the best value of the
parameters of the crossover.
The case of fF le and fLan is similar, as the linear model hardly gives any information
about the effect of the parameters on the fitness. The most adequate pair for the optimization of these two functions is (0.95, 5). The test shows that the best values of n are n = 5
and n = 10. On the other hand, there are no significant differences among the performance
of the crossover for the different values of 1 − α.
The overall results show that the selection of the best n = 5 individuals of the population
would suffice for obtaining a localization estimator good enough to guide the search process
even for multimodal functions where a small value of n could favor the convergence to
local optima. However, if the virtual parents have a worse fitness than the parent from the
population, the offspring is generated near the latter, and the domain can be explored in
multiple directions. In this way, the premature convergence to suboptimal virtual parents
is avoided.
However, if the best n individuals are concentrated in a local optimum the algorithm
will very likely converge to such optimum. That is the reason why in complex functions a
larger value of n may be reasonable, adding to the confidence interval individuals located in
or near different optima. As an example of this, the case of fGri for which the best results
are achieved with n = 90 and n = 60 is noteworthy.
The confidence coefficient bounds the error in the determination of the localization
parameter and is responsible for focussing the search. The multiple comparison tests show
that the value 1 − α = 0.70 is the best for 6 problems, and is, as least, no worse than the
best one in the other problems. So it can be chosen as the most adequate value of the
parameter.

6. Comparative Study of the Crossovers
Due to the large amount of different crossovers available, it is unfeasible to make a comprehensive comparison between all those crossovers and CIXL2. We have chosen those
crossovers that obtain interesting results and those whose features are similar to our crossover,
that is, which are self-adaptive and establish a balance between the exploration and the exploitation of the search space. The way in which these two features are balanced is regulated
by one or more parameters of each crossover. These parameters have been chosen following
the authors’ recommendations and the papers devoted to the comparison of the different
operators.
The crossovers used in the comparison are: BLXα (Eshelman & Schaffer, 1993) with
different degrees of exploration determined by the values α = {0.2, 0.5} (Herrera et al.,
2003); fuzzy recombination (Voigt et al., 1995); based on fuzzy connectives of the logical
family (logical crossover) (Herrera et al., 1998) using S2 strategies and λ = 0.5 (Herrera &
Lozano, 2000), SBX (Deb & Agrawal, 1995) using the values ν = {2, 5} (Deb & Beyer, 2001);
√ (Kita, Ono, & Kobayashi, 1998;
UNDX (Ono & Kobayashi, 1997) with σξ = 21 and ση = 0.35
p
Kita, 2001). For CIXL2, as we have determined in the previous study, we will use n = 5
and 1 − α = 0.70.
18

CIXL2: A Crossover Operator for Evolutionary Algorithms

Following the setup of the previous study, we performed an ANOVA II analysis and
a multiple comparison test. As might have been expected, keeping in mind the “no-free
lunch” theorem and the diversity of the functions of the test set, the tests show that there is
no crossover whose results are significatively better than the results of all other crossovers.
This does not mean that these differences could not exist for certain kinds of functions.
So, in order to determine for each kind of function whether a crossover is better than the
others, we have performed an ANOVA I analysis — where the only factor is the crossover
operator — and a multiple comparison test. Additionally, we graphically study the speed
of convergence of the RCGA with regard to the crossover operator. In order to enforce the
clearness of the graphics for each crossover, we show only the curve of the best performing
set of parameters for BLX and SBX crossovers.
Crossover
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Ext. F.
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Ext. F.
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Ext. F.
Logical
UNDX

Mean

St.Dev.
fSph
6.365e-16 2.456e-16
3.257e-16 1.396e-16
4.737e-16 4.737e-16
1.645e-12 8.874e-13
4.873e-12 3.053e-12
2.739e-15 1.880e-15
3.695e-13 1.670e-13
2.910e-05 1.473e-05
fRas
2.919e+00 1.809e+00
2.189e+00 1.417e+00
3.018e+00 1.683e+00
1.844e+01 4.417e+00
1.419e+01 3.704e+00
2.245e+01 4.914e+00
6.325e+01 1.012e+01
1.107e+02 1.242e+01
fGri
1.525e-02 1.387e-02
4.749e-02 4.579e-02
3.760e-02 2.874e-02
2.196e-02 1.874e-02
3.128e-02 2.737e-02
1.315e-03 3.470e-03
6.078e-03 6.457e-03
7.837e-02 4.438e-02

Mean
St.Dev.
fSchDS
1.995e-03 2.280e-03
1.783e-02 1.514e-02
9.332e-03 1.086e-02
2.033e-01 1.966e-01
3.933e-01 2.881e-01
3.968e+01 1.760e+01
1.099e+01 7.335e+00
2.080e+01 7.216e+00
fSch
6.410e+02 2.544e+02
3.695e+02 1.595e+02
4.200e+02 1.916e+02
1.470e+03 3.827e+02
1.104e+03 3.353e+02
3.049e+03 2.876e+02
2.629e+03 9.749e+01
8.050e+03 3.741e+02
fFle
1.523e+04 1.506e+04
1.570e+04 1.515e+04
1.802e+04 1.483e+04
3.263e+04 3.110e+04
3.333e+04 2.973e+04
1.691e+04 1.446e+04
2.718e+04 1.388e+04
3.469e+04 2.136e+04

Mean

St.Dev.
fRos
2.494e+01 1.283e+00
2.923e+01 1.723e+01
3.161e+01 2.094e+01
2.775e+01 9.178e+00
3.111e+01 1.971e+01
2.743e+01 1.394e+01
2.703e+01 8.358e-02
2.840e+01 3.606e-01
fAck
1.378e-08 5.677e-09
4.207e-08 1.713e-08
6.468e-08 1.928e-08
5.335e-06 1.453e-06
9.662e-06 2.377e-06
1.797e-07 5.823e-08
2.531e-06 7.129e-07
3.551e-02 1.224e-02
fLan
-2.064e-01 9.346e-02
-3.003e-01 1.388e-01
-3.457e-01 1.684e-01
-1.939e-01 1.086e-01
-1.866e-01 9.080e-02
-1.064e-01 5.517e-02
-7.396e-08 2.218e-07
-2.130e-01 9.116e-02

Table 3: Average values and standard deviation for the 30 runs of every crossover operator.

Table 3 shows the average values and standard deviations for the 30 runs performed for
each crossover operator. Table 10 in Appendix A shows how, for all the functions, except
fRos , the crossover operator has a significant effect on the linear model. The table also
shows that the results of the Levene test indicate the inequality of the variances of the
results of all the functions, excepting fF le . So, we use the Bonferroni test for fF le , and the
Tamhane test for all the others. The results of the multiple comparison test, the ranking
established by the tests and the significant level of the differences among the results of the
crossovers are shown on Tables 14, 15 and 16 (Appendix A). Figures 5 - 13, in Appendix
B, show, in logarithmic scale, the convergence curves for each function.
19

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

For fSph the high value of the determination coefficient shows that the linear model
explains much of the variance of the fitness. The best values are obtained with BLX(0.3),
BLX(0.5) and CIXL2, in this order. With these operators we obtain precisions around
1e-16. Figure 5 shows that CIXL2 is the fastest in convergence, but it is surpassed by BLX
in the last generations.
For fSchDS and fRos the best results are obtained with CIXL2. For fSchDS the difference
in performance with the other crossovers is statistically significant. For f Ros the differences
are significant, when CIXL2 is compared with Logical and UNDX. For f SchDS the Figure 6
shows how CIXL2 achieves a quasi-exponential convergence and a more precise final result.
For fRos , in the Figure 7 we can see how the speed of convergence of CIXL2 is the highest,
although the profile of all the crossovers is very similar with a fast initial convergence
followed by a poor evolution due to the high epistasis of the function. The differences in
the overall process are small. This fact explains that in the linear model the influence of
the factor crossover is not significant and the determination coefficient is small.
For fRas , BLX(0.3) again obtains the best results but without significant difference to the
average values obtained with CIXL2 and BLX(0.5). These three operators also obtain the
best results for fSch ; however, the tests show that there are significant differences between
CIXL2 and BLX(0.5), and that there are no differences between BLX(0.5) and BLX(0.3).
The latter obtains the best results. Figures 8 and 9 show that BLX is the best in terms of
convergence speed followed by CIXL2. The large value of R 2 means that the crossover has
a significant influence on the evolutive process.
For fAck , CIXL2 obtains significantly better results. In Figure 10 we can see how it
also converges faster. The large value of R2 means that the crossover has a significant
influence on the evolutive process. For fGri , the Fuzzy operator obtains significantly better
results. The following ones, with significant differences between them, are Logical and
CIXL2. Figure 11 shows a fast initial convergence of CIXL2, but in the end Logical and
Fuzzy obtain better results.
For fF le the best results are obtained with CIXL2, but the difference is only significant
with SBX and UNDX. Figure 12 shows that CIXL2 is the fastest in convergence, but with
a curve profile similar to BLX and Fuzzy. For fLan , the best operator is BLX(0.5), with
differences that are significant for all the other operators with the exception of BLX(0.3).
UNDX and CIXL2 are together in third place. Figure 13 shows that the behavior of all
crossovers is similar, except for the Logical crossover that converges to a value far from the
other operators.

7. Comparison with Estimation of Distribution Algorithms
EDAs are evolutionary algorithms that use, as CIXL2, the best individuals of the population
to direct the search. A comparison with this paradigm is interesting, although there are
significant differences between EDAs and RCGAs.
EDAs remove the operators of crossover and mutation. In each generation a subset of the
population is selected and the distribution of the individuals of this subset is estimated. The
individuals of the population for the next generation are obtained sampling the estimated
distribution. Although any selection method could be applied, the most common one is the
selection of the best individuals of the population.
20

CIXL2: A Crossover Operator for Evolutionary Algorithms

The first EDAs were developed for discrete spaces. Later, they were adapted to continuous domains. We can distinguish two types of EDAs, whether they take into account
dependencies between the variables or not. One of the most used among the EDAs that do
not consider dependencies is U M DAc (Univariate Marginal Distribution Algorithm for continuous domains) (Larrañaga, Etxeberria, Lozano, & Peña, 2000). In every generation and
for every variable the U M DAc carries out some statistical test in order to find the density
function that best fits the variable. Once the densities have been identified, the estimation
of parameters is performed by their maximum likelihood estimates. If all the distributions
are normal, the two parameters are the mean and the standard deviation. This particular
case will be denoted U M DAG
c (Univariate Marginal Distribution Algorithm for Gaussian
models).
Among the other type of EDAs, we can consider EGN ABGe (Estimation of Gaussian
Network Algorithm) (Larrañaga et al., 2000) whose good results in function optimization
are reported by Bengoetxea and Miquélez (2002). In each generation, EGN A BGe learns
the Gaussian network structure by using a Bayesian score that gives the same value for
Gaussian networks reflecting the same conditional dependencies are used. Next, it calculates
estimations for the parameters of the Gaussian network structure.
In the experiments we have used the parameters reported by Bengoetxea and T. Miquélez
(2002): a population of 2000 individuals, initialized using a uniform distribution, from which
a subset of the best 1000 individuals are selected to estimate the density function, and the
elitist approach was chosen (the best individual is included for the next population and 1999
individuals are simulated). Each algorithm has been run 30 times with a stop criterion of
300,000 evaluations of the fitness function.
The results of EDAs are compared with the results of a RCGA with CIXL2 of parameters
n = 5 and 1 − α = 0.70. We performed an ANOVA I analysis where the three levels of the
factor are the different algorithms: RCGA with CIXL2, U M DAc and EGN ABGe . We also
carried out a multiple comparison test.
Table 4 shows the average values and standard deviations for 30 runs for each algorithm.
Table 11 in Appendix A shows how, for all the functions excepting fAck , the type of algorithm has a significant effect over the linear model and exist inequality of the variances of
the results (Levene test). So, we have used Tamhane test for all the functions and Bonferroni test for fAck . Table 17 (Appendix A) shows the results of the multiple comparison test
and the ranking established by the test.
For fSph the results are very similar. The fitness behaves as a singular random variable
with sample variance near 0 and the statistical tests are not feasible.
For fSchDS the results of CIXL2 are significantly better than the results of U M DAc and
EGN ABGe . The same situation occurs for fRos , fRas , fSch and fAck , with the exception
that in these four functions there are no significant differences between the two EDAs. For
fGri , EGN ABGe and U M DAc achieve the best results, significantly better than CIXL2.
For fF le , U M DAc is significantly better than EGN ABGe and CIXL2, but there are no
differences between these two. For fLan , CIXL2 obtains the best results, but there are no
significant differences among the three algorithms.
The estimation of the distribution function of the best individuals of the population
performed by EDAs is an advantage in fSph , unimodal and separable, and fGri and fAck
whose optima are regularly distributed. The results of EDAs for fGri are better than
21

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

the results of CIXL2, but the results for fAck are worse. The results for fSph of all the
algorithms are very similar. For non-separable unimodal functions, such as f SchDS and fRos ,
the interdependence among their variables should favor the performance of EGN A BGe over
U M DAc and CIXL2. Nevertheless, CIXL2 achieves the best results for these two functions.
For multimodal separable functions, fRas and fSch , it is difficult to identify the distribution
of the best individuals and the performance of EDAs is below the performance of CIXL2.
For extremely complex functions, such as fF le and fLan , the results are less conclusive.
For fF le the best results are obtained with U M DAc , and there are no differences between
EGN ABGe and CIXL2. For fLan , CIXL2 achieves the best results, but the differences
among the three algorithms are not statistically significant.
EA
CIXL2
U M DAc
EGN ABGe
CIXL2
U M DAc
EGN ABGe
CIXL2
U M DAc
EGN ABGe

Mean

St.Dev.
fSph
6.365e-16 2.456e-16
1.196e-16 1.713e-17
1.077e-16 1.001e-17
fRas
2.919e+00 1.809e+00
1.576e+02 7.382e+00
1.563e+02 8.525e+00
fGri
1.525e-02 1.387e-02
9.465e-16 1.207e-16
8.200e-16 1.149e-16

Mean
St.Dev.
fSchDS
1.995e-03 2.280e-03
2.221e+01 3.900e+00
2.096e-01 1.189e-01
fSch
6.410e+02 2.544e+02
1.153e+04 9.167e+01
1.155e+04 8.754e+01
fFle
1.523e+04 1.506e+04
5.423e+03 1.562e+03
9.069e+03 7.592e+03

Mean

St.Dev.
fRos
2.494e+01 1.283e+00
2.787e+01 2.278e-02
2.785e+01 1.629e-01
fAck
1.378e-08 5.677e-09
2.478e-08 1.831e-09
2.297e-08 2.095e-09
fLan
-2.064e-01 9.346e-02
-1.734e-01 4.258e-11
-1.734e-01 1.864e-11

Table 4: Average values and standard deviation for the 30 runs of three evolutionary algorithms: RCGA with CIXL2 crossover, U M DAc and EGN ABGe .

8. Application to Artificial Intelligence
Genetic algorithms have been applied to almost any kind of problem, such as, object recognition for artificial vision (Singh, Chatterjee, & Chaudhury, 1997; Bebis, Louis, Varol, &
Yfantis, 2002), robotics path planing (Davidor, 1991; Sedighi, Ashenayi, Manikas, Wainwright, & Tai, 2004), parameter estimation (Johnson & Husbands, 1990; Ortiz-Boyer,
Hervás-Martı́nez, & Muñoz-Pérez, 2003), instance selection (Cano, Herrera, & Lozano,
2003; Kuncheva, 1995), reinforcement learning (Moriarty, Schultz, & Grefenstette, 1999),
and neural network (Miller, Todd, & Hedge, 1991; Andersen & Tsoi, 1993; Bebis, Georgiopoulos, & Kasparis, 1997) and ensemble design (Zhou, Wu, & Tang, 2002).
Real-coded genetic algorithms using CIXL2 can be applied to any of these problems
provided they are defined in a continuous domain. We have chosen an application of RCGAs
to the estimation of the weight of each network in an ensemble. This is an interesting
problem where standard methods encounter many difficulties.
8.1 Estimation of the Weights of the Networks of an Ensemble
Neural network ensembles (Perrone & Cooper, 1993) (Garcı́a-Pedrajas, Hervás-Martı́nez,
& Ortiz-Boyer, 2005) are receiving increasing attention in recent neural network research,
due to their interesting features. They are a powerful tool specially when facing complex
22

CIXL2: A Crossover Operator for Evolutionary Algorithms

problems. Network ensembles are made up of a linear combination of several networks that
have been trained using the same data, although the actual sample used by each network to
learn can be different. Each network within the ensemble has a potentially different weight
in the output of the ensemble. Several papers have shown (Perrone & Cooper, 1993) that
the network ensemble has a generalization error generally smaller than that obtained with
a single network and also that the variance of the ensemble is lesser than the variance of a
single network. The output of an ensemble, y, when an input pattern x is presented, is:
y(x) =

k
X

αi yi (x),

(15)

i=1

where yi is the output of network i, and wi is the weight associated to that network. If the
networks have more than one output, a different weight is usually assigned to each output.
The ensembles of neural networks have some of the advantages of large networks without
their problems of long training time and risk of over-fitting.
Moreover, this combination of several networks that cooperate in solving a given task
has other important advantages, such as (Liu, Yao, & Higuchi, 2000; Sharkey, 1996):
• They can perform more complex tasks than any of their subcomponents.
• They can make an overall system easier to understand and modify.
• They are more robust than a single network.
Techniques using multiple models usually consist of two independent phases: model
generation and model combination (Merz, 1999b). Once each network has been trained and
assigned a weights (model generation), there are, in a classification environment three basic
methods for combining the outputs of the networks (model combination):
1. Majority voting. Each pattern is classified into the class where the majority of networks places it (Merz, 1999b). Majority voting is effective, but is prone to fail in two
scenarios:
(a) When a subset of redundant and less accurate models comprise the majority, and
(b) When a dissenting vote is not recognized as an area of specialization for a particular model.
2. Sum of the outputs of the networks. The output of the ensemble is just the sum of
the outputs of the individual networks.
3. Winner takes all. The pattern is assigned to the class with the highest output over all
the outputs of all the networks. That is, the network with the largest outputs directly
classify the pattern, without taking into account the other networks.
The most commonly used methods for combining the networks are the majority voting
and sum of the outputs of the networks, both with a weight vector that measures the
23

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

confidence in the prediction of each network. The problem of obtaining the weight vector
α is not an easy task. Usually, the values of the weights αi are constrained:
N
X

αi = 1,

(16)

i=1

in order to help to produce estimators with lower prediction error (Leblanc & Tibshirani,
1993), although the justification of this constraint is just intuitive (Breiman, 1996). When
the method of majority voting is applied, the vote of each network is weighted before it is
counted:
F (x) = arg maxy

X

αi .

(17)

i:fi (x)=y

The problem of finding the optimal weight vector is a very complex task. The “Basic
ensemble method (BEM)”, as it is called by Perrone and Cooper (1993), consists of weighting
all the networks equally. So, having N networks, the output of the ensembles is:
F (x) =

N
1 X
fi (x).
N

(18)

i=1

Perrone and Cooper (1993) defined the Generalized Ensemble Method, which is equivalent to the Mean Square Error - Optimal Linear Combination (MSE-OLC) without a
constant term of Hashem (Hashem, 1997). The form of the output of the ensemble is:
fGEM (x) ≡

N
X

αi fi (x),

(19)

i=1

where the αi0 s are real and satisfy the constraint
by:

PN

i=1 αi

−1
j Cij
αi = P P −1 .
k
j Ckj

P

= 1. The values of αi are given

(20)

where Cij is the symmetric correlation matrix Cij ≡ E[mi (x)mj (x)], where mk (x) defines
the misfit of function k, that is the deviation from the true solution f (x), mk (x) ≡ f (x) −
fk (x). The previous methods are commonly used. Nevertheless, many other techniques
have been proposed over the last few years. Among others, there are methods based on
linear regression (Leblanc & Tibshirani, 1993), principal components analysis and leastsquare regression (Merz, 1999a), correspondence analysis (Merz, 1999b), and the use of a
validation set (Opitz & Shavlik, 1996).
In this application, we use a genetic algorithm for obtaining the weight of each component. This approach is similar to the use of a gradient descent procedure (Kivinen &
Warmuth, 1997), avoiding the problem of being trapped in local minima. The use of a
genetic algorithm has an additional advantage over the optimal linear combination, as the
former is not affected by the collinearity problem (Perrone & Cooper, 1993; Hashem, 1997).
24

CIXL2: A Crossover Operator for Evolutionary Algorithms

8.1.1 Experimental Setup
Each set of available data was divided into two subsets: 75% of the patterns were used
for learning, and the remaining 25% for testing the generalization of the networks. There
are two exceptions, Sonar and Vowel problems, as the patterns of these two problems are
prearranged in two specific subsets due to their particular features. A summary of these
data sets is shown in Table 5. No validation set was used in our experiments.
Data set
Anneal
Autos
Balance
Breast-cancer
Card
German
Glass
Heart
Hepatitis
Horse
Ionosphere
Iris
Labor
Liver
Lymphography
Pima
Promoters
Segment
Sonar
Soybean
TicTacToe
Vehicle
Vote
Vowel
Zoo

Cases
Train Test
674
224
154
51
469
156
215
71
518
172
750
250
161
53
226
76
117
38
273
91
264
87
113
37
43
14
259
86
111
37
576
192
80
26
1733 577
104
104
513
170
719
239
635
211
327
108
528
462
76
25

Classes
5
6
3
2
2
2
6
2
2
3
2
3
2
2
4
2
2
7
2
19
2
4
2
11
7

Features
C B N
6 14 18
15 4
6
4
–
–
–
3
6
6
4
5
6
3 11
9
–
–
6
3
4
6 13 –
13 2
5
33 1
–
4
–
–
8
3
5
6
–
–
–
9
6
8
–
– 57
19 –
–
60 –
–
– 16 19
–
–
9
18 –
–
– 16 –
10 –
–
1 15 –

Inputs
59
72
4
15
51
61
9
22
19
58
34
4
29
2
38
8
114
19
60
82
9
18
16
10
16

Table 5: Summary of data sets. The features of each data set can be C(continuous),
B(binary) or N(nominal). The Inputs column shows the number of inputs of
the network as it depends not only on the number of input variables but also on
their type.

These data sets cover a wide variety of problems. There are problems with different
numbers of available patterns, from 57 to 2310, different numbers of classes, from 2 to
19, different kinds of inputs, nominal, binary and continuous, and of different areas of
25

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

application, from medical diagnosis to vowel recognition. Testing our model on this wide
variety of problems can give us a clear idea of its performance. These are all the sets to
which the method has been applied.
In order to test the efficiency of the proposed crossover in a classical artificial intelligence
problem, we have used a RCGA to adjust the weight of each network within the ensemble.
Our method considers each ensemble as a chromosome and applies a RCGA to optimize
the weight of each network. The weight of each network of the ensemble is codified as a
real number. The chromosome formed in this way is subject to CIXL2 crossover and nonuniform mutation. The parameters of CIXL2 are the same used in the rest of the paper,
n = 5 and 1 − α = 0.7. The combination method used in the weighted sum of the outputs
of the networks. Nevertheless, the same genetic algorithm could be used for weighting each
network if a majority voting model is used.
The exact conditions of the experiments for each run of all the algorithms were the
following:
• The ensemble was formed by 30 networks. Each network was trained separately using
and standard back-propagation algorithm using the learning data.
• Once the 30 networks have been trained, the different methods for obtaining the
weights were applied. So, all the methods use the same ensemble of networks on each
run of the experiment. For the genetic algorithm, the fitness of each individual of the
population is the classification accuracy over the learning set.
• After obtaining the vector of weights, the generalization error of each method is evaluated using the testing data.
Tables 6 and 7 show the results in terms of accurate classification for the 25 problems.
The tables show the results using a RCGA with CIXL2, and the standard BEM and GEM
methods. In order to compare the three methods we have performed a sign test over the
win/draw/loss record of the three algorithms (Webb, 2000). These tests are shown in Table
8.
Table 8 shows the comparison statistics for the three models (Webb, 2000). For each
model we show the win/draw/loss statistic, where the first value is the number of data sets
for which col < row, the second is the number for which col = row, and the third is the
number for which col > row. The second row shows the p-value of a two-tailed sign test
on the win-loss record. The table shows that the genetic algorithm using CIXL2 is able
to outperform the two standard algorithms BEM and GEM with a 10% confidence. On
the other hand, there are no significant differences between BEM and GEM. This result
is especially interesting because we have used a comprehensive set of problems from very
different domains, different types of inputs, and different numbers of classes.

9. Conclusions and Future Work
In this paper we have proposed a crossover operator that allows the offspring to inherit
features common to the best individuals of the population. The extraction of such common
features is carried out by the determination of confidence intervals of the mean of the
26

CIXL2: A Crossover Operator for Evolutionary Algorithms

Problem
Anneal

Autos

Balance

Breast

Cancer

Card

German

Glass

Heart

Hepa.

Horse

Ionos.

Iris

CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM

Mean
0.9933
0.9879
0.9915
0.8957
0.8649
0.8740
0.9340
0.9179
0.9148
0.8575
0.8321
0.8274
0.9723
0.9678
0.9673
0.9201
0.9074
0.9049
0.8785
0.8587
0.8642
0.8509
0.8043
0.8246
0.9297
0.9089
0.9182
0.9385
0.9131
0.9179
0.8723
0.8444
0.8485
0.9635
0.9481
0.9554
1.0000
1.0000
1.0000

Learning
St.Dev.
Best
0.0046 0.9985
0.0054 0.9955
0.0054 0.9985
0.0233 0.9416
0.0211 0.9091
0.0262 0.9351
0.0067 0.9446
0.0068 0.9318
0.0101 0.9318
0.0195 0.8930
0.0287 0.8698
0.0314 0.8791
0.0021 0.9771
0.0034 0.9733
0.0034 0.9733
0.0087 0.9363
0.0088 0.9247
0.0093 0.9208
0.0080 0.8973
0.0090 0.8827
0.0099 0.8827
0.0225 0.9006
0.0246 0.8447
0.0293 0.8820
0.0216 0.9653
0.0214 0.9604
0.0239 0.9554
0.0224 0.9744
0.0253 0.9573
0.0289 0.9744
0.0174 0.9084
0.0194 0.8718
0.0207 0.8864
0.0164 0.9886
0.0171 0.9773
0.0205 0.9886
0.0000 1.0000
0.0000 1.0000
0.0000 1.0000

Worst
0.9777
0.9733
0.9777
0.8506
0.8312
0.8182
0.9232
0.9019
0.8785
0.8047
0.7395
0.7488
0.9676
0.9600
0.9581
0.9054
0.8880
0.8822
0.8653
0.8440
0.8427
0.8075
0.7578
0.7640
0.8861
0.8663
0.8663
0.8718
0.8462
0.8376
0.8315
0.7949
0.8095
0.9356
0.9167
0.9167
1.0000
1.0000
1.0000

Mean
0.9778
0.9729
0.9780
0.7261
0.7052
0.7033
0.9201
0.9158
0.9158
0.6892
0.6826
0.6817
0.9799
0.9793
0.9785
0.8574
0.8521
0.8533
0.7333
0.7355
0.7377
0.6962
0.6824
0.6855
0.8358
0.8333
0.8279
0.8702
0.8658
0.8711
0.7044
0.7000
0.7004
0.8950
0.8920
0.8958
1.0000
1.0000
1.0000

Test
St.Dev.
Best
0.0090 0.9911
0.0091 0.9911
0.0103 0.9911
0.0577 0.8235
0.0586 0.8039
0.0707 0.8039
0.0118 0.9487
0.0111 0.9423
0.0110 0.9359
0.0322 0.7465
0.0375 0.7606
0.0354 0.7324
0.0065 0.9885
0.0076 0.9943
0.0084 0.9885
0.0153 0.8895
0.0212 0.8953
0.0203 0.8953
0.0184 0.7640
0.0141 0.7600
0.0149 0.7680
0.0365 0.7736
0.0424 0.7925
0.0479 0.7736
0.0271 0.8971
0.0263 0.8824
0.0312 0.8971
0.0372 0.9211
0.0319 0.9211
0.0399 0.9474
0.0313 0.7692
0.0301 0.7582
0.0300 0.7802
0.0225 0.9195
0.0206 0.9195
0.0198 0.9310
0.0000 1.0000
0.0000 1.0000
0.0000 1.0000

Worst
0.9420
0.9464
0.9420
0.5882
0.5686
0.5294
0.8910
0.8910
0.8910
0.6338
0.6056
0.6056
0.9655
0.9655
0.9598
0.8256
0.7965
0.7965
0.7000
0.7040
0.7160
0.6038
0.6038
0.6038
0.7794
0.7794
0.7794
0.8158
0.8158
0.7895
0.6264
0.6374
0.6484
0.8276
0.8276
0.8621
1.0000
1.0000
1.0000

Table 6: Ensemble results using real-coded genetic algorithm (CIXL2), basic ensemble
method (BEM), and generalized ensemble method (GEM). For each problem
we have marked whichever CIXL2 is better (+), equal, (=), or worse (-) than
BEM/GEM.
27

+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
=
=

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Problem
Labor

Liver

Lymph

Pima

Promot.

Segment

Sonar

Soybean

TicTacToe

Vote

Vowel

Zoo

CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM

Mean
0.9651
0.9488
0.9527
0.8126
0.7799
0.7744
0.9456
0.9318
0.9306
0.7982
0.7782
0.7752
0.9496
0.9300
0.9263
0.9502
0.9339
0.9423
0.9074
0.8859
0.8907
0.9758
0.9602
0.9691
0.9913
0.9868
0.9876
0.9832
0.9793
0.9801
0.9146
0.8733
0.9157
0.9807
0.9671
0.9750

Learning
St.Dev.
Best
0.0257 1.0000
0.0283 0.9767
0.0270 0.9767
0.0175 0.8494
0.0176 0.8108
0.0198 0.8108
0.0208 0.9730
0.0242 0.9640
0.0254 0.9730
0.0073 0.8194
0.0079 0.7934
0.0089 0.7882
0.0304 1.0000
0.0357 0.9875
0.0319 0.9875
0.0030 0.9544
0.0042 0.9411
0.0044 0.9521
0.0236 0.9519
0.0266 0.9423
0.0277 0.9519
0.0114 0.9903
0.0130 0.9805
0.0157 0.9883
0.0027 0.9972
0.0020 0.9917
0.0024 0.9930
0.0055 0.9939
0.0060 0.9908
0.0062 0.9908
0.0148 0.9432
0.0179 0.9015
0.0129 0.9394
0.0175 1.0000
0.0215 1.0000
0.0203 1.0000

Worst
0.8837
0.8837
0.8837
0.7761
0.7336
0.7336
0.8919
0.8739
0.8559
0.7830
0.7535
0.7431
0.8875
0.8500
0.8625
0.9446
0.9256
0.9319
0.8654
0.8269
0.8365
0.9454
0.9240
0.9376
0.9847
0.9847
0.9847
0.9725
0.9664
0.9664
0.8845
0.8371
0.8845
0.9211
0.9079
0.9211

Mean
0.8857
0.8833
0.8833
0.6992
0.6950
0.6826
0.7847
0.7775
0.7784
0.7811
0.7885
0.7793
0.8244
0.8269
0.8218
0.9259
0.9183
0.9236
0.7849
0.7865
0.7853
0.9057
0.9039
0.9067
0.9794
0.9791
0.9792
0.9278
0.9284
0.9262
0.4925
0.4913
0.4973
0.9360
0.9307
0.9307

Test
St.Dev.
Best
0.0550 1.0000
0.0663 1.0000
0.0689 1.0000
0.0276 0.7442
0.0253 0.7442
0.0337 0.7442
0.0538 0.8649
0.0539 0.8649
0.0504 0.8378
0.0209 0.8177
0.0199 0.8177
0.0222 0.8281
0.0726 1.0000
0.0612 0.9231
0.0711 0.9615
0.0057 0.9376
0.0054 0.9341
0.0061 0.9359
0.0286 0.8462
0.0286 0.8365
0.0266 0.8462
0.0165 0.9353
0.0182 0.9353
0.0187 0.9353
0.0024 0.9874
0.0000 0.9791
0.0008 0.9833
0.0110 0.9537
0.0068 0.9444
0.0107 0.9444
0.0293 0.5606
0.0331 0.5584
0.0342 0.5541
0.0290 0.9600
0.0392 0.9600
0.0347 0.9600

Worst
0.7857
0.7143
0.7143
0.6512
0.6395
0.6047
0.6486
0.6486
0.6486
0.7292
0.7448
0.7292
0.7308
0.7308
0.6923
0.9151
0.9081
0.9116
0.7404
0.7212
0.7404
0.8706
0.8647
0.8706
0.9749
0.9791
0.9791
0.8889
0.9167
0.8981
0.4459
0.4264
0.4221
0.8800
0.8400
0.8400

Table 7: Ensemble results using real-coded genetic algorithm (CIXL2), basic ensemble
method (BEM), and generalized ensemble method (GEM). For each problem
we have marked whichever CIXL2 is better (+), equal, (=), or worse (-) than
BEM/GEM.

28

+
+
+
+
+
+
–
+
–
+
+
+
–
–
+
–
+
+
–
+
+
–
+
+

CIXL2: A Crossover Operator for Evolutionary Algorithms

Algorithm
CIXL2
BEM

BEM
19/1/5
0.0066

GEM
17/1/7
0.0639
9/4/12
0.6636

win/draw/loss
p-value
win/draw/loss
p-value

Table 8: Comparison of the three methods. Win/draw/loss record of the algorithms against
each other and p-value of the sign test.

best individuals of the population. From these confidence intervals, CIXL2 creates three
virtual parents that are used to implement a directed search towards the region of the fittest
individuals. The amplitude and speed of the search is determined by the number of best
individuals selected and the confidence coefficient.
The study carried out in order to obtain the best parameters for CIXL2 concludes that
the value of n = 5 best individuals is suitable to obtain the localization estimator to guide
the search in most of the problems tested. However, in very difficult problems, it would be
advisable to have a larger value of n to avoid the premature convergence of the evolutionary
process. The confident coefficient, 1 − α, is responsible, together with the dispersion of the
best individuals, for the modulation of the wideness of the confidence interval centered on
the localization estimator. The study results in the best value of 1 − α = 0.70. This pair of
values has an acceptable performance for all problems, although there is not an optimum
pair of values for all problems.
The comparative analysis of the crossover operators shows that CIXL2 is a good alternative to widely used crossovers such as BLXα for unimodal function such as fSph , fSchDS ,
and fRos . Noteworthy is the performance of CIXL2 in the two non-separable functions,
fSchDS and fRos , where the other crossovers have a disparate behavior.
If in unimodal functions the strategy of extracting the statistical features of localization
and dispersion of the best individuals is a guarantee of good performance, the case for
multimodal functions is quite different, and the performance of the algorithm is not assured
a priori. Nevertheless, the results obtained for this kind of functions show that CIXL2 is
always one of the best performing operators. For instance, in functions of a high complexity
such as fAck — multimodal, non-separable and regular — and fF le — multimodal, nonseparable and irregular — CIXL2 obtains the best results. This behavior reveals that
the determination of the region of the best individuals by means of confidence intervals
provides a robust methodology that, applied to crossover operator, shows an interesting
performance even in very difficult functions. In summary, we can affirm that this paper
proves that CIXL2 is a promising alternative to bear in mind, when we must choose which
crossover to use in a real-coded genetic algorithm.
EDAs have shown very good performance for unimodal and separable functions, f Sph ,
and for functions whose optima are regularly distributed, fAck and fGri . The performance
of EDAs decreases in multimodal, fRas and fSch , and epistatic functions, fSchDS and fRos .
On the other hand, CIXL2 is less sensitive to the type of function. The main reason for this
behavior may be found in the fact that CIXL2 uses the distribution information obtained
from the best individuals of the population differently. CIXL2 creates three virtual parents
29

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

from this distribution, but if the virtual parents have worse fitness than the individual
which mates, the offspring is not generated near these virtual parents. In this way, CIXL2
prevents a shifting of the population to the confidence interval if the improvement of the
performance is not significant.
The applicability of the proposed crossover to a problem of artificial neural network
ensembles shows how this model can be used for solving standard artificial intelligence
problems. RCGAs with CIXL2 can also be used in other aspects of ensemble design, such
as, selection of a subset of networks, and sampling of the training set of each network.
These promising results motivate the beginning of a new line of research geared to the
study of the distribution of the best individuals taking into account the kind of problem at
hand. We aim to propose new techniques of selection of individuals to be considered for
obtaining the confidence interval in a more reliable way. In multimodal, irregular, or with
many chaotically scattered optima functions the difficulty of obtaining the distributions
of the best individuals is enormous. In these kind of functions it would be interesting to
perform a cluster analysis of the selected best individuals and to obtain a confidence interval
for every cluster. This idea would allow the implementation of a multi-directional crossover
towards different promising regions.
On the other hand, it is likely that as the evolutive process progresses the distribution
of the best individuals changes. In such a case, it would be advisable to perform, at regular
intervals, statistical tests to determine the distribution that best reflects the features of the
best individuals on the population.
Alternatively, we are considering the construction of non-parametric confidence intervals.
In this way, we need more robust estimators of the parameters of localization and dispersion
of the genes of the best individuals. We have performed some preliminary studies using the
median and different measures of dispersion and the results are quite encouraging.
Another research line currently open is the study of the application of CIXL2 to problems of optimization with restrictions, especially in the presence of non-linearity, where
the generation of individuals in the feasible region is a big issue. The orientation of the
search based on the identification of the region of the best individuals that is implemented
by CIXL2 could favor the generation of feasible individuals. This feature would be an
interesting advantage with respect to other crossover operators.

Acknowledgments
The authors would like to acknowledge R. Moya-Sánchez for her helping in the final version
of this paper.
This work has been financed in part by the project TIC2002-04036-C05-02 of the Spanish
Inter-Ministerial Commission of Science and Technology (CICYT) and FEDER funds.

30

CIXL2: A Crossover Operator for Evolutionary Algorithms

Appendix A. Results of the Statistical Study
Function
fSph
fSchDS
fRos
fRas
fSch
fAck
fGri
fF le
fLan

α∗ C
1.000
0.000
0.005
0.000
0.000
0.095
0.149
0.410
0.040

α∗ B
1.000
0.000
0.000
0.000
0.000
0.000
0.001
0.000
0.000

α∗ CB
—–
0.000
0.006
0.000
0.000
0.019
—–
—–
0.024

R2
—–
0.601
0.526
0.617
0.805
0.083
0.040
0.054
0.159

α∗ T. Levene
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.003
0.000

Table 9: Significant levels, α∗ , of each term of the linear model, determination coefficient
R2 , and value of Levene test of the statistical analysis of CIXL2 parameters.

Function
fSph
fSchDS
fRos
fRas
fSch
fAck
fGri
fF le
fLan

α∗ Crossover
0.000
0.000
0.573
0.000
0.000
0.000
0.000
0.000
0.000

R2
0.779
0.786
0.024
0.971
0.987
0.884
0.421
0.137
0.486

α∗ Levene test
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.091
0.000

Table 10: Significance level of the crossover operator and determination coefficient R 2 of the
linear model, and value of Levene test of the comparative study of the crossovers.

Function
fSchDS
fRos
fRas
fSch
fAck
fGri
fF le
fLan

α∗ EA
0.000
0.000
0.000
0.000
1.000
0.000
0.001
0.027

R2
0.955
0.778
0.992
0.999
0.641
0.455
0.150
0.079

α∗ Levene test
0.000
0.000
0.000
0.000
1.000
0.000
0.000
0.000

Table 11: Significance level of the evolutionary algorithms and determination coefficient R 2
of the linear model, and value of Levene test of the comparative study betwen
CIXL2 and EDAs.

31

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

I

J

5

10
30
60
90
10
5
30
60
90
30
5
10
60
90
60
5
10
30
90
90
5
10
30
60
Ranking

5

10
30
60
90
10
5
30
60
90
30
5
10
60
90
60
5
10
30
90
90
5
10
30
60
Ranking

5

10
30
60
90
10
5
30
60
90
30
5
10
60
90
60
5
10
30
90
90
5
10
30
60
Ranking

α∗

µI − µJ
fSph
-2.683e-15
-4.144e-11
-1.836e-07
-5.554e-08
2.683e-15
-4.144e-11
-1.836e-07
-5.554e-08
4.144e-11
4.144e-11
-1.835e-07
-5.549e-08
1.836e-07
1.836e-07
1.835e-07
1.281e-07
5.554e-08
5.554e-08
5.549e-08
-1.281e-07
µ60 > µ90 > µ30
fRas
-5.79e+00
-6.72e+00
-1.01e+01
-1.51e+01
5.79e+00
-9.31e-01
-4.30e+00
-9.32e+00
6.72e+00
9.31e-01
-3.37e+00
-8.39e+00
1.01e+01
4.30e+00
3.37e+00
-5.02e+00
1.51e+01
9.32e+00
8.39e+00
5.02e+00
µ90 > µ60 >
µ30 >

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.003
0.000
0.000
0.000
0.003
> µ10 > µ5

0.000
0.000
0.000
0.000
0.000
0.807
0.000
0.000
0.000
0.807
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
µ10 > µ5
µ5

fGri
-7.207E-03
0.174
-3.896E-03
0.864
2.329E-03
1.000
8.649E-03
0.001
7.207E-03
0.174
3.311E-03
0.983
9.535E-03
0.533
1.586E-02
0.000
3.896E-03
0.864
-3.311E-03
0.983
6.225E-03
0.930
1.254E-02
0.000
-2.329E-03
1.000
-9.535E-03
0.533
-6.225E-03
0.930
6.320E-03
0.884
-8.649E-03
0.001
-1.586E-02
0.000
-1.254E-02
0.000
-6.320E-03
0.884
µ60 ≥ µ90
µ5 > µ90
µ10 > µ90
µ30 > µ90

µI − µJ

α∗

fSchDS
-2.540e-02
0.000
-1.899e-01
0.000
-2.371e-01
0.000
-1.004e+00
0.000
2.540e-02
0.000
-1.645e-01
0.000
-2.117e-01
0.000
-9.785e-01
0.000
1.899e-01
0.000
1.645e-01
0.000
-4.720e-02
0.572
-8.140e-01
0.000
2.371e-01
0.000
2.117e-01
0.000
4.720e-02
0.572
-7.668e-01
0.000
1.004e+00
0.000
9.785e-01
0.000
8.140e-01
0.000
7.668e-01
0.000
µ90 > µ30 > µ10 > µ5
µ60 > µ5
fSch
-2.691e+02
0.082
-7.338e+02
0.000
-9.559e+02
0.000
-1.148e+03
0.000
2.691e+02
0.082
-4.647e+02
0.000
-6.868e+02
0.000
-8.786e+02
0.000
7.338e+02
0.000
4.647e+02
0.000
-2.221e+02
0.000
-4.139e+02
0.000
9.559e+02
0.000
6.868e+02
0.000
2.221e+02
0.000
-1.918e+02
0.000
1.148e+03
0.000
8.786e+02
0.000
4.139e+02
0.000
1.918e+02
0.000
µ90 > µ60 > µ30 > µ5
µ10 ≥ µ5
fFle
-2.776e+03
0.885
-7.968e+03
0.004
-7.342e+03
0.008
-1.268e+04
0.000
2.776e+03
0.885
-5.192e+03
0.234
-4.566e+03
0.378
-9.899e+03
0.006
7.968e+03
0.004
5.192e+03
0.234
6.254e+02
1.000
-4.707e+03
0.678
7.342e+03
0.008
4.566e+03
0.378
-6.254e+02
1.000
-5.333e+03
0.491
1.268e+04
0.000
9.899e+03
0.006
4.707e+03
0.678
5.333e+03
0.491
µ10 ≥ µ5
µ30 > µ5
µ60 > µ5
µ90 > µ5

µI − µJ
fRos
-9.433e-01
-1.486e+00
-1.058e+00
-8.375e-01
9.433e-01
-5.425e-01
-1.142e-01
1.058e-01
1.486e+00
5.425e-01
4.283e-01
6.483e-01
1.058e+00
1.142e-01
-4.283e-01
2.200e-01
8.375e-01
-1.058e-01
-6.483e-01
-2.200e-01
µ30 > µ60 > µ10

α∗
0.000
0.000
0.000
0.000
0.000
0.000
0.025
0.014
0.000
0.000
0.000
0.000
0.000
0.025
0.000
0.000
0.000
0.014
0.000
0.000
> µ90 > µ5

fAck
-1.063e-07
0.000
-2.384e-05
0.000
-1.508e-03
0.000
-6.769e-02
0.216
1.063e-07
0.000
-2.373e-05
0.000
-1.508e-03
0.000
-6.769e-02
0.216
2.384e-05
0.000
2.373e-05
0.000
-1.484e-03
0.000
-6.767e-02
0.216
1.508e-03
0.000
1.508e-03
0.000
1.484e-03
0.000
-6.619e-02
0.242
6.769e-02
0.216
6.769e-02
0.216
6.767e-02
0.216
6.619e-02
0.242
µ60 > µ30 > µ10 > µ5
µ90 ≥ µ5
fLan
-1.354e-02
0.998
-5.881e-02
0.009
-8.794e-02
0.000
-1.142e-01
0.000
1.354e-02
0.998
-4.527e-02
0.082
-7.440e-02
0.000
-1.007e-01
0.000
5.881e-02
0.009
4.527e-02
0.082
-2.913e-02
0.354
-5.540e-02
0.000
8.794e-02
0.000
7.440e-02
0.000
2.913e-02
0.354
-2.627e-02
0.247
1.142e-01
0.000
1.007e-01
0.000
5.540e-02
0.000
2.627e-02
0.247
µ10 ≥ µ5
µ30 > µ5
µ60 > µ5
µ90 > µ5

Table 12: Results for all the functions of the multiple comparison test and the ranking
obtained depending on the number of best individuals n.

32

CIXL2: A Crossover Operator for Evolutionary Algorithms

I

J

0.70

0.90
0.95
0.99
0.90
0.70
0.95
0.99
0.95
0.70
0.90
0.99
0.99
0.70
0.90
0.95
Ranking

µ I − µJ
fSph
-1.361e-08
-4.394e-08
-1.302e-07
1.361e-08
-3.033e-08
-1.166e-07
4.394e-08
3.033e-08
-8.628e-08
1.302e-07
1.166e-07
8.628e-08

α∗
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.019
0.000
0.000
0.019

µ0.99 > µ0.95 > µ0.90 > µ0.70

µI − µ J
fSchDS
-3.985e-01
-3.783e-02
8.165e-02
3.985e-01
3.607e-01
4.802e-01
3.783e-02
-3.607e-01
1.195e-01
-8.165e-02
-4.802e-01
-1.195e-01

α∗
0.000
0.967
0.114
0.000
0.001
0.000
0.967
0.001
0.013
0.114
0.000
0.013

µ0.90 > µ0.95 > µ0.99

µI − µ J
fRos
-1.360e-01
-1.693e-01
-1.813e-01
1.360e-01
-3.333e-02
-4.533e-02
1.693e-01
3.333e-02
-1.200e-02
1.813e-01
4.533e-02
1.200e-02

α∗
0.281
0.131
0.310
0.281
0.995
0.996
0.131
0.995
1.000
0.310
0.996
1.000

µ0.99 ≥ µ0.95 ≥ µ0.90 ≥ µ0.70

µ0.70 ≥ µ0.99

0.70

0.90
0.95
0.99
0.90
0.70
0.95
0.99
0.95
0.70
0.90
0.99
0.99
0.70
0.90
0.95
Ranking

0.70

0.90
0.95
0.99
0.90
0.70
0.95
0.99
0.95
0.70
0.90
0.99
0.99
0.70
0.90
0.95
Ranking

fRas
-4.23e+00
-3.59e+00
-5.56e+00
4.23e+00
6.40e-01
-1.33e+00
3.59e+00
-6.40e-01
-1.97e+00
5.56e+00
1.33e+00
1.97e+00

0.000
0.000
0.000
0.000
0.966
0.551
0.000
0.966
0.044
0.000
0.551
0.044

µ0.99 > µ0.95 > µ0.70
µ0.90 > µ0.70

fGri
-7.196E-03
-2.027E-03
-5.667E-03
7.196E-03
5.168E-03
1.529E-03
2.027E-03
-5.168E-03
-3.640E-03
5.667E-03
-1.529E-03
3.640E-03

0.395
0.945
0.155
0.395
0.791
1.000
0.945
0.791
0.747
0.155
1.000
0.747

µ0.90 ≥ µ0.99 ≥ µ0.95 ≥ µ0.70

fSch
1.198e+02
8.247e+01
-3.008e+02
-1.198e+02
-3.736e+01
-4.206e+02
-8.247e+01
3.736e+01
-3.833e+02
3.008e+02
4.206e+02
3.833e+02

0.714
0.919
0.001
0.714
0.997
0.000
0.919
0.997
0.000
0.001
0.000
0.000

µ0.70 ≥ µ0.95 ≥ µ0.90
µ0.99 > µ0.90

fFle
-2.986e+03
-3.241e+03
-3.079e+03
2.986e+03
-2.547e+02
-9.255e+01
3.241e+03
2.547e+02
1.622e+02
3.079e+03
9.255e+01
-1.622e+02

0.717
0.635
0.644
0.717
1.000
1.000
0.635
1.000
1.000
0.644
1.000
1.000

µ0.95 ≥ µ0.99 ≥ µ0.90 ≥ µ0.70

fAck
-2.471e-04
-1.944e-02
-3.541e-02
2.471e-04
-1.919e-02
-3.516e-02
1.944e-02
1.919e-02
-1.597e-02
3.541e-02
3.516e-02
1.597e-02

0.000
0.617
0.382
0.000
0.631
0.390
0.617
0.631
0.985
0.382
0.390
0.985

µ0.99 ≥ µ0.95 ≥ µ0.70
µ0.90 > µ0.70

fLan
6.105e-03
2.867e-02
3.309e-02
-6.105e-03
2.257e-02
2.698e-02
-2.867e-02
-2.257e-02
4.415e-03
-3.309e-02
-2.698e-02
-4.415e-03

0.998
0.272
0.133
0.998
0.585
0.363
0.272
0.585
1.000
0.133
0.363
1.000

µ0.70 ≥ µ0.90 ≥ µ0.95 ≥ µ0.99

Table 13: Results for all the functions of the multiple comparison test and the ranking
obtained depending on the confidence coefficient 1 − α.

33

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

I

Crossover
J

CIXL2

BLX(0.3)

BLX(0.5)

SBX(2)

SBX(5)

Fuzzy

Logical

UNDX

Function
fSph
fSchDS
fRos

BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical

fSph
µI − µJ
3.109e-16
1.628e-16
-1.644e-12
-4.873e-12
-2.102e-15
-3.689e-13
-2.910e-05
-3.109e-16
-1.480e-16
-1.644e-12
-4.873e-12
-2.413e-15
-3.692e-13
-2.910e-05
-1.628e-16
1.480e-16
-1.644e-12
-4.873e-12
-2.265e-15
-3.690e-13
-2.910e-05
1.644e-12
1.644e-12
1.644e-12
-3.229e-12
1.642e-12
1.275e-12
-2.910e-05
4.873e-12
4.873e-12
4.873e-12
3.229e-12
4.871e-12
4.504e-12
-2.910e-05
2.102e-15
2.413e-15
2.265e-15
-1.642e-12
-4.871e-12
-3.668e-13
-2.910e-05
3.689e-13
3.692e-13
3.690e-13
-1.275e-12
-4.504e-12
3.668e-13
-2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05

fSchDS
µI − µJ

α∗

α∗

fRos
µI − µJ

α∗

0.000
0.212
0.000
0.000
0.000
0.000
0.000
0.000
0.074
0.000
0.000
0.000
0.000
0.000
0.212
0.074
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

-1.583e-02
0.000
-4.283e+00
0.997
-7.337e-03
0.028
-6.667e+00
0.933
-2.014e-01
0.000
-2.809e+00
0.958
-3.913e-01
0.000
-6.165e+00
0.944
-3.968e+01
0.000
-2.487e+00
1.000
-1.098e+01
0.000
-2.092e+00
0.000
-2.080e+01
0.000
-3.460e+00
0.000
1.583e-02
0.000
4.283e+00
0.997
8.495e-03
0.357
-2.384e+00
1.000
-1.855e-01
0.000
1.473e+00
1.000
-3.755e-01
0.000
-1.882e+00
1.000
-3.966e+01
0.000
1.796e+00
1.000
-1.097e+01
0.000
2.191e+00
1.000
-2.078e+01
0.000
8.225e-01
1.000
7.337e-03
0.028
6.667e+00
0.933
-8.495e-03
0.357
2.384e+00
1.000
-1.940e-01
0.000
3.857e+00
1.000
-3.840e-01
0.000
5.019e-01
1.000
-3.967e+01
0.000
4.179e+00
1.000
-1.098e+01
0.000
4.575e+00
1.000
-2.079e+01
0.000
3.206e+00
1.000
2.014e-01
0.000
2.809e+00
0.958
1.855e-01
0.000
-1.473e+00
1.000
1.940e-01
0.000
-3.857e+00
1.000
-1.900e-01
0.115
-3.355e+00
1.000
-3.948e+01
0.000
3.222e-01
1.000
-1.078e+01
0.000
7.179e-01
1.000
-2.060e+01
0.000
-6.508e-01
1.000
3.913e-01
0.000
6.165e+00
0.944
3.755e-01
0.000
1.882e+00
1.000
3.840e-01
0.000
-5.019e-01
1.000
1.900e-01
0.115
3.355e+00
1.000
-3.929e+01
0.000
3.678e+00
1.000
-1.059e+01
0.000
4.073e+00
1.000
-2.041e+01
0.000
2.705e+00
1.000
3.968e+01
0.000
2.487e+00
1.000
3.966e+01
0.000
-1.796e+00
1.000
3.967e+01
0.000
-4.179e+00
1.000
3.948e+01
0.000
-3.222e-01
1.000
3.929e+01
0.000
-3.678e+00
1.000
2.870e+01
0.000
3.957e-01
1.000
1.888e+01
0.000
-9.730e-01
1.000
1.098e+01
0.000
2.092e+00
0.000
1.097e+01
0.000
-2.191e+00
1.000
1.098e+01
0.000
-4.575e+00
1.000
1.078e+01
0.000
-7.179e-01
1.000
1.059e+01
0.000
-4.073e+00
1.000
-2.870e+01
0.000
-3.957e-01
1.000
-9.812e+00
0.000
-1.369e+00
0.000
2.080e+01
0.000
3.460e+00
0.000
2.078e+01
0.000
-8.225e-01
1.000
2.079e+01
0.000
-3.206e+00
1.000
2.060e+01
0.000
6.508e-01
1.000
2.041e+01
0.000
-2.705e+00
1.000
-1.888e+01
0.000
9.730e-01
1.000
9.812e+00
0.000
1.369e+00
0.000
Ranking
µU N DX > µSBX(5) > µSBX(2) > µLogical > µExt.F. > µCIXL2 ≥ µBLX(0.5) ≥ µBLX(0.3)
µExt.F. > µU N DX > µLogical > µSBX(5) ≥ µSBX(2) > µBLX(0.3) ≥ µBLX(0.5) > µCIXL2
µBLX(0.5) ≥ µSBX(5) ≥ µBLX(0.3) ≥ µU N DX ≥ µSBX(2) ≥ µExt.F. ≥ µLogical > µCIXL2

Table 14: Results of the multiple comparison tests for fSph , fSchDS y fRos functions and
the ranking established by the test regarding the crossover operator.

34

CIXL2: A Crossover Operator for Evolutionary Algorithms

I

Crossover
J

CIXL2

BLX(0.3)

BLX(0.5)

SBX(2)

SBX(5)

Fuzzy

Logical

UNDX

Function
fRas
fSch
fAck

BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical

fRas
µI − µJ
7.296e-01
-9.950e-02
-1.552e+01
-1.128e+01
-1.953e+01
-6.033e+01
-1.078e+02
-7.296e-01
-8.291e-01
-1.625e+01
-1.201e+01
-2.026e+01
-6.106e+01
-1.085e+02
9.950e-02
8.291e-01
-1.542e+01
-1.118e+01
-1.943e+01
-6.023e+01
-1.077e+02
1.552e+01
1.625e+01
1.542e+01
4.245e+00
-4.013e+00
-4.481e+01
-9.227e+01
1.128e+01
1.201e+01
1.118e+01
-4.245e+00
-8.258e+00
-4.905e+01
-9.651e+01
1.953e+01
2.026e+01
1.943e+01
4.013e+00
8.258e+00
-4.079e+01
-8.826e+01
6.033e+01
6.106e+01
6.023e+01
4.481e+01
4.905e+01
4.079e+01
-4.746e+01
1.078e+02
1.085e+02
1.077e+02
9.227e+01
9.651e+01
8.826e+01
4.746e+01

fSch
µI − µJ

α∗

α∗

fAck
µI − µJ

α∗

0.923
1.000
0.000
0.000
0.000
0.000
0.000
0.923
0.713
0.000
0.000
0.000
0.000
0.000
1.000
0.713
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.005
0.042
0.000
0.000
0.000
0.000
0.000
0.005
0.000
0.000
0.000
0.000
0.000
0.000
0.042
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

2.715e+02
0.000
-2.830e-08
0.000
2.210e+02
0.010
-5.090e-08
0.000
-8.287e+02
0.000
-5.322e-06
0.000
-4.631e+02
0.000
-9.649e-06
0.000
-2.408e+03
0.000
-1.659e-07
0.000
-1.988e+03
0.000
-2.517e-06
0.000
-7.409e+03
0.000
-3.550e-02
0.000
-2.715e+02
0.000
2.830e-08
0.000
-5.050e+01
1.000
-2.261e-08
0.000
-1.100e+03
0.000
-5.293e-06
0.000
-7.346e+02
0.000
-9.620e-06
0.000
-2.680e+03
0.000
-1.376e-07
0.000
-2.260e+03
0.000
-2.488e-06
0.000
-7.680e+03
0.000
-3.550e-02
0.000
-2.210e+02
0.010
5.090e-08
0.000
5.050e+01
1.000
2.261e-08
0.000
-1.050e+03
0.000
-5.271e-06
0.000
-6.841e+02
0.000
-9.598e-06
0.000
-2.629e+03
0.000
-1.150e-07
0.000
-2.209e+03
0.000
-2.466e-06
0.000
-7.630e+03
0.000
-3.550e-02
0.000
8.287e+02
0.000
5.322e-06
0.000
1.100e+03
0.000
5.293e-06
0.000
1.050e+03
0.000
5.271e-06
0.000
3.655e+02
0.006
-4.327e-06
0.000
-1.579e+03
0.000
5.156e-06
0.000
-1.159e+03
0.000
2.805e-06
0.000
-6.580e+03
0.000
-3.550e-02
0.000
4.631e+02
0.000
9.649e-06
0.000
7.346e+02
0.000
9.620e-06
0.000
6.841e+02
0.000
9.598e-06
0.000
-3.655e+02
0.006
4.327e-06
0.000
-1.945e+03
0.000
9.483e-06
0.000
-1.525e+03
0.000
7.132e-06
0.000
-6.946e+03
0.000
-3.550e-02
0.000
2.408e+03
0.000
1.659e-07
0.000
2.680e+03
0.000
1.376e-07
0.000
2.629e+03
0.000
1.150e-07
0.000
1.579e+03
0.000
-5.156e-06
0.000
1.945e+03
0.000
-9.483e-06
0.000
4.199e+02
0.000
-2.351e-06
0.000
-5.001e+03
0.000
-3.550e-02
0.000
1.988e+03
0.000
2.517e-06
0.000
2.260e+03
0.000
2.488e-06
0.000
2.209e+03
0.000
2.466e-06
0.000
1.159e+03
0.000
-2.805e-06
0.000
1.525e+03
0.000
-7.132e-06
0.000
-4.199e+02
0.000
2.351e-06
0.000
-5.421e+03
0.000
-3.550e-02
0.000
7.409e+03
0.000
3.550e-02
0.000
7.680e+03
0.000
3.550e-02
0.000
7.630e+03
0.000
3.550e-02
0.000
6.580e+03
0.000
3.550e-02
0.000
6.946e+03
0.000
3.550e-02
0.000
5.001e+03
0.000
3.550e-02
0.000
5.421e+03
0.000
3.550e-02
0.000
Ranking
µU N DX > µLogical > µExt.F. > µSBX(2) > µSBX(5) > µBLX(0.5) ≥ µCIXL2 ≥ µBLX(0.3)
µU N DX > µExt.F. > µLogical > µSBX(2) > µSBX(5) > µCIXL2 > µBLX(0.5) ≥ µBLX(0.3)
µU N DX > µSBX(5) > µSBX(2) > µLogical > µExt.F. > µBLX(0.5) > µBLX(0.3) > µCIXL2

Table 15: Results of the multiple comparison tests for fRas , fSch and fAck functions and
the ranking established by the test regarding the crossover operator.

35

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

I

Crossover
J

CIXL2

BLX(0.3)

BLX(0.5)

SBX(2)

SBX(5)

Fuzzy

Logical

UNDX

Function
fGri
fF le
fLan

BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical

fGri
µI − µJ
-3.224e-02
-2.235e-02
-6.710e-03
-1.603e-02
1.394e-02
9.173e-03
-6.312e-02
3.224e-02
9.893e-03
2.553e-02
1.621e-02
4.618e-02
4.142e-02
-3.088e-02
2.235e-02
-9.893e-03
1.564e-02
6.320e-03
3.629e-02
3.152e-02
-4.077e-02
6.710e-03
-2.553e-02
-1.564e-02
-9.320e-03
2.065e-02
1.588e-02
-5.641e-02
1.603e-02
-1.621e-02
-6.320e-03
9.320e-03
2.997e-02
2.520e-02
-4.709e-02
-1.394e-02
-4.618e-02
-3.629e-02
-2.065e-02
-2.997e-02
-4.763e-03
-7.706e-02
-9.173e-03
-4.142e-02
-3.152e-02
-1.588e-02
-2.520e-02
4.763e-03
-7.229e-02
6.312e-02
3.088e-02
4.077e-02
5.641e-02
4.709e-02
7.706e-02
7.229e-02

fFle
µI − µJ

α∗

α∗

fLan
µI − µJ

α∗

0.021
0.012
0.973
0.167
0.000
0.057
0.000
0.021
1.000
0.188
0.952
0.000
0.001
0.252
0.012
1.000
0.361
1.000
0.000
0.000
0.003
0.973
0.188
0.361
0.980
0.000
0.003
0.000
0.167
0.952
1.000
0.980
0.000
0.001
0.000
0.000
0.000
0.000
0.000
0.000
0.025
0.000
0.057
0.001
0.000
0.003
0.001
0.025
0.000
0.000
0.252
0.003
0.000
0.000
0.000
0.000

-4.779e+02
1.000
9.384e-02
0.091
-2.789e+03
1.000
1.392e-01
0.007
-1.740e+04
0.034
-1.253e-02
1.000
-1.810e+04
0.022
-1.982e-02
1.000
-1.686e+03
1.000
-1.000e-01
0.000
-1.196e+04
0.709
-2.064e-01
0.000
-1.947e+04
0.009
6.557e-03
1.000
4.779e+02
1.000
-9.384e-02
0.091
-2.311e+03
1.000
4.540e-02
1.000
-1.693e+04
0.046
-1.064e-01
0.046
-1.763e+04
0.029
-1.137e-01
0.013
-1.208e+03
1.000
-1.938e-01
0.000
-1.148e+04
0.888
-3.003e-01
0.000
-1.899e+04
0.012
-8.728e-02
0.151
2.789e+03
1.000
-1.392e-01
0.007
2.311e+03
1.000
-4.540e-02
1.000
-1.461e+04
0.179
-1.518e-01
0.004
-1.531e+04
0.121
-1.591e-01
0.001
1.104e+03
1.000
-2.392e-01
0.000
-9.169e+03
1.000
-3.457e-01
0.000
-1.668e+04
0.054
-1.327e-01
0.012
1.740e+04
0.034
1.253e-02
1.000
1.693e+04
0.046
1.064e-01
0.046
1.461e+04
0.179
1.518e-01
0.004
-7.002e+02
1.000
-7.285e-03
1.000
1.572e+04
0.095
-8.747e-02
0.008
5.446e+03
1.000
-1.939e-01
0.000
-2.061e+03
1.000
1.909e-02
1.000
1.810e+04
0.022
1.982e-02
1.000
1.763e+04
0.029
1.137e-01
0.013
1.531e+04
0.121
1.591e-01
0.001
7.002e+02
1.000
7.285e-03
1.000
1.642e+04
0.063
-8.018e-02
0.004
6.146e+03
1.000
-1.866e-01
0.000
-1.361e+03
1.000
2.637e-02
1.000
1.686e+03
1.000
1.000e-01
0.000
1.208e+03
1.000
1.938e-01
0.000
-1.104e+03
1.000
2.392e-01
0.000
-1.572e+04
0.095
8.747e-02
0.008
-1.642e+04
0.063
8.018e-02
0.004
-1.027e+04
1.000
-1.064e-01
0.000
-1.778e+04
0.027
1.066e-01
0.000
1.196e+04
0.709
2.064e-01
0.000
1.148e+04
0.888
3.003e-01
0.000
9.169e+03
1.000
3.457e-01
0.000
-5.446e+03
1.000
1.939e-01
0.000
-6.146e+03
1.000
1.866e-01
0.000
1.027e+04
1.000
1.064e-01
0.000
-7.507e+03
1.000
2.130e-01
0.000
1.947e+04
0.009
-6.557e-03
1.000
1.899e+04
0.012
8.728e-02
0.151
1.668e+04
0.054
1.327e-01
0.012
2.061e+03
1.000
-1.909e-02
1.000
1.361e+03
1.000
-2.637e-02
1.000
1.778e+04
0.027
-1.066e-01
0.000
7.507e+03
1.000
-2.130e-01
0.000
Ranking
µU N DX ≥ µBLX(0.3) ≥ µBLX(0.5) ≥ µSBX(5) ≥ µSBX(2) ≥ µCIXL2 ≥ µLogical > µExt.F.
µU N DX ≥ µSBX(5) ≥ µSBX(2) ≥ µLogical ≥ µBLX(0.5) ≥ µExt.F. ≥ µBLX(0.3) ≥ µCIXL2
µLogical > µExt.F. > µSBX(5) ≥ µSBX(2) ≥ µCIXL2 ≥ µU N DX ≥ µBLX(0.3) ≥ µBLX(0.5)

Table 16: Results of the multiple comparison tests for fGri , fF le and fLan functions and
the ranking established by the test regarding the crossover operator.

36

CIXL2: A Crossover Operator for Evolutionary Algorithms

I

J

CIXL2

U M DAc
EGN ABGe
CIXL2
EGN ABGe
CIXL2
U M DAc

U M DAc
EGN ABGe
Function
fSchDS
fRos
fRas
fSch
CIXL2
U M DAc
EGN ABGe
Function
fAck
fGri
fF le
fLan

µ I − µJ
α∗
µI − µ J
α∗
µI − µ J
fSchDS
fRos
fRas
-2.221e+01 0.000 -2.928e+00 0.000 -1.547e+02
-2.076e-01 0.000 -2.906e+00 0.000 -1.533e+02
2.221e+01 0.000 2.928e+00 0.000 1.547e+02
2.200e+01 0.000 2.207e-02 0.856 1.360e+00
2.076e-01 0.000 2.906e+00 0.000 1.533e+02
-2.200e+01 0.000 -2.207e-02 0.856 -1.360e+00
Ranking
µU M DAc > µEGN A

BGe

α∗

α∗

µI − µ J
fSch
-1.089e+04
-1.091e+04
1.089e+04
-2.390e+01
1.091e+04
2.390e+01

0.000
0.000
0.000
0.677
0.000
0.677

fLan
0.004
-3.306e-02
0.150
-3.306e-02
0.004
3.306e-02
0.049 1.33781e-11
0.150
3.306e-02
0.049 -1.33781e-11

0.176
0.176
0.176
0.325
0.176
0.325

0.000
0.000
0.000
0.888
0.000
0.888

> µCIXL2

µU M DAc ≥ µEGN A
> µCIXL2
BGe
µU M DAc ≥ µEGN A
> µCIXL2
BGe

µEGN A

U M DAc
EGN ABGe
CIXL2
EGN ABGe
CIXL2
U M DAc

fAck
-1.101e-08
-9.194e-09
1.101e-08
1.817e-09
9.194e-09
-1.817e-09

0.000
0.000
0.000
0.175
0.000
0.175

BGe

fGri
1.525e-02
1.525e-02
-1.525e-02
1.266e-16
-1.525e-02
-1.266e-16

≥ µU M DAc > µCIXL2

fFle
0.000 9.803e+03
0.000 6.157e+03
0.000 -9.803e+03
0.000 -3.646e+03
0.000 -6.157e+03
0.000 3.646e+03
Ranking

µU M DAc ≥ µEGN A

BGe

> µCIXL2

µCIXL2 > µU M DAc > µEGN A

BGe

µCIXL2 ≥ µEGN A
> µU M DAc
BGe
µU M DAc ≥ µEGN A
≥ µCIXL2
BGe

Table 17: Results for all the functions of the multiple comparison test and the ranking
obtained depending on the evolutionary algorithm.

37

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Appendix B. Convergence Graphics

Average fitness of the best individual in 30 runs

100

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

1
0.01
0.0001
1e-06
1e-08
1e-10
1e-12
1e-14
1e-16

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 5: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fSph .

Average fitness of the best individual in 30 runs

100000

CIXL2(0.70,5)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX

10000

1000

100

10

1

0.1

0.01

0.001

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 6: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fSchDS .

38

CIXL2: A Crossover Operator for Evolutionary Algorithms

Average fitness of the best individual in 30 runs

1000

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

100

10

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 7: Evolution of the averaged fitness, in logarithmic scale, using different crossover
operators for the function fRos .

Average fitness of the best individual in 30 runs

1000

CIXL2(0.70,5)
BLX(0.3)
SBX(5)
Fuzzy
Logical
UNDX

100

10

1

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 8: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fRas .

39

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Average fitness of the best individual in 30 runs

100000

CIXL2(0.70,5)
BLX(0.3)
SBX(5)
Fuzzy
Logical
UNDX

10000

1000

100

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 9: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fSch .

Average fitness of the best individual in 30 runs

100

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

1

0.01

0.0001

1e-06

1e-08

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 10: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fAck .

40

CIXL2: A Crossover Operator for Evolutionary Algorithms

Average fitness of the best individual in 30 runs

100

CIXL2(0.70,5)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX

10

1

0.1

0.01

0.001

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 11: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fGri .

Average fitness of the best individual in 30 runs

1e+07

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

1e+06

100000

10000

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 12: Evolution of the average, in logarithmic scale, using different crossover operators
for the function fF le .

41

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Average fitness of the best individual in 30 runs

1

0.01

0.0001

1e-06

1e-08

1e-10
CIXL2(0.70,5)
BLX(0.3)
SBX(5)
Fuzzy
Logical
UNDX

1e-12

1e-14

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 13: Evolution of the average fitness, in logarithmic scale, using different crossover
operators for the function fLan .

42

CIXL2: A Crossover Operator for Evolutionary Algorithms

References
Ackley, D. (1987). An empirical study of bit vector function optimizacion. Genetic Algorithms and Simulated Annealing, 170–215.
Andersen, H. C., & Tsoi, A. C. (1993). A constructive algorithm for the training of a
multilayer pereptron based on the genetic algorithm. Complex Systems, 7 (4), 249–
268.
Arabas, J., Michalewicz, Z., & Mulawka, J. (1994). Gavaps - a genetic algorithm with
varying population size. In Michalewicz, Z., Krawczyk, J., Kazemi, M., & Janikow,
C. (Eds.), First IEEE International Conference on Evolutionary Computation, Vol. 1,
pp. 73–78, Orlando. IEEE Service Center, Piscataway, NJ.
Bebis, G., Georgiopoulos, M., & Kasparis, T. (1997). Coupling weight elimination with genetic algorithms to reduce network size and preserve generalization. Neurocomputing,
17, 167–194.
Bebis, G., Louis, S., Varol, Y., & Yfantis, A. (2002). Genetic object recognition using
combinations of views. IEEE Transactions on Evolutionary Computation, 6 (2), 132.
Bengoetxea, E., & Miquélez, T. (2002). Estimation of distribution algorithms: A new tool for
evolutionary computation (D.E. Goldberg edition)., Vol. 2 of Genetic algorithms and
evolutionary computation, chap. Experimental result in function optimization with
EDAs in continuous Domain. Kluwer.
Bersini, H., Dorigo, M., Langerman, S., Seront, G., & Gambardella, L. M. (1996). Results of
the first international contest on evolutionary optimisation (1st iceo). In Proceedings
of IEEE International Conference on Evolutionary Computation, IEEE-EC 96, pp.
611–615, Nagoya, Japan. IEEE Press.
Beyer, H.-G., & Deb, K. (2001). On self-adapting features in real-parameter evolutionary
algorithms. IEEE Transactions on evolutionary computation, 5 (3), 250–270.
Breiman, L. (1996). Stacked regressions. Machine Learning, 24 (1), 49–64.
Bäck, J. H. (1996). Evolutionary Algorithms in Theory and Practice. Oxford University
Press, Oxford.
Bäck, T., Fogel, D., & Michalewicz, Z. (1997). Handbook of Evolutionary Computation.
Institute of Physics Publishing Ltd, Bristol and Oxford University Press, New York.
Bäck, T., & Schwefel, H. P. (1993). An overview of evolutionary algorithms for parameter
optimization. Evolutionary Computation, 1 (1), 1–23.
Cano, J., Herrera, F., & Lozano, M. (2003). Using evolutionary algorithms as instance
selection for data reduction in kdd: an experimental study. IEEE Transactions on
Evolutionary Computation, 7 (6), 561–575.
Davidor, Y. (1991). Genetic Algorithms and Robotics: A Heuristic Strategy for Optimization,
Vol. 1 of Robotics and Automated Systems. World Scientific.
De Jong, K. D. (1975). An analysis of the behavior of a class of genetic adaptive systems.
Ph.D. thesis, Departament of Computer and Communication Sciences, University of
Michigan, Ann Arbor.
43

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

De Jong, M. B., & Kosters, W. (1998). Solving 3-sat using adaptive sampling. In Poutré,
H., & van den Herik, J. (Eds.), Proceedings of the Tenth Dutch/Belgian Artificial
Intelligence Conference, pp. 221–228.
Deb, K., & Agrawal, R. B. (1995). Simulated binary crossover for continuous search space.
Complex Systems, 9, 115–148.
Deb, K., & Beyer, H. (2001). Self-adaptive genetic algorithms with simulated binary
crossover. Evolutionary Computation, 9 (2), 195–219.
Dixon, L. C. W. (1974). Nonlinear optimization: A survey of the state of the art. Software
for Numerical Mathematics, 193–216. Academic Press.
Dunn, O. J., & Clark, V. (1974). Applied Statistics: Analysis of Variance and Regression.
Wiley, New York.
Eiben, A., & Bäck, T. (1997a). Multi-parent recombination operators in continuous search
spaces. Tech. rep. TR-97-01, Leiden University.
Eiben, A. E., & Bäck, T. (1997b). Empirical investigation of multi-parent recombination
operators in evolution strategies. Evolutionary Computation, 5 (3), 347–365.
Eiben, A., van der Hauw, J., & van Hemert, J. (1998). Graph coloring with adaptive
evolutionary algorithms. Journal of Heuristics, 4 (1), 25–46.
Eshelman, L. J., & Schaffer, J. D. (1993). Real-coded genetic algorithms and intervalschemata. In Whitley, L. D. (Ed.), Foundation of Genetic Algorithms 2, pp.
187C3.3.7:1–C3.3.7:8.–202, San Mateo. Morgan Kaufmann.
Fletcher, R., & Powell, M. J. D. (1963). A rapidly convergent descent method for minimization. Computer Journal, pp. 163–168.
Fogel, D. B. (1995). Evolutionary Computation: Toward a New Philosophy of Machine
Intelligence. IEEE Press, Piscataway, New Jork.
Fogel, L. J., Owens, A. J., & Walsh, M. J. (1966). Artificial Intelligence Through Simulated
Evolution. John Wiley & Sons.
Friedman, J. H. (1994). An overview of predictive learning and function approximation.
In Cherkassky, V., Friedman, J. H., & Wechsler, H. (Eds.), From Statistics to Neural
Networks, Theory and Pattern Recognition Applications, Vol. 136 of NATO ASI Series
F, pp. 1–61. Springer-Verlag.
Garcı́a-Pedrajas, N., Hervás-Martı́nez, C., & Ortiz-Boyer, D. (2005). Cooperative coevolution of artificial neural network ensembles for pattern classification. IEEE Transactions on Evolutionary Computation, 9 (3), 271–302.
Goldberg, D. E. (1989a). Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, New York.
Goldberg, D. E. (1989b). Sizing populations for serial and parallel genetic algorithms. In
Schaffer, J. (Ed.), 3rd International Conference on Genetic Algorithms, pp. 70–79,
San Mateo, CA. Morgan Kaufmann.
Goldberg, D. E. (1991). Real-coded genetic algorithms, virtual alphabets, and blocking.
Complex Systems, pp. 139–167.
44

CIXL2: A Crossover Operator for Evolutionary Algorithms

Goldberg, D. E., & Deb, K. (1991). A comparative analysis of selection schemes used in
genetic algorithms. In Rawlins, G. J. E. (Ed.), Foundations of Genetic Algorithms,
pp. 69–93, San Mateo, CA. Morgan Kaufmann.
Gordon, V. S., & Whitley, D. (1993). Serial and parallel genetic algorithms as function
optimizers. In Forrest, S. (Ed.), Fifth International Conference on Genetic Algorithms,
pp. 177–183. Morgan Kaufmann.
Grefenstette, J. J. (1986). Optimization of control parameters for genetic algorithms. IEEE
Transactions on Systems, Mans, and Cybernetics, 16 (1), 122–128.
Hadley, G. (1964). Nonlinear and Dynamics Programming. Addison Wesley.
Hashem, S. (1997). Optimal linear combinations of neural networks. Neural Networks,
10 (4), 599–614.
Herrera, F., Herrera-Viedma, E., Lozano, E., & Verdegay, J. L. (1994). Fuzzy tools to
improve genetic algorithms. In Second European Congress on Intelligent Techniques
and Soft Computing, pp. 1532–1539.
Herrera, F., & Lozano, M. (2000). Gradual distributed real-coded genetic algorithms. IEEE
Transactions on Evolutionary Computation, 4 (1), 43–63.
Herrera, F., Lozano, M., & Sánchez, A. M. (2003). A taxonomy for the crossover operator
for real-coded genetic algorithms: An experimental study. International Journal of
Intelligent Systems, 18, 309–338.
Herrera, F., Lozano, M., & Verdegay, J. L. (1998). Tackling real-coded genetic algorithms:
Operators and tools for behavioural analysis. Artificial Inteligence Review, pp. 265–
319. Kluwer Academic Publisher. Printed in Netherlands.
Hervás-Martı́nez, C., & Ortiz-Boyer, D. (2005). Analizing the statistical features of cixl2
crossover offspring. Soft Computing, 9 (4), 270–279.
Holland, J. H. (1975). Adaptation in natural and artificial systems. The University of
Michigan Press, Ann Arbor, MI.
Johnson, T., & Husbands, P. (1990). System identification using genetic algorithms. In
Parallel Problem Solving from Nature, Vol. 496 of Lecture Notes in Computer Science,
pp. 85–89, Berlin. Springer-Verlag.
Jong, K. A. D., & Sarma, J. (1993). Generation gaps revisited. In Whitley, L. D. (Ed.),
Foundations of Genetic Algorithms, Vol. 2, pp. 19–28. Morgan Kaufmann, San Mateo.
Kendall, M., & Stuart, S. (1977). The advanced theory of statistics, Vol. 1. Charles GriOEn
& Company.
Kita, H. (2001). A comparison study of self-adaptation in evolution strategies and real-code
genetic algorithms. Evolutionary Computation, 9 (2), 223–241.
Kita, H., Ono, I., & Kobayashi, S. (1998). Theoretical analysis of the unimodal normal distribution crossover for real-coded genetic algorithms. In IEEE International Conference
on Evolutionary Computation ICEC’98, pp. 529–534, Anchorage, Alaska, USA.
Kivinen, J., & Warmuth, M. (1997). Exponential gradient descent versus gradient descent
for linear predictors. Information and Computation, 132 (1), 1–63.
45

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Kuncheva, L. (1995). Editing for the k-nearest neighbors rule by a genetic algorithm.
Pattern Recognition Letter, 16, 809–814.
Larrañaga, P., Etxeberria, R., Lozano, J., & Peña, J. (2000). Optimization in continuous
domains by learning and simulation of gaussian networks. In Wu, A. (Ed.), Proceeding
of the 2000 Genetic and Evolutionary Computation Conference Workshop Program,
pp. 201–204.
Leblanc, M., & Tibshirani, R. (1993). Combining estimates in regression and classification.
Tech. rep., Department of Statistics, University of Toronto.
Levene, H. (1960). In Contributions to Probability and Statistics, chap. Essays in Honor of
Harold Hotelling, pp. 278–292. Stanford University Press.
Liu, Y., Yao, X., & Higuchi, T. (2000). Evolutionary ensembles with negative correlation
learning. IEEE Transactions on Evolutionary Computation, 4 (4), 380–387.
Merz, C. J. (1999a). A principal components approach to combining regression estimates.
Machine Learning, 36 (1), 9–32.
Merz, C. J. (1999b). Using correspondence analysis to combine classifiers. Machine Learning, 36 (1), 33–58.
Michalewicz, Z. (1992). Genetic Algorithms + Data Structures = Evolution Programs.
Springer-Verlag, New York.
Miller, G. F., Todd, P. M., & Hedge, S. U. (1991). Designing neural networks. Neural
Networks, 4, 53–60.
Miller, R. G. (1981). Simultaneous Statistical Inference (2 edition). Wiley, New York.
Miller, R. G. (1996). Beyond ANOVA, Basics of Applied Statistics (2 edition). Chapman
& Hall, London.
Mizumoto, M. (1989). Pictorial representations of fuzzy connectives. part i: Cases of tnorms, t-conorms and averaging operators. Fuzzy Sets Systems, 31, 217–242.
Moriarty, D., Schultz, A., & Grefenstette, J. (1999). Evolutionary algorithms for reinforcement learning. Journal Artificial Intelligence Reserarch, 11.
Mühlenbein, H., Mahnig, T., & Rodriguez, O. (1999). Schemata, distributions and graphical
models in evolutionary optimazation. Journal of Heuristics, pp. 215–247.
Mühlenbein, H., & Paaβ, G. (1998). From recombination of genes to the estimation of
distributions i. binary parameters.. In Eiben, A. E., Bäck, T., Schoenauer, M., &
Schwefel, H.-P. (Eds.), The 5th Conference on Parallel Problem Solving from Nature,
pp. 178–187. Springer.
Ono, I., Kita, H., & Kobayashi, S. (1999). A robust real-coded genetic algorithm using
unimodal normal distribution crossover augmented by uniform crossover: Effects of
self-adaptation of crossover probabilities. In Banzhaf, W., Daida, J., Eiben, A. E.,
Garzon, M. H., Honavar, V., Jakiela, M., & Smith, R. E. (Eds.), Genetic and Evolutionary Computation Conf. (GECCO’99), pp. 496–503, San Francisco, CA. Morgan
Kaufmann.
46

CIXL2: A Crossover Operator for Evolutionary Algorithms

Ono, I., & Kobayashi, S. (1997). A real-coded genetic algorithm for function optimization
using unimodal normal distribution crossover. In 7th International Conference on
Genetic Algorithms, pp. 246–253, Michigan, USA. Michigan State University, Morgan
Kaufman.
Ono, I., Kobayashi, S., & Yoshida, K. (2000). Optimal lens design by real-coded genetic
algorithms using undx. Computer methods in applied mechanics and engineering, pp.
483–497.
Opitz, D. W., & Shavlik, J. W. (1996). Actively searching for an effective neural network
ensemble. Connection Science, 8 (3), 337–353.
Oren, S. S. (1974). On the selection of parameters in self scaling variable metric algorithms.
Mathematical Programming, pp. 351–367.
Ortiz-Boyer, D., Hervás-Martı́nez, C., & Muñoz-Pérez, J. (2003). Metaheuristics: Computer
Decision-Making, chap. Study of genetic algorithms with crossover based on confidence
intervals as an alternative to classic least squares estimation methods for non-linear
models, pp. 127–151. Kluwer Academic Publishers.
Perrone, M. P., & Cooper, L. N. (1993). When networks disagree: Ensemble methods for
hybrid neural networks. In Mammone, R. J. (Ed.), Neural Networks for Speech and
Image Processing, pp. 126–142. Chapman – Hall.
Rastrigin, L. A. (1974). Extremal control systems. In Theoretical Foundations of Engineering Cybernetics Series. Moscow: Nauka, Russian.
Rechenberg, I. (1973). Evolutionsstrategie-Optimierum technischer Systeme nach Prinzipien der biologischen Evolution. Ph.D. thesis, Stuttgart-Bad Cannstatt: FrommannHolzboog.
Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value of
a function. Computer Journal, pp. 175–184.
Rudolph, G. (1994). Convergence analysis of canonical genetic algorithms. IEEE Transactions on Neural Networks, special issue on evolutionary computation, 5 (1), 96–101.
Salomon, R. (1996). Reevaluating genetic algorithm performance under coordinate rotation
of benchmark functions. BioSystems, pp. 263–278.
Satoh, H., Yamamura, M., & Kobayashi, S. (1996). Minimal generation gap model for
gas considering both exploration and exploitation.. In Proceeding of the IIZUKA:
Methodologies for the Conception, Design, and Application of Intelligent Sstems, pp.
494–497.
Schaffer, J., Caruana, R., Eshelman, L., & Das, R. (1989). A study of control parameters affecting online performance of genetic algorithms for function optimization. In
Schaffer, J. (Ed.), 3rd International Conference on Genetic Algorithms, pp. 51–60,
San Mateo, CA. Morgan Kaufmann.
Schlierkamp-Voosen, D. (1994). Strategy adaptation by competition. In Second European
Congress on Intelligent Techniques and Soft Computing, pp. 1270–1274.
47

Ortiz-Boyer, Hervás-Martı́nez, & Garcı́a-Pedrajas

Schwefel, H. P. (1981). Numerical Optimization of Computer Models. John Wiley & Sons.
English translation of Numerische Optimierung von Computer-Modellen mittels der
Evolutionsstrategie, 1977.
Schwefel, H. P. (1995). Evolution and Optimum Seeking. John Wiley & Sons.
Sedighi, K., Ashenayi, K., Manikas, T., Wainwright, R., & Tai, H. (2004). Autonomous
local path planning for a mobile robot using a genetic algorithm. In IEEE Congress
on Evolutionary Computation.
Sharkey, A. J. C. (1996). On combining artificial neural nets. Connection Science, 8,
299–313.
Singh, M., Chatterjee, A., & Chaudhury, S. (1997). Matching structural shape descriptions
using genetic algorithms. Pattern Recognition, 30 (9), 1451–1462.
Smith, R. E. (1993). Adaptively resizing populations: An algorithm and analysis. In Forrest,
S. (Ed.), 5th International Conference on Genetic Algorithms, p. 653, San Mateo, CA.
Morgan Kaufmann.
Snedecor, G. W., & Cochran, W. G. (1980). Statistical Methods (7 edition). Iowa State
University Press, Ames, Iowa.
Spedicato, E. (1975). Computational experience with quasi-newton algorithms for minimization problems of moderately large size. Tech. rep. CISE-N-175, Centro Informazioni
Studi Esperienze, Segrate (Milano), Italy.
Takahashi, O., Kita, H., & Kobayashi, S. (1999). A distance dependent alternation model on
real-coded genetic algorithms. In IEEE International Conference on Systems, Man,
and Cybernetics, pp. 619–624.
Tamhane, A. C., & Dunlop, D. D. (2000). Statistics and Data Analysis. Prentice Hall.
Voigt, H. M., Mühlenbein, H., & Cvetkovic, D. (1995). Fuzzy recombination for the breeder
genetic algorithms. In Eshelman, L. (Ed.), The 6th International Conference Genetic
Algorithms, pp. 104–111, San Mateo, CA. Morgan Kaufmann.
Webb, G. I. (2000). Multiboosting: A technique for combining boosting and wagging.
Machine Learning, 40 (2), 159–196.
Whitley, D., Mathias, K., Rana, S., & Dzubera, J. (1995). Building better test functions.
In Eshelman, L. (Ed.), Sixth International Conference on Genetic Algorithms, pp.
239–246. Morgan Kaufmann.
Wolpert, D. H., & Macready, W. G. (1995). No free-lunch theorems for search. Tech. rep.
95-02-010, Santa Fe Institute.
Wright, A. (1991). Genetic algorithms for real parameter optimization. In Rawlin, G.
J. E. (Ed.), Foundations of Genetic Algorithms 1, pp. 205–218, San Mateo. Morgan
Kaufmann.
Zhang, B. T., & Kim, J. J. (2000). Comparison of selection methods for evolutionary
optimization. Evolutionary Optimization, 2 (1), 55–70.
Zhou, Z.-H., Wu, J., & Tang, W. (2002). Ensembling neural networks: Many could be better
than all. Artificial Intelligence, 137 (1–2), 239–253.

48

Journal of Artificial Intelligence Research 24 (2005) 623-639

Submitted 12/04; published 11/05

Hiding Satisfying Assignments: Two are Better than One
Dimitris Achlioptas

optas@microsoft.com

Microsoft Research
Redmond, Washington

Haixia Jia

hjia@cs.unm.edu

Computer Science Department
University of New Mexico

Cristopher Moore

moore@cs.unm.edu

Computer Science Department
University of New Mexico

Abstract
The evaluation of incomplete satisfiability solvers depends critically on the availability
of hard satisfiable instances. A plausible source of such instances consists of random kSAT formulas whose clauses are chosen uniformly from among all clauses satisfying some
randomly chosen truth assignment A. Unfortunately, instances generated in this manner
tend to be relatively easy and can be solved efficiently by practical heuristics. Roughly
speaking, for a number of different algorithms, A acts as a stronger and stronger attractor
as the formula’s density increases. Motivated by recent results on the geometry of the space
of satisfying truth assignments of random k-SAT and NAE-k-SAT formulas, we introduce
a simple twist on this basic model, which appears to dramatically increase its hardness.
Namely, in addition to forbidding the clauses violated by the hidden assignment A, we also
forbid the clauses violated by its complement, so that both A and A are satisfying. It
appears that under this “symmetrization” the effects of the two attractors largely cancel
out, making it much harder for algorithms to find any truth assignment. We give theoretical
and experimental evidence supporting this assertion.

1. Introduction
Recent years have witnessed the rapid development and application of search methods for
constraint satisfaction and Boolean satisfiability. An important factor in the success of these
algorithms is the availability of good sets of benchmark problems to evaluate and fine-tune
them. There are two main sources of such problems: the real world, and random instance
generators. Real-world problems are arguably the best benchmarks, but unfortunately are
in short supply. Moreover, using real-world problems carries the risk of tuning algorithms
toward the specific application domains for which good benchmarks are available. In that
sense, random instance generators are a good additional source, with the advantage of
controllable characteristics, such as size and expected hardness.
Hard random instances have led to the development of new stochastic search methods
such as WalkSAT (Selman, Kautz, & Cohen, 1996), the breakout procedure (Morris, 1993),
and Survey Propagation (Mézard & Zecchina, 2002), and have been used in detailed comparisons of local search methods for graph coloring and related problems (Johnson, Aragon,
McGeoch, & Shevon, 1989). The results of various competitions for CSP and SAT algoc
2005
AI Access Foundation. All rights reserved.

Achlioptas, Jia, & Moore

rithms show a fairly direct correlation between the performance on real-world benchmarks
and on hard random instances (Johnson & Trick, 1996; Du, Gu, & Pardalos, 1997; Johnson
et al., 1989). Nevertheless, a key limitation of current problem generators concerns their use
in evaluating incomplete satisfiability solvers such as those based on local search methods.
When an incomplete algorithm does not find a solution, it can be difficult to determine
whether this is because the instance is in fact unsatisfiable, or simply because the algorithm
failed to find a satisfying assignment. The standard way of dealing with this problem is to
use a complete search method to filter out the unsatisfiable cases. However, this greatly
limits the size and difficulty of problem instances that can be considered. Ideally, one would
use problem generators that generate satisfiable instances only. One relatively recent source
of such problems is the quasigroup completion problem (Shaw, Stergiou, & Walsh, 1998;
Achlioptas, Gomes, Kautz, & Selman, 2000; Kautz, Ruan, Achlioptas, Gomes, Selman, &
Stickel, 2001). However, a generator for random hard satisfiable instances of 3-SAT, say,
has remained elusive.
Perhaps the most natural candidate for generating random hard satisfiable 3-SAT formulas is the following. Pick a random truth assignment A, and then generate a formula with
n variables and rn random clauses, rejecting any clause that is violated by A. In particular,
we might hope that if we work close to the satisfiability threshold region r ≈ 4.25, where the
hardest random 3-SAT problems seem to be (Cheeseman, Kanefsky, & Taylor, 1991; Hogg,
Huberman, & Williams, 1996; Mitchell, Selman, & Levesque, 1992), this would generate
hard satisfiable instances. Unfortunately, this generator is highly biased towards formulas
with many assignments clustered around A. When given to local search methods such as
WalkSAT, the resulting formulas turn out to be much easier than formulas of comparable
size obtained by filtering satisfiable instances from a 3-SAT generator. More sophisticated
versions of this “hidden assignment” scheme (Asahiro, Iwama, & Miyano, 1996; Van Gelder,
1993) improve matters somewhat but still lead to easily solvable formulas.
In this paper we introduce a new generator of random satisfiable problems. The idea
is simple: we pick a random 3-SAT formula that has a “hidden” complementary pair of
satisfying assignments, A and A, by rejecting clauses that are violated by either A or A.
We call these “2-hidden” formulas. Our motivation comes from recent work (Achlioptas &
Moore, 2002b, 2005) which showed that moving from random k-SAT to random NAE-kSAT (in which every clause in the formula must have at least one true and at least one false
literal) tremendously reduces the correlation between solutions. That is, whereas in random
k-SAT, satisfying assignments tend to form clumps, in random NAE-k-SAT the solutions
appear to be scattered throughout {0, 1} n in a rather uniform “mist,” even for densities
extremely close to the threshold. An intuitive explanation for this phenomenon is that since
the complement of every NAE-assignment is also an NAE-assignment, the attractions of
solution pairs largely “cancel out.” In this paper we exploit this phenomenon to impose a
similar symmetry with the hidden assignments A and A, so that their attractions cancel
out, making it hard for a wide variety of algorithms to “feel” either one.
A particularly nice feature of our generator is that it is based on an extremely simple
probabilistic procedure, in sharp contrast with 3-SAT generators based on, say, cryptographic ideas (Massacci, 1999). In particular, our generator is readily amenable to all
the mathematical tools that have been developed for the rigorous study of random k-SAT
formulas. Here we make two first steps in that direction. In Section 2, via a first mo624

Hiding Satisfying Assignments: Two are Better than One

ment calculation we study the distribution of the number of solutions as a function of their
distance from the hidden assignments. In Section 3, we use the technique of differential
equations to analyze the performance of the Unit Clause (UC) heuristic on our formulas.
Naturally, mathematical simplicity would not be worth much if the formulas produced by
our generator were easily solvable. In Section 4, we compare experimentally the hardness of
“2-hidden” formulas with that of “1-hidden” and “0-hidden” formulas. That is, we compare
our formulas with random 3-SAT formulas with one hidden assignment and with standard
random 3-SAT formulas with no hidden assignment. We examine four leading algorithms:
two complete solvers, zChaff and Satz, and two incomplete ones, WalkSAT and the recently
introduced Survey Propagation (SP).
For all these algorithms, we find that our formulas are much harder than 1-hidden
formulas and, more importantly, about as hard as 0-hidden formulas, of the same size and
density.

2. A picture of the space of solutions
In this section we compare 1-hidden and 2-hidden formulas with respect to the expected
number of solutions at a given distance from the hidden assignment(s).
2.1 1-hidden formulas
Let X be the number of satisfying truth assignments in a random k-SAT formula with n
variables and m = rn clauses chosen uniformly and independently among all k-clauses with
at least one positive literal, i.e., 1-hidden formulas where we hide the all–ones truth assignment. To calculate the expectation E[X], it is helpful to parametrize truth assignments
according to their overlap with the hidden assignment, i.e., the fraction α of variables on
which they agree with A, which in this case is the fraction of variables that are set to one.
Then, linearity of expectation gives (1), clause independence gives (2), selecting the literals
in each clause uniformly and independently gives (3), and, finally, writing z = αn and using
Stirling’s approximation for the factorial gives (4) below:
X
E[X] =
Pr[A is satisfying]
(1)
A∈{0,1}n
n  
X

n
Pr[a truth assignment with z ones satisfies a random clause] m (2)
z
z=0
m

k  
n  
X
1 X k
n 
(3)
(1 − z/n)j (z/n)k−j 
1− k
=
j
z
2 −1
z=0
j=1
m
n  
X
n
1 − (z/n)k
=
1−
z
2k − 1
z=0


r n
1 − αk
1
(4)
= poly(n) × max
1− k
2 −1
α∈[0,1] αα (1 − α)1−α
=

= poly(n) × max [fk,r (α)]n
α∈[0,1]

625

Achlioptas, Jia, & Moore

where
1
fk,r (α) = α
α (1 − α)1−α



1 − αk
1− k
2 −1

r

.

From this calculation we see that E[X] is dominated by the contribution of the truth assignments that maximize fk,r (α) (since we raise fk,r to the nth power all other contributions
vanish). Now, note that f is the product of an “entropic” factor 1/(α α (1 − α)1−α ) which
is symmetric around α = 1/2, and a “correlation” factor which is strictly increasing in α.
As a result, it is always maximized for some α > 1/2. This means that the dominant contribution to E[X] comes from truth assignments that agree with the hidden assignment on
more that half the variables. That is, the set of solutions is dominated by truth assignments
that can “feel” the hidden assignments. Moreover, as r increases this phenomenon becomes
more and more acute (see Figure 1 below).
2.2 2-hidden formulas
Now let X be the number of satisfying truth assignments in a random k-SAT formula with
n variables and m = rn clauses chosen uniformly among all k-clauses that have at least
one positive and at least one negative literal, i.e., 2-hidden formulas where we hide the all–
ones assignment and its complement. To compute E[X] we proceed as above, except that
now (3) is replaced by
m

k−1  
n  
X
X
k
1
n 
(1 − z/n)j (z/n)k−j  .
1− k
j
z
2 −2
z=0

j=1

Carrying through the ensuing changes we find that now
E[X] = poly(n) × max [gk,r (α)]n
α∈[0,1]

where
1
gk,r (α) = α
α (1 − α)1−α



1 − αk − (1 − α)k
1−
2k − 2

r

.

This time, both the entropic factor and the correlation factor comprising g are symmetric
functions of α, so gk,r is symmetric around α = 1/2 (unlike f k,r ). Indeed, one can prove
that for all r up to extremely close to the random k-SAT threshold r k , the function gk,r has
its global maximum at α = 1/2. In other words, for all such r, the dominant contribution
to E[X] comes from truth assignments at distance n/2 from the hidden assignments, i.e.,
the hidden assignments are “not felt.” More precisely, there exists a sequence  k → 0 such
that gk,r has a unique global maximum at α = 1/2, for all
r ≤ 2k ln 2 −

ln 2
− 1 − k .
2

(5)

Contrast this with the fact (implicit in Kirousis, Kranakis, Krizanc, & Stamatiou, 1998)
that for
ln 2 1
r ≥ 2k ln 2 −
− ,
(6)
2
2
626

Hiding Satisfying Assignments: Two are Better than One

a random k-SAT formula with n variables and m = rn clauses is unsatisfiable with probability 1 − o(1). Moreover, the convergence of the sequence  k → 0 is rapid, as can be seen from
the concrete values in table 1. Thus the gap between the values of r given by equations (5)
and (6) quickly converges to 1/2, even as the threshold becomes exponentially large.
k
Eq. (5)
Eq. (6)

3
7/2
4.67

4
35/4
10.23

5
20.38
21.33

7
87.23
87.88

10
708.40
708.94

20
726816.15
726816.66

Table 1: The convergence (in k) to the asymptotic gap of 1/2 is rapid
In Figure 1 we plot fk,r and gk,r for k = 5 and r = 16, 18, 20, 22, 24 (from top to
bottom). We see that in the case of 1-hidden formulas, i.e., f k,r , the maximum always
occurs to the right of α = 1/2. Moreover, observe that for r = 22, 24, i.e., after we cross
the 5-SAT threshold (which occurs at r ≈ 21) we have a dramatic shift in the location of
the maximum and, thus, in the extent of the bias. Specifically, since the expected number
of satisfying assignments is roughly f k,r (α)n , and since fk,r (α) < 1 except for α ≈ 1, with
high probability the only remaining satisfying assignments in the limit n → ∞ are those
extremely close to the hidden assignment.
In the case of 2-hidden formulas, on the other hand, we see that for r = 16, 18, 20
the global maximum occurs at α = 1/2. For r = 20, just below the threshold, we also
have two local maxima near α = 0, 1, but since g k,r is raised to the nth power, these are
exponentially suppressed. Naturally, for r above the threshold, i.e., r = 22, 24, these local
maxima become global, signifying that indeed the only remaining truth assignments are
those extremely close to one of the two hidden ones.
Intuitively, we expect that because g is flat at α = 1/2 where random truth assignments
are concentrated, for 2-hidden formulas local search algorithms like WalkSAT will essentially
perform a random walk until they are lucky enough to get close to one of the two hidden
assignments. Thus we expect WalkSAT to take about as long on 2-hidden formulas as it does
on 0-hidden ones. For 1-hidden formulas, in contrast, we expect the nonzero gradient of f
at α = 1/2 to provide a strong “hint” to WalkSAT that it should move towards the hidden
assignment, and that therefore 1-hidden formulas will be much easier for it to solve. We
will see below that our experimental results bear out these intuitions perfectly.

3. The Unit Clause heuristic and DPLL algorithms
Consider the following linear-time heuristic, called Unit Clause (UC), which permanently
sets one variable in each step as follows: pick a random literal and satisfy it, and repeatedly
satisfy any 1-clauses present. Chao and Franco showed that UC succeeds with constant
probability on random 3-SAT formulas with r < 8/3, and fails with high probability, i.e.,
with probability 1 − o(1) as n → ∞, for r > 8/3 (Chao & Franco, 1986). One can think of
UC as the first branch of the simplest possible DPLL algorithm S: set variables in a random
order, each time choosing randomly which branch to take first. Their result then shows
that, with constant probability, S solves random 3-SAT formulas with r < 8/3 with no
backtracking at all.
627

Achlioptas, Jia, & Moore

1.3
1.2
1.1
1
0.9
0.8
0.7
r=16
r=18
r=20
r=22
r=24

0.6
0.5
0.4
0

0.2

0.4

α

0.6

0.8

1

1-hidden formulas
1.25

r=16
r=18
r=20
r=22
r=24

1.2
1.15
1.1
1.05
1
0.95
0.9
0

0.2

0.4

α

0.6

0.8

1

2-hidden formulas
Figure 1: The nth root of the expected number of solutions f k,r and gk,r for 1-hidden and
2-hidden formulas respectively, as a function of the overlap fraction α = z/n with
the hidden assignment. Here k = 5 and r = 16, 18, 20, 22, 24 from top to bottom.

628

Hiding Satisfying Assignments: Two are Better than One

It is conjectured that the running time of S goes from linear to exponential at r = 8/3,
with no intermediate regime. Calculations using techniques from statistical physics (Cocco
& Monasson, 2001a, 2001b; Monasson, 2005) show that this is true of the expected running
time. Achlioptas, Beame and Molloy show that the running time is exponential with high
probability for r > 3.81; moreover, they show that if the “tricritical point” of (2 + p)-SAT
is r = 2/5, then this is the case for r > 8/3 (Achlioptas, Beame, & Molloy, 2001).
In this section we analyze the performance of UC on 1-hidden and 2-hidden formulas.
Specifically, we show that UC fails for 2-hidden formulas at precisely the same density as for
0-hidden ones. Based on this, we conjecture that the running time of S, and other simple
DPLL algorithms, becomes exponential for 2-hidden formulas at the same density as for
0-hidden ones.
To analyze UC on random 1-hidden and 2-hidden formulas we actually analyze UC on
arbitrary initial distributions of 3-clauses, i.e., where for each 0 ≤ j ≤ 3 we specify the initial
number of 3-clauses with j positive literals and 3 − j negative ones. We use the method of
differential equations; see the article by Achlioptas(2001) for a review. To simplify notation,
we assume that A is the all–ones assignment, so that 1-hidden formulas forbid clauses where
all literals are negative, while 2-hidden formulas forbid all-negative and all-positive clauses.
A round of UC consists of a “free” step, in which we satisfy a random literal, and the
ensuing chain of “forced” steps or unit-clause propagations. For 0 ≤ i ≤ 3 and 0 ≤ j ≤ i,
let Si,j = si,j n be the number of clauses of length i with j positive literals
P and i − j negative
ones. We will also refer to the total density of clauses of size i as s i = j si,j . Let X = xn
be the number of variables set so far. Our goal is to write the expected change in these
variables in a given round as a function of their values at the beginning of the round. Note
that at the beginning of each round S 1,0 = S1,1 = 0 by definition, so the “state space” of
our analysis will consist of the variables S i,j for i ≥ 2.
It is convenient to define two new quantities, m T and mF , which are the expected
number of variables set True and False in a round. We will calculate these below. Then, in
terms of mT , mF , we have
3s3,j
1−x
2s2,j
(j + 1)s3,j+1
(3 − j)s3,j
E[∆S2,j ] = −(mT + mF )
+ mF
+ mT
1−x
1−x
1−x
E[∆X] = −(mT + mF ) .

E[∆S3,j ] = −(mT + mF )

(7)
(8)

To see this, note that a variable appears positively in a clause of type i, j with probability
j/(n − X), and negatively with probability (i − j)/(n − X). Thus, the negative terms in (7)
and (8) correspond to clauses being “hit” by the variables set, while the positive term is
the “flow” of 3-clauses to 2-clauses.
To calculate mT and mF , we consider the process by which unit clauses are created
during a round. We can model this with a two-type branching process, which we analyze
as in the article by Achlioptas and Moore(2002a). Since the free step gives the chosen
variable a random value, we can think of it as creating a unit clause, which is positive or
negative with equal probability. Thus the initial expected population of unit clauses can be
629

Achlioptas, Jia, & Moore

represented by a vector
p0 =



1/2
1/2



where the first and second components count the negative and positive unit clauses respectively. Moreover, at time X = xn, a unit clause procreates according to the matrix


1
s2,1 2s2,0
.
M=
1 − x 2s2,2 s2,1
In other words, satisfying a negative unit clause creates, in expectation, M 1,1 = s2,1 /(1 − x)
negative unit clauses and M2,1 = 2s2,2 /(1 − x) positive unit clauses, and similarly for
satisfying a positive unit clause.
Thus, as long as the largest eigenvalue λ 1 of M is less than 1, the expected number of
variables set true or false during the round is given by


mF
= (I + M + M 2 + · · · ) · p0 = (I − M )−1 · p0
mT
where I is the identity matrix. Moreover, as long as λ 1 < 1 throughout the algorithm, i.e., as
long as the branching process is subcritical for all x, UC succeeds with constant probability.
On the other hand, if λ1 ever exceeds 1, then the branching process becomes supercritical,
with high probability the unit clauses proliferate, and the algorithm fails. Note that
√
s2,1 + 2 s2,0 s2,2
.
(9)
λ1 =
1−x
Now let us rescale (7) to give a system of differential equations for the s i,j . Wormald’s
Theorem (Wormald, 1995) implies that with high probability the random variables S i,j (xn)
will be within o(n) of si,j (x) · n for all x, where si,j (x) is the solution of the following:
ds3,j
dx
ds2,j
dx

3s3,j
1−x
2s2,j
(j + 1)s3,j+1
(3 − j)s3,j
mF
mT
= −
+
+
1 − x mT + m F
1−x
mT + m F
1−x
= −

(10)

Now, suppose our initial distribution of 3-clauses is symmetric, i.e., s 3,0 (0) = s3,3 (0)
and s3,1 (0) = s3,2 (0). It is easy to see from (10) that in that case, both the 3-clauses and
the 2-clauses are symmetric at all times, i.e., s i,j = si,i−j and mF = mT . In that case
√
s2,1 + 2 s2,0 s2,2 = s2 , so the criterion for subcriticality becomes
λ1 =

s2
<1 .
1−x

Moreover, since the system (10) is now symmetric with respect to j, summing over j gives
the differential equations
ds3
dx
ds2
dx

3s3
1−x
2s2
3s3
= −
+
1 − x 2(1 − x)
= −

630

Hiding Satisfying Assignments: Two are Better than One

which are precisely the differential equations for UC on 0-hidden formulas, i.e., random
instances of 3-SAT.
Since 2-hidden formulas correspond to symmetric initial conditions, we have thus shown
that UC succeeds on them with constant probability if and only if r < 8/3, i.e., that UC fails
on these formulas at exactly the same density for which it fails on random 3-SAT instances.
(In contrast, integrating (10) with the initial conditions corresponding to 1-hidden formulas
shows that UC succeeds for them at a slightly higher density, up to r < 2.679.)
Of course, UC can easily be improved by making the free step more intelligent: for
instance, choosing the variable according to the number of its occurrences in the formula,
and using the majority of these occurrences to decide its truth value. The best known
heuristic of this type (Kaporis, Kirousis, & Lalas, 2003; Hajiaghayi & Sorkin, 2003) succeeds
with constant probability for r < 3.52. However, we believe that much of the progress that
has been made in analyzing the performance of such algorithms can be “pushed through”
to 2-hidden formulas. Specifically, nearly all algorithms analyzed so far have the property
that given as input a symmetric initial distribution of 3-clauses, e.g. random 3-SAT, their
residual formulas consist of symmetric mixes of 2- and 3-clauses. As a result, we conjecture
that the above methods can be used to show that such algorithms act on 2-hidden formulas
exactly as they do on 0-hidden ones, failing with high probability at the same density.
More generally, call a DPLL algorithm myopic if its splitting rule consists of choosing a
random clause of a given size, based on the current distribution of clause sizes, and deciding
how to satisfy it based on the number of occurrences of its variables in other clauses.
For a given myopic algorithm A, let r A be the density below which A succeeds without any
backtracking with constant probability. The results of Achlioptas, Beame and Molloy (2001)
imply the following statement: if the tricritical point for random (2 + p)-SAT is p c = 2/5
then every myopic algorithm A takes exponential time for r > r A . Thus, not only UC, but in
fact a very large class of natural DPLL algorithms, would go from linear time for r < r A to
exponential time for r > rA . The fact that the linear-time heuristics corresponding to the
first branch of A act on 2-hidden formulas just as they do on 0-hidden ones suggests that,
for a wide variety of DPLL algorithms, 2-hidden formulas become exponentially hard at the
same density as 0-hidden ones. Proving this, or indeed proving that 2-hidden formulas take
exponential time for r above some critical density, appears to us a very promising direction
for future work.

4. Experimental results
In this section we report experimental results on our 2-hidden formulas, and compare them
to 1-hidden and 0-hidden ones. We use two leading complete solvers, zChaff and Satz,
and two leading incomplete solvers, WalkSAT and the new Survey Propagation algorithm
SP. In an attempt to avoid the numerous spurious features present in “too-small” random
instances, i.e., in non-asymptotic behavior, we restricted our attention to experiments where
n ≥ 1000. This meant that zChaff and Satz could only be examined at densities significantly above the satisfiability threshold, as neither algorithm could practically solve either
0-hidden or 2-hidden formulas with n ∼ 1000 variables close to the threshold. For WalkSAT
and SP, on the other hand, we can easily run experiments in the hardest range (around the
satisfiability threshold) for n ∼ 10 4 .
631

Achlioptas, Jia, & Moore

4.1 zChaff and Satz
In order to do experiments with n ≥ 1000 with zChaff and Satz, we focused on the
regime where r is relatively large, 20 < r < 60. As stated above, for r near the satisfiability
threshold, 0-hidden and 2-hidden random formulas with n ∼ 1000 variables seem completely
out of the reach of either algorithm. While formulas in this overconstrained regime are still
challenging, the presence of many forced steps allows both solvers to completely explore the
space fairly quickly.
We obtained zChaff from the Princeton web site (Moskewicz, Madigan, Zhao, Zhang,
& Malik, 2001). The first part of Figure 2 shows its performance on random formulas of
all three types (with n = 1000 for 20 ≤ r ≤ 40 and n = 3000 for 40 ≤ n ≤ 60). We see
that the number of decisions for all three types of problems decreases rapidly as r increases,
consistent with earlier findings for complete solvers on random 3-SAT formulas.
Figure 2 shows that zChaff finds 2-hidden formulas almost as difficult as 0-hidden ones,
which for this range of r are unsatisfiable with overwhelming probability. On the other
hand, the 1-hidden formulas are much easier, with a number of branchings between 2 and
5 orders of magnitude smaller. It appears that while zChaff’s smarts allow it to quickly
“zero in” on a single hidden assignment, the attractions exerted by a complementary pair of
assignments do indeed cancel out, making 2-hidden formulas almost as hard as unsatisfiable
ones. That is, the algorithm eventually “stumbles” upon one of the two hidden assignments
after a search that is nearly as exhaustive as for the unsatisfiable random 3-SAT formulas
of the same density.
We obtained Satz from the SATLIB web site (Li & Anbulagan, 1997b). The second
part of Figure 2 shows experiments on random formulas of all three types with n = 3000.
As can be seen, the median number of branches explored by Satz for all three types of
formulas are within a factor of five, with 0-hidden being the hardest and 2-hidden being the
easiest (note that a factor of five corresponds to setting fewer than 3 variables).
The reason for this is simple: while Satz makes intelligent decisions about which variable
to branch on, it tries these branches in a fixed order, attempting first to set each variable
false (Li & Anbulagan, 1997a). Therefore, a single hidden assignment will appear at a
uniformly random leaf in Satz’s search tree. In the 2-hidden case, since the two hidden
assignments are complementary, one will appear in a random position and the other one
in the symmetric position with respect to the search tree. Naturally, trying branches in
a fixed order is a good idea when the goal is to prove that a formula is unsatisfiable, e.g.
in hardware verification. However, we expect that if Satz were modified to, say, use the
majority heuristic to choose a variable’s first value, its performance on the three types of
problems would be similar to zChaff’s.
4.2 SP
SP is an incomplete solver recently introduced by Mézard and Zecchina (2002) based on a
generalization of belief propagation the authors call survey propagation. It is inspired by the
physical notion of “replica symmetry breaking” and the observation that for 3.9 < r < 4.25,
random 3-SAT formulas appear to be satisfiable, but their satisfying assignments appear to
be organized into clumps.
632

Hiding Satisfying Assignments: Two are Better than One

Median number of decisions over 25 trials

6

10

5

10

zChaff performance on HIDDEN 1, 2 and 0 formulas
HIDDEN−1
HIDDEN−2
HIDDEN−0

4

10

3

10

2

10

1

10
20

5

Median number of branches over 25 trials

10

25

30

35

40
r

45

50

55

60

Satz performance on HIDDEN 1, 2 and 0 formulas
HIDDEN−1
HIDDEN−2
HIDDEN−0

4

10

3

10

2

10

1

10
20

25

30

35

40
r

45

50

55

60

Figure 2: The median number of branchings made by zChaff and Satz on random instances
with 0, 1, and 2 hidden assignments (on a log 10 scale). For zChaff we use
n = 1000 for r = 20, 30, 40 and n = 3000 for r = 40, 50, 60, and for Satz we
use n = 3000 throughout. Each point is the median of 25 trials. The 2-hidden
formulas are almost as hard for both algorithms as the 0-hidden ones, while the
1-hidden formulas are much easier for zChaff.

633

Achlioptas, Jia, & Moore

1

SP performance on HIDDEN 1, 2 and 0 formulas

The fraction solved over 30 trials

0.9
0.8
0.7
0.6
0.5
HIDDEN−0
HIDDEN−1
HIDDEN−2

0.4
0.3
0.2
0.1
0
4

4.5

5
r

5.5

6

Figure 3: The fraction of problems successfully solved by SP as a function of density, with
n = 104 and 30 trials for each value of r. The threshold for solving 2-hidden
formulas is somewhat higher than for 0-hidden ones, and for 1-hidden formulas it
is higher still.

In Figure 3 we compare SP’s performance on the three types of problems near the
satisfiability threshold. (Because SP takes roughly the same time on all inputs, we do
not compare the running times.) For n = 10 4 SP solves 2-hidden formulas at densities
somewhat above the threshold, up to r ≈ 4.8, while it solves the 1-hidden formulas at still
higher densities, up to r ≈ 5.6.
Presumably the 1-hidden formulas are easier for SP since the “messages” from clauses
to variables, like the majority heuristic, tend to push the algorithm towards the hidden
assignment. Having two hidden assignments appears to cancel these messages out to some
extent, causing SP to fail at a lower density. However, this argument does not explain
why SP should succeed at densities above the satisfiability threshold; nor does it explain
why SP does not solve 1-hidden formulas for arbitrarily large r. Indeed, we find this latter
result surprising, since as r increases the majority of clauses should point more and more
consistently towards the hidden assignment in the 1-hidden case.
We note that we also performed the above experiments with n = 2 × 10 4 and with
5000 iterations, instead of the default 1000, for SP’s convergence procedure. The thresholds
of Figure 3 for 1-hidden and 2-hidden formulas appeared to be stable under both these
changes, suggesting that they are not merely artifacts of our particular experiments. We
propose investigating these thresholds as a direction for further work.
4.3 WalkSAT
We conclude with a local search algorithm, WalkSAT. Unlike the complete solvers, WalkSAT
can solve problems with n = 104 fairly close to the threshold. We performed experiments
both with a random initial state, and with a biased initial state where the algorithm starts
with 75% agreement with one of the hidden assignments (note that this is exponentially
634

Hiding Satisfying Assignments: Two are Better than One

unlikely). In both cases, we performed trials of 10 8 flips for each formula, without random
restarts, where each step does a random or greedy flip with equal probability. Since random
initial states almost certainly have roughly 50% agreement with both hidden assignments,
we expect their attractions to cancel out so that WalkSAT will have difficulty finding either
of them. On the other hand, if we begin with a biased initial state, then the attraction from
the nearby assignment will be much stronger than the other one; this situation is similar
to a 1-hidden formula, and we expect WalkSAT to find it easily. Indeed our data confirms
these expectations.
In the first part of Figure 4 we measure WalkSAT’s performance on the three types of
problems with n = 104 and r ranging from 3.7 to 7.9, and compare them with 0-hidden
formulas for r ranging from 3.7 up to 4.1, just below the threshold where they become
unsatisfiable. We see that, below the threshold, 2-hidden formulas are just as hard as
0-hidden ones when WalkSAT sets its initial state randomly; indeed, their running times
coincide to within the resolution of the figure! They both become hardest when r ≈ 4.2,
where 108 flips no longer suffice to solve them. Unsurprisingly, 2-hidden formulas are much
easier to solve when we start with a biased initial state, in which case the running time is
closer to that of 1-hidden formulas.
In the second part of Figure 4, we compare the three types of formulas at a density
very close to the threshold, r = 4.25, and measure their running times as a function of
n. The data suggests that 2-hidden formulas with random initial states are much harder
than 1-hidden ones, while 2-hidden formulas with biased initial states have running times
within a constant of that of 1-hidden formulas. Note that the median running time of all
three types of problems is polynomial in n, consistent with earlier experiments (Barthel,
Hartmann, Leone, Ricci-Tersenghi, Weigt, & Zecchina, 2002).
On the other hand, while 1-hidden formulas are much easier than 2-hidden ones for
sufficiently large or small r, they appear to be slightly harder than 2-hidden ones for 5.3 <
r < 6.3. One possible explanation for this is that while i) the solutions of a 2-hidden
formula are harder to find due to their balanced distribution, ii) there are exponentially
more solutions for 2-hidden formulas than for 1-hidden ones of the same size and density.
It seems that in this range of r, the second effect overwhelms the first, and WalkSAT finds
a solution more quickly in the 2-hidden case; but we have no explanation for why this is so
for this particular range of r. At higher densities, such as r = 8 shown in Figure 5, 2-hidden
formulas again appear to be harder than 1-hidden ones.

5. Conclusions
We have introduced an extremely simple new generator of random satisfiable 3-SAT instances which is amenable to all the mathematical tools developed for the rigorous study of
random 3-SAT instances. Experimentally, our generator appears to produce instances that
are as hard as random 3-SAT instances, in sharp contrast to instances with a single hidden
assignment. This hardness appears quite robust; our experiments have demonstrated it
both above and below the satisfiability threshold, and for algorithms that use very different
strategies, i.e., DPLL solvers (zChaff and Satz), local search algorithms (WalkSAT), and
survey propagation (SP).
635

Achlioptas, Jia, & Moore

7

Median number of flips over 100 trials

10

WalkSAT performance on HIDDEN 1, 2 and 0 formulas
HIDDEN−0
HIDDEN−1
HIDDEN−2 init 75% true
HIDDEN−2

6

10

5

10

4

10

3

10

3

Median number of flips over 100 trials

10 7
10

6

10

5

10

4

5

r

6

7

8

WalkSAT performance as a function of n
slope 2.8

HIDDEN−0
HIDDEN−2
HIDDEN−1
HIDDEN−2 init 75% true

slope 2.7
slope 1.3

4

10

3

10

2

slope 1.3

1

10
100

200

400
n

800

1600

Figure 4: The top part of the figure shows the median number of flips needed by WalkSAT
for formulas of all three types below and above the threshold, with n = 10 4 .
Below the threshold, 2-hidden formulas are just as hard as 0-hidden ones (they
coincide to within the resolution of the figure) and their running time increases
steeply as we approach the threshold. Except in the range 5.3 < r < 6.3, 2hidden formulas are much harder than 1-hidden ones unless the algorithm starts
with an (exponentially lucky) biased initial state. The bottom part of the figure
shows the median number of flips needed by WalkSAT to solve the three types of
formulas at r = 4.25 as a function of n. Here n ranges from 100 to 2000. While
the median running time for all three is polynomial, the 2-hidden problems are
much harder than the 1-hidden ones unless we start with a biased initial state.
Again, the running time of 2-hidden problems scales similarly to 0-hidden ones,
i.e., to random 3-SAT without a hidden assignment.

636

Hiding Satisfying Assignments: Two are Better than One

WalkSAT performance on HIDDEN 1 and 2 formulas with r=8

6

Median number of flips over 100 trials

10

HIDDEN−1
HIDDEN−2

5

10

4

10

3

10

2

10

1

10 2
10

3

10

4

N

10

Figure 5: The median number of flips needed by WalkSAT to solve the two types of formulas
at r = 8, above the range where 1-hidden formulas are harder. At these densities,
2-hidden formulas are again harder than 1-hidden ones, although both are much
easier than at densities closer to the threshold.

We believe that random 2-hidden instances could make excellent satisfiable benchmarks,
especially just around the satisfiability threshold, say at r = 4.25 where they appear to be
the hardest for WalkSAT (although beating SP requires somewhat higher densities).
Several aspects of our experiments suggest exciting directions for further work, including:
1. Proving that the expected running time of natural Davis-Putnam algorithms on 2hidden formulas is exponential in n for r above some critical density.
2. Explaining the different threshold behaviors of SP on 1-hidden and 2-hidden formulas.
3. Understanding how long WalkSAT takes at the midpoint between the two hidden assignments, before it becomes sufficiently unbalanced to converge to one of them.
4. Studying random 2-hidden formulas in the dense case where the number of clauses
grows more than linearly in n.

References
Achlioptas, D. (2001). Lower bounds for random 3-SAT via differential equations. Theor.
Comp. Sci., 265, 159–185.
Achlioptas, D., Beame, P., & Molloy, M. (2001). A sharp threshold in proof complexity. In
Proc. STOC, pp. 337–346.
Achlioptas, D., Gomes, C., Kautz, H., & Selman, B. (2000). Generating satisfiable problem
instances. In Proc. AAAI, pp. 256–261.
637

Achlioptas, Jia, & Moore

Achlioptas, D., & Moore, C. (2002a). Almost all graphs with average degree 4 are 3colorable. In Proc. STOC, pp. 199–208.
Achlioptas, D., & Moore, C. (2002b). The asymptotic order of the random k-SAT threshold.
In Proc. FOCS, pp. 779–788.
Achlioptas, D., & Moore, C. (2005). Two moments suffice to cross a sharp threshold. In
SIAM J. Comput. To appear.
Asahiro, Y., Iwama, K., & Miyano, E. (1996). Random generation of test instances with
controlled attributes. DIMACS Series in Disc. Math. and Theor. Comp. Sci., 26,
377–393.
Barthel, W., Hartmann, A., Leone, M., Ricci-Tersenghi, F., Weigt, M., & Zecchina, R.
(2002). Hiding solutions in random satisfiability problems: A statistical mechanics
approach. Phys. Rev. Lett., 88 (188701).
Chao, M., & Franco, J. (1986). Probabilistic analysis of two heuristics for the 3-satisfiability
problem. SIAM J. Comput., 15 (4), 1106–1118.
Cheeseman, P., Kanefsky, R., & Taylor, W. (1991). Where the really hard problems are. In
Proc. IJCAI, pp. 163–169.
Cocco, S., & Monasson, R. (2001a). Statistical physics analysis of the computational complexity of solving random satisfiability problems using backtrack algorithms. Eur.
Phys. J. B, 22, 505–531.
Cocco, S., & Monasson, R. (2001b). Trajectories in phase diagrams, growth processes and
computational complexity: how search algorithms solve the 3-satisfiability problem.
Phys. Rev. Lett, 86, 1654–1657.
Du, D., Gu, J., & Pardalos, P. (1997). Dimacs workshop on the satisfiability problem, 1996.
In DIMACS Discrete Math. and Theor. Comp. Sci., Vol. 35. AMS.
Hajiaghayi, M., & Sorkin, G. (2003). The satisfiability threshold for random 3-SAT is at
least 3.52..
Hogg, T., Huberman, B., & Williams, C. (1996). Phase transitions and complexity. Artificial
Intelligence, 81. Special issue.
Johnson, D., & Trick, M. (1996). Second dimacs implementation challenge, 1993. In DIMACS Series in Disc. Math. and Theor. Comp. Sci., Vol. 26. AMS.
Johnson, D., Aragon, C., McGeoch, L., & Shevon, C. (1989). Optimization by simulated
annealing: an experimental evaluation. Operations Research, 37 (6), 865–892.
Kaporis, A., Kirousis, L., & Lalas, E. (2003). Selecting complementary pairs of literals. In
Proc. LICS Workshop on Typical Case Complexity and Phase Transitions.
Kautz, H., Ruan, Y., Achlioptas, D., Gomes, C., Selman, B., & Stickel, . (2001). Balance
and filtering in structured satisfiable problems. In Proc. IJCAI, pp. 351–358.
Kirousis, L., Kranakis, E., Krizanc, D., & Stamatiou, Y. (1998). Approximating the unsatisfiability threshold of random formulas. Random Structures Algorithms, 12 (3),
253–269.
638

Hiding Satisfying Assignments: Two are Better than One

Li, C., & Anbulagan (1997a). Heuristics based on unit propagation for satisfiability problems. In Proc. IJCAI, pp. 366–371.
Li, C., & Anbulagan (1997b). Look-ahead versus look-back for satisfiability problems. In
Proc. 3rd Intl. Conf. on Principles and Practice of Constraint Programming, pp. 341–
355.
Massacci, F. (1999). Using walk-SAT and rel-SAT for cyptographic key search. In Proc.
IJCAI, pp. 290–295.
Mézard, M., & Zecchina, R. (2002).
Random k-satisfiability: from an analytic
solution to a new efficient algorithm.
Phys. Rev. E, 66.
Available at:
http://www.ictp.trieste.it/˜zecchina/SP/.
Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT
problems. In Proc. AAAI, pp. 459–465.
Monasson, R. (2005). Average case analysis of DPLL for random decision problems. In
Proc. RANDOM.
Morris, P. (1993). The breakout method for escaping from local minima. In Proc. AAAI,
pp. 40–45.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: engineering
an efficient SAT solver. In Proc. 38th Design Automation Conference, pp. 530–535.
Selman, B., Kautz, H., & Cohen, B. (1996). Local search strategies for satisfiability testing.
In Proc. 2nd DIMACS Challange on Cliques, Coloring, and Satisfiability.
Shaw, P., Stergiou, K., & Walsh, T. (1998). Arc consistency and quasigroup completion. In
Proc. ECAI, workshop on binary constraints.
Van Gelder, A. (1993). Problem generator mkcnf.c. In Proc. DIMACS. Challenge archive.
Wormald, N. (1995). Differential equations for random processes and random graphs. Ann.
Appl. Probab., 5 (4), 1217–1235.

639

Journal of Artificial Intelligence Research 24 (2005) 919-931

Submitted 01/05; published 12/05

Engineering Note
Optiplan: Unifying IP-based and Graph-based Planning
Menkes H.L. van den Briel

menkes@asu.edu

Department of Industrial Engineering
Arizona State University, Tempe, AZ 85281 USA

Subbarao Kambhampati

rao@asu.edu

Department of Computer Science and Engineering
Arizona State University, Tempe, AZ 85281 USA

Abstract
The Optiplan planning system is the first integer programming-based planner that
successfully participated in the international planning competition. This engineering note
describes the architecture of Optiplan and provides the integer programming formulation
that enabled it to perform reasonably well in the competition. We also touch upon some
recent developments that make integer programming encodings significantly more competitive.

1. Introduction
Optiplan is a planning system that uses integer linear programming (IP) to solve STRIPS
planning problems. It is the first such system to take part in the international planning
competition (IPC) and was judged the second best performer in four competition domains
of the optimal track for propositional domains. Optiplan’s underlying integer programming
formulation extends the state change model by Vossen and his colleagues (1999). Its architecture is very similar to that of Blackbox (Kautz & Selman, 1999) and GP-CSP (Do &
Kambhampati, 2001), but instead of unifying satisfiability (SAT) or constraint satisfaction
(CSP) with graph based planning, Optiplan uses integer programming. Like Blackbox and
GP-CSP, Optiplan works in two phases. In the first phase a planning graph is built and
transformed into an IP formulation, then in the second phase the IP formulation is solved
using the commercial solver ILOG CPLEX (ILOG Inc., 2002).
A practical difference between the original state change model and Optiplan is that the
former takes as input all ground actions and fluents over all initialized plan steps, while
the latter takes as input just those actions and fluents that are instantiated by Graphplan
(Blum & Furst, 1995). It is well known that the use of planning graphs has a significant
effect on the size of the final encoding no matter what combinatorial transformation method
(IP, SAT, or CSP) is used. For instance, Kautz and Selman (1999) as well as Kambhampati
(1997) pointed out that Blackbox’s success over Satplan (Kautz & Selman, 1992) was mainly
explained by Graphplan’s ability to produce better, more refined, propositional structures
than Satplan. In addition, Optiplan allows propositions to be deleted without being required
as preconditions. Such state changes are not modeled in the original state change model, and
therefore Optiplan can be considered to be a more general encoding. One more, although
c
°2005
AI Access Foundation. All rights reserved.

Van den Briel, & Kambhampati

minor, implementation detail between Optiplan and the state change model is that Optiplan
reads in PDDL files.
This engineering note is organized as follows. Section 2 provides a brief background on
integer programming and Section 3 discusses previous IP approaches to planning. Section
4 describes the Optiplan planning system and its underlying IP formulation. In Section 5
we give some experimental results and look at Optiplan’s performance in the international
planning competition of 2004 (IPC4). Conclusions and a brief discussion on some recent
developments is given in Section 6.

2. Background
A linear program is represented by a linear objective function and a set of inequalities, such
as min{cx : Ax ≥ b, x ≥ 0} where x an n-dimensional column vector of variables, A is an
m by n matrix, c an n-dimensional row vector, and b an m-dimensional column vector. If
all variables are constrained to be integers then we have an integer (linear) program, and if
all variables are restricted to 0-1 values then we have a binary integer program.
The most widely used method for solving general integer programs is by using branch
and bound on the linear programming relaxation. Branch and bound is a general search
method in which subproblems are created that restrict the range of the integer variables,
and the linear programming relaxation is a linear program obtained from the original integer
program by omitting the integrality constraints. An ideal formulation of an integer program
is one for which the solution of the linear programming relaxation is integral. Even though
every integer program has an ideal formulation (Wolsey, 1998), in practice it is very hard to
characterize the ideal formulation as it may require an exponential number of inequalities.
In problems where the ideal formulation cannot be determined, it is often desirable to find
a strong formulation of the integer program. Suppose that P1 = min{cx : A1 x ≥ b1 , x ≥ 0}
and P2 = min{cx : A2 x ≥ b2 , x ≥ 0} are the linear programming relaxations of two IP
formulations of a problem, then we say that formulation P1 is stronger than formulation P2
if P1 ⊂ P2 . That is, the set of solutions of P1 is subsumed by the set of solutions of P2 .

3. Integer Programming Approaches to Planning
Despite the vast amount of research that has been conducted in the field of AI planning, the
use of linear programming (LP) and integer linear programming have only been explored at
a marginal level. This is quite surprising since (mixed) integer linear programming provide
feasible environments for using numeric constraints and arbitrary linear objective functions,
two important aspects in real-world planning problems.
Only a handful of works have explored the use of LP and IP techniques in AI planning.
Bylander (1997) developed an IP formulation for classical planning and used the LP relaxation as a heuristic for partial order planning. The results, however, do not seem to scale
well compared to planning graph and satisfiability based planners.
The difficulty in developing strong IP formulations is that the performance often depends on the way the IP formulation is constructed. Vossen et al. (1999) compared two
formulations for classical planning. First, they consider a straightforward IP formulation
based on converting the propositional representation given by Satplan (Kautz & Selman,
920

Optiplan: Unifying IP-based and Graph-based Planning

1992) to an IP formulation with variables that take the value 1 if a certain proposition
is true, and 0 otherwise. In this formulation, the assertions expressed by IP constraints
directly correspond to the logical conditions of the propositional representation. Second,
they consider an IP formulation in which the original propositional variables are replaced
by state change variables. State change variables take the value 1 if a certain proposition is
added, deleted, or persisted, and 0 otherwise. Vossen et al. show that the formulation based
on state change variables outperforms the straightforward formulation based on converting
the propositional representation.
Dimopoulos (2001) improves the IP formulation based on state change variables by identifying valid inequalities that tighten the formulation. Yet, even stronger IP formulations
are given by Bockmayr and Dimopoulos (1998, 1999), but their IP formulations contain
domain dependent knowledge and are, therefore, limited to solving problems of specific
problem domains only.
LP and IP techniques have also been explored for non-classical planning. Dimopoulos
and Gerevini (2002) describe a mixed integer programming formulation for temporal planning and Wolfman and Weld (1999) use an LP formulation in combination with a SAT
formulation to solve resource planning problems. Kautz and Walser (1999) also use IP
formulations for resource planning problems but, in addition, incorporate action costs and
complex objectives.
So far, none of these IP approaches to AI planning ever participated in the IPC, making
it harder to assess the relative effectiveness of this line of work. Optiplan, a planner based
on the state change formulation, is the first IP-based planner to do so.

4. Optiplan
Optiplan is a planning graph based planner and works as follows. First we build the
planning graph to the level where all the goal fluents appear non-mutex. Then we compile
the planning graph into an integer program and solve it. If no plan is found, the planning
graph is extended by one level and the new graph is again compiled into an integer program
and solved again. This process is repeated until a plan is found.
In the remainder of this section we give the IP formulation that is used by Optiplan. In
order to present the IP formulation we will use the following notation. F is the set of fluents
and A is the set of actions (operators). The fluents that are true in the initial state and
the fluents that must be true in the goal are given by I and G respectively. Furthermore,
we will use the sets:
• pref ⊆ A, ∀f ∈ F, set of actions that have fluent f as precondition;
• addf ⊆ A, ∀f ∈ F, set of actions that have fluent f as add effect;
• delf ⊆ A, ∀f ∈ F, set of actions that have fluent f as delete effect;
Variables are defined for each layer 1 ≤ t ≤ T in the planning graph. There are
variables for the actions and there are variables for the possible state changes a fluent can
make, but only those variables that are reachable and relevant by planning graph analysis
are instantiated. For all a ∈ A, t ∈ 1, ..., T we have the action variables
921

Van den Briel, & Kambhampati

ya,t =

½

1 if action a is executed in period t,
0 otherwise.

The “no-op” actions are not included in the ya,t variables but are represented separately
by the state change variable xmaintain
.
f,t
Optiplan is based on the state change formulation (Vossen et al., 1999). In this formulation fluents are not represented explicitly, instead state change variables are used to model
transitions in the world state. That is, a fluent is true if and only if it is added to the state by
preadd
xadd
, or if it is persisted from the previous state by xmaintain
. Optiplan extends
f,t or xf,t
f,t
the state change formulation (Vossen et al., 1999) by introducing an extra state change
variable, xdel
f,t , that allows actions to delete fluents without requiring them as preconditions.
The original state change formulation did not allow for these actions, so therefore we added
these new state change variables to keep track of such state changes and altered the model
to take these new variables into account. In the IPC4 domains of Airport and PSR there are
many actions that delete fluents without requiring them as preconditions, therefore making
the original state change formulation ineffective. Also, Optiplan instantiates only those
variables and constraints that are reachable and relevant through planning graph analysis,
and therefore creates a smaller encoding than the original one. For all f ∈ F, t ∈ 1, ..., T
we have the following state change variables:
½
1 if fluent f is propagated in period t,
maintain
=
xf,t
0 otherwise.
½
1 if action a is executed in period t such that a ∈ pref ∧ a ∈
/ delf ,
preadd
xf,t
=
0 otherwise.
½
1 if action a is executed in period t such that a ∈ pref ∧ a ∈ delf ,
predel
xf,t
=
0 otherwise.
½
1 if action a is executed in period t such that a ∈
/ pref ∧ a ∈ addf ,
=
xadd
f,t
0 otherwise.
½
1 if action a is executed in period tsuch that a ∈
/ pref ∧ a ∈ delf ,
xdel
=
f,t
0 otherwise.
In summary: xmaintain
= 1 if the truth value of a fluent is propagated; xpreadd
= 1 if an
f,t
f,t
action is executed that requires a fluent and does not delete it; xpredel
= 1 if an action is
f,t
add
executed that requires a fluent and deletes it; xf,t = 1 if an action is executed that does
not require a fluent and adds it; and xdel
f,t = 1 if an action is executed that does not require
a fluent and deletes it. The complete IP formulation of Optiplan is given by the following
objective function and constraints.

4.1 Objective
For classical AI planning problems, no minimization or maximization is required, instead
we want to find a feasible solution. The search for a solution, however, may be guided by
922

Optiplan: Unifying IP-based and Graph-based Planning

an objective function such as the minimization of the number of actions, which is currently
implemented in Optiplan. The objective function is given by:
min

XX

ya,t

(1)

a∈A i∈T

Since the constraints guarantee feasibility we could have used any linear objective function. For example, we could easily set up an objective to deal with cost-sensitive plans
(in the context of non-uniform action cost), utility-sensitive plans (in the context of oversubscription and goal utilities), or any other metric that can be transformed to a linear
expression. Indeed this flexibility to handle any linear objective function is one of the advantages of IP formulations.

4.2 Constraints
The requirements on the initial and goal transition are given by:
xadd
f,0 = 1
maintain preadd
xadd
, xf,0
f,0 , xf,0
maintain
xadd
+ xpreadd
f,T + xf,T
f,T

∀f ∈ I

(2)

=0

∀f ∈
/I

(3)

≥1

∀f ∈ G

(4)

Where constraints (2), and (3) add the initial fluents in step 0 so that they can be used
by the actions that appear in the first layer (step 1) of the planning graph. Constraints
(4) represent the goal state requirements, that is, fluents that appear in the goal must be
added or propagated in step T .
The state change variables are linked to the actions by the following effect implication
constraints. For each f ∈ F and 1 ≤ t ≤ T we have:
X
ya,t ≥ xadd
(5)
f,t
a∈addf \pref

ya,t ≤ xadd
f,t
X

ya,t ≥

∀a ∈ addf \ pref

xdel
f,t

(6)
(7)

a∈delf \pref

ya,t ≤ xdel
f,t
X

∀a ∈ delf \ pref

ya,t ≥ xpreadd
f,t

(8)
(9)

a∈pref \delf

ya,t ≤ xpreadd
f,t
X

ya,t = xpredel
f,t

∀a ∈ pref \ delf

(10)
(11)

a∈pref ∧delf

Where constraints (5) to (11) represent the logical relations between the action and
state change variables. The equality sign in (11) is because all actions that have f as a
923

Van den Briel, & Kambhampati

precondition and as a delete effect are mutually exclusive. This also means that we can
substitute out the xpredel
variables, which is what we have done in the implementation of
f,t
Optiplan. We will, however, use the variables here for clarity. Mutexes also appear between
different state change variables and these are expressed by constraints as follows:

predel
maintain
≤1
+ xdel
xadd
f,t + xf,t
f,t + xf,t

(12)

predel
xpreadd
+ xmaintain
+ xdel
≤1
f,t
f,t + xf,t
f,t

(13)

Where constraints (12) and (13) restrict certain state changes from occurring in parallel.
del
For example, xmaintain
(propagating fluent f at step t) is mutually exclusive with xadd
f,t
f,t , xf,t ,
and xpredel
(adding or deleting f at t).
f,t
Finally, the backward chaining requirements and binary constraints are represented by:

add
maintain
xpreadd
+ xmaintain
+ xpredel
≤ xpreadd
f,t
f,t
f,t
f,t−1 + xf,t−1 + xf,t−1

∀f ∈ F, t ∈ 1, ..., T

(14)

del maintain
xpreadd
, xpredel
, xadd
∈{0, 1}
f,t , xf,t , xf,t
f,t
f,t

(15)

ya,t ∈{0, 1}

(16)

Where constraints (14) describe the backward chaining requirements, that is, if a fluent
f is added or maintained in step t−1 then the state of f can be changed by an action in step
t through xpreadd
, or xpredel
, or it can be propagated through xmaintain
. Constraints (15)
f,t
f,t
f,t
and (16) are the binary constraints for the state change and action variables respectively.

Loc1

Truck1

Loc2

Truck2

Figure 1: A simple logistics example

4.3 Example
In this example, we show how some of the constraints are initialized and we comment on
the interaction between the state change variables and the action variables.
Consider a simple logistics example in which there are two locations, two trucks, and
one package. The package can only be transported from one location to another by one of
the trucks. We built a formulation for three plan steps. The initial state is that the package
924

Optiplan: Unifying IP-based and Graph-based Planning

and the trucks are all at location 1 as given in Figure 1. The initial state constraints are:
xadd
pack1

at loc1,0

xadd
truck1

at loc1,0
add
xtruck2 at loc1,0
add maintain preadd
xf,0 , xf,0
, xf,0

=1
=1
=1
=0

f 6= I

The goal is to get the package at location 2 in three plan steps, which is expressed as
follows:
xadd
pack1

at loc2,3

+ xmaintain
pack1 at

loc2,3

+ xpreadd
pack1 at

loc2,3

≥1

We will not write out all effect implication constraints, but we will comment on a few
of them. If xadd
f,t = 1 for a certain fluent f , then we have to execute at least one action a
that has f as an add effect and not as a precondition. For example:
yunload

truck1 at loc2,t

+ yunload

truck2 at loc2,t

≥ xadd
pack1

at loc2,t

preadd
The state changes for xdel
have a similar requirement, that is if we change
f,t and xf,t
the state through del or preadd then we must execute at least one action a with the corresponding effects. On the other hand, if we execute an action a then we must change all
fluent states according to the effects of a. For example:

yunload

truck1 at loc2,t

≤ xadd
pack1

yunload

truck1 at loc2,t

≤

yunload

truck1 at loc2,t

=

at loc2,t
preadd
xtruck at loc2,t
xpredel
pack1 in truck1,t

There is a one-to-one correspondence (note the equality sign) between the execution of
actions and the xpredel
state change variables. This is because, actions that have the same
f,t
predel effect must be mutex. Mutexes are also present between state changes. For example,
a fluent f that is maintained (propagated) cannot be added or deleted. The only two state
changes that are not mutex are the add and the preadd. This is because the add state
change behaves like the preadd state change if the corresponding fluent is already present
in the state of the world. This is why we introduce two separate mutex constraints, one
that includes the add state change and one that includes the preadd. An example for the
constraints on the mutex state changes are as follows:
xadd
pack1

in truck1,t

xpreadd
pack1 in

truck1,t

+ xmaintain
pack1 in

truck1,t

+ xdel
pack1

in truck1,t

+ xpredel
pack1

in truck1,t

≤ 1

+ xmaintain
pack1 in

truck1,t

+ xdel
pack1

in truck1,t

+ xpredel
pack1

in truck1,t

≤ 1

The state of a fluent can change into another state only if correct state changes have
occurred previously. Hence, a fluent can be deleted, propagated, or used as preconditions
in step t if and only if it was added or propagated in step t − 1. For example:
xpreadd
pack1 in

predel
maintain
truck1,t + xpack1 in truck1,t + xpack1 in truck1,t
add
maintain
xpreadd
pack1 in truck1,t−1 + xpack1 in truck1,t−1 + xpack1 in truck1,t−1

925

≤

Van den Briel, & Kambhampati

t=0
xadd
pack1
xadd
truck1
xadd
truck2

at loc1,0

t=1
yload truck1 at loc1,1
xadd
pack1 in truck1,1
xpredel
pack1 at loc1,1
xpreadd
truck1 at loc1,1

at loc1,0

xmaintain
truck2 at

at loc1,0

t=2
ydrive truck1 loc1 loc2,2
xmaintain
pack1 in truck1,2
xadd
truck1
xpredel
truck1

at loc2,2

t=3
yunload truck1 at loc2,3
xadd
pack1 at loc2,3
xpredel
pack1 in truck1,3
xpreadd
truck1 at loc2,3

at loc1,2

loc1,1

Table 1: Solution to the simple logistics example. All displayed variables have value 1 and
all other variables have value 0.

This simple problem has a total of 107 variables (41 action and 66 state change) and
91 constraints. However, planning graph analysis fixes 53 variables (28 action and 25 state
change) to zero. After substituting these values and applying presolve techniques that are
built in the ILOG CPLEX solver, this problem has only 13 variables and 17 constraints.
The solution for this example is given in Table 1. Note that, when there are no actions
that actively delete f , there is nothing that ensures xmaintain
to be true whenever f was
f,t
true in the preceding state (for example, see the fluent truck2 at loc1). Since negative
preconditions are not allowed, having the option of letting xmaintain
be false when it should
f,t
have been true cannot cause actions to become executable when they should not be. We
will not miss any solutions because constraints (4) ensure that the goal fluents are satisfied,
therefore forcing xmaintain
to be true whenever this helps us generate a plan.
f,t

5. Experimental Results
First we compare Optiplan with the original state change model, and then we check how
Optiplan performed in the IPC of 2004.
Optiplan and the original state change formulation are implemented in two different
languages. Optiplan is implemented in C++ using Concert Technology, which is a set of
libraries that allow you to embed ILOG CPLEX optimizers (ILOG Inc., 2002), and the
original state change model is implemented in AMPL (Fourer, Gay, & Kernighan, 1993),
which is a modeling language for mathematical programming. In order to compare the
formulations that are produced by these two implementations, they are written to an output
file using the MPS format. MPS is a standard data format that is often used for transferring
linear and integer linear programming problems between different applications. Once the
MPS file, which contains the IP formulation for the planning problem, is written, it is read
and solved by ILOG CPLEX 8.1 on a Pentium 2.67 GHz with 1.00 GB of RAM.
Table 3 shows the encoding size of the two implementations, where the encoding size is
characterized by the number of variables and the number of constraints in the formulation.
Both the encoding size before and after applying ILOG CPLEX presolve is given. Presolve is
a problem reduction technique (Brearley, Mitra, & Williams, 1975) that helps most linear
programming problems by simplifying, reducing and eliminating redundancies. In short,
926

Optiplan: Unifying IP-based and Graph-based Planning

Problem
bw-sussman
bw-12step
bw-large-a
att-log0
log-easy
log-a

State change model
Before presolve
After presolve
#Var. #Cons. #Var. #Cons.
486
878
196
347
3900
7372
1663
3105
6084
11628
2645
5022
1932
3175
25
35
24921
41457
1348
2168
50259
85324
3654
6168

Optiplan
Before presolve
After presolve
#Var. #Cons. #Var. #Cons.
407
593
105
143
3534
4998
868
1025
5639
8690
1800
2096
117
149
0
0
2534
3029
437
592
5746
7480
1479
2313

Table 2: Encoding size of the original state change formulation and Optiplan before and
after ILOG CPLEX presolve. #Var. and #Cons. give the number of variables
and constraints respectively.

Problem
bw-sussman
bw-12step
bw-large-a
bw-large-b
att-log0
att-log1
att-log2
att-log3
att-log4
att-loga
rocket-a
rocket-b
log-easy
log-a
log-b
log-c

#Var.
196
1663
2645
6331
25
114
249
2151
2147
2915
1532
1610
1348
3654
4255
5457

State change model
#Cons. #Nodes
347
0
3105
19
5022
2
12053
14
35
0
164
0
371
10
3686
15
3676
12
4968
975
2653
517
2787
191
2168
43
6168
600
6989
325
9111
970

Time
0.01
4.28
8.45
581.92
0.01
0.03
0.07
0.64
0.71
173.56
32.44
9.90
0.96
145.31
96.47
771.36

#Var.
105
868
1800
4780
0
29
81
181
360
1479
991
1071
437
1479
1718
2413

Optiplan
#Cons. #Nodes
143
0
1025
37
2096
0
5454
10
0
0
35
0
99
0
228
0
507
0
2312
19
1644
78
1788
24
592
0
2313
19
2620
187
3784
37

Time
0.01
1.65
0.72
72.58
0.01
0.01
0.01
0.03
0.04
2.71
5.48
3.12
0.04
2.66
14.06
16.07

Table 3: Performance and encoding size of the original state change formulation and Optiplan. #Var. and #Cons. give the number of variables and constraints after
ILOG CPLEX presolve, and #Nodes give the number of nodes explored during
branch-and-bound before finding the first feasible solution.

927

Van den Briel, & Kambhampati

presolve tries to remove redundant constraints and fixed variables from the formulation,
and aggregate (substitute out) variables if possible.
From the encoding size before presolve, which is the actual encoding size of the problem,
we can see how significant the use of planning graphs is. Optiplan, which instantiates
only those fluents and actions that are reachable and relevant through planning graph
analysis, produces encodings that in some cases are over one order of magnitude smaller than
the encodings produced by the original state change model, which instantiates all ground
fluents and actions. Although the difference in the encoding size reduces substantially
after applying presolve, planning graph analysis still finds redundancies that presolve fails
to detect. Consequently, the encodings produced by Optiplan are still smaller than the
encodings that are produced by the original state change model.
The performance (and the encoding size after presolve) of Optiplan and the original
state change model are given in Table 3. Performance is measured by the time to find the
first feasible solution. The results show the overall effectiveness of using planning graph
analysis. For all problems Optiplan not only generates smaller encodings it also performs
better than the encodings generated by the state change model.
5.1 IPC Results
Optiplan participated in the propositional domains of the optimal track in the IPC 2004. In
this track, planners could either minimize the number of actions, like BFHSP and Semsyn;
minimize makespan, like CPT, HSP*a, Optiplan, Satplan04, and TP-4; or minimize some
other metric.
The IPC results of the makespan optimal planners are given in Figure 2. All results were
evaluated by the competition organizers by looking at the runtime and plan quality graphs.
Also, all planners were compared to each other by estimating their asymptotic runtime and
by analyzing their solution quality performance. Out of the seven competition domains,
Optiplan was judged second best in four of them. This is quite remarkable because integer
programming has hitherto not been considered competitive in planning.
Optiplan reached second place in the Optical Telegraph and the Philosopher domains.
In these domains Optiplan is about one order of magnitude slower than Satplan04, but it
clearly outperforms all other participating planners. In the Pipesworld Tankage domain,
Optiplan was awarded second place together with Satplan04, and in the Satellite domain
Optiplan, CPT, and Semsyn all tied for second place. In the other domains Optiplan did
not perform too well. In the Airport domain, Optiplan solves the first 17 problems and
problem 19, but it takes the most time to do so. For the Pipesworld Notankage and the
PSR domains, Optiplan not only is the slowest it also solves the fewest number of problems
among the participating planners.
In looking at the domains and problems where Optiplan has difficulty scaling, we notice
that these are problems that lead to very large IP encodings. Since the size of the encoding is
a function of plan length, Optiplan often fails to solve problems that have long solution plans.
One way to resolve this issue is to de-link the encoding size from solution length, which is
what we have done in some of our recent work (van den Briel, Vossen, & Kambhampati,
2005). In fact, in the year following the IPC4 we developed novel IP encodings that (1)
928

Optiplan: Unifying IP-based and Graph-based Planning

model transitions in the individual fluents as separate but loosely coupled network flow
problems, and that (2) control the encoding length by generalizing the notion of parallelism.

6. Conclusions
The Optiplan planning system performs significantly better than the original state change
model by Vossen and his colleagues (1999). It performed respectably at the IPC4, but still
lags behind SAT- and CSP-based planners, like Blackbox(Chaff), Satplan04(Siege), and
GP-CSP. We believe, however, that this performance gap is not because IP techniques are
inferior to SAT and CSP, but rather a reflection of the types of IP formulations that have
been tried so far. Specifically, the encodings that have been tried until now have not been
tailored to the strengths of the IP solvers (Chandru & Hooker, 1999).
Our experience with Optiplan has encouraged us to continue working on improved IP
formulations for AI planning. In our recent work (van den Briel, Vossen, & Kambhampati,
2005) we model fluents as loosely coupled network flow problems and control the encoding
length by generalizing the notion of parallelism. The resulting IP encodings are solved
within a branch-and-cut algorithm and yield impressive results. Also, this new approach
has been shown to be highly competitive with the state-of-the-art SAT-based planners.

References
Blum, A., & Furst, M. (1995). Fast planning through planning graph analysis. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), pp.
1636–1642.
Bockmayr, A., & Dimopoulos, Y. (1998). Mixed integer programming models for planning problems. In Working notes of the CP-98 Constraint Problem Reformulation
Workshop.
Bockmayr, A., & Dimopoulos, Y. (1999). Integer programs and valid inequalities for planning problems. In Proceedings of the European Conference on Planning (ECP-99),
pp. 239–251. Springer-Verlag.
Brearley, A., Mitra, G., & Williams, H. (1975). Analysis of mathematical programming
problems prior to applying the simplex algorithm. Mathematical Programming, 8,
54–83.
Bylander, T. (1997). A linear programming heuristic for optimal planning. In AAAI97/IAAI-97 Proceedings, pp. 694–699.
Chandru, V., & Hooker, J. (1999). Optimization Methods for Logical Inference. John Wiley
& Sons, New York.
Dimopoulos, Y. (2001). Improved integer programming models and heuristic search for
AI planning. In Proceedings of the European Conference on Planning (ECP-01), pp.
301–313. Springer-Verlag.
929

Van den Briel, & Kambhampati

10000

10000

1000

1000

100
Time in sec.

Time in sec.

100
10
Optiplan

1

10
1
Optiplan

Satplan04
0.1

0.1

CPT

0.01

0.01
0

5

10

15

20

25

30

35

40

45

Satplan04
CPT

TP4

0

50

5

10

10000

10000

1000

1000

100

100

Time in sec.

Time in sec.

15

20

25

30

35

Satellite problem nr.

Airport problem nr.

10
Optiplan

1

10
1

Satplan04
0.1

Optiplan
0.1

CPT

Satplan04

TP4

CPT

0.01

0.01
0

5

10

15

20

25

30

35

40

45

50

0

5

10

20

25

30

1000

1000

100

100

10

Time in sec.

10000

Optiplan
Satplan04

1

HSPS-A
3

4

5

6

7

8

9

10

11

12 13

0

14

5

Satplan04

CPT

TP4

10

15

20

Philosophers problem nr.

Optical telegraph problem nr.

10000
Optiplan

1000

Satplan04
HSPS-A

100

TP4
CPT

10
1
0.1
0.01
0

50

Optiplan
HSPS-A

0.01

0.01
2

45

1
0.1

TP4

1

40

10

CPT
0.1

0

35

Pipesworld tankge problem nr.

10000

Time in sec.

Time in sec.

Pipesworld notankage problem nr.

15

5

10

15

20

25

30

35

40

45

50

PSR problem nr.

Figure 2: IPC 2004 results for the makespan optimal planners.

930

25

Optiplan: Unifying IP-based and Graph-based Planning

Dimopoulos, Y., & Gerevini, A. (2002). Temporal planning through mixed integer programming. In Proceeding of the AIPS Workshop on Planning for Temporal Domains,
pp. 2–8.
Do, M., & Kambhampati, S. (2001). Planning as constraint satisfaction: Solving the planning graph by compiling it into CSP. Artificial Intelligence, 132 (2), 151–182.
Fourer, R., Gay, D., & Kernighan, B. (1993). AMPL: A Modeling Language for Mathematical Programming. Duxbury Press, Belmont, CA.
ILOG Inc., Mountain View, CA (2002). ILOG CPLEX 8.0 user’s manual.
Kambhampati, S. (1997). Challenges in bridging plan synthesis paradigms. In Proceedings
of the 16th International Joint Conference on Artificial Intelligence (IJCAI-97), pp.
44–49.
Kautz, H., & Selman, B. (1992). Planning as satisfiability. In Proceedings of the European
Conference on Artificial Intelligence (ECAI).
Kautz, H., & Selman, B. (1999). Blackbox: Unifying sat-based and graph-based planning.
In Proceedings of the 18th International Joint Conference on Artificial Intelligence
(IJCAI-99), pp. 318–325.
Kautz, H., & Walser, J. (1999). State-space planning by integer optimization. In AAAI99/IAAI-99 Proceedings, pp. 526–533.
van den Briel, M., Vossen, T., & Kambhampati, S. (2005). Reviving integer programming approaches for AI planning: a branch-and-cut framework. In Proceedings of the
International Conference on Automated Planning and Scheduling (ICAPS-05), pp.
310–319.
Vossen, T., Ball, M., Lotem, A., & Nau, D. (1999). On the use of integer programming
models in AI planning. In Proceedings of the 18th International Joint Conference on
Artificial Intelligence (IJCAI-99), pp. 304–309.
Wolfman, S., & Weld, D. (1999). The LPSAT engine and its application to resource planning. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-99), pp. 310–317.
Wolsey, L. (1998). Integer Programming. Wiley-Interscience Series in Discrete Mathematics
and Optimization. John Wiley & Sons, New York.

931

Journal of Artificial Intelligence Research 24 (2005) 799-849

Submitted 08/04; published 12/05

Probabilistic Hybrid Action Models
for Predicting Concurrent Percept-driven Robot Behavior
Michael Beetz

BEETZ @ IN . TUM . DE

Department of Computer Science IX, Technische Universität München,
Boltzmannstr. 3, D-81667 Garching, Germany,

Henrik Grosskreutz

GROSSKREUTZ @ CS . RWTH - AACHEN . DE

Department of Computer Science, Aachen University of Technology
D-52056 Aachen, Germany

Abstract
This article develops Probabilistic Hybrid Action Models ( PHAMs), a realistic causal model
for predicting the behavior generated by modern percept-driven robot plans. PHAMs represent
aspects of robot behavior that cannot be represented by most action models used in AI planning: the
temporal structure of continuous control processes, their non-deterministic effects, several modes
of their interferences, and the achievement of triggering conditions in closed-loop robot plans.
The main contributions of this article are: (1) PHAMs, a model of concurrent percept-driven
behavior, its formalization, and proofs that the model generates probably, qualitatively accurate
predictions; and (2) a resource-efficient inference method for PHAMs based on sampling projections
from probabilistic action models and state descriptions. We show how PHAMs can be applied
to planning the course of action of an autonomous robot office courier based on analytical and
experimental results.

1. Introduction
Most autonomous robots are equipped with restricted, unreliable, and inaccurate sensors and effectors and operate in complex and dynamic environments. A successful approach to deal with the
resulting uncertainty is the use of controllers that prescribe the robots’ behavior in terms of concurrent reactive plans (CRPs) — plans that specify how the robots are to react to sensory input
in order to accomplish their jobs reliably (e.g., McDermott, 1992a; Beetz, 1999). Reactive plans
are successfully used to produce situation specific behavior, to detect problems and recover from
them automatically, and to recognize and exploit opportunities (Beetz et al., 2001). These kinds
of behaviors are particularly important for autonomous robots that have only uncertain information
about the world, act in dynamically changing environments, and are to accomplish complex tasks
efficiently.
Besides reliability and flexibility, foresight is another important capability of competent autonomous robots (McDermott, 1992a). Temporal projection, the computational process of predicting what will happen when a robot executes its plan, is essential for the robots to plan their intended
courses of action successfully. To be able to project their plans, robots must have causal models
that represent the effects of their actions. Most robot action planners use representations that include discrete action models and plans that define partial orders on actions. Therefore, they cannot
automatically generate, reason about, and revise modern reactive plans. This has two important
drawbacks. First, the planners cannot accurately predict and diagnose the behavior generated by
their plans because they abstract away from important aspects of reactive plans. Second, the planc
2005
AI Access Foundation. All rights reserved.

B EETZ & G ROSSKREUTZ

ners cannot exploit the control structures provided by reactive plan languages to make plans more
flexible and reliable.
In this article we develop PHAMs (Probabilistic Hybrid Action Models), action models that
have the expressiveness for the accurate prediction of behavior generated by concurrent reactive
plans. To the best of our knowledge, PHAMs are the only action representation used in action
planning that provides programmers with means for describing the interference of simultaneous,
concurrent effects, probabilistic state and action models, as well as exogenous events. PHAMs have
been successfully applied by an autonomous robot office courier and a museum tour-guide robot to
make predictions of full-size robot plans during the execution of these plans (Beetz, 2001).
This article makes several important contributions to the area of decision-theoretic robot action
planning. First, we describe PHAMs, formal action models that allow for the prediction of the
qualitative behavior generated by concurrent reactive plans. Second, we show how PHAMs can be
implemented in a resource efficient way such that predictions based on PHAMs can be performed
by robots while executing their plans. Third, we apply the plan projection method to probabilistic
prediction-based schedule debugging and analyze it in the context of a robot office courier (Beetz,
2001).
Before starting with the technical part of the article we would like to make several remarks. In
this article we restrict ourselves to navigation actions and model them exactly as they are implemented in one of the most successful autonomous robot navigation systems (Burgard et al., 2000).
The reason is that we want to close the gap between action models used in AI planning systems and
the control programs that are used by autonomous robots and the behavior they produce. The control
programs that we model have proven themselves to achieve reliable, high performance navigation
behavior. In the Minerva experiment, they controlled the navigation in a crowded museum for more
than 93 hours. During their execution, the navigation plans have been revised by a planning module
about 3200 times without causing any deadlocks between interacting, concurrent control processes
(Beetz, 2002a; Beetz et al., 2001). In robot office courier experiments, we have applied plan revision
methods that enabled the robot to plan ahead for about 15-25 minutes. We consider this to be a time
scale sufficient for improving the robot’s performance through planning. However, the performance
gains that can in principle be achieved through navigation planning are often small compared to
those that can be achieved by planning manipulation tasks.
Although we use navigation as our only example, the same modeling techniques apply to other
mechanisms of autonomous robots, such as vision (Beetz et al., 1998), communication (Beetz &
Peters, 1998), and manipulation (Beetz, 2000) equally well. The reasons that we do not cover
these kinds of actions in this article are that they require additional reasoning capabilities and at the
moment these models can only be validated with respect to robot simulations. The additional robot
capabilities that would have to be modeled include symbol grounding/object recognition (Beetz,
2000), changing states of objects, and more thorough models of the belief states of robots (Schmitt
et al., 2002). Addressing these issues is well beyond the scope of this article.
In the remainder of the article we introduce the basic conceptualization underlying PHAMs and
describe two realizations of them: one for studying their formal properties and another one targeted at their efficient implementation. We also show how PHAMs are employed in the context of
transformational robot planning.
The article is organized as follows. Section 2 describes everyday activity as our primary class
of application problems. We introduce concurrent reactive plans (CRPs) as means for producing
characteristic patterns of everyday activity and identify technical problems in the prediction of the
800

P ROBABILISTIC H YBRID ACTION M ODELS

physical robot behavior that CRPs generate. Section 3 explains how the execution of CRPs and
the physical and computational effects of plan execution can be modeled using PHAMs. PHAMs
describe the behavior of the robot as a sequence of control modes where in each mode the continuous
behavior is specified by a control law. Mode transitions are triggered by the controlled system
satisfying specified mode transition conditions. We then introduce a set of predicates that we use
to represent our conceptualization formally. Section 4 and 5 describe two different approaches to
predicting the behavior produced by concurrent reactive plans in the context of PHAMs. In the
first one the behavior is approximated by discretizing time into a sequence of clock ticks that can
be made arbitrarily dense. This model is used to derive formal properties for the projection of
concurrent reactive plans. The second approach, described in Section 5, describes a much more
efficient approach to the projection of CRPs. In this approach only those time ticks are explicitly
considered and represented where discrete events may occur. At all other time instances the system
state can be inferred through interpolation using the control laws of the respective modes. This is
the projection mechanism that is used at execution time on board the robots. We show how this
implementation of PHAMs is employed for prediction-based tour scheduling for an autonomous
robot office courier. We conclude with an evaluation and a discussion of related work.

2. Structured Reactive Controllers and the Projection of Delivery Tour Plans
Plan-based robot control has been successfully applied to tasks such as the control of space probes
(Muscettola et al., 1998b), disaster management and surveillance (Doherty et al., 2000), and the
control of mobile robots for office delivery (Simmons et al., 1997; Beetz et al., 2001) and tourguide
scenarios (Alami et al., 2000; Thrun et al., 2000). A class of tasks that has received little attention is
the plan-based robot control for everyday activity in human living and working environments, tasks
that people are usually very good at.
To get a better intuition of the activity patterns to be produced in everyday activity, let us consider the chores of a hypothetical household robot. Household chores entail complex routine jobs
such as cooking dinner, cleaning the kitchen, loading the dish washer, etc. The routine jobs are
typically performed in parallel. A household robot might have to clean up the living room while the
soup is cooking on the stove. While cleaning up, the phone might ring and the robot has to interrupt cleaning in order to go and answer the phone. After having completed the telephone call the
robot has to continue cleaning right where it stopped. Thus, the robot’s activity must be concurrent,
percept-driven, interruptible, flexible, and robust, and it requires foresight.
The fact that people manage and execute their daily tasks effectively suggests, in our view,
that the nature of everyday activity should permit agents to make assumptions that simplify the
computational tasks required for competent activity. As Horswill (1996) puts it, everyday life must
provide us with some loopholes, structures and constraints that make activity tractable.
We believe that in many applications of robotic agents that are to perform everyday activities, the
following assumptions are valid and allow us to simplify the computational problems of controlling
a robot competently:
1. Robotic agents are familiar with the activities for satisfying individual tasks and the situations
that typically occur while performing them. They carry out everyday activities over and over
again and are confronted with the same kinds of situations many times. As a consequence,
conducting individual everyday activities can be learned from experience and is simple in the
sense that it does not require a lot of plan generation from first principles.
801

B EETZ & G ROSSKREUTZ

2. Appropriate plans for satisfying multiple, possibly interfering, tasks can be determined in a
greedy manner. The robot can first determine a default plan performing the individual tasks
concurrently with some additional ordering constraints through simple and fast heuristic plan
combination methods. The robot can then avoid the remaining interferences between its subactivities by predicting and forestalling them.
3. Robotic agents can monitor the execution of their activities and thereby detect situations in
which their intended course of action might fail to produce the desired effects. If such situations are detected, the robots can adapt their intended course of action to the specific situations
they encounter, if necessary based on foresight.
In our previous research we have proposed Structured Reactive Controllers (SRCs) as a computational model for the plan-based control of everyday activity. SRCs are collections of concurrent
reactive control routines that adapt themselves to changing circumstances during their execution by
means of planning. SRCs are based upon the following computational principles:
1.

SRC s are equipped with a library of plan schemata for routine tasks in common situations.
These plan schemata are — for now — provided by programmers and designed to have high
expected utility at the cost of not having to deal with all conceivable problems. We know
from our AI courses that plans that check the tailpipes every time before starting a car have
typically lower expected utility than the ones that do not check them, even though having no
bananas stuck in the tailpipe is a necessary precondition for starting a car successfully.

The robustness, flexibility, and reactivity of plan schemata is achieved by implementing them
as concurrent percept-driven plans — even at the highest level of abstraction. The plans employ control structures including conditionals, loops, program variables, processes, and subroutines. They also make use of high-level constructs (interrupts, monitors) to synchronize
parallel actions and make plans reactive and robust by incorporating sensing and monitoring actions and reactions triggered by observed events. Goals of sub-plans are represented
explicitly as annotations such that planning algorithms can infer the purpose of sub-plans
automatically.
2.

SRC s have fast heuristic methods for putting plans together from routine activities. They are
able to predict problems that are likely to occur and revise their course of action to avoid
them. Predictive plan debugging requires the SRC to reason through, and predict the effects
of, highly conditional and flexible plans — the subject of this article.

3.

SRC s perform execution time plan management. They run processes that monitor the beliefs
of the robot and are triggered by certain belief changes. These processes revise plans while
they are executed.

Structured reactive controllers work as follows. When given a set of requests, structured reactive controllers retrieve routine plans for individual requests and execute the plans concurrently.
These routine plans are general and flexible — they work for standard situations and when executed concurrently with other routine plans. Routine plans can cope well with partly unknown and
changing environments, run concurrently, handle interrupts, and control robots without assistance
over extended periods. For standard situations, the execution of these routine plans causes the robot
802

P ROBABILISTIC H YBRID ACTION M ODELS

to exhibit an appropriate behavior in achieving their purpose. While they execute routine plans,
the robot controllers also try to determine whether their routines might interfere with each other
and watch out for exceptional situations. If they encounter exceptional situations they will try to
anticipate and forestall behavior flaws by predicting how their routine plans might work in this kind
of situation. If necessary, they revise their routines to make them robust for the respective kinds
of situations. Finally, they integrate the proposed revisions smoothly into their ongoing course of
actions.
2.1 Plan-based Control for a Robot Office Courier
Before we describe our approach to predicting concurrent percept-driven robot behavior we first
give a comprehensive example of a plan-based robot office courier performing a delivery tour and
exhibiting aspects of everyday activity. The description of the example includes the presentation of
key plan schemata used by the robot, a sketch of the heuristic plan combination method, the prediction of behavior flaws, and the revision of delivery plans. This example run has been performed
with the mobile robot RHINO acting as a robot office courier (Beetz, 2001; Beetz, Bennewitz, &
Grosskreutz, 1999).
2.1.1 P LANS

AND

P LAN S CHEMATA

OF THE

ROBOT C OURIER

The robot courier is equipped with a library of plan schemata for its standard tasks including delivering items, picking up items, and navigating from one place to another. The presentation of the
plans and plan schemata proceeds bottom up. We start with the low-level plans for navigation and
end with the comprehensive object delivery plans.
A low-level navigation plan specifies how the robot is to navigate from one location in its environment, typically its current position, to another one, its destination. Figure 1 depicts such a
low-level navigation plan for going from a location in room A-117 to the location 5 in room A-111.
The plan consists of two components: a sequence of intermediate target points (the locations indexed by the numbers 1 to 5 in Figure 1) to be sequentially visited by the robot and a specification
of when and how the robot is to adapt its travel modes as it follows the navigation path. In many
environments it is advantageous to adapt the travel mode to the surroundings: to drive carefully (and
therefore slowly) within offices because offices are cluttered, to switch off the sonars when driving
through doorways (to avoid sonar crosstalk), and to drive quickly in the hallways. The second part
of the plan is depicted through regions with different textures for the different travel modes “office”,
“hallway,” and “doorway.” Whenever the robot crosses the boundaries between regions it adapts the
parameterization of the navigation system. Thus, low-level navigation plans start and terminate navigation processes and change the parameterization of the navigation system through control mode
switches (SET- NAVIGATION - MODE) and adding and deleting intermediate target points (MOVE - TO).
We specify reactive plans in RPL (McDermott, 1991; Beetz & McDermott, 1992), a plan language that provides high-level control structures for specifying concurrent, event-driven robot behavior. The pseudo code in Figure 2 sketches the initial part of the plan depicted in Figure 1. The
plan for leaving an office consists of two concurrent sub-plans: one for following the (initial part of
the) prescribed path and one for adapting the travel mode. The second sub-plan adapts the navigation mode of the robot dynamically. Initially, the navigation mode is set to “office”. Upon entering
and leaving the doorway the navigation mode is adapted. The plan uses fluents, conditions that
803

B EETZ & G ROSSKREUTZ












































































 






5 

















































 A-111
 
 
 
 











































A-113
A-110 
4













 
 
 
 
  








3















 
2















































 
 
 
 
 
 











A-121













1












A-120






A-117





































Dieter’s


















Desk






















 
 
 
 
 
 
























 
 
 
 
 
 

Legend
doorway travelmode
office travelmode
hallway travelmode
Navigation Plan
waypoint waypoint coordinates
1
h2300, 800i
2
h2300, 900i
3
h1200, 1100i
4
h1200, 1200i
5
h1250, 1400i

Figure 1: Graphical representation of a navigation plan. Topological navigation plan for navigating
from room A-117 to A-111 with regions indicating different travel modes and small black
circles indicating additional navigation path constraints.
are updated asynchronously based on new sensor readings. Fluents can trigger (whenever ) and
terminate (wait for ) plan steps.
execute concurrently
execute-in-order

(1); wait for (go-to-completed?);
(2); wait for (go-to-completed?);
with local fluents distance-to-doorway
← fluent-network (| hx, yi − hxdw , ydw i |)
entering-dw?-fl ← distance-to-doorway < 1m
entering-hw?-fl ← distance-to-doorway > 1m
MOVE - TO
MOVE - TO

execute-in-order

SET- NAVIGATION - MODE (office); wait for (entering-dw?-fl);
SET- NAVIGATION - MODE (doorway); wait for (entering-hw?-fl);
SET- NAVIGATION - MODE (hallway)

Figure 2: The plan sketches the specification of a navigation process for leaving an office. The two
components following the prescribed path and adapting the travel mode are implemented
as concurrent sub-plans. The second component uses a fluent to measure the distance
to the center of the doorway and two dependent fluents that signal the robot’s entering
and leaving the doorway. Initially, the travel mode is set to “office”. Upon entering and
leaving the doorway the travel mode is adapted.

The low-level navigation plan instances are used by higher-level navigation plans that make the
navigation processes flexible, robust, and embeddable into concurrent task contexts. The higherlevel plans generate the low-level plans based on the robot’s map of its environment (GENERATE NAV- PLAN ). A slightly simplified version of this high-level plan is listed below.
804

P ROBABILISTIC H YBRID ACTION M ODELS

highlevel-plan ACHIEVE(loc(rhino, hx, yi))
1 with cleanup routine ABORT- NAVIGATION - PROCESS
2 do with valve wheels
3
do loop
4
try in parallel
5
wait for navigation-interrupted?
6
with local vars NAV- PLAN ← GENERATE - NAV- PLAN(c,d)
7
do swap-plan (NAV- PLAN,NAV- STEP)
8
named subplan NAV- STEP
9
do DUMMY
10
until IS - CLOSE ?(hx, yi)

We explain the plan going from the inner parts, which generate the robot behavior, to the outer
ones, which modify the behavior. Lines 6 to 8 make the navigation plan independent of its starting
position and thereby more general: given a destination d, the plan piece computes a low-level
navigation plan from the robot’s current location c to d using the map of the environment and
executes it (Beetz & McDermott, 1996).
In order to run navigation plans in less constrained task contexts we must prevent other —
concurrent — routines from directing the robot to different locations while the navigation plan
is executed. We accomplish this by using semaphores or “valves”, which can be requested and
released. Any plan asking the robot to move or stand still must request the valve wheels, perform its
actions only after it has received wheels, and release wheels after it is done. This is accomplished
by the statement with valve in line 2.
In many cases processes with higher priorities must move the robot urgently. In this case,
blocked valves are simply pre-empted. To make our plan interruptible, robust against such interrupts, the plan has to do two things. First, it has to detect when it gets interrupted and second,
it has to handle such interrupts appropriately. This is done by a loop that generates and executes
navigation plans for the navigation task until the robot is at its destination. We make the routine
cognizant of interrupts by using the fluent navigation-interrupted?. Interrupts are handled by terminating the current iteration of the loop and starting the next iteration, in which a new navigation
plan starting from the robot’s new position is generated and executed. Thus, the lines 3-5 make the
plan interruptible.
To make the navigation plan transparent we name the routine plan ACHIEVE(loc(rhino,hx,yi))
and thereby enable the planning system to infer the purpose of the sub-plan syntactically. Interruptible and embeddable plans can be used in task contexts with higher priority concurrent sub-plans.
For instance, a monitoring plan used by our controller estimates the opening angles of doors whenever the robot passes one. Another monitoring plan localizes the robot actively whenever it has lost
track of its position.
To facilitate online rescheduling we have modularized the plans with respect to the locations
where sub-plans are to be executed using the at location plan schema. The at location hx,yi p plan
schema specifies that plan p is to be performed at location hx,yi. Here is a simplified version of the
plan schema for at location .
805

B EETZ & G ROSSKREUTZ

named subplan Ni
do at location hx, yipby
with valve wheels
do with local vars DONE ? ← FALSE
do loop
try in parallel
wait for Task-Interrupted?(Ni )
sequentially
do NAVIGATE -T Ohx, yi

p
DONE ?

←

TRUE

until DONE ? = TRUE

The plan schema accomplishes the performance of plan p at location hx,yi by navigating to the
location hx,yi, performing sub-plan p, and signalling that p has been completed (the inner sequence).
The with valve statement obtains the semaphore wheels that must be owned by any process changing
the location of the robot. The loop makes the execution of p at hx,yi robust against interrupts from
higher priority processes. Finally, the named sub-plan statement gives the sub-plan a symbolic name
that can be used for addressing the sub-plan for scheduling purposes and in plan revisions. Using
the at location plan schema, a plan for delivering an object o from location p to location d can be
roughly specified as a plan that carries out pickup(o) at location p and put-down(o) at location d with
the additional constraint that pickup(o) is to be carried out before putdown(o). If every sub-plan p
that is to be performed at a particular location l has the form at location hx,yi p, then a scheduler
can traverse the plan recursively and collect the at location sub-plans and install additional ordering
constraints on these sub-plans to maximize the plan’s expected utility.
To allow for smooth integration of revisions into ongoing scheduled activities, we designed the
plans such that each sub-plan keeps a record of its execution state and, if started anew, skips those
parts of the plan that no longer have to be executed (Beetz & McDermott, 1996). We made the plans
for single deliveries restartable by equipping the plan p with a variable storing the execution state
of p that is used as a guard to determine whether or not a sub-plan is to be executed. The variable
has three possible values: to-be-acquired denoting that the object must still be acquired; loaded
denoting that the object is loaded; and delivered denoting that the delivery is completed. The plan
schema for the delivery of a single object consists of two fairly independent plan steps: the pick-up
and the put-down step.
if EXECUTION - STATE(p, to-be-acquired)
then AT-L OCATION
L
PICK - UP(o)
if EXECUTION - STATE(p, loaded)
then AT-L OCATION
D
PUT- DOWN (o)

2.1.2 G ENERATING D EFAULT D ELIVERY P LANS
The heuristic plan generator for delivery tours is simple: it inserts the pick-up and put-down subplans of all delivery requests into the overall plan and determines an appropriate order on the
at location sub-plans. The ordering is determined by a heuristic that performs a simple topological
806

P ROBABILISTIC H YBRID ACTION M ODELS

sort on the sub-plans based on the locations where the sub-plans are to be executed. The heuristic considers additional constraints such as executing pick-up steps always before the respective
put-down plan-steps.
2.1.3 P REDICTION - BASED P LAN D EBUGGING

BY THE

ROBOT O FFICE C OURIER

Let us now contemplate a specific scenario in which the robot office courier RHINO performs an office delivery that requires the prediction and forestalling of plan failures at execution time. Consider
the following situation in the environment pictured in Figure 3. A robot office courier is to deliver
a letter in a yellow envelope from room A-111 to A-117 (cmd-1) and another letter for which the
envelope’s color is unknown from A-113 to A-120 (cmd-2). The robot has already tried to accomplish cmd-2 but because it has recognized room A-113 as closed (using its range sensors) it revises
its intended course of action into achieving cmd-2 opportunistically. That is, if it later detects that
A-113 is open it will interrupt its current activity and reconsider its intended course of action under
the premise that the steps for accomplishing cmd-2 are executable.
To perform its tasks quickly the robot schedules the pick-up and delivery actions to minimize
execution time and assure that letters are picked up before they are delivered. To ensure that the
schedules will work, the robot has to take into account how its own state and the world changes as it
carries out the scheduled activities. Aspects of states that the robot has to consider when scheduling
its activities are the locations of the letters. Constraints on the state variables that schedules have to
satisfy are that they only ask the robot to pick up letters that are currently at the robot’s location and
that the robot does not carry two letters in envelopes with the same color.
(58) (DONE GOTO (1000.0 1600.0))
(58) (DO LOAD-LETTER y-letter)
(62) (ACTIVATE GOTO (2300.0 600.0))






(2) (ACTIVATE GOTO (1000.0 1600.0))

A-111



 

 

(136) (DONE GOTO (2300.0 600.0))
(136) (DO UNLOAD-LETTER y-letter)

   



A-117

Figure 3: A possible projected execution scenario for the initial plan. The opportunity of loading
the letter of the unknown color is ignored.
Suppose our robot is standing in front of room A-117. The belief state of the robot contains
probabilities for the colors of letters on the desk in A-113. The robot also has received some evidence that A-113 has been opened in the meantime. Therefore its belief state assigns probability p
for the value true of random variable open-A113.
807

B EETZ & G ROSSKREUTZ

This update of the belief state requires the robot to reevaluate its options for accomplishing
its jobs with respect to its changed belief state. Executing its current plan without modifications
might yield mix ups because the robot might carry two letters in envelopes with the same color.
The different options are: (1) to skip the opportunity, (2) to ask immediately for the letter from
A-113 to be put into an envelope that is not yellow (to exclude mix ups when taking the opportunity
later); (3) to constrain later parts of the schedule such that no two yellow letters will be carried even
when the letter in A-113 turns out to be yellow; and (4) to keep picking up the letter in A-113 as an
opportunistic sub-plan. Which option the robot should take depends on its belief state with respect
to the states of doors and locations of letters. To find out which schedules will probably work, in
particular, which ones might result in mixing up letters, the robot must apply a model of the world
dynamics to the state variables.
With respect to this belief state, different scenarios are possible. The first one, in which A-113
is closed, is pictured in Figure 3. Points on the trajectories represent predicted events. The events
without labels are actions in which the robot changes its heading (on an approximated trajectory) or
events representing sensor updates generated by passive sensing processes. For example, a passive
sensor update event is generated when the robot passes a door. In this scenario no intervention by
prediction-based debugging is necessary and no flaw is projected.
A-111

A-113

A-111

(95) (DONE GOTO (1000.0 1600.0))
(95) (DO LOAD-LETTER Y-LETTER)
(95) (FAILURE SAME-COLOR LETTER)
(33) (DONE GOTO (1850.0 1350.0))
(33) (DO LOAD-LETTER OPP)
(34) (ACTIVATE GOTO (1000.0 1600.0))





 

 

		




   

A-113

(102) GOTO (1000.0 1600.0))
(102) LOAD-LETTER Y-LETTER)
(103) GOTO (1100.0 400.0))

		 	
	 	

		 	 400.0))		
(174) (DONE GOTO (1100.0
(174) UNLOAD-LETTER OPP)
	
(175) GOTO (2300.0 600.0))

(12) (RECOGNIZE LOAD-LETTER OPP)
(13) (ACTIVATE GOTO (1850.0 1350.0))

		

(2) (ACTIVATE GOTO (1000.0 1600.0))

	

 

(30) GOTO (1850.0 1350.0))
(30) LOAD-LETTER OPP)
(31) GOTO (1000.0 1600.0))

	

		 		 	
		

			 		
	
	
	

(11) (RECOGNIZE LOAD-LETTER OPP)
(12) (ACTIVATE GOTO (1850.0 1350.0))
(2)
(ACTIVATE GOTO (1000.0 1600.0))

A-120

(248) (DONE GOTO (2300.0 600.0))
(248) (DO UNLOAD-LETTER Y-LETTER)

A-117

A-120

(a)

A-117

(b)

Figure 4: Two possible predicted scenarios for the opportunity being taken. In scenario (a) the letter
turns out to have the same color as the one that is to be loaded afterwards. Therefore, the
second loading fails. In scenario (b) the letter turns out to have a different color than the
one that is to be loaded afterwards. Therefore, the second loading succeeds.
In the scenarios in which office A-113 is open the controller is projected to recognize the opportunity and to reschedule its enabled plan steps as described above. 1 The resulting schedule asks the
robot to enter A-113 first, and pickup the letter for cmd-2, then enter A-111 and pick up the letter
for cmd-1, then deliver the letter for cmd-2 in A-120, and the last one in A-117. This category of
scenarios can be further divided into two categories. In the first sub-category shown in Figure 4(a)
the letter to be picked up is yellow. Performing the pickup thus would result in the robot carrying
1. Another category of scenarios is characterized by A-113 becoming open after the robot has left A-111. This may also
result in an execution failure if the letter loaded in A-113 is yellow, but is not discussed here any further.

808

P ROBABILISTIC H YBRID ACTION M ODELS

A-111

A-113

(39) (DONE GOTO (1850.0 1450.0))
(39) (DO LOAD-LETTER NIL)
(70) (ACTIVATE GOTO (1100.0 400.0))



(200) (DONE GOTO (1000.0 1600.0))
(202) (DO LOAD-LETTER Y-LETTER)
(211) (ACTIVATE GOTO (2300.0 600.0))







 






(2) (ACTIVATE GOTO
(1850.0 1450.0))





 

(19) (USE OPPORTUNITY)

 







(263) (DONE GOTO (2300.0 600.0))
(147) (DONE GOTO (1100.0 400.0))(263) (DO UNLOAD-LETTER Y)
(162) (DO UNLOAD-LETTER OPP)
(178) (ACTIVATE GOTO (1000.0 1600.0))

A-120

A-117

Figure 5: Projected scenario for a plan suggested by the plan debugger. The letter with the unknown
color is picked up and also delivered first. This plan is a little less efficient but avoids the
risk of not being able to load the second letter.

two yellow letters and therefore an execution failure is signalled. In the second sub-category shown
in Figure 4(b) the letter has a different color and therefore the robot is projected to succeed by taking the same course of action for all these scenarios. Note that the possible flaw is introduced by
the reactive rescheduling because the rescheduler does not consider how the state of the robot will
change in the course of action, in particular that a state may be caused in which the robot is to carry
two letters with the same color.
In this case, the plan-based controller will probably detect the flaw if it is likely with respect to
the robot’s belief state. This enables the debugger to forestall the flaw, for instance, by introducing
an additional ordering constraint, or by sending an email that increases the probability that the letter
will be put into a particular envelope. These are the revision rules introduced in the last section.
Figure 5 shows a projection of a plan that has been revised by adding the ordering constraint that
the letter for A-120 is delivered before entering A-111.
Figure 6(a) shows the event trace generated by the initial plan and executed with the RHINO
control system (Thrun et al., 1998) for the critical scenario without prediction based schedule debugging; Figure 6(b) shows the one with the debugger adding the additional ordering constraint.
This scenario shows that reasoning about the future execution of plans enables the robot to improve
its behavior.
In this article, we describe the probabilistic models of reactive robot behavior that are necessary to predict scenarios such as the one described above for the purpose of prediction-based plan
debugging.
2.2 The Projection of Low-level Navigation Plans
Now that we know what the robot plans look like we can turn to the question of how to predict the
effects of executing a delivery plan. The input data for plan projection are the probabilistic beliefs
809

B EETZ & G ROSSKREUTZ

A-111

  %%<< %% 2? #. +2136@+,25"A5..+,BC4( .#+2#+,=255.-,(
  %%  << 2#.+.3= 25.5.>!
-,#.( #8   %&   %& %( (

21:12:31 ARRIVED AT (1000.0 1600.0)
21:13:06 LOADING BLUE LETTER
21:13:06 GOING TO (2300.0 600.0)

A-113
21:10:13 ARRIVED AT (1850.0 1450.0)
21:10:37 LOADING BLUE LETTER
21:10:37 GOING TO (1100.0 400.0)

P OP P O O
POOP
G G G G G FG F F F
NN QOQP
G F GH
QN
NN NQQQ
EH HE HEFE HD
H DS S SSS
NR
JJ MI M MRIM I R I RI R R R IIIIII R I R I R IR R I R I RS I S HI S H S H S HS HS HS S S S S D DD S DDD S D ST D T T T DD TT D D
MJM J
UTU
LL LM KKJ
LL KL KK

21:09:38 GOING TO (1850.0 1450.0)

  %% )*)*++ .,-..#+ !+-/ "!01"2
##.+#83. 4  259%5:%.& ;-, 6:%#+& %7-( 7( (

21:09:50 INSTALL NEW SCHEDULE
TO AVOID
CARRYING SAME COLOR

       "!
##$  % & ' % & %( (

21:11:24 ARRIVED AT (1100.0 400.0)
21:11:59 UNLOADING BLUE LETTER
21:11:59 GOING TO (1000.0 1600.0)

21:14:26 ARRIVED AT (2300.0 600.0)
21:14:58 UNLOADING BLUE LETTER

A-120

(a)

A-117

(b)

Figure 6: The trajectory without prediction-based plan revision (Sub-figure (a)) fails because the
courier did not foresee the possible complications with loading the second letter. Subfigure (b) shows a trajectory where the possible flaw is forestalled by the planning mechanism.

of the robot with respect to the current state of the world, probabilistic models of exogenous events
that are assumed to be Poisson distributed, probabilistic models of low-level plans, and probabilistic
rules for guessing missing pieces of information. The output of the projection process is a sequence
of dated events along with the estimated state at the time of each event.
Plan projection is identical to plan execution with two exceptions. First, whenever the plan
projector interprets a wait for or whenever it records the corresponding fluents as active triggering
conditions. This way, the plan projection mechanism can automatically generate percepts when continuous control processes or exogenous events make the triggering conditions true. For example,
when the navigation plan is waiting for the robot to enter the hallway the plan projector probabilistically guesses when the robot motion causes the respective triggering condition to become true. For
this time instant, the plan projector generates a sensor input event with the corresponding sensor
reading.
Plan projection also differs from plan interpretation in that whenever the robot interacts with
the real world, the projected robot must interact with the symbolic representations of the world.
The places where this happens are the low-level plans. Thus instead of executing a low-level plan
the projector guesses the results of executing these plans and asserts their effects in the form of
propositions to the timeline. There are three kinds of effects that are generated by the interpretation
of low-level plans: (1) physical changes, such as the robot changing its position, (2) the low-level
plan changing the dynamical state of the robot, such as the direction the robot is heading to, and
(3) computational effects, such as changing the values of program variables or signalling the success
and failure of control routines. Thus the model of a low-level plan used for plan projection is a
probability distribution over the sequence of events that it generates and the delays between the
subsequent events.
Thinking procedurally, the plan projector works as follows. It iteratively infers the occurrence
of the next event until the given plan is completely interpreted. The next event can either be the next
810

P ROBABILISTIC H YBRID ACTION M ODELS

event that the low-level plan generates if the computational state of the controller does not change,
or a sensor input event if an active triggering condition is predicted to become true, or an exogenous
event if one is predicted to occur. The next predicted event is the earliest of these events.
We will now consider a particular instance of low-level plans: the low-level navigation plans
used in the example of the previous section. Navigation is a key action of autonomous mobile
robots. While predicting the path that a robot will take it is necessary to predict where the robot
will be, which is a prerequisite for predicting what the robot will be able to perceive. For example,
whether the robot will perceive that a door is open depends on the robot taking a path that passes
by the door, executing the door angle estimation routine while passing the door and the door being
within sensor range. Passing the door is perceived based on the robot’s position estimate and the
environment map. Consequently, if the robot executes a plan step only if a door is open, then in the
end the execution of this plan step depends on the actual path the robot will take. This implies that
an action planning process must be capable of predicting the trajectory accurately enough to predict
the global course of action correctly.
Navigation actions are representative for a large subset of physical robot actions: they are movements controlled by motors. Physical movements have a number of typical characteristics. First,
they are often inaccurate and unreliable. Second, they cause continuous (and sometimes discontinuous) change of the respective part of the robot’s state. Third, the interference of concurrent
movements can often be described as the superposition of the individual movements.
To discuss the issues raised by the projection of concurrent reactive plans, we sketch a delivery
tour plan that specifies how a robot is to deliver mail to the rooms A-113, A-111, and A-120 in
Figure 1 (Beetz, 2001). The mail for room A-120 has to be delivered by 10:30 (a strict deadline).
Initially, the planner asks the robot to perform the deliveries in the order A-113, A-111, and A120. As the room A-113 is closed the corresponding delivery cannot be completed. Therefore, the
planning system revises the overall plan such that the robot is to accomplish the delivery for A-113
as an opportunity. In other words, the robot will interrupt its current delivery to deliver the mail to
A-113 (see Figure 7) if the delivery can be completed.
with policy as long as in-hallway?
whenever passing-a-door?

ESTIMATE - DOOR - ANGLE ()

with policy seq wait for open?(A-113)

DELIVER - MAIL - TO (D IETER )

1.
2.

GO - TO (A-111)
GO - TO (A-120) before

10:30

Figure 7: Delivery tour plan with a concurrent monitoring process triggered by the continuous effects of a navigation plan (passing a door) and an opportunistic step. This concurrent
reactive plans serve as an example for discussing the requirements that the causal models
must satisfy.
The plan contains constraining sub-plans such as “whenever the robot passes a door it estimates
the opening angle of the door using its laser range finders” and opportunities such as “complete
811

B EETZ & G ROSSKREUTZ

the delivery to room A-113 as soon as you learn the office is open”. These sub-plans are triggered
or completed by the continuous effects of the navigation plans. For example, the event passing a
door occurs when the robot traverses a rectangular region in front of the door. We call these events
endogenous events.

A-111

A-113

end(low-level-nav-plan(...))
leaving
doorway
entering
doorway

leaving
hallway

VW
VW

VW VW

leaving
doorway

VW VW
VW

entering
hallway
entering
doorway
begin(low-level-nav-plan(...))
A-120

A-117

Figure 8: Visualization of a projected execution scenario. The following types of events are depicted by specific symbols: change travel mode event by rhombuses, start/stop passing
doorway by small circles, start/stop low-level navigation plan by double circles, and entering doorway/hallway by boxes.

Figure 8 shows a projected execution scenario for a low-level navigation plan embedded in the
plan depicted in Figure 7. The behavior generated by low-level navigation plans is modeled as
a sequence of events that either cause qualitative behavior changes (e.g. adaptations of the travel
mode) or trigger conditions that the plan is reacting to (e.g. entering the hallway or passing a door).
The events depicted by rhomboids denote events where the CRP changes the direction and the target
velocity of the robot. The squares denote the events entering and leaving offices. The small circles
denote the events starting and finishing passing a door, which are predicted because a concurrent
monitoring process estimates the opening angles of doors while the robot is passing them.
Such projected execution scenarios have been used for prediction-based debugging of delivery
tours of an autonomous robot office courier. Beetz et al. (1999) have shown that a controller employing predictive plan scheduling using the causal models described in this article can perform
better than it possibly could without predictive capabilities (see also Section 6.1).
812

P ROBABILISTIC H YBRID ACTION M ODELS

2.3 Peculiarities of Projecting Concurrent Reactive Plans
There are several peculiarities in the projection of concurrent reactive plans that we want to point
out here.
Continuous Change. Concurrent reactive plans activate and deactivate control processes and
thereby cause continuous change of states such as the robot’s position. Continuous change must
be represented explicitly because CRPs employ sensing processes that continually measure relevant
states (for example, the robot’s position) and promptly react to conditions caused by the continuous
effects (for example, entering an office).
Reactive Control Processes. Because of the reactive nature of robot plans, the events that have to
be predicted for a continuous navigation process depend not only on the process itself but also on
the monitoring processes that are simultaneously active and wait for conditions that the continuous
effects of the navigation process might cause. Suppose the robot controller is running a monitoring
process that stops the robot as soon as it passes an open door. In this case the planner must predict
“robot passes door” events for each door the robot passes during a continuous navigation action.
These events then trigger a sensing action that estimates the door angle, and if the predicted percept is an “open door detected” then the navigation process is deactivated. Other discrete events
that might have to be predicted based on the continuous effects of navigation include entering and
leaving a room, having come within one meter of the destination, etc.
Interference between continuous effects. For the control processes that set voltages for the robot’s
motors, the possible modes of interference between control processes are limited. If they generate
signals for the same motors the combined effects are determined by the so-called task arbitration
scheme (Arkin, 1998). The most common task arbitration schemes are (1) behavior blending (where
the motor signal is a weighted sum of the current input signals) (Konolige, Myers, Ruspini, &
Saffiotti, 1997); (2) prioritized control signals (where the motor signal is the signal of the process
with the highest priority) (Brooks, 1986); and (3) exclusion of concurrent control signals through
the use of semaphores. In our plans, we exclude multiple control signals to the same motors but
they can be easily incorporated in the prediction mechanism. Thus the only remaining type of
interference is the superposition of movements such as turning the camera while moving.
Uncertainty. There are various kinds of uncertainty and non-determinism in the robot’s actions that
a causal model should represent. It is often necessary to specify a probability distribution over the
average speed and the displacements of points on the paths to enable models to predict the range of
spatio-temporal behavior that a navigation plan can generate. Another important issue is to model
probability distributions over the occurrence of exogenous events. In most dynamic environments
exogenous events such as opening and closing doors might occur at any time.

3. Modeling Reactive Control Processes and Continuous Change
Let us now conceptualize the behavior generated by modern robot plans and the interaction between
behavior and the interpretation of reactive plans. We base our conceptualization on the vocabulary
of hybrid systems. Hybrid systems have been developed to design, implement, and verify embedded
systems, collections of computer programs that interact with each other and an analog environment
(Alur, Henzinger, & Wong-Toi, 1997; Alur, Henzinger, & Ho, 1996).
The advantage of a hybrid system based conceptualization over state-based ones is that hybrid
systems are designed to represent concurrent processes with interfering continuous effects. They
also allow for discrete changes in process parameterization, which we need to model the activation,
813

B EETZ & G ROSSKREUTZ

deactivation, and reparameterization of control processes through reactive plans. In addition, hybrid system based conceptualizations can model the procedural meaning of wait for and whenever
statements.
As pictured in Figure 9, we consider the robot and its operating environment as two interacting processes: the environment including the robot hardware, which is also called the controlled
process, and the concurrent reactive plan, which is the controlling process. The state of the environment is represented by state variables including the variables x and y, the robot’s real position
and door-anglei representing the opening angle of door i. The robot controller uses fluents to store
the robot’s measurements of these state variables (robot-x, robot-y, door-a120, etc.). The fluents
are continually updated by the self-localization process and a model-based estimator for estimating
the opening angles of doors. The control inputs of the plan for the environment process is a vector
that includes the travel mode, the parameterization of the navigation processes and the current target
point to be reached by the robot.
Environment
State Variables:
X

Y

DOORANGLEi

Control Inputs

Exogenous Events

- Travel Mode
- Target Point

going-for-lunch(person)

Sensing Process
- self localization
- door angle estimation

Concurrent Reactive Plan
Delivery Plan
from Figure 1

robot-x
robot-y
door-i

Figure 9: The figure shows our conceptualization of the execution of navigation plans. The relevant
state variables are the x and y coordinates of the robot’s position and opening angles of the
doors. The fluents that estimate these state variables are robot-x, robot-y, and door-a120.

3.1 A Hybrid System Model for Reactive Robot Behavior
We will now model the controlled process as a hybrid system (Alur et al., 1997, 1996). Hybrid
systems are continuous variable, continuous time systems with a phased operation. Within each
phase, called control mode, the system evolves continuously according to the dynamical law of
that mode, called continuous flow. Thus the state of the hybrid system can be thought of as a
pair — the control mode and the continuous state. The control mode identifies a flow, and the
814

P ROBABILISTIC H YBRID ACTION M ODELS

continuous flow identifies a position in it. Also associated with each control mode are so-called
jump conditions, specifying the conditions that the discrete state and the continuous state together
must satisfy to enable a transition to another control mode. The transitions can cause abrupt changes
of the discrete as well as the continuous state. The jump relation specifies the valid settings of the
system variables that might occur during a jump. Then, until the next transition, the continuous state
evolves according to the flow identified by the new control mode.
When considering the interpretation of concurrent reactive plans as a hybrid system the control
mode is determined by the set of active control processes and their parameterization. The continuous
state is characterized by the system variables x, y, and o that represent the robot’s position and
orientation. The continuous flow describes how these state variables change as a result of the active
control processes. This change is represented by the component velocities ẋ, ẏ, and ȯ. Thus for
each control mode the robot’s velocity is constant. Linear flow conditions are sufficient because
the robot’s paths can be approximated accurately enough using polylines (Beetz & Grosskreutz,
1998). They are also computationally much easier and faster to handle. The jump conditions are the
conditions that are monitored by constraining control processes which activate and deactivate other
control processes.
Thus the interpretation of a navigation plan according to the hybrid systems model works as
follows. The hybrid system starts at some initial state hcm0 , x0 i. The state trajectory evolves with
the control mode remaining constant and the continuous state x evolving according to the flow
condition of cm. When the (estimated) continuous state satisfies the transition condition of an edge
from mode cm to a mode cm0 a jump must be made to mode cm0 , where the mode might be chosen
probabilistically. During the jump the continuous state may get initialized to a new value x 0 . The
new state is the pair hcm0 , x0 i. The continuous state x0 evolves according to the flow condition of
cm0 .
The construction of the hybrid system for a given concurrent plan is straightforward. We start
at the current plan execution state. For every concurrent active statement of the form wait for cond
and whenever cond we add cond as a jump condition to the current control mode. In addition we
have one additional jump condition for the completion of the plan step.
Figure 10 depicts the interpretation of the first part of the navigation plan shown in Figure 2.
The interpretation is represented as a tree where the nodes represent the control modes of the corresponding hybrid system and the node labels the continuous flow. The edges are the control mode
transitions labeled with the jump conditions. The robot starts executing the plan in room A-117.
The initial control mode of the hybrid system is the root of the state tree depicted in Figure 10. The
initial state represents the state of computation where the first control processes of the two parallel
branches are active, that is the processes for going to the intermediate target point 1 and maintaining the “office mode” as the robot’s travel mode. The flow specifies that while the robot is in the
initial control mode the absolute value of the derivative of the robot’s position is a function of the
robot’s navigation mode (office, doorway, or hallway) and the next intermediate target point. The
hybrid system makes a transition into one of the subsequent states when either the first target point is
reached or when the distance to the doorway becomes less than one meter. The transition condition
for the upper edge is that the robot has come sufficiently close to the doorway, for the lower edge
that it has reached the first target point. For the lower edge, the hybrid system goes into the state
where the robot goes to the target point 2 while still keeping the office mode as its current travel
mode. In the other transition the robot changes its travel mode to doorway and keeps approaching
the first target point. The only variables that are changed through the control mode transitions are
815

B EETZ & G ROSSKREUTZ

control mode: cm3
ẋ = f1 (hallway, h2300, 800i)
ẏ = f2 (hallway, h2300, 800i)
e3 : dist(hx, yi, hxdw , ydw i) > 100
control mode: cm1
ẋ = f1 (doorway, h2300, 800i)
ẏ = f2 (doorway, h2300, 800i)
e4 : x = 2300 ∧ y = 800
control mode: cm4
ẋ = f1 (doorway, h2300, 900i)
ẏ = f2 (doorway, h2300, 900i)

e1 : dist(hx, yi, hxdw , ydw i) < 100

control mode: cm0
x0 =2400, y0 =600
ẋ = f1 (office, h2300, 800i)
ẏ = f2 (office, h2300, 800i)
control mode: cm5
ẋ = f1 (office, h1200, 1100i)
ẏ = f2 (office, h1200, 1100i)

e2 : x = 2300 ∧ y = 800

e5 : x = 2300 ∧ y = 900
control mode: cm2
ẋ = f1 (office, h2300, 900i)
ẏ = f2 (office, h2300, 900i)
e6 : dist(hx, yi, hxdw , ydw i) < 100
control mode: cm6
ẋ = f1 (doorway, h2300, 900i)
ẏ = f2 (doorway, h2300, 900i)

Figure 10: The figure shows the hybrid automaton for the interpretation of the navigation plan in
Figure 2. The possible control modes with the continuous flow equations are depicted
as nodes and the mode transitions as edges. The edges are labeled with jump conditions:
entering doorway (e1 , e6 ), leaving doorway (e3 ), reaching first waypoint (e2 , e4 ), and
reaching second waypoint (e5 ).

the velocity of the robot and its orientation. Both settings are implied by the flow condition of the
respective successor states.
There is one issue that we have not yet addressed in our conceptualization: uncertainty. We
model uncertainty with respect to the continuous effects and the achievement of jump conditions
using multiple alternative successor modes with varying flows and jump conditions. We associate
a probability of occurrence with each mode transition. This way we can, for example, represent
rotational inaccuracies of navigation actions that are typical for mobile robots.
816

P ROBABILISTIC H YBRID ACTION M ODELS

3.2 Representation of the Hybrid System Model
Let us now formalize our hybrid system conceptualization using a logical notation. To do so, we
are going to use the following predicates to describe the evolution of the system states: jump~ f lows),
~
Condition(cm,e,c), jumpSuccessor(e,cm’,probRange), jumpRelation(cm’, vals,
and probRange(e,max). jumpCondition(cm,e,c) represents mode cm being left along edge e when condition
c becomes true. jumpSuccessor(e,cm’,probRange) defines the non-deterministic successor states
cm’ and the probability ProbRange with which they are entered if the system makes a transition
~ f lows)
~
along e. jumpRelation(cm’, vals,
defines the initial values of the state variables and the flow
conditions upon entering state cm’. A jump e causes the automaton to transit probabilistically into
a successor mode.
For each possible successor we define a probability range probRange. For reasons that are explained below we represent the probability ranges such that they are non-overlapping, their relative
sizes are proportional to the probability they represent (the sum of the ranges is 1) and that their
boundaries have the form 2in , where i and n are integers. The predicate probRange(e,2 n ) defines the
sum of the ranges. A possible transition with a probability range [ 2in , 2jn ] is represented as jumpSuc~ f lows)
~
cessor(e,cm’,[i,j]). The predicate jumpRelation(cm’, vals,
means that upon entering control
~ and f lows.
~
mode cm’ the system variables and flows are initialized as specified by vals
Using the predicates introduced above, we can state a probabilistic hybrid automaton (Figure 10)
for the interpretation of our navigation plan using the following facts.
jumpRelation(cm0 ,h2400,600i, hf1 (office, h2300, 800i), f2 (office, h2300, 800i)i)
jumpCondition(cm0 ,e1 ,dist(hx, yi, hxdw , ydw i) < 100)
jumpCondition(cm0 ,e2 ,x = 2300 ∧ y = 800)
jumpSuccessor(e1 ,cm1 ,[1,1])
probRange(e1 ,1)
jumpRelation(cm1 ,h i,hf1 (doorway, h2300, 800i), f2 (doorway, h2300, 800i)i)
jumpRelation(cm2 ,h i,hf1 (office, h2300, 900i), f2 (office, h2300, 900i)i)
...
The robot starts at position h2400, 600i in control mode cm 0 in which the robot leaves the
lower office on the right. In this control mode the robot moves with hf 1 (office, h2300, 800i),
f2 (office, h2300, 800i)i. The navigation system leaves control mode cm 0 when coming close to
the door (dist(hx, yi, hxdw , ydw i) < 100) by performing the transition e1 . If the system performs
the transition e1 then the control flow changes because the low-level navigation plan switches into
the navigation mode doorway. In our example, this transition is deterministic.
To account for uncertainty in control we make these transitions probabilistically. Thus we can
substitute the control mode cm1 by multiple control modes, say cm01 and cm001 where the control
flows of the modes are sampled from a probability distribution. We can then state for example that
4
0
with a probability of 75% ( 12
16 ) the system transits into control mode cm1 and with 25% ( 16 ) into the
mode cm001 by defining the effects of the transition e1 as follows:
jumpCondition(cm0 ,e1 ,dist(hx, yi, hxdw , ydw i) < 100)
jumpSuccessor(e1 ,cm01 ,[1,12])
jumpSuccessor(e1 ,cm001 ,),[13,16])
probRange(e1 ,16)
817

B EETZ & G ROSSKREUTZ

To represent the state of a hybrid automaton we use the predicates mode(cm) and startTime(cm,t)
~ and
to represent that the current control mode is cm and that cm started at time t. We use flow( flow)
~
valuesAt(ti ,vali ) to assert the flows and values of system variables for given time points. Further,
the values of system variables can be inferred for arbitrary time points through interpolation on the
~ i ). This is done using the predicate
basis of the current flow and the last instances of valuesAt(ti ,val
stateVarsVals:
~ ≡ valuesAt(t0 ,vals
~ 0 ) ∧ now(t)
stateVarVals(vals)
~ = vals
~ 0 + (t − t0 )flow
~ ∧ vals
~
∧f low(flow)
where now(t) specifies that t is the current time. Note, in this conceptualization we represent the discrete state changes explicitly and the states within a mode using the mode’s initial state and its flow.
A particular state within a mode can be derived on demand using the predicate stateVarVals. Interferences between different movements of the robot issued in different control threads are modeled
through the mode’s flow.
Figure 11 depicts an execution scenario, a possible evolution of the hybrid system representing
how the execution of a robot controller might go. An execution scenario is a consistent set of jumps
and values from the hybrid model over time. From this we can extract event histories that can be
used to simulate plan execution and look for flaws.
An execution scenario consists of a timeline, a linear sequence of events and their results. Timelines represent the effects of plan execution in terms of time instants, occasions, and co-occurring
events. This implies that several events can occur at the same time instant but only one of them,
the primary one, changes the state of the world. Time instants are points in time at which the world
changes due to an action of the robot or an exogenous event. Each time instant has a date which
holds the time on the global clock at which the time instant occurred. An occasion is a stretch of
time over which a world state P holds and is specified by a proposition, which describes P, and the
time interval for which the proposition is true.
We deal with other kinds of uncertainty by representing our model using McDermott’s rule
language for probabilistic, totally-ordered temporal projection (McDermott, 1994). Using this language we can represent Poisson distributed exogenous events, probability distributions over the
current world state, and probabilistic sensor and action models in a way that is consistent with our
model presented so far.
3.3 Discussion of the Model
Let us now discuss how our hybrid system model addresses the issues raised in Section 2.3. There
are two inference tasks concerning the issue of continuous change caused by concurrent reactive
plans that are supported by our model. The first one is inferring the state at a particular time instant.
For example, if the projection mechanism predicts the occurrence of an exogenous event, such as an
object falling out of the robot’s gripper, then the projection mechanism has to infer where the robot
is at this time instant to assert the position of the object after falling down. This can be done using
the initial state and the flow condition of the active control mode. The second important inference
task is the prediction of control mode jumps caused by the continuous effects of low-level plans,
such as the robot entering the hallway. This can be inferred using the jump conditions of the active
control mode in addition to the initial state and the flow condition.
818

P ROBABILISTIC H YBRID ACTION M ODELS

mode(cm0)

mode(cm1)

initialValues(cm0,<2400,600>)

initialValues(cm1,
startTime(cm1,5)

startTime(cm0,0)

flow(<f1(off...

flow(<f1(office,<2300,900>),f2(office,<2300,900>)>)
valuesAt(t3,<2360,850>)

t1

clock-tick(t1)

t2

t3

t4

t5

t6

clock-tick(t2)

clock-tick(t3)

clock-tick(t4)

clock-tick(t5)

clock-tick(t6)

jump(e1)

Figure 11: Part of a timeline that represents a projected execution scenario for a low-level navigation plan. Time instants are depicted as circles, events as rectangles, and occasions as
rectangles with round corners.

The second issue that we have raised in Section 2.3 is the prediction of the robot’s reactions to
instantaneous events, such as dropping an object. Typically, a reactive plan for carrying an object
contains sub-plans that ask the robot to stop and pick up the object again as soon as the sensed force
in the gripper drops. These kinds of reactions are handled by checking all active jump conditions
immediately after an instantaneous event has occurred.
The third issue in projecting concurrent reactive plans is the interference between simultaneous
continuous effects. In our model, interference is modelled by describing the effects of control mode
jumps on the flow condition of the subsequent control mode. The programmer must specify rules
describing the physics of the domain for specifying the flow condition of the next mode. Thus, when
a sub-plan for moving the robot’s arm is started while the robot is moving, then the rule describing
the effects of the corresponding control mode jump asserts the flow condition specifying that the
world coordinates of the gripper are determined by the gripper position at the mode jump and the
transposition of the two motions in the successor mode.
Our last issue is that of uncertainty. One aspect of uncertainty that our model supports are
inaccuracies in the physical behavior of the robot. This is modelled by specifying probability distributions over the successor modes when a control mode jump occurs. Other aspects of uncertainty
including probabilistic sensor models, uncertainty of instantaneous physical effects, uncertainty
about the state of the world, and Poisson distributed exogenous events are handled by the rule language that we describe in the next section. In particular, we will give examples of exogenous events
and passive sensors in Section 5.3 and a detailed probabilistic model of a complex sensing action in
Section 5.4.
819

B EETZ & G ROSSKREUTZ

Our approach does not explicitly reason about the belief state. We assume that the belief state is
computed by probabilistic state estimators (Thrun et al., 2000). Such state estimators not only return
the most likely state but also infer additional properties of the belief state such as its ambiguity and
the expected accuracy of its global maximum. The plan-based controller interrupts delivery tours as
soon as the position estimate is ambiguous or too inaccurate. Details of this mechanism as well as
motivations for it can be found in the work of Beetz, Burgard, Fox, and Cremers (1998).

4. Probabilistic, Totally-Ordered Temporal Projection
In the last section we have seen how hybrid systems and execution scenarios are represented. In
this section, we will see how we can predict execution scenarios from the specification of a hybrid system. For this purpose we use McDermott’s rule language for probabilistic, totally-ordered
temporal projection (McDermott, 1994). This rule language has the expressiveness needed for our
purpose: we can specify the probabilities of event effects depending on the respective situation,
Poisson distributed events, and probability distributions over the delays between subsequent plan
generated events. Uncertainty about the current state of the world can be specified in the form of
probabilistic effect rules of the distinct start event.
This rule language is an excellent basis for formalizing our model introduced in the last section.
A set of rules that satisfies certain conditions implies a unique distribution of dated event sequences
that satisfies the probabilistic conditions of the individual rules (see Definition 2 in Section 4.1).
Thus if we give probabilistic formalizations of the behavior within control modes and mode jumps
in McDermott’s rule language then we define a unique probability distribution over the state trajectories of the hybrid automaton that satisfies our probabilistic constraints. Moreover, McDermott
has developed a provably correct projection algorithm that samples dated event sequences from this
unique distribution.
In the remainder of this section we will proceed as follows. We start by presenting McDermott’s
rule language for probabilistic, totally-ordered temporal projection and summarize the main properties of this language. We will then represent our hybrid system model using this rule language.
Based on this representation we can, loosely speaking, show that when applying McDermott’s projection algorithm to the representation of the hybrid system, the algorithm returns dated event sequences drawn from the unique distribution implied by our rules with arbitrarily high probability.
Note, to obtain these results we will use a discretized model of time with clock tick events that
can be spaced arbitrarily close together resulting in higher accuracy of the projection algorithm.
This makes the use of this representation infeasible in practice. Therefore, we eliminate the need
for clock tick events in Section 5 by making use of McDermott’s persist effects.
4.1 McDermott’s Rule Language for Probabilistic, Totally-ordered Temporal Projection
The different kinds of rules provided by this language are projection rules, effect rules, and exogenous event rules. Projection rules specify the sequence of dated events caused by low-level plans,
effect rules specify causal models of sensing processes and actions, and exogenous event rules are
used to specify the occurrence of events not under the control of the robot. We will describe these
kinds of rules below.
• Projection rules can be used to specify the sequence of events caused by the interpretation of
a low-level plan. Projection rules have the form
820

P ROBABILISTIC H YBRID ACTION M ODELS

project rule name(args)
if cond
with a delay of δ1 occurs ev1

...
with a delay of δn occurs evn

and specify that if low-level plan name(args) is executed and condition cond holds then the
low-level plan generates the events ev1 , ..., evn with relative delays δ1 , ..., δn , respectively.
Thus, projection rules generate a sequence of dated events.
Uncertain models can be represented by sampling the δ i from a probability distribution over
durations and by specifying conditions that are satisfied only with a certain probability.
• Effect rules are used to specify conditional probabilistic effects of events. They have the form
e→p rule name
if cond
then with probability θ event ev causes effs

and specify that whenever event ev occurs and cond holds, then with probability θ create and
clip states as specified in effs. The effects of the e→p rule rules have the form A, causing the
occasion A to hold, clip A, causing A to cease to hold, and persist t A, causing A to hold for t
time units.
• Exogenous event rules are used to specify the conditional occurrence of exogenous events.
The rule
p→e rule name
while cond with an avg spacing of τ occurs ev

specifies that over any interval in which cond is true, exogenous events ev are generated
Poisson-distributed with an average spacing of τ time units.
Before proving properties of our model we must first introduce McDermott’s semantics of possible worlds. To do so, we define the key notions of the underlying conceptualization in Definition 1.
The evolution of the world is described as a sequence of dated instantaneous events where an occurrence e@t specifies that event e occurs at time instant t. In addition, we have a function mapping
time instants into world states. More precisely these notions are defined as follows (see McDermott,
1994).
Definition 1 A world state is a function from propositions to {T,F,⊥} and is extended to boolean
formulas in the usual way. An occurrence e@t is a pair c = (e, t), where e is an event and t a
time (t ∈ <+ ). An occurrence sequence is a finite sequence of occurrences, ordered by date. Its
duration is the date of the last occurrence. A world of duration L, where L ∈ < + , is a complete
history of duration L, that is, it is a pair (C, H), where C is an occurrence sequence with duration
≤ L, and H is a function from [0, L] to world states. In H(0) all propositions are mapped to F, and
if t1 < t2 and H(t1) 6= H(t2) then there must be an occurrence e@t with t1≤t≤t2.
821

B EETZ & G ROSSKREUTZ

We use the following abbreviations: (A ↓ t)(W ), “A after t in execution scenario W”, to mean
that there is some δ > 0 such that ∀t0 : t < t0 < t + δ ⇒ H(t0 )(A) = T . (A ↑ t)(W ), “A before t
in W” is similarly defined, but the upper bound for t0 includes t.
After we have described how we represent the change in the world, we state the conditions
under which worlds of duration are consistent with a given set of event effects and exogenous event
rules. To do so, we have to state the constraints that the rules, the local probabilistic models, impose
on state evolution. For this definition we take the plan generated events that are typically specified
through project rules, as given.
Definition 2 If T is a set of rules as defined above, Exog is an occurrence sequence 2 , P is the set
of propositions, and L is a real number ≥ duration(Exog), then an L-Model of T and Exog is a
pair (U, M ), where U is a set of worlds of duration L such that ∀(C, H) ∈ U : Exog ⊂ C, and
M is a probability measure on U that obeys the following restrictions: (A↓t), (A↑t) and e@t are
considered as random variables. A is the “annihilation” of a conjunction A, that is the conjunction
of the negations of the conjuncts of A.
1. Initial blank state: ∀A ∈ P : M (A ↑ 0) = 0.
2. Event-effect rules: If T contains a rule instance
e→p rule name if A then with probability r event e causes B,

then for every date t, require that, for all nonempty conjunctions C of literals from B:
M (C↓t|e@t ∧ A↑t ∧ B↑t) = r.
3. Event-effect rules when the events don’t occur: Suppose B is an atomic formula, and let
R = {Ri } be the set of all instances of e→p rules whose consequents contain B or ¬B. If
e→p rule Ri = if Ai then with probability pi event Ei causes Ci , then let Di = Ai ∧ Ci ,
Then M (B↓t|B↑t ∧ N ) = 1 and M (B↓t|¬B↑t ∧ N ) = 0 where N = (¬E1 @t ∨ ¬D1 ) ∧
(¬E2 @t ∨ ¬D2 ) ∧ ....
4. Event-occurrence rules: For every time point t such that no occurrence with date t is in
Exog and every event E, such that there is exactly one instance
p→e rule name while A with an avg spacing of d occurs E

with M (a↑t) > 0 require
limdt→0

M (occ. of class E between t and t+dt|A↑t)
= 1/d
dt

limdt→0

M (occ. of class E between t and t+dt|¬A↑t)
=0
dt

2. Exog is an occurrence sequence, which represents the events generated by the interpretation of the robot’s plan and
modeled using projection rules.

822

P ROBABILISTIC H YBRID ACTION M ODELS

if there exists no so such rule, require
limdt→0

M (occ. of class E between t and t+dt)
=0
dt

5. Conditional independence: If one of the previous clauses defines a conditional probability
M (α|β), which mentions times t, then α is conditionally independent, given β, of all other
random variables mentioning times on or before t. That is, for arbitrary γ mentioning times
on or before t, M (α|β) = M (α|β ∧ γ).
McDermott (1994) shows that this definition yields a unique probability distribution M . He also
gives a proof that his projection algorithm draws random execution scenarios sampled from this
unique probability distribution implied by the given probabilistic models.
4.2 Probabilistic Temporal Rules for PHAMs
In order to predict the evolution of a hybrid system, we specify rules in McDermott’s rule language
that, given the state of the system and a time t, predict the successor state at t. To predict the
successor state, we must distinguish three cases: first, a control mode transition occurs; second, an
exogenous event occurs; and third, the system changes according to the flow of the current mode.
We will start with the rules for predicting control mode jumps. To ensure that mode transitions
are generated as specified by the probability distributions over the successor modes, we will use
the predicate randomlySampledSuccessorMode(e,cm) and realize a random number generator using
McDermott’s rule language.
randomlySampledSuccessorMode(e,cm) ≡
probRange(e, max) ∧ randomN umber(n, max)
∧jumpSuccessor(e, cm, range) ∧ n ∈ range
In order to sample values from probability distributions we have to axiomatize a random number
generator that asserts instances of the predicate randomNumber(n,max) used above (see Beetz &
Grosskreutz, 2000). We do this by formalizing a randomize event. McDermott (1994) discusses the
usefulness of, and the difficulties in, realizing nondeterministic exclusive outcomes. Therefore in
his implementation he escapes to Lisp and uses a function that returns a random element.
Lemma 1 At any time point randomNumber has exactly one extension randomNumber(r,max) where
r is an unbiased random between 0 and max.
Proof: Let max∗ be the largest probRange extension and randomBit(i,value) the i-th random bit.
The start event that causes the initial state timeline causes randomBit(i,0) ∀0 ≤ i ≤ log max ∗ .
Thereafter, a randomize event is used to sample their value:
e→p rule RANDOMIZE
if randomBit(i,val) ∧ negation(val,neg)
then with probability 0.5
event randomize
causes randomBit(i,neg) ∧ clip randomBit(i,val)

823

B EETZ & G ROSSKREUTZ

Rule M ODE -J UMP causes a control mode transition as soon as the jump condition cond becomes
true. The rule says that in any interval in which cm is the current control mode and in which the
jump condition cond for leaving cm following edge edge a jump along edge will occur with an
average delay of τ time units.
p→e rule M ODE -J UMP
while mode(cm) ∧ jumpCondition(cm,cond,edge)

~ ∧ satisfies(vals,cond)
∧ stateVarsVal(vals)
with an average spacing of τ time units
occurs jump(edge)
Rule J UMP -E FFECTS specifies the effects of an jump event on the control mode, system variables, and the flow. If cm is a control mode randomly sampled from the probability distribution
over successor nodes for jumps along edge then the jump along edge has the following effects. The
values of the state variables and the flow condition of the previous control mode cm old are retracted
and the ones for the new control mode cm are asserted.
e→p rule J UMP -E FFECTS
if randomlySampledSuccessorMode(edge,cm)

~ ∧ flowCond(cm,flow)
~ ∧ now(t)
∧ initialValues(cm,val)
∧ mode(cmold ) ∧ flow(flowold ) ∧ valuesAt(told ,valold )
then with probability 1.0
event jump(edge)
~
~ ∧ valuesAt(transTime,val)
causes mode(cm) ∧ flow(flow)
∧ clip mode(cmold ) ∧ clip flow(flowold ) ∧ clip valuesAt(told ,valold )
Time is advanced using clock-tick events. With every C LOCK -T ICK (?t ) event the now predicate
is updated by clipping the previous time and asserting the new one. Note, the time differs at most
dtclock time units from the actual time.
e→p rule C LOCK -RULE
if now(to )
then with probability 1.0
event clock-tick(t)
causes now(t) ∧ clip now(to )

Exogenous events are modeled using rules of the following structure. When the navigation
~ of the state variables satisfy the condition for
process is in the control mode cm and the values vals
the occurrence of the exogenous event ev, then the event ev occurs with average spacing of τ time
units.
p→e rule C AUSE -E XO -E VENT
while mode(cm) ∧ exoEventCond(cm,cond,ev)

~ ∧ satisfies(vals,cond)
∧ stateVarsVal(vals)
with an average spacing of τ time units
occurs exoEvent(ev)
824

P ROBABILISTIC H YBRID ACTION M ODELS

The effects of exogenous event rules are specified by rules of the following form. The exoge~
nous event exoEvent(ev) with effect specification exoEffect(ev, val))
causes the values of the state
~
~
variables to change from valo to val.
e→p rule E XO -E VENT-E FFECT

~ ∧ valuesAt(to ,valo ) ∧ now(t)
if exoEffect(ev,val))
then with probability 1.0
event exoEvent(ev)
~ ∧ clip valuesAt(to ,val
~ o)
causes valuesAt(t,val)
4.3 Properties of PHAMs
We have seen in the last section that a PHAM consists of the rules above and a set of facts that
constitute the hybrid automata representation of a given CRP. In this section we investigate whether
PHAM s make the “right” predictions.
There are essentially three properties of predicted execution scenarios that we want to ensure.
First, predicted control mode sequences are consistent with the specified hybrid system. Second,
mode jumps are predicted according to the specified probability distribution over successor modes.
Third, between two successive events, the behavior is predicted according to the flow of the respective control mode.
As McDermott’s formalism does not allow for modeling instantaneous state transitions we can
only show that control mode sequences in execution scenarios are probably approximately accurate.
In our view, this is a low price for the expressiveness we gain through the availability of Poisson
distributed exogenous events.
The subsequent lemma 2 states that control mode jumps can be predicted with arbitrary accuracy
and arbitrarily high probability by decreasing the time between successive clock ticks.
Lemma 2 For each probability  and delay δ, there exists a τ (average delay of the occurrence of
an event after the triggering condition has become true) and a dt clock (time between two subsequent
clock ticks) such that whenever a jump condition becomes satisfied, then with probability ≥ 1 −  a
jump event will occur within δ time units.

Proof: Let t be the time where the jump condition is fulfilled. If τ ≤ δ/(2 log(1/)) and dt clock ≤
δ/2 then at most δ/2 time units after t the antecedent of rule M ODE -J UMP is fulfilled. The probability that no event of class jump(cm0 ) occurs between t+δ/2 and t+δ is ≤ e−δ/(2τ ) = e−log(1/) = ,
so with probability ≥ 1 −  such an event will occur at most δ time units after t.

This implies that there is always a non-zero chance that control mode sequences are predicted
incorrectly. It happens only when two jump conditions become true and the jump triggered by the
later condition occurred before the other one. However, the probability of such incorrect predictions
can be made arbitrarily small by the choice of τ and dtclock .
The basic framework of hybrid systems does not take the possibility of exogenous events into
account and thereby allows for proving strong system properties such as the reachability of goal
states from arbitrary initial conditions or safety conditions for the system behavior (Alur et al.,
825

B EETZ & G ROSSKREUTZ

1997, 1996). For the prediction of robot behavior in dynamic environments these assumptions,
however, are unrealistic. Therefore, we only have a weaker property, namely the correspondence
between the predicted behavior and the flows specified by the hybrid system between immediate
subsequent events.
Lemma 3 Let W be an execution scenario, e1 @t1 and e2 @t2 be two immediate subsequent events
of type jump or exoEvent, and cm be the control mode after t 1 in W . Then, for every occurrence
~ )) is unique. Further, vals
~ = vals
~ 1 + (t - t1 ) *
e@t with t1 < t ≤ t2 W(t)(stateVarVals(vals
~
flow(cm), where vals1 are the values of the state variables at t1 .

Proof: There are only two classes of rules that affect the value of valuesAt and flow: rule JUMP E FFECTS, and rule E XO -E VENT-E FFECT. These rules always clip and set exactly one extension of
the predicates, thus together with the fact that the initial event asserts exactly one such predicate,
the determined value is unique.
During the interval between t1 and t2 the extension of stateVarVals evolves according to the flow
condition of mode cm due to the fact that flow is not changed by rule E XO -E VENT-E FFECT. Thus
it remains as initially set by rule JUMP -E FFECTS, which asserts exactly the flow corresponding to
cm. The proposition then follows from the assumption of a correct axiomatization of addition and
scalar-vector multiplication.

Another important property of our representation is that jumps are predicted according to the
probability distributions specified for the hybrid automaton.
Lemma 4 Whenever a jump along an edge e occurs, the successor state is chosen according to the
probability distribution implied by probRange and jumpSuccessor.

Proof: This follows from the properties of the randomize event and Rule Jump-Effects.

Using the lemmata we can state and show the central properties of PHAMs: (1) the predicted
control mode transitions correspond to those specified by the hybrid automaton; and (2) the same
holds for the continuous predicted behavior between exogenous events; (3) Exogenous events are
generated according to their probabilities over a continuous domain (this is shown in McDermott,
1994).
Theorem 1 Every sequence of mode(cm) occasions follows a branch (cm i ), ..., (cmj ) of the hybrid automaton.

Proof: Each occasion mode(cm) must be asserted by rule J UMP -E FFECTS. Therefore there must
have been a jump(e) event. Consequently, there must have been a jumpCondition from the previous
control mode to cm.

826

P ROBABILISTIC H YBRID ACTION M ODELS

Because jump events are modeled as Poisson distributed events there is always the chance of
predicting control mode sequences that are not valid with respect to the original hybrid system. So
next we will bound the probability of predicting such mode sequences by choosing the parameterization of the jump event and clock tick event rules appropriately.
Theorem 2 For every probability  there exists an average delay of a mode jump event τ and a
delay dtclock with which the satisfaction of jump conditions is realized such that with probability
~ of stateVarVals occasions between two immediate subsequent exogenous events
≥ 1 −  the vals
follow a state trajectory of the hybrid automaton.

Proof: The proof is based on the property that jumps occur in their correct order with an arbitrarily
high probability. In particular, we can choose δ as a function of the minimal delay between jump
conditions becoming true. Then, the jumps to successor modes occur with arbitrarily high probability (Lemma 2). Finally, according to Lemma 3 the trajectory of stateVarVals between transitions
is accurate.


5. The Implementation of PHAMs
We have now shown that PHAMs define probability distributions over possible execution scenarios
with respect to a given belief state. The problem of using PHAMs is obvious. Nontrivial CRPs for
controlling robots reliably require hundreds of lines of code. There are typically several control
processes active, many more are dormant, waiting for conditions that trigger their execution. The
hybrid automata for such CRPs are huge, the branching factors for mode transitions are immense.
Let alone the distribution of execution scenarios that they might generate. The accurate computation
of this probability distribution is prohibitively expensive in terms of computational resources.
There is a second source of inefficiency in the realization of PHAMs. In PHAMs we have used
clock tick rules, Poisson distributed events, that generate clock ticks with an average spacing of
τ time units. We have done so, in order to formalize the operation of CRPs in a single concise
framework. The problem with this approach is that in order to predict control mode jumps accurately
we must choose τ to be very small. This, however, increases the number of clock tick events
drastically and makes the approach infeasible for all but the most simple scenarios.
In order to draw sample execution scenarios from the distribution implied by the causal model
and the initial state description we use an extension of the XFRM projector (McDermott, 1992b) that
employs the RPL interpreter (McDermott, 1991) together with McDermott’s algorithm for probabilistic temporal projection (McDermott, 1994). The projector takes as its input a CRP, rules for
generating exogenous events, a set of probabilistic rules describing the effects of events and actions,
and a (probabilistic) initial state description. To predict the effects of low-level plans the projector samples effects from the probabilistic causal models of the low-level plans and asserts them as
propositions to the timeline. Similarly, when the plan activates a sensor, the projector makes use of
a model of the sensor and the state of the world as described by the timeline to predict the sensor
reading.
In this section we investigate how we can make effective and informative predictions on the basis
of PHAMs that can be performed at a speed sufficient for prediction-based online plan revision.
To achieve effectiveness we use two means. First, we realize weaker inference mechanisms that
827

B EETZ & G ROSSKREUTZ

are based on sampling execution scenarios from the distribution implied by the causal models and
the initial state description. Second, we replace the clock tick event mechanism with a different
mechanism that infers the occurrence of control mode jumps and uses the persist effect to generate
the respective delay. We will detail these two mechanisms in the remainder of this section.
5.1 Projection with Adaptive Causal Models
Let us first turn to the issue of eliminating the inefficiencies caused by the clock tick mechanism.
We will do so by replacing clock tick rules with a mechanism for tailoring causal models on the fly
and using the persist effects of the probabilistic rule language.
For efficiency reasons the process of projecting a continuous process p is divided into two
phases. The first phase estimates a schedule for endogenous events caused by p while considering possible effects of p on other processes but not the effects of the other processes on p. This
schedule is transformed into a context-specific causal model tailored for the plan which is to be
projected. The second phase projects the plan p using the model of endogenous events constructed
in the first phase. This phase takes into account the interferences with concurrent events and revises the causal model if situations arise in which the assumptions of the precomputed schedule are
violated.
The projection module uses a model of the dynamic system that specifies for each continuous
control process the state variables it changes and for each state variable the fluents that measure that
state variable. For example, consider the low-level navigation plans that steadily change the robot’s
position (that is the variables x and y). The estimated position of the robot is stored in the fluents
robot-x and robot-y:
changes(low-level-navigation-plan, x)
changes(low-level-navigation-plan, y)
measures(robot-x, x)
measures(robot-y, y)
Extracting relevant conditions. When the projector starts projecting a low-level navigation plan
it computes the set of pending conditions that depend on robot-x and robot-y, which are the fluents
that measure the state variables of the dynamic system and are changed by the low-level navigation
plan. These conditions are implemented as fluent networks.
Fluent networks are digital circuits where the components of the circuit are fluents. Figure 12
shows a fluent network where the output fluent is true, if and only if the robot is in room A-120. The
inputs of the circuit are the fluents robot-x and robot-y and the circuit is updated whenever robot-x
and robot-y change.
Our reactive plans are set up such that the fluent networks that compute conditions for which
the plan is waiting can be determined automatically using (P ROLOG-like) relational queries:
setof ?fl-net ( fluent(?fl) ∧ status(?fl,pending)

∧ changes(low-level-nav-plan, ?state-var)
∧ measures(?state-var-fl, ?state-var)
∧ depends-on(?fl, ?state-var-fl)
∧ fluent-network(?fl, ?fl-net) )
?pending-fl-nets
828

P ROBABILISTIC H YBRID ACTION M ODELS

1265.0
robot-x

<
>
AND

860.0
817.0
robot-y

IN-A-120?

<

Figure 12: Fluent network for being in room A-120. The robot believes it is in room A-120 if its
estimated x-coordinate is between 860 and 1265 and the y-coordinate is smaller than
817, where the hallway begins.

This query determines ?pending-fl-nets, the set of fluent networks ?fl-net such that ?fl-net is a network with output fluent ?fl. ?fl causes a plan thread to pend and depends on a fluent measuring
a state variable ?state-var changed by the low-level navigation plan. The extraction of these conditions is done automatically. The automatic extraction requires the conditions be in a particular
form and the effects of low-level plans on state variables and the sensing of state variables to be
represented explicitly.
To predict when the fluent IN-A-120? will become true or false, we have to compute the region
in the state space that corresponds to the fluent and compute the intersections of the robot’s state
trajectories with this region.
Endogenous event schedules. For each class of continuous processes we have to provide an endogenous event scheduler that takes the initial conditions and the parameterization of the process,
and the fluent networks that might be triggered and computes the endogenous event schedule. The
endogenous event scheduler for the low-level navigation plans is described in the next section. Given
the kind of process (e.g., low-level navigation plan), the process parameters (e.g., the destination of
the robot), and the pending fluent networks, the scheduler returns a sequence of composite endogenous events. Composite events are represented as triples of the form (∆t, hsv 1 , ..., svn i, {ev1 , ...,
evm }). ∆t is the delay between the ith and the i+1st event in the schedule, hsv 1 , ..., svn i the values
of the state variables, and {ev1 , ..., evm } the atomic events that are to take place.
If a state for which the plan is waiting, becomes true at a time instance t, then at t a passivesensor-update event is triggered. passive-sensor-update is an event model that takes a set of fluents
as its parameters, retrieves the values of the state variables measured by these fluents, applies the
sensor model to these values, and then sets the fluents accordingly.
A causal model of low-level navigation plans. Projecting the initiation of the execution of a
navigation plan causes two events: the start event and a hypothetical completion event after an
infinite number of time units. This is shown in the following projection rule.
829

B EETZ & G ROSSKREUTZ

project rule LOW- LEVEL - NAVIGATION - PLAN
if true
with a delay of 0
occurs begin(low-level-nav-plan(?dest-descr, ?id, ?fluent)
with a delay of ∞
occurs end(low-level-nav-plan(?dest-descr, ?id, ?fluent)

The effect rule of the start event of the low-level navigation plan computes the endogenous event
schedule and asserts the next endogenous navigation event into the timeline.
e→p rule ENDOGENOUS - EVENTS
if endogenous-event-schedule(low-level-nav-plan(?dest-descr, ?schedule))
then with probability 1.0
event begin(low-level-nav-plan(?dest-descr, ?id, ?fluent))
causes predicted-events(?id, ?schedule)

∧ running(robot-goto(?descr, ?id))
∧ next-nav-event(?id))
The occasion next-nav-event(?id) triggers the next endogenous event begin(follow-path(?here
h?x,?yi) ?dt ?id)). The remaining two conditions determine the parameters of the follow-path event:
the next scheduled event and the robot’s position.
p→e rule C AUSE -E XO -E VENT
while next-nav-event(?id)

∧ predicted-events(?id, ((?dt h?x,?yi ?evs) !?remaining-evs)
∧ robot-loc(?here)
with an average spacing of 0.0001
occurs begin(follow-path(?here, h?x,?yi, ?dt, ?id))
The effect rule of the begin(follow-path (...)) event specifies among other things that the next
endogenous event will occur after ?dt time units (persist ?dt sleeping(?id)).
e→p rule FOLLOW- PATH
if robot-loc(?coords)
then with probability 1.0
event begin(follow-path(?from, ?to, ?dt, ?id))
causes running(follow-path(?from, ?to, ?dt, ?id))
∧ clip robot-loc(?coords)
∧ clip next-nav-event(?id)
∧ persist ?dt sleeping(?id)

If a running follow path event has finished sleeping the end (follow-path (...)) event occurs.
p→e rule T ERMINATE -F OLLOW-PATH
while not sleeping(?id)

∧ running(follow-path(?from, ?to, ?time, ?id))
with an average spacing of 0.0001
occurs end(follow-path(?from, ?to, ?time, ?id))
830

P ROBABILISTIC H YBRID ACTION M ODELS

Our model of low-level navigation plan presented so far suffices as long as nothing important
happens while carrying out the plan. However, suppose that an exogenous event that causes an
object to slip out of the robot’s hand is projected at time instant t while the robot is in motion. To
predict the new location of the object the projector predicts the location l of the robot at the time t
using the control flow and asserts it in the timeline.
Qualitative changes in the behavior of the robot caused by adaptations of the travel mode
are described through e→p -rules. The following e→p -rule describes the effects of the event
nav-event(set-travel-mode(?n)), which represents the low-level navigation plan resetting the travel
mode:
e→p rule SET- DOORWAY- MODE
if travel-mode(?m)
then with probability 1.0
event nav-event(set-travel-mode(doorway))
causes clip travel-mode(?m)
∧ clip obstacle-avoidance-with(sonar)

∧ travel-mode(doorway)
The rule specifies that if at a time instant at which an event nav-event(set-travel-mode(?n))
occurs the state travel-mode(?m) holds for some ?m, then the states travel-mode(?m) and obstacleavoidance-with(sonar) will (with a probability of 1.0) not persist after the event has occurred, i.e.,
they are clipped by the event. The event causes the state travel-mode(doorway) to hold until it is
adapted next time.
The rules listed above are hand-coded and plan-specific. An investigation of whether the plans
can be coded such that the rule specification can be automated is on our agenda for future research.
5.2 Endogenous Event Scheduler
We have just shown how events are projected from a given endogenous event schedule, but we
have not shown how the schedule is constructed. Thus, this section describes the endogenous event
scheduler for low-level navigation plans. The scheduler predicts the effects of the low-level navigation plan on the state variables x and y. The endogenous event scheduler assumes the robot is
following a straight path between locations 1 to 5. As we have pointed out earlier, there are two
kinds of events that need to be predicted: the ones causing qualitative physical change and the ones
causing the trigger conditions that the plan is waiting for.
The qualitative events caused by the low-level navigation plan pictured in Figure 13 are the
ones that occur when the robot arrives at the locations 1, 2, 3, 4, and 5 in which the robot either
changes its travel mode or arrives at its destination. For each of these time instants the occurrence
of a set-travel-mode-event is predicted.
The scheduler for triggering events works in two phases: (1) it transforms the fluent network
into a condition that it is able to predict and (2) it applies an algorithm for computing when these
events occur. The conditions that are caused by the low-level navigation plan can be represented
as regions in the environment such that the condition is true if and only if the robot is within this
region. The elementary conditions are numeric constraints on the robot’s position or the distance of
the robot to a given target point. The scheduler assumes that robot-x and robot-y are the only fluents
831

B EETZ & G ROSSKREUTZ

in these networks that change their value during the execution of the plan. More complex networks
can be constructed as conjunctions and disjunctions of the elementary conditions.

6
A−111

X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X 5ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZX
ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZX
ZXYZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZX
ZXYZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X 4ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZX
ZXYZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X 3ZY
X ZY
X ZX
ZXYZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X ZY
X 2 ZY
X ZY
X ZY
X ZX
ZXYZY
1

A−117

Figure 13: Initially predicted endogenous events.
In the next step the endogenous event scheduler overlays the straight line path through the intermediate goal points of the topological navigation path (see Figure 7) with the regions computed
in the previous step. It then computes a schedule for the endogenous events by following the navigation path and collecting the intersections with the regions (see Figure 13). The result of the
scheduling step is a sequence of triples of the form (∆ti , hxi , yi i, {ev1 , ..., evn }).
Rescheduling endogenous events. One problem that our temporal projector has to deal with is that
a wait for step might be executed while a low-level navigation plan is projected. For example, when
the robot enters the hallway, the policy that looks for the opening angles of doors when passing them
is triggered. Therefore, the causal model that was computed by the endogenous event scheduler is
no longer sufficient. It fails to predict the “passing a door” events.
These problems are handled by modifying the endogenous event schedule: whenever the robot
starts waiting for a condition that is a function of the robot’s position, it interrupts the projection of
the low-level navigation plan, adapts the causal model of the low-level navigation plan, and continues with the projection. In the case of entering the hallway, a new endogenous event schedule that
contains endogenous events for passing doorways is computed. This updated schedule of endogenous events is pictured in Figure 14.
5.3 Projecting Exogenous Events, Passive Sensors and Obstacle Avoidance
One type of exogenous event is an event for which we have additional information about its time
of occurrence, such as the event that Dieter will be back from lunch around 12:25. These kinds of
events are represented by a p→e rule together with an e→p rule. The e→p rule specifies that the
832

P ROBABILISTIC H YBRID ACTION M ODELS

c dY
c dc
dY
c dc
dcYdY
c dc
dcYdY
c dc
dcYdY
c dc
dcYdY

[Y\Y
[ \Y
[ \Y
[ \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ \Y
[ \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ \Y
[ \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ \Y
[ \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ \Y
[ \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ \Y
[ 8 \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ \Y
[ \Y
[ \Y
[ \[
[Y\Y
[A−111
[ \Y
[ \Y
[ \Y
[ \[
\Y
[Y[Y
\ [Y
\ [Y
\ [Y
\ [Y
\ [\
[Y\Y
[ \Y
[ 7\Y
[ \Y
[ \Y
[ \[
[Y\Y
[ \Y
[ bY
[ bY
[ ba \Y
[ \[
a \Y
a \Y
6
a bY
a ba
bY
a 5bY
a ba 4
bY
a bY
a ba
bY
a bY
a ba
bY

_ `Y
_ `Y
_ `_
`Y
_ `Y
_ `_
`_Y`Y
_ `Y
_ `_
`_Y`Y
_ `Y
_ `_
`_Y`Y
_ `Y
_ `_
`_Y`Y

] ^Y
] ^Y
] ^]
^Y
] ^Y
] ^]
^]Y^Y
] ^Y
] ^]
^]Y^Y
3 ^Y
] ^Y
] ^Y
] ^]
] ^Y
] ^Y
] ^]
^Y

e fY
e fY
e fe
fY
e fY
e fe
feYfY
e fY
e fe
2 feYfY
e fY
e fY
e 1fe
fY
e fY
e fY
e fe
fY
1

A−117

Figure 14: Modified endogenous event schedule.

start event causes the state before-dieters-door-opens() to hold and persist for ?time time units. The
event dieters-door-opens() is triggered as soon as before-the-door-opens() no longer holds.
e→p rule BACK - FROM -L UNCH
if about(?time, 12:25) ∧ difference(?time, *now*, ?wait-for))
then with probability 1.0
event start
causes persist ?wait-for before-the-door-opens
p→e rule D OOR -O PENS
while thnot before-the-door-opens
with an average spacing of 0.0001
occurs dieters-door-is-opened

In order to predict the occurrence of exogenous events, the plan projector does the following.
It first computes the time when the robot will cause the next event e next . Let us assume that this
event occurs t time units after the last event elast and that c is the strongest condition that holds
from elast until enext .3 The following algorithm predicts the occurrence of the next exogenous
event accurately. First, for every p→e rule ri whose enabling condition is satisfied by c randomly
decide whether ei will occur between elast and enext based on the average temporal spacing of
ei events in situations where c holds. If ei is predicted to occur, select its occurrence time by
randomly selecting a time instant in the time interval between e last and enext (the exogenous events
3. The cases where enabling conditions of exogenous events are caused by the continuous effects between e last and
enext are handled analogously to the achievement of triggering conditions.

833

B EETZ & G ROSSKREUTZ

are Poisson distributed). Select the exogenous event that is predicted to occur the earliest, assert it
to the timeline, and continue the projection after the occurrence of this event.
The last two components we need to describe are passive sensors, which are steadily updated
and do not change the configuration of the robot and the behavior of the collision avoidance routines.
Readings of passive sensors only need to be projected if the measured state variables change
significantly or if the state variables traverse values that satisfy conditions for which the robot is
waiting. For each of these situations there is an update-passive-sensors event.
Collision avoidance is not modeled except in situations in which the robot is told about objects
that are moved around. In this case the endogenous event scheduler adds a region corresponding to
the object. If the region blocks the way to the destination — that is the robot cannot move around
the region — then a possible-bump-event is generated. The effect rule for a possible bump event
specifies that, if the robot has activated sensors that can detect the object, the low-level navigation
plan fails with a failure description “path blocked.” Otherwise a bump event is generated. For
example, since sonar sensors are the only sensors placed at table height, the collision avoidance
module can avoid a collision with a table only if the sonar sensors are active. Thus, to predict a
bump, the projector has to determine how long the sonar sensors have been switched off before the
possible bump event occurs.
5.4 Models of Complex Sensing Actions
To understand how other types of uncertainty can be modeled and how the causal models interact
with the plan interpretation let us look at a more complex sensing action realized through a low-level
plan look-for. The sub-plan is called with a visual description (?pl) of objects it is supposed to look
for.
Typically, in order to model a low-level plan we need a set of projection rules that probabilistically describe the possible event sequences and outcomes when activating a behavior module. In
each situation exactly one projection rule is applied (although the decision about which one might
be probabilistic).
One of these projection rules for look-for is listed below. The model consists of three parts.
The first part (line 1 to 7) specifies the condition under which this rule predicts the behavior of the
look-for correctly. The second part (lines 8 to 11) lists the events that look-for will cause if this
rule is applicable. Finally, the last line specifies how the low-level plan signals the completion of
its interpretation. In our case, the low-level plan succeeds and returns a list of object descriptions
(?desigs) as its value.
The condition of the projection rule determines where the robot is (1), probabilistically decides
whether the look-for is “normal” based on the camera used and the specifics of the location (2), and
infers what objects are located there (3). This inference is performed based on the robot’s probabilistic belief about the state of the world and the predicted exogenous events. The condition then
uses the sensor model for the camera in order to decide probabilistically how the robot perceives
each object. For each object that is perceived as matching the perceptual description ?pl a local
designator ?desig is created and collected in the variable desigs. The last condition (7) estimates
the time ?dt after which the look-for behavior completes. Upon completion of the look-for behavior
the projected interpretation process sends a success signal with the return value ?desigs as its argument. The projected behavior consists of three events: two that change the world begin(look-for(?pl,
?cam)) and end(look-for(?pl, ?cam)), which occurs ?dt later. The third event changes the compu834

P ROBABILISTIC H YBRID ACTION M ODELS

(1)
(2)
(3)
(4)
(5)
(6)

(7)
(8)
(9)
(10)
(11)
(12)

project rule look-for(?pl, ?cam)
if ( loc(robot, h?x,?yi)

∧ normal-look-for-behavior(?cam, ?loc)
∧ setof ?ob loc(?ob, h?x,?yi) ?obs-here
∧ sensor-model(?cam, ?sensor-model)
∧ features(?pl,?features)
∧ setof ?desig
( member(?ob,?obs-here)
∧ obj-seen(?ob, ?sensor-model)
∧ perceived-properties(?ob, ?features, ?sensor-model, ?pl)
∧ local-desig(?desig,?ob,?pl,?x,?y))
?desigs
∧ look-time(h?x,?yi, ?features, ?dt))
with a delay of 0 occurs mode transition begin(look-for(?pl, ?cam))
with a delay of ?dt occurs mode transition end(look-for(?pl, ?cam))
with a delay of 0 occurs trigger fluent visual-inputs-fluent(?cam)
with a delay of 0 occurs set fluent obs-pos-fluent ← ?seen)
with a delay of 0 occurs succeed ?desigs
Figure 15: A projection rule describing the behavior module look-for.

tational state of the structured reactive controller after passing ?dt time units. This event pulses the
fluent visual-inputs-fluent(?cam) and sets the fluent obs-pos-fluent(?cam).
Besides asserting the events that take place during the execution of a plan we have to specify
how these events change the world. This is done by using effect rules. One of them is shown in
Figure! 16. The rule specifies that if at a time instant at which an event end(look-for(?pl, ?cam))
occurs the state visual-track(?desig, ?ob) holds for some ?desig and ?ob, then the states visualtrack(?desig, ?ob) will not (with a probability of 1.0) persist after the event has occurred, i.e., they
are clipped by the event.
e→p rule VISUAL - TRACKING
if visual-track(?desig, ?ob)
then with probability 0.9
event end(look-for(?pl, ?cam))
causes clip visual-track(?desig, ?ob))

Figure 16: An e→p rule describing the effects of the event end(look-for(?pl, ?cam)).

5.5 Probabilistic Sampling-based Projection
So far we have looked at the issue of efficiently predicting an individual execution scenario. We
will now investigate the issue of drawing inferences that are useful for planning based on sampled
execution scenarios.
835

B EETZ & G ROSSKREUTZ

Recently, probabilistic sampling-based inference methods have been proposed to infer information from complex distributions quickly and with bounded risk (Fox, Burgard, Dellaert, & Thrun,
1999; Thrun, 2000). We will now discuss how we can use sampling-based projection for anticipating likely flaws with high probability.
Advantages of applying probabilistic sampling-based projection to the prediction of the effects
of CRPs are that it works independently of the branching factor of the modes of the hybrid automaton
and that it only constructs a small part of the complete PHAM.
But what kinds of prediction-based inferences can be drawn from samples of projected execution scenarios? The inference that we found most valuable for online revisions of robot plans
is: do projected execution scenarios drawn from this distribution satisfy a given property p with a
probability greater than θ? A robot action planner can use this type of inference to decide whether
or not it should revise a plan to eliminate a particular kind of flaw: it should revise the plan if it
believes that the flaw’s likelihood exceeds some threshold and ignore it otherwise. Of course, such
inferences can be drawn based on samples only with a certain risk of being wrong. Suppose we want
the planner to classify any flaw with probability greater than θ as to be eliminated and to ignore any
flaw less likely than τ . We assume that flaws with probability between τ and θ have no large impact
on the robot’s performance. How many execution scenarios should the plan revision module project
in order to classify flaws correctly with a probability greater than 95%?
A main factor that determines the performance of sample-based predictive flaw detection is the
flaw detector. A flaw detector classifies a flaw as to be eliminated if the probability of the flaw with
respect to the robot’s belief state is greater than a given threshold probability θ. A flaw detector
classifies a flaw as hallucinated if the probability of the flaw with respect to the robot’s belief state
is smaller than a given threshold τ . So far we do not consider the severity of flaws, which is an
obvious extension. Typically, we choose θ starting at 50% and τ smaller than 5%.
Specific flaw detectors can be realized that differ with respect to (1) the time resources they
require; (2) the reliability with which they detect flaws that should be eliminated; and (3) the probability that they hallucinate flaws. That is, they signal a flaw that is so unlikely that eliminating the
flaw would decrease the expected utility.
To be more precise consider a flaw f that occurs in the distribution of execution scenarios of
a given scheduled plan with respect to the agent’s belief state with probability p. Further, let X i (f)
represent the event that behavior flaw f occurs in the ith execution scenario: X i (f ) = 1, if f occurs
in the ith projection and 0 otherwise.
P
The random variable Y(f,n) = ni=1 Xi (f) represents the number of occurrences of the flaw f in
n execution scenarios. Define a probable schedule flaw detector D ET such that D ET(f,n,k) = true
iff Y(f,n) ≥ k, which means that the detector classifies a flaw f as to be eliminated if and only if f
occurs in at least k of n randomly sampled execution scenarios. Thus D ET(f,n,k) works as follows.
It first projects n execution scenarios. Then it counts the number of occurrences of the flaw f in the
n execution scenarios. If it is greater or equal to k then the D ET(f,n,k) returns true, false otherwise.
Now that we have defined the schedule flaw detector, we can characterize it. Since the occurrence of schedule flaws in randomly sampled execution scenarios are independent from each other,
the value of Y(f) can be described by the binomial distribution b(n,p). Using b(n,p) we can compute
the likelihood of overlooking a probable schedule flaw f with probability p in n execution scenarios:

j−1 
X
n
∗ pk ∗ (1 − p)n−k
P (Y (f ) < j) =
k
k=0

836

P ROBABILISTIC H YBRID ACTION M ODELS

D ET(f,3,2)
D ET(f,4,2)
D ET(f,5,2)

50%
50.0
68.8
81.2

Prob. of Flaw θ
60% 70% 80%
64.8 78.4 89.6
81.2 91.6 97.3
91.3 96.9 99.3

90%
97.2
99.6
99.9

Figure 17: The table shows the probability of the flaw detectors D ET(f,i,2) detecting flaws that have
the probability θ = 50%, 60%, 70%, 80%, and 90%.

Figure 17 shows the probability that the flaw detector D ET(f,n,2) for n = 3,...,5 will detect a
schedule flaw with probability θ. The probability that the detectors classify flaws less likely than τ
as to be eliminated is smaller than 2.3% (for all n≤5).
When using the prediction-based scheduling as a component in the controller of the robot office
courier we typically use D ET(f,3,2), D ET(f,4,2), and D ET(f,5,2) for the different experiments, which
means a detected flaw is classified as probable if it occurs at least twice in three, four, or five
detection readings.
Figure 18 shows the number of necessary projections to achieve β = 95% accuracy. For a
detailed discussion see the work of Beetz et al. (1999).
θ
τ =.1%
τ =1%
τ =5%

1%
1331
⊥
⊥

10%
100
121
392

20%
44
49
78

40%
17
17
22

60%
8
8
9

80%
3
3
3

Figure 18: The table lists the number of randomly sampled projections needed to differentiate failures with an occurrence probability lower than τ from those that have a probability
higher than θ with an accuracy of 95%.
The probabilistic sampling-based projection mechanism becomes extremely useful for improving robot plans during their execution once the execution scenarios can be sampled fast enough. At
the moment a projection takes a couple of seconds. The overhead is mainly caused by recording
the interpretation of RPL plans in a manner that is far too detailed for our purposes. Through a
simplification of the models we expect an immediate speed up of up to one order of magnitude. It
seems that with a projection frequency of about 100 Hz one could start tackling a number of realistic
problems that occur at execution time continually.

6. Evaluation
We have validated our causal model of low-level navigation plans and their role in office delivery plans with respect to computational resources and qualitative prediction results in a series of
experiments.
837

B EETZ & G ROSSKREUTZ

6.1 Generality
PHAM s are capable of predicting the behavior generated by flexible plans written in plan execution
languages such as RAP (Firby, 1987) and PRS (Myers, 1996). To do so, we code the control
structures provided by these languages as RPL macros. To the best of our knowledge PHAMs are
the first realistic symbolic models of the sequencing layer of 3T architectures, the most commonly
used software architectures for controlling intelligent autonomous robots (Bonasso et al., 1997).
These architectures run planning and execution at different software layers and different time scales
where a sequencing layer synchronizes between both layers. Each layer uses a different form of
plan or behavior specification language. The planning layer typically uses a problem space plan,
the execution layer employs feedback control routines that can be activated and deactivated. The
intermediate layer typically uses a reactive plan language. The use of PHAMs enables 3T planning
systems to make more realistic predictions of the robot behavior that is generated from their abstract
plans. PHAMs are also capable of modeling different arbitration schemes and superpositions of the
effects of concurrent control processes.

The causal models proposed here complement those introduced by Beetz (2000). He describes
sophisticated models of object recognition and manipulation that allow for the prediction of plan
failures including those that are caused by the robot overlooking or confusing objects, objects changing their location and appearance, and faulty operation of effectors. These models, however, were
given for a simulated robot acting in a grid world. In this article, we have restricted ourselves to the
prediction of behavior generated by modern autonomous robot controllers. Unfortunately, object
recognition and manipulation skills of current autonomous service robots are not advanced enough
for action planning. On the other hand, it is clear that action planning capabilities pay off much
better if robots manipulate their environments and there is a risk of manipulating the wrong objects.
6.2 Assumptions and Restrictions
The control problem for autonomous robots is to generate effective and goal-directed control signals for the robot’s perceptual and effector apparatus within a feedback loop. Plan-based robot
control is a specialization of this control problem, in which the robot generates the control signals
by maintaining and executing a plan that is effective and has a high expected utility with respect to
the robot’s dynamically changing belief state. This problem is so general that we cannot hope to
solve it in this form.
In Computer Science it is common to characterize the computational problems a program can
solve through the language in which the input for the program is specified. For example, we distinguish compilers for regular and context-free programming languages. The same is true for planbased control of agents. Typically, planning problems are described in terms of an initial state
description, a description of the actions available for the agents, their applicability conditions and
effects, and a description of the goal state.
The three components of planning problems are typically expressed in some formal language.
The problem solving power of the planning systems is characterized by the expressiveness of the
languages for the three inputs. Some classes of planning problems are entirely formulated in propositional logic while others are formulated in first order logic. We further classify the planning
problems with respect to the expressiveness of the action representations that they use; whether
they allow for disjunctive preconditions, conditional effects, quantified effects, and model resource
838

P ROBABILISTIC H YBRID ACTION M ODELS

consumption. Some planning systems even solve planning problems that involve different kinds of
uncertainty.
In contrast, SRCs use methods that make strong assumptions about plans to simplify the computational problems. As a consequence, SRCs can apply reliable and fast algorithms for the construction and installment of sub-plans, the diagnosis of plan failures, and for editing sub-plans during
their execution. Making assumptions about plans is attractive because planning algorithms construct and revise the plans and can thereby enforce that the assumptions hold.
In a nutshell, the set of plans that an SRC generates is the reflexive, transitive closure of the
routine plans with respect to the application of plan revision rules. Thus, to enforce that all plans
have a property Q it is sufficient that the routine plans satisfy Q and that the revision rules preserve Q.
These properties make it particularly easy to reason about the plans while the plans can still specify
the same range of concurrent percept-driven behavior that RPL can. The properties of plans that play
an important role in this article are their generality, flexibility, and reliability. These properties are
achieved through careful design and hand-coding. As a consequence plan generation and revision
can be performed by programmed heuristic rules. We believe, however, that such plans and rules
can be learned from experience.
We make two other important assumptions. First, we assume that the tasks and the environment
is benign and therefore behavior flaws do not result in disasters. This is important, because robots
must make errors in order to learn the competent performance of tasks from experience. And only
if the planner is allowed occasionally to propose worse plans we can apply fast planning methods
based on Monte Carlo methods to improve the average performance of the robot.
Another design decision is that we do not explicitly represent the belief state of the robot, that is
the probability distributions over the values of the state variables. This, however, does not need to
imply that we cannot reason about inaccuracies and uncertainties of the robot’s estimate of the world
state. Beetz et al. (1998) describe how to couple plan-based high-level control with probabilistic
state estimation. In this article the state estimator automatically computes and signals properties of
the belief state such as the ambiguity and inaccuracy of state estimates to the plan-based controller.
The plan-based controller, on the other hand, uses these signals in order to decide when to interrupt
its missions to re-localize the robot.
6.3 Scaling Up
The causal models that we have described in Section 5 have been used for execution time planning
for a robot office courier. The plans that have been projected were the original plans for this application and typically several hundreds of code lines long. The projected execution scenarios contained
hundreds of events. Because the projection of single execution scenarios can cost up to a second,
robots must revise plans based on very few samples. Thus, the robot can only detect probable flaws
with high reliability.
The computational resources are mainly consumed by bookkeeping mechanisms that record
the computational state of the robot at any time instant represented in the execution scenario and
not by the mechanisms proposed in this article. The recorded computational state is used by the
planning mechanisms in order to diagnose behavior flaws that are caused by discrepancies between
the computational state of the robot and the state of the environment. The ability to reconstruct
regularly updated fluent values is computationally very costly. We intend to provide programming
839

B EETZ & G ROSSKREUTZ

constructs that let programmers declare the parts of the computational state that are irrelevant for
planning and do not need to be recorded.
Even with this severe limitation we were able to show that with this preliminary implementation the robot can outperform controllers that lack predictive capabilities. The main source of
inefficiency is the bookkeeping needed to reconstruct the entire computational state of the plan
for any predicted time instant, an issue that we have not addressed in this article. Using a more
parsimonious representation of the computational state we expect drastic performance gains.
6.4 Qualitatively Accurate Predictions
Projecting the plan listed in Figure 7 generates a timeline that is about 300 events long. Many of
these events are generated through rescheduling the endogenous events (21 times). Figure 19 shows
the predicted endogenous events (denoted by the numbered circles) and the behavior generated by
the navigation plan in 50 runs using the robot simulator (we assume that the execution is interrupted
in room A-111 because the robot realizes that the deadline can not be achieved). The qualitative
predictions of behavior relevant for plan debugging are perfect. The projector predicts correctly that
the robot will exploit the opportunity to go to location 5 while going from location 1 to 9.

A−111
9

8
7
6

3
4

2
1

A−117
5

A−118

Figure 19: The figure shows the trajectories of multiple executions of the navigation plan and the
events that are predicted by the symbolic plan projector.

6.5 Prediction-based Plan Debugging
Beetz (2002a and 2000) describes experiments showing that prediction-based plan debugging can
improve the performance of robot controllers substantially.
840

P ROBABILISTIC H YBRID ACTION M ODELS

7. Related Work
PHAM s represent external events, probabilistic action models, action models with rich temporal
structure, concurrent interacting actions, and sensing actions in the domain of autonomous mobile
robot control. There are many research efforts that formalize and analyze extended action representations and develop prediction and planning techniques for them. We know, however, only of
approaches that address subsets of the aspects addressed by our representation. Related work comprises research on reasoning about action and change, probabilistic planning, numerical simulation,
and qualitative reasoning.

Reasoning about action and change. Allen and Ferguson (1994) give an excellent and detailed
discussion of important issues in the representation of temporally complex and concurrent actions
and events. One important point that they make is that if actions have interfering effects then, in the
worst case, causal models for all possible combinations of actions must be provided. In this paper,
we have restricted ourselves to one kind of interference between actions: the transposition of movements which is the dominant kind of interference in physical robot behavior. In their article they do
not address the issues of reasoning under uncertainty and efficiency with respect to computational
resources.
A substantial amount of work has been done to extend the situation calculus (McCarthy, 1963)
to deal with time and continuous change (Pinto, 1994; Grosskreutz & Lakemeyer, 2000a), exogenous (natural) actions (Reiter, 1996), complex robot actions (plans) (Levesque et al., 1997; Giacomo et al., 1997) using sensing to determine which action to execute next (Levesque, 1996; Lakemeyer, 1999) as well as probabilistic state descriptions and probabilistic action outcomes (Bacchus,
Halpern, & Levesque, 1999; Grosskreutz & Lakemeyer, 2000b). The main difference to our work is
that their representation is more limited with respect to the kinds of events and interactions between
concurrent actions they allow. In particular, we know of no effort to integrate all of these aspects.
Some of the most advanced approaches in this area are formalizations of various variants of
the high-level robot control language G OLOG, in particular C ON G OLOG (Giacomo et al., 1997).
Boutilier, Reiter, Soutchanski, and Thrun (2000) have applied decision theoretic means for optimally completing a partially specified G OLOG program. A key difference is that in the G OLOG
approach the formalization includes the operation of the plan language whereas in our approach a
procedural semantics realized through the high-level projector is used.
Hanks, Madigan, and Gavrin (1995) present a very interesting and expressive framework for representing probabilistic information, and exogenous and endogenous events for medical prediction
problems. Because of their application domain they do not have to address issues of sophisticated
percept-driven behavior as is done in this article.
Extensions to Classical Action Planning Systems. Planning algorithms, such as SNLP (McAllester
& Rosenblitt, 1991), have been extended in various ways to handle more expressive action models and different kinds of uncertainty (about the initial state and the occurrence and outcome of
events) (Kushmerick, Hanks, & Weld, 1995; Draper, Hanks, & Weld, 1994; Hanks, 1990). These
planning algorithms compute bounds for the probabilities of plan outcomes and are computationally very expensive. In addition, decision-theoretic action planning systems (see Blythe, 1999, for
a comprehensive overview) have been proposed in order to determine plans with the highest, or
at least, sufficiently high expected utility (Haddawy & Rendell, 1990; Haddawy & Hanks, 1992;
841

B EETZ & G ROSSKREUTZ

Williamson & Hanks, 1994). These approaches abstract away from the rich temporal structure of
events by assuming discrete atomic actions and ignore various kinds of uncertainty.
Planning with action models that have rich temporal structure has also been investigated intensively (Allen, Kautz, Pelavin, & Tenenberg, 1990; Dean, Firby, & Miller, 1988). IxTeT (Ghallab
& Laruelle, 1994) is a planning system that has been applied to robot control and reasons about
the temporal structure of plans to identify interferences between plan steps and resource conflicts.
The planner/scheduler of the Remote Agent (Muscettola et al., 1998b) plans space maneuvers and
experiments based on rich temporal causal models (Muscettola et al., 1998a; Pell et al., 1997). A
good overview of the integration of action planning and scheduling technology can be found in an
overview article by Smith, Frank, and Jonsson (2000). So far they have considered uncertainty only
with respect to the durations of actions.
Kabanza, Barbeau, and St-Denis (1997) model actions and behaviors as state transition systems
and synthesize control rules for reactive robots from these descriptions. Their approach can be used
to generate plans that satisfy complex time, safety, and liveness constraints. These approaches too
are limited with respect to the temporal structure of the (primitive) actions being modeled and the
kinds of interferences between concurrent actions that can be considered.
MDP-based planning approaches. In recent years MDP (Markov decision process) planning has
become a very active research field (Boutilier, Dean, & Hanks, 1998; Kaelbling, Cassandra, &
Kurien, 1996). In the MDP approach robot behavior is modeled as a finite state automaton in which
discrete actions cause stochastic state transitions. The robot is rewarded for reaching its goals
quickly and reliably. A solution for such problems is a policy, a mapping from discretized robot
states into, often fine-grained, actions.
MDP s form an attractive framework for action planning because they use a uniform mechanism
for action selection and a parsimonious problem encoding. The action policies computed by MDPs
aim at robustness and optimizing the average performance. A number of researchers have successfully considered navigation as an instance of Markov decision problems (MDPs) (Burgard et al.,
2000; Kaelbling et al., 1996).

One of the main problems in the application of MDP planning techniques is to keep the problem
encoding small enough so that the MDPs are still solvable. A number of techniques for complexity
reduction can be found in the article written by Boutilier et al. (1998). Yet, it is still very difficult
to solve big planning problems in the MDP framework unless the state and action spaces are well
structured.
Besides reducing the complexity of specifying models for, and solving MDP problems, extending the expressiveness of MDP formalisms is a very active research area. Semi Markov decision
problems (Bradtke & Duff, 1995; Sutton, Precup, & Singh, 1999) add a notion of continuous time
to the discrete model of change used in MDPs: transitions from one state to another one no longer
occur immediately, but according to a probability distribution. Others investigate mechanisms for
hierarchically structuring MDPs (Parr & Russell, 1998), decomposing MDPs into loosely coupled
sub-problems (Parr, 1998), and making them programmable (Andre & Russell, 2001). Rohanimanesh and Mahadevan (2001) propose an approach for extending MDP-based planning to concurrent temporally extended actions. All these efforts are steps towards the kind of functionality
provided in the PHAM framework. Another relationship between the research reported here and the
MDP research is that the navigation routines that are modeled with PHAM s are implemented on top
842

P ROBABILISTIC H YBRID ACTION M ODELS

of MDP navigation planning. Belker, Beetz, and Cremers (2002) use the
action models for the improved execution of navigation plans.

MDP

framework to learn

The application of MDP based planning to reasoning about concurrent reactive plans is complicated by the fact that, in general, any activation and termination of a concurrent sub-plan might
require a respective modification of the state and action space of the MDP.
Weaver (Blythe, 1995, 1996) is another probabilistic plan debugger capable of reasoning about
exogenous events. Weaver uses Markov decision processes as its underlying model of planning.
Weaver provides much of the expressiveness of PHAMs. Unlike Weaver, PHAMs are designed for
reasoning about the physical behavior of autonomous mobile robots. Therefore, PHAMs add to
Weaver’s expressiveness in that they extensively support reasoning about concurrent reactive plans.
For example, PHAMs can predict when the continuous effects of actions will trigger a concurrent
monitoring process. PHAMs have built-in capabilities to infer the combined effects of two continuous motions of the robot.
Qualitative reasoning about physical processes. Work in qualitative reasoning has researched
issues in the quantization of continuous processes and focussed among other things on quantizations
that are relevant to the kind of reasoning performed. Hendrix (1973) points out the limitations
of discrete event representations and introduces a very limited notion of continuous process as a
representation of change. He does not consider the influence of multiple processes on state variables.
Hayes (1985) represents events as histories, spatially bounded, but temporally extended, pieces in
time space, and proposes that histories which do not intersect do not interact. In Forbus’ Qualitative
Process Theory (Forbus, 1984) a technique called limit analysis is applied to predict qualitative state
transitions caused by continuous events. Also, work on simulation often addresses the adequacy of
causal models for a given range of prediction queries, an issue that is neglected in most models
used for AI planning. Planners that predict qualitative state transitions caused by continuous events
include EXCALIBUR (Drabble, 1993).
Planning as model checking. Planning as model checking (Bertoli, Cimatti, & Roveri, 2001;
Cimatti & Roveri, 2000) represents domains as finite-state systems. Planning problems are solved
by searching through the state space, checking for the existence of a plan that satisfies the goals.
Goals are formalized as logical requirements about the desired behavior for plans. Unlike planning
as model checking we consider continuous control processes, plan interpretation as well as the
physical effects of actions, and concurrency. This extended representational power comes at the
cost of probably finding behavior flaws rather than proving their absence.
Design and verification of embedded systems based on hybrid automata. The formalization
of embedded software systems (Alur et al., 1997, 1996) using hybrid automata aims at proving
critical aspects of the software rather than the physical effects of running this software. In our
approach we have used the ideas of this research field as the basis of our conceptualization but
added additional mechanisms to model the effects of actions and sensing mechanisms. Again, the
additional complexity of our model is compensated by solving more restrictive inference problems:
the detection of probable behavior flaws with high probability rather than safety of the system and
the reachability of goals.
843

B EETZ & G ROSSKREUTZ

8. Conclusion
The successful application of AI planning to autonomous mobile robot control requires the planning systems to have more realistic models of the operation of modern robot control systems and
the physical effects caused by their execution. In this article we have presented probabilistic hybrid
action models (PHAMs), which are capable of representing the temporal structure of continuous
feedback control processes, their non-deterministic effects, several modes of their interferences,
and exogenous events. We have shown that PHAMs allow for predictions that are, with high probability, qualitatively correct. We have also shown that powerful prediction-based inferences such as
deciding whether a plan is likely to cause a flaw with a probability exceeding a given threshold can
be drawn fast and with bounded risk.
We believe that equipping autonomous robot controllers with concurrent reactive plans and
prediction-based online plan revision based on PHAMs is a promising way to improve the performance of autonomous service robots through AI planning both significantly and substantially.
The rules that we have used for projecting navigation behavior were hand-coded and plan and
possibly even environment specific. On our research agenda is the development of transformational
mechanisms for learning high performance and task specific plans. After having learned the plans
the robot should then learn the projection rules by applying data mining techniques to the plan
execution traces. To enable this approach we must invent novel representational mechanisms for
the plans that allow for the automatic extraction of the rules. Initial steps into this direction can be
found in the work of Belker et al. (2002), Beetz and Belker (2000), Beetz (2002b).

References
Alami, R., Chatila, R., Fleury, S., Ingrand, M. H. F., Khatib, M., Morisset, B., Moutarlier, P., &
Simeon, T. (2000). Around the lab in 40 days .... In Proceedings of the IEEE International
Conference on Robotics and Automation (ICRA 2000), pp. 88–94.
Allen, J., & Ferguson, G. (1994). Actions and events in interval temporal logic. Journal of Logic
and Computation, 4(5), 531–579.
Allen, J., Kautz, H., Pelavin, R., & Tenenberg, J. (Eds.). (1990). Reasoning about Plans. Morgan
Kaufmann.
Alur, R., Henzinger, T., & Ho, P. (1996). Automatic symbolic verification of embedded systems.
IEEE Transactions on Software Engineering, 22(3), 181–201.
Alur, R., Henzinger, T., & Wong-Toi, H. (1997). Symbolic analysis of hybrid systems. In Proceedings of the Thirtyssixth IEEE Conference on Decision and Control (CDC), pp. 702–707.
IEEE Press.
Andre, D., & Russell, S. (2001). Programmable reinforcement learning agents. In Advances in Neural Information Processing Systems 13, Papers from Neural Information Processing Systems
(NIPS) 2000, pp. 1019–1025. MIT Press.
Arkin, R. (1998). Behavior based Robotics. MIT Press.
Bacchus, F., Halpern, J., & Levesque, H. (1999). Reasoning about noisy sensors and effectors in the
situation calculus. Artificial Intelligence 111(1-2).
844

P ROBABILISTIC H YBRID ACTION M ODELS

Beetz, M. (1999). Structured Reactive Controllers — a computational model of everyday activity. In Etzioni, O., Müller, J., & Bradshaw, J. (Eds.), Proceedings of the Third International
Conference on Autonomous Agents, pp. 228–235.
Beetz, M. (2000). Concurrent Reactive Plans: Anticipating and forestalling execution failures, Vol.
LNAI 1772 of Lecture Notes in Artificial Intelligence. Springer Publishers.
Beetz, M. (2001). Structured Reactive Controllers. Journal of Autonomous Agents and Multi-Agent
Systems. Special Issue: Best Papers of the International Conference on Autonomous Agents
’99, 4, 25–55.
Beetz, M. (2002a). Plan-based Control of Robotic Agents, Vol. LNAI 2554 of Lecture Notes in
Artificial Intelligence. Springer Publishers.
Beetz, M. (2002b). Plan representation for robotic agents. In Proceedings of the Sixth International
Conference on AI Planning and Scheduling, pp. 223–232.
Beetz, M., Arbuckle, T., Bennewitz, M., Burgard, W., Cremers, A., Fox, D., Grosskreutz, H.,
Hähnel, D., & Schulz, D. (2001). Integrated plan-based control of autonomous service robots
in human environments. IEEE Intelligent Systems, 16(5), 56–65.
Beetz, M., Arbuckle, T., Cremers, A., & Mann, M. (1998). Transparent, flexible, and resourceadaptive image processing for autonomous service robots. In Prade, H. (Ed.), Proceedings of
the Thirteenth European Conference on Artificial Intelligence (ECAI-98), pp. 632–636.
Beetz, M., & Belker, T. (2000). Environment and task adaptation for robotic agents. In Horn, W.
(Ed.), Proceedings of the Fourteenth European Conference on Artificial Intelligence (ECAI2000), pp. 648–652.
Beetz, M., Bennewitz, M., & Grosskreutz, H. (1999). Probabilistic, prediction-based schedule debugging for autonomous robot office couriers. In Proceedings of the Twentythird German
Conference on Artificial Intelligence (KI 99), Bonn, Germany, pp. 243–254. Springer Publishers.
Beetz, M., Burgard, W., Fox, D., & Cremers, A. (1998). Integrating active localization into highlevel control systems. Robotics and Autonomous Systems, 23, 205–220.
Beetz, M., & Grosskreutz, H. (1998). Causal models of mobile service robot behavior. In Simmons,
R., Veloso, M., & Smith, S. (Eds.), Proceedings of the Fourth International Conference on AI
Planning Systems, pp. 163–170, Morgan Kaufmann.
Beetz, M., & Grosskreutz, H. (2000). Probabilistic hybrid action models for predicting concurrent
percept-driven robot behavior. In Proceedings of the Sixth International Conference on AI
Planning Systems, Toulouse, France. AAAI Press.
Beetz, M., & McDermott, D. (1992). Declarative goals in reactive plans. In Hendler, J. (Ed.),
Proceedings of the First International Conference on AI Planning Systems, pp. 3–12, Morgan
Kaufmann.
Beetz, M., & McDermott, D. (1996). Local planning of ongoing activities. In Drabble, B. (Ed.), Proceedings of the Third International Conference on AI Planning Systems, pp. 19–26, Morgan
Kaufmann.
Beetz, M., & Peters, H. (1998). Structured reactive communication plans — integrating conversational actions into high-level robot control systems. In Proceedings of the Twentysecond
845

B EETZ & G ROSSKREUTZ

German Conference on Artificial Intelligence (KI 98), Bremen, Germany. Springer Publishers.
Belker, T., Beetz, M., & Cremers, A. (2002). Learning action models for the improved execution of
navigation plans. Robotics and Autonomous Systems, 38(3-4), 137–148.
Bertoli, P., Cimatti, A., & Roveri, M. (2001). Planning in nondeterministic domains under partial
observability via symbolic model checking. In Proceedings of the Seventeenth International
Joint Conference on Artificial Intelligence (IJCAI-01). AAAI Press.
Blythe, J. (1995). AI planning in dynamic, uncertain domains. In Extending Theories of Action:
Formal Theory & Practical Applications: Papers from the 1995 AAAI Spring Symposium, pp.
28–32. AAAI Press, Menlo Park, CA.
Blythe, J. (1996). Decompositions of Markov chains for reasoning about external change in planners. In Drabble, B. (Ed.), Proceedings of the 3rd International Conference on Artificial
Intelligence Planning Systems (AIPS-96), pp. 27–34. AAAI Press.
Blythe, J. (1999). Decision-theoretic planning. AI Magazine, 20(2), 37–54.
Bonasso, P., Firby, J., Gat, E., Kortenkamp, D., Miller, D., & Slack, M. (1997). Experiences with an
architecture for intelligent, reactive agents. Journal of Experimental and Theoretical Artificial
Intelligence, 9(1).
Boutilier, C., Dean, T., & Hanks, S. (1998). Decision theoretic planning: Structural assumptions
and computational leverage. Journal of Artificial Intelligence Research, 11, 1–94.
Boutilier, C., Reiter, R., Soutchanski, M., & Thrun, S. (2000). Decision-theoretic, high-level robot
programming in the situation calculus. In Proceedings of the Seventeenth AAAI National
Conference on Artificial Intelligence, pp. 355–362, Austin, TX.
Bradtke, S., & Duff, M. (1995). Reinforcement learning methods for continuous-time Markov
decision problems. In Tesauro, G., Touretzky, D., & Leen, T. (Eds.), Advances in Neural
Information Processing Systems, Vol. 7, pp. 393–400. MIT Press.
Brooks, R. (1986). A robust layered control system for a mobile robot. IEEE Journal of Robotics
and Automation, 2(1), 14–23.
Burgard, W., Cremers, A., Fox, D., Hähnel, D., Lakemeyer, G., Schulz, D., Steiner, W., & Thrun,
S. (2000). Experiences with an interactive museum tour-guide robot. Artificial Intelligence,
114(1-2), 3–55.
Cimatti, A., & Roveri, M. (2000). Conformant planning via symbolic model checking. Journal of
Artificial Intelligence Research (JAIR), 13, 305–338.
Dean, T., Firby, J., & Miller, D. (1988). Hierarchical planning involving deadlines, travel time and
resources. Computational Intelligence, 4(4), 381–398.
Doherty, P., Granlund, G., Krzysztof, G., Sandewall, E., Nordberg, K., Skarman, E., & Wiklund,
J. (2000). The WITAS unmanned aerial vehicle project. In Proceedings of the Fourteenth
European Conference on Artificial Intelligence (ECAI-00), pp. 747–755, Berlin, Germany.
Drabble, B. (1993). Excalibur: a program for planning and reasoning with processes. Artificial
Intelligence, 62, 1–40.
846

P ROBABILISTIC H YBRID ACTION M ODELS

Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning with information gathering and
contingent execution. In Proceedings of the Second International Conference on AI Planning
Systems, p. 31.
Firby, J. (1987). An investigation into reactive planning in complex domains. In Proceedings of the
Sixth National Conference on Artificial Intelligence, pp. 202–206, Seattle, WA.
Forbus, K. (1984). Qualitative process theory. Artificial Intelligence, 24, 85–168.
Fox, D., Burgard, W., Dellaert, F., & Thrun, S. (1999). Monte Carlo localization: Efficient position estimation for mobile robots. In Proceedings of the Sixteenth National Conference on
Artificial Intelligence, Orlando, FL.
Ghallab, M., & Laruelle, H. (1994). Representation and control in IxTeT, a temporal planner. In
Hammond, K. (Ed.), Proceedings of the Second International Conference on AI Planning
Systems, pp. 61–67, Morgan Kaufmann.
Giacomo, G. D., Lesperance, Y., & Levesque, H. (1997). Reasoning about concurrent execution,
prioritized interrupts, and exogene ous actions in the situation calculus. In Proceedings of the
Fifteenth International Joint Conference on Artificial Intelligence, Nagoya, Japan.
Grosskreutz, H., & Lakemeyer, G. (2000a). cc-Golog: Towards more realistic logic-based robot
controllers. In Proceedings of the Seventeenth National Conference on Artificial Intelligence.
Grosskreutz, H., & Lakemeyer, G. (2000b). Turning high-level plans into robot programs in uncertain domains. In Proceedings of the Fourteenth European Conference on Artificial Intelligence (ECAI-00), pp. 548–552.
Haddawy, P., & Hanks, S. (1992). Representations for decision-theoretic planning: Utility functions
for deadline goals. In Nebel, B., Rich, C., & Swartout, W. (Eds.), Proceedings of the Third
International Conference on Principles of Knowledge Representation and Reasoning, pp. 71–
82, Cambridge, MA. Morgan Kaufmann.
Haddawy, P., & Rendell, L. (1990). Planning and decision theory. The Knowledge Engineering
Review, 5, 15–33.
Hanks, S. (1990). Practical temporal projection. In Proceedings of the Eighth National Conference
on Artificial Intelligence (AAAI-90), pp. 158–163.
Hanks, S., Madigan, D., & Gavrin, J. (1995). Probabilistic temporal reasoning with endogenous
change. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence,
pp. 245–254. Morgan Kaufmann.
Hayes, P. (1985). The second naive physics manifesto. In Hobbs, J. R., & Moore, R. C. (Eds.),
Formal Theories of the Commonsense World, pp. 1–36. Ablex, Norwood, NJ.
Hendrix, G. (1973). Modeling simultaneous actions and continuous processes. Artificial Intelligence, 4, 145–180.
Horswill, I. (1996). Integrated systems and naturalistic tasks. In: Strategic Directions in Computing
Research, AI Working Group.
Kabanza, F., Barbeau, M., & St-Denis, R. (1997). Planning control rules for reactive agents. Artificial Intelligence, 95, 67–113.
847

B EETZ & G ROSSKREUTZ

Kaelbling, L., Cassandra, A., & Kurien, J. (1996). Acting under uncertainty: Discrete Bayesian
models for mobile-robot navigation. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems.
Konolige, K., Myers, K., Ruspini, E., & Saffiotti, A. (1997). The Saphira architecture: A design for
autonomy. Journal of Experimental and Theoretical Artificial Intelligence, 9(2).
Kushmerick, N., Hanks, S., & Weld, D. (1995). An algorithm for probabilistic planning. Artificial
Intelligence, 76, 239–286.
Lakemeyer, G. (1999). On sensing and off-line interpreting in golog. In Levesque, H., & Pirri, F.
(Eds.), Logical Foundations for Cognitive Agents. Springer Publishers.
Levesque, H., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). Golog: A logic programming
language for dynamic domains. Journal of Logic Programming, 31, 59–84.
Levesque, H. J. (1996). What is planning in the presence of sensing. In Proceedings of the Thirteenth
National Conference on Artificial Intelligence, pp. 1139–1146, Portland, OR.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. In Proceedings of the
Ninth National Conference on Artificial Intelligence, pp. 634–639, Anaheim, CA.
McCarthy, J. (1963). Situations, actions and causal laws. Tech. rep., Stanford University. Reprinted
1968 in Semantic Information Processing (M. Minsky ed.).
McDermott, D. (1991). A Reactive Plan Language. Research Report YALEU/DCS/RR-864, Yale
University.
McDermott, D. (1992a). Robot planning. AI Magazine, 13(2), 55–79.
McDermott, D. (1992b). Transformational planning of reactive behavior.
YALEU/DCS/RR-941, Yale University.

Research Report

McDermott, D. (1994). An algorithm for probabilistic, totally-ordered temporal projection. Research Report YALEU/DCS/RR-941, Yale University.
Muscettola, N., Morris, P., Pell, B., & Smith, B. (1998a). Issues in temporal reasoning for autonomous control systems. In Sycara, K., & Wooldridge, M. (Eds.), Proceedings of the
Second International Conference on Autonomous Agents (AGENTS-98), pp. 362–368. ACM
Press.
Muscettola, N., Nayak, P., Pell, B., & Williams, B. (1998b). Remote Agent: to boldly go where no
AI system has gone before. Artificial Intelligence, 103(1–2), 5–47.
Myers, K. (1996). A procedural knowledge approach to task-level control. In Drabble, B. (Ed.),
Proceedings of the Third International Conference on AI Planning Systems, pp. 158–165,
Edinburgh, GB. AAAI Press.
Parr, R. (1998). Flexible decomposition algorithms for weakly coupled Markov decision problems.
In Cooper, G. F., & Moral, S. (Eds.), Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98), pp. 422–430, San Francisco. Morgan Kaufmann.
Parr, R., & Russell, S. (1998). Reinforcement learning with hierarchies of machines. In Jordan,
M. I., Kearns, M. J., & Solla, S. A. (Eds.), Advances in Neural Information Processing Systems, Vol. 10. MIT Press.
848

P ROBABILISTIC H YBRID ACTION M ODELS

Pell, B., Gat, E., Keesing, R., Muscettola, N., & Smith, B. (1997). Robust periodic planning and execution for autonomous spacecraft. In Proceedings of the 15th International Joint Conference
on Artificial Intelligence (IJCAI-97), pp. 1234–1239, San Francisco. Morgan Kaufmann.
Pinto, J. (1994). Temporal Reasoning in the Situation Calculus. Ph.D. thesis, Department of Computer Science, University of Toronto, Toronto, Ontario, Canada.
Reiter, R. (1996). Natural actions, concurrency and continuous time in the situation calculus. In
Proceedings of the Fifth International Conference on Principles of Knowledge Representation
and Reasoning (KR-96), pp. 2–13.
Rohanimanesh, K., & Mahadevan, S. (2001). Decision-theoretic planning with concurrent temporally extended actions. In Proceedings of the Seventeenth Conference on Uncertainty in
Artificial Intelligence (UAI), pp. 472–479.
Schmitt, T., Hanek, R., Beetz, M., Buck, S., & Radig, B. (2002). Cooperative probabilistic state
estimation for vision-based autonomous mobile robots. IEEE Transactions on Robotics and
Automation, 18(5), 670–684.
Simmons, R., Goodwin, R., Haigh, K., Koenig, S., & O’Sullivan, J. (1997). A modular architecture for office delivery robots. In Proceedings of the First International Conference on
Autonomous Agents, pp. 245–252.
Smith, D., Frank, J., & Jonsson, A. (2000). Bridging the gap between planning and scheduling. The
Knowledge Engineering Review, 15(1), 47–83.
Sutton, R., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for
temporal abstraction in reinforcement learning. Artificial Intelligence 112(1-2), 181–211.
Thrun, S. (2000). Monte Carlo POMDPs. In Advances in Neural Information Processing Systems
12, pp. 1064–1070. MIT Press.
Thrun, S., Beetz, M., Bennewitz, M., Cremers, A., Dellaert, F., Fox, D., Hähnel, D., Rosenberg, C.,
Roy, N., Schulte, J., & Schulz, D. (2000). Probabilistic algorithms and the interactive museum
tour-guide robot Minerva. International Journal of Robotics Research, 19(11), 972–999.
Thrun, S., Bücken, A., Burgard, W., Fox, D., Fröhlinghaus, T., Hennig, D., Hofmann, T., Krell, M.,
& Schmidt, T. (1998). Map learning and high-speed navigation in RHINO. In Kortenkamp,
D., Bonasso, R., & Murphy, R. (Eds.), AI-based Mobile Robots: Case studies of successful
robot systems, pp. 21 – 52. MIT Press.
Williamson, M., & Hanks, S. (1994). Utility-directed planning. In Proceedings of the Twelfth
National Conference on Artificial Intelligence, p. 1498, Seattle, WA.

849

Journal of Artificial Intelligence Research 24 (2005) 685-758

Submitted 03/05; published 11/05

Where “Ignoring Delete Lists” Works:
Local Search Topology in Planning Benchmarks
Jörg Hoffmann

hoffmann@mpi-sb.mpg.de

Max Planck Institute for Computer Science,
Stuhlsatzenhausweg 85,
66123 Saarbrücken
Germany

Abstract
Between 1998 and 2004, the planning community has seen vast progress in terms of
the sizes of benchmark examples that domain-independent planners can tackle successfully.
The key technique behind this progress is the use of heuristic functions based on relaxing
the planning task at hand, where the relaxation is to assume that all delete lists are empty.
The unprecedented success of such methods, in many commonly used benchmark examples,
calls for an understanding of what classes of domains these methods are well suited for.
In the investigation at hand, we derive a formal background to such an understanding. We perform a case study covering a range of 30 commonly used STRIPS and ADL
benchmark domains, including all examples used in the first four international planning
competitions. We prove connections between domain structure and local search topology
– heuristic cost surface properties – under an idealized version of the heuristic functions
used in modern planners. The idealized heuristic function is called h+ , and differs from
the practically used functions in that it returns the length of an optimal relaxed plan,
which is NP-hard to compute. We identify several key characteristics of the topology under h+ , concerning the existence/non-existence of unrecognized dead ends, as well as the
existence/non-existence of constant upper bounds on the difficulty of escaping local minima
and benches. These distinctions divide the (set of all) planning domains into a taxonomy
of classes of varying h+ topology. As it turns out, many of the 30 investigated domains lie
in classes with a relatively easy topology. Most particularly, 12 of the domains lie in classes
where FF’s search algorithm, provided with h+ , is a polynomial solving mechanism.
We also present results relating h+ to its approximation as implemented in FF. The
behavior regarding dead ends is provably the same. We summarize the results of an empirical investigation showing that, in many domains, the topological qualities of h+ are
largely inherited by the approximation. The overall investigation gives a rare example of a
successful analysis of the connections between typical-case problem structure, and search
performance. The theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques. We outline some
preliminary steps we made into that direction.

1. Introduction
Between 1998 and 2004, one of the strongest trends in the planning community has been that
towards heuristic planners, more specifically towards the use of heuristic distance (in most
cases, goal distance) estimation functions. The best runtime results, progressing far beyond
the sizes of benchmark examples that previous domain-independent planners could tackle
successfully, have been achieved based upon a technique phrased “ignoring delete lists”.
c
2005
AI Access Foundation. All rights reserved.

Hoffmann

There, the heuristic function is derived by considering a relaxation of the planning task at
hand, where the relaxation is to assume that all delete lists (i.e. the negative effects of the
available planning operators) are empty. During search, may it be forward or backward,
state space or plan space, the heuristic value of a search state in this framework is (an
estimate of) the difficulty of extending the state to a solution using the relaxed operators,
where “difficulty” is defined as the number of (relaxed) actions needed.
The number of real actions needed to extend a search state to a solution is at least
as high as the number of relaxed actions needed. So optimal (shortest) relaxed solutions
can, in principle, be used to derive admissible heuristic functions. However, as was first
proved by Bylander (1994), deciding bounded plan existence, i.e., the existence of a plan
with at most some given number of actions, is NP-hard even when there are no delete lists.1
Thus there is not much hope to find optimal relaxed plans (i.e., optimal relaxed solutionextensions of search states) fast. Instead, one can approximate the length of an optimal
relaxed plan to a search state. Techniques of this kind were first, independently, proposed by
McDermott (1996) and by Bonet, Loerincs, and Geffner (1997), who developed the planners
Unpop (McDermott, 1996, 1999), and HSP1 (Bonet et al., 1997). Both these planners
perform forward state space search guided by an approximation of relaxed goal distance.
Unpop approximates that distance by backchaining from the goals, HSP1 approximates
that distance by a forward value iteration technique.
In the 1st international planning competition, IPC-1 (McDermott, 2000), hosted at
AIPS-1998, HSP1 compared well with the four other competitors. This inspired the development of HSP-r and HSP2 (Bonet & Geffner, 1999, 2001b, 2001a), GRT (Refanidis &
Vlahavas, 1999, 2001), AltAlt (Nguyen & Kambhampati, 2000; Srivastava, Nguyen, Kambhampati, Do, Nambiar, Nie, Nigenda, & Zimmermann, 2001), as well as FF (Hoffmann,
2000; Hoffmann & Nebel, 2001a; Hoffmann, 2001a). HSP-r avoids heuristic re-computations
by changing the search direction. HSP2 implements the various HSP versions in a configurable hybrid system. GRT avoids heuristic re-computations by changing the heuristic
direction (the direction in which relaxed plans are computed). AltAlt uses a planning
graph to extract heuristic values. FF uses a modified technique for approximating optimal
relaxed plan length (namely, by computing a not necessarily optimal relaxed plan, which
can be done in polynomial time), as well as new pruning and search techniques. FF inspired the integration of heuristic search engines into Mips (Edelkamp & Helmert, 2001)
and STAN4 (Fox & Long, 2001), using elaborated variations of FF’s relaxed plan length
estimation technique.
In the 2nd international planning competition, IPC-2 (Bacchus, 2001), hosted at AIPS2000, the heuristic planners dramatically outperformed the other approaches runtime-wise,
scaling up to benchmark examples far beyond reach of previous, e.g., Graphplan-based
(Blum & Furst, 1995, 1997), systems. This caused the trend towards heuristic planners
to still increase. Various researchers extended relaxed plan distance estimation techniques
to temporal and numeric settings (Do & Kambhampati, 2001; Hoffmann, 2002, 2003a;
Edelkamp, 2003b). Others adapted them for use in partial order plan-space search (Nguyen
1. For parallel planning, where the bound is on the number of parallel time steps needed, deciding bounded
plan existence is easy without delete lists. However, heuristic functions based on this observation have
generally not been found to provide useful search guidance in practice (see, for example, Haslum &
Geffner, 2000; Bonet & Geffner, 2001b).

686

Where “Ignoring Delete Lists” Works

& Kambhampati, 2001; Younes & Simmons, 2002), developed variations of them to provide
new means of heuristic guidance (Onaindia, Sapena, Sebastia, & Marzal, 2001; Sebastia,
Onaindia, & Marzal, 2001), or modified them to take exclusion relations in a planning graph
into account (Gerevini & Serina, 2002; Gerevini, Serina, Saetti, & Spinoni, 2003).
In the 3rd international planning competition, IPC-3 (Long & Fox, 2003), hosted at
AIPS-2002, out of 11 domain-independent competing systems, 7 were using relaxed plan
distance estimations in one or the other form. The 1st prize winner LPG (Gerevini & Serina,
2002; Gerevini, Saetti, & Serina, 2003) uses, amongst other heuristics, a relaxed planning
technique to estimate the difficulty of sub-goal achievement in a planning graph. In the
4th international planning competition, IPC-4 (Hoffmann & Edelkamp, 2005; Edelkamp,
Hoffmann, Englert, Liporace, Thiebaux, & Trüg, 2005), hosted at ICAPS-2004, out of 13
competing sub-optimal systems, 12 were using relaxed plan based heuristics. There were
two 1st prize winners in that category: Fast-Downward (Helmert, 2004; Helmert & Richter,
2004) and SGPlan (Chen & Wah, 2003; Chen, Hsu, & Wah, 2004). The latter uses the
numeric version of FF as a sub-process. One version of the former combines FF’s heuristic
estimates with a new heuristic function based on causal graph analysis (Helmert, 2004).
In the investigation at hand, we derive a formal background as to what classes of domains methods of the kind described above are well suited for. We make two simplifying
assumptions. First, we consider forward state space search only, as used by, for example,
Unpop, HSP, Mips, FF, and Fast-Downward. In a forward state space search, one starts
at the initial state and explores the space of reachable states until a goal state is found.
The state transitions follow a sequential planning framework, where only a single action is
applied at a time.2 Assuming forward search makes the investigation easier since such a
search is a very natural and simple framework. Our second simplifying assumption is to
idealize matters in that we consider the heuristic value given by the optimal relaxed plan
length (the length of a shortest sequential relaxed plan) to each search state s; we denote
that value with h+ (s). Under this assumption, as we will see there are many provable
connections between domain structure and heuristic quality. Of course, the simplifying assumptions restrict the relevance of the results for practical planners. More on this is said
below in this section, and in Section 7. Another, more benign, restriction we make is to
consider solvable tasks only. This is a very common restriction in AI Planning, particularly
in the competitions, where the main focus is on how good planners are at finding plans.
More specifically, the main focus of the investigation at hand is to characterize the kinds of
domains in which (relaxed-plan based) heuristic planners can find plans fast.
It is common knowledge that the behavior of heuristic search methods (may they be
global or local, i.e., with or without backtracking mechanisms) depends crucially on the
quality of the underlying heuristic function. This has, for example, been studied in the
SAT community, for example by Frank, Cheeseman, and Stutz (1997). In their work, these
authors empirically investigate properties of the local search topology, i.e., of topological
properties like the sizes of local minima etc., in SAT instances under a standard heuristic
function. We adapt Frank et al.’s definitions to AI planning. In difference to Frank et
al., we take a more analytical approach where we prove properties that are valid across
2. In principle, a parallel forward search is possible, too. To the best of the author’s knowledge, there
is no published work about an implementation of this, at the time of writing. The main difficulty is,
presumably, the high branching factor.

687

Hoffmann

certain ranges, namely domains, of example problem instances. We investigate a range of
30 commonly used STRIPS and ADL benchmark domains including all examples used in
the first four international planning competitions. We identify several key characteristics of
the topology of the respective search spaces under h+ . The characteristics are the following.
1. In 24 of the benchmark domains, there are no unrecognized dead ends, i.e., no states
from which the goal is unreachable but for which there is a relaxed plan.
2. In 17 of the above 24 benchmark domains, the “maximal exit distance from local
minima” is constantly bounded, i.e., one can always escape local minima (regions
where all neighbors have a higher heuristic value) within a number of steps that is
constant across all instances of the domain, regardless of their size (in fact, in 13 of
these domains there are no local minima at all).
3. In 12 of the above 17 benchmark domains, the “maximal exit distance from benches”
is constantly bounded, i.e., one can always escape benches (regions where all states
have the same heuristic value) within a number of steps that is constant across all
instances of the domain, regardless of their size (in 6 domains the bound is 1, in one
domain it is even 0).
Beside the “positive” results proving characteristic qualities of the h+ function, the
investigation also provides (parameterized) counter-examples in the negative cases. The
results divide the investigated domains (more generally, all possible planning domains) into
a meaningful taxonomy of classes which differ in terms of their topological behavior with
respect to h+ . Many of the 30 investigated domains lie in relatively easy classes, i.e., classes
where h+ is a – provably – high-quality heuristic. Most particularly, the 12 domains with
all the above properties lie in classes where FF’s search algorithm is a polynomial solving
mechanism, under the idealizing assumption that FF’s approximative heuristic function
identifies the real h+ distances. FF’s search algorithm, called enforced hill-climbing, tries
to escape local minima or benches by means of a breadth-first search. Breadth-first search
is exponential only in the search depth. So if local minima and benches can always be
escaped from within a constant number of steps – as is the case in these 12 domains – then
the effort spent in the search is polynomially bounded. In this way, our results provide
non-trivial insights into typical-case problem structure (in benchmarks), and its possible
effects on search performance. Examples of successful theoretical investigations of this kind
are extremely rare in the AI literature.
To give the reader a feeling for what we are looking at, Figure 1 shows two visualized
state spaces. The shown tasks are instances of two domains from the easiest classes of the
taxonomy, Gripper and Logistics. The graph nodes are the states, the edges are the state
transitions (action applications), the height is given by the h+ value.3 In both pictures, the
initial state is somewhere in the left top part. The goal states are, of course, the states with
minimal – zero – h+ value. The Gripper picture speaks for itself. The Logistics topology
is less extreme, but still the state space forms one big valley at the bottom of which there
are the goal states.
3. The h+ values here, and in an empirical investigation (Hoffmann, 2001b, 2003b) preceding our theoretical
analysis, were computed by an iterative deepening forward search in the space of relaxed action sequences.

688

Where “Ignoring Delete Lists” Works

(a)

(b)

Figure 1: Visualized state space under h+ of (a) a Gripper and (b) a Logistics instance.

Of course, FF’s approximation of h+ , which we refer to as hF F , does not always identify
the real h+ values, and so it is a priori not evident what relevance the theoretical results
about h+ have for FF’s efficiency in practice. Additionally, most forward searching planners
do not use enforced hill-climbing, for which the topological results have the most striking
impact. Finally, and most importantly, several competitive other planners do not even
perform a forward search, or use additional/new techniques in the heuristic function that
are explicitly aimed at identifying better information than relaxed plans. Prominent systems
of the former kind are HSP-r and LPG, prominent systems of the latter kind are LPG and
Fast-Downward.
As for the relevance of the results for the performance of FF, the practical performance of
FF coincides quite well with them. More concretely, the behavior of h+ with respect to dead
ends is provably the same as that of hF F . Moreover, a large-scale empirical investigation
(contained in Hoffmann, 2003b) has shown that, in many domains, the topology of h+ is
largely preserved by hF F . We include a section containing a brief summary of these results.
The relevance of the topological results for forward search algorithms other than enforced
hill-climbing, and the performance of planners using other search paradigms or enhanced
heuristics, is discussed in Section 7.
We remark that our topological investigation was not specifically intended to identify
properties relevant to enforced hill-climbing. The theoretical investigation was preceded
by an empirical investigation (Hoffmann, 2001b, 2003b) where we measured all kinds of
topological parameters, including, for example, size and diameter of local minima, benches,
and other structures such as so-called “valley” regions. It turned out that the only topology
parameters that showed interesting behavior across a significant number of domains were
the maximal exit distance parameters considered in the investigation at hand. This, in fact,
came as a surprise to us – we invented enforced hill-climbing in FF before it became clear
689

Hoffmann

that many of the planning benchmarks share topological properties favoring precisely this
particular search algorithm.
Observe that the proved results are of a worst-case nature, i.e., a heuristic search using
+
h can show good behavior in an example suite of a domain even if that domain lies in
a very difficult class of the taxonomy – given the particular example instances in the test
suite do not emphasize on the worst cases possible in the domain. Where relevant, we will
discuss this issue with regards to the example suites used in the competitions.
The employed proof methods give hints as to how the topological phenomena might be
automatically detectable using general domain analysis techniques. In an extra section, we
report on a first (not yet very successful) attempt we made to do that.
The proofs for the individual planning domains are, in most cases, not overly difficult,
but the full details for all domains are extremely space consuming. The details (except for
the 5 IPC-4 domains), i.e., PDDL-like definitions of the domains as well as fully detailed
proofs, can be looked up in a long (138 pages) technical report (Hoffmann, 2003c) that also
forms an online appendix to the article.4 The article itself provides proof sketches, which
are much better suited to get an overall understanding of the investigation and its results.
Since even the proof sketches are sometimes hard to read, they are moved into an appendix;
another appendix provides brief descriptions of all domains. The main body of text only
gives the results and an outline of the main proof arguments used to obtain them.
The paper is organized as follows. Section 2 provides the necessary background, i.e.
a straightforward formal framework for STRIPS and ADL domains, an overview of the
investigated domains, and the definitions of local search topology. Section 3 presents some
core lemmas underlying many of the proofs in the single domains, and illustrates the lemmas’
application in a small example. Section 4 gives all the results with a brief proof outline,
and shows the resulting planning domain taxonomy. Section 5 presents the results relating
h+ to hF F , and Section 6 reports on our first attempt to design domain analysis techniques
for automatically detecting the h+ topological phenomena. Section 7 concludes the article
with a brief discussion of our contributions and of future work. Appendix A contains the
proof sketches for the individual domains, Appendix B contains the domain descriptions.

2. Background
Background is necessary on the planning framework, the investigated domains, and local
search topology.
2.1 Planning Framework
To enable theoretical proofs to properties of planning domains rather than single tasks, we
have defined a formal framework for STRIPS and ADL domains, formalizing in a straightforward manner the way domains are usually dealt with in the community. We only outline
the rather lengthy definitions, and refer the reader to the TR (Hoffmann, 2003c) for details.
In what follows, by sets we mean finite sets unless explicitly said otherwise.
4. We remark that the TR is not a longer version of the paper at hand. The TR’s overall structure and
presentation angle are very different, and it is only intended as a source of details if needed.

690

Where “Ignoring Delete Lists” Works

A planning domain is defined in terms of a set of predicates, a set of operators, and
a possibly infinite set of instances. All logical constructs in the domain are based on the
set of predicates. A fact is a predicate applied to a tuple of objects. The operators are
(k-ary, where k is the number of operator parameters) functions from the (infinite) set of
all objects into the (infinite) set of all STRIPS or ADL actions. A STRIPS action a is a
triple (pre(a), add(a), del(a)): a’s precondition, which is a conjunction of facts; a’s add list,
a fact set; and a’s delete list, also a fact set. An ADL action a is a pair (pre(a), E(a)) where
the precondition pre(a) is a first order logical formula without free variables, and E(a) is
a set of effects e of the form (con(e), add(e), del(e)) where con(e), the effect condition, is
a formula without free variables, and add(e) (the effect’s add list) as well as del(e) (the
effect’s delete list) are fact sets. If the add list of an action/effect contains a fact p, we also
say that the action/effect achieves p.
An instance of a domain is defined in terms of a set of objects, an initial state, and a goal
condition. The initial state is a set of facts, and the goal condition is a formula without free
variables (in the STRIPS case, a conjunction of facts). The facts that are contained in the
initial state are assumed to be true, and all facts not contained in it are assumed to be false,
i.e., as usual we apply the closed-world assumption. An instance of a domain constitutes,
together with the domain’s operators, a planning task (A, I, G) where the action set A is
the result of applying the operators to the instance’s objects (i.e., to all object tuples of the
appropriate lengths), and the initial state I and goal condition G are those of the instance.
We identify instances with the respective planning tasks.
A state s is a set of facts. A logical formula holds in a state if the state is a model of
the formula according to the standard definition for first order logic (where a logical atom,
a fact, holds iff it is contained in the state). The result Result(s, hai) of applying an action
sequence consisting of a single STRIPS or ADL action a to a state s is defined as follows.
If the action’s precondition does not hold in s, then Result(s, hai) is undefined. Otherwise,
Result(s, hai) is obtained from s by including all of a’s add effects, and (thereafter) removing
all of a’s delete effects – if a is an ADL action, only those add effects add(e) are included
(delete effects del(e) are removed) for which the respective effect condition con(e) holds in
s. The result of applying a sequence ha1 , . . . , an i consisting of more than one action to a
state s is defined as the iterative application of the single actions in the obvious manner:
apply a1 to s, then apply a2 to Result(s, ha1 i), and so on.
A plan, or solution, for a task (A, I, G) is a sequence of actions P ∈ A∗ that, when
successively applied to I, yields a goal state, i.e., a state in which G holds. (We use the
standard notation M ∗ , where M is a set, to denote the set of all sequences of elements of
M .) For many proofs we need the notion of optimality. A plan P for a task (A, I, G) is
optimal if there is no plan for (A, I, G) that contains fewer actions than P .
Note that, as announced in the introduction, the definition, in particular the definition
of plan optimality, stays within the forward state space search framework where plans are
simple sequences of actions. Note also that ignoring the delete lists simplifies a task only if
all formulas are negation free. For a fixed domain, tasks can be polynomially normalized to
have that property: compute the negation normal form to all formulas (negations only in
front of facts), then introduce for each negated fact ¬B a new fact not-B and make sure it
is true in a state iff B is false (Gazen & Knoblock, 1997). This is the pre-process done in,
691

Hoffmann

for example, FF. In the investigation at hand, we have considered the normalized versions
of the domains.5
We also consider a few domains, from the IPC-4 collection, that feature derived predicates. Such predicates are not affected by the effects of the operators, and their truth value
is instead derived from the values of the other, basic, predicates, via a set of derivation
rules. A derivation rule has the form φ(x) ⇒ P (x) where P is the derived predicate and
φ (a formula) is the rule’s antecedent, both using the free variables x. The obvious idea
is that, if φ(x) holds, then P (x) can be concluded. In a little more detail, the semantics
are defined as follows. In the initial state, and whenever an action was applied, first all
derived predicate instances (derived facts) are assumed to be false, then all derivation rules
are applied until a fixpoint occurs. The derived facts that could not be concluded until
then are said to be false (this is called negation as failure). Derived predicates can be used
just like any other predicate in the operator preconditions, in the effect conditions, and in
the goal condition. However, to ensure that there is a unique fixpoint of rule application,
the use of derived predicates in derivation rule antecedents is restricted (in the context of
IPC-4) to a positive use in the sense that these predicates do not appear negated in the
negation normal form of any rule antecedent (Hoffmann & Edelkamp, 2005).
To make ignoring delete lists a simplification, one also needs that the derived facts are
used only positively in the operator preconditions, effect conditions, and goal condition
(otherwise the derived predicates can, for example, be used to model negated preconditions etc.). Due to the negation as failure semantics of derived predicates, there isn’t a
simple compilation of negations as in the pure ADL case. The approach we take here,
and that is implemented in, for example, the version of FF that treats derived predicates
(Thiebaux, Hoffmann, & Nebel, 2003, 2005), is to simply ignore (replace with true) negated
derived predicates in (the negation normal form of) operators and the goal (see also below,
Section 2.3).
2.2 Domains Overview
As said before, our case study covers a total of 30 commonly used STRIPS and ADL benchmark domains. These include all the examples from the first four international competitions,
plus 7 more domains used in the literature. Brief descriptions of all domains can be looked
up in Appendix B, full formal definitions of the domains (except the 5 IPC-4 domains) are
in the TR (Hoffmann, 2003c). Note that, for defining a domain, one must amongst other
things decide what exactly the instances are. Naturally, to do so we have abstracted from
the known example suites. In most cases the abstraction is obvious, in the less obvious
cases the respective subsection of Appendix B includes some explanatory remarks.
Here, we provide a brief overview of the 30 analyzed domains. The domains are categorized into three groups according to their semantics, at a high level of abstraction. The
categorization is not, in any way, related to the topological characterization we will derive
later. We use it only to give the overview some structure.
5. Ignoring delete lists in the normalized domains comes down to a relaxation that, basically, allows (the
translated) facts to take on both truth values at the same time.

692

Where “Ignoring Delete Lists” Works

1. Transportation domains. These are domains where there are locations, objects
that must be transported, and vehicles that are the means of transportation.6 Operators mostly either move a vehicle, or load (unload) an object onto (from) a vehicle. The domains differ in terms of various constraints. An important one is that, in
many domains, vehicles can move instantaneously between any two locations, while in
other domains the movable links between locations form arbitrary road maps. There
are 13 transportation domains in the collection we look at. Logistics – the classical transportation domain, where trucks/airplanes transport objects within/between
cities. Gripper – a robot with two gripper hands transports a number of balls (one
at a time in each hand) from one room to another. Ferry – a single ferry transports
cars one at a time. Driverlog – trucks need drivers on board in order to move, the
location links form bi-directional road maps (which can be different for trucks and
drivers). Briefcaseworld – a briefcase moves, by conditional effects, all objects along
that are inside it. Grid – a robot transports keys on a grid-like road map where
positions can be locked and must be opened with keys of matching shapes. MiconicSTRIPS – an elevator transports passengers, using explicit actions to board/deboard
passengers. Miconic-SIMPLE – like Miconic-STRIPS, but passengers board/deboard
“themselves” by conditional effects of the action that stops the elevator at a floor.
Miconic-ADL – like Miconic-SIMPLE, but various constraints must be obeyed (for
example, VIPs first). Zenotravel – airplanes use fuel items that can be replenished
one by one using a refuel operator. Mprime – on an arbitrary road map, trucks use
non-replenishable fuel items, and fuel can be transferred between locations. Mystery
– like Mprime, but without the possibility to transfer fuel. Airport – inbound and
outbound planes must be moved safely across the road map of an airport.
2. Construction domains. These are generally not as closely related as the transportation domains above. What the construction domains have in common, roughly,
is that a complex object must be built out of its individual parts. There are 6 such
domains in the collection we look at. Blocksworld-arm – the classical construction
domain, where blocks are picked up/put down or stacked onto/unstacked from each
other by means of a robot arm. Blocksworld-no-arm – like above, but blocks are
moved around directly from a block to a block / from the table to a block / from a
block to the table. Depots – a combination of Blocksworld-arm and Logistics, where
objects must be transported between locations before they can be stacked onto each
other. Freecell – an encoding of the solitaire card game that comes with Microsoft
Windows (the “complex object” to be constructed is the final position of the cards).
Hanoi – an encoding of the classical Towers of Hanoi problem. Assembly – a complex
object must be assembled together out of its parts, which themselves might need to
be assembled beforehand.
3. Other domains. There are 11 domains in the collection whose semantics do not
quite fit into either of the above groups. Simple-Tsp – a trivial STRIPS version of the
6. The term “transportation domains” was suggested, for example, by Long and Fox (2000) and Helmert
(2003). The transportation benchmarks are generally more closely related than the other groups of
domains overviewed below, and we will sometimes discuss transportation domains on a rather generic
level.

693

Hoffmann

TSP problem, where the move operator can be applied between any two locations.
Movie – in order to watch a movie, one must buy snacks, set the counter on the video
to zero, and rewind the tape. Tireworld – a number of flat tires must be replaced,
which involves various working steps (like removing a flat tire and putting on a new
one). Fridge – for a number of fridges, the broken compressors must be replaced,
which involves various working steps (like loosening/fastening the screws that hold
a compressor). Schedule – objects must be processed (painted, for example) on a
number of machines. Satellite – satellites must take images (of phenomena in space),
using appropriate instruments. Rovers – rovers must navigate along a road map, take
soil/rock samples as well as images, and communicate the resulting data to a lander.
Pipesworld – oil derivatives must be propagated through a pipeline network. PSR –
some lines must be re-supplied in a faulty electricity network. Dining-Philosophers –
the deadlock situation in the Dining-Philosophers problem, translated to ADL from
the automata-based “Promela” language (Edelkamp, 2003a), must be found. OpticalTelegraph – similar to Dining-Philosophers, but considering an encoding of a telegraph
communication system.
2.3 Local Search Topology
Remember that we only consider solvable tasks, since the main focus of the investigation is
to characterize the kinds of domains in which heuristic planners can find plans fast. Some
discussion of unsolvable tasks is in Section 7.
Given a planning task (A, I, G). The state space (S, T ) is a graph where S are all states
that are reachable from the initial state, and T is the set of state transitions, i.e., the set of
all pairs (s, s0 ) ∈ S × S of states where there is an action that leads to s0 when executed in
s. The goal distance gd(s) for a state s ∈ S is the length of a shortest path in (S, T ) from s
to a goal state, or gd(s) = ∞ if there is no such path. In the latter case, s is a dead end; we
discuss such states directly below. A heuristic is a function h : S 7→ N ∪ {∞}.7 A heuristic
can return ∞ to indicate that the state at hand might be a dead end.
Given a STRIPS action a = (pre(a), add(a), del(a)), the relaxation a+ of a is
(pre(a), add(a), ∅). Given an ADL action a = (pre(a), E(a)), the relaxation a+ of a is
(pre(a), E(a)+ ) where E(a)+ is the same as E(a) except that all delete lists are empty. For
a set A of actions, the relaxation A+ of A is A+ := {a+ | a ∈ A}. An action sequence
+
+
ha1 , . . . , an i is a relaxed plan for (A, I, G) if ha+
1 , . . . , an i is a plan for (A , I, G). With that,
+
∗
for any state s, h (s) = min{n | P = ha1 , . . . , an i ∈ A , P is relaxed plan for (A, s, G)},
where the minimum over an empty set is ∞.
In the presence of derived predicates, as said above we additionally relax the planning
task by ignoring (replacing with true) all negated derived predicates in the negation normal
forms of preconditions, effect conditions, and the goal condition. Note that, with this
additional simplification, it can happen that h+ (s) is 0 although s is not a goal state,
because the simplification might relax the goal condition itself. Indeed, this happens in the
PSR domain. In all other domains we consider here, derived predicates are either not used
at all or used only positively, so there h+ (s) = 0 iff s is a goal state.
7. While the article focuses mainly on the h+ heuristic, we keep the topology definitions – which do not
depend on the specific heuristic used – somewhat more general.

694

Where “Ignoring Delete Lists” Works

One phenomenon that is clearly relevant for the performance of heuristic state space
search is that of dead end states s, gd(s) = ∞. A heuristic function h can return h(s) = ∞.
Taking this as an indication that s is a dead end, the obvious idea is to remove s from the
search space (this is done in, for example, HSP and FF). This technique is only adequate if
h is completeness preserving in the sense that h(s) = ∞ ⇒ gd(s) = ∞ for all s ∈ S. With
a completeness-preserving heuristic, a dead end state s is called recognized if h(s) = ∞ and
unrecognized otherwise. Note that h+ is completeness preserving. If a task can not be solved
even when ignoring the delete lists, then the task is unsolvable. From now on we assume
that the heuristic we look at is completeness preserving. With respect to dead ends, any
planning state space falls into one of the following four classes. The state space is called:
1. Undirected, if, for all (s, s0 ) ∈ T , (s0 , s) ∈ T .
2. Harmless, if there exists (s, s0 ) ∈ T such that (s0 , s) 6∈ T , and, for all s ∈ S, gd(s) < ∞.
3. Recognized, if there exists s ∈ S such that gd(s) = ∞, and, for all s ∈ S, if gd(s) = ∞
then h(s) = ∞.
4. Unrecognized, if there exists s ∈ S such that gd(s) = ∞ and h(s) < ∞.
In the first class, there can be no dead ends because everything can be undone. In the
second class, some things can not be undone, but those single-directed state transitions do
not “do any harm”, in the sense that there are no dead end states. In the third class, there
are dead end states, but all of them are recognized by the heuristic function. The only
critical case for heuristic search is class four, where a search algorithm can run into a dead
end without noticing it. This is particularly relevant if, potentially, large regions of the
state space consist of unrecognized dead end states. To capture this, we define the depth of
an unrecognized dead end s as the number of states s0 such that s0 is an unrecognized dead
end, and s0 is reachable from s by a path that moves only through unrecognized dead ends.
Our investigation determines, for each of the 30 benchmark domains looked at, exactly in
which of the above four dead end classes the instances of the domain belong. For the domains
where it turns out that there can be unrecognized dead ends, we construct parameterized
examples showing that the unrecognized dead ends can be arbitrarily deep. In several
domains, individual instances can fall into different classes. In this case we associate the
overall domain with the worst-case class, i.e., the class with highest index in the above.
For example, in Miconic-ADL, if there are no additional constraints to be obeyed on the
transportation of passengers then the state space is harmless as in Miconic-SIMPLE. But
if constraints on, for example, the possible direction of travel and the access to floors are
given, then unrecognized dead ends can arise. To avoid clumsy language, henceforth, if we
say that a state space is harmless/recognized/unrecognized, then we mean that it falls into
the respective class, or into a class below it.
We now get into the definitions of general topological phenomena, i.e., of relevant properties of the search space surface. We adapt the definitions given for SAT by Frank et al.
(1997). The difference between the SAT framework there, and the planning formalism here,
lies in the possibly single-directed state transitions in planning. In the search spaces considered by Frank et al., all state transitions can be traversed in both directions. Single-directed
695

Hoffmann

state transitions can have an important impact on the search space topology, enabling, for
example, the existence of dead ends.8
The base entity in the state space topology are what Frank et al. name plateaus. These
are regions that are equivalent under reachability aspects, and look the same from the point
of view of the heuristic function. For l ∈ N ∪ {∞}, a plateau P of level l is a maximal subset
of S for which the induced subgraph in (S, T ) is strongly connected, and h(s) = l for each
s ∈ P .9 Plateaus differ in terms of the possibilities of leaving their heuristic level, i.e., of
reaching an exit. For a plateau P of level l, an exit is a state s reachable from P , such that
h(s) = l and there exists a state s0 , (s, s0 ) ∈ T , with h(s0 ) < h(s). Based on the behavior
with respect to exits, we distinguish between five classes of plateaus. We need the notion
of flat paths. These are paths in (S, T ) on that the value of h remains constant.
1. A recognized dead end is a plateau P of level l = ∞.
2. A local minimum is a plateau P of level 0 < l < ∞ from that no exit is reachable on
a flat path.
3. A bench is a plateau P of level 0 < l < ∞, such that at least one exit is reachable
from P on a flat path, and at least one state on P is not an exit.
4. A contour is a plateau P of level 0 < l < ∞ that consists entirely of exits.
5. A global minimum is a plateau P of level 0.
Each plateau belongs to exactly one of these classes. Intuitively, the roles that the different
kinds of plateaus play for heuristic search are the following. Recognized dead ends can
be ignored with a completeness-preserving heuristic function. Local minima are difficult
because all neighbors look worse, so it is not clear in which direction to move next. Benches
are potentially easier, because one can step off them without temporarily worsening the
heuristic value. From contours, one can step off immediately.10
The main difficulty for a heuristic search is how to deal with the local minima and the
benches. In both cases, the search algorithm must (eventually) find a path to an exit in
order to get closer to the goal (as far as the heuristic function is informed about what is
closer to the goal and what is not). How difficult it is to find an exit can be assessed by a
variety of different parameters. The size (number of states) or diameter (maximum distance
between any two states) of the local minimum/the bench, and the number of nearby exit
states, to name some important ones. In the benchmarks considered, as mentioned in the
introduction, we empirically found that there are no (or very few) interesting observations
to be made about these parameters (Hoffmann, 2001b, 2003b).
8. One can, of course, introduce backtracking mechanisms into a search space, such as always giving the
planner the possibility to retract the last step. But that does not affect the relevant topological differences
between search spaces – instead of domains with/without dead ends, one gets domains where backtracking
is necessary/not necessary.
9. The difference to the undirected case is that we require the states on the plateau to be strongly connected
– with undirected state transitions this is trivially fulfilled by any set of connected states.
10. The differences to the undirected case lie in that there can be plateaus of level ∞, and that we allow exits
to not lie on the plateaus themselves. The latter is just a minor technical device to obtain a compact
terminology.

696

Where “Ignoring Delete Lists” Works

What one can frequently observe are interesting properties of the distance to the nearest
exit state. The distance dist(s, s0 ) between any two states s, s0 ∈ S is the usual graph
distance, i.e., the length of a shortest path from s to s0 in (S, T ), or ∞ if there is no such
path. The exit distance ed(s) of a search state s is the distance to the nearest exit, i.e.:
ed(s) = min{d | d is the length of a path in (S, T ) from s to a state s0 s.t. h(s0 ) = h(s),
and there exists a state s00 s.t. (s0 , s00 ) ∈ T , and h(s00 ) < h(s0 ) },
where, as before, the minimum over an empty set is ∞. Note that we do not require the
path in the definition to be flat, i.e., it may be that, in order to reach s0 , we temporarily
have to increase the h+ value. This is because we want the definition to capture possible
“escape” routes from any state in the state space, including states that lie on local minima.
The maximal local minimum exit distance, mlmed(S, T ), of a state space (S, T ) is the
maximum over the exit distances of all states on local minima, or 0 if there are no such
states. The maximal bench exit distance, mbed(S, T ), of a state space (S, T ) is the maximum
over the exit distances of all states on benches, or 0 if there are no such states. We will find
that, in many of the considered domains, there are constant upper bounds on mlmed(S, T )
and/or mbed(S, T ) under h+ , i.e., bounds that are valid irrespectively of the (size of the)
instance chosen.
The following is an implication that is relevant for the subsequent investigation.
Proposition 1 Given a solvable task (A, I, G), with state space (S, T ) and a completenesspreserving heuristic h, where h(s) = 0 ⇒ gd(s) = 0 for s ∈ S. If there exists an unrecognized
dead end s ∈ S, then mlmed(S, T ) = ∞.
Proof: Let s be an unrecognized dead end, and let s0 be a state reachable from s so that
the h value of s0 is minimal. Then s0 is an unrecognized dead end, too (in particular, s is
considered reachable from itself), and since h(s0 ) = 0 ⇒ gd(s0 ) = 0 we have h(s0 ) > 0. We
further have, since the h value of s0 is minimal among the states reachable from s, that
h(s00 ) ≥ h(s0 ) for all states s00 reachable from s0 . Thus the plateau on which s0 lies is a local
minimum – no exits are reachable, in particular not on flat paths. This also shows that s0
has infinite exit distance.
2
Proposition 1 says that, in every region of unrecognized dead ends, there is a local
minimum, given h(s) = 0 ⇒ gd(s) = 0.11 With the above definitions, that unrecognized
dead end state yields an infinite local minimum exit distance. It makes sense to define
things this way because an (arbitrarily deep) unrecognized dead end is worse than any local
minimum: it can not be escaped from at all.
11. Remember that the latter can be untrue for h+ only if the domain features derived predicates that
appear negated in the negation normal form of the goal condition. And even then, by the argument in
the proposition, every region of unrecognized dead ends would contain a global minimum consisting of
non-solution states. We could have defined such “fake”-global minima to be local minima, but decided
against it in order to not overly complicate the topological definitions, and since that detail does not
seem very important. As said before, in all but one of our 30 domains we have h+ (s) = 0 ⇔ gd(s) = 0
anyway.

697

Hoffmann

3. Some Core Lemmas
In many of the investigated domains, intuitively similar patterns of problem structure cause
the characteristic qualities of h+ . Some of this common structure can be generalized and
captured in concise definitions and lemmas. The lemmas formulate sufficient criteria implying that (the state space of) a planning task has certain topological properties. Proofs
for domains proceed, where possible, by applying the lemmas to arbitrary instances. In
several domains where the lemmas can not be applied immediately (due to syntactic details
of the domain definitions), similar proof arguments suffice to show the desired topological
properties.
We restrict ourselves to STRIPS tasks in the lemmas. Appropriate extensions to ADL
and/or to derived predicates are probably possible at least in certain cases, but we have not
investigated this in detail – such extensions are likely to be rather complicated notationally,
and the simpler STRIPS case suffices to transport the ideas.

initial state:
at(V, L1 ), at(O1 , L1 ), at(O2 , L2 )
goal:
at(O1 , L2 ), at(O2 , L1 )
actions:
name
precondition
move(l, l0 )
at(V, l)
load(o, l)
at(V, l), at(o, l)
unload(o, l) at(V, l), in(o, V )

add list
at(V, l0 )
in(o, V )
at(o, l)

delete list
at(V, l)
at(o, l)
in(o, V )

Figure 2: A simple STRIPS transportation task.
Throughout the section, we assume we are given a STRIPS task (A, I, G). As an illustrative example for the definitions and lemmas, we will use the simple transportation task
defined in Figure 2. In what follows, there are three separate sections, concerned with dead
ends, local minima, and benches, respectively.
The definitions and lemmas in the following are not syntactical, in the sense that they
make use of informations that can not be computed efficiently (for example, inconsistencies
between facts). We do not discuss this, and focus exclusively on the role of the definitions
and lemmas as tools for proving h+ topology. The role of the definitions and lemmas as
tools for automatically detecting h+ topology will be discussed in Section 6.
3.1 Dead Ends
We first focus on criteria sufficient for the non-existence of dead ends. Our starting point is
a reformulated version of a simple result mentioned by, for example, Koehler and Hoffmann
(2000). We need the notion of inconsistency. Two facts are inconsistent if there is no
reachable state that contains both of them. A set of facts F is inconsistent with another set
698

Where “Ignoring Delete Lists” Works

of facts F 0 if each fact in F is inconsistent with at least one fact in F 0 .12 An action a ∈ A
is invertible if:
(1) add(a) is inconsistent with pre(a);
(2) del(a) ⊆ pre(a);
(3) there is an action a ∈ A such that
(a) pre(a) ⊆ (pre(a) ∪ add(a)) \ del(a),
(b) add(a) = del(a), and
(c) del(a) = add(a).
The intentions behind these requirements are the following. (1) and (2) ensure that
a’s effects all occur, (3a) ensures that a is applicable, and (3b) and (3c) ensure that a
undoes a’s effects. As an example, all actions in the illustrative task from Figure 2 are
invertible. For example, an a = move(l, l0 ) action is inverted by a = move(l0 , l). To see that,
simply insert the definitions: add(a) = {at(V, l0 )} is inconsistent with pre(a) = {at(V, l)};
del(a) = {at(V, l)} = pre(a); pre(a) = {at(V, l0 )} = add(a); add(a) = {at(V, l)} = del(a);
del(a) = {at(V, l0 )} = add(a). Similarly easily, one sees that load(o, l) and unload(o, l)
invert each other. Examples of benchmark domains with invertible actions are Blocksworld
(in both variants), Logistics, and Gripper.
Lemma 1 [Koehler & Hoffmann, 2000] Given a STRIPS planning task (A, I, G). If all
actions a ∈ A are invertible, then the state space of the task is undirected.
Proof: For any state s and applicable action a, a is applicable in Result(s, hai) due to
condition (3a) of invertibility. Conditions (1) and (2) make sure that a’s effects do in fact
appear (condition (1) requires that each fact in the add list is inconsistent with at least one
fact in the precondition), and conditions (3b) and (3c) make sure that a undoes exactly
those effects.
2
We remark that, in contrast to what one may think at first sight, a task can have an
undirected state space even if some actions are not invertible in the above sense. Imagine,
for example, an action a where del(a) = {p} and pre(a) = {p0 }, and, due to the domain
semantics, if p0 is true then p is also true. This means that a’s delete effect always appears;
however, this can not be detected with the simple syntax check, del(a) ⊆ pre(a), used in
the definition above.
We next provide a new criterion that is weaker – more broadly applicable – than
Lemma 1, and that only implies the non-existence of dead ends. The criterion is based
on a weaker version of invertibility, and on two alternative properties whose combination
can make an action “safe”.
To make an action a not lead into a dead end, it is already sufficient if the inverse action
re-achieves at least what has been deleted, and does not delete any facts that have been true
12. It may seem more natural to define inconsistency between fact sets in a symmetrical fashion, demanding
that every fact in F be inconsistent with every fact in F 0 . In our context here, that definition would be
stronger than what we need.

699

Hoffmann

before. That is, given a state s in which a is applicable, applying a in Result(s, hai) leads
us back to a state s0 that satisfies s0 ⊇ s. Formally, an action a ∈ A is at least invertible if
there is an action a ∈ A such that:
(1) pre(a) ⊆ (pre(a) ∪ add(a)) \ del(a),
(2) add(a) ⊇ del(a), and
(3) del(a) is inconsistent with pre(a).
Condition (1) here ensures, as before, that a is applicable in Result(s, hai). Condition
(2) ensures that a re-achieves every fact that was deleted by a. Condition (3) ensures
that the facts deleted by a were not true in s anyway. Note that any invertible action is
also at least invertible. Conditions (1) and (2) are obviously given. As for condition (3),
if del(a) = add(a) (condition (3c) of invertibility), and add(a) is inconsistent with pre(a)
(condition (1) of invertibility), then del(a) is inconsistent with pre(a). So “invertible” is
stronger than “at least invertible”; we chose the name “at least” for the latter to illustrate
that, with this definition of invertibility, a potentially “re-”achieves more facts than we had
in the original state s.
As an example, consider what happens if we modify the move(l, l0 ) action in Figure 2 to
include a visited(l0 ) fact in its add list. The resulting action is no longer invertible because
move(l0 , l) does not delete visited(l0 ). If we apply, in state s, move(l, l0 ) and move(l0 , l)
in sequence, then now that gets us to a state s0 that is identical to s except that it also
includes visited(l) and visited(l0 ), which may not have been true before. Move actions of
this kind form the Simple-Tsp domain. They are at least invertible in the above sense:
pre(move(l0 , l)) = {at(V, l0 )} = add(move(l, l0 )); add(move(l0 , l)) = {at(V, l), visited(l)} ⊇
{at(V, l)} = del(move(l, l0 )); del(move(l, l0 )) = {at(V, l0 )} is inconsistent with {at(V, l)} =
pre(move(l, l0 )).
Another property implying that an action can not lead into dead ends is this. If the
action must be applied at most once (because its add effects will remain true), and it deletes
nothing but its own preconditions, then that action needs not be inverted. Formally, an
action a ∈ A has static add effects if:
[

add(a) ∩

del(a0 ) = ∅.

a0 ∈A

An action a ∈ A has relevant delete effects, if:
del(a) ∩ (G ∪

[

pre(a0 )) 6= ∅.

a6=a0 ∈A

S

If del(a)∩ (G∪ a6=a0 ∈A pre(a0 )) = ∅, then we say that a has no relevant delete effects, which
is the property we will actually be interested in. In the illustrative task from Figure 2,
imagine we disallow unloading an object at its initial location, and loading an object at
its goal location. Then the remaining unload actions (unload(O1 , L2 ) and unload(O2 , L1 ))
have static add effects – no action can delete the goal position of an object – and no relevant
delete effects – the only action that needs an object to be in the vehicle is the respective
unload at the goal location. Actions that have such characteristics are, for example, the
700

Where “Ignoring Delete Lists” Works

actions that make passengers get out of the lift in Miconic-STRIPS (a passenger can get
into the lift only at his/her origin floor, and get out of the lift only at his/her destination
floor). Another example is contained in the Tireworld domain, where there is an action
that inflates a flat wheel: there is no “de-flating” action and so the add effects are static;
no action nor the goal needs a wheel to be flat so there are no relevant delete effects.
Lemma 2 Given a solvable STRIPS planning task (A, I, G). If it holds for all actions
a ∈ A that either
1. a is at least invertible, or
2. a has static add effects and no relevant delete effects,
then the state space of the task is harmless.
Proof: In short, to any reachable state s = Result(I, ha1 , . . . , an i) a plan can be constructed
by inverting ha1 , . . . , an i (applying the respective inverse actions in the inverse order), and
executing an arbitrary plan for (A, I, G) thereafter. In these processes, actions that are not
(at least) invertible can be skipped because by prerequisite they have static add effects and
no relevant delete effects.
In more detail, the proof argument proceeds as follows. To any reachable state s =
Result(I, ha1 , . . . , an i) ∈ S, we identify a solution P for (A, s, G). Let hp1 , . . . , pm i ∈ A∗ be
a solution for (A, I, G) (which exists as (A, I, G) is solvable by prerequisite). We construct
P with the algorithm shown in Figure 3.
M := ∅
for i := n . . . 1 do
if ai is at least invertible by ai then
if ai 6∈ M apply ai endif
else M := M ∪ {ai }
endif
endfor
for i := 1 . . . m do
if pi 6∈ M then apply pi endif
endfor
Figure 3: Constructing plans in tasks where all actions are either at least invertible, or have
static add effects and no relevant delete effects.
In the algorithm, M serves as a kind of memory set for the actions that could not be
inverted. We need to prove that the preconditions of all applied actions are fulfilled in the
state where they are applied, and that the goals are true upon termination. Let us start
with the first loop. We denote by si := result(I, ha1 , . . . , ai i) the state after executing the
701

Hoffmann

ith action on the path to s, and by s0i the state before the first loop starts with value i. We
prove:
[
[
s0i ⊇ (si ∩ (G ∪
add(a)
pre(a))) ∪
a∈Mi

a∈A\Mi

Mi here denotes the current state of the set. We proceed by backward induction over i.
If i = n, we got s0i = si and Mi = ∅, so the equation is trivially true. Now assume the
equation is true for i ≥ 1. We prove that the equation holds for i − 1. If ai is not at least
invertible, then no action is applied, s0i−1 = s0i , and Mi−1 = Mi ∪ {ai }. Concerning the
left hand side of the expression on the right hand side of the equation, we observe that ai
S
does by prerequisite not delete any fact from G ∪ a∈A\Mi−1 pre(a) (Mi−1 contains ai ), so
all relevant facts from si−1 have already been true in s0i . Concerning the right hand side
of the expression on the right, we observe that the facts in add(ai ) are never deleted by
S
prerequisite, so a∈Mi−1 add(a) is contained in s0i . Now assume that ai is at least invertible
by ai . We got Mi−1 = Mi . Assume ai is applied, i.e., ai 6∈ Mi . It is applicable because its
preconditions are contained in si , and it is not an element of Mi . For the resulting state
s0i−1 , all facts that ai has deleted from si−1 are added, and only facts are deleted that have
not been true in si−1 anyway; also, none of the add effects of actions in Mi is deleted, so the
equation is fulfilled. Finally, if ai is not applied, ai ∈ Mi , then ai has static add effects and
was applied before, so its add effects are contained in s0i , and ai ’s delete effects are empty.
Inserting i = 0 in the equation we have just proved, we get
s0 ⊇ (I ∩ (G ∪

[

pre(a))) ∪

a∈A\M0

[

add(a)

a∈M0

The second loop starts from s0 . So we start a solution plan, excluding the actions in a
set M0 , from a state including all initial facts that are contained in the goal or in the
precondition of any action not in M0 . As the state additionally contains all add effects of
all actions in M0 , and those add effects are not deleted by any action, it is clear that we
can simply skip the actions in M0 and achieve the goal.
2
As an example to illustrate the proof, consider a reachable state in the Tireworld domain.
Every action is invertible, except the action that inflates a wheel. Say, as in the proof, we
are in a state s reached by the action sequence ha1 , . . . , an i. What the algorithm in Figure 3
will do is, undo everything we have done, by applying the respective ai actions, except for
the inflating actions ai . The latter will be stored in the set M . This gets us to a state
that is identical to the initial state, except that we have already inflated some of the flat
wheels (those corresponding to the actions in M ). From that state, the algorithm executes
an arbitrary solution, skipping the previously applied inflating actions (in M ).
3.2 Local Minima
We define an important kind of relationship between the role of an action in the real task
and its role in the relaxed task. Combining this definition with the notions of at least
invertible actions, and (no) relevant delete effects, yields a criterion that is sufficient for the
non-existence of local minima under h+ (or, equivalently, for 0 being an upper bound on
the maximal local minimum exit distance). The criterion can be directly applied in 7 of the
702

Where “Ignoring Delete Lists” Works

30 investigated domains, and can be applied with slight modifications in 2 more domains.
Many of the more individual proofs make use of similar, albeit somewhat more complicated,
proof arguments.
The key property behind the lack of local minima under h+ is, most of the time, that
every action that is good for solving the real task is also good for solving the relaxed task.
Formally, an action a ∈ A is respected by the relaxation if:
for any reachable state s ∈ S such that a starts an optimal plan for (A, s, G), there is an
optimal relaxed plan for (A, s, G) that contains a.
Note that one can assume the relaxed plan to start with a, since in the relaxation it can
only be better to apply an action earlier.
All actions in the illustrative task from Figure 2 are respected by the relaxation. Consider the move(l, l0 ) actions, for example. If, in a state s, an optimal plan starts with
move(l, l0 ), then there must be a good reason for this. Either a) at l0 there is an object that
has yet to be transported, or b) an object is in the truck that must be transported to l0 . In
both cases, any relaxed plan must also transport the object, and there is no chance of doing
so without moving to l0 at some point. Similarly, if an optimal plan starts with a load(o, l)
action, then this means that o must be transported somewhere else, and the relaxed plan
does not get around loading it. Finally, if an optimal plan starts with an unload(o, l) action,
then this means that l is the goal location of o, and any relaxed plan will have to include
that action.
Similar arguments as the above can be applied in many transportation domains. The
argument regarding move actions becomes a little more complicated if there are non-trivial
road maps, unlike in the illustrative example where there are only two locations that are
reachable in a single step from each other. Say the road map is a (any) directed graph,
and we modify the move action from Figure 2 only in that we add a precondition fact
demanding the existence of an edge from l to l0 . Then all move actions are still respected by
the relaxation, because ignoring delete lists does not affect the shape of the road map. Any
optimal real path from a location l to a location l0 coincides with an optimal relaxed path
of movements from l to l0 (even though the result of executing the path will be different).
From there, the claim follows with the same argument as above, namely, that an optimal
plans moves from l to l0 only if some object provides a reason for doing so.
If a transportation domain features additional constraints on, or side effects of, the move
actions, then they may not be respected by the relaxation. We give an example below, after
formulating our main lemma regarding local minima under h+ .
Note that there can exist local minima even if all actions are respected by the relaxation.
Consider the following transportation task, featuring single-directional edges in the road
map graph. As argued above, all actions are respected by the relaxation. A vehicle and
two objects o1 , o2 are initially at l; o1 must go to l1 and o2 must go to l2 ; the edge from l
to l1 is single-directed and the edge from l to l2 is single-directed; between l1 and l2 , there
is a path of n bi-directional (undirected) edges. The optimal relaxed plan for the state s
where, from the initial state, o1 and o2 were loaded, has length 4: move from l to l1 and l2 ,
and unload o1 and o2 at l1 and l2 , respectively. However, once one moved, in s, to either l1
or l2 , the optimal relaxed plan length goes up to n + 2, since the entire path between l1 an
703

Hoffmann

l2 must be traversed. So s lies on a local minimum, given that n > 2; note that, by setting
n to arbitrarily high values, we get a local minimum with arbitrarily large exit distance.
It turns out that preventing the above example, precisely, making use of the notions of
invertibility and of relevant delete effects, as introduced above, suffices to get rid of local
minima under h+ .
Lemma 3 Given a solvable STRIPS task (A, I, G), such that the state space (S, T ) does
not contain unrecognized dead ends. If each action a ∈ A
1. is respected by the relaxation, and
2. is at least invertible or has no relevant delete effects,
then there are no local minima in (S, T ) under evaluation with h+ .
Proof: The states with gd(s) = ∞ are not on local minima by prerequisite, with h+ (s) = ∞.
We will prove that, in every reachable state s with 0 < gd(s) 6= ∞, if an action a starts
an optimal plan for (A, s, G), then h+ (Result(s, hai)) ≤ h+ (s). This proves the lemma:
iterating the argument, we obtain a path from s to a goal state s0 , where the value of h+
does not increase on the path. This means that from s an exit is reachable on a flat path –
h(s0 ) = 0 < h(s) so at some point on the path the h+ value becomes lower than h(s), thus
s can not lie on a local minimum.
Let s be a reachable state with 0 < gd(s) 6= ∞. Let a be an action that starts an
optimal plan for (A, s, G). We denote s0 := Result(s, hai). The action is respected by the
relaxation, so there is an optimal relaxed plan P + (s) for (A, s, G) that starts with a.
Case (A), removing a from P + (s) yields a relaxed plan for (A, s0 , G). Then h+ (s0 ) <
+
h (s) follows, and we are finished. This is the case, in particular, if a has no relevant delete
effects: the facts that a deletes are not needed by any other action nor by the goal, so P + (s)
without a achieves the goal starting from s0 (where a has already been applied).
Case (B), assume removing a from P + (s) does not yield a relaxed plan for s0 . Then,
with what was said before, a does have relevant delete effects, and must thus be at least
invertible. That is, there is an action a ∈ A with pre(a) ⊆ (pre(a) ∪ add(a)) \ del(a) and
add(a) ⊇ del(a). The action a is guaranteed to be applicable in s0 , and it re-achieves a’s
delete effects. Denote by P + (s0 ) the action sequence that results from replacing, in P + (s),
a with a. Then P + (s0 ) is a relaxed plan for (A, s0 , G). This can be seen as follows. Observe
that, by definition, P + (s) without a is a relaxed plan for Result(s, ha+ i) (we abbreviate
the notation somewhat to improve readability). The desired property now follows because
Result(s0 , ha+ i) is a superset of Result(s, ha+ i): we have Result(s, ha+ i) = s ∪ add(a),
s0 = (s ∪ add(a)) \ del(a), and add(a) ⊇ del(a). So P + (s0 ) is a relaxed plan for (A, s0 , G),
yielding h+ (s0 ) ≤ h+ (s).
2
The proof to Lemma 3 demonstrates along which lines, typically, the proof arguments
in this investigation proceed. Given a state s, consider an action a that starts an optimal
plan for s, and consider an optimal relaxed plan P + for s (that contains a, ideally). Then,
determine how P + can be modified to obtain a relaxed plan for the state that results from
a’s execution. This technique forms the basis of literally all proofs except those concerned
with dead ends. Note that the second prerequisite of Lemma 3 is fulfilled by planning
704

Where “Ignoring Delete Lists” Works

tasks qualifying for the undirectedness or harmlessness criteria given by Lemmas 1 and 2.
Note also that, with what was said above, we have now proved that the state space of the
illustrative example in Figure 2 is undirected, and does not contain any local minima under
h+ .
Domains where all actions are respected by the relaxation are, for example, the STRIPS
transportation domains Logistics, Gripper, Ferry, and Miconic-STRIPS. In all these cases,
the respective proof arguments are very similar to what we said above. It is instructive
to have a look at some examples where an action is not respected by the relaxation. In a
transportation domain, this can, for example, happen due to fuel usage as a “side effect”
of moving. Concretely, in the Mystery domain, applying a move action deletes a fuel unit
at the start location (the location in that the move starts). If fuel is running low at some
locations, a (real) plan may have to move along fuel-rich deviations in the road map. A
relaxed plan does not need to do that – it can always move along the shortest connections
on the map – because, there, the actions do not delete the fuel units.
Formulated somewhat more generally, relaxed plans can take “short-cuts” that don’t
work in reality. If these short-cuts are disjoint (in the starting actions) with the real solution
paths, then local minima may arise even if all actions are (at least) invertible. In the above
discussed transportation case, the short-cuts correspond in a very intuitive manner to what
one tends to think about as short-cuts (on a road map, namely). This is not the case in
general, i.e., in other kinds of domains. Consider the Blocksworld-arm state depicted in
Figure 4.

C
B

C

A

B

Figure 4: A local minimum state in Blocksworld-arm. The goal is to have B on the table,
and C on B.

In the depicted state, denoted s, B is on A is on the table, and the arm holds C. The
goal is to have B on the table, and C on B.13 The only optimal plan for s is to put C down
on the table, then unstack B from A and put it down on the table, then pickup C and stack
it onto B. The only optimal relaxed plan for s, however, is to stack C onto B immediately,
then unstack B from A, then put B down to the table. The “short-cut” here is that the
relaxed plan does not have to put C down on the table, because stacking C onto B does
not delete the fact that declares B’s surface as unoccupied. As a result, s lies on a local
13. Usually, in Blocksworld there are no goals demanding a block to be on the table. In the example, this is
done only for the sake of simplicity: one could just introduce one more block D and demand that B be
on D for the goal.

705

Hoffmann

minimum under h+ .14 The reason, intuitively, why h+ does not yield any local minima in
many domains, is that “vicious” short-cuts like in this example just don’t happen.
3.3 Benches
We could not find a nice general sufficient criterion implying upper bounds on the maximal
exit distance from local minima – except the special case above where there are no local
minima at all and thus 0 is an upper bound on the maximal local minimum exit distance.
We did, however, find a simple proof argument determining an upper bound on the maximal
exit distance from benches, in tasks that qualify for the application of Lemma 3. The proof
argument works, sometimes with slight modifications, in all the 7 domains where Lemma 3
can be directly applied – in all these domains, the maximal bench exit distance is bounded
by 1 (bounded by 0, in one case).
The proof argument is based on observing that, in many domains, some of the actions
have only delete effects that are irrelevant (for the relaxed plan, at least) once the action was
applied on an optimal solution path. Formally, an action a ∈ A has relaxed-plan relevant
delete effects if:
for any reachable state s ∈ S such that a starts an optimal plan for (A, s, G), there is no
S
optimal relaxed plan ha, a1 , . . . , an i for (A, s, G) such that del(a) ∩ (G ∪ ni=1 pre(ai )) = ∅.
If, for any reachable state s ∈ S such that a starts an optimal plan for (A, s, G), there is an
S
optimal relaxed plan ha, a1 , . . . , an i for (A, s, G) such that del(a) ∩ (G ∪ ni=1 pre(ai )) = ∅,
then we say that a has no relaxed-plan relevant delete effects, which is the property we will
actually be interested in. With this notation, if a has no relaxed-plan relevant delete effects,
and it starts an optimal plan for s, then a relaxed plan for Result(s, hai) can be constructed
as the sequence ha1 , . . . , an i, i.e., by skipping a from the relaxed plan for s. Thus the h+
value decreases from s to Result(s, hai). Note that n can be set to 0 if a results in a goal
state from s. Note also that, by definition, any action with no relaxed-plan relevant delete
effects is respected by the relaxation; if an action is not respected by the relaxation, then
we can not claim anything about h+ anyway. Note finally that, assuming an action a that
is respected by the relaxation, if a has no relevant delete effects, i.e., if a does not delete a
goal or any precondition of another action, then a also has no relaxed-plan relevant delete
effects in the sense of our definition.
Consider again our illustrative example from Figure 2. Say we have a state s in which
load(o, l) starts an optimal plan. This means that o has yet to be transported, to a location
l0 6= l. In particular, it means that at(o, l) is not a goal, and it follows that the action
– whose only delete effect is at(o, l) – has no relevant delete effects (no other action has
at(o, l) in its precondition). Further, say unload(o, l) starts an optimal plan in s. This
means that l is the goal location of o. After applying the action, the goal for o will be
achieved, and no action will need to refer to o again, in particular no action will require o to
be inside the vehicle, which is the only delete effect of unload(o, l). So that action neither
14. We have h+ (s) = 3. The h+ value after, in s, putting C down to the table is 4 (any relaxed plan has to
apply two actions for each of the two goals). The h+ value after stacking, in s, C onto B is still 3 (the
relaxed plan is unstack C B, unstack B A, put down B), but from there the only successor state is to
unstack C from B again, going back to s.

706

Where “Ignoring Delete Lists” Works

has relaxed-plan relevant delete effects. In contrast, consider the move(l, l0 ) action, that
deletes at(V, l). Say we are in the state s where O1 has been loaded into V from the initial
state of the task. Then move(L1 , L2 ) starts an optimal plan for s, and any relaxed plan
for Result(s, hmove(L1 , L2 )i) has to include the action move(L2 , L1 ), moving back from L2
to L1 in order to be able to transport O2 . So the delete effect of move(L1 , L2 ), namely
at(V, L1 ), is relaxed-plan relevant.
If, in a task satisfying the prerequisites of Lemma 3, an optimal starting action has no
relaxed-plan relevant delete effects, then one can apply case (A) in the proof of Lemma 3,
and obtain a smaller h+ value. To bound the maximal exit distance from benches, all we
need to do is to identify a maximum number of steps after which that will happen.
Lemma 4 Given a solvable STRIPS task (A, I, G) that satisfies the prerequisites of
Lemma 3. Let d be a constant so that, for every non dead-end state s ∈ S, there is an
optimal plan ha1 , . . . , an i where the d-th action, ad , has no relaxed-plan relevant delete effects. Then mbed(S, T ) ≤ d − 1.
Proof: Let s be a reachable state with 0 < gd(s) 6= ∞. Let ha1 , . . . , an i be an optimal plan
for (A, s, G), where ad has no relaxed-plan relevant delete effects. Denote, for 0 ≤ i ≤ n,
si := Result(s, ha1 , . . . , ai i). With the argumentation in Lemma 3, we have h+ (si ) ≤ h+ (s)
for all i. Consider the state sd−1 . By prerequisite, there is an optimal relaxed plan for
S
0
(A, sd−1 , G) that has the form had , a01 , . . . , a0m i, where del(ad ) ∩ (G ∪ m
i=1 pre(ai )) = ∅. But
0
0
+
+
then, obviously, ha1 , . . . , am i is a relaxed plan for sd , and so h (sd ) ≤ h (sd−1 ) − 1. The
distance from s to sd−1 is d − 1, and so the lemma follows.
2
Lemma 4 can be directly applied in 5 of the 7 domains that qualify for Lemma 3. Its
proof argument can, in a somewhat more general version, be applied in the 2 other domains
as well – namely, in Ferry and Gripper, where loading an object deletes space in the vehicle –
and in one more domain – namely, Miconic-SIMPLE, that uses some simple ADL constructs.
In the other domains where we proved an upper bound on the maximal exit distance from
benches (and/or an upper bound on the maximal exit distance from local minima), the
proof arguments are (a lot, sometimes) more complicated. Reconsidering the illustrative
example, as stated above the load and unload actions have no relaxed-plan relevant delete
effects, while the move actions do. Now, obviously, since the two locations are accessible
from each other with a single move, no optimal plan applies more than one move action
in a row, i.e., in any optimal plan the first or second action will be a load/unload. With
Lemma 4 this tells us that the maximal exit distance from benches is bounded by 1. A very
similar argument can be applied in all other transportation domains where every pair of
locations is connected via a single move (as in, for example, Logistics). More generally, in
(the standard encoding of) a transportation domain with no other constraints (regarding,
for example, fuel), and with an undirected road map graph, the exit distance is bounded
by the diameter of the road map graph, i.e., by the maximum distance of any two locations
(nodes) in the graph. The “worst” thing a solution plan might have to do is to traverse the
entire road map before loading/unloading an object.15
15. With directed road map graphs, as explained above, local minima can arise. More technically, Lemma 3
can not be applied, and so Lemma 4 can not be applied either.

707

Hoffmann

4. A Planning Domain Taxonomy
We now list our proved results, with brief explanations of how we obtained these results.
We then summarize the results in the form of a planning domain taxonomy.
We group the “positive” results – those which prove the non-existence of topological
phenomena that are problematic for heuristic search – together in single theorems. The
“negative” results are shown separately by sketching counter examples. We consider dead
ends, local minima, and benches in that order. Remember that, with respect to dead ends,
the only problematic case for heuristic search is when there are unrecognized dead ends, c.f.
Section 2.3.
Theorem 1 The state space of any solvable instance of
1. Blocksworld-arm, Blocksworld-no-arm, Briefcaseworld, Depots, Driverlog, Ferry,
Fridge, Gripper, Hanoi, or Logistics is undirected,
2. Grid, Miconic-SIMPLE, Miconic-STRIPS, Movie, Pipesworld, PSR, Satellite,
Simple-Tsp, Tireworld, or Zenotravel is harmless,
3. Dining-Philosophers, Optical-Telegraph, Rovers, or Schedule is recognized under evaluation with h+ .
In Blocksworld-arm, Blocksworld-no-arm, Driverlog, Ferry, Gripper, Hanoi, and Logistics, Lemma 1 can be directly applied. In Briefcaseworld, Depots, and Fridge, due to some
subtleties the actions are not invertible in the syntactical sense, but it is easy to show
that every action has an inverse counterpart. In Movie, Miconic-STRIPS, Simple-Tsp, and
Tireworld, Lemma 2 can be directly applied, in Grid and Miconic-SIMPLE similar proof
arguments as used in Lemma 2 suffice. In Pipesworld, PSR, Satellite, and Zenotravel, some
easy-to-see more individual domain properties prove the absence of dead ends. In the domains where all dead ends are recognized by h+ , the individual domain properties exploited
in the proofs are somewhat more involved. For example, in Rovers there is a plan to a state
if and only if, for all soil/rock samples and images that need to be taken, there is a rover
that can do the job, and that can communicate the gathered data to a lander. The only
chance to run into a dead end is to take a soil/rock sample with a rover that can not reach
a lander (the soil/rock sample is available only once). But then, there is no relaxed plan to
the state either.
In the 6 domains not mentioned in Theorem 1 (Airport, Assembly, Freecell, MiconicADL, Mprime, Mystery), it is easy to construct arbitrarily deep unrecognized dead ends
(arbitrarily long paths of unrecognized dead ends). For example, in Mystery and Mprime
the relaxed plan can still achieve the goal in situations where too much fuel was consumed
already; in Airport, two planes that block each other’s paths may move “across” each other
in the relaxed plan.
The positive results regarding local minima are these.
Theorem 2 Under h+ , the maximal local minimum exit distance in the state space of any
solvable instance of
708

Where “Ignoring Delete Lists” Works

1. Blocksworld-no-arm, Briefcaseworld, Ferry, Fridge, Grid, Gripper, Hanoi, Logistics,
Miconic-SIMPLE, Miconic-STRIPS, Movie, Simple-Tsp, or Tireworld is 0,
2. Zenotravel is at most 2, Satellite is at most 4, Schedule is at most 5, DiningPhilosophers is at most 31.
In Ferry, Gripper, Logistics, Miconic-STRIPS, Movie, Simple-Tsp, and Tireworld,
Lemma 3 can be applied. In Fridge and Miconic-SIMPLE, the actions do not adhere syntactically to the definitions of invertibility and (no) relevant delete effects, but have similar
semantics. So Lemma 3 can not be directly applied, but similar arguments suffice: it is easy
to see that all actions are respected by the relaxation, and the proof of Lemma 3 can be individually adapted to take into account the particular properties regarding invertibility and
relevant delete effects. (For example, if a passenger gets out of the lift in Miconic-SIMPLE,
then the delete effect is that the passenger is no longer inside the lift, which does not matter
since the passenger has reached her destination.) In Blocksworld-no-arm, Briefcaseworld,
and Grid, rather individual (and sometimes quite involved) arguments prove the absence
of local minima under h+ . The proof method is, in all cases, to consider some state s and
identify a flat path from s to a state with better h+ value. For example, in Grid this is done
by moving along a path of locations contained in the relaxed plan for s, until a key can
be picked up/put down, or a lock can be opened (this is a very simplified description, the
actual procedure is quite complicated). In Hanoi, one can prove that the optimal relaxed
solution length for any state is equal to the number of discs that are not yet in their final
goal position. This suffices because no optimal plan moves a disc away from its final position. Note that, thus, the Hanoi state spaces under h+ are a sequence of benches decreasing
exponentially in diameter and size.
In Zenotravel, Satellite, and Schedule, the proofs proceed by identifying a constant
number of steps that suffices to execute one action a in the optimal relaxed plan for a state
s, and, without deleting a’s relevant add effects, to re-achieve all relevant facts that were
deleted by a. In Dining-Philosophers (as well as Optical-Telegraph), due to the subtleties of
the PDDL encoding – which was, as said, obtained by an automatic compilation from the
automata-based “Promela” language (Edelkamp, 2003a) – h+ is only very loosely connected
to goal distance: in the relaxation, an automaton (for example, a philosopher) can always
“block itself” with at most 3 actions. The bound for Dining-Philosophers follows from
the rather constant and restrictive domain structure, where a constant number of process
transitions, namely 6, always suffices to block one more philosopher. The proved bound
is derived from this, by considering that 4 planning actions are needed for each process
transition, and that certain additional actions may be needed due to the subtleties of the
PDDL encoding (where a process can be “in between” two of its internal states). We remark
that the bound is valid even for the trivial heuristic function returning the number of yet
un-blocked philosophers. In fact, the proof for h+ can be viewed as a corollary of a proof for
this heuristic function; we get back to this at the end of this section. We finally remark that
the highest exit distance under h+ that we could actually construct in Dining-Philosophers
was 15. We conjecture that this is a (tight) upper bound.
In Satellite, Schedule, and Zenotravel, the proved upper bounds are tight. In all of
Dining-Philosophers, Satellite, Schedule, and Zenotravel, the bounds are valid for any nondead end state s. So, beside a bound on the local minimum exit distance, these results
709

Hoffmann

also provide a bound on the bench exit distance, and will be re-used for that below in this
section.
In Airport, Assembly, Freecell, Miconic-ADL, Mprime, and Mystery, as stated above
there can be unrecognized dead ends, so by Proposition 1 the local minimum exit distance
in these domains is unbounded. In all other domains not mentioned in Theorem 2, i.e., in
Blocksworld-arm, Depots, Driverlog, Optical-Telegraph, Pipesworld, PSR, and Rovers, one
can construct local minima with arbitrarily large exit distances. The most complicated
example is Optical-Telegraph, where, in difference to Dining-Philosophers, one can construct
situations where the number of process state transitions needed to block one more process is
arbitrarily high. Optical-Telegraph is basically a version of Dining-Philosophers with more
complicated philosophers, that have more freedom of what to do next. This freedom enables
situations where a whole row of philosophers at the table must perform two transitions each
in order to block one more philosopher. Details are in Appendix A.2. A simpler example is
Blocksworld-arm (as well as Depots, in which Blocksworld-arm situations can be embedded).
Consider the following situation. There are n blocks b1 , . . . , bn that initially form a stack
where bi is on bi+1 and bn is on the table. The goal is to build the same stack on top of
another block bn+1 , i.e., the goal is a stack b1 , . . . , bn , bn+1 . Reaching, from the initial state,
a state with better h+ value involves disassembling the entire stack b1 , . . . , bn . During the
disassembling process, h+ increases. Note that this is basically an extended version of the
illustrative example from Figure 4.
As an interesting side remark, note that we have now proved a topological difference
between Blocksworld-arm and Blocksworld-no-arm: in the latter, there are no local minima
at all under h+ , in the former, the exit distance from them can be arbitrarily large. While
this is intriguing, it is not quite clear if there is a general message to learn from it. One might
interpret it as telling us, in a formal way, that encoding details can have a significant impact
on topology, and with that on search performance. FF, for example, is much more efficient
in Blocksworld-no-arm than in Blocksworld-arm. It should be noted, however, that the two
domains differ also semantically, namely in that plans in Blocksworld-no-arm are half as
long as plans in Blocksworld-arm. From a practical point of view, it would be interesting
to explore if this Blocksworld observation can be generalized into encoding methods trying
to model a domain in a way making it best suited for h+ . Some more on this is said in
Section 7.
The positive results regarding benches are these.
Theorem 3 Under h+ , the maximal bench exit distance in the state space of any solvable
instance of Simple-Tsp is 0, Ferry is at most 1, Gripper is at most 1, Logistics is at most
1, Miconic-SIMPLE is at most 1, Miconic-STRIPS is at most 1, Movie is at most 1,
Zenotravel is at most 2, Satellite is at most 4, Schedule is at most 5, Tireworld is at most
6, and Dining-Philosophers is at most 31.
In Simple-Tsp, Ferry, Gripper, Logistics, Miconic-STRIPS, Movie, and Tireworld,
Lemma 4 can be directly applied. Determining what actions have (no) relaxed-plan relevant delete effects is easy in all the domains; in Tireworld it is somewhat complicated to
see when, at the latest, such an action can be applied in an optimal plan. For MiconicSIMPLE, similar arguments as in Lemma 4 suffice. For Zenotravel, Satellite, Schedule, and
Dining-Philosophers, the respective bounds were shown above already.
710

Where “Ignoring Delete Lists” Works

Note that, in Simple-Tsp, we proved that there are no local minima and that the exit
distance is 0. This implies that h+ is, in fact, identical to the real goal distance: the entire
state space consists of contours and global minima.
Our topological distinctions divide planning domains into a taxonomy of classes which
differ in terms of the behavior of their state spaces with respect to h+ . A visualization of
the taxonomy, with the results for the 30 investigated domains, is given in Figure 5.

Blocksworld−arm
Depots
Driverlog

Pipesworld
PSR

Rovers
Optical−Telegraph

Mystery
Mprime
Miconic−ADL
Freecell
Assembly
Airport

mbed <= c

mlmed <= c

Hanoi [0]
Blocksworld−no−arm [0]
Fridge [0]
Grid [0]
Briefcaseworld [0]

Logistics [0,1]
Ferry [0,1]
Gripper [0,1]
undirected

Tireworld [0,6]
Satellite [4,4]
Zenotravel [2,2]
Miconic−SIMPLE [0,1]
Miconic−STRIPS [0,1]
Movie [0,1]
Simple−Tsp [0,0]
harmless

Dining−Phil. [31,31]
Schedule [5,5]

recognized

unrecognized

Figure 5: A planning domain taxonomy, overviewing our results.
The taxonomy, as shown in Figure 5, has two dimensions. The x-axis corresponds to the
four dead end classes. The y-axis corresponds to the existence or non-existence of constant
upper bounds on the local minimum exit distance, and on the bench exit distance. Note
that this visualization makes the simplifying assumption that the domains with bounded
bench exit distance are a subset of the ones with bounded local minimum exit distance. This
assumption is not justified in general, but holds true in our specific collection of domains.
Also, the question whether there is a bound on the difficulty of escaping benches does not
seem as relevant when, anyway, it can be arbitrarily difficult to escape local minima.16 The
specific bounds proved for the individual domains are given in parentheses, local minimum
exit distance bound preceding bench exit distance bound in the cases where there are both.
The bottom right corner of the taxonomy is crossed out because no domain can belong to
the respective classes.17
16. Similarly, when benches can be arbitrarily large it is not as relevant if or if not the local minima are
small or non-existent. In that sense the respective results for Briefcaseworld, Fridge, Grid, Blocksworldno-arm, and Hanoi are only moderately important. Still they constitute interesting properties of these
domains.
17. By Proposition 1, the existence of unrecognized dead ends implies the non-existence of constant upper
bounds on the local minimum exit distance, given there are no states with gd(s) 6= 0 but h+ (s) = 0. Such
states can exist, but only if the domain features derived predicates that appear negated in the negation

711

Hoffmann

What Figure 5 suggests is that h+ -approximating heuristic planners are fast because
many of the common benchmark domains lie in the “easy” regions of the taxonomy. More
concretely, as described in the introduction, when provided with the h+ function, FF’s
search algorithm enforced hill-climbing is polynomial in the domains located in the lowermost classes of the taxonomy (i.e., in domains with constant bounds on both maximal
exit distances). From a more empirical perspective, the distinction lines in the taxonomy
coincide quite well with the practical performance of FF. FF excels in 11 of the 12 domains
that belong to the lowermost classes of the taxonomy (the more difficult domain is DiningPhilosophers, whose upper bound is exceptionally high). In the 5 “middle” domains (no
local minima but potentially large benches) FF performs well, but does not scale up as
comfortably as in the easier domains. As for the more complex domains: Blocksworld-arm,
Depots, Driverlog, Optical-Telegraph, Pipesworld, and PSR are amongst the most challenging domains for FF. In Mprime and Mystery, FF performs just as bad as most other
planners. In Freecell and Miconic-ADL, FF is among the top performing planners, but often
runs into unrecognized dead ends in the larger instances (for example, the larger Freecell
instances used at AIPS-2000). In Airport, Assembly and Rovers, FF performs pretty well in
the respective competition example suites; however, in these domains the competition suites
hardly explore the worst-cases of the domain topology (details on this are in Appendix A).
We do not discuss in detail the relation between the taxonomy and the empirical performance of all the other heuristic planners that make use of an h+ approximation in one
or the other way. One observation that can definitely be made is that all these planners
have no trouble in solving instances from the domains with the most extreme h+ properties.
In Simple-Tsp, Ferry, Gripper, Logistics, Miconic-SIMPLE, Miconic-STRIPS, and Movie,
to some extent also Zenotravel, all such planners scale up very comfortably. In particular,
they scale up much more comfortably in these domains than they typically do in the other
domains, at least without additional (for example, goal ordering) techniques.
In the next section, we treat the connection between the taxonomy and FF’s performance
in a more analytical way, by relating the properties of h+ to properties of FF’s approximation
of h+ , called hF F . Before we do so, some remarks on the relation of the taxonomy to
complexity theory are in order. The question is whether there is a provable relation, i.e., a
relation between the distinction lines in the taxonomy, and the complexity of deciding
plan existence in the respective domains. We were able to construct an NP-hard domain
(a domain where deciding plan existence is NP-hard) where h+ does not yield any local
minima; the maximal bench exit distance in that domain is, however, unbounded. We tried,
but we were not able to come up with an NP-hard domain that has constant bounds on
both maximal exit distances. It remains an open question whether such a domain exists
or not. If the answer is “yes”, then the lowermost classes of the taxonomy form a group
of domains that are worst-case hard, but typically very easy to solve (at least as far as

normal form of the goal condition. But even then, in the presence of unrecognized dead ends there
would be “fake”-global minima, i.e., global minima consisting of non-solution states, in fact consisting
of unrecognized dead ends.

712

Where “Ignoring Delete Lists” Works

reflected by the hitherto benchmarks). If the answer is “no”, then we have identified a very
large polynomial sub-class of planning.18
Talking about polynomial sub-classes, an intriguing observation can be made here about
the trivial heuristic function returning, for a state s, the number of goals that are not true
in s. Let’s call this function hG . With a little thinking, one realizes that, in fact, all the
12 domains where we proved constant bounds on both maximal exit distances under h+ also
have such constant bounds under hG . On the other hand, for the remaining 18 of the 30
domains (except Miconic-ADL) it is easy to see that there are no constant bounds for hG .
In Logistics, for example, clearly the maximum number of steps needed to achieve one
more goal is 12: 4 steps each (move, load, move, unload) within a package’s origin city,
between the origin city and the destination city, and within the destination city. In DiningPhilosophers, for example, the upper bound for h+ was, as said, proved as a corollary of
an upper bound for hG . In Blocksworld, for example, clearly it can take arbitrarily many
steps to achieve one more goal, namely if a block that must be moved is buried beneath n
other blocks that do not need to be moved.
While the above observation appears rather significant at first sight, it is probably not
very important, neither in theory nor in practice. For one thing, it is a coincidence that,
here, the set of domains with both constant bounds under h+ is the same as the set of
domains with both constant bounds under hG . A simple counter example for the general
case is a “graph-search” domain, where the task is to find a path between two nodes in a
directed graph, using the obvious “at”-predicate and “connected”-predicate based encoding.
There, h+ is equal to the real goal distance (since one never needs to move back), while
hG can, clearly, be arbitrarily bad. For another thing, while domains like Logistics have
constant exit distance bounds under hG , these bounds are too large to be practically useful.
For example, with h+ , FF needs to look at most 2 steps forward in each breadth-first
search iteration of enforced hill-climbing, in any Logistics instance. With hG , breadth-first
searches up to depth 12 would be needed. So, at most, the observation regarding hG is a
noteworthy statement about the current planning benchmarks. It remains an open question
whether the (coincidental) correspondence between the bounds for h+ , and for hG , in the
investigated 30 domains, can be exploited for, e.g., detecting such bounds automatically.

5. Relating h+ to hF F
Our discussion relating h+ to hF F is structured in two separate sections. The first one
briefly discusses provable relations between h+ and hF F . The second section summarizes
the results of a large-scale empirical investigation aimed at identifying to what extent the
topological properties of h+ , in the benchmarks, get preserved by hF F .
5.1 Provable Relations between h+ and hF F
One thing that is very easy to observe is that the behavior of h+ and hF F is provably the
same with respect to dead ends, i.e., both heuristics return ∞ in the same cases. This
is simply because both heuristics return ∞ in a state s iff there is no relaxed plan for s.
18. Presumably, to prove the latter, one would need to characterize that class in a purely syntactic manner
on the level of PDDL definitions, since h+ is derived directly from the PDDL syntax. The author’s wild
guess it that this is not going to work, and that the answer is “yes”.

713

Hoffmann

For h+ this follows by definition. For hF F it follows from the completeness, relative to the
relaxation, of the algorithm that computes relaxed plans (Hoffmann & Nebel, 2001a). That
algorithm is a relaxed version of Graphplan (Blum & Furst, 1995, 1997). In each state s,
FF runs Graphplan on the task where s is the initial state, and the delete lists of all actions
are empty. Without delete lists, Graphplan is guaranteed to terminate in polynomial time.
If Graphplan terminates unsuccessfully, then hF F (s) is set to ∞. Otherwise, the number of
actions in the returned plan is taken as the heuristic value hF F (s) of the state.19 Graphplan
is a complete algorithm – it terminates successfully if and only if there is a plan – and so
hF F is set to ∞ iff there is no relaxed plan for s. It follows that the dead end classes of the
benchmarks are the same under h+ and hF F .
The relaxed plans found by Graphplan have (just as in general STRIPS) the property
that they are optimal in terms of the number of parallel time steps, but not in terms of the
number of actions. So, in general, hF F is not the same as h+ (even if P is the same as NP).
FF uses the following heuristic techniques for action choice in relaxed Graphplan, aiming
at minimizing the number of selected actions (Hoffmann & Nebel, 2001a). First, if a fact
can be achieved by a NOOP (a dummy action propagating a fact from time step t to time
step t + 1 in Graphplan’s planning graph), then that NOOP is selected. This guarantees
that every non-NOOP action is selected at most once (of course, selected NOOP actions are
not counted into the relaxed plan). Second, if there is no NOOP available then an action
with minimal precondition weight is chosen, where “weight” is defined as the summedup indices of the first layers of appearance (in the planning graph) of the precondition
facts. Third, actions selected at the same parallel time step are assumed to be linearized
by order of selection; so an action a selected after a0 will be assumed to achieve a fact
p ∈ add(a) ∪ pre(a0 ) even if a and a0 are selected at the same parallel time step.
There are two very restrictive sub-classes of STRIPS in which hF F is provably the same
as h+ . The first demands that every fact has at most one achiever.
Proposition 2 Let (A, I, G) be a STRIPS planning task so that, for all facts p, there is at
most one action a ∈ A with p ∈ add(a). Then, for all states s in the task, h+ (s) = hF F (s).
Proof: The proposition follows from the observation that, when running relaxed Graphplan, the only choice points are those for action selection; these choice points will always
be empty or unary in our case. This implies that all actions selected by Graphplan are
contained in any relaxed plan. In more detail, the latter can be proved by an induction
over the regression steps in relaxed Graphplan. Let s be a state for which there is a relaxed
plan. At the top level of the regression, actions a are selected to support all goals that are
not contained in s. These goals need to be supported in any relaxed plan, and there are
no other actions for doing so. The same holds true for the preconditions of the selected
actions: if p ∈ pre(a) is not in s, then a supporter must be present in any relaxed plan,
and that supporter will be selected by relaxed Graphplan. Iterating the argument, we get
the desired property. The claim then follows because, as proved by Hoffmann and Nebel
(2001a), relaxed Graphplan selects every action at most once.
2
19. Note that this is an estimate of sequential relaxed plan length. The length of the planning graph built by
Graphplan corresponds to the optimal length of a parallel relaxed plan, an admissible heuristic estimate.
However, as indicated before, such heuristic functions have generally not been found to provide useful
search guidance in practice (see, for example, Haslum & Geffner, 2000; Bonet & Geffner, 2001b).

714

Where “Ignoring Delete Lists” Works

Our second sub-class of STRIPS demands that there is at most one goal, and at most
one precondition per action.
Proposition 3 Let (A, I, G) be a STRIPS planning task so that |G| ≤ 1 and, for all a ∈ A,
|pre(a)| ≤ 1. Then, for all states s in the task, h+ (s) = hF F (s).
Proof: Under the given restrictions, relaxed planning comes down to finding paths in the
graph where the nodes are the facts, and an edge is between p and p0 iff there is an action
with pre(a) = p and add(a) = p0 (empty preconditions can be modelled by a special fact
node that is assumed to be always true). A state has a relaxed plan iff it makes a fact node
true from which there is a path to the goal node. Relaxed Graphplan identifies a shortest
such path.
2
The prerequisites of Propositions 2 and 3 are maximally generous, i.e., when relaxing one
of the requirements, one loses the h+ (s) = hF F (s) property. To obtain sub-optimal relaxed
plans with Graphplan, i.e., to construct cases where h+ (s) 6= hF F (s), it suffices to have one
fact with two achievers, and either two goal facts or one action with two preconditions. The
following is such an example. There are the facts g1 , g2 , p, and p0 . The goal is {g1 , g2 }, the
current state is empty. The actions are shown in Figure 6.
name
opg1
opg2 -p
opg2 -p0
opp
opp0

=
=
=
=
=

(pre,

add,

del)

({p},
({p},
({p0 },
(∅,
(∅,

{g1 },
{g2 },
{g2 },
{p},
{p0 },

∅)
∅)
∅)
∅)
∅)

Figure 6: Actions in an example task where hF F 6= h+ .
The optimal relaxed plan here is hopp, opg1 , opg2 -pi. However, Graphplan might choose
to achieve g2 with opg2 -p0 , ending up with the (parallel) relaxed plan h{opp, opp0 }, {opg1 ,
opg2 -p0 }i. Note that each action has only a single precondition, only a single fact has more
than one achiever, and there are only two goals. A similar example can be constructed for
the case where there is only one goal but one action with two preconditions.
Obviously, the syntax allowed by either of Propositions 2 or 3 is far too restrictive to
be adequate for formulating practical domains.20 We did not investigate whether there are
any more interesting situations where h+ and hF F are the same; our intuition is that this
is not the case.
A different question is whether there are provable relations between h+ and hF F in (some
of) the 30 benchmark domains considered in the h+ investigation. We did not investigate
this question in detail – note that such an investigation would involve constructing detailed
20. We remark that the syntax identified by Proposition 3 is a sub-class of a tractable class of STRIPS
planning identified by Bylander (1994). In Bylander’s class, a constant number g of goal facts is allowed,
where g can be greater than 1; the preconditions may be positive or negative.

715

Hoffmann

arguments about all the individual domains, which is clearly beyond the scope of this paper.
None of the domains is captured by either of Propositions 2 or 3. A few results that are
easy to obtain are the following. In Simple-TSP, Movie, and Miconic-STRIPS, h+ and hF F
are the same. This follows from the extremely simple structure of these domains, where
finding step-optimal relaxed plans with Graphplan always results in relaxed plans with an
optimal number of actions. However, even in the only slightly more complicated domains
Ferry, Gripper, Logistics, Miconic-SIMPLE, and Zenotravel, one can easily construct states
where Graphplan’s relaxed plans may be unnecessarily long. In Miconic-STRIPS this does
not happen because there is only a single vehicle (the lift), with no capacity restrictions
(on the number of loaded objects, i.e., passengers). With several vehicles and transportable
objects, as can occur in Logistics and Zenotravel (as well as Driverlog, Depots, Mprime,
Mystery, and Rovers), the difference between h+ and hF F can become arbitrarily large.
Just imagine that n objects must be transported from l to l0 , and n vehicles are available
at l. For parallel relaxed planning, it makes no difference if a single vehicle transports all
objects, or if one different vehicle is selected per individual object. In particular, even with
FF’s action choice heuristics in relaxed Graphplan, hF F may be 2n + 1 just as well as 3n.21
In Ferry and Gripper, where there is only a single vehicle (with capacity restrictions), it
may be that there is an upper bound on the difference between h+ and hF F ; we did not
check that in detail.
In spite of the above, the author’s personal experience from developing FF is that, at
least in relatively simply structured domains with not many different operators/different
ways to achieve facts, the relaxed plans found by relaxed Graphplan are typically pretty
close to optimal. There are, presumably, the following two reasons for this. First, the
employed action choice heuristics. For example, in the Grid domain, a relaxed plan may
choose to pick up a key k with the sole purpose of dropping it again when picking up
another key k0 with a pickup-and-lose action (c.f. Appendix B.12). This does not happen
when selecting actions with minimal precondition weight (the pickup-and-lose action has a
higher weight than the pickup action unless one already holds k in the considered state).
Second, many of the published benchmark instance suites are quite restricted. In Logistics,
for example, the situation outlined above, n objects and n vehicles waiting at a location l,
can not happen for trucks because there is only a single truck in each city. As for airplanes,
in the published benchmark instances there usually are only few of these, and so n will be
small.
5.2 Empirical Relations between h+ and hF F
In a large-scale empirical investigation (Hoffmann, 2003b), it turned out that hF F typically
preserves the quality of h+ . The investigation was aimed at verifying, in those domains
where h+ has some “positive” topological property (for example, yielding no local minima),
to what extent that property is inherited by hF F . We considered 20 benchmark domains,
namely the same domains as in the paper at hand, except the 10 IPC-3 and IPC-4 domains.
21. One could circumvent this particular phenomenon by, when selecting an action in relaxed Graphplan,
employing a minimization of the summed up weight of the preconditions of all actions selected so far. It
is a topic for future work to explore if this has any effect on FF’s performance.

716

Where “Ignoring Delete Lists” Works

Note that, of the latter 10 domains, only three, namely Dining-Philosophers, Satellite, and
Zenotravel, have positive topological properties.
The experimental approach was to take samples from state spaces (a technique adapted
from work by Frank et al., 1997). More precisely, the method was the following. Of each
domain, a random generator was used to produce a large set of example instances. The
instances were grouped together according to the values of the domain parameters, i.e., the
input parameters to the generator (for example, number of floors and number of passengers
in Miconic-SIMPLE). Then, for each single instance, 100 states were sampled, i.e., 100
random sequences of actions were executed in the initial state, where the sequence length
was chosen randomly in the interval between 0 and 2 times FF’s plan length.22 Of each
resulting state s, the exit distance ed(s) was computed by a breadth-first search, and another
search determined whether s was located on a valley, i.e., whether there was no path from
s to a goal state on which the hF F value decreased monotonically.23 The maximal exit
distance of an instance was approximated as the maximum over the exit distances of the
sample states. For every group of instances, the mean number of states on valleys, and the
mean maximal exit distance, were computed. The results were visualized by plotting these
values over the scaling domain parameters. We give some examples for this directly below,
after summarizing the overall results.
The results of the experiment strongly suggested that hF F typically preserves the quality
of h+ , in the considered benchmark domains. Of the 13 domains in which h+ provably yields
no local minima, almost no sample states were located on valleys except in 2 domains,
namely Grid and Hanoi. More precisely, in the 11 other domains the experiment considered
a total of 230 groups of random instances; in one of these groups, 5.0% of the sample states
lay on valleys, in another group it were 2.2%, in another eight groups it were below 1.0%,
and in the remaining 220 groups not a single valley state was found. As for the maximal
exit distance from benches, of all the tested instances of domains in which there is a bound
under h+ , only a single sample state had an exit distance larger than that bound, namely
an exit distance of 2 instead of 1 in the Logistics domain.24
Blocksworld-no-arm
Gripper
Hanoi
Tireworld

0.0
0.0
0.0
0.0

0.0
0.0
0.0
0.0

0.0
0.0
96.0
0.0

0.1
0.0
100.0
0.0

0.0
0.0
100.0
0.0

Figure 7: Percentage of sample states on valleys. Mean values for a linear increase of the
respective domain parameter.
Figure 7 provides the results regarding sample states on valleys, in those considered
domains where there are no local minima (and thus no valleys) under h+ , and where in22. We tried a few other sampling strategies and found that they did not make much difference in terms of
the obtained results.
23. Intuitively, each local minimum lies at the bottom of a valley. We used valleys in the experiment since
it may be hard to find a local minimum state by sampling.
24. The author’s guess is that the results of a similar empirical investigation in Dining-Philosophers, Satellite,
and Zenotravel would be similar, i.e., that the sampled maximal exit distances would hardly increase
above the upper bounds proved for h+ .

717

Hoffmann

stances are characterized by a single domain parameter (Movie and Simple-Tsp are left out
since there hF F is provably the same as h+ ). In Blocksworld-no-arm, that parameter is
the number of blocks (plus randomization of initial and goal states); in Gripper it is the
number of balls to be transported; in Hanoi it is the number of discs; in Tireworld it is the
number of flat tires. In each domain, from left to right the table entries correspond to a
linear increase in the domain parameter (2 . . . 11 blocks, 1 . . . 100 balls, 3 . . . 10 discs, and
1 . . . 5 tires, respectively). Obviously, the only domain that does not “behave” is Hanoi –
where h+ isn’t a very useful heuristic anyway, yielding very large benches, c.f. Section 4.
Blocksworld-no-arm
Gripper
Hanoi
Tireworld

0.3
1.0
6.0
6.0

1.8
1.0
23.0
6.0

2.8
1.0
12.0
6.0

3.8
1.0
2.0
6.0

3.7
1.0
2.0
2.0

Figure 8: Sampled maximal exit distance. Mean values for a linear increase of the respective
domain parameter.
Figure 8 shows the results regarding the sampled maximal exit distance in domains
characterized by a single domain parameter. In Gripper and Tireworld, the sampled values
respect the bound that is valid for h+ (in the largest Tireworld example, sampling did not
find a maximum state in the rather large state space). By comparison, the sampled values
in Blocksworld-no-arm, where there is no bound for h+ , show a clear increase. Again, the
behavior of Hanoi is odd.
Figure 9 shows (part of) the results for a domain that is characterized by more than one
domain parameter, namely Logistics. In domains with at least two domain parameters, the
experimental method was to run one experiment for each pair of them. In each experiment,
all parameters except the respective pair was set to some fixed value. The data could then
be visualized in 3-dimensional plots like the ones in Figure 9. In the figure, the parameters
scaled are the number of cities and the number of objects (“packages”) to be transported;
the parameter range is 1 . . . 9 in both cases. City size and number of airplanes are both
fixed to 3. Of each parameter value combination, 10 random instances were generated (and
100 states were sampled per instance). No valley states were found, except with 3 cities
and 9 objects, where 2 of the 1000 sample states were located on a valley. With 5 cities and
3 objects, in a single instance one sample state had exit distance 2, rather than the bound
1 valid for h+ – the single such bound violation found in the entire experiment.25
As indicated before, the Grid domain was, with Hanoi, the only domain for that the
experiment suggested a major difference between the topologies of h+ and hF F . Large
fractions of the sample states, up to 62.4%, were located on valleys. There was a clear
tendency of increase of the percentage, both with increasing grid size and with increasing
number of keys to be transported.
All in all, the experiment confirmed that, in all of the Blocksworld-no-arm, Briefcaseworld, Ferry, Fridge, Gripper, Logistics, Miconic-SIMPLE, and Tireworld domains, hF F
25. The decrease in the mean sampled maximal exit distance for very large parameter values suggests that
it becomes harder, for sampling, to find the maximum states in the rather large state spaces.

718

Where “Ignoring Delete Lists” Works

Z

Z

1

2

0.5

1

0

0
9

9

7
1

7
1

5

3
X

5

3

Y
5

Y
5

3
7

X

9 1

(a)

3
7

9 1

(b)

Figure 9: Mean sampled valley percentage (a) and maximal exit distance (b) in Logistics,
when scaling cities (x-axis) against objects (y-axis).
largely preserves the quality of h+ (no local minima and/or a constant bound on the maximal exit distance from benches). Remember that Miconic-STRIPS, Movie, and Simple-Tsp
are three more domains where this, provably, applies.

6. Towards Automatically Detecting h+ Phenomena
The lemmas presented in Section 3 provide a natural starting point for investigations into
domain analysis techniques trying to detect the topological phenomena automatically. Such
domain analysis techniques would be useful for configuring hybrid systems, i.e., for the
automatic selection of heuristic functions that are likely to be well-suited for solving a given
planning task. Further, such techniques would be useful for avoiding the need to re-do
the h+ investigation for every single new planning domain. Finally, on the basis of such
analysis techniques one may be able to compute good lower bounds on h+ , and with that an
informative admissible heuristic function. Some more discussion of these points is contained
in Section 7.
The question to be addressed is if, to what extent, and how, the application of the
lemmas from Section 3 can be automated, i.e., if and how one can automatically check
whether their prerequisites are satisfied in a given STRIPS task. In the section at hand,
we present a preliminary attempt we made to do that. While the attempt was not very
successful, we believe that the investigation has value in showing up what one can achieve
with some simple analysis techniques, and what weak points would be needed to be improved
upon in order to obtain better results.
Invertible (or at least invertible) actions, and actions with irrelevant delete/static add
effects, are syntactically defined in Section 3 and thus easy to “detect”. The only difficulty
is to find inconsistencies between facts. While this is as hard as planning itself, there are
several approximation techniques in the literature (for example, Blum & Furst, 1995, 1997;
Fox & Long, 1998; Gerevini & Schubert, 2000, 2001; Rintanen, 2000), which tend to work
very well, at least in the current benchmarks. The challenge is to find more syntactical
characterizations of actions that are respected by the relaxation, and of actions that have
719

Hoffmann

no relaxed-plan relevant delete effects. Now, in many domains where these phenomena
occur, such as for example Ferry, Gripper, Logistics, Miconic-STRIPS, Movie, Simple-Tsp,
and Tireworld, intuitively when one looks at the domains the causes of the phenomena
seem similar. But when getting down to the actual syntax of these domain descriptions,
the individual details are very different and it becomes very difficult to get a hold on the
common ground. There does not seem to be a simple syntactical definition that captures
the behavior of the actions in all these domains; at least we did not find such a syntactical
definition. Instead, we tried to reason about the “additive structure” of the domains, and
its possible “interactions” with the delete effects. (The intuition being that, in the domains
with very simple h+ topology, the interactions aren’t very harmful.) We captured the
additive structure of a domain/of an instance in a data structure called fact generation
trees. The next subsection describes this data structure and its basic properties, then a
subsection gives our results in an extreme case of h+ topology, then a subsection outlines a
somewhat more advanced analysis technique we developed.
6.1 Fact Generation Trees
The fact generation tree, short FGT, to a planning instance is basically the AND/OR tree
that results from a regression search starting at the goals, when ignoring the delete effects of
the actions. Tree nodes are labelled with facts and actions alternatingly. Fact nodes are OR
nodes – they represent a choice of achieving actions – and action nodes are AND nodes –
their preconditions represent sets of facts that must be achieved together. We assume a goal
achievement action, as known from, for example, the description of UCPOP (Penberthy &
Weld, 1992). That action is the root (AND) node of the FGT, and the top level goals form
its sons. Obviously, the sons of a fact node are all the actions that achieve the fact, and
the sons of each action node are all the precondition facts of the action. (For the sake of
simplicity, we stayed in a pure STRIPS framework in this investigation.) Tree structures of
this kind were, for example, described and used by Nebel, Dimopoulos, and Koehler (1997)
in their work on automatically detecting irrelevant facts and operators. Note that the FGT
does not take account of the interactions that may arise when trying to achieve the facts
below an AND node together. As an effect of ignoring the delete lists, the FGT treats all
these facts completely separately.
We terminate the FGT by applying the following two rules.
1. Say we just inserted an action node N (a) labeled with action a. If there is a fact
p ∈ pre(a) so that a fact node labeled with p occurs on the path from the root node
to N (a), then N (a) is pruned.
2. Say we just inserted, as a son of an action node N (a), a fact node N (p) labeled with
fact p. If there is an action a0 with p ∈ pre(a0 ), so that an action node labeled with
a0 occurs on the path from the root node to N (a), then N (p) is pruned.
Intuitively, the rules disallow the generation of branches in the FGT that would be redundant for a relaxed plan. Formally, we call a relaxed plan non-redundant if no strict subsequence of it is still a relaxed plan (i.e., no action can be omitted). Every non-redundant
relaxed plan, for every (not necessarily reachable) state, can be embedded into a connected,
rooted, and non-redundant sub-tree of the FGT built in the way described above. We will
720

Where “Ignoring Delete Lists” Works

be more precise after introducing the illustrative example in Figure 10, that we will use
throughout this section.

E

at E

== 1 EUR

mv D E

D
+= 1 EUR

B

C
at D

1 EUR

A
mv B D

mv C D

at B

at C

mv D C

mv A B

at A

Figure 10: Sketch and FGT of the illustrative example.
In the example, the task is to reach location E. The available actions are moves along
(bi-directional) graph edges in the obvious encoding using an “at” predicate, except the
move from D to E, which requires as an additional precondition that we be in possession
of 1 EUR. We can acquire the 1 EUR as an add effect of the action that moves from D to
C. The main part of Figure 10 shows the FGT to the example, the picture in the top left
corner illustrates the example by showing its road map graph and an indication of the role
of the 1 EUR constructs. The root node, i.e., the artificial goal-achievement action, is not
included in the figure, for simplicity. Due to termination rule 1, (for example) moving from
E to D is not included as a son of the fact node labeled “at D” (the precondition “at E” is
the root node). Due to termination rule 2, “at D” is not a son of the action node labeled
“mv D C” (“at D” already occurs as a precondition of “mv D E” above).
Every action in a non-redundant relaxed plan (to some arbitrary state) achieves some
unique “needed” fact that is not achieved by any preceding action, and that is needed for
the goal or for the precondition of a subsequent action.26 It is not overly difficult to prove
that one can thus embed such a relaxed plan into the FGT by processing the relaxed plan
from back to front, associating each action with the corresponding node below a needed
fact added by the action, starting at the goal facts. The resulting sub-tree is connected and
rooted in the sense that actions are only associated with consecutive AND nodes, starting at
the root node. The sub-tree is non-redundant in the sense that, of every OR node, at most
26. This observation was made by, for example, Hoffmann and Nebel (2001b), where it is used to detect
actions that do not participate in any non-redundant relaxed plan, and that thus do not need to be
considered in the heuristic computations done by planners such as FF or HSP.

721

Hoffmann

one son gets associated with an action. Termination rule 1 is valid since a fact that is needed
at the end of a relaxed plan can not also be needed at its start. Termination rule 2 is valid
since for every needed fact there is at least one representative node in the corresponding subtree. For illustration, consider the different locations in the graph underlying the example
in Figure 10. If one is located, for example, at A and does not have the 1 EUR, then the
entire FGT except the “mv C D” node corresponds to the sub-tree for a non-redundant
relaxed plan. This sub-tree is obtained as follows. The relaxed plan is “mv A B”, “mv B
D”, “mv D C”, “mv D E”. The needed facts added by these actions are “at B”, “at D”, “1
EUR”, and “at E”, respectively. Starting from the goal fact “at E”, first “mv D E” gets
associated with the respective action node. Then the fact nodes “at D” and “1 EUR” –
the preconditions of the action just dealt with – become open, and “mv B D” as well as
“mv D C” get associated with the respective node below their respective needed fact. As a
consequence of the “mv B D” action, fact node “at B” becomes open, and “mv A B” gets
associated with the action node below it. Then the process stops. If, in the current state,
one is, for example, located at C with the 1 EUR, then the process selects the sub-tree that
consists of the “mv C D” and “mv D E” nodes only.
Every non-redundant relaxed plan in the instance, in particular every optimal relaxed
plan in the instance, corresponds to a sub-tree of the FGT. The FGT being a summary of all
possible relaxed plans in that sense, our idea is to examine the FGT for harmful interactions
– conflicts – with the potential to appear in a relaxed plan. The hope is to be able to draw
conclusions from the non-existence/restricted form of conflicts to topological properties of
h+ . We next outline an extreme case analysis of this kind, namely one that postulates the
absence of any conflicts in the FGT. Note here that, in difference to the situation in the
illustrative example, in general the FGT can contain action/fact labels in multiple nodes.
The worst-case size of the FGT is exponential in the size of the instance description. So, to
design practically usable domain analysis techniques, one would need to approximate the
FGT, instead of building it completely. This aspect is not treated at all in what follows,
where our objective is (only) to find implications between FGT structure and h+ topology
in the first place.
6.2 Interaction-free Planning Tasks
Think of a conflict as a situation where one part of a (non-redundant) relaxed plan can
hinder the execution/success of another part of the relaxed plan. If there are no such
conflicts, then every (non-redundant) relaxed plan is executable in reality, implying that
h+ is equal to the real goal distance (which of course implies that there are no local minima
etc). In the investigated 30 benchmark domains, this is the case (only) in Simple-Tsp, which
we use as a motivating example.
We define three kinds of conflicts in the FGT. We call two action nodes, labeled by
actions a and a0 , allied if they participate together in a non-redundant sub-tree, i.e., they
can occur together in the embedding of a relaxed plan, but are not descendants of each
other. (This is the case iff the paths from the root node to a and a0 separate in an AND
node.) Our first kind of conflicts is given by a pair of allied action nodes labeled a and a0 ,
where a deletes a precondition of a0 . Second kind of conflicts, a pair of action nodes labeled
a and a0 , where a is a descendant of a0 , and a deletes a precondition of a0 that is not added
722

Where “Ignoring Delete Lists” Works

by any action on the path from a to a0 . Third kind, an action node labeled a, where a
deletes a goal fact that is not added by any action on the path from a to the respective root
node.
If there are no conflicts in the FGT, then we call the task interaction-free. It is relatively
easy to see that, without conflicts, for every non-redundant relaxed plan (for every nonredundant sub-tree of the FGT) there is an execution order that works in reality. So h+
equals goal distance in interaction-free tasks.
In the illustrative example from Figure 10, the only conflict in the FGT is that between
the nodes “mv D C” and “mv D E” – these nodes are allied, and “mv D C” deletes the
precondition “at D” of “mv D E”. Note that this conflict does indeed capture the reason
why h+ is not equal to goal distance in the example. In order to be able to move from D to
E, one has to first move from D to C and get the 1 EUR. Doing the latter deletes the “at
D” precondition of the former. But in the relaxation, after the move from D to C, one is
located in both D and C at the same time, and so the relaxed plan needs one step less to
achieve the goal (from all states where the move to C has yet to be done).
An example of a domain with interaction-free tasks is the graph-search domain mentioned earlier, where the tasks demand to find a path between two nodes in a directed
graph, using the obvious “at”-predicate and “connected”-predicate based encoding. (Our
illustrative example above becomes an instance of this domain if one removes the “1 EUR”
constructs.) We can even come up with a purely syntactic criterion that captures this
example domain.
Proposition 4 Let (A, I, G) be a STRIPS planning task so that
1. |G| ≤ 1,
2. for all a ∈ A: |pre(a)| ≤ 1, and
3. for all a ∈ A: del(a) ⊆ pre(a).
Then (A, I, G) is interaction-free.
Proof: Due to prerequisites 1 and 2, the AND nodes in the FGT all have at most one son.
This implies that there are no allied action nodes. Together with prerequisite 3 and our
termination rule 1, it implies that no action node can delete the goal fact, or the precondition
fact of an ancestor node.
2
Instances of the graph-search domain fulfill the prerequisites of Proposition 4 if the
static “connected” facts are removed prior to planning. Note that the syntax identified by
Proposition 4 is a subset of the syntax identified by Proposition 3, and thus in such tasks
hF F is identical to h+ , and, since h+ is identical to the real goal distance, plan existence
can be decided in polynomial time. Intuitively, this is because the captured syntax can not
express more than the graph-search domain: plans in a task qualifying for Proposition 4
correspond exactly to paths in the graph where the nodes are the facts, and the edges go
from preconditions to add effects. The same is true for relaxed plans.
The instances of the Simple-Tsp domain are not interaction-free. There are conflicts
in the FGT between pairs of actions achieving different “visited” goals. For example, say
723

Hoffmann

there are three locations to visit, l1 , l2 , and l3 . The action nodes “mv l1 l2 ” and “mv l1
l3 ” are allied since they achieve the goals “visited l2 ” and “visited l3 ” that both participate
in the root AND node. But these actions mutually delete their precondition, “at l1 ”, so
they constitute a conflict in the FGT. If they appear together in a relaxed plan, then that
relaxed plan is not executable in reality (unless the relaxed plan happens to move back to
l1 in between). Observe, however, that after the execution of, for example, “mv l1 l2 ”, one
can replace “mv l1 l3 ” with “mv l2 l3 ” and so repair the conflict in the relaxed plan. All
conflicts in Simple-Tsp FGTs behave this way.
In general, we say that a conflict between allied action nodes a and a0 can be repaired if
there is an action a00 such that pre(a00 ) ⊆ (pre(a) ∪ add(a)) \ del(a) (thus a00 can be executed
after a), and add(a00 ) ⊇ add(a0 ) (thus a00 achieves what a0 should have achieved). Similar
repairable cases can be identified for the two other kinds of conflicts. If all conflicts in the
FGT can be repaired, then to any non-redundant relaxed plan there is a relaxed plan of the
same length that is executable in reality, and so again h+ equals goal distance. This is the
case in the Simple-Tsp domain.
We made a preliminary implementation of the above FGT analysis techniques. The
implementation correctly detects that in Simple-Tsp instances (as well as in graph-search
instances), h+ equals goal distance. In Simple-Tsp, with less than 18 locations the analysis
takes only split seconds; with more than 18 locations, the runtime taken explodes fairly
quickly.
6.3 A More Advanced Analysis
While the above results are encouraging, the technique’s applicability – the h+ topology it
can detect – is clearly far too severely restricted. It turns out extremely difficult to find
less restrictive implications from FGT structure to h+ topology, i.e., sufficient criteria for
weaker topological properties. The best we could come up with is a criterion that implies
the non-existence of local minima under h+ , and that holds true in the Movie domain and
in some extremely simple Logistics instances.
The idea behind the criterion is the following. To imply the non-existence of local
minima under h+ , it suffices to know that, in every state s, there is a starting action a of
an optimal solution so that h+ (Result(s, hai)) ≤ h+ (s). Say we are considering a planning
task where all actions are (at least) invertible. Let s be a state and a be the starting action
of an optimal solution from s. If there is an optimal relaxed plan for s that contains a, then
we are done with the argument used in Lemma 3. Else, let P + be an optimal relaxed plan
for s that does not contain a. P + can be embedded in a sub-tree of the FGT. If a does not
delete any leaf nodes of that sub-tree – any facts that P + assumes to be true in the state of
execution – then P + is a relaxed plan for Result(s, hai) and we are done, too. The case left
open is when a does delete a leaf node of the sub-tree occupied by P + . Observe that this
does not matter if we have that there are no (or only repairable) conflicts in the sub-tree.
Then, P + is executable in reality, so P + is an optimal plan for s, so the starting action of
P + falls into the first case above and we are done again. We get the following sufficient
criterion:
724

Where “Ignoring Delete Lists” Works

There are no local minima under h+ if for all actions a it holds that a is at least
invertible, and for all non-redundant sub-trees of the FGT that do not contain a, either a
does not delete a leaf of the sub-tree, or the sub-tree does not contain any conflicts.
To test this criterion, all one needs to do is to consider the (redundant) sub-tree of the FGT
where the only branches left out are those that start in nodes labeled with a. If this sub-tree
contains a conflict, and a deletes some fact occurring in the sub-tree, then the criterion does
not apply. Otherwise, if the test succeeds for all actions, it is proved that there are no local
minima under h+ .
Reconsider the illustrative example from Figure 10, where as said above the only conflict
in the FGT is that between the nodes “mv D C” and “mv D E”. Any sub-tree that does not
contain one of these nodes is conflict-free. So “mv D C” and “mv D E” do not violate the
above criterion. Neither do “mv B D”, “mv C D”, and “mv A B” violate the criterion, since
none of these actions deletes a fact occurring anywhere else but in its own precondition.
However, for “mv B A” and “mv D B” the sub-tree looked at is the entire FGT including
the conflict, and both these actions delete a fact that occurs in the FGT. So the criterion
does not apply to our illustrative example. Note that “mv B A” and “mv D B” never start
an optimal plan so really they could be left out of the considerations; but it is unclear how
to detect this automatically, in a general way.
A remark on the side is in order here. If an action a does not appear in the FGT, then,
in difference to what one may think at first sight, this does not imply that a does not appear
in an optimal plan. Our FGT termination rules, while adequate for relaxed planning, are
too restrictive for real planning. The following is an example. There are the facts g1 , g2 ,
and p. The goal is {g1 , g2 }, the current state is {g1 }. The actions are shown in Figure 11.
name
opp
opg2
opg1

=
=
=

(pre,

add,

del)

({g1 },
(∅,
({p},

{p},
{g2 },
{g1 },

∅)
{¬g1 })
∅)

Figure 11: Actions in an example task where the FGT does not contain an action (opp,
namely) needed in reality.
The only optimal plan here is hopp, opg2 , opg1 i: in order to be able to re-achieve g1
after applying opg2 , we must achieve p first. However, opp does not appear in the FGT.
The only location in the FGT where a node N labeled with opp could be inserted is as a
son of the precondition node p of opg1 , which is inserted as a son of g1 . But N is pruned
by termination rule 1, because opp has g1 in its precondition, and g1 appears on the path
from the root node to N . Note that, indeed, opp is never part of a relaxed plan because
achieving p is only good for re-achieving g1 if that is deleted by other actions necessary to
reach the goals.
Our implementation of the criterion given above easily – within split seconds – proves
the non-existence of local minima in Movie instances, regardless of the size of the instance.
725

Hoffmann

The technique does not, however, work in any other domain we tried, except Logistics
instances where there is only a single city, with only two locations in it, only a single truck,
and only a single package to be transported. Note that this is even simpler than the small
illustrative example used in Section 3, where two objects need to be transported.
It is an open question how better results can be achieved, i.e., how more state spaces can
be recognized to not feature any local minima under h+ . Our feeling is that the backward
chaining approach to domain analysis is promising. But, to be successful, the analysis
technique should probably invest much more effort into analyzing the way in which the
goals can be achieved, and with how many steps, rather than doing just the very crude
FGT approximation. With more information available about how goals can be achieved,
maybe it would be possible to discover non-trivial cases in which actions are respected by
the relaxation.27 As for detecting actions that have no relaxed-plan relevant delete effects,
it is yet completely unclear to us how this could be accomplished.

7. Discussion
We have derived a formal background to an understanding of what classes of domains relaxed plan-based heuristic methods, the most wide-spread methods in the modern planning
landscape at the time of writing, are well suited for. The formal approach taken is to
identify characteristics of the local search topology – the heuristic cost surface – under the
idealized heuristic function h+ , in a forward searching framework. For 30 commonly used
benchmark domains including all competition examples, i.e., for basically all STRIPS and
ADL benchmark domains that are used in the field at the time of writing, we proved what
the relevant topological properties are. The results coincide well with the runtime behavior
of FF. Indeed, empirical results suggest that the quality of h+ is often preserved in FF’s
approximation of it.
The results are interesting in that they give a rare example of a successful theoretical
analysis of the connections between typical-case problem structure, and search performance.
From a more practical point of view, the results provide a clear picture of where the strengths
and weaknesses of h+ lie, and so form a good basis for embarking on improving the heuristic
in the weak cases. Approaches of this kind have already appeared in the literature (Fox &
Long, 2001; Gerevini et al., 2003). Most particularly, Fast-Downward’s heuristic function
(Helmert, 2004) is motivated by observations regarding unrecognized dead ends under h+
in the Mystery domain, and large benches in transportation domains with non-trivial road
maps.
Regarding the relevance of our topological results for forward search algorithms other
than enforced hill-climbing, note that things like the non-existence of unrecognized dead
ends or the non-existence of local minima are certainly useful for any heuristic search
algorithm, albeit not in the form of a provable polynomiality result.28 More generally,
the relevance of the topological results for the performance of planners using other search
27. We remark that it is not easy to find even trivial syntactical restrictions under which actions are, in
general, respected by the relaxation. For example, even when every fact is added by only a single action,
one can construct cases of non-respected actions. One such case is the example from Figure 11, where
opp is not respected by the relaxation.
28. Except in the case where the heuristic function identifies the precise goal distances, which is the case for
h+ in 1 of the 30 domains, namely, the Simple-Tsp domain.

726

Where “Ignoring Delete Lists” Works

paradigms, or enhanced heuristics, like LPG and Fast-Downward, is a matter needing further investigation. One thing that is certainly clear is that, in the easiest classes of the
taxonomy, particularly in domains where in the state space there are no local minima under h+ , and benches can be escaped in a single step, any planner using an approximation of
h+ is likely to work quite well. Indeed that’s what one observes in practice. The intuition
of the author is that the topology of h+ plays a large role for the efficiency of these planners
more generally, i.e., also in other domains. Proving or disproving this is beyond the scope
of this paper. In any case, our investigation provides a nice theoretical background with
proved results in an idealized setting, and these results can be used as a starting point into
investigations tailored to individual systems other than FF.
Our investigation considers solvable planning tasks only, which is well justified by the
focus set in the international planning competitions. Turning the focus on unsolvable tasks,
one realizes that much of our techniques and results become useless. In a search space
with no solution, the only difference a heuristic function can make lies in the states with
infinite heuristic value, i.e., in the states recognized as dead ends. Which means that
the only interesting question remaining is for what kinds of dead end states there is no
relaxed plan. What do the results herein tell us about this? In those domains where we
identified unrecognized dead ends, the results tell us that relaxed plans are a too generous
approximation.29 In the other domains, things look more hopeful. Still, these results are
relative to solvable instances. Whether or not h+ will detect many of the dead end states
in unsolvable tasks will depend on what reasons there can be for such states. The dead
ends in unsolvable tasks may be caused by other reasons than those in solvable tasks,
since the assumptions making the tasks solvable are not given. Note that many of the
benchmarks (for example, Blocksworld and Logistics) do not have any unsolvable instances
in their standard definition. To some extent, this makes the existence or non-existence of
unrecognized dead ends a choice of the domain designer extending the domain definition.
Exploring these issues in detail is a topic for future work.
Talking about future work, the biggest drawback of this research in its current form is,
obviously, that it needs to be re-done for every single new planning domain. It would be very
desirable, but turns out to be very hard, to come up with more generic – ideally, automatic
– methods to determine the topological properties of a domain. We have outlined an
attempt we made to develop such automatic methods, based on analyzing properties of fact
generation trees. We presented some first promising results, but regarding the applicability
to domains of the complexity one would like to be able to handle, our methods are yet far
too weak. It is left for future research to answer the question if there are approaches to
the topic that work better in practice. As said, our intuition is that there are such better
approaches, based on more intelligent backchaining-style reasoning about how the goals can
be achieved in a domain. But, at the time of writing, this is pure speculation.
Beside easening the burden of doing all the proofs by hand, the benefits of automatic domain analysis techniques would be twofold. First, an ambitious long-term vision in domainindependent planning is to have an arsenal of complementary heuristics, and combine these
into a hybrid system that can automatically be configured to best suit a given arbitrary
planning task. The contribution made towards this vision by the results at hand is a very
29. Unsurprisingly, seeing as deciding plan existence is NP-hard in, for example, Mystery, Mprime, MiconicADL, and Freecell (Helmert, 2003).

727

Hoffmann

clear picture of where the strengths of h+ lie; to be able to automatically configure a hybrid system, one would need multiple heuristics with different strengths and weaknesses
(i.e., heuristics that are of high quality in different classes of domains), as well as the ability
to determine automatically what heuristic is likely to work best. (At least such an approach could be more cost-effective, beside being much more insightful, than just trying out
all possible combinations of techniques.)
Another benefit from enhanced domain analysis techniques might lie in the ability to
generate a high-quality admissible heuristic function for sequential planning. In many domains, optimal relaxed plans mostly consist of actions of which it is easy – for a human
– to see that they (or one of a set of similar actions) must be contained in any optimal
relaxed plan (for example, all the loading and unloading actions that can’t be avoided in a
transportation task). So the number of such actions in a state could provide a good lower
bound on the value of h+ . Note that this phenomenon – actions that must be contained in
every relaxed plan – is a stronger version of the notion of actions that are respected by the
relaxation. A promising approach seems to be to try to detect the former as a sufficient
approximation of the latter.
Since we observed that there are arbitrarily deep local minima under h+ in Blocksworldarm, but none in Blocksworld-no-arm, one might try to come up with encoding methods
trying to model a domain in a way making it best suited for h+ . Since Blocksworld-no-arm
is basically a version of Blocksworld-arm where all possible pairs of consecutive actions
(pickup-stack, unstack-stack, unstack-putdown) were replaced with macro-actions, a good
(but somewhat obvious) heuristic for modeling is probably to choose the domain granularity
on as high a level of abstraction as possible. More insightful heuristics may be obtained
when considering the h+ topology in planning benchmarks enriched with automatically
detected macro actions (Botea, Müller, & Schaeffer, 2004, 2005).
Apart from the above, the most important future direction is the adaption of the formal
framework, and of the theoretical analysis methods, to the temporal and numeric settings
dealt with in modern planning benchmarks and in modern planning systems. The needed
adaptations are straightforward for the numeric framework used in Metric-FF (Hoffmann,
2003a). As for temporal planning, if the objective function estimated by the heuristic is the
number of actions needed to complete the partial plan, then the adaptation of the framework
is probably straightforward as well. If, however, makespan is estimated by the heuristic,
then most of what is said in this article does not apply. At most, in such a setting our
analysis techniques could be relevant if the search uses an estimation of remaining action
steps as a secondary heuristic.

Acknowledgments
I would like to thank Drew McDermott, Fahiem Bacchus, Maria Fox, and Derek Long
for their responses to various questions concerning the definitions of/intentions behind the
competition domains. I also thank the anonymous reviewers, whose comments helped to
improve the paper.

728

Where “Ignoring Delete Lists” Works

Appendix A. Proof Sketches
We list the proof sketches in sections concerning dead ends, local minima, and benches, in
that order.
A.1 Dead Ends
Theorem 1 The state space of any solvable instance of
1. Blocksworld-arm, Blocksworld-no-arm, Briefcaseworld, Depots, Driverlog, Ferry,
Fridge, Gripper, Hanoi, or Logistics is undirected,
2. Grid, Miconic-SIMPLE, Miconic-STRIPS, Movie, Pipesworld, PSR, Satellite,
Simple-Tsp, Tireworld, or Zenotravel is harmless,
3. Dining-Philosophers, Optical-Telegraph, Rovers, or Schedule is recognized under evaluation with h+ .
Most of the proofs are simple applications of Lemma 1 or 2. As said, descriptions of the
domains can be looked up in Appendix B.
Proof Sketch: [Theorem 1]
All actions in Blocksworld-arm, Blocksworld-no-arm, Driverlog, Ferry, Gripper, Hanoi,
and Logistics instances are invertible, so we can apply Lemma 1 and are finished. The
inverse actions are the obvious ones in all cases, like stacking/unstacking a block onto/from
some other block, loading/unloading an object onto/from a vehicle, or moving from l to
l0 /moving from l0 to l (in the case of Driverlog, the latter can always be done as the
underlying road map is bi-directional, c.f. Appendix B.8). In the Briefcaseworld, Depots,
and Fridge domains, while the actions do not strictly obey the definition of being invertible
(neither that of being at least invertible), they still invert each other in an obvious way,
i.e., for every state s and applicable action a there is an action a so that Result(s, ha, ai) = s.
In Movie, actions getting snacks have irrelevant delete effects and static add effects, while
rewinding the movie and resetting the counter are at least invertible. A Simple-Tsp action
moving from l to l0 is at least invertible by moving back. In Tireworld, to all working steps
there is an inverse one, except to inflating a wheel. But that has irrelevant delete effects
and static add effects. In Miconic-STRIPS, moving a lift is invertible, boarding a passenger
is at least invertible, and departing a passenger has irrelevant delete effects and static add
effects. In all the four domains, Lemma 2 can thus be applied. In the Miconic-SIMPLE and
Grid domains, while the actions do not strictly adhere to the relevant definitions, similar
arguments like Lemma 2 prove the non-existence of dead ends. In Miconic-SIMPLE, moving
the lift is invertible. Letting passengers in or out of the lift can not be inverted, but those
actions need to be applied at most once (similar to static add effects), and they do not
interfere with anything else (similar to irrelevant deletes). In Grid, to all actions there is an
inverse action, except opening a lock. The latter action excludes only other actions opening
the same lock (similar to irrelevant deletes), and each lock needs to be opened at most once,
as locks can not be closed (static add effects). In Zenotravel and Satellite, all facts can be
re-achieved but sometimes one has to apply several actions to do so. In Zenotravel, after
729

Hoffmann

flying an airplane from l to l0 , to get back to l0 one might have to refuel the airplane on
top of flying it back. In Satellite, after switching an instrument on, one might have to recalibrate it, which can always be done but can involve several actions (turning the satellite
into the right direction before applying the actual calibration action). In Pipesworld, any
push action is inverted by the respective pop action, and vice versa. The state space is not
undirected since the pushs/pops for non-unitary pipeline segments are split into two parts.
In PSR, there are no dead end states since one can always reach a goal state by waiting,
if necessary, then opening all breakers, then bringing the (non-breaker) devices into a goal
position, then closing the needed breakers.
In Dining-Philosophers, dead ends arise only when a process (a philosopher) has initiated an impossible reading or writing command (from/to an empty/a full queue) – the
queue contents can then not be updated, and no more actions are applicable. (The derived
predicate rules that determine if a process is blocked do not apply in this case, since they
require that no read/write command has been initiated yet.) Obviously, with no applicable
actions there is no relaxed plan either. In all other states, the goal can be reached by
traversing individual process state transitions until all philosophers have one fork, and try
to take up the other.
In Optical-Telegraph, dead ends arise in two kinds of situations. First, when a process
has initiated an impossible reading or writing command, similarly as in Dining-Philosophers,
there are no applicable actions and thus no relaxed plan. The second possibility is that the
two processes in a pair may take different decisions of where to go next in their communication sequence: one may decide to stop data exchange, while the other may decide to
send or receive more data. In such a situation, at least one of the processes is in a state
where it has two transitions available, has already activated one of these transitions, and
might have already initiated the respective write/read command. The write/read command
is impossible (since the other process took a different decision), and no more actions are
applicable for that process. The derived predicate “blocking” rules do not apply to the
process, because they never apply in process states with more than one available transition.
So neither a real nor a relaxed plan exist for the state. From all other reachable states,
the goal can be reached by traversing individual process state transitions until all pairs of
communicating processes occupy one control channel, and try to write into the other.
In Rovers, there is a plan to a state if and only if, for all soil/rock samples and images
that need to be taken, there is a rover that can do the job, and that can communicate the
gathered data to a lander. The only chance to run into a dead end is to take a soil/rock
sample with a rover that can not reach a lander (the soil/rock sample is available only once).
But then, there is no relaxed plan to the state either.
In Schedule, any state s with gd(s) < ∞ can be solved by applying, to each object o in
turn, a certain sequence of working steps. If the sequence can not be applied for some object
o then it follows that the preconditions of a needed action are not fulfilled, which must be
the case because o is not cold in s (a “do-roll” action has been applied to o previously,
making o hot). No operator can make o cold again, i.e., no operator adds the respective
fact. Thus there is no relaxed plan for s either.
2
Note that the worst cases in Theorem 1 can occur, i.e., in the domains whose instances
are harmless, there can be directed state transitions, and in the domains whose instances are
730

Where “Ignoring Delete Lists” Works

recognized, there can be dead ends. We remark that the dead ends in Dining-Philosophers
and Optical-Telegraph are due to what seem to be bugs in the encoding of the queues
(whose contents aren’t always updated correctly) and of the blocked situations (whose rules
for detection seem to be incomplete). Modifying the operators in a straightforward way to
fix these (apparent) bugs, one gets dead-end free (harmless) state spaces.
The domains not mentioned in Theorem 1 are Airport, Assembly, Freecell, MiconicADL, Mprime, and Mystery. In all these domains, one can construct arbitrarily deep
unrecognized dead ends. In Airport, unrecognized dead ends arise when two planes move
towards each other on a line of segments, with no possibility of changing the direction.
Such deadlock situations aren’t recognized by relaxed planning since, in the relaxation, the
free space left between the two planes remains free, and can be used to navigate the planes
“across” each other. The dead end becomes arbitrarily deep when, independently of the
deadlock situation, other planes can still be moved. We remark that, in reality – and in
the IPC-4 example instances – deadlock situations like this rarely occur. Airplanes are only
movable along standard paths that serve to avoid such deadlocks on the main connecting
routes of the airport. The only places on the airport where deadlocks can occur, both in
reality and in the IPC-4 example instances, are near the parking areas, where space can
be dense, and airplanes need to move in both directions on the same airport segment. If
no deadlocks can occur at all, i.e., if all planes can move to their target positions one after
the other without hindering each other, then h+ delivers the exact goal distance. This is
presumably the reason why the heuristic planners performed very well in the IPC-4 Airport
test suites. The performance would probably become worse if one were to use (unrealistic)
instances with excessively many potential deadlock situations.
In Assembly, unrecognized dead ends can arise when several objects are “stuck” due to
complex ordering constraints, which imply that any solution plan would need to go through
a cyclic assembly pattern. The details are rather complicated, and the interested reader is
referred to the TR (Hoffmann, 2003c). It can be proved that, unless the ordering constraints
in an Assembly instance have the potential to yield a cyclic situation, there are no dead
ends at all. In all but one of the IPC-1 competition instances, the ordering constraints do
not have this potential. This helps to explain how FF can be so efficient in that test suite
(it solves even the largest task within half a second search time, finding a plan with 112
steps).
In Freecell, unrecognized dead ends can arise, for example, when one is not cautious
enough about moving cards into the free cells. A relaxed plan can still achieve the goal
with a single free cell, using that cell as an intermediate store for all cards. In reality,
however, moving a card into a free cell occupies space (by deleting the availability of the
free cell), and can thus exclude possibilities of reaching the goal. Thus moving a card into
a free cell can lead into an unrecognized dead end state. The unrecognized dead end can be
arbitrarily deep when other cards can still be moved around independently of the deadlock
situation.
In Miconic-ADL, unrecognized dead ends arise when a problem constraint is violated,
but this violation goes unrecognized by the relaxed plan. An example is when two passengers
p1 and p2 are in the lift, such that p1 can only be transported downwards, p2 has no access
to p1 ’s destination floor, and p2 ’s destination floor is below p1 ’s. The state is a dead end
because one can not let p1 get out first – p2 has no access to the respective floor – but
731

Hoffmann

neither can one let p2 get out first – afterwards, the lift would need to drive upwards,
which it can’t with p1 on board. In the relaxation, one can stop at both destination floors
“simultaneously” because the at-facts are not deleted. The unrecognized dead end becomes
arbitrarily deep when several other passengers can be moved around before reaching p1 ’s
destination floor.
In Mystery, unrecognized dead ends arise when fuel is scarce, and a vehicle makes suboptimal moves. The relaxed plan can achieve the goal as long as all relevant locations are
still accessible at least once. But that may not suffice in reality. The dead end becomes arbitrarily deep when additional objects can be transported independently of the problematic
situation. Mprime behaves similarly. The only difference to the Mystery example is that, to
avoid the possibility of transferring fuel items to the problematic locations, one must make
sure that there is just enough fuel to enable the transportation of the additional objects.
A.2 Local Minima
Theorem 2 Under h+ , the maximal local minimum exit distance in the state space of any
solvable instance of
1. Blocksworld-no-arm, Briefcaseworld, Ferry, Fridge, Grid, Gripper, Hanoi, Logistics,
Miconic-SIMPLE, Miconic-STRIPS, Movie, Simple-Tsp, or Tireworld is 0,
2. Zenotravel is at most 2, Satellite is at most 4, Schedule is at most 5, DiningPhilosophers is at most 31.
We present the proof sketch to Theorem 2 in terms of three groups of domains with
similar proofs. Note that the domains where the maximal local minimum exit distance is 0
are domains where there are no local minima at all. We first focus on the domains where
Lemma 3, or slight extensions of it, can be applied.
Proof Sketch: [Theorem 2, Ferry, Fridge, Gripper, Logistics, Miconic-SIMPLE, MiconicSTRIPS, Movie, Simple-Tsp, Tireworld]
With Theorem 1, none of the listed domains contains dead ends. As said in the proof
sketch to the theorem, all actions in the Ferry, Gripper, Logistics, Miconic-STRIPS, Movie,
Simple-Tsp, and Tireworld domains are either at least invertible, or have irrelevant delete
effects. With Lemma 3 it suffices to show that all actions are respected by the relaxation.
In all cases, except the driving/flying actions in Logistics, it is very easy to see that any
optimal starting action does something that can not be avoided in the relaxed plan. (For
example, the relaxed plan can not avoid to load/unload objects onto/from vehicles, and
it can not avoid missing working steps in Tireworld.) If an optimal starting action a in
Logistics drives a truck/flies an airplane to some location l, then some object must either
be loaded or unloaded at l, so a relaxed plan from s has no choice but to apply some action
that moves a transportation vehicle (of a’s kind) there. All vehicles are equally good, except
when there is a clever choice, i.e., a vehicle that already carries objects to be unloaded at
l. But then, a will move one of those vehicles just like an optimal relaxed plan will, and all
such vehicles are equally good in the relaxation. (In Ferry, Gripper, and Miconic-STRIPS,
732

Where “Ignoring Delete Lists” Works

there is only a single vehicle, which makes the moving actions in these domains easier to
reason about.)
In the Fridge and Miconic-SIMPLE domains, the actions do not adhere strictly to the
definitions of invertibility and irrelevant delete effects. But the proof to Theorem 1 has
shown that they have similar semantics, i.e., they can either be inverted, or delete only
facts that are no longer needed once they are applied. Furthermore, all actions in these
domains are respected by the relaxation. In Fridge, missing working steps must also be
done in the relaxed plan. In Miconic-SIMPLE, lift moves are trivially respected, and lift
stops are respected since clever choices in reality coincide with clever choices in the relaxed
plan.
2
In the next four domains, there are no local minima either, but the proofs are more
sophisticated and make use of rather individual properties of the respective domains. In all
cases it is proved that there is a path to the goal on which h+ does not increase.
Proof Sketch: [Theorem 2, Blocksworld-no-arm, Briefcaseworld, Grid, Hanoi]
With Theorem 1, none of these domains contains dead ends. In Blocksworld-no-arm,
if an optimal starting action a stacks a block into its goal position, then a also starts
an optimal relaxed plan (because there is no better thing to do than to achieve this goal
immediately); in that relaxed plan, a can be replaced with its inverse counterpart to form
a relaxed plan for the successor state. If there is no such action a in a state s, then one
optimal plan starts by putting some block b – that must be moved in order to access a block
below it – from some other block c onto the table, yielding the state s0 . A relaxed plan
for s0 can be constructed from a relaxed plan P + for s by, taking account of various case
distinctions, replacing the move actions regarding b in P + with the same number of other
such move actions. The case distinctions are about what kind of action P + uses to move b
away from c – one such action a0 must be contained in P + . If a0 moves b to the table then
we can replace a0 in P + with the action that moves b back onto c, and are finished. Else, we
must distinguish between the cases where b is required to be on c for the goal, or on some
other block. In both cases, we can make successful use of the fact that b can be moved from
any position to any other position within a single action, enabling us to exchange actions
in P + quite flexibly.
In Briefcaseworld, all actions can be inverted. Actions that put objects into the briefcase
are trivially respected by the relaxation. In a state s where an optimal plan starts with a
take-out action, an optimal relaxed plan for s can also be used for the successor state, since
taking out an object does not delete important facts. In a state s where an optimal plan
starts with a move action from l to l0 , and P + is a relaxed plan for s, a relaxed plan for
the successor state can be constructed by replacing moves from l to l00 , l00 6= l0 , in P + , with
moves from l0 to l00 .
In Grid, a rather complex procedure can be applied to identify a flat path to a state
with better h+ value. In a state s, let P + be an optimal relaxed plan for s, and let a be the
first unlock action in P + , or a putdown if there is no such unlock action. Identifying a flat
path to a state s0 where a can be applied suffices, because unlocking deletes only facts that
are irrelevant once the lock is open, and the deletes of putting down a key are irrelevant if
there are no more locks that must be opened. The selected action a uses some key k at a
733

Hoffmann

position p. P + contains a sequence of actions moving to p. Moving along the path defined
by those actions does not increase h+ since those actions are contained in the relaxed plan,
and they can be inverted. If k is already held in s, then we can now apply a. If the hand is
empty in s, or some other key is held, then one can use P + to identify a flat path to a state
where one does hold the appropriate key k. If the hand is empty, then P + must contain a
sequence of actions moving to a location where k can be picked up. If some other key is
held, then P + must contain sequences of actions moving between locations where a series
of keys are picked up and put down, where the key series ends with picking up k.
In Hanoi, it can be proved that the optimal relaxed solution length for any state is equal
to the number of discs that are not yet in their final goal position – proceeding from the
smallest to the largest disc, each respective goal can be achieved with a single action. No
optimal plan moves a disc away from its final position, so h+ does not increase on optimal
solution paths.
2
We finally consider those four domains where there are local minima, but one can always
escape them within a constant number of steps. In all cases, we prove an upper bound d on
the distance of any non-dead end state s to a state s0 with h+ (s0 ) < h+ (s). This immediately
implies that d − 1 is an upper bound on the maximal local minimum exit distance (it also
implies that d − 1 is an upper bound on the maximal bench exit distance; the results will
be re-used in Appendix A.3).
In Dining-Philosophers, h+ is only loosely connected to goal distance, and the bound,
which holds even for the trivial heuristic function returning the number of yet un-blocked
philosophers, follows from the rather constant and restrictive domain structure. In the other
three domains, the proofs proceed as follows. For a reachable state s, we identify a constant
number of steps that suffices to execute one action a in the optimal relaxed plan for s, and,
without deleting a’s relevant add effects, to re-achieve all relevant facts that were deleted
by a. Then, a state s0 with h+ (s0 ) < h+ (s) is reached.
Proof Sketch: [Theorem 2, Dining-Philosophers, Satellite, Schedule, Zenotravel]
By Theorem 1, the dead ends in Dining-Philosophers are all recognized. In any non
dead end state s, the shortest relaxed plan blocks all processes (philosophers) that are not
yet blocked. For each individual process, at most 3 steps are needed – in the relaxation,
to block a process it always suffices to activate a state transition, to initiate a read/write
command, and to do the queue update. After the update, the queue is both empty and full,
and the read/write is impossible in the sense that the “blocking” rules apply. (With this, a
process can block itself in the relaxation, and the h+ value is only fairly loosely correlated
with the true goal distance.) Thus, to reach a state with lower h+ value, obviously it always
suffices to block one more process. We prove our upper bound by determining a constant
bound on the number of steps needed to do that. Such a bound exists because, beside the
fact that the philosopher processes are constant and can interfere only with their respective
two neighbors at the table, the philosophers have a fixed order in which they try to pick up
the forks: they always first try to pick up the fork to their right, then the fork to their left.
This restricts the possible combinations of internal states of neighbored philosophers.
In more detail, a philosopher is blocked iff he tries to pick up a fork that is not on the
table. For a philosopher p, we refer by pL to p’s neighbor philosopher on the left side. A
734

Where “Ignoring Delete Lists” Works

description of the 5 different states of each philosopher process is in Appendix B.7. Let s
be a non dead end state. Let p be a philosopher that is not blocked in s (if no such p exists,
then s is a goal state and there is nothing to prove). We can prove the desired upper bound
by an exhaustive case distinction over the states of p and pL. For each state i ∈ {1, . . . , 5}
of p, we consider each state iL ∈ {1, . . . , 5} of pL. If the combination of i and iL is not
possible, then we do nothing. Else, we determine a number k of process state transitions
that leads to a state where either: p is blocked and pL is still blocked if it was blocked
in s; or pL is blocked and was not blocked in s. In a few cases, to do so we also have to
make distinctions over the internal state of pL’s left neighbor pLL. The worst case, k = 6,
occurs when i = 3, i.e., when p holds both adjacent forks. Then, pL has to be in either
state iL = 1 or iL = 4 (which means, pL can’t hold the fork between pL and p since that
is held by p). If iL = 4, then pL is not blocked in s; pL can put down its left fork, getting
to state 1 where pL is blocked since it waits to pick up its right fork, held by p. If iL = 1
then we have to distinguish two cases about the state of pLL. We have that i = 3 (p holds
both adjacent forks), and iL = 1 (pL waits to pick up the fork between p and pL); pL is
blocked. Case A, if the state of pLL is 0, 2, or 3, then pLL holds the fork between pLL and
pL. We go with p from 3 to 4, from 4 to 1, and from 1 to 2, and we go with pL from 1 to 2.
Then, both p and pL are blocked since they wait to pick up the fork to their left. Case B,
if the state of pLL is 1 or 4, then the fork between pLL and pL is on the table, and pLL is
not blocked. We go with pLL from 4 to 1 (if in 4), and from 1 to 2. After that, pLL holds
the fork between pLL and pL; we are in case A and can apply that sequence, getting us to
a state where pLL is possibly blocked, and both p and pL are definitely blocked.
We always need at most 6 process state transitions to block one more philosopher.
The process state transitions take 4 planning actions each, and so this makes 24 planning
steps. Some more planning steps are needed due to the subtleties of the PDDL encoding.
Subtlety A, a process may have already decided to go to a state, but not yet arrived there
– i.e., the respective transition is activated and the read/write command is initiated, so
that communication channel/queue is occupied but the transition is not yet complete. At
most 2 steps are needed to reach the next internal state (update the queue and wrap up
the transition). Subtlety B, to be blocked in a state a process must activate its outgoing
transition. In the worst case described above, each of p, pL, and pLL may require the 2
steps induced by subtlety A; both p and pL require the step induced by subtlety B. So all
in all we get to (at most) 32 planning actions. In effect of the last action, one more process
becomes blocked, so an upper bound on the exit distance is 31.
By Theorem 1, there are no dead ends in Satellite. Let s be a reachable state. To
determine an upper bound d on the distance from s to a state s0 with h+ (s0 ) < h+ (s),
one can look at an optimal relaxed plan P + for s, and distinguish four cases regarding the
existence of applicable actions of different types in P + . For each action type, a constant
number of steps suffices to re-achieve the deleted facts after application of the action. The
worst case, d = 5, arises when a switch-on action is applied. Switching on an instrument
deletes the instrument’s calibration. To re-achieve this, one must turn the satellite and
calibrate it. After another turn and taking an image, a state with lower h+ value is reached.
By Theorem 1, the dead ends in Schedule are all recognized. Let s be a non-dead
end state. To determine an upper bound d on the distance from s to a state s0 with
h+ (s0 ) < h+ (s), one can look at an optimal relaxed plan P + for s and distinguish seven
735

Hoffmann

cases regarding the kinds of applicable actions that P + contains. The worst case, d = 6,
arises when only a do-roll action is available (and applicable) in P + . One then needs to
apply a time-step, a do-lathe action to achieve the desired effects of do-roll, another time
step, a do-polish or a do-grind action to re-achieve the previous surface condition, another
time step, and a do-immersion-paint action to re-achieve the previous color.
By Theorem 1, there are no dead ends in Zenotravel. In a reachable state s, to determine
the desired constant d, distinguishing two cases does the job. If the relaxed plan P + for s
contains an applicable boarding, departing, or refueling action, then applying that action
leads into a state with lower h+ value. Else, P + starts with a flying action, and a better
state can be reached by executing the flight, refueling once, and boarding or departing a
person. We get d = 3.
2
Note that the proved bound for Dining-Philosophers holds even if we take the heuristic
function to be the trivial one that returns the number of yet un-blocked philosophers. It is
extremely cumbersome to figure out what exactly the worst-case exit distance is in DiningPhilosophers under h+ – to do so, one has to consider all combinations of possible states of
neighbored processes, and their possible developments over a lot of action steps, in a rather
un-intuitive PDDL encoding made by an automated translation machinery. The highest
exit distance we could actually construct in Dining-Philosophers was 15. We conjecture
that this is a (tight) upper bound.
In Satellite, Schedule, and Zenotravel, the proved upper bounds are tight. In all of
Dining-Philosophers, Satellite, Schedule, and Zenotravel, the bounds are valid for any nondead end state s. So, beside a bound on the local minimum exit distance, these results also
provide a bound on the bench exit distance; we will re-use them below in Appendix A.3.
In Blocksworld-arm, Depots, Driverlog, Optical-Telegraph, Pipesworld, PSR, and Rovers,
one can construct local minima with arbitrarily large exit distances. In Blocksworld-arm,
an example situation is that where n blocks b1 , . . . , bn initially form a stack where bi is
on bi+1 and bn is on the table, and where the goal is to build the same stack on top of
another block bn+1 , i.e., the goal is a stack b1 , . . . , bn , bn+1 . Reaching, from the initial state,
a state with better h+ value, involves disassembling the entire stack b1 , . . . , bn . During the
disassembling process, h+ increases. The same example can be used in Depots.
In Driverlog, local minima can arise due to the different road maps for trucks and drivers,
for example, when it takes one step to drive from a location l to another location l0 , but n
steps to walk. In the relaxed plan, the driver can drive the truck to its goal while himself
staying where he is, but in reality, the driver will have to walk all the way back.
As for Optical-Telegraph, this is treated most easily by reconsidering the DiningPhilosophers domain, for which we proved a constant upper bound above. The reason is
that Optical-Telegraph is basically a more permissive version of Dining-Philosophers, where
the philosophers can choose which fork to pick up first, and, if they hold both forks, which
fork they want to put down again first. Consider the configuration depicted in Figure 12.
That configuration is not reachable given the automata underlying Dining-Philosophers,
but is reachable given the automata underlying Optical-Telegraph.
In Figure 12, Nietzsche holds both adjacent forks, while Kant holds none and tries to
get access to the fork to his right. In between Nietzsche and Kant, there are arbitrarily
many other philosophers that all hold one fork each, and are trying to access the other.
736

Where “Ignoring Delete Lists” Works

Kant

Nietzsche

Figure 12: An unreachable situation in Dining-Philosophers, in which an unbounded local
minimum under h+ would arise. Arrows indicate “pickup”-requests.

The only non-blocked philosopher is Nietzsche, who can put down the forks again. In the
PDDL encoding of this, in the world state where Nietzsche has just activated the transition
putting down the right (or left) fork, the h+ value is 2: in the relaxation, it suffices to
initiate the write command, and to update the queue contents. After the write command
was initiated, however, h+ goes up to 3 because the transition has become non-activated;
so the relaxed plan has to update the queue contents, wrap up the transition, then activate
the (same) transition again. Reaching a state where the h+ value is 1 involves propagating
forks through the entire sequence of philosophers between Nietzsche and Kant, either on the
right hand side, or on the left hand side. For example, say Nietzsche puts down both forks
and then picks up the right fork. Then the philosopher to the left of Nietzsche can pick up
his requested fork (or Nietzsche can pick it up which gets us back to where we started). In
the resulting state, we are in the same situation as before, except that now the philosopher
with the “Nietzsche-role” sits one more position to the left. After iterating the procedure
around the left side of the table, Kant can pick up the requested fork, and request to get the
other, giving us a goal state where all philosophers are blocked. The state with h+ value 1
is the one where Kant has not yet activated the transition to request the other fork.
The configuration in Figure 12 is not reachable in the Dining-Philosophers domain as
used in IPC-4, because, there, a philosopher can not pick up the fork on his left hand
side first – as is done in Figure 12 by all the philosophers between Nietzsche and Kant
on Nietzsche’s left hand side. As said, in Optical-Telegraph the “philosophers” do have
this freedom of choice, and so the situation is reachable. In more detail, as described in
Appendix B.22, in Optical-Telegraph there are n pairs of communicating processes. The
pairs are arranged in a cycle, where between each pair there is a control channel. Internally,
the two processes within each pair can go through a fairly long, heavily interactive, sequence
of operations, implementing the possibility to exchange data between the two stations.
737

Hoffmann

Before these operations can begin, each of the processes has to occupy (write into) one
control channel. That is, one of the processes occupies a channel, then it waits for a signal
from the other process, indicating that the second control channel was occupied as well.
After the data exchange was terminated, the control channels get released (read) in an
arbitrary order. The overall system is blocked iff all process pairs are in the state where
they have occupied one control channel, and are waiting to occupy the other. Thus, the
process pairs correspond exactly to philosophers that can choose which fork to pick up (put
down) first, and Figure 12 provides an example with arbitrarily high exit distance from a
local minimum state. Precisely, the local minimum state is the one where the “Nietzsche”
process pair has just occupied both channels, and the process that blocked the second
channel has just activated the transition sending the “occupied-the-other-one” signal: in
that state, h+ has value 2 (all processes except the active one are blocked).
In Pipesworld, consider the situation where several areas form a circle with unitary
connections. In the local minimum state s, a single goal batch g has to go into an area a;
g is currently in a segment s adjacent to a, a contains a batch b, all other areas are empty.
The shortest plan is to push b into the other segment (not s) adjacent to a, and propagate
the batches around in the circle until g can be pushed into a. The shortest relaxed plan for
s is, however, to push b into s and then push g into s from the other side – i.e., g is used
to push itself into the goal area. Reaching the nearest state with h+ value 1 requires n − 1
steps when there are n areas in the circle, and on the path the h+ value increases. Note that
this example uses neither tankage restrictions, nor interface restrictions, nor non-unitary
pipeline segments.
In PSR, a deep local minimum is given when n breakers each feed an individual goal line,
in a way so that no breaker can feed any other breaker’s goal line without that other breaker
being also closed, and the breakers are all connected to some faulty line. All but one of the
breakers are closed. The h+ value of such a state is 1 (close the single open breaker) since
the only unsatisfied goal condition (beside supplying the line fed by the open breaker) is the
one postulating that no breaker is affected; that condition is a negated derived predicate,
and thus ignored in the relaxation. The only applicable action in the state is to wait. After
that, all breakers are open, and the shortest relaxed plan is to close them all, yielding the
h+ value n. Obviously, the nearest state with h+ value 0 is at least n steps away.30
In Rovers, local minima can arise because taking an image deletes the calibration of
the camera. An example is this. There are n waypoints w1 , . . . , wn connected in a line
(i.e., wi−1 is connected to wi ), a lander at w1 , one rover has a camera c that must be used
to take two images at w1 , and c can be calibrated (only) at wn . When the rover is at w1 ,
and c is calibrated, the relaxed plan is to take the two images and communicate the two
data pieces. But after taking one image, one has to navigate all the way to wn , calibrate
c, and get back. Note that this example makes use of a road map with arbitrarily large
diameter, where the diameter of a Rovers instance is the longest way any rover must travel
in order to get from one waypoint to another. In general, the distance to a state with better
h+ value is bounded by 3d + 2 where d is the diameter of the instance (see the details in
the TR). The road map diameter in the IPC-3 Rovers instances varies around 1 to 6.
30. We remark that this counter-example remains valid in the IPC-4 SIMPLE-ADL and STRIPS formulations
of PSR, which use a different encoding of the derived predicates, not using a negation to formulate the
goal that no breaker is affected.

738

Where “Ignoring Delete Lists” Works

As for the Airport, Assembly, Freecell, Miconic-ADL, Mprime, and Mystery domains, we
have seen in Appendix A.1 that these contain unrecognized dead ends, so, by Proposition 1,
the local minimum exit distance in these domains is unbounded. For Assembly, as the
TR describes in detail, the initial state of an instance has a path to the goal on which
h+ decreases monotonically, unless there are complex interactions between the ordering
constraints present in the instance. None of the IPC-1 instances features such complex
interactions. Assuming that FF’s search algorithm sticks to the monotonically decreasing
paths, this gives another indication as to how the system can be so efficient in that example
suite.
A.3 Benches
Theorem 3 Under h+ , the maximal bench exit distance in the state space of any solvable
instance of Simple-Tsp is 0, Ferry is at most 1, Gripper is at most 1, Logistics is at most
1, Miconic-SIMPLE is at most 1, Miconic-STRIPS is at most 1, Movie is at most 1,
Zenotravel is at most 2, Satellite is at most 4, Schedule is at most 5, Tireworld is at most
6, and Dining-Philosophers is at most 31.
As before, we subdivide the proof sketch to Theorem 3 into groups of domains with
similar proofs. We first consider the transportation-type domains. In all of them, Lemma 4,
or very similar proof arguments, can be applied.
Proof Sketch: [Theorem 3, Ferry, Gripper, Logistics, Miconic-SIMPLE, Miconic-STRIPS]
The proofs to Theorems 1 and 2 have shown that, in all these domains, the actions
are respected by the relaxation, and, in all these domains except in Miconic-SIMPLE, the
actions are either invertible, or have no relevant delete effects. To determine an upper bound
d on the exit distance from benches, we can thus apply Lemma 4. This requires us to show
that, for any state s, there is an optimal plan in that the d + 1th action has no relaxedplan relevant delete effects. In Miconic-SIMPLE, we have seen that the actions, while not
adhering to the syntactic conditions of invertibility and (no) relevant delete effects, have
similar semantics; so the same proof technique can be applied there.
In all the (transportation-type) domains under consideration, the argument is, roughly,
that load-type and unload-type actions have no relaxed-plan relevant delete effects, while
move-type actions need not be applied more than once in a row because all locations are
immediately accessible from each other. This implies an upper bound of 1 on the maximal
exit distance. Concretely, say s is a reachable state in a Logistics instance, a starts an
optimal plan from s, P + is an optimal relaxed plan for s that starts with a, and applying
a to s yields the state s0 . If a is a loading (unloading) action, its only delete is the at(in-) fact of the transported object; as the object is loaded from the respective location
(unloaded from the respective vehicle) only once in the optimal relaxed plan P + , a has no
relaxed-plan relevant delete effects, so s is an exit. Otherwise, if a drives or flies some vehicle
v from l to l0 , then s0 is an exit because an optimal plan for s0 starts by loading (unloading)
some package to (from) v. For Miconic-STRIPS and Miconic-SIMPLE, the same arguments
apply. In Ferry, the arguments also remain valid except that, if the optimal start action
a in the state s boards a car, then this action also deletes the available free space on the
739

Hoffmann

ferry. But then, the relaxed plan P + for s also contains actions that move the ferry to a
location l, and that debark the car at l (otherwise there would be no point in boarding the
car). Placing these actions up front in P + , and removing a, yields a relaxed plan for the
state that results from applying a in s. A similar argument can be applied to prove the
claim for Gripper, where gripper hands can hold only one ball at a time. (Note that the
argument for Ferry and Gripper uses a somewhat weaker notion than relaxed-plan relevant
delete effects, where there are such effects, but they are undone by actions contained in the
relaxed plan.)
2
Next come some non-transportation domains where also Lemma 4 can be applied.
Proof Sketch: [Theorem 3, Movie, Simple-Tsp, Tireworld]
The proofs to Theorems 1 and 2 have shown that in these domains all actions are
respected by the relaxation, and either at least invertible, or have irrelevant delete effects.
We apply Lemma 4 in all cases.
In Movie, all actions have no, and therefore no relaxed-plan relevant, delete effects,
with the single exception of rewinding the movie (which deletes the counter being at zero).
Obviously, no optimal plan rewinds the movie twice in a row. Thus, d = 1 is the desired
upper bound.
In Simple-Tsp, d = 0 suffices. Say we are in a reachable state s where one is at location
l. An optimal plan starts with an action a visiting a yet unvisited location l0 . An optimal
relaxed plan for s is to start with a, then visit each remaining unvisited location l00 by a
move from l0 to l00 . The latter actions do not require preconditions deleted by a, and so a
– every action – has no relaxed-plan relevant delete effects.
In Tireworld, the lowest constant upper bound is d = 6. Some non-final working steps
(like jacking up a hub with a flat wheel on) need to be undone later on, i.e., they have
relaxed-plan relevant delete effects. Other final working steps (like jacking down the hub)
need not be undone, i.e., they have no relaxed-plan relevant delete effects. The longest
sequence of non-final working steps that any optimal plan does in a row is the following
6-step one: open the boot (it must be closed again), fetch the wrench and the jack (they
must be put away again), loose the nuts on a hub that’s got a flat wheel on (the nuts must
be tightened again), jack up the respective hub (it must be jacked down again), and undo
the nuts (they must be done up again). In the resulting state, one can remove the flat
wheel, which needs not be undone.
2
For the remaining domains where Theorem 3 claims a constant upper bound on the
maximal bench exit distance, we have seen in Appendix A.2 that there are upper bounds
on the distance from any reachable state s to a state s0 with h+ (s0 ) < h+ (s). These upper
bounds trivially also imply upper bounds on the maximal bench exit distance.
Proof Sketch: [Theorem 3, Dining-Philosophers, Satellite, Schedule, Zenotravel]
Follows directly from the proof to Theorem 2.

2

For all of the above domains, except the last four, one can easily construct examples
where the bench exit distance is equal to the proved upper bound. For Satellite, Schedule,
740

Where “Ignoring Delete Lists” Works

and Zenotravel, it is an open question whether there are tighter bounds on the bench exit
distance than on the local minimum exit distance; this does not seem particularly relevant,
though. (For Dining-Philosophers, as said above it may be that not even the bound on the
local minimum exit distance is tight.)
For the Blocksworld-no-arm, Briefcaseworld, Fridge, Grid, and Hanoi domains, Theorem 2 proves that there are no local minima. So there it is important to know whether it can
be arbitrarily difficult to escape benches. The answer is “yes” in all cases. In Blocksworldno-arm, the example is the same one that we already used in Blocksworld-arm and Depots
(to show that there are no bounds on the local minimum exit distances). There are n blocks
b1 , . . . , bn that initially form a stack where bi is on bi+1 and bn is on the table, and the goal is
to build the same stack on top of another block bn+1 , i.e., the goal is a stack b1 , . . . , bn , bn+1 .
The shortest relaxed plan for the initial state is n steps long (remove the stack on top of
bn , then move bn onto bn+1 ). The nearest state with h+ value n − 1 is the one where bn has
already been stacked onto bn+1 . That state is n steps away from the initial state.
In Briefcaseworld, the bench exit distance becomes large when many objects must be
taken out of the briefcase – in the relaxation, there is no point in taking objects out, since
moving the briefcase does not delete any at-facts. Consider the state s where n objects
o1 , . . . , on are inside the briefcase at a location l, and the goal is to have o1 , . . . , on at l and
the briefcase at another location l0 . We have h+ (s) = 1: moving the briefcase to l0 suffices
in the relaxation. But the nearest goal state, h+ = 0, is n + 1 steps away: one must take
all the objects out before moving to l0 .
In Fridge, if in a single fridge the compressor is held by n screws, then the exit distance
of the initial state is n + 1. To reach a better state, one must: stop the fridge (which must
be turned back on in the relaxed plan); unfasten the n screws (which must be fastened
again in the relaxed plan); and remove the broken compressor (which needs not be undone
as it only deletes the fact that the broken compressor is attached to the fridge).31
In Grid, consider the instances where the robot is located on a n×1 grid (a line) without
locked locations, the robot starts at the leftmost location, and shall transport a key from
the rightmost location to the left end. The initial value of h+ is n + 2 (walk over to the
key, pick it up, and put it down – the at-facts are not deleted), and the value does not get
better until the robot has actually picked up the key.
In Hanoi, we have seen that h+ is always equal to the number of discs that are not yet
in their goal position. Thus the maximal bench exit distance grows exponentially with the
number of discs. From the initial state in an instance with n discs, it takes 2n−1 steps to
move the first (i.e., the largest) disc into its goal position.
For the 9 domains where the local minimum exit distance can be arbitrarily large, it
is not as relevant whether the bench exit distance is bounded or not. Escaping a bench
might do the planner no better than ending up in a huge local minimum. We remark that,
for example, in Driverlog, Rovers, Mprime, and Mystery, one can easily construct examples
with large bench exit distances, by defining road maps with large diameters – i.e., by using
basically the same example as used above in the Grid domain.
31. In fact, one can easily prove that n + 1 is also an upper bound on the bench exit distance, in Fridge
instances where compressors are held by n screws (details are in the TR).

741

Hoffmann

Appendix B. Domain Descriptions
The following is a list of brief descriptions of the 30 investigated domains. We explain the
overall idea behind each domain, the available operators, and what the initial states and
goals are. In most cases the set of instances is obvious; restrictions, if any, are explained.
We remark that, at some points, the domain semantics seem a bit odd (for example, in
Zenotravel, the only difference between flying and zooming a plane is that zooming consumes
more fuel). The odd points are, presumably, domain bugs that have been overlooked by the
respective domain designers. We have not corrected these bugs as, after all, the investigation
is meant to determine the properties of the benchmarks as they are used by the community.
The domains are listed in alphabetical order.
B.1 Airport
In the Airport domain, the planner has to safely navigate the ingoing and outgoing traffic,
at a given point in time, across an airport. The main problem constraint is that planes
must not endanger each other, which they do if they come too close to each other’s running
engines. The constraint is modeled by letting each plane “block” the segments that its
engines currently endanger. Planes can not enter blocked areas. There are five operators.
A plane can be moved from one airport segment to another, if the plane is facing the right
direction, and no planes get endangered by the action. Similarly, a plane can be pushed
back if that does not cause trouble. One can start up the engines of a plane, let the plane
take off, or let the plane settle at a parking position. The initial state specifies the current
positions and orientations of the planes, the goal specifies which planes are outbound (have
to take off), and which are inbound and to what parking positions.
B.2 Assembly
In the Assembly domain, a complex object must be constructed by assembling its parts
together, obeying certain ordering constraints. The parts themselves might need to be
assembled in the same way beforehand. Some parts are “transient”, which means that they
must be integrated only temporarily. There is a collection of machines, “resources”, which
might be needed by the working steps. There are four operators. An available resource can
be committed to an object, deleting the resource’s availability. Releasing a resource from
an object is the inverse action. An available object x can be assembled into an object y,
if x is either a part or a transient part of y, if all resources that y requires are committed
to y, and if all objects that have an assemble order before x are already incorporated into
y. In effect, x is incorporated into y but no longer available, and y becomes available if all
parts of y except x are already incorporated, and no transient part of y is incorporated. An
incorporated object x can be removed from y, if all resources that y requires are committed
to y, and, given x is a transient part of y (a part of y), if all objects with a remove order
(an assemble order) before x are incorporated (not incorporated). In effect, x is available
but no longer incorporated, and y becomes available if all parts of y are incorporated, and
all transient parts of y except x are not incorporated. In the instances, the part-of relation
forms a tree where the goal is to make the root object of the tree available. Also, the
742

Where “Ignoring Delete Lists” Works

assemble and remove order constraints are consistent (cycle-free). These restrictions hold
true in the AIPS-1998 competition examples.
B.3 Briefcaseworld
In Briefcaseworld, a number of portables must be transported, where the transportation is
done via conditional effects of the move actions. There are three operators. Putting in a
portable at a location can be done if the portable and the briefcase are at the respective
location, and the portable is not yet inside. Taking a portable out can be done if it is inside.
A move can be applied between two locations, and achieves, beside the is-at-fact for the
briefcase, the respective at-facts for all portables that are inside (i.e., the portables inside
are moved along by conditional effects). The goal is to have the briefcase, and a subset of
the portables, at their goal locations.
B.4 Blocksworld-no-arm
Blocksworld-no-arm is a variant of the widely known Blocksworld domain. There are three
operators. One can move a block from the table onto another block. One can move a block
from another block to the table. One can move a block from another block onto a third
block. The initial state of an instance specifies the initial positions of the blocks, the goal
state specifies a (consistent, i.e., cycle-free) set of on facts.
B.5 Blocksworld-arm
The instances of Blocksworld-arm are the same as those of Blocksworld-no-arm. The difference is that blocks are moved via a single robot arm that can hold one block at a time.
There are four operators. One can pickup a block that is on the table. One can put a block,
that the arm is holding, down onto the table. One can unstack a block from some other
block. Finally, one can stack a block, that the arm is holding, onto some other block.
B.6 Depots
The Depots domain is a kind of mixture between Logistics and Blocksworld-arm. There is
a set of locations, a set of trucks, a set of pallets, a set of hoists, and a set of crates. The
trucks can transport crates between locations, the hoists can be used to stack crates onto
other crates, or onto pallets. There are six operators, to move a truck between (different)
locations, to load a crate that is held by a hoist onto a truck at a location, to unload a
crate with a hoist from a truck at a location, to lift a crate with a hoist from a surface (a
pallet or a crate) at a location, and to drop a crate that is held by a hoist onto a surface at
a location. A hoist can hold only one crate at a time. The crates are initially arranged in
arbitrary stacks, where the bottom crate in each stack is standing on a pallet. The goal is
to arrange the crates in some other arbitrary stacks on (possibly) other pallets, which can
involve transporting crates to other locations (as pallets can not be moved).
743

Hoffmann

B.7 Dining-Philosophers
Dining-Philosophers is an encoding of the well-known Dining-Philosophers problem, where
the task for the planner is to find the deadlock situation that arises when every philosopher
has taken up a single fork. The PDDL domain was created by an automatic translation
from the automata-based Promela language. The automata are also referred to as processes.
In Promela, each philosopher is a finite automaton/process that works as follows. From the
start state, state 0, a transition puts the right fork onto the table (this is just an initialization
step), getting him to state 1. Then there is a loop of four states. From state 1 to state
2, the philosopher takes up the right fork. From 2 to 3, he takes up the left fork, 3 to 4
he puts down the right fork, in state 4 he puts down the left fork and gets back to state
1. Each such process communicates with each of its neighbors through a communication
channel, a queue, that either contains a fork, or is empty (if one of the adjacent philosophers
is currently holding that fork).
In the PDDL encoding, each process state transition is broken down into four actions.
The first action activates the chosen transition. The second action initiates a write or read
command to the needed queue, deleting the activation of the transition and setting flags for
queue update. The third action updates, if possible, the queue contents. An update is not
possible if a write command shall be done to a full queue (a queue that already contains a
fork), or if a read command shall be done to an empty queue. The fourth action wraps the
process state transition up, re-setting all flags.
Derived predicates are used to model the conditions under which a process is blocked.
The rules require that all outgoing transitions of the current state of the process are blocked.
A transition is blocked if it is activated, and would need to perform an impossible queue
write/read operation – in the sense that this impossible write/read operation has not yet
been initiated.32 After applying the planning action initiating the impossible write/read
command, the blocking rules don’t apply anymore and so the resulting state is a dead end
in the planning task’s state space (but not a blocking situation in the process network,
according to the derived predicate rules modeling the blocking).
We remark that, in IPC-4, there was also a version of Dining-Philosophers that modeled
process blocking via additional planning operators, not derived predicates. We chose to
consider the other, above, domain version since it constitutes the more natural and concise
formulation, and since planners at IPC-4 scaled further up in it than in the version without
derived predicates.
B.8 Driverlog
Driverlog is a variation of Logistics, where drivers are needed for the trucks, and where
drivers and trucks can move along arbitrary (bi-directional) road maps. The road maps for
drivers and trucks can be different. There are operators to load/unload an object onto/from
a truck at a location, to board/disembark a driver onto/from a truck at a location, to walk
32. Only one outgoing transition can be activated at any time, so a process can never become blocked in a
state with more than one outgoing state transition. This appears to be a bug in the translation from
Promela to PDDL – the more intuitive requirement would be that only the activated transition needs
to be blocked, or that an outgoing transition does not need to be activated in order to be blocked. Note
that, in Dining-Philosophers, every automaton state has just one outgoing transition.

744

Where “Ignoring Delete Lists” Works

a driver from a location to another one, and to drive a truck with a driver from a location
to another one. The preconditions and effects of loading/unloading objects are the obvious
ones. A driver can board a truck only if the truck is empty; in effect, the truck is no longer
empty (as well as driven by the driver). Disembarking a driver is the inverse action. In
order to walk a driver from l to l0 , there must be a path between l and l0 . In order to drive
a truck from l to l0 , there has to be a link from l to l0 (and there must be a driver on the
truck). Paths and links form arbitrary (in particular, potentially different) graphs over the
locations, the only restriction being that they are undirected, i.e., if a truck or driver can
move from l to l0 then it can also move back. This restriction is imposed on the Driverlog
instances as can be generated with the IPC-3 generator.
B.9 Ferry
In Ferry, a single ferry is used to transport cars between locations, one at a time. There are
three operators. One can sail the ferry between two locations. One can board a car onto
the ferry at a location, which deletes an empty-ferry fact (plus adding that the car is on
the ferry and deleting that the car is at the location). One can debark a car from the ferry
at a location, which achieves empty-ferry (plus adding that the car is at the location and
deleting that the car is on the ferry). The goal is to have a subset of the cars at their goal
locations.
B.10 Freecell
The Freecell domain is a STRIPS formulation of the widely known solitaire card game
that comes with Microsoft Windows. A number of cards from different suits are initially
arranged in random stacks on a number of columns. The cards must be put home. For each
suit of cards, there is a separate home column, on which the cards from that suit must be
stacked in increasing order of card value. There is a number of free cells. The cards can be
moved around according to certain rules. A card is clear if it has no other card on top of
it. Any clear card can be put into a free cell (if it’s not already there), each free cell holds
only one card at a time. Any clear card can be moved onto an empty column. A clear card
c can be put home if the last card put home in the same suit was the one preceding c. If c
and c0 are clear cards from differently colored suits, then one can stack c on top of c0 if c0 is
not in a free cell, and c’s card value is one less than the card value of c0 (so stacks can only
be built on columns, in decreasing order of card value, and in alternating colors). The goal
is reached when the topmost cards of all suits have been put home.
B.11 Fridge
In Fridge, one must replace the broken compressor in a fridge. To do this, one must remove
the compressor; this involves unfastening the screws that hold the compressor, which in
turn involves first switching the fridge off. The goal is to have the new compressor attached
to the fridge, all screws fastened, and the fridge switched back on. The origin of this domain
is a STRIPS formulation. We consider an adaptation that allows for an arbitrary number of
fridges and screws, where each compressor is fastened by the same (arbitrary, at least one)
number of screws. The adaptation involves an ADL precondition: a compressor can only be
745

Hoffmann

removed if all screws are unfastened. There are six operators. One can stop/start a fridge.
One can unfasten/fasten a screw from/to a compressor attached to a fridge; to do so, the
fridge needs to be turned off, the compressor needs to be attached, and the screw must fit
the compressor. Finally, one can remove/attach a compressor from/to a fridge. Removing
a compressor requires that the fridge is turned off, and that none of the screws that fit the
compressor are fastened. In effect, the compressor is no longer attached to the fridge, and
both the fridge and the compressor are free. Attaching a compressor requires that the fridge
is turned off, and that the compressor fits the fridge. In effect, the compressor is attached,
and the compressor and fridge are no longer free.
B.12 Grid
In Grid, a robot must move along positions that are arranged in a grid-like reachability
relation. The positions can be locked, and there are keys of different shapes to open them.
The goal is to have some keys at their goal positions. There are five operators. One can
move from position p to position p0 , which requires (apart from the obvious preconditions)
that p and p0 are connected, and that p0 is open (not locked). One can pick up a key at a
position, which requires that the arm is empty (one can only hold one key at a time), and
has as effects that one holds the key, that the arm is no longer empty, and that the key
is no longer at the position. Putting a key down at a position is the inverse action. One
can abbreviate the two previous actions by doing a pickup-and-lose of keys k and k0 at a
position; to do this, one must hold k, which is directly exchanged for k0 , i.e., the effects are
that one holds k and that k0 is at the position. Finally, one can unlock a position p0 if one is
at a position p that is connected to p0 , and holds a key that has the same shape as the locked
position p0 ; the add effect is that p0 is open, the delete effect is that the position is no longer
locked. The instances specify the initial locations of all keys, of all locked positions, and
of the robot, as well as the shapes of the keys and the locked positions. The goal specifies
positions for a subset of the keys. The robot always starts at an open position. This does
make a significant difference: if the robot is allowed to start at a locked position, there can
be local minima under h+ .33 Otherwise there are none, c.f. Theorem 2. Intuitively, it makes
more sense to let the robot be located in open positions only; the restriction also holds true
in the published benchmark examples.
B.13 Gripper
In Gripper, the task is to transport a number of balls from one location to another. There
are three operators. One can move between locations. One can pick up a ball at a location
with a hand; apart from the obvious preconditions this requires that the hand is empty; the
effects are the obvious ones (the ball is in the hand and no longer in the room) plus that
the hand is no longer empty. One can drop a ball at a location from a hand, which inverts
the effects of the picking action. There are always exactly two locations, and two gripper
hands. Instances thus differ only in terms of the number of balls. These severe restrictions
hold true in the AIPS-1998 instances. We remark that adding more locations and/or hands
33. Moving away from a locked initial position can lead to the need of applying several steps to re-open that
position. The relaxed plan to the initial state does not realize this, since it ignores the delete on the
initial at-fact.

746

Where “Ignoring Delete Lists” Works

does not affect the topological properties under h+ , in fact the proof arguments given in
Theorems 1, 2, and 3 remain valid in this case.
B.14 Hanoi
The Hanoi domain is a STRIPS encoding of the classical Towers of Hanoi problem. There
are n discs d1 , . . . , dn , and three pegs p1 , p2 , and p3 . There is a single operator that moves
an object x from an object y onto an object z (the operator parameters can be grounded
with discs as well as pegs). The preconditions of the move are that x is on y, x is clear, z
is clear, and x is smaller than z. The effects are that x is on z and y is clear, while x is no
longer on y and z is no longer clear. The semantics of Towers of Hanoi are encoded via the
smaller relation. This relation holds in the obvious way between the discs, and all discs are
smaller than the pegs (the pegs are not smaller than anything so they can not be moved).
The instances differ in terms of the number n of discs that must be transferred from p1 to
p3 .
B.15 Logistics
Logistics is the classical transportation domain, where objects must be transported within
cities using trucks, and between different cities using airplanes. There are six operators, to
drive a truck between two locations within a city, to fly an airplane between two airports,
to load (unload) an object onto (from) a truck at a location, and to load (unload) an object
onto (from) an airplane at an airport. The operators all have the obvious preconditions
and effects (the most “complicated” operator is that moving a truck, whose precondition
requires that both locations are within the city). There is always at least one city, and each
city has a non-zero number of locations one of which is an airport. There is an arbitrary
number of objects, and of airplanes (which are located at airports). The goal is to have a
subset of the objects at their goal locations.
B.16 Miconic-ADL
Miconic-ADL is an ADL formulation of a complex elevator control problem occurring in a
real-world application of planning (Koehler & Schuster, 2000). A number of passengers are
waiting at a number of floors to be transported with a lift, obeying a variety of constraints.
There is always at least one floor, and an arbitrary number of passengers, each of which is
given an origin and a destination floor. There are three operators. The lift can move up
from floor f to floor f’ if f’ is (transitively) above f, and vice versa for moving downwards.
The lift can also stop at a floor. When it does so at floor f, by conditional effects of the
stopping action all passengers waiting at f are boarded, and all passengers wanting to get
out at f depart. The goal is to serve all passengers, i.e., to bring them to their destination
floor. The constraints that must be obeyed are the following.
• In some cases, a passenger p has no access to a floor f; the lift can then not stop at f
while p is boarded.
• Some passengers are VIPs; as long as these are not all served, the lift can only stop
at floors where a VIP is getting on or off.
747

Hoffmann

• Some passengers must be transported non-stop, i.e., if they are boarded, the lift can
make no intermediate stops before stopping at their destination floor.
• Some passengers can not travel alone, others can attend them; if one of the former
kind is boarded, then so must be at least one of the latter kind.
• There are groups A and B of passengers such that it is not allowed to have people
from both groups boarded simultaneously.
• Some passengers can only be transported in the direction of their own travel, i.e., if
they need to go up (down), then, while they are boarded, the lift can not move
downwards (upwards).
All of these constraints are formulated by means of complex first order preconditions of
the operators.
B.17 Miconic-SIMPLE
The Miconic-SIMPLE domain is the same as Miconic-ADL described above, except that
there are no constraints at all.
B.18 Miconic-STRIPS
The Miconic-STRIPS domain is almost the same as the Miconic-SIMPLE domain, see above.
The only difference is that boarding and departing passengers is not done by conditional
effects of a stopping operator, but explicitly by separate STRIPS operators. One can board
a passenger at a floor. The precondition is that the (current) floor is the passenger’s origin,
and the only effect is that the passenger is boarded. One can let a passenger depart at a
floor. The preconditions are that the (current) floor is the passenger’s destination and that
the passenger is boarded, and the effects are that the passenger is served but no longer
boarded.
B.19 Movie
In Movie, the task is to prepare for watching a movie. There are seven different operators.
One can rewind the tape, which adds that the tape is rewound, and deletes that the counter
is at zero. One can reset the counter, the only effect being that the counter is at zero. One
can get five different kinds of snacks, the only (add) effect being that one has the respective
snack. Instances differ only in terms of the number of items that there are of each sort of
snacks. The goal is always to have one snack of each sort, to have the tape rewound, and
to have the counter at zero.
B.20 Mprime
Mprime is a transportation kind of domain, where objects must be transported between
locations by means of vehicles, and vehicles use non-replenishable fuel. In an instance,
there are a set L of locations, a set O of objects, and a set V of vehicles. There also are
sets F and S of fuel numbers and space numbers. Each location initially has a certain fuel
748

Where “Ignoring Delete Lists” Works

number – the number of fuel items available at the location – and each vehicle has a certain
space number – the number of objects the vehicle can carry at a time. There are operators
to move vehicles between locations, to load (unload) objects onto (from) vehicles, and to
transfer fuel units between locations. A move from location l to location l0 can only be
made if l and l0 are connected (where the connection relation is an arbitrary graph), and if
there is at least one fuel unit available at l (l has a fuel number that has a lower neighbor).
In effect of the move, the respective vehicle is located at l0 , and the amount of fuel at l is
decreased by one unit, i.e., l is assigned the next lower fuel number. In a similar fashion, an
object can only be loaded onto a vehicle if there is space for that, and in effect the available
space decreases. Unloading the object frees the space again. The transfer operator can
transfer one fuel unit from location l to location l0 , if l and l0 are connected, and l has at
least two fuel units left. As the result of applying the operator, l’s fuel number decreases by
one, while l0 ’s fuel number increases by one. Note that there is no way to re-gain fuel items
(one can transfer them around but one can not obtain new ones). The goal is to transport
a subset of the objects to their goal locations.
B.21 Mystery
Mystery is exactly the same as the Mprime domain described above, except that there is
no operator to transfer fuel items.
B.22 Optical-Telegraph
Like the Dining-Philosophers domain described above in Appendix B.7, Optical-Telegraph
is a PDDL compilation of a problem originally formulated in the automata-based Promela
language. The mechanics of the PDDL compilation are the same as in Dining-Philosophers,
using derived predicates to detect blocked situations. The problem involves n pairs of communicating processes, each pair featuring an “up” and a “down” process. Such a pair can go
through a fairly long, heavily interactive, sequence of operations, implementing the possibility to exchange data between the two stations. Before data is exchanged, various initializing
steps must be taken, to ensure the processes are working synchronously. Most importantly,
each of the process writes a token into a “control channel” (queue) at the beginning of
the sequence, and reads the token out again at the end. This causes a deadlock situation
because there are only n control channels, each of which is accessed by two processes. More
precisely, the process pairs are arranged in a cycle, where between each pair there is a control channel. The overall system is blocked iff all process pairs are in a state where they
have occupied (written into) one control channel, and are waiting to occupy the other. In
that sense, Optical-Telegraph can be viewed as a version of Dining-Philosophers where the
internal states of the philosophers are more complicated. In particular, the “philosophers”
(process pairs) here can choose in which order to “pick up the forks” (occupy the control
channels). As it turns out, see Appendix A.2, the latter has an important impact on the
topology under h+ .
We remark that, in IPC-4, there was also a version of Optical-Telegraph that modeled
process blocking via additional planning operators, not derived predicates. We chose to
consider the other, above, domain version since it constitutes the more natural and concise
749

Hoffmann

formulation, and since planners at IPC-4 scaled further up in it than in the version without
derived predicates.
B.23 Pipesworld
In Pipesworld, units of oil derivatives, called “batches”, must be propagated through a
pipeline network. The network consists of areas connected with pipe segments of different
length. The pipes are completely filled with batches at all times, and if one pushes a batch
in at the one end of a pipe, the last batch currently in that pipe comes out at the other end.
There can be interface restrictions concerning the types of oil derivatives that are allowed
to be adjacent to each other inside a pipe, and there can be tankage restrictions concerning
the number of batches (of each derivative type) that can be stored at any point in time in
the individual areas.
The only available planning operator is to push a batch into a pipe. In the IPC4 encoding of the domain, which we look at here, for non-unitary pipe segments (pipes
containing more than one batch) this operator is split into two parts, a start and a finish
action (in order to reduce the number of operator parameters needed to correctly update the
pipe contents). Also, pipe segments are encoded in a directed fashion, making it necessary
to distinguish between (symmetrical) push and pop actions. The initial state specifies the
current batch positions etc., the goal specifies what batches have to be brought to what
areas.
B.24 PSR
In the PSR domain, as used in IPC-4, the task is to re-supply a given set of lines in a faulty
electricity network. The nodes of the network are “breakers”, which feed electricity into
the network, and “devices”, which are just switches that can be used to change the network
configuration. The edges in the network are the lines, each connecting two or three nodes.
The breakers and devices can be open or closed. If they are open, then they disconnect the
lines adjacent to them. If breakers are closed, then they feed electricity into the adjacent
lines. Some of the lines are faulty. The goal is to ensure that none of the breakers is
“affected”, i.e., feeds electricity into a faulty line, through the transitive connections in the
network. Also, the goal requires that each of the given set of lines is (transitively) fed with
electricity from some breaker.
The transitive network semantics, determining if a breaker feeds electricity into some
line, and if a breaker is affected, are modeled by means of various derived predicates (with
recursive rule antecedents to enable the computation of transitive closure). There are three
planning operators. One can open a device or breaker that is currently closed, and one
can do the inverse closing action. Both actions require as a precondition that no breaker
is currently affected. If the latter is untrue, i.e., if a breaker is currently affected, then the
only available action is to wait. Its effect is to open all breakers that are affected.
We remark that, in IPC-4, there was also a different version of PSR, formulated in
pure STRIPS without derived predicates. That version constitutes, however, a relatively
superficial pre-compiled form of the domain (Hoffmann & Edelkamp, 2005; Edelkamp et al.,
2005). It was included in IPC-4 only in order to provide the pure STRIPS planners with a
750

Where “Ignoring Delete Lists” Works

domain formulation they could tackle (the pre-compilation was necessary in order to enable
the formulation in pure STRIPS).
B.25 Rovers
In Rovers, a number of rovers must navigate through a road map of waypoints, take rock
and soil samples as well as images, and communicate the data to a number of landers. The
nine available operators are the following. One can navigate a rover from one waypoint
to another – to do this, the waypoints must be connected for the rover. One can sample
soil/rock with a rover at a waypoint using a store – to do so, the rover must have the
(empty) store and be equipped for soil/rock analysis, and there must be soil/rock to sample
at the waypoint; in effect one has the soil/rock analysis, the store is full, and the soil/rock
sample is no longer at the waypoint. One can empty a full store by dropping the store. One
can calibrate a camera at a waypoint using an objective, and one can take an image of an
objective in a mode with a camera at a waypoint. For both operators, the object must be
visible from the waypoint, and the camera must be on board a rover that is equipped for
imaging. To calibrate the camera, the object must be a calibration target for it; the only
effect of the operator is the calibration of the camera. To take an image, the camera must
be calibrated, and support the required mode. The effects are that one has the image data,
and that the camera is no longer calibrated. Finally, there are three operators with which
a rover can communicate soil/rock/image data to a lander. To do so, the lander’s waypoint
must be visible from that of the rover; the only effect is that the data is communicated.
The instances are restricted in that the visibility and connectivity between waypoints are
bi-directional – if waypoint w is visible from waypoint w0 then the same holds true vice
versa; if a rover can move from w to w0 then it can also move back. Another restriction is
that no camera is initially calibrated (this serves to make sure that, in a reachable state,
each calibrated camera has at least one calibration target). Both restrictions are imposed
on the Rovers instances as can be generated with the IPC-3 generator.
B.26 Satellite
In Satellite, satellites need to take images in different directions, in certain modes, using
appropriate instruments. There is a number of satellites, a number of directions, a number
of instruments, and a number of modes. There are the following five operators. One can
turn a satellite from a direction to another one; the preconditions and effects are the obvious
ones, the action can be applied between any pair of directions (no connectivity constraints).
One can switch on an instrument on board a satellite, if the satellite has power available; in
effect, the instrument has power but is no longer calibrated, and the satellite has no more
power available. One can switch off an instrument on board a satellite, if the instrument
has power; in effect, the satellite has power available, but the instrument not anymore.
One can calibrate an instrument on board a satellite in a direction, if the satellite points
into the direction, the instrument has power, and the direction is a calibration target for
the instrument. The only effect is the calibration of the camera. Finally, one can take an
image with an instrument on board a satellite in a direction and a mode. To do so, the
satellite must point into the direction, and the camera must support the mode, have power,
and be calibrated; the only effect is that one has an image of the direction in the mode.
751

Hoffmann

The goal is to have images of a number of direction/mode pairs; also, satellites can have a
goal requirement to point into a specified direction. The initial states are such that each
satellite (but no instrument) has power available, and no instrument is calibrated. The
former restriction makes sure that each satellite has the power to run one instrument at a
time; the latter restriction makes sure that, in a reachable state, each calibrated instrument
has at least one calibration target. Both restrictions are imposed on the Satellite instances
as can be generated with the IPC-3 generator.

B.27 Schedule
In Schedule, a collection of objects must be processed on a number of machines, applying
working steps that change an object’s shape, surface condition, or color; one can also drill
holes of varying widths in varying orientations. There are nine operators. Eight of these
describe working steps for an object o on a machine. Amongst other things, these operators’
preconditions require that o is not scheduled elsewhere and that the machine is not busy,
and these operators’ effect is that o is scheduled, and that the machine is busy. The ninth
operator does a time step, whose effect is that no object is scheduled, and no machine is
busy, any longer. One can apply a do-roll action to an object o, which makes o cylindrical
and hot (no longer cold, see also below), while deleting any surface conditions, colors, and
holes that o might have. One can apply a do-lathe to o, making it cylindrical with a rough
surface, and deleting any colors that o might have been painted in before. One can apply
a do-polish to o if it is cold, giving it a polished surface. One can apply a do-grind to o,
giving it a smooth surface with no colors. One can apply a do-punch to o, with width w in
orientation o, if o is cold, resulting in o having a hole in w and o, and a rough surface. One
can also apply a do-drill-press to o, if o is cold, making a hole of width w and orientation
o into o (changing none of o’s properties except making the hole). If o is cold, then one
can also apply a do-spray-paint in color c, deleting all surface conditions that o might have.
Finally, one can apply a do-immersion-paint to o, changing none of o’s properties except
the color. Note that there is no operator that can change o’s temperature, except do-roll
which makes o hot; after that, o can not be made cold again (this is the reason why dead
ends can arise, c.f. Theorem 1). Initially, all objects are cold, and have a shape and a
surface condition specified. Some of the objects are also painted initially, and an object
can have none or several holes. In the goal condition, some of the objects can be required
to have cylindrical shape (the only shape that can be produced by the machines), some
need a surface condition, some must be painted, and each object can be required to have
an arbitrary number of holes.

B.28 Simple-Tsp
Simple-Tsp is a trivial version of the TSP problem. There is a single operator to move
between locations. This can be applied between any two (different) locations, and the effect
(besides the obvious ones) is that the destination location is visited. The instances specify
a number of locations that must all be visited, starting in one of them.
752

Where “Ignoring Delete Lists” Works

B.29 Tireworld
In Tireworld, one must replace a number of flat tires. This involves a collection of objects
that must be used in the appropriate working steps. Briefly summarized, the situation is as
follows. There are thirteen operators. There is a boot that can be either opened or closed;
it is initially closed and shall be so in the end. There are a pump, a wrench, and a jack
which can be fetched or put away (from/into the boot); they are initially in the boot and
shall be put back in the end. The spare wheels are initially not inflated, and can be inflated
using the pump (the add effect is that the wheel is inflated, the delete effect is that it is no
longer not-inflated). Each hub is fastened with nuts; these can be loosened or tightened,
using the wrench, while the respective hub is on ground. The jack can be used to either
jack up or jack down a hub. Once a hub is jacked up, one can undo the (loose) nuts, or do
them up; if the nuts are undone, one can remove the respective wheel, or put on one. An
optimal solution plan is this: open the boot; fetch the tools; inflate all spare wheels; loosen
all nuts; in turn jack up each hub, undo the nuts, remove the flat wheel, put on the spare
wheel, do up the nuts, and jack the hub down again; tighten all nuts; put away the tools;
and close the boot.
B.30 Zenotravel
Zenotravel is a transportation domain variant where the vehicles (called aircrafts) use fuel
units that can be replenished using a refueling operator. There are a number of cities, a
number of aircrafts, a number of persons, and a number of different possible fuel levels.
The fuel levels encode natural numbers by a next predicate – next(f,f0 ) is true iff f0 is the
next higher fuel level than f. The task is to transport a subset of the persons from their
initial locations to their goal locations. There are the following five operators. One can
board/debark a person onto/from an aircraft at a city; this has just the obvious preconditions and effects. One can fly an aircraft from a city to a different city, decreasing the
aircraft’s fuel level from f to f0 ; f must be the aircraft’s current fuel level, and f0 must be the
next lower level. One can also zoom the aircraft; this is exactly the same as flying it, except
that zooming uses more fuel – the aircraft’s fuel level is decreased by two units. Finally,
one can refuel an aircraft at a city from fuel level f to fuel level f0 . The conditions are that
f is the aircraft’s current fuel level, and that f0 is the next higher level. Thus aircrafts can
be refueled at any city, and in steps of one unit.

References
Bacchus, F. (2001). The AIPS’00 planning competition. The AI Magazine, 22 (3), 47–56.
Biundo, S., & Fox, M. (Eds.). (1999). Recent Advances in AI Planning. 5th European
Conference on Planning (ECP’99), Lecture Notes in Artificial Intelligence, Durham,
UK. Springer-Verlag.
Blum, A. L., & Furst, M. L. (1995). Fast planning through planning graph analysis. In
Mellish, S. (Ed.), Proceedings of the 14th International Joint Conference on Artificial
Intelligence (IJCAI-95), pp. 1636–1642, Montreal, Canada. Morgan Kaufmann.
753

Hoffmann

Blum, A. L., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90 (1-2), 279–298.
Bonet, B., & Geffner, H. (1999). Planning as heuristic search: New results.. In Biundo, &
Fox (Biundo & Fox, 1999), pp. 60–72.
Bonet, B., & Geffner, H. (2001a). Heuristic search planner 2.0. The AI Magazine, 22 (3),
77–80.
Bonet, B., & Geffner, H. (2001b). Planning as heuristic search. Artificial Intelligence,
129 (1–2), 5–33.
Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism
for planning. In Kuipers, B. J., & Webber, B. (Eds.), Proceedings of the 14th National
Conference of the American Association for Artificial Intelligence (AAAI-97), pp.
714–719, Portland, OR. MIT Press.
Botea, A., Müller, M., & Schaeffer, J. (2004). Using component abstraction for automatic
generation of macro-actions.. In Koenig et al. (Koenig, Zilberstein, & Koehler, 2004),
pp. 181–190.
Botea, A., Müller, M., & Schaeffer, J. (2005). Learning partial-order macros from solutions.
In Biundo, S., Myers, K., & Rajan, K. (Eds.), Proceedings of the 15th International
Conference on Automated Planning and Scheduling (ICAPS-05), pp. 231–240, Monterey, CA, USA. Morgan Kaufmann.
Brazdil, P., & Jorge, A. (Eds.)., EPIA-01 (2001). Proceedings of the 10th Portuguese Conference on Artificial Intelligence (EPIA-01), Porto, Portugal. Springer-Verlag.
Bylander, T. (1994). The computational complexity of propositional STRIPS planning.
Artificial Intelligence, 69 (1–2), 165–204.
Cesta, A., & Borrajo, D. (Eds.). (2001). Recent Advances in AI Planning. 6th European
Conference on Planning (ECP’01), Lecture Notes in Artificial Intelligence, Toledo,
Spain. Springer-Verlag.
Chen, Y., Hsu, C., & Wah, B. (2004). SGPlan: Subgoal partitioning and resolution in
planning.. In Edelkamp et al. (Edelkamp, Hoffmann, Littman, & Younes, 2004).
Chen, Y., & Wah, B. (2003). Automated planning and scheduling using calculus of variations
in discrete space.. In Giunchiglia et al. (Giunchiglia, Muscettola, & Nau, 2003), pp.
2–11.
Chien, S., Kambhampati, R., & Knoblock, C. (Eds.)., AIPS-00 (2000). Proceedings of the
5th International Conference on Artificial Intelligence Planning Systems (AIPS-00),
Breckenridge, CO. AAAI Press, Menlo Park.
Do, M. B., & Kambhampati, S. (2001). Sapa: A domain-independent heuristic metric
temporal planner.. In Cesta, & Borrajo (Cesta & Borrajo, 2001), pp. 109–120.
Edelkamp, S. (2003a). Promela planning. In Ball, T., & Rajamani, S. (Eds.), Proceedings
of the 10th International SPIN Workshop on Model Checking of Software (SPIN-03),
pp. 197–212, Portland, OR. Springer-Verlag.
Edelkamp, S. (2003b). Taming numbers and durations in the model checking integrated
planning system. Journal of Artificial Intelligence Research, 20, 195–238.
754

Where “Ignoring Delete Lists” Works

Edelkamp, S., & Helmert, M. (2001). MIPS: The model checking integrated planning system.
AI Magazine, 22 (3), 67–71.
Edelkamp, S., Hoffmann, J., Englert, R., Liporace, F., Thiebaux, S., & Trüg, S. (2005).
Engineering benchmarks for planning: the domains used in the deterministic part of
IPC-4. Journal of Artificial Intelligence Research. Submitted.
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.)., IPC-04 (2004). Proceedings
of the 4th International Planning Competition, Whistler, BC, Canada. JPL.
Fox, M., & Long, D. (1998). The automatic inference of state invariants in TIM. Journal
of Artificial Intelligence Research, 9, 367–421.
Fox, M., & Long, D. (2001). STAN4: A hybrid planning strategy based on subproblem
abstraction. The AI Magazine, 22 (3), 81–84.
Frank, J., Cheeseman, P., & Stutz, J. (1997). When gravity fails: Local search topology.
Journal of Artificial Intelligence Research, 7, 249–281.
Gazen, B. C., & Knoblock, C. (1997). Combining the expressiveness of UCPOP with the
efficiency of Graphplan.. In Steel, & Alami (Steel & Alami, 1997), pp. 221–233.
Gerevini, A., & Schubert, L. (2000). Inferring state constraints in DISCOPLAN: Some new
results.. In Kautz, & Porter (Kautz & Porter, 2000), pp. 761–767.
Gerevini, A., & Schubert, L. (2001). DISCOPLAN: an efficient on-line system for computing
planning domain invariants.. In Cesta, & Borrajo (Cesta & Borrajo, 2001), pp. 433–
436.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning through stochastic local search and
temporal action graphs. Journal of Artificial Intelligence Research, 20, 239–290.
Gerevini, A., & Serina, I. (2002). LPG: A planner based on local search for planning graphs
with action costs.. In Ghallab et al. (Ghallab, Hertzberg, & Traverso, 2002), pp.
13–22.
Gerevini, A., Serina, I., Saetti, A., & Spinoni, S. (2003). Local search techniques for temporal
planning in LPG.. In Giunchiglia et al. (Giunchiglia et al., 2003). Accepted for
publication.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.)., AIPS-02 (2002). Proceedings of the 6th
International Conference on Artificial Intelligence Planning and Scheduling (AIPS02), Toulouse, France. Morgan Kaufmann.
Giunchiglia, E., Muscettola, N., & Nau, D. (Eds.)., ICAPS-03 (2003). Proceedings of the
13th International Conference on Automated Planning and Scheduling (ICAPS-03),
Trento, Italy. Morgan Kaufmann.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning.. In Chien
et al. (Chien, Kambhampati, & Knoblock, 2000), pp. 140–149.
Helmert, M. (2003). Complexity results for standard benchmark domains in planning.
Artificial Intelligence, 143, 219–262.
Helmert, M. (2004). A planning heuristic based on causal graph analysis.. In Koenig et al.
(Koenig et al., 2004), pp. 161–170.
755

Hoffmann

Helmert, M., & Richter, S. (2004). Fast downward – making use of causal dependencies in
the problem representation.. In Edelkamp et al. (Edelkamp et al., 2004).
Hoffmann, J. (2000). A heuristic for domain independent planning and its use in an enforced
hill-climbing algorithm. In Ras, Z. W., & Ohsuga, S. (Eds.), Proceedings of the 12th
International Symposium on Methodologies for Intelligent Systems (ISMIS-00), pp.
216–227, Charlotte, NC. Springer-Verlag.
Hoffmann, J. (2001a). FF: The fast-forward planning system. The AI Magazine, 22 (3),
57–62.
Hoffmann, J. (2001b). Local search topology in planning benchmarks: An empirical analysis.. In Nebel (Nebel, 2001), pp. 453–458.
Hoffmann, J. (2002). Extending FF to numerical state variables. In Harmelen, F. V. (Ed.),
Proceedings of the 15th European Conference on Artificial Intelligence (ECAI-02), pp.
571–575, Lyon, France. Wiley.
Hoffmann, J. (2003a). The Metric-FF planning system: Translating “ignoring delete lists”
to numeric state variables. Journal of Artificial Intelligence Research, 20, 291–341.
Hoffmann, J. (2003b). Utilizing Problem Structure in Planning: A Local Search Approach,
Vol. 2854 of Lecture Notes in Artificial Intelligence. Springer-Verlag.
Hoffmann, J. (2003c).
Where ignoring delete lists works: Local search topology in planning benchmarks.
Tech. rep. 185, Albert-Ludwigs-Universität,
Institut für Informatik, Freiburg, Germany.
Available at http://www.mpiinf.mpg.de/∼hoffmann/papers/jair05report.ps.gz.
Hoffmann, J., & Edelkamp, S. (2005). The deterministic part of IPC-4: An overview. Journal
of Artificial Intelligence Research. To appear.
Hoffmann, J., & Nebel, B. (2001a). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Hoffmann, J., & Nebel, B. (2001b). RIFO revisited: Detecting relaxed irrelevance.. In Cesta,
& Borrajo (Cesta & Borrajo, 2001), pp. 325–336.
Kautz, H. A., & Porter, B. (Eds.)., AAAI-00 (2000). Proceedings of the 17th National
Conference of the American Association for Artificial Intelligence (AAAI-00), Austin,
TX. MIT Press.
Koehler, J., & Hoffmann, J. (2000). On reasonable and forced goal orderings and their use
in an agenda-driven planning algorithm. Journal of Artificial Intelligence Research,
12, 338–386.
Koehler, J., & Schuster, K. (2000). Elevator control as a planning problem.. In Chien et al.
(Chien et al., 2000), pp. 331–338.
Koenig, S., Zilberstein, S., & Koehler, J. (Eds.)., ICAPS-04 (2004). Proceedings of the
14th International Conference on Automated Planning and Scheduling (ICAPS-04),
Whistler, Canada. Morgan Kaufmann.
Long, D., & Fox, M. (2000). Automatic synthesis and use of generic types in planning.. In
Chien et al. (Chien et al., 2000), pp. 196–205.
756

Where “Ignoring Delete Lists” Works

Long, D., & Fox, M. (2003). The 3rd international planning competition: Results and
analysis. Journal of Artificial Intelligence Research, 20, 1–59.
McDermott, D. (1996). A heuristic estimator for means-ends analysis in planning. In Drabble, B. (Ed.), Proceedings of the 3rd International Conference on Artificial Intelligence
Planning Systems (AIPS-96), pp. 142–149. AAAI Press, Menlo Park.
McDermott, D. (2000). The 1998 AI planning systems competition. The AI Magazine,
21 (2), 35–55.
McDermott, D. V. (1999). Using regression-match graphs to control search in planning.
Artificial Intelligence, 109 (1-2), 111–159.
Nebel, B. (Ed.)., IJCAI-01 (2001). Proceedings of the 17th International Joint Conference
on Artificial Intelligence (IJCAI-01), Seattle, Washington, USA. Morgan Kaufmann.
Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts and operators in
plan generation.. In Steel, & Alami (Steel & Alami, 1997), pp. 338–350.
Nguyen, X., & Kambhampati, S. (2000). Extracting effective and admissible heuristics from
the planning graph.. In Kautz, & Porter (Kautz & Porter, 2000), pp. 798–805.
Nguyen, X., & Kambhampati, S. (2001). Reviving partial order planning.. In Nebel (Nebel,
2001), pp. 459–464.
Onaindia, E., Sapena, O., Sebastia, L., & Marzal, E. (2001). Simplanner: an executionmonitoring system for replanning in dynamic worlds.. In Brazdil, & Jorge (Brazdil &
Jorge, 2001), pp. 393–400.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: A sound, complete, partial order planner
for ADL. In Nebel, B., Swartout, W., & Rich, C. (Eds.), Principles of Knowledge
Representation and Reasoning: Proceedings of the 3rd International Conference (KR92), pp. 103–114, Cambridge, MA. Morgan Kaufmann.
Refanidis, I., & Vlahavas, I. (1999). GRT: a domain independent heuristic for STRIPS
worlds based on greedy regression tables.. In Biundo, & Fox (Biundo & Fox, 1999),
pp. 47–59.
Refanidis, I., & Vlahavas, I. (2001). The GRT planning system: Backward heuristic construction in forward state-space planning. Journal of Artificial Intelligence Research,
15, 115–161.
Rintanen, J. (2000). An iterative algorithm for synthesizing invariants.. In Kautz, & Porter
(Kautz & Porter, 2000), pp. 806–811.
Sebastia, L., Onaindia, E., & Marzal, E. (2001). Stella: An optimal sequential and parallel
planner.. In Brazdil, & Jorge (Brazdil & Jorge, 2001), pp. 409–416.
Srivastava, B., Nguyen, X., Kambhampati, S., Do, M. B., Nambiar, U., Nie, Z., Nigenda, R.,
& Zimmermann, T. (2001). Altalt: Combining graphplan and heuristic state search.
The AI Magazine, 22 (3), 88–90.
Steel, S., & Alami, R. (Eds.). (1997). Recent Advances in AI Planning. 4th European Conference on Planning (ECP’97), Vol. 1348 of Lecture Notes in Artificial Intelligence,
Toulouse, France. Springer-Verlag.
757

Hoffmann

Thiebaux, S., Hoffmann, J., & Nebel, B. (2003). In defence of PDDL axioms. In Gottlob, G.
(Ed.), Proceedings of the 18th International Joint Conference on Artificial Intelligence
(IJCAI-03), pp. 961–966, Acapulco, Mexico. Morgan Kaufmann.
Thiebaux, S., Hoffmann, J., & Nebel, B. (2005). In defence of PDDL axioms. Artificial
Intelligence. To appear.
Younes, H., & Simmons, R. (2002). On the role of ground actions in refinement planning..
In Ghallab et al. (Ghallab et al., 2002), pp. 54–61.

758

Journal of Artificial Intelligence Research 24 (2005) 889-917

Submitted 11/04; published 12/05

Ignorability in Statistical and Probabilistic Inference
Manfred Jaeger

jaeger@cs.aau.dk

Institut for Datalogi, Aalborg Universitet
Fredrik Bajers Vej 7 E, DK-9220 Aalborg Ø

Abstract
When dealing with incomplete data in statistical learning, or incomplete observations
in probabilistic inference, one needs to distinguish the fact that a certain event is observed
from the fact that the observed event has happened. Since the modeling and computational
complexities entailed by maintaining this proper distinction are often prohibitive, one asks
for conditions under which it can be safely ignored. Such conditions are given by the missing
at random (mar) and coarsened at random (car) assumptions. In this paper we provide
an in-depth analysis of several questions relating to mar/car assumptions. Main purpose
of our study is to provide criteria by which one may evaluate whether a car assumption
is reasonable for a particular data collecting or observational process. This question is
complicated by the fact that several distinct versions of mar/car assumptions exist. We
therefore first provide an overview over these different versions, in which we highlight the
distinction between distributional and coarsening variable induced versions. We show that
distributional versions are less restrictive and sufficient for most applications. We then
address from two different perspectives the question of when the mar/car assumption is
warranted. First we provide a “static” analysis that characterizes the admissibility of the
car assumption in terms of the support structure of the joint probability distribution of
complete data and incomplete observations. Here we obtain an equivalence characterization
that improves and extends a recent result by Grünwald and Halpern. We then turn to a
“procedural” analysis that characterizes the admissibility of the car assumption in terms
of procedural models for the actual data (or observation) generating process. The main
result of this analysis is that the stronger coarsened completely at random (ccar) condition
is arguably the most reasonable assumption, as it alone corresponds to data coarsening
procedures that satisfy a natural robustness property.

1. Introduction
Probabilistic models have become the preeminent tool for reasoning under uncertainty in
AI. A probabilistic model consists of a state space W , and a probability distribution over
the states x ∈ W . A given probabilistic model is used for probabilistic inference based on
observations. An observation determines a subset U of W that the true state now is known
to belong to. Probabilities then are updated by conditioning on U .
The required probabilistic models are often learned from empirical data using statistical
parameter estimation techniques. The data can consist of sampled exact states from W ,
but more often it consists of incomplete observations, which only establish that the exact
data point x belongs to a subset U ⊆ W . Both when learning a probabilistic model, and
when using it for probabilistic inference, one should, in principle, distinguish the event that
a certain observation U has been made (“U is observed”) from the event that the true
state of W is a member of U (“U has occurred”). Ignoring this distinction in probabilistic
c
2005
AI Access Foundation. All rights reserved.

Jaeger

inference can lead to flawed probability assignments by conditioning. Illustrations for this
are given by well-known probability puzzles like the Monty-Hall problem or the three prisoners paradox. Ignoring this distinction in statistical learning can lead to the construction
of models that do not fit the true distribution on W . In spite of these known difficulties,
one usually tries to avoid the extra complexity incurred by making the proper distinction
between “U is observed” and “U has occurred”. In statistics there exists a sizable literature
on “ignorability” conditions that permit learning procedures to ignore this distinction. In
the AI literature dealing with probabilistic inference this topic has received rather scant attention, though it has been realized early on (Shafer, 1985; Pearl, 1988). Recently, however,
Grünwald and Halpern (2003) have provided a more in-depth analysis of ignorability from
a probabilistic inference point of view.
The ignorability conditions required for learning and inference have basically the same
mathematical form, which is expressed in the missing at random (mar ) or coarsened at
random (car ) conditions. In this paper we investigate several questions relating to these
formal conditions. The central theme of this investigation is to provide a deeper insight into
what makes an observational process satisfy, or violate, the coarsened at random condition.
This question is studied from two different angles: first (Section 3) we identify qualitative properties of the joint distribution of true states and observations that make the car
assumption feasible at all. The qualitative properties we here consider are constraints on
what states and observations have nonzero probabilities. This directly extends the work of
Grünwald and Halpern (2003) (henceforth also referred to as GH ). In fact, our main result
in Section 3 is an extension and improvement over one of the main results in GH. Secondly
(Section 4), we investigate general types of observational procedures that will lead to car
observations. This, again, directly extends some of the material in GH, as well as earlier
work by Gill, van der Laan & Robins (1997) (henceforth also referred to as GvLR). We
develop a formal framework that allows us to analyze previous and new types of procedural
models in a unified and systematic way. In particular, this framework allows us to specify
precise conditions for what makes certain types of observational processes “natural” or “reasonable”. The somewhat surprising result of this analysis is that the arguably most natural
classes of observational processes correspond exactly to those processes that will result in
observations that are coarsened completely at random (ccar ) – a strengthened version of
car that often has been considered an unrealistically strong assumption.

2. Fundamentals of Coarse Data and Ignorability
There exist numerous definitions in the literature of what it means that data is missing or
coarsened at random (Rubin, 1976; Dawid & Dickey, 1977; Heitjan & Rubin, 1991; Heitjan,
1994, 1997; Gill et al., 1997; Grünwald & Halpern, 2003). While all capture the same
basic principle, various definitions are subtly different in a way that can substantially affect
their implications. In Section 2.1 we give a fairly comprehensive overview of the variant
definitions, and analyze their relationships. In this survey we aim at providing a uniform
framework and terminology for different mar /car variants. Definitions are attributed to
those earlier sources where their basic content has first appeared, even though our definitions
and our terminology can differ in some details from the original versions (cf. also the remarks
at the end of Section 2.1).
890

Ignorability in Statistical and Probabilistic Inference

Special emphasis is placed on the distinction between distributional and coarsening
variable induced versions of car . In this paper the main focus will then be on distributional
versions. In Section 2.2 we summarize results showing that distributional car is sufficient
to establish ignorability for probabilistic inference.
2.1 Defining Car
We begin with the concepts introduced by Rubin (1976) for the special case of data with
missing values. Assume that we are concerned with a multivariate random variable X =
(X1 , . . . , Xk ), where each Xi takes values in a finite state space V i . Observations of X are
incomplete, i.e. we observe values y = (y 1 , . . . , yk ), where each yi can be either the value
xi ∈ Vi of Xi , or the special ’missingness symbol’ ∗. One can view y as the realization of
a random variable Y that is a function of X and a missingness indicator, M , which is a
random variable with values in {0, 1} k :
Y = f (X, M ),

(1)



(2)

where y = f (x, m) is defined by
yi =

xi if mi = 0
.
∗ if mi = 1

Rubin’s (1976) original definition of missing at random is a condition on the conditional
distribution of M : the data is missing at random iff for all y and all m:
P (M = m | X) is constant on {x | P (X = x) > 0, f (x, m) = y}.

(3)

We refer to this condition as the M -mar condition, to indicate the fact that it is expressed
in terms of the missingness indicator M .
Example 2.1 Let X = (X1 , X2 ) with V1 = V2 = {p, n}. We interpret X1 , X2 as two
medical tests with possible outcomes positive or negative. Suppose that test X 1 always is
performed first on a patient, and that test X 2 is performed if and only if X1 comes out
positive. Possible observations that can be made then are
(n, ∗) = f ((n, n), (0, 1)) = f ((n, p), (0, 1)),
(p, n) = f ((p, n), (0, 0)),
(p, p) = f ((p, p), (0, 0)).
For y = (n, ∗) and m = (0, 1) we obtain
P (M = m | X = (n, n)) = P (M = m | X = (n, p)) = 1,
so that (3) is satisfied. For other values of y and m condition (3) trivially holds, because
the sets of x-values in (3) then are singletons (or empty).
We can also eliminate the random vector M from the definition of mar , and formulate
a definition directly terms of the joint distribution of Y and X. For this, observe that each
observed y can be identified with the set
U (y) := {x | for all i : yi 6= ∗ ⇒ xi = yi }.
891

(4)

Jaeger

The set U (y) contains the complete data values consistent with the observed y. We can
now rephrase M -mar as
P (Y = y | X) is constant on {x | P (X = x) > 0, x ∈ U (y)}.

(5)

We call this the distributional mar condition, abbreviated d-mar , because it is in terms of
the joint distribution of the complete data X, and the observed data Y .
Example 2.2 (continued from Example 2.1) We have
U ((n, ∗)) = {(n, n), (n, p)}, U ((p, n)) = {(p, n)}, U ((p, p)) = {(p, p)}.
Now we compute
P (Y = (n, ∗) | X = (n, n))) = P (Y = (n, ∗) | X = (n, p))) = 1.
Together with the (again trivial) conditions for the two other possible Y -values, this shows
(5).
M -mar and d-mar are equivalent, because given X there is a one-to-one correspondence
between M and Y , i.e. there exists a function h such that for all x, y with x ∈ U (y):
y = f (x, m) ⇔ m = h(y)

(6)

(h simply translates y into a {0, 1}-vector by replacing occurrences of ∗ with 1, and all other
values in y with 0). Using (6) one can easily derive a one-to-one correspondence between
conditions (3) and (5), and hence obtain the equivalence of M -mar and d-mar .
One advantage of M -mar is that it easily leads to the strengthened condition of missing
completely at random (Rubin, 1976):
P (M = m | X) is constant on {x | P (X = x) > 0}.

(7)

We refer to this as the M -mcar condition.
Example 2.3 (continued from Example 2.2) We obtain
P (M = (0, 1) | X = (n, p)) = 1 6= 0 = P (M = (0, 1) | X = (p, p)).
Thus, the observations here are not M -mcar.
A distributional version of mcar is slightly more complex, and we defer its statement to
the more general case of coarse data, which we now turn to.
Missing attribute values are only one special way in which observations can be incomplete. Other possibilities include imperfectly observed values (e.g. X i is only known to be
either x ∈ Vi or x0 ∈ Vi ), partly attributed values (e.g. for x ∈ V i = Vj it is only known
that Xi = x or Xj = x), etc. In all cases, the incomplete observation of X defines the set
of possible instantiations of X that are consistent with the observation. This leads to the
general concept of coarse data (Heitjan & Rubin, 1991), which generalizes the concept of
missing data to observations of arbitrary subsets of the state space. In this general setting
892

Ignorability in Statistical and Probabilistic Inference

it is convenient to abstract from the particular structure of the state space as a product
×ki=1 Vi induced by a multivariate X, and instead just assume a univariate random variable
X taking values in a set W = {x1 , . . . , xn } (of course, this does not preclude the possibility
that in fact W = ×ki=1 Vi ). Abstracting from the missingness indicator M , one can imagine
coarse data as being produced by X and a coarsening variable G. Again, one can also take
the coarsening variable G out of the picture, and model coarse data directly as the joint
distribution of X and a random variable Y (the observed data) with values in 2 W . This is
the view we will mostly adopt, and therefore the motivation for the following definition.
Definition 2.4 Let W = {x1 , . . . , xn }. The coarse data space for W is
Ω(W ) := {(x, U ) | x ∈ W, U ⊆ W : x ∈ U }.
A coarse data distribution is any probability distribution P on Ω(W ).
A coarse data distribution can be seen as the joint distribution P (X, Y ) of a random
variable X with values in W , and a random variable Y with values in 2 W \ ∅. The joint
distribution of X and Y is constrained by the condition X ∈ Y . Note that, thus, coarse
data spaces and coarse data distributions actually represent both the true complete data
and its coarsened observation. In the remainder of this paper, P without any arguments
will always denote a coarse data distribution in the sense of Definition 2.4, and can be
used interchangeably with P (X, Y ). When we need to refer to (joint) distributions of other
random variables, then these are listed explicitly as arguments of P . E.g.: P (X, G) is the
joint distribution of X and G.
Coarsening variables as introduced by the following definition are a means for specifying
the conditional distribution of Y given X.
Definition 2.5 Let G be a random variable with values in a finite state space Γ, and
f : W × Γ → 2W \ ∅,

(8)

such that
• for all x with P (X = x) > 0: x ∈ f (x, g);
• for all x, x0 with P (X = x) > 0, P (X = x0 ) > 0, all U ∈ 2W \ ∅, and all g ∈ Γ:
f (x, g) = U, x0 ∈ U ⇒ f (x0 , g) = U.

(9)

We call the pair (G, f ) a coarsening variable for X. Often we also refer to G alone as a
coarsening variable, in which case the function f is assumed to be implicitly given.
A coarse data distribution P is induced by X and (G, f ) if P is the joint distribution
of X and f (X, G).
The condition (9) has not always been made explicit in the introduction of coarsening
variables. However, as noted by Heitjan (1997), it is usually implied in the concept of
a coarsening variable. GvLR (pp. 283-285) consider a somewhat more general setup in
which f (x, g) does not take values in 2 W directly, but y = f (x, g) is some observable
893

Jaeger

from which U = α(y) ⊆ W is obtained via a further mapping α. The introduction of
such an intermediate observable Y is necessary, for example, when dealing with real-valued
random variables X. Since we then will not have any statistically tractable models for
general distributions on 2R , a parameterization Y for a small subset of 2 R is needed. For
example, Y could take values in R × R, and α(y 1 , y2 ) might be defined as the interval
[min{y1 , y2 }, max{y1 , y2 }]. GvLR do not require (9) in general; instead they call f Cartesian
when (9) is satisfied.
The following definition generalizes property (6) of missingness indicators to arbitrary
coarsening variables.
Definition 2.6 The coarsening variable (G, f ) is called invertible if there exists a function
h : 2W \ ∅ → Γ,

(10)

such that for all x, U with x ∈ U , and all g ∈ Γ:
U = f (x, g) ⇔ g = h(U ).

(11)

An alternative reading of (11) is that G is observable: from the coarse observation U
the value g ∈ Γ can be reconstructed, so that G can be treated as a fully observable random
variable.
We can now generalize the definition of missing (completely) at random to the coarse
data setting. We begin with the generalization of M -mar .
Definition 2.7 (Heitjan, 1997) Let G be a coarsening variable for X. The joint distribution
P (X, G) is G-car if for all U ⊆ W , and g ∈ Γ:
P (G = g | X) is constant on {x | P (X = x) > 0, f (x, g) = U }.

(12)

By marginalizing out the coarsening variable G (or by not assuming a variable G in the
first place), we obtain the following distributional version of car .
Definition 2.8 (Heitjan & Rubin, 1991) Let P be a coarse data distribution. P is d-car if
for all U ⊆ W
P (Y = U | X) is constant on {x | P (X = x) > 0, x ∈ U }.

(13)

If X is multivariate, and incompleteness of observations consists of missing values, then
d-car coincides with d-mar , and M -car with M -mar .
Condition (12) refers to the joint distribution of X and G, condition (13) to the joint
distribution of X and Y . Since Y is a function of X and G, one can always determine
from the joint distribution of X and G whether d-car holds for their induced coarse data
distribution. Conversely, when only the coarse data distribution P (X, Y ) and a coarsening
variable G inducing P (X, Y ) are given, it is in general not possible to determine whether
P (X, G) is G-car , because the joint distribution P (X, G) cannot be reconstructed from
the given information. However, under suitable assumptions on G it is possible to infer
that P (X, G) is G-car only from the induced P (X, Y ) being d-car . With the following
two theorems we clarify these relationships between G-car and d-car . These theorems are
essentially restatements in our conceptual framework of results already given by GvLR (pp.
284-285).
894

Ignorability in Statistical and Probabilistic Inference

Theorem 2.9 A coarse data distribution P (X, Y ) is d-car iff there exists a coarsening
variable G inducing P (X, Y ), such that P (X, G) is G-car.
Proof: First assume that P (X, Y ) is d-car . We construct a canonical coarsening variable
G inducing P (X, Y ) as follows: let Γ = 2 W \ ∅ and f (x, U ) := U for all x ∈ W and U ∈ Γ.
Define a Γ-valued coarsening variable G by P (G = U | X = x) := P (Y = U | X = x).
Clearly, the coarse data distribution induced by G is the original P (X, Y ), and P (X, G) is
G-car .
Conversely, assume that P (X, G) is G-car for some G inducing P (X, Y ). Let U ⊆ W ,
x ∈ U . Then
P (Y = U | X = x) = P (G ∈ {g ∈ Γ | f (x, g) = U } | X = x)
X
=
P (G = g | X = x).
g∈Γ:f (x,g)=U

Because of (9) the summation here is over the same values g ∈ Γ for all x ∈ U . Because
of G-car , the conditional probabilities P (G = g | X = x) are constant for x ∈ U . Thus
P (Y = U | X = x) is constant for x ∈ U , i.e. d-car holds.

The following example shows that d-car does not in general imply G-car , and that a
fixed coarse data distribution P (X, Y ) can be induced both by a coarsening variable for
which G-car holds, and by another coarsening variable for which G-car does not hold.
Example 2.10 (continued from Example 2.3) We have already seen that the coarse data
distribution here is d-mar and M -mar, and hence d-car and M -car.
M is not the only coarsening variable inducing P (X, Y ). In fact, it is not even the
simplest: let G1 be a trivial random variable that can only assume one state, i.e. Γ 1 = {g}.
Define f1 by
f1 ((n, n), g) = f1 ((n, p), g) = {(n, n), (n, p)},
f1 ((p, n), g) = {(p, n)},
f1 ((p, p), g) = {(p, p)}.
Then G1 induces P (X, Y ), and P (X, G1 ) also is trivially G-car.
Finally, let G2 be defined by Γ2 = {g1 , g2 } and f2 (x, gi ) = f1 (x, g) for all x ∈ W and
i = 1, 2. Thus, G2 is just like G1 , but the trivial state space of G1 has been split into two
elements with identical meaning. Let the conditional distribution of G 2 given X be
P (G2 = g1 | X = (n, n)) = P (G2 = g2 | X = (n, p)) = 2/3,
P (G2 = g2 | X = (n, n)) = P (G2 = g1 | X = (n, p)) = 1/3,
P (G2 = g1 | X = (p, n)) = P (G2 = g1 | X = (p, p)) = 1.
Again, G2 induces P (X, Y ). However, P (X, G2 ) is not G-car, because
f2 ((n, n), g1 ) = f2 ((n, p), g1 ) = {(n, n), (n, p)},
P (G2 = g1 | X = (n, n)) 6= P (G2 = g1 | X = (n, p))
violates the G-car condition. G2 is not invertible in the sense of Definition 2.6: when, for
example, U = {(n, n), (n, p)} is observed, it is not possible to determine whether the value
of G2 was g1 or g2 .
895

Jaeger

The following theorem shows that the non-invertibility of G 2 in the preceding example
is the reason why we cannot deduce G-car for P (X, G 2 ) from the d-car property of the
induced P (X, Y ). This theorem completes our picture of the G-car / d-car relationship.
Theorem 2.11 Let P (X, Y ) be a coarse data distribution, G an invertible coarsening variable inducing P (X, Y ). If P (X, Y ) is d-car, then P (X, G) is G-car.
Proof: Let U ⊆ W , g ∈ Γ, and x ∈ U , such that P (X = x) > 0 and f (x, g) = U . Since G
is invertible, we have that f (x, g 0 ) 6= U for all g 0 6= g, and hence
P (G = g | X = x) = P (Y = U | X = x).
From the assumption that P is d-car it follows that the right-hand probability is constant
for x ∈ U , and hence the same holds for the left-hand side, i.e. G-car holds.

We now turn to coarsening completely at random (ccar ). It is straightforward to generalize the definition of M -mcar to general coarsening variables:
Definition 2.12 (Heitjan, 1994) Let G be a coarsening variable for X. The joint distribution P (X, G) is G-ccar if for all g ∈ Γ
P (G = g | X) is constant on {x | P (X = x) > 0}.

(14)

A distributional version of ccar does not seem to have been formalized previously in the
literature. GvLR refer to coarsening completely at random, but do not provide a formal
definition. However, it is implicit in their discussion that they have in mind a slightly
restricted version of our following definition (the restriction being a limitation to the case
k = 1 in Theorem 2.14 below).
We first observe that one cannot give a definition of d-ccar as a variant of Definition 2.12
in the same way as Definition 2.8 varies Definition 2.7, because that would lead us to the
condition that P (Y = U | X) is constant on {x | P (X = x) > 0}. This would be inconsistent
with the existence of x ∈ W \ U with P (X = x) > 0. However, the real semantic core of
d-car , arguably, is not so much captured by Definition 2.8, as by the characterization given
in Theorem 2.9. For d-ccar , therefore, we make an analogous characterization the basis of
the definition:
Definition 2.13 A coarse data distribution P (X, Y ) is d-ccar iff there exists a coarsening
variable G inducing P (X, Y ), such that P (X, G) is G-ccar.
The following theorem provides a constructive characterization of d-ccar .
Theorem 2.14 A coarse data distribution P (X, Y ) is d-ccar iff there exists a family {W 1 ,
. . . , Wk } of partitions of W , and a probability distribution (λ 1 , . . . , λk ) on (W1 , . . . , Wk ),
such that for all x ∈ W with P (X = x) > 0:
X
P (Y = U | X = x) =
λi .
(15)
i∈1,...,k

x∈U ∈Wi

896

Ignorability in Statistical and Probabilistic Inference

⇐⇒

Missing values

d-mar ⇐⇒ M -mar

Coarse observations
(c)
⇐⇒

⇑
M -mcar

G-car

(a)(b)
⇐=
=⇒

⇑
(c)
⇐⇒

G-ccar

d-car
⇑

(a)
⇐=
=⇒

d-ccar

Figure 1: Versions of car . (a): there exists G such that this implication holds; (b): for all
invertible G this implication holds; (c): equivalence holds for G = M .

Proof: Assume that P is d-ccar . Let G be a coarsening variable inducing P (X, Y ), such
that P (X, G) is G-ccar . Because of (9), each value g i ∈ Γ induces a partition Wi =
{Ui,1 , . . . , Ui,k(i) }, such that f (x, gi ) = Ui,j ⇔ x ∈ Ui,j . The partitions Wi together with
λi := P (G = gi | X) then provide a representation of P (X, Y ) in the form (15).
Conversely, if P (X, Y ) is given by (15) via partitions W 1 , . . . , Wk and parameters λi ,
one defines a coarsening variable G with Γ = {1, . . . , k}, P (G = g i | X = x) = λi for all x
with P (X = x) > 0, and f (x, i) as that U ∈ W i containing x. P (X, G) then is G-ccar and
induces P (X, Y ), and hence P (X, Y ) is d-ccar .

As before, we have that the G-ccar property of P (X, G) cannot be determined from the
induced coarse data distribution:
Example 2.15 (continuation of Example 2.10) P (X, Y ) is d-ccar and induced by any of
the three coarsening variables M , G 1 , G2 . However, P (X, G1 ) is G-ccar, while P (X, M )
and P (X, G2 ) are not.
The previous example also shows that no analog of Theorem 2.11 holds for ccar : M
is invertible, but from d-ccar for the induced P (X, Y ) we here cannot infer G-ccar for
P (X, M ).
Figure 1 summarizes the different versions of mar /car we have considered. The distributional versions d-car and d-ccar are weaker than their M - and G- counterparts, and
therefore the less restrictive assumptions. At the same time, they are sufficient to establish
ignorability for most statistical learning and probabilistic inference tasks. For the case of
probabilistic inference this will be detailed by Theorem 2.18 in the following section. For
statistical inference problems, too, the required ignorability results can be obtained from
the distributional car versions, unless a specific coarsening variable is explicitly part of the
inference problem. Whenever a coarsening variable G is introduced only as an artificial
construct for modeling the connection between incomplete observations and complete data,
one must be aware that the G-car and G-ccar conditions can be unnecessarily restrictive,
and may lead us to reject ignorability when, in fact, ignorability holds (cf. Examples 2.3
and 2.15).
897

Jaeger

We conclude this section with three additional important remarks on definitions of car ,
which are needed to complete the picture of different approaches to car in the literature:
Remark 1: All of the definitions given here are weak versions of mar /car . Corresponding strong versions are obtained by dropping the restriction P (X = x) > 0 from
(3),(5),(7),(12),(13),(14), respectively (15). Differences between weak and strong versions
of car are studied in previous work (Jaeger, 2005). The results there obtained indicate that
in the context of probability updating the weak versions are more suitable. For this reason
we do not go into the details of strong versions here.
Remark 2: Our definitions of car differ from those originally given by Rubin and Heitjan in that our definitions are “global” definitions that view mar /car as a property of a
joint distribution of complete and coarse data. The original definitions, on the other hand,
are conditional on a single observation Y = U , and do not impose constraints on the joint
distribution of X and Y for other values of Y . These “local” mar /car assumptions are
all that is required to justify the application of certain probabilistic or statistical inference
techniques to the single observation Y = U . The global mar /car conditions we stated justify these inference techniques as general strategies that would be applied to any possible
observation. Local versions of car are more natural under a Bayesian statistical philosophy,
whereas global versions are required under a frequentist interpretation. Global versions of
car have also been used in other works (e.g., Jacobsen & Keiding, 1995; Gill et al., 1997;
Nielsen, 1997; Cator, 2004).
Remark 3: The definitions and results stated here are strictly limited to the case of
finite W . As already indicated in the discussion following Definition 2.5, extensions of car
to more general state spaces C typically require a setup in which observations are modeled
by a random variable taking values in a more manageable state space than 2 C . Several such
formalizations of car for continuous state spaces have been investigated (e.g., Jacobsen &
Keiding, 1995; Gill et al., 1997; Nielsen, 2000; Cator, 2004).
2.2 Ignorability
Car and mar assumptions are needed for ignoring the distinction between “U is observed”
and “U has occurred” in statistical inference and probability updating. In statistical inference, for example, d-car is required to justify likelihood maximizing techniques like the
EM algorithm (Dempster, Laird, & Rubin, 1977) for learning from incomplete data. In this
paper the emphasis is on probability updating. We therefore briefly review the significance
of car in this context. We use the well-known Monty Hall problem.
Example 2.16 A contestant at a game show is asked to choose one from three closed doors
A, B, C, behind one of which is hidden a valuable prize, the others each hiding a goat. The
contestant chooses door A, say. The host now opens door B, revealing a goat. At this point
the contestant is allowed to change her choice from A to C. Would this be advantageous?
Being a savvy probabilistic reasoner, the contestant knows that she should analyze the
situation using the coarse data space Ω({A, B, C}), and compute the probabilities
P (X = A | Y = {A, C}), P (X = C | Y = {A, C}).
898

Ignorability in Statistical and Probabilistic Inference

She makes the following assumptions: 1. A-priori all doors are equally likely to hide the
prize. 2. Independent from the contestants choice, the host will always open one door. 3.
The host will never open the door chosen by the contestant. 4. The host will never open
the door hiding the prize. 5. If more than one possible door remain for the host, he will
determine by a fair coin flip which one to open. From this, the contestant first obtains
P (Y = {A, C} | X = A) = 1/2, P (Y = {A, C} | X = C) = 1,

(16)

and then
P (X = A | Y = {A, C}) = 1/3, P (X = C | Y = {A, C}) = 2/3.
The conclusion, thus, is that it will be advantageous to switch to door C. A different
conclusion is obtained by simply conditioning in the state space W on “{A, C} has occurred”:
P (X = A | X ∈ {A, C}) = 1/2, P (X = C | X ∈ {A, C}) = 1/2.
Example 2.17 Consider a similar situation as in the previous example, but now assume
that just after the contestant has decided for herself that she would pick door A, but before
communicating her choice to the host, the host says “let me make things a little easier for
you”, opens door B, and reveals a goat. Would changing from A to C now be advantageous?
The contestant performs a similar analysis as before, but now based on the following
assumptions: 1. A-priori all doors are equally likely to hide the prize. 2. The host’s decision
to open a door was independent from the location of the prize. 3. Given his decision to open
a door, the host chose by a fair coin flip one of the two doors not hiding the prize. Now
P (Y = {A, C} | X = A) = P (Y = {A, C} | X = C),

(17)

and hence
P (X = A | Y = {A, C}) = 1/2, P (X = C | Y = {A, C}) = 1/2.
In particular here
P (X = A | Y = {A, C}) = P (X = A | X ∈ {A, C})
P (X = C | Y = {A, C}) = P (X = C | X ∈ {A, C}),
i.e. the difference between “{A, C} is observed” and “{A, C} has occurred” can be ignored
for probability updating.
The coarse data distribution in Example 2.16 is not d-car (as evidenced by (16)), whereas
the coarse data distribution in Example 2.17 is d-car (as shown, in part, by (17)). The
connection between ignorability in probability updating and the d-car assumption has been
shown in GvLR and GH. The following theorem restates this connection in our terminology.
Theorem 2.18 Let P be a coarse data distribution. The following are equivalent:
(i) P is d-car.
(ii) For all x ∈ W , U ⊆ W with x ∈ U and P (Y = U ) > 0:
P (X = x | Y = U ) = P (X = x | X ∈ U ).
899

Jaeger

(iii) For all x ∈ W , U ⊆ W with x ∈ U and P (X = x) > 0:
P (Y = U | X = x) =

P (Y = U )
.
P (X ∈ U )

The equivalence (i)⇔(ii) is shown in GH, based on GvLR. For the equivalence with (iii)
see (Jaeger, 2005).

3. Criteria for Car and Ccar
Given a coarse data distribution P it is, in principle, easy to determine whether P is d-car
(d-ccar ) based on Definition 2.8, respectively Theorem 2.14 (though in case of d-ccar a
test might require a search over possible families of partitions). However, typically P is
not completely known. Instead, we usually have some partial information about P . In the
case of statistical inference problems this information consists of a sample U 1 , . . . , UN of
the coarse data variable Y . In the case of conditional probabilistic inference, we know the
marginal of P on W . In both cases we would like to decide whether the partial knowledge
of P that we possess, in conjunction with certain other assumptions on the structure of
P that we want to make, is consistent with d-car , respectively d-ccar , i.e. whether there
exists a distribution P that is d-car (d-ccar ), and satisfies our partial knowledge and our
additional assumptions.
In statistical problems, additional assumptions on P usually come in the form of a
parametric representation of the distribution of X. When X = (X 1 , . . . , Xk ) is multivariate,
such a parametric representation can consist, for example, in a factorization of the joint
distribution of the Xi , as induced by certain conditional independence assumptions. In
probabilistic inference problems an analysis of the evidence gathering process can lead to
assumptions about the likelihoods of possible observations. In all these cases, one has to
determine whether the constraints imposed on P by the partial knowledge and assumptions
are consistent with the constraints imposed by the d-car assumption. In general, this will
lead to computationally very difficult optimization or constraint satisfaction problems.
Like GH, we will focus in this section on a rather idealized special problem within this
wider area, and consider the case where our constraints on P only establish what values the
variables X and Y can assume with nonzero probability, i.e. the constraints on P consist
of prescribed sets of support for X and Y . We can interpret this special case as a reduced
form of a more specific statistical setting, by assuming that the observed sample U 1 , . . . , UN
only is used to infer what observations are possible, and that the parametric model for
X, too, only is used to determine what x ∈ W have nonzero probabilities. Similarly,
in the probabilistic inference setting, this special case occurs when the knowledge of the
distribution of X only is used to identify the x with P (X = x) > 0, and assumptions on
the evidence generation only pertain to the set of possible observations.
GH represent a specific support structure of P in form of a 0, 1-matrix, which they
call the “CARacterizing matrix”. In the following definition we provide an equivalent, but
different, encoding of support structures of P .
Definition 3.1 A support hypergraph (for a given coarse data space Ω(W )) is a hypergraph
of the form (N , W 0 ), where
900

Ignorability in Statistical and Probabilistic Inference

• N ⊆ 2W \ ∅ is the set of nodes,
• W 0 ⊆ W is the set of edges, such that each edge x ∈ W 0 just contains the nodes
{U ∈ N | x ∈ U }.
(N , W 0 ) is called the support hypergraph of the distribution P on Ω(W ) iff N = {U ⊆
2W \ ∅ | P (Y = U ) > 0}, and W 0 = {x ∈ W | P (X = x) > 0}. A support hypergraph is
car-compatible iff it is the support hypergraph of some d-car distribution P .
PSfrag replacements
A
{A, C}

{A, B}

B

{A, B}
{A, C}

C B
A

{B, C}
C
(b)

(a)

Figure 2: Support hypergraphs for Examples 2.16 and 2.17
Example 3.2 Figure 2 (a) shows the support hypergraph of the coarse data distribution in
Example 2.16; (b) for Example 2.17.
The definition of support hypergraph may appear strange, as a much more natural definition would take the states x ∈ W with P (X = x) > 0 as the nodes, and the observations
U ⊆ W as the edges. The support hypergraph of Definition 3.1 is just the dual of this
natural support hypergraph. It turns out that these duals are more useful for the purpose
of our analysis.
A support hypergraph can contain multiple edges containing the same nodes. This
corresponds to multiple states that are not distinguished by any of the possible observations.
Similarly, a support hypergraph can contain multiple nodes that belong to exactly the same
edges. This corresponds to different observations U, U 0 with U ∩ {x | P (X = x) > 0} =
U 0 ∩ {x | P (X = x) > 0}. On the other hand, a support hypergraph cannot contain any
node that is not contained in at least one edge (this would correspond to an observation U
with P (Y = U ) > 0 but P (X ∈ U ) = 0). Similarly, it cannot contain empty edges. These
are the only restrictions on support hypergraphs:
Theorem 3.3 A hypergraph (N , E) with finite N and E is the support hypergraph of some
distribution P , iff each node in N is contained in at least one edge from E, and all edges
are nonempty.
Proof: Let W = E and define P (X = x) = 1/ | E | for all x ∈ W . For each node n ∈ N let
U (n) be {x ∈ W | n ∈ x} (nonempty!), and define P (Y = U (n) | X = x) = 1/ | x |. Then
(N , E) is the support hypergraph of P .


901

Jaeger

While (almost) every hypergraph, thus, can be the support hypergraph of some distribution, only rather special hypergraphs can be the support hypergraphs of a d-car distribution.
Our goal, now, is to characterize these car -compatible support hypergraphs. The following
proposition gives a first such characterization. It is similar to lemma 4.3 in GH.
Proposition 3.4 The support hypergraph (N , W 0 ) is car-compatible iff there exists a function ν : N → (0, 1], such that for all x ∈ W 0
X

ν(U ) = 1

(18)

U ∈N :U ∈x

Proof: First note that in this proposition we are looking at x and U as edges and nodes,
respectively, of the support hypergraph, so that writing U ∈ x makes sense, and means the
same as x ∈ U when x and U are seen as states, respectively sets of states, in the coarse
data space.
Suppose (N , W 0 ) is the support hypergraph of a d-car distribution P . It follows from
Lemma 2.18 that ν(U ) := P (Y = U )/P (X ∈ U ) defines a function ν with the required
property. Conversely, assume that ν is given. Let P (X) be any distribution on W with
support W 0 . Setting P (Y = U | X = x) := ν(U ) for all U ∈ N , and x ∈ W 0 ∩ U extends P
to a d-car distribution whose support hypergraph is just (N , W 0 ).


Corollary 3.5 If the support hypergraph contains (properly) nested edges, then it is not
car-compatible.
Example 3.6 The support hypergraph from Example 2.16 contains nested edges. Without
any numerical computations, it thus follows alone from the qualitative analysis of what
observations could have been made, that the coarse data distribution is not d-car, and hence
conditioning is not a valid update strategy.
The proof of Proposition 3.4 shows (as already observed by GH ) that if a support
hypergraph is car -compatible, then it is car -compatible for any given distribution P (X)
with support W 0 , i.e. the support assumptions encoded in the hypergraph, together with the
d-car assumption (if jointly consistent), do not impose any constraints on the distribution
of X (other than having the prescribed set of support). The same is not true for the
marginal of Y : for a car -compatible support hypergraph (N , W 0 ) there will usually also
exist distributions P (Y ) on N such that P (Y ) cannot be extended to a d-car distribution
with the support structure specified by the hypergraph (N , W 0 ).
Proposition 3.4 already provides a complete characterization of car -compatible support
hypergraphs, and can be used as the basis of a decision procedure for car -compatibility
using methods for linear constraint satisfaction. However, Proposition 3.4 does not provide
very much real insight into what makes an evidence hypergraph car -compatible. Much
more intuitive insight is provided by Corollary 3.5. The criterion provided by Corollary 3.5
is not complete: as the following example shows, there exist support hypergraphs without
nested edges that are not car -compatible.
902

PSfrag replacements

Ignorability in Statistical and Probabilistic Inference

x1

x2

U1

U4

U2

U5

U3

U6

x3
x4

x5
Figure 3: car -incompatible support hypergraph without nested edges
Example 3.7 Let (N , W 0 ) be as shown in Figure 3. By assuming the existence of a suitable
function ν, and summing
(18) once over x 1 and x2 , and once over x3 , x4 , x5 , we obtain the
P
contradiction 2 = 6i=1 ν(Ui ) = 3. Thus, (N , W 0 ) is not car-compatible.
We now proceed to extend the partial characterization of car -compatibility provided by
Corollary 3.5 to a complete characterization. Our following result improves on theorem 4.4
of GH by giving a necessary and sufficient condition for car -compatibility, rather than just
several necessary ones, and, arguably, by providing a criterion that is more intuitive and
easier to apply. Our characterization is based on the following definition.

Definition 3.8 Let (N , W 0 ) be a support hypergraph. Let x = x1 , . . . , xk be a finite sequence of edges from W 0 , possibly containing repetitions of the same edge. Denote the
length k of the sequence by | x |. For x ∈ W we denote with 1 x the indicator function on N
induced by x, i.e.

1 if U ∈ x
1x (U ) :=
(U ∈ N ).
0 else
P
The function 1x (U ) := x∈x 1x (U ) then counts the number of edges in x that contain U .
For two sequences x, x0 we write 1x ≤ 1x0 iff 1x (U ) ≤ 1x0 (U ) for all U .
Example 3.9 For the evidence hypergraph in Figure 3 we have that 1 (x1 ,x2 ) = 1(x3 ,x4 ,x5 ) is
the function on N which is constant 1.
For x = (x1 , x3 , x4 , x5 ) one obtains 1x (U ) = 2 for U = U1 , U2 , U3 , and 1x (U ) = 1 for
U = U4 , U5 , U6 . The same function also is defined by x = (x 1 , x1 , x2 ).
In any evidence hypergraph, one has that for two single edges x, x 0 : 1x < 1x0 iff x is a
proper subset of x0 .
We now obtain the following characterization (which is partly inspired by known conditions for the existence of finitely additive measures, see Bhaskara Rao & Bhaskara Rao,
1983):
Theorem 3.10 The support hypergraph (N , W 0 ) is car-compatible iff for every two sequences x, x0 of edges from W 0 we have
1x = 1 x 0

⇒ | x |=| x0 |, and

(19)
0

1x ≤ 1x0 , 1x 6= 1x0 ⇒ | x |<| x | .
903

(20)

Jaeger

Proof: Denote k :=| W 0 |, l :=| N |. Let A = (ai,j ) be the incidence matrix of (N , W 0 ), i.e.
A is an k × l matrix with ai,j = 1 if Uj ∈ xi , and ai,j = 0 if Uj 6∈ xi (using some indexings
i, j for W 0 and N ).
Condition (3.4) now reads:
exists ν ∈ (0, 1]l with Aν = 1

(21)

(here 1 is a vector of k ones). An edge indicator function 1 x can be represented as a row
vector z ∈ Nk , where zi is the number of times xi occurs in x. Then 1x can be written as
the row vector zA, and the conditions of Theorem 3.10 become: for all z, z 0 ∈ Nk :
zA = z 0 A ⇒ z · 1 = z 0 · 1,
0

(22)

0

0

zA ≤ z A, zA 6= z A ⇒ z · 1 < z · 1.

(23)

Subtracting right sides, this is equivalent to: for all z ∈ Z k :
zA = 0 ⇒ z · 1 = 0,

(24)

zA ≤ 0, zA 6= 0 ⇒ z · 1 < 0.

(25)

Using Farkas’s lemma (see e.g. Schrijver, 1986, Section 7.3), one now obtains that conditions (24) and (25) are necessary and sufficient for (21). For the application of Farkas’s
lemma to our particular setting one has to observe that since A and 1 are rational, it is
sufficient to have (24) and (25) for rational z (cf. Schrijver, 1986[p.85]). This, in turn, is
equivalent to having (24) and (25) for integer z. The strict positivity of the solution ν can
be derived from conditions (24) and (25) by analogous arguments as for Corollary 7.1k in
(Schrijver, 1986).

Example 3.11 From Example 3.9 we immediately obtain that nested edges x, x 0 violate
(20), and hence we again obtain Corollary 3.5. Also the sequences (x 1 , x2 ) and (x3 , x4 , x5 ) of
the support hypergraph in Figure 3 violate (19), so that we again obtain the car-incompatibility of that hypergraph.
PSfrag replacements

(a)

(b-i)

(b-ii)

(c)

(d)

Figure 4: Car -compatible support hypergraphs with three nodes
Example 3.12 GH (Example 4.6) derive a complete characterization of car-compatibility
for the case that exactly three different observations can be made with positive probability. In
our framework this amounts to finding all support hypergraphs with three nodes that satisfy
the conditions of Theorem 3.10. The possible solutions are shown in Figure 4 (omitting
equivalent solutions obtained by duplicating edges). The labeling (a)-(d) of the solutions
904

Ignorability in Statistical and Probabilistic Inference

corresponds to the case enumeration in GH. It is easy to verify that the shown support
hypergraphs all satisfy (19) and (20). That these are the only such hypergraphs follows from
the facts that adding a new edge to any of the shown hypergraphs either leads to a hypergraph
already on our list (only possible for the pair (b-i) and (b-ii)), or introduces a pair of nested
edges. Similarly, deleting any edge either leads to a hypergraph already shown, or to an
invalid hypergraph in which not all nodes are covered.

4. Procedural Models
So far we have emphasized the distributional perspective on car . We have tried to identify car from the joint distribution of complete and coarse data. From this point of view
coarsening variables are an artificial construct that is introduced to describe the joint distribution. In some cases, however, a coarsening variable can also model an actual physical,
stochastic process that leads to the data coarsening. In such cases, the analysis should
obviously take this concrete model for the underlying coarsening process into account. In
this section we study d-car distributions in terms of such procedural models for the data
generating mechanism. Our results in this section extend previous investigations of car
mechanisms in GvLR and GH.
Our first goal in this section is to determine canonical procedural models for coarsening mechanisms leading to d-car data. Such canonical models can be used in practice for
evaluating whether a d-car assumption is warranted for a particular data set under investigation by matching the (partially known or hypothesized) coarsening mechanism of the
data against any of the canonical models. Our investigation will then focus on properties
that one may expect reasonable or natural procedural models to possess. These properties
will be captured in two formal conditions of honesty and robustness. The analysis of these
conditions will provide new strong support for the d-ccar assumption.
The following definition of a procedural model is essentially a generalization of coarsening
variables, obtained by omitting condition (9), and by replacing the single variable G with
a (potentially infinite) sequence G.
Definition 4.1 Let P be a coarse data distribution on Ω(W ). A procedural model for P
is given by
• A random variable X distributed according to the marginal of P on W .
• A finite or infinite sequence G = G1 , G2 , . . . , of random variables, such that G i takes
values in a finite set Γi (i ≥ 1).
• A function f : W × ×i Γi → 2W \ ∅, such that (X, f (X, G)) is distributed according
to P .
We also call a procedural model (X, G, f ) a car model (ccar model), if the coarse data
distribution P it defines is d-car (d-ccar ). In the following we denote × i Γi with Γ.
Some natural coarsening processes are modeled with real-valued coarsening variables
(e.g. censoring times, Heitjan & Rubin, 1991). We can accommodate real-valued variables
Z in our framework by identifying Z with a sequence of binary random variables Z i (i ≥ 1)
defining its binary representation. A sequence G containing a continuous Z = G i can then
905

Jaeger

be replaced with a sequence G0 in which the original variables Gj (j 6= i) are interleaved
with the binary Zi .
In the following, we will not discuss measurability issues in detail (only in Appendix A
will a small amount of measure theory be needed). It should be mentioned, however, that it
is always assumed that W and the Γi are equipped with a σ-algebra equal to their powerset,
that on W × Γ we have the generated product σ-algebra, and that f −1 (U ) is measurable
for all U ⊆ W .
f2.16
h
t

A
{A, C}
{A, B}

B
{A, B}
{A, B}

C
{A, C}
{A, C}

f2.17
h
t

A
{A, C}
{A, B}

B
{B, C}
{A, B}

C
{B, C}
{A, C}

Table 1: Procedural models for Examples 2.16 and 2.17

Example 4.2 Natural procedural models for the coarse data distributions of Examples 2.16
and 2.17 are constructed by letting G = F represent the coin flip that determines the door
to be opened. Suppose that in both examples the host has the following rule for matching
the result of the coin flip with potential doors for opening: when the coin comes up heads,
then the host opens the door that is first in alphabetical order among all doors (two, at
most) that his rules permit him to open. If the coin comes up tails, he opens the door last
in alphabetical order. This is formally represented by a procedural model in which X is
a {A, B, C}-valued, uniformly distributed random variable, G consists of a single {h, t}valued, uniformly distributed random variable F , and X and F are independent.
Table 1 completes the specification of the procedural models by defining the functions
f2.16 and f2.17 for the respective examples. Note that neither (F, f 2.16 ) nor (F, f2.17 ) are
coarsening variables in the sense of Definition 2.5, as e.g. A ∈ f 2.16 (B, h) 6= f2.16 (A, h), in
violation of (9).
The two procedures described in the preceding example appear quite similar, yet one
produces a d-car distribution while the other does not. We are now interested in identifying
classes of procedural models that are guaranteed to induce d-car (d-ccar ) distributions.
Conversely, for any given d-car distribution P , we would like to identify procedural models
that might have induced P . We begin with a class of procedural models that stands in a
trivial one-to-one correspondence with d-car distributions.
Example 4.3 ( Direct car model) Let X be a W -valued random variable, and G = G 1 with
Γ1 = 2W \ ∅. Let the joint distribution of X and G 1 be such that P (G1 = U | X = x) = 0
when x 6∈ U , and
P (G1 = U | X = x) is constant on {x | P (X = x) > 0, x ∈ U }.

(26)

Define f (x, U ) = U . Procedural models of this form are just the coarsening variable representations of d-car distributions that we already encountered in Theorem 2.9. Hence, a
coarse data distribution P is d-car iff it is induced by a direct car model.
906

Ignorability in Statistical and Probabilistic Inference

The direct car models are not much more than a restatement of the d-car definition.
They do not help us very much in our endeavor to identify canonical observational or datagenerating processes that will lead to d-car distributions, because the condition (26) does
not correspond to an easily interpretable condition on an experimental setup.
For d-ccar the situation is quite different: here a direct encoding of the d-ccar condition
leads to a rather natural class of procedural models. The class of models described next
could be called, in analogy to Example 4.3, “direct ccar models”. Since the models here
described permit a more natural interpretation, we give it a different name, however.
Example 4.4 ( Multiple grouped data model, MGD) Let X be a W -valued random variable. Let (W1 , . . . , Wk ) be a family of partitions of W (cf. Theorem 2.14). Let G = G 1 ,
where G1 takes values in {1, . . . , k} and is independent of X. Define f (x, i) as that U ∈ W i
that contains x. Then (X, G1 , f ) is ccar. Conversely, every d-ccar coarse data model is
induced by such a multiple grouped data model.
The multiple grouped data model corresponds exactly to the CARgen procedure of GH.
It allows intuitive interpretations as representing procedures where one randomly selects one
out of k different available sensors or tests, each of which will reveal the true value of X only
up to the accuracy represented by the set U ∈ W i containing x. In the special case k = 1 this
corresponds to grouped or censored data (Heitjan & Rubin, 1991). GH introduced CARgen
as a procedure that is guaranteed to produce d-car distributions. They do not consider dccar , and therefore do not establish the exact correspondence between CARgen and d-ccar .
In a similar vein, GvLR introduced a general procedure for generating d-car distributions.
The following example rephrases the construction of GvLR in our terminology.
Example 4.5 ( Randomized monotone coarsening, RMC) Let X be a W -valued random
variable. Let G = H1 , S1 , H2 , S2 , . . . , Sn−1 , Hn , where the Hi take values in 2W , and the Si
are {0, 1}-valued. Define

Hi
if X ∈ Hi
H̄i :=
W \ Hi if X 6∈ Hi .
Let the conditional distribution of H i given X and H1 , . . . , Hi−1 be concentrated on subsets
of ∩i−1
j=1 H̄j .
This model represents a procedure where one successively refines a “current” coarse data
set Ai := ∩i−1
h=1 H̄h by selecting a random subset H i of Ai and checking whether X ∈ Hi or
not, thus computing H̄i and Ai+1 . This process is continued until for the first time S i = 1
(i.e. the Si represent stopping conditions). The result of the procedure, then is represented
by the following function f :
min{k|Sk =1}

f (X, (H1 , S1 , . . . , Sn−1 , Hn )) = ∩i=1

H̄i .

Finally, we impose the conditional independence condition that the distribution of the
Hi , Si depend on X only through H̄1 , . . . , H̄i−1 , i.e.
P (Hi | X, H1 , . . . , Hi−1 ) = P (Hi | H̄1 , . . . , H̄i−1 )
P (Si | X, H1 , . . . , Hi−1 ) = P (Si | H̄1 , . . . , H̄i−1 ).
907

Jaeger

As shown in GvLR, an RMC model always generates a d-car distribution, but not every
d-car distribution can be obtained in this way. GH state that RMC models are a special case
of CARgen models. As we will see below, CARgen and RMC are actually equivalent, and
thus, both correspond exactly to d-ccar distributions. The distribution of Example 2.17
is the standard example (already used in a slightly different form in GvLR) of a d-car
distribution that cannot be generated by RMC or CARgen. A question of considerable
interest, then, is whether there exist natural procedural models that correspond exactly to
d-car distributions. GvLR state that they “cannot conceive of a more general mechanism
than a randomized monotone coarsening scheme for constructing the car mechanisms which
one would expect to meet with in practice,. . . ”(p.267). GH, on the other hand, generalize
the CARgen models to a class of models termed CARgen ∗ , and show that these exactly
comprise the models inducing d-car distributions.
However, the exact extent to which CARgen ∗ is more natural or reasonable than the
trivial direct car models has not been formally characterized. We will discuss this issue
below. First we present another class of procedural models. This is a rather intuitive class
which contains models not equivalent to any CARgen/RMC model.
Example 4.6 ( Uniform noise model) Let X be a W -valued random variable. Let G =
N1 , H1 , N2 , H2 , . . ., where the Ni are {0, 1}-valued, and the Hi are W -valued with
P (Hi = x) = 1/ | W |

(x ∈ W ).

(27)

Let X, N1 , H1 , . . . be independent. Define for hi ∈ W, ni ∈ {0, 1}:
f (x, (n1 , h1 , . . .)) = {x} ∪ {hi | i : ni = 1}.

(28)

This model describes a procedure where in several steps (perhaps infinitely many) uniformly
selected states from W are added as noise to the observation. The random variables N i
represent events that cause additional noise to be added. The distributions generated by this
procedure are d-car, because for all x, U with x ∈ U :
P (Y = U | X = x) = P ({hi | i : ni = 1} = U ) + P ({hi | i : ni = 1} = U \ {x}).
By the uniformity condition (27), and the independence of the family {X, N 1 , H1 , . . .}, the
last probability term in this equation is constant for x ∈ U .
The uniform noise model can not generate exactly the d-car distribution of Example 2.17.
However, it can generate the variant of that distribution that was originally given in GvLR.
The uniform noise model is rather specialized, and far from being able to induce every
possible d-car distribution. As mentioned above, GH have proposed a procedure called
CARgen∗ for generating exactly all d-car distributions. This procedure is described in GH
in the form of a randomized algorithm, but it can easily be recast in the form of a procedural
model in the sense of Definition 4.1. We shall not pursue this in detail, however, and instead
present a procedure that has the same essential properties as CARgen ∗ (especially with
regard to the formal “reasonableness conditions” we shall introduce below), but is somewhat
simpler and perhaps slightly more intuitive.
908

Ignorability in Statistical and Probabilistic Inference

Example 4.7 (Propose and test model, P&T)) Let X be a W -valued random variable. Let
G = G1 , G2 , . . . be an infinite sequence of random variables taking values in 2 W \ ∅. Let
X, G1 , G2 , . . . be independent, and the Gi be identically distributed, such that
X
P (Gi = U ) is constant on {x ∈ W | P (X = x) > 0}.
(29)
U :x∈U

Define
f (x, (U1 , U2 , . . .) :=



Ui
W

if i = min{j ≥ 1 | x ∈ Uj }
.
if {j ≥ 1 | x ∈ Uj } = ∅

The P&T model describes a procedure where we randomly propose a set U ⊆ W , test
whether x ∈ U , and return U if the result is positive (else continue). The condition (29)
can be understood as an unbiasedness condition, which ensures that for every x ∈ W (with
P (X = x) > 0) we are equally likely to draw a positive test for x. The following theorem
is analogous to Theorem 4.9 in GH ; the proof is much simpler, however.
Theorem 4.8 A coarse data distribution P is d-car iff it can be induced by a P&T model.
Proof: That every distribution induced by a P&T model is d-car follows immediately from
X
P (Y = U | X = x) = P (Gi = U )/
P (Gi = U 0 ).
(30)
U 0 :x∈U 0

By (29) this is constant on {x ∈ U | P (X = x) > 0} (note, too, that (29) ensures that the
sum in the denominator of (30) is nonzero for all x, and that in the definition of f the case
{j ≥ 1 | x ∈ Uj } = ∅ only occurs with probability zero).
P
Conversely, let P be a d-car distribution on Ω(W ). Define c := U ∈2W P (Y = U | X ∈
U ), and
P (Gi = U ) = P (Y = U | X ∈ U )/c.
Since
P (Y = U | X ∈ U ) = P (Y = U | X = x) for all x ∈ U with P (X = x) > 0, we have
P
U :x∈U P (Y = U | X ∈ U ) = 1 for all x ∈ W with P (X = x) > 0. It follows that (29) is
satisfied with 1/c being the constant. The resulting P &T model induces the original P :
P (f (X, G) = U | X = x) = (P (Y = U | X ∈ U )/c)/(

X

P (Y = U 0 | X ∈ U 0 )/c)

U 0 :x∈U 0

= P (Y = U | X ∈ U ) = P (Y = U | X = x).

The P&T model looks like a reasonable natural procedure. However, it violates a desideratum that GvLR have put forward for a natural coarsening procedure:
(D) In the coarsening procedure, no more information about the true value
of X should be used than is finally revealed by the coarse data variable Y (Gill
et al., 1997, p.266, paraphrased).
909

Jaeger

The P&T model violates desideratum (D), because when we first unsuccessfully test U 1 , . . . ,
Uk , then we require the information x 6∈ ∪ ki=1 Ui , which is not included in the final data
Y = Uk+1 . The observation generating process of Example 2.17, too, appears to violate
(D), as the host requires the precise value of X when following his strategy. Finally, the
uniform noise model violates (D), because in the computation (28) of the final coarse data
output the exact value of X is required. These examples suggest that (D) is not a condition
that one must necessarily expect every natural coarsening procedure to possess. (D) is
most appropriate when coarse data is generated by an experimental process that is aimed
at determining the true value of X, but may be unable to do so precisely. In such a scenario,
(D) corresponds to the assumption that all information about the value of X that is collected
in the experimental process also is reported in the final result. Apart from experimental
procedures, also ’accidental’ processes corrupting complete data can generate d-car data (as
represented, e.g., by the uniform noise model). For such procedures (D) is not immediately
seen as a necessary feature. However, Theorem 4.17 below will lend additional support to
(D) also in these cases.
GH argue that their class of CARgen∗ procedures only contains reasonable processes,
because “each step of the algorithm can depend only on information available to the experimenter, where the ’information’ is encoded in the observations made by the experimenter
in the course of running the algorithm”(GH, p. 260). The same can be said about the
P&T procedure. The direct car model would not be reasonable in this sense, because for
the simulation of the variable G one would need to pick a distribution dependent on the
true value of X, which is not assumed to be available. However, it is hard to make rigorous
this distinction between direct car models on the one hand, and CARgen ∗ /P&T on the
other hand, because the latter procedures permit tests for the value of X (through checking
X ∈ U for test sets U – using singleton sets U one can even query the exact value of X),
and the continuation of the simulation is dependent on the outcome of these tests.
We will now establish a more solid foundation for discussing reasonable vs. unreasonable coarsening procedures by introducing two different rigorous conditions for natural or
reasonable car procedures. One is a formalization of desideratum (D), while the other
expresses an invariance of the car property under numerical parameter changes. We will
then show that these conditions can only be satisfied when the generated distribution is
d-ccar . For the purpose of this analysis it is helpful to restrict attention to a special type
of procedural models.
Definition 4.9 A procedural model (X, G, f ) is a Bernoulli-model if the family X, G 1 ,
G2 , . . . is independent.
The name Bernoulli model is not quite appropriate here, because the variables X, G i
are not necessarily binary. However, it is clear that one could also replace the multinomial
X and Gi with suitable sets of (independent) binary random variables. In essence, then,
a Bernoulli model in the sense of Definition 4.9 can be seen as an infinite sequence of
independent coin tosses (with coins of varying bias). Focusing on Bernoulli models is no
real limitation:
Theorem 4.10 Let (X, G, f ) be a procedural model. Then there exists a Bernoulli model
(X, G∗ , f ∗ ) inducing the same coarse data distribution.
910

Ignorability in Statistical and Probabilistic Inference

The reader may notice that the statement of Theorem 4.10 really is quite trivial: the
coarse data distribution induced by (X, G, f ) is just a distribution on the finite coarse
data space Ω(W ), and there are many simple, direct constructions of Bernoulli models for
such a given distribution. The significance of Theorem 4.10, therefore, lies essentially in
the following proof, where we construct a Bernoulli model (X, G ∗ , f ∗ ) that preserves all
the essential procedural characteristics of the original model (X, G, f ). In fact, the model
(X, G∗ , f ∗ ) can be understood as an implementation of the procedure (X, G, f ) using a
generator for independent random numbers.
To understand the intuition of the construction, consider a randomized algorithm for
simulating the procedural model (X, G, f ). The algorithm successively samples values for
X, G1 , G2 , . . ., and finally computes f (for most natural procedural models the value of f
is already determined by finitely many initial G i -values, so that not infinitely many G i
need be sampled, and the algorithm actually terminates; for our considerations, however,
algorithms taking infinite time pose no conceptual difficulties). The distribution used for
sampling Gi may depend on the values of previously sampled G 1 , . . . , Gi−1 , which, in a
computer implementation of the algorithm are encoded in the current program state.
The set of all possible runs of the algorithm can be represented as a tree, where branching nodes correspond to sampling steps for the G i . A single execution of the algorithm
generates one branch in this tree. One can now construct an equivalent algorithm that,
instead, generates the whole tree breadth-first, and that labels each branching node with
a random value for the Gi associated with the node, sampled according to the distribution
determined by the program state corresponding to that node. In this algorithm, sampling
of random values is independent. The labeling of all branching nodes identifies a unique
branch in the tree, and for each branch, the probability of being identified by the labeling is
equal to the probability of this branch representing the execution of the original algorithm
(a similar transformation by pre-computing all random choices that might become relevant
is described in by Gill & J.M.Robins, 2001[Section 7]). The following proof formalizes the
preceding informal description.
Proof of Theorem 4.10: For each random variable G i we introduce a sequence of random
variables G∗i,1 , . . . , G∗i,K(i) , where K(i) =| W × ×i−1
j=1 Γj | is the size of the joint state space of
∗
X, G1 , . . . , Gi−1 . The state space of the Gi,h is Γi (with regard to our informal explanation,
G∗i,h corresponds to the node in the full computation tree that represent the sampling of G i
when the previous execution has resulted in the hth out of K(i) possible program states).
We construct a joint distribution for X and the G ∗i,h by setting P (G∗i,h = v) = P (Gi = v |
i−1
Γj ), and by taking
(X, G1 , . . . , Gi−1 ) = sh ) (sh the hth state in an enumeration of W × × j=1
∗
X and the Gi,h to be independent.
It is straightforward to define a mapping
K(i)

h∗ : W × ×i≥1 Γi

→Γ

such that (X, h∗ (X, G∗ )) is distributed as (X, G) (the mapping h ∗ corresponds to the extraction of the “active” branch in the full labeled computation tree). Defining f ∗ (x, g ∗ ) :=
f (x, h∗ (x, g ∗ )) then completes the construction of the Bernoulli model.


911

Jaeger

Definition 4.11 The Bernoulli model (X, G ∗ , f ∗ ) obtained via the construction of the proof
of Theorem 4.10 is called the Bernoulli transform of (X, G, f ).
Example 4.12 For a direct car model (X, G, f ) we obtain the Bernoulli transform (X, (G ∗1 ,
. . . , G∗n ), f ∗ ), where
P (G∗i = U ) = P (G = U | X = xi ),
h∗ (xi , U1 , . . . , Un ) = (xi , Ui ),
and so f ∗ (xi , U1 , . . . , Un ) = Ui .
When the coarsening procedure is a Bernoulli model, then no information about X is
used for sampling the variables G. The only part of the procedure where X influences the
outcome is in the final computation of Y = f (X, G). The condition that in this computation
only as much knowledge of X should be required as finally revealed by Y now is basically
condition (9) for coarsening variables. The state space Γ for G now being (potentially)
uncountable, it is however more appropriate to replace the universal quantification “for all
g” in (9) with “for almost all g” in the probabilistic sense. We thus define:
Definition 4.13 A Bernoulli model is honest, if for all x, x 0 with P (X = x) > 0, P (X =
x0 ) > 0, and all U ∈ 2W \ ∅:
P (G ∈ {g | f (x, g) = U, x0 ∈ U ⇒ f (x0 , g) = U }) = 1.

(31)

Example 4.14 The Bernoulli model of Example 4.12 is not honest, because one can have
for some U1 , . . . , Un with P (G = (U1 , . . . , Un )) > 0: Uj 6= Ui , and xi , xj ∈ Ui , such that
f ∗ (xi , U1 , . . . , Un ) = Ui 6= Uj = f ∗ (xj , U1 , . . . , Un ).
Honest Bernoulli models certainly satisfy (D). On the other hand, there can be nonBernoulli models that also seem to satisfy (D) (notably the RMC models, which were
developed with (D) in mind). However, for non-Bernoulli models it appears hard to make
precise the condition that the sampling of G does not depend on X beyond the fact that
X ∈ Y 1 . The following theorem indicates that our formalization of (D) in terms of Bernoulli
models only is not too narrow.
Theorem 4.15 The Bernoulli transforms of MGD, CARgen and RMC models are honest.
The proof for all three types of models are elementary, though partly tedious. We omit
the details here.
We now turn to a second condition for reasonable procedures. For this we observe that
the MGD/CARgen/RMC models are essentially defined in terms of the “mechanical procedure” for generating the coarse data, whereas the direct car , the uniform noise, and the
P&T models (and in a similar way CARgen ∗ ) rely on the numerical conditions (26),(27),
respectively (29), on distributional parameters. These procedures, therefore, are fragile in
the sense that slight perturbations of the parameters will destroy the d-car property of the
induced distribution. We would like to distinguish robust car procedures as those for which
1. The intuitive condition that G must be independent of X given Y turns out to be inadequate.

912

Ignorability in Statistical and Probabilistic Inference

the d-car property is guaranteed through the mechanics of the process alone (as determined
by the state spaces of the Gi , and the definition of f ), and does not depend on parameter
constraints for the Gi (which, in a more or less subtle way, can be used to mimic the brute
force condition (26)). Thus, we will essentially consider a car procedure to be robust, if it
stays car under changes of the parameter settings for the G i . There are two points to consider before we can state a formal definition of this idea. First, we observe that our concept
of robustness should again be based on Bernoulli models, since in non-Bernoulli models even
arbitrarily small parameter changes can create or destroy independence relations between
the variables X, G, and such independence relations, arguably, reflect qualitative rather
than merely quantitative aspects of the coarsening mechanism.
Secondly, we will want to limit permissible parameter changes to those that do not lead to
such drastic quantitative changes that outcomes with previously nonzero probability become
zero-probability events, or vice versa. This is in line with our perspective in Section 3,
where the set of support of a distribution on a finite state space was viewed as a basic
qualitative property. In our current context we are dealing with distributions on uncountable
state spaces, and we need to replace the notion of identical support with the notion of
absolute continuity: recall that two distributions P, P̃ on a state space Σ are called mutually
absolutely continuous, written P ≡ P̃ , if P (S) = 0 ⇔ P̃ (S) = 0 for all measurable S ⊆ Σ.
For a distribution P (G) on Γ, with G an independent family, we can obtain P̃ (G) with
P ≡ P̃ , for example, by changing for finitely many i parameter values P (G i = g) = r > 0
to new values P̃ (Gi = g) = r̃ > 0. On the other hand, if e.g. Γ i = {0, 1}, P (Gi = 0) = 1/2,
and P̃ (Gi = 0) = 1/2 +  for all i and some  > 0, then P (Γ) 6≡ P̃ (Γ). For a distribution
P (X) of X alone one has P (X) ≡ P̃ (X) iff P and P̃ have the same support.
Definition 4.16 A Bernoulli model (X, G, f ) is robust car ( robust ccar), if it is car (ccar),
and remains car (ccar) if the distributions P (X) and P (G i ) (i ≥ 1) are replaced with
distributions P̃ (X) and P̃ (Gi ), such that P (X) ≡ P̃ (X) and P (G) ≡ P̃ (G).
The Bernoulli transforms of MGD/CARgen are robust ccar . Of the class RMC we
know, so far, that it is car . The Bernoulli transform of RMC can be seen to be robust car .
The Bernoulli transforms of CARgen ∗ /P&T, on the other hand, are not robust (and neither
is the uniform noise model, which already is Bernoulli). We now come to the main result of
this section, which basically identifies the existence of ’reasonable’ procedural models with
d-ccar .
Theorem 4.17 The following are equivalent for a distribution P on Ω(W ):
(i) P is induced by a robust car Bernoulli model.
(ii) P is induced by a robust ccar Bernoulli model.
(iii) P is induced by an honest Bernoulli model.
(iv) P is d-ccar.
The proof is given in Appendix A. Theorem 4.17 essentially identifies the existence of a
natural procedural model for a d-car distribution with the property of being d-ccar , rather
913

Jaeger

than merely d-car . This is a somewhat surprising result at first sight, given that M -mcar
is usually considered an unrealistically strong assumption as compared to M -mar . There
is no real contradiction here, however, as we have seen before that d-ccar is weaker than
M -mcar . Theorem 4.17 indicates that in practice one may find many cases where d-ccar
holds, but M -mcar is not fulfilled.

5. Conclusion
We have reviewed several versions of car conditions. They differ with respect to their
formulation, which can be in terms of a coarsening variable, or in terms of a purely distributional constraint. The different versions are mostly non-equivalent. Some care, therefore,
is required in determining for a particular statistical or probabilistic inference problem the
appropriate car condition that is both sufficient to justify the intended form of inference,
and the assumption of which is warranted for the observational process at hand. We argue
that the distributional forms of car are the more relevant ones: when the observations are
fully described as subsets of W , then the coarse data distribution is all that is required
in the analysis, and the introduction of an artificial coarsening variable G can skew the
analysis.
Our main goal was to provide characterizations of coarse data distributions that satisfy
d-car . We considered two types of such characterizations: the first type is a “static”
description of d-car distributions in terms of their sets of support. Here we have derived a
quite intuitive, complete characterization by means of the support hypergraph of a coarse
data distribution.
The second type of characterizations is in terms of procedural models for the observational process that generates the coarse data. We have considered several models for such
observational processes, and found that the arguably most natural ones are exactly those
that generate observations which are d-ccar , rather than only d-car . This is somewhat
surprising at first, because M -ccar is typically an unrealistically strong assumption (cf.
Example 2.3). The distributional form, d-ccar , on the contrary, turns out to be the perhaps
most natural assumption. The strongest support support for the d-ccar assumption is provided by the equivalence (i) ⇔ (iv) in Theorem 4.17: assuming d-car , but not d-ccar , means
that we must be dealing with a fragile coarsening mechanism that produces d-car data only
by virtue of some specific parameter settings. Since we usually do not know very much
about the coarsening mechanism, the assumption of such a special parameter-equilibrium
(as exemplified by (29)) will typically be unwarranted.

Acknowledgments
The author would like to thank Ian Pratt for providing the initial motivation for investigating the basis of probabilistic inference by conditioning. Richard Gill, Peter Grünwald,
and James Robins have provided valuable comments to earlier versions of this paper. I am
particularly indebted to Peter Grünwald for suggestions on the organization of the material
in Section 2.1, which led to a great improvement in the presentation. Richard Gill must
be credited for the short proof of Theorem 3.10, which replaced a previous much more
laborious one.
914

Ignorability in Statistical and Probabilistic Inference

Appendix A. Proof of Theorem 4.17
Theorem 4.17 The following are equivalent for a distribution P on Ω(W ):
(i) P is induced by a robust car Bernoulli model.
(ii) P is induced by a robust ccar Bernoulli model.
(iii) P is induced by an honest Bernoulli model.
(iv) P is d-ccar .
We begin with some measure theoretic preliminaries. Let A be the product σ-algebra
on Γ generated by the powersets 2Γi . The joint distribution P (X, G) then is defined on the
product of 2W and A. The σ-algebra A is generated by the cylinder sets (g 1∗ , g2∗ , . . . , gk∗ ) ×
×j>k Γj (k ≥ 0, gh∗ ∈ Γh for h = 1, . . . , k). The cylinder sets also are the basis for a topology
O on Γ. The space (Γ, O) is compact (this can be seen directly, or by an application
of Tikhonov’s theorem). It follows that every probability distribution P on A is regular,
especially for all A ∈ A:
P (A) = inf{P (O) | A ⊆ O ∈ O}
(see e.g. Cohn, 1993, Prop. 7.2.3). Here and in the following we use interchangeably
the notation P (A) and P (G ∈ A). The former notation is sufficient for reasoning about
probability distributions on A, the latter emphasizes the fact that we are always dealing
with distributions induced by the family G of random variables.
Lemma A.1 Let P (G) be the joint distribution on A of an independent family G. Let
A1 , A2 ∈ A with A1 ∩ A2 = ∅ and P (G ∈ A1 ) = P (G ∈ A2 ) > 0. Then there exists a joint
distribution P̃ (G) with P (G) ≡ P̃ (G) and P̃ (G ∈ A1 ) 6= P̃ (G ∈ A2 ).
Proof: Let p := P (A1 ). Let  = p/2 and O ∈ O such that A1 ⊆ O and P (O) < p + . Using
the disjointness of A1 and A2 one obtains P (A1 | O) > P (A2 | O). Since the cylinder sets are
a basis for O, we have O = ∪i≥0 Zi for a countable family of cylinders Z i . It follows that also
for some cylinder set Z = (g1∗ , g2∗ , . . . , gk∗ ) × ×j>k Γj with P (Z) > 0: P (A1 | Z) > P (A2 | Z).
Now let δ > 0 and define for h = 1, . . . , k:
X
P̃ (Gh = gh∗ ) := 1 − δ; P̃ (Gh = g) := δ(P (Gh = g)/
P (Gh = g 0 )) (g 6= gh∗ )
∗
g 0 :g 0 6=gh

For h ≥ k + 1: P̃ (Gh ) := P (Gh ). Then P (G) ≡ P̃ (G), P̃ (A1 | Z) = P (A1 | Z), P̃ (A2 |
Z) = P (A2 | Z), and therefore:
P̃ (A1 ) ≥ (1 − δ)k P (A1 | Z),

P̃ (A2 ) ≤ (1 − δ)k P (A2 | Z) + 1 − (1 − δ)k .

For sufficiently small δ this gives P̃ (A1 ) > P̃ (A2 ).



Proof of Theorem 4.17: For simplification we may assume that P (x) > 0 for all x ∈ W .
This is justified by the observation that none of the conditions (i)-(iv) are affected by adding
or deleting states with zero probability from W .
915

Jaeger

The implication (iv)⇒(ii) follows from Example 4.4 by the observation that MGD models
are robust d-ccar Bernoulli models. (ii)⇒(i) is trivial. We will show (i)⇒(iii) and (iii)⇒(iv).
First assume (i), and let (X, G, f ) be a robust car Bernoulli model inducing P . For
x ∈ W and U ⊆ W denote
A(x, U ) := {g ∈ Γ | f (x, g) = U }.
The d-car property of P is equivalent to
P (G ∈ A(x, U )) = P (G ∈ A(x0 , U )).

(32)

for all x, x0 ∈ U .
Condition (31) is equivalent to the condition that P (G ∈ A(x, U ) \ A(x 0 , U )) = 0 for
x, x0 ∈ U . Assume otherwise. Then for A1 := A(x, U ) \ A(x0 , U ), A2 := A(x0 , U ) \ A(x, U ):
0 < P (G ∈ A1 ) = P (G ∈ A2 ). Applying Lemma A.1 we obtain a Bernoulli model
P̃ (X, G) = P (X)P̃ (G) with P̃ (X, G) ≡ P (X, G) and P̃ (G ∈ A1 ) 6= P̃ (G ∈ A2 ). Then also
P̃ (G ∈ A(x, U )) 6= P̃ (G ∈ A(x0 , U )), so that P̃ (X, G) is not d-car, contradicting (i).
(iii)⇒(iv): Let
\
Γ∗ :=
{g | f (x, g) = U, x0 ∈ U ⇒ f (x0 , g) = U }.
x,x0 ∈W,U ⊆W :
x,x0 ∈U

Since the intersection is only over finitely many x, x 0 , U , we obtain from (iii) that P (G ∈
Γ∗ ) = 1. For U ⊆ W define A(U ) := A(x, U ) ∩ Γ∗ , where x ∈ U is arbitrary. By the
definition of Γ∗ the definition of A(U ) is independent of the particular choice of x. Define
an equivalence relation ∼ on Γ∗ via
g ∼ g0

⇔

∀U ⊆ W : g ∈ A(U ) ⇔ g 0 ∈ A(U ).

(33)

This equivalence relation partitions Γ ∗ into finitely many equivalence classes Γ ∗1 , . . . , Γ∗k .
We show that for each Γ∗i and g ∈ Γ∗i the system
Wi := {U | ∃x ∈ W : f (x, g) = U }

(34)

is a partition of W , and that the definition of W i does not depend on the choice of g. The
latter claim is immediate from the fact that for g ∈ Γ ∗
f (x, g) = U

⇔

g ∈ A(U ) and x ∈ U.

(35)

For the first claim assume that f (x, g) = U, f (x 0 , g) = U 0 with U 6= U 0 . In particular,
g ∈ A(U ) ∩ A(U 0 ). Assume there exists x00 ∈ U ∩ U 0 . Then by (35) we would obtain
both f (x00 , g) = U and f (x00 , g) = U 0 , a contradiction. Hence, the sets U in the W i are
pairwise disjoint. They also are a cover of W , because for every x ∈ W there exists U with
x ∈ U = f (x, g).
We thus obtain that the given Bernoulli model is equivalent to the multiple grouped

data model defined by the partitions W i and parameters λi := P (G ∈ Γ∗i ).

916

Ignorability in Statistical and Probabilistic Inference

References
Bhaskara Rao, K. P. S., & Bhaskara Rao, M. (1983). Theory of Charges: a Study of Finitely
Additive Measures. Academic Press.
Cator, E. (2004). On the testability of the CAR assumption. The Annals of Statistics,
32 (5), 1957–1980.
Cohn, D. (1993). Measure Theory. Birkhäuser.
Dawid, A. P., & Dickey, J. M. (1977). Likelihood and bayesian inference from selectively
reported data. Journal of the American Statistical Association, 72 (360), 845–850.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical Society, Ser. B, 39, 1–38.
Gill, R., & J.M.Robins (2001). Causal inference for complex longitudinal data: the continuous case. The Annals of Statistics, 29 (6), 1785–1811.
Gill, R. D., van der Laan, M. J., & Robins, J. M. (1997). Coarsening at random: Characterizations, conjectures, counter-examples. In Lin, D. Y., & Fleming, T. R. (Eds.),
Proceedings of the First Seattle Symposium in Biostatistics: Survival Analysis, Lecture
Notes in Statistics, pp. 255–294. Springer-Verlag.
Grünwald, P. D., & Halpern, J. Y. (2003). Updating probabilities. Journal of Artificial
Intelligence Research, 19, 243–278.
Heitjan, D. F. (1994). Ignorability in general incomplete-data models. Biometrika, 81 (4),
701–708.
Heitjan, D. F. (1997). Ignorability, sufficiency and ancillarity. Journal of the Royal Statistical
Society, B, 59 (2), 375–381.
Heitjan, D. F., & Rubin, D. B. (1991). Ignorability and coarse data. The Annals of Statistics,
19 (4), 2244–2253.
Jacobsen, M., & Keiding, N. (1995). Coarsening at random in general sample spaces and
random censoring in continuous time. The Annals of Statistics, 23 (3), 774–786.
Jaeger, M. (2005). Ignorability for categorical data. The Annals of Statistics, 33 (4), 1964–
1981.
Nielsen, S. F. (1997). Inference and missing data: Asymptotic results. Scandinavian Journal
of Statistics, 24, 261–274.
Nielsen, S. F. (2000). Relative coarsening at random. Statistica Neerlandica, 54 (1), 79–99.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems : Networks of Plausible
Inference (rev. 2nd pr. edition). The Morgan Kaufmann series in representation and
reasoning. Morgan Kaufmann, San Mateo, CA.
Rubin, D. (1976). Inference and missing data. Biometrika, 63 (3), 581–592.
Schrijver, A. (1986). Theory of Linear and Integer Programming. John Wiley & Sons.
Shafer, G. (1985). Conditional probability. International Statistical Review, 53 (3), 261–277.

917

Journal of Artificial Intelligence Research 24 (2005) 851-887

Submitted 08/05; published 12/05

The First Probabilistic Track of the
International Planning Competition
Håkan L. S. Younes

lorens@cs.cmu.edu

Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213 USA

Michael L. Littman
David Weissman
John Asmuth

mlittman@cs.rutgers.edu
dweisman@cs.rutgers.edu
jasmuth@cs.rutgers.edu

Department of Computer Science
Rutgers University
Piscataway, NJ 08854 USA

Abstract
The 2004 International Planning Competition, IPC-4, included a probabilistic planning
track for the first time. We describe the new domain specification language we created for
the track, our evaluation methodology, the competition domains we developed, and the
results of the participating teams.

1. Background
The Fourth International Planning Competition (IPC-4) was held as part of the International Conference on Planning and Scheduling (ICAPS’04) in Vancouver, British Columbia
in June 2004. By request of the ICAPS’04 organizers, Sven Koenig and Shlomo Zilberstein,
we were asked to create the first probabilistic planning track as part of IPC-4.
The overriding goal of the first probabilistic planning track was to bring together two
communities converging on a similar set of research issues and aid them in creating comparable tools and evaluation metrics. One community consists of Markov decision process
(MDP) researchers interested in developing algorithms that apply to powerfully expressive
representations of environments. The other consists of planning researchers incorporating
probabilistic and decision theoretic concepts into their planning algorithms. Cross fertilization has begun, but the intent of the probabilistic planning track was to create a set of
shared benchmarks and metrics that could crystallize efforts in this area of study.
We created a new domain description language called PPDDL1.0, described in Section 2. PPDDL stands for “Probabilistic Planning Domain Definition Language”, in analogy to PDDL (McDermott, 2000), which was introduced in IPC-1. PPDDL is modeled on
PDDL2.1 (Fox & Long, 2003), the domain-description language for deterministic domains
used in IPC-3. Syntactically, this language has a STRIPS/ADL-like flavor, but includes
probabilistic constructs. To focus the energy of participants on issues of dealing with uncertainty, we chose not to include constructs for durative actions in PPDDL1.0.
By basing the domain-description language on PDDL, we sought to remain in the spirit
of the existing planning competition, which we hope will further bring the communities
c
2005
AI Access Foundation. All rights reserved.

Younes, Littman, Weissman & Asmuth

together. The PPDDL representation itself is relational. Although representations with
explicit objects are not a traditional feature of MDP-based domain-description languages,
algorithms that exploit these features have begun to appear. We expected participants to
propositionalize the domains before running their planning algorithms and, for the most
part, they did so.
A fully functional parser for PPDDL was provided to participants in C++ in the form of
a plan validator and very simple planner. Some basic tools to convert PPDDL to a decisiondiagram representation were also provided. In many ways, handling the rich constructs
of PPDDL was the main hurdle for many participants and we tried to provide as much
assistance as we could on this dimension.
Although PPDDL supports numerical fluents, this feature was not used to its fullest
extent in the competition. Numerical quantities were used only for representing reward
values, and reward effects were required to be additive.
Since the classical track is well established at this point, it is helpful to contrast how
the probabilistic track differs. The defining difference, of course, is that actions can have
uncertain effects. That is, a “pickup” action in a Blocksworld might behave differently
on different occasions, even from the same state. This single difference has a number of
significant consequences. First, the optimal action choices for reaching a goal may be a
function of the probabilistic outcomes along the way—a single sequence of actions may
not be sufficient. As a result, it can be difficult to output a “plan”. For this reason, we
decided not to separate plan synthesis and execution into two phases, but instead evaluated
planners online. Second, because of the unpredictability of effects, even an optimal plan
for reaching a goal may get “unlucky” and fail with some probability. For this reason,
we evaluated each planner multiple times on each problem and did not include a separate
“optimal planner” track. In addition, since some planners may fail to reach the goal state
for some executions, we needed a way of trading off between goal attainment and action
cost. We decided to score an execution as goal reward minus action cost and chose a goal
reward for each problem. Section 3 describes the evaluation methodology in further detail.
In total, we designed eight domains for the competition (Section 4). Some domains
were simply noisy versions of classical planning domains, while other domains were designed
specifically to thwart greedy replanning approaches that ignore uncertainty.
Ten planners from seven different groups entered the competition. The results of the
competition are presented in Section 5. A deterministic replanner performed best overall,
primarily due to a disproportionate number of noisy classical planning problems in the
evaluation suite. Some domains proved challenging for all participating planners. These
latter domains could serve as a basis for future probabilistic planning competitions.

2. Probabilistic PDDL
This section describes the input language, PPDDL1.0, that was used for the probabilistic
track. PPDDL1.0 is essentially a syntactic extension of Levels 1 and 2 of PDDL2.1 (Fox
& Long, 2003). The complete syntax for PPDDL1.0 is given in Appendix A. We assume
that the reader is familiar with PDDL2.1, so we focus on the new language features, which
include probabilistic effects and rewards. A more detailed account of PPDDL1.0 is provided
852

The First Probabilistic Track of IPC

Name
bomb-in-package package1
bomb-in-package package2
toilet-clogged
bomb-defused

Type
boolean
boolean
boolean
boolean

Init 1
true
false
false
false

Init 2
false
true
false
false

Table 1: State variables and their initial values for the “Bomb and Toilet” problem.
by Younes and Littman (2004). The semantics of a PPDDL1.0 planning problem is given
in terms of a discrete-time Markov decision process (Howard, 1960, 1971; Puterman, 1994).
2.1 Probabilistic Effects
To define probabilistic and decision theoretic planning problems, we need to add support
for probabilistic effects. The syntax for probabilistic effects is
(probabilistic p1 e1 . . . pk ek )
meaning
Pk that effect ei occurs with probability pi . We require that the constraints pi ≥ 0
and i=1 pi = 1 are fulfilled: a probabilistic effect declares an exhaustive set of probabilityweighted outcomes. We do, however, allow a probability-effect pair to be left out if the
effect is empty. In other words,
(probabilistic p1 e1 . . . pl el )
with

Pl

i=1 pi

< 1 is syntactic sugar for
(probabilistic p1 e1 . . . pl el q (and))
Pl

with q = 1 − i=1 pi and (and) representing an empty effect (that is, no state changes).
For example, the effect (probabilistic 0.9 (clogged)) means that with probability 0.9
the state variable clogged becomes true in the next state, while with probability 0.1 the
state remains unchanged.
Figure 1 shows an encoding in PPDDL of the “Bomb and Toilet” example described
by Kushmerick, Hanks, and Weld (1995). The requirements flag :probabilistic-effects
signals that probabilistic effects are used in the domain definition. In this problem, there
are two packages, one of which contains a bomb. The bomb can be defused by dunking the
package containing the bomb in the toilet. There is a 0.05 probability of the toilet becoming
clogged when a package is placed in it, thus rendering the goal state unreachable.
The problem definition in Figure 1 also shows that initial conditions in PPDDL can be
probabilistic. In this particular example, we define two possible initial states with equal
probability (0.5) of being the true initial state for any given execution. Table 1 lists the
state variables for the “Bomb and Toilet” problem and their values in the two possible initial
states. Intuitively, we can think of the initial conditions of a PPDDL planning problem as
being the effects of an action forced to be scheduled right before time 0. Also, note that
the goal of the problem involves negation, which is why the problem definition declares the
:negative-preconditions requirements flag.
853

Younes, Littman, Weissman & Asmuth

(define (domain bomb-and-toilet)
(:requirements :conditional-effects :probabilistic-effects)
(:predicates (bomb-in-package ?pkg) (toilet-clogged)
(bomb-defused))
(:action dunk-package
:parameters (?pkg)
:effect (and (when (bomb-in-package ?pkg)
(bomb-defused))
(probabilistic 0.05 (toilet-clogged)))))
(define (problem bomb-and-toilet)
(:domain bomb-and-toilet)
(:requirements :negative-preconditions)
(:objects package1 package2)
(:init (probabilistic 0.5 (bomb-in-package package1)
0.5 (bomb-in-package package2)))
(:goal (and (bomb-defused) (not (toilet-clogged)))))
Figure 1: PPDDL encoding of “Bomb and Toilet” example.

PPDDL allows arbitrary nesting of conditional and probabilistic effects (see example in
Figure 2). This feature is in contrast to popular encodings, such as probabilistic STRIPS
operators (PSOs; Kushmerick et al., 1995) and factored PSOs (Dearden & Boutilier, 1997),
which do not allow conditional effects nested inside probabilistic effects. While arbitrary
nesting does not add to the expressiveness of the language, it can allow for exponentially
more compact representations of certain effects given the same set of state variables and
actions (Rintanen, 2003). Any PPDDL action can, however, be translated into a set of PSOs
with at most a polynomial increase in the size of the representation. Consequently, it follows
from the results of Littman (1997) that PPDDL, after grounding (that is, full instantiation
of action schemata), is representationally equivalent to dynamic Bayesian networks (Dean
& Kanazawa, 1989), which is another popular representation for MDP planning problems.
Still, it is worth noting that a single PPDDL action schema can represent a large number
of actions and a single predicate can represent a large number of state variables, meaning
that PPDDL often can represent planning problems more succinctly than other representations. For example, the number of actions that can be represented using m objects and n
action schemata with arity c is m · nc , which is not bounded by any polynomial in the size
of the original representation (m + n). Grounding is by no means a prerequisite for PPDDL
planning, so planners could conceivably take advantage of the more compact representation
by working directly with action schemata.
2.2 Rewards
Markovian rewards, associated with state transitions, can be encoded using fluents (numeric
state variables). PPDDL reserves the fluent reward , accessed as (reward) or reward, to
represent the total accumulated reward since the start of execution. Rewards are associated
854

The First Probabilistic Track of IPC

(define (domain coffee-delivery)
(:requirements :negative-preconditions
:disjunctive-preconditions
:conditional-effects :mdp)
(:predicates (in-office) (raining) (has-umbrella) (is-wet)
(has-coffee) (user-has-coffee))
(:action deliver-coffee
:effect (and (when (and (in-office) (has-coffee))
(probabilistic
0.8 (and (user-has-coffee)
(not (has-coffee))
(increase (reward) 0.8))
0.2 (and (probabilistic 0.5 (not (has-coffee)))
(when (user-has-coffee)
(increase (reward) 0.8)))))
(when (and (not (in-office)) (has-coffee))
(and (probabilistic 0.8 (not (has-coffee)))
(when (user-has-coffee)
(increase (reward) 0.8))))
(when (and (not (has-coffee)) (user-has-coffee))
(increase (reward) 0.8))
(when (not (is-wet))
(increase (reward) 0.2))))
... )

Figure 2: Part of PPDDL encoding of “Coffee Delivery” domain.

with state transitions through update rules in action effects. The use of the reward fluent
is restricted to action effects of the form (hadditive-opi hreward fluenti hf-expi), where
hadditive-opi is either increase or decrease, and hf-expi is a numeric expression not involving reward . Action preconditions and effect conditions are not allowed to refer to the
reward fluent, which means that the accumulated reward does not have to be considered
part of the state space. The initial value of reward is zero. These restrictions on the use
of the reward fluent allow a planner to handle domains with rewards without having to
implement full support for fluents.
A new requirements flag, :rewards, is introduced to signal that support for rewards is
required. Domains that require both probabilistic effects and rewards can declare the :mdp
requirements flag, which implies :probabilistic-effects and :rewards.
Figure 2 shows part of the PPDDL encoding of a coffee delivery domain described by
Dearden and Boutilier (1997). A reward of 0.8 is awarded if the user has coffee after the
“deliver-coffee” action has been executed, and a reward of 0.2 is awarded if is-wet is false
after execution of “deliver-coffee”. Note that a total reward of 1.0 can be awarded as a
result of executing the “deliver-coffee” action if execution of the action leads to a state
where both user -has-coffee and ¬is-wet hold.
855

Younes, Littman, Weissman & Asmuth

2.3 Plan Objectives
Regular PDDL goals are used to express goal-type performance objectives. A goal statement
(:goal φ) for a probabilistic planning problem encodes the objective that the probability
of achieving φ should be maximized, unless an explicit optimization metric is specified for
the planning problem. For planning problems instantiated from a domain declaring the
:rewards requirement, the default plan objective is to maximize the expected reward. A
goal statement in the specification of a reward oriented planning problem identifies a set of
absorbing states. In addition to transition rewards specified in action effects, it is possible to
associate a one-time reward for entering a goal state. This is done using the (:goal-reward
f ) construct, where f is a numeric expression.
In general, a statement (:metric maximize f ) in a problem definition means that the
expected value of f should be maximized. Reward-oriented problems, for example a problem instance of the coffee-delivery domain in Figure 2, would declare (:metric maximize
(reward)) as the optimization criterion (this declaration is the default if the :rewards
requirement has been specified). PPDDL defines goal-achieved as a special optimization
metric, which can be used to explicitly specify that the plan objective is to maximize (or
minimize) the probability of goal achievement. The value of the goal-achieved fluent is 1
if execution ends in a goal state. The expected value of goal-achieved is therefore equal to
the probability of goal achievement. A declaration (:metric maximize (goal-achieved))
takes precedence over any reward specifications in a domain or problem definition, and it
is the default if the :rewards requirement has not been specified (for example, the “Bomb
and Toilet” problem in Figure 1).
2.4 PPDDL Semantics
For completeness, we present a formal semantics for PPDDL planning problems in terms of
a mapping to a probabilistic transition system with rewards. A planning problem defines
a set of state variables V , possibly containing both Boolean and numeric state variables,
although we only consider planning problems without any numeric state variables in this
section. An assignment of values to state variables defines a state, and the state space S
of the planning problem is the set of states representing all possible assignments of values
to variables. In addition
to V , a planning problem defines an initial-state distribution
P
p0 : S → [0, 1] with s∈S p0 (s) = 1 (that is, p0 is a probability distribution over states), a
formula φG over V characterizing a set of goal states G = {s | s |= φG }, a one-time reward
rG associated with entering a goal state, and a set of actions A instantiated from PPDDL
action schemata. For goal-directed planning problems, without explicit rewards, we use
rG = 1.
2.4.1 Probability and Reward Structure
An action a ∈ A consists of a precondition φa and an effect ea . Action a is applicable
in a state s if and only if s |= ¬φG ∧ φa . It is an error to apply a to a state such that
s 6|= ¬φG ∧ φa . Goal states are absorbing, so no action may be applied to a state satisfying
φG . The requirement that φa must hold in order for a to be applicable is consistent with
the semantics of PDDL2.1 (Fox & Long, 2003) and permits the modeling of forced chains
of actions. Effects are recursively defined as follows (see also, Rintanen, 2003):
856

The First Probabilistic Track of IPC

1. > is the null-effect, represented in PPDDL by (and).
2. b and ¬b are effects if b ∈ V is a Boolean state variable.
3. r ↑ v, for v ∈ R, is an effect.
4. c  e is an effect if c is a formula over V and e is an effect.
5. e1 ∧ · · · ∧ en is an effect if e1 , . . . , en are effects.
6. p
1 e1 | . . . |pn en is an effect if e1 , . . . , en are effects, pi ≥ 0 for all i ∈ {1, . . . , n}, and
P
n
i=1 pi = 1.
The effect b sets the Boolean state variable b to true in the next state, while ¬b sets b to false
in the next state. Effects of the form r ↑ v are used to associate rewards with transitions
as described below.
An action a = hφa , ea i defines a transition probability matrix Pa and a state reward
vector Ra , with Pa (i, j) being the probability of transitioning to state j when applying a
in state i, and Ra (i) being the expected reward for executing action a in state i. We can
readily compute the entries of the reward vector from the action effect formula ea . Let χc
be the characteristic function for the Boolean formula c, that is, χc (s) is 1 if s |= c and 0
otherwise. The expected reward for an effect e applied to a state s, denoted R(e; s), can be
computed using the following inductive definition:
.
R(>; s) = 0
.
R(b; s) = 0
.
R(¬b; s) = 0
.
R(r ↑ v; s) = v
.
R(c  e; s) = χc (s) · R(e; s)
n
. X
R(e1 ∧ · · · ∧ en ; s) =
R(ei ; s)
i=1
n
. X
R(p1 e1 | . . . |pn en ; s) =
pi · R(ei ; s).
i=1

A factored representation of the probability matrix Pa can be obtained by generating
a dynamic Bayesian network (DBN) representation of the action effect formula ea . We
can use Bayesian inference on the DBN to obtain a monolithic representation of Pa , but
the structure of the factored representation can be exploited by algorithms for decision
theoretic planning (see, for example, work by Boutilier, Dearden, & Goldszmidt, 1995;
Hoey, St-Aubin, Hu, & Boutilier, 1999; Boutilier, Dean, & Hanks, 1999; Guestrin, Koller,
Parr, & Venkataraman, 2003).
A Bayesian network is a directed graph. Each node of the graph represents a state
variable, and a directed edge from one node to another represents a causal dependence. With
each node is associated a conditional probability table (CPT). The CPT for state variable
X’s node represents the probability distribution over possible values for X conditioned on
the values of state variables whose nodes are parents of X’s node. A Bayesian network is a
857

Younes, Littman, Weissman & Asmuth

factored representation of the joint probability distribution over the variables represented
in the network.
A DBN is a Bayesian network with a specific structure aimed at capturing temporal
dependence. For each state variable X, we create a duplicate state variable X 0 , with X
representing the situation at the present time and X 0 representing the situation one time
step into the future. A directed edge from a present-time state variable X to a future-time
state variable Y 0 encodes a temporal dependence. There are no edges between two presenttime state variables, or from a future-time to a present-time state variable (the present does
not depend on the future). We can, however, have an edge between two future-time state
variables. Such edges, called synchronic edges, are used to represent correlated effects. A
DBN is a factored representation of the joint probability distribution over present-time and
future-time state variables, which is also the transition probability matrix for a discrete-time
Markov process.
We now show how to generate a DBN representing the transition probability matrix for
a PPDDL action. To avoid representational blowup, we introduce a multi-valued auxiliary
variable for each probabilistic effect of an action effect. These auxiliary variables are introduced to indicate which of the possible outcomes of a probabilistic effect occurs, allowing
the representation to correlate all the effects of a specific outcome. The auxiliary variable
associated with a probabilistic effect with n outcomes can take on n different values. A
PPDDL effect e of size |e| can consist of at most O(|e|) distinct probabilistic effects. Hence,
the number of auxiliary variables required to encode the transition probability matrix for
an action with effect e will be at most O(|e|). Only future-time versions of the auxiliary
variables are necessary. For a PPDDL problem with m Boolean state variables, we need
on the order of 2m + maxa∈A |ea | nodes in the DBNs representing transition probability
matrices for actions.
We provide a compositional approach for generating a DBN that represents the transition probability matrix for a PPDDL action with precondition φa and effect ea . We assume
that the effect is consistent, that is, that b and ¬b do not occur in the same outcome with
overlapping conditions. The DBN for an empty effect > simply consists of 2m nodes, with
each present-time node X connected to its future-time counterpart X 0 . The CPT for X 0
has the non-zero entries Pr[X 0 = > | X = >] = 1 and Pr[X 0 = ⊥ | X = ⊥] = 1. The same
holds for a reward effect r ↑ v, which does not change the value of state variables.
Next, consider the simple effects b and ¬b. Let Xb be the state variable associated with
the PPDDL atom b. For these effects, we eliminate the edge from Xb to Xb0 . The CPT for
Xb0 has the entry Pr[Xb0 = >] = 1 for effect b and Pr[Xb0 = ⊥] = 1 for effect ¬b.
For conditional effects, c  e, we take the DBN for e and add edges between the presenttime state variables mentioned in the formula c and the future-time state variables in the
DBN for e.1 Entries in the CPT for a state variable X 0 that correspond to settings of the
present-time state variables that satisfy c remain unchanged. The other entries are set to
1 if X is true and 0 otherwise (the value of X does not change if the effect condition is not
satisfied).
The DBN for an effect conjunction e1 ∧ · · · ∧ en is constructed from the DBNs for the
n effect conjuncts. The value for Pr[X 0 = > | X] in the DBN for the conjunction is set to
1. This transformation can increase the size of the DBNs exponentially unless context-specific DBNs are
used (Boutilier, Friedman, Goldszmidt, & Koller, 1996).

858

The First Probabilistic Track of IPC

R

R′

R:
HU:
IW :
UHC:
HC:
IO:

raining
has-umbrella
is-wet
user-has-coffee
has-coffee
in-office

HU

HU′

IW

IW′

UHC

UHC′

Aux′1

HC

HC′

Aux′2

IO

IO′

Aux′3

Aux 01
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2

IO
>
>
>
>
⊥
⊥
⊥
⊥
>
>
>
>
⊥
⊥
⊥
⊥

HC
>
>
⊥
⊥
>
>
⊥
⊥
>
>
⊥
⊥
>
>
⊥
⊥

UHC
>
⊥
>
⊥
>
⊥
>
⊥
>
⊥
>
⊥
>
⊥
>
⊥

UHC 0
> ⊥
1
0
1
0
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1

Figure 3: DBN structure (left) for the “deliver-coffee” action of the “Coffee Delivery” domain, with the CPT for UHC 0 (the future-time version of the state variable
user -has-coffee) shown to the right.

the maximum of Pr[X 0 = > | X] over the DBNs for the conjuncts. The maximum is used
because a state variable is set to true (false) by the conjunctive effect if it is set to true
(false) by one of the effect conjuncts (effects are assumed to be consistent, so that the result
of taking the maximum over the separate probability tables is still a probability table).
Finally, to construct a DBN for a probabilistic effect p1 e1 | . . . |pn en , we introduce an
auxiliary variable Y 0 that is used to indicate which one of the n outcomes occurred. The
node for Y 0 does not have any parents, and the entries of the CPT are Pr[Y 0 = i] = pi .
Given a DBN for ei , we add a synchronic edge from Y 0 to all state variables X. The
value of Pr[X 0 = > | X, Y 0 = j] is set to Pr[X 0 = > | X] if j = i and 0 otherwise.
This transformation is repeated for all n outcomes, which results in n DBNs. These DBNs
can trivially be combined into a single DBN for the probabilistic effect because they have
mutually exclusive preconditions (the value of Y).
As an example, Figure 3 shows the DBN encoding of the transition probability matrix
for the “deliver-coffee” action, whose PPDDL encoding was given in Figure 2. There are
three auxiliary variables because the action effect contains three probabilistic effects. The
node labeled UHC 0 (the future-time version of the state variable user -has-coffee) has four
parents, including one auxiliary variable. Consequently, the CPT for this node will have
24 = 16 rows (shown to the right in Figure 3).
2.4.2 Optimality Criteria
We have shown how to construct an MDP from the PPDDL encoding of a planning problem.
The plan objective is to maximize the expected reward for the MDP. This objective can be
interpreted in different ways, for example as expected discounted reward or expected total
859

Younes, Littman, Weissman & Asmuth

reward. The suitable interpretation depends on the problem. For process-oriented planning problems (for example, the “Coffee Delivery” problem), discounted reward is typically
desirable, while total reward often is the interpretation chosen for goal-oriented problems
(for example, the “Bomb and Toilet” problem). PPDDL does not include any facility for
enforcing a given interpretation or specifying a discount factor.
For the competition, we used expected total reward as the optimality criterion. Without
discounting, some care is required in the design of planning problems to ensure that the
expected total reward is bounded for the optimal policy. The following restrictions were
made for problems used in the planning competition:
1. Each problem had a goal statement, identifying a set of absorbing goal states.
2. A positive reward was associated with transitioning into a goal state.
3. A negative reward (cost) was associated with each action.
4. A “done” action was available in all states, which could be used to end further accumulation of reward.
These conditions ensure that an MDP model of a planning problem is a positive bounded
model (Puterman, 1994). The only positive reward is for transitioning into a goal state.
Since goal states are absorbing (that is, they have no outgoing transitions), the maximum
value for any state is bounded by the goal reward. Furthermore, the “done” action ensures
that there is an action available in each state that guarantees a non-negative future reward.

3. Evaluation Methodology
In classical planning, a plan is a series of operators. A successful plan is one that, when
applied to the initial state, achieves the goal. In probabilistic planning, there are many
proposals for plan representations (straight-line plans, plan trees, policy graphs, and triangle
tables, for example), but none is considered a widely accepted standard. In addition, even
simple plans are challenging to evaluate exactly in a non-deterministic environment, as
all possible outcomes need to be checked and results combined (Littman, Goldsmith, &
Mundhenk, 1998).
For these reasons, we chose to evaluate planners by simulation. That is, our plan validator was a server, and individual planning algorithms acted as clients. Planners connected
to the validator, received an initial state, and returned an operator/action. This dialog
continued until a terminating condition was reached at which point the validator evaluated
the performance of the planner during that trajectory from initial state to terminating condition. This entire process was repeated several times and results averaged over the multiple
runs.
Because this evaluation scheme blurs the distinction between a planner and an executor,
it means that computation is no longer a one-time preprocessing cost, but something integrated with action selection itself. Planner quality, therefore, needs to be a combination of
expected utility and running time. For simplicity, we set a time threshold and only allowed
reward to be gathered until time ran out. This time threshold was known to competitors,
whose planners could take it into consideration when deciding how to balance computation
860

The First Probabilistic Track of IPC

and action. Since we did not know whether participants would reuse results from one trajectory to speed planning in the next, we set an overall time limit that applied to the total
of all repetitions of the evaluator for a given domain.
Concretely, in our evaluations, participants were presented with twenty previously unseen problems in PPDDL format. To evaluate each problem, participants connected to one
of our evaluation servers (at CMU or Rutgers). The server provided the planner with an
initial state and the planner selected and returned an action. This dialogue was iterated
until a goal was reached, time ran out, or the planner sent a “done” action. The value
obtained for the problem was the goal reward, if the goal was reached, minus the sum of the
action costs (if any). For each problem, this procedure was repeated 30 times in a maximum
of 15 minutes and the results averaged.
There were two types of problems in the evaluation set: reward problems and goal
problems. For goal problems, the success percentage determined a participant’s score for
the problem (no action costs). In reward problems, every action had a fixed cost. The times
to completion were recorded, but not explicitly used for ranking. Planners that completed
less than 30 runs in 15 minutes were given a score of 0 for the unfinished runs.
In the design of the server, we believed that the time needed for computation in the
planner would far outweigh any possible communication delay. However, in preliminary
evaluations, participants—especially those halfway across the world—experienced disruptive levels of latency when evaluating their planners by connecting remotely to the server.
Before the formal evaluation, we offered participants local accounts at CMU and nearly all
availed themselves of this option.
3.1 Communication between Client and Server
The communication between a participant’s client program and our server took place in
XML. We made this decision for two reasons: The first is that parsing the messages into an
easily managed format was trivial for all parties involved—many solid XML parsers exist in
the public domain. The second is that bandwidth was not a great concern—as mentioned
in the previous section, most participants ran their clients on the machine that hosted the
server. While it is true that excessively large messages can take up valuable processing
time, in our specific case those large messages corresponded to large state spaces, which
took somewhat longer to process altogether, and the XML parsing was not the limiting
factor.
When a client connected to a server, it would request a certain problem to run. The
server would then lead the client through running that problem 30 times, sending the state
of the problem, receiving the client’s action, and then creating a new state from the old
state and the action, and sending it back again. Figure 4 gives a schematic illustration of
the conversation between the client and server. The specific format of each XML element
is described in Appendix B.
Prior to the competition, an example client was written in C++ and distributed to
the participants to minimize difficulties in dealing with the nuts and bolts of the protocol,
allowing them to instead focus on the design of their algorithms.
861

Younes, Littman, Weissman & Asmuth

client: hsession-requesti
server: hsession-initi
–loop through 30 rounds
client: hround-requesti
server: hround-initi
–loop until termination conditions
server: hstatei
client: hacti | hnoopi | hdonei
server: hend-round i
– server: hend-sessioni

Figure 4: The interaction between client (planners) and server (environment) in our evaluation system.

3.2 Generator-Based Domains
Several example domains were provided to participants in advance to serve as testbeds
for parser and planner development. In addition, parameterized problem generators were
provided for two domain classes—Blocksworld and Boxworld, described in more detail in
Section 4. The availability of these domains served to allow participants to learn, either manually or automatically, about the domains and to create domain-specific solutions. These
approaches were evaluated independently in a separate category.

4. Competition Domains
This section describes the domains used in the competition. Machine readable versions of
the domains themselves can be found online at the competition Web site:
http://www.cs.rutgers.edu/˜mlittman/topics/ipc04-pt/
4.1 Blocksworld (Traditional)
Our traditional Blocksworld domain does not stray far from the original Blocksworld domain. The domain consists of two types of objects, blocks and tables. The domain has
exactly one table and each problem instance has some number of blocks (the number of
blocks is problem specific). The actions of the domain are “pick-up-block-from” and “putdown-block-on”. For each problem instance, an initial configuration and a goal configuration
of the blocks is given. The goal of the problem is to move the blocks from the initial configuration into the goal configuration. This domain comes in two flavors: a goal version and a
reward version. Within the reward version, there is a cost of one unit every time the action
“pick-up-block-from” is executed, and the reward is 500 for reaching the goal configuration.
As with all of the other domains used in the competition, the Blocksworld domain
incorporates probabilistic effects and does so by adding a “slip” probability. That is, each
time a block is picked up or put down, the block will slip and fall onto the table with
862

The First Probabilistic Track of IPC

probability 0.25. (Of course, if the intended action is to put the block down onto the table,
then this effect will always be achieved.) The Blocksworld domain is an extremely simple
domain, yet it offers a lot of insight into the planning process. Two important features of
the domain are:
1. A basic policy to solve the domain is:
(a) From the initial configuration, place all of the blocks onto the table with no block
on top of another block.
(b) Starting from the bottom up, place each block into its place in the final configuration.
Note that without noise, if there are n blocks, this policy takes 4n steps (2 steps for
each block on Part 1a, and 2 steps for each block on Part 1b) and hence costs 2n
units. So, there is a very basic, very inexpensive way to solve this domain.
2. The state space of this domain increases exponentially with the number of blocks.
Thus, this domain aims at testing if planners could find the easy (maybe slightly more
expensive) policy when the state space was too large to find a good policy. As far as the
complexity of this domain is concerned, it is one of the easier domains to plan for and our
hope was that many planners would do quite well in this domain.
A generation program for random traditional Blocksworld domains was provided to
participants and the competition problems were generated from this same program. The
availability of the generator allowed participants to test their planners on as many problems
as they liked in advance of the evaluation.
4.2 Blocksworld (Color)
The colored Blocksworld domain is a variant of the traditional Blocksworld presented above.
As in the traditional Blocksworld, colored Blocksworld consists of two types of objects,
tables and blocks. Again, the domain has exactly one table and each problem instance
has some number of blocks. The actions of the domain are still “pick-up-block-from” and
“put-down-block-on”, and this domain also comes in two flavors: goal and reward. The
major difference from the traditional Blocksworld domain is that each block in the colored
Blocksworld domain is assigned a color, and the goal configuration is specified in terms of
block colors rather than specific blocks. Thus, in general, there are many different valid goal
configurations. Goal conditions are expressed with existential quantification. For example,
the PPDDL fragment
(:goal (and (exists (?b1) (is-green ?b1))
(exists (?b2) (and (is-blue ?b2) (on-top-of ?b1 ?b2)))))
states that the goal is to have any green block on top of any blue block.
The noise in the colored Blocksworld domain is the same as in the traditional Blocksworld domain. That is, the colored Blocksworld domain incorporates probabilistic effects
by adding a “slip” probability. Each time a block is picked up or put down, the block will
slip and fall onto the table with probability 0.25.
863

Younes, Littman, Weissman & Asmuth

Notice that although the goal configuration is existentially quantified and hence not precisely specified, the same basic policy that can be used to solve the traditional Blocksworld
can be used to solve the colored Blocksworld. To solve a colored Blocksworld problem,
unstack all of the blocks and then, in a bottom up fashion, choose a block that satisfies a
color constraint and place it in the appropriate position.
The colored Blocksworld domain aims to add complexity to the traditional Blocksworld domain by incorporating existential quantification into the goal configuration. The
indeterminacy of the goal in the colored Blocksworld domain can make the planning problem
considerably harder than its traditional counterpart. Thus, a colored Blocksworld problem
may be impossible for a given planner to solve in a reasonable amount of time, whereas
that same planner would have no problem on a traditional Blocksworld problem of the same
size.2
A generation program for random colored Blocksworld domains was provided to participants and the competition problems were generated from this same program.
4.3 Boxworld
The Boxworld domain is modeled after the traditional logistics domain. The domain consists
of four types of objects: cities, boxes, trucks and planes. For each problem, there is a graph
superimposed on the cities with two different types of edges, one denoting the ability to
drive from one city to another and the other denoting the ability to fly from one city
to the other. The actions of the domain are “load-box-on-truck-in-city”, “unload-boxfrom-truck-in-city”, “load-box-on-plane-in-city”, “unload-box-from-plane-in-city”, “drivetruck” and “fly-plane”. Both goal and reward versions of this domain were included in
the evaluation. Within the reward version, there was a cost of 1 unit every time either
“load-box-on-truck-in-city” or “load-box-on-plane-in-city” was executed, a cost of 5 units
every time “drive-truck” was executed and a cost of 25 units every time “fly-plane” was
executed. For each problem instance, the initial configuration determines the graph that
is superimposed on the cities, identifies the locations of the boxes, trucks and planes and
determines the final destination where each box should arrive. The goal configuration
specifies a destination for every box. The goal of the problem is to move from the initial
configuration to a state where each box is in its destined location.
Noise enters this domain in the action “drive-truck”. When this action is executed, the
desired effect is achieved with probability 0.8 (that is, with probability 0.8 the truck will
end up in its expected destination). However, with probability 0.2, the truck will get lost
and end up in the wrong destination. For each city, there are three cities that the truck
may get lost to when trying to execute the “drive-truck” action. If the truck actually gets
lost it will end up in each of these cities with equal probability (that is, with probability
1/3).
As with the Blocksworld domains, a generation program for random Boxworld domains
was provided to participants and the competition problems were generated from this same
program.
2. It is important to note that the existentially quantified goal formula for colored Blocksworld, when
grounded, can be excessively long. This fact is a serious bottleneck for larger instances of this domain.
Planners that avoid grounding should have a benefit here, but did not in the competition because our
plan validator grounded the goal formula.

864

The First Probabilistic Track of IPC

4.4 Exploding Blocksworld
The exploding Blocksworld domain is a “dead-end” version of the traditional Blocksworld
domain described earlier. As in the traditional Blocksworld domain, there are two types
of objects (tables and blocks) and two actions (“pick-up-block-from” and “put-down-blockon”). An initial configuration and a goal configuration of the blocks are given and the goal
of the domain is to move the blocks from the initial configuration to the goal configuration.
The key difference between this domain and the traditional Blocksworld domain is that
every block in the exploding Blocksworld domain is initially set to “detonate”. Every time
a “put-down-block” action is executed, if the block that is being put down has not yet
detonated it will detonate with probability 0.3; this is the only noise in the domain. If a
block detonates when executing a “put-down-block” action, the object beneath the block
(whether it is the table or another block) is destroyed and is no longer accessible within the
domain. Once a block detonates, it is safe and can no longer detonate.
The exploding Blocksworld domain aims at testing a planner’s ability to “think ahead”.
More formally, as actions are executed it is possible to reach a state from which the goal
cannot be reached. Consider, for example, executing the standard Blocksworld approach
in which all blocks are unstacked to the table before the goal configuration is constructed.
After seven blocks have been unstacked, there is a 92% (1 − (1 − 0.3)7 ) probability that the
table is destroyed, rendering the problem unsolvable.
One strategy for solving an exploding Blocksworld problem is to never place an unsafe
block on top of something valuable (the table or a block needed in the final stack). Instead,
a block should first be “disarmed”, by placing it on top of some block that is not needed
for the final configuration, if such a block exists.
We illustrate this strategy on the problem instance that was used in the planning competition, shown in Figure 5. Four blocks are not needed for the goal configuration: 4, 8, 9,
and 10. We start by repeatedly picking up Block 0 and placing it on Block 9 until Block 0
detonates. Next, we detonate Block 1 in the same way using Block 10. With both Block 0
and Block 1 safe, we place Block 1 on the table and Block 0 on top of Block 1. This completes the left-most tower. At this stage, there are no safe moves because Blocks 4 and 8 are
not clear. We pick up Block 6 and put it on Block 2. The last action leads to failure with
probability 0.3. If successful, the right-most tower is completed. Block 8 is now clear and
we use it to detonate Block 3. Block 3 is then safely placed on top of Block 5. Finally, the
center tower is completed by placing Block 7 on top of Block 3, which can result in failure
with probability 0.3. In total, the success probability of the given plan is (1 − 0.3)2 = 0.49,
which, in fact, is optimal for the given problem (there are no action costs).
Along with several other test domains, exploding Blocksworld was specifically designed
so that a replanning strategy performs suboptimally (gets stuck with high probability). A
replanning strategy would be to ignore the 0.3 probability of detonation and try to replan
if something unexpected happens. However, there is a high probability that this approach
will render the goal state unreachable.
4.5 Fileworld
Fileworld is a fairly basic domain. It consists of k files and n folders for the files to be
filed into. The actions of the domain are “get-type” (reports which folder the given file
865

Younes, Littman, Weissman & Asmuth

initial state
0
1
3
7

2
4

5

6
8

9

goal

10

0
1

7
3
5

6
2

Figure 5: Exploding Blocksworld problem used in the planning competition. Note that the
goal condition does not require Block 2 to be on the table.

belongs in), “get-folder-Fi” (one for each i ∈ {0, . . . , n − 1}, retrieves Folder i from the filing
cabinet), “file-Fi” (one for each i ∈ {0, . . . , n − 1}, inserts the given file into Folder i) and
“return-Fi” (one for each i ∈ 0, . . . , n − 1}, returns Folder i to the filing cabinet). This
domain comes only in a reward version. There is a cost of 100 for executing the action
“get-folder-Fi” and a cost of 1 for executing the action “file-Fi”. The other actions have
no explicit costs since they must be used in conjunction with get-folder-Fi and file-Fi. The
initial configuration of the problem specifies how many folders there are (the competition
problem used 30 files and 5 folders) and the goal configuration specifies that all of the files
must be filed. Note that the initial configuration does not specify which folder a file is to go
into, but files cannot be filed into just any folder; this constraint is where the noise comes
into the domain.
Before a file can be filed, its destination folder must be determined. The destination
folder of a file is obtained by executing the action “get-type” with the file in question as a
parameter. When this action is executed, the file passed as a parameter is assigned a folder,
with each folder being the file’s destination with equal probability (that is, probability 1/n).
Once a file has a destination folder, it can be filed into this (and only this) folder.
The Fileworld domain tests a planner’s ability to consider all of its strategies and choose
the one that minimizes the cost. In particular, a straightforward plan to achieve the goal
is to carry out the following series of actions on each file in turn:
1. Get its type with “get-type”
2. Get its destination folder by executing “get-folder-Fi”
3. Place the file in the appropriate folder by executing the “file-Fi” action
4. Return the folder by executing the “return-Fi” action
Although this plan works, it is very costly. Its cost would be 101k where k is the number of
files, because “get-folder-Fi” (expensive) and “file-Fi” (cheap) are executed for every file. A
less costly (in fact, the optimal) plan can be described. It first executes “get-type” on every
file. Then, for each folder i ∈ {0, . . . , n − 1} that at least one file has as its destination, it
runs “get-folder-Fi”. Next, it files every file that belongs in folder i using “file-Fi”. It then
uses “return-Fi” in preparation for getting the next folder.
866

The First Probabilistic Track of IPC

The expected reward for the optimal plan is 600 − (100n + k), where n is the number of
folders and k is the number of files (this analysis gives 70 as the optimal expected reward
for the competition problem). The domain is designed to reward planners that are able to
reason about the initial destination uncertainty of the files and recognize that the second
plan is much less costly and should be preferred to the straightforward brute-force plan.
4.6 Tireworld
Tireworld is another domain that tests the planners’ ability to plan ahead under uncertainty.
The domain consists of one type of object, namely locations. This domain comes in two
flavors, a goal version and a reward version. The actions common to both versions are
“move-car”, “load-tire” and “change-tire”. In the reward version, there is the additional
action “call-AAA”.
Within the reward version, there is a cost of 1 every time one of the actions “move-car”,
“load-tire” or “change-tire” is executed and a cost of 100 every time the action “callAAA” is executed. The initial configuration of the problem defines a set of locations, a
superimposed graph on these locations (roads), a subset of all locations representing the
locations with spare tires, and the starting location on the graph. The goal configuration
defines a destination location on the graph. The goal of the problem is to move from the
starting location to the goal location.
The noise in Tireworld comes from the action “move-car”. Each time this action is
executed, the car drives from one city to another and will get a flat tire with probability
0.15. Once a car has a flat tire, it cannot execute the action “move-car” again until the
tire is fixed. The car has the ability to store at most one spare tire, which it can pick up
by executing the action “load-tire” when it is in a location with a spare tire. If the car is
holding a spare tire, the “change-tire” action can be invoked to fix the flat. However, if
the car does not currently have a spare then this action is disabled. In the goal version,
a flat tire may result in a dead end if a car gets a flat and carries no spare tire. In the
reward version, the planner has the choice of executing one of the actions “change-tire” (if
the car has a spare) or “call-AAA” (at a high cost) to repair the flat. Thus, in the reward
version, there are no dead ends and the goal is always reachable. Notice that since the cost
of “call-AAA” is large compared to the costs of “change-tire” and “load-tire”, fixing a flat
is always less expensive if the car has a spare tire.
Figure 6 illustrates the Tireworld problem used in the competition. We next compare
the probability of reaching a goal state for two different plans for this problem to illustrate
what an ideal plan looks like in this domain.
An optimal plan would look ahead and attempt to keep spare tires as accessible as
possible to avoid dead ends. From the start state, the car must make three steps without a
flat tire to reach the first spare at cc, which will occur with probability 0.853 ≈ 0.61. Now,
the car needs to go four steps without getting two flats to make it to the next spare at d5. It
gets zero flats with probability 0.854 ≈ 0.52 and one flat with probability 4 × 0.853 × 0.15 ≈
0.37, so a four-step segment can be traversed with probability 0.52 + 0.37 = 0.89 with one
spare tire. There are three four-step segments that must be traversed successfully to reach
ck. Finally, with a spare, the last two steps can be traveled with certainty. Thus, the total
success probability of this event sequence is 0.61 × 0.893 ≈ 0.43. Note that this estimate is a
867

Younes, Littman, Weissman & Asmuth

spare tire
(all boxed locations)
start

goal
d6

d5

ca
c1

cn

cd

cc

c0

cm

cb
c2

ce

cf

cg

ch

ci
c6

c3

c4

cj

ck

c7 c8

c5

cl
c9
d4

d0
d1

d2

d3

Figure 6: The Tireworld domain used in the competition.
lower bound on the success probability of the optimal strategy, because it does not factor in
the probability of getting a flat tire upon arrival to a state with a spare tire. Furthermore,
if the car is in location cf or ch with a spare and no flat, it is unnecessary to traverse the
loop to pick up the spare tire in location d5 or cm. By accounting for these factors we get
a success probability of just over 0.57.
In contrast, a greedy replanning algorithm would not gather spares, since their utility
comes from the realization that something might go wrong. For such a planner, the best
plan is to go directly from c0 to c9 on the shortest (9-step) route. Its success probability
is 0.859 ≈ 0.23, which is just 40 percent of the best success probability computed above.
In the reward version of the planning problem, the optimal success probability is one
because the “call-AAA” action is always available. However, the cost of this action equals
the reward for reaching the goal, so it is always better to end execution with the “done”
action than to repair a flat tire with the “call-AAA” action. Hence, the best strategy for
the goal version is optimal for the reward version as well and gives a reward of just under
45. The greedy strategy outlined above would result in an expected reward of just over 22.
If the “call-AAA” action is used to fix flat tires, then the expected reward drops to −29.
4.7 Towers of Hanoise
As the name suggests, this domain is a noisy version of the famous Towers of Hanoi
problem. The domain has two types of objects, disks and pegs. The problem that was
used in the competition had five disks and three pegs. The actions of the domain are
“single-move-big-not-moved”, “single-move-big-moved”, “double-move-big-not-moved” and
“double-move-big-moved”. As the action names suggest, one can move either one or two
868

The First Probabilistic Track of IPC

disks at a time (single-move/double-move) and the outcome of the move is dependent on
whether or not the biggest disk has been moved yet (big-not-moved/big-moved). The objective of the domain is to maximize the probability of reaching a goal configuration (no
rewards).
The initial configuration defines the starting positions of the disks (as in Towers of
Hanoi, the five disks are stacked on the first peg in bottom to top, largest to smallest
order). The goal configuration defines the destination positions of the disks (again, the
destination positions are the same as that of Towers of Hanoi, namely all five disks are
stacked in the same order as the initial configuration but on the last peg). The goal of the
problem is to move the disks from the starting configuration to the goal configuration. All
actions in Towers of Hanoise have noisy outcomes. In particular, when executing an action
it is possible to drop a disk and have it be lost forever, thus bringing execution to a dead
end. The success probabilities are:

Action
“single-move-big-not-moved”
“single-move-big-moved”
“double-move-big-not-moved”
“double-move-big-moved”

Success Probability
0.99
0.95
0.80
0.90

Notice that the probability of succeeding with a move is dependent on the number of disks
moved and whether or not the big disk has been moved yet.
Every sequence of actions has some success probability less than one in this problem, so
it is not possible to reach the goal with certainty. To maximize the probability of reaching
the goal, a careful comparison must be made. To move the big disk from the first to last
peg, it is necessary to move the four smaller disks to the middle peg. This subgoal can
be achieved by executing “single-move-big-not-moved” fifteen times on the smaller disks,
resulting in a success probability of 0.9915 ≈ 0.86. It can also be accomplished by moving
the four smaller disks as two units of two using “double-move-big-not-moved” three times,
resulting in a low success probability of approximately 0.51.
Next, the big disk can be moved from the first to last peg with a success probability of
0.99 (“single-move-big-not-moved”). Then, the four smaller disks again need to be moved,
this time from the middle peg to the last peg. Now that the big disk has been moved, the
success probabilities change and the two strategies yield success probabilities of about 0.46
for “single-move-big-moved” and 0.73 for “double-move-big-moved”.
A planner that chooses optimally at each step would switch from single moves to double
moves after the big disk is in place resulting in a total success probability of 0.9915 ×
0.99 × 0.93 ≈ 0.62. One that ignores probabilities and always uses single moves has a lower
success probability of 0.9915 × 0.99 × 0.9515 ≈ 0.39. A planner that ignores probabilities and
minimizes the number of steps by always using double moves has a lower success probability
still of 0.83 ×0.99×0.93 ≈ 0.37. Thus, for optimum performance, a planner must realize that
its policy should consider the success probabilities of actions and how they are influenced
by the status of the big disk.
869

Younes, Littman, Weissman & Asmuth

4.8 Zeno Travel
Our last domain is Zeno Travel, based on a domain used in IPC-3. Problem instances
of this domain involve using airplanes to move people between cities. An airplane requires fuel to fly. It can be flown at two different speeds—the higher speed requiring
more fuel. Our problem instance used one aircraft, two people, three cities and seven
fuel levels. The actions of the domain are “start-boarding”, “complete-boarding”, “startdebarking”, “complete-debarking”, “start-refueling”, “complete-refueling”, “start-flying”,
“complete-flying”, “start-zooming”, and “complete-zooming”. The initial configuration
specifies the location of the plane, the initial fuel level of the plane and the location of
all people (as well as some initializations to allow for arithmetic type operations on the
fuel-level objects). The goal configuration specifies a destination for the plane and destinations for all people. The noise in this domain comes from the family of “complete-X”
actions. Each time a “complete-X” action is executed it will have the desired effect with
probability 1/k for some positive integer k (note that k is a function of the action executed, specifically k = 20 for “complete-boarding” and k = 30 for “complete-debarking”).
If the desired effect is not achieved then there is no effect, and this occurs with probability
1 − (1/k). This structure is meant to represent actions with random duration. Each “durative” action X is represented by two primitive actions “start-X” and “complete-X”, giving
X a duration that is geometrically distributed.
Ultimately, this problem presented no real challenge because we neglected to include
action costs. Since actions have either standard desired effect or none at all, a planner can
simple continue to execute an action until its effect is achieved, without incurring any cost.

5. Competition Results
Based on the initial announcement of the competition, we put together a mailing list of
87 researchers expressing interest. As development of PPDDL, the server, the evaluation
criteria, and the practice domains progressed, we kept the community informed by releasing
a series of FAQs (May 2003, FAQ 0.1; September 2003, FAQ 0.5; November 2003 FAQ 1.01).
By early 2004, a core group of participants became evident and the competition logistics
were finalized. Leading up to June 2004, participants ran their planners on the previously
unseen test problems. We tabulated the scores in each of a set of evaluation categories and
presented them at ICAPS’04 in Vancouver, Canada.
The following subsections describe the competition’s participants, evaluation tracks, and
results.
5.1 Participants
Although twenty teams registered for the competition initially, seven teams from four continents ultimately competed. They produced ten different planners, which were evaluated
on various subsets of the problem domains. The groups and their planners were:
• Group C. UMass
Participants: Zhengzhu Feng (University of Massachusetts) and Eric Hansen (Mississippi State University).
870

The First Probabilistic Track of IPC

Description: Symbolic heuristic search.
• Group E. Dresden (“FluCaP”, formerly “FCPlanner”)
Participants: Eldar Karabaev and Olga Skvortsova (both of Dresden University of
Technology).
Description: First-order heuristic search.
• Group G. ANU (“NMRDPP”)
Participants: Charles Gretton, David Price and Sylvie Thiébaux (all of The Australian
National University).
Descriptions: G1: Planner primarily designed for domains with non-Markovian rewards, and G2: NMRDPP augmented with control knowledge.
• Group J. Purdue
Participants: SungWook Yoon, Alan Fern and Robert Givan (all of Purdue University).
Descriptions: J1: Human-written policy in Classy’s policy language (“Purdue-Humans”), J2: Offline policy iteration by reduction to classification, automatically acquiring a domain-specific policy (“Classy”), and J3: Deterministic replanner using
FF (“FF-rePlan”).
• Group P. Simón Bolı́var (“mGPT”)
Participants: Blai Bonet (Universidad Simón Bolı́var) and Héctor Geffner (Universitat
Pompeu Fabra).
Description: Labeled RTDP with lower bounds extracted from the problem description.
• Group Q. Michigan Tech (“Probapop”)
Participants: Nilufer Onder, Garrett C. Whelan and Li Li (all of Michigan Technological University).
Description: POP-style planner (no sensing).
• Group R. CERT
Participants: Florent Teichteil-Königsbuch and Patrick Fabiani (both of CERT).
Description: Probabilistic reachability heuristic and DBNs.
5.2 Evaluation Tracks
It was clear from the discussions leading up to the competition that different groups were
prioritizing their efforts differently. We wanted to ensure that a diverse set of powerful
approaches were recognized and decided to tabulate results in several different ways to
acknowledge the value of these different approaches. The six tracks were:
871

Younes, Littman, Weissman & Asmuth

• Overall. This track used a reward-based evaluation criterion for all domains (goal
achievement counted as 500 for goal-based domains). Domains: Blocksworld (7 problems), Colored Blocksworld (2), Boxworld (5), Exploding Blocksworld (1), Fileworld
(1), Tireworld (2), Towers of Hanoise (1), Zeno Travel (1).
• Goal-based. For this track, we ignored action costs and counted goal achievement
as a unit reward (thus emphasizing approaches that maximized the probability of
reaching a goal state). The domains and problems used were the same as in the
Overall track: Blocksworld (7), Colored Blocksworld (2), Boxworld (5), Exploding
Blocksworld (1), Fileworld (1), Tireworld (2), Towers of Hanoise (1), Zeno Travel (1).
• Overall, Non-Blocks/Box. Blocksworld and Boxworld dominated the full set
and we wanted to see how subtler problems were handled. Domains: Exploding
Blocksworld (1), Fileworld (1), Tireworld (2), Towers of Hanoise (1), Zeno Travel (1).
• Domain-specific. “Domain-specific” allowed human-tuned rules; “Domain-specific,
No Tuning” did not (only automatically generated rules specific to the domain were
allowed). They were evaluated using the generated domains: Blocksworld (8), Colored
Blocksworld (6), Boxworld (5).
• Conformant. Planners in this category had to produce straight-line plans, “blind” to
intermediate states encountered. We prepared “unobservable” versions of the domains
to evaluate planners in this category. Domains: Blocksworld (7), Colored Blocksworld
(2), Boxworld (5), Exploding Blocksworld (1), Fileworld (1), Tireworld (2), Towers of
Hanoise (1), Zeno Travel (1).
5.3 Results
To display the results for each evaluation track, we plotted the cumulative reward achieved
by each participant over the set of evaluation problems (reward is accumulated left to right).
In the reward-based tracks, goal achievement was counted as 500 for problems without an
explicitly specified goal reward. These plots highlight where one planner has an advantage
over another (greater slope) as well as the total difference in score (height difference between
the lines).
Figure 7 displays the results in the Overall category. Two planners, J3 and P, produced
significantly more positive results than the others, with the replanning algorithm J3 clearly
dominating the others. J3 was crowned Overall winner, with P as runner up. The figure
also displays the results for the Conformant category, which consisted solely of Q, the
uncontested winner of the category.
Similar results are visible in the Goal-based track, displayed in Figure 8, in which J3
again comes out ahead with P achieving runner-up status. Comparing Figures 7 and 8
reveals that the margin of victory between J3 and P, R and G1 is diminished in the Goalbased category. This suggests that J3 is more sensitive to the rewards themselves—choosing
cheaper paths among the multiple paths available to the goal. In the set of problems used in
the competition, this distinction was not very significant and the graphs look very similar.
However, a different set of test problems might have revealed the fundamental tradeoff
872

873
Zeno Travel

Tower of Hanoise

Tireworld (reward)

Tireworld (goal)

Fileworld

Exploding Blocksworld

Boxworld (10, 10; goal)

Boxworld (5, 10; goal)

Boxworld (15, 10)

Boxworld (10, 10)

Boxworld (5, 10)

Colored Blocksworld (8; goal)

Colored Blocksworld (8)

Blocksworld (8; goal)

Blocksworld (21)

Blocksworld (18)

Blocksworld (15)

5000

Blocksworld (11)

Blocksworld (8)

Blocksworld (5)

cumulative reward

The First Probabilistic Track of IPC

J3
P
C
G1
R
Q

4000

3000

2000

1000

0

Figure 7: Competition results in the Overall category. The result for the Conformant category is the line marked “Q”. The numbers in parentheses indicate problem size:
number of blocks for Blocksworld domains; number of cities and boxes, respectively, for Boxworld domains.

Younes, Littman, Weissman & Asmuth

J3
P
C
G1
R
Q

14

cumulative goal probability

12

10

8

6

4

2

Zeno Travel

Tower of Hanoise

Tireworld (reward)

Tireworld (goal)

Fileworld

Exploding Blocksworld

Boxworld (10, 10; goal)

Boxworld (5, 10; goal)

Boxworld (15, 10)

Boxworld (10, 10)

Boxworld (5, 10)

Colored Blocksworld (8; goal)

Colored Blocksworld (8)

Blocksworld (8; goal)

Blocksworld (21)

Blocksworld (18)

Blocksworld (15)

Blocksworld (11)

Blocksworld (8)

Blocksworld (5)

0

Figure 8: Competition results in the Goal-based category.
between seeking to maximize reward and seeking to reach the goal with high probability.
Future competitions could attempt to highlight this important issue.
It is very interesting to note that J3’s outstanding performance stems primarily from
the early problems, which are the Blocksworld and Boxworld problems that are amenable
to replanning. The later problems in the set were not handled as well by J3 as by most
other planners.
Figure 9 displays the results for the Non-Block/Box category. Indeed, J3 performed
much more poorly on the problems is this category, with Planner C taking the top spot.
The runner-up spot was closely contested between planners R and G1, but G1 pulled ahead
on the last problem to claim the honors. Planner P also performed nearly as well on this
set.
Figure 10 gives a more detailed view of the results in the Non-Blocks/Box category.
The optimal score for each problem is indicated in the graphs.3 Note that Planner C’s
performance in the Tireworld domain is well above optimal, the result of a now-fixed bug in
the competition server that allowed disabled actions to be executed. Planner P displayed
outstanding performance on the Fileworld and goal-based Tireworld problems, but did not
attempt to solve Tower of Hanoise and therefore fell behind G1 and R overall. Planner R
used more time per round than Planner G1 in the Zeno Travel domain, which ultimately
cost R the second place because it could only complete 27 of the 30 runs in this domain.
Note that some planners received a negative score on the reward-oriented problems. We
3. The optimal scores do not necessarily apply to Planner Q, which is a conformant planner.

874

The First Probabilistic Track of IPC

C
G1
R
P
J3
Q

1000

cumulative reward

800

600

400

200

Zeno Travel

Tower of Hanoise

Tireworld (reward)

Tireworld (goal)

Fileworld

Exploding Blocksworld

0

Figure 9: Summary of competition results in the Overall, Non-Blocks/Box category.
counted negative scores on individual problems as zero in the overall evaluation so as not to
give an advantage to planners that did not even attempt to solve some problems. Planner Q
was the only entrant (except, possibly, for C) to receive a positive score on the reward-based
Tireworld problem. The planners with negative score for this problem used the expensive
“call-AAA” action to ensure that the goal was always reached.
The results for the domain-specific planners are shown in Figure 11. The highest scoring
planners were J1 and G2, with the difference between them primarily due to the two largest
Blocksworld problems, which J1 solved more effectively than G2. The performance of the
five domain-specific planners on the colored Blocksworld problems is virtually indistinguishable. As mentioned earlier, grounding of the goal condition in the validator prevented us
from using larger problem instances, which might otherwise have separated the planners in
this domain.
The two planners that won the “domain specific” category were ineligible for the “no
tuning” subcategory because they were hand-tuned for these domains. Thus, J3 and J2
took the top spots in the subcategory. It is interesting to note that J3 won in spite of being
a general-purpose planner—it was not, in fact, created to be domain specific. It overtook
J2 due to two small Boxworld problems that J3 solved but J2 missed.
Figure 12 summarizes the competition results in the six evaluation categories.

6. Conclusion
We are happy with the outcomes of the first probabilistic track of the International Planning
Competition. In addition to bringing attention to this important set of planning challenges,
875

Younes, Littman, Weissman & Asmuth

Exploding Blocksworld

Tireworld (goal)

1

1

**
max prob.

0.8

goal probability

0.6
0.4
0.2
0

0.8
0.6
0.4
0.2

*
C

G1

*
P

J3

*
Q

*
R

0
C

1

100
*

*

*

0
-100

prob.
max prob.
reward
max reward
G1

J3

Q

R

P

Q

prob.
max prob.
reward
max reward

0.8

200

C

P

**

300
goal probability

1
0.8
0.6
0.4
0.2
0

J3

Tireworld (reward)

reward

goal probability

Fileworld

G1

0.6

80
60

0.4

40

0.2

20

0

-200

reward

goal probability

max prob.

0

-300

-20

-400
R

C

G1

Tower of Hanoise

J3

P

Q

R

Zeno Travel
max prob.

1

1

0.8

goal probability

goal probability

max prob.

0.6
0.4
0.2
0

0.8
0.6
0.4
0.2

*
C

G1

J3

*
P

*
Q

0
R

C

G1

J3

P

Q

R

Figure 10: Competition results for Non-Blocks/Box problems (* indicates that a planner
did not attempt to solve a problem; ** indicates anomalous results due to a bug
in the server that allowed the execution of disabled actions). Note that the two
graphs in the center have reward scales to the right.

876

The First Probabilistic Track of IPC

J1*
G2*
J3
J2
E*

8000
7000

cumulative reward

6000
5000
4000
3000
2000
1000

Boxworld (10, 10; goal)

Boxworld (5, 10; goal)

Boxworld (15, 10)

Boxworld (10, 10)

Boxworld (5, 10)

Colored Blocksworld (11; goal)

Colored Blocksworld (8; goal)

Colored Blocksworld (5; goal)

Colored Blocksworld (11)

Colored Blocksworld (8)

Colored Blocksworld (5)

Blocksworld (8; goal)

Blocksworld (21)

Blocksworld (18)

Blocksworld (15)

Blocksworld (11)

Blocksworld (8)

Blocksworld (5)

0

Figure 11: Competition results in the Domain-specific categories. For the “No Tuning”
category results, ignore the J1, G2, and E lines on the graph (marked with
asterisks).

Category
Overall
Goal-based Domains
Overall, Non-Blocks/Box
Domain-specific, No Tuning
Domain-specific
Conformant

1st
J3
J3
C
J3
J1
Q

2nd
P
P
G1
J2
G2

Figure 12: Summary of competition results by category.

877

Younes, Littman, Weissman & Asmuth

it appears to have helped spur the community to use uniform comparison problems by
providing a domain language and a set of benchmarks (Yoon, Fern, & Givan, 2005).
In spite of the success, we feel there are changes that could be made in future competitions that would increase their value to the community. First, on the competition logistics
side, our server logged outcomes of the interactions between planners and domains, but did
not keep an exhaustive record of the actions taken and timing information. In retrospect,
such information would have been helpful in identifying how the planners addressed the
domains and whether they took suboptimal actions or just got unlucky. In addition, our
server had no provisions for security. A simple password and/or reservation system would
have helped the evaluations go much more smoothly as it would have prevented inadvertent
access to the server by one group when another was assigned an evaluation slot.
On the domain side, we hope future competitions are able to focus on more interesting
domains. We found that simply adding noisy action failures to a deterministic domain was
not enough to produce interesting probabilistic problems—for such domains, straightforward replanning can be very effective. The non-Blocksworld domains we created were not
mastered by any of the planners and we hope that they are retained in some form in future
evaluations.
Like the progression of competitions in the classical track, we hope future competitions
in the probabilistic track move toward domains grounded in real-life data and real-world
problems including the handling of partially observability and time. A second competition
is slated to be held in conjunction with IPC in 2006 and we urge interested members of the
planning community to participate to help keep the competition moving in a productive
direction for the benefit of the field.

Acknowledgments
We appreciate the support of the National Science Foundation and the Royal Swedish
Academy of Engineering Sciences, as well as the feedback of Sven Koenig, Shlomo Zilberstein, Paul Batchis, Bob Givan, Hector Geffner and other participants who contributed to
the design of the competition. JAIR editor David Smith and his anonymous reviewers provided invaluable insights on the document that we tried to reflect in this final manuscript.
This material is based upon work supported by the National Science Foundation under
Grant No. 0315909 and the Royal Swedish Academy of Engineering Sciences (IVA) with
grants from the Hans Werthén fund. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect
the views of the National Science Foundation or IVA.

878

The First Probabilistic Track of IPC

Appendix A. BNF Grammar for PPDDL1.0
We provide the full syntax for PPDDL1.0 using an extended BNF notation with the following conventions:
• Each rule is of the form hnon-terminal i ::= expansion.
• Alternative expansions are separated by a vertical bar (“|”).
• A syntactic element surrounded by square brackets (“[“ and “]”) is optional.
• Expansions and optional syntactic elements with a superscripted requirements flag are
only available if the requirements flag is specified for the domain or problem currently
being defined. For example, [htypes-def i]:typing in the syntax for domain definitions
means that htypes-def i may only occur in domain definitions that include the :typing
flag in the requirements declaration.
• An asterisk (“*”) following a syntactic element x means zero or more occurrences of
x ; a plus (“+ ”) following x means at least one occurrence of x.
• Parameterized non-terminals, for example htyped list (x )i, represent separate rules for
each instantiation of the parameter.
• Terminals are written using typewriter font.
• The syntax is Lisp-like. In particular, case is not significant (for example, ?x and ?X
are equivalent), parenthesis are an essential part of the syntax and have no semantic
meaning in the extended BNF notation, and any number of whitespace characters
(space, newline, tab, etc.) may occur between tokens.
A.1 Domains
The syntax for domain definitions is the same as for PDDL2.1, except that durative actions
are not allowed. Declarations of constants, predicates, and functions are allowed in any
order with respect to one another, but they must all come after any type declarations and
precede any action declarations.
hdomaini

hrequire-def i
hrequire-keyi
htypes-def i
hconstants-def i
hpredicates-def i

::= ( define ( domain hnamei )
[hrequire-def i]
[htypes-def i]:typing
[hconstants-def i]
[hpredicates-def i]
[hfunctions-def i]:fluents
hstructure-def i* )
::= ( :requirements hrequire-keyi* )
::= See Section A.4
::= ( :types htyped list (name)i )
::= ( :constants htyped list (name)i )
::= ( :predicates hatomic formula skeletoni* )
879

Younes, Littman, Weissman & Asmuth

hatomic formula skeletoni
hpredicatei
hfunctions-def i
hfunction skeletoni
hfunction symbol i
hstructure-def i
haction-def i
htyped list (x )i
htypei
hprimitive typei
hfunction typed list (x )i
hfunction typei

::= ( hpredicatei htyped list (variable)i )
::= hnamei
::= ( :functions hfunction typed list (function skeleton)i )
::= ( hfunction symbol i htyped list (variable)i )
::= hnamei
::= haction-def i
::= See Section A.2
::= hx i* |:typing hx i+ - htypei htyped list (x )i
::= ( either hprimitive typei+ ) | hprimitive typei
::= hnamei
::= hx i*
|:typing hx i+ - hfunction typei hfunction typed list (x )i
::= number

A hnamei is a string of characters starting with an alphabetic character followed by a
possibly empty sequence of alphanumeric characters, hyphens (“-”), and underscore characters (“ ”). A hvariablei is a hnamei immediately preceded by a question mark (“?”). For
example, in-office and ball 2 are names, and ?gripper is a variable.
A.2 Actions
Action definitions and goal descriptions have the same syntax as in PDDL2.1.
haction-def i

::= ( :action haction symbol i
[:parameters ( htyped list (variable)i )]
haction-def bodyi )
haction symbol i
::= hnamei
haction-def bodyi
::= [:precondition hGDi]
[:effect heffecti]
hGDi
::= hatomic formula (term)i | ( and hGDi* )
|:equality ( = htermi htermi )
|:equality ( not ( = htermi htermi ) )
|:negative-preconditions ( not hatomic formula (term)i )
|:disjunctive-preconditions ( not hGDi )
|:disjunctive-preconditions ( or hGDi* )
|:disjunctive-preconditions ( imply hGDi hGDi )
|:existential-preconditions ( exists ( htyped list (variable)i )
hGDi )
|:universal-preconditions ( forall ( htyped list (variable)i )
hGDi )
|:fluents hf-compi
hatomic formula (x )i ::= ( hpredicatei hx i* ) | hpredicatei
htermi
::= hnamei | hvariablei
hf-compi
::= ( hbinary-compi hf-expi hf-expi )
hbinary-compi
::= < | <= | = | >= | >
hf-expi
::= hnumber i | hf-head (term)i
880

The First Probabilistic Track of IPC

hf-head (x )i
hbinary-opi

| ( hbinary-opi hf-expi hf-expi ) | ( - hf-expi )
::= ( hfunction symbol i hx i* ) | hfunction symbol i
::= + | - | * | /

A hnumber i is a sequence of numeric characters, possibly with a single decimal point (“.”)
at any position in the sequence. Negative numbers are written as (- hnumber i).
The syntax for effects has been extended to allow for probabilistic effects, which can be
arbitrarily interleaved with conditional effects and universal quantification.
heffecti

::= hp-effecti | ( and heffecti* )
|:conditional-effects ( forall ( htyped list (variable)i ) heffecti )
|:conditional-effects ( when hGDi heffecti )
|:probabilistic-effects ( probabilistic hprob-effecti+ )
hp-effecti
::= hatomic formula (term)i | ( not hatomic formula (term)i )
|:fluents ( hassign-opi hf-head (term)i hf-expi )
|:rewards ( hadditive-opi hreward fluenti hf-expi )
hprob-effecti
::= hprobabilityi heffecti
hassign-opi
::= assign | scale-up | scale-down | hadditive-opi
hadditive-opi ::= increase | decrease
hreward fluenti ::= ( reward ) | reward

A hprobabilityi is a hnumber i with a value in the interval [0, 1].
A.3 Problems
The syntax for problem definitions has been extended to allow for the specification of a
probability distribution over initial states, and also to permit the association of a one-time
reward with entering a goal state. It is otherwise identical to the syntax for PDDL2.1
problem definitions.
hproblemi

hobjects-def i
hiniti
hinit-el i
hp-init-el i
hprob-init-el i
ha-init-el i
hgoal i
hgoal-speci
hmetric-speci

::= ( define ( problem hnamei )
( :domain hnamei )
[hrequire-def i]
[hobjects-def i]
[hiniti]
hgoal i )
::= ( :objects htyped list (name)i )
::= ( :init hinit-el i* )
::= hp-init-el i
|:probabilistic-effects ( probabilistic hprob-init-el i* )
::= hatomic formula (name)i |:fluents ( = hf-head (name)i hnumber i )
::= hprobabilityi ha-init-el i
::= hp-init-el i | ( and hp-init-el i* )
::= hgoal-speci [hmetric-speci] | hmetric-speci
::= ( :goal hGDi ) [( :goal-reward hground-f-expi )]:rewards
::= ( :metric hoptimizationi hground-f-expi )
881

Younes, Littman, Weissman & Asmuth

hoptimizationi ::= minimize | maximize
hground-f-expi ::= hnumber i | hf-head (name)i
| ( hbinary-opi hground-f-expi hground-f-expi )
| ( - hground-f-expi )
| ( total-time ) | total-time
| ( goal-achieved ) | goal-achieved
|:rewards hreward fluenti
A.4 Requirements
Below is a table of all requirements in PPDDL1.0. Some requirements imply others; some
are abbreviations for common sets of requirements. If a domain stipulates no requirements,
it is assumed to declare a requirement for :strips.
Requirement
:strips
:typing
:equality
:negative-preconditions
:disjunctive-preconditions
:existential-preconditions
:universal-preconditions
:quantified-preconditions
:conditional-effects
:probabilistic-effects
:rewards
:fluents
:adl

:mdp

Description
Basic STRIPS-style adds and deletes
Allow type names in declarations of variables
Support = as built-in predicate
Allow negated atoms in goal descriptions
Allow disjunctive goal descriptions
Allow exists in goal descriptions
Allow forall in goal descriptions
= :existential-preconditions
+ :universal-preconditions
Allow when and forall in action effects
Allow probabilistic in action effects
Allow reward fluent in action effects and
optimization metric
Allow numeric state variables
= :strips + :typing + :equality
+ :negative-preconditions
+ :disjunctive-preconditions
+ :quantified-preconditions
+ :conditional-effects
= :probabilistic-effects + :rewards

882

The First Probabilistic Track of IPC

CLIENT

SERVER

/ session-request \
\
/
/ session-init \
\
/

/ round-request \
\
/
/ round-init \
\
/

/ state \
\
/
/ action
\

spec \/

..
.
/ state \
\
/
/ action
\

spec \/

/ end-round \
\
/










 repeat










/ end-session \
\
/

Figure 13: Successful communication session.

Appendix B. Communication Protocol
We adopt an XML-like syntax for the client/server communication protocol. We use the
same extended BNF notation as in Appendix A to describe the syntax of protocol messages.
The hnamei and hnumber i terminals are defined in exactly the same way as for PPDDL. An
hinteger i is a nonempty string of numeric characters. A hmessagei is an arbitrary character
string, possibly empty.
Figure 13 shows the expected sequence of messages. A session starts by the client
sending a hsession-requesti message to the server. The server replies with a hsession-initi
message, which tells the client the number of evaluation rounds that will be run. To start
an evaluation round, the client sends a hround-requesti message, to which the server replies
with a hround-initi message. At this point the evaluation round starts. The server sends
a hturn-responsei message to the client, which can be a hstatei message or an hend-round i
message. For every hstatei message that the client receives, it sends an haction speci message
in return. Once the client receives an hend-round i message, it ends the current evaluation
round. The client then starts a new evaluation round with a hround-requesti message to
the server, or waits for an hend-sessioni message from the server in case all rounds have
already been run. The server sends an herror i message to the client if an error occurs, for
example if the server receives an unexpected message from the client.
883

Younes, Littman, Weissman & Asmuth

B.1 Client Messages
Client messages have the following form:
hsession-requesti ::= <session-request>
<name> hnamei </name>
<problem> hnamei </problem>
</session-request>
hround-requesti

::= <round-request/>

haction speci
hactioni
htermi

::= <act> hactioni </act> | <done/>
::= <action> <name> hnamei </name> htermi* </action>
::= <term> hnamei </term>

B.2 Server Messages
Server messages have the following form:
hsession-initi

::= <session-init>
<sessionID> hinteger i </sessionID>
<setting>
<rounds> hinteger i </rounds>
<allowed-time> hinteger i </allowed-time>
<allowed-turns> hinteger i </allowed-turns>
</setting>
</session-init>

hround-initi

::= <round-init>
<round> hinteger i </round>
<sessionID> hinteger i </sessionID>
<time-left> hinteger i </time-left>
<rounds-left> hinteger i </rounds-left>
</round-init>

hturn-responsei ::= hstatei | hend-round i
hend-round i
::= <end-round>
hstatei [<goal-reached/>]
<time-spent> hinteger i </time-spent>
<turns-used> hinteger i </turns-used>
</end-round>
hstatei
::= <state> [<is-goal/>] hatomi* hfluenti* </state>
hatomi
::= <atom> hpredicatei htermi* </atom>
hfluenti
::= <fluent> hfunctioni htermi* hvaluei </fluent>
hpredicatei
::= <predicate> hnamei </predicate>
hfunctioni
::= <function> hnamei </function>
htermi
::= <term> hnamei </term>
884

The First Probabilistic Track of IPC

hvaluei

::= <value> hnumber i </value>

hend-sessioni

::= <end-session>
<sessionID> hinteger i </sessionID>
<problem> hnamei </problem>
<rounds> hinteger i </rounds>
<goals>
<failed> hinteger i </failed>
<reached>
<successes> hinteger i </successes>
[<time-average> hnumber i </time-average>]
</reached>
</goals>
[<metric-average> hnumber i </metric-average>]
</end-session>

herror i

::= <error> hmessagei </error>

885

Younes, Littman, Weissman & Asmuth

References
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial Intelligence Research,
11, 1–94.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure in policy construction. In Mellish, C. S. (Ed.), Proceedings of the Fourteenth International Joint
Conference on Artificial Intelligence, pp. 1104–1111, Montreal, Canada. Morgan Kaufmann Publishers.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence in Bayesian networks. In Proceedings of the Twelfth Annual Conference on
Uncertainty in Artificial Intelligence (UAI 96), pp. 115–123, Portland, OR.
Dean, T., & Kanazawa, K. (1989). A model for reasoning about persistence and causation.
Computational Intelligence, 5 (3), 142–150.
Dearden, R., & Boutilier, C. (1997). Abstraction and approximate decision-theoretic planning. Artificial Intelligence, 89 (1–2), 219–283.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. Journal of Artificial Intelligence Research, 20, 61–124.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
for factored MDPs. Journal of Artificial Intelligence Research, 19, 399–468.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning using
decision diagrams. In Laskey, K. B., & Prade, H. (Eds.), Proceedings of the Fifteenth
Conference on Uncertainty in Artificial Intelligence, pp. 279–288, Stockholm, Sweden.
Morgan Kaufmann Publishers.
Howard, R. A. (1960). Dynamic Programming and Markov Processes. John Wiley & Sons,
New York, NY.
Howard, R. A. (1971). Dynamic Probabilistic Systems, Vol. I: Markov Models. John Wiley
& Sons, New York, NY.
Kushmerick, N., Hanks, S., & Weld, D. S. (1995). An algorithm for probabilistic planning.
Artificial Intelligence, 76 (1–2), 239–286.
Littman, M. L. (1997). Probabilistic propositional planning: Representations and complexity. In Proceedings of the Fourteenth National Conference on Artificial Intelligence,
pp. 748–754, Providence, RI. American Association for Artificial Intelligence, AAAI
Press.
Littman, M. L., Goldsmith, J., & Mundhenk, M. (1998). The computational complexity of
probabilistic planning. Journal of Artificial Intelligence Research, 9, 1–36.
McDermott, D. (2000). The 1998 AI planning systems competition. AI Magazine, 21 (2),
35–55.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, New York, NY.
886

The First Probabilistic Track of IPC

Rintanen, J. (2003). Expressive equivalence of formalisms for planning with sensing. In
Giunchiglia, E., Muscettola, N., & Nau, D. S. (Eds.), Proceedings of the Thirteenth International Conference on Automated Planning and Scheduling, pp. 185–194, Trento,
Italy. AAAI Press.
Yoon, S., Fern, A., & Givan, R. (2005). Learning measures of progress for planning domains.
In Proceedings of the Twentieth National Conference on Artificial Intelligence, pp.
1217–1222.
Younes, H. L. S., & Littman, M. L. (2004). PPDDL1.0: An extension to PDDL for expressing
planning domains with probabilistic effects. Tech. rep. CMU-CS-04-167, Carnegie
Mellon University, Pittsburgh, PA.

887

Journal of Artificial Intelligence Research 24 (2005) 305–339

Submitted 11/04; published 8/05

Learning Concept Hierarchies from Text Corpora
using Formal Concept Analysis
CIMIANO @ AIFB . UNI - KARLSRUHE . DE

Philipp Cimiano
Institute AIFB, University of Karlsruhe
Englerstr. 11, 76131 Karlsruhe, Germany

HOTHO @ CS . UNI - KASSEL . DE

Andreas Hotho
Knowledge and Data Engineering Group, University of Kassel
Wilhelmshöher Allee 73, 34121 Kassel, Germany

STAAB @ UNI - KOBLENZ . DE

Steffen Staab
Institute for Computer Science, University of Koblenz-Landau
Universitätsstr. 1, 56016 Koblenz, Germany

Abstract
We present a novel approach to the automatic acquisition of taxonomies or concept hierarchies
from a text corpus. The approach is based on Formal Concept Analysis (FCA), a method mainly
used for the analysis of data, i.e. for investigating and processing explicitly given information. We
follow Harris’ distributional hypothesis and model the context of a certain term as a vector representing syntactic dependencies which are automatically acquired from the text corpus with a linguistic parser. On the basis of this context information, FCA produces a lattice that we convert into
a special kind of partial order constituting a concept hierarchy. The approach is evaluated by comparing the resulting concept hierarchies with hand-crafted taxonomies for two domains: tourism
and finance. We also directly compare our approach with hierarchical agglomerative clustering as
well as with Bi-Section-KMeans as an instance of a divisive clustering algorithm. Furthermore, we
investigate the impact of using different measures weighting the contribution of each attribute as
well as of applying a particular smoothing technique to cope with data sparseness.

1. Introduction
Taxonomies or concept hierarchies are crucial for any knowledge-based system, i.e. a system
equipped with declarative knowledge about the domain it deals with and capable of reasoning on the
basis of this knowledge. Concept hierarchies are in fact important because they allow to structure
information into categories, thus fostering its search and reuse. Further, they allow to formulate
rules as well as relations in an abstract and concise way, facilitating the development, refinement
and reuse of a knowledge-base. Further, the fact that they allow to generalize over words has shown
to provide benefits in a number of applications such as Information Retrieval (Voorhees, 1994) as
well as text clustering (Hotho, Staab, & Stumme, 2003) and classification (Bloehdorn & Hotho,
2004). In addition, they also have important applications within Natural Language Processing (e.g.
Cimiano, 2003).
However, it is also well known that any knowledge-based system suffers from the so-called
knowledge acquisition bottleneck, i.e. the difficulty to actually model the domain in question. In

c 2005 AI Access Foundation. All rights reserved.

C IMIANO , H OTHO , & S TAAB

order to partially overcome this problem we present a novel approach to automatically learning a
concept hierarchy from a text corpus.
Making the knowledge implicitly contained in texts explicit is a great challenge. For example,
Brewster, Ciravegna, and Wilks (2003) have argued that text writing and reading is in fact a process
of background knowledge maintenance in the sense that basic domain knowledge is assumed, and
only the relevant part of knowledge which is the issue of the text or article is mentioned in a more
or less explicit way. Actually, knowledge can be found in texts at different levels of explicitness
depending on the sort of text considered. Handbooks, textbooks or dictionaries for example contain
explicit knowledge in form of definitions such as “a tiger is a mammal” or “mammals such as
tigers, lions or elephants”. In fact, some researchers have exploited such regular patterns to discover
taxonomic or part-of relations in texts (Hearst, 1992; Charniak & Berland, 1999; Iwanska, Mata, &
Kruger, 2000; Ahmad, Tariq, Vrusias, & Handy, 2003). However, it seems that the more technical
and specialized the texts get, the less basic knowledge we find stated explicitly. Thus, an interesting
alternative is to derive knowledge from texts by analyzing how certain terms are used rather than to
look for their explicit definition. In these lines the distributional hypothesis (Harris, 1968) assumes
that terms are similar to the extent to which they share similar linguistic contexts.
In fact, different methods have been proposed in the literature to address the problem of (semi-)
automatically deriving a concept hierarchy from text based on the distributional hypothesis. Basically, these methods can be grouped into two classes: the similarity-based methods on the one hand
and the set-theoretical on the other hand. Both methods adopt a vector-space model and represent
a word or term as a vector containing features or attributes derived from a certain corpus. There is
certainly a great divergence in which attributes are used for this purpose, but typically some sort of
syntactic features are used, such as conjunctions, appositions (Caraballo, 1999) or verb-argument
dependencies (Hindle, 1990; Pereira, Tishby, & Lee, 1993; Grefenstette, 1994; Faure & Nédellec,
1998).
The first type of methods is characterized by the use of a similarity or distance measure in
order to compute the pairwise similarity or distance between vectors corresponding to two words
or terms in order to decide if they can be clustered or not. Some prominent examples for this type
of method have been developed by Hindle (1990), Pereira et al. (1993), Grefenstette (1994), Faure
and Nédellec (1998), Caraballo (1999) as well as Bisson, Nédellec, and Canamero (2000). Settheoretical approaches partially order the objects according to the inclusion relations between their
attribute sets (Petersen, 2002; Sporleder, 2002).
In this paper, we present an approach based on Formal Concept Analysis, a method based on
order theory and mainly used for the analysis of data, in particular for discovering inherent relationships between objects described through a set of attributes on the one hand, and the attributes
themselves on the other (Ganter & Wille, 1999). In order to derive attributes from a certain corpus,
we parse it and extract verb/prepositional phrase (PP)-complement, verb/object and verb/subject
dependencies. For each noun appearing as head of these argument positions we then use the corresponding verbs as attributes for building the formal context and then calculating the formal concept
lattice on its basis.
Though different methods have been explored in the literature, there is actually a lack of comparative work concerning the task of automatically learning concept hierarchies with clustering techniques. However, as argued by Cimiano, Hotho, and Staab (2004c), ontology engineers need guidelines about the effectiveness, efficiency and trade-offs of different methods in order to decide which
techniques to apply in which settings. Thus, we present a comparison along these lines between our
306

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

FCA-based approach, hierarchical bottom-up (agglomerative) clustering and Bi-Section-KMeans
as an instance of a divisive algorithm. In particular, we compare the learned concept hierarchies in
terms of similarity with handcrafted reference taxonomies for two domains: tourism and finance. In
addition, we examine the impact of using different information measures to weight the significance
of a given object/attribute pair. Furthermore, we also investigate the use of a smoothing technique
to cope with data sparseness.
The remainder of this paper is organized as follows: Section 2 describes the overall process
and Section 3 briefly introduces Formal Concept Analysis and describes the nature of the concept
hierarchies we automatically acquire. Section 4 describes the text processing methods we apply to
automatically derive context attributes. In Section 5 we discuss in detail our evaluation methodology
and present the actual results in Section 6. In particular, we present the comparison of the different
approaches as well as the evaluation of the impact of different information measures as well as of
our smoothing technique. Before concluding, we discuss some related work in Section 7.

2. Overall Process
The overall process of automatically deriving concept hierarchies from text is depicted in Figure 1.
First, the corpus is part-of-speech (POS) tagged 1 using TreeTagger (Schmid, 1994) and parsed using
LoPar2 (Schmid, 2000), thus yielding a parse tree for each sentence. Then, verb/subject, verb/object
and verb/prepositional phrase dependencies are extracted from these parse trees. In particular, pairs
are extracted consisting of the verb and the head of the subject, object or prepositional phrase they
subcategorize. Then, the verb and the heads are lemmatized, i.e. assigned to their base form. In
order to address data sparseness, the collection of pairs is smoothed, i.e. the frequency of pairs
which do not appear in the corpus is estimated on the basis of the frequency of other pairs. The
pairs are then weighted according to some statistical measure and only the pairs over a certain
threshold are transformed into a formal context to which Formal Concept Analysis is applied. The
lattice resulting from this, ( , ), is transformed into a partial order ( , ) which is closer to a
concept hierarchy in the traditional sense. As FCA typically leads to a proliferation of concepts, the
partial order is compacted in a pruning step, removing abstract concepts and leading to a compacted
partial order ( , ) which is the resulting concept hierarchy. This process is described in detail in
Section 3. The process is described more formally by Algorithm 1.



 

   

3. Formal Concept Analysis
Formal Concept Analysis (FCA) is a method mainly used for the analysis of data, i.e. for deriving
implicit relationships between objects described through a set of attributes on the one hand and
these attributes on the other. The data are structured into units which are formal abstractions of
concepts of human thought, allowing meaningful comprehensible interpretation (Ganter & Wille,
1999). Thus, FCA can be seen as a conceptual clustering technique as it also provides intensional
descriptions for the abstract concepts or data units it produces. Central to FCA is the notion of a
formal context:
1. Part-of-speech tagging consists in assigning each word its syntactic category, i.e. noun, verb, adjective etc.
2. http://www.ims.uni-stuttgart.de/projekte/gramotron/SOFTWARE/LoPar-en.html

307

C IMIANO , H OTHO , & S TAAB

Algorithm 1 ConstructConceptHierarchy(D,T)
/* construct a hierarchy for the terms in on the basis of the documents in
1: Parses = parse(POS-tag( ));
2: SynDeps = tgrep(Parses);
3: lemmatize(SynDeps);
4: smooth(SynDeps);
5: weight(SynDeps);
6: SynDeps’ = applyThreshold(SynDeps);
7:
= getFormalContext( ,SynDeps’);
8:
computeLattice
;
9:
transform
;
compact
;
10:
11: return
;





  	
      	
     
      



*/


 
  
    

Parser

tgrep

Lattice
Compaction

Lemmatizer

Smoothing

Pruning

FCA

Weighting

Figure 1: Overall Process

Definition 1 (Formal Context)
A triple ( , , ) is called a formal context if and
are sets and
relation between and . The elements of are called objects, those of
the incidence of the context.

  











 is a binary
 attributes and I is

   !"# $&%(')"*  '+,#-".0/
and dually for 123 : 1 ( 4'5".6$7%(8"*1  '+,#-"59/
Intuitively speaking,   is the set of all attributes common to the objects of  , while 1  is the
set of all objects that have all attributes in 1 . Furthermore, we define what a formal concept is:
For



, we define:

Definition 2 (Formal Concept)
A pair ( , ) is a formal concept of ( ,

 1

  , ) if and only if :;1<3=> +?1

@A1  .
In other words, ( , 1 ) is a formal concept if the set of all attributes shared by the objects of 
is identical with 1 and on the other hand  is also the set of all objects that have all attributes in 1 .
 is then called the extent and 1 the intent of the formal concept ( ,1 ). The formal concepts of a
308

and

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

given context are naturally ordered by the subconcept-superconcept relation as defined by:


! 1  =
  1  >  	 1;31 
Thus, formal concepts are partially ordered with regard to inclusion of their extents or (which is
equivalent) inverse inclusion of their intent.
We now give some examples to illustrate our definitions. In the context of the tourism domain
one knows for example that things like a hotel, an apartment, a car, a bike, a trip or an excursion can
be booked. Furthermore, we know that we can rent a car, a bike or an apartment. Moreover, we can
drive a car or a bike, but only ride a bike 3 . In addition, we know that we can join an excursion or a
trip. We can now represent the formal context corresponding to this knowledge as a formal context
(see Table 1). The lattice produced by FCA is depicted in Figure 2 (left) 4 . It can be transformed into
a special type of concept hierarchy as shown in Figure 2 (right) by removing the bottom element,
introducing an ontological concept for each formal concept (named with the intent) and introducing
a subconcept for each element in the extent of the formal concept in question.
into the partial order
,
In order to formally define the transformation of the lattice
we assume that the lattice is represented using reduced labeling. Reduced labeling as defined in
(Ganter & Wille, 1999) means that objects are in the extension of the most specific concept and
attributes conversely in the intension of the most general one. This reduced labeling is achieved by
introducing functions 
 and  . In particular, the name of an object is attached to the lower half
of the corresponding object concept, i.e. 

, while the name of attribute
is
located at the upper half of the attribute concept, i.e. 
. Now given a lattice
of formal concepts for a formal context
, we transform it into a partial order
as follows:

  

   

'
'     4'9/    4'9/  
#  2 ! /   ! /   
     @  

  
    

         

to
)
Definition 3 (Transformation of
First of all
contains objects as well as intents (sets of attributes):

 

   @# 1 $
  1  " #/
Further:

   7'( 1  $
 '   
  1  /.7
1  1   $ 
  1  =
   1   /
Finally, as FCA typically produces a high number of concepts, we compress the resulting hierarchy of ontological concepts by removing any inner node whose extension in terms of leave nodes
 as follows:
subsumed is the same as the one of its child, i.e. we create a partial order

      

      

Definition 4 (Compacted Concept Hierarchy
)
Assuming that  	 is the set of leave nodes dominated by  according to

 !

 

:

     ! "   $ %"#>"   !   # %$&'( 	) +
* '( 	#  /
Further:
3. According to the Longman Dictionary, in American English it is also possible to ride vehicles in general. However,
for the purposes of our example we gloss over this fact.
4. The Concept Explorer software was used to produce this lattice (see http://sourceforge.net/projects/conexp).

309

C IMIANO , H OTHO , & S TAAB

bookable

joinable

excursion

rentable

hotel

driveable

trip

apartment

rideable

car

bike

Figure 2: The lattice of formal concepts (left) and the corresponding hierarchy of ontological concepts (right) for the tourism example

i.e.

  

is the relation

 

      $   
restricted to pairs of elements of

 .

In particular for the hierarchy in figure 2 (right) we would remove the rideable concept.
hotel
apartment
car
bike
excursion
trip

bookable
x
x
x
x
x
x

rentable

driveable

rideable

x
x
x

x
x

x

joinable

x
x

Table 1: Tourism domain knowledge as formal context
At a first glance, it seems that the hierarchy shown in Figure 2 (right) is somehow odd due
to the fact that the labels of abstract concepts are verbs rather than nouns as typically assumed.
However, from a formal point of view, concept identifiers have no meaning at all so that we could
have just named the concepts with some other arbitrary symbols. The reason why it is handy to
introduce ’meaningful’ concept identifiers is for the purpose of easier human readability. In fact,
if we adopt an extensional interpretation of our hierarchy, we have no problems asserting that the
extension of the concept denoted by bike is a subset of the extension of the concept of the rideable
objects in our world. This view is totally compatible with interpreting the concept hierarchy in
310

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

%  7 

& ,

terms of formal subsumption as given by the logical formula: 
   $     . We
thus conclude that from an extensional point of view the ’verb-like’ concept identifiers have the
same status as any concept label based on a noun. From an intensional point of view, there may
not even exist a hypernym with the adequate intension to label a certain abstract concept, such that
using a verb-like identifier may even be the most appropriate choice. For example, we could easily
replace the identifiers joinable, rideable and driveable by activity, two-wheeled vehicle and vehicle,
respectively. However, it is certainly difficult to substitute rentable by some ’meaningful’ term
denoting the same extension, i.e. all the things that can be rented.
It is also important to mention that the learned concept hierarchies represent a conceptualization
of a domain with respect to a given corpus in the sense that they represent the relations between
terms as they are used in the text. However, corpora represent a very limited view of the world or
a certain domain due to the fact that if something is not mentioned, it does not mean that it is not
relevant, but simply that it is not an issue for the text in question. This also leads to the fact that
certain similarities between terms with respect to the corpus are actually accidental, in the sense
that they do not map to a corresponding semantic relation, and which are due to the fact that texts
represent an arbitrary snapshot of a domain. Thus, the learned concept hierarchies have to be merely
regarded as approximations of the conceptualization of a certain domain.
The task we are now focusing on is: given a certain number of terms referring to concepts
relevant for the domain in question, can we derive a concept hierarchy between them? In terms of
FCA, the objects are thus given and we need to find the corresponding attributes in order to build
an incidence matrix, a lattice and then transform it into a corresponding concept hierarchy. In the
following section, we describe how we acquire these attributes automatically from the underlying
text collection.

	

4. Text Processing
As already mentioned in the introduction, in order to derive context attributes describing the terms
we are interested in, we make use of syntactic dependencies between the verbs appearing in the text
collection and the heads of the subject, object and PP-complements they subcategorize. In fact, in
previous experiments (Cimiano, Hotho, & Staab, 2004b) we found that using all these dependencies
in general leads to better results than any subsets of them. In order to extract these dependencies
automatically, we parse the text with LoPar, a trainable, statistical left-corner parser (Schmid, 2000).
From the parse trees we then extract the syntactic dependencies between a verb and its subject, object and PP-complement by using tgrep 5 . Finally, we also lemmatize the verbs as well as the head of
the subject, object and PP-complement by looking up the lemma in the lexicon provided with LoPar.
Lemmatization maps a word to its base form and is in this context used as a sort of normalization
of the text. Let’s take for instance the following two sentences:
The museum houses an impressive collection of medieval and modern art. The building combines geometric abstraction with classical references that allude to the Roman influence on the
region.
After parsing these sentences, we would extract the following syntactic dependencies:
5. see http://mccawley.cogsci.uiuc.edu/corpora/treebank3.html

311

C IMIANO , H OTHO , & S TAAB

houses subj(museum)
houses obj(collection)
combines subj(building)
combines obj(abstraction)
combine with(references)
allude to(influence)
By the lemmatization step, references is mapped to its base form reference and combines and
houses to combine and house, respectively, such that we yield as a result:
house subj(museum)
house obj(collection)
combine subj(building)
combine obj(abstraction)
combine with(reference)
allude to(influence)
In addition, there are three further important issues to consider:
1. the output of the parser can be erroneous, i.e. not all derived verb/argument dependencies are
correct,
2. not all the derived dependencies are ’interesting’ in the sense that they will help to discriminate between the different objects,
3. the assumption of completeness of information will never be fulfilled, i.e. the text collection
will never be big enough to find all the possible occurrences (compare Zipf, 1932).
To deal with the first two problems, we weight the object/attribute pairs with regard to a certain
information measure and only process further those verb/argument relations for which this measure
is above some threshold  . In particular, we explore the following three information measures (see
Cimiano, S.Staab, & Tane, 2003; Cimiano et al., 2004b):


  $ 	
 	       
  


(   
    '    $   
#      
   
     $   
 !

   $    '     .

#      	     	


	









	

Furthermore,    "  is the total number of occurrences of a term  as argument arg of a


verb  ,   
  is the number of occurrences of verb  with such an argument and    is the
relative frequency of a term  compared to all other terms. The first information measure is simply
the conditional probability of the term  given the argument ' of a verb  . The second mea
sure (     is the so called pointwise mutual information and was used by Hindle (1990) for
where   

	

 

312

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

discovering groups of similar terms. The third measure is inspired by the work of Resnik (1997)
and introduces an additional factor    
 which takes into account all the terms appearing in
the argument position   of the verb  in question. In particular, the factor measures the relative
entropy of the prior and posterior (considering the verb it appears with) distributions of  and thus
the ’selectional strength’ of the verb at a given argument position. It is important to mention that in
our approach the values of all the above measures are normalized into the interval [0,1].
The third problem requires smoothing of input data. In fact, when working with text corpora,
data sparseness is always an issue (Zipf, 1932). A typical method to overcome data sparseness is
smoothing (Manning & Schuetze, 1999) which in essence consists in assigning non-zero probabilities to unseen events. For this purpose we apply the technique proposed by Cimiano, Staab, and
Tane (2003) in which mutually similar terms are clustered with the result that an occurrence of an
attribute with the one term is also counted as an occurrence of that attribute with the other term.
As similarity measures we examine the Cosine, Jaccard, L1 norm, Jensen-Shannon divergence and
Skew Divergence measures analyzed and described by Lee (1999):

 

'



 $   $ 

 $ 
  $ 
  

 $    $  
	


$



	 
   (  	 $  
 $    $ 
  	       $ $ 
 	 	 / / $ $

  (   	       $    $        $  4$
)    (   

        
     

                  
















  (  	


 


       $$  '   (   ,      : $$  '   (   ,




  (          $$       @  ! "     :,
!


 
where  # : $$  #:,	   
   ' %'&
$ ! and  '   (    	


	



)(*$

 !,+



)(,&

 !

In particular, we implemented these measures using the variants relying only on the elements
 common to  and   as described by Lee (1999). Strictly speaking, the Jensen-Shannon as well
as the Skew divergences are dissimilarity functions as they measure the average information loss
when using one distribution instead of the other. In fact we transform them into similarity measures


as 
, where  is a constant and the dissimilarity function in question. We cluster all the
terms which are mutually similar with regard to the similarity measure in question, counting more
attribute/object pairs than are actually found in the text and thus obtaining also non-zero frequencies
for some attribute/object pairs that do not appear literally in the corpus. The overall result is thus
a ’smoothing’ of the relative frequency landscape by assigning some non-zero relative frequencies
to combinations of verbs and objects which were actually not found in the corpus. Here follows the
formal definition of mutual similarity:
Definition 5 (Mutual Similarity)
Two terms  and   are mutually similar iff 

 
 
   ( .



'7

  

 
313

 

'7       (  


and

C IMIANO , H OTHO , & S TAAB

Figure 3: Examples of lattices automatically derived from tourism-related texts without smoothing
(left) and with smoothing (right)

According to this definition, two terms  and   are mutually similar if  is the most similar
term to   with regard to the similarity measure in question and the other way round. Actually, the
definition is equivalent to the reciprocal similarity of Hindle (1990).
Figure 3 (left) shows an example of a lattice which was automatically derived from a set of texts
acquired from http://www.lonelyplanet.com as well as http://www.all-in-all.de, a web page containing information about the history, accommodation facilities as well as activities of Mecklenburg
Vorpommern, a region in northeast Germany. We only extracted verb/object pairs for the terms in
Table 1 and used the conditional probability to weight the significance of the pairs. For excursion,
no dependencies were extracted and therefore it was not considered when computing the lattice.
	 		 . Assuming that
The corpus size was about a million words and the threshold used was 
car and bike are mutually similar, they would be clustered, i.e. car would get the attribute startable
and bike the attribute needable. The result here is thus the lattice in Figure 3 (right), where car and
bike are in the extension of one and the same concept.







5. Evaluation
In order to evaluate our approach we need to assess how good the automatically learned ontologies
reflect a given domain. One possibility would be to compute how many of the superconcept relations
in the automatically learned ontology are correct. This is for example done by Hearst (1992) or
Caraballo (1999). However, due to the fact that our approach, as well as many others (compare
Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994), does not produce appropriate names for
the abstract concepts generated, it seems difficult to assess the validity of a given superconcept
relation. Another possibility is to compute how ’similar’ the automatically learned concept hierarchy
is with respect to a given hierarchy for the domain in question. Here the crucial question is how
to define similarity between concept hierarchies. Though there is a great amount of work in the
AI community on how to compute the similarity between trees (Zhang, Statman, & Shasha, 1992;
Goddard & Swart, 1996), concept lattices (Belohlavek, 2000), conceptual graphs (Maher, 1993;
Myaeng & Lopez-Lopez, 1992) and (plain) graphs (Chartrand, Kubicki, & Schultz, 1998; Zhang,
Wang, & Shasha, 1996), it is not clear how these similarity measures also translate to concept

314

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

hierarchies. An interesting work in these lines is the one presented by Maedche and Staab (2002)
in which ontologies are compared along different levels: semiotic, syntactic and pragmatic. In
particular, the authors present measures to compare the lexical and taxonomic overlap between two
ontologies. Furthermore, they also present an interesting study in which different subjects were
asked to model a tourism ontology. The resulting ontologies are compared in terms of the defined
similarity measures thus yielding the agreement of different subjects on the task of modeling an
ontology.
In order to formally define our evaluation measures, we introduce a core ontology model in line
with the ontological model presented by Stumme et al. (2003):
Definition 6 (Core Ontology)
 consisting of (i) a set of concept identifiers, (ii)
A core ontology is a structure
  #
a designated root element representing the top element of the (iii) partial order  on   
    , called concept hierarchy or taxonomy.
such that "

   ;

% " 

 











/

For the sake of notational simplicity we adopt the following convention: given an ontology
 , the corresponding set of concepts will be denoted by  and the partial order representing the
concept hierarchy by  .
It is important to mention that in the approach presented here, terms are directly identified with
concepts, i.e. we neglect the fact that terms can be polysemous. 6 Now, the Lexical Recall (LR) of
and  is measured as follows:7
two ontologies





 

   	 $  $    $  $

Take for example the concept hierarchies 
 	 and   depicted in Figure 4. In this example, the
 
(
Lexical Recall is
 
	 (    
	 .
In order to compare the taxonomy of two ontologies, we use the Semantic Cotopy (SC) presented by Maedche and Staab (2002). The Semantic Cotopy of a concept is defined as the set of all
its super- and subconcepts:













 	         "   $      

or

     / 

In what follows we illustrate these and other definitions on the basis of several example concept
hierarchies. Take for instance the concept hierarchies in Figure 5. We assume that the left concept
hierarchy has been automatically learned with our FCA approach and that the concept hierarchy
on the right is a handcrafted one. Further, it is important to point out that the left ontology is, in
terms of the arrangement of the leave nodes and abstracting from the labels of the inner nodes, a
perfectly learned concept hierarchy. This should thus be reflected by a maximum similarity between
both ontologies. The Semantic Cotopy of the concept vehicle in the right ontology in Figure 5 is
for example car, bike, two-wheeled vehicle, vehicle, object-to-rent and the Semantic Cotopy of
driveable in the left ontology is bike, car, rideable, driveable, rentable, bookable .
It becomes thus already clear that comparing the cotopies of both concepts will not yield the desired
results, i.e. a maximum similarity between both concepts. Thus we use a modified version SC’ of



/



/

6. In principle, FCA is able to account for polysemy of terms. However, in this paper we neglect this aspect.
7. As the terms to be ordered hierarchically are given there is no need to measure the lexical precision.

315

C IMIANO , H OTHO , & S TAAB

root

activity

object_to_rent

hotel

root
excursion

runable

apartment

offerable

startable

needable

attemptable, ...

bike

car

trip

vehicle

trip

two−wheeled
vehicle

car

bike

hotel



Figure 4: Example for an automatically acquired concept hierarchy
reference concept hierarchy  (right)

joinable

activity

rentable

hotel

driveable

trip

	

(

(left) compared to the

root

bookable

excursion

apartment

rideable

excursion

apartment

object_to_rent

hotel

vehicle

trip

two−wheeled
vehicle

car

apartment

car

bike

bike

Figure 5: Example for a perfectly learned concept hierarchy
ence concept hierarchy  (right)


 
 
(

(left) compared to the refer-

the Semantic Cotopy in which we only consider the concepts common to both concept hierarchies
in the Semantic Cotopy 
(compare Cimiano et al., 2004b, 2004c), i.e.

 



  	         "     $    $        $   /

By using this Common Semantic Cotopy we thus exclude from the comparison concepts such
as runable, offerable, needable, activity, vehicle etc. which are only in one ontology. So, the
of the concepts vehicle and driveable is identical in both ontologies
Common Semantic Cotopy 
in Figure 5, i.e. bike, car thus representing a perfect overlap between both concepts, which
certainly corresponds to our intuitions about the similarity of both concepts. However, let’s now
consider the concept hierarchy in Figure 6. The common cotopy of the concept bike is bike in



/

 



316

/

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

F ORMAL C ONCEPT A NALYSIS

USING

root

activity

excursion

vehicle

trip

two−wheeled
vehicle

root

excursion

trip

hotel

object_to_rent

hotel

apartment

car

apartment

car

bike

bike

Figure 6: Example for a trivial concept hierarchy
hierarchy  (right)
(

     (left) compared to the reference concept

both concept hierarchies. In fact, every leave concept in the left concept hierarchy has a maximum
overlap with the corresponding concept in the right ontology. This is certainly undesirable and in
fact leads to very high baselines when comparing such trivial concept hierarchies with a reference
standard (compare our earlier results Cimiano et al., 2004b, 2004c). Thus, we introduce a further
modification of the Semantic Cotopy by excluding the concept itself from its Common Semantic
Cotopy, i.e:


   	         "     $   $        $   /

This maintains the perfect overlap between vehicle and driveable in the concept hierarchies in
Figure 5, while yielding empty common cotopies for all the leave concepts in the left ontology of
Figure 6.
Now, according to Maedche et al. the Taxonomic Overlap (
) of two ontologies
and 
is computed as follows:

  

	   
  
$
where

  	 $  $

	      
and TO’ and TO” are defined as follows:

 	    
  	    



 	       $$   		 
  	          

   

     
     
$   	 
& $   	 
317

if
if

	   
	   
    
    

 "  
 " * %

4$
4 $

 	    4 $
 	     4$

C IMIANO , H OTHO , & S TAAB



So,
gives the similarity between concepts which are in both ontologies by comparing their
gives the similarity between a concept 
and
respective semantic cotopies. In contrast,
that concept  in  which maximizes the overlap of the respective semantic cotopies, i.e. it makes
an optimistic estimation assuming an overlap that just does not happen to show up at the immediate
lexical surface (compare Maedche & Staab, 2002). The Taxonomic Overlap
   between
the two ontologies is then calculated by averaging over all the taxonomic overlaps of the concepts
in . In our case it doesn’t make sense to calculate the Semantic Cotopy for concepts which are
in both ontologies as they represent leave nodes and thus their common semantic cotopies 
are
empty. Thus, we calculate the Taxonomic Overlap between two ontologies as follows:


	      	   
  

  

    
  &   	(   	      	   
$  &
Finally, as we do not only want to compute the Taxonomic Overlap in one direction, we introduce the precision, recall and an F-Measure calculating the harmonic mean of both:

" 



 

  



 

$      
$      

   	 $   $






   	
   	 
 !   

       4 $
       4$

    







    

          
       


The importance of balancing recall and precision against each other will be clear in the discussion of a few examples below. Let’s consider for example the concept hierarchy   
 
(
in Figure 5. For the five concepts bookable, joinable, rentable, driveable and rideable we find
a corresponding concept in   with a maximum Taxonomic Overlap
and the other way
round for the concepts activity, object-to-rent, vehicle and two-wheeled-vehicle in  , such that





   
    (   
       (   
       (   
	
	  .





















In the concept hierarchy  shown in Figure 7 the precision is still 100% for the same reasons as above, but due to the fact that the rideable concept has been removed there is no corresponding concept for two-wheeled-vehicle. The concept maximizing the taxonomic similarity
in   for two-wheeled-vehicle is driveable+ with
+ + a Taxonomic Overlap of 0.5. The recall is

$











thus   
  
&   and the F-Measure decreases to







  
   .











5





5









In the concept hierarchy of  in Figure 8, an additional
+ + + + concept planable has been introduced,

$





which reduces the precision to  
the recall stays obvi& 	 , while





		 and thus the F-Measure is       .
ously the same at    



 	
)



 







)



It becomes thus clear why it is important to measure the precision and recall of the automatically learned concept hierarchies and balance them against each other by the harmonic mean or
F-Measure. For the automatically
learned concept hierarchy  	
in Figure
+
+
+
+
+!" + " 4+ the precision is
(


&  $

$ $
$



	

	










 &
 (  
	 , the recall 

$#
 and
& &
&



(





	





thus the F-Measure 
#  .
(  





 











	









318



 







L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

root

activity

bookable

joinable

excursion

excursion

rentable

hotel

driveable

trip

vehicle

trip

two−wheeled
vehicle

apartment

bike

object_to_rent

hotel

apartment

car

bike

car

Figure 7: Example for a concept hierarchy with lower recall (   ) compared to the reference concept hierarchy 

root

bookable

joinable

planable

driveable

trip

activity

rentable

hotel

excursion

apartment

vehicle

trip

excursion

rideable

object_to_rent

hotel

two−wheeled
vehicle

car

apartment

car

bike

bike

Figure 8: Example for a concept hierarchy with lower precision (  ) compared to reference concept hierarchy 

     in Figure 6 we get         
As a comparison, for the trivial concept hierarchy
! + +
+
(




&  &  $ (
      

		 (per definition),         
   and 
	 .
(
(
It is important to mention that though in our toy examples the difference with respect to these
measures between the automatically learned concept hierarchy  	 and the trivial concept hier( 
archy      is not so big, when considering real-world concept hierarchies with a much higher
(
number of concepts it is clear that the F-Measures for trivial concept hierarchies will be very low
(see the results in Section 6).
Finally, we also calculate the harmonic mean of the lexical recall and the F-Measure as follows:


  





   

 
  
    
   







  ! 





     
     
319







	







C IMIANO , H OTHO , & S TAAB

No. Concepts
No. Leaves
Avg. Depth
Max. Depth
Max. Children
Avg. Children

Tourism
293
236
3.99
6
21
5.26

Finance
1223
861
4.57
13
33
3.5

Table 2: Ontology statistics
For the automatically learned concept hierarchy 
 	 , we get for example:
( 



 
	  
# 

  
# 
	  
# 

  !  





























6. Results
As already mentioned above, we evaluate our approach on two domains: tourism and finance. The
ontology for the tourism domain is the reference ontology of the comparison study presented by
Maedche and Staab (2002), which was modeled by an experienced ontology engineer. The finance
ontology is basically the one developed within the GETESS project (Staab et al., 1999); it was
designed for the purpose of analyzing German texts on the Web, but also English labels are available
for many of the concepts. Moreover, we manually added the English labels for those concepts whose
German label has an English counterpart with the result that most of the concepts (  95%) finally
yielded also an English label.8 The tourism domain ontology consists of 293 concepts, while the
finance domain ontology is bigger with a total of 1223 concepts 9 . Table 2 summarizes some facts
about the concept hierarchies of the ontologies, such as the total number of concepts, the total
number of leave concepts, the average and maximal length of the paths from a leave to the root node
as well as the average and maximal number of children of a concept (without considering leave
concepts).
As domain-specific text collection for the tourism domain we use texts acquired from the above
mentioned web sites, i.e. from http://www.lonelyplanet.com as well as from http://www.all-in-all.de.
Furthermore, we also used a general corpus, the British National Corpus 10 . Altogether, the corpus
size was over 118 Million tokens. For the finance domain we considered Reuters news from 1987
with over 185 Million tokens11 .
6.1 Comparison
The best F-Measure for the tourism dataset is
 	 	 
corresponding to a precision of


(






 	    
 (
 






(at a threshold of 
  	 
and a recall of
(  

'	











	 		 ),
#
   .








8. There were some concepts which did not have a direct counterpart in the other language.
9. The ontologies can be downloaded at http://www.aifb.uni-karlsruhe.de/WBS/pci/TourismGoldStandard.isa and
http://www.aifb.uni-karlsruhe.de/WBS/pci/FinanceGoldStandard.isa, respectively
10. http://www.natcorp.ox.ac.uk/
11. http://www.daviddlewis.com/resources/testcollections/reuters21578/

320

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS



          ,        
For the finance dataset, the corresponding values are


        	  .
   and
The Lexical Recall obviously also decreases with increasing threshold  such that overall the


F-Measure
also decreases inverse proportionally to  . Overall, the best results in terms of F’




	






are
   for the finance
  #  for the tourism dataset and 	      

(  reason that the results on the finance dataset are slightly lower is probably due to the
dataset. The
more technical nature of the domain (compared to the tourism domain) and also to the fact that the
concept hierarchy to be learned is bigger.
In order to evaluate our FCA-based approach, we compare it with hierarchical agglomerative
clustering and Bi-Section-KMeans. Hierarchical agglomerative clustering (compare Duda, Hart, &
Stork, 2001) is a similarity-based bottom-up clustering technique in which at the beginning every
term forms a cluster of its own. Then the algorithm iterates over the step that merges the two most
similar clusters still available, until one arrives at a universal cluster that contains all the terms.
In our experiments, we use three different strategies to calculate the similarity between clusters:
complete, average and single-linkage. The three strategies may be based on the same similarity
measure between terms, i.e. the cosine measure in our experiments, but they measure the similarity
between two non-trivial clusters in different ways.

   
,
Single linkage defines the similarity between two clusters and as


considering the closest pair between the two clusters. Complete linkage considers the two most
   
dissimilar terms, i.e.
. Finally, average-linkage computes the average simi


. The reader should note that
larity of the terms of the two clusters, i.e.          
 rather order them under a fictive universal

we prohibit the merging of clusters with similarity
0 and
cluster ‘root’. This corresponds exactly to the way FCA creates and orders objects with no attributes
in common. The time complexity of a naive implementation of agglomerative clustering is  ,
 	   for complete linkage
while efficient implementations have a worst-time complexity of 
 for average linkas it requires sorting of the similarity matrix (Day & Edelsbrunner, 1984), 
age if the vectors are length-normalized and the similarity measure is the cosine (see Manning &

Schuetze, 1999) and O( ) for single linkage (compare Sibson, 1973). 12
Bi-Section-KMeans is defined as an outer loop around standard KMeans (Steinbach, Karypis,
& Kumar, 2000). In order to generate  clusters, Bi-Section-KMeans repeatedly applies KMeans.
Bi-Section-KMeans is initiated with the universal cluster containing all terms. Then it loops: It
selects the cluster with the largest variance 13 and it calls KMeans in order to split this cluster into

exactly two subclusters. The loop is repeated " times such that  non-overlapping subclusters are
generated. As similarity measure we also use the cosine measure. The complexity of Bi-SectionKMeans is    . As we want to generate a complete cluster tree with  clusters the complexity

is thus O( ). Furthermore, as Bi-Section-KMeans is a randomized algorithm, we produce ten runs
and average the obtained results.
We compare the different approaches along the lines of the measures described in Section 5.


Figure 9 shows the results in terms of F-Measure over Lexical Recall for both domains and all
the clustering approaches. In particular, it shows 8 data points corresponding to the thresholds
0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7 and 0.9. First of all it seems important to discuss the baselines
for our approach. The baselines for our approach are the trivial concept hierarchies which are
generated when no objects have attributes in common. Such trivial concept hierarchies are generated



























 




	  

 



 


	  


	  










' 

 4

 





12. See also http://www-csli.stanford.edu/ schuetze/completelink.html on this topic.
13. Though we don’t make use of it in our experiments, it is also possible to select the largest cluster for splitting.

321

C IMIANO , H OTHO , & S TAAB

Tourism
FCA
Complete Linkage
Average Linkage
Single Linkage
Bi−Section KMeans

0.5

F−Measure

0.4
0.3
0.2
0.1
0
0.36

0.38

0.4

0.42
0.44
Lexical Recall

0.46

0.48

0.5

Finance
FCA
Complete Linkage
Average Linkage
Single Linkage
Bi−Section KMeans

0.5

F−Measure

0.4
0.3
0.2
0.1
0
0.38

0.4

0.42

0.44
Lexical Recall

0.46

0.48

0.5

Figure 9: Results for the FCA-based approach: F-Measure over Lexical Recall for the tourism and
finance domains

from threshold 0.7 on our datasets and by definition have a precision of 100% and a recall close
to 0. While the baselines for FCA and the agglomerative clustering algorithm are the same, BiSection-KMeans is producing a hierarchy by random binary splits which results in higher F’ values.
These trivial hierarchies represent an absolute baseline in the sense that no algorithm could perform
worse. It can also be seen in Figure 9 that our FCA-based approach performs better than the other
322

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

Tourism

0.7

FCA
Complete Linkage
Average Linkage
Single Linkage
Bi−Section KMeans

0.6

Recall

0.5
0.4
0.3
0.2
0.1
0
0.2

0.3

0.4

0.5

0.6
Precision

0.7

0.8

0.9

1

Finance

0.7

FCA
Complete Linkage
Average Linkage
Single Linkage
Bi−Section KMeans

0.6

Recall

0.5
0.4
0.3
0.2
0.1
0
0.2

0.3

0.4

0.5

0.6
Precision

0.7

0.8

0.9

1

Figure 10: Results for the FCA-based approach: Recall over precision for the tourism and finance
domains

approaches on both domains. As can be observed in Figure 10, showing recall over precision,
the main reason for this is that the FCA-based approach yields a higher recall than the other a
approaches, while maintaining the precision at reasonable levels.
On the tourism domain, the second best result is achieved by the agglomerative algorithm with
the single-linkage strategy, followed by the ones with average-linkage and complete-linkage (in
323

C IMIANO , H OTHO , & S TAAB

FCA
Complete Link
Average Link
Single Link
Bi-Sec. KMeans

P
29.33%
34.67%
35.21%
34.78%
32.85%

Tourism
R
F
65.49% 40.52%
31.98% 33.27%
31.45% 33.23%
28.71% 31.46%
28.71% 30.64%

F’
44.69%
36.85%
36.55%
38.57%
36.42%

P
29.93%
24.56%
29.51%
25.23%
34.41%

Finance
R
F
37.05% 33.11%
25.65% 25.09%
24.65% 26.86%
22.44% 23..75%
21.77% 26.67%

F’
38.85%
33.35%
32.92%
32.15%
32.77%

Table 3: Results of the comparison of different clustering approaches
this order), while the worst results are obtained when using Bi-Section-KMeans (compare Table 3).
On the finance domain, the second best results are achieved by the agglomerative algorithm with
the complete-linkage strategy followed by the one with the average-linkage strategy, Bi-SectionKMeans and the one with the single-linkage strategy (in this order). Overall, it is valid to claim that
FCA outperforms the other clustering algorithms on both datasets. Having a closer look at Table 3,
the reason becomes clear, i.e. FCA has a much higher recall than the other approaches, while the
precision is more or less comparable. This is due to the fact that FCA generates a higher number of
concepts than the other clustering algorithms thus increasing the recall. Interestingly, at the same
time the precision of these concepts remains reasonably high thus also yielding higher F-Measures




and .
An interesting question is thus how big the produced concept hierarchies are. Figure 11 shows
the size of the concept hierarchies in terms of number of concepts over the threshold parameter  for
the different approaches on both domains. It is important to explain why the number of concepts
is different for the different agglomerative algorithms as well as Bi-Section-KMeans as in principle

the size should always be   , where  is the number of objects to be clustered. However, as
objects with no similarity to other objects are added directly under the fictive root element, the size
of the concept hierarchies varies depending on the way the similarities are calculated. In general, the
sizes of the agglomerative and divisive approaches are similar, while at lower thresholds FCA yields
concept hierarchies with much higher number of concepts. From threshold 	  on, the sizes of the
hierarchies produced by all the different approaches are quite similar. Table 4 shows the results
for all approaches using the thresholds 0.3 and 0.5. In particular we can conclude that FCA also
outperforms the other approaches on both domains when producing a similar number of concepts.
In general, we have not determined the statistical significance of the results presented in this paper as FCA, in contrast to Bi-Section-K-Means, is a deterministic algorithm which does not depend
on any random seeding. Our implementation of the agglomerative clustering algorithm is also deterministic given a certain order of the terms to be clustered. Thus, the only possibility to calculate
the significance of our results would be to produce different runs by randomly leaving out parts of
the corpus and calculating a statistical significance over the different runs. We have not pursued this
direction further as the fact that FCA performs better in our setting is clear from the results in Table
3.





324

L EARNING C ONCEPT H IERARCHIES

Threshold
FCA
Complete Link
Single Link
Average Link
Bi-Sec. KMeans

FROM

T EXT C ORPORA

Tourism
0.3
0.5
37.53% 37.74%
36.85% 36.78%
29.84% 35.79%
35.36% 36.55%
31.50% 35.02%

USING

F ORMAL C ONCEPT A NALYSIS

Finance
0.3
0.5
37.59% 34.92%
33.05% 30.37%
29.34% 27.79%
32.92% 31.30%
32.77% 31.38%

Table 4: Comparison of results at thresholds 0.3 and 0.5 in terms of F’

Tourism
Finance
Tourism
Finance
Tourism
Finance
Tourism
Finance
Tourism
Finance

Conditional
PMI
FCA
44.69%
44.51%
38.85%
38.96%
Complete Linkage
36.85%
27.56%
33.35%
22.29%
Average Linkage
36.55%
26.90%
32.92%
23.78%
Single Linkage
38.57%
30.73%
32.15%
25.47%
Bi-Section-KMeans
36.42%
27.32%
32.77%
26.52%

Resnik
43.31%
38.87 %
23.52%
22.96%
23.93%
23.26%
28.63%
23.46%
29.33%
24.00%

Table 5: Comparison of results for different information measures in terms of F’
6.2 Information Measures
As already anticipated in Section 4, the different information measures are also subject of our analysis. Table 5 presents the best results for the different clustering approaches and information measures. It can be concluded from these results that using the PMI or Resnik measures produces worse
results on the tourism dataset, while yielding only slightly better results on the finance dataset for
the FCA-based approach. It is also interesting to observe that compared to the FCA-based approach,
the other clustering approaches are much more sensitive to the information measure used. Overall,
the use of the Conditional information measure seems a reasonable choice.
6.3 Smoothing
We applied our smoothing method described in section 4 to both datasets in order to find out in
how far the clustering of terms improves the results of the FCA-based approach. As information
measure we use in this experiment the conditional probability as it performs reasonably well as
325

C IMIANO , H OTHO , & S TAAB

Tourism

2200

FCA
Complete Linkage
Average Linkage
Single Linkage
Bi−Section−KMeans

2000
1800
1600
1400
1200
1000
800
600
400
200
0

0

0.1

0.2

0.3

0.4

0.5
0.6
threshold t

0.7

0.8

0.9

1

0.9

1

Finance

7000

FCA
Complete Linkage
Average Linkage
Single Linkage
Bi−Section−KMeans

6000
5000
4000
3000
2000
1000
0

0

0.1

0.2

0.3

0.4

0.5
0.6
threshold t

0.7

0.8

Figure 11: Sizes of concept hierarchies for the different approaches on the tourism and finance
domains: number of concepts over threshold 
shown in Section 6.2. In particular we used the following similarity measures: the cosine measure, the Jaccard coefficient, the L1 norm as well as the Jensen-Shannon and the Skew divergences
(compare Lee, 1999). Table 6 shows the impact of this smoothing technique in terms of the number
of object/attribute terms added to the dataset. The Skew Divergence is excluded because it did not
326

L EARNING C ONCEPT H IERARCHIES

Baseline
525912
577607

Tourism
Finance

FROM

Jaccard
531041 (+ 5129)
599691 (+ 22084)

T EXT C ORPORA

USING

Cosine
534709 (+ 8797)
634954 (+ 57347)

F ORMAL C ONCEPT A NALYSIS

L1
530695 (+ 4783)
584821 (+ 7214)

JS
528892 (+ 2980)
583526 (+ 5919)

Table 6: Impact of Smoothing Technique in terms of new object/attribute pairs
Tourism
Finance

Baseline
44.69%
38.85%

Jaccard
39.54%
38.63%

Cosine
41.81%
36.69%

L1
41.59%
38.48%

JS
42.35%
38.66%

Table 7: Results of Smoothing in terms of F-Measure F’
yield any mutually similar terms. It can be observed that smoothing by mutual similarity based
on the cosine measure produces the most previously unseen object/attribute pairs, followed by the
Jaccard, L1 and Jensen-Shannon divergence (in this order). Table 7 shows the results for the different similarity measures. The tables in appendix A list the mutually similar terms for the different
domains and similarity measures. The results show that our smoothing technique actually yields
worse results on both domains and for all similarity measures used.
6.4 Discussion
We have shown that our FCA-based approach is a reasonable alternative to similarity-based cluster

measure defined
ing approaches, even yielding better results on our datasets with regard to the
in Section 5. The main reason for this is that the concept hierarchies produced by FCA yield a
higher recall due to the higher number of concepts, while maintaining the precision relatively high
at the same time. Furthermore, we have shown that the conditional probability performs reasonably
well as information measure compared to other more elaborate measures such as PMI or the one
used by Resnik (1997). Unfortunately, applying a smoothing method based on clustering mutually
similar terms does not improve the quality of the automatically learned concept hierarchies. Table
8 highlights the fact that every approach has its own benefits and drawbacks. The main benefit of
using FCA is on the one hand that on our datasets it performed better than the other algorithms thus
producing better concept hierarchies On the other hand, it does not only generate clusters - formal
concepts to be more specific - but it also provides an intensional description for these clusters thus
contributing to better understanding by the ontology engineer (compare Figure 2 (left)). In contrast,
similarity-based methods do not provide the same level of traceability due to the fact that it is the
numerical value of the similarity between two high-dimensional vectors which drives the clustering
process and which thus remains opaque to the engineer. The agglomerative and divisive approach
are different in this respect as in the agglomerative paradigm, initial merges of small-size clusters
correspond to high degrees of similarity and are thus more understandable, while in the divisive
paradigm the splitting of clusters aims at minimizing the overall cluster variance thus being harder
to trace.
A clear disadvantage of FCA is that the size of the lattice can get exponential in the size of
the context in the worst case thus resulting in an exponential time complexity — compared to
   and   for agglomerative clustering and Bi-Section-KMeans, respectively. The







 

327

C IMIANO , H OTHO , & S TAAB

FCA
Agglomerative Clustering:
Complete Linkage
Average Linkage
Single Linkage
Bi-Section-KMeans

Effectiveness (F’)
Tourism Finance
44.69% 38.85%
36.85%
36.55%
38.57%
36.42%

33.35%
32.92%
32.15%
32.77%

Worst Case
Time Complexity
 

 

      
  
  
 

Traceability
Good

Size of
Hierarchies
Large

Fair

Small

Weak

Small

Table 8: Trade-offs between different taxonomy construction methods
implementation of FCA we have used is the concepts tool by Christian Lindig 14 , which basically
implements Ganter’s Next Closure algorithm (Ganter & Reuter, 1991; Ganter & Wille, 1999) with
the extension of Aloui for computing the covering relation as described by (Godin, Missaoui, &
Alaoui, 1995). Figure 12 shows the number of seconds over the number of attribute/object pairs
it took FCA to compute the lattice of formal concepts compared to the time needed by a naive
 implementation of the agglomerative algorithm with complete linkage. It can be seen that
FCA performs quite efficiently compared to the agglomerative clustering algorithm. This is due to
the fact that the object/attribute matrix is sparsely populated. Such observations have already been
made before. Godin et al. (1995) for example suspect that the lattice size linearly increases with the
number of attributes per object. Lindig (2000) presents empirical results analyzing contexts with a
fill ratio below 0.1 and comes to the conclusion that the lattice size grows quadratically with respect
to the size of the incidence relation . Similar findings are also reported by Carpineto and Romano
(1996).
Figure 13 shows the number of attributes over the terms’ rank, where the rank is a natural
number indicating the position of the word in a list ordered by decreasing term frequencies. It can
be appreciated that the amount of (non-zero) attributes is distributed in a Zipfian way (compare
Zipf, 1932), i.e. a small number of objects have a lot of attributes, while a large number of them
has just a few. In particular, for the tourism domain, the term with most attributes is person with
3077 attributes, while on average a term has approx. 178 attributes. The total number of attributes
considered is 9738, so that we conclude that the object/attribute matrix contains almost 98% zero
values. For the finance domain the term with highest rank is percent with 2870 attributes, the
average being ca. 202 attributes. The total number of attributes is 21542, so that we can state that
in this case more than 99% of the matrix is populated with zero-values and thus is much sparser
than the ones considered by Lindig (2000). These figures explain why FCA performs efficiently in
our experiments. Concluding, though the worst-time complexity is exponential, FCA is much more
efficient than the agglomerative clustering algorithm in our setting.

 !



7. Related Work
In this section, we discuss some work related to the automatic acquisition of taxonomies. The main
paradigms for learning taxonomic relations exploited in the literature are on the one hand clustering
14. http://www.st.cs.uni-sb.de/ lindig/src/concepts.html

328

L EARNING C ONCEPT H IERARCHIES

T EXT C ORPORA

FROM

USING

F ORMAL C ONCEPT A NALYSIS

Tourism
200
FCA
Complete Linkage

180
160

time (sec.)

140
120
100
80
60
40
20
0

0

2000

4000

6000

8000
10000 12000
object/attribute pairs

14000

16000

18000

Finance
6000
FCA
Complete Linkage
5000

time (sec.)

4000

3000

2000

1000

0

0

10000

20000

30000

40000 50000 60000
object/attribute pairs

70000

80000

90000

Figure 12: Comparison of the time complexities for FCA and agglomerative clustering for the
tourism and finance domains
approaches based on the distributional hypothesis (Harris, 1968) and on the other hand approaches
based on matching lexico-syntactic patterns in a corpus which convey a certain relation.
One of the first works on clustering terms was the one by Hindle (1990), in which nouns are
grouped into classes according to the extent to which they appear in similar verb frames. In particular, he uses verbs for which the nouns appear as subjects or objects as contextual attributes. Further,
he also introduces the notion of reciprocal similarity, which is equivalent to our mutual similarity.
Pereira et al. (1993) also present a top-down clustering approach to build an unlabeled hierarchy
of nouns. They present an entropy-based evaluation of their approach, but also show results on a
329

C IMIANO , H OTHO , & S TAAB

3500
tourism
finance
3000

no. features

2500
2000
1500
1000
500
0

0

100

200

300

400

500

600

700

rank

Figure 13: Distribution of Features: number of (non-zero) features over word rank



linguistic decision task: i.e. which of two verbs  and  is more likely to take a given noun  as
object. Grefenstette has also addressed the automatic construction of thesauri (Grefenstette, 1994).
He presents results on different and various domains. Further, he also compares window-based and
syntactic approaches, finding out that the results depend on the frequency of the words in question.
In particular, he shows that for frequent words, the syntactic-based approaches are better, while for
rare words the window-based approaches are preferable (Grefenstette, 1992). The work of Faure
and Nédellec (1998) is also based on the distributional hypothesis; they present an iterative bottomup clustering approach of nouns appearing in similar contexts. In each step, they cluster the two
most similar extents of some argument position of two verbs. Interestingly, this way they not only
yield a concept hierarchy, but also ontologically generalized subcategorization frames for verbs.
Their method is semi-automatic in that it involves users in the validation of the clusters created in
each step. The authors present the results of their system in terms of cluster accuracy in dependency
of percentage of the corpus used. Caraballo (1999) also uses clustering methods to derive an unlabeled hierarchy of nouns by using data about conjunctions of nouns and appositions collected from
the Wall Street Journal corpus. Interestingly, in a second step she also labels the abstract concepts
of the hierarchy by considering the Hearst patterns (see below) in which the children of the concept
in question appear as hyponyms. The most frequent hypernym is then chosen in order to label the
concept. At a further step she also compresses the produced ontological tree by eliminating internal
nodes without a label. The final ontological tree is then evaluated by presenting a random choice
of clusters and the corresponding hypernym to three human judges for validation. Bisson et al.
(2000) present an interesting framework and a corresponding workbench - Mo’K - allowing users
to design conceptual clustering methods to assist them in an ontology building task. In particular
they use bottom-up clustering and compare different similarity measures as well as different pruning
parameters.
In earlier work we used collocation statistics to learn relations between terms using a modification of the association rules extraction algorithm (Maedche & Staab, 2000). However, these
relations were not inherently taxonomic such that the work described in this paper can not be di330

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

rectly compared to it. Maedche, Pekar, and Staab (2002) examined different supervised techniques
based on collocations to find the appropriate hypernym for an unknown term, reaching an accuracy
of around 15% using a combination of a tree ascending algorithm and  -Nearest-Neighbors as well
as the Skew Divergence as similarity measure. These results are neither comparable to the task
at hand. Recently, Reinberger and Spyns (2005) have presented an application of clustering techniques in the biomedical domain. They evaluate their clusters by directly comparing to the UMLS
thesaurus. Their results are very low (3-17% precision depending on the corpus and clustering technique) and comparable to the results we obtained when comparing our clusters directly with our
gold standards and which are not reported in this paper though.
Furthermore, there is quite a lot of work related to the use of linguistic patterns to discover
certain ontological relations from text. Hearst’s seminal approach aimed at discovering taxonomic
relations from electronic dictionaries (Hearst, 1992). The precision of the isa-relations learned
 %
is #
	 # (57.55%) when measured against WordNet as gold standard. Hearst’s idea has been

reapplied by different researchers with either slight variations in the patterns used (Iwanska et al.,
2000), in very specific domains (Ahmad et al., 2003), to acquire knowledge for anaphora resolution
(Poesio, Ishikawa, im Walde, & Viera, 2002), or to discover other kinds of semantic relations such
as part-of relations (Charniak & Berland, 1999) or causation relations (Girju & Moldovan, 2002).
The approaches of Hearst and others are characterized by a (relatively) high precision in the
sense that the quality of the learned relations is very high. However, these approaches suffer from
a very low recall which is due to the fact that the patterns are very rare. As a possible solution to
this problem, in the approach of Cimiano, Pivk, Schmidt-Thieme, and Staab (2004, 2005) Hearst
patterns matched in a corpus and on the Web as well as explicit information derived from other
resources and heuristics are combined yielding better results compared to considering only one
source of evidence on the task of learning superconcept relations. In general, to overcome such data
sparseness problems, researchers are more and more resorting to the WWW as for example Markert,
Modjeska, and Nissim (2003). In their approach, Hearst patterns are searched for on the WWW by
using the Google API in order to acquire background knowledge for anaphora resolution. Agirre,
Ansa, Hovy, and Martinez (2000), download related texts from the Web to enrich a given ontology.
Cimiano, Handschuh, and Staab (2004a) as well as Cimiano, Ladwig, and Staab (2005) have used
the Google API to match Hearst-like patterns on the Web in order to (i) find the best concept for
an unknown instance as well as (ii) the appropriate superconcept for a certain concept in a given
ontology (Cimiano & Staab, 2004).
Velardi, Fabriani, and Missikoff (2001) present the OntoLearn system which discovers i) the
domain concepts relevant for a certain domain, i.e. the relevant terminology, ii) named entities, iii)
’vertical’ (is-a or taxonomic) relations as well as iv) certain relations between concepts based on
specific syntactic relations. In their approach a ’vertical’ relation is established between a term 
and a term  , i.e. is-a() , ), if  can be gained out of ) by stripping of the latter’s prenominal
modifiers such as adjectives or modifying nouns. Thus, a ’vertical’ relation is for example established between the term international credit card and the term credit card, i.e. is-a(international
credit card,credit card). In a further paper (Velardi, Navigli, Cuchiarelli, & Neri, 2005), the main
focus is on the task of word sense disambiguation, i.e. of finding the correct sense of a word with
respect to a general ontology or lexical database. In particular, they present a novel algorithm called
SSI relying on the structure of the general ontology for this purpose. Furthermore, they include
an explanation component for users consisting in a gloss generation component which generates
definitions for terms which were found relevant in a certain domain.
331

C IMIANO , H OTHO , & S TAAB

Sanderson and Croft (1999) describe an interesting approach to automatically derive a hierarchy
by considering the document a certain term appears in as context. In particular, they present a
document-based definition of subsumption according to which a certain term  is more special than
a term   if   also appears in all the documents in which  appears.
Formal Concept Analysis can be applied for many tasks within Natural Language Processing.
Priss (2004) for example, mentions several possible applications of FCA in analyzing linguistic
structures, lexical semantics and lexical tuning. Sporleder (2002) and Petersen (2002) apply FCA
to yield more concise lexical inheritance hierarchies with regard to morphological features such
as numerus, gender etc. Basili, Pazienza, and Vindigni (1997) apply FCA to the task of learning
subcategorization frames from corpora. However, to our knowledge it has not been applied before
to the acquisition of domain concept hierarchies such as in the approach presented in this paper.

8. Conclusion
We have presented a novel approach to automatically acquire concept hierarchies from domainspecific texts. In addition, we have compared our approach with a hierarchical agglomerative clustering algorithm as well as with Bi-Section-KMeans and found that our approach produces better
results on the two datasets considered. We have further examined different information measures
to weight the significance of an attribute/object pair and concluded that the conditional probability
works well compared to other more elaborate information measures. We have also analyzed the
impact of a smoothing technique in order to cope with data sparseness and found that it doesn’t
improve the results of the FCA-based approach. Further, we have highlighted advantages and disadvantages of the three approaches.
Though our approach is fully automatic, it is important to mention that we do not believe in
fully automatic ontology construction without any user involvement. In this sense, in the future we
will explore how users can be involved in the process by presenting him/her ontological relations
for validation in such way that the necessary user feedback is kept at a minimum. On the other
hand, before involving users in a semi-automatic way it is necessary to clarify how good a certain
approach works per se. The research presented in this paper has had this aim. Furthermore, we have
also proposed a systematic way of evaluating ontologies by comparing them to a certain humanmodeled ontology. In this sense our aim has also been to establish a baseline for further research.

Acknowledgments
We would like to thank all our colleagues for feedback and comments, in particular Gerd Stumme
for clarifying our FCA-related questions. We would also like to thank Johanna Völker for comments on a first version as well as for proof-reading the final version of the paper. All errors are
of course our own. We would also like to acknowledge the reviewers of the Journal of Artificial
Intelligence Research as well as the ones of the earlier workshops (ATEM04, FGML04) and conferences (LREC04, ECAI04) on which this work was presented for valuable comments. Philipp
Cimiano is currently supported by the Dot.Kom project (http://www.dot-kom.org), sponsored by
the EC as part of the framework V, (grant IST-2001-34038) as well as by the SmartWeb project
(http://smartweb.dfki.de), funded by the German Ministry of Education and Research.

332

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

Appendix A. Mutually Similar Terms
Jaccard
(art exhibition,thing)
(autumn,spring)
(balcony,menu)
(ballroom,theatre)
(banquet,ship)
(bar,pub)
(basilica,hair dryer)
(beach,swimming pool)
(billiard,sauna)
(bus,car)
(caravan,tree)
(casino,date)
(cinema,fitness studio)
(city,town)
(conference,seminar)
(conference room,volleyball field)
(cure,washing machine)
(day tour,place)
(distance,radio)
(exhibition,price list)
(ferry,telephone)
(gallery,shop)
(golf course,promenade)
(holiday,service)
(journey,terrace)
(kiosk,time interval)
(law,presentation)
(lounge,park)
(motel,port)
(nature reserve,parking lot)
(night,tourist)
(region,situation)

Cosine
(agreement,contract)
(animal,plant)
(art exhibition,washing machine)
(basilica,hair dryer)
(boat,ship)
(cabaret,email)
(cheque,pension)
(city,town)
(conference room,volleyball field)
(golf course,promenade)
(group,party)
(inn,yacht)
(journey,meal)
(kiosk,tennis court)
(law,view)
(library,museum)
(money,thing)
(motel,port)
(pilgrimage,whirlpool)
(sauna,swimming)

L1 norm
(day,time)
(golf course,promenade)
(group,person)

Jensen-Shannon divergence
(group,person)

Table 9: Mutually Similar Terms for the tourism domain

333

C IMIANO , H OTHO , & S TAAB

Jaccard
(action,average)
(activity,downturn)
(addition,liquidity)
(afternoon,key)
(agency,purchase)
(agreement,push)
(alliance,project team)
(allocation,success)
(analysis,negotiation)
(animal,basis)
(anomaly,regression)
(archives,futures)
(area,profitability)
(argument,dismantling)
(arrangement,capital market)
(arranger,update)
(assembly,price decline)
(assurance,telephone number)
(automobile,oil)
(backer,trade partner)
(balance sheet,person)
(balancing,countenance)
(behaviour,business partnership)
(bike,moment)
(billing,grade)
(board,spectrum)
(board chairman,statement)
(bonus,nationality)
(bonus share,cassette)
(branch office,size)
(broker,competition)
(budget,regulation)
(builder,devices)
(building,vehicle)
(business volume,outlook)
(business year,quota)
(capital,material costs)
(capital increase,stock split)
(capital stock,profit distribution)
(caravan,seminar)
(cent,point)
(chance,hope)
(change,subsidiary)
(charge,suspicion)
(chip,woman)
(circle,direction)
(clock,ratio)
(code,insurance company)
(comment,foundation)
(commission,expansion)
(communication,radio)
(community,radius)
(company profile,intangible)
(compensation,participation)
(complaint,petition)
(computer,cooperation)
(conference,height)
(confidentiality,dollar)
(consultant,survey)
(contact,hint)
(contract,copyright)
(control,data center)
(conversation,output)
(copper,replacement)
(corporation,liabilities)
(cost,equity capital)
(course,step)
(court,district court)
(credit,disbursement)
(credit agreement,overview)
(currency,faith)
(curve,graph)
(decision,maximum)
(deficit,negative)
(diagram,support)
(difference,elimination)

Cosine
(access,advantage)
(acquisition,merger)
(action,measure)
(administration costs,treasury stock)
(advice,assurance)
(allocation,length)
(amount,total)
(analysis,component)
(area,region)
(arrangement,regime)
(assembly,chamber)
(assessment,receipt)
(backer,gamble)
(balancing,matrix)
(bank,company)
(barometer,market price)
(bid,offer)
(bond,stock)
(bonus share,cassette)
(boom,turnaround)
(bull market,tool)
(business deal,graph)
(buy,stop)
(capital stock,profit distribution)
(caravan,software company)
(cent,point)
(change,increase)
(commission,committee)
(company profile,intangible)
(complaint,request)
(controller,designer)
(copper,share index)
(copy,push)
(credit,loan)
(credit agreement,credit line)
(currency,dollar)
(decision,plan)
(detail,test)
(diagram,support)
(dimension,surcharge)
(discussion,negotiation)
(diversification,milestone)
(do,email)
(document,letter)
(effect,impact)
(equity fund,origin)
(evaluation,examination)
(example,hint)
(first,meter)
(forecast,stock market activity)
(function,profile)
(gesture,input)
(guarantee,solution)
(half,quarter)
(increment,rearrangement)
(information,trading company)
(insurance,percentage)
(interest rate,tariff)
(man,woman)
(maximum,supervision)
(meeting,talk)
(merchant,perspective)
(month,week)
(press conference,seminar)
(price,rate)
(productivity,traffic)
(profit,volume)
(share price,stock market)
(stock broker,theory)

L1 norm
(archives,futures)
(assurance,telephone number)
(balancing,countenance)
(cent,point)
(creation,experience)
(government,person)
(loss,profit)
(month,year)

Jensen-Shannon divergence
(cent,point)
(government,person)
(month,year)

Table 10: Mutually Similar Terms for the finance domain

334

L EARNING C ONCEPT H IERARCHIES

Jaccard
(disability insurance,pension)
(discrimination,union)
(diversification,request)
(do,email)
(effect,help)
(employer,insurance)
(energy,test)
(equity fund,origin)
(evening,purpose)
(event,manager)
(examination,registration)
(example,source)
(exchange,volume)
(exchange risk,interest rate)
(experience,questionnaire)
(expertise,period)
(faculty,sales contract)
(fair,product)
(flop,type)
(forecast,stock market activity)
(fusion,profit zone)
(gamble,thing)
(good,service)
(government bond,life insurance)
(happiness,question)
(hold,shareholder)
(hour,pay)
(house,model)
(idea,solution)
(impact,matter)
(improvement,situation)
(index,wholesale)
(information,trading company)
(initiation,middle)
(input,traffic)
(institute,organization)
(investment,productivity)
(knowledge,tradition)
(label,title)
(letter,reception)
(level,video)
(license,reward)
(loan,project)
(location,process)
(loss,profit)
(man,trainee)
(margin,software company)
(market,warranty)
(market access,name)
(matrix,newspaper)
(meeting,oscillation)
(meter,share)
(method,technology)
(milestone,state)
(month,year)
(mouse,option)
(multiplication,transfer)
(noon,press conference)
(occasion,talk)
(opinion,rivalry)
(personnel,resource)
(picture,surcharge)
(plane,tool)
(police,punishment)
(profession,writer)
(property,qualification)
(provision,revenue)
(requirement,rule)
(risk,trust)
(sales revenue,validity)
(savings bank,time)
(segment,series)
(show,team)
(speech,winter)
(stock broker,theory)
(supplier,train)
(tariff,treasury stock)
(weekend,wisdom)

FROM

T EXT C ORPORA

Cosine

L1 norm

USING

F ORMAL C ONCEPT A NALYSIS

Jensen-Shannon divergence

Table 11: Mutually Similar Terms for the finance domain (Cont’d)

335

C IMIANO , H OTHO , & S TAAB

References
Agirre, E., Ansa, O., Hovy, E., & Martinez, D. (2000). Enriching very large ontologies using the
WWW. In Proceedings of the ECAI Ontology Learning Workshop.
Ahmad, K., Tariq, M., Vrusias, B., & Handy, C. (2003). Corpus-based thesaurus construction for
image retrieval in specialist domains. In Proceedings of the 25th European Conference on
Advances in Information Retrieval (ECIR), pp. 502–510.
Basili, R., Pazienza, M., & Vindigni, M. (1997). Corpus-driven unsupervised learning of verb
subcategorization frames. In Proceedings of the 5th Congress of the Italian Association for
Artificial Intelligence (AI*IA97).
Belohlavek, R. (2000). Similarity relations in concept lattices. Journal of Logic and Computation,
10(6), 823–845.
Bisson, G., Nédellec, C., & Canamero, L. (2000). Designing clustering methods for ontology building - The Mo’K workbench. In Proceedings of the ECAI Ontology Learning Workshop, pp.
13–19.
Bloehdorn, S., & Hotho, A. (2004). Text classification by boosting weak learners based on terms
and concepts. In Proceedings of the 4th IEEE International Conference on Data Mining
(ICDM), pp. 331–334.
Brewster, C., Ciravegna, F., & Wilks, Y. (2003). Background and foreground knowledge in dynamic
ontology construction. In Proceedings of the SIGIR Semantic Web Workshop.
Caraballo, S. (1999). Automatic construction of a hypernym-labeled noun hierarchy from text. In
Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics
(ACL), pp. 120–126.
Carpineto, C., & Romano, G. (1996). A lattice conceptual clustering system and its application to
browsing retrieval. Machine Learning, 24, 95–122.
Charniak, E., & Berland, M. (1999). Finding parts in very large corpora. In Proceedings of the 37th
Annual Meeting of the Association for Computational Linguistics (ACL), pp. 57–64.
Chartrand, G., Kubicki, G., & Schultz, M. (1998). Graph similarity and distance in graphs. Aequationes Mathematicae, 55(1-2), 129–145.
Cimiano, P. (2003). Ontology-driven discourse analysis in GenIE. In Proceedings of the 8th International Conference on Applications of Natural Language to Information Systems, pp. 77–90.
Cimiano, P., Handschuh, S., & Staab, S. (2004a). Towards the self-annotating web. In Proceedings
of the 13th World Wide Web Conference, pp. 462–471.
Cimiano, P., Hotho, A., & Staab, S. (2004b). Clustering ontologies from text. In Proceedings of
the 4th International Conference on Language Resources and Evaluation (LREC), pp. 1721–
1724.
Cimiano, P., Hotho, A., & Staab, S. (2004c). Comparing conceptual, divisive and agglomerative
clustering for learning taxonomies from text. In Proceedings of the European Conference on
Artificial Intelligence (ECAI), pp. 435–439.
Cimiano, P., Ladwig, G., & Staab, S. (2005). Gimme’ the context: Context-driven automatic semantic annotation with C-PANKOW. In Proceedings of the 14th World Wide Web Conference.
336

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

Cimiano, P., Pivk, A., Schmidt-Thieme, L., & Staab, S. (2004). Learning taxonomic relations from
heterogeneous sources. In Proceedings of the ECAI 2004 Ontology Learning and Population
Workshop.
Cimiano, P., Pivk, A., Schmidt-Thieme, L., & Staab, S. (2005). Learning taxonomic relations from
heterogeneous evidence. In Buitelaar, P., Cimiano, P., & Magnini, B. (Eds.), Ontology Learning from Text: Methods, Applications and Evaluation. IOS Press. to appear.
Cimiano, P., S.Staab, & Tane, J. (2003). Automatic acquisition of taxonomies from text: FCA
meets NLP. In Proceedings of the PKDD/ECML’03 International Workshop on Adaptive
Text Extraction and Mining (ATEM), pp. 10–17.
Cimiano, P., & Staab, S. (2004). Learning by googling. SIGKDD Explorations, 6(2), 24–34.
Cimiano, P., Staab, S., & Tane, J. (2003). Deriving concept hierarchies from text by smooth formal
concept analysis. In Proceedings of the GI Workshop ”Lehren Lernen - Wissen - Adaptivit ät”
(LLWA), pp. 72–79.
Day, W., & Edelsbrunner, H. (1984). Efficient algorithms for agglomerative hierarchical clustering
methods. Journal of Classification, 1, 7–24.
Duda, R. O., Hart, P. E., & Stork, D. G. (2001). Pattern Classification. John Wiley & Sons, Inc.
Faure, D., & Nédellec, C. (1998). A corpus-based conceptual clustering method for verb frames
and ontology. In Velardi, P. (Ed.), Proceedings of the LREC Workshop on Adapting lexical
and corpus resources to sublanguages and applications, pp. 5–12.
Ganter, B., & Reuter, K. (1991). Finding all closed sets: A general approach. Order, 8, 283–290.
Ganter, B., & Wille, R. (1999). Formal Concept Analysis – Mathematical Foundations. Springer
Verlag.
Girju, R., & Moldovan, M. (2002). Text mining for causal relations. In Proceedings of the FLAIRS
Conference, pp. 360–364.
Goddard, W., & Swart, H. (1996). Distance between graphs under edge operations. Discrete Mathematics, 161, 121–132.
Godin, R., Missaoui, R., & Alaoui, H. (1995). Incremental concept formation algorithms based on
galois (concept) lattices. Computational Intelligence, 11(2), 246–267.
Grefenstette, G. (1994). Explorations in Automatic Thesaurus Construction. Kluwer.
Grefenstette, G. (1992). Evaluation techniques for automatic semantic extraction: Comparing syntactic and window-based approaches. In Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text.
Harris, Z. (1968). Mathematical Structures of Language. Wiley.
Hearst, M. (1992). Automatic acquisition of hyponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational Linguistics (COLING), pp. 539–545.
Hindle, D. (1990). Noun classification from predicate-argument structures. In Proceedings of the
Annual Meeting of the Association for Computational Linguistics (ACL), pp. 268–275.
Hotho, A., Staab, S., & Stumme, G. (2003). Ontologies improve text document clustering. In
Prodeedings of the IEEE International Conference on Data Mining (ICDM), pp. 541–544.
337

C IMIANO , H OTHO , & S TAAB

Iwanska, L., Mata, N., & Kruger, K. (2000). Fully automatic acquisition of taxonomic knowledge
from large corpora of texts. In Iwanksa, L., & Shapiro, S. (Eds.), Natural Language Processing and Knowledge Processing, pp. 335–345. MIT/AAAI Press.
Lee, L. (1999). Measures of distributional similarity. In 37th Annual Meeting of the Association for
Computational Linguistics (ACL), pp. 25–32.
Lindig, C. (2000). Fast concept analysis. In Stumme, G. (Ed.), Proceedings of the International
Conference on Conceptual Structures (ICCS). Shaker Verlag, Aachen, Germany.
Maedche, A., Pekar, V., & Staab, S. (2002). Ontology learning part one - on discovering taxonomic
relations from the web. In Proceedings of the Web Intelligence conference, pp. 301–322.
Springer Verlag.
Maedche, A., & Staab, S. (2002). Measuring similarity between ontologies. In Proceedings of the
European Conference on Knowledge Engineering and Knowledge Management (EKAW), pp.
251–263. Springer Verlag.
Maedche, A., & Staab, S. (2000). Discovering conceptual relations from text. In Horn, W. (Ed.),
Proceedings of the 14th European Conference on Artificial Intelligence (ECAI).
Maher, P. (1993). A similarity measure for conceptual graphs. Intelligent Systems, 8, 819–837.
Manning, C., & Schuetze, H. (1999). Foundations of Statistical Language Processing. MIT Press.
Markert, K., Modjeska, N., & Nissim, M. (2003). Using the web for nominal anaphora resolution.
In EACL Workshop on the Computational Treatment of Anaphora.
Myaeng, S., & Lopez-Lopez, A. (1992). Conceptual graph matching: A flexible algorithm and
experiments. Experimental and Theoretical Artificial Intelligence, 4, 107–126.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering of english words. In Proceedings
of the 31st Annual Meeting of the Association for Computational Linguistics (ACL), pp. 183–
190.
Petersen, W. (2002). A set-theoretical approach for the induction of inheritance hierarchies. Electronic Notes in Theoretical Computer Science, 51.
Poesio, M., Ishikawa, T., im Walde, S. S., & Viera, R. (2002). Acquiring lexical knowledge for
anaphora resolution. In Proceedings of the 3rd Conference on Language Resources and Evaluation (LREC).
Priss, U. (2004). Linguistic applications of formal concept analysis. In Stumme, G., & Wille, R.
(Eds.), Formal Concept Analysis - State of the Art. Springer.
Reinberger, M.-L., & Spyns, P. (2005). Unsupervised text mining for the learning of dogma-inspired
ontologies. In Buitelaar, P., Cimiano, P., & Magnini, B. (Eds.), Ontology Learning from Text:
Methods, Evaluation and Applications. IOS Press. to appear.
Resnik, P. (1997). Selectional preference and sense disambiguation. In Proceedings of the ACL
SIGLEX Workshop on Tagging Text with Lexical Semantics: Why, What, and How?
Sanderson, M., & Croft, B. (1999). Deriving concept hierarchies from text. In Research and Development in Information Retrieval, pp. 206–213.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of the
International Conference on New Methods in Language Processing.
338

L EARNING C ONCEPT H IERARCHIES

FROM

T EXT C ORPORA

USING

F ORMAL C ONCEPT A NALYSIS

Schmid, H. (2000). Lopar: Design and implementation. In Arbeitspapiere des Sonderforschungsbereiches 340, No. 149.
Sibson, R. (1973). SLINK: an optimally efficient algorithm for the single-link cluster method. The
Computer Journal, 16(1), 30–34.
Sporleder, C. (2002). A galois lattice based approach to lexical inheritance hierarchy learning. In
Proceedings of the ECAI Workshop on Machine Learning and Natural Language Processing
for Ontology Engineering (OLT 2002).
Staab, S., Braun, C., Bruder, I., Düsterhöft, A., Heuer, A., Klettke, M., Neumann, G., Prager, B.,
Pretzel, J., Schnurr, H.-P., Studer, R., Uszkoreit, H., & Wrenger, B. (1999). Getess - searching the web exploiting german texts. In Proceedings of the 3rd Workshop on Cooperative
Information Agents, pp. 113–124. Springer Verlag.
Steinbach, M., Karypis, G., & Kumar, V. (2000). A comparison of document clustering techniques.
In KDD Workshop on Text Mining.
Stumme, G., Ehrig, M., Handschuh, S., Hotho, A., Maedche, A., Motik, B., Oberle, D., Schmitz, C.,
Staab, S., Stojanovic, L., Stojanovic, N., Studer, R., Sure, Y., Volz, R., & Zacharias, V. (2003).
The karlsruhe view on ontologies. Tech. rep., University of Karlsruhe, Institute AIFB.
Velardi, P., Fabriani, P., & Missikoff, M. (2001). Using text processing techniques to automatically enrich a domain ontology. In Proceedings of the International Conference on Formal
Ontology in Information Systems (FOIS), pp. 270–284.
Velardi, P., Navigli, R., Cuchiarelli, A., & Neri, F. (2005). Evaluation of ontolearn, a methodology
for automatic population of domain ontologies. In Buitelaar, P., Cimiano, P., & Magnini, B.
(Eds.), Ontology Learning from Text: Methods, Evaluation and Applications. IOS Press. to
appear.
Voorhees, E. (1994). Query expansion using lexical-semantic relations. In Proceedings of the 17th
Annual International ACM SIGIR Conference on Research and Development in Information
Retrieval, pp. 61–69.
Zhang, K., Statman, R., & Shasha, D. (1992). On the editing distance between unordered labeled
trees. Information Processing Letters, 42(3), 133–139.
Zhang, K., Wang, J., & Shasha, D. (1996). On the editing distance between undirected acyclic
graphs. International Journal of Foundations of Computer Science, 7(1), 43–57.
Zipf, G. (1932). Selective Studies and the Principle of Relative Frequency in Language. Cambridge.

339

Journal of Artificial Intelligence Research 24 (2005) 81-108

Submitted 12/04; published 07/05

Risk-Sensitive Reinforcement Learning Applied to Control
under Constraints
Peter Geibel

pgeibel@uos.de

Institute of Cognitive Science, AI Group
University of Osnabrück, Germany

Fritz Wysotzki

wysotzki@cs.tu-berlin.de

Faculty of Electrical Engineering and Computer Science, AI Group
TU Berlin, Germany

Abstract
In this paper, we consider Markov Decision Processes (MDPs) with error states. Error
states are those states entering which is undesirable or dangerous. We define the risk
with respect to a policy as the probability of entering such a state when the policy is
pursued. We consider the problem of finding good policies whose risk is smaller than
some user-specified threshold, and formalize it as a constrained MDP with two criteria.
The first criterion corresponds to the value function originally given. We will show that
the risk can be formulated as a second criterion function based on a cumulative return,
whose definition is independent of the original value function. We present a model free,
heuristic reinforcement learning algorithm that aims at finding good deterministic policies.
It is based on weighting the original value function and the risk. The weight parameter is
adapted in order to find a feasible solution for the constrained problem that has a good
performance with respect to the value function. The algorithm was successfully applied
to the control of a feed tank with stochastic inflows that lies upstream of a distillation
column. This control task was originally formulated as an optimal control problem with
chance constraints, and it was solved under certain assumptions on the model to obtain an
optimal solution. The power of our learning algorithm is that it can be used even when
some of these restrictive assumptions are relaxed.

1. Introduction
Reinforcement Learning, as a research area, provides a range of techniques that are applicable to difficult nonlinear or stochastic control problems (see e.g. Sutton & Barto, 1998;
Bertsekas & Tsitsiklis, 1996). In reinforcement learning (RL) an agent is considered that
learns to control a process. The agent is able to perceive the state of the process, and
it acts in order to maximize the cumulative return that is based on a real valued reward
signal. Often, experiences with the process are used to improve the agent’s policy instead
of a previously given analytical model.
The notion of risk in RL is related to the fact, that even an optimal policy may perform
poorly in some cases due to the stochastic nature of the problem. Most risk-sensitive RL
approaches are concerned with the variance of the return, or with its worst outcomes,
(e.g. Coraluppi & Marcus, 1999; Heger, 1994; Neuneier & Mihatsch, 1999), see also the
discussion in section 3. We take the alternative view of risk defined by Geibel (2001) that
is not concerned with the variability of the return, but with the occurrence of errors or
c
2005
AI Access Foundation. All rights reserved.

Geibel & Wysotzki

undesirable states in the underlying Markov Decision Process (MDP). This means that we
address a different class of problems compared to approaches referring to the variability of
the return.
In this paper, we consider constrained MDPs with two criteria – the usual value function and the risk as a second value function. The value is to be optimized while the risk
must remain below some specified threshold. We describe a heuristic algorithm based on a
weighted formulation that finds a feasible policy for the original constrained problem.
In order to offer some insight in the behavior of the algorithm, we investigate the application of the algorithm to a simple grid world problem with a discounted criterion function.
We then apply the algorithm to a stochastic optimal control problem with continuous states,
where the set of feasible solutions is restricted by a constraint that is required to hold with
a certain probability, thus demonstrating the practical applicability of our approach. We
consider the control of a feed tank that lies upstream of a distillation column with respect
to two objectives: (1) the outflow of the tank is required to stay close to a specified value in
order to ensure the optimal operation of the distillation column, and (2) the tank level and
substance concentrations are required to remain within specified intervals, with a certain
admissible chance of constraint violation.
Li, Wendt, Arellano-Garcia, and Wozny (2002) formulate the problem as a quadratic
program with chance constraints1 (e.g. Kall & Wallace, 1994), which is relaxed to a nonlinear
program for the case of Gaussian distributions for the random input variables and systems
whose dynamics is given by linear equations. The nonlinear program is solved through
sequential quadratic programming.
Note that the approach of Li et al. involves the simulation based estimation of the
gradients of the chance constraints (Li et al., 2002, p. 1201). Like Q-learning (Watkins,
1989; Watkins & Dayan, 1992; Sutton & Barto, 1998), our learning algorithm is based on
simulating episodes and estimating value and risk of states, which for the tank control task
correspond to a measure for the deviation from the optimal outflow and the probability of
constraint violation, respectively.
In contrast to the approach of Li et al. (2002), our RL algorithm is applicable to systems
with continuous state spaces, whose system dynamics is governed by nonlinear equations
and involve randomization or noise with arbitrary distributions for the random variables,
for it makes no prior assumptions of either aspect. This it not a special property of our
learning algorithm, and also holds true for e.g. Q-learning and other RL algorithms. The
convergence of Q-learning combined with function approximation techniques necessary for
continuous state spaces cannot be guaranteed in general (e.g. Sutton & Barto, 1998). The
same holds true for our algorithm. Nevertheless, RL algorithms were successfully applied
to many difficult problems with continuous state spaces and nonlinear dynamics (see e.g.
Sutton & Barto, 1998; Crites & Barto, 1998; Smart & Kaelbling, 2002; Stephan, Debes,
Gross, Wintrich, & Wintrich, 2001).
1. A constraint can be seen as a relation on the domains of variables restricting their possible values. If
the variables in a constraint C = C(x1 , . . . , xn ) are random, the constraint will hold with a certain
probability. Chance constrained programming is a particular approach to stochastic programming that
considers constrained optimization problems containing random variables for which so-called chance
constraints of the form P(C) ≥ p with p ∈ [0, 1] are formulated.

82

Risk-Sensitive Reinforcement Learning

This article is organized as follows. In section 2, the RL framework is described. Section 3 reviews related work on risk-sensitive approaches. Section 4 describes our approach
to risk-sensitive RL. In section 5, we elucidate a heuristic learning algorithm for solving the
constrained problem using a weighted formulation. In section 6, we describe its application
to a grid world problem. The tank control task is described in section 7. In section 8,
experiments with the feed tank control are described. Section 9 concludes with a short
summary and an outlook.

2. The RL Framework
In RL one considers an agent that interacts with a process that is to be controlled. At
each discrete time-step, the agent observes the state x and takes an action u that in general
depends on x. The action of the agent causes the environment to change its state to x′
according to the probability px,u (x′ ). Until section 7, we will consider the set of states, X,
to be a finite set.
The action set of the agent is assumed to be finite, and it is allowed to depend on the
current state. In each state x, the agent uses an action from the set U (x) of possible actions.
After taking action u ∈ U (x), the agent receives a real valued reinforcement signal rx,u (x′ )
that depends on the action taken and the successor state x′ . In the case of a random reward
signal, rx,u (x′ ) corresponds to its expected value. The Markov property of an MDP requires
that the probability distribution on the successor states and the one on the rewards depend
on the current state and action only. The distributions do not change when additional
information on past states, actions and rewards is considered, i.e. they are independent of
the path leading to the current state.
The aim of the agent is to find a policy π for selecting actions that maximizes the
cumulative reward, called the return. The return is defined as
R=

∞
X

γ t rt ,

(1)

t=0

where the random variable rt denotes the reward occurring in the t-th time step when the
agent uses policy π. Let x0 , x1 , x2 , . . . denote the corresponding probabilistic sequence of
states, and ui the sequence of actions chosen according to policy π.
The constant γ ∈ [0, 1] is a discount factor that allows to control the influence of future
rewards. The expectation of the return,
h

i

V π (x) = E R | x0 = x ,

(2)

is defined as the value of x with respect to π. It is well-known that there exist stationary
∗
deterministic policies π ∗ for which V π (x) is optimal (maximal) for every state x. A stationary deterministic policy is a function that maps states to actions and is particularly defined
as being independent of time and Markovian (independent of history). In this work, we will
use the term of maximum-value policies instead of optimal policies just to distinguish them
from minimum-risk policies that are also optimal in some sense, see section 4.1.
As usual, we define the state/action value function



h

i

Qπ (x, u) = E r0 + γV π (x1 )  x0 = x, u0 = u .
83

(3)

Geibel & Wysotzki

Qπ (x, u) is the expected return when the agent first chooses action u, and acts according to
π in subsequent time steps. From the optimal Q-function Q∗ , optimal policies π ∗ and the
unique optimal values V ∗ are derived by π ∗ (x) ∈ argmaxu Q∗ (x, u) and V ∗ (x) = Q∗ (x, π ∗ (x)).
Q∗ can be computed by using Watkin’s Q-learning algorithm.
In RL one in general distinguishes episodic and continuing tasks that can be treated
in the same framework (see e.g. Sutton & Barto, 1998). In episodic tasks, the agent may
reach some terminal or absorbing state at same time t′ . After reaching the absorbing state,
the agent stays there and executes some dummy action. The reward is defined by rt = 0
for t ≥ t′ . During learning the agent is “restarted” according to some distribution on the
initial states after it has reached the absorbing state.

3. Related Work
P

t
The random variable R = ∞
t=0 γ rt (return) used to define the value of a state possesses
a certain variance. Most risk-averse approaches in dynamic programming (DP) and reinforcement learning are concerned with the variance of R, or with its worst outcomes. An
example of such an approach is worst case control (e.g. Coraluppi & Marcus, 1999; Heger,
1994), where the worst possible outcome of R is to be optimized. In risk-sensitive control based on the use of exponential utility functions (e.g. Liu, Goodwin, & Koenig, 2003a;
Koenig & Simmons, 1994; Liu, Goodwin, & Koenig, 2003b; Borkar, 2002), the return R
is transformed so as to reflect a subjective measure of utility. Instead of maximizing the
expected value of R, now the objective is to maximize e.g. U = β −1 log E(eβR ), where β is
a parameter and R is the usual return. It can be shown that depending on the parameter
β, policies with a high variance V(R) are penalized (β < 0) or enforced (β > 0). The αvalue-criterion introduced by Heger (1994) can be seen as an extension of worst case control
where bad outcomes of a policy that occur with a probability less than α are neglected.
Neuneier and Mihatsch (1999) give a model- free RL algorithm which is based on a
parameterized transformation of the temporal difference errors occurring (see also Mihatsch
& Neuneier, 2002). The parameter of the transformation allows to “switch” between riskaverse and risk-seeking policies. The influence of the parameter on the value function cannot
be expressed explicitly.
Our view of risk is not concerned with the variance of the return or its worst possible
outcomes, but instead with the fact that processes generally possess dangerous or undesirable states. Think of a chemical plant where temperature or pressure exceeding some
threshold may cause the plant to explode. When controlling such a plant, the return corresponds to the plant’s yield. But it seems inappropriate to let the return also reflect the
cost of the explosion, e.g. when human lives are affected.
In this work, we consider processes that have such undesirable terminal states. A seemingly straightforward way to handle these error states of the system is to provide high
negative rewards when the systems enters an error state. An optimal policy will then avoid
the error states in general. A drawback of the approach is the fact that it is unknown how
large the risk (probability) of entering an error state is. Moreover, we may want to provide
a threshold ω for the probability of entering an error state that must not be exceeded by
the agent’s policy. In general, it is impossible to completely avoid error states, but the risk
should be controllable to some extend. More precisely, if the agent is placed in some state

84

Risk-Sensitive Reinforcement Learning

x, then it should follow a policy whose risk is constrained by ω. The parameter ω ∈ [0, 1]
reflects the agent’s risk-averseness. That is, our goal is not the minimization of the risk,
but the maximization of V π while the risk is kept below the threshold ω.
Markowitz (1952) considers the combination of different criteria with equal discount
factors in the context of portfolio selection. The risk of the selected portfolio is related to
the variance of the combined (weighted) criteria. Markowitz introduces the notion of the
(E, V )-space. Our notion of risk is not related to the variance of V , but depends on the
occurrence of error states in the MDP. Therefore the risk is conceptually independent of V ,
see e.g. the tank control problem described in section 7.
The idea of weighting return and risk (Markowitz, 1959; Freund, 1956; Heger, 1994) leads
to the expected-value-minus-variance-criterion, E(R) − kV(R), where k is a parameter. We
use this idea for computing a feasible policy for the problem of finding a good policy that
has a constrained risk (in regard to the probability of entering an error state): value and
risk are weighted using a weight ξ for the value and weight −1 for the risk. The value of
ξ is increased, giving the value more weight compared to the risk, until the risk of a state
becomes larger than the user-specified threshold ω.
In considering an ordering relation for tuples of values, our learning algorithm for a
fixed value of ξ is also related to the ARTDP approach by Gabor, Kalmar, and Szepesvari
(1998). In their article, Gabor et al. additionally propose a recursive formulation for an
MDP with constraints that may produce suboptimal solutions. It is not applicable in our
case because their approach requires a nonnegative reward function.
It should be noted that the aforementioned approaches based on the variability of the
return are not suited for problems like the grid world problem discussed in section 6, or the
tank control task in section 7 where risk is related to the parameters (variables) of the state
description. For example, in the grid world problem, all policies have the same worst case
outcome. In regard to approaches based on the variance, we found that a policy leading to
the error states as fast as possible does not have a higher variance than one that reaches the
goal states as fast as possible. A policy with a small variance can therefore have a large risk
(with respect to the probability of entering an error state), which means that we address a
different class of control problems. We underpin this claim in section 8.1.3.
Fulkerson, Littman, and Keim (1998) sketch an approach in the framework of probabilistic planning that is similar to ours although based on the complementary notion of
safety. Fulkerson et al. define safety as the probability of reaching a goal state (see also
the BURIDAN system of Kushmerick, Hanks, & Weld, 1994). Fulkerson et al. discuss the
problem of finding a plan with minimum cost subject to a constraint on the safety (see
also Blythe, 1999). For an episodic MDP with goal states, the safety is 1 minus risk. For
continuing tasks or if there are absorbing states that are neither goal nor error states, the
safety may correspond to a smaller value. Fulkerson et al. (1998) manipulate (scale) the
(uniform) step reward of the undiscounted cost model in order to enforce the agent to reach
the goal more quickly (see also Koenig & Simmons, 1994). In contrast, we also consider
discounted MDPs, and neither require the existence of goal states. Although we do not
change the original reward function, our algorithm in section 5 can be seen as a systematic
approach for dealing with the idea of Fulkerson et al. that consists in modification of the
relative importance of the original objective (reaching the goal) and the safety. In contrast
to the aforementioned approaches belonging to the field of probabilistic planning, which
85

Geibel & Wysotzki

operate on an previously known finite MDP, we have designed an online learning algorithm
that uses simulated or actual experiences with the process. By the use of neural network
techniques the algorithm can also be applied to continuous-state processes.
Dolgov and Durfee (2004) describe an approach that computes policies that have a
constrained probability for violating given resource constraints. Their notion of risk is
similar to that described by Geibel (2001). The algorithm given by Dolgov and Durfee
(2004) computes suboptimal policies using linear programming techniques that require a
previously known model and, in contrast to our approach, cannot be easily extended to
continuous state spaces. Dolgov and Durfee included a discussion on DP approaches for
constrained MDPs (e.g. Altman, 1999) that also do not generalize to continuous state
spaces (as in the tank control task) and require a known model. The algorithm described
by Feinberg and Shwartz (1999) for constrained problems with two criteria is not applicable
in our case, because it requires both discount factors to be strictly smaller than 1, and
because it is limited to finite MDPs.
“Downside risk” is a common notion in finance that refers to the likelihood of a security
or other investment declining in price, or the amount of loss that could result from such
potential decline. The scientific literature on downside risk (e.g. Bawas, 1975; Fishburn,
1977; Markowitz, 1959; Roy, 1952) investigates risk-measures that particularly consider the
case in which a return lower than its mean value, or below some target value is encountered.
In contrast, our notion of risk is not coupled with the return R, but with the fact that a state
x is an error state, for example, because some parameters describing the state lie outside
their permissible ranges, or because the state lies inside an obstacle which may occur in
robotics applications.

4. Risk
To define our notion of risk more precisely, we consider a set
Φ⊆X

(4)

of error states. Error states are terminal states. This means that the control of the agent
ends when it reaches a state in Φ. We allow an additional set of non-error terminal states
Γ with Γ ∩ Φ = ∅.
Now, we define the risk of x with respect to π as the probability that the state sequence
(xi )i≥0 with x0 = x, which is generated by executing policy π, terminates in an error state
x′ ∈ Φ.
Definition 4.1 (Risk) Let π be a policy, and let x be some state. The risk is defined as




ρπ (x) = P ∃i xi ∈ Φ | x0 = x .

(5)

By definition, ρπ (x) = 1 holds if x ∈ Φ. If x ∈ Γ, then ρπ (x) = 0 because of Φ ∩ Γ = ∅. For
states 6∈ Φ ∪ Γ, the risk depends on the action choices of the policy π.
In the following subsection, we will consider the computation of minimum-risk policies
analogous to the computation of maximum-value policies.
86

Risk-Sensitive Reinforcement Learning

4.1 Risk Minimization
The risk ρπ can be considered a value function defined for a cost signal r̄. To see this, we
augment the state space of the MDP with an additional absorbing state η to which the
agent is transfered after reaching a state from Φ ∪ Γ. The state η is introduced for technical
reasons.
If the agent reaches η from a state in Γ, both the reward signals r and r̄ become zero.
We set r = 0 and r̄ = 1, if the agent reaches η from an error state. Then the states in Φ ∪ Γ
are no longer absorbing states. The new cost function r̄ is defined by
′

r̄x,u (x ) =

(

1 if x ∈ Φ and x′ = η
0 else.

(6)

With this construction of the cost function r̄, an episode of states, actions and costs
starting at some initial state x contains exactly once the cost of r̄ = 1 if an error state
occurs in it. If the process does not enter an error state, the sequence of r̄-costs contains
zeros only. Therefore, the probability defining the risk can be expressed as the expectation
of a cumulative return.
Proposition 4.1 It holds
π

ρ (x) = E

"∞
X
i=0

with the “discount” factor γ̄ = 1.

#


γ̄ r̄i  x0 = x
i

(7)

Proof: r̄0 , r̄1 , . . . is the probabilistic sequence of the costs related to the risk. As stated
P
i
above, it holds that R̄ =def ∞
i=0 γ̄ r̄i = 1 if the trajectory leads to an error state; otherwise
P∞ i
i=0 γ̄ r̄i = 0. This means that the return R̄ is a Bernoulli random variable, and the
probability q of R̄ = 1 corresponds to the risk of x with respect to π. For a Bernoulli random
variable it holds that ER̄ = q (see e.g. Ross, 2000). Notice that the introduction of η together
with the fact that r̄ = 1 occurs during the transition from an error state
not iwhen

hP to η, and

∞
i
entering the respective error state, ensures the correct value of E
i=0 γ̄ r̄i  x0 = x also
for error states x. q.e.d.
Similar to the Q-function we define the state/action risk as
h

Q̄π (x, u) = E r̄0 + γ̄ρπ (x1 ) | x0 = x, u0 = u
=

X





px,u (x′ ) r̄x,u (x′ ) + γ̄ρπ (x′ ) .

x′

i

(8)
(9)

Minimum-risk policies can be obtained with a variant of the Q-learning algorithm (Geibel,
2001).
4.2 Maximized Value, Constrained Risk
In general, one is not interested in policies with minimum risk. Instead, we want to provide
a parameter ω that specifies the risk we are willing to accept. Let X ′ ⊆ X be the set of
states we are interested in, e.g. X ′ = X − (Φ ∪ {η}) or X ′ = {x0 } for a distinguished
87

Geibel & Wysotzki

starting state x0 . For a state x ∈ X ′ , let px be the probability for selecting it as a starting
state. The value of
X
V π =def
px V π (x)
(10)
x∈X ′

corresponds to the performance on the states in X ′ . We consider the constrained problem
max V π

(11)

for all x ∈ X ′ : ρπ (x) ≤ ω .

(12)

π

subject to
A policy that fulfills (12) will be called feasible. Depending on ω, the set of feasible policies
may be empty. Optimal policies generally depend on the starting state, and are nonstationary and randomized (Feinberg & Shwartz, 1999; Gabor et al., 1998; Geibel, 2001). If
we restrict the considered policy class to stationary deterministic policies, the constrained
problem is generally only well defined if X ′ is a singleton, because there need not be a
stationary deterministic policy being optimal for all states in X ′ . Feinberg and Shwartz
(1999) have shown for the case of two unequal discount factors smaller than 1 that there
exist optimal policies that are randomized Markovian until some time step n (i.e. they do
not depend on the history, but may be non-stationary and randomized), and are stationary
deterministic (particularly Markovian) from time step n onwards. Feinberg and Shwartz
(1999) give a DP algorithm for this case (cp. Feinberg & Shwartz, 1994). This cannot be
applied in our case because γ̄ = 1, and also because it does not generalize to continuous state
spaces. In the case of equal discount factors, it is shown by Feinberg and Shwartz (1996) that
(for a fixed starting state) there also exist optimal stationary randomized policies that in
the case of one constraint consider at most one action more than a stationary deterministic
policy, i.e. there is at most one state where the policy chooses randomly between two
actions.

5. The Learning Algorithm
For reasons of efficiency and predictability of the agent’s behavior and because of what
has been said at the end of the last section, we will restrict our consideration to stationary deterministic policies. In the following we present a heuristic algorithm that aims at
computing a good policy. We assume that the reader is familiar with Watkin’s Q-learning
algorithm (Watkins, 1989; Watkins & Dayan, 1992; Sutton & Barto, 1998).
5.1 Weighting Risk and Value
We define a new (third) value function Vξπ and a state/action value function Qπξ that is the
weighted sum of the risk and the value with
Vξπ (x) = ξV π (x) − ρπ (x)
Qπξ (x, u)

π

π

= ξQ (x, u) − Q̄ (x, u) .

(13)
(14)

The parameter ξ ≥ 0 determines the influence of the V π -values (Qπ -values) compared to the
ρπ -values (Q̄π -values). For ξ = 0, Vξπ corresponds to the negative of ρπ . This means that
88

Risk-Sensitive Reinforcement Learning

the maximization of V0π will lead to a minimization of ρπ . For ξ → ∞, the maximization of
Vξπ leads to a lexicographically optimal policy for the unconstrained, unweighted 2-criteria
problem. If one compares the performance of two policies lexicographically, the criteria are
ordered. For large values of ξ, the original value function multiplied by ξ dominates the
weighted criterion.
The weight is successively adapted starting with ξ = 0, see section 5.3. Before adaptation
of ξ, we will discuss how learning for a fixed ξ proceeds.
5.2 Learning for a fixed ξ
For a fixed value of ξ, the learning algorithm computes an optimal policy πξ∗ using an
algorithm that resembles Q-Learning and is also based on the ARTDP approach by Gabor
et al. (1998).
During learning, the agent has estimates Qt , Q̄t for time t ≥ 0, and thus an estimate
Qtξ for the performance of its current greedy policy, which is the policy that selects the best
action with respect to the current estimate Qtξ . These values are updated using example
state transitions: let x be the current state, u the chosen action, and x′ the observed
successor state. The reward and the risk signal of this example state transition are given by
r and r̄ respectively. In x′ , the greedy action is defined in the following manner: an action
u is preferable to u′ if Qtξ (x′ , u) > Qtξ (x′ , u′ ) holds. If the equality holds, the action with
the higher Qt -value is preferred. We write u  u′ , if u is preferable to u′ .
Let u∗ be the greedy action in x′ with respect to the ordering . Then the agent’s
estimates are updated according to
Qt+1 (x, u) = (1 − αt )Qt (x, u) + αt (r + γQt (x′ , u∗ ))

(15)

Q̄t+1 (x, u) = (1 − αt )Q̄t (x, u) + αt (r̄ + γ̄ Q̄t (x′ , u∗ ))

(16)

t+1
Qt+1
(x, u) − Q̄t+1 (x, u)
ξ (x, u) = ξQ

(17)

Every time a new ξ is chosen, the learning rate αt is set to 1. Afterwards αt decreases over
time (cp. Sutton & Barto, 1998).
For a fixed ξ, the algorithm aims at computing a good stationary deterministic policy πξ∗
for the weighted formulation that is feasible for the original constrained problem. Existence
of an optimal stationary deterministic policy for the weighted problem and convergence of
the learning algorithm can be guaranteed if both criteria have the same discount factor, i.e.
γ = γ̄, even when γ̄ < 1. In the case γ = γ̄, Qξ forms a standard criterion function with
rewards ξr − r̄. Because we consider the risk as the second criterion function, γ = γ̄ implies
that γ = γ̄ = 1. To ensure convergence in this case it is also required that either (a) there
exists at least one proper policy (defined as a policy that reaches an absorbing state with
probability one), and improper policies yield infinite costs (see Tsitsiklis, 1994), or (b), all
policies are proper. This is the case in our application example. We conjecture that in the
case γ < γ̄ convergence to a possibly suboptimal policy can be guaranteed if the MDP forms
a directed acyclic graph (DAG). In other cases oscillations and non-convergence may occur,
because optimal policies for the weighted problem are generally not found in the considered
policy class of stationary deterministic policies (as for the constrained problem).
89

Geibel & Wysotzki

5.3 Adaptation of ξ
When learning starts, the agent chooses ξ = 0 and performs learning steps that will lead,
after some time, to an approximated minimum-risk policy π0∗ . This policy allows the agent
to determine if the constrained problem is feasible.
Afterwards the value of ξ is increased step by step until the risk in a state in X ′ becomes
larger than ω. Increasing ξ by some ǫ increases the influence of the Q-values compared to
the Q̄-values. This may cause the agent to select actions that result in a higher value,
but perhaps also in a higher risk. After increasing ξ, the agent again performs learning
steps until the greedy policy is sufficiently stable. This is aimed at producing an optimal
deterministic policy πξ∗ . The computed Q- and Q̄-values for the old ξ (i.e. estimates for
∗

∗

∗ .
Qπξ and Q̄πξ ) are used as the initialization for computing πξ+ǫ
The aim of increasing ξ is to give the value function V the maximum influence possible.
This means that the value of ξ is to be maximized, and needs not be chosen by the user.
The adaptation of ξ provides a means for searching the space of feasible policies.

5.4 Using a Discounted Risk
In order to prevent oscillations of the algorithm in section 5.2 for the case γ < γ̄, it may be
advisable to set γ̄ = γ corresponding to using a discounted risk defined as
ρπγ (x)

=E

"∞
X
i=0

#


γ r̄i  x0 = x .
i

(18)

Because the values of the r̄i are all positive, it holds ρπγ (x) ≤ ρπ (x) for all states x. The discounted risk ρπγ (x) gives more weight to error states occurring in the near future, depending
on the value of γ.
For a finite MDP and a fixed ξ, the convergence of the algorithm to an optimal stationary
policy for the weighted formulation can now be guaranteed because Qξ (using ρπγ (x)) forms
a standard criterion function with rewards ξr − r̄. For terminating the adaptation of ξ in
the case that the risk of a state in X ′ becomes larger than ω, one might still use the original
(undiscounted) risk ρπ (x) while learning is done with its discounted version ρπγ (x), i.e. the
learning algorithm has to maintain two risk estimates for every state, which is not a major
problem. Notice that in the case of γ = γ̄, the effect of considering the weighted criterion
ξV π − ρπγ corresponds to modifying the unscaled original reward function r by adding a
negative reward of − 1ξ when the agent enters an error state: the set of optimal stationary
deterministic policies is equal in both cases (where the added absorbing state η with its
single dummy action can be neglected).
In section 6, experiments for the case of γ < 1 = γ̄, X ′ = X − (Φ ∪ {η}), and a finite
state space can be found. In the sections 7 and 8 we will consider an application example
with infinite state space, X ′ = {x0 }, and γ = γ̄ = 1.

6. Grid World Experiment
In the following we will study the behaviour of the learning algorithm for a finite MDP
with a discounted criterion. In contrast to the continuous-state case discussed in the next
90

Risk-Sensitive Reinforcement Learning

E
E
E
a)
E
E G
E E
E →
E ↓
E ↓
c)
E ↓
E G
E E

E → → → → G
E → → → ↑ ↑
E → → ↑ ↑ ↑
b)
E ↓ → ↑ ↑ ↑
E G ← ↑ ↑ ↑
E E E E E E
E → → → → G
E → → → → ↑
E ↓ → → ↑ ↑
d)
E ↓ ↓ ↑ ↑ ↑
E G ← ← ↑ ↑
E E E E E E

G

E
→
→
↓
↓
←
E

E
→
→
→
←
←
E

E E
→ G
↑ ↑
↑ ↑
↑ ↑
← ↑
E E

Figure 1: a) An example grid world, x : horizontal, y : vertical. For further explanation see
text. b) Minimum risk policy (ξ = 0) with 11 unsafe states. c) Maximum value
policy (ξ = 4.0) with 13 unsafe states. d) Result of algorithm: policy for ξ = 0.64
with 11 unsafe states.

section, no function approximation with neural networks is needed because both the value
function and the risk can be stored in a table. For the grid world, we have chosen γ <
1 = γ̄, X ′ = X − Φ, and a state graph that is not a DAG. This implies that there is no
stationary policy which is optimal for every state in X ′ . Although oscillations can therefore
be expected, we have found that the algorithm stabilizes at a feasible policy because the
learning rate αt tends to zero. We also investigated the use of the discounted risk that
prevents an oscillatory behaviour.
We consider the 6 × 6 grid world that is depicted in Figure 1(a). An empty field denotes
some state, Es denote error states, and the two Gs denote two goal states. We describe
states as pairs (x, y) where x, y ∈ {1, 2, 3, 4, 5, 6}. I.e. Γ = {(2, 2), (6, 6)}, Φ = {(1, 1), (1,
2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)}. The additional absorbing
state η is not depicted.
We have chosen the error states as if the lower, i.e. extremal values of x and y were
dangerous. One of the goal states is placed next to the error states, the other in a safer
part of the state space.
The agent has the actions U = {→, ←, ↑, ↓}. An action u ∈ U takes the agent in the
denoted direction if possible. With a probability of 0.21, the agent is not transported to
the desired direction but to one of the three remaining directions.
The agent receives a reward of 1 if it enters a goal state. The agent receives a reward of
0 in every other case. It should be noted that there is no explicit punishment for entering an
error state, but there is an implicit one: if the agent enters an error state, then the current
episode ends. This means that the agent will never receive a positive reward after it has
reached an error state. Therefore, it will try to reach one of the goal states, and because
γ < 1, it will try to do this as fast as possible.
91

Geibel & Wysotzki

We have chosen X ′ = X − (Φ ∪ {η}), γ = 0.9, and equal probabilities px for all states.
Although the convergence of the algorithm cannot be guaranteed in this case, the experimental results show that the algorithm yields a feasible policy.
We have selected ω = 0.13. In order to illustrate the behaviour of the algorithm we
have also computed the minimum-risk and maximum-value policy. Figure 1(b) shows the
minimum risk policy. Though the reward function r defined above plays no role for the
minimum risk policy, the agent tries to reach one of the two goal states. This is so because
from a goal state the probability of reaching an error state is 0. Clearly, with respect to the
value function V , the policy in Figure 1(b) is not optimal: e.g. from state (3, 3) the agent
tries to reach the more distant goal, which causes higher discounting of the goal reward.
The minimum risk policy in Figure 1(b) has 25 safe states, defined as states for which the
risk is below ω. The minimum risk policy has an estimated mean value of V π = 0.442.
In Figure 1(c) the maximum-value policy is shown. The maximum-value policy that
optimizes the value without considering the risk has an estimated value of V π = 0.46.
Thus, it performs better than the minimum-risk policy in Figure 1(b), but the risk in (5, 2)
and (2, 5) has become greater than ω. Our algorithm starts with ξ = 0 and computes the
minimum-risk policy in Figure 1(b). ξ is increased step by step until the risk for a state
changes from a value lower than ω to a value > ω. Our algorithm stops at ξ = 0.64. The
policy computed is shown in Figure 1(d). Obviously, it lies “in between” the minimum risk
policy in Figure 1(b) and the maximum-value policy in Figure 1(c).
We also applied the algorithm with the discounted version of the risk, ρπγ , to the grid
world problem. The discounted risk was used for learning, whereas the original risk, ρπ ,
was used for selecting the best weight ξ. For the parameters described above, the modified
algorithm also produced the policy depicted in figure 1(d). Seemingly, in the grid world
example, oscillations do not present a major problem.
For the tank control task described in the next section, it holds that ρπγ = ρπ because
γ = γ̄.

7. Stochastic Optimal Control with Chance Constraints
In this section, we consider the solution of a stochastic optimal control problem with chance
constraints (Li et al., 2002) by applying our risk-sensitive learning method.
7.1 Description of the Control Problem
In the following, we consider the plant depicted in Figure 2. The task is to control the
outflow of the tank that lies upstream of a distillation column in order to fulfill several
objectives that are described below. The purpose of the distillation column is the separation
of two substances 1 and 2. We consider a finite number of time steps 0, . . . , N . The outflow
of the tank, i.e. the feedstream of the distillation column, is characterized by a flowrate
F (t) which is controlled by the agent, and the substance concentrations c1 (t) and c2 (t) (for
0 ≤ t ≤ N ).
The purpose of the control to be designed is to keep the outflow rate F (t) near a specified
optimal flow rate Fspec in order to guarantee optimal operation of the distillation column.
92

Risk-Sensitive Reinforcement Learning

F1
c11
c12

distillation column

F2
c21
c22

ymax
y, h, c1, c2
ymin

F

tank

Fspec
c1min, c1max
c2min, c2max

Figure 2: The plant. See text for description.
Using a quadratic objective function, this goal is specified by
min

F (0),...,F (N −1)

N
−1
X

(F (t) − Fspec )2 ,

(19)

t=0

where the values obey
for 0 ≤ t ≤ N − 1 : Fmin ≤ F (t) ≤ Fmax .

(20)

The tank is characterized by its tank level y(t) and its holdup h(t), where y = A−1 h with
some constant A for the footprint of the tank. The tank level y(t) and the concentrations
c1 (t) and c2 (t) depend on the two stochastic inflow streams characterized by the flowrates
F1 (t) and F2 (t), and the inflow concentrations c1,j (t) and c2,j (t) for substances j ∈ {1, 2}.
The linear dynamics of the tank level is given by
y(t + 1) = y(t) + A−1 ∆t ·

 X



Fj (t) − F (t) .

(21)


A−1 ∆t X
Fj (t)(cj,i (t) − ci (t))
(
y(t) j=1,2

(22)

j=1,2

The dynamics of the concentrations is given by
for i = 1, 2 : ci (t + 1) = ci (t) +

The initial state of the system is characterized by
y(0) = y0 , c1 (0) = c01 , c2 (0) = c02 .

(23)

The tank level is required to fulfill the constraint ymin ≤ y(t) ≤ ymax . The concentrations inside the tank correspond to the concentrations of the outflow. The substance
concentrations c1 (t) and c2 (t) are required to remain in the intervals [c1,min , c1,max ] and
93

Geibel & Wysotzki

[c2,min , c2,max ], respectively. We assume that the inflows Fi (t) and inflow concentrations
ci,j (t) are random, and that they are governed by a probability distribution. Li et al. (2002)
assume a multivariate Gaussian distribution. Because of the randomness of the variables,
the tank level and the feedstream concentrations may violate the given constraints. We
therefore formulate the stochastic constraint




P ymin ≤ y(t) ≤ ymax , ci,min ≤ ci (t) ≤ ci,max , 1 ≤ t ≤ N, i = 1, 2 ≥ p

(24)

The expression in (24) is called a (joint) chance constraint, and 1 − p corresponds to the
permissible probability of constraint violation. The value of p is given by the user.
The stochastic optimization problem SOP-YC is defined by the quadratic objective function (19) describing the sum of the quadratic differences of the outflow rates and Fspec , the
linear dynamics of the tank level in (21), the nonlinear dynamics of the concentrations in
(22), the initial state given in (23), and the chance constraint in (24).
Li et al. describe a simpler problem SOP-Y where the concentrations are not considered;
see Figure 3. For SOP-Y we use the cumulative inflow FΣ = F1 + F2 in the description
of the tank level dynamics, see (27). SOP-Y describes the dynamics of a linear system.
Li et al. solve SOP-Y by relaxing it to a nonlinear program that is solved by sequential
quadratic programming. This relaxation is possible because SOP-Y is a linear system, and
a multivariate Gaussian distribution is assumed. Solving of nonlinear systems like SOP-YC
and non-Gaussian distributions is difficult (e.g. Wendt, Li, & Wozny, 2002), but can be
achieved with our RL approach.

min

F (0),...,F (N −1)

subject to
for 0 ≤ t ≤ N − 1 :
y(t + 1)

N
−1
X

(F (t) − Fspec )2

(25)

t=0

Fmin ≤ F (t) ≤ Fmax


= y(t) + A−1 ∆t · FΣ (t) − F (t)
y(0) = y0



P ymin ≤ y(t) ≤ ymax , 1 ≤ t ≤ N ≥ p

(26)
(27)
(28)
(29)

Figure 3: The problem SOP-Y.
Note that the control F (t) in the optimization problems only depends on the time step
t. This means that the solutions of SOP-YC and SOP-Y yield open loop controls. Because of
the dependence on the initial condition in (23), a moving horizon approach can be taken to
design a closed loop control. We will not discuss this issue, as it goes beyond the scope of
the paper.
94

Risk-Sensitive Reinforcement Learning

7.2 Formulation as a Reinforcement Learning Problem
Using RL instead of an analytical approach has the advantage that the probability distribution doesn’t have to be Gaussian and it can be unknown. The state equations also need not
be known, and they can be nonlinear. But the learning agent must have access to simulated
or empirical data, i.e. samples of at least some of the random variables.
Independent of the chosen state representation, the immediate reward is defined by
rx,u (x′ ) = −(u − Fspec )2 ,

(30)

where u is a chosen action – the minus is required because the RL value function is to be
maximized. The reward signal only depends on the action chosen, not on the current and
the successor state.
In this work we only consider finite (discretized) action sets, although our approach can
also be extended to continuous action sets, e.g. by using an actor-critic method (Sutton &
Barto, 1998). In the following, we assume that the interval [Fmin , Fmax ] is discretized in an
appropriate manner.
The process reaches an error state if one of the constraints in (24) (or in (29), respectively) is violated. The process is then artificially terminated by transferring the agent to
the additional absorbing state η giving a risk signal of r̄ = 1. The V ∗ -value of error states
is set to zero, because the controller could choose the action Fspec after the first constraint
violation, as subsequent constraint violations do not make things worse with respect to the
chance constraints (24) and (29), respectively.
7.3 Definition of the State Space
In the following we consider the design of appropriate state spaces that result either in an
open loop control (OLC) or a closed loop control (CLC).
7.3.1 Open Loop Control
We note that SOP-YC and SOP-Y are time-dependent finite horizon problems where the
control F (xt ) = F (t) depends on t only. This means that there is no state feedback and
the resulting controller is open-looped. With respect to the state definition xt = (t), the
Markov property defined in section 2 clearly holds for the probabilities and rewards defining
V π . But the Markov property does not hold for the rewards defining ρπ . Using xt = (t)
implies that the agent has no information about the state of the process. By including
information about the history in the form of its past action, the agent gets an “idea” about
the current state of the process. Therefore, the inclusion of history information changes the
probability for r̄ = 1, and the Markov property is violated. Including the past actions in
the state description ensures the Markov property for r̄. The Markov property is therefore
recovered by considering the augmented state definition
xt = (t, ut−1 , . . . , u0 ) ,

(31)

with past actions (ut−1 , . . . , u0 ). The first action u0 depends on the fixed initial tank level y0
and the fixed initial concentrations only. The second action depends on the first action, i.e.
also on the initial tank level and the initial concentrations and so on. Therefore, learning
95

Geibel & Wysotzki

with states (31) results in an open loop control, as in the original problems SOP-YC and
SOP-Y.
It should be noted that for an MDP, the risk does not depend on past actions, but
on future actions only. For the choice xt = (t), there is hidden state information, and we
do not have an MDP because the Markov property is violated. Therefore the probability
of entering an error state conditioned on the time step, i.e. P (r̄0 = 1|t), changes if it is
additionally conditioned on the past actions yielding the value P (r̄0 = 1|t, ut−1 , . . . , u0 )
(corresponding to an agent that remembers its past actions). For example, if the agent
remembers that in the past time steps of the current learning episode it has always used
action F = 0 corresponding to a zero outflow, it can conclude that there is an increased
probability that the tank level exceeds ymax , i.e. it can have knowledge of an increased risk.
If, on the other hand, it does not remember its past actions, it cannot know of an increased
risk because it only knows the index of the current time step, which carries less information
about the current state.
It is well-known that the Markov property can generally be recovered by including the
complete state history into the state description. For xt = (t), the state history contains
the past time indices, actions and r̄-costs. For the tank control task, the action history is
the relevant part of the state history because all previous r̄-costs are necessarily zero, and
the indices of the past time steps are already given with the actual time t that is known
to the agent. Therefore, the past rewards and the indices of the past time steps need not
be included into the expanded state. Although still not the complete state information is
known to the agent, knowledge of past actions suffices to recover the Markov property.
With respect to the state choice (31) and the reward signal (30), the expectation from
the definition of the value function is not needed, cp. eq. (2). This means that
h

i

V π (x) = E R | x0 = x = −

N
−1
X

(F (t) − Fspec )2

t=0

holds, i.e. there is a direct correspondence between the value function and the objective
function of SOP-YC and SOP-Y.
7.3.2 Closed Loop Control
We will now define an alternative state space, where the expectation is needed. We have
decided to use the state definition
xt = (t, y(t), c1 (t), c2 (t))

(32)

xt = (t, y(t))

(33)

for the problem SOP-YC and
for the simpler problem SOP-Y. The result of learning is a state and time-dependent closed
loop controller, which can achieve a better regulation behavior than the open loop controller,
because it reacts on the actual tank level and concentrations, whereas an open loop control
does not. If the agent has access to the inflow rates or concentrations, they too can be
included in the state vector, yielding improved performance of the controller.
96

Risk-Sensitive Reinforcement Learning

Parameter
N
y0
[ymin , ymax ]
A−1 ∆t
Fspec
[Fmin , Fmax ]
only RL-YC-CLC:
c01
c02
[c1,min , c1,max ]
[c2,min , c2,max ]

Table 1: Parameter settings
Value
Explanation
16
number of time steps
0.4
initial tank level
[0.25, 0.75] admissible interval for tank level
0.1
constant, see (22)
0.8
optimal action value
[0.55, 1.05] interval for actions, 21 discrete values
0.2
0.8
[0.1, 0.4]
[0.6, 0.9]

initial concentration subst. 1
initial concentration subst. 2
interval for concentration 1
interval for concentration 2

7.4 The RL Problems
With the above definitions, the optimization problem is defined via (11) and (12) with
ω = 1−p (see (24) and (29)). The set X ′ (see (10) and (12)) is defined to contain the unique
starting state, i.e X ′ = {x0 }. In our experiments we consider the following instantiations
of the RL problem:
• RL-Y-CLC Reduced problem SOP-Y using states xt = (t, y(t)), with x0 = (0, y0 ) resulting in a closed loop controller (CLC).
• RL-Y-OLC Open loop controller (OLC) for reduced problem SOP-Y. The state space is
defined by the action history and time, see eq. (31). The starting state is x0 = (0).
• RL-YC-CLC Closed loop controller for full problem SOP-YC using states xt =
(t, y(t), c1 (t), c2 (t)) with x0 = (0, y0 , c01 , c01 ).
Solving the problem RL-Y-OLC yields an action vector. The problems RL-YC-CLC and
RL-Y-CLC result in state dependent controllers. We do not present results for the fourth
natural problem RL-YC-OLC, because they offer no additional insights.
For interpolation between states we used 2 × 16 multilayer perceptrons (MLPs, e.g.
Bishop, 1995) in the case of RL-Y-OLC because of the extremely large state space (15 dimensions for t = N − 1). We used radial basis function (RBF) networks in the case of
RL-YC-CLC and RL-Y-CLC, because they produced faster, more stable and robust results
compared to MLPs.
For training the respective networks, we used the “direct method” that corresponds
to performing one gradient descent step for the current state-action pair with the new
∗
estimate as the target value (see e.g. Baird, 1995). The new estimate for Qπξ is given
∗
by r + γQt (x′ , u∗ ), and for Q̄πξ by r̄ + γ̄ Q̄t (x′ , u∗ ) (compare the right sides of the update
equations (15)-(17)).
97

Geibel & Wysotzki

(a)
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

outflow rate

Inflow

(b)

0

2

4

6

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65

8 10 12 14 16
time

(c)

omega=0.01
0.8

0

2

4

6

8 10 12 14 16
time

omega=0.05
0.8
outflow rate

outflow rate

(d)
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0

2

4

6

8 10 12 14 16
time

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65

omega=0.1
0.8

0

2

4

6

8 10 12 14 16
time

Figure 4: RL-Y-CLC: (a) The inflow rates FΣ (t) for 10 runs. (b), (c), (d) Example runs of
policies for ω = 0.01, 0.05, 0.10 (i.e. p = 0.99, 0.95, 0.90). It holds Fspec = 0.8.

8. Experiments
In this section, we examine the experimental results obtained for the tank control task
(γ = γ̄ = 1). In section 8.1 we discuss the linear case and compare the results to those of Li
et al. (2002). For the linear case, we consider the closed loop controller obtained by solving
RL-Y-CLC (sect. 8.1.1) and the open loop controller related to the RL problem RL-Y-OLC
(sect. 8.1.2). For the closed loop controller, we discuss the problem of non-zero covariances
between variables of different time steps. The nonlinear case is discussed in section 8.2.
8.1 The Problems RL-Y-CLC and RL-Y-OLC
We start with the simplified problems, RL-Y-CLC and RL-Y-OLC, derived from SOP-Y that
is discussed by Li et al. (2002). In SOP-Y the concentrations are not considered, and there
is only one inflow rate FΣ (t) = F1 (t) + F2 (t). The parameter settings in Table 1 (first five
lines) were taken from Li et al. (2002). The minimum and maximum values for the actions
were determined by preliminary experiments.
Li et al. define the inflows (FΣ (0), . . . , FΣ (15))T as having a Gaussian distribution with
the mean vector
(1.8, 1.8, 1.5, 1.5, 0.7, 0.7, 0.5, 0.3, 0.2, 0.2, 0.2, 0.2, 0.2, 0.6, 1.2, 1.2)T .
98

(34)

Risk-Sensitive Reinforcement Learning

0.3
0.2
0.1
risk
0
-0.1
value

-0.2

weighted
-0.3
-0.4
-0.5
0

5

10

15

20

xi
∗

π∗

∗

Figure 5: RL-Y-CLC: Estimates of the risk ρπξ (x0 ), the value V πξ (x0 ), and of Vξ ξ (x0 ) =
∗

∗

ξV πξ (x0 ) − ρπξ (x0 ) for different values of ξ.

The covariance matrix is given by





C=

σ0 σ1 r01
σ02
σ0 σ1 r01
...
···
···
σ0 σN −1 r0(N −1)
···

· · · σ0 σN −1 r0(N −1)
...
...
···
···
2
···
σN
−1







(35)

with σi = 0.05. The correlation of the inflows of time i and j is defined by
rij = rji = 1 − 0.05(j − i)

(36)

for 0 ≤ i ≤ N − 1, i < j ≤ N − 1 (from Li et al., 2002). The inflow rates for ten example
runs are depicted in Figure 4(a).
8.1.1 The Problem RL-Y-CLC (Constraints for Tank Level)
We start with the presentation of the results for the problem RL-Y-CLC, where the control
(i.e. the outflow F ) depends only on the time t and the tank level. Because X ′ = {x0 } the
overall performance of the policy as defined in (10) corresponds to its performance for x0 ,
∗

∗

V πξ = V πξ (x0 ) .
∗

It holds that x0 = (0, y0 ). V πξ (x0 ) is the value with respect to the policy πξ∗ learned for the
weighted criterion function Vξπ , see also (13). The respective risk is
∗

ρπξ (x0 ) .
∗

∗

In Figure 5 the estimated2 risk ρπξ (x0 ) and the estimated value V πξ (x0 ) are depicted
∗
∗
for different values of ξ. Both the estimate for the risk ρπξ (x0 ) and that for value V πξ (x0 )
2. All values and policies presented in the following were estimated by the learning algorithm. Note that
in order to enhance the readability, we have also denoted the learned policy as πξ∗ .

99

Geibel & Wysotzki

0.5
0.4
0.3
0.2
0.1
0
-0.1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
xi

Figure 6: RL-Y-CLC: Difference between the weighted criteria. For an explanation see text.

increase with ξ. Given a fixed value p for the admissible probability of constraint violation,
∗
the appropriate ξ = ξ(p) can be obtained as the value for which the risk ρπξ (x0 ) is lower
∗
than ω = 1−p and has the maximum V πξ (x0 ). Due to the variation of the performance (see
Fig. 5) we found that this works better than just selecting the maximum ξ. The estimate
π∗

∗

∗

of the weighted criterion Vξ ξ (x0 ) = ξV πξ (x0 ) − ρπξ (x0 ) is also shown in Figure 5.
The outflow rate F (control variable) for different values of ω can be found in Figure 4(bc). Note that the rates have a certain variance since they depend on the probabilistic tank
level. We randomly picked one example run for each value of ω. It is found that the control
values F (t) tend to approach Fspec with increasing values of ω (i.e. decreasing values of p).
Correlations The definition of the covariance matrix in (35) and (36) reveals a high
correlation of the inflow rates in neighboring time steps. In order to better account for this,
it is possible to include information on past time steps in the state description at time t.
Because the level y changes according to the inflow rate FΣ , we investigated the inclusion
of past values of y. If the inflow rates were measured, they too could be included in the
state vector. Former rewards need not be included because they depend on the past tank
levels, i.e. they represent redundant information.
We have compared the performance of the algorithm for the augmented state space
defined by x̃t = (t, y(t), y(t − 1), y(t − 2)) (depth 2 history) and the normal state space
xt = (t, y(t)) (no history). Fig. 6 shows
∗





∗





V π̃ξ (0, y0 , 0, 0) − V πξ (0, y0 ) ,
|

{z
x̃0

}

| {z }
x0

i.e. the difference in the weighted criteria for the starting state with respect to the learned
policies π̃ξ∗ (history) and πξ∗ (no history). Note that for the starting state x̃0 , the past values
have been defined as 0. The curve in Figure 6 runs mainly above 0. This means that using
the augmented state space results in a better performance for many values of ξ. Note that
100

Risk-Sensitive Reinforcement Learning

(a)

(b)
0.3

1
Risk

0.2

0.95

0.1
outflow rate

0.9

0
-0.1
Value

-0.2

0.85
0.8
0.75

-0.3

0.7

-0.4

0.65
0

2

4

6

8

10

12

14

0

xi
∗

2

4

6

8
time

10

12

14

∗

Figure 7: RL-Y-OLC: (a) Estimates of the risk ρπξ (x0 ) and the value V πξ (x0 ) for increasing
∗
∗
values of ξ. (b) Learned policy πξ∗ with ρπξ (x0 ) ≈ 0.098 and V πξ (x0 ) ≈ −0.055

for larger values of ξ the original value function overweights the risk so that in both cases
the policy that always chooses the outflow Fspec is approximated. This means that the
difference in performance tends to zero.
A similar, but not quite pronounced effect can be observed when using a history of
length 1 only. In principle, we assume that it is possible to achieve even better performance
by including the full history of tank levels in the state description, but there is a tradeoff between this objective and the difficulty of network training caused by the number of
additional dimensions.
8.1.2 RL-Y-OLC (History of Control Actions)
The RL problem RL-Y-OLC comprises state descriptions consisting of an action history
together with the time, see eq. (31). The starting state has an empty history, i.e. x0 = (0).
The result of learning is a time-dependent policy with an implicit dependence on y0 . The
learned policy πξ∗ is therefore a fixed vector of actions F (0), . . . , F (15) that forms a feasible,
but in general suboptimal solution to the problem SOP-Y in Figure 3.
∗
∗
The progression of the risk estimate, i.e. of ρπξ (x0 ), and that for the value, V πξ (x0 ),
for different values of ξ can be found in Figure 7. The results are not as good as the ones
∗
for RL-Y-CLC in Figure 5: the estimated minimum risk is 0.021, and the risk ρπξ (x0 ) grows
much faster than the RL-Y-CLC-risk in Figure 5.
∗
A policy having a risk ρπξ (x0 ) ≈ 0.098 is depicted in Figure 7(b). In contrast to the
policies for RL-Y-CLC (see Figure 4(b-c)), the control values do not change in different runs.
8.1.3 Comparison
In Table 2, we have compared the performance of the approach of Li et al. with RL-Y-CLC
and RL-Y-OLC for p = 0.8 and p = 0.9. For both RL-Y-CLC and RL-Y-OLC we performed 10
learning runs. For the respective learned policy π, the risk ρπ (x0 ) and value V π (x0 ) were
estimated during 1000 test runs. For RL-Y-CLC and RL-Y-OLC, the table shows the mean
performance averaged over this 10 runs together with the standard deviation in parentheses.
101

Geibel & Wysotzki

Table 2: Comparison of est. squared deviation to Fspec (i.e. −V π (x0 )) for results of Li et
al. with results for RL-Y-CLC and RL-Y-OLC for p = 0.8 (ω = 0.2) and p = 0.9
(ω = 0.1). Smaller values are better.
approach
Li et al. (2002)
RL-Y-CLC
RL-Y-OLC

p = 0.8
0.0123
0.00758 (0.00190)
0.0104 (0.000302)

p = 0.9
0.0484
0.02 (0.00484)
0.0622 (0.0047)

It is found that, in average, the policy determined for RL-Y-CLC performs better than that
obtained through the approach of Li et al. (2002) (with respect to the estimated squared
deviation to the desired outflow Fspec , i.e. with respect to −V π (x0 ).) The policy obtained
for RL-Y-OLC performs better for p = 0.8 and worse for p = 0.9. The maximal achievable
probability for holding constraints was 1.0 (sd 0.0) for RL-Y-CLC, and and 0.99 (sd 0.0073)
for RL-Y-OLC. Li et al. report p = 0.999 for their approach.
The approach of Neuneier and Mihatsch (1999) considers the worst-case outcomes of a
policy, i.e. risk is related to the variability of the return. Neuneier and Mihatsch show that
the learning algorithm interpolates between risk-neutral and the worst-case criterion and
has the same limiting behavior as the exponential utility approach.
0.6

risk
value

0.5
0.4
0.3

risk

0.2
value

0.1
0
-0.1
-1

-0.5

0

0.5

1

kappa

Figure 8: Risk and value for several values of κ
The learning algorithm of Neuneier and Mihatsch has a parameter κ ∈ (−1.0, 1.0) that
allows to switch between risk-averse behavior (κ → 1), risk-neutral behavior (κ = 0), and
risk-seeking behavior (κ → −1). If the agent is risk-seeking, it prefers policies with a
good best-case outcome. Figure 8 shows risk (probability of constraint violation) and value
for the starting state in regard to the policy computed with the algorithm of Neuneier and
Mihatsch. Obviously, the algorithm is able to find the maximum-value policy yielding a zero
deviation of Fspec , corresponding to choosing F = Fspec = 0.8 in all states, but the learning
result is not sensitive to the risk parameter κ. The reason for this is that the worst-case and
the best-case returns for the policy that always chooses the outflow 0.8 also correspond to
102

Risk-Sensitive Reinforcement Learning

(a)

Inflow

(b)
1

1

0.8

0.8
ymax
mu(t)+0.04

0.6

y

0.6

0.4

0.4
mu(t)-0.04

c1max
c1

ymin
0.2

0.2

c1min
0

0
0

2

4

6

8

10

12

14

0

Time

2

4

6

8

10

12

14

16

Time

Figure 9: RL-YC-CLC: (a) µ(t) + 0.04 and µ(t) − 0.04 (profiles of the two mode means).
(b) The tank level y(t) and the concentration c1 (t) for 10 example runs using the
minimum risk policy.

0, which is the best return possible (implying a zero variance of the return). The approach
of Neuneier and Mihatsch and variance-based approaches are therefore unsuited for the
problem at hand.
8.2 The Problem RL-YC-CLC (Constraints for Tank Level and Concentrations)
In the following we will consider the full problem RL-YC-CLC. The two inflows F1 and F2 are
assumed to have equal Gaussian distributions such that the distribution of the cumulative
inflow FΣ (t) = F1 (t) + F2 (t) is described by the covariance matrix in (35) and the mean
vector µ in (34); see also Figure 4(a).
In order to demonstrate the applicability of our approach to non-Gaussian distributions,
we have chosen bimodal distributions for the inflow concentrations c1 and c2 . The underlying
assumption is that the upstream plants either all have an increased output, or all have a
lower output, e.g. due to different hours or weekdays.
The distribution of the inflow concentration ci,1 (t) is characterized by two Gaussian
distributions with means
µ(t) + (−1)k 0.04 ,
where k = 1, 2 and σ 2 = 0.0025. The value of k ∈ {0, 1} is chosen at the beginning of
each run with equal probability for each outcome. This means that the overall mean value
of ci,1 (t) is given by µ(t). The profiles of the mean values of the modes can be found in
Figure 9(a). ci,2 is given as ci,2 (t) = 1.0 − ci,1 (t). The minimum and maximum values for
the concentrations ci (t) can be found in Table 1, and also in Figure 9(b). Note that the
concentrations have to be controlled indirectly by choosing an appropriate outflow F .
The developing of the risk and the value of the starting state is shown in Figure 10.
The resulting curves behave similar to that for the problem RL-Y-CLC depicted in Figure 5:
both value and risk increase with ξ. It can be seen that the algorithm covers a relatively
broad range of policies with different value-risk combinations.
103

Geibel & Wysotzki

0.4
0.3
0.2
risk
0.1
0
-0.1
-0.2

value

-0.3
-0.4
0

5

10

15

20

25

30

35

40

45

50

xi
∗

∗

π∗

∗

Figure 10: RL-YC-CLC: Estimated risk ρπξ (x0 ), value V πξ (x0 ), and Vξ ξ (x0 ) = ξV πξ (x0 ) −
∗

ρπξ (x0 ) for different values of ξ.

For the minimum risk policy, the curves of the tank level y and the concentration
c1 can be found in Figure 9(b). The bimodal characteristics of the substance 1 inflow
concentrations are reflected by c1 (t) (it holds c2 (t) = 1 − c1 (t)). The attainable minimum
risk is 0.062. Increasing the weight ξ leads to curves similar to that shown in the Figures 5
and 7. We assume that the minimum achievable risk can be decreased by the inclusion of
additional variables, e.g. inflow rates and concentrations, and/or by the inclusion of past
values as discussed in section 8.1.1. The treatment of a version with an action history is
analogous to section 8.1.2. We therefore conclude presentation of the experiments at this
point.

9. Conclusion
In this paper, we presented an approach for learning optimal policies with constrained risk
for MDPs with error states. In contrast to other RL and DP approaches that consider risk
as a matter of the variance of the return or of its worst outcomes, we defined risk as the
probability of entering an error state.
We presented a heuristic algorithm that aims at learning good stationary policies that is
based on a weighted formulation of the problem. The weight of the original value function
is increased in order to maximize the return while the risk is required to stay below the
given threshold. For a fixed weight and a finite state space, the algorithm converges to an
optimal policy in the case of an undiscounted value function. For the case that the state
space is finite, contains no cycles, and that γ < 1 holds, we conjecture the convergence of
the learning algorithm to a policy, but assume that it can be suboptimal for the weighted
formulation. If an optimal stationary policy exists for the weighted formulation, it is a
feasible, but generally suboptimal solution for the constrained problem.
104

Risk-Sensitive Reinforcement Learning

The weighted approach that is combined with an adaptation of ξ is a heuristic for searching the space of feasible stationary policies of the original constrained problem, which to
us seems relatively intuitive. We conjecture that better policies could be found by allowing
state-dependent weights ξ(x) with a modified adaptation strategy, and by extending the
considered policy class.
We have successfully applied the algorithm to the control of the outflow of a feed tank
that lies upstream of a distillation column. We started with a formulation as a stochastic
optimal control problem with chance constraints, and mapped it to a risk-sensitive learning
problem with error states (that correspond to constraint violation). The latter problem was
solved using the weighted RL algorithm.
The crucial point in reformulation as an RL problem was the design of the state space.
We found that the algorithm consistently performed better when state information was
provided to the learner. Using the time and the action history resulted in very large state
spaces, and a poorer learning performance. RBF networks together with sufficient state
information facilitated excellent results.
It must be mentioned that the use of RL together with MLP or RBF network based
function approximation suffers from the usual flaws: non-optimality of the learned network,
potential divergence of the learning process, and long learning times. In contrast to an
exact method, no a priori performance guarantee can be given, but of course an a posteriori
estimate of the performance of the learned policy can be made. The main advantage of the
RL method lies in its broad applicability. For the tank control task, we achieved very good
results compared to those obtained through a (mostly) analytical approach.
For the cases |X| > 1 or γ < 1 further theoretical investigations of the convergence and
more experiments are required. Preliminary experiments have shown that oscillations may
occur in our algorithm, but the behavior tends to oscillate between sensible policies without
getting too bad in-between although the convergence and usefulness of the policies remains
an open issue.
Oscillations can be prevented by using a discounted risk that leads to an underestimation
of the actual risk. The existence of an optimal policy and convergence of the learning
algorithm for a fixed ξ can be guaranteed in the case of a finite MDP. A probabilistic
interpretation of the discounted risk can be given by considering 1 − γ as the probability of
exiting from the control of the MDP (Bertsekas, 1995). The investigation of the discounted
risk may be worthwhile in its own right. For example, if the task has long episodes, or if
it is continuing, i.e. non-episodic, it can be more natural to give a larger weight to error
states occurring closer to the current state.
We have designed our learning algorithm as an online algorithm. This means that learning is accomplished using empirical data obtained through interaction with a simulated or
real process. The use of neural networks allows to apply the algorithm to processes with
continuous state spaces. In contrast, the algorithm described by Dolgov and Durfee (2004)
can only be applied in the case of a known finite MDP. Such a model can be obtained
in the case of a continuous-state process by finding an appropriate discretization and estimating the state transition probabilities together with the reward function. Although
such discretization prevents the application of Dolgov and Durfee’s algorithm to RL-Y-OLC,
where a 15-dimensional state space is encountered, it can probably be applied in the case
of RL-Y-OLC. We plan to investigate this point in future experiments.
105

Geibel & Wysotzki

The question arises as to whether our approach can also be applied to stochastic optimal
control problems with other types of chance constraints. Consider a conjunction of chance
constraints
P(C0 ) ≥ p1 , . . . , P(CN −1 ) ≥ pN −1 ,
(37)
where each Ct is a constraint system containing only variables at time t, and pt is the
respective probability threshold. (37) requires an alternative RL formulation where the risk
of a state only depends on the next reward, and where each time-step has its own ωt . The
solution with a modified version of the RL algorithm is not difficult.
If each of the Ct in (37) is allowed to be a constraint system over state variables depending on t′ ≥ t, things get more involved because several risk functions are needed for
each state. We plan investigating these cases in the future.

Acknowledgments We thank Dr. Pu Li for providing the application example and for his
helpful comments. We thank Önder Gencaslan for conducting first experiments during his
master’s thesis.

References
Altman, E. (1999). Constrained Markov Decision Processes. Chapman and Hall/CRC.
Baird, L. (1995). Residual algorithms: reinforcement learning with function approximation. In Proc. 12th International Conference on Machine Learning, pp. 30–37. Morgan
Kaufmann.
Bawas, V. S. (1975). Optimal rules for ordering uncertain prospects. Journal of Finance,
2 (1), 1975.
Bertsekas, D. P. (1995). Dynamic Programming and Optimal Control. Athena Scientific,
Belmont, Massachusetts. Volumes 1 and 2.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford University Press,
Oxford.
Blythe, J. (1999). Decision-theoretic planning. AI Magazine, 20 (2), 37–54.
Borkar, V. (2002). Q-learning for risk-sensitive control. Mathematics of Operations Research,
27 (2), 294–311.
Coraluppi, S., & Marcus, S. (1999). Risk-sensitive and minimax control of discrete-time,
finite-state Markov decision processes. Automatica, 35, 301–309.
Crites, R. H., & Barto, A. G. (1998). Elevator group control using multiple reinforcement
learning agents. Machine Learning, 33 (2/3), 235–262.
Dolgov, D., & Durfee, E. (2004). Approximating optimal policies for agents with limited
execution resources. In Proceedings of the Eighteenth International Joint Conference
on Artificial Intelligence, pp. 1107–1112. AAAI Press.
106

Risk-Sensitive Reinforcement Learning

Feinberg, E., & Shwartz, A. (1994). Markov decision models with weighted discounted
criteria. Math. of Operations Research, 19, 152–168.
Feinberg, E., & Shwartz, A. (1996). Constrained discounted dynamic programming. Math.
of Operations Research, 21, 922–945.
Feinberg, E., & Shwartz, A. (1999). Constrained dynamic programming with two discount
factors: Applications and an algorithm. IEEE Transactions on Automatic Control,
44, 628–630.
Fishburn, P. C. (1977). Mean-risk analysis with risk associated with below-target returns.
American Economics Review, 67 (2), 116–126.
Freund, R. (1956). The introduction of risk into a programming model. Econometrica, 21,
253–263.
Fulkerson, M. S., Littman, M. L., & Keim, G. A. (1998). Speeding safely: Multi-criteria
optimization in probabilistic planning. In Proceedings of the Fourteenth National
Conference on Artificial Intelligence, p. 831. AAAI Press/MIT Press.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning. In
Proc. 15th International Conf. on Machine Learning, pp. 197–205. Morgan Kaufmann,
San Francisco, CA.
Geibel, P. (2001). Reinforcement learning with bounded risk. In Brodley, E., & Danyluk,
A. P. (Eds.), Machine Learning - Proceedings of the Eighteenth International Conference (ICML01), pp. 162–169. Morgan Kaufmann Publishers.
Heger, M. (1994). Consideration of risk in reinforcement learning. In Proc. 11th International Conference on Machine Learning, pp. 105–111. Morgan Kaufmann.
Kall, P., & Wallace, S. W. (1994). Stochastic Programming. Wiley, New York.
Koenig, S., & Simmons, R. G. (1994). Risk-sensitive planning with probabilistic decision
graphs. In Doyle, J., Sandewall, E., & Torasso, P. (Eds.), KR’94: Principles of Knowledge Representation and Reasoning, pp. 363–373, San Francisco, California. Morgan
Kaufmann.
Kushmerick, N., Hanks, S., & Weld, D. S. (1994). An algorithm for probabilistic leastcommitment planning.. In AAAI, pp. 1073–1078.
Li, P., Wendt, M., Arellano-Garcia, & Wozny, G. (2002). Optimal operation of distillation
processes under uncertain inflows accumulated in a feed tank. AIChe Journal, 48,
1198–1211.
Liu, Y., Goodwin, R., & Koenig, S. (2003a). Risk-averse auction agents. In Rosenschein, J.,
Sandholm, T., & Wooldridge, M. Yokoo, M. (Eds.), Proceedings of the Second International Joint Conference on Autonomous Agents and MultiAgent Systems (AAMAS03), pp. 353–360. ACM Press.
Liu, Y., Goodwin, R., & Koenig, S. (2003b). Risk-averse auction agents.. In AAMAS, pp.
353–360.
Markowitz, H. M. (1952). Portfolio selection. The Journal of Finance, 7 (1), 77–91.
Markowitz, H. M. (1959). Portfolio Selection. John Wiley and Sons, New York.
107

Geibel & Wysotzki

Mihatsch, O., & Neuneier, R. (2002). Risk-sensitive reinforcement learning. Machine Learning, 49 (2-3), 267–290.
Neuneier, R., & Mihatsch, O. (1999). Risk-sensitive reinforcement learning. In Michael
S. Kearns, Sara A. Solla, D. A. C. (Ed.), Advances in Neural Information Processing
Systems, Vol. 11. MIT Press.
Ross, S. M. (2000). Introduction to Probability Models. Academic Press, New York.
Roy, A. D. (1952). Safety first and the holding of assets. Econometrica, 20 (3), 431–449.
Smart, W. D., & Kaelbling, L. P. (2002). Effective reinforcement learning for mobile robots. In Proceedings of the 2002 IEEE International Conference on Robotics and
Automation (ICRA 2002).
Stephan, V., Debes, K., Gross, H.-M., Wintrich, F., & Wintrich, H. (2001). A new control
scheme for combustion processes using reinforcement learning based on neural networks. International Journal of Computational Intelligence and Applications, 1 (2),
121–136.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning – An Introduction. MIT
Press.
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine
Learning, 16 (3), 185–202.
Watkins, C. J. C. H. (1989). Learning from Delayed Rewards. Ph.D. thesis, King’s College,
Oxford.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4). Special
Issue on Reinforcement Learning.
Wendt, M., Li, P., & Wozny, G. (2002). Non-linear chance constrained process optimization
under uncertainty. Ind. Eng. Chem. Res., 21, 3621–3629.

108

Journal of Artificial Intelligence Research 24 (2005) 933–944

Submitted 12/04; published 12/05

Engineering Note
mGPT: A Probabilistic Planner Based on Heuristic Search
Blai Bonet

bonet@ldc.usb.ve

Departamento de Computación
Universidad Simón Bolı́var, Venezuela

Héctor Geffner

hector.geffner@upf.edu

ICREA & Universitat Pompeu Fabra
Paseo de Circunvalación 8, Barcelona 08003, Spain

Abstract
We describe the version of the GPT planner used in the probabilistic track of the 4th
International Planning Competition (ipc-4). This version, called mGPT, solves Markov
Decision Processes specified in the ppddl language by extracting and using different classes
of lower bounds along with various heuristic-search algorithms. The lower bounds are
extracted from deterministic relaxations where the alternative probabilistic effects of an
action are mapped into different, independent, deterministic actions. The heuristic-search
algorithms use these lower bounds for focusing the updates and delivering a consistent
value function over all states reachable from the initial state and the greedy policy.

1. Introduction
mGPT is a planner based on heuristic search for solving Markov Decision Processes (MDPs)
specified in the high-level planning language ppddl. mGPT captures a fragment of the
functionality of the GPT system that handles non-determinism and incomplete information,
in both qualitative and probabilistic forms, including pomdps and Conformant planning
(Bonet & Geffner, 2000).
mGPT supports several algorithms and admissible heuristic functions (lower bounds)
that when combined generate a wide range of solvers. The main algorithms are lrtdp and
hdp. Both are heuristic-search algorithms for solving MDPs that make use of lower bounds
for computing a consistent value function V : a function with Bellman residuals bounded
by a user-provided parameter  over all states reachable from a given initial state s0 and
the greedy policy based on V (Bonet & Geffner, 2003b, 2003a).
The lower bounds are derived by solving relaxations of the input problem. Since the algorithms for solving the relaxations are also based on heuristic search, we have implemented
“stackable” software components that are created in sequence for computing complex heuristic functions from simpler ones.

2. Algorithms
We divide the algorithms into two groups: those that deliver consistent value functions with
respect to a user-provided parameter , and those that select actions in real time. The first

c
2005
AI Access Foundation. All rights reserved.

Bonet & Geffner

class of algorithms compute an -consistent value function V for all states reachable from
the initial state s0 , and the greedy policy πV based on V .
In the following subsection, we give definitions of admissible and consistent value functions, and greedy, partial and proper policies. Then, we present the algorithms implemented
by mGPT.
2.1 Consistent Value Functions, and Greedy, Partial and Proper Policies
A value function V is admissible if it is non-overestimating; i.e. if the value V (s) at each
state s is a lower bound on the optimal expected cost of starting at s. V is -consistent at
state s if its Bellman residual at s,



X


def
0
0 

R(s) = V (s) − min c(s, a) +
P r(s |s, a)V (s )  ,
(1)
a∈A(s)

s0 ∈S

is less than or equal to . Here, A(s) denotes the actions that are applicable at s, c(s, a) is
the cost of applying action a in s, and P r(·) is the probabilistic transition function. If V is
0-consistent at s, then we say that V is consistent at s.
A state s is reachable from the initial state s0 and policy π if there exists a trajectory
s0 , s1 , . . . , sn such that sn = s and P (sk+1 |sk , π(sk )) > 0 for all 0 ≤ k < n. In other words,
if the state s can be reached with positive probability from s0 in zero or more steps using
the policy π.
It is known that the greedy policy πV based on the value function V , defined as


X
def
0
0
πV (s) = argmin c(s, a) +
P r(s |s, a)V (s ) ,
(2)
a∈A(s)

s0 ∈S

is optimal when V is -consistent over all states for a sufficiently small . Yet, since our goal
is to find an optimal policy with respect to the initial state s0 and the states reachable from
it, it is sufficient for V to be admissible and -consistent over the states that are reachable
from s0 and πV .
A partial policy π is a policy that doesn’t need to be defined for all states. It is closed
with respect to a state s if π is defined over s and all states reachable from s and π, it is
proper with respect to s if a goal state can be reached from every state reachable from s
and π, and finally it is proper if it is proper with respect to all states.
2.2 Algorithms that Compute -Consistent Value Functions
For the first group of algorithms, mGPT implements Value Iteration (vi), Labeled RealTime Dynamic Programming (lrtdp), and Heuristic Dynamic Programming (hdp).
Value Iteration (Bertsekas, 1995) is applied over the states that can be reached from
the given initial state and the available operators, and yields an -consistent value function
over all of them.1 mGPT’s vi serves as a bottom-line reference for comparison with the
other algorithms.
1. On undiscounted problems like those in probabilistic planning, some conditions are neeeded in order for
VI to finish with an -consistent value function (Bertsekas, 1995).

934

mGPT: A Probabilistic Planner Based on Heuristic Search

Labeled Real-Time Dynamic Programming (Bonet & Geffner, 2003b) is a heuristicsearch algorithm that implements a labeling scheme on top of the rtdp algorithm (Barto,
Bradtke, & Singh, 1995) to improve its convergence. Lrtdp works by performing simulated
trials that start at the initial state and end at “solved” states, selecting actions according
to the greedy policy πV and successor states according to their corresponding transition
probabilities. Initially, V is the input heuristic function, and the only solved states are the
goal states. Then, each time an action is picked at state s, the value of s is updated by
making it consistent with the value of its successors. At the end of each trial, a labeling
procedure is called that checks whether new states can be labeled as solved: a state is solved
if its value and the value of all its descendents are -consistent. The algorithm ends when
the initial state is labeled solved. At that point all states reachable from the initial state
s0 and the greedy policy πV are -consistent. The labeling mechanism also guarantees that
πV is a proper partial policy with respect to s0 .
Heuristic Dynamic Programming (Bonet & Geffner, 2003a) is the second heuristic-search
algorithm supported in mGPT for solving MDPs. Hdp performs systematic depth-first
searches over the set of states reachable from the initial state s0 and the greedy policy
πV looking for -inconsistent states and updating their values. On top of this search,
a labeling scheme based on Tarjan’s strongly-connected components procedure (Tarjan,
1972), identifies the states that are solved and that do not need to be revisited. The initial
value function is given by a heuristic function, and the algorithm ends when the initial state
is solved. As with lrtdp, the labeling mechanism guarantees that πV is proper with respect
to s0 .
2.3 Algorithms for Real-Time Action Selection
The second class of algorithms do not attempt to solve the given MDP; they rather select
actions in real-time after a limited amount of processing without offering any guarantees
on the quality of the resulting policies. Algorithms in this group include an extension of
the Action Selection for Planning algorithm (asp) (Bonet, Loerincs, & Geffner, 1997) for
probabilistic domains, which is basically an rtdp algorithm with lookahead. Asp, like rtdp,
performs value function updates over states and so cannot get trapped into a loop. Thus,
although the policy delivered by asp is suboptimal, it is a proper policy; i.e. a policy that
is guaranteed to reach a goal state.

3. Heuristics
All these algorithms assume that the initial value function is given by a heuristic function
that provides good cost estimates, and in particular, lrtdp and hdp expect this heuristic
to be admissible. As described by Pearl (1983), informative admissible heuristics can be
obtained by solving suitable relaxations of the input problem. Two such relaxations are
supported in mGPT: the min-min relaxation, and the Strips relaxation. The first defines a
(deterministic) shortest-path problem in the original state space; the second is used to define
(deterministic) shortest-path problems in atom space.2 Thus, while the first is solved in
2. Atoms refer to the propositional symbols used in the representation language, ppddl in our case, to
define the problem. The number of atoms is polynomial in the size of the input, while the size of the
state space is, in general, exponential in the number of atoms.

935

Bonet & Geffner

time polynomial in the number of states, the shortest-path problems defined by the second
are solved in time polynomial in the number of atoms. Both methods yield lower bounds on
the expected cost to the goal from a given state, yet the bounds produced by the min-min
relaxation are stronger than those produced by the Strips relaxation.
3.1 Min-Min State Relaxation
The idea behind the min-min relaxation is to transform the input probabilistic problem,
described by its Bellman equations


X
def
∗
0
∗ 0
V (s) = min c(s, a) +
P r(s |s, a)V (s ) ,
(3)
a∈A(s)

s0 ∈S

into a deterministic shortest-path problem with Bellman equations of the form,
∗
Vmin
(s) =

def

∗
min c(s, a) + min {Vmin
(s0 ) : P (s0 |s, a) > 0} .

(4)

a∈A(s)

At the level of the representation language, the min-min relaxation is built by transforming each probabilistic operator of the form:
o = h ϕ, [ p1 : α1 , . . . , pn : αn ] i ,

(5)

where ϕ is the precondition of o and each αi is the ith probabilistic effect (with probability
pi ), into a set of independent and deterministic operators of the form:
oi = h ϕ, αi i ,

1 ≤ i ≤ n.

(6)

Thus, in the min-min relaxation one can actually choose the most convenient non-deterministic effect of an operator, and hence, the cost of the relaxation is a lower bound on the
expected cost of the original probabilistic problem.
The min-min relaxation is a deterministic problem that can be solved by means of
standard path-finding algorithms. For example, it can be solved with Dijkstra’s algorithm,
a*, ida*, or a deterministic version of lrtdp (i.e. a labeled lrta algorithm (Korf, 1990)).
mGPT provides two methods for computing the min-min heuristic from this relaxation:
min-min-ida*, which uses ida*, and min-min-lrtdp, which uses lrtdp. Both versions
are lazy in the sense that the heuristic values of states are computed as needed when the
planner requires them.
3.2 Strips Relaxation
The Strips relaxation in turn converts the deterministic problem obtained from the min-min
relaxation into a Strips problem, and then obtains lower bounds for the original MDP by
computing lower bounds for the resulting Strips problem using the methods developed in
classical planning (e.g., Bonet & Geffner, 2001; Haslum & Geffner, 2000; Hoffmann & Nebel,
2001; Edelkamp, 2001; Nguyen & Kambhampati, 2000). These methods run in polynomial
time in the number of atoms yet, unlike the min-min relaxation, require casting the minmin relaxation into Strips format, a conversion that, like the conversion of ADL into Strips
(Gazen & Knoblock, 1997), may require exponential time and space (see below).
936

mGPT: A Probabilistic Planner Based on Heuristic Search

In mGPT, the Strips relaxation is obtained directly from the original problem, by first
transforming the probabilistic operator into the form:
o = h prec, [ p1 : (add1 , del1 ), . . . , pn : (addn , deln ) ] i ,

(7)

where prec, addi , deli are conjunctions of literals that represents the precondition, the ith
add list, and the ith delete list of operator o respectively, and pi are probabilities that sum
to 1. In order to take the operators into form (7), all disjunctive preconditions, conditional
effects, and quantifiers are removed as described by Gazen and Knoblock (1997).
Once all operators have the form (7), the Strips relaxation is generated by splitting the
operators into n independent Strips operators of the form:
oi = h prec, addi , deli i ,

1 ≤ i ≤ n.

(8)

The following heuristics are implemented in mGPT upon the Strips relaxation. The
first two are lower bounds on the optimal cost of the Strips relaxation and hence on the
optimal (expected) cost of the original MDP, the third one is not necessarily a lower bound
on either cost.
• The hm heuristics (h-m) (Haslum & Geffner, 2000) are heuristics that recursively
approximate the cost of achieving a set of atoms C from the initial state by the cost
of achieving the most costly subset of size m in C. They are computed by a shortestpath algorithm over a graph with nodes standing for sets of at most m atoms, and
result in values hm (s) that estimate the cost of reaching a goal state from s. We use
the option h-m-k in mGPT to refer to the hm heuristic with m = k.
• Pattern database heuristics (patterndb) (Edelkamp, 2001) compute optimal costs of
relaxations of the Strips problem defined by some of the multi-valued variables that
are implicit in the problem (e.g. the location of a block in the blocksworld domain is
an implicit multi-valued variable whose possible values are either the table or the top
of any other block). This heuristic is also precomputed only once, at the beginning,
and provides a lower bound on the cost of an arbitrary state to the goal. A pattern
database is computed by projecting the Strips problem with respect to a set of atoms A
(those that define the multi-valued variables) and then solving the resulting problem
optimally with Dijkstra’s algorithm. Multiple pattern databases can be combined
either by taking max or sum. In the latter case, the pattern database is referred to
as additive.3 We use additive pattern databases as defined by Haslum, Bonet, and
Geffner (2005) where some constraints of the original problem are preserved into the
projection; something that often results in stronger heuristics. Patterndb-k refers to
a pattern database heuristic defined by k multi-valued variables.
• The FF (ff) heuristic implements the heuristic function used in the FF planner (Hoffmann & Nebel, 2001). It is computed by building the so-called relaxed planning graph
and finding a plan in it. The heuristic is then the number of operators in such a plan.
3. Some conditions are required for adding two pattern databases such that the result remains admissible.
A sufficient condition is that A ∩ B = ∅ if the sets A and B are those used to build the projections
respectively.

937

Bonet & Geffner

The relaxed planning graph is the version of the graph constructed by Graphplan
(Blum & Furst, 1997) when delete lists are ignored. It can be shown that computing
the ff heuristic can be done in polynomial time in the size of the input problem
(Hoffmann & Nebel, 2001). This heuristic however is informative but non-admissible.
As it is shown below, these heuristics can be plugged directly into the planning algorithm
or they can be used to compute more informative heuristics. For example, the patterndb
heuristic can be used within ida* to solve the min-min relaxation, which gives a stronger
heuristic than the patterndb heuristic. Thus, mGPT implements algorithms and heuristics
as stackable software components so that an element in the stack is used to solve the elements
above it.

4. Implementation
This section gives some details on the implementation of mGPT together with examples on
its use. The mGPT system is implemented in C++ upon a preliminary parser offered by
the organizers of ipc-4.
4.1 Hash Tables
Perhaps the most important component of modern search-based planners is the internal
representation of states and hash tables. Since mGPT uses different search algorithms and
hash tables to solve a given instance (e.g. when more informative heuristics are computed
from less informative ones), good internal representations and hash table implementation
are critical for good performance.
After grounding all atoms and operators, a state is represented by the ordered list of
the atoms that hold true in the state. A state s can appear associated with different
data in multiple hash tables simultaneously. Thus, instead of having multiples “copies” of
s, mGPT implements a system-wide state-hash-table that stores the representation of the
states referenced in all hash tables so that entries in such tables simply contain a reference
into the state-hash-table. In this way, the planner saves time and space.
Another issue that has large impact on performance is the average number of collisions
in each hash table. Two points are relevant for keeping the number of collisions low:
the hashing function and the size of the hash table. For the former, we have seen that
cryptographic hashing functions like md4 behave very well even though they are slower
than more traditional choices. For the latter, mGPT uses hash tables whose size is equal
to a large prime number (Cormen, Leiserson, & Rivest, 1990).
4.2 Algorithms and Heuristics
Each algorithm in mGPT is implemented as a subclass of the abstract algorithm class
whose members are a reference to a problem and, in some cases, a reference to a hash table
and a parameter . Similarly, each heuristic in mGPT is implemented as a subclass of the
abstract heuristic class whose members are a reference to a problem and a function that
maps states to non-negative values. Simple heuristics like the constant-zero function are
straightforward, others like min-min-lrtdp are implemented by a class whose members are,
in addition to above, references to a hash table and to an lrtdp algorithm.
938

mGPT: A Probabilistic Planner Based on Heuristic Search

4.3 Examples
The main parameters on a call to mGPT are “-a <algorithm>” that specifies the algorithm
to use, “-h <heuristic>” that specifies the heuristic function, and “-e <epsilon>” that
specifies the threshold  for the consistency check. A typical call looks like:
mGPT -a lrtdp -h h-m-1 -e .001 <domain> <problem>
which instructs mGPT to use the lrtdp algorithm with the h-m-1 heuristic and  = 0.001
over the domain and problem files specified.
The h-m-1 heuristic is admissible but very weak. The following example shows how to
compute the min-min-lrtdp heuristic using h-m-1 as the base heuristic:
mGPT -a lrtdp -h "h-m-1|min-min-lrtdp" -e .001 <domain> <problem>
The pipe symbol is used to instruct the planner how heuristics are to be computed using
other heuristics.
Another possibility is to use mGPT as a reactive planner in which decisions are taken
on-line with respect to a heuristic function that is improved over time. For example,
mGPT -a asp -h ff <domain> <problem>
uses the asp algorithm with the ff heuristic, while
mGPT -a asp -h "zero|min-min-ida*" <domain> <problem>
uses the asp algorithm with the min-min-ida* heuristic computed from the constant-zero
heuristic. Other combinations of algorithms and heuristics are possible. mGPT also accepts
parameters to control initial hash size, a weight on the heuristic function, values for dead-end
states, verbosity level, lookahead settings for asp, etc.

5. The Competition
The competition suite consisted of 7 probabilistic domains named blocksworld, explodingblocksworld, boxworld, fileworld, tireworld, towers-of-hanoise, and zeno. Blocksworld and
exploding-blocksworld are variations of the standard blocksworld domain for classical planning. Boxworld is a logistics-like transportation domain. Fileworld is a file/folder domain
where the uncertainty is only present at the initial situation where the destination of each
file is set. Tireworld and towers-of-hanoise are variations of the classical tireworld domain
and towers-of-hanoi. Zeno is a traveling domain with a fuel resource.
Some of the domains come in two variations: a goal-oriented version where the goal is to
be achieved with certainty while minimizing expected costs, and a reward-oriented version
that involves rewards. The mGPT planner handles the first type of tasks only.
In the competition we used the lrtdp algorithm with the patterndb-1 heuristic, a
parameter  = 0.001, and a weight W = 5 for the heuristic function. In some cases, when
the patterndb-1 heuristic was too poor, the planner switched automatically to the asp
algorithm with the ff heuristic.

939

Bonet & Geffner

problem name
blocksworld-5
blocksworld-8
blocksworld-11
blocksworld-15
blocksworld-18
blocksworld-21
exploding-bw
boxworld-c5-b10
boxworld-c10-b10
boxworld-c15-b10
fileworld-30-5
towers-of-hanoise
tireworld-g
tireworld-r
zeno

runs
30
30
30
30
—
—
—
30
—
—
30
—
30
30
30

failed
0
0
0
0
—
—
—
0
—
—
0
—
14
0
0

successful
30
30
30
30
—
—
—
30
—
—
30
—
16
30
30

time
43
60
130
7,706
—
—
—
6,370
—
—
2,220
—
48
39
162

reward
494.1
487.7
465.7
397.2
—
—
—
183.6
—
—
57.6
—
266.6
0
500

Table 1: Results for the mGPT planner over the competition problems. The table shows
problem name, number of runs, number of failed and successful runs (see text),
and time and reward averages. A dash means that mGPT was not able to solve
the problem. Times are in milliseconds.

5.1 Results
The competition was held through a client/server model. Each planner was evaluated in
each problem over a number of runs under supervision of the server. The planner initiated
the session by connecting to the server and then interacted with it by exchanging messages.
Each run consisted of actions sent by the planner whose effects were transmitted back from
the server to the planner. Thus, the current state of the problem was maintained both by
the planner and the server.
Table 1 shows the results for mGPT over the competition problems. For each problem,
30 runs were executed. The table shows the number of runs, the number of failed runs
(i.e. those that finished without reaching a goal state), the number of successful runs (i.e.
those that finished at goal states), and the time and reward averages per run.4 For the
blocksworld, the problem blocksworld-xx means a problem with xx blocks, for boxworld,
the problem boxworld-cxx-byy means a problem with xx cities and yy boxes.
As it can be seen from the table, mGPT did not solve exploding-bw, the larger instances
of blocksworld and boxworld, and it also failed on approximately half of the instances in
tireworld-g. The difficulties encountered by mGPT in solving these problems often had
not so much to do with the probabilities involved, but with the domains, and in particular,
with the encodings. The basic algorithms used by mGPT try to solve the problems by
4. The competition format was reward-based while our presentation here is cost-based. It is straightforward
to go from one format to the other.

940

mGPT: A Probabilistic Planner Based on Heuristic Search

computing a value function with -residuals over the relevant states (those reachable from
the initial state by an optimal policy). For this, mGPT computes an admissible heuristic
function by solving either the min-min relaxation, the Strips relaxation, or both. A problem
faced by this approach is that in many instances neither of these relaxations could be
solved. Here, we give a detailed explanation of the problems encountered by mGPT over
the different domains. It is worth noting that many of these difficulties would surface in
any Strips planner as well, even if the probabilities are ignored.
• Blocksworld and exploding blocksworld: the operator encodings have preconditions
containing universally-quantified negative literals, as the result of not using a ‘clear’
predicate. For example,
(:action pick-up-block-from
:parameters (?top - block ?bottom)
:precondition (and (not (= ?top ?bottom))
(forall (?b - block) (not (holding ?b)))
(on-top-of ?top ?bottom)
(forall (?b - block) (not (on-top-of ?b ?top))))
:effect (and (decrease (reward) 1)
(probabilistic
0.75 (and (holding ?top) (not (on-top-of ?top ?bottom)))
0.25 (when (not (= ?bottom table))
(and (not (on-top-of ?top ?bottom))
(on-top-of ?top table)))))
)

This complex encoding is not standard in planning and makes our atom-based heuristics almost useless. mGPT could solve the instances with 5, 8, 11 and 15 blocks but
not those with 18 and 21 blocks. For exploding blocksworld, mGPT was unable to
solve it as the parser is incomplete and does not parse some complex constructs.
• Boxworld: the encoding contains a ‘drive-truck’ operator that moves the truck to
its intended destination with probability 0.8 and to one of three “wrong destinations”
with probability 0.2/3 each. The encoding specifies the unintended effects by means
of nested conditional effects of the form
(:action drive-truck
:parameters (?t - truck ?src - city ?dst - city)
:precondition (and (truck-at-city ?t ?src) (can-drive ?src ?dst))
:effect (and (not (truck-at-city ?t ?src))
(probabilistic
0.2 (forall (?c1 - city)
(when (wrong-drive1 ?src ?c1)
(forall (?c2 - city)
(when (wrong-drive2 ?src ?c2)
(forall (?c3 - city)
(when (wrong-drive3 ?src ?c3)
(probabilistic
1/3 (truck-at-city ?t ?c1)
1/3 (truck-at-city ?t ?c2)
1/3 (truck-at-city ?t ?c3))))))))
0.8 (truck-at-city ?t ?dst)))
)
941

Bonet & Geffner

Our Strips relaxation, like any planner that converts ADL-style operators into Strips,
suffers an exponential blow up in this domain: with 10 cities, there are more than
a thousand operators for each grounded ADL-operator. This set included problems
with 5, 10 and 15 cities.
• Fileworld: in this domain, there are 30 files that need to be filed into one of 5 different
folders: the exact destination determined probabilistically. The optimal policy for this
problem, and any proper policy, must prescribe an action for more than 530 states, all
of them relevant. The consequence is a problem with millions of relevant states that
need to be stored into the hash table if the task is to compute a proper policy. The
patterndb-1 heuristic for this problem is not informative, as revealed by an analysis
of the values stored in the pattern database, and thus mGPT switched automatically
to the asp algorithm with the ff heuristic.
• Towers-of-hanoise: as in the blocksworld domain, the encoding is complex with operators that have disjunctions and universally-quantified negative literals in the preconditions, and complex conditional effects. Yet the problem that prevented mGPT from
solving any problem in this domain is a bug in the code that implements conditional
effects which did not surface in the other domains.
• Tireworld: there are two versions: a goal-based version called tireworld-g and a
reward-based version called tireworld-r. The domain contains multiple dead ends
at locations where the car gets a flat tire and no spare tire is available. Some of the
dead ends are unavoidable; i.e. there is no proper policy for this problem. All trials for
the reward-based version end successfully since there is no requirement to reach a goal
position, rather the objective is to maximize the accumulated reward. mGPT treated
both versions as goal-based problems as it does not deal directly with reward-based
problems.

6. Conclusions
The mGPT planner entered into the probabilistic planning competition combines heuristicsearch algorithms with methods for obtaining lower bounds from deterministic relaxations.
The results obtained at the competition were mixed with some of the difficulties having to do
with the selection of domains and encodings which do not match the capabilities of mGPT:
mGPT tries to compute proper solutions using heuristics derived from the Strips relaxations.
As we have described, some of the domains could not be solved due to the number of relevant
states, and others due to the complexity of the Strips relaxations themselves.
For the definition of good benchmarks for MDP solvers, it is crucial to define what
constitutes a solution and what is the bottom line for assessing performance. In classical
planning, for example, the solutions are plans and the bottom line is given by blind-search
algorithms; progress in the field can then be measured by the distance to this bottom line.
In the probabilistic setting, this is more difficult as it is not always clear what it means to
solve a problem. This, however, needs to be defined in some way, otherwise performance
comparisons are not meaningful. Indeed, in the classical setting, one no longer compares
optimal with non-optimal planners since both types of planners are very different: one
provides guarantees that apply to all solutions, while the other provides guarantees that

942

mGPT: A Probabilistic Planner Based on Heuristic Search

apply to one solution only. In the probabilistic setting this is even more subtle as there are
different types of guarantees. For example, if we restrict ourselves to the class of MDPs
that constitute the simplest generalization of the classical setting — the task of reaching
the goal with certainty while minimizing the expected number of steps from a given initial
state s0 — there are methods that yield solutions (policies) that ensure that the goal will be
reached with certainty in a finite number of steps (not necessarily optimal), and methods
with no such guarantees. Both types of methods are necessary in practice, yet it is crucial
to make a distinction among them and to identify useful benchmarks in each class. For
methods that yield optimal policies, or at least policies with finite expected costs, standard
dynamic programming methods like value iteration provide a useful bottom-line reference
for assessing performance. In any case, we believe that useful benchmarks need to be defined
taking into account the types of tasks that the various algorithms aim to solve, and the
types of guarantees, if any, that they provide in their solutions.
GPT and mGPT are available for download at http://www.ldc.usb.ve/∼bonet.
Acknowledgements
mGPT was built upon a parser developed by John Asmuth from Rutgers University and
Håkan Younes from Carnegie Mellon University. We also thank David E. Smith for comments that helped us to improve this note.

References
Barto, A., Bradtke, S., & Singh, S. (1995). Learning to act using real-time dynamic programming. Artificial Intelligence, 72, 81–138.
Bertsekas, D. (1995). Dynamic Programming and Optimal Control, (2 Vols). Athena Scientific.
Blum, A., & Furst, M. (1997). Fast planning through planning graph analysis. Artificial
Intelligence, 90, 281–300.
Bonet, B., & Geffner, H. (2000). Planning with incomplete information as heuristic search
in belief space. In Chien, S., Kambhampati, S., & Knoblock, C. (Eds.), Proc. 6th
International Conf. on Artificial Intelligence Planning and Scheduling, pp. 52–61,
Breckenridge, CO. AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning as heuristic search. Artificial Intelligence, 129 (1–
2), 5–33.
Bonet, B., & Geffner, H. (2003a). Faster heuristic search algorithms for planning with
uncertainty and full feedback. In Gottlob, G. (Ed.), Proc. 18th International Joint
Conf. on Artificial Intelligence, pp. 1233–1238, Acapulco, Mexico. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2003b). Labeled RTDP: Improving the convergence of real-time
dynamic programming. In Giunchiglia, E., Muscettola, N., & Nau, D. (Eds.), Proc.
13th International Conf. on Automated Planning and Scheduling, pp. 12–21, Trento,
Italy. AAAI Press.

943

Bonet & Geffner

Bonet, B., Loerincs, G., & Geffner, H. (1997). A robust and fast action selection mechanism
for planning. In Kuipers, B., & Webber, B. (Eds.), Proc. 14th National Conf. on
Artificial Intelligence, pp. 714–719, Providence, RI. AAAI Press / MIT Press.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction to Algorithms. MIT Press.
Edelkamp, S. (2001). Planning with pattern databases. In Cesta, A. (Ed.), Proc. 6th
European Conf. on Planning, pp. 13–24, Toledo, Spain. Springer: LNCS.
Gazen, B., & Knoblock, C. (1997). Combining the expressiveness of UCPOP with the
efficiency of Graphplan. In Steel, S., & Alami, R. (Eds.), Proc. 4th European Conf.
on Planning, pp. 221–233, Toulouse, France. Springer: LNCS.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics for domainindependent planning. In Veloso, M., & Kambhampati, S. (Eds.), Proc. 20 National
Conf. on Artificial Intelligence, pp. 1163–1168, Pittsburgh, PA. AAAI Press / MIT
Press.
Haslum, P., & Geffner, H. (2000). Admissible heuristic for optimal planning. In Chien, S.,
Kambhampati, S., & Knoblock, C. (Eds.), Proc. 6th International Conf. on Artificial
Intelligence Planning and Scheduling, pp. 140–149, Breckenridge, CO. AAAI Press.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42 (2–3), 189–211.
Nguyen, X., & Kambhampati, S. (2000). Extracting effective and admissible state-space
heuristics from the planning graph. In Kautz, H., & Porter, B. (Eds.), Proc. 17th
National Conf. on Artificial Intelligence, pp. 798–805, Austin, TX. AAAI Press /
MIT Press.
Pearl, J. (1983). Heuristics. Morgan Kaufmann.
Tarjan, R. E. (1972). Depth first search and linear graph algorithms. SIAM Journal on
Computing, 1 (2), 146–160.

944

Journal of Artificial Intelligence Research 24 (2005) 195-220

Submitted 11/04; published 08/05

Perseus: Randomized Point-based
Value Iteration for POMDPs
Matthijs T. J. Spaan
Nikos Vlassis

mtjspaan@science.uva.nl
vlassis@science.uva.nl

Informatics Institute, University of Amsterdam
Kruislaan 403, 1098 SJ Amsterdam, The Netherlands

Abstract
Partially observable Markov decision processes (POMDPs) form an attractive and principled framework for agent planning under uncertainty. Point-based approximate techniques for POMDPs compute a policy based on a finite set of points collected in advance
from the agent’s belief space. We present a randomized point-based value iteration algorithm called Perseus. The algorithm performs approximate value backup stages, ensuring
that in each backup stage the value of each point in the belief set is improved; the key
observation is that a single backup may improve the value of many belief points. Contrary
to other point-based methods, Perseus backs up only a (randomly selected) subset of
points in the belief set, sufficient for improving the value of each belief point in the set.
We show how the same idea can be extended to dealing with continuous action spaces.
Experimental results show the potential of Perseus in large scale POMDP problems.

1. Introduction
A major goal of Artificial Intelligence is to build intelligent agents (Russell & Norvig, 2003).
An intelligent agent, whether physical or simulated, should be able to autonomously perform
a given task, and is often characterized by its sense–think–act loop: it uses sensors to observe
the environment, considers this information to decide what to do, and executes the chosen
action. The agent influences its environment by acting and can detect the effect of its actions
by sensing: the environment closes the loop. In this work we are interested in computing a
plan that maps sensory input to the optimal action to execute for a given task. We consider
types of domains in which an agent is uncertain about the exact consequence of its actions.
Furthermore, it cannot determine with full certainty the state of the environment with a
single sensor reading, i.e., the environment is only partially observable to the agent.
Planning under these kinds of uncertainty is a challenging problem as it requires reasoning over all possible futures given all possible histories. Partially observable Markov decision
processes (POMDPs) provide a rich mathematical framework for acting optimally in such
partially observable and stochastic environments (Dynkin, 1965; Åström, 1965; Aoki, 1965;
Sondik, 1971; Lovejoy, 1991; Kaelbling, Littman, & Cassandra, 1998). The POMDP defines a sensor model specifying the probability of observing a particular sensor reading in
a specific state and a stochastic transition model which captures the uncertain outcome of
executing an action. The agent’s task is defined by the reward it receives at each time step
and its goal is to maximize the discounted cumulative reward. Assuming discrete models,
the POMDP framework allows for capturing all uncertainty introduced by the transition
and observation model by defining and operating on the belief state of an agent. A belief
c
2005
AI Access Foundation. All rights reserved.

Spaan & Vlassis

state is a probability distribution over all states and summarizes all information regarding
the past.
The use of belief states allows one to transform the original discrete state POMDP
into a continuous state Markov decision process (MDP), which in turn can be solved by
corresponding MDP techniques (Bertsekas & Tsitsiklis, 1996). However, the optimal value
function in a POMDP exhibits particular structure (it is piecewise linear and convex) that
one can exploit in order to facilitate the solving. Value iteration, for instance, is a method
for solving POMDPs that builds a sequence of value function estimates which converge
to the optimal value function for the current task (Sondik, 1971). The value function is
parameterized by a finite number of hyperplanes, or vectors, over the belief space, which
partition the belief space in a finite amount of regions. Each vector maximizes the value
function in a certain region and has an action associated with it, which is the optimal action
to take for beliefs in its region. Computing the next value function estimate—looking one
step deeper into the future—requires taking into account all possible actions the agent
can take and all subsequent observations it may receive. Unfortunately, this leads to an
exponential growth of vectors with the planning horizon. Many of the computed vectors
will be useless in the sense that their maximizing region is empty, but identifying and
subsequently pruning them is an expensive operation.
Exact value iteration algorithms (Sondik, 1971; Cheng, 1988; Kaelbling et al., 1998)
search in each value iteration step the complete belief simplex for a minimal set of belief
points that generate the necessary set of vectors for the next horizon value function. This
typically requires linear programming and is therefore costly in high dimensions. Zhang
and Zhang (2001) argued that value iteration still converges to the optimal value function if
exact value iteration steps are interleaved with approximate value iteration steps in which
the new value function is an upper bound to the previously computed value function. This
results in a speedup of the total algorithm, however, linear programming is again needed
in order to ensure that the new value function is an upper bound to the previous one
over the complete belief simplex. In general, computing exact solutions for POMDPs is an
intractable problem (Papadimitriou & Tsitsiklis, 1987; Madani, Hanks, & Condon, 1999),
calling for approximate solution techniques (Lovejoy, 1991; Hauskrecht, 2000).
In practical tasks one would like to compute solutions only for those parts of the belief
simplex that are reachable, i.e., that can be actually encountered by interacting with the
environment. This has recently motivated the use of approximate solution techniques which
focus on the use of a sampled set of belief points on which planning is performed (Hauskrecht,
2000; Poon, 2001; Roy & Gordon, 2003; Pineau, Gordon, & Thrun, 2003; Spaan & Vlassis,
2004), a possibility already mentioned by Lovejoy (1991). The idea is that instead of
planning over the complete belief space of the agent (which is intractable for large state
spaces), planning is carried out only on a limited set of prototype beliefs that have been
sampled by letting the agent interact (randomly) with the environment. PBVI (Pineau
et al., 2003), for instance, builds successive estimates of the value function by updating the
value and its gradient only at the points of a (dynamically growing) belief set.
In this work we describe Perseus, a randomized point-based value iteration algorithm
for POMDPs (Vlassis & Spaan, 2004; Spaan & Vlassis, 2004). Perseus operates on a large
set of beliefs which are gathered by simulating random interactions of the agent with the
POMDP environment. On this belief set a number of value backup stages are performed.
196

Perseus: Randomized Point-based Value Iteration for POMDPs

The algorithm ensures that in each backup stage the value of each point in the belief set is
improved (or at least does not decrease). Contrary to other point-based methods, Perseus
backs up only a random subset of belief points; the key observation is that a single backup
may improve the value of many points in the set. This allows us to compute value functions
that consist of only a small number of vectors (relative to the belief set size), leading to
significant speedups. We evaluate the performance of Perseus on benchmark problems
from literature, and show that it is very competitive to other methods in terms of solution
quality and computation time.
Furthermore, we extend Perseus to compute plans for agents which have a continuous
(or very large discrete) set of actions at their disposal (Spaan & Vlassis, 2005). Examples
include navigating to an arbitrary location, or rotating a pan-and-tilt camera at any desired
angle. Most work on POMDP solution techniques targets discrete action spaces; exceptions
include the application of a particle filter to a continuous state and action space (Thrun,
2000) and certain policy search methods (Ng & Jordan, 2000; Baxter & Bartlett, 2001).
We report on experiments in a domain in which an agent equipped with proximity sensors
can move at a continuous heading and distance, and we present experimental results from
a navigation task involving a mobile robot with omnidirectional vision in a perceptually
aliased office environment.
The remainder of the paper is structured as follows: in Section 2 we review the POMDP
framework from an AI perspective, and we discuss exact methods for solving POMDPs
and their tractability problems. Next, we outline a class of approximate value iteration
algorithms, the so-called point-based techniques. In Section 3 we describe and discuss the
Perseus algorithm, as well as the extension to continuous action spaces. Related work
on approximate techniques for POMDP planning is discussed in Section 4. We present
experimental results from several problem domains in Section 5. Finally, we wrap up with
some conclusions in Section 6.

2. Partially Observable Markov Decision Processes
A partially observable Markov decision process (POMDP) models the repeated interaction
of an agent with a stochastic environment, parts of which are hidden from the agent’s view.
The agent’s goal is to perform a task by choosing actions which fulfill the task best. Stated
otherwise, the agent has to compute a plan that optimizes the given performance measure.
We assume that time is discretized in time steps of equal length, and at the start of each
step the agent has to execute an action. At each time step the agent also receives a scalar
reward from the environment, and the performance measure directs the agent to maximize
the cumulative reward it can gather. The reward signal allows one to define a task for the
agent, e.g., one can give the agent a large positive reward when it accomplishes a certain
goal and a small negative reward for each action leading up to it. In this way the agent is
steered toward finding the plan which will let it accomplish its goal as fast as possible.
The POMDP framework models stochastic environments in which an agent is uncertain
about the exact effect of executing a certain action. This uncertainty is captured by a probabilistic transition model as is the case in a fully observable Markov decision process (MDP)
(Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996). An MDP defines a transition model
which specifies the probabilistic effect of how each action changes the state. Extending
197

Spaan & Vlassis

the MDP setting, a POMDP also deals with uncertainty resulting from the agent’s imperfect sensors. It allows for planning in environments which are only partially observable to
the agent, i.e., environments in which the agent cannot determine with full certainty the
true state of the environment. In general the partial observability stems from two sources:
(1) multiple states give the same sensor reading, in case the agent can only sense a limited
part of the environment, and (2) its sensor readings are noisy: observing the same state can
result in different sensor readings. The partial observability can lead to “perceptual aliasing”: different parts of the environment appear similar to the agent’s sensor system, but
require different actions. The POMDP captures the partial observability by a probabilistic
observation model, which relates possible observations to states.
More formally, a POMDP assumes that at any time step the environment is in a state
s ∈ S, the agent takes an action a ∈ A and receives a reward r(s, a) from the environment
as a result of this action, while the environment switches to a new state s0 according to
a known stochastic transition model p(s0 |s, a). The Markov property entails that s0 only
depends on the previous state s and the action a. The agent then perceives an observation
o ∈ O, that may be conditional on its action, which provides information about the state s0
through a known stochastic observation model p(o|s, a). All sets S, O, and A are assumed
discrete and finite here (but we will generalize to continuous A in Section 3.3).
In order for an agent to choose its actions successfully in partially observable environments some form of memory is needed, as the observations the agent receives do not provide
an unique identification of s. Given the transition and observation model the POMDP can
be transformed to a belief-state MDP: the agent summarizes all information about its past
using a belief vector b(s). The belief b is a probability distribution over S, which forms a
Markovian signal for the planning task. All beliefs are contained in a (|S| − 1)-dimensional
simplex ∆, which means we can represent a belief using |S| − 1 numbers. Each POMDP
problem assumes an initial belief b0 , which for instance can be set to a uniform distribution
over all states (representing complete ignorance regarding the initial state of the environment). Every time the agent takes an action a and observes o, its belief is updated by
Bayes’ rule:
p(o|s0 , a) X
p(s0 |s, a)b(s),
(1)
boa (s0 ) =
p(o|a, b)
s∈S
P
P
where p(o|a, b) = s0 ∈S p(o|s0 , a) s∈S p(s0 |s, a)b(s) is a normalizing constant.
As we discussed above, the goal of the agent is to choose actions which fulfill its task
as well as possible, i.e., to compute an optimal plan. Such a plan is called a policy π(b)
and maps beliefs to actions. Note that, contrary to MDPs, the policy π(b) is a function
over a continuous set of probability distributions over S. A policy π can be characterized
by a value function V π : ∆ → R which is defined as the expected future discounted reward
V π (b) the agent can gather by following π starting from belief b:
V π (b) = Eπ

∞
hX
t=0


i

γ t r(bt , π(bt ))b0 = b ,

(2)

P
where r(bt , π(bt )) = s∈S r(s, π(bt ))bt (s), and γ is a discount rate, 0 ≤ γ < 1. The discount
rate ensures a finite sum and is usually chosen close to 1. A policy π which maximizes V π
is called an optimal policy π ∗ ; it specifies for each b the optimal action to execute at the
198

Perseus: Randomized Point-based Value Iteration for POMDPs

current step, assuming the agent will also act optimally at future time steps. The value of
an optimal policy π ∗ is defined by the optimal value function V ∗ , that satisfies the Bellman
optimality equation V ∗ = HV ∗ :
∗

V (b) = max
a∈A

hX

r(s, a)b(s) + γ

s∈S

X

p(o|a, b)V

∗

i

(boa )

,

(3)

o∈O

with boa given by (1), and H is the Bellman backup operator (Bellman, 1957). When (3)
holds for every b ∈ ∆ we are ensured the solution is optimal.
V ∗ can be approximated by iterating a number of stages, as we will see in the next
section, at each stage considering a step further into the future. For problems with a finite
planning horizon V ∗ will be piecewise linear and convex (PWLC) (Smallwood & Sondik,
1973), and for infinite horizon tasks V ∗ can be approximated arbitrary well by a PWLC
value function. We parameterize a value function Vn at stage n by a finite set of vectors
(hyperplanes) {αni }, i = 1, . . . , |Vn |. Additionally, with each vector an action a(αni ) ∈ A
is associated, which is the optimal one to take in the current step. Each vector defines a
region in the belief space for which this vector is the maximizing element of Vn . These
regions form a partition of the belief space, induced by the piecewise linearity of the value
function. Examples of a value function for a two state POMDP are shown in Fig. 1(a) and
|Vn |
at stage n, the value of a belief b is given by
1(d). Given a set of vectors {αni }i=1
Vn (b) = max b · αni ,
{αin }i

(4)

where (·) denotes inner product. The gradient of the value function at b is given by the
vector αnb = arg max{αin }i b · αni , and the policy at b is given by π(b) = a(αnb ).
2.1 Exact Value Iteration
Computing an optimal plan for an agent means solving the POMDP, and a classical method
is value iteration (Puterman, 1994). In the POMDP framework, value iteration involves
approximating V ∗ by applying the exact dynamic programming operator H above, or some
approximate operator H̃, to an initially piecewise linear and convex value function V0 . For
H, and for many commonly used H̃, the produced intermediate estimates V1 , V2 , . . . will
also be piecewise linear and convex. The main idea behind many value iteration algorithms
for POMDPs is that for a given value function Vn and a particular belief point b we can
b
easily compute the vector αn+1
of HVn such that
i
b
,
αn+1
= arg max b · αn+1

(5)

{αin+1 }i

|HV |

i
where {αn+1
}i=1 n is the (unknown) set of vectors for HVn . We will denote this operation
b
αn+1 = backup(b). It computes the optimal vector for a given belief b by back-projecting
all vectors in the current horizon value function one step from the future and returning the
vector that maximizes the value of b. In particular, defining ra (s) = r(s, a) and using (1),

199

Spaan & Vlassis

(3), and (4) we have:
h
i
X
Vn+1 (b) = max b · ra + γ
p(o|a, b)Vn (boa )
a

(6)

o

i
h
X
X
boa (s0 )αni (s0 )
p(o|a, b) max
= max b · ra + γ

(7)

h
i
X
X
X
= max b · ra + γ
max
p(o|s0 , a)
p(s0 |s, a)b(s)αni (s0 )

(8)

h
i
X
i
= max b · ra + γ
max b · ga,o
,

(9)

a

{αin }i

o

a

o

a

o

{αin }i

s0

s

s0

i }
{ga,o
i

where
i
ga,o
(s) =

X

p(o|s0 , a)p(s0 |s, a)αni (s0 ).

(10)

s0

Applying the identity maxj b · αj = b · arg maxj b · αj in (9) twice, we can compute the vector
backup(b) as follows:
backup(b) = arg max b · gab ,

where

(11)

{gab }a∈A

gab = ra + γ

X
o

i
.
arg max b · ga,o

(12)

i }
{ga,o
i

Although computing the vector backup(b) for a given b is straightforward, locating the
(minimal) set of points b required to compute all vectors ∪b backup(b) of HVn is very costly.
As each b has a region in the belief space in which its αnb is maximal, a family of algorithms
tries to identify these regions (Sondik, 1971; Cheng, 1988; Kaelbling et al., 1998). The
corresponding b of each region is called a “witness” point, as it testifies to the existence
of its region. Another set of exact POMDP value iteration algorithms do not focus on
searching in the belief space, but instead consider enumerating all possible vectors of HVn
and then pruning useless vectors (Monahan, 1982; Cassandra, Littman, & Zhang, 1997).
As an example of exact value iteration let us consider the most straightforward way of
computing HVn due to Monahan (1982). This involves calculating all possible ways HVn
could be constructed, exploiting the known structure of the value function. We operate
independent of a particular b now so (12) can no longer be applied. Instead we have to
i for all o:
include all ways of selecting ga,o
HVn =

[

Ga , with Ga =

a

o
Mn 1
i
ra + γga,o
,
|O|
i
o

(13)

L
where
denotes the cross-sum operator.1 Unfortunately, at each stage a number of vectors
exponential in |O| are generated: |A||Vn ||O| . The regions of many of the generated vectors
will be empty and these vectors are useless as such, but identifying and subsequently pruning
them requires linear programming which introduces considerable additional cost (e.g., when
the state space is large).
1. Cross-sum of sets is defined as:

L

k

Rk = R1 ⊕ R2 ⊕ . . . ⊕ Rk , with P ⊕ Q = { p + q | p ∈ P, q ∈ Q }.

200

Perseus: Randomized Point-based Value Iteration for POMDPs

Zhang and Zhang (2001) proposed an alternative approach to exact value iteration,
designed to speed up each exact value iteration step. It turns out that value iteration still
converges to the optimal value function if exact value update steps are interleaved with
approximate update steps in which a new value function Vn+1 is computed from Vn such
that
Vn (b) ≤ Vn+1 (b) ≤ HVn (b),
for all b ∈ ∆.
(14)
This additionally requires that the value function is appropriately initialized, by choosing
1
mins,a r(s, a). Such a vector
V0 to be a single vector with all its components equal to 1−γ
represents the minimum of cumulative discounted reward obtainable in the POMDP, and is
guaranteed to be below V ∗ . Zhang and Zhang (2001) compute Vn+1 by backing up witness
points of Vn for a number of steps. As we saw above, backing up a set of belief points is
a relatively cheap operation. Thus, given Vn , a number of vectors of HVn are created by
applying backup to the witness points of Vn , and then a set of linear programs are solved
to ensure that Vn+1 (b) ≥ Vn (b), ∀b ∈ ∆. This is repeated for a number of steps, before
an exact value update step takes place. The authors demonstrate experimentally that a
combination of approximate and exact backup steps can speed up exact value iteration.
In general, however, computing optimal planning solutions for POMDPs is an intractable
problem for any reasonably sized task (Papadimitriou & Tsitsiklis, 1987; Madani et al.,
1999). This calls for approximate solution techniques. We will describe next a recent line
of research on approximate POMDP algorithms which focus on planning on a fixed set of
belief points.
2.2 Approximate Value Iteration
The major cause of intractability of exact POMDP solution methods is their aim of computing the optimal action for every possible belief point in ∆. For instance, if we use (13)
we end up with a series of value functions whose size grows exponentially in the planning
horizon. A natural way to sidestep this intractability is to settle for computing an approximate solution by considering only a finite set of belief points. The backup stage reduces to
applying (11) a fixed number of times, resulting in a small number of vectors (bounded by
the size of the belief set). The motivation for using approximate methods is their ability
to compute successful policies for much larger problems, which compensates for the loss of
optimality.
Such approximate POMDP value iteration methods operating on a fixed set of points are
explored by Lovejoy (1991) and in subsequent works (Hauskrecht, 2000; Poon, 2001; Pineau
et al., 2003; Spaan & Vlassis, 2004). Pineau et al. (2003) for instance, use an approximate
backup operator H̃PBVI instead of H, that computes in each value backup stage the set
[
H̃PBVI Vn =
backup(b)
(15)
b∈B

using a fixed set of belief points B. The general assumption underlying these so-called
point-based methods is that by updating not only the value but also its gradient (the α
vector) at each b ∈ B, the resulting policy will generalize well and be effective for beliefs
outside the set B. Whether or not this assumption is realistic depends on the POMDP’s
structure and the contents of B, but the intuition is that in many problems the set of
201

Spaan & Vlassis

‘reachable’ beliefs (reachable by following an arbitrary policy starting from b0 ) forms a low
dimensional manifold in the belief simplex, and thus can be covered densely enough by a
relatively small number of belief points.
Crucial to the control quality of the computed approximate solution is the makeup of B.
A number of schemes to build B have been proposed. For instance, one could use a regular
grid on the belief simplex, computed, e.g., by Freudenthal triangulation (Lovejoy, 1991).
Other options include taking all extreme points of the belief simplex or use a random grid
(Hauskrecht, 2000; Poon, 2001). An alternative scheme is to include belief points that
can be encountered by simulating the POMDP: we can generate trajectories through the
belief space by sampling random actions and observations at each time step (Lovejoy, 1991;
Hauskrecht, 2000; Poon, 2001; Pineau et al., 2003; Spaan & Vlassis, 2004). This sampling
scheme focuses the contents of B to be beliefs that can actually be encountered while
experiencing the POMDP model.
The PBVI algorithm (Pineau et al., 2003) is an instance of such a point-based POMDP
algorithm. PBVI starts by selecting a small set of beliefs B0 , performs a number of backup
stages (15) on B0 , expands B0 to B1 by sampling more beliefs, performs again a series
of backups, and repeats this process until a satisfactory solution has been found (or the
allowed computation time expires). The set Bt+1 grows by simulating actions for every
b ∈ Bt , maintaining only the new belief points that are furthest away from all other points
already in Bt+1 . This scheme is a heuristic to let Bt cover a wide area of the belief space,
but comes at a cost as it requires computing distances between all b ∈ Bt . By backing up
all b ∈ Bt the PBVI algorithm generates at each stage approximately |Bt | vectors, which
can lead to slow performance in domains requiring large Bt .
In the next section we will present a point-based POMDP value iteration method which
does not require backing up all b ∈ B. We compute backups for a subset of B only, but
seeing to it that the computed solution will be effective for the complete set B. As a result
we limit the growth of the number of vectors in the successive value function estimates,
leading to significant speedups.

3. Randomized Point-based Backup Stages
We have introduced the POMDP framework which models agents inhabiting stochastic
environments that are partially observable to them, and discussed exact and approximate
methods for computing successful plans for such agents. Below we describe Perseus, an
approximate solution method capable of computing competitive solutions in large POMDP
domains.
3.1 Perseus
Perseus is an approximate point-based value iteration algorithm for POMDPs (Vlassis &
Spaan, 2004; Spaan & Vlassis, 2004). The value update scheme of Perseus implements
a randomized approximate backup operator H̃Perseus that increases (or at least does not
decrease) the value of all belief points in B. Such an operator can be very efficiently
implemented in POMDPs given the shape of the value function. The key idea is that in
each value backup stage we can improve the value of all points in the belief set by only
updating the value and its gradient of a (randomly selected) subset of the points. In each
202

Perseus: Randomized Point-based Value Iteration for POMDPs

backup stage, given a value function Vn , we compute a value function Vn+1 that improves
the value of all b ∈ B, i.e., we build a value function Vn+1 = H̃Perseus Vn that upper bounds
Vn over B (but not necessarily over ∆ which would require linear programming):
Vn (b) ≤ Vn+1 (b),

for all b ∈ B.

(16)

We first let the agent randomly explore the environment and collect a set B of reachable
belief points, which remains fixed throughout the complete algorithm. We initialize the value
1
mins,a r(s, a) (Zhang &
function V0 as a single vector with all its components equal to 1−γ
Zhang, 2001). Starting with V0 , Perseus performs a number of backup stages until some
convergence criterion is met. Each backup stage is defined as follows (where B̃ is an auxiliary
set containing the non-improved points):
Perseus backup stage: Vn+1 = H̃Perseus Vn
1. Set Vn+1 = ∅. Initialize B̃ to B.
2. Sample a belief point b uniformly at random from B̃ and compute α = backup(b).
3. If b · α ≥ Vn (b) then add α to Vn+1 , otherwise add α0 = arg max{αin }i b · αni to Vn+1 .
4. Compute B̃ = {b ∈ B : Vn+1 (b) < Vn (b)}.
5. If B̃ = ∅ then stop, else go to 2.
Often, a small number of vectors will be sufficient to improve Vn (b) ∀b ∈ B, especially
in the first steps of value iteration. The idea is to compute these vectors in a randomized
greedy manner by sampling from B̃, an increasingly smaller subset of B. We keep track
of the set of non-improved points B̃ consisting of those b ∈ B whose new value Vn+1 (b) is
still lower than Vn (b). At the start of each backup stage, Vn+1 is set to ∅ which means B̃
is initialized to B, indicating that all b ∈ B still need to be improved in this backup stage.
As long as B̃ is not empty, we sample a point b from B̃ and compute α = backup(b). If
α improves the value of b (i.e., if b · α ≥ Vn (b) in step 3), we add α to Vn+1 and update
Vn+1 (b) for all b ∈ B by computing their inner product with the new α. The hope is that
α improves the value of many other points in B, and all these points are removed from B̃.
As long as B̃ is not empty we sample belief points from it and add their α vectors.
To ensure termination of each backup stage we have to enforce that B̃ shrinks when
adding vectors, i.e., that each α actually improves at least the value of the b that generated
it. If not (i.e., b · α < Vn (b) in step 3), we ignore α and insert a copy of the maximizing
vector of b from Vn in Vn+1 . Point b is now considered improved and is removed from B̃
in step 4, together with any other belief points which had the same vector as maximizing
one in Vn . This procedure ensures that B̃ shrinks and the backup stage will terminate. A
pictorial example of a backup stage is presented in Fig. 1.
Perseus performs backup stages until some convergence criterion is met. For pointbased methods several convergence criteria can be considered, one could for instance bound
the difference between successive value function estimates maxb∈B (Vn+1 (b) − Vn (b)). Another option would be to track the number of policy changes: the number of b ∈ B which
had a different optimal action in Vn compared to Vn+1 (Lovejoy, 1991).
203

Spaan & Vlassis

replacemen
Vn

(1, 0)

Vn+1

b1 b2 b3 b4 b5

b6 b7 (0, 1)

(1, 0)

(0, 1)

b6
(b)

(a)

Vn+1

Vn+1

(1, 0)

(1, 0)

(0, 1)

(0, 1)

b3
(c)

(d)

Figure 1: Example of a Perseus backup stage in a two state POMDP. The belief space is
depicted on the x-axis and the y-axis represents V (b). Solid lines are αni vectors
i
from the current stage n and dashed lines are αn−1
vectors from the previous
stage. We operate on a B of 7 beliefs, indicated by the tick marks. The backup
stage computing Vn+1 from Vn proceeds as follows: (a) value function at stage n;
(b) start computing Vn+1 by sampling b6 , add α = backup(b6 ) to Vn+1 which
improves the value of b6 and b7 ; (c) sample b3 from {b1 , . . . , b5 }, add backup(b3 )
to Vn+1 which improves b1 through b5 ; and (d) the value of all b ∈ B has improved,
the backup stage is finished.

3.2 Discussion
The key observation underlying the Perseus algorithm is that when a belief b is backed
up, the resulting vector improves not only V (b) but often also the value of many other
belief points in B. This results in value functions with a relatively small number of vectors
(as compared to, e.g., Poon, 2001; Pineau et al., 2003). Experiments show indeed that
the number of vectors grows modestly with the number of backup stages (|Vn |  |B|).
In practice this means that we can afford to use a much larger B than other point-based
methods, which has a positive effect on the approximation accuracy as dictated by the
bounds of Pineau et al. (2003). Furthermore, compared with other methods that build
the set B based on various heuristics (Pineau et al., 2003; Smith & Simmons, 2004), our
build-up of B is cheap as it only requires sampling random trajectories starting from b0 .
Moreover, duplicate entries in B will only affect the probability that a particular b will be
sampled in the value update stages, but not the size of Vn .
204

Perseus: Randomized Point-based Value Iteration for POMDPs

An alternative to using a single fixed set B that is collected by following a fixed policy
at the beginning of the algorithm, would be to resample a new Bt after every t-th backup
stage (or at fixed intervals) by following the most recent policy. Such an approach could
be justified by the fact that an agent executing an optimal policy will most probably visit
only a (small) subset of the beliefs in B. We have not tested how such a scheme would
affect the solution quality of Perseus and what trade-offs we can achieve for the additional
computational cost of sampling multiple sets B. We note that similar ‘off-policy’ learning
using a fixed set of sampled states has also been adopted by other recent algorithms like
LSPI (Lagoudakis & Parr, 2003) and PSDP (Bagnell, Kakade, Ng, & Schneider, 2004).
The backups of Perseus on a fixed set B can be viewed as a particular instance of
asynchronous dynamic programming (Bertsekas & Tsitsiklis, 1989). In asynchronous dynamic programming algorithms no full sweeps over the state space are made, but the order
in which states are backed up is arbitrary. This allows an algorithm to focus on backups
which may have a high potential impact, as for instance in the prioritized sweeping algorithm for solving fully observable MDPs (Moore & Atkeson, 1993; Andre, Friedman, &
Parr, 1998). A drawback is that the notion of an exact planning horizon is somewhat lost:
in general, after performing n backup stages the computed plan will not be considering n
steps into the future, but less. By backing up non-improved belief points asynchronously
Perseus focuses on interesting regions of the (reachable) belief space, and by sampling at
random ensures that eventually all b ∈ B will be taken into account. As we ensure that the
value of a particular belief point never decreases, we are guaranteed that Perseus will converge: the proof only requires observing that every added vector is always below V ∗ (Poon,
2001; Vlassis & Spaan, 2004). Moreover, as we explained above, Perseus can handle large
belief sets B, thus obviating the use of dynamic belief point selection strategies like those
proposed by Hauskrecht (2000), Poon (2001), and Pineau et al. (2003). Note that the only
parameter to be set by the user is the size of B; however, the complexity of the resulting
policy seems to be only mildly dependent on the size of B.
An interesting issue is how many new vectors are generated in each backup stage of
Perseus, and how this may affect the speed of convergence of the algorithm. In general,
the smaller the size |Vn | of a value function, the faster the backups (since the backup operator
has linear dependence on |Vn |). On the other hand, two consecutive value functions may
differ arbitrarily in size—and we have observed cases where the new value function has
fewer vectors than the old value function—which makes it hard to derive bounds on the
speed of convergence of Perseus and complicates the analysis of the involved trade-offs.
We have mainly identified two cases where only a small number of new vectors are added
to a value function. The first case is during the initial backup stages, and when V0 has
been initialized very low (e.g., for large γ and large negative immediate reward). In this
case a single vector may improve all points, for a number of backup stages, until the value
function has reached some sufficient level. The second case is near convergence, when the
value function has almost converged in certain regions of the belief space. Sampling a belief
point in such a region will result in a (near) copy of the old vector. Whereas the former
case provides evidence that the value function has been initialized too low (and adding a
single vector is an efficient way to ‘correct’ this), the latter case may be viewed as providing
evidence for the convergence of Perseus.
205

Spaan & Vlassis

3.3 Extension to Very Large or Continuous Action Spaces
An attractive feature of Perseus is that it can be naturally extended to very large or
continuous action spaces, due to the ‘improve–only’ principle of its backup stage. Note
that the backup operator in (11) involves a maximization over all actions in A. When the
action space A is finite and small, one can cache in advance the transition, observation,
and reward models for all a ∈ A, and therefore achieve an optimized implementation of the
backup operator. For very large or continuous action spaces, the full maximization over
actions in (11) is clearly infeasible, and one has to resort to sampling-based techniques. The
idea here is to replace the full maximization over actions with a sampled max operator that
performs the maximization over a random subset of A (Szepesvári & Littman, 1996). This
also means that one has to compute the above models ‘on the fly’ for each sampled action,
which requires an algorithm (a parameterized model family) that can generate all needed
models for any action that is given as input. Such generated models can be cached for later
use in case the same action is considered again in future iterations (see the experimental
section for using these so-called ‘old’ actions).
The use of such a sampled max operator is very well suited for the backup scheme of
Perseus in which we only require that the values of belief points do not decrease over two
consecutive backup stages. In particular, we can replace the backup operator in (11) with
a new backup operator α = backup0 defined as follows (Spaan & Vlassis, 2005):
backup0 (b) = arg max b · gab ,

(17)

{gab }a∈A0

b

where A0b is a random set of actions drawn from A, and gab is defined in (12). For each
sampled action a ∈ A0b we generate the POMDP models on the fly as mentioned above, and
from these models we compute the required vectors gab to be used in backup0 .
The backup0 operator can now simply replace the backup operator in step 2 of Section 3.1. As in the full maximization case, we need to check in step 3 whether any of the
vectors generated by the actions in A0b improves the value of the particular belief point.
If not, we keep the old vector with its associated action that was selected in a previous
backup stage. Concerning the sample complexity of the backup0 operator, we can derive
simple bounds that involve the number of actions drawn and the probability to find a ‘good’
action from A (good in terms of value improvement of b). We can easily show that with
probability at least 1 − δ, the best action among n = |A0b | actions selected uniformly at
random from A is among the best  fraction of all actions from A, if n ≥ dlog δ/ log(1 − )e.
In practice various sampling schemes are possible, which vary in the way A0b is constructed. We have identified a number of proposal distributions from which to sample
actions: (1) uniform from A, (2) a Gaussian distribution centered on the best known action
for the particular b, i.e., a(αnb ), and (3) a Dirac distribution on a(αnb ). The latter two take
into account the policy computed so far by focusing on the current action associated with
the input belief b (as recorded in Vn ), while sampling uniformly at random uses no such
knowledge. Actions sampled uniformly at random can be viewed as exploring actions, while
the other two distributions are exploiting current knowledge. As we can select the makeup
of A0b , we can choose any combination of the distributions mentioned above, allowing us to
explore and exploit at the same time. In our experiments (see Section 5.2) we implement
the backup0 operator using a number of different combinations and analyze their effects.
206

Perseus: Randomized Point-based Value Iteration for POMDPs

4. Related Work
In Section 2.2 we reported on a class of approximate solution techniques for POMDPs that
focus on computing a value function approximation based on a fixed set of prototype belief
points. Here we will broaden the picture to other approximate POMDP solution methods.
A related overview is provided by Hauskrecht (2000).
A few heuristic control strategies have been proposed which rely on a solution of the
underlying MDP. A popular technique is QMDP (Littman, Cassandra, & Kaelbling, 1995),
a simple approximation technique that treats the POMDP as if it were fully observable
and solves the MDP, e.g., using value iteration.
The resulting Q(s, a) values are used to
P
define a control policy by π(b) = arg maxa s b(s)Q(s, a). QMDP can be very effective in
some domains, but the policies it computes will not take informative actions, as the QMDP
solution assumes that any uncertainty regarding the state will disappear after taking one
action. As such, QMDP policies will fail in domains where repeated information gathering is
necessary.
One way to sidestep the intractability of exact POMDP value iteration is to grid the
belief simplex, using either a fixed grid (Lovejoy, 1991; Bonet, 2002) or a variable grid
(Brafman, 1997; Zhou & Hansen, 2001). Value backups are performed for every grid point,
but only the value of each grid point is preserved and the gradient is ignored. The value of
non-grid points is defined by an interpolation rule. The grid based methods differ mainly on
how the grid points are selected and what shape the interpolation function takes. In general,
regular grids do not scale well in problems with high dimensionality and non-regular grids
suffer from expensive interpolation routines.
An alternative to computing an (approximate) value function is policy search: these
methods search for a good policy within a restricted class of controllers. For instance, policy iteration (Hansen, 1998b) and bounded policy iteration (BPI) (Poupart & Boutilier,
2004) search through the space of (bounded-size) stochastic finite state controllers by performing policy iteration steps. Other options for searching the policy space include gradient
ascent (Meuleau, Kim, Kaelbling, & Cassandra, 1999; Kearns, Mansour, & Ng, 2000; Ng
& Jordan, 2000; Baxter & Bartlett, 2001; Aberdeen & Baxter, 2002) and heuristic methods like stochastic local search (Braziunas & Boutilier, 2004). In particular, the Pegasus
method (Ng & Jordan, 2000) estimates the value of a policy by simulating a (bounded)
number of trajectories from the POMDP using a fixed random seed, and then takes steps in
the policy space in order to maximize this value. Policy search methods have demonstrated
success in several cases, but searching in the policy space can often be difficult and prone
to local optima.
Another approach for solving POMDPs is based on heuristic search (Satia & Lave, 1973;
Hansen, 1998a; Smith & Simmons, 2004). Defining an initial belief b0 as the root node, these
methods build a tree that branches over (a, o) pairs, each of which recursively induces a new
belief node. These methods bear a similarity to Perseus since they also focus on reachable
beliefs from b0 . However, they differ in the way belief points are selected to back up; in the
above methods branch and bound techniques are used to maintain upper and lower bounds
to the expected return at fringe nodes in the search tree. Hansen (1998a) proposes a policy
iteration method that represents a policy as a finite state controller, and which uses the
belief tree to focus the search on areas of the belief space where the controller can most
207

Spaan & Vlassis

Name

|S|

|O|

|A|

Tiger-grid
Hallway
Hallway2
Tag
Continuous navigation
cTRC

33
57
89
870
200
200

17
21
17
30
16
10

5
5
5
5
∞
∞

Table 1: Characteristics of problem domains.
likely be improved. However, its applicability to large problems is limited by its use of full
dynamic programming updates. HSVI (Smith & Simmons, 2004) is an approximate value
iteration technique that performs a heuristic search through the belief space for beliefs at
which to update the bounds, similar to work by Satia and Lave (1973). An alternative
recent approach to maintaining uncertainty estimates of an approximate value function is
based on Gaussian Processes (Tuttle & Ghahramani, 2004).
Compression techniques can be applied to large POMDPs to reduce the dimensionality
of the belief space, facilitating the computation of an approximate solution. Roy, Gordon,
and Thrun (2005) apply Exponential family PCA to a sample set of beliefs to find a lowdimensional representation, based on which an approximate solution is sought. Such a
non-linear compression can be very effective, but requires learning a reward and transition
model in the reduced space. After such a model is learned, one can compute an approximate
solution for the original POMDP using, e.g., MDP value iteration. Alternatively linear
compression techniques can be used which preserve the shape of value function (Poupart
& Boutilier, 2003). Such a property is desirable as it allows one to exploit the existing
POMDP machinery. For instance, linear compression has been applied as a preprocessing
step for BPI (Poupart & Boutilier, 2005) as well as Perseus (Poupart, 2005).
The literature on POMDPs with continuous actions is still relatively sparse (Thrun,
2000; Ng & Jordan, 2000; Baxter & Bartlett, 2001). Thrun (2000) applies real-time dynamic
programming on a POMDP with a continuous state and action space. In that work beliefs
are represented by sets of samples drawn from the state space, while Q(b, a) values are
approximated by nearest-neighbor interpolation from a (growing) set of prototype values
and are updated by on-line exploration and the use of sampling-based Bellman backups.
Pegasus can also handle continuous action spaces, at the cost of a sample complexity that
is polynomial in the size of the state space (Theorem 3, Ng & Jordan, 2000).

5. Experiments
We will show experimental results applying Perseus on benchmark problems from the
POMDP literature, and present two POMDP domains for testing Perseus in problems
with continuous action spaces. Table 5 summarizes these domains in terms of the size of
S, O and A. Each belief set was gathered by simulating trajectories of interactions of the
agent with the POMDP environment starting at a random state sampled from b0 , and at
208

Perseus: Randomized Point-based Value Iteration for POMDPs

each time step the agent picked an action uniformly at random. In all domains the discount
factor γ was set to 0.95.
5.1 Discrete Action Spaces
The Hallway, Hallway2 and Tiger-grid problems (introduced by Littman et al., 1995) are
maze domains that have been commonly used to test scalable POMDP solution techniques
(Littman et al., 1995; Brafman, 1997; Zhou & Hansen, 2001; Pineau et al., 2003; Smith &
Simmons, 2004; Spaan & Vlassis, 2004; Poupart, 2005). The Tag domain (Pineau et al.,
2003) is an order of magnitude larger than the first three problems, and is a recent benchmark problem (Pineau et al., 2003; Smith & Simmons, 2004; Braziunas & Boutilier, 2004;
Poupart & Boutilier, 2004; Spaan & Vlassis, 2004; Poupart, 2005).
5.1.1 Benchmark Mazes
Littman et al. (1995) introduced three benchmark maze domains: Tiger-grid, Hallway,
and Hallway2. All of them are navigation tasks: the objective for an agent is to reach a
designated goal state as quickly as possible. The agent observes each possible combination
of the presence of a wall in four directions plus a unique observation indicating the goal
state; in the Hallway problem three other landmarks are also available. At each step the
agent can take one out of five actions: {stay in place, move forward, turn right, turn
left, turn around}. Both the transition and the observation model are noisy. Table 2(a)
through (c) compares the performance of Perseus to other algorithms. For each problem
we sampled a set B of 1,000 beliefs, and executed Perseus 10 times for each problem using
different random seeds. The average expected discounted reward R is computed from 1,000
trajectories starting from random states (drawn according to b0 ) for each of the 10 Perseus
runs, and following the computed policy. The reported reward R is the average over these
10,000 trajectories. Perseus reaches competitive control quality using a small number of
vectors resulting in a considerable speedup.2
5.1.2 Tag
The goal in the Tag domain, described by Pineau et al. (2003), is for a robot to search
for a moving opponent robot and tag it. The chasing robot cannot observe the opponent
until they occupy the same position, at which time it should execute the tag action in order
to win the game, and receive a reward of 10. If the opponent is not present at the same
location, the reward will be −10, and the robot is penalized with a −1 reward for each
motion action it takes. The opponent tries to escape from being tagged by moving away
of the chasing robot, however, it has a 0.2 probability of remaining at its location. The
chasing and opponent robot both start at a random location. The chasing robot has perfect
information regarding its own position and its movement actions {north, east, south, west}
are deterministic. The state space is represented as the cross-product of the states of the
two robots. Both robots can be located in one of the 29 positions depicted in Fig. 2(a), and
the opponent can also be in a tagged state, resulting in a total of 870 states. Tag is a rather
2. Perseus and QMDP results (in Section 5.1) were computed in Matlab on an Intel Pentium IV 2.4 GHz;
other results were obtained on different platforms, so time comparisons are rough.

209

Spaan & Vlassis

V

reward

#

O

C

10

0

(a) State space.

10

1

10

2

10

time (s)

3

−4
−6
−8
−10
−12
−14
−16
−18
−20
10

0

(b) Value.

10

1

10

2

time (s)

10

3

(c) Reward.

2
8000
6000

10

1

∆π

# of vectors

10000

10

4000
2000

10

0

10

0

10

1

2

10
time (s)

10

0

3

10

(d) Nr. of vectors.

0

10

1

10

2

time (s)

10

3

(e) Policy changes.

Figure 2: Tag: (a) state space with chasing and opponent robot; (b)–(e) performance of
Perseus.

large benchmark problem compared to other POMDP problems studied in literature, but
it exhibits a sparse structure. We applied Perseus to a belief set B of 10,000 points.
In Fig. 2(b)–(e) we show the performance of Perseus averaged over 10 runs, where
error bars indicate standard deviation within these runs. To evaluate the computed policies
we tested each of them on 10 trajectories (of at most 100 steps) times 100 starting positions
(sampled
from the starting belief b0 ). Fig. 2(b) displays the value as estimated on B,
P
V
(b);
(c) the expected discounted reward averaged over the 1,000 trajectories; (d)
b∈B
the number of vectors in the value function estimate, |{αni }|; and (e) the number of policy
changes: the number of b ∈ B which had a different optimal action in Vn−1 compared to Vn .
The latter can be regarded as a measure of convergence for point-based solution methods
(Lovejoy, 1991). We can see that in almost all experiments Perseus reaches solutions of
virtually equal quality and size.
Table 2(d) compares the performance of Perseus with other state-of-the-art methods.
The results show that in the Tag problem Perseus displays better control quality than
any other method and computes its solution an order of magnitude faster than most other
methods. Specifically, its solution computed on |B| = 10,000 beliefs consists of only 280
vectors, much less than PBVI which maintains a vector for each of its 1334 b ∈ B. This
indicates that the randomized backup stage of Perseus is justified: it takes advantage of
a large B while the size of the value function grows moderately with the planning horizon,
leading to significant speedups. It is interesting to compare the two variations of BPI, with
bias (w/b) (Poupart, 2005) or without (n/b) (Poupart & Boutilier, 2004). The bias focuses
210

Perseus: Randomized Point-based Value Iteration for POMDPs

R

|π|

T

2.35
2.34
2.30
2.25
2.22
0.94
0.23

4860
134
660
470
120
174
n.a.

10341
104
12116
3448
1000
n.a.
2.76

Tiger-grid
HSVI
Perseus
PBUA
PBVI
BPI w/b
Grid
QMDP

Hallway

R

|π|

T

PBVI
PBUA
HSVI
Perseus
BPI w/b
QMDP

0.53
0.53
0.52
0.51
0.51
0.27

86
300
1341
55
43
n.a.

288
450
10836
35
185
1.34

(b) Results for Hallway.

(a) Results for Tiger-grid.

Hallway2

R

|π|

T

Perseus
HSVI
PBUA
PBVI
BPI w/b
QMDP

0.35
0.35
0.35
0.34
0.32
0.09

56
1571
1840
95
60
n.a.

10
10010
27898
360
790
2.23

Tag
Perseus
HSVI
BPI w/b
BBSLS
BPI n/b
PBVI
QMDP

(c) Results for Hallway2.

R

|π|

T

−6.17
−6.37
−6.65
≈ −8.3
−9.18
−9.18
−16.9

280
1657
17
30
940
1334
n.a.

1670
10113
250
105
59772
180880
16.1

(d) Results for Tag.

Table 2: Experimental comparisons of Perseus with other algorithms. Perseus results
are averaged over 10 runs. Each table lists the method, the average expected
discounted reward R, the size of the solution |π| (value function or controller
size), and the time T (in seconds) used to compute the solution. Sources: PBVI
(Pineau et al., 2003), BPI no bias (Poupart & Boutilier, 2004), BPI with bias
(Poupart, 2005), HSVI (Smith & Simmons, 2004), Grid (Brafman, 1997), PBUA
(Poon, 2001), and BBSLS (Braziunas & Boutilier, 2004) (approximate, read from
figure).

on the reachable belief space by incorporating the initial belief which dramatically increases
its performance in solution size and computation time, but it does not reach the control
quality of Perseus.
5.2 Continuous Action Spaces
We applied Perseus in two domains with continuous action spaces: an agent equipped
with proximity sensors moving at a continuous heading and distance, and a navigation
task involving a mobile robot with omnidirectional vision in a perceptually aliased office
environment.
211

Spaan & Vlassis

(a) Continuous Navigation: state space.

(b) cTRC: example image.

(c) cTRC: environment.

Figure 3: Continuous action space domains: the points indicate the states, F depicts the
goal state. (a) Environment of the Continuous Navigation problem: the black
square represents the agent, the four beams indicate the range of its proximity
sensors. (b) cTRC Problem: panoramic image corresponding to a prototype
feature vector ok ∈ O, and (c) its induced p(s|ok ). The darker the dot, the higher
the probability.

5.2.1 Continuous Navigation
We first tested our approach on a navigation task in a simulated environment, in which an
agent can move at a continuous heading and distance. The Continuous Navigation environment represents a 20×10m hallway which is highly perceptually aliased (see Fig. 3(a)). The
agent inhabiting the hallway is equipped with four proximity sensors, each observing one
compass direction. We assume that a proximity sensor can only detect whether there is a
wall within its range of 2m or not, resulting in a total number of 16 possible sensor readings.
The agent’s sensor system is noisy: with 0.9 probability the correct wall configuration is
observed, otherwise one of the other 15 observations is returned with equal probability. The
task is to reach a goal location located in an open area where there are no walls near enough
for the agent to detect. The agent is initialized at a random state in the environment, and
it should learn what movement actions to take in order to reach the goal as fast as possible.
As Perseus assumes a finite and discrete state space S (the set of all possible locations
of the agent) we need to discretize this space; we performed a simple k-means clustering
on a random subset of all possible locations, resulting in a grid of 200 locations depicted
in Fig. 3(a). The agent’s actions are defined by two parameters: the heading θ to which
the agent turns and the distance d it intends to move in this direction. Executing an
action transports it according to a Gaussian distribution centered on the expected resulting
position, which is defined as its current (x, y) position translated d meter in the direction
θ. The standard deviation of the Gaussian transition model is 0.25d I, which means the
further the agent wants to travel, the more uncertainty there will be regarding its resulting
212

Perseus: Randomized Point-based Value Iteration for POMDPs

position. The distance parameter d is limited to the interval [0, 2]m and the heading θ
ranges on [0, 2π]. Each movement is penalized with a reward of −0.1 per step and the
reward obtainable at the goal location is 10.
To test the feasibility of Perseus in continuous action spaces, i.e., whether it can
compute successful policies by sampling actions at random, we experimented with a number
of different sampling schemes for the backup0 operator. Each scheme is defined by the
old
makeup of A0b = {AU , AN
b , Ab }, which is composed of samples from three distributions:
U
N
A : uniformly at random; Ab : a Gaussian distribution centered on the best known action
a(αnb ) for b so far, with standard deviation σθ = π5 for θ and σd = 0.1 for d; and Aold
b : a Dirac
0
distribution on the best known action. We will describe Ab by the number of samples from
old
each distribution {|AU |, |AN
b |, |Ab |}. We tested the following schemes: sampling a single
action uniformly at random {1, 0, 0}, or from a Gaussian distribution on a(αnb ) {0, 1, 0};
adding a(αnb ) to both schemes resulting in {1, 0, 1} and {0, 1, 1}; and {k, k, 1}, sampling k
actions from the uniform and Gaussian distributions and including the old action. The latter
scheme explores the option of sampling more than one action from a particular distribution,
and we tested k = {1, 3, 10}. The option to try the best known (‘old’) action for the
particular b is relatively cheap as we can cache its transition, observation, and reward
model the first time it is chosen (at a previous backup stage).
In this problem we used a set B of 10,000 belief points. To evaluate the control quality
of the computed value functions we collected rewards by sampling 10 trajectories from 100
random starting locations at particular time intervals, while following the policy computed
so far. Each trajectory was stopped after a maximum of 100 steps (if the agent had not
reached the goal by then), and the collected reward was properly discounted. All results
are averaged over 10 runs of Perseus with a different random seed and are computed in
Matlab on an Intel Xeon 3.4GHz.
Fig. 4 shows the results for each of the sampling schemes mentioned above. The top row
displays the control quality as indicated by the average discounted reward. In Fig. 4(a) we
can see that just sampling a single action uniformly at random {1, 0, 0} already gives good
performance, while extending A0b to include the best known action {1, 0, 1} improves control
quality. The Gaussian sampling schemes {0, 1, 0} and {0, 1, 1} learn slower as they can take
only small steps in action space. An additional disadvantage of Gaussian sampling is the
need for the user to specify the standard deviation. Fig. 4(b) depicts the control quality
of the schemes in which we sample from three distributions {k, k, 1}, for different values of
k. The figure shows that all tested variations reach similar control quality, but trying more
actions for a particular b can slow down learning. However, when looking at the size of the
value function (Fig. 4(c)–(d)), we see that for k = 10 the resulting value function is smaller
than for any other scheme tested. It appears in this experiment that sampling more actions
increases the chance of finding a high quality action that generalizes well (so fewer vectors
are eventually needed to reach the same control quality), but at a higher computational
cost per backup stage. Note that for all tested schemes the number of vectors in the value
function remains two orders of magnitude lower than the size of B (10,000 belief points),
confirming the efficient behavior of the Perseus randomized backup scheme.
To obtain more insight in the effect of sampling from different distributions in A0b , we
computed the relative frequency of occurrence of the maximizing action in AU , AN
b , and
0 we check whether the vector computed using the returned
Aold
.
When
executing
a
backup
b
213

7

7

6

6

5

5

4

4
reward

reward

Spaan & Vlassis

3

3
2

2
1

1

Uniform
Uniform/Old
Gauss
Gauss/Old

0
−1 1
10

2

3

10

−1 1
10

4

10

Uniform/Gauss/Old
Uniform*3/Gauss*3/Old
Uniform*10/Gauss*10/Old

0

10

2

350

350

300

300

250

250

# of vectors

# of vectors

400

200
150
100
50
3

200
150

50
0 1
10

4

10

Uniform/Gauss/Old
Uniform*3/Gauss*3/Old
Uniform*10/Gauss*10/Old

100

Uniform
Uniform/Old
Gauss
Gauss/Old
2

10

2

10

(d) Number of vectors.

1

1

0.8

0.8
Origin of best action

Origin of best action

4

10
time (s)

(c) Number of vectors.

Improved: Uniform
Improved: Old
Not improved

0.4

3

10

time (s)

0.6

10

(b) Reward.

400

10

4

10
time (s)

(a) Reward.

0 1
10

3

10

time (s)

0.2

0.6

Improved: Uniform
Improved: Gauss
Improved: Old
Not improved

0.4

0.2

0 0
10

1

10

2

10
time (s)

3

10

0 0
10

4

10

(e) Origin of maximizing action.

1

10

2

10
time (s)

3

10

4

10

(f) Origin of maximizing action.

Figure 4: Perseus results on the Continuous Navigation problem, averaged over 10 runs.
old
The left column shows the performance of {|AU |, |AN
b |, |Ab |} = {{1, 0, 0},
{1, 0, 1}, {0, 1, 0}, {0, 1, 1}}, and the right column displays {k, k, 1} for k =
{1, 3, 10}. The top row figures display the average discounted reward obtained
vs. computation time, the figures in the middle row show the size of the value
function, and the bottom row details the origin of the maximizing vector (see
main text).

214

Perseus: Randomized Point-based Value Iteration for POMDPs

V
3

2
1.5
Discrete 4
Discrete 8
Discrete 16
Uniform*3/Old

1

2

10

3

10
time (s)

(a) Reward.

4

10

Origin of best action

Discrete 4
Discrete 8
Discrete 16
Uniform*3/Old

2000
# of vectors

reward

2.5

0.5 1
10

1

2500

1500
1000
500
0 1
10

2

10

3

10
time (s)

(b) Number of vectors.

4

10

0.8
0.6

Improved: Uniform
Improved: Old
Not improved

0.4
0.2
0 1
10

2

10

3

10
time (s)

4

10

(c) Origin of best action.

Figure 5: Performance of Perseus in cTRC domain, averaged over 10 runs.
action actually improves V (b), and if so, we record whether this action originated from AU ,
old
AN
b , or Ab . For every backup stage we normalize these counts with respect to the total
number of backups in that backup stage (including those that did not improve V (b)). The
resulting frequencies are plotted on the bottom row of Fig. 4 for two sampling schemes:
sampling uniform and old {1, 0, 1} (Fig. 4(e)) and sampling one action from all three distributions {1, 1, 1} (Fig. 4(f)). We can see that over time the relative frequency of the best
known action grows (“Improved: Old”), while the number of instances in which none of the
sampled actions improves V (b) drops to almost zero (“Not improved”). The frequencies
of actions sampled from an uniform or Gaussian distribution (“Improved: Uniform” resp.
“Improved: Gauss”) resulting in the best action in A0b (and improving V (b)) also drop.
These observations confirm the intuition that by sampling actions at random Perseus can
effectively explore the action space (which is advantageous at the beginning of the algorithm), while as time progresses the algorithm seems to be able to exploit the actions that
turn out to be useful.
5.2.2 Arbitrary Heading Navigation
To evaluate Perseus with continuous actions on a more realistic problem and compare
with discretized action spaces we also include the cTRC domain. In this problem (adapted
from Spaan & Vlassis, 2005) a mobile robot with omnidirectional vision has to navigate in
a highly perceptually aliased office environment (see Fig. 3(b) and (c)). We use the MEMORABLE3 robot database that contains a set of approximately 8000 panoramic images
collected manually by driving the robot around in a 17 × 17 meters office environment.
The robot can decide to move 5m in an arbitrary direction, i.e., its actions are parameterized by its heading ranging on [0, 2π]. We applied the same technique as in the Continuous
Navigation domain to grid our state space in 200 states (Fig. 3(c)) and assume a Gaussian
error on the resulting position. For our observation model we compressed the images with
PCA and applied k-means clustering to create 10 three-dimensional prototype feature vectors {o1 , . . . , o10 }. Fig. 3(c) shows the inverse observation model p(s|o) for one observation,
and Fig. 3(b) displays the image in the database closest to this particular prototype obser3. The MEMORABLE database has been provided by the Tsukuba Research Center in Japan, for the Real
World Computing project.

215

Spaan & Vlassis

vation. The task is to reach a certain goal state at which a reward of 10 can be obtained;
each action yields a reward of −0.1. The belief set B contained 10,000 belief points.
We compared the continuous action extension of Perseus to three discretized versions
of this problem, in which we applied regular Perseus to a fixed discrete action set of 4,
8 or 16 headings with equal separation (offset with a random angle to prevent any bias).
old
Fig. 5 displays results for Perseus with {|AU |, |AN
b |, |Ab |} = {3, 0, 1} (other schemes
turned out to give similar results), and the three discrete action spaces. Fig. 5(a) shows
that sampling from a continuous A results in the same control quality as in the discrete 16
version, but it needs more time to reach it (as the backup0 requires to generate transition,
observation and reward models on the fly). As the discrete cases benefit from an optimized
implementation (we can cache all transition, observation and reward models) the continuous
action scheme needs some computation time to match performance or outperform them.
However, when employing the continuous scheme, Perseus exploits the ability to move at
an arbitrary heading to find a better policy than the discrete 4 and 8 cases. We see that
providing the robot with a more fine-grained action space leads to better control quality,
and in this problem a discretization of 16 headings appears to be fine-grained enough for
good control performance. Fig. 5(b) plots the number of vectors in the value function for
each scheme, where we see that for reaching the same control quality the continuous and
discrete 16 version need a similar amount of vectors. Fig. 5(c) shows the relative frequency
of occurrence of the maximizing action in AU or Aold
b , as detailed in Section 5.2.1. As in
Fig. 4(e)–(f) we see that over time the best known action is exploited, while the frequency
of instances in which no sampled action improves the value of b is diminished to near zero.

6. Conclusions
The partially observable Markov decision process (POMDP) framework provides an attractive and principled model for sequential decision making under uncertainty. It models the
interaction between an agent and the stochastic environment it inhabits. A POMDP assumes that the agent has imperfect information: parts of the environment are hidden from
the agent’s sensors. The goal is to compute a plan that allows the agent to act optimally
given uncertainty in sensory input and the uncertain effect of executing an action. Unfortunately, the expressiveness of POMDPs is counterbalanced by the intractability of computing
exact solutions, which calls for efficient approximate solution techniques. In this work we
considered a recent line of research on approximate point-based POMDP algorithms that
plan on a sampled set of belief points.
We presented Perseus, a randomized point-based value iteration algorithm for planning in POMDPs. Perseus operates on a large belief set sampled by simulating random
trajectories through belief space. Approximate value iteration is performed on this belief set
by applying a number of backup stages, ensuring that in each backup stage the value of each
point in the belief set is improved; the key observation is that a single backup may improve
the value of many belief points. Contrary to other point-based methods, Perseus backs
up only a (randomly selected) subset of points in the belief set, sufficient for improving the
value of each belief point in the set. Experiments confirm that this allows us to compute
value functions that consist of only a small number of vectors (relative to the belief set
size), leading to significant speedups. We performed experiments in benchmark problems
216

Perseus: Randomized Point-based Value Iteration for POMDPs

from literature, and Perseus turns out to be very competitive to other methods in terms of
solution quality and computation time. We extended Perseus to compute plans for agents
which have a very large or continuous set of actions at their disposal, by sampling actions
from the action space. We demonstrated the viability of Perseus on two POMDP problems with continuous action spaces: a continuous navigation task and a robotic problem
involving a mobile robot with omnidirectional vision. We analyzed a number of different
action sampling schemes and compared with discretized action spaces.
Perseus has been recently extended to deal with structured state spaces (Poupart, 2005;
Boger, Poupart, Hoey, Boutilier, Fernie, & Mihailidis, 2005), continuous observation spaces
(Hoey & Poupart, 2005), and continuous state spaces (Porta, Spaan, & Vlassis, 2005). As
future work we would like to explore alternative compact representations (Guestrin, Koller,
& Parr, 2001; Theocharous, Murphy, & Kaelbling, 2004), as well as applying Perseus to
cooperative multiagent domains, extending recent approaches (Emery-Montemerlo, Gordon,
Schneider, & Thrun, 2004; Becker, Zilberstein, Lesser, & Goldman, 2004; Paquet, Tobin, &
Chaib-draa, 2005).

Acknowledgments
We would like to thank Bruno Scherrer, Geoff Gordon, Pascal Poupart, and the anonymous
reviewers for their comments. This research is supported by PROGRESS, the embedded
systems research program of the Dutch organization for Scientific Research NWO, the Dutch
Ministry of Economic Affairs and the Technology Foundation STW, project AES 5414.

References
Aberdeen, D., & Baxter, J. (2002). Scaling internal-state policy-gradient methods for
POMDPs. In International Conference on Machine Learning, Sydney, Australia.
Andre, D., Friedman, N., & Parr, R. (1998). Generalized prioritized sweeping. In Advances
in Neural Information Processing Systems 10. MIT Press.
Aoki, M. (1965). Optimal control of partially observable Markovian systems. Journal of
The Franklin Institute, 280 (5), 367–386.
Åström, K. J. (1965). Optimal control of Markov processes with incomplete state information. Journal of Mathematical Analysis and Applications, 10, 174–205.
Bagnell, J. A., Kakade, S., Ng, A. Y., & Schneider, J. (2004). Policy search by dynamic
programming. In Advances in Neural Information Processing Systems 16. MIT Press.
Baxter, J., & Bartlett, P. (2001). Infinite-horizon policy-gradient estimation. Journal of
Artificial Intelligence Research, 15, 319–350.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. (2004). Solving transition independent
decentralized Markov decision processes. Journal of Artificial Intelligence Research,
22, 423–455.
Bellman, R. (1957). Dynamic programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Parallel and Distributed Computation: Numerical
Methods. Prentice-Hall.
217

Spaan & Vlassis

Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Boger, J., Poupart, P., Hoey, J., Boutilier, C., Fernie, G., & Mihailidis, A. (2005). A decisiontheoretic approach to task assistance for persons with dementia. In Proc. Int. Joint
Conf. on Artificial Intelligence.
Bonet, B. (2002). An epsilon-optimal grid-based algorithm for partially observable Markov
decision processes. In International Conference on Machine Learning, pp. 51–58,
Sydney, Australia. Morgan Kaufmann.
Brafman, R. I. (1997). A heuristic variable grid solution method for POMDPs. In Proc. of
the National Conference on Artificial Intelligence.
Braziunas, D., & Boutilier, C. (2004). Stochastic local search for POMDP controllers. In
Proc. of the National Conference on Artificial Intelligence, San Jose, CA.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: A simple,
fast, exact method for partially observable Markov decision processes. In Proc. of
Uncertainty in Artificial Intelligence, Providence, Rhode Island.
Cheng, H. T. (1988). Algorithms for partially observable Markov decision processes. Ph.D.
thesis, University of British Columbia.
Dynkin, E. B. (1965). Controlled random sequences. Theory of probability and its applications, 10 (1), 1–14.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions for partially observable stochastic games with common payoffs. In Proc. of Int.
Joint Conference on Autonomous Agents and Multi Agent Systems.
Guestrin, C., Koller, D., & Parr, R. (2001). Solving factored POMDPs with linear value
functions. In IJCAI-01 workshop on Planning under Uncertainty and Incomplete
Information.
Hansen, E. A. (1998a). Finite-memory control of partially observable systems. Ph.D. thesis,
University of Massachusetts, Amherst.
Hansen, E. A. (1998b). Solving POMDPs by searching in policy space. In Proc. of Uncertainty in Artificial Intelligence, pp. 211–219.
Hauskrecht, M. (2000). Value function approximations for partially observable Markov
decision processes. Journal of Artificial Intelligence Research, 13, 33–95.
Hoey, J., & Poupart, P. (2005). Solving POMDPs with continuous or large discrete observation spaces. In Proc. Int. Joint Conf. on Artificial Intelligence.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in
partially observable stochastic domains. Artificial Intelligence, 101, 99–134.
Kearns, M., Mansour, Y., & Ng, A. Y. (2000). Approximate planning in large POMDPs
via reusable trajectories. In Advances in Neural Information Processing Systems 12.
MIT Press.
Lagoudakis, M. G., & Parr, R. (2003). Least-squares policy iteration. Journal of Machine
Learning Research, 4, 1107–1149.
218

Perseus: Randomized Point-based Value Iteration for POMDPs

Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies for partially observable environments: Scaling up. In International Conference on Machine
Learning, San Francisco, CA.
Lovejoy, W. S. (1991). Computationally feasible bounds for partially observed Markov
decision processes. Operations Research, 39 (1), 162–175.
Madani, O., Hanks, S., & Condon, A. (1999). On the undecidability of probabilistic planning
and infinite-horizon partially observable Markov decision problems. In Proc. of the
National Conference on Artificial Intelligence, Orlando, Florida.
Meuleau, N., Kim, K.-E., Kaelbling, L. P., & Cassandra, A. R. (1999). Solving POMDPs by
searching the space of finite policies. In Proc. of Uncertainty in Artificial Intelligence.
Monahan, G. E. (1982). A survey of partially observable Markov decision processes: theory,
models and algorithms. Management Science, 28 (1).
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with
less data and less time. Machine Learning, 13, 103–130.
Ng, A. Y., & Jordan, M. (2000). PEGASUS: A policy search method for large MDPs and
POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of Markov decision processes. Mathematics of operations research, 12 (3), 441–450.
Paquet, S., Tobin, L., & Chaib-draa, B. (2005). An online POMDP algorithm for complex
multiagent environments. In Proc. of Int. Joint Conference on Autonomous Agents
and Multi Agent Systems.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: An anytime
algorithm for POMDPs. In Proc. Int. Joint Conf. on Artificial Intelligence, Acapulco,
Mexico.
Poon, K.-M. (2001). A fast heuristic algorithm for decision-theoretic planning. Master’s
thesis, The Hong-Kong University of Science and Technology.
Porta, J. M., Spaan, M. T. J., & Vlassis, N. (2005). Robot planning in partially observable
continuous domains. In Robotics: Science and Systems, MIT, Cambridge, MA.
Poupart, P., & Boutilier, C. (2003). Value-directed compression of POMDPs. In Advances
in Neural Information Processing Systems 15. MIT Press.
Poupart, P., & Boutilier, C. (2004). Bounded finite state controllers. In Advances in Neural
Information Processing Systems 16. MIT Press.
Poupart, P. (2005). Exploiting Structure to Efficiently Solve Large Scale Partially Observable
Markov Decision Processes. Ph.D. thesis, University of Toronto.
Poupart, P., & Boutilier, C. (2005). VDCBPI: an approximate scalable algorithm for large
scale POMDPs. In Advances in Neural Information Processing Systems 17. MIT
Press.
Puterman, M. L. (1994). Markov Decision Processes—Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc., New York, NY.
219

Spaan & Vlassis

Roy, N., & Gordon, G. (2003). Exponential family PCA for belief compression in POMDPs.
In Advances in Neural Information Processing Systems 15. MIT Press.
Roy, N., Gordon, G., & Thrun, S. (2005). Finding approximate POMDP solutions through
belief compression. Journal of Artificial Intelligence Research, 23, 1–40.
Russell, S. J., & Norvig, P. (2003). Artificial Intelligence: a modern approach (2nd edition).
Prentice Hall.
Satia, J. K., & Lave, R. E. (1973). Markovian decision processes with probabilistic observation of states. Management Science, 20 (1).
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable
Markov decision processes over a finite horizon. Operations Research, 21, 1071–1088.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration for POMDPs. In Proc.
of Uncertainty in Artificial Intelligence.
Sondik, E. J. (1971). The optimal control of partially observable Markov decision processes.
Ph.D. thesis, Stanford University.
Spaan, M. T. J., & Vlassis, N. (2004). A point-based POMDP algorithm for robot planning.
In Proceedings of the IEEE International Conference on Robotics and Automation, pp.
2399–2404, New Orleans, Louisiana.
Spaan, M. T. J., & Vlassis, N. (2005). Planning with continuous actions in partially observable environments. In Proceedings of the IEEE International Conference on Robotics
and Automation, pp. 3469–3474, Barcelona, Spain.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Szepesvári, C., & Littman, M. L. (1996). Generalized Markov decision processes: Dynamicprogramming and reinforcement-learning algorithms. Tech. rep. CS-96-11, Brown
University, Department of Computer Science.
Theocharous, G., Murphy, K., & Kaelbling, L. P. (2004). Representing hierarchical
POMDPs as DBNs for multi-scale robot localization. In Proceedings of the IEEE
International Conference on Robotics and Automation.
Thrun, S. (2000). Monte Carlo POMDPs. In Advances in Neural Information Processing
Systems 12. MIT Press.
Tuttle, E., & Ghahramani, Z. (2004). Propagating uncertainty in POMDP value iteration
with Gaussian processes. Tech. rep., Gatsby Computational Neuroscience Unit.
Vlassis, N., & Spaan, M. T. J. (2004). A fast point-based algorithm for POMDPs. In
Benelearn 2004: Proceedings of the Annual Machine Learning Conference of Belgium
and the Netherlands, pp. 170–176, Brussels, Belgium. (Also presented at the NIPS 16
workshop ‘Planning for the Real-World’, Whistler, Canada, Dec 2003).
Zhang, N. L., & Zhang, W. (2001). Speeding up the convergence of value iteration in partially observable Markov decision processes. Journal of Artificial Intelligence Research,
14, 29–51.
Zhou, R., & Hansen, E. A. (2001). An improved grid-based approximation algorithm for
POMDPs. In Proc. Int. Joint Conf. on Artificial Intelligence, Seattle, WA.

220

  	

 	
    


		 !" #
 $!

 	

 
  	 


 
 
 
  
	
  

 

	

		
	

		
	
	

		

  	
 
 	 


	  

 
    



 	

  
 
 
  	
    

 
    		  
   		 

   
   	
 	 
     
	 
  

    
 
    
 

  

      	 
  
    
 


 	      
  
   
	    
 

   

 
 
  
  
 

  	
 

 


   
	 
   
	


 	
  !	   
	  
	  
   


 	 
  "
     
 


  
! 
 	
   	
   

 # 
  
  
  
 
     		 
  
  	 	   
 

 
 
 
 $


  	
	 
	  


 
  	 
 
 
   
  %
	
 &
 '


	 (
 )*+&, 
  	 
 
  	 
		
  
  


 
   	   	
 

	   
 
 
    -./ 
   


 0 
 
 1.../ 
    

 
	
 0 
	 
    .23 
  	

 	



 	 
 
	     
  
	 
  


	 	   
 
  

  
 
 
      	 
 
   

 
  
	   
	    	  
    
                

  	     

	    !  
 
 
  !$ 
 
 
   
Æ   


 	
  
 

 
 
	 
 
 


  
  	 

 	     		    	 	  	 	
		 
     !" #  #  !$ % &' & 

  
	' 	  	 &    	   
	' 		 
 '
	 

 (		 (& ) *	 *  	  
  " +
 , -.."  +  %  !$
    % 	
&  
	 ' &

   




  "#$  

 
 % 

 
   	 
 
  
  
  

	 
 
  
 
 
    
	  
 	
      
 &
  	   

  
    
  
 
 	  
  

	 

 
 
   
'  
	     	
 
	  	  ! 
     (  ) *
	
+,,-. ( )  +,,-  
 +   
 / 	
   	
  
   

         
 

 

      	    
 

 
  	     
 !	  
   
   
   
 
 !	       

	
	  !  
 
   

 0  
 
 
 	   
 


	 

 
    

 
 
	  0 
     

 

  

    
 

    	   
       	   
   

   

 
  
  

   

  
   &




1
  
 
 
  
  
 
    	 
  	 
        0  	 
 	 	      ! 

 

 2

 
 	
	   	      
	  

 
 
  
	 

 
  	
	 
  	
	 '



0 
  

 
 
	  ) 3 4556 78  
  9 1
 ) :! +,,;.  ! ( ) 1

455<. 	 +,,+  	  
 
 
 
 
  
 +
  

   
 78  
 
  
	 !  
    
 
=  
 
	 	 
  

>?  !
	  
 
  	
     !  

  '
 
 	
   =   
 
 $
	   78 

  
 	
	  
     
& 
 


&   
 
  	 
  	
 
   

 
	 
  ! 
 

 
 
 
 

  @
	 :!
 ) ( 455+ 
  78 
 
   

 
    
 
	 
 !	    

$
	 @
	 
 
 

	   	  
 
    !

& 
 
 
&    
      
 /  
 
	 
     	 
   
 	      
     
 !   
3 
 
   
 
 
 

  
   	  
       
 

  
   
    


 

 
  
   
       

 
  
  
 & !   	
4.5

 

 
     

   
 


     &
  Æ



      
 	      
 
  
  
 
  
     

 
 	 # " 	   
 	 ! 
  
	 
	
 ! 
	 
  
       

 
 
 

       ! 
  0 

 

 
	  
  ! 
	  

 
   
	      ! $ 

	 
 

  	
'  
 
 
       	

  
  	
 
 
 
      
 

= 
  
  
 A / 
= 
  # "
  
   	
  
 2  
 
 ;  

  
  # " 
 
   
	    	 
 
 
 
      # " 
     


= 
 

  
    
 
 
	  
  
	   
 
    
	 
 


 
  


 
 

 
 

	 


  ( *
	 )  +,,A
 # "  
 

   
    
	 

 
	  

    
	     00

 
	  ! 
   
   
  
	  /
! 4566.
 +,,-   
 
       '
  

 
	  # " 

 	
   00 
=  00  


     
 
	      


      
 	  	 2  
  
 

 
      
 
00 
 A 
  
       8 ) " +,,-. B
	 455C. B
8 ) D

 +,,4   
  
	 

  	  


  
	 
    

   


   	E  

!
	     
 

 
 


	   
 +   
 3    
 
     

   
	  

   
  
 
 

  
 
	   

 
  &
 
   

	 
 
 
   F !     

	  
= 


      
 


	
 
 !	 
 
    # " 
	  
 
2
   
 
     
 	   
  
    00   &
    

 
	  
 

  
   
  
 / 	 

	 	 0		1   	  	 &	 '	  	   	 & 
	 2
! /  3
 

	    4 0	1   	  		  &	 4
	  	 &	 &  	 	
2 #   	  	    	  &   
	  56
 --7" 5  8 --9" 8 :  ;	 --.$  & ' 	 
   	 		  	6 		  
 
 '		  
	6   	      	 &  )		 )
	 <
8    2$

4.6

   


 
 
  !   	    
  
  "

 

    # " 

  
   
  	 
 
 
  
 C   

     
 	
  ! 	
   ! 
	G 3  8
E
@
	 3 ) 8
 455-    HE  #

 F' 


 7
 8
	  #F 78  ) H 4555   
 
 
 
     
  ! 
 
	 ! 
	 


 " ) (
	 4556. #!
 )  +,,+.  +,,,   
 
    
	     
  
  
 
+,,, /  
  	
   !  

 
=

  # "  
	    
  

 
 
 Æ

    = 
  
  # "
9

 
      
 	  
 
	 

	 
& ! 	
 !   
 	
	  
 
 

  

	   # "  
   


 
         C,I   

  
  
 

 
 
	 !    4,,,I.  
 


	    

  
   

 	    
,+A   # "     ! 	
 
  
 
	
.   
	 

 
	
=   5AI = 
 
	 
  

  
	     G
: 	     

 
	   

 
  


 

  
	
:   

  
  
=  J 
!
 
	 J  	
	 	 
  
	 	  
  

: 
 
	  
  	   # "  
 	
	
  
  
	   
 


  

  
00 


	 
:   &
   # "  
	 

 
 
  

 
	 
    
 
  


	  	
  	   	

 
  
  
 	
   
 + 
  	 


  
	 

     
=   
	 
 
	

' 
 
      

 
  
   
 -
 

   
 
	   
  
  
7 #  		
	 	 
  	 &	   	   &  	 
 	  	  		 	 &' 	 		    & 
	 ' &	 	 
 
'  			  		6 	 	  	
:'	 	  	 	     
	 

	  	 	 3		  '
 & ' & & 
 	   	6 

	     &	    <%=
) >
	 
  <(   & 	  	    	 	 	
 
&	 	 		     3
	 %&'  		 
		  	 	
	    		  	 	

41.

 

 
     

 
   
  
 

   ! 

   

 

   

 

 
 ;   '

 	
   
  
	 
 

  
 
	  
 # " 

 
 
 
      

 
 
	
	
 
 

   

   ! 	
 @
	
 #F 78 
 
 

   
 A    
  
	   	   
	  
 	
	  

 '  
 

   

 
 
 
 
 C 

 < 
    	
 
  
 	
  
       

 
 2
 
 6  
	
!  

=    !

  

 
 
  = 
  K 
  

 !  

         	
	 
 
  
  

 

  
	 

 
   
 


 +4  
 ++ 
 
   

   78 


= 
 
 4 78 ! 
 

!
	 
 
 

 

!	  =   

  
 
    
 

!
	
!   	 
 
 
=      

 
 


    
 


      
	
    

=   

     

	
 
 
	
	 2
 
 +- 
  
 
    !

 

 
 
 

   

 
  

=  

	  
&  

 
 
    

  
	  
  K  

 
    
  

	   
 
 	
 
 		
	

 
  
 2 %"    	  


 
    	 	  	 
 
 

 

  	
 

   
  
   8
 9 4566   
   	 

 

   


    
   
 	 
 /   

    

  
   
 
  



 	   

  / 
  

 
 
 

  &
  
  	 
      	  


 
  
 	  
 
 
  2 %" 
   
  

  	 
 
   	  
	 

 

=   ! '
 
 
 4 0 2 %"  
 >? 
 
  
L 	   
 
 
     

'
    
  
  >? 
   
 F !   
   
= 

 	   	  
  
	 
411

   


 
 
 
   
  
  
	  

    
  

   K
  =   	

	 
  2 %"    




	
 

   
 	

  J   

 
 

  
   	   


  
	



 	  J 
  	  " ) 1 4555  
 
 
  	
  	   
	 !	     
    	  
  

 
  !    
 
	    
   '
 

 4  


	 
	=
 
   
      	
 
 
 
 
 	     	

   
  !     

  >
 ?  ) 

45A6 


   !  	    =    


    
 
 

  
   	E
    
   
   
 
   ! 
 
  
 
  
 
 '

	 

 	


   
=
 	
 	
 		

 

   	    
  
	 #1#
 ( ) 8 4554    
	 	   
  	 

   


   
 
  


  

 	  
  K
   
!
0 
 #1#  	    
 
  

 	
 
  	

 

  	
 
 
	  
	 


 

 
  	   !  
 


 

  
   	 

 
    #1#E J  
      	E 
   	  
	 

 
    	   
  	 	     
! 
 
   

	  	 
 / 
 	 
 '
  
 4   

 
	 
 


9
	   #1#     ! 
 9
 
	
 

 ! (! 455A 455A      
 
 

   ! 
    

 $
	  


 
 
      	 
G     

   
     
	 

.    
	  
    
	.     !%! 

   	    
  
    	
 

 	 
	 1#1# 
   	  

 
   
  
  (! ) 8 455A (
	   

 
 ! 
 	   
 

 
 $
! #1#
 1#1# 

	
  
	   	  
 

 


G
 

 
 
  	  
      
   

	   
   & 
  
 
412

 

 
     


 	 
 

  
	 

   

 

 1#1# 
    

  
	 
   
  
   

= 
 
 4
/  #1#  1#1#  
 
    
 



   

     
' 	
   

	
    
 
  	 

 
 
 

    
 	 


    
 
 




      
	    	E 
    
  
  
 
  	  
   	 
 '
   
= 

 
 
	 
 	 	 !	

#   
 
   ! 

	 
  


  9    455<   *
	 455A. 7
 ) 
 455<  
 
  

   K
 

  " ) 8' 4554  K


 *
	 455-  
  	 

  >K
 
 	?
*#1    
 K
 
   
	   

 
 


	    
	 
 	 
  9  
   *#1  	
	  G
>'?      
 


 
 
  

	 
	 	  >=? 
 
   	    
 

      >'? 2  9   

   >
?  1L ) M 455C         
   
 
	  9  
  

 !  
	  

 

! 

	  K
 
 
 	   
 

 	 
 
 
 
!     !    	    



 '
  
 
 
 
 
  
 
   	
    ! 
  
   
 	 	
 9   
 
  
	  
	  
 '
  

 K
 
 	     
	
=   


 	    
	  
  
 
 
 
  	
 '
  
 
	
= 
  

  
 
 
   

     	  
 
	   ! 
 '

	
  
  
 
  
	  F 
	

   
 	

 
 ! 
 
 
0  
  ! 

   
 
	 
'  
	 

	 
 
  "#$	  %
 &	

78 
 
     
 

  
	 
  
 

 !  3  8
 3 ) 8
 455-     ! !

	   
     

    
	 C  C
	
   
   

 
    
 
	 ! 
417

   



& 

    !   	
 

  	 '  

 
	
	 	  
  
   

  $ 

	
  
  
 
	  '
	 	   
	 
 


  
    !    
	  @
	

 
  
   

 
  
  
 
  


	
   
   
	 

 
  
  
  	    

 
   
	  
	
	 	  
	
  	
 

      	
  
&  
   !  
 


	   
	 	
 
 
  
	 

   	 
 !    
 
! 

  

    
 	
 
 	 
  !
	   !
	  
!  
   
    
Æ 	
	 
& 	
 
  !    
	
  

   
 @
	 	
     	 
    


   	

 
 !  3  8
    
 

 
   	  
  
  !	   ! 
	
 0  

        	 

 
 

 
	  
  !	 
   
 	
  

   	      

	  	

  

      
  	 
 
 
 

     
   	 
 
   
  
   
 
 
	   
	 	 


     	  
	 	   ! 
  	
 

   
  
   
 
   

     
    
  3  8
 455-
 
 
 !  	  
 @
 
  	 

 
 
	 0  
  
	      
 
   
  
  (, 
	  4566  

 
  >? 
  	     
	   	
# "      

	      


 	  
 
  	  # " 
  
	    
	    
3  8
 455-  
 
    @
	 	
    
!  

 
	   	  # "
 


 '
	 
( 
	 $)	 


 " 

 #' "

 
 
	 	
  #F 78 
+,,,   K
  !  
	 ! 
 
     
	   



   
  
	     

    


 	    			
    
  
 

	 	
 	 " 455C. 
	 *!! ) * 455A    
L 
 
	 ! / #F 78 
&     
 



	      

   &
 
	  
= 
        &  
   	  ! 
 	   

  
	 
 
   
    

414

 

 
     


 
    	    

 
  
 !
  	   
  
 
	 
	 
 
 ! 

	  #F 78 
  &
 	
 
   
  


 
 
  ) H 4555
     #F 78 	
 
  
 ! 
	

  
  
 
        E 
K 
!  	 
 
  
 	 
  ! 
 
	
  2  
    
	 

 
 

     

 
  
	  = 
 
 
#F 78 
   

  

 

 	  	 



 
   #F 78    
 	
! 
 
     @
	   
	 
  


	 Æ 

 J  	
 
 
  

   
 
!   

  	    
& Æ  0
  
	 


  
	 
 
 ! 2
 


	 

   
       

 
 
  
   	    
  	E 

    


   '
   	   
  
  
 


 !	     
     '   

   =
   
 
  
 
 
 
        
 
  !   +,,,

  	  
 
   E 
   
	  
  

 
	  

= 
 
 ++4  
	  
  
   !   
  


  


	   
  !  	 
     2
 
 #F 78 


 
 

  	 
  +,,,      

	 


 
  
 
 
  
  
   
 	 
 
 


 
	 	  / 
 	    

 
   # "     

 

 

  !

  
	  


   	
   
 
 &

   


   #F 78    !  



 	
 # "
  '	 *
 


   
	 78  
 

  
	 
   

	
     
	 )  +,,,   
  78  

 
	       
  
 
   
     	

  
 =
	      

	  
 


 	
  
  
  
  

     
 	 	     
 

 2  
  ! 
	
      

   !   	
	 
!  
	    

         	

   
    

  
  	
    


 	
      ! 
	 
     
	
 
 
   !   
 

    
 	
 

 
    	  


  
     
 !
413

   


  	   
   
  
  

  
 	 !
  	   
 	
 	
 
   J 
  

  !	  E   
 

 	 K 
 

 
  

   	
   	     
	
 	  

 J    
 
  


   
 !      

    /
    	  

    
    
! 
    

 
 
 

   
  =

 
 	  	  
  
   
 
    
  
 	 
 	 
	  
	

  
	    	 
 
	 
 

  ! 

	      !   
   	   
  

   	  	
 
 
 4 
	 
   
Æ
 

  
   
 
    :

 455+ 3  3 4555
 #!
   +,,+ 	     !    

	
  
   
   	 
	  
  ! 	
      	
 / 
  ! 
 :

 455+  3  3 4555  
	 	   


 '
     	 
 
      	 


   
 
  
  
   


  
 
   

   


  
   
 

 

 


 
 
! 
  !  #!
   +,,+ 	
    
    

 
  
	 

 

 
  
 
!   
  

	 
  
    


 

 '
 
  
 	
  
 
  ! 

 
     	  
  	
  
   	      
 
 
  

  ! &  
 
 
   
 	
 
  	
	 %	  +# 		
	

B 8  D

 +,,4   

 

  
	 
 	E 

  
 
  
 

     

  	    ! (

 # (# 2
	 ) L +,,4
 	   

 
   	
	 
   


	  	   	
  

 
     
  	 /     

 
   	 

   

	  

 

 
  	E  (#  	

 
  
  

 
  /  	   
	 

  

 
 '
     

    	E


 
 

  
  
 !   
 
  

	 (#  3

 4555   	     	  



=     
 

  	
	 

 


9  	  	  	 	 & 	 ' 	  	 	 % 	 	
 ' 	 &   	 	 	 &	  

41-

 

 
     

 
    
 
  
	   (#  	 

 	 

   
 
 
	  
  

     

    !  B   +,,4 
     / 

   

 
    	   	E 

 

 

   

 :   '

  


 
   
= 


	  # "       
F ! 
&    B   +,,4 
  
	 

  2


 
 !  	    
 	  '  
  
	   :    	
     

      
 

.  

 
  	 
 
   

 F    
   



   
  =
  
 
 

 2
  


 

        	 


 

	 
 
	  

	   	  	   !  
 
 

 ! 
	   	  !   
   

 
 	 	   !   
 
 
 - / 
  !



 	  
  	   ! 
   
.
     !
	 
  
 
	 
    
 ,  +
 
 	

 

  # " 


     

  
 
 
	
8 :
 ) /& 45C,       
	 

  

 !   


  
 /  
&   
	 



G
 8   =      

  >?   
  

      
 

 	
  
# "     	
 !	       	 
 

 	 	 	 
  
 8  	 
   
	  
  
     

	 

  # "    	  
  

     	   =    !

 8 
   

 
 
 
 F    
    # " 
 


	 
	=
 
 
 

  
 

 # " 
 

     
	 
	


  
 
  

 
 
  !	   
 '   

   


	 
 
  
	      
	  	  

455<     	  
	  	   
	  
  

 
   
 
	   	   
	   
	 

 @
    

   
    
 
  

  
	 	    

 
	 
	  ! 
	 
 
 
   
	 
	


  2 
  	    
 
  
   
	 	 
	  ! 

  
418

   



 !	   

 '   !    


  

 
 
  
 

    
  
	
 
  &
   


	  

  
 
	  

	 
   '
   	 
	  

  
 
  


 
 
  7 
   	  '
   
  
	 
  
 	    ! 
 
    
! 

  &
 



$ 	%& !%
! '


0! 
 


 
 
    ! 


   
    
 !  &
 


  

  
 


   
 
 
 
! 
 F 

  # "   
 

  
	 
     
 
    
    @
	 :!

) ( 455+  
 

  
 	   
 

 
   	  
 
     




   ! 
 

  !	 
   

        
   
K
	 
 Æ

  !   
	      

 

  

Æ *
 ) (
 +,,+ / 
 

   


 
!  
 Æ    
  
  
  	E 
 
   
	 
 	  >?  
Æ 

   !    
	  " ) (
	 4556
4556  
 !  

    	  
 

 
 	 3
  
  
  
 
  
 
 	   
 


 
 
   


     N
 
   Æ  8
 ) 3! 4555.
7

 " 7

  /
! ) 3
! +,,, / 
   
'
 

   
      
  
 

 
	 

 
     # " 	 
 
  
	


 

   
 	     2  

   
 
 
  
	  ! 
  &  

 =
 2 
 
  !  7

   +,,, 
!  
    
	 ' 
 
	  &  
 '
	
 	     
 3 
	 @
	  
    


   
 


  
 

	 
 	
	 

  = 

  
 
     !    	  	 1 !
	 
 
	 


   	    
   	  = 

 3 
   
  
	 
&      !
 
& 	  '
  ! 
    
 '

 


 
	   
  4566. #! 3! ) "!
+,,4   
    
      
 
	
415

 

 
     

 
&       
  

	  


 
 
=   
          *
	
) 3 +,,- 
     !    
 '
 

	 2  
  

 
  
  	  

 '
    	  
 '  
   
  


   # " 

 
  '
 
 /  
   '
 

  

  


  # ". # " 
     
  '
 !

	 
 ;   
 0 
 
   
 
 
 !   

    ! H
 ) H 455< 

 (! D
 ) :! +,,; 

 
 
 

M )  4555 	    
&    
&  
	
 '
 ! 
	 / 
   
=  	 



!  
 '
  
 
	   
  
	

  '
 ! 
   2  



	 


 
   

  
 '
  
	 

 
  
  
   

     
  
	 
     ! 
    
 !

= 
 
   
  
  !  0   

 

 
 
  
 

 

   # " 
  

 0   
 
   

  



   
 
  
 

  

     
 	 

     
    	

  
G 
 	 
 
 
& 
    
!  
    	 	 
	 
 
   


 
    	 
	  
 
  
	 

 
  
 
  	 ! 
	      

  

      
 
 
   
 	 Æ
 
'
        
 0  


 

   
    
        

  
  9 
	  
 
  	    
   
       


    
 9 	  
     

     
    
  
   



 

 	 J    >
	?  "  =
 
  
	

   
    " 
	
  
 
 
     
   
 
  K
    	 
 
 
        
 
	    

 
  	
	  
   	 
  
 
 
 

 

 7
	     	
 
 	   	    

  Æ
   
  
      

    
	 
    	E 
   	
  
 ! 
 
 
     /  ! 
   
'   
  
	  
 

 	  
 

 	 
  ! 
      

   

	
 :      
	     ! 
	 
 
 A
 
 
  

 !  
  
&   
 	
416

   


 # "   00  
 

   

  
 	



 
	  
 
 
 
 
 C
  
    # " 

  
  

 
	
  
 

 
 
	 
  0 
 2  

 
 ! 	
 @
	  #F 78   


 
 	

  	  !



 
 &
 
	 
 
 
  
	  !	 
	  

 	  
  
	 

G
"
 Æ
 !

(
)  
   
 
 


/   
  

  
 
   	  

!  

 
 
  

)  
 	
	 
 
 
 


  	  
 
 
  	   
 0 
   

 
  
1
  
  # " 
    &
 	  


	 
  

  	

*

 

 	 	     
   
 	           	 	  


   
       	   
  


 

  
	 
  
    	  

 
 
	 

 !   


    	    	 
  	 
  
	  !  

 
    !
	  



 
 
	  ' !   
     	

  	   	
 !   
	  

  



  ! 
  
      	  

 

 ! 
   
    	    & 
   !   
	  
   
	 
 
  	
    

 

 
  
Æ
     
 
  

	  '
 

=   
 

 
 A 
  
  
	  

  
 '

 
# " 	
    00 
 
 
    # " 
  	 

  
= 
  # "  

 
  	
  F 

  # " 
  0 
 
 


 ;+ 
 K   
 2 	  
  


  # "

  '
 
   
  
	    
 0  



  ! 
	  	  
     
 
  
  K
  


	 
   	E  
 7 
     !    !  
 
	 	     
  / 
  
	 
 !   	 '
 
       
 
42.

 

 
     

 # "     

 

     

 ! 
   
  '
 '
 
	  


	
    
      	  

 	

   # " 

   
 
	   
  
  
	
 
 
      
 
 
  
  
 
  = 
  
  @
	 ' 

  # " 
 
 
  0  
  

 
	  




  @
	  #F 78 

   
 
  

	

+ ,
 

@
	 
  	
    
 
  	  !    

 
 

   '
 

  
 

  	 78

 
 !  
	  !	    	   
 
 
       
 
 !  

  78 '

  
      G       
  =
  
   =
   	 
  G     	    

 


 
 	   	
 
 ! 
  	
   G     
   
        
 ! 
   0  
   78 	
G    
  
	     
 1
  
        
   
    	 
  
	 
	    
 '  (  4 




  O
4
(


























 

	

)

 , 
4 
  
   
  
 
	  


        
    

 
	

  0    
   
 =  

 



   	  !
	  
 
  	
     

 	E >
? 
    
=
    
   
=
 ' 
	 
  
G
 O 	   
+


 
















	

 
 	      
 
  
G




  O 	










 P
 








   









 

-




   
 
	   
 
  9

- 

   
 
   '
  
	  
  

   0 
  
	  !	    
 

 / 
  
 
   !	    


  

	
 
 !	    @
	 
    
 
 
 
	    
  

 !  

	 
 
  
	








421



   




G


   P
 



 









   







 


;

	

 
 
    
  	      
   O 	  
  





A

  



  
  
 
  




  O 
 



C

   	


 =

    
 




   P
 



 







     


   



 



<

	

:
  !	      

 
   
 
 <   
 	  

	  
  
	


	    :!
  (   
  Q   	  

  :!
 ) ( 455+G
Q ($   4   Q    P R   P  Q    S
6







 

 

 

   









	

/      
 
    P4 

	 


,   4 	  
	 
	 	    Q 
  

    
     !
	 
 
  
	  
 
 3 K
	 	 
	      Q 
  
  	     :!
 ) ( 455+  
 
   	   

	  6    


























+ 
  
 -
 
  
 !%
!

 (

	  0     	   
  
     O    O
4
  
  

       	
 !   	
 
  

	  	 
  
   
 
	        



 
 - 
 

 
    	E @
 
=    @
  	          

  
	 
	
    

 
    
L   @   	 
       
  
    
	 	  
   0    @ 
    
    

 !"#$    	  

     0 
 
 
   @
 
    
            	     
  
   
  
   &
  	      	 
 
  
 
 3:    	      


   	   

  

  '  
       	   
  &
 

  


			 









 















422



 

 
     

    	   @  
	
=  >&
 

? 
 
  
 
.  
	    
   
  

 
  
 
	    
	  	
 
	    	  
 @

    

 

   


	 	
 

 0  

 @
    
   
	 


	 
	 @
	 3 ) 8
 455-
     	           

    

	        
	  3: 


   
	 
   O 	     


 
   
	
	 	  

  

    3LE 
 :!
 4565    



 
	  
	    
 


 	  

 	 $E '  
 
	        

  

  
 2
	 4 3
	 
  
   ' 
 
   


  
	  ' $   
    

2
	 4     
  
 
 
 
    
 
   
	 
 2
	 4 
  

 
  
   
 
  	 
 
 ! 	   	 

     
  	       
 9 	
   
    
    

    


  
      
 2
	 4 

	 
 	 	 
 
      	   
/  	     ! 
	    	 

   

   ! 
    '  ' 
 

   
    # " 

  =

 4  


 
= 
  0 
  
  
  
/  
	   
  	 
     
  
   	 
  	  
     
  
       
 2
	 4
 

     
    
     

   
 
 	  
 
  
	   
 
   
  
 

   
 
 
 '
   
     

 

	    
	  
	   2
 
  	 
 
	
     
   !   '  ' 
 	  	

    
 
   	    
   


	 
 
  
        
 '  
 	   
 
        
 
 
 

	  	      
 

 
    
       
  
 C+
  
	     

   '
 
 

!    
	 	   
	  = 
   # " 


 
 
 
 
 
	   

   ! 
 













 













? /   	 ' 	& 	 &  	   	 	   		' 
	 

 	 	 
	 
   	   	 	 '  ' 	

	  	 
	 	 '   	     	
& 
	  		 @

427

   


Network

2

a2

sa

est

equ

r
a1

a1

(a)

an

Call source

Call destination

node / agent
a2 forwards request
a2

pre−allocate

a3

(b)
a1

an

Final route
a3

a2

an

sen

Allocate

a1

ds

con

nec

tm

ess

age

(c)

an

drop call
a3

a2

no bandwidth with a3

(d)

de−allocate

a1

2
	 4G   
	 

  

 
 C

 
 

    

 
 


  '	#
#		 !)	
	 


"
  
    
   $    

 	 
 $     


   # " 

 	   

	 

   
  0  E  
    
 3: 
   
  3: 
    
    

	  


    

 	  
 
 

  $ 
 
   
$  
 
 
   E 
 3: 
 $ 


 
	   @  '
 6G $   4  $  P
            
  
  >?  
@   
 
 @
 ' $ 
   
 
   
    4     O      
 

 

	 	   

 
  
   
 
   

  

   
    
   

 
       
	      
 C- 
 


  
	 
* 	  

  
 
 
   




   4 
	  

 
  $ 
  



	 
    
 
  

 
 
  
	  $  
  
  

     
  

  	   
         
 





 			

















 



  

  



 





 





 

 















 





.  	  		 	  
	  	  A5 

 & 
 		 		
	   	&  
	 
 	 	 
	

424

 

 
     


 C-  


     	E     
 	E  
  
 			
	 

  
   

 
    / 
 
    
  0 
  !	  

 


 
 Æ
   	  ! &
 
	 

 
	 
'      ' F    


	  


 


   
  	 
	  
 
 
    
	 

 / 
  
& 
 
 
 


   	  

   
   	 
  

	 
 
	   # " 


 

  # " 
  0 
   	   
  



  
 

 
	 
 
 G 
  	 
  0  


    
   ! 
 '  

	 


. 

  	  
  
 

 
	 .  


 


 

   '   

    
   
   / 
  
& 
	 
  


 # "  
 
 	 
	 

    
 
	  


 
  	
 
 
 


  # "     

 
 
	 
  

    

     
   
 


 3  

  
 	   
  
  

     
    # "   
=
  


	


 	  	     !  ! 
 



	 

 	  
	   
 
=
 0 

 	
	    

	
	       
 
 	


  
 	   
=   
 
   

  !

   '   
 



 
  

    >?  @
	  
   

  
 

 
 

 
 

	  '
  
@
 
 
  
 
  &
   	E 

  

 

"#	

 @
	 	
 
    00   
 
 
 ++4 
@
	     @7  	    
	    
	
($  
  E   
   3: 


   


 P +  
   
	 ($ 
  
 Q ($ O 
 ($ 
  ($



 
    
	 (  ($  
   	
  
  0
  
  

 
 


 
  
  
     
   ($ 
  

  
  




 
 >?   ($  
 
   '  	
P 4 G  P 4 
   

	 
 
  
 
 
  
4     P 4 P Q ($  
   !
	 
	  

	  E

 
 
  

 
   2
	 4   
	 

 
  2
	 4    
   
 @7 
 

 
    
 # "   
 
 ;+4  
&  
 
 


















 











 

 





423

 



 

   


G  
 @7   
  
   

 
	 
 # " 
 

      

   '  	
  '#$

#F 78 
 
 
  0 
 
	  

 	
   +,,,
 

   #F 78   +,,,   
  

 G 
  


	 
  

  
  
  
	  
   4,,   	   
! 
	
  
	 
   A, J   
 
 
    = 


 <+  

     +,,,.  


   
   

  4,, 
 C+ 
 
  
        	
  
    :
 
	     
	 #F 78 

  '  
  
   
 


   
	
  

 9 	      
 	   
 
! 
	  
 
	  @ 
  
 


 
  	
 

 
  	
 
	   	
 
! 	  

   	 
  
	 

 
 !  
	 
3L 
   @
 7 


  
 #F 78 

   
   
=       
 
	 

    

   
  

	  	       
   
  


   
	   

 ! 	  
	
   	  	      
 @  

	 



     	   $          	  	  
      
 '  ($     
	 
 
  

@ 0     			
   

 
  '
	 

   
 # " 
 ;+4












"   
   # 
 #


 
 
     
  
  	   



	  
 	
	  
	   00  :
 
 
 
=  # "   
 	 
  
  
   

  
  

 
   
	  =   

  
  
   
  

    
 
  
 
   

  


	 

   	
 


 
      
     

 

 

. 
 
 ! 	

 
 - 
     
 ! 
  '
  '
 



   
 	   
  
   
 
  
   	  O  $
   	 $ 


  ! 

  

	   !    
   	
   

 
  	  ! 
G $       
 
 

  
 

! 
	 
 $  
 !	      	    


			 











42-

 

 
     

  !  

      
 $   
 
  	
 
  ! 
	 
  
  
 
 
 ;+ 
   
 0 0     
   	     ! '

 
  !    
  	 
      
 

   
   	     ' 	 
	 
	 
 

     
  
 


 	    	 # "  00  
	   
 

 
    

=  
    

 	 &  
  	
 	E !	   	 
   
  
 

  
 

'
  
	     	      
  
  
  
 
2  
   	  !   	 
 
 
 -  
        = 
  
 

!  	   
  


 
  ! 
 
  


  
 ' 
 
 AC       	


 
   	 
 
  '  	  K
 

 
 
	  !  
  

  >'? 	      >
	? 	   	  
  = :   	 '   	    

= ($ 
  
	   $ 
 	    
 ;+ 
    
 
  2 

  ! 
       '
  
   

       
 
 
 ;+  	   	
 
	  
 

   	  
 
 '
	        	      
  
 
 
   	 E !	  	 E   
 
    O   !	 
    	 
   
 
  G
 O   
 O4

5
 
	        	 
  
  G
 O     O 4

4,
0  
 
    	 
 
 2      


  
 
 
  
 
   !  
 
&     
  
 	       ' 	   
 
 ! 
 

 
  &   

   
	 ! 
 
   
    	  
  ' 	 
  !   
 

   0    	   
' 	  
 
 
   	       	
  

  
 


  
 

   &  
	  
	   
        
 
  

   
  
   F 
          



 7 
 
    
 
   


  	
 


	 	  	  !	     
     
  	E 

 
 / 
    






















  













  











  

  





			  	

			  	





428

  



  

   


         
   
 	 
 
 

	 
     	E 

 

     
 
	


    
  

 

    &

 	E 

   
	 
    

  



	   
& 

 	


. "

  -
 

(
 
 		
 
 
     
   	  	   

 
  
  
	  00 	 2 
    $ E !	   
  $
   
   
 
 $   !	  
   4  /

 !	    	 4   O +
  
&   
	
     '      4   0   

  
	 

   	
 
 
     
 

  
  	   
 ;++   

    
 


@7   00  !

 ! 
	
   
   	 	  O 4
 4 ' 
 
' 	 	($   E !	   
  2
	  ' 
 
  	 
    	($     +T  
	  '

  	($     T      	($  !  	   
   T   P +T  0  T    

    	
 
 

	 	 7
	   

 
 
 ;++  
+
 	($ 
  

  
      	  	($
  /
	 ($ E !	     

 
 
  	 (  
 
 '
 	(  
 '  	($  	(  
	  
	   
 !
 P T    
 
  
    P T   P +T 
   
    T   +T    

     	(  	($ 
 


 	( 
  
 
 '  	(  '  	(  	(  
P +T  +   
 
  
    P +T  +  P +T 
   
 +   T   +T  9
	 
    ' 	   

P+T   

  	     ' 	  
     
     4   T   +T P T   O   0    T 
 
    	    
  P   4T    4  P +T 
 
 
 

    
 P   4T    4  P +T  T 
  

 
 
 
  4  
     	

  	  

	         


 
  
         
	  
= 
  4
    
   	      
 


  
 ' 	  
 
   	 $   

  '    E !	  
   $ 
 


   P+T  4  + 
  4 /  !	   '
	   
 $     '   	   
 




			 









 



  

			 



 











			 































			 









































































































































- :	 		  	 2 &  BC &  	
  	 		  	 @	 	 
	 / 	 	 &   	 	 
	   	 		  	
	   	 
 3
 	 
	   ' 	 		  	 
  	4 &

425

 

 
     

 4G 
 
	  
	 
	
	
$

 P +T 







$


 P +T  

 P T P +T     





 P +T    +  
 P T P +T    + 

 P   +T P +T    + 






























 
      =     /  
 P +T   
  

   	  $
   $  
  
	G




$(,

			 

O  4 P +T     P 






  







 +T P +T    +  T   O +



			  	

44

0   

 T 
  
     ' 	 
 44

 
  
  4    
  	  
 

  

' 	 
 

 
 
       T .   T 

 2 
 P +T 
 44 
    








$



O  4       P 








  



 +T    +  T   O +



			  	

     	  
  

4+





O     O 4
  



4-



			  	

 4+    !	  	 $     	 
  
 
       
       
=
 !	   	      	  	   
   
 
  G
4;
 O   4   T  P T

    
 
 00 
      
 # "
















	

.$ "

  -
 

(
 
 "
 
 
     
   	  	   

 
  
  
	  # "  

   
 
 
 A+  
  ! 
	
   
   $ 


  
	   ! 
  '   

	 	  
    # " 
	  
 

  	  
    ! 
  
  !
    
  
   
   


   


  	    
  
   ! 
   
	
  

     
 

	  
 











426

			 

t + (n−2)

t −2 tc

t −2 tc

t + (n−2)

t − tc

t + (n−2)

t

t− tc
t

1
Agent

2

3

Time

   


n

2 communicating to 1

n communicating to n−1

Information transmission

2
	 +G 
 
	  # " 
	
  
 
  

  $  
 
   ! 
 
          P  P+     1
 
 $ 


     

 
      P T     P T P 
PT P+     

	    T   

   $  
9
	 
  
   
   
 
    
 !	 
   	  
   $  P +T     P +T P   P +T P+  
   2
	 +  
   
 =	  	 
  
  

 
 
 
 
  

  
 
 	    


 	 $ 
  
 ' 	 
 
   
  

 
      
  T   $   
	 

  
' 	  
	 
 
 

    
 G
$ O       4T   O 4

4A
     	  
 
   
 4-   
&  

 4A    

  	 $     	 
  

        
      

=  

   	      	  	   
 
 
    G

4C
 O T
  
	 
       
 4C  4;  

 	  # "   00


















































			 



































  









			  	















	

.+ 
 "

  -
 

(
 
 " ! 		
 
  
 A-   
	  # " 	  	    

   ! 
 
    

   	  
  T   4C  
         00 	
    	 
 E  

       4   T  P T
  4; "
	    
	   


   
    	         % 


























 &&    	 
      
  		 

   	  
  		  

47.

 

 
     


 
  

4

 







+T



  T 
   4  T P T	

4<

  
    
  
 
  ! 
   
 
	   

    
 

	 	 T $
/     # " 
   
       00
/
	 
  # " 

 

 
   
Æ
 
 00     
	  
 

  # "   







.. -! '

  
 "
 
 
  
 
Æ
 

 


  # " 
   

 '
  

   # "  00  ! 
 
   
   
 

 	    
  !	  
 
      
  
 /   	
 
 
       
    
         
$$
 4   
 
O     

 
 
  
   
   
	  
  
  : 
   
   
 
 

	  
   	  
   
     1
   

 
        	
    	    
 	

  	
 
 
    : 
  
  
    
      	 

    
  
 

 
	 
 
	   
   	 

    
 00 
 	   # " 
 A; 
 
  
	   	 


 # "  
   =  
	G
*

  '      O  
  O 4
 ! O  $   
 
    	  
  
  	 
    

   
 
/ 


 
          
(    !  $














































  







			 





 
















   

$
	       
&  	 
 
 	
 G
  

 P 4  
 


	





  





 

46




($

     
 	  
L P 4  
	 
&
        
   
   
	 P4

&    








 ; 3
  	 	
  	 	& &  	  	  	 	
 
	& '    	   	 & 	 	 	 	& D	
   	   
  
		   $    	 	 	 	 	 		 '  	  	 	  

 &	   / 	 	   &    	  	 	 		
' 		 	  	 	 	  	   

    		 :'	
     $ E  

471

   



$
0 
  
  
    

   
 
 

  



      
           
 
   


  E  >=? 

   
 = 

    
   
 
     O     /    46 
      
  '        
	 
	    
  
   
	 
&  	    
 
 

	
  
 
   	 

 
   
  
 
 
 
	  
 A; # " 
  
  00  
 
  
	   


  %         	  &&
 
      

 
   ! 

  
   	   
	    

 
 

 

 
       
 


 



 



 

 














 

















.0 	
!
 "& '

   
 
 A+  A-     
  ! 

 
    
  
       4; 
4C  
 
	     
    	 
	   !

	 
   

 
  
  
 ! 


 
	          

	  


       
     

 



  00   4;         
 


 
 
      00  
	 

 


     



"

	   
  
 
 A+  	  

   
	 
 
     

 

  4; 	 
$

	 
    +T
45
 O






















)$

 	$        
 	     

  
$
	   

   
 
 
 A; 
 	
  4C  
   	
  45     

 
G


	 

+  +T
+,















 





 

	


)$

 
	 
 
 



 $ )  
  	  	 
	 
 

  %   &&   	         
  
  		  
   	  
  		  



 /  E
  	 	 3
	 '  '  		6  @ 	     / 
	 &  

 	    '  	  	   &     
 	  

472


  $ 	






 

 
     


 
   

 +,   4
	 

+T  
+4



#

 - 
 

  

 4  
    
   

 
  
 ! 
	 
 
 
  	  

 

    	  
 

	 

    
 

 
 
  
 # "  
 00

    

  
 AA    

 ! 
 
!	     	  # "   	 
  
    00


 + )  
  	 %        











 

	

	  &&

 
	 
     
   

 


 


   '
    

    # "



	 	     
	 	  	


 
 :
  
  
 
   
  # "
  	  !  
 

  00 
 
  	 
    
   
 	  # " 
 

	 

 
 
  
 
  !  
	 


  

 


$ 	

 	

%
  
   

 &'

 
 
  =     
 
 
  



  
 

  
  
 
    

  
 &  
  '  
  


  
  
   
	  
  


 !
 
 
   

   # " @7  #F 78 	


 
 
 ;+ 
  

 2
    

   
# " 

  			
	  

 
 
 
 ;+4

0 
 

:   
	 

 
   

   0  

 
      
  ! M	 ) 3
 +,,;  
  
  
 

 

 
 

 
 


 
 

 	 0   
     	


   0 
  
 
 -
 

   

 
      
      
 


   
 
     

 
 

 	

 

 
	
"  
	
%
      
	
 	  


     =
 
  


	  
 	   !
477

   


E  
 
 
 

 
  	G    
     
         



   	
  
    
 
  
 F 
  
 

     
    
 
      
  
 	  
     
  

  

  


 !  
 
      


9  
    	 9 	   
  
	    

	 
 
 
  =       

 
  

 
 



: =        	
 
   
     


 
     
   
  
   9
	    
       
   	  
   	

0 
 !%
! 
   

 
 ;+  
    
	 	  
 
 ' 
       

  /   
 

 
 

  
  
   
   

   


 
 ;+   
& 
  	  
      


	G 
 '        

 '       


 '
       
 '  
  	     
	 



    

 
 

  = =  
	  

  
  

G      	 
  !  $
  
O  .      
	  	    O  .  @
    	
 @
   
  E 
   

    G     R, 4S



 
           
 
 
	    .  G 
    . 	G            .   G ! 



    
 
    
 
 
    
   .       	   ! 
   	 
  .    
 
       
 
 
 

     #F 
78   
  
& 
  
   @
   

 
 
  
 #F 78  	   
  


 
   &  
 
  
 
 


 


 
    
	    
! 
	   
 
	
 
  
    
   E   
 
	 
     
 
 
	   E @
  
 



   

  
  
G     
  
 >
	? 
    
   
 	 
 
 
   
    

  >? 
 
 ;+- 
=       

 

	        
  
	 

G 
    
   

       
    	 
 
     
























 









 

			 



 

			  



 















































 











474

 

 
     

 
  
	

     	 !       
  
  
	   


	 	       
    	 
 
       	 
 
 
   
  
           	
  ($  
          	 
   

       


       	   
 $      
      
    /              
:
 
 
  2
	 -   	 


 $ 

	   
 4 
	   !  
    
    
	 
    
  
 

 
 +  
   
	
 
    
   
 A 	   
 ;   

        
 	 $ 

	  
 -- 
	       
 
 -;      
 
 	 
 -C 
   
 
 
   

 
 
 -  
=  = ! 
     
!
	 

  
 
  
  
  
 <  	   	

 6     O 4, 5($ 
 4,  
     
      
     =  
   
 
  
 44     
 
  
     
     
   
 4+  
  	 
 
   
 	 
  
 4- $ 

	   	 
 A,     
 
  
 
 @
   

 . 
	    	
 
 #F 78       
 A+  A; 
 	 
' 
  
   	      
 AA 
   
 
 
  
 
	 
   
     
   
       
  
  
 A<
2
 
 
     
 	 
 A6  


  
  
   
       
 
 
  
	  
	  
/     
 	$
 !
	     ! 
 
 
  

     

4;  
 
 
 
   
 
 
 4C     	  
  
    
 	     
  
 +4  


   
 
 ;+  
	  	 
 ;5 
   
 .
 
 
   

 
 
 / 
  	   #F 78
	  
	   
 +4 
   
 
 	 
  

 +,.   
  	 
 #F 78 
 
 
   


	 

    
	   # " 


 
 C-4  C-+  	   
         	
$ 9  	
 
 46  ;<
  
	 # "  

	    




























































 



	



!







































 





! /  3
	 & ' ' 		 	 	 		  	   =  

% 	 	'  &	     
 
	 	 
	   

	
2  A5 

 '	 	 		  	 	 
	 %&'  	 36
	 
     	    3
 

	$ :	 		  	 		 A5
  	 	   	 '	 	 	 	 		 # '  	 
	 
	 	 		 	     & ' ' 		 	 (# 	
@	  		 	 		 	  	  

473

   


  		
       
 
     	



               

    


	

   !   !

  ½     "     !

	

       


	

   ! # !

  	   "  "   


   ·½  #     "  $  !

 		 
 
 

 

	
              

  ½        !

          

	
	

   !   !


	 	!	 

 !"   %&'&(  %&'&)

#   	 

 ! $!%&'   *&)&'


"
#
   

  ½   	  "    !

	

  !   +, "  ! 






#!$ %    


&
  , 

 ! $!%&'

# % 
  ·½      ·½       


    "    !

 (&   *&)&)

 ! 
     "  #


    '  ) %
     
 ¾Ã


%    
   (%    
  (
   
 	   -  	


 	
&
  , 
      "  

  ½    
 	 	  ..
/
	


  ! $!%&' # 	  !  /-/,

)

)
 	 ·½ ´	 µ ´	 µ 

% 
  ·½   	   ·½ 	   (% 
  ·½   	   ·½ 	  () 
 	


	 	!	 

 !"   %&'&(  %&'&)

   !    0

)

)
 	 ·½ ´	 µ ´	 µ 

%  	   ·½ 	   (%  	   ·½ 	  () 

#   	 
    	  "  

  ½ 	  	 
 	   -
./1 	


  ! $!%&'

% 
  ·½      ·½    (% 
  ·½      ·½   ( 
 
 	

%        (%     ·½   ( 
 
     ·½

	  	   "  "   


   ·½   		 
 
  	
&
  , 

  ½    
  ! $!%&' 2  $  /-/,
 * #
   ! "   
  

  * %	
  

 ++# 

	
	

     !

  ½   	     !   0 
   




















































 





2
	 -G 	 
 
   
 	 

47-

 

 
     


    

 
   	     
 
     
    
 	
  
 
  

;A 
 C-4  C-+ =  

  
	 
    

    
     
 @
 
 ;C   
  @
	  
  
	   
 ;4   	

  
  

     
 
 
 
   
F    
  
   

  
  
    
 
	

	    
 
 
        ' 
 +; 


   =
	  

 


  E   @
  
 
	
 
  

  
	  
	 	 
 	
 G














 	 O

 

 	 
'  

 	 
  
   )
   
 

++

	



0  '
 ++    
 
  # "  @7 
  
#F 78 
    	    	    @ 
 '
 ++ /

  >?  	G R,4S      
 
&
  
 @
  &  
 


  
 
    	  ! 
 
   

  3L 

 :!
 4565  


   
 
 '
   
 
  
   
 +A     	 

+6   @7 	   
	 	    
 
  

	       
 


     

     
 -4 
 

'  
K 
@-  .    
 ;++  '
	 	  
, 
 
 	       
 
 
     	  
 -+  #F 78 
 
 @
    
 +<  
	    	  
 
  
'  
 	 	 	 
   
	 ' 
 #F 78
0  
    


   	   
 #F 78 
 
2
  
  
! 	    
 
	 
 C, 
 

 
 
     	      
    
  

  
 C4   
	  	 	      

   
	  
    E  
   
	  


=   	  	     	   

 
    	 
     
 
	 
   
 
 
 -6  

	
      	  
  

 
 2
	 - 9 
         

     
 C-   
   
	 
 C; 
 
 


   	    $ 

	     	  
 
	  			
   

   '   
 -5 


 # "   
 @ 
 
  
 ;,  
	   
      

    	     
 #F 
78   @
   	   
 
	    
   
 	
  
 
 A<  	      : 
   			
   #F 78  
 
 
 C-+










"

"











































































478

   


  

     
 

   	 



  
 -  

= 
  

   
    


  

    
= 
	     
 

 
 

	
 &
   
 
  
	  
 
 

 
 !  
 @
         
 



 
 
   

 !   	  
 
   



	 	   !  
 

	  

    
	 
  
 

  # "     = 



  


# $

0$ " 
 1


 

  	 

 
 
 C+ 
   	 
	 # " 


	  

 
   
   
   	 	  
  			  

 
   >?     


   
  	 :    
 

  


			
   # " 
L     
  # "    !
 			  
  # "   
 
  
	$
- ' !)	 .
 

 /'#0

 2
	 -  

	    	 
	  # " 

   
                G


  		
+-
O










 



 



% 





 
     
      
 
   

 2
	 - 
 
 ' 
  
    


 @
   
   
  

   
	   >?      
 
% 











# $

-

' !)	 + 

 /'#+0


 
 

  # " G  
  	 
 
  			 


 
    2  	  
	  # " 

  
 G
 
+;
 O 
  P 4

 
   
 
  
 


   	 

  
   	   

       	


	  
 


 
  
 

 
    			

 
 #F 78 
  





 

			  

	

7 % 	   	 		 	    	 7    	 	 ' 
	
		  ' 	 	  	 		   	  	 &'  	

		 
3	 	  	   	 		'  3
 &	 		
 '  	 6	 
3	  	 	 	 	 
	 	&  	&  
@	  	     	 
 	 &	   	  )D	4 6 		

	 	 5 + C'	  	 
$

475

 

 
     

( )*
 ) 

3   
 
 
 
 C     
  
 


   &
  # "    ! @7  #F 
78  
 
    
	     

  
 '   
   
    


2  
 
	          	
 


	 
 

	 
	 
  0
1 &2 	) ) 


  K
   
 	 ! 
  

    
   	
      
 
   

   
	
   
  
     
 	     
    
	 	
  
	
               
 
	
              
     O 
 
  
      

 
 
 
    !
 

  
	        ! !   
       
  	  	 !	   !

 


  
  
	    
 
 
 
 	 
	   


       
  
  

   	

	   

 
  :  
 
 



  >- 3  ? D( 
 
 
 
 !  =

  
       D( 
    
  
 


 =      
 % $        
 
 
  4    O +	 	
    

   
    	
 

$  
	 	
    	

         	 

     D(
+	    
 

   
   
  D(    

+	

  
  =
            


 
 D(   	 
  
        

	 
     
   
 


 
     D(
    !  	   
   
 !  =  
 
&

'

!

'

!

!

9  /F)  	 	   
	      	 	 	 	 	 	 
' '
? :	 /F)  	  
	   	 	 	 	       	 	 
	   		  	  		  		      
		
	 	 
    	
 /F)

476

   


1

) $	 	) %)) 

    
 
 
 
 <44      
 
   
 
  
    
 

  
       
 
  

     


	 
 
 

	    / 
   

  &

 	 
  
	    	
 
 
    
  
 
  
      '
     

     


     
	
 " 
       
 

 
       
 
    
 
         

   
  
  
 A
   	  	 
     

  
    
   
  
 
         
   
  

 	       	

    
      
  
   	 
   

 
	 '
        
  
 
 
    

   
=  

    
    

    
  	 !	   !
	 
    
 
           


 
 
    
    
     
	  
   
      O 
      
	

	
 
  

 
   2 
&     

&         

  
& 

 	

    
& 	


'





!

&





!

1 $
 !)	
	 +


 	 
    
& 	   

 
 C+ 
  /   
 	        


  
   !	   	 
 
   
 

 
	 
 

  	  
 
 
   ! 
   	 
 
   '
  
	    
     
 
	      @7   


  	   	 
  
 
	  
	      

	 '  
 ;++  
     
	  
 	 
 	    	  
   
	 	 
 # "           	 
 
    
    

 
 C-4  C-+ 
 
 0 
 
   	 	   

   
 
	  
  	    $* 	  
  	 	       	   	 
 
  
	  
    &  
	 '
   F   








. # ' 	 	  	 & A5  	  	    	 	 
	 '  	  	 	 	 
 	 & 	 		  BC 
A>6C+ %&' 	 & 		 &	 	 	 	 A5 
	  	 	  
 &	  		 	 	 	 A5  	 
  )		 	   7$

44.

 

 
     

2
	 ;G -C  
	 	


  

  #F 78  	 
 
	   
 	 

  
  
   
 


 	     


' 
 

 
     @   	  
 <+
 
   	 &  
	  	 
     

 # " /  	   #F 78 
   
	  	
    	          Æ
 
 	
 

 	 J      	  
	 
 
Æ
 
	  	
          
  
=     

 	    
 

	  	E  !	    
    
 

	  

   O 
   	
    	 
 
 

  
	  
    

 
 

 
  
 
      
 	   


   	 
 
& 

 	
    	
 
 	 












$ 7   


$

  


2
	 AG 7 ! 	


2  ! 

 
 
    @7 # "  # "  #F 78   	

  
 
 
 <4
441

   


9
       
& ! 	
   

	      
      2
	 ;  A 

  -C 
	 	
    	 	
 J  A,
 	 2
	 A   4,,  	 2
	 A  -C 

	 	
 	    
 
    

  78 
 !

	  
 ++   

        
  ! 


  
  
   
 	  	
    
	
  	 =	 A  A      
& 
L$+ 
 =	          	   


    

  E 
 	 0    	  

	     
 
!    

  
 
 
 


 
  

 	  
 
 
	       
   

 G 
	  O ,,- 3L 
  O ,4 
 
  -C A,  4,, 
    	
 
 =	 ; A  A 

 :    	   
 
  
		 	
  	
  
 	 
L   !  	  
  +, 
   
  
 >? 
  ! 
   
	
	  

 
 
  
	
  

  
  ! 
  
	

 

  
    & 

& !     "    
	
  

     :    	
 
	  
  
 

    
	

 

 
 

   	 


   

  
 	
	  

    
	




 
 	 
	     

  :   
& 
     
	

 

 	 
  
	 

    
&  
 	   
     # " @7  #F 78
  
 
	   
	

 

 
 	  ' 
 

 

  
	 

    A,,,,, 
    	 

2
	 ;  4,,,,,, 
    	 
 2
	 A   +,,,,,, 
 
  	 
 2
	 A 7  	  4, 

   =	
 

 
	
=   5AI =       

  
  4, 

 
	 
 
         
 


 

 
 
	  
 
 <+4  &
   

 

 
	 	
  
 
 <++      


	 	 
 
 <+-


"

1  ')	
 3 
  $


: 

   
	     
    
   	 
   
	    

 
   

 / 
   
	 
          


  D(     
	       
=    
  
 
	 
- G 	 	
 &  &	 '     	'	 
		  	 
 	  	 	 & ' %  & 
	  	 
 	


442

 

 
     

 ! : 
 
  	
  G 
    
 

  	 

        D(  
 

 
    	 
       
	     	 D(     
	  
    

   	  4, 

    	
 
   
	

 

 2    
  	
 

 
	

 


    
  !      
        	

 +G "     	
 J 	  2
	 ;
&	
.1
.2
.4
.-

9
3.64
7241
1666
1458



(	
...45
...74
...15
...12

:;<(
82--.-6
3378
3458

9
3.65
72-4
2.28
13.8


(	
...8...78
...21
...11



	

:;<( 9 (	 :;<( 9 (	
8211 

 ...32 8.23 2384 ...22
-.3-  ...76 355- 1664 ...13327  ...2 3286 1452 ....6
3744 
 ...14 3.- 1132 ...12

:;<(
6638
68-3
5644
532

 -G "     	
 J 	  2
	 A
&	
.1
.2
.4
.-

9
3871
7811
2265
1818



(	
...42
...2
...12
...1

:;<(
5183
-683
-144
355

9
3513
7872
2773
1848


(	
...32
...25
...12
....6



	

:;<( 9 (	 :;<( 9 (	
51-8 
 ...32 5.6 1215 ...28
-587 
 ...2- -577 1.21 ...11
-2- 
 ...17 -.47 84- ....8
-126  ....5 3837 -11 ....4

:;<(
6666
665
665
665

 ;G "     	
 J 	  2
	 A
&	
.1
.2
.4

9
7386
2418
1355



(	
...72
...18
...11

:;<(
--32
3841
34.1

9
7352431-1-


(	
...74
...2
....8



	

:;<( 9 (	 :;<( 9
-335  ...7 -2-3 537
3615  ...14 3428 -12
3337  ....5 3.82 486

(	
...71
...17
....-

:;<(
6655
6657
666

  
  +   	     
  

& 	
   D(  	
  	
  
& ! 
  :	  : 

	 
	   	 &  &	 '	  &	
	 &   
  3	  		$  B6' 		 	 &	 '	
  &	 '	    	 B6' %&' 	   	   '
	  &   	 		 &  		  	 
	  

447

   


 
	

 


 
  	  2
	 ;  -  ;   
   	
 
 2
	 A  A 
   


  	         > 	? 
  

 
 # " 
     	
  
& ! 
2  
  +    
 ,4 # " 
    	 
   A46AI # " 
 A,56I @7 A,5;I  #F 78 +A<;I 


   
  
 A # " 

   

  
!   
	 # "  	    !
	  
 


 
	   
 
    
	 
!
   
 


  
  

 
	
= 
	    
  # "  
 	
  
 
  


 

 
 


   
 


 
 	  
	      	

 


-15

PTC-A / QR
PTC-M / QR

% improvement of deviation from IZDS (relative to TPORL)

% improvement of deviation from IZDS (relative to QR)

0

-2

-4

-6

-8

-10

-12

-14
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6
Call origination probability

$ A5 	 BC

PTC-A / TPOT-RL
PTC-M / TPOT-RL

-20
-25
-30
-35
-40
-45
-50
-55
-60
-65
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6
Call origination probability

$ A5 	 A>6C+

2
	 CG      

  D( 
  @7  #F 78
J 	  2
	 ;
:     
 
     
  
# " 

 	
 
    @7  #F 78    = 

  

  D( 
   O +	
+	  
 <44 '
%)  , 
 , -,,  -,)*)* ,  
                  ! G
%)

  	 
 2
	 C   
 
   	 
 

  D( 
  
	 # "  @7 2
	 C   #F 78
2
	 C   	 
 2
	 ; 
  0  
 
 =	   




 
     
 	  )*+   ,-  .-/ 
  
  	   %

 2  	  
	
= 
 
  

# " 
   @7  #F 78  
&   !  



	
= 
    5AI =  2     
	
 >8? O ,4 
  + 
   
	

 

  ,4  

  
     D( 
 +C+I O $*   # "  +55I O + 
444

 

 
     

1

-20

PTC-A / QR
PTC-M / QR

% improvement of deviation from IZDS (relative to TPORL)

% improvement of deviation from IZDS (relative to QR)

0
-1
-2
-3
-4
-5
-6
-7
-8
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6

PTC-A / TPOT-RL
PTC-M / TPOT-RL

-25
-30
-35
-40
-45
-50
-55
-60
-65
-70
0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6

Call origination probability

Call origination probability

$ A5 	 BC

$ A5 	 A>6C+

2
	 <G      

  D( 
  @7  #F 78
J 	  2
	 A

2

-25

PTC-A / QR
PTC-M / QR

% improvement of deviation from IZDS (relative to TPORL)

% improvement of deviation from IZDS (relative to QR)

0

-2

-4

-6

-8

-10

-12

-14
0.1

0.15

0.2

0.25

0.3

0.35

0.4

Call origination probability

PTC-A / TPOT-RL
PTC-M / TPOT-RL

-30

-35

-40

-45

-50

-55

-60
0.1

0.15

0.2

0.25

0.3

0.35

0.4

Call origination probability

$ A5 	 BC

$ A5 	 A>6C+

2
	 6G      

  D( 
  @7  #F 78
J 	  2
	 A

443

   


+    D(
 @7       # " 
 4+-<I O ++
+

    @7    

   D(  # "  
 

  @7 

 
	 # " 
 #F 78      # "

  +C+I 

  D( 
  
	 =	  #F 78 
 <;4AI
$   
O ++++        # " 
 C;CCI O 
$
D( 
    #F 78    # " 
   # "
	     @7  #F 78    
  
 

 
 

    	 
    

  	  
 
 
 A   

	  '


 # "     D(  @7  

 
 
 

 

	   
 
  # "  @7  #F 78 
 
 2
	 C  
  # "  @7 
 4+-<I 
   ,4 


 
  -<I 
   ,C 

 
 2
	 C # " 
 C;CCI  
#F 78   ,4  466+I    ,C 0   
 
  



 
  
  
 ! 
	 
 
  
	
 
' 
 
   
 
 
 A       

 
 ! 
	 
      
 	
 
  
 
     
&  # "  
 	
 
:  

  
    # " 
   !

   	
 2
	 <      	 
 2
	 A 
2
	 6   	 
 2
	 A

 
 !         

       


        !
  
   
        

	
	   	
  
 


	 	  
  2
	 5
  


       # " @7  #F 78   

	

 

 
 
  ,4  ,C 
  

  
	  	 
2
	 ;$     # " 
 
    # ". # " 
 
   
 
  
	   # "    @7  #F 
78    
  
	      
 ,4  
 O A,
  
 
  ,C  
 O A,       	
 & 
 
  
 
   
     

 
  
       
 
     + : 

 
 
  
   

  2
	 5  
     	  = 
& G ,4 ,+ ,; ,C  ,6

      # " 
     
	     
# " @7  #F 78 0  
       
& 
  
G  	 
 ,4  ,C  
 

	 
         

    
 

	 
= 

 


 	 #
     
 
 

!  
   

   
    	 
   	 
    
 

       &
  
  # ' 3
	 &	  	 	 	
  	     
 
		 '  	 	  	    	 % &   

	
 	 
	  	  	 



44-

 

 
     

  
 
 

 
   
 
    
	   
Æ
 
  

   
	   # "  
!  
 

  
	  

     
  
0.45

PTC-M
QR
TPOTRL

0.45

0.4

0.4

0.35

0.35

0.3

Call success rate

Call success rate

0.5

0.3

0.25

0.25

0.2

0.2

0.15

0.15

0.1

0.1

0

10

20

30

40

50
60
Time

70

80

90

PTC-M
QR
TPOTRL

0.05

100

$ #	    

0

10

20

30

40

50
60
Time

70

80

90

100

$ #	 7   

% Difference in call success rates of QR and PTC-M

2
	 5G 
 

      @7 # "  #F 78 
 !
 
 J 	  2
	 ;
-2
-4
-6
-8
-10
-12
-14
-16
min
avg
max

-18
-20

1

2

3

4

5

6

Degree of dynamism

2
	 4,G  

    
&  @7  # " 
 
!  
 J 	  2
	 ;
0  
  
  &  
 	
	    ! 
  	
  


	 	   
	  = 
	  
   
  

    >	  
? 2  2
	 5 
 	  
  +  2
	 5  A 	  
    	

	  
    	 
&       @7
448

   





 , "    

 , "
  ,  -,)*, "
 #F 78 
	 # "   
 G %),"
 

	    0  
 
&  
  

 
  
     
	 # "  =  

 
   
  
 
&   
 
 
	 
  

   

      
 	 

 

 
    
   
  '    
 

     

     
 
&  
 
   
  	     
& 

	    
 2  
 2
	 5  

  

 	 
&   
  
   O ,4  
 

   O ,C '     

   
  
& 

   2
	 4,   
  
 	  
  
  
  @7 
   # " 

#F 78 
    
         # " 
@7     
  #F 78 
 # " 
 
  	
 
L 
  2
	 4, 
  	  
 
   

 
 	   
&  	  
 
  
 =	  	 

 + 

    
 	  ,4  ,+. - 

  	  ,4 ,+
,;. ; 

 ,4 ,+ ,; ,C.  A 

 ,4 ,+ ,; ,C ,6  	
  
    
& 

  
          =	 
  	   

   
 
&  
   
	  
 2 
 
 	 
 
 6I 
 	  

+  
 
  ;AI   	  
 
 A 
 
  
  
	
   	 

  	
 

   
  
   	 
  	 
  
 /       
 

    
  + 0  	 

    
 
&   # " 
  
	
= 
	     
	  
 2  
 
  4,+I 
 + 	  
  5AI

 A 	  

 
 
      	
 

 







  	 	  	
  			   
  
      	   
 
(  
 
 
 
  
 
	  0  


 2  
    


      D( 
   4+-<I  
 # "  @7    CAI
 
 # "  #F 78 2 
 
 	
	  # " 

 	 4,I 
 
     @7 
 = 
&  	

  

 
1 

')	
 3  $
 )	 
 	) %)) 

      
 
 <+4 

    # "
 @7  #F 78  

    &
   


	


 
 
	     

  
   	
 
   
    
 <4+  
 =

 
   

	   
7     
   
 
 
 :
 

	
	    

  
 
 
 <+4    


    # "   !      
  
!





445

 

 
     

 AG "    
 
 J 	  2
	 ;
&	
.1
.2
.4
.-

(
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&

1



.5-7
.538
.-77



.887
.8-5
.354


.-83
.--8
.46-


.-1
.36.415

$
  

7
4
3
8
 .-7 .31 .42 .78 .733
.845 .-72 .31- .42- .784 .733
.837  
   
.372 .73- .215 .1-4 .145 .12
 .44 .71 .222 .185 .135
.357 .44 .713 .271 .151 .1-4
.357     
.4-2 .2-- .176 .1.1 ..65 ..5 .2-- .1-1 .1.7 ..84 ..-4
.418 .282 .18 .111 ..51 ..-5
.41-     
.783 .165 ..5- ..36 ..-7 ..34
 .153 .1.7 ..-2 ..43 ..78
.727 .16 .111 ..-5 ..48 ..4
.722     
.7.1 .145 ..-3 ..47 ..4- ..78
2

5
6
1.
.747 .744 .74
.743 .745 .734


 
 

..64 ..81 ..36
.148 .148 .176
.133 .131 .131



 
 


..-- ..3- ..45
..3-7 ..37 ..3
..- ..33 ..35

 
 

..4 ..78 ..26
..7 ..26 ..28
..73 ..72 ..7


  

..27 ..13 ..17

 CG "    
 
 J 	  2
	 A
&	
.1
.2
.4
.-

(
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&

1


.58.587
.158


.88
.8-.214



.-45
.-4
.2.2




.3-.338
.188

$
  

7
4
3
8
 .- .3.4 .425 .786 .737
.84- .35- .463 .42- .751 .7-3
.837  
   
.126 .11 .1.2 .1.4 .176 .2.7

 .781 .2-5 .168 .1-1 .141
.3-1 .7-6 .2-3 .2 .1-7 .143
.3-4 
    

.128 ..53 ..-3 ..33 ..- ..-1
.757 .2.7 .12 .852 .35 .462

 .2.5 .12- .525 .-71 .37
.754     


.1.6 ..34 ..23 ..21 ..24 ..45
.25- .177 .812 .425 .7.3 .246
 .14 .851 .451 .748 .258
.258   
  

..63 ..4- ..18 ..11 ...8. ...3.
2

      # " 
 
  

 
 
 <+4 
       
& 
    

 
 



446

   


 <G "    
 
 J 	  2
	 A
&	
.1
.2
.4
.-

(
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&
'+
)=9
)=$
)*+&

1

2

7

 

 
.56 .846 .356
.55 .825 .365
.22 .182 .14

  

.578 .-25 .43
.527 .-22 .445
.2.2 .135 .11

 
 


$
  

3
8
5

 .724 .248 .2.5 .183
.472 .718 .248 .22 .168
.447    
.1.4 ..86 ..38 ..31 ..43
 .156 .127 ..65 ..86
.258 .185 .124 .1 ..58
.262    
..82 ..48 ..72 ..26 ..23
.18 ..64 ..38 ..41 ..71
.1-1 ..6 ..38 ..42 ..74
4

.8-- .454 .265
.84- .48- .7.7 
.156 .141 ..6 ..3  
 .112
.8.- .767 .21- .1.6
.-55 .75- .221 
.181 .128 ..86 ..48

6
1.
11
12
.146 .12 ..58 ..52
.1-6 .141 .111 .1

   
..78 ..26 ..25 ..26
..-1 ..48 ..7- ..26
..81 ..3- ..4- ..76

  
 
..1- ..12 ...5 .....22 ..18 ..11 ..12
..2- ..21 ..143 ..13

       

..74 ..2 ..1-8 ..11 ...- ...7 ...2 1. 4
..- ..7 ..24 ..15 ..12 ...6 ...- ...3
..38 ..71 ..23 ..168 ..14 ..118 ...5 ...83

       

..28 ..13 ..1 ...- ...7 1. 4 1. 4 1. 4

  
  A C  <       
&    >

 ?   
& !    	
  =	 ; A  A

   

  
	  2   
  
  
   - @7 
  
	     # " /
   
  	 
 
 
 # " 	
  
  @7 

 # "    #F 78   
  
 
 
   
 	   # " 	
  @7  #F 78

 
     
 

  ,

 
  
	  # "
	  . ,	  -, )*
	 ,  . %)
	
	
  @7  #F 78
 ,  -, )*  
 
%)
	
	
    	 
 

    

 
 

 2
	
 44   	 	   
  
	 # "  @7 2
	 44
# "  @7 2
	 44 # "  #F 78 2
	 44  # " 
#F 78 2
	 44  

	    

     
 

  
  
	 	
 	  2
	 ; 9  
   
=	 
   
&     
	

 

  	  2
	 4+
 4-  

  
	  	
  2
	 A  A 

2
	  =	 44  44 
 
     	
  
	

 


    

        
   >


 ? 
    4  + @7  
	   
 # " 
# "  

    	
 

 2    ,C @7 

  AI 
  # " 
 
	    4    2
	
 44 / 
 

	 
      

 
!





!







(

)



!

  3 '  	  
 	  	 

	  	 
 	&

43.

 

 
     


   >

  ? 
 -       
 

 
	   # " 	
  @7  

   

 2
 
 2
	 44      ,C # " 
   4AI 

  @7 
  
      
 

   6 
     
 
 
 
   	 
 



	 
   # " 

 
	 	
   &
 
 

	   
    

      =	 44
 44 # "  # "    #F 78   
   
 
20

load
load
load
load

20

0.1
0.2
0.4
0.6

10

5

0

10

5

0

-5

-10

0.1
0.2
0.4
0.6

15

% improvement in success rates

% improvement in success rates

15

load
load
load
load

1

2

3

4
5
6
7
Minimum hop count

8

9

-5

10

1

2

3

$ A56 	 BC

load
load
load
load

500

4
5
6
7
Minimum hop count

8

9

10

8

9

10

$ A56 	 BC
500

0.1
0.2
0.4
0.6

load
load
load
load

450

0.1
0.2
0.4
0.6

400

% improvement in success rates

% improvement in success rates

400

300

200

350
300
250
200
150
100

100

50
0

1

2

3

4

5

6

7

8

9

10

Minimum hop count

0

1

2

3

4

5

6

7

Minimum hop count

$ A56 	 A>6C+

$ A56 	 A>6C+

2
	 44G "    
 
 J 	  2
	 ;
 
 
 	  # "   	   
  

 
   
 	   
 


 
    
 
	 

   	       
	
   

 



	  
 
      

 
 
  
 
	  
	   
  

   
  
	 
   
431

   


25

load
load
load
load

16

0.1
0.2
0.4
0.6

20

0.1
0.2
0.4
0.6

12

% improvement in success rates

% improvement in success rates

load
load
load
load

14

15

10

5

10
8
6
4
2
0

0

-2
-5

1

2

3
4
5
Minimum hop count

6

-4

7

1

2

$ A56 	 BC
1000

load
load
load
load

1000

0.1
0.2
0.4
0.6

7

load
load
load
load

0.1
0.2
0.4
0.6

800

% improvement in success rates

% improvement in success rates

6

$ A56 	 BC

800

600

400

200

0

3
4
5
Minimum hop count

600

400

200

1

2

3
4
5
Minimum hop count

6

0

7

$ A56 	 A>6C+

1

2

3
4
5
Minimum hop count

$ A56 	 A>6C+

2
	 4+G "    
 
 J 	  2
	 A

432

6

7

 

 
     

60

load
load
load
load

40

30

20

10

30

20

10

1

2

3

4
5
6
7
8
9
Minimum hop count

10

11

-10

12

1

2

3

$ A56 	 BC
1000

load
load
load
load

1000

0.1
0.2
0.4
0.6

10

11

12

load
load
load
load

10

11

12

0.1
0.2
0.4
0.6

% improvement in success rates

800

600

400

200

0

4
5
6
7
8
9
Minimum hop count

$ A56 	 BC

800

% improvement in success rates

0.1
0.2
0.4
0.6

0

0

-10

load
load
load
load

40

% improvement in success rates

% improvement in success rates

50

50

0.1
0.2
0.4
0.6

600

400

200

1

2

3

4
5
6
7
8
9
Minimum hop count

10

11

12

$ A56 	 A>6C+

0

1

2

3

4
5
6
7
8
9
Minimum hop count

$ A56 	 A>6C+

2
	 4-G "    
 
 J 	  2
	 A

437

   


   
   
 

   	

	   ! 
 
 
  
 
  
!    
     
  
 
   
	   
      :

  
 
  # "   
  
  
 A 
	 
   !
	  
	 

 
   
	 
  



  
	 @7 
 
    
	    
  # "
	
  @7        	 
  
 

	

  
	   
Æ 
    

  
   
 1
 
  

	  

 

  
  
 

	 
   
 2
	 44 

  # " 
 
  &  
 @7  #F 78     	 
 # "

  &
 
 
	 	
   # " 	     @7
 #F 78 
 
 
   
	 
	 

 

  # "  @7
2
	 44    # "  @7 2
	 44    
	 

 


 # "  #F 78 2
	 44    # "  #F 78 2
	 44
    
	

 


  
  2
	 44   
 =	 4+  4- 

 

   =	 
 
     	
 
   

    
 	 
	   
	  
	

 

  K
 
 
 
      	
 	  
 

	   
 <+4
 
   
   
	

	      


      

     
 0 


 	  	  
    	   

 

	    


	  
	    
 
 

 
 
  
  
 @7  #F 78    # "
	
 
   
    
  
 @7  #F 78  
 # "
 # " 
 

	  /  

      # " 
 ! 
 
  2  
 2
	 44    	  6 


   
  +AI 
  ,4    4AI 
  ,C
 
 
 
 
   
   	  
!



!

!

!

	        
     	  
  
 
 (         



 

   A,I 
  
      
  # "  @7
 
  4+  
  4,,  	 
  
	 !    	
    ,C 
 2
	 4- 2     # " 
  	

  #F 78    4,,,I  2
	 4-
1  ')	
 3 !)	
	 +
 $


    	  
 <4-  
 =

 
 
  

! 
   
    

  	    	
 
 
 
           
   	
 7   
   =  
   
 	
	 
 !  6   	  
 
    	
 
  

 
  	  2
	 ;  5  4,     
	
  2
	 A  2
	 A 
      
 
 
434

 

 
     

   	
  

  	    > 	? 
 
	
=
 
  # " 	
   @7  #F 78 2  
  6 
 
 ,+A
 # " ,+C 
 # "  ,-5  @7  ,A+-  #F 78 
     ,4
 

 
 
   
 
   	  
 >I 
	?    
  
	 , # ." 
 






%) 
 

 ,  . 
,  -, )* 


 

	      



%)

,  -, )*
   	   	    	
 

 	
  	
     
   2  
  6   ,4
# "   -ACI O $++     	  @7   A+I
O $    #F 78 / 
 
	 
  	  
 
 6,-I O *$$  
  @7    <+6I O *$$  
 
#F 78    

 
 ,C 
 
  
 

	 ! 
 
 
    	 
  @7  #F 78 
  
	  

 
  # " 	
 	 
  6  	  
  ,-5
 4;A 
 @7 J  +<+I 
  ,A+-  4,A 
 #F 78 J  4,4I 
 
 ,+A4  ,+6A 
 # " J   4-AI 
 J    
  ,4 
,C










(

)



 6G 
 	    	
 J 	  2
	 ;

	
.1
.2
.4
.-

'+
9 (	
.76 ...45
.-- ...86
11 ..1
143 ..123



  !

)=9
)=$
)*+& !"%.  !"%/  !"%. 
9 (	 9 (	 9 (	 (&
(&
! $!%&'
.2- ...16 
 ...16 .327 ...27 7777 73-4 3.25
.251 ...2 
 ...22 .-57 ...26 3842 3577 3553
.256 ...15  ...1- .561 ...7- 8787 8415 -83.2564 ...18 
 ...13 1.3 ...88 5..4 5.74 8247

!"%/ 
! $!%&'

32..
3687
-512
8253

 5G 
 	    	
 J 	  2
	 A

	
.1
.2
.4
.-

'+
9 (	
.77 ...27
.33 ...45
.6 ...46
12 ...34



  !

)=9
)=$
)*+& !"%.  !"%/  !"%. 
9 (	 9 (	 9 (	 (&
(&
! $!%&'
.27 ...12  ...14 .836 ...34 7.7. 7.7. -6-6
.2- ...17 
 ...12 142 ...84 3287 3433 51-6
.28 ...1  ....6 21-7 ...61 8.. 8111 5831
.282 ....8  ...12 28. ..1.- 8877 8883 5662

!"%/ 
! $!%&'

-6-6
5276
5868
6.11

0  
 @7   	 
   

 	   
	
 	       
	   
 
  

	 
  	 
 
    	      
    #F 78    
  

 
 
	   
	  


  
 #F 78  
   
  
 

 	 	 
	  
	 
   	
433

   


 4,G 
 	    	
 J 	  2
	 A

	

'+
9 (	
.35 ..118
.57 ..112
12- ...65
1-7 ...63

.1
.2
.4
.-



  !

)=9
)=$
)*+& !"%.  !"%/  !"%. 
9 (	 9 (	 9 (	 (&
(&
! $!%&'
.214 ....6  ...14 142 ..15 -71. -786 5462
.273 ...1  ...1 216 ...61 81-6 8241 5628
.246 ...1  ....5 265 ...-1 5.24 5.58 61-4
.237 ....8 
 ....5 778 ...86 5445 5468 6246

!"%/ 
! $!%&'

5321
5634
6161
6282

 
 
     	         
	
      	 
  # " 	
   
 
  
	
= 
	 
  	   
	  

 



      

	  
	 	   

   
 
  
	   

      	 	


   

 	  
	    
   3
@7  #F 78      
 
  	
	   
  
= 
    

  # " 	
   	  
	
	  @7  #F 78 
        3  	

	 # " 
 	     
       
     
  	    
	 
  



 
 ! 

        

 <+4  


   

 	      	

  !   
 
  

 
1.6

1.8

PTC-M
QR
TPOTRL

PTC-M
QR
TPOTRL

1.6

1.4

1.4

1.2

1.2

Message rate

Message rate

1

0.8

1
0.8

0.6
0.6
0.4

0.4

0.2

0

0.2

0

10

20

30

40

50

60

70

80

90

100

Time

0

0

10

20

30

40

50

60

70

80

90

100

Time

$ #	    

$ #	 7   

2
	 4;G 
 

  	   @7 # "  #F 78 
 ! 

 J 	  2
	 ;
43-

% Difference in message rates of QR and PTC-M

500

max
avg
min

450
400
350
300
250
200
150
100
50

1

2

3
4
Degree of dynamism

5

6

$ BC 
 	 A56

% Difference in message rates of TPOTRL and PTC-M

 

 
     

500

max
avg
min

450
400
350
300
250
200
150
100

1

2

3
4
Degree of dynamism

5

6

$ A>6C+ 
 	 A56

2
	 4AG  

  	  
&  @7 #F 78  # "
 
 !  
 J 	  2
	 ;
2
	 4;   	  

  # " @7  #F 78 
   	  ,4  ,C 
  

  
	  	  2
	 ;
2
	 4;       
 
  ,4 ,+ ,; ,C  ,6  '

  
 
     

  3   =	   # "
   
	
=  	    @7  #F 78  
	
 

 2   !  
 
  @7  #F 78

  	 
 
  	  
  
 
 # " 
 

	
=  
@7  #F 78 

 
 
 
	   	  
 
 #F 78   
   
  /    
	


 

	 !      

 	
 
 
 
	
  
    
 

	  #F 78    	 
 @7 : 

  
 
 	  #F 78 	
 @7 
   

     @7  
 
 <+4 
 #F 78 
  Æ

 @7 
 
	  
 

  
 #F 78  	    
   
  	 

 
  
	 #F 78
  
     
 
    	  
  @7 



	 
   

      
 
	     

& 
 	   @7  # "   #F 78  # "
  
 =	 4A  4A 
     =	  	 

 
  	  
L 
 
  	 
  	 

	 @7  #F 78 	
 # " 
  	  
 
   

     2
	 4, 
  2
	 4A  4A  	  


   
 
&  	   
 

	 	 
 2 
 
 2
	 4A 
 	 
  4;,I 
 + 	  

 
 
 C,I 
 A 	   0   
&  
	   # "  @7    # "  #F 78 
 

438

   




	 	  
 2    
& 
  4A4I 
-+4I 
 2
	 4A 
 
 
  +,,I  +6;I 
 2
	 4A 
 

 
	  # " 
   


	  

   	  
  @7  #F 78
2  
 
       	  

         !  123$ 
  (    
   
  
  
 !  122$    	  
          4     	 
  

 
	   
 


2  # " 
  6,I 
	 

	    @7   <+I 
	   #F 78 
  	
 	
 
	 !  2 
  
  
	 
 = 	 
  !
 # "   -+,I 
 	   @7   +6;I  #F 78
 # " 
     	 Æ   
 
    
 

     	

+ 

 
 , 


     

 
  
	 	 
 
 

  

    !  '
  


  
 	 
 
  
 
 	 
 
 '
   	 ! 
 

  
      	    ! 
  
  
 
 0 
 
	   
     
 
 
Æ 
 	    
  
Æ 
 

	  K
 
 
   
      K  
 	
/ 
  
  	      
	 
 
 
 
	
1
 
    
 

 
	 
  
  &
 

    	  
    " 
 
 

  @
	     
 

     
 

= 

  
     

    

 



 

 J !
 

 
	 J  
  
	  
 
  
     
	   
	E 


   
 
	  
   	 
   


	
  
  
	   
 


  

 2
  
    

 

    # "
	 


   
  
 
	 
   
	 
  
! :   
 

 
  
	  	 	

   ! 	
   
 	  
 
	  
	

& ! 	
 !   
 	
	   7
  
       !   
 

	      C,I   
     4,,,I 
 


      	 
       
	 
  
   
	
=  	    -,,I
  !
: 
   # "
	 
   
  
  
	 
 
  

 	 
 
  
  
  
 A  


= 
   / 
= 

 

 
435

 

 
     

 # "  # " 
 
    
	  
 
   	

   
	   	 
 
   
 	  ! 

 
 
 
 ;  
  
   
	  # " 


  

  
 
  !   

	 

	 


 
 

 =   


	 	 
  
 
     

 
 
    

 
   

 
 



     
 
 =
 
    !   
  
 	 
 '   
 
  	
  


 	   

 	 
  

  
 
	
 
 	 
 
 
  
  &
   

	 
	 
 
   
      ! 	 
 
 
	  
  
	
  '
 

 !
	 
 
 
 7 ) 0
	 +,,+
   
	 
  
    
 ! (

 #
(9"(# 3! D

 8 ) 1 +,,-. 3
 1
 
) D

 +,,+  
      	  ! 
 	
 

   
  
   
   

 	 

 
	
=  	
	 
	=
 
   (9"(#   

 
   3

	  
 
    

	   
	
	 


 	 
       
   '
  

	


'

F 
 !   
  

  

	   
   
   
 : ! B	 8  1 

(   3

  
   		
 2
  

	  9#7"  
	  K .
 5 ) %
 	 &!
F/" 0 17%7-+C5<%,4  
 
 ! 
  




 ( 0 3 # 3 *  ( 9 / M
  M! " 

 0   )  " : +,,+ "

 	 
G 
	

 
 

 

 ) ) 	 31 - +CU-A
3 * ) 3 # 8 4555 (
 	
 
 
	G  1

 

 	
   7   
 
 
9	

	 
 0
 $


3! 7 D

  8 H ) 1 " H +,,- 




L ! 

    
    
 )
6 %   	  
  	 ! 2778$ 
;4U;6  " # 0 V!
3
 ( 1
 7  0 ) D

  +,,+  
  

L   ! 

  	   . - 
21 ; 645U6;,
436

   


3

 " 4555 '
 

  

 
 
	  
 
     ) 6 %   4  ) 
!)6%)99$

  ;<6U;6A !
3 *  ) 8
  8 455- #! 
	 
 
 	
	 !G

 
	   " * (  1 )  * 9

   & )	   	 H C  C<4UC<6
" M  ) 1 8 4555 "
 	

   :
 1 9
 	:  
    +
 4  )   +55U
--,   # "
	  
" 1 ( ) (
	  4556 0G (

 
		
   


 ! 6  4  )  -  9 -4<U-CA
" 1 ( ) (
	  4556    	
  & 
	 

	 !   
    )+ ) %  
 
 +
 %	 
 	  A;4UA;C   9(% "
#
"
 3 455A 
 

  

  %		   
% 8; 44 ;5UA-
"! ( ) *
	 0 7 455C G 

 
=
 

	 
 

 

  FE/ 1  # ) *
	 0 7 9 <

 +
 4  )   -45U-;; :

" # 7 ) 8' / * 4554 ! & 8= ; ;6<UA4+ 
  
"	

 
  
=
 
	
"  / 8
 " 9 7
 7 8 ) 
 " +,,4 )
   	
+ 

  +;G 
	   #  #
(! M 455A 9G !  
  
  
	
 

 
  FE/ 1 ) *
	 0 9 <
 
+
 4  )   4C :
 

(! M  455A 	 
  
 
  
 	 	
#( 
 $

    
(! M  ) 8 H 7 455A (
	
	  
  

 	
 
 
   < ) %    	  <-U6,
 2

(! 3 D
  ) :! # +,,; 5 
  	   
 H +<;;%+,,;  /  &  %	     
	
 
	  
	 
    
  4,, U 4,5 
	
H	 /
	
( 9 / ) 8 H 4554 #
 	 
	G 

 !  


 
 
 )    	  
 % 
23 A 44C<U446-
( #  (  1  7 *
	 0 7 )  8 +,,; "

 

 
	  
 

 
	  7  
/ 
   "
 	  46U+-
4-.

 

 
     

( #  *
	 0 7 )  8 +,,A 
	 

  '
	
! 
 

  ! 
 
    

  < ) 6 %   	  
 
	 ! $


( #   8 ) *
	 0 7 +,,- 2

	 

  
	
	

 

 	
  #    )6%)2778  
% 
   
  )   ;CUAA
( #  )   +,,- 2
	  
 % 	 - 
 - +44U++4
9 ( 1
  ) :! 8 +,,; #  

  G 7


	 ! )     	 39 4 ;+<U ;-A
2
	 9 ) L  +,,4 5
   +   : 
 

  M 
 #

1L 3 ) M  455C "
    	 
 4 
)  ;> + +C5U-A<
/
! " 4566 - )	   !-<% 37=;$ 0! :!
	 1
9 2 	
*
  ) (
 " +,,+ #G     


   
    
   	 !$ #
 4;U+A
*
	 0 7 455- "
  
G  
  

 


	   ?
  - ; - ++-U+A,
*
	 0 7 455A "
	 
  
	 
 

 
	
 
	 K
 

 4  )  1= + 45AU+;,
*
	 0 7 ) 3  +,,- 	   ) % 	
@ 28 - C4U<;
*
	 0 7 0  * ) 2
 # 4556 G  	  

  	 % )'.+ 21 ; -+U-5
M	  ) 3
  +,,; #  & FE7
 :
 (
		
 
8
 M ) 3!  4555 
	 
   
   ) )&<.%.
!3$  +-AU+;A
8 H 7 ) 9 8 ( 4566 (

 

G   

 3  / ) 1 8 9 -
  +
 4  )  
4+,U4-5 	 M   " 
  ! 0 (  ) 1
  455< 

	  


 
	 

 	  
 
	   

  < )   / %   +,+U+4, 	
M

 7 8 H ) /
	 3 +,,- "
 	

   



  
   
    
 ) 6 %
4-1

   


   	  
  	 ! 2778$

  A<CUA6-

 " #
 * 4566 	
	    

	  -  
 %	)

   -%; A+AUA-,
 * 1 ) 
 /  45A6 .@ * :
 ) 
" 7  455C /
   
 
	 
 

 

=
 )    	  
 %  
" "
 +C ;C; U ;<-

 0 M M / )  # 4555 "
	 
 	  
 !

	   
      < %		  	

	H	 30 -A;,CAA<6C

   455<   /  4-G 7
 8
	 1/

#! / H ( 3!  ( ) "!  * +,,4  
 	 
G 2

	 '
  	  
	 )
 %	


 ; 4 ;AUA6
#!
 8 )  H +,,+ 7
 
	  
 
	   

  ) 6 %   & & !)6%&&$
7

 H "  7

 7   /
! 3 ) 3
! 7 +,,,

 Æ 

  )% %   ) Æ  
 

	
7
 " ) 
 " 8 455< "F88 190G : 	  
  
 
   < ) %   	  ! A91$
 +6;U+54
7  * ) 0
	 # +,,+ 4  ) :  
   + 


 4<G !
	 " (

 #
 /
 * 8 H ) " 0 +,,- 


L
	 

  
  


3
 ! 
	  
L (#   
    
 )
6 %   	  
  	 ! 2778$ 
C<6UC6A " #

	  # *!!  ) *   455A 7
 
	 
  
			
   1 L! ( ) 8  9 
   &
)	   	 H <  -C4U-C6   #
 # +,,, #F 78 
  ! 
	   
  )%/ 2777 
5-AU5;+
 # ) H  4555  

 ' 

 
 
	
  
   
  %   	   +,CU+4+
 7   ( 
	  )  V +,,, #
	
  

 
	 
 
 

 
   & )	
  	 32 4,A<U4,C-
 7  4566 8
	  
      
&  
/ 8 5U;;
4-2

 

 
     

 7  ) 3  1 4556

- 	 /:  )
  !

%	 
   /$

  #
!
 9 ) ! V +,,- 
 &
 
 
 

 		  	 2 + -+-U--6
  455<  
 ! 6  4  )  -  1
6-U4+;
   +,,- %	 & ; 

  AG  0! 8
#
 / # 7
	 / +,,+ 7
 
	   

   
	  '

 
 
 
 

 !   / 9 + 444U4-5
H
 7 ) H # M 455< (

 
 
 
 G #
     
   ) H 6A4  A;UC-
: : 9 ) :  # +,,- (
L  
 
G ! 
  

 '


 
 6  4  )  - 
39 A4-UAC<
:!
 " * " / 4565 / 	 

 
 #( 
 #	 (
 $

  "
	
:!
 " * " / ) ( # 455+ 
 G @
	   / ;
+<5U+5+
:
 3 ) /&  9 45C, 
 

	 

  #%.& %
- 
  )B  5CU4,;
:

 7 * 455+ 
 

 	

	 	
  



 
	   / ; - ++5U+AC
B
	 V 455C 


 !  
 
	 

 


  

L
  

 4  )  ;1 4+ +5AU-;+
B # 8 H ) D

  +,,4 "

 

 
 
	 

G   
   
   < ) % 
 	  !73$  C4CUC+- 

4-7

Journal of Articial Intelligence Research 24 (2005) 263-303

Submitted 06/04; published 08/05

Integrating Learning from Examples into the Search for
Diagnostic Policies
Valentina Bayer-Zubek
Thomas G. Dietterich

School of Electrical Engineering and Computer Science,
Dearborn Hall 102, Oregon State University,
Corvallis, OR 97331-3102 USA

bayer@cs.orst.edu
tgd@cs.orst.edu

Abstract

This paper studies the problem of learning diagnostic policies from training examples. A
diagnostic policy is a complete description of the decision-making actions of a diagnostician
(i.e., tests followed by a diagnostic decision) for all possible combinations of test results.
An optimal diagnostic policy is one that minimizes the expected total cost, which is the
sum of measurement costs and misdiagnosis costs. In most diagnostic settings, there is a
tradeo between these two kinds of costs.
This paper formalizes diagnostic decision making as a Markov Decision Process (MDP).
The paper introduces a new family of systematic search algorithms based on the AO algorithm to solve this MDP. To make AO ecient, the paper describes an admissible heuristic
that enables AO to prune large parts of the search space. The paper also introduces several
greedy algorithms including some improvements over previously-published methods. The
paper then addresses the question of learning diagnostic policies from examples. When the
probabilities of diseases and test results are computed from training data, there is a great
danger of overtting. To reduce overtting, regularizers are integrated into the search algorithms. Finally, the paper compares the proposed methods on ve benchmark diagnostic
data sets. The studies show that in most cases the systematic search methods produce
better diagnostic policies than the greedy methods. In addition, the studies show that
for training sets of realistic size, the systematic search algorithms are practical on today's
desktop computers.

1. Introduction
A patient arrives at a doctor's oce complaining of symptoms such as fatigue, frequent
urination, and frequent thirst. The doctor performs a sequence of measurements. Some
of the measurements are simple questions (e.g., asking the patient's age, medical history,
family history of medical conditions), others are simple tests (e.g., measure body mass index,
blood pressure), and others are expensive tests (e.g., blood tests). After each measurement,
the doctor analyzes what is known so far and decides whether there is enough information
to make a diagnosis or whether more tests are needed. When making a diagnosis, the doctor
must take into account the likelihood of each possible disease and the costs of misdiagnoses.
For example, diagnosing a diabetic patient as healthy incurs the cost of aggravating the
patient's medical condition and delaying treatment; diagnosing a healthy patient as having
diabetes incurs the costs of unnecessary treatments. When the information that has been
gathered is suciently conclusive, the doctor then makes a diagnosis.
c 2005


AI Access Foundation. All rights reserved.

Bayer-Zubek & Dietterich

We can formalize this diagnostic task as follows. Given a patient, the doctor can execute
a set of N possible measurements x1 ; : : : ; xN . When measurement xn is executed, the result
is an observed value vn . For example, if x1 is \patient's age", then v1 could be 36 (years).
Each measurement xn has an associated cost C (xn ). The doctor also can choose one of K
diagnosis actions. Diagnosis action fk diagnoses the patient as suering from disease k. We
will denote the correct diagnosis of the patient by y. The misdiagnosis cost of predicting
disease k when the correct diagnosis is y is denoted by MC (fk ; y).
The process of diagnosis consists of a sequence of decisions. In the starting state, no
measurements or diagnoses have been made. We denote this by the empty set fg. Suppose
that in this starting \knowledge state", the doctor chooses measurement x1 and receives
the result that x1 = 36 at a cost of $0.50. This is modeled as a transition to the knowledge
state fx1 = 36g with a cost of C (x1 ) = 0:5. Now suppose the doctor chooses x3 , which
measures body mass index, and receives a result x3 = small at a cost of $1. This changes the
knowledge state to fx1 = 36; x3 = smallg at a cost of C (x3 ) = 1. Finally, the doctor makes
the diagnosis \healthy". Suppose that the correct diagnosis is y = diabetes. For illustrative
purposes,1 suppose that the cost of this misdiagnosis is MC (healthy; diabetes) = $100. The
diagnosis action terminates the process, with a total cost of :5 + 1 + 100 = 101:5.
We can summarize the decision-making process of the doctor in terms of a diagnostic
policy, . The diagnostic policy species for each possible knowledge state s, what action
(s) to take, where the action can be one of the N measurement actions or one of the K
diagnosis actions. Every diagnostic policy has an expected total cost, which depends on
the joint probability distribution P (x1 ; : : : ; xN ; y) over the test results and the true disease
of the patients and on the costs C (xn ) and MC (fk ; y). The optimal diagnostic policy
minimizes this expected total cost by choosing the best tradeo point between the cost of
performing more measurements and the cost of misdiagnosis. Every measurement gathers
information, which reduces the risk of a costly misdiagnosis. But every measurement incurs
a measurement cost.
Diagnostic decision making is most challenging when the costs of measurement and
misdiagnosis have similar magnitudes. If measurement costs are very small compared to
misdiagnosis costs, then the optimal diagnostic policy is to measure everything and then
make a diagnostic decision. Conversely, if misdiagnosis costs are very small compared to
measurement costs, then the best policy is to measure nothing and just diagnose based on
misdiagnosis costs and prior probabilities of the diseases.
Learning cost-sensitive diagnostic policies is important in many domains, from medicine
to automotive troubleshooting to network fault detection and repair (Littman et al., 2004).
We note that this formulation of optimal diagnosis assumes that all costs can be expressed on a single numerical scale, that, although it need not correspond to economic cost,
must support the principle of choosing actions by minimizing expected total cost. In medical diagnosis, there is a large body of work on methods for eliciting the patient's preferences
and summarizing them as a utility or cost function (e.g., Lenert & Soetikno, 1997).
This paper studies the problem of learning diagnostic policies from training examples.
We assume that we are given a representative set of complete training examples drawn from
P (x1 ; : : : ; xN ; y) and that we are told the measurement costs and misdiagnosis costs. This
1. The true cost of misdiagnosing diabetes would depend on the age of the patient and the degree of
progression of the disease, but in any case, it would probably be much higher than $100.

264

Learning Diagnostic Policies from Examples

kind of training data could be collected, for example, through a clinical trial in which all
measurements were performed on all patients. Because of the costs involved in collecting
such data, we assume that the training data sets will be relatively small (hundreds or a
few thousands of patients; not millions). The goal of this paper is to develop learning
algorithms for nding good diagnostic policies from such modest-sized training data sets.
Unlike other work on test selection for diagnosis (Heckerman, Horvitz, & Middleton, 1993;
van der Gaag & Wessels, 1993; Madigan & Almond, 1996; Dittmer & Jensen, 1997), we do
not assume that a Bayesian network or inuence diagram is provided; instead we directly
learn a diagnostic policy from the data.
This framework of diagnosis ignores several issues that we hope to address in future
research. First, it assumes that each measurement action has no eect on the patient.
Each measurement action is a pure observation action. In real medical and equipment
diagnosis situations, some actions may also be attempted therapies or attempted repairs.
These repairs may help cure the patient or x the equipment, in addition to gathering
information. Our approach does not handle attempted repair actions.
Second, this framework assumes that measurement actions are chosen and executed oneat-a-time and that the cost of an action does not depend on the order in which the actions
are executed. This is not always true in medical diagnosis. For example, when ordering
blood tests, the physician can choose to order several dierent tests as a group, which costs
much less than if the tests are ordered individually.
Third, the framework assumes that the result of each measurement action is available before the diagnostician must choose the next action. In medicine, there is often a (stochastic)
delay between the time a test is ordered and the time the results are available. Fragmentary
results may arrive over time, which may lead the physician to order more tests before all
previously-ordered results are available.
Fourth, the framework assumes that measurement actions are noise-free. That is, repeating a measurement action will obtain exactly the same result. Therefore once a measurement
action is executed, it never needs to be repeated.
Fifth, the framework assumes that the results of the measurements have discrete values.
We enforce this via a pre-processing discretization step.
These assumptions allow us to represent the doctor's knowledge state by the set of
partial measurement results: fx1 = v1 ; x3 = v3 ; : : :g and to represent the entire diagnostic
process as a Markov Decision Process (MDP). Any optimal solution to this MDP provides
an optimal diagnostic policy.
Given this formalization, there are conceptually two problems that must be addressed in
order to learn good diagnostic policies. First, we must learn the joint probability distribution
P (x1 ; : : : ; xN ; y). Second, we must solve the resulting MDP for an optimal policy.
In this paper, we begin by addressing the second problem. We show how to apply the

AO algorithm to solve the MDP for an optimal policy. We dene an admissible heuristic
for AO that allows it to prune large parts of the state space, so that this search becomes
more ecient. This addresses the second conceptual problem.
However, instead of solving the rst conceptual problem (learning the joint distribution
P (x1 ; : : : ; xN ; y)) directly, we argue that the best approach is to integrate the learning
process into the AO search. There are three reasons to pursue this integration. First,
by integrating learning into the search, we ensure that the probabilities computed during
265

Bayer-Zubek & Dietterich

learning are the probabilities relevant to the task. If instead we had just separately learned
some model of the joint distribution P (x1 ; : : : ; xN ; y), those probabilities would have been
learned in a task-independent way, and long experience in machine learning has shown that
it is better to exploit the task in guiding the learning process (e.g., Friedman & Goldszmidt,
1996; Friedman, Geiger, & Goldszmidt, 1997).
Second, by integrating learning into the search, we can introduce regularization methods
that reduce the risk of overtting. The more thoroughly a learning algorithm searches the
space of possible policies, the greater the risk of overtting the training data, which results in
poor performance on new cases. The main contribution of this paper (in addition to showing
how to model diagnosis as an MDP) is the development and careful experimental evaluation
of several methods for regularizing the combined learning and AO search process.
Third, the integration of learning with AO provides additional opportunities to prune
the AO search and thereby improve the computational eciency of the learning process.
We introduce a pruning technique, called \statistical pruning", that simultaneously reduces
the AO search space and also regularizes the learning procedure.
In addition to applying the AO algorithm to perform a systematic search of the space of
diagnostic policies, we also consider greedy algorithms for constructing diagnostic policies.
These algorithms are much more ecient than AO , but we show experimentally that they
give worse performance in several cases. Our experiments also show that AO is feasible on
all ve diagnostic benchmark problems that we studied.
The remainder of the paper is organized as follows. First, we discuss the relationship
between the problem of learning minimum cost diagnostic policies and previous work in
cost-sensitive learning and diagnosis. In Section 3, we formulate this diagnostic learning
problem as a Markov Decision Problem. Section 4 presents systematic and greedy search
algorithms for nding good diagnostic policies. In Section 5, we take up the question of
learning good diagnostic policies and describe our various regularization methods. Section 6
presents a series of experiments that measure the eectiveness and eciency of the various
methods on real-world data sets. Section 7 summarizes the contributions of the paper and
discusses future research directions.

2. Relationship to Previous Research

The problem of learning diagnostic policies is related to several areas of previous research
including cost-sensitive learning, test sequencing, and troubleshooting. We discuss each of
these in turn.

2.1 Cost-Sensitive Learning

The term \cost-sensitive learning" denotes any learning algorithm that is sensitive to one or
more costs. Turney (2000) provides an excellent overview. Cost-sensitive learning employs
classication terminology in which a class is a possible outcome of the classication process.
This corresponds in our case to the diagnosis. The forms of cost-sensitive learning most
relevant to our work concern methods sensitive to misclassication costs, methods sensitive
to measurement costs, and methods sensitive to both kinds of costs.
Learning algorithms sensitive to misclassication costs have received signicant attention. In this setting, the learning algorithm is given (at no cost) the results of all possible
266

Learning Diagnostic Policies from Examples

measurements, (v1 ; : : : ; vN ). It must then make a prediction y^ of the class of the example,
and it pays a cost MC (^y; y) when the correct class is y. Important work in this setting
includes the papers of Breiman et al. (1984), Pazzani et al. (1994), Fawcett and Provost
(1997), Bradford et al. (1998), Domingos (Domingos, 1999), Zadrozny and Elkan (2001),
and Provost and Fawcett (2001).
A few researchers in machine learning have studied application problems in which there
is a cost for measuring each attribute (Norton, 1989; Nunez, 1991; Tan, 1993). In this
setting, the goal is to minimize the number of misclassication errors while biasing the
learning algorithm in favor of less-expensive attributes. From a formal point of view, this
problem is ill-dened, because there is no explicit denition of an objective function that
trades o the cost of measuring attributes against the number of misclassication errors.
Nonetheless, several interesting heuristics were implemented and tested in these papers.
More recently, researchers have begun to consider both measurement and misclassication costs (Turney, 1995; Greiner, Grove, & Roth, 2002). The objective is identical to
the one studied in this paper: to minimize the expected total cost of measurements and
misclassications. Both algorithms learn from data as well.
Turney developed ICET, an algorithm that employs genetic search to tune parameters
that control a classication-tree learning algorithm. Each classication tree is built using
a criterion that selects attributes greedily, based on their information gain and estimated
costs. The measurement costs are adjusted in order to build dierent classication trees;
these trees are evaluated on an internal holdout set using the real measurement and misclassication costs. The best set of measurement costs found by the genetic search is employed
to build the nal classication tree on the entire training data set.
Greiner et al.'s paper provides a PAC-learning analysis of the problem of learning an
optimal diagnostic policy|provided that the policy makes no more than L measurements,
where L is a xed constant. Recall that N is the total number of measurements. They prove
that there exists an algorithm that runs in time polynomial in N , consumes a number of
training examples polynomial in N , and nds a diagnostic policy that, with high probability,
is close to optimal. Unfortunately, the running time and the required number of examples is
exponential in L. In eect, their algorithm works by estimating, with high condence, the
transition probabilities and the class probabilities in states where at most L of the values
x1 = v1, . . . , xN = vN have been observed. Then the value iteration dynamic programming
algorithm is applied to compute the best diagnostic policy with at most L measurements.
In theory, this works well, but it is dicult to convert this algorithm to work in practice.
This is because the theoretical algorithm chooses the space of possible policies and then
computes the number of training examples needed to guarantee good performance, whereas
in a real setting, the number of available training examples is xed, and it is the space of
possible policies that must be adapted to avoid overtting.

2.2 Test Sequencing
The eld of electronic systems testing has formalized and studied a problem called the
test sequencing problem (Pattipati & Alexandridis, 1990). An electronic system is viewed
as being in one of K possible states. These states include one fault-free state and K , 1
faulty states. The relationship between tests (measurements) and system states is specied
267

Bayer-Zubek & Dietterich

as a binary diagnostic matrix which tells whether test xn detects fault fi or not. The
probabilities of the dierent system states y are specied by a known distribution P (y).
A test sequencing policy performs a series of measurements to identify the state of the
system. In test sequencing, it is assumed that the measurements are sucient to determine
the system state with probability 1. The objective is to nd the test sequencing policy that
achieves this while minimizing the expected number of tests. Hence, misdiagnosis costs are
irrelevant, because the test sequencing policy must guarantee zero misdiagnoses. Several
heuristics for AO have been applied to compute the optimal test sequencing policy (Pattipati & Alexandridis, 1990).
The test sequencing problem does not involve learning from examples. The required
probabilities are provided by the diagnostic matrix and the fault distribution P (y).

2.3 Troubleshooting
Another task related to our work is the task of troubleshooting (Heckerman, Breese, &
Rommelse, 1994). Troubleshooting begins with a system that is known to be functioning
incorrectly and ends when the system has been restored to a correctly-functioning state.
The troubleshooter has two kinds of actions: pure observation actions (identical to our measurement actions) and repair actions (e.g., removing and replacing a component, replacing
batteries, lling the gas tank, rebooting the computer, etc.). Each action has a cost, and
the goal is to nd a troubleshooting policy that minimizes the expected cost of restoring
the system to a correctly-functioning state.
Heckerman et al. (1994, 1995) show that for the case where the only actions are pure
repair actions and there is only one broken component, there is a very ecient greedy
algorithm that computes the optimal troubleshooting policy. They incorporate pure observation actions via a one-step value of information (VOI) heuristic. According to this
heuristic, they compare the expected cost of a repair-only policy with the expected cost of a
policy that makes exactly one observation action and then executes a repair-only policy. If
an observe-once-and-then-repair-only policy is better, they execute the chosen observation
action, obtain the result, and then again compare the best repair-only policy with the best
observe-once-and-then-repair-only policy. Below, we dene a variant of this VOI heuristic
and compare it to the other greedy and systematic search algorithms developed in this
paper.
Heckerman et al. consider only the case where the joint distribution P (x1 ; : : : ; xN ; y) is
provided by a known Bayesian network. To convert their approach into a learning approach,
they could rst learn the Bayesian network and then compute the troubleshooting policy.
But we suspect that an approach that integrates the learning of probabilities into the
search for good policies|along the lines described in this paper|would give better results.
Exploring this question is an important direction for future research.

3. Formalizing Diagnosis as a Markov Decision Problem
The process of diagnosis is a sequential decision making process. After every decision, the
diagnostician must decide what to do next (perform another measurement, or terminate by
making a diagnosis). This can be modeled as a Markov Decision Problem (MDP).
268

Learning Diagnostic Policies from Examples

An MDP is a mathematical model for describing the interaction of an agent with an
environment. An MDP is dened by a set of states S (including the start state), an action set
A, the transition probabilities Ptr (s0 js; a) of moving from state s to state s0 after executing
action a, and the (expected immediate) costs C (s; a; s0 ) associated with these transitions.
Because the state representation contains all the relevant information for future decisions,
it is said to exhibit the Markov property.
A policy  maps states into actions. The value of a state s under policy , V  (s), is the
expected sum of future costs incurred when starting in state s and following  afterwards
(Sutton & Barto, 1999, chapter 3). The value function V  of a policy  satises the following
recursive relationship, known as the Bellman equation for V  :

V  (s) =

X

s 2S
0

Ptr (s0 js; (s))  C (s; (s); s0) + V  (s0 ) ; 8; 8s:




(1)

This can be viewed as a one-step lookahead from state s to each of the next states s0 reached
after executing action (s). Given a policy , the value of state s can be computed from
the value of its successor states s0 , by adding the expected costs of the transitions, then
weighting them by the transition probabilities.
Solving the MDP means nding a policy with the smallest value. Such a policy is called
the optimal policy  , and its value is the optimal value function V  . Value iteration is an
algorithm that solves MDPs by iteratively computing V  (Puterman, 1994).
The problem of learning diagnostic policies can be represented as an MDP. We rst
dene the actions of this MDP, then the states, and nally the transition probabilities and
costs. All costs are positive.
As discussed above, we assume that there are N measurement actions (tests) and K
diagnosis actions. Measurement action n (denoted xn ) returns the value of attribute xn ,
which we assume is a discrete variable with Vn possible values. Diagnosis action k (denoted
fk ) is the act of predicting that the correct diagnosis of the example is k. An action
(measurement or diagnosis) is denoted by a.
In our diagnostic setting, a case is completely described by the results of all N measurement actions and the correct diagnosis y: (v1 ; : : : ; vN ; y). In our framework, each case is
drawn independently according to an (unknown) joint distribution P (x1 ; : : : ; xN ; y). Once
a case is drawn, all the values dening it stay constant. Test xn reveals to the diagnostic
agent the value xn = vn of this case. As a consequence, once a case has been drawn, the
order in which the tests are performed does not change the values that will be observed.
That is, the joint distribution P (xi = vi ; xj = vj ) is independent of the order of the tests
xi and xj .
It follows that we can dene the state of the MDP as the set of all attribute-value pairs
observed thus far. This state representation has the Markov property because it contains all
relevant past information. There is a unique start state, s0 = fg, in which no attributes have
been measured. The set of all states S contains one state for each possible combination of
measured attributes, as found in the training data. Each training example provides evidence
for the reachability of 2N states. The set A(s) of actions executable in state s consists of
those attributes not yet measured plus all of the diagnosis actions.
We also dene a special terminal state sf . Every diagnosis action makes a transition to
sf with probability 1 (i.e., once a diagnosis is made, the task terminates). By denition, no
269

Bayer-Zubek & Dietterich

actions are executable in the terminal state, and its value function is zero. Note that the
terminal state is always reached, because there are only nitely-many measurement actions
after which a diagnosis action must be executed.
We now dene the transition probabilities and the immediate costs of the MDP. For
measurement action xn executed in state s, the result state will be s0 = s [ fxn = vn g,
where vn is the observed value of xn . The expected cost of this transition is denoted C (xn),
since we assume it depends only on which measurement action xn is executed, and not on
the state in which it is executed nor the resulting value vn that is observed. The probability
of this transition is Ptr (s0 js; xn ) = P (xn = vn js).
The misdiagnosis cost of diagnosis action fk depends on the correct diagnosis y of the
example. Let MC (fk ; y) be the misdiagnosis cost of guessing diagnosis k when the correct
diagnosis is y. Because the correct diagnosis y of an example is not part of the state
representation, the cost of a diagnosis action (which depends on y) performed in state s
must be viewed as a random variable whose value is MC (fk ; y) with probability P (yjs),
which is the probability that the correct diagnosis is y given the current state s. Hence,
our MDP has a stochastic cost function for diagnosis actions. This does not lead to any
diculties, because all that is required to compute the optimal policy for an MDP is the
expected cost of each action. In our case, the expected cost of diagnosis action fk in state s
is
X
C (s; fk ) = P (yjs)  MC (fk ; y);
(2)
y

which is independent of y.
For uniformity of notation, we will write the expected immediate cost of action a in
state s as C (s; a), where a can be either a measurement action or a diagnosis action.
For a given start state s0 , the diagnostic policy  is a decision tree (Raia, 1968).
Figure 1 illustrates a simple example of a diagnostic policy. The root is the starting state
s0 = fg. Each node is labeled with a state s and a corresponding action (s). If the action
is a measurement action, xn , the possible results are the dierent possible observed values
vn, leading to children nodes. If the action is a diagnosis action, fk , the possible results are
the diagnoses y. If (s) is a measurement action, the node is called an internal node, and if
(s) is a diagnosis action, the node is called a leaf node. Each branch in the tree is labeled
with its probability of being followed (conditioned on reaching its parent node). Each node
s is labeled with V  (s), the expected total cost of executing the diagnostic policy starting
at node s. Notice that the value of a leaf is the expected cost of diagnosis, C (s; fk ).
The fact that a diagnostic policy is a decision tree is potentially confusing, because a
similar data structure, the classication tree (often also called a decision tree), has been
the focus of so much work in the machine learning literature (e.g., Quinlan, 1993). It
is important to remember that whereas the evaluation criterion for a classication tree
is the misclassication error rate, the evaluation criterion for a decision tree diagnostic
policy is the expected total cost of diagnosis. One way of clarifying this dierence is to
note that a given classication tree can be transformed into many equivalent classication
trees by changing the order in which the tests are performed (see Utgo's work on tree
manipulation operators, Utgo, 1989). These equivalent classiers all implement the same
classication function y = f (x1 ; : : : ; xN ). But if we consider these \equivalent" trees as
diagnostic policies, they will have dierent expected total diagnosis costs, because tests
270

Learning Diagnostic Policies from Examples

low

{ BMI = large, Insulin = low}

.8
large

45.98

.5
{}
28.99

{ BMI = large }

.3

24

.2

BMI
1
.5
{ BMI = small }

.7

Insulin
22.78
high

small

Diabetes

Healthy

10

.9
.1

y = Healthy
y = Diabetes

{ BMI = large, Insulin = high }

Healthy

.8
.2

20

y = Diabetes
y = Healthy
y = Healthy
y = Diabetes

0
80

0
100

0
100

Figure 1: An example of diagnostic policy  for diabetes. Body Mass Index (BMI) is tested
rst. If it is small, a Healthy diagnosis is made. If BMI is large, Insulin is tested
before making a diagnosis. The costs of measurements (BMI and Insulin) are
written below the name of the variable. The costs of misdiagnoses are written
next to the solid squares. Probabilities are written on the branches. The values
of the states are written below each state. The value of the start state, V  (s0 ) =
28:99, can be computed in a single sweep, starting at the leaves, as follows. First
the expected costs of the diagnosis actions are computed (e.g., the upper-most
Diabetes diagnosis action has an expected cost of 0:7  0 + 0:3  80 = 24). Then
the value of the Insulin subtree is computed as the cost of measuring Insulin
(22.78) + 0:8  24 + 0:2  20 = 45:98. Finally, the value of the whole tree is
computed as the cost of measuring BMI (1) + 0:5  45:98 + 0:5  10 = 28:99.

large
low
.57

.7
{ Insulin = low } BMI
1
21.4
.3
small

Insulin
{}
22.78
40.138
.43
high

{ Insulin = high }
12

Healthy .88
.12

{ BMI = large, Insulin = low}
24

{ BMI = small, Insulin = low }
12

y = Healthy
y = Diabetes

Diabetes

.7
.3

Healthy .88
.12

y = Diabetes
y = Healthy
y = Healthy
y = Diabetes

0
80

0
100

0
100

Figure 2: Another diagnostic policy 2 , making the same classication decisions as  in
Figure 1, but with a changed order of tests, and therefore with a dierent policy
value.
closer to the root of the tree will be executed more often, so their measurement costs will
make a larger contribution to the total diagnosis cost. For example, the policy  in Figure 1
rst performs a cheap test, BMI. This policy has a value of 28:99. The tree 2 in Figure 2
makes the same classication decisions (with an error rate of 19%), but it rst tests Insulin,
which is more expensive, and this increases the policy value to 40:138.
271

Bayer-Zubek & Dietterich

4. Searching for Good Diagnostic Policies
We now consider systematic and greedy search algorithms for computing diagnostic policies.
In this section, we will assume that all necessary probabilities are known. We defer the
question of learning those probabilities to Section 5. We note that this is exactly what all
previous uses of AO have done. They have always assumed that the required probabilities
and costs were known.
Given the MDP formulation of the diagnostic process, we could proceed by constructing
the entire state space and then applying dynamic programming algorithms (e.g., value
iteration or policy iteration) to nd the optimal policy. However, the size of the state space
is exponential: given N measurement actions, each with V possible outcomes, there are (V +
1)N + 1 states in the MDP (counting the special terminal state sf , and taking into account
that each measurement may not have been performed yet). We seek search algorithms that
only consider a small fraction of this huge space. In this section, we will study two general
approaches to dealing with this combinatorial explosion of states: systematic search using
the AO algorithm and various greedy search algorithms.

4.1 Systematic Search

When an MDP has a unique start state and no (directed) cycles, the space of policies can
be represented as an AND/OR graph (Qi, 1994; Washington, 1997; Hansen, 1998). An
AND/OR graph is a directed acyclic graph that alternates between two kinds of nodes:
AND nodes and OR nodes. Each OR node represents a state s in the MDP state space.
Each child of an OR node is an AND node that represents one possible action a executed
in state s. Each child of an AND node is an OR node that represents a state s0 that results
from executing action a in state s. Figure 3 shows an example of an AND/OR graph for a
diabetes diagnosis problem with three tests (BMI, Glucose, and Insulin) and two diagnosis
actions (Diabetes and Healthy).
In our diagnostic setting, the root OR node corresponds to the starting state s0 = fg.
Each OR node s has one AND child (s; xn ) for each measurement action (test) xn that can
be executed in s. Each OR node could also have one child for each possible diagnosis action
fk that could be performed in s, but to save time and memory, we include only the one
diagnosis action fk that has the minimum expected cost. We will denote this by fbest . Each
time an OR node is created, an AND child for fbest is created immediately. This leaf AND
node stores the action-value function Q(s; fbest ) = C (s; fbest ). Note that multiple paths
from the root may lead to the same OR node, by changing the order of the tests.
In our implementation, each OR node stores a representation of the state s, a current
policy (s) which species a test or a diagnosis action, and a current value function V  (s).
Each AND node (s; xn ) stores a probability distribution over the outcomes of xn, and an
action-value function Q (s; xn ), the expected cost of measuring xn and then continuing
with policy .
Every possible policy  corresponds to a subtree of the full AND/OR graph. Each
OR node s in this subtree (starting at the root) contains only the one AND child (s; a)
corresponding to the action a = (s) chosen by policy .
The AO algorithm (Nilsson, 1980) computes the optimal policy for an AND/OR graph.

AO is guided by a heuristic function. We describe the heuristic function in terms of state272

Learning Diagnostic Policies from Examples

{}
OR node
Healthy

Insulin
Diabetes

Glucose
BMI

AND node
AND node

AND node
small

large

OR
H
D

G
AND

OR
H

I

D

AND

G

I

AND

AND

low

high

OR

OR

H

H
D

G

D

G

AND
low

high

OR
H

AND
low

OR
D

H

high

OR
D

H

OR
D

H

D

Figure 3: An example of an AND/OR graph. The root OR node corresponds to the state
s0 = fg. There is a child AND node for each of the test actions (BMI, Glucose
and Insulin), and also for the diagnosis actions (Healthy and Diabetes). The
choice of the BMI test in the root node leads to the AND node (s0 ; BMI ), which
species the expectation over the outcomes of the test BMI (small and large).
If BMI is small, the child of AND node (s0 ; BMI ) is the OR node with state
fBMI = smallg; in this OR node, there is a choice among the actions Healthy,
Diabetes, Glucose and Insulin.
action pairs, h(s; a), instead of in terms of states. The heuristic function is admissible if
h(s; a)  Q(s; a) for all states s and actions a. This means that h underestimates the
total cost of executing action a in state s and following the optimal policy afterwards. The
admissible heuristic allows the AO algorithm to safely ignore an action a0 if there is another
action a for which it is known that Q (s; a) < h(s; a0 ). Under these conditions, (s; a0 ) cannot
be part of any optimal policy.
The AO search begins with an AND/OR graph containing only the root node. It then
repeats the following steps: In the current best policy, it selects an AND node to expand; it
expands it (expanding an AND node creates its children OR nodes); and then it recomputes
(bottom-up) the optimal value function and policy of the revised graph. The algorithm
terminates when the best policy has no unexpanded AND nodes (in other words, the leaf
273

Bayer-Zubek & Dietterich

s’
v
x

use hopt

s

to evaluate

Figure 4: Qopt (s; x) for unexpanded AND node (s; x) is computed using one-step lookahead
and hopt to evaluate the resulting states s0 . x is an attribute not yet measured in
state s, and v is one of its values.
OR nodes of the policy specify diagnosis actions, so this policy is a complete diagnostic
policy).
During AO search, we maintain two policies, whose actions and value functions are
stored in the nodes of the AND/OR graph. We call the rst policy the optimistic policy,
opt . As we show below, its value function V opt is a lower bound on the optimal value
function V  . This is the policy that appears in Nilsson's original description of AO , and
it provides enough information to compute an optimal policy  (Martelli & Montanari,
1973). During the search, the optimistic policy opt is an incomplete policy, because it
includes some unexpanded AND nodes; when opt becomes a complete policy, it is in fact
an optimal policy.
The second policy that we maintain is called the realistic policy, real . We will show
that its value function, V real , is an upper bound on the optimal value function V  . The
realistic policy is always a complete policy, so it is executable after each iteration of AO .
By maintaining the realistic policy, AO becomes an anytime algorithm.
We now dene these two policies in more detail and introduce our admissible heuristic.
4.1.1 Admissible Heuristic

Our admissible heuristic provides an optimistic estimate, Qopt (s; x), of the expected cost
of an unexpanded AND node (s; x). It is based on an incomplete two-step lookahead search
(see
Figure 4). The rst step of the lookahead search computes Qopt (s; x) = C (s; x) +
P
0
opt 0
0
s Ptr (s js; x)  h (s ). Here s iterates over the states resulting from measuring test x.
The second step of the lookahead is dened by the function hopt (s0 ) = mina 2A(s ) C (s0 ; a0 );
which is the minimum over the cost of the diagnosis action fbest and the cost of each of the
remaining tests x0 in s0. That is, rather than considering the states s00 that would result from
measuring x0 , we only consider the cost of measuring x0 itself. ItPfollows immediately that
hopt (s0)  V  (s0 ); 8s0, because C (s0; x0 )  Q (s0 ; x0 ) = C (s0; x0 ) + s Ptr (s00 js0; x0 )  V  (s00 ).
The key thing to notice is that the cost of a single measurement x0 is less than or equal to
the cost of any policy that begins by measuring x0 , because the policy must pay the cost
of at least one more action (diagnosis or measurement) before entering the terminal state
sf . Consequently, Qopt(s; x)  Q (s; x), so Qopt is an admissible heuristic for state s and
action x.
0

0

00

274

0

Learning Diagnostic Policies from Examples

4.1.2 Optimistic Values and Optimistic Policy

The denition of the optimistic action-value value Qopt can be extended to all AND nodes
in the AND/OR graph through the following recursion:
8
>
< C (s; a), if a = fk (a diagnosis action)
opt
Q (s; a) = > C (s; a) + P
Ptr (s0js; a)  hopt (s0 ), if (s; a) is unexpanded
(3)
Ps
:
0
opt
0
C (s; a) + s Ptr (s js; a)  V (s ), if (s; a) is expanded,
0
0

where V opt (s) def
= mina2A(s) Qopt (s; a). Recall that A(s) consists of all attributes not yet
measured in s and all diagnosis actions.
The optimistic policy is opt (s) = argmina2A(s) Qopt (s; a): Every OR node s stores its
optimistic value V opt (s) and policy opt (s), and every AND node (s; a) stores Qopt (s; a).
Theorem 4.1 proves that Qopt and V opt form an admissible heuristic. The proofs for all
theorems in this paper appear in the thesis of Bayer-Zubek (2003).

Theorem 4.1 For all states s and all actions a 2 A(s), Qopt(s; a)  Q (s; a); and V opt (s) 
V  (s):

4.1.3 Realistic Values and Realistic Policy

In the current graph constructed by AO , suppose that we delete all unexpanded AND
nodes (s; a). We call the resulting graph the realistic graph, because every leaf node will
select a diagnosis action. The optimal policy computed from this graph is called the realistic
policy, real . It is a complete policy leaves specify diagnosis actions of minimum expected
misdiagnosis cost.
Every OR node s stores the realistic value V real (s) and policy real (s), and every AND
node (s; a) stores a realistic action-value value, Qreal (s; a). For a 2 A(s), dene
8
>
a = fk (a diagnosis action)
< C (s; a), ifP
real
Q (s; a) = > C (s; a) + s Ptr (s0 js; a)  V real (s0 ), if (s; a) is expanded
(4)
:
ignore, if (s; a) is unexpanded
0

and V real (s) = mina2A (s) Qreal (s; a); where the set A0 (s) is A(s) without the unexpanded actions. The realistic policy is real (s) = argmina2A (s) Qreal (s; a):
0

0

Theorem 4.2 The realistic value function V real is an upper bound on the optimal value
function: V  (s)  V real (s); 8s:
4.1.4 Selecting a Node for Expansion

In the current optimistic policy opt , we choose to expand the unexpanded AND node
(s; opt (s)) with the largest impact on the root node. This is dened as
argmax [V real (s) , V opt (s)]  Preach (sjopt );

s
opt
where Preach(sj ) is the probability of reaching state s from the start state while following
policy opt . The dierence V real (s) , V opt (s) is an upper bound on how much the value
of state s could change if opt (s) is expanded.

275

Bayer-Zubek & Dietterich

The rationale for this selection is based on the observation that AO terminates when

V opt(s0 ) = V real (s0 ). Therefore, we want to expand the node that makes the biggest step

toward this goal.

4.1.5 Our Implementation of AO (High Level)

Our implementation of AO is the following:
repeat

select an AND node (s; a) to expand (using opt; V opt; V real ).
expand (s; a).
do bottom-up updates of Qopt; V opt; opt and of Qreal; V real; real.
until there are no unexpanded nodes reachable by  opt .

The updates of value functions are based on one-step lookaheads (Equations 3 and 4), using the value functions of the children. At each iteration, we start from the newly expanded
AND node (s; a), compute its Qopt (s; a) and Qreal (s; a), then compute V opt (s); opt (s),
V real (s); and real (s) in its parent OR node, and propagate these changes up in the
AND/OR graph all the way to the root. Full details on our implementation of AO appear
in the thesis of Bayer-Zubek (2003).
As more nodes are expanded, the optimistic values V opt increase, becoming tighter lower
bounds to the optimal values V  , and the realistic values V real decrease, becoming tighter
upper bounds. V opt and V real converge to the value of the optimal policy: V opt (s) =
V real (s) = V  (s), for all states s reached by  .
The admissible heuristic avoids exploring expensive parts of the AND/OR graph; indeed,
when V real (s) < Qopt (s; a), action a does not need to be expanded (this is a heuristic
cuto). Initially, V real (s) = C (s; fbest ), and this explains why measurement costs that are
large relative to misdiagnosis costs produce many cutos.

4.2 Greedy Search

Now that we have considered the AO algorithm for systematic search, we turn our attention
to several greedy search algorithms for nding good diagnostic policies. Greedy search
algorithms grow a decision tree starting at the root, with state s0 = fg. Each node in the
tree corresponds to a state s in the MDP, and it stores the corresponding action a = (s)
chosen by the greedy algorithm. The children of node s correspond to the states that result
from executing action a in state s. If a diagnosis action fk is chosen in state s, then the
node has no children in the decision tree (it is a leaf node).
All of the greedy algorithms considered in this paper share the same general template,
which is shown as pseudo-code in Table 1. At each state s, the greedy algorithm performs
a limited lookahead search and then commits to the choice of an action a to execute in
s, which thereby denes (s) = a. It then generates child nodes corresponding to the
states that could result from executing action a in state s. The algorithm is then invoked
recursively on each of these child nodes.
Once a greedy algorithm has committed to xn = (s), that choice is nal. Note however,
that some of our regularization methods may prune the policy by replacing a measurement
action (and its descendents) with a diagnosis action. In general, greedy policies are not
optimal, because they do not perform a complete analysis of the expected total cost of
276

Learning Diagnostic Policies from Examples

Table 1: The Greedy search algorithm. Initially, the function Greedy() is called with the
start state s0 .

function Greedy(state s) returns a policy  (in the form of a decision tree).
(1) if (stopping conditions are not met)
(2) select measurement action xn to execute
set (s) := xn
for each resulting value vn of the test xn add the subtree
Greedy(state s [ fxn = vn g)

else

(3) select diagnosis action fbest , set (s) := fbest :
executing xn in s before committing to an action. Nevertheless, they are ecient because
of their greediness.
In the following discussion, we describe several dierent greedy algorithms. We dene
each one by describing how it renes the numbered lines in the template of Table 1.
4.2.1 InfoGainCost Methods

InfoGainCost methods are inspired by the C4.5 algorithm for constructing classication
trees (Quinlan, 1993). C4.5 chooses the attribute xn with the highest conditional mutual
information with the class labels in the training examples. In our diagnostic setting, the
analogous criterion is to choose the measurement action that is most predictive of the correct
diagnosis. Specically, let xn be a proposed measurement action, and dene P (xn ; yjs) to
be the joint distribution of xn and the correct diagnosis y conditioned on the information
that has already been collected in state s. The conditional mutual information between xn
and y, I (xn ; yjs), is dened as

I (xn; yjs) = H (yjs) , H
(yjs; xn )
X
= H (yjs) , P (xn = vn js)  H (yjs [ fxn = vn g)
vn

where H (y) = y ,P (y) log P (y) is the Shannon entropy of random variable y.
The mutual information is also called the information gain, because it quanties the
average amount of information we gain about y by measuring xn .
The InfoGainCost methods penalize the information gain by dividing it by the cost of the
test. Specically, they choose the action xn that maximizes I (xn ; yjs)=C (xn ). This criterion
was introduced by Norton (1989). Other researchers have considered various monotonic
transformations of the information gain prior to dividing by the measurement cost (Tan,
1993; Nunez, 1991). This denes step (2) of the algorithm template.
All of the InfoGainCost methods employ the stopping conditions dened in C4.5. The
rst stopping condition applies if P (yjs) is 1 for some value y = k. In this case, the
diagnosis action is chosen to be fbest = k. The second stopping condition applies if no more
P

277

Bayer-Zubek & Dietterich

measurement actions are available (i.e., all tests have been performed). In this case, the
diagnosis action is set to the most likely diagnosis: fbest := argmaxy P (yjs).
Notice that the InfoGainCost methods do not make any use of the misdiagnosis costs
MC (fk ; y).
4.2.2 Modified InfoGainCost Methods (MC+InfoGainCost)

We propose extending the InfoGainCost methods so that they consider misdiagnosis costs
in the stopping conditions. Specically, in step (3), the MC+InfoGainCost methods set
fbest to be the diagnosis action with minimum expected cost:

(s) := fbest = argmin

X

fk

y

P (yjs)  MC (fk ; y):

4.2.3 One-step Value of Information (VOI)

While the previous greedy methods either ignore the misdiagnosis costs or only consider
them when choosing the nal diagnosis actions, the VOI approach considers misdiagnosis
costs (and measurement costs) at each step.
Traditionally, the value of information of a measurement is dened as the dierence
between the expected value of the best action after performing the measurement and the
expected value of the best action before performing the measurement. Since our objective is
cost minimization, we need to reverse the sign in the above denition. However, we still keep
the notation VOI instead of cost of information. Instead of taking into account all future
decisions, we make a greedy approximation to VOI, called one-step VOI, in which we only
consider the cost of the best diagnosis action before and after performing the measurement
xn in state s:
X

1-step-VOI(s; xn ) = min
P (yjs)  MC (fk ; y)
fk y
"

,

X

vn

P (xn = vn js)  min
f
k

X

y

#

P (yjs [ fxn = vn g)  MC (fk ; y) :

The test xn is performed only if its value exceeds its cost, 1-step-VOI(s; xn ) > C (xn ).
Intuitively, the one-step VOI method repeatedly asks the following question: Is it worth
executing one more measurement before making a diagnosis, or is it better to make a
diagnosis now?
In state s, the one-step VOI method rst computes the cost of stopping and choosing
the action fbest that minimizes expected misdiagnosis costs:

C (s; fbest ) = min
f
k

X

y

P (yjs)  MC (fk ; y):

Then, for each possible measurement action xn , the method computes the expected cost of
measuring xn and then choosing minimum cost diagnosis actions in each of the resulting
278

Learning Diagnostic Policies from Examples

states:
1-step-LA(s; xn ) = C (xn) +

X

vn

"

P (xn = vnjs)  min
f
k

X

y

#

P (yjs [ fxn = vn g)  MC (fk ; y) :
(5)

Dene xbest = argminxn 1-step-LA(s; xn ).
With these denitions, we can describe the one-step VOI method in terms of the template in Table 1 as follows. The stopping condition (1) is that C (s; fbest )  1-step-LA(s; xbest );
the method also stops when all tests have been performed. The choice of measurement action (2) is xbest . And the choice of the nal diagnosis action in step (3) is fbest .

5. Learning, Overtting, and Regularization

In the previous section, we considered search algorithms for nding good diagnostic policies.
All of these algorithms require various probabilities, particularly P (xn = vn js) and P (yjs)
for every state-action pair (s; xn ) or (s; fk ) generated during their search.
One way to obtain these probabilities is to t a probabilistic model P (x1 ; : : : ; xN ; y)
to the training data and then apply probabilistic inference to this model to compute the
desired probabilities. For example, an algorithm such as K2 (Cooper & Herskovits, 1992)
could be applied to learn a Bayesian network from the training data. The advantage of
such an approach is that it would cleanly separate the process of learning the probabilities
from the process of searching for a good policy.
But the chief disadvantage of such an approach is that it prevents us from exploiting
the problem solving task to determine which probabilities should be learned accurately
and which probabilities can be ignored (or learned less accurately). Consequently, we have
adopted a dierent approach in which the learning is fully integrated into the search process.
This is very important, because it enables us to control overtting and it also provides
additional opportunities for speeding up the search.
The basic way to integrate learning into the search process is very simple. Each time
the search algorithm needs to estimate a probability, the algorithm examines the training
data and computes the required probability estimate. For example, if an algorithm needs
to estimate P (x1 = v1 jfx3 = v3 ; x5 = v5 g), it can make a pass over the training data
and count the number of training examples where x3 = v3 and x5 = v5 . Denote this by
#(x3 = v3 ; x5 = v5 ). It can make a second pass over the data and count #(x1 = v1 ; x3 =
v3 ; x5 = v5). From these two quantities, it can compute the maximum likelihood estimate:

P^ (x1 = v1 j fx3 = v3 ; x5 = v5g) = #(x1#(=xv1 ;=xv3 ;=xv3 ;=xv5 )= v5 ) :
3

In general,

3

fxn = vng) :
P^ (xn = vnjs) = #(s [ #(
s)

5

5

Similarly, P (yjs) is estimated as the fraction of training examples matching state s that
have diagnosis y:
s; y) :
P^ (yjs) = #(
#(s)
279

Bayer-Zubek & Dietterich

This process can obviously be made more ecient by allowing the training data to
\ow" through the AND/OR graph (for AO algorithm) or the classication tree (for greedy
algorithms) as it is being constructed. Hence, the starting state (the root) stores a list of
all of the training examples. The OR node for state s stores a list of all of the training
examples that match s. An example matches a state if it agrees with all of the measurement
results that dene that state. An AND node that measures xn in state s can be viewed as
partitioning the training examples stored in OR node s into disjoint subsets according to
their observed values on test xn . The same method has been employed in classication tree
algorithms for many years (Breiman et al., 1984; Quinlan, 1993).
Unfortunately, this straightforward approach, when combined with both the systematic
and greedy search algorithms, often results in overtting|that is, nding policies that give
very good performance on the training data but that give quite poor performance on new
cases.
Figure 5 illustrates this for AO . This gure shows an anytime graph in which the
value V real (s0 ) of the current realistic policy, real , is plotted after each node expansion (or
iteration of the algorithm). V real is evaluated both on the training data and on a disjoint
test data set. On the training data, the quality of the learned policy improves monotonically
with the number of iterations|indeed, this is guaranteed by the AO algorithm. But on
the test data, the performance of the realistic policies gets worse after 350 iterations. Upon
convergence, AO has learned the optimal policy with respect to the training data, but this
policy performs badly on the test data.
Machine learning research has developed many strategies for reducing overtting. The
remainder of this section describes the regularizers that we have developed for both systematic and greedy search algorithms. First, we discuss regularizers for AO . Then we discuss
regularizers for greedy search.

5.1 Regularizers for AO Search
Overtting tends to occur when the learning algorithm extracts too much detailed information from the training data. This can happen, for example, when the learning algorithm
considers too many alternative policies for a given amount of training data. It can also
occur when the algorithm estimates probabilities from very small numbers of training examples. Both of these problems arise in AO . AO considers many dierent policies in a
large AND/OR graph. And as the AND/OR graph grows deeper, the probabilities in the
deeper nodes are estimated from fewer and fewer training examples.
We have pursued three main strategies for regularization: (a) regularizing the probability
estimates computed during the search, (b) reducing the amount of search through pruning
or early stopping, and (c) simplifying the learned policy by post-pruning to eliminate parts
that may have overt the training data.
5.1.1 Laplace Correction (denoted by `L')

To regularize probability estimates, a standard technique is to employ Laplace corrections. Suppose measurement xn has Vn possible outcomes. As discussed above, the
280

Learning Diagnostic Policies from Examples

45
AO* on training data
AO* on test data

Value of realistic policy

40

35

30

25

20

15
1

10

100
iteration

1000

10000

Figure 5: Illustration of AO overtting. This anytime graph shows that the best realistic
policy, according to the test data, was discovered after 350 iterations, after which
AO overts.
maximum likelihood estimate for P (xn = vn js) is

fxn = vng) :
P^ (xn = vnjs) = #(s [ #(
s)
The Laplace-corrected estimate is obtained by adding 1 to the numerator and Vn to the
denominator:
fxn = vng) + 1 :
P^L (xn = vn j s) = #(s [#(
s) + V
n

Similarly, the Laplace-corrected estimate for a diagnosis y is obtained by adding 1 to
the numerator and K (the number of possible diagnoses) to the denominator:

s; y) + 1 :
P^L (yjs) = #(
#(s) + K
One advantage of the Laplace correction is that no probability value will ever be estimated as 0 or 1. Those probability values are extreme, and hence, extremely dangerous.
281

Bayer-Zubek & Dietterich

45
AO* on training data
AO* on test data
AO*-L on training data
AO*-L on test data

Value of realistic policy

40

35

30

25

20

15
1

10

100
iteration

1000

10000

Figure 6: Anytime graphs of AO and AO with Laplace correction. The Laplace regularizer
helps AO , both in the anytime graph and in the value of the last policy learned.
For example, if AO believes that P (xn = vn js) = 0, then it will not expand this branch
further in the tree. Even more serious, if AO believes that P (y = cjs) = 0, then it will not
consider the potential misdiagnosis cost MC (fk ; y = c) when computing the expected costs
of diagnosis actions fk in state s.
Figure 6 shows that AO with the Laplace regularizer gives worse performance on the
training data but better performance on the test data than AO . Despite this improvement,
AO with Laplace still overts: a better policy that was learned early on is discarded later
for a worse one.
5.1.2 Statistical Pruning (SP)

Our second regularization technique reduces the size of the AO search space by pruning
subtrees that are unlikely to improve the current realistic policy.
The statistical motivation is the following: given a small training data sample, there are
many pairs of diagnostic policies that are statistically indistinguishable. Ideally, we would
like to prune all policies in the AND/OR graph that are statistically indistinguishable from
the optimal policies. Since this is not possible without rst expanding the graph, we need
a heuristic that approximately implements the following indierence principle:
282

Learning Diagnostic Policies from Examples

state s

unexpanded
optimistic policy
opt
V

realistic
policy

V

real

Figure 7: Statistical pruning (SP) checks whether V opt (s) falls inside a condence interval
around V real (s). If it does, then SP prunes opt (s) (the unexpanded optimistic
policy).

Indierence Principle. Given two diagnostic policies whose values are statistically

indistinguishable based on the training data set, a learning algorithm can choose arbitrarily
between them.
This heuristic is called statistical pruning (abbreviated SP), and it is applied in each
OR node s whose optimistic policy is selected for expansion. The two diagnostic policies
under consideration are the currently unexpanded optimistic policy opt (s) and the current
realistic policy real (s). The action specied by opt (s) will be pruned from the graph if a
statistical test cannot reject the null hypothesis that V opt (s) = V real (s). In other words,
between an incomplete policy opt and a complete policy real , we prefer the latter.
The statistical test is computed as follows. To each of the training examples te that
matches state s, we can apply real and compute the total cost of diagnosis (starting from
state s). From this information, we can compute a 95% condence interval on V real (s) (e.g.,
using a standard normal distribution assumption). If V opt (s) falls inside this condence
interval, then we cannot reject the null hypothesis that V opt (s) = V real (s). Therefore, by
the indierence principle, we can choose real (s) and prune opt (s). This is illustrated in
Figure 7.
Because V opt (s) is a lower bound on V  (s) (see Theorem 4.1) and V real (s) is an upper
bound on V  (s) (see Theorem 4.2), we can relate statistical pruning to the indierence
principle in a slightly stronger way. If V opt (s) falls inside the condence interval for V real (s),
then V  (s) must also fall inside the condence interval, because V opt (s)  V  (s)  V real (s).
Hence, with at least 95% condence, we cannot reject the null hypothesis that V real (s) =
V  (s). Hence, the indierence principle authorizes us to choose real , since it is statistically
indistinguishable from the optimal policy. However, this argument only remains true as
long as real remains unchanged. Subsequent expansions by AO may change real and
invalidate this statistical decision.
The SP heuristic is applied as the AND/OR graph is grown. When an AND node (s; a)
is selected for expansion, SP rst checks to see if this AND node should be pruned instead.
If it can be pruned, the action a will be ignored in further computations (SP deletes it from
283

Bayer-Zubek & Dietterich

the set of available actions A(s)). AO then updates Qopt ; V opt ; and opt in the graph. No
updates to real or V real are needed, because pruning a does not change the realistic graph.
In previous work (Bayer-Zubek & Dietterich, 2002), we described a version of the SP
heuristic that employed a paired-dierence statistical test instead of the simple condence
interval test described here. On synthetic problems, these two statistical tests gave nearly
identical results. We prefer the condence interval test, because it allows us to relate
V real (s) and V (s).
Some care must be taken when combining statistical pruning with Laplace corrections.
With Laplace corrections, the mean of the observed total cost of the training examples
matching state s when processed by real is not the same as V real (s), because the latter is
computed using Laplace-corrected probabilities. To x this problem, we compute the width
of the condence interval by applying real to each training example matching state s and
then use V real (s) as the center of the condence interval.
5.1.3 Early Stopping (ES)

Another way to limit the size of the search space considered by AO is to halt the search
early. This method has long been applied to regularize neural networks (e.g., Lang, Waibel,
& Hinton, 1990). Early stopping employs an internal validation set to decide when to halt
AO . The training data is split in half. One half is called the \subtraining data", and the
other half is called the \holdout data". AO is trained on the subtraining data, and after
every iteration, real is evaluated on the holdout data. The real that gives the lowest total
cost on the holdout data is remembered, and when AO eventually terminates, this best
realistic policy is returned as the learned policy.
Early stopping can be combined with the Laplace correction simply by running AO
with Laplace corrections on the subtraining set. There is no need to Laplace-correct the
evaluation of real on the holdout set.
5.1.4 Pessimistic Post-Pruning (PPP) Based on Misdiagnosis Costs

Our nal AO regularizer is pessimistic post-pruning. It is based on the well-known method
invented by Quinlan for pruning classication trees in C4.5 (Quinlan, 1993). PPP takes a
complete policy  and the training data set and produces a \pruned" policy 0 with the hope
that 0 exhibits less overtting. This PPP is applied to the nal realistic policy computed
by AO .
The central idea of PPP is to replace the expected total cost V  (s) at each state s with
a statistical upper bound UB (s) that takes into account the uncertainty due to the amount
and variability of the training data. At internal node s, if the upper bound shows that
selecting the best diagnosis action would be preferred to selecting measurement action (s),
then node s is converted to a leaf node (and the UB estimates of its ancestors in the policy
are updated). PPP can be performed in a single traversal of the decision tree for .
Computation begins at the leaves of policy  (i.e., the states s where (s) chooses a
diagnosis action fk ). Let UB (s) be the upper limit of a 95% normal condence interval
for C (s; fk ) (i.e., the expected misdiagnosis cost of choosing action fk in state s). This is
computed by taking each training example that matches state s, assigning it the diagnosis
284

Learning Diagnostic Policies from Examples

fk , and then computing the misdiagnosis cost MC (fk ; y), where y is the correct diagnosis
of the training example.
The upper bound at an internal node is computed according to the recursion
X
UB (s) = C (s; (s)) + Ptr (s0 js; (s))  UB (s0):
s

0

This is just the Bellman equation for state s but with the value function replaced by the UB
function. (s) will be pruned, and replaced by the diagnosis action fbest with the minimum
expected cost, if the upper bound on C (s; fbest ) is less than UB (s) for the internal node,
computed above. In this case, UB (s) is set to be the upper bound on C (s; fbest ).
PPP can be combined with Laplace regularization as follows. First, in computing UB (s)
for a leaf node, K \virtual" training examples are added to state s, such that there is one virtual example for each diagnosis. In other words, the normal condence interval is computed
using the misdiagnosis costs of the training examples that match s plus one MC ((s); y) for
each possible diagnosis y. Note that all probabilities P (yjs) and Ptr (s0 js; (s)) were already
Laplace-corrected when running AO with Laplace corrections.
5.1.5 Summary of AO Regularizers

We have described the following regularizers: Laplace corrections (L), statistical pruning
(SP), early stopping (ES), and pessimistic post-pruning (PPP). We have also shown how
to combine Laplace regularization with each of the others.

5.2 Regularizers for Greedy Search

We now describe four regularizers that we employed with greedy search.
5.2.1 Minimum Support Pruning

For the InfoGainCost and InfoGainCost+MC methods, we adopt the minimum support
stopping condition of C4.5 (Quinlan, 1993). In order for measurement action xn to be
chosen, at least two of its possible outcomes vn must lead to states that have at least 2
training examples matching them. If not, then xn is not eligible for selection in step (2) of
Table 1.
5.2.2 Pessimistic Post-Pruning (PPP) Based on Misdiagnosis Rates

For the InfoGainCost method, we applied C4.5's standard pessimistic post-pruning. After
InfoGainCost has grown the decision tree, the tree is traversed in post-order. For each leaf
node s, the pessimistic error estimate is computed as
2

s

3

UB (s) = n  4p + zc  p(1 n, p) + 21n 5 ;
where n is the number of training examples reaching the leaf node, p is the error rate
committed by the diagnosis action on the training examples at this leaf, and zc = 1:15
is the 75% critical value for the normal distribution. UB (s) is the upper limit of a 75%
condence interval for the binomial distribution (n; p) plus a continuity correction.
285

Bayer-Zubek & Dietterich

At an internal node s, the pessimistic error estimate is simply the sum of the pessimistic
error estimates of its children. An internal node is converted to a leaf node if the sum of its
children's pessimistic errors is greater than or equal to the pessimistic error that it would
have if it were converted to a leaf node.
Laplace regularization can be combined with PPP by replacing the observed error rate
p with its Laplace-corrected version (this is computed by adding one \virtual" example for
each diagnosis).
5.2.3 Post-Pruning Based on Expected Total Costs

For the MC+InfoGainCost method, we apply a post-pruning procedure that is based not
on a pessimistic estimate but rather on the estimated total cost of diagnosis. Recall that
MC+InfoGainCost grows the decision tree in the same way as InfoGainCost, but it assigns diagnosis actions to the leaf nodes by choosing the action with the smallest expected
misdiagnosis cost on the training data.
This can be further regularized by traversing the resulting decision tree and converting an
internal node s where (s) = xn into a leaf node (where (s) = fbest ) if the expected cost of
choosing diagnosis action fbest is less than the expected total cost of choosing measurement
action xn . This is implemented by computing C (s; fbest ) and Q (s; xn ) and comparing
them. If C (s; fbest)  Q (s; xn ), then node s is converted to a leaf node. This computation
can be carried out in a single post-order traversal of the decision tree corresponding to .
Laplace corrections can be combined with this pruning procedure by applying Laplace
corrections to all probabilities employed in computing Q (s; xn ) and C (s; fbest).
Bradford et al. (1998) present a similar method of pruning decision trees based on
misclassication costs (and zero attribute costs), combined with Laplace correction for class
probability estimates (there is no Laplace correction for transition probabilities).
It is interesting to note that this post-pruning based on total costs is not necessary for
VOI, because pruning is already built-in. Indeed, any internal node s in the VOI policy
, with (s) = xn, has Q (s; xn )  V OI (s; xn ) < C (s; fbest ) (the proof of this theorem
appears in the thesis of Bayer-Zubek (2003)).
5.2.4 Laplace Correction

As with AO , we could apply Laplace corrections to all probabilities computed during
greedy search.
For the InfoGainCost method, Laplace correction of diagnosis probabilities P (yjs) does
not change the most likely diagnosis. For the MC+InfoGainCost method, Laplace correction
of diagnosis probabilities may change the diagnosis action with the minimum expected cost.
Laplace correction is not applied in the computation of the information gain I (xn ; yjs). For
the InfoGainCost method, Laplace correction is only applied in the pruning phase, to the
error rate p. For the MC+InfoGainCost method, Laplace correction is applied, as the policy
is grown, to P (yjs) when computing C (s; fk ), and it is also applied during the post-pruning
based on expected total costs, to both P (xn = vn js) and P (yjs).
For the VOI method, Laplace correction is applied to all probabilities employed in
Equation 5 and in the computation of C (s; fbest).
286

Learning Diagnostic Policies from Examples

6. Experimental Studies
We now present an experimental study to measure and compare the eectiveness and efciency of the various search and regularization methods described above. The goal is to
identify one or more practical algorithms that learn good diagnostic policies on real problems with modest-sized training data sets. The main questions are: Which algorithm is the
best among all the algorithms proposed? If there is no overall winner, which is the most
robust algorithm?

6.1 Experimental Setup
We performed experiments on ve medical diagnosis problems based on real data sets found
at the University of California at Irvine (UCI) repository (Blake & Merz, 1998). The
ve problems are listed here along with a short name in parentheses that we will use
to refer to them: Liver disorders (bupa), Pima Indians Diabetes (pima), Cleveland Heart
Disease (heart), the original Wisconsin Breast Cancer (breast-cancer), and the SPECT heart
database (spect). These data sets describe each patient by a vector of attribute values and
a class label. We dene a measurement action for each attribute; when executed, the action
returns the measured value of that attribute. We dene one diagnosis action for each class
label.
The domains were chosen for two reasons. First, they are all real medical diagnosis
domains. Second, measurement costs have been provided for three of them (bupa, pima, and
heart) by Peter Turney (Turney, 1995); for the other two domains, we set all measurement
costs to be 1. Table 2 briey describes the medical domains; more information is available
in the thesis of Bayer-Zubek (2003).
Some pre-processing steps were applied to all domains. First, all training examples
that contained missing attribute values were removed from the data sets. Second, if a data
set contained more than two classes, selected classes were merged so that only two classes
(healthy and sick) remained. Third, any existing division of the data into training and
test sets was ignored, and the data were simply merged into a single set. Each real-valued
attribute xn was discretized into 3 levels (as dened by two thresholds, 1 and 2 ) such that
the discretized variable takes on a value of 0 if xn  1 , a value of 1 if 1 < xn  2 and a
value of 2 otherwise. The values of the thresholds were chosen to maximize the information
gain between the discretized variable and the class. The information gain was computed
using the entire data set.
For each domain, the transformed data (2 classes, discretized attributes with no missing
values) was used to generate 20 random splits into training sets (two thirds of data) and
test sets (one third of data), with sampling stratied by class. Such a split (training data,
test data) is called a replica. We repeated each of our experiments on each of the replicas
to obtain a rough idea of the amount of variability that can be expected from one replica to
another. However, it is important to note that because the replicas are not independent (i.e.,
they share data points), we must use caution in combining the results of dierent replicas
when drawing conclusions about the superiority of one algorithm compared to another.
287

Bayer-Zubek & Dietterich

Table 2: Medical domains. For each domain, we list the number of examples, the number
of tests, and the minimum and the maximum cost for a test.
domain
# examples # tests min test cost max test cost
bupa
345
5
7.27
9.86
pima
768
8
1
22.78
heart
297
13
1
102.9
breast-cancer
683
9
1
1
spect
267
22
1
1
6.1.1 Setting the Misdiagnosis Costs (MC)

None of the ve UCI domains species misdiagnosis costs, so we performed our experiments
using ve dierent levels of misdiagnosis costs for each domain. These cost levels were
designed such that in the initial state s0 both diagnosis decisions f0 and f1 have equal
expected cost and so that the diagnostic policies are non-trivial (i.e., they perform at least
one measurement, but do not perform all possible measurements). We call the ve MC
levels MC1, MC2, MC3, MC4, and MC5, and they progressively make misdiagnosis more
expensive. Full details of the methodology are given in the thesis of Bayer-Zubek (2003).
6.1.2 Memory Limit

For large domains (with many measurements), the AND/OR graph constructed by AO
grows very large, especially in the following cases: the measurements are not very informative; the measurement costs are low relative to the misdiagnosis costs, so our admissible
heuristic does not produce many cutos; the optimal policy is very deep; and there are
many policies tied with the optimal one and AO needs to expand all of them to prove to
itself that there is no better alternative.
To make systematic search feasible, we need to prevent the AND/OR graph from growing
too large. We do this by imposing a limit on the total amount of memory that the AND/OR
graph can occupy. We measure memory usage based on the amount of memory that would
be required by an optimized AND/OR graph data structure. This \theoretical" memory
limit is set to 100 MB. Because our actual implementation is not optimized, this translates
into a limit of 500 MB. When the memory limit is reached, the current realistic policy
is returned as the result of the search. All of our algorithms (greedy and systematic)
converge within this memory limit on all ve domains, with one exception: AO with large
misdiagnosis costs reaches the memory limit on the spect data set.
It is interesting to note that even on a domain with many measurements, the systematic
search algorithms may converge before reaching the memory limit. This is a consequence of
the fact that for modest-sized training data sets, the number of reachable states in the MDP
(i.e., states that can be reached with non-zero probability by some policy) is fairly small,
288

Learning Diagnostic Policies from Examples

because not all possible combinations of attribute values can appear in a modest-sized data
set.
6.1.3 Notations for our Learning Algorithms

In the remainder of this paper, we will employ the following abbreviations to identify the
various search algorithms and their regularizers. In all cases, the sux \L" indicates that
Laplace corrections were applied to the algorithm as described in Section 5.

 Nor, Nor-L denote InfoGainCost with Norton's criterion for selecting actions and pes






simistic post-pruning based on misdiagnosis rates.
MC-N, MC-N-L denote MC+InfoGainCost with Norton's criterion for selecting measurement actions. Diagnosis actions are selected to minimize expected misdiagnosis
costs. Post-pruning is based on expected total costs.
VOI, VOI-L denote the one-step Value of Information greedy method.
AO , AO -L denote AO .
SP, SP-L denote AO with Statistical Pruning.
ES, ES-L denote AO with Early Stopping. For early stopping, half of the training
data is held out to choose the stopping point, and the other half is used by AO to
compute transition probabilities.
PPP, PPP-L denote AO with Pessimistic Post-Pruning.

6.1.4 Evaluation Methods

To evaluate each algorithm, we train it on the training set to construct a policy. Then we
compute the value of this policy on the test set, which we denote by Vtest . To compute
Vtest , we sum the measurement costs and misdiagnosis cost for each test example, as it is
processed by the policy, and then divide the total cost for all examples by the number of
test examples.
Note that in our framework, Vtest is always computed using both measurement costs
and misdiagnosis costs, even if the policy was constructed by a learning algorithm (e.g.,
InfoGainCost) that ignores misdiagnosis costs.
In order to compare learning algorithms, we need some way of comparing their Vtest
values to see if there is a statistically signicant dierence among them. Even if two learning
algorithms are equally good, their Vtest values may be dierent because of random variation
in the choice of training and test data sets. Ideally, we would employ a statistical procedure
similar to analysis of variance to determine whether the observed dierences in Vtest can be
explained by dierences in the learning algorithm (rather than by random variation in the
data sets). Unfortunately, no such procedure exists that is suitable for comparing diagnostic
policies. Hence, we adopted the following procedure.
As discussed above, we have generated 20 replicas of each of our data sets. In addition,
we have built ve misdiagnosis cost matrices for each data set. We apply each learning
algorithm to each replica using each of the ve MC matrices, which requires a total of 500
289

Bayer-Zubek & Dietterich

runs of each learning algorithm for all domains. For each replica and cost matrix and each
pair of learning algorithms (call them alg1 and alg2), we apply the BDeltaCost bootstrap
statistical test (Margineantu & Dietterich, 2000) to decide whether the policy constructed
by alg1 is better than, worse than, or indistinguishable from the policy constructed by alg2
(based on a 95% condence level). Depending on the BDeltaCost results, we say that
alg1 wins, loses, or ties alg2.
The BDeltaCost test is applied to each replica of each data set. These BDeltaCost
results are then combined to produce an overall score for each algorithm according to the
following chess metric. For a given pair of algorithms, alg1 and alg2, and a domain D, let
(wins; ties; losses) be the cumulative BDeltaCost results of alg1 over alg2, across all ve
misdiagnosis cost matrices and all 20 replicas. The chess metric is computed by counting
each win as one point, each tie as half a point, and each loss as zero points:
Score(alg1; alg2; D) def
= wins + 0:5  ties:
We can also compute the overall chess score for an algorithm by summing its chess scores
against all of the other algorithms:
X
Score(alg1; D) =
Score(alg1; alg2; D):
alg26=alg1

Note that if the total number of \games" played by an algorithm is Total = wins +
ties + losses, and if all the games turned out to be ties, the chess score would be 0:5  Total,

which we will call the Tie-Score. If the algorithm's chess score is greater than the Tie-Score,
then the algorithm has more wins than losses.
The pairwise BDeltaCost tests account for variation in Vtest resulting from the random choice of the test sets. The purpose of the 20 replicas is to account also for random
choice of training sets. Ideally, the 20 training sets would be disjoint, and this would allow us to compute an unbiased estimate of the variability in Vtest due to the training sets.
Unfortunately, because the amount of training data is limited, we cannot make the training sets independent, and as a result, the overall chess scores probably underestimate this
source of variability.

6.2 Results

We now present the results of the experiments.
6.2.1 Laplace Correction Improves All Algorithms

We rst studied the eect of the Laplace regularizer on each algorithm. For each of the
seven algorithms with the Laplace correction, we computed its chess score with respect
to its non-Laplace version, on each domain. The total number of \games" an algorithm
plays against its non-Laplace version is 100 (there are 5 misdiagnosis costs and 20 replicas);
therefore, Tie-Score = 50.
Figure 8 shows that on each domain, the Laplace-corrected algorithm scores more wins
than losses versus the non-Laplace-corrected algorithm (because each score is greater than
Tie-Score). This supports the conclusion that the Laplace correction improves the performance of each algorithm. Some algorithms, such as Nor and AO , are helped more than
others by Laplace.
290

Learning Diagnostic Policies from Examples

Chess score of Laplace-corrected alg. vs. non-Laplace alg.

100

bupa
90

pima
heart

80

b-can

70

spect

60

Tie-Score

50
40
30
20
10
0
Nor-L

MC-N-L
greedy

VOI-L

AO*-L

SP-L

ES-L

PPP-L

systematic

Figure 8: The score of each Laplace-corrected algorithm versus its non-Laplace version, on
each domain, is greater than the Tie-Score. Therefore the Laplace version has
more wins than losses.
Since the Laplace regularizer improved each algorithm, we decided to compare only the
Laplace-corrected versions of the algorithms in all subsequent experiments.
6.2.2 The Most Robust Algorithm

To determine which algorithm is the most robust across all ve domains, we computed
the overall chess score of each Laplace-corrected algorithm against all the other Laplacecorrected algorithms, on each domain. The total number of \games" is 600 (there are
5 misdiagnosis costs matrices, 20 replicas, and 6 \opponent" algorithms); therefore, the
Tie-Score is 300.
Figure 9 shows that the best algorithm varies depending on the domain: ES-L is best
on bupa, VOI-L is best on pima and spect, SP-L is best on heart, and MC-N-L is best on
breast-cancer. Therefore no single algorithm is best everywhere. Nor-L is consistently bad
on each domain; its score is always below the Tie-Score. This is to be expected, since
Nor-L does not use misdiagnosis costs when learning its policy. MC-N-L, which does use
misdiagnosis costs, always scores better than Nor-L.
291

Bayer-Zubek & Dietterich

400
bupa
380

pima
heart

360

b-can

Overall chess score

340

spect

320
Tie-Score

300
280
260
240
220
200
Nor-L

MC-N-L
greedy

VOI-L

AO*-L

SP-L

ES-L
systematic

PPP-L

Figure 9: The overall chess score of each Laplace-corrected algorithm, versus all the other
Laplace-corrected algorithms. The most robust algorithm is SP-L; it was the only
one whose score is greater than Tie-Score (and therefore it has more wins than
losses) on every domain.
The fact that VOI-L is best in two domains is very interesting, because it is an ecient
greedy algorithm. Unfortunately, VOI-L obtains the worst score in two other domains: heart
and breast-cancer.
The only algorithm that has more wins than losses in every domain is SP-L, which combines AO search, Laplace corrections, and statistical pruning. SP-L always scored among
the top three algorithms. Consequently, we recommend it as the most robust algorithm.
However, in applications where SP-L (or any of the systematic search algorithms) is too
expensive to run, VOI-L can be recommended, since it is the best of the greedy methods.
In addition to looking at the overall chess scores, we also studied the actual Vtest values.
To visualize the dierences in Vtest values, we plotted a graph that we call a \pair graph".
Figure 10 shows pair graphs comparing VOI-L and SP-L on all ve domains. The horizontal
axis in each graph corresponds to the 20 replicas, and the vertical axis to Vtest values for the
two algorithms (VOI-L and SP-L) on that replica. The 20 replicas are sorted according to
292

Learning Diagnostic Policies from Examples

the Vtest of VOI-L. If the two algorithms were tied on a replica (according to BDeltaCost),
then their Vtest values are connected by a vertical dotted line.
On bupa and heart, the Vtest of SP-L is mostly smaller (better) than the Vtest of VOI-L,
but BDeltaCost nds them tied. On pima and spect, the situation is reversed (VOI-L is
almost always better than SP-L), and on several replicas the dierence is statistically significant. On breast-cancer, SP-L is better than VOI-L, and again the dierence is sometimes
signicant. In general, the pair graphs conrm the chess score results and support our main
conclusion that SP-L is the most robust learning algorithm.
6.2.3 Impact of Heuristics and Regularizers on Memory Consumption

We now consider the eect of the admissible heuristic and the Laplace and Statistical Pruning regularizers on the amount of memory required for AO search. To do this, we measured
the amount of memory consumed by ve dierent algorithm congurations: AO without
the admissible heuristic, AO with the admissible heuristic, AO with the admissible heuristic and the Laplace correction, AO with the admissible heuristic and statistical pruning,
and, nally, AO with the admissible heuristic, Laplace correction, and statistical pruning.
For AO without the admissible heuristic, we set the action-value of every unexpanded
AND node (s; xn ) to zero, i.e., Qopt (s; xn ) = 0. The results are plotted in Figure 11. The
memory amounts plotted are computed by taking the actual memory consumed by our
implementation and converting it to the memory that would be consumed by an optimized
implementation.
There are several important conclusions to draw from these gures. First, note that AO
without the admissible heuristic requires much more memory than AO with the admissible
heuristic. Hence, the admissible heuristic is pruning large parts of the search space. This is
particularly evident at low settings of the misdiagnosis costs (MC1 and MC2). At these low
settings, AO is able to nd many cutos because the expected cost of diagnosis is less than
the cost of making additional measurements (as estimated by the admissible heuristic). The
savings is much smaller at MC levels 4 and 5.
The second important conclusion is that the Laplace correction increases the size of the
search space and the amount of memory consumed. The reason is that without the Laplace
correction, many test outcomes have zero probability, so they are pruned by AO . With
the Laplace correction, these outcomes must be expanded and evaluated. The eect is very
minor at low MC levels, because the AND/OR graph is much smaller, and consequently
there is enough training data to prevent zero-probability outcomes. But at high MC levels,
the Laplace correction can cause increases of a factor of 10 or more in the amount of memory
consumed.
The third important conclusion is that statistical pruning signicantly decreases the size
of the AND/OR graph in almost all cases. The only exception is heart at MC4 and MC5,
where statistical pruning increases the amount of memory needed by AO . It seems paradoxical that statistical pruning could lead to an overall increase in the size of the AND/OR
graph that is explored. The explanation is that there can be an interaction between statistical pruning at one point in the AND/OR graph and additional search elsewhere. If a
branch is pruned that would have given a signicantly smaller value for V  , this can pre293

Bayer-Zubek & Dietterich

130

2400
VOI-L
SP-L

VOI-L
SP-L
2300

125

2200
120
2100

V_test[replica]

V_test[replica]

115

110

105

2000
1900
1800
1700

100
1600
95

1500

90

1400
replica

replica

(a) bupa

(b) pima

550

9
VOI-L
SP-L

VOI-L
SP-L
8

500

450

V_test[replica]

V_test[replica]

7

400

6

5

4
350
3

300

2
replica

replica

(c) heart

(d) breast-cancer

45
VOI-L
SP-L

V_test[replica]

40

35

30

25

20
replica

(e) spect
Figure 10: Pair graphs for VOI-L and SP-L for every domain and replica, for the largest
misdiagnosis costs MC5. The replicas are sorted by increasing order of Vtest for
VOI-L. Vertical lines connect Vtest values that are tied according to BDeltaCost.
294

Learning Diagnostic Policies from Examples

BUPA

Pima

30000

1e+07

AO*+L
AO*+L

20000
AO* no heur

AO*
AO*+SP

Memory Consumed (bytes)

Memory Consumed (bytes)

AO*+SP+L

AO*+SP+L
AO* no heur
AO*

1e+06

AO*+SP

100000

10000
MC1

MC2

MC3

MC4

MC5

MC1

MC2

Misdiagnosis Cost Level

MC3

MC4

MC5

Misdiagnosis Cost Level

(a) bupa

(b) pima

Heart

Breast-cancer

1e+09

1e+07
AO*+L
AO*+L

1e+08

1e+06

AO* no heur
AO*+SP
AO*

1e+06

100000

Memory Consumed (bytes)

Memory Consumed (bytes)

AO*+SP+L
1e+07

AO*+SP+L
AO* no heur

AO*

100000

AO*+SP

10000

10000
1000
1000

100

100
MC1

MC2

MC3

MC4

MC5

MC1

Misdiagnosis Cost Level

MC2

MC3

MC4

MC5

Misdiagnosis Cost Level

(c) heart

(d) breast-cancer

Spect

Memory Consumed (bytes)

2e+08

1e+08
AO* no heur
AO*
AO*+L
AO*+SP
AO*+SP+L
5e+07

2e+07
MC1

MC2

MC3

MC4

MC5

Misdiagnosis Cost Level

(e) spect

Figure 11: Memory consumed in each domain for ve combinations of AO with and without the admissible heuristic, Laplace corrections, and statistical pruning and for
ve levels of misdiagnosis costs.

295

Bayer-Zubek & Dietterich

vent heuristic cutos elsewhere in the graph. So in some cases, pruning can increase overall
memory consumption.
The nal conclusion is that statistical pruning dramatically reduces the amount of memory required by AO with the Laplace correction. Even in cases, such as heart, where statistical pruning causes AO (without Laplace) to consume more space, SP reduces the amount
of space needed with the Laplace correction by nearly an order of magnitude. Nonetheless,
statistical pruning is not able to completely compensate for the extra memory consumption
of the Laplace corrections, so the nal algorithm (AO + SP + L) requires more memory
than AO without any admissible heuristic at high MC levels, and AO + SP + L requires
much more memory than AO with the admissible heuristic.
Despite the large amount of memory required, there was only one domain (spect at
MC4 and MC5) where the AO hit the memory limit. Hence, we can see that in terms of
memory, systematic search with AO is feasible on today's desktop workstations.
6.2.4 CPU Time

In addition to measuring memory consumption, we also measured the CPU time required
by all of our algorithms. The results are plotted in Figure 12. As expected, the systematic
search algorithms require several orders of magnitude more CPU time than the greedy
methods. However, even the most expensive algorithm congurations require less than
1000 seconds to execute. Note that as the misdiagnosis cost level increases, the amount of
CPU time increases. This is a direct reection of the corresponding increase in the size of
the AND/OR graph that is explored by the algorithms.

7. Conclusions
The problem addressed in this paper is to learn a diagnostic policy from a data set of labeled
examples, given both measurement costs and misdiagnosis costs. The tradeo between these
two types of costs is an important issue that machine learning research has only just begun
to study.
We formulated the process of diagnosis as a Markov Decision Problem. We then showed
how to apply the AO algorithm to solve this MDP to nd an optimal diagnostic policy. We
also showed how to convert the AO algorithm into an anytime algorithm by computing the
realistic policy at each point in the search (the realistic policy is the best complete policy
found so far). We dened an admissible heuristic for AO that is able to prune large parts
of the search space on our problems. We also presented three greedy algorithms for nding
diagnostic policies.
The paper then discussed the interaction between learning from training data and searching for a good diagnostic policy. Experiments demonstrated that overtting is a very serious
problem for AO . The central contribution of the paper is the development of methods for
regularizing the AO search to reduce overtting and, in some cases, also to reduce the
size of the search space. Four regularization techniques (Laplace corrections, statistical
pruning, early stopping, and pessimistic post-pruning) were presented. The paper also introduced regularizers for the greedy search algorithms by extending existing methods from
classication tree learning.
296

Learning Diagnostic Policies from Examples

1

1000
MC1
MC3
MC5

MC1
MC3
MC5

average_replica CPU Time (in seconds)

average_replica CPU Time (in seconds)

100

0.1

0.01

10

1

0.1

0.001

0.01
Nor

Nor-L MC-NMC-N-L VOI VOI-L AO* AO*-L

ES

ES-L

SP

SP-L

PPP PPP-L

Nor

(a) bupa

Nor-L MC-N MC-N-L VOI

VOI-L AO* AO*-L

ES

ES-L

SP

SP-L

PPP PPP-L

ES

ES-L

SP

SP-L

PPP PPP-L

(b) pima

1000

100
MC1
MC3
MC5

MC1
MC3
MC5

100
average_replica CPU Time (in seconds)

average_replica CPU Time (in seconds)

10

10

1

0.1

1

0.1

0.01
0.01

0.001

0.001
Nor

Nor-L MC-NMC-N-L VOI VOI-L AO* AO*-L

ES

ES-L

SP

SP-L

PPP PPP-L

Nor

(c) heart

Nor-L MC-NMC-N-L VOI VOI-L AO* AO*-L

(d) breast-cancer

1000
MC1
MC3
MC5

average_replica CPU Time (in seconds)

100

10

1

0.1

0.01
Nor

Nor-L MC-N MC-N-L VOI

VOI-L AO* AO*-L

ES

ES-L

SP

SP-L

PPP PPP-L

(e) spect
Figure 12: CPU time for all 14 algorithm congurations on the ve domains (in each case
averaged over 20 replicas). The three curves plot CPU time for misdiagnosis
cost levels MC1, MC3, and MC5.

297

Bayer-Zubek & Dietterich

The various search and regularization algorithms were tested experimentally on ve
classication problems drawn from the UCI repository. A methodology for assigning misdiagnosis costs was developed so that these problems could be converted into cost-sensitive
diagnosis problems. The paper also introduced a methodology for combining the results
of multiple training/test replicas into an overall \chess score" for evaluating the learning
algorithms.
The experiments showed that all of the search algorithms were improved by including
Laplace corrections when estimating probabilities from the training data. The experiments
also showed that the systematic search algorithms were generally more robust than the
greedy search algorithms across the ve domains. The best greedy algorithm was VOI-L,
but although it obtained the best score on two domains, it produced the worst score on two
other domains. The most robust learning algorithm was SP-L. It combines systematic AO
search with Laplace corrections and statistical pruning.
Systematic search for diagnostic policies has not been studied previously by machine
learning researchers, probably because it has generally been regarded as computationally
infeasible. A surprising conclusion of this paper is that AO is computationally feasible
when applied to the problem of learning diagnostic policies from training examples. This
conclusion is based both on experimental evidence|AO required less than 500 MB of
memory on virtually all of our benchmark scenarios|and theoretical analysis.
From a theoretical perspective, there are ve factors that help make AO feasible in
this setting: the modest amount of training data, the modest number of possible tests, the
small number of outcomes for each test, the admissible heuristic, and the statistical pruning
regularizer. We discuss each of these factors in turn:
Modest amount of training data. In learning for diagnosis, there is a cost for measuring each attribute of each training example. Consequently, each training example is
expensive to collect, and this puts a practical limit on the size of the training data
set. This in turn limits the space of reachable states in the MDP. As a result, the
AND/OR graph searched by AO does not grow too large. As the amount of training
data grows, this graph will gradually grow larger and at some point, it will become
too large for the available memory. Good results may still be obtained by imposing a
memory limit, as we did in the spect experiments.
Modest number of possible tests. Our experiments only considered domains with 22
or fewer tests. The number of tests determines the branching factor of the OR nodes
in the graph, so the size of the graph scales exponentially in this quantity. However, if
most of the tests can be pruned by the admissible heuristic or by statistical pruning,
the exponential explosion can be avoided. Whether this is possible in any particular
problem depends on the relative costs and informativeness of the dierent tests.
Small number of outcomes for each test. We discretized each continuous measurement
to have only 3 outcomes. The number of outcomes determines the branching factor
of the AND nodes in the graph, so the graph size scales exponentially in this quantity
as well. This quantity can be controlled through discretization (see below).
The admissible heuristic. The problem of learning for diagnosis is non-trivial only when
the costs of making measurements are comparable to the costs of misdiagnosis. But
298

Learning Diagnostic Policies from Examples

when this is true, our admissible heuristic is able to prune large parts of the search
space.

Statistical pruning. Finally, the statistical pruning regularizer is able to prune parts of
the search space that are unlikely to produce improved policies.

Notice that the size of the AND/OR graph does not increase as the number of possible
diagnoses increases. Hence, the AO* search approach scales well with the number of possible
diagnostic outcomes.
In cases where the AND/OR graph becomes infeasibly large, we recommend VOI-L, since
our experiments showed that it was the best greedy method.
The MDP framework for diagnosis is general enough to handle several extensions to the
learning algorithms studied in this paper. For example, in our experiments, we considered
only diagnosis problems that involve two classes, \healthy" and \sick." But this could easily
be generalized to consider an arbitrary number of classes. Our implementations assumed
that the cost of a measurement depends only on the attribute being measured, C (xn ). This
can easily be generalized so that the cost of a measurement depends on which tests have
already been executed and the results that they produced, and it can also depend on the
result of the measurement. In other words, the cost function of a measurement can be
generalized to C (s; xn ; s0 ), where s is the current state of the MDP, xn is the measurement,
and s0 is the resulting state s0 = s [ fxn = vn g. Our implementations also assumed that
the misdiagnosis costs were xed for all patients, but this could be extended to allow the
costs to vary from one patient to another. These changes in the diagnosis problem (multiple
classes and complex costs) do not modify the size or complexity of the MDP.
Some important extensions to the diagnostic setting will require extensions to the MDP
framework as well. For example, to handle treatment actions that have side eects, noisy
actions that may need to be repeated, or actions that have delayed results, the denition
of a state in the MDP needs to be extended. An initial examination of these extensions
suggest that each of them will cause the MDP state space to grow signicantly, and this
may make it infeasible to search the space of diagnostic policies systematically. Hence, these
extensions will probably require new ideas for their solution.
Another important direction for future work is to extend our approach to handle tests
with a large number of possible outcomes, including particularly tests with continuous
measured values. We applied standard information-gain methods for discretizing continuous attributes, but an interesting direction for future work is to develop cost-sensitive
discretization methods.
A nal challenge for future research is to learn good diagnostic policies from incomplete training data. The algorithms presented in this paper assume that each attribute of
each training example has been measured. Such data are hard to obtain. But every day,
thousands of patients are seen by physicians, medical tests are performed, and diagnostic
decisions are made. These data are incomplete, because each physician is following his or
her own diagnostic policy that certainly does not perform all possible medical tests. The
resulting training examples have many missing values, but these values are not \missing at
random", so standard methods for handling missing values cannot be applied. Methods for
learning diagnostic policies from such data would be very valuable in many applications.
299

Bayer-Zubek & Dietterich

The problem of learning a diagnostic policy from data collected while executing some
other diagnostic policy is identical to the problem of \o-policy" reinforcement learning
(Sutton & Barto, 1999). In reinforcement learning, the diagnostic policy generating the
data is called the exploration policy. Much is known about creating exploration policies
that enable learning of optimal policies. For example, if the exploration policy has non-zero
probability of executing every action in every state, then the optimal policy can still be
learned. If the exploration policy can be controlled by the learning system, then much more
selective exploration can produce the optimal policy (Kearns & Singh, 1998). By extending
these ideas, it may be possible to learn diagnostic policies from data collected routinely in
hospitals and clinics.
The problem of learning diagnostic policies is fundamental to many application domains
including medicine, equipment diagnosis, and autonomic computing. A diagnostic policy
must balance the cost of gathering information by performing measurements with the cost
of making incorrect diagnoses. This paper has shown that AO -based systematic search,
when combined with regularization methods for preventing overtting, is a feasible method
for learning good diagnostic policies from labeled examples.

Acknowledgments
The authors gratefully acknowledge the support of the National Science Foundation under
grants IRI-9626584, EIA-9818414, IIS-0083292, EIA-0224012, and ITR-5710001197. The
authors also gratefully acknowledge the support of the Air Force Oce of Scientic Research
under grant number F49620-98-1-0375.
This paper extends the conference paper of Bayer-Zubek (2004).
The authors thank the anonymous reviewers for their comments.

References
Bayer-Zubek, V. (2003). Learning Cost-sensitive Diagnostic Policies from Data. Ph.D.
thesis, Department of Computer Science, Oregon State University, Corvallis,
http://eecs.oregonstate.edu/library/?call=2003-13.
Bayer-Zubek, V. (2004). Learning diagnostic policies from examples by systematic search.
In Proceedings of the Twentieth Conference on Uncertainty in Articial Intelligence,
pp. 27{35, Ban, Canada.
Bayer-Zubek, V., & Dietterich, T. (2002). Pruning improves heuristic search for costsensitive learning. In Proceedings of the Nineteenth International Conference of Machine Learning, pp. 27{35, Sydney, Australia. Morgan Kaufmann.
Blake, C., & Merz, C. (1998). UCI repository of machine learning databases.
http://www.ics.uci.edu/mlearn/MLRepository.html.
Bradford, J. P., Kunz, C., Kohavi, R., Brunk, C., & Brodley, C. E. (1998). Pruning decision trees with misclassication costs. In European Conference on Machine Learning,
pp. 131{136. Longer version from http://robotics.stanford.edu/users/ronnyk/ronnykbib.html, or as ECE TR 98-3, Purdue University.
300

Learning Diagnostic Policies from Examples

Breiman, L., Friedman, J., Olshen, R. A., & Stone, C. (1984). Classication and Regression
Trees. Wadsworth, Monterey, California.
Cooper, G. F., & Herskovits, E. (1992). A Bayesian method for the induction of probabilistic
networks from data. Machine Learning, 9, 309{347.
Dittmer, S., & Jensen, F. (1997). Myopic value of information in inuence diagrams. In
Proceedings of the Thirteenth Conference on Uncertainty in Articial Intelligence, pp.
142{149, San Francisco.
Domingos, P. (1999). MetaCost: A general method for making classiers cost-sensitive. In
Knowledge Discovery and Data Mining, pp. 155{164.
Fawcett, T., & Provost, F. (1997). Adaptive fraud detection. Data Mining and Knowledge
Discovery, 1(3), 1{28.
Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classiers. Machine
Learning, 29, 131{163.
Friedman, N., & Goldszmidt, M. (1996). Building classiers using Bayesian networks. In
Proceedings of the Thirteenth National Conference on Articial Intelligence, pp. 1277{
1284, Cambridge, MA. AAAI Press/MIT Press.
Greiner, R., Grove, A. J., & Roth, D. (2002). Learning cost-sensitive active classiers.
Articial Intelligence, 139:2, 137{174.
Hansen, E. (1998). Solving POMDPs by searching in policy space. In Proceedings of
the Fourteenth International Conference on Uncertainty in Articial Intelligence, pp.
211{219, San Francisco. Morgan Kaufmann.
Heckerman, D., Breese, J., & Rommelse, K. (1994). Troubleshooting under uncertainty.
Tech. rep., MSR-TR-94-07, Microsoft Research.
Heckerman, D., Horvitz, E., & Middleton, B. (1993). An approximate nonmyopic computation for value of information. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 15, 292{298.
Heckerman, D., Breese, J., & Rommelse, K. (1995). Decision-theoretic troubleshooting.
Communications of the ACM, 38, 49{57.
Kearns, M., & Singh, S. (1998). Near-optimal reinforcement learning in polynomial time.
In Proceedings of the Fifteenth International Conference on Machine Learning, pp.
260{268. Morgan Kaufmann, San Francisco, CA.
Lang, K. J., Waibel, A. H., & Hinton, G. E. (1990). A time-delay neural network architecture
for isolated word recognition. Neural Networks, 3, 33{43.
Lenert, L. A., & Soetikno, R. M. (1997). Automated computer interviews to elicit utilities:
potential applications in the treatment of deep venous thrombosis. Journal of the
American Medical Informatics Association, 4 (1), 49{56.
Littman, M. L., Ravi, N., Fenson, E., & Howard, R. (2004). An instance-based state representation for network repair. In Proceedings of the Nineteenth National Conference
on Articial Intelligence (in press), San Jose, California.
301

Bayer-Zubek & Dietterich

Madigan, D., & Almond, R. (1996). On test selection strategies for belief networks. In
Fisher, D., & Lenz, H. (Eds.), Learning from Data: AI and Statistics, pp. 89{98.
Morgan Kaufmann.
Margineantu, D. D., & Dietterich, T. (2000). Bootstrap methods for the cost-sensitive
evaluation of classiers. In Proceedings of the Seventeenth International Conference
of Machine Learning, pp. 583{590, San Francisco, CA. Morgan Kaufmann.
Martelli, A., & Montanari, U. (1973). Additive AND/OR graphs. In Proceedings of the
Third International Joint Conference on Articial Intelligence, pp. 1{11.
Nilsson, N. (1980). Principles of Articial Intelligence. Tioga Publishing Co., Palo Alto,
CA.
Norton, S. W. (1989). Generating better decision trees. In Proceedings of the Eleventh
International Joint Conference on Articial Intelligence, pp. 800{805, San Francisco.
Morgan Kaufmann.
Nunez, M. (1991). The use of background knowledge in decision tree induction. Machine
Learning, 6(3), 231{250.
Pattipati, K. R., & Alexandridis, M. G. (1990). Application of heuristic search and information theory to sequential fault diagnosis. IEEE Transactions on Systems, Man and
Cybernetics, 20(4), 872{887.
Pazzani, M., Merz, C., Murphy, P., Ali, K., Hume, T., & Brunk, C. (1994). Reducing
misclassication costs. In Proceedings of the Eleventh International Conference of
Machine Learning, pp. 217{225, New Brunswick, New Jersey. Morgan Kaufmann.
Provost, F. J., & Fawcett, T. (2001). Robust classication for imprecise environments.
Machine Learning, 42 (3), 203{231.
Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley & Sons, New York.
Qi, R. (1994). Decision Graphs: Algorithms and Applications to Inuence Diagram Evaluation and High-Level Path Planning Under Uncertainty. Ph.D. thesis, University of
British Columbia.
Quinlan, J. R. (1993). C4.5: Programs for Machine Learning. Morgan Kaufmann, San
Mateo, California.
Raia, H. (1968). Decision Analysis. Adison-Wesley, Reading, MA.
Sutton, R. S., & Barto, A. (1999). Reinforcement Learning: An Introduction. MIT Press,
Cambrdige, Massachusetts.
Tan, M. (1993). Cost-sensitive learning of classication knowledge and its applications in
robotics. Machine Learning, 13(1), 1{33.
Turney, P. (2000). Types of cost in inductive concept learning. In Workshop on CostSensitive Learning at ICML2000, pp. 15{21, Stanford University, California.
Turney, P. D. (1995). Cost-sensitive classication: Empirical evaluation of a hybrid genetic
decision tree induction algorithm. Journal of Articial Intelligence Research, 2, 369{
409.
302

Learning Diagnostic Policies from Examples

Utgo, P. E. (1989). Incremental induction of decision trees. Machine Learning, 4, 161{186.
van der Gaag, L., & Wessels, M. (1993). Selective evidence gathering for diagnostic belief
networks. AISB Quarterly, 86, 23{34.
Washington, R. (1997). BI-POMDP: Bounded, incremental partially-observable Markovmodel planning. In Proceedings of the Fourth European Conference on Planning.
Zadrozny, B., & Elkan, C. (2001). Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the Seventh International Conference on
Knowledge Discovery and Data Mining, pp. 204{213. ACM Press.

303

Journal of Artificial Intelligence Research 24 (2005) 109–156

Submitted 10/04; published 7/05

Solving Set Constraint Satisfaction Problems using ROBDDs
Peter Hawkins
Vitaly Lagoon

hawkinsp@cs.mu.oz.au
lagoon@cs.mu.oz.au

Department of Computer Science and Software Engineering
The University of Melbourne, VIC 3010, Australia

Peter J. Stuckey

pjs@cs.mu.oz.au
NICTA Victoria Laboratory, Department of Computer Science and Software Engineering
The University of Melbourne, VIC 3010, Australia

Abstract
In this paper we present a new approach to modeling finite set domain constraint problems using Reduced Ordered Binary Decision Diagrams (ROBDDs). We show that it is
possible to construct an efficient set domain propagator which compactly represents many
set domains and set constraints using ROBDDs. We demonstrate that the ROBDD-based
approach provides unprecedented flexibility in modeling constraint satisfaction problems,
leading to performance improvements. We also show that the ROBDD-based modeling
approach can be extended to the modeling of integer and multiset constraint problems in
a straightforward manner. Since domain propagation is not always practical, we also show
how to incorporate less strict consistency notions into the ROBDD framework, such as set
bounds, cardinality bounds and lexicographic bounds consistency. Finally, we present experimental results that demonstrate the ROBDD-based solver performs better than various
more conventional constraint solvers on several standard set constraint problems.

1. Introduction
It is often natural to express a constraint satisfaction problem (CSP) using finite domain
variables and relations between those variables, where the values for each variable are drawn
from a finite universe of possible values. One of the most common methods of solving finite
domain CSPs is through maintaining and updating a domain for each variable using the
combination of a backtracking search and an incomplete local propagation algorithm. The
local propagation algorithm attempts to enforce consistency on the values in the variable
domains by removing values that cannot form part of a solution to the system of constraints.
Various levels of consistency can be defined, each with associated costs and benefits. Most
consistency algorithms are incomplete—that is, they are incapable of solving the problem
by themselves, so they must be combined with a backtracking search procedure to produce
a complete constraint solver.
When attempting to apply this general scheme to the task of solving constraint satisfaction problems over finite set variables we quickly run into practical problems. Since the
universe of possible values for a set variable is usually very large, the naı̈ve representation of
the domain of a set variable as a set of sets is too unwieldy to solve realistic problems. For
example, if a set variable can take on the value of any subset of the set {1, . . . , N }, then its
domain contains 2N elements, which quickly becomes infeasible to represent as N increases
c
°2005
AI Access Foundation. All rights reserved.

Hawkins, Lagoon, & Stuckey

in magnitude. Accordingly, most set constraint solvers to date have used an approximation
to the true domain of a set variable in order to avoid this combinatorial explosion.
The most common approximation to the true domain of set variable v has been to
maintain an upper bound U and a lower bound L of the domain under the subset partial
ordering relation and to perform set bounds propagation on these bounds. That is, L
contains elements that must be in the set v, and U is the complement of the set of elements
that must not be in v. Conventionally, a fixed set of inference rules specific to each constraint
are used to enforce consistency on these upper and lower bounds. This basic scheme was
proposed by Puget (1992), and has been implemented by set solvers such as Conjunto
(Gervet, 1997), the fd sets and ic sets libraries of ECLi PSe (IC-PARC, 2003), Mozart
(Müller, 2001), and ILOG Solver (ILOG, 2004).
Set bounds are a crude approximation to a set domain at best, and thus various refinements to the bounds propagation scheme have been proposed so as to more effectively
capture the nature of a set domain. Azevedo (2002) demonstrated that maintaining and
performing inferences upon the upper and lower bounds on the cardinality of a set domain
leads to a significant performance improvement on a variety of problems. While earlier
set solvers such as Conjunto also maintained cardinality bounds, only partial usage was
made of this information. More recently, Sadler and Gervet (2004) showed that incorporating upper and lower bounds under a lexicographic ordering leads to significantly stronger
propagation, albeit at the cost of a marked increase in propagation time, leading to only a
marginal performance improvement overall. While both of these approaches provide more
effective propagation than the simple set bounds scheme, they do not approach the effectiveness of a true set domain propagator, which ensures that every value in the domain
of a set variable can be extended to a complete assignment of every variable in any given
constraint.
Consequently, we would like to devise a representation for set domains and constraints
on those domains that is tractable enough to permit domain propagation. The principal
observation that permits the implementation of a set domain propagator is that a finite
integer set v can be represented by its characteristic function χv :
χv : Z → {0, 1} where χv (i) = 1 iff i ∈ v
Accordingly we can use a set of Boolean variables vi to represent the set v, which correspond
to the propositions vi ↔ i ∈ v. We can describe set domains and set constraints in terms
of these Boolean variables. Interestingly, set bounds propagation as described above is
equivalent to performing domain propagation in a naı̈ve way on this Boolean representation.
In this paper we investigate this Boolean modeling approach for modeling finite domain
constraints, and in particular set constraints. We show that it is possible to represent the
domains of set variables using Reduced Ordered Binary Decision Diagrams (ROBDDs), a
data structure for representing and manipulating Boolean formulæ. Such representations
are usually fairly compact, even if they correspond to very large domains. In addition, it
is possible to represent the constraints themselves as ROBDDs, permitting us to produce
efficient set domain constraint propagators solely using ROBDD operations.
The ROBDD-based representation allows us to easily conjoin constraints and existentially quantify variables, thus permitting us to remove intermediate variables and merge
constraints for stronger propagation. The construction of global constraints becomes an
110

Solving Set Constraint Satisfaction Problems using ROBDDs

almost trivial exercise, without the requirement to write laboriously new propagators for
every new constraint that we would like to use.
We also demonstrate that only minor changes are needed to the operation of the set
domain propagator in order to implement other, less strict notions of consistency. In particular, we show how to construct set bounds, cardinality bounds and lexicographic bounds
propagators, and how to utilize these notions to produce a split domain solver that combines
bounds and domain reasoning.
A key theme of this paper is the flexibility of the ROBDD-based modeling approach.
We are not limited only to modeling set variables—for example ROBDDs can be used
to model integer variables and integer constraints. While the ROBDD-based approach
to modeling finite domain integer variables is in general not as efficient as many existing
finite domain integer solvers, the ability to model integer constraints using ROBDDs is an
essential building block which allows us to construct set constraints such as cardinality and
weighted sum, as well as allowing us to represent multisets and multiset constraints.
Finally, we present experiments using a variety of standard set problems to demonstrate
the advantages of our modeling approach.
Many of the ideas from this paper have been previously published in two previous works
(Lagoon & Stuckey, 2004; Hawkins, Lagoon, & Stuckey, 2004). This paper contains a more
complete exposition of those ideas, as well as important extensions of the work that has
previously been presented. These include substantial improvements in the complexity of the
propagation algorithm, as well as cardinality and lexicographic bounds solvers implemented
using ROBDDs. In addition, we show how to model integer expressions, allowing us to
implement a weighted-sum constraint, and to model multisets and multiset constraints.
Using this, we present new experimental results for the Hamming Code and Balanced
Academic Curriculum problems. Finally, we present results comparing the ROBDD-based
solver against a solver with good cardinality reasoning (Mozart).
The remainder of this paper is structured as follows. Section 2 contains essential concepts
and definitions necessary when discussing finite domain solvers and ROBDDs. Section 3
shows how to model set domains and set constraints using ROBDDs, and presents a basic
outline of an ROBDD-based set constraint solver. Section 4 demonstrates how to improve
the performance of the ROBDD-based set solver through the construction of global constraints, the removal of intermediate variables, and through symmetry-breaking approaches.
Section 5 demonstrates how to model integer and multiset expressions, as well as how to
implement a weighted-sum constraint for set and multiset variables. Section 6 shows how
to construct a more efficient domain propagator, as well as set bounds, set cardinality, and
lexicographic bounds propagators. Finally, in Section 7 we present experimental results
comparing the ROBDD-based solver with more conventional set solvers on a variety of
standard problems.

2. Preliminaries
In this section we define the concepts and notation necessary when discussing propagationbased constraint solvers. These definitions are largely identical to those presented by Lagoon
and Stuckey (2004) and others. For simplicity we shall present all of our definitions in the
case of finite set variables; the extensions to multiset and integer variables are trivial.
111

Hawkins, Lagoon, & Stuckey

2.1 Lattices, Domains, and Valuations
Let L be the powerset lattice hP(U), ⊂i, where P(x) denotes the powerset of x and the
universe U is a finite subset of Z. We say a subset M ⊆ L is convex if for any a, c ∈ M
the relation a ⊆ b ⊆ c implies b ∈ M for all b ∈ L. The interval [a, b] is the set M = {x ∈
L | a ⊆ x ⊆ b}. Intervals are obviously convex. Given a subset K ⊆ L, we define the convex
closure of K:
"
#
\
[
conv (K) =
x,
x
x∈K

x∈K

The convex closure operation satisfies the properties of extension (x ⊆ conv (x)), idempotence (conv (x) = conv (conv (x))), and monotonicity (if x ⊆ y then conv (x) ⊆ (y)) (Gervet,
1997).

Example 2.1. The set X = {{1}, {1, 3}, {1, 4}, {1, 3, 4}} is convex and equivalent to the interval [{1}, {1, 3, 4}]. Conversely, the set Y = {{1}, {1, 3}, {1, 3, 4}} is not convex. However,
the convex closure of Y is precisely the interval X, i.e. conv (Y ) = X.
Let V denote the fixed finite collection of all set variables. Each variable has a domain,
which is a finite collection of possible values from L (which are themselves sets). More
generally, we define a domain D to be a complete mapping from the set V to finite collections
of finite sets of integers. When we speak of the domain of a variable v, we mean D(v). A
domain D1 is said to be stronger than a domain D2 , written D1 v D2 , if D1 (v) ⊆ D2 (v)
for all v ∈ V. Two domains are said to be equal, written D1 = D2 , if D1 (v) = D2 (v)
for all v ∈ V. We call a domain D a range domain if D(v) is an interval for all v ∈ V.
We extend the concept of convex closure to domains by defining ran(D) to be the unique
(range) domain such that ran(D)(v) = conv (D(v)) for all v ∈ V.
A valuation is a function from V to L, which we write using the mapping notation
{v1 7→ d1 , v2 7→ d2 , . . . , vn 7→ dn }, where vi ∈ V and di ∈ L for all i ∈ N. Clearly a valuation
can be extended to constraints involving the variables in the obvious way. We define vars to
be the function that returns the set of variables that are involved in a constraint, expression
or valuation. In an abuse of notation, a valuation θ is said to be an element of a domain
D, written θ ∈ D, if θ(v) ∈ D(v) for all v ∈ vars(θ).
We say a domain D is a singleton or valuation domain if |D(v)| = 1 for all v ∈ V. In
this case D corresponds to a single valuation θ where D(v) = {θ(v)} for all v ∈ V.
2.2 Constraints, Propagators, and Propagation Solvers
A constraint is a restriction placed upon the allowable values for a collection of variables.
We are interested in the following primitive set constraints, where k is an integer, d is a
ground set value, and u, v and w are set variables: k ∈ v (membership), k ∈
/ v (nonmembership), u = v (equality), u = d (constant equality), u ⊆ v (non-strict subset),
u = v ∪ w (union), u = v ∩ w (intersection), u = v \ w (set difference), u = v (complement),
u 6= v (disequality), |u| = k (cardinality), |u| ≥ k (lower cardinality bound), and |u| ≤ k
(upper cardinality bound). Later we shall introduce non-primitive set constraints which are
formed by composing primitive set constraints.
We define the solutions of a constraint c to be the set of valuations that make that
constraint true, i.e. solns(c) = {θ | vars(θ) = vars(c) and ² θ(c)}.
112

Solving Set Constraint Satisfaction Problems using ROBDDs

Example 2.2. Suppose v and w are set variables, and D is a domain with D(v) =
{{1}, {1, 3}, {2, 3}}, and D(w) = {{2}, {1, 2}, {1, 3}}. Let c be the constraint v ⊆ w.
Then the solutions to c in the domain D are the valuations {v 7→ {1}, w 7→ {1, 2}},
{v 7→ {1}, w 7→ {1, 3}}, and {v 7→ {1, 3}, w 7→ {1, 3}}.
With every constraint we associate a propagator f , which is a monotonic decreasing
function from domains to domains, so if D1 v D2 then f (D1 ) v f (D2 ) and f (D1 ) v D1 . A
propagator f is said to be correct for a constraint c if:
{θ | θ ∈ D} ∩ solns(c) = {θ | θ ∈ f (D)} ∩ solns(c)
Correctness is not a strong restriction, since the identity propagator is correct for all constraints c. We usually assume that all propagators are both correct and checking, that is, if
D is a singleton domain formed through propagation then its associated valuation θ makes
θ(c) true.
We can use propagators to form the basis for a constraint solver. A propagation solver
solv (F, D) takes a set of propagators F and a current domain D, and repeatedly applies
propagators from F to the current domain until a fixpoint is reached. In other words
solv (F, D) is the weakest domain D 0 v D which is a fixpoint (i.e. f (D 0 ) = D0 ) for all
f ∈ F . This fixpoint is unique (Apt, 1999).
2.3 Local Consistency
The notion of local consistency is of importance when considering the solution of constraint
satisfaction problems. We can define various levels of consistency of different strengths and
levels of difficulty to enforce. We describe two here.
A domain D is said to be domain consistent for a constraint c if D is the strongest
domain which contains all solutions θ ∈ D of c; in other words there does not exist a
domain D 0 v D such that θ ∈ D and θ ∈ solns(c) implies θ ∈ D 0 . A set of propagators
maintains domain consistency if solv (F, D) is domain consistent for all constraints c.
Definition 1. A domain propagator for a constraint c is a function dom(c) mapping domains to domains which satisfies the following identity:
(
{θ(v) | θ ∈ D ∧ θ ∈ solns(c)} if v ∈ vars(c)
dom(c)(D)(v) =
D(v)
otherwise
Lemma 2.1. A domain propagator dom(c) for a constraint c is correct, checking, monotonic, and idempotent.
Proof. Straightforward from the definitions.
Example 2.3. Consider the constraint v ⊆ w and domain D of Example 2.2. The domain
propagation D 0 = dom(c)(D) returns domain D 0 where D 0 (v) = {{1}, {1, 3}} and D 0 (w) =
{{1, 2}, {1, 3}}. The missing values are those that did not take part in any solution.
Domain consistency may be difficult to achieve for set constraints, so instead we often
need a weaker notion of consistency. The notion of set bounds consistency is commonly
113

Hawkins, Lagoon, & Stuckey

used. A domain D is bounds consistent for a constraint c if for every variable v ∈ vars(c)
the upper bound of D(v) is the union of the values of v in all solutions of c in D, and the
lower bound of D(v) is the intersection of the values of v in all solutions of c in D. As
above, a set of propagators F is said to maintain set bounds consistency for a constraint c
if solv (F, D) is bounds consistent for all domains D.
Definition 2. A set bounds propagator for a constraint c is a function sb(c) mapping
domains to domains satisfying the following identity:
(
conv (dom(c)(ran(D))(v)) if v ∈ vars(c)
sb(c)(D)(v) =
D(v)
otherwise
Lemma 2.2. A set bounds propagator sb(c) for a constraint c is correct, checking, and
idempotent. A propagator sb(c) is also monotonic on range domains.
Proof. All of these follow trivially from the properties of dom(c) and from the extension
and idempotence properties of conv and ran.
Clearly for any constraint c and any v ∈ vars(c) we have dom(c)(D)(v) ⊆ sb(c)(D)(v)
for all domain propagators dom(c) and all set bounds propagators sb(c).
2.4 Boolean Formulæ and Binary Decision Diagrams
We use Boolean formulæ extensively to model sets and set relations. In particular, we make
use of the following Boolean operations: ∧ (conjunction), ∨ (disjunction), ¬ (negation), →
(implication), ↔ (bi-directional implication), ⊕ (exclusive OR), and ∃ (existential quantification). We use the shorthand ∃V F for ∃x1 · · · ∃xn F where V = {x1 , . . . , xn }, and we use
∃V F to mean ∃V 0 F where V 0 = vars(F ) \ V .
Binary Decision Trees (BDTs) are a well-known method of modeling Boolean functions
on Boolean variables. A Binary Decision Tree is a complete binary tree, in which each
internal node n(v, t, f ) is labelled with a Boolean variable v and each leaf node is labelled
with a truth value 0 (false) or 1 (true). Each internal node corresponds to an if-then-else
test of the labelled variable, and has two outgoing arcs—the “false” arc (to BDT f ) and
the “true” arc (to BDT t). In order to evaluate the function represented by the binary tree,
the tree is traversed from the root to a leaf. On reaching any internal node, the value of
the Boolean variable whose label appears on the node is examined, and the corresponding
arc is followed. The traversal stops upon reaching a leaf node, whereupon the value of the
function is taken to be the label of that node.
A Binary Decision Diagram (BDD) is a variant of a Binary Decision Tree, formed by
relaxing the tree structure requirement, instead representing functions as directed acyclic
graphs. In a Binary Decision Diagram, a node is permitted to have multiple parents, as
opposed to a tree structure which requires any node to have at most one parent. This
effectively permits common portions of the tree to be shared between multiple branches,
allowing a compact representation of many functions.
The primary disadvantage of both Binary Decision Trees and Binary Decision Diagrams
is that the representation of a given function is not canonical (i.e. one function may have
many representations). Two additional canonicity properties allow many operations on
114

Solving Set Constraint Satisfaction Problems using ROBDDs

0123
7654
0123
7654
0123
7654
v E
v RRR
v RRR
EE
RRRR
ll
RR)
ll
l
u
l
"0123
0123
7654
0123
7654
l
(0123
v FF
v FF
0123
7654
7654
7654
v Ev
v E
v
¢
|
"
|
"
E
E
y
y
y
E"
E"
0123
7654
0123
7654
0123
7654
0123
7654
°
y
y
y
|
|
|
v
v
v
v FF
FF
FF
FF
0123
7654
0123
7654
0123
7654
0123
7654
0123
7654
v
v
v
v
v
"0123
"
"
"
E
EE
º
2
E
y
y
y
7654
0123
7654
0123
7654
0123
7654
22
¯ E" y|
v
v
v
v
' E" y|
Ã
y|
22
0123
7654
0123
7654
0123
7654
|
|
|
|
v
v
v
¯
DD
0123
7654
0123
7654
0123
7654
0123
7654
/
(¥ EEE
2
v
v
v
v
z
"
< 5 DDDD ¼ ¦¯ z z
Â /
|
|
|
|
0123
7654
v D
0123
7654
0123
7654
0123
7654
0123
7654
5
i
6
h
M
0
v
v
v
v
R
D
FF
FF
FF
FF
: ¼
Y b eiDD
z
z DD
"
"
"
"
"
! }z
# rt|z
0123
7654
0123
7654
0123
7654
0123
7654
v
v
v
v FF
F
F
F
0
1
1
F"
F"
F"
"
0123
7654
7654
0123
7654
7654
v FF 0123
v
v FF 0123
v
"0123
|
"
|
7654
0123
7654
v RRR
v
RR
l l
w

3

1

2

4

5

1

8

2

2

8

8

3

8

2

3

4

6

9

9

7

6

7

3

4

5

5

6

3

4

5

5

6

6

7

8

7

7

8

8

9

(b)

8

9

R(

(a)

4

1

vl

(c)

Figure 1: ROBDDs for (a) LU = v3 ∧ ¬v4 ∧ ¬v5 ∧ v6 ∧ v7 (b) R = ¬(v1 ↔ v9 ) ∧ ¬(v2 ↔ v8 )
and (c) LU ∧ R (omitting the node 0 and arcs to it). Solid arcs are “then” arcs,
dashed arcs are “else” arcs.

BDDs to be performed efficiently (Bryant, 1986, 1992). A BDD is said to be reduced if it
contains no identical nodes (that is, there are no nodes with the same label and identical
“then” and “else” arcs), and no redundant tests (there are no nodes with both “then” and
“else” arcs to the same node). A BDD is said to be ordered if there is a total order ≺
of the variables, such that if there is an arc from a node labelled v1 to a node labelled v2
then v1 ≺ v2 . A Reduced Ordered BDD (ROBDD) is canonical function representation up to
reordering, which permits an efficient implementation of many Boolean function operations.
For more details the reader is referred to the work of Bryant (1992), or the introduction of
Andersen (1998).
We define the size |R| to be the number of non-leaf nodes of the ROBDD R, as well as
defining VAR(R) to be the set of ROBDD variables that appear as labels on the internal
nodes of R. We shall be interested in stick ROBDDs, in which every internal node n(v, t, f )
has exactly one of t or f as the constant 0 node.
Example 2.4. Figure 1(a) gives an example of a stick ROBDD LU representing the formula
v3 ∧ ¬v4 ∧ ¬v5 ∧ v6 ∧ v7 . |LU | = 5 and V AR(LU ) = {v3 , v4 , v5 , v6 , v7 }. Figure 1(b) gives
an example of a more complex ROBDD representing the formula ¬(v1 ↔ v9 ) ∧ ¬(v2 ↔ v8 ).
|R| = 9 and V AR(R) = {v1 , v2 , v8 , v9 }. One can verify that the valuation {v1 7→ 1, v2 7→
0, v8 7→ 1, v9 7→ 0} makes the formula true by following the path right, left, right, left from
the root.
2.5 ROBDD Operations
There are efficient algorithms for many Boolean operations applied to ROBDDs. The
complexity of the basic operations for constructing new ROBDDs is O(|R 1 ||R2 |) for R1 ∧R2 ,
115

Hawkins, Lagoon, & Stuckey

node(v, t, f ) if (t = f ) return t else return n(v, t, f )
and(R1 , R2 )
if (R1 = 0 or R2 = 0) return 0
if (R1 = 1) return R2
if (R2 = 1) return R1
n(v1 , t1 , f1 ) := R1
n(v2 , t2 , f2 ) := R2
if (v1 ¹ v2 ) return node(v1 ,and(t1 , R2 ),and(f1 , R2 ))
else if (v2 ¹ v1 ) return node(v2 ,and(t2 , R1 ),and(f2 , R1 ))
else return node(v1 ,and(t1 , t2 ),and(f1 , f2 ))
exists(v, R)
if (R = 0) return 0
if (R = 1) return 1
n(vr , t, f ) := R
if (vr ¹ v) return node(vr ,exists(v, t),exists(v, f ))
else if (v ¹ vr ) return R
else return or(t, f )
Figure 2: Example ROBDD operations
R1 ∨ R2 and R1 ↔ R2 , O(|R|) for ¬R, and O(|R|2 ) for ∃v R. Note however that we can test
whether two ROBDDs are identical, whether R1 ↔ R2 is equivalent to true (1), in O(1).
We give code for conjunction R1 ∧ R2 = and(R1 , R2 ) and existential quantification (of
one variable) ∃v R = exists(v, R). The code for disjunction R1 ∨ R2 = or(R1 , R2 ) is dual
to and and is very similar in structure. The code make use of the auxiliary function node
which builds a new ROBDD node. The node function returns t if t = f , and is in practice is
memoed so any call to node with the same arguments as a previous call returns a reference
to the previously created ROBDD node.
Modern BDD packages provide many more operations, including specialized implementations of some operations for improved speed. Some important operations for our purposes
are existential quantification of multiple variables V for a formula R ∃V R, and the combination of conjunction and existential quantification ∃V R1 ∧ R2 .
Although in theory the number of nodes in an ROBDDs can be exponential in the
number of variables in the represented Boolean function, in practice ROBDDs are often
very compact and computationally efficient. This is due to the fact that ROBDDs exploit
to a very high-degree the symmetry of models of a Boolean formula.

3. Modeling Set CSPs Using ROBDDs
In this section we discuss how to solve set constraint satisfaction problems using ROBDDs.
There are three parts to this—the modeling of set domains as ROBDDs, the modeling of
set constraints as ROBDDs, and how to use both to produce a set solver.
116

Solving Set Constraint Satisfaction Problems using ROBDDs

z}
0123
7654
v D
DD

0123
7654
v PP
PP

zz

1

0123
7654

PPP
P'
v2
±
DD
±
!
±
»
v3 @
"
~ @@@ ±
Ã ¨±
+
Â£
1
7 <
H @Â
U - o
2

0123
7654
0

(a)

0123
7654
x E
EE
1

0123
7654

0123
7654

s
{ 1 CCCC
{
C!
}{
s2 C
s
C
{
{ 2 CCCC
CC
{
{
C
C!
! }{
}{
s3 C
s3 C
s
{ CCC
{ CCC
{ 3
{
{
{
C
C
! }{
}{
! }{
s4 C
s4
s4 C
C
C
{
{
C
C
¸
CC
CC
{
{
! }{
! }{
º
s5 C
s
CC
{ 5
Á
³
C! }{
(
º
1
7
1
L
W U^ \` /0 pqt
0

0123
7654

EE
"
x
°
z 2
z
¹
|z
Â
x3 E
( £ EEE
E"
1¨
x
z 4AAA
½ :
z
AÃ
L Y .Â q|z
0
1
¥

0123
7654

0123
7654

(b)

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

(c)

Figure 3: ROBDDs for (a) {{1}, {1, 3}, {2, 3}}, (b) [{1, 3, 4, 5}, {1, 3, 4, 5, 6, . . . N }] and (c)
{s ⊆ {1, 2, 3, 4, 5} | |s| = 2}.

3.1 Modeling Set Domains using ROBDDs
Suppose the universe U = {1, . . . , N }. We assume that all set values are subsets of U.
Let x be a set variable, and let D be a domain over V. Given that the size of the universe is bounded, we can associate a Boolean variable xi with each potential element
i ∈ {1, . . . , N } of x. Hence a set variable x is represented by a vector of Boolean variables V (x) = hx1 , . . . , xN i.
Take any set A ∈ D(x). Then we can represent A as a valuation on the variables
hx1 , . . . , xN i defined as θA = {x1 7→ (1 ∈ A), . . . , xn 7→ (n ∈ A)}. We can represent
this valuation θA and consequently the set A as a Boolean formula B(A) which has this
valuation as its unique solution:
(
^
xi
if i ∈ A
B(A) =
yi where yi =
¬xi otherwise
i∈U
Given that we can represent any element of D(x) as a valuation, we can represent
D(x) itself as a Boolean formula B(D(x)) whose solutions θA correspond to the elements
A ∈ D(x). That is, B(D(x)) is the disjunction of B(A) over all possible sets A ∈ D(x):
_
B(D(x)) =
B(A) where B(A) is defined above
A∈D(x)

Any solution θ(x) ∈ D(x) then corresponds to a satisfying assignment of the Boolean
formula B(D(x)). From now on we will overload the notion of a domain D to equivalently
return a set of possible sets D(x) for a variable x, or its equivalent Boolean representation
B(D(x)).
Example 3.1. Let U = {1, 2, 3}. Suppose v is a set variable with D(v) = {{1}, {1, 3}, {2, 3}}.
We associate Boolean variables {v1 , v2 , v3 } with v. We can then represent D(v) as the
117

Hawkins, Lagoon, & Stuckey

Boolean formula (v1 ∧ ¬v2 ∧ ¬v3 ) ∨ (v1 ∧ ¬v2 ∧ v3 ) ∨ (¬v1 ∧ v2 ∧ v3 ). The three solutions to
this formula correspond to the elements of D(v). The corresponding ROBDD B(D(v)) is
shown in Figure 3(a).
This representation is useful since such Boolean formulæ can be directly represented as
reasonably compact ROBDDs. Given a Boolean formula representing a domain, we can
clearly construct the corresponding ROBDD in a bottom-up fashion, but in practice we
only ever construct the ROBDD for a domain implicitly through constraint propagation.
As we will show, ROBDDs permit a surprisingly compact representation of (many) subsets
of P(U).
Since ROBDDs are ordered, we must specify a variable ordering for the Boolean variables
that we use. We arbitrarily order the ROBDD variables for a single set variable x as
follows: x1 ≺ x2 ≺ . . . ≺ xn . Assuming there is no special relationship between the
elements of the universe, the choice of the ordering of ROBDD variables that represent a
particular set variable is unimportant. However, the relative ordering of ROBDD variables
comprising different set variables has a drastic effect on the size of the ROBDDs representing
constraints, and is discussed in Section 3.2.
Example 3.2. An ROBDD representing the 2N −5 sets in the interval [{1, 3, 4, 5}, {1, 3, 4, 5, 6, . . . N }]
is shown in Figure 3(b).
The ROBDD representation is flexible enough to be able to represent compactly a wide
variety of domains, even those that might at first seem difficult to represent. For example,
the set of all subsets of {1, 2, 3, 4, 5} of cardinality 2 can be represented by the ROBDD
shown in Figure 3(c).
For convenience, we set all of the initial variable domains Dinit (x) = P(U), which
corresponds to the constant “1” ROBDD. Restrictions on the initial bounds of a domain
can instead be expressed as unary constraints.
3.2 Modeling Primitive Set Constraints Using ROBDDs
A major benefit of using ROBDDs to model set constraint problems is that ROBDDs can
be used to model the constraints themselves, not just the set domains. Any set constraint
c can be converted to a Boolean formula B(c) on the Boolean variables comprising the set
variables vars(c), such that B(c) is satisfied if and only if the corresponding set variable
valuations satisfy c. As usual, we represent B(c) as an ROBDD.
Example 3.3. Let v and w be set variables over the universe U = {1, 2, 3}, and let c
denote the constraint v ⊆ w. Assume that the Boolean variables associated with v and
w are hv1 , v2 , v3 i and hw1 , w2 , w3 i respectively. Then c can be represented by the Boolean
formula (v1 → w1 ) ∧ (v2 → w2 ) ∧ (v3 → w3 ). This formula can in turn be represented by
either of the ROBDDs shown in Figure 4 (depending on the variable order).
Example 3.4 demonstrates that the order of the variables within the ROBDDs has a
very great effect on the size of the formula representations.
Example 3.4. Consider again the constraint v ⊆ w as described in Example 3.3. Figure 4
shows the effect of two different variable orderings on the size of the resulting BDD. In this
118

Solving Set Constraint Satisfaction Problems using ROBDDs

0123
7654
v
z
z
²
z} 0123
0123
7654
7654
v
vÂ
2

3

0123
7654

v
y 1 FFFF
F"
y|
y

3

Â

#

$
)

;
/
7
?
G

1

0123
7654
v F
F
F"
²
0123
7654
0123
7654
v
v F
x
F
x ²
F"
²
|
x
0123
7654
0123
7654
0123
7654
0123
7654
w
w
w
w
1

-

0123
7654
v EE
E
%

+

2
3

3

1

1

1

0123
7654
0123
7654
w
w
² }
+ 0123
7654
w
x

! ²

2

' ²

xx
% ² |xs x
1

2
3

(a) Order: v1 ≺ v2 ≺ v3 ≺ w1 ≺ w2 ≺ w3

0123
7654
wE
EE

EE
"

5

1

0123
7654

E
Eº
Q Z ` E0 " v
2 EE
% EEE
¿
"
+
!
w2 E
5
Â EEE
'
E
Q ¾ Z ` E0 " v
3 EE
º
EEE
¸
µ
"
4
¿
w3
°
2 l n
;
¨
Â ¡Ä b d f h j R Y - }
0p
1

0123
7654

0123
7654

0123
7654

(b) Order: v1 ≺ w1 ≺ v2 ≺ w2 ≺ v3 ≺ w3

(Arcs to ‘0’ are not shown for clarity)

Figure 4: Two ROBDDs for v ⊆ w

case, the variable ordering v1 ≺ w1 ≺ v2 ≺ w2 gives a much more compact representation
of constraint than the ordering v1 ≺ v2 ≺ w1 ≺ w2 . As can be seen from the figure, the
latter ordering has size exponential in n, whereas the former has size linear in n.

If the variable set V = {v1 , . . . , vm } has corresponding Boolean variables hv1,1 , v1,2 , . . . , v1,N i,
hv2,1 , v2,2 , . . . , v2,N i, . . . , hvm,1 , vm,2 , . . . , vm,N i, we choose to order the Boolean variables
v1,1 ≺ · · · ≺ vm,1 ≺ v1,2 ≺ · · · ≺ vm,2 ≺ · · · ≺ v1,N ≺ · · · ≺ vm,N . This ordering guarantees a linear representation for all of the primitive set constraints except cardinality. The
reason is that primitive set constraints except cardinality are defined elementwise, that is
an element i ∈ v never constrains whether j ∈ w or j ∈
/ w for i 6= j. For this reason
there are no interactions between bits vi and wj , i 6= j. Placing all the bits for the same
element adjacent in the order means that the interactions of bits vi and wi are captured in
a small ROBDD, which is effectively separate from the ROBDD describing the interactions
of the next element’s bits. Table 1 contains a list of the primitive set constraints, their
corresponding Boolean formulæ and the sizes of the ROBDDs representing those formulæ
under this point-wise ordering.
Cardinality constraints can be represented in a quadratic number of ROBDD nodes
using a simple recursive definition. We define card (hvi1 , . . . , vin i , l, u) to be the Boolean
formula which restricts the number of true bits in the vector hvi1 , . . . , vin i to between l and
119

Hawkins, Lagoon, & Stuckey

c
Boolean expression B(c)
k∈v
vk
k 6∈ v
¬v
V
Vk
u ∧ 1≤i≤N,i6∈d ¬ui
u=d
Vi∈d i
u=v
(u ↔ vi )
V1≤i≤N i
u⊆v
(u → vi )
V1≤i≤N i
u=v∪w
(u ↔ (vi ∨ wi ))
V1≤i≤N i
(u ↔ (vi ∧ wi ))
u=v∩w
V1≤i≤N i
(u ↔ (vi ∧ ¬wi ))
u=v−w
V1≤i≤N i
¬(ui ↔ vi )
u=v
W1≤i≤N
u 6= v
1≤i≤N ¬(ui ↔ vi )
|u| = k
card (V (u), k, k))
|u| ≥ k
card (V (u), k, N )
card (V (u), 0, k)
|u| ≤ k

size of ROBDD
O(1)
O(1)
O(N )
O(N )
O(N )
O(N )
O(N )
O(N )
O(N )
O(N )
O(k(N − k))
O(k(N − k))
O(k(N − k))

Table 1: Boolean representation of set constraints and the size of the corresponding
ROBDD.

u inclusive.


1



0
card (hvi1 , . . . , vin i , l, u) =

(¬vi1 ∧ card (hvi2 , . . . , vin i , l, u)) ∨



(v ∧ card (hv , . . . , v i , l − 1, u − 1)
i1
i2
in

if l ≤ 0 ∧ n ≤ u
if n < l ∨ u < 0
otherwise

It is clear from the structure of card (hvi1 , . . . , vin i , l, u) that the resulting ROBDD is O(n2 )
in size. A more general method of characterising cardinality constraints will be presented
in Section 5.5.
3.3 A Basic Set Constraint Solver
We now show how to construct a simple set domain propagator dom(c) for a constraint c.
If vars(c) = {v1 , . . . , vn }, then we define a function dom(c) mapping domains to domains
as follows:
(
V
∃V (vi ) B(c) ∧ ni=1 D(vi ) if vi ∈ vars(c)
dom(c)(D)(vi ) =
D(vi )
otherwise
In other words, to perform propagation we take the conjunction of the Boolean representations of the current domains of the variables in vars(c) with the Boolean representation
of the constraint B(c) and project the result onto the Boolean variables V (v i ) representing
each variable vi . Since B(c) and all D(vi ) are ROBDDs, this formula can be implemented
directly using ROBDD operations.
120

Solving Set Constraint Satisfaction Problems using ROBDDs

0123
7654

w
x 1FFFF
x
F"
|x
w2 WWWWW
w
x
WWWxWW 2FFFF
¸
|x WWWWWF+ "
%
w
w3 F
FF
x 3
1
F
x
F" |x
= ½H
H
L 1
O G"
W - p

0123
7654

0123
7654

0123
7654

0

0123
7654
v FF
F
1

0123
7654
½

½

½

½

½

½

½

½

½

0123
7654
wE
EE

FF
"

µ

µ

µ

µ

µ

µ

1

µ

|y
0123
7654
wE
® EE

0123
7654 0123
7654
µ

®

1

0123
7654

D
- D!
v2
B{ @@@@
}{
U Z Ã.
1
0

0123
7654

EE
"
v2
y
y

2

®

0123
7654
v D
' DD

0123
7654

EE
"
v3
y
y

0123
7654
w F
FF
1

~

0123
7654

FF
"
w2 F
¥
FFF
x
¯
F"
|x x
w3 F
w
$
x 3
| FFF
2½
x
F" |x
~|
0c
1
Ä

0123
7654

0123
7654

½ µ ¦®
|y
½ µ w3 I w36
I
v
­
I
v
½ µ
vv III 6
II 6
½ µ ­ ­vvvv
II 6
I$ ½
¯½ ©µ¦­{vvv
p
n
0
1

(a)

(b)

(c)

(d)

Figure 5: ROBDDs used in the domain propagation of c ≡ v ⊆ w (a) D(w), (b) B(c) ∧
D(v) ∧ D(w), (c) D 0 (v), and (d) D 0 (w)

Example 3.5. Consider the domain propagation of constraint v ⊆ w with initial domain
D(v) = {{1}, {1, 3}, {2, 3}}, and D(w) = {{2}, {1, 2}, {1, 3}} from Examples 2.2 and 2.3.
We conjoin the ROBDD B(c) shown in Figure 4(b) with the domain ROBDD D(v) shown
in Figure 3(a) and the ROBDD for D(w) shown in Figure 5(a). The result is an ROBDD
representing solutions of the formula v ⊆ w ∧ v ∈ D(v) ∧ w ∈ D(w) shown in Figure 5(b).
We project the resulting onto V (v) and V (w), individually obtaining the ROBDDs D 0 (v) =
{{1}, {1, 3}} and D 0 (w) = {{1, 2}, {1, 3}} shown in Figure 5(c) and (d) respectively.
We need to verify the correctness of the propagator:
Lemma 3.1. Let V be the collection of all set variables, and let c be a set constraint with
vars(c) = {v1 , . . . , vk } ⊆ V. Then dom(c) is a domain propagator for c.
Proof. We need to verify that dom(c) satisfies the identity of Definition 1. Suppose D is a domain
over V. We need V
to check forªeach v ∈ vars(c) that {char (θ(v)) | θ ∈ D ∧ θ ∈ solns(c)} =
©
u | u ² ∃V (vi ) B(c) ∧ ni=1 D(vi ) , where char (X) denotes the characteristic vector of X.
This is clearly true, since the values θ ∈ solns(c) are by definition the satisfying assignments
of B(c), and the values θ ∈ D are by definition the satisfying assignments of ∧ni=1 D(vi ).
Hence the equality holds, implying that dom(c) is a domain propagator.
If F is the set of all such domain propagators, then we can define a complete propagation algorithm solv (F, D): For simplicity we use the set of constraints C rather than the
corresponding propagators.
solv(C, D)
121

Hawkins, Lagoon, & Stuckey

Q := C
while (∃ c ∈ Q)
D0 := dom(c)(D)
if (| vars(c)| = 1) C := C − {c}
V := {v ∈ V | D(v) 6= D 0 (v)}
Q := (Q ∪ {c0 ∈ C | vars(c0 ) ∩ V 6= ∅}) − {c}
D := D0
return D
We maintain a queue Q of constraints to be (re-)propagated, initially all C. We select a
constraint c from the queue to propagate, calculating a new domain D 0 . If the constraint is
unary we remove it from C, never to be considered again, since all information is captured
in the domain. We then determine which variables V have changed domain, and add to
the queue constraints c0 ∈ C involving these variables, with the exception of the current
constraint c.
By combining this algorithm with the modeling techniques described earlier, we have
shown how to construct a simple ROBDD-based set domain propagator. Various improvements to this basic scheme will be discussed in Section 4 and Section 6.
The reader may wonder whether in this section we have done anything more than map set
constraints to Boolean constraints and then apply a Boolean ROBDD solver to them. This
is not the case. Crucially the domain propagation is on the original set variables, not on the
Boolean variables that make them up. In fact the set bounds consistency approach (Puget,
1992; Gervet, 1997) can be considered as simply a mapping of set constraints to Boolean
constraints.

4. Effective Modeling of Set Constraints Using ROBDDs
In this section we demonstrate that the ROBDD-based modeling approach is very flexible,
allowing us to produce highly efficient implementations of many complex constraints.
4.1 Combining Constraints and Removing Intermediate Variables
It is rarely possible to express all of the constraints in a real set problem directly as primitive
set constraints on the variables of the original problem. Instead, we would usually like to
express more complicated set constraints, which can be decomposed into multiple primitive
set constraints, often requiring the introduction of intermediate variables.
Example 4.1. Let c be the constraint |v ∩ w| ≤ k, which requires that v and w have at
most k elements in common. This constraint is used in modeling the problem of finding
Steiner systems (see Section 7.1). Since this is not a primitive set constraint, existing
set solvers would usually implicitly decompose it into two primitive set constraints and
introduce an intermediate variable u. The representation of the constraint then becomes
∃u u = v ∩ w ∧ |u| ≤ k.
In the case of Example 4.1 the decomposition does not affect the strength of the resulting
propagator. To prove this fact, we use two results presented by Choi, Lee, and Stuckey
122

Solving Set Constraint Satisfaction Problems using ROBDDs

(2003). Choi, Lee, and Stuckey prove these results in the case of a finite integer domain
solver, although the proofs in the set domain case are identical. 1
Lemma 4.1 (Choi et al., 2003). Let c1 and c2 be set constraints. Then solv ({dom(c1 ∧
c2 )}, D) v solv ({dom(c1 ), dom(c2 )}, D) for all domains D.
Lemma 4.2 (Choi et al., 2003). Let c1 and c2 be two set constraints sharing at most
one variable x ∈ V. Then solv ({dom(c1 ), dom(c2 )}, D) = solv ({dom(c1 ∧ c2 )}, D) for all
domains D.
Even if the strength of the propagator is unaffected by the decomposition, splitting a
propagator introduces a new variable, thus slowing down the propagation process. In cases
where two set constraints share more than one variable Lemma 4.2 does not apply, so in
such a case there can also be a loss of propagation strength.
The ROBDD representation of the constraints allows us to utilise such complex constraints directly, thus avoiding the problems associated with splitting the constraint. We
can directly construct an ROBDD for complex constraints by forming the conjunction of
the corresponding primitive constraints and existentially quantifying away the intermediate
variables.
Example 4.2. Consider the constraint c ≡ |v ∩ w| ≤ k as discussed in Example 4.1.
We can build a domain propagator dom(c) directly for c by constructing the ROBDD
∃V (u) u = v ∩ w ∧ |u| ≤ k. In this case the size of the resulting ROBDD is O(kN ).
The ROBDD for |v ∩ w| ≤ 2, that is ∃V (u) B(u = v ∩ w) ∧ B(|u| ≤ 2), where U =
{1, 2, 3, 4, 5} is shown in Figure 6(a). The ROBDD for |u| ≤ 2 is shown in Figure 6(b) for
comparison. Note how each ui node in the Figure 6(b) is replaced by vi ∧ wi (the formula
for ui ) in Figure 6(a).
4.2 Modeling Global Constraints
As the previous section demonstrates, it is possible to join the ROBDDs representing primitive set constraints into a single ROBDD representing the conjunction of the constraints.
We can use this to join large numbers of primitive constraints to form global constraints,
which in many cases will improve performance due to stronger propagation.
It is one of the strengths of the ROBDD-based modeling approach that it is trivial to
construct global constraints simply using ROBDD operations on primitive set constraints,
without laboriously writing code to perform propagation on the global constraint. This
approach is very powerful, but is not feasible for all combinations of primitive constraints.
As we shall see, some global constraints that we might desire to construct lead to ROBDDs
that are exponentially large.
A useful global constraint is the constraint partition(v1 , . . . , vn ), which requires that
the sets v1 , v2 , . . . , vn form a partition of the universe U = {1, . . . , N }. We can easily
1. The observation of Choi et al. that Lemma 4.2 does not apply if the shared variable is a set variable is
only true if we are performing set bounds propagation, not set domain propagation.

123

Hawkins, Lagoon, & Stuckey

0123
7654

v
y 1 EEEE
E"
y
w1 E
h
y
EEE
y h h h h
E"
y| sh h
v2 EE
v
y
y 2 EEEE
E
E
y
y
E"
E"
y
y
h w2EE
h w2EE
y
y
h
h
EEE
EEE
y
y
h h
h h
y| sh h h
"
" y| sh h h
v3, EE
v3 EE
v3 EE
y
y
EEE
EEE
E
y
y
, EE"
"
"
y
y
w
, w'3E
h w3
h 3EE
y
y
h
h
EEE
h
h
©
E
,
y
y
h
h
EE
©
" y| sh h h
, ' E" v y| sh h h
©©
v
'
©
4
4
E
E
Â EE
,
©
y EEE
y
E"
©©
, '
Â EE"
©
y
w4 E
, '
h w4 ©©
Â
y
µ EEEE
, ' Â
y h h h h µµ ©©
©
" y| sh h
, ' Â
µ
v5 EE
µµ ©©©
µ
,'
y
µ
EEE
µ ©©
y
,' Â µ
"
µµ µ ©©
w
', Â µ y y
h h 5DDD
µ ©
,'¹ ¶ Â² ªµ y h h h h
DD ªµµ¤©©©
"
|ysh h
y

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

u
y 1 EEEE
y
E"
y|
u2 E
u2 E
EEE
EEE
y
y
y
y
E"
E
y|
" y|
u3, E
u
u3 E
EEE
yµ 3
y EEE
y
y
E" y| µµ
, E" y|
u
µ
, uÂ 4 EE
y 4 µµ
, Â EEE" y| y
µµ
, Â
u5 D
µ
,¸ Â² z z DDDD ² ªµµµ
|z
"

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

0123
7654

1

0123
7654

0

0123
7654

0123
7654

1

0

(a)

(b)

Figure 6: ROBDDs for (a) |v ∩ w| ≤ 2 where v, w ⊆ {1, 2, 3, 4, 5}, and (b) |s| ≤ 2 where
s ⊆ {1, 2, 3, 4, 5}.

construct this constraint from primitive set constraints as follows:
partition(v1 , . . . , vn ) =

n−1
^

n
^

∃uij (uij = vi ∩ vj ∧ uij = φ)

i=1 j=i+1

∧ ∃w0 · · · ∃wn (w0 = φ ∧ (

n
^

wi = wi−1 ∪ vi ) ∧ wn = U)

i=1

The propagator dom(partition(· · · )) is stronger than a domain propagator on the
decomposition. For example, consider the constraint c ≡ partition(x, y, z) as depicted
in Figure 7, and the domain D where D(x) = D(y) = {{1}, {2}} and D(z) = {{2}, {3}}.
Domain propagation using the decomposition of the constraint will not alter D, but domain
propagation using the global constraint will give dom(c)(D)(z) = {{3}}. Hence stronger
propagation can be gained from global propagation using this constraint.
Unfortunately not all global constraints can be modelled efficiently using this approach.
In particular, there is a risk that the ROBDD representation of a global constraint could
be extremely large, making it infeasible to construct and use for propagation.
For example, consider the constraint atmost(hv1 , . . . , vn i , k) proposed by Sadler and
Gervet (2001), which requires that each of the sets v1 , . . . , vn has cardinality k and the
intersection of any pair of the sets is at most 1 element in size. This constraint models the
124

Solving Set Constraint Satisfaction Problems using ROBDDs

0123
7654

x
z 1 DDDD
z
D!
z}
y1
y1 QQQ
QQQ
Âw
Q
G&
QQ(
z1 DD
z
z 1
DDD
z
" |z
x2 D
DD
z
DD
z
z}
!
y2 QQQ
y2
w
QQQ
Â
QQQ
G&
(z
z2 DD
2
z
DDD
z
" |z
x3 D
DDD
z
z
D!
z}
y3
y1 QQQ
QQQ
Âw
QQQ
G&
(z
z3 DD
3
z
DD
D! }z z

0123
7654
0123
7654

0123
7654
0123
7654

0123
7654
0123
7654

0123
7654

0123
7654

0123
7654

x
z 1 DDDD
z
D!
z}
y1 D
y1
DD
zz 9 3
z
! z} z
x2 D
(
DDD
z
z
D
z}
!
!
y2 D
y2
z
.
DD
Â
zz
! z} z
) Â
x3$
z -Â Á
z} z
-y3 H
H
½ ¿
-H
H -- ¸ ¼
H - ³ ·
H# ¹ ¨ ª
º²
,1
0v

0123
7654 G Â
0123
7654 x w

0123
7654
0123
7654

0123
7654 G Â
0123
7654 x w

0123
7654

0123
7654 G Â
0123
7654 x w

0123
7654

0123
7654

0123
7654
0123
7654

1

partition(x, y, z)

lexlt(x, y)

(Arcs to ‘0’ are omitted for clarity)

Figure 7: ROBDDs for the constraints partition(x, y, z) and lexlt(x, y), where x, y and z
are set variables taking values from the universe U = {1, 2, 3}.

Steiner triple systems of Section 7.1. Bessiere, Hebrard, Hnich, and Walsh (2004) proved
that enforcing bounds consistency on this constraint is NP-hard, so it follows that enforcing
domain consistency on this constraint is at least NP-hard as well. Theoretically we can still
construct an ROBDD representing this constraint from primitive constraints as follows:
n
^

i=1

|vi | = n ∧

n−1
^

n
^

∃uij (uij = vi ∩ vj ∧ |uij | ≤ 1)

i=1 j=i+1

Unfortunately, the resulting ROBDD turns out to be exponential in size, making it impractical for use as a global propagator. This is not surprising in light of NP-hardness of
the problem in general—in fact it would be surprising if the resulting ROBDD was not
exponential in size!
4.3 Avoiding Symmetry Through Ordering Constraints
It is important in modeling a constraint satisfaction problem to minimize symmetries in
the model of the problem. A model that contains symmetrical solutions often has a greatly
enlarged search space, leading to large amounts of time being spent in searching sections
of a search tree that are identical up to symmetric rearrangement. It is therefore highly
desirable to remove whatever symmetries exist in a problem.
125

Hawkins, Lagoon, & Stuckey

One approach to symmetry-breaking is the introduction of additional ordering constraints between the variables of the problem. A convenient ordering to use on sets is a
lexicographic order on the characteristic bit vectors of the sets. In other words, if v and w
are set variables, then v < w in the lexicographic ordering if and only if the list of bits V (v)
is lexicographically smaller than V (w). We can model this lexicographic ordering constraint
as lexlt(v, w, 1), which is defined recursively in the following manner:
(
0
if n > N
lexlt(v, w, n) =
(¬vn ∧ wn ) ∨ ((vn ↔ wn ) ∧ lexlt(v, w, n + 1) otherwise
An example of such an ROBDD is depicted in Figure 7.
We shall make use of the lexicographic ordering constraint extensively in our experiments
in Section 7.

5. Modeling Integers, Multisets, and Weighted Sum Constraints
In this section we show how to model integer variables and integer constraints using ROBDDs. In general such representations can be very large, which limits the usefulness of
ROBDDs as the basis for a general purpose finite-domain constraint solver. Despite this,
the ability to represent integers is extremely useful as a component of a set solver. We
propose two major uses of the integer representation.
Firstly, we can use the integer representation to model values such as the weighted sum
of the elements of a set. Constraints on the weighted sum of elements of a set have been
shown to be useful in practical applications (Mailharro, 1998).
Secondly, we can model finite multisets using ROBDDs by replacing the individual
ROBDD variables of the set representation with bundles of ROBDD variables, each bundle corresponding to a binary integer. Multiset operations can then be constructed by
composing integer operations on the variable bundles.
In addition, the integer representation as described here could be used as an interface between an ROBDD-based set solver and a more conventional integer finite-domain
solver. Such an interface could easily be implemented using channeling constraints between
ROBDD integer and finite-domain versions of the same variable.
5.1 Representing Integer Values using ROBDDs
In order to model integers and integer operations we must choose an appropriate representation in terms of Boolean formulæ. In general we are free to use any encoding of an integer
as a binary sequence, such as unary, unsigned binary and twos-complement encodings, but
for simplicity we choose to represent all of our integers in unsigned binary form.
We can represent an arbitrary integer expression e by a list of Boolean formulæ. Each
formula in the list corresponds to a single bit of the unsigned binary value of the expression.
We will denote such a list by hen−1 , en−2 , . . . , e1 , e0 i, where each ei is a Boolean formula, in
order from the most significant bit to the least significant bit. We interpret a formula as
a “1” bit if the formula is logically true, and as a “0” bit otherwise; for simplicity we will
just call the formulæ the “bits of the expression”. We will also use the expression e and its
list of constituent bits interchangeably. As usual, we will represent the Boolean formulæ
126

Solving Set Constraint Satisfaction Problems using ROBDDs

as ROBDDs. As we shall see, this notation is flexible enough to represent arbitrary integer
expressions.
Example 5.1. Consider the integer constant k = 25, which is 11001 in unsigned binary.
We will represent k as the list h1, 1, 0, 0, 1i.
In order to represent an integer variable x, we associate with x a fixed set of Boolean
variables {xk−1 , . . . , x0 }. The value of x is then taken to be the value of the unsigned binary
integer hxk−1 , . . . , x0 i. By varying the value of the xi variables, the value of x can range
from 0 to 2k − 1 inclusive. As always the ordering of the Boolean variables has a significant
effect on the sizes of the ROBDD representations of formulæ. If x = hx k−1 , . . . , x0 i, y =
hyk−1 , . . . , y0 i, and z = hzk−1 , . . . , z0 i, then we choose to order the corresponding ROBDD
variables in a interleaved most-significant-bit-first order, i.e. xk−1 ≺ yk−1 ≺ zk−1 ≺ xk−2 ≺
· · · ≺ x 0 ≺ y0 ≺ z0 .
Since we permit arbitrary Boolean formulæ as the bits of an expression, we can also
model arbitrary integer expressions. For example, suppose x and y are integer variables with
bits hx2 , x1 , x0 i and hy2 , y1 , y0 i respectively. Then, for example, we can represent the expression x∧y (where the ∧ denotes the bitwise AND operator) as the list hx 2 ∧ y2 , x1 ∧ y1 , x0 ∧ y0 i.
We are not limited to logical operations, as we shall see in the next section.
5.2 Representing Integer Operations using ROBDDs
We can also use ROBDDs to model arithmetic operations such as addition, by analogy
with the design of the corresponding logic circuits. For the purpose of the set and multiset
solvers, we only require implementations of the operations of addition, left shift, minimum,
maximum, multiplication by a constant, and multiplication by a single variable bit. We do
not require a general implementation of multiplication using ROBDDs.
It is convenient to assume that all integer expressions have the same number of bits. We
may assume this without loss of generality since we can freely pad the left of the shorter of
any pair of expressions with “0” bits.
To model addition, we simulate the operation of a full binary adder. Suppose x and y
are integer expressions, with bit representations hxl−1 , . . . , x0 i and hyl−1 , . . . , y0 i. We can
use ROBDDs to compute the output bits plus(x, y) of the operation x + y as follows (here
ci denotes a carry bit and si denotes a sum bit):
c−1 = 0
si = xi ⊕ yi ⊕ ci−1

0≤i<l

ci = (¬ci−1 ∧ xi ∧ yi ) ∨ (ci−1 ∧ (xi ∨ yi ))

0≤i<l

plus(x, y) = hcl−1 , sl−1 , . . . , s1 , s0 i
Note we avoid overflow by extending the size of the result by one bit.
Example 5.2. Suppose x is an integer variable, with bits hx1 , x0 i. We can represent the
expression x+3 by the bits h(¬x0 ∧ x1 ) ∨ x0 ), x1 ⊕ 1 ⊕ x0 , x0 ⊕ 1i = hx0 ∨ x1 , x0 ↔ x1 , ¬x0 i.
The left shift operation is trivial to implement. If x is an integer expression represented
by hxl−1 , . . . , x0 i and k is a non-negative integer, then we can represent the left shift of x
127

Hawkins, Lagoon, & Stuckey

e3

0123
7654
x
7654
± 0123
x
}
1

¦ ²

¯ ¸¨

0

0

²

1

(x0 ∧ x1 )

0123
7654
x
1

Â

»

e2

e1

0123
7654
x G
Â
»

& zzz
· |zz

0

0

1

ww

0123
7654

m x1

0123
7654
x RRR
R
§ª

Âw
G'

0

0

(¬x0 ∧ x1 )

z

e0

0123
7654

¨

»

l x0 G Â
RlRlRl
l
l
RR( w w
lv ll

}

0123
7654
x
0

º

0

1

1

(x0 ⊕ x1 )

(x0 )

Figure 8: ROBDDs representing the bits he3 , e2 , e1 , e0 i of the expression e =
mul (hx1 , x0 i , 3), together with the simplified Boolean expressions for each ei .

by k bits shl (x, k) by the following formula:
shl (hxl−1 , . . . , x0 i , k) = hxl−1 , . . . , x0 , 0, . . . , 0i
| {z }
k bits

We can implement the operation of multiplication by a constant using the plus and shl
operators. If x is an integer expression with bits hxl−1 , . . . , x0 i and k is a non-negative
integer, then x × k corresponds to mul (x, k) in the following formula:


if k = 0
hi
k
(1)
mul (x, k) = mul (shl (x, 1), 2 )
if k is even and k > 0


k
plus(x, mul (shl (x, 1), b 2 c)) if k is odd and k > 0
Example 5.3. Let x = hx1 , x0 i, and consider the expression e = mul (x, 3). By applying
Equation (1), we obtain e = hx1 ∧ (x1 ∧ x0 ), x1 ⊕ (x0 ∧ x1 ), x0 ⊕ x1 , x0 i which can be simplified to hx0 ∧ x1 , ¬x0 ∧ x1 , x0 ⊕ x1 , x0 i. The corresponding ROBDD representations are
shown in Figure 8.
5.3 Integer Constraints using ROBDDs
We can also express constraints on integer expressions using ROBDDs. In particular, we
show how to implement equality and inequality constraints. As usual, we assume any two
expressions x and y have equal lengths of l bits; if not, we pad the shorter expression with
“0” bits on the right.
Equality of two integer expressions x and y is easy to represent as an ROBDD—the
corresponding bits of x and y must be equal. Hence we can represent the equality constraint
x = y as the ROBDD B(x = y):
B(x = y) =

l−1
^

i=0

128

xi ↔ y i

Solving Set Constraint Satisfaction Problems using ROBDDs

Note that this is identical to the implementation of equality for two set expressions with
the addition of zero-padding.
It turns out that we already have an implementation for inequality constraints, albeit
in a different guise. Inequalities of the binary integers correspond to inequalities on the
lexicographic ordering of the bit representations, so we can implement, for example, the
strict less-than constraint x < y for two integer variables x and y using the lexlt operation
from Section 4.3.
B(x < y) = lexlt(x, y)
We can then use this to construct the reverse inequality by swapping the order of the
operands, and the non-strict inequality by negating the formula (reversed as necessary).
The implementation of inequalities also leads us to an implementation of the minimum
and maximum expressions. Consider the problem of finding the smaller of two integer
expressions x and y. If x and y have bit vectors hxl−1 , . . . , x0 i and hyl−1 , . . . , y0 i respectively,
we recursively define min(x, y) as follows:
Ll = R l = 0
Li = Li+1 ∨ (¬Li+1 ∧ ¬Ri+1 ∧ ¬xi ∧ yi )

1≤i<l

Ri = Ri+1 ∨ (¬Li+1 ∧ ¬Ri+1 ∧ xi ∧ ¬yi )

1≤i<l

mi = (Li+1 ∧ xi ) ∨ (Ri+1 ∧ yi ) ∨ (¬Li+1 ∧ ¬Ri+1 ∧ xi ∧ yi )

0≤i<l

min(x, y) = hml−1 , . . . , m0 i
In the equation above, the Li and Ri values are flag bits which state whether the higher
order bits have already allowed us to conclude that one of the two values is the minimum.
The maximum operation is defined similarly.
5.4 Modeling Multisets and Multiset Constraints
Various authors have suggested that multisets are a valuable addition to the modeling
abilities of a set constraint solver (Kiziltan & Walsh, 2002). In this section we briefly show
how multisets and multiset constraints can be modelled using ROBDDs by making use of
the integer building blocks described above.
A multiset m is a unordered list of elements {{m0 , . . . , mn }} drawn from the universe U,
in which (unlike a set) repetition of elements is permitted. Most set operations have parallel
operations on multisets, although the multiset operations are not strict generalisations of
the set operations. Let occ(i, m) denote the number of occurrences of an element i in a
multiset m. Suppose m and n are multisets, and k is an integer constant. We define the
following multiset relations and operations by their actions on the number of occurrences
of each element i in the universe:
• Equality: m = n iff occ(i, m) = occ(i, n) for all i ∈ U.
• Subset: m ⊆ n iff occ(i, m) ≤ occ(i, n) for all i ∈ U.
• Union: occ(i, m ∪ n) = occ(i, m) + occ(i, n) for all i ∈ U.
• Intersection: occ(i, m ∩ n) = min{occ(i, m), occ(i, n)} for all i ∈ U.
129

Hawkins, Lagoon, & Stuckey

• Difference: occ(i, m \ n) = max{0, occ(i, m) − occ(i, n)} for all i ∈ U.
• Cardinality: |m| =

P

i∈U

occ(i, m) for all i ∈ U.

To represent a set variable x we associated a vector of Boolean variables hx 1 , . . . , xn i
with the bits of the characteristic vector of a valuation of x. In the case of a multiset, the
characteristic vector of a multiset m is a vector of integers, and so we need to associate an
integer value mi with each potential element i of the multiset m. We can model each such
integer value using the approach described above.
If m is a multiset variable, then we associate a bundle of ROBDD variables with every
i ∈ U, the contents of which comprise the bits of the corresponding integer expression m i . In
order that we can represent multisets in a finite number of bits, we assume that the number
of occurrences of any element in a multiset variable is bounded above by a reasonably small
M , allowing us to use only k = dlog2 M e Boolean variables per bundle. We then write m
as a list of bundles hm1 , . . . , mn i, where each mi is in turn a list of bits hmi,k−1 , . . . , mi,0 i.
Given a representation of multiset variables, we now turn our attention to the implementation of multiset expressions and constraints. Multiset expressions can be implemented in
the obvious way as sequences of integer expressions. For example, suppose x and y are multiset variables with associated bundles hx1 , . . . , xN i and hy1 , . . . , yN i respectively. Then the
bundles corresponding to the expression x∪y are hplus(x1 , y1 ), plus(x2 , y2 ), . . . , plus(xN , yN )i.
Similarly, the bundles corresponding to the expression x∩y are hmin(x1 , y1 ), . . . , min(xN , yN )i,
and so on for other expressions. We show how to implement cardinality and weighted sum
constraints in Section 5.5.
Multiset constraints are also trivial to implement—for example, two multisets x and y
are equal if and only if all of their constituent bundles are equal, and so multiset equality
can be modelled by a conjunction of integer equalities. Relations such as subset correspond
to a conjunction of integer inequalities on the constituent bundles, the implementation of
which was described in Section 5.3.
Up until this point we have left the ordering of the ROBDD variables that make up a
multiset variable unspecified. Unfortunately, unlike the set case, there is no single optimal
variable ordering that is guaranteed to produce compact descriptions of all the primitive
multiset constraints. For example, a subset constraint can be compactly represented by
a bundle-major bit ordering but not a bit-major ordering (see Figure 9), since a subset
constraint consists of a series of integer inequalities between the corresponding bundles of
two multiset variables, and so a bundle-major ordering gives an interleaving of variables as
above. However, the opposite is true for a cardinality constraint, which consists of a sum
of the values of the bundles within a variable, the variables of which are interleaved under
a bit-major ordering but not a bundle-major ordering. These two orderings are mutually
exclusive, and hence we can conclude that in general there need not be an optimal variable
ordering for modeling multiset constraints.
5.5 Weighted Sum and Cardinality Constraints
In many practical applications we are interested in placing constraints on a weighted sum of
the elements of a set variable. For example, in the Balanced Academic Curriculum problem
(problem prob030 of CSPLib), every course has an associated weight corresponding to its
130

Solving Set Constraint Satisfaction Problems using ROBDDs

Bundle 1

Bundle 2

Bundle 3

Bundle 1

Variable 1

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Variable 2

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Bit major

Bundle 2

Bundle 3

Variable 1

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Variable 2

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Bit 1

Bit 2

Bit 3

Bundle major

Figure 9: Bit and bundle major orderings for two multiset variables

academic load, and there is a limit to the total academic load that can be undertaken during
any given time period. In the set model of the problem proposed by Hnich, Kiziltan, and
Walsh (2002), the limits on the academic load in a period are made through the use of
a weighted sum constraint. In addition the cardinality constraint which we have already
described is a special case of the weighted sum constraint where all the weights are set to
“1”. It therefore seems essential to implement this constraint in the ROBDD framework.
Suppose x is a set or multiset expression with bit bundles hx1 , . . . , xn i. (If x is a set
expression, then each bit bundle has size 1). We can use the integer operations described
earlier to produce an integer expression wsum(x, w) corresponding to the weighted sum
P
n
i=1 xi wi , where w is a vector of integers hw1 , . . . , wn i:
φ0 = hi

φi = plus(φi−1 , mul (xi , wi ))
wsum(x, w) = φn
Expressions involving the cardinality of set and multiset variables can then be expressed
as a special case of the weighted sum expression where wi = 1 for all 1 ≤ i ≤ n. We have
already seen one method of constructing ROBDDs for the constraints |x| = k, |x| ≤ k and
|x| ≥ k in the set variable case; this method is more general, since it permits us to directly
model constraints such as |x| + 5 ≤ |y| + |z|. This is of great practical value—for example it
is needed to implement the Hamming Code experiment of Section 7.3. In the cases already
discussed in Section 3.2 the ROBDDs produced by the two methods are identical since
ROBDDs are a canonical representation.

6. Efficient Constraint Propagation Using ROBDDs
In this section we discuss improvements to and variants on the basic domain propagation
scheme presented in Section 3.3.
The implementation of the domain propagator dom(c) has a substantial effect on the
performance of the solver. The definition of dom(c) was given in its purest mathematical
form for simplicity. Section 6.1 discusses several implementation details that can lead to
greatly improved efficiency when performing domain propagation.
In general, the inferences that can be obtained using the domain propagator may be
too costly to be practical, and so in some circumstances it may be desirable to enforce a
131

Hawkins, Lagoon, & Stuckey

weaker form of consistency. Weaker consistency may be substantially cheaper to enforce,
permitting more time to be spent in searching the solution space.
As always there is a compromise between propagation time and search time, and for
certain problems it may be more productive to spend more time searching than performing
more accurate propagation, while for other problems the converse may be true. The ROBDD
based representation allows this to be taken to extremes—in theory it is possible to form
a single ROBDD representing the solutions to a constraint satisfaction problem by forming
the ROBDD conjunction of the constraints. A solution could then be trivially read from
the ROBDD as a satisfying assignment. Usually such an ROBDD would be prohibitively
expensive to construct in both time and space, forcing us to maintain less strict consistency
levels.
Accordingly, we show how to implement weaker levels of consistency using the ROBDD
representation by combining the domain propagator with an approximation operation. The
approximation operation simplifies the ROBDD representing a domain to its bounds closure
under a suitable definition of bounds. The ROBDD representing the bounds of a domain is
almost always smaller than the domain itself, leading to better performance for all future
operations on that domain. As we shall see, this can lead to substantial performance
improvements for the overall solver.
6.1 Domain Propagation
Let c be a constraint, with vars(c) = {v1 , . . . , vn }. Section 3.3 gave the following definition
of a domain propagator:
dom(c)(D)(vi ) = ∃V (vi ) (B(c) ∧

n
^

D(vj ))

(2)

j=1

Since B(c) and D(vj ) are ROBDDs, we can directly implement Equation (2) using
ROBDD operations. In practice it is more efficient to perform the existential quantification
as early as possible to limit the size of the intermediate ROBDDs. Here we make use of
the efficient combined conjunction and existential quantification operation, which we’ll call
and-abstraction, provided by most ROBDD packages.
This leads to the following implementation:
φ0i = B(c)
(
∃V (vj ) (D(vj ) ∧ φj−1
)
i
φji =
i−1
φi

1 ≤ i, j ≤ n, i 6= j (and-abstraction)
i=j

(3)

dom(c)(D)(vi ) = D(vi ) ∧ φni
Q
The worst case complexity is still O(|B(c)| × nj=1 |D(vj )|) for each vj . Clearly some of the
computation can be shared between propagation of c for different variables since φ ji = φji0
when j < i and j < i0 . Even with this improvement the algorithm of Equation (3) uses
O(n2 ) and-abstraction operations (which experimentally have been shown to occupy the
majority of the execution time of the set solver).
The domain propagator implementation of Equation (3) can be significantly improved
by observing that in the case of an n-variable constraint (n ≥ 3) many similar sub-formulæ
132

Solving Set Constraint Satisfaction Problems using ROBDDs

dom divide conquer(D, φ, V ):
if (|V | = 0) return D
else if (V = {vi })
D(vi ) := D(vi ) ∧ φ
return D
else
{v1 , . . . , vk } := V
h := b k2 c
R := ∃V (v1 ) D(v1 ) ∧ ∃V (v2 ) D(v2 ) ∧ · · · ∃V (vh ) D(vh ) ∧ φ
L := ∃V (vh+1 ) D(vh+1 ) ∧ ∃V (vh+2 ) D(vh+2 ) ∧ · · · ∃V (vk ) D(vk ) ∧ φ
D1 := dom divide conquer(D, L, {v1 , . . . , vh })
D2 := dom divide conquer(D1 , R, {vh+1 , . . . , vk })
return D2
Figure 10: A divide and conquer algorithm for domain propagation

are computed. Due to the need to perform the existential quantification operations as early
as possible, we do not have complete freedom to rearrange the order of evaluation as we see
fit. However, a simple divide-and-conquer strategy for calculating dom(c)(D)(v i ) allows us
to perform domain propagation using just O(n log n) and-abstraction operations. We define
dom(c)(D) = dom divide conquer(D, B(c), vars(c)), where dom divide conquer is defined in
Figure 10.
6.2 Set Bounds Propagation
As domain propagation may be prohibitively expensive to enforce for some problems, it is
useful to investigate less strict notions of consistency. In such cases, we can speed propagation by simplifying the domains through approximation. Since set bounds under the
subset partial ordering relation are one of the most commonly used approximations to a set
domain, it seems natural to implement a set bounds propagator in the ROBDD framework.
Only relatively minor changes are needed to the domain propagator to turn it into a set
bounds propagator.
Given the ROBDD representation of a set domain, we can easily identify the corresponding set bounds. In an ROBDD-based set domain representation, the set bounds on a
domain correspond to the fixed variables of the ROBDD representing the domain. We say
an ROBDD variable v is fixed if either for all nodes n(v, t, e) t is the constant 0 node, or
for all nodes n(v, t, e) e is the constant 0 node, and such a node appears in every path from
the root of the diagram to the “1” node.
Such nodes can be identified in a single pass over the domain ROBDD, in time proportional to its size. If φ is an ROBDD, we will write JφK to denote the ROBDD representing
the conjunction of the fixed variables of φ. If φ represents a set of sets S, then JφK represents
conv (S). An ROBDD φ where φ = JφK is a stick ROBDD by definition.
133

Hawkins, Lagoon, & Stuckey

Example 6.1. Let φ be the ROBDD depicted in Figure 1(c). Then JφK is the ROBDD of
Figure 1(a).
Using this operation we can convert our domain propagator into a set bounds propagator
by discarding all of the non-fixed variables from the domain ROBDDs after each propagation
step. Suppose that D(v) is a stick ROBDD for each v ∈ V. If c is a constraint, with
vars(c) = {v1 , . . . , vn }, we define a set bounds propagator sb(c) thus:
n
z
r
^
D(vj ))
sb(c)(D)(vi ) = ∃V (vi )(B(c) ∧

(4)

j=1

Despite only relatively minor differences between the set bounds propagator and the
domain propagator, the set bounds propagator is usually significantly faster than a domain propagator for two reasons. Firstly, as the domains D(v) are all sticks, all of the
ROBDD operations are cheap, compared to operations on the possibly very large ROBDDs
representing arbitrary domains. The entire propagator can be implemented with O(|B(c)|)
complexity, since all of the other ROBDDs are sticks. Secondly, we can use the updated set
bounds to simplify the propagator ROBDD B(c). Since domains are monotonic decreasing
in size, fixed variables will remain fixed up to backtracking, and so we can project them
out of B(c), thus reducing the size of the propagator ROBDD in future propagation steps.
This leads us to the following implementation of the propagator:
φ0 = B(c)
φj = ∃V AR(D(vj )) D(vj ) ∧ φj−1 1 ≤ j ≤ n
¯V (v ) Jφn K
sb(c)(D)(vi ) = D(vi ) ∧ ∃
1≤i≤n
i

(5)

After this propagation step we can replace the representation of the constraint B(c) by φ n
since the fixed variables will no longer have any new impact.
Example 6.2. Consider bounds propagation for the constraint c ≡ v ⊆ w where N = 3.
The ROBDD representation B(c) is given in Figure 4. Assume the domains of v and w are
respectively [{1}, {1, 2, 3}], represented by the formula v1 , and [∅, {1, 2}], represented by the
formula ¬w3 . The ROBDD φn ≡ ∃v1 ∃w3 B(c) ∧ v1 ∧ ¬w3 is shown in Figure 11(a). We
have that Jφn K = w1 ∧ ¬v3 . We can project out the fixed variables v1 , w1 , v3 , w3 from B(c)
to get a new simplified form of the constraint v2 → w2 shown in Figure 11(b).
This set bounds solver retains all of the modeling advantages of the domain solver,
including the ability to easily conjoin and existentially quantify constraints, to remove intermediate variables and to form global constraints. In some cases this permits a substantial
performance improvement over more traditional set bounds solvers.
Experimentally it appears that a direct implementation of Equation (4), written to
use a divide and conquer approach to calculate the and-abstractions, is faster than an
implementation of Equation (5), even if a divide and conquer approach is used in calculating
the existential quantification. The former approach calculates fewer intermediate results,
which leads to a faster propagator overall. Experimental results for the bounds propagator
are given in Section 7.
134

Solving Set Constraint Satisfaction Problems using ROBDDs

0123
7654
wE
) EE
1

0123
7654

0123
7654
v EE
( E
2

E
+ E"
. v2 EE
1 % EEE
"
5 +
9 5 w(2EEE
E
DE Q Z. E"
`
L Pl Cl0 v3 @@
l S V @@Ã
XU [Z -.
vl l
1
0

0123
7654

0123
7654

(a)

0123
7654

EE
"
w
z 2BBB
C
|z z U [BÃ.
0
1
.

(b)

Figure 11: Set bounds propagation on the constraint v ⊆ w showing (a) resulting ROBDD
after conjunction with domains, and (b) simplified constraint ROBDD after
removing fixed variables.

6.3 Split Domain Propagation
We can combine the set bounds propagator with the domain propagator to produce a more
space efficient split domain propagator. By separating the domain representation into fixed
and unfixed parts, we can reduce the total size of the representation, also hopefully speeding
propagation.
One of the unfortunate characteristics of ROBDDs is that the size of the ROBDD
representing a domain is highly dependent on the variable ordering. Consider an ROBDD
representing a set domain which contains several fixed variables. If these variables do
not appear at the beginning of the variable ordering, then the ROBDD representing the
domain will in effect contain several copies of the sticks representing the fixed variables.
For example, Figure 1(c) contains several copies of the stick in Figure 1(a). Since many of
our ROBDD operations take time proportional to the product of the number of ROBDD
nodes of their arguments, this overly large representation has a performance cost. We can
solve this problem in two ways—either by reordering the ROBDD variables or by splitting
up the domain representation.
Variable reordering is capable of eliminating redundancy in the representation of any
individual domain, but in general cannot eliminate redundancy across a set of domains. By
reordering the ROBDD variables, we can reduce the size of a domain by placing the fixed
variables at the beginning of the variable order, thus removing the unnecessary duplication
for that domain. Unfortunately, the variable order is a global property of all ROBDDs in
existence, whereas the fixed variables of a domain are a local property specific to a particular
domain, so there may not be a variable ordering that is optimal for all of the domains in a
problem.
In the context of applying ROBDDs to the groundness analysis of logic programs, Bagnara (1996) demonstrated that the performance of an ROBDD-based program analyzer
could be improved by splitting up ROBDDs into their fixed and non-fixed parts. We can
apply the same technique here.
135

Hawkins, Lagoon, & Stuckey

We split the ROBDD representing a domain D(v) into a pair of ROBDDs (LU , R). LU is
a stick ROBDD representing the lower and upper set bounds on D(v), and R is a remainder
ROBDD representing the information on the unfixed part of the domain. Logically D =
LU ∧ R. We will write LU (D(v)) and R(D(v)) to denote the LU and R parts of D(v)
respectively.
The following results provide an upper bound of the size of the split domain representation:
Lemma 6.1. Let G be an ROBDD, and let v be a fixed variable of G. Then |∃v G| < |G|.
Proof. Since v is a fixed variable, either for every node n(v, t, f ) in G t is the constant 0
node, or for every such node f is the constant 0 node. Since a node n(v, t, f ) corresponds to
the proposition (v ∧ t) ∨ (¬v ∧ f ), it is clear that ∃v n(v, t, f ) corresponds to simply t ∨ f , and
moreover since v is a fixed node one of t or f is zero. Hence the existential quantification
of a fixed variable v simply removes all nodes labelled v from D. Since there is at least one
such node, the result follows.
Lemma 6.2. Let G be an ROBDD, LU = JGK, and R = ∃VAR(LU ) G. Then G ↔ LU ∧ R
and |LU | + |R| ≤ |G|.
Proof. The result G ↔ LU ∧ R is straightforward, so we only prove the result on sizes.
Suppose that VAR(LU ) = {v1 , . . . , vn } is the set of fixed variables of G. Then, since
|∃v1 G| < |G|, 1 + |∃v1 G| ≤ |G|. By repeating this operation for each vi , we obtain n +
|∃VAR(LU ) G| ≤ |G|. But as LU is a stick, trivially |LU | = n, and as R = ∃VAR(LU ) G by
definition this is the required inequality.
Note that |D| can be O(|LU | × |R|). For example, considering the ROBDDs in Figure 1
where LU is shown in (a), R in (b), and D = LU ∧ R in (c), we have that |LU | = 5 and
|R| = 9 but |D| = 9 + 4 × 5 = 29.
We now show how to construct a propagator on split domains. Firstly, we eliminate
any fixed variables (as in the bounds propagator) and then apply domain propagation on
the remainder domains R. The propagator produces a new pair (LU , R) consisting of new
fixed variables and a new remainder. This process is shown below:
φ0 = B(c)
φj = ∃VAR(LU (D(vj ))) (LU (D(vj )) ∧ φj−1 )
δi = ∃V (vi ) (φn ∧

n
^

R(D(vj )))

(6)

j=1

βi = LU (D(vi )) ∧ Jδi K

dom(c)(D)(vi ) = (βi , ∃VAR(βi ) δi )

For efficiency the δi components can be calculated using the divide-and-conquer approach described for the domain propagator.
The split domain representation has three main advantages. Proposition 6.2 tells us that
the split domain representation is no larger than the original domain representation. However, often the split representation is substantially smaller, which can lead to improvements
136

Solving Set Constraint Satisfaction Problems using ROBDDs

{1,2,3,4}
Upper bound
{1,2,3} {1,2,4} {1,3,4} {2,3,4}

{1,2}

{1,3}

{1,4}

{2,3}

{2,4}

{3,4}
Lower bound

{1}

{2}

{3}

{4}

{}

Figure 12: The set interval [∅, {1, 2, 3, 4}] with upper and lower cardinality bounds u = 3
and l = 2 respectively

in propagation performance. The split solver can also use the propagator simplification
technique from the bounds solver by abstracting the fixed variables out of the propagator ROBDDs. Finally using the split solver we can mix the usage of domain and bounds
propagators in the same problem.
Experimental results for the split domain propagator are given in Section 7.
6.4 Cardinality Bounds Propagation
Given that we are able to model a set bounds propagator using ROBDDs, it is also appropriate to consider how we might model other levels of consistency for set constraint problems.
One level of consistency that is commonly used (Azevedo, 2002; Müller, 2001) is that of
combined set bounds and cardinality consistency, in which upper and lower bounds on the
cardinality of each domain are maintained in addition to the bounds under the subset partial ordering. This hybrid approach allows a more accurate representation of some domains,
particularly those with constrained cardinality, which are common in set problems.
Example 6.3. Figure 12 depicts the set interval [∅, {1, 2, 3, 4}], together with lower and
upper cardinality bounds 2 and 3 respectively. In general an interval consists of a large
number of sets, making it a crude approximation to a set domain. Cardinality bounds
permit a more fine grained representation by effectively allowing us to select a subset of the
rows of the lattice diagram.
Just as we were able to implement a set bounds propagator using the domain propagator together with a function which extracts the set bounds of a domain, so too can we
create a combined set bounds and set cardinality propagator. Here we will extend the split
domain solver by simplifying the “remainder” component of a split domain to an ROBDD
representing its cardinality bounds.
137

Hawkins, Lagoon, & Stuckey

bdd count cardinality(D, V ):
if (D = 0) return h∞, −∞i
else if (D = 1)
if (|V | = 0) return h0, 0i
else
hv1 , v2 , . . . , vn i := V
return h0, ni
else
if (|V | = 0) error
n(v, t, e) := D
hv1 , v2 , . . . , vn i := V
if (v1 Â v) error
else if (v = v1 )
hlt , ut i := bdd count cardinality(t, hv2 , . . . , vn i)
hle , ue i := bdd count cardinality(e, hv2 , . . . , vn i)
return hmin(lt + 1, le ), max(ut + 1, ue )i
else
hl, ui := bdd count cardinality(D, hv2 , . . . , vn i)
return hl, u + 1i
bdd card bounds(D, V ):
hl, ui := bdd count cardinality(D, V )
if (l = ∞ or u = −∞) return 0
return card (V, l, u)
Figure 13: An algorithm to determine the cardinality bounds on the domain of a set variable
represented by an ROBDD D, where V is the vector of bits in the Boolean
representation of the set variable

As before, we need a method for extracting an ROBDD representing the cardinality
bounds of an arbitrary domain ROBDD. We perform this operation in two stages. Firstly
we define a function bdd count cardinality which takes an ROBDD representing a set domain
and returns upper and lower bounds on its cardinality. We can represent these bounds
in ROBDD form by constructing a new cardinality constraint ROBDD as described in
Section 3.2.
An implementation of bdd count cardinality is shown in Figure 13. This function can
be implemented to run in O(|D||V |) time if dynamic programming/caching is used to save
the results of the intermediate recursive calls. In practice since D and V are highly interrelated, it is O(|D|). In our implementation we utilise the global cache mechanism of
the ROBDD library, which also permits caching of partial results between multiple calls to
bdd count cardinality.
138

Solving Set Constraint Satisfaction Problems using ROBDDs

0123
7654
x E
EE
1

¼
Â
%

0123
7654
xÂ E
EE

EE
"

,

2

h0, 2i

1

h0, 2i

0123
7654

EE
"
4
x3
Â
> Â z z AAA
AÃ
G % ² z|
Â

zt

t

JJJ
J$

h0, 1i

h0, 1i

zt

t

0
h0, 0i

(a)

JJJ
J$
zt

h0, 0i N

t

NNN
N&

h∞, −∞i

(b)

Figure 14: Cardinality propagation example showing (a) resulting ROBDD projected onto
x, and (b) calculation of cardinality bounds

We can now use this function to construct a function bdd card bounds which takes an
ROBDD D and a list of Boolean variables V and returns a new ROBDD representing the
bounds on the cardinality of the solutions of D. The implementation of bdd card bounds is
shown in Figure 13.
The algorithm for a split set bounds and set cardinality propagator for a constraint c is
given by sbc(c) in the following equation:
φ0 = B(c)
φj = ∃VAR(LU (D(vj ))) (LU (D(vj )) ∧ φj−1 )
δi = ∃V (vi ) (φn ∧

n
^

R(D(vj )))

(7)

j=1

βi = LU (D(vi )) ∧ Jδi K

sbc(c)(D)(vi ) = (βi , bdd card bounds(∃VAR(βi ) δi , V (vi ) \ VAR(βi )))
Note that we only keep the cardinality bounds on the remaining non-fixed Boolean
variables for a set variable v rather than on all the original variables V (v), since we do
not need to consider fixed variables again, and it leads to a slightly smaller cardinality
ROBDD. As usual Equation 7 should be implemented using a divide-and-conquer approach
for efficiency. Experimental results for this propagator are shown in Section 7.
Example 6.4. We illustrate set bounds and cardinality propagation on the constraint
c ≡ lexlt(x, y) whose ROBDD B(c) is shown in Figure 7(b). Assume the original domains
for x and y are universal, so D(x) = D(y) = [∅, {1, 2, 3}], represented by the ROBDD 1.
¯V (x) B(c) is shown in Figure 14(a), and βx = Jδx K = 1. The tree
The ROBDD for δx = ∃
of calculations for bdd card bounds(δx , hx1 , x2 , x3 i) is shown in Figure 14(b). Overall the
cardinality of x is determined to be in the range [0, 2].

139

Hawkins, Lagoon, & Stuckey

6.5 Lexicographic Bounds Propagation
An alternative form of set consistency proposed by Sadler and Gervet (2004) is to maintain
bounds under a lexicographic ordering in addition to set bounds. The lexicographic ordering
is a total ordering on sets which embeds the subset partial ordering. Bounds under the
lexicographic ordering alone are not sufficient to express the effects of many constraints (in
particular the inclusion of a single element), so Sadler and Gervet constructed a hybrid solver
which combines lexicographic bounds with traditional set bounds. They demonstrated that
a set solver based upon lexicographic bounds consistency techniques produced stronger
propagation than a traditional set bounds solver, although this came at a substantial cost
in propagation performance. However, given that the use of the ROBDD representation
leads to a performance improvement in the case of set bounds propagation, it is worth
investigating the performance of an ROBDD-based lexicographic bounds propagator.
The lexicographic bounds of a domain can be very compactly represented as an ROBDD.
Like set bounds, an ROBDD representing the upper or lower lexicographic bounds of a
domain is an ROBDD of size O(N ), and so is the combination. Because these ROBDDs are
compact this hopefully leads to fast propagation. Moreover, given an ROBDD domain, it is
very easy to extract the lexicographic bounds on that domain in a single pass. By a process
analogous to the construction of the bounds and cardinality propagator, we can use a split
domain propagator combined with a function which determines the lexicographic bounds
of a domain to construct a highly efficient lexicographic bounds solver.
We define two functions bdd lex lower bound and bdd lex upper bound. These functions,
given an ROBDD representing the domain D of a variable V together with a list of Boolean
variables B corresponding to the bits of V , return the lower and upper lexicographic bounds
(respectively) of D. bdd lex lower bound is implemented as shown in Figure 15 (the implementation of bdd lex upper bound is very similar).
We define:
bdd lex bounds(D, B) = bdd lex lower bound(D, B) ∧ bdd lex upper bound(D, B)
The split set and lexicographic bounds propagator can then be implemented exactly as
in Equation (7), using bdd lex bounds in place of bdd card bounds.
Example 6.5. Consider lexicographic bounds propagation on the constraint c ≡ |s| = 2
where s ⊆ {1, 2, 3, 4, 5}. The ROBDD B(c) is shown in Figure 3(b) and initial domain
D(s) = [∅, {1, 2, 3, 4, 5}]. Then δs = B(c) and βs = 1 so the call to bdd lex bounds calculates the lower bounds lexicographic ROBDD shown in Figure 16(a), the upper bounds
lexicographic ROBDD shown in Figure 16(b), and final answer the conjunction shown in
Figure 16(c). Note that we have lost some information relative to the original cardinality
ROBDD.
As observed in Section 5.3, the lexicographic ordering for set variables actually corresponds to a numeric ordering on integer variables, so a pure lexicographic bounds propagator
would also be coincidentally an integer bounds propagator.
Experimental results for the lexicographic bounds propagator are given in Section 7.
140

Solving Set Constraint Satisfaction Problems using ROBDDs

bdd lex lower bound(D, B):
if (|B| = 0 or D = 1) return 1
if (D = 0) error
n(v, t, e) := D
hb1 , . . . , bn i := B
if (b1 Â v) error
else if (b1 = v and e = 0)
return b1 ∧ bdd lex lower bound(t, hb2 , . . . , bn i)
else
if (b1 = v) r := bdd lex lower bound(e, hb2 , . . . , bn i)
else r := bdd lex lower bound(D, hb2 , . . . , bn i)
return b1 ∨ (¬b1 ∧ r)
Figure 15: An algorithm to extract the lower lexicographic bounds on the domain of a set
variable represented by an ROBDD D, where B is the vector of (non-fixed) bits
in the Boolean representation of the set variable

0123
7654
s

}{ {

0123
7654
s

{

{ 2
{
}{
s
{ 3++
++
}{ {
s4 C
++
++
¸ CCCC
!
º
s5 C +++
CC +
Á
³
C! ¸ § |
(
º
1
7
1
L
W U^ \` /0
0

0123
7654

0123
7654

0123
7654

(a)

0123
7654
s C
CC

1

1

0123
7654

0123
7654

0123
7654

0123
7654

s
{ 1 CCCC
C!
s2
s2 C
{
° CCCC
{
!
}{
°
s
s
°
{ 3
{ 3++
{
{
+
°
}{
}{
++
s4 C
s4
°
++
C
{
C
° {
¸
CC
+
!
}{
º
s5 C +++ ° ° s5
CC +
{
Á
³
C! ¸ §°}{
(
º
1
7
1
L
W U^ \` /0 qtp
0

CC
­
!
s2 C
²
° CCCC
!
·
°
s3
°
{
»
{
°
}{
s4
°
À
{
° {
!
° s }{
$ °~5
¶ §°~~

0123
7654

0123
7654

1

0 p qt
(b)

0123
7654

0123
7654

0123
7654

}{ {

0123
7654

0123
7654

0123
7654

0123
7654

(c)

Figure 16: Calculating lexicographic (a) lower and (b) upper bounds and (c) their conjunction.

7. Experiments
We have implemented a set solver using the ideas described in this paper. Our implementation is written primarily in Mercury (Somogyi, Henderson, & Conway, 1996), using an
interface to the C language ROBDD library CUDD as a platform for ROBDD manipulations (Somenzi, 2004). The ROBDD library is effectively treated as a black box.
141

Hawkins, Lagoon, & Stuckey

We used our solver to implement a series of standard constraint benchmarks as described
below. Many of these problems are in the CSPLib library of constraint satisfaction problems
(Gent, Walsh, & Selman, 2004). For the purposes of comparison, we also implemented
our benchmarks using the ic sets library of ECLi PSe v5.62 and the finite sets library of
Mozart v1.3.0. We conducted all of our experiments on a cluster of 8 identical 2.8GHz
Pentium 4 machines, each with 1 Gb of RAM and running Debian GNU/Linux 3.1. All
three solvers were limited to 1 Gb of memory to minimise swapping. All experiments were
repeated three times, and the lowest time out of the three runs was taken as the result.
7.1 Steiner Systems
A commonly used benchmark for set constraint solvers is the calculation of small Steiner
systems. A Steiner system S(t, k, N ) is a set X of cardinality N and a collection C of
subsets of X of cardinality k (called “blocks”), such that any t elements of X are in exactly
one block. Steiner systems have been extensively studied in combinatorial mathematics.
If t = 2 and k = 3, we have the so-called Steiner triple systems, which are often used as
benchmarks (Gervet,
¡ ¢ ¡ ¢ 1997; Azevedo, 2002; Müller, 2001). Any Steiner system must have
exactly m = Nt / kt blocks (Theorem 19.2 of van Lint & Wilson, 2001).
It is natural to model a Steiner system using set variables s1 , . . . , sm , where each set
variable corresponds to a single block, subject to the following constraints:
m
^

(|si | = k)

(8)

(∃uij uij = si ∩ sj ∧ |uij | ≤ (t − 1)) ∧ (si < sj )

(9)

i=1

∧

m−1
^

m
^

i=1 j=i+1

The lexicographic ordering constraint si < sj has been added to remove symmetries from
the problem formed by permuting the blocks.
¡ −i¢ ¡k−i¢
/ t−i is an
A necessary condition for the existence of a Steiner system is that Nt−i
integer for all i ∈ {0, 1, . . . , t − 1} (van Lint & Wilson, 2001); we say a set of parameters
(t, k, N ) is admissible if it satisfies this condition. In order to choose test cases, we ran
each solver on every admissible set of (t, k, N ) values for N < 32. Results are shown for
every test case that at least one solver was able to solve within a time limit of 10 minutes.
“×” denotes abnormal termination due to exceeding an arbitrary limit on the maximum
number of ROBDD variables imposed by the CUDD package, while “—” denotes failure to
complete a testcase within the time limit.
In all cases we use a sequential variable-ordering heuristic, and a value-ordering heuristic
that chooses the largest unfixed value within a variable’s domain. Once a variable and
an unfixed value have been chosen for labeling, either that value is a member of the set
represented by the variable or it is not. The order in which we try the two alternatives
has a significant effect on the performance of the solver. In this case we elect to choose the
element-not-in-set option first.
2. Very recently a new sets library Cardinal has been added to ECLi PSe which supports better cardinality
reasoning. Unfortunately we cannot directly compare with it on these benchmarks since it does not

142

Solving Set Constraint Satisfaction Problems using ROBDDs

Table 2: Results for Steiner systems, with split constraints and intermediate variables.
Testcase ECLi PSe Mozart Bounds
Domain
LU+R
LU+Lex LU+Card
Time Fails Time Fails Time Fails Time Fails Time Fails Time Fails Time Fails
/s

/s

S(2,3,7)
0.3
10
S(3,4,8)
0.5
21
S(2,3,9)
7.7 1394
S(2,4,13) 1.8 313
S(2,3,15) 3.6
65
S(3,4,16) 67.5 289
S(2,5,21) 3.2 421
S(3,6,22) 49.7 1619

/s

/s

/s

/s

/s

0.1 21 <0.1 10 0.1
0 0.1
0 0.1
4 <0.1
2
0.1 52 0.1 21 0.4
0 0.4
0 0.4
4 0.1
4
1.0 5102 1.6 1394 1.0 100 1.3 100 3.3 421 2.4 1072
0.4 1685 0.6 313 1.7 32 1.5 32 2.1 127 0.7 157
0.5 354 2.2 65 20.2
0 19.6
0 20.4 127 3.3 41
— —
× ×
× ×
× ×
× ×
× ×
0.4 668 2.3 421 110.2
0 59.8
0 21.5 139 2.6 124
— —
× ×
× ×
× ×
× ×
× ×

Table 3: Results for Steiner systems, with merged constraints and no intermediate variables.
Testcase

Bounds
Domain
LU+R
LU+Lex
LU+Card
Time Fails Time Fails Time Fails Time Fails Time Fails
/s

/s

/s

S(2,3,7) <0.1
8 <0.1
0
S(3,4,8)
0.1
18
0.1
0
S(2,3,9)
0.2
325
0.1
9
S(2,3,13)
—
— 109.2 24723
S(2,4,13) 0.1
157
0.1
0
S(2,3,15) 0.4
56
1.3
0
S(2,4,16) 421.4 522706
0.6
15
S(2,6,16)
—
— 80.7 15205
S(3,4,16) 9.7
274 548.7
0
S(2,5,21) 0.5
413
1.4
0
S(3,6,22) 8.3 1608
—
—
S(2,3,31) 23.3
280
—
—

/s

<0.1
0
0.1
0
0.1
9
144.6 24723
0.1
0
1.4
0
0.6
15
82.7 15205
485.3
0
1.4
0
—
—
—
—

/s

<0.1
0
0.1
0
0.1
11
518.1 30338
0.4
11
2.9
0
2.5
16
—
—
428.9
0
32.9
0
—
—
—
—

<0.1
0
0.1
0
0.2
113
—
—
0.1
27
0.7
32
577.0 209799
—
—
18.4
162
0.6
116
12.7
381
48.6
224

Table 4: All-solutions results on Steiner systems. “—” denotes failure to complete a test
case within one hour
ECLi PSe
Bounds
Problem Solns. time fails time fails
/s

S(2,3,7)
S(3,4,8)
S(2,3,9)
S(2,6,16)

/s

Domain
LU+R
time fails time fails
/s

/s

LU+Lex
LU+Card
time fails time fails
/s

/s

30 16.3 3,015 0.2
537 0.1
47 0.1
47 0.2
76 0.3
267
30 —
— 726.5 610271 1.4 492 2.1 492 22.4 3248 951.2 431801
840 —
— 398.1 391691 23.4 16794 37.8 16794 110.4 29133 593.3 224131
0 —
—
—
— 80.9 15205 83.0 15205
—
—
—
—

143

Hawkins, Lagoon, & Stuckey

In order to compare the raw performance of the various solvers, irrespective of any
modeling advantages of the ROBDD-based solver, we performed experiments using a model
of the problem which contains only primitive constraints and makes use of intermediate
variables. This “split” model contains m unary constraints corresponding to Equation (8)
and 3m(m−1)/2 binary constraints corresponding to Equation (9) (containing intermediate
variables uij ). The same model was used in the ECLi PSe , Mozart and ROBDD-based
solvers, permitting a direct comparison of propagation performance. Results for this model
are shown in Table 2. In particular, observe that with this model ECLi PSe and the ROBDDbased bounds solver produce the same number of failures, demonstrating that the search
spaces explored by the two solvers are identical.
One of the main strengths of the ROBDD-based modeling approach is that it gives us
the freedom to merge arbitrary constraints and existentially quantify away intermediate
variables, allowing us to model set constraint problems more efficiently. In the case of
Steiner systems, this allows us to model the problem as m unary constraints corresponding
to Equation (8), and just m(m − 1)/2 binary constraints ψij of the form ψij = (|si ∩ sj | ≤
(t − 1)) ∧ (si < sj ). The binary constraints do not contain the intermediate variables
uij —they are not required since they can be existentially quantified out of the ROBDD
representation. Experimental results for this revised model are shown in Table 3.
In all cases the revised model propagates much more strongly than the original model,
leading to a substantial decrease in solution time. In addition, the decrease in the number
of set variables required permits the solution of larger test cases. Clearly it is beneficial to
remove intermediate variables and merge constraints.
Despite weaker propagation the ROBDD bounds solver is often the fastest method of
finding a single solution to a Steiner System. In order to determine whether this was due
to the efficiency of the solver, or whether the solver was just “lucky” in finding a solution
quickly, we also ran experiments to find all solutions of the Steiner systems up to reordering
of the blocks. The results for all test cases that at least one of the solvers was able to solve
within a time limit of one hour are shown in Table 4. In all cases the reduction both of
time and number of fails demonstrate the superiority of the propagation approaches based
on domain consistency.
7.2 Social Golfers
Another problem often used as a set benchmark is the “Social Golfers” problem (problem
prob010 of CSPLib). The aim of this problem is to arrange N = g × s golfers into g groups
of s players for each of w weeks, such that no two players play together more than once.
We can model this problem as a set constraint problem using a w × g matrix of set variables
vij , where 1 ≤ i ≤ w is the week index and 1 ≤ j ≤ g is the group index.

support lexicographic ordering constraints. Testing without lexicographic orderings showed that it was
about 2–3 times slower than LU + Card.

144

Solving Set Constraint Satisfaction Problems using ROBDDs

We use the following model of the problem:
w
^

(partition< (vi1 , . . . , vig )) ∧

^

|vij | = s

i=1 j=1

i=1

∧

g
w ^
^

^

|vik ∩ vjl | ≤ 1 ∧

w−1
^

w
^

(10)
vi1 ≤ vj1

i=1 j=i+1

i,j∈{1,...,w} k,l∈{1,...,g}
i6=j

The partition< global constraint is a combined partitioning and lexicographical ordering constraint, formed by merging the partition constraint of Section 4.2 with constraints
imposing a lexicographic order on the variables. This constraint is trivial to construct using
ROBDDs, but is not available in either ECLi PSe or Mozart.3
Results for each of the solvers are shown in Table 5 and Table 6. In the former table,
a sequential smallest-element-in-set labeling strategy was used to enable a fair comparison
of propagation performance, whereas in the latter table a first-fail labeling strategy was
used in order to give a measure of the peak performance of each solver. For every test case
in both tables, with a single exception (5-8-3), at least one of the ROBDD-based solvers
performs equal or better to both ECLi PSe and Mozart. It should also be observed that
when using a first-fail labeling strategy, the domain and split domain solvers are the only
solvers able to solve every test case.
There are several other features of the results that are worth noting. As in the case
of Steiner systems, the ROBDD-based set bounds solver is often the fastest, despite weak
propagation. Amongst the solvers with stronger propagation, the split domain solver is
almost always faster than the original domain solver due to smaller domain sizes. It is,
however, slower than the original domain solver in the presence of backtracking (due to
the requirement to trail more values—in particular the propagator ROBDDs). The lexicographic bounds solver is almost as effective as the domain solvers in restricting search
space, although it is usually outperformed by the domain and bounds solvers.
7.3 Weighted Hamming Codes
The problem of finding maximal Hamming Codes can be modelled as a set constraint
problem.
We define an l-bit codeword to be a bit-string (or vector of Boolean values) of length l.
Given two l-bit codewords A and B, we define the Hamming distance d(A, B) between A
and B to be the number of positions at which the two bit-strings differ. An (l, d)-Hamming
Code is a set of l-bit codewords such that the Hamming distance between any two codewords
in the set is at least d.
Given a codeword length l and the minimum Hamming distance d, the problem is to
construct a Hamming code with the largest possible number of codewords. A variant of this
problem, used as a benchmark by Sadler and Gervet (2004), has the additional requirement
that each codeword have a fixed weight w, where the weight of a codeword is defined to be
3. It would, of course, be possible to implement such a constraint in both ECLi PSe and Mozart, but
such an implementation would be a fairly laborious process. A strength of the ROBDD-based modeling
approach is that we can construct global constraints with no extra code.

145

Hawkins, Lagoon, & Stuckey

Table 5: First-solution performance results on the Social Golfers problem, using a sequential
“smallest-element-in-set” labeling strategy. Time and number of failures are given
for all solvers. “—” denotes failure to complete a test case within 10 minutes. The
cases 5-4-3, 6-4-3, and 7-5-5 have no solutions
Problem
w-g-s
2-5-4
2-6-4
2-7-4
2-8-5
3-5-4
3-6-4
3-7-4
4-5-4
4-6-5
4-7-4
4-9-4
5-4-3
5-5-4
5-7-4
5-8-3
6-4-3
6-5-3
6-6-3
7-5-5

ECLi PSe
time fails
/s

Mozart
time fails
/s

7.6 10468
49.2 64308
95.1 114818
—
—
12.5 14092
76.3 83815
146.8 146419
14.1 14369
—
—
169.3 149767
27.3 19065
—
—
350.6 199632
—
—
5.0 2229
—
—
458.9 240296
3.3 1462
—
—

1.0 7638
6.4 42346
10.9 66637
—
—
2.5 10311
14.0 51134
27.3 88394
3.9 10715
—
—
46.4 90712
7.8 12489
—
—
217.7 416889
—
—
0.9 1820
—
—
287.4 471485
1.0 1462
—
—

Bounds
Domain LU+R LU+Lex LU+Card
time fails time fails time time fails time fails
/s

/s

0.1
30 0.1
0
0.6 2036 0.2
0
1.7 4447 0.4
0
—
— 2.0
0
0.1
30 0.3
0
1.6 2039 1.6
0
4.6 4492 8.9
0
0.2
30 0.8
0
21.9 12747 118.6
0
8.7 4498
—
—
2.6
71
—
—
113.9 63642 28.6 5165
7.0 2686 3.8 41
14.6 4583
—
—
1.1
14 9.2
0
158.2 61770 20.3 2132
4.1 1455 1.8 82
0.5
5 1.8
0
—
— 0.5
0

/s

0.1
0.1
0.4
1.6
0.3
1.4
8.4
0.6
80.7
481.6
—
32.1
2.3
—
7.8
23.0
1.5
1.4
0.4

/s

0.1
3
0.7 194
2.3 692
—
—
0.4
3
2.2 194
7.6 695
0.7
3
19.3 499
14.1 696
22.2
8
88.9 10210
12 313
23.7 700
3.2
3
60.8 4506
4.1 202
1.2
0
2.4
0

/s

0.1
5
0.4 326
1.6 1608
—
—
0.2
5
1.9 328
5.1 1629
0.4
5
32.0 2122
10.5 1632
8.9
33
202.7 50542
20.8 1584
17.5 1683
2.1
4
293.5 49966
8.3 1078
0.7
1
1.3
22

the number of “1” bits that codeword contains. We will denote an instance of this problem
by H(l, d, w).
As proposed by Müller and Müller (1997), we can model this problem for n codewords
using n set variables Si , where 1 ≤ i ≤ n . A codeword Ci corresponds to the characteristic
function of the set Si , i.e. bit j is set in codeword Ci if and only if j ∈ Si . The Hamming
distance d(Ci , Cj ) between two codewords Ci and Cj can be calculated from the associated
sets Si and Sj thus:

d(Ci , Cj ) = l − |Si ∩ Sj | − |{1, . . . , l} \ (Si ∪ Sj )|

We can remove symmetries created by permuting the codewords by introducing lexicographic ordering constraints Si < Sj , for all 1 ≤ i < j ≤ n. The complete model of the
146

Solving Set Constraint Satisfaction Problems using ROBDDs

Table 6: First-solution performance results on the Social Golfers problem, using a first-fail
“smallest-element-in-set” labeling strategy. Time and number of failures are given
for all solvers. The cases 5-4-3, 6-4-3, and 7-5-5 have no solutions
Problem
w-g-s
2-5-4
2-6-4
2-7-4
2-8-5
3-5-4
3-6-4
3-7-4
4-5-4
4-6-5
4-7-4
4-9-4
5-4-3
5-5-4
5-7-4
5-8-3
6-4-3
6-5-3
6-6-3
7-5-3
7-5-5

ECLi PSe
time fails

Mozart
time fails

/s

/s

7.9 10468
51.3 64308
99.9 114818
—
—
14.5 14092
91.8 83815
183.0 146419
18.0 14369
—
—
243.9 149767
40.9 19065
—
—
394.9 199632
—
—
5.4 2229
—
—
501.0 240296
3.6 1462
—
—
—
—

Bounds
time fails
/s

1.1 7638
6.5 42346
11.1 66637
—
—
2.6 10311
15.1 51134
28.7 88394
4.1 10715
—
—
49.6 90712
8.3 12489
—
—
224.5 416889
—
—
0.9 1820
—
—
294.2 471485
1.0 1462
—
—
—
—

Domain LU+R LU+Lex LU+Card
time fails time time fails time fails
/s

0.1
30
0.6 2036
1.7 4447
—
—
0.1
44
1.9 2361
5.2 5140
0.3
47
38.5 19376
9.9 5149
2.7
143
187.5 103972
4.5 2388
17.6 5494
1.1
19
234.2 90428
1.6
495
—
—
—
—
—
—

/s

0.1
0
0.2
0
0.4
0
1.9
0
0.3
0
1.1
0
1.9
0
0.8
0
62.5
0
6.5
0
152.0
0
23.2 3812
2.5 18
18.2
0
4.5
0
14.6 1504
1.2 34
1.6
7
16.9 528
0.5
0

0.1
0.1
0.4
1.6
0.3
0.9
1.7
0.6
40.1
5.1
107.4
26.0
1.7
12.8
3.9
15.2
1.0
1.3
13.1
0.4

/s

0.1
3
0.7 186
2.0 390
—
—
0.4
5
2.4 209
6.2 512
0.7
7
24.6 607
10.6 405
22.6
13
91.4 10422
23.4 776
16.9 447
3.4
2
54.0 4013
58.2 3787
5.1 292
288.8 9829
2.4
0

/s

0.4 447
3.8 3820
4.6 6424
—
—
1.3 481
13.7 3722
15.6 6067
2.6 494
—
—
29.5 5717
12.5 516
309.1 72669
41.4 4730
47.1 5473
2.7
83
349.9 59805
473.6 68673
1.0
8
—
—
1.3
22

problem is:
n
^

|Si | = w

(11)

i=1

∧

n−1
^

n
^

(|Si ∩ Sj | + |(Si ∪ Sj )| ≤ l − d) ∧ (Si < Sj )

(12)

i=1 j=i+1

The constraints described by Equation (12) are implemented using a single ROBDD for
each pair of i and j values. This is possible since we can model the integer addition and
comparison operations as ROBDDs as described in Section 5, using the representation of
the cardinality of a set variable as an integer expression as described in Section 5.5.
In order to find an optimal solution, we initially set n to 1, and repeatedly solve instances
of the problem, progressively incrementing n to find larger and larger codes. We prove
optimality of a solution for n = k by failing to solve the problem for n = k + 1; we then
know that the optimal value for n was k.
147

Hawkins, Lagoon, & Stuckey

Table 7: Statistics for the 51 Weighted Hamming Code testcases that were solved by all of
the solvers.
Bounds
time
fails
/s

Domain
LU+R
time fails time fails
/s

Mean
17.7 110034
Total
903.3
Minimum
0.03
0
25th Percentile 0.03
19
Median
0.05
196
75th Percentile 0.67
5604
Maximum
415.55 3021057

LU+Lex
LU+Card
time fails time fails

/s

0.2 210.7
11.4
0.03
0
0.03
0
0.04
2
0.06
25
4.16 3740

/s

0.3 210.7
14.2
0.03
0
0.03
0
0.04
2
0.06
25
4.69 3740

/s

4.4 1886
222.6
0.03
0
0.03
3
0.04
20
0.18 143.5
113.9 45667

3.6 6604
184.6
0.03
0
0.03
0
0.04
2
0.09
112
92.09 211677

Table 8: Weighted Hamming Code testcases that were solved by at least one but not all
solvers
Bounds
Domain
LU+R
time fails time fails time fails
/s

H(8,4,4)
H(9,4,3)
H(9,4,6)
H(10,6,5)

—
—
—
—

/s

/s

LU+Lex
time fails
/s

LU+Card
time fails
/s

— 1.6 224 1.9 224 8.5
986
—
—
— 11.3 5615 20.6 5615
—
—
—
—
— 25.4 16554 45.7 16554 321.9 56599
—
—
— 26.7 16635 29.6 16635 528.8 169457 428.3 762775

In order to assist with timing comparisons, we use the same set of problem instances
as Sadler and Gervet (2004). We consider sets of values H(l, d, w) where l ∈ {6, 7, 8, 9, 10},
d ∈ {4, 6, 8, 10, 12}, and w ∈ {3, 4, 5, 6, 7, 8}, with d < l and w ≤ l (trivially there is at most
one solution if d ≥ l and none if w > l). There are 62 such testcases; some of these are almost
identical—in particular testcases H(l, d, w) and H(l, d, l − w) have solutions that differ only
through complementation of the bits. The ROBDD-based solver can solve all but seven test
cases (namely H(9, 4, 4), H(9, 4, 5), H(10, 4, 3), H(10, 4, 4), H(10, 4, 5), H(10, 4, 6), H(10, 4, 7)),
which in reality contains three pairs of mirror image testcases.
Since there are too many results to list each testcase individually, performance statistics
for the testcases that all the solvers were able to solve are shown in Table 7. Those cases
that were solved by at least one but not all of the ROBDD-based solvers are shown in
Table 8.
Sadler and Gervet (2004) report results for this problem using set bounds and lexicographic bounds solvers implemented in ECLi PSe . Their solvers were able to solve 50 of the
testcases with a time limit of 240 seconds for each testcase. Some individual testcases took
upwards of 100 seconds. By contrast, the ROBDD-based domain solver is capable of solving 55 testcases in 76.4 seconds in total. Clearly in this case enforcing domain consistency
brings a considerable reduction in search space, leading to a highly efficient solver.
148

Solving Set Constraint Satisfaction Problems using ROBDDs

Moreover, the set bounds and lexicographic bounds solvers implemented using the
ROBDD platform appear to perform considerably better than those of Sadler and Gervet.
Due to the graphical nature of their presentation it is difficult to quantify this performance difference; exact figures are presented for only two testcases—namely H(9, 4, 7) and
H(10, 6, 7). For the testcase H(9, 4, 7) the ROBDD set bounds and lexicographic bounds
solvers each found and proved an optimal solution in 0.6 and 0.3 seconds respectively,
compared with the > 240 and 167.1 seconds respectively quoted for the bounds and lexicographic solvers of Sadler and Gervet. Similarly for H(10, 6, 7) the ROBDD set bounds and
lexicographic bounds solvers found and proved an optimal solution in 1.9 and 1.1 seconds
respectively, as compared with > 240 and 98.5 seconds. Given this dramatic performance
difference, it appears the additional modeling flexibility of the ROBDD-based solver provides a substantial performance gain.
It should be noted that the performance results reported by Sadler and Gervet were run
on a slightly slower machine (a 2 Ghz Pentium 4 machine). Nonetheless, the contribution
to the performance difference due to machine speed is dwarfed by the performance gap
between the two solvers.
7.4 Balanced Academic Curricula
The Balanced Academic Curriculum problem (problem prob030 of CSPLib) involves planning an academic curriculum over a sequence of academic periods in order to provide a
balanced load in each period.
A curriculum consists of m courses (1 ≤ i ≤ m) and n academic periods. Each course i
has a set of prerequisites and an associated academic load ti . Every course must be assigned
to exactly one period. In any given period the total number of courses must be at least
some minimum number c and at most a maximum number d. In addition, within any given
period the total academic load of the courses must be at least some minimum load a and
at most a maximum load b. Let R be the set of prequisite pairs hi, ji, where i and j are
courses. Prerequisite relationships must be observed, so for each pair of courses hi, ji ∈ R,
if course i is scheduled in period p, then course j must be scheduled in a period strictly
prior to p.
A model of this problem using set variables and set constraints was proposed by Hnich
et al. (2002), although no experimental results were presented for that model. In this
“primal” model, we use one set variable Si per academic period. Each Si represents the
set of courses assigned to academic period i, so k ∈ Si if and only if course k is assigned to
period i. We can model the problem using the following constraints:
• (S1) Every course is taken exactly once: partition(S1 , . . . , Sn )
• (S2) The number of courses in a period is between c and d:
∀ni=1 ((c ≤ |Si |) ∧ (|Si | ≤ d))
• (S3) The total academic load in a period is between a and b:
∀ni=1 ((a ≤ wsum(Si , ht1 , . . . , tm i)) ∧ (wsum(Si , ht1 , . . . , tm i) ≤ b))
• (S4) Prerequisites
are respected:
V
/ Sj ))
∀ni=1 ∀ij=1 ( hc,pi∈R (p ∈ Si ) → (c ∈

149

Hawkins, Lagoon, & Stuckey

In general the partition and wsum constraints of the primal set model can be very
large, making domain propagation impractical.
In addition to presenting results for the primal model, Choi et al. (2003) demonstrated
that a substantial performance improvement can be obtained for this problem through the
use of redundant models together with channelling constraints.
We can obtain better results from a “dual” set model, where additional set variables
are introduced to model each course. We define set variables Xi (1 ≤ i ≤ m) representing
each course, such that if k ∈ Xi then course i is assigned to period k. We can then define
the following constraints:
n
• (CX) Channelling constraints: ∀m
i=1 ∀j=1 (i ∈ Sj ) ↔ (j ∈ Xi )

• (X1) Each course may be assigned to at most one period: ∀m
i=1 |Xi | = 1
• (X2) Prerequisites are respected: ∀ hi, ji ∈ R lexlt(Xj , Xi )
We define the dual model to consist of the constraints {S2, S3, CX, X1, X2}. Constraints
S1 and S4 are no longer required, since they are propagation redundant.
Unfortunately, while the dual model performs better than the primal set model, it is still
incapable of proving the optimality of the solutions to any of the problems. We can obtain
stronger propagation by introducing redundant global constraints on the total academic
load and number of courses of all academic periods, as suggested by Choi et al. (2003).
For each academic period j define two integer variables lj , representing the academic
load in period j and qj , representing the number of courses in period j. We can then define
the following constraints on these new integer variables:
• (CI1) Channeling constraints on li : ∀ni=1 wsum(Si , ht1 , . . . , tm i) = li
• (CI2) Channeling constraints on qi : ∀ni=1 |Si | = qi
• (I1) Range constraints on li : ∀ni=1 (a ≤ li ) ∧ (li ≤ b)
• (I2) Range constraints on qi : ∀ni=1 (c ≤ qi ) ∧ (qi ≤ d)
Pn
Pn
• (I3) All loads should be undertaken:
i=1 ti
i=1 lj =
Pn
• (I4) All courses must be taken:
i=1 qj = m

We then define the “hybrid dual” model consisting of the constraints {CX, X1, X2,
CI1, CI2, I1, I2, I3, I4}. Note that none of the original constraints S1–S4 remain. For
completeness it is also possible to define a “hybrid primal” model consisting of constraints
{S1, S4, CI1, CI2, I1, I2, I3, I4}, although as before the large ROBDD sizes make domain
propagation impractical.
Experimental results for the Balanced Academic Curriculum problem are shown in Table 9. The timing results for the Hybrid Dual model are comparable to the best results
obtained by Hnich et al. (2002) and Choi et al. (2003). The number of failures required to
solve the problem is orders of magnitude smaller than the best results presented in either
paper, emphasising the value of domain propagation in this case. These examples also show
a case where the LU+Lex solver is competitive and clearly the most robust solver over the
different models.
150

Solving Set Constraint Satisfaction Problems using ROBDDs

Table 9: Performance results for the Balanced Academic Curriculum Problem for the 8, 10
and 12 period problems. The first column contains the number of periods, the
second column contains the model type (HD=hybrid dual, D=dual, P=primal,
HP=hybrid primal), and the third column contains the maximum load per period
b. In all cases a sequential “smallest-element-in-set” labeling method was used

Problem

Bounds
Domain LU+R LU+Lex
time fails time fails time time fails
/s

HD

D
8
P

HP

HD

D
10
P

HP

HD

D
12
P

HP

/s

16
—
— 0.3
17
8.1 5586 1.7
18 35.7 30430 1.7
16
—
— —
17
7.7 5348 1.1
18 32.8 29781 1.2
16
—
— —
17
6.0 5353 —
18 16.3 29781 —
16
—
— —
17
6.8 5586 —
18 19.3 30431 —
13
—
— 0.3
14 17.2 11439 1.3
15
2.1 630 1.4
13
—
— —
14 26.8 21103 1.0
15
2.1 711 1.0
13
—
— —
14 14.7 21105 —
15
3.5 711 —
13
—
— —
14 11.8 11440 —
15
3.6 630 —
16
—
— 1.1
17
—
— 6.0
18
6.1 235 8.1
16
—
— —
17
—
— —
18
6.3 220 6.8
16
—
— —
17
—
— —
18 101.7 225 —
16
—
— —
17
—
— —
18 100.9 236 —

/s

0
3
5
—
3
5
—
—
—
—
—
—
0
2
1
—
2
1
—
—
—
—
—
—
0
9
4
—
—
4
—
—
—
—
—
—

151

/s

0.3
0.3
0
2.7
1.0
3
3.0
1.0
5
—
—
—
1.2
0.6
3
1.3
0.7
5
—
—
—
— 13.9 2046
— 53.1 26431
—
0.3
0
— 12.8 1510
— 69.8 26145
0.3
0.3
0
1.5
0.9
2
1.7
0.9
1
—
—
—
1.6
0.6
2
1.6
0.6
1
—
—
—
— 53.3 16275
—
9.6
593
—
0.4
0
— 25.6 4672
— 10.6 516
1.1
0.9
0
9.3
3.0
9
9.2
3.4
4
—
—
—
—
—
—
9.0
3.0
4
—
—
—
—
—
—
— 153.8 224
—
3.4
0
— 216.8 526
— 166.0 224

LU+Card
time fails
/s

—
—
2.3
3
2.3
5
—
—
1.7
3
1.6
5
—
—
32.7 4757
61.2 29628
—
—
40.5 5410
70.5 30141
—
—
2.6
2
2.6
1
—
—
2.4
2
2.1
1
—
—
52.1 11772
31.5 515
—
—
46.9 5773
31.7
533
—
—
—
—
15.7
4
—
—
—
—
13.1
4
—
—
—
—
—
—
—
—
—
—
—
—

Hawkins, Lagoon, & Stuckey

Steiner system S(2, 3, 15)
500

Domain
Bounds
Lex Bounds
Card Bounds

1500

Total BDD size

log(Total Domain Size)

400

300

200

Domain
Split
Bounds
Lex Bounds
Card Bounds

1000

500

100

0
0

100

200

300

0

100

Labelling step

200

300

Labelling step

Social Golfers problem 4-5-4

1000

Total BDD size

log(Total Domain Size)

Domain
Split
Bounds
Lex Bounds
Card Bounds

Domain
Bounds
Lex Bounds
Card Bounds

300

200

500

100

0
0

20

40

60

80

100

Labelling step

0

20

40

60

80

100

Labelling step

Figure 17: Comparison of total domain and total ROBDD sizes with labeling step for set
bounds, set domain, split set domain, set and lexicographic bounds, and set and
cardinality bounds solvers on the Steiner System S(2, 3, 15) and Social Golfers
problem 4-5-4. Note that the y axes of the total domain size graphs have a
logarithmic scale (base 2).

7.5 Comparing the Propagation Performance of the Solvers
It is instructive to compare the propagation performance of the various ROBDD-based
solvers graphically. Here we pick two small examples, namely the Steiner System S(2, 3, 15)
and the Social Golfers problem 4-5-4, using the default labeling for each problem, and graph
BDD and domain sizes against number of labeling steps and time.
152

Solving Set Constraint Satisfaction Problems using ROBDDs

500

log(Total Domain Size)

400

300

200

Domain
Split
Bounds
Lex Bounds
Card Bounds

300

log(Total Domain Size)

Domain
Split
Bounds
Lex Bounds
Card Bounds

200

100

100

0

0
1

2

0.2

Time/s

0.4

0.6

0.8

Time/s

Steiner system S(2, 3, 15)

Social Golfers problem 4-5-4

Figure 18: Comparison of total domain size with time for set bounds, set domain, split set
domain, set and lexicographic bounds, and set and cardinality bounds solvers
on the Steiner System S(2, 3, 15) and Social Golfers problem 4-5-4. Note that
the y axes have a logarithmic scale (base 2).

Figure 17 depicts how the total domain sizes and total ROBDD sizes (total number of
internal nodes) vary with each labeling step for each of the solvers for the Steiner System
S(2, 3, 15) and for the Social Golfers problem 4-5-4. In particular, observe that the domain
and split-domain propagators are the most effective at reducing the domain size and thus
restricting the search space, although maintaining domain consistency can be costly due to
the size of the ROBDDs required for representing arbitrary domains. This can be seen in
the ROBDD size against labeling step graph, most notably in the case of the Social Golfers
problem 4-5-4. The lexicographic bounds solver is the next most effective in domain size
reduction, and requires a smaller number of ROBDD nodes to store its bounds than the
domain or cardinality bounds solvers. The bounds solver is clearly the weakest in terms of
domain size reduction, and as opposed to the other solvers starts from a zero size domain
representation and builds slowly to ROBDDs representing the answer.
We are not only concerned with the strength of a propagator in the abstract, since
the efficiency of the solver as a whole is dependent on both the propagation and labeling
processes. Figure 18 depicts graphs of domain size against time for each of the propagators discussed in this paper. Here we see that the most effective propagator does not
necessarily lead to the most efficient set solver. Despite comparatively weak propagation,
the set and cardinality bounds solvers lead to the fastest reduction in domain size per unit
time. Nonetheless, the experimental timing results given above demonstrate that for harder
cases the additional cost of maintaining domain consistency or lexicographic bounds can be
justified through the consequent reduction in search space.
153

Hawkins, Lagoon, & Stuckey

Interestingly we can see from the graphs in Figure 18 that the initial domain reduction
before labeling, which is the gap from time 0 to where the line starts, can be a significant
part of the computation. In particular, one of the weaknesses of the lexicographic bounds
solver is the large time required to reach an initial fixpoint through lexicographic bounds
propagation. For practical applications it might be preferable to implement a hybrid solver
which utilises one of the other propagators to generate the initial domains, and uses the
lexicographic bounds propagator during labeling.

8. Conclusion
We have demonstrated that ROBDDs form a highly flexible platform for constructing set
constraint solvers. ROBDDs allow a compact representation of many set domains and
set constraints, making them an effective basis for an efficient set solver. Since ROBDDs
can represent arbitrary Boolean formulæ we can easily conjoin and existentially quantify
them, permitting the removal of intermediate variables and making the construction of
global constraints trivial. We have demonstrated how to efficiently enforce various levels of
consistency, including set domain, set bounds, cardinality bounds and lexicographic bounds
consistency. Finally, we have presented experimental results that demonstrate that the
ROBDD-based solver outperforms other common set solvers on a wide variety of standard
set constraint satisfaction problems.
No single set solver is uniformly better than the others. For many examples simple
bounds propagation is the best approach, while in other cases, particularly when we ask
for all solutions, domain consistency is preferable. There are also examples where lexicographic bounds or cardinality bounds are the best approach. The split-domain approach
is somewhat disappointing, since it appears that in many cases the overhead of the more
complicated calculation (Equation 6) is not rewarded in terms of smaller ROBDD sizes.
In the future we plan to explore a robust general set constraint solver that dynamically
chooses which level of consistency to maintain by examining how big the domain ROBDDs
are becoming as search progresses.

References
Andersen, H. R. (1998). An introduction to Binary Decision Diagrams. [Online, accessed
30 July 2004]. http://www.itu.dk/people/hra/notes-index.html.
Apt, K. R. (1999). The essence of constraint propagation. Theoretical Computer Science,
221 (1–2), 179–210.
Azevedo, F. (2002). Constraint Solving over Multi-valued Logics. Ph.D. thesis, Faculdade
de Ciências e Tecnologia, Universidade Nova de Lisboa.
Bagnara, R. (1996). A reactive implementation of Pos using ROBDDs. In Procs. of PLILP,
Vol. 1140 of LNCS, pp. 107–121. Springer.
Bessiere, C., Hebrard, E., Hnich, B., & Walsh, T. (2004). Disjoint, partition and intersection
constraints for set and multiset variables. In Wallace, M. (Ed.), Proceedings of the
154

Solving Set Constraint Satisfaction Problems using ROBDDs

10th International Conference on Principles and Practice of Constraint Programming,
Vol. 3258 of LNCS, pp. 138–152. Springer-Verlag.
Bryant, R. E. (1986). Graph-based algorithms for Boolean function manipulation. IEEE
Trans. Comput., 35 (8), 677–691.
Bryant, R. E. (1992). Symbolic Boolean manipulation with ordered binary-decision diagrams. ACM Comput. Surv., 24 (3), 293–318.
Choi, C., Lee, J., & Stuckey, P. J. (2003). Propagation redundancy in redundant modelling.
In Rossi, F. (Ed.), Proceedings of the 9th International Conference on Principles and
Practices of Constraint Programming, Vol. 2833 of LNCS, pp. 229–243. SpringerVerlag.
Gent, I. P., Walsh, T., & Selman, B. (2004). CSPLib: a problem library for constraints.
[Online, accessed 24 Jul 2004]. http://www.csplib.org/.
Gervet, C. (1997). Interval propagation to reason about sets: Definition and implementation
of a practical language. Constraints, 1 (3), 191–246.
Hawkins, P., Lagoon, V., & Stuckey, P. (2004). Set bounds and (split) set domain propagation using ROBDDs. In Webb, G., & Yu, X. (Eds.), AI 2004: Advances in Artificial
Intelligence, 17th Australian Joint Conference on Artificial Intelligence, Vol. 3339 of
LNCS, pp. 706–717. Springer-Verlag.
Hnich, B., Kiziltan, Z., & Walsh, T. (2002). Modelling a balanced academic curriculum
problem. In Proceedings of the Fourth International Workshop on Integration of AI
and OR Techniques in Constraint Programming for Combinatorial Optimization Problems, pp. 121–131.
IC-PARC (2003). The ECLiPSe constraint logic programming system. [Online, accessed 31
May 2004]. http://www.icparc.ic.ac.uk/eclipse/.
ILOG (2004). ILOG Solver. [Online, accessed 30 Aug 2004]. http://www.ilog.com/.
Kiziltan, Z., & Walsh, T. (2002). Constraint programming with multisets. In Proceedings
of the 2nd International Workshop on Symmetry in Constraint Satisfaction Problems
(SymCon-02).
Lagoon, V., & Stuckey, P. (2004). Set domain propagation using ROBDDs. In Wallace, M.
(Ed.), Proceedings of the 10th International Conference on Principles and Practice of
Constraint Programming, Vol. 3258 of LNCS, pp. 347–361. Springer-Verlag.
van Lint, J. H., & Wilson, R. M. (2001). A Course in Combinatorics (2nd edition). Cambridge University Press.
Mailharro, D. (1998). A classification and constraint-based framework for configuration.
Artificial Intelligence for Engineering Design, Analysis and Manufacturing, 12, 383–
397.
Müller, T. (2001). Constraint Propagation in Mozart. Doctoral dissertation, Universität des
Saarlandes, Naturwissenschaftlich-Technische Fakultät I, Fachrichtung Informatik,
Saarbrücken, Germany.
155

Hawkins, Lagoon, & Stuckey

Müller, T., & Müller, M. (1997). Finite set constraints in Oz. In Bry, F., Freitag, B., &
Seipel, D. (Eds.), Workshop Logische Programmierung, Vol. 13. Technische Universität
München.
Puget, J.-F. (1992). PECOS: a high level constraint programming language. In Proceedings
of SPICIS’92, Singapore.
Sadler, A., & Gervet, C. (2001). Global reasoning on sets. In FORMUL’01 workshop on
modelling and problem formulation, in conjunction with CP’01.
Sadler, A., & Gervet, C. (2004). Hybrid set domains to strengthen constraint propagation
and reduce symmetries. In Wallace, M. (Ed.), Proceedings of the 10th International
Conference on Principles and Practice of Constraint Programming, Vol. 3258 of LNCS,
pp. 604–618. Springer-Verlag.
Somenzi, F. (2004). CUDD: Colorado University Decision Diagram package. [Online, accessed 31 May 2004]. http://vlsi.colorado.edu/~fabio/CUDD/.
Somogyi, Z., Henderson, F., & Conway, T. (1996). The execution algorithm of Mercury, an
efficient purely declarative logic programming language. Journal of Logic Programming, 29 (1–3), 17–64.

156

Journal of Artificial Intelligence Research 24 (2005) 49-79

Submitted 09/04; published 07/05

A Framework for Sequential Planning in Multi-Agent Settings
Piotr J. Gmytrasiewicz
Prashant Doshi

PIOTR @ CS . UIC . EDU
PDOSHI @ CS . UIC . EDU

Department of Computer Science
University of Illinois at Chicago
851 S. Morgan St
Chicago, IL 60607

Abstract
This paper extends the framework of partially observable Markov decision processes (POMDPs)
to multi-agent settings by incorporating the notion of agent models into the state space. Agents
maintain beliefs over physical states of the environment and over models of other agents, and they
use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions.
Models of other agents may include their belief states and are related to agent types considered in
games of incomplete information. We express the agents’ autonomy by postulating that their models are not directly manipulable or observable by other agents. We show that important properties
of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework. Our approach complements a
more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm.
We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture
off-equilibrium behaviors. We do so at the cost of having to represent, process and continuously
revise models of other agents. Since the agent’s beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable. However, approximate
belief updates and approximately optimal plans are computable. We illustrate our framework using
a simple application domain, and we show examples of belief updates and value functions.

1. Introduction
We develop a framework for sequential rationality of autonomous agents interacting with other
agents within a common, and possibly uncertain, environment. We use the normative paradigm of
decision-theoretic planning under uncertainty formalized as partially observable Markov decision
processes (POMDPs) (Boutilier, Dean, & Hanks, 1999; Kaelbling, Littman, & Cassandra, 1998;
Russell & Norvig, 2003) as a point of departure. Solutions of POMDPs are mappings from an
agent’s beliefs to actions. The drawback of POMDPs when it comes to environments populated by
other agents is that other agents’ actions have to be represented implicitly as environmental noise
within the, usually static, transition model. Thus, an agent’s beliefs about another agent are not part
of solutions to POMDPs.
The main idea behind our formalism, called interactive POMDPs (I-POMDPs), is to allow
agents to use more sophisticated constructs to model and predict behavior of other agents. Thus,
we replace “flat” beliefs about the state space used in POMDPs with beliefs about the physical
environment and about the other agent(s), possibly in terms of their preferences, capabilities, and
beliefs. Such beliefs could include others’ beliefs about others, and thus can be nested to arbitrary
levels. They are called interactive beliefs. While the space of interactive beliefs is very rich and
updating these beliefs is more complex than updating their “flat” counterparts, we use the value
c
2005
AI Access Foundation. All rights reserved.

G MYTRASIEWICZ & D OSHI

function plots to show that solutions to I-POMDPs are at least as good as, and in usual cases superior
to, comparable solutions to POMDPs. The reason is intuitive – maintaining sophisticated models of
other agents allows more refined analysis of their behavior and better predictions of their actions.
I-POMDPs are applicable to autonomous self-interested agents who locally compute what actions they should execute to optimize their preferences given what they believe while interacting
with others with possibly conflicting objectives. Our approach of using a decision-theoretic framework and solution concept complements the equilibrium approach to analyzing interactions as used
in classical game theory (Fudenberg & Tirole, 1991). The drawback of equilibria is that there could
be many of them (non-uniqueness), and that they describe agent’s optimal actions only if, and when,
an equilibrium has been reached (incompleteness). Our approach, instead, is centered on optimality
and best response to anticipated action of other agent(s), rather then on stability (Binmore, 1990;
Kadane & Larkey, 1982). The question of whether, under what circumstances, and what kind of
equilibria could arise from solutions to I-POMDPs is currently open.
Our approach avoids the difficulties of non-uniqueness and incompleteness of traditional equilibrium approach, and offers solutions which are likely to be better than the solutions of traditional
POMDPs applied to multi-agent settings. But these advantages come at the cost of processing and
maintaining possibly infinitely nested interactive beliefs. Consequently, only approximate belief
updates and approximately optimal solutions to planning problems are computable in general. We
define a class of finitely nested I-POMDPs to form a basis for computable approximations to infinitely nested ones. We show that a number of properties that facilitate solutions of POMDPs carry
over to finitely nested I-POMDPs. In particular, the interactive beliefs are sufficient statistics for the
histories of agent’s observations, the belief update is a generalization of the update in POMDPs, the
value function is piece-wise linear and convex, and the value iteration algorithm converges at the
same rate.
The remainder of this paper is structured as follows. We start with a brief review of related
work in Section 2, followed by an overview of partially observable Markov decision processes in
Section 3. There, we include a simple example of a tiger game. We introduce the concept of
agent types in Section 4. Section 5 introduces interactive POMDPs and defines their solutions. The
finitely nested I-POMDPs, and some of their properties are introduced in Section 6. We continue
with an example application of finitely nested I-POMDPs to a multi-agent version of the tiger game
in Section 7. There, we show examples of belief updates and value functions. We conclude with
a brief summary and some current research issues in Section 8. Details of all proofs are in the
Appendix.

2. Related Work
Our work draws from prior research on partially observable Markov decision processes, which
recently gained a lot of attention within the AI community (Smallwood & Sondik, 1973; Monahan,
1982; Lovejoy, 1991; Hausktecht, 1997; Kaelbling et al., 1998; Boutilier et al., 1999; Hauskrecht,
2000).
The formalism of Markov decision processes has been extended to multiple agents giving rise to
stochastic games or Markov games (Fudenberg & Tirole, 1991). Traditionally, the solution concept
used for stochastic games is that of Nash equilibria. Some recent work in AI follows that tradition
(Littman, 1994; Hu & Wellman, 1998; Boutilier, 1999; Koller & Milch, 2001). However, as we
mentioned before, and as has been pointed out by some game theorists (Binmore, 1990; Kadane &
50

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Larkey, 1982), while Nash equilibria are useful for describing a multi-agent system when, and if,
it has reached a stable state, this solution concept is not sufficient as a general control paradigm.
The main reasons are that there may be multiple equilibria with no clear way to choose among them
(non-uniqueness), and the fact that equilibria do not specify actions in cases in which agents believe
that other agents may not act according to their equilibrium strategies (incompleteness).
Other extensions of POMDPs to multiple agents appeared in AI literature recently (Bernstein,
Givan, Immerman, & Zilberstein, 2002; Nair, Pynadath, Yokoo, Tambe, & Marsella, 2003). They
have been called decentralized POMDPs (DEC-POMDPs), and are related to decentralized control
problems (Ooi & Wornell, 1996). DEC-POMDP framework assumes that the agents are fully cooperative, i.e., they have common reward function and form a team. Furthermore, it is assumed that
the optimal joint solution is computed centrally and then distributed among the agents for execution.
From the game-theoretic side, we are motivated by the subjective approach to probability in
games (Kadane & Larkey, 1982), Bayesian games of incomplete information (see Fudenberg &
Tirole, 1991; Harsanyi, 1967, and references therein), work on interactive belief systems (Harsanyi,
1967; Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Fagin, Halpern, Moses, & Vardi,
1995; Aumann, 1999; Fagin, Geanakoplos, Halpern, & Vardi, 1999), and insights from research on
learning in game theory (Fudenberg & Levine, 1998). Our approach, closely related to decisiontheoretic (Myerson, 1991), or epistemic (Ambruster & Boge, 1979; Battigalli & Siniscalchi, 1999;
Brandenburger, 2002) approach to game theory, consists of predicting actions of other agents given
all available information, and then of choosing the agent’s own action (Kadane & Larkey, 1982).
Thus, the descriptive aspect of decision theory is used to predict others’ actions, and its prescriptive
aspect is used to select agent’s own optimal action.
The work presented here also extends previous work on Recursive Modeling Method (RMM)
(Gmytrasiewicz & Durfee, 2000), but adds elements of belief update and sequential planning.

3. Background: Partially Observable Markov Decision Processes
A partially observable Markov decision process (POMDP) (Monahan, 1982; Hausktecht, 1997;
Kaelbling et al., 1998; Boutilier et al., 1999; Hauskrecht, 2000) of an agent i is defined as
POMDPi = hS, Ai , Ti , Ωi , Oi , Ri i

(1)

where: S is a set of possible states of the environment. Ai is a set of actions agent i can execute. Ti is
a transition function – Ti : S ×Ai ×S → [0, 1] which describes results of agent i’s actions. Ωi is the
set of observations the agent i can make. Oi is the agent’s observation function – Oi : S ×Ai ×Ωi →
[0, 1] which specifies probabilities of observations given agent’s actions and resulting states. Finally,
Ri is the reward function representing the agent i’s preferences – R i : S × Ai → <.
In POMDPs, an agent’s belief about the state is represented as a probability distribution over S.
Initially, before any observations or actions take place, the agent has some (prior) belief, b 0i . After
some time steps, t, we assume that the agent has t + 1 observations and has performed t actions 1 .
t
These can be assembled into agent i’s observation history: h ti = {o0i , o1i , .., ot−1
i , oi } at time t. Let
Hi denote the set of all observation histories of agent i. The agent’s current belief, b ti over S, is
continuously revised based on new observations and expected results of performed actions. It turns
1. We assume that action is taken at every time step; it is without loss of generality since any of the actions maybe a
No-op.

51

G MYTRASIEWICZ & D OSHI

out that the agent’s belief state is sufficient to summarize all of the past observation history and
initial belief; hence it is called a sufficient statistic.2
t−1
The belief update takes into account changes in initial belief, b t−1
i , due to action, ai , executed
t
t
at time t − 1, and the new observation, oi . The new belief, bi , that the current state is st , is:
bti (st ) = βOi (oti , st , at−1
i )

X

bit−1 (st−1 )Ti (st , ati , st−1 )

(2)

st−1 ∈S

where β is the normalizing constant.
It is convenient to summarize the above update performed for all states in S as
t
bti = SE(bit−1 , at−1
i , oi ) (Kaelbling et al., 1998).
3.1 Optimality Criteria and Solutions
The agent’s optimality criterion, OCi , is needed to specify how rewards acquired over time are
handled. Commonly used criteria include:
• A finite horizon criterion,P
in which the agent maximizes the expected value of the sum of the
following T rewards: E( Tt=0 rt ). Here, rt is a reward obtained at time t and T is the length
of the horizon. We will denote this criterion as fhT .
• AnP
infinite horizon criterion with discounting, according to which the agent maximizes
γ
t
E( ∞
t=0 γ rt ), where 0 < γ < 1 is a discount factor. We will denote this criterion as ih .

• An infinite horizon criterion with averaging, according to which the agent maximizes the
average reward per time step. We will denote this as ihAV .

In what follows, we concentrate on the infinite horizon criterion with discounting, but our approach can be easily adapted to the other criteria.
The utility associated with a belief state, bi is composed of the best of the immediate rewards
that can be obtained in bi , together with the discounted expected sum of utilities associated with
belief states following bi :

U (bi ) = max

ai ∈Ai

X

bi (s)Ri (s, ai ) + γ

X

P r(oi |ai , bi )U (SEi (bi , ai , oi ))

oi ∈Ωi

s∈S



(3)

Value iteration uses the Equation 3 iteratively to obtain values of belief states for longer time
horizons. At each step of the value iteration the error of the current value estimate is reduced by the
factor of at least γ (see for example Russell & Norvig, 2003, Section 17.2.) The optimal action, a ∗i ,
is then an element of the set of optimal actions, OP T (bi ), for the belief state, defined as:

OP T (bi ) = argmax
ai ∈Ai

X

bi (s)Ri (s, ai ) + γ

X

oi ∈Ωi

s∈S

2. See (Smallwood & Sondik, 1973) for proof.

52

P r(oi |ai , bi )U (SE(bi , ai , oi ))



(4)

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

L
OR

OL

10

Value Function(U)

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
POMDP with noise

POMDP

Figure 1: The value function for single agent tiger game with time horizon of length 1, OC i = fh1 .
Actions are: open right door - OR, open left door - OL, and listen - L. For this value of
the time horizon the value function for a POMDP with noise factor is identical to single
agent POMDP.

3.2 Example: The Tiger Game
We briefly review the POMDP solutions to the tiger game (Kaelbling et al., 1998). Our purpose is
to build on the insights that POMDP solutions provide in this simple case to illustrate solutions to
interactive versions of this game later.
The traditional tiger game resembles a game-show situation in which the decision maker has
to choose to open one of two doors behind which lies either a valuable prize or a dangerous tiger.
Apart from actions that open doors, the subject has the option of listening for the tiger’s growl
coming from the left, or the right, door. However, the subject’s hearing is imperfect, with given
percentages (say, 15%) of false positive and false negative occurrences. Following (Kaelbling et al.,
1998), we assume that the value of the prize is 10, that the pain associated with encountering the
tiger can be quantified as -100, and that the cost of listening is -1.
The value function, in Figure 1, shows values of various belief states when the agent’s time
horizon is equal to 1. Values of beliefs are based on best action available in that belief state, as
specified in Eq. 3. The state of certainty is most valuable – when the agent knows the location of
the tiger it can open the opposite door and claim the prize which certainly awaits. Thus, when the
probability of tiger location is 0 or 1, the value is 10. When the agent is sufficiently uncertain, its
best option is to play it safe and listen; the value is then -1. The agent is indifferent between opening
doors and listening when it assigns probabilities of 0.9 or 0.1 to the location of the tiger.
Note that, when the time horizon is equal to 1, listening does not provide any useful information
since the game does not continue to allow for the use of this information. For longer time horizons
the benefits of results of listening results in policies which are better in some ranges of initial belief.
Since the value function is composed of values corresponding to actions, which are linear in prob53

G MYTRASIEWICZ & D OSHI

L\();L\(GL),OL\(GR)

L\();L\(*)

L\();OR\(GL),L\(GR)
L\();OR\(*)
OR\();L\(*)

L\();OL\(*)
OL\();L\(*)

8

Value Function(U)

6

4

2

0

-2
0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
POMDP with noise

POMDP

Figure 2: The value function for single agent tiger game compared to an agent facing a noise factor, for horizon of length 2. Policies corresponding to value lines are conditional plans.
Actions, L, OR or OL, are conditioned on observational sequences in parenthesis. For
example L\();L\(GL),OL\(GR) denotes a plan to perform the listening action, L, at the
beginning (list of observations is empty), and then another L if the observation is growl
from the left (GL), and open the left door, OL, if the observation is GR. ∗ is a wildcard
with the usual interpretation.

ability of tiger location, the value function has the property of being piece-wise linear and convex
(PWLC) for all horizons. This simplifies the computations substantially.
In Figure 2 we present a comparison of value functions for horizon of length 2 for a single
agent, and for an agent facing a more noisy environment. The presence of such noise could be
due to another agent opening the doors or listening with some probabilities. 3 Since POMDPs do
not include explicit models of other agents, these noise actions have been included in the transition
model, T .
Consequences of folding noise into T are two-fold. First, the effectiveness of the agent’s optimal
policies declines since the value of hearing growls diminishes over many time steps. Figure 3 depicts
a comparison of value functions for horizon of length 3. Here, for example, two consecutive growls
in a noisy environment are not as valuable as when the agent knows it is acting alone since the noise
may have perturbed the state of the system between the growls. For time horizon of length 1 the
noise does not matter and the value vectors overlap, as in Figure 1.
Second, since the presence of another agent is implicit in the static transition model, the agent
cannot update its model of the other agent’s actions during repeated interactions. This effect becomes more important as time horizon increases. Our approach addresses this issue by allowing
explicit modeling of the other agent(s). This results in policies of superior quality, as we show in
Section 7. Figure 4 shows a policy for an agent facing a noisy environment for time horizon of 3.
We compare it to the corresponding I-POMDP policy in Section 7. Note that it is slightly different
3. We assumed that, due to the noise, either door opens with probabilities of 0.1 at each turn, and nothing happens with
the probability 0.8. We explain the origin of this assumption in Section 7.

54

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

L\();L\(*);OL\(GR;GR),L\(?)
L\();L\(GL),OL\(GR);OL\(GL;GR),L\(?)

L\();L\(*);OR\(GL;GL),L\(?)
L\();OR\(GL),L\(GR);OR\(GR;GL),L\(?)
L\();L\(*);OR\(GL;GL),OL\(GR;GR),L\(?)
OR\();L\(*);L\(*)
L\();L\(*);OR\(*)

OL\();L\(*);L\(*)
L\();L\(*);OL\(*)
8

Value Function(U)

7
6
5
4
3
2
1
0

0.2

0.4

0.6

0.8

1

p (TL)
p_i(TL)
i

POMDP with noise

POMDP

Figure 3: The value function for single agent tiger game compared to an agent facing a noise factor,
for horizon of length 3. The “?” in the description of a policy stands for any of the
perceptual sequences not yet listed in the description of the policy.

[0−0.045)
OL
*

[0.045−0.135) [0.135−0.175) [0.175−0.825)
L

L

GR

GL

GR

L
GR

GL

*

OL

L
GR

L
GL

GR

OR
GL

*

GL

L
GL

[0.865−0.955) [0.955−1]

L

GR

OL

[0.825−0.865)

L
*

L

GR

OR
GL

*

OR

Figure 4: The policy graph corresponding to value function of POMDP with noise depicted in
Fig. 3.

55

G MYTRASIEWICZ & D OSHI

than the policy without noise in the example by Kaelbling, Littman and Cassandra (1998) due to
differences in value functions.

4. Agent Types and Frames
The POMDP definition includes parameters that permit us to compute an agent’s optimal behavior, 4
conditioned on its beliefs. Let us collect these implementation independent factors into a construct
we call an agent i’s type.
Definition 1 (Type). A type of an agent i is, θi = hbi , Ai , Ωi , Ti , Oi , Ri , OCi i, where bi is agent i’s
state of belief (an element of ∆(S)), OCi is its optimality criterion, and the rest of the elements are
as defined before. Let Θi be the set of agent i’s types.
Given type, θi , and the assumption that the agent is Bayesian-rational, the set of agent’s optimal
actions will be denoted as OP T (θi ). In the next section, we generalize the notion of type to situations which include interactions with other agents; it then coincides with the notion of type used in
Bayesian games (Fudenberg & Tirole, 1991; Harsanyi, 1967).
It is convenient to define the notion of a frame, θbi , of agent i:

b i be the set of
Definition 2 (Frame). A frame of an agent i is, θbi = hAi , Ωi , Ti , Oi , Ri , OCi i. Let Θ
agent i’s frames.

For brevity one can write a type as consisting of an agent’s belief together with its frame: θ i =
hbi , θbi i.
In the context of the tiger game described in the previous section, agent type describes the
agent’s actions and their results, the quality of the agent’s hearing, its payoffs, and its belief about
the tiger location.
Realistically, apart from implementation-independent factors grouped in type, an agent’s behavior may also depend on implementation-specific parameters, like the processor speed, memory
available, etc. These can be included in the (implementation dependent, or complete) type, increasing the accuracy of predicted behavior, but at the cost of additional complexity. Definition and use
of complete types is a topic of ongoing work.

5. Interactive POMDPs
As we mentioned, our intention is to generalize POMDPs to handle presence of other agents. We
do this by including descriptions of other agents (their types for example) in the state space. For
simplicity of presentation, we consider an agent i, that is interacting with one other agent, j. The
formalism easily generalizes to larger number of agents.
Definition 3 (I-POMDP). An interactive POMDP of agent i, I-POMDPi , is:
I-POMDPi = hISi , A, Ti , Ωi , Oi , Ri i

(5)

4. The issue of computability of solutions to POMDPs has been a subject of much research (Papadimitriou & Tsitsiklis,
1987; Madani, Hanks, & Condon, 2003). It is of obvious importance when one uses POMDPs to model agents; we
return to this issue later.

56

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

where:
• ISi is a set of interactive states defined as ISi = S × Mj ,5 interacting with agent i, where
S is the set of states of the physical environment, and Mj is the set of possible models of agent
j. Each model, mj ∈ Mj , is defined as a triple mj = hhj , fj , Oj i, where fj : Hj → ∆(Aj )
is agent j’s function, assumed computable, which maps possible histories of j’s observations to
distributions over its actions. hj is an element of Hj , and Oj is a function specifying the way the
environment is supplying the agent with its input. Sometimes we write model m j as mj = hhj , m
b j i,
where m
b j consists of fj and Oj . It is convenient to subdivide the set of models into two classes.
The subintentional models, SMj , are relatively simple, while the intentional models, IMj , use the
notion of rationality to model the other agent. Thus, Mj = IMj ∪ SMj .
Simple examples of subintentional models include a no-information model and a fictitious play
model, both of which are history independent. A no-information model (Gmytrasiewicz & Durfee,
2000) assumes that each of the other agent’s actions is executed with equal probability. Fictitious
play (Fudenberg & Levine, 1998) assumes that the other agent chooses actions according to a fixed
but unknown distribution, and that the original agent’s prior belief over that distribution takes a form
of a Dirichlet distribution.6 An example of a more powerful subintentional model is a finite state
controller.
The intentional models are more sophisticated in that they ascribe to the other agent beliefs,
preferences and rationality in action selection.7 Intentional models are thus j’s types, θj = hbj , θbj i,
under the assumption that agent j is Bayesian-rational.8 Agent j’s belief is a probability distribution
over states of the environment and the models of the agent i; b j ∈ ∆(S × Mi ). The notion of a type
we use here coincides with the notion of type in game theory, where it is defined as consisting of
all of the agent i’s private information relevant to its decision making (Harsanyi, 1967; Fudenberg
& Tirole, 1991). In particular, if agents’ beliefs are private information, then their types involve
possibly infinitely nested beliefs over others’ types and their beliefs about others (Mertens & Zamir,
1985; Brandenburger & Dekel, 1993; Aumann, 1999; Aumann & Heifetz, 2002). 9 They are related
to recursive model structures in our prior work (Gmytrasiewicz & Durfee, 2000). The definition of
interactive state space is consistent with the notion of a completely specified state space put forward
by Aumann (1999). Similar state spaces have been proposed by others (Mertens & Zamir, 1985;
Brandenburger & Dekel, 1993).
• A = Ai × Aj is the set of joint moves of all agents.
• Ti is the transition model. The usual way to define the transition probabilities in POMDPs
is to assume that the agent’s actions can change any aspect of the state description. In case of IPOMDPs, this would mean actions modifying any aspect of the interactive states, including other
agents’ observation histories and their functions, or, if they are modeled intentionally, their beliefs
and reward functions. Allowing agents to directly manipulate other agents in such ways, however,
violates the notion of agents’ autonomy. Thus, we make the following simplifying assumption:
−1
5. If there are more agents, say N > 2, then ISi = S ×N
j=1 Mj
6. Technically, according to our notation, fictitious play is actually an ensemble of models.
7. Dennet (1986) advocates ascribing rationality to other agent(s), and calls it ”assuming an intentional stance towards
them”.
8. Note that the space of types is by far richer than that of computable models. In particular, since the set of computable
models is countable and the set of types is uncountable, many types are not computable models.
9. Implicit in the definition of interactive beliefs is the assumption of coherency (Brandenburger & Dekel, 1993).

57

G MYTRASIEWICZ & D OSHI

Model Non-manipulability Assumption (MNM): Agents’ actions do not change the other
agents’ models directly.
Given this simplification, the transition model can be defined as T i : S × A × S → [0, 1]
Autonomy, formalized by the MNM assumption, precludes, for example, direct “mind control”,
and implies that other agents’ belief states can be changed only indirectly, typically by changing the
environment in a way observable to them. In other words, agents’ beliefs change, like in POMDPs,
but as a result of belief update after an observation, not as a direct result of any of the agents’
actions.10
• Ωi is defined as before in the POMDP model.
• Oi is an observation function. In defining this function we make the following assumption:
Model Non-observability (MNO): Agents cannot observe other’s models directly.
Given this assumption the observation function is defined as O i : S × A × Ωi → [0, 1].
The MNO assumption formalizes another aspect of autonomy – agents are autonomous in that
their observations and functions, or beliefs and other properties, say preferences, in intentional
models, are private and the other agents cannot observe them directly. 11
• Ri is defined as Ri : ISi × A → <. We allow the agent to have preferences over physical
states and models of other agents, but usually only the physical state will matter.
As we mentioned, we see interactive POMDPs as a subjective counterpart to an objective external view in stochastic games (Fudenberg & Tirole, 1991), and also followed in some work in
AI (Boutilier, 1999) and (Koller & Milch, 2001) and in decentralized POMDPs (Bernstein et al.,
2002; Nair et al., 2003). Interactive POMDPs represent an individual agent’s point of view on the
environment and the other agents, and facilitate planning and problem solving at the agent’s own
individual level.
5.1 Belief Update in I-POMDPs
We will show that, as in POMDPs, an agent’s beliefs over their interactive states are sufficient
statistics, i.e., they fully summarize the agent’s observation histories. Further, we need to show how
beliefs are updated after the agent’s action and observation, and how solutions are defined.
t−1
The new belief state, bti , is a function of the previous belief state, bt−1
i , the last action, ai ,
t
and the new observation, oi , just as in POMDPs. There are two differences that complicate belief
update when compared to POMDPs. First, since the state of the physical environment depends on
the actions performed by both agents the prediction of how the physical state changes has to be
made based on the probabilities of various actions of the other agent. The probabilities of other’s
actions are obtained based on their models. Thus, unlike in Bayesian and stochastic games, we do
not assume that actions are fully observable by other agents. Rather, agents can attempt to infer what
actions other agents have performed by sensing their results on the environment. Second, changes in
the models of other agents have to be included in the update. These reflect the other’s observations
and, if they are modeled intentionally, the update of the other agent’s beliefs. In this case, the agent
has to update its beliefs about the other agent based on what it anticipates the other agent observes
10. The possibility that agents can influence the observational capabilities of other agents can be accommodated by
including the factors that can change sensing capabilities in the set S.
11. Again, the possibility that agents can observe factors that may influence the observational capabilities of other agents
is allowed by including these factors in S.

58

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

and how it updates. As could be expected, the update of the possibly infinitely nested belief over
other’s types is, in general, only asymptotically computable.
Proposition 1. (Sufficiency) In an interactive POMDP of agent i, i’s current belief, i.e., the probability distribution over the set S × Mj , is a sufficient statistic for the past history of i’s observations.
t−1
The next proposition defines the agent i’s belief update function, b ti (ist ) = P r(ist |oti , at−1
i , bi ),
where ist ∈ ISi is an interactive state. We use the belief state estimation function, SE θi , as an abt−1 t
breviation for belief updates for individual states so that bti = SEθi (bt−1
i , ai , oi ).
t−1 t−1 t t
t−1
t−1
τθi (bi , ai , oi , bi ) will stand for P r(bti |bi , ai , oti ). Further below we also define the set of
type-dependent optimal actions of an agent, OP T (θi ).

Proposition 2. (Belief Update) Under the MNM and MNO assumptions, the belief update function
for an interactive POMDP hISi , A, Ti , Ωi , Oi , Ri i, when mj in ist is intentional, is:
bti (ist ) = β
×Ti

P

t−1 )
bt−1
i (is

ist−1 :m
b t−1
=θbjt
j
P
t−1
t−1
(s , a , st ) τ
otj

P

t−1
t t−1 , ot )
P r(at−1
i
j |θj )Oi (s , a

at−1
j
t−1 t−1 t t
t t−1 , ot )
t
(b
θj j , aj , oj , bj )Oj (s , a
j

(6)

=m
b tj ,
When mj in ist is subintentional the first summation extends over ist−1 : m
b t−1
j
t−1
t−1
t−1
t−1 t−1 t t
P r(at−1
j |θj ) is replaced with P r(aj |mj ), and τθjt (bj , aj , oj , bj ) is replaced with the
t
t
Kronecker delta function δK (APPEND(ht−1
j , oj ), hj ).

Above, bt−1
and btj are the belief elements of θjt−1 and θjt , respectively, β is a normalizing constant,
j
t−1
t−1
and P r(at−1
is Bayesian rational for agent described by type
j |θj ) is the probability that aj
t−1
t−1
1
θj . This probability is equal to |OP T (θj )| if aj ∈ OP T (θj ), and it is equal to zero otherwise.
We define OP T in Section 5.2.12 For the case of j’s subintentional model, is = (s, mj ), ht−1
and
j
t respectively, O is the observation
htj are the observation histories which are part of mt−1
,
and
m
j
j
j
t−1
t−1
t−1
function in mtj , and P r(at−1
|m
)
is
the
probability
assigned
by
m
to
a
j
j
j
j . APPEND returns
a string with the second argument appended to the first. The proofs of the propositions are in the
Appendix.
Proposition 2 and Eq. 6 have a lot in common with belief update in POMDPs, as should be
expected. Both depend on agent i’s observation and transition functions. However, since agent i’s
observations also depend on agent j’s actions, the probabilities of various actions of j have to be
included (in the first line of Eq. 6.) Further, since the update of agent j’s model depends on what
j observes, the probabilities of various observations of j have to be included (in the second line of
Eq. 6.) The update of j’s beliefs is represented by the τθj term. The belief update can easily be
generalized to the setting where more than one other agents co-exist with agent i.
P
12. If the agent’s prior belief over ISi is given by a probability density function then the
ist−1 is replaced by
:
, otj , btj ) takes the form of Dirac delta function over argument bt−1
, at−1
an integral. In that case τθjt (bt−1
j
j
j
, otj ) − btj ).
, at−1
δD (SEθjt (bt−1
j
j

59

G MYTRASIEWICZ & D OSHI

5.2 Value Function and Solutions in I-POMDPs
Analogously to POMDPs, each belief state in I-POMDP has an associated value reflecting the maximum payoff the agent can expect in this belief state:


P
P
b
ERi (is, ai )bi (is) + γ
U (θi ) = max
P r(oi |ai , bi )U (hSEθi (bi , ai , oi ), θi i)
(7)
ai ∈Ai

oi ∈Ωi

is

P
where, ERi (is, ai ) =
aj Ri (is, ai , aj )P r(aj |mj ). Eq. 7 is a basis for value iteration in IPOMDPs.
Agent i’s optimal action, a∗i , for the case of infinite horizon criterion with discounting, is an
element of the set of optimal actions for the belief state, OP T (θi ), defined as:


P
P
OP T (θi ) = argmax
ERi (is, ai )bi (is) + γ
P r(oi |ai , bi )U (hSEθi (bi , ai , oi ), θbi i)
ai ∈Ai

oi ∈Ωi

is

(8)
As in the case of belief update, due to possibly infinitely nested beliefs, a step of value iteration
and optimal actions are only asymptotically computable.

6. Finitely Nested I-POMDPs
Possible infinite nesting of agents’ beliefs in intentional models presents an obvious obstacle to
computing the belief updates and optimal solutions. Since the models of agents with infinitely
nested beliefs correspond to agent functions which are not computable it is natural to consider
finite nestings. We follow approaches in game theory (Aumann, 1999; Brandenburger & Dekel,
1993; Fagin et al., 1999), extend our previous work (Gmytrasiewicz & Durfee, 2000), and construct
finitely nested I-POMDPs bottom-up. Assume a set of physical states of the world S, and two
agents i and j. Agent i’s 0-th level beliefs, bi,0 , are probability distributions over S. Its 0-th level
types, Θi,0 , contain its 0-th level beliefs, and its frames, and analogously for agent j. 0-level types
are, therefore, POMDPs.13 0-level models include 0-level types (i.e., intentional models) and the
subintentional models, elements of SM . An agent’s first level beliefs are probability distributions
over physical states and 0-level models of the other agent. An agent’s first level types consist of
its first level beliefs and frames. Its first level models consist of the types upto level 1 and the
subintentional models. Second level beliefs are defined in terms of first level models and so on.
Formally, define spaces:
ISi,0 = S,
Θj,0 = {hbj,0 , θbj i : bj,0 ∈ ∆(ISj,0 )}, Mj,0 = Θj,0 ∪ SMj
ISi,1 = S × Mj,0 ,
Θj,1 = {hbj,1 , θbj i : bj,1 ∈ ∆(ISj,1 )}, Mj,1 = Θj,1 ∪ Mj,0
.
.
.
.
.
.
ISi,l = S × Mj,l−1 , Θj,l = {hbj,l , θbj i : bj,l ∈ ∆(ISj,l )}, Mj,l = Θj,l ∪ Mj,l−1
Definition 4. (Finitely Nested I-POMDP) A finitely nested I-POMDP of agent i, I-POMDP i,l , is:
I-POMDPi,l = hISi,l , A, Ti , Ωi , Oi , Ri i
13. In 0-level types the other agent’s actions are folded into the T , O and R functions as noise.

60

(9)

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

The parameter l will be called the strategy level of the finitely nested I-POMDP. The belief update,
value function, and the optimal actions for finitely nested I-POMDPs are computed using Equation 6
and Equation 8, but recursion is guaranteed to terminate at 0-th level and subintentional models.
Agents which are more strategic are capable of modeling others at deeper levels (i.e., all levels
up to their own strategy level l), but are always only boundedly optimal. As such, these agents
could fail to predict the strategy of a more sophisticated opponent. The fact that the computability
of an agent function implies that the agent may be suboptimal during interactions has been pointed
out by Binmore (1990), and proved more recently by Nachbar and Zame (1996). Intuitively, the
difficulty is that an agent’s unbounded optimality would have to include the capability to model the
other agent’s modeling the original agent. This leads to an impossibility result due to self-reference,
which is very similar to Gödel’s incompleteness theorem and the halting problem (Brandenburger,
2002). On a positive note, some convergence results (Kalai & Lehrer, 1993) strongly suggest that
approximate optimality is achievable, although their applicability to our work remains open.
As we mentioned, the 0-th level types are POMDPs. They provide probability distributions
over actions of the agent modeled at that level to models with strategy level of 1. Given probability
distributions over other agent’s actions the level-1 models can themselves be solved as POMDPs,
and provide probability distributions to yet higher level models. Assume that the number of models
considered at each level is bound by a number, M . Solving an I-POMDP i,l in then equivalent to
solving O(M l ) POMDPs. Hence, the complexity of solving an I-POMDPi,l is PSPACE-hard for
finite time horizons,14 and undecidable for infinite horizons, just like for POMDPs.
6.1 Some Properties of I-POMDPs
In this section we establish two important properties, namely convergence of value iteration and
piece-wise linearity and convexity of the value function, for finitely nested I-POMDPs.
6.1.1 C ONVERGENCE

OF

VALUE I TERATION

For an agent i and its I-POMDPi,l , we can show that the sequence of value functions, {U n }, where
n is the horizon, obtained by value iteration defined in Eq. 7, converges to a unique fixed-point, U ∗ .
Let us define a backup operator H : B → B such that U n = HU n−1 , and B is the set of all
bounded value functions. In order to prove the convergence result, we first establish some of the
properties of H.
Lemma 1 (Isotonicity). For any finitely nested I-POMDP value functions V and U , if V ≤ U , then
HV ≤ HU .
The proof of this lemma is analogous to one due to Hauskrecht (1997), for POMDPs. It is
also sketched in the Appendix. Another important property exhibited by the backup operator is the
property of contraction.
Lemma 2 (Contraction). For any finitely nested I-POMDP value functions V , U and a discount
factor γ ∈ (0, 1), ||HV − HU || ≤ γ||V − U ||.
The proof of this lemma is again similar to the corresponding one in POMDPs (Hausktecht,
1997). The proof makes use of Lemma 1. || · || is the supremum norm.
14. Usually PSPACE-complete since the number of states in I-POMDPs is likely to be larger than the time horizon
(Papadimitriou & Tsitsiklis, 1987).

61

G MYTRASIEWICZ & D OSHI

Under the contraction property of H, and noting that the space of value functions along with
the supremum norm forms a complete normed space (Banach space), we can apply the Contraction
Mapping Theorem (Stokey & Lucas, 1989) to show that value iteration for I-POMDPs converges
to a unique fixed point (optimal solution). The following theorem captures this result.
Theorem 1 (Convergence). For any finitely nested I-POMDP, the value iteration algorithm starting from any arbitrary well-defined value function converges to a unique fixed-point.
The detailed proof of this theorem is included in the Appendix.
As in the case of POMDPs (Russell & Norvig, 2003), the error in the iterative estimates, U n , for
finitely nested I-POMDPs, i.e., ||U n − U ∗ ||, is reduced by the factor of at least γ on each iteration.
Hence, the number of iterations, N , needed to reach an error of at most  is:
N = dlog(Rmax /(1 − γ))/ log(1/γ)e

(10)

where Rmax is the upper bound of the reward function.
6.1.2 P IECEWISE L INEARITY

AND

C ONVEXITY

Another property that carries over from POMDPs to finitely nested I-POMDPs is the piecewise
linearity and convexity (PWLC) of the value function. Establishing this property allows us to decompose the I-POMDP value function into a set of alpha vectors, each of which represents a policy
tree. The PWLC property enables us to work with sets of alpha vectors rather than perform value
iteration over the continuum of agent’s beliefs. Theorem 2 below states the PWLC property of the
I-POMDP value function.
Theorem 2 (PWLC). For any finitely nested I-POMDP, U is piecewise linear and convex.
The complete proof of Theorem 2 is included in the Appendix. The proof is similar to one
due to Smallwood and Sondik (1973) for POMDPs and proceeds by induction. The basis case is
established by considering the horizon 1 value function. Showing the PWLC for the inductive step
requires substituting the belief update (Eq. 6) into Eq. 7, followed by factoring out the belief from
both terms of the equation.

7. Example: Multi-agent Tiger Game
To illustrate optimal sequential behavior of agents in multi-agent settings we apply our I-POMDP
framework to the multi-agent tiger game, a traditional version of which we described before.
7.1 Definition
Let us denote the actions of opening doors and listening as OR, OL and L, as before. TL and
TR denote states corresponding to tiger located behind the left and right door, respectively. The
transition, reward and observation functions depend now on the actions of both agents. Again, we
assume that the tiger location is chosen randomly in the next time step if any of the agents opened
any doors in the current step. We also assume that the agent hears the tiger’s growls, GR and GL,
with the accuracy of 85%. To make the interaction more interesting we added an observation of
door creaks, which depend on the action executed by the other agent. Creak right, CR, is likely due
62

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

to the other agent having opened the right door, and similarly for creak left, CL. Silence, S, is a good
indication that the other agent did not open doors and listened instead. We assume that the accuracy
of creaks is 90%. We also assume that the agent’s payoffs are analogous to the single agent versions
described in Section 3.2 to make these cases comparable. Note that the result of this assumption is
that the other agent’s actions do not impact the original agent’s payoffs directly, but rather indirectly
by resulting in states that matter to the original agent. Table 1 quantifies these factors.

hai , aj i
hOL, ∗i
hOR, ∗i
h∗, OLi
h∗, ORi
hL, Li
hL, Li

State
*
*
*
*
TL
TR

TL
0.5
0.5
0.5
0.5
1.0
0

TR
0.5
0.5
0.5
0.5
0
1.0

hai , aj i
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

Transition function: Ti = Tj

TL
10
-100
10
-100
-1
-1
10
-1
-100

TR
-100
10
-100
10
-1
-1
-100
-1
10

hai , aj i
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

TL
10
-100
-100
10
-1
10
-1
-100
-1

TR
-100
10
10
-100
-1
-100
-1
10
-1

Reward functions of agents i and j

hai , aj i
hL, Li
hL, Li
hL, OLi
hL, OLi
hL, ORi
hL, ORi
hOL, ∗i
hOR, ∗i

State
TL
TR
TL
TR
TL
TR
∗
∗

h GL, CL i
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR i
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL, S i
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL i
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR i
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR, S i
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

hai , aj i
hL, Li
hL, Li
hOL, Li
hOL, Li
hOR, Li
hOR, Li
h∗, OLi
h∗, ORi

State
TL
TR
TL
TR
TL
TR
∗
∗

h GL, CL i
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR i
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL, S i
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL i
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR i
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR, S i
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

Observation functions of agents i and j.
Table 1: Transition, reward, and observation functions for the multi-agent Tiger game.
When an agent makes its choice in the multi-agent tiger game, it considers what it believes
about the location of the tiger, as well as whether the other agent will listen or open a door, which in
turn depends on the other agent’s beliefs, reward function, optimality criterion, etc. 15 In particular,
if the other agent were to open any of the doors the tiger location in the next time step would be
chosen randomly. Thus, the information obtained from hearing the previous growls would have to
be discarded. We simplify the situation by considering i’s I-POMDP with a single level of nesting,
assuming that all of the agent j’s properties, except for beliefs, are known to i, and that j’s time
horizon is equal to i’s. In other words, i’s uncertainty pertains only to j’s beliefs and not to its
frame. Agent i’s interactive state space is, ISi,1 = S × Θj,0 , where S is the physical state, S={TL,
15. We assume an intentional model of the other agent here.

63

G MYTRASIEWICZ & D OSHI

TR}, and Θj,0 is a set of intentional models of agent j’s, each of which differs only in j’s beliefs
over the location of the tiger.
7.2 Examples of the Belief Update
In Section 5, we presented the belief update equation for I-POMDPs (Eq. 6). Here we consider
examples of beliefs, bi,1 , of agent i, which are probability distributions over S × Θ j,0 . Each 0-th
level type of agent j, θj,0 ∈ Θj,0 , contains a “flat” belief as to the location of the tiger, which can be
represented by a single probability assignment – bj,0 = pj (T L).
0.506
0.504

0.504

Pr(TL,p
Pr(TL,b_j) )
j

Pr(TL,p
Pr(TL,b_j)

j

)

0.506
0.502
0.5
0.498

0.502
0.5
0.498

0.496
0.496

0.494
0

0.2

0.4

0.6

0.8

1

0.494
0

0.2

0.4

0.6

0.8

1

0.8

1

pb_j
j (TL)

0.506

0.506

0.504

0.504

j

)

0.502

Pr(TR,p
Pr(TR,b_j)

Pr(TR,p
Pr(TR,b_j)

j

)

pjb_j
(TL)
j(TL)

0.5
0.498

0.502
0.5
0.498

0.496
0.494

0.496

0

0.2

0.4 0.6 0.8
ppb_j
(TL)
(TR)
(TL)

1
0.494

jj

0

0.2

0.4

0.6

p j (TL)
b_j

(i)

(ii)

Figure 5: Two examples of singly nested belief states of agent i. In each case i has no information
about the tiger’s location. In (i) agent i knows that j does not know the location of the
tiger; the single point (star) denotes a Dirac delta function which integrates to the height
of the point, here 0.5 . In (ii) agent i is uninformed about j’s beliefs about tiger’s location.

In Fig. 5 we show some examples of level 1 beliefs of agent i. In each case i does not know
the location of the tiger so that the marginals in the top and bottom sections of the figure sum up to
0.5 for probabilities of TL and TR each. In Fig. 5(i), i knows that j assigns 0.5 probability to tiger
being behind the left door. This is represented using a Dirac delta function. In Fig. 5(ii), agent i is
uninformed about j’s beliefs. This is represented as a uniform probability density over all values of
the probability j could assign to state TL.
To make the presentation of the belief update more transparent we decompose the formula in
Eq. 6 into two steps:
64

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

t−1
• Prediction: When agent i performs an action at−1
i , and given that agent j performs aj , the
predicted belief state is:

bbt (ist ) = P r(ist |at−1 , at−1 , bt−1 ) = P t−1 bt−1 bt bt−1 (ist−1 )P r(at−1 |θt−1 )
i
j
j
i
j
i
is
|θj =θj i
P
×T (st−1 , at−1 , st ) Oj (st , at−1 , otj )

(11)

otj

t−1 t t
×τθjt (bt−1
j , a j , o j , bj )

• Correction: When agent i perceives an observation, o ti , the predicted belief states,
t−1 t−1
P r(·|at−1
i , aj , bi ), are combined according to:
bti (ist ) = P r(ist |oti , ait−1 , bt−1
i )=β

X

t−1 t−1
Oi (st , at−1 , oti )P r(ist |at−1
i , a j , bi )

(12)

at−1
j

where β is the normalizing constant.

t−1

t

0.496
0.494
0

0.2

0.4 0.6 0.8
pb_j(TL)

1

j

0.496
0.494
0.4 0.6 0.8
pb_j(TL)

1

0.8

1

0.7
0.6
0.5
0.4
0.3
L,<GL,S>
0.2
0.1
L,<GL,S> 0
0
0.8
1

L,<GL,S>

0

0.2

0.4

0.6

pjb_j(TL)

<GL,S>

<GL,S>
0.2

0.4

0.6

0.8

L,<GL,S>

0.1

L,<GL,S>

0.06

0.8

1

0.4 0.6 0.8
pb_j(TL)

1

0.02

0.01

0.005

0.04

L,<GL,S>
0.2

0.4

pjb_j
(TL)

0.6

pjb_j
(TL)

(b)

0.6

pjb_j(TL)

0.015

0.08

0

1

0.4

0.025

0.12

0.02
0

0.2

L,<GL,S>

0.14

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05

j

(a)

0.6

pjb_j
(TL)

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

j)

j)
0.4

Pr(TR,pj )

0.498

Pr(TR,b_j)

j)

Pr(TR,p
Pr(TR,b_j)

L,(L,GL)

0.5

0.2

0.2

L,(L,GR)

0.504

0

<GL,S>

0

0.506
0.502

L,<GL,S>

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05

Pr(TL,p
Pr(TL,b_j)

L,(L,GR)

0.5
0.498

bi

Pr(TR,b_j)
Pr(TR,p
)
j

0.502

t+1

bi

<GL,S>

Pr(TL,p
Pr(TL,b_j)

0.504

Pr(TL,p
Pr(TL,b_j) )
j

Pr(TL,p
Pr(TL,b_j)

j)

0.506

t

bi

L,(L,GL)

Pr(TR,p
Pr(TR,b_j) )
j

bi

(c)

0.8

1

0
0

0.2

j

(d)

Figure 6: A trace of the belief update of agent i. (a) depicts the prior. (b) is the result of prediction
given i’s listening action, L, and a pair denoting j’s action and observation. i knows that
j will listen and could hear tiger’s growl on the right or the left, and that the probabilities
j would assign to TL are 0.15 or 0.85, respectively. (c) is the result of correction after
i observes tiger’s growl on the left and no creaks, hGL,Si. The probability i assigns to
TL is now greater than TR. (d) depicts the results of another update (both prediction and
correction) after another listen action of i and the same observation, hGL,Si.
Each discrete point above denotes, again, a Dirac delta function which integrates to the height of
the point.
In Fig. 6, we display the example trace through the update of singly nested belief. In the first
column of Fig. 6, labeled (a), is an example of agent i’s prior belief we introduced before, according
65

G MYTRASIEWICZ & D OSHI

to which i knows that j is uninformed of the location of the tiger. 16 Let us assume that i listens and
hears a growl from the left and no creaks. The second column of Fig. 6, (b), displays the predicted
belief after i performs the listen action (Eq. 11). As part of the prediction step, agent i must solve
j’s model to obtain j’s optimal action when its belief is 0.5 (term P r(a t−1
j |θj ) in Eq. 11). Given the
value function in Fig. 3, this evaluates to probability of 1 for listen action, and zero for opening of
any of the doors. i also updates j’s belief given that j listens and hears the tiger growling from either
t−1 t t
the left, GL, or right, GR, (term τθjt (bt−1
j , aj , oj , bj ) in Eq. 11). Agent j’s updated probabilities
for tiger being on the left are 0.85 and 0.15, for j’s hearing GL and GR, respectively. If the tiger is
on the left (top of Fig. 6 (b)) j’s observation GL is more likely, and consequently j’s assigning the
probability of 0.85 to state TL is more likely (i assigns a probability of 0.425 to this state.) When
the tiger is on the right j is more likely to hear GR and i assigns the lower probability, 0.075, to
j’s assigning a probability 0.85 to tiger being on the left. The third column, (c), of Fig. 6 shows
the posterior belief after the correction step. The belief in column (b) is updated to account for i’s
hearing a growl from the left and no creaks, hGL,Si. The resulting marginalised probability of the
tiger being on the left is higher (0.85) than that of the tiger being on the right. If we assume that in
the next time step i again listens and hears the tiger growling from the left and no creaks, the belief
state depicted in the fourth column of Fig. 6 results.
In Fig. 7 we show the belief update starting from the prior in Fig. 5 (ii), according to which
agent i initially has no information about what j believes about the tiger’s location.
The traces of belief updates in Fig. 6 and Fig. 7 illustrate the changing state of information agent
i has about the other agent’s beliefs. The benefit of representing these updates explicitly is that, at
each stage, i’s optimal behavior depends on its estimate of probabilities of j’s actions. The more
informative these estimates are the more value agent i can expect out of the interaction. Below, we
show the increase in the value function for I-POMDPs compared to POMDPs with the noise factor.
7.3 Examples of Value Functions
This section compares value functions obtained from solving a POMDP with a static noise factor,
accounting for the presence of another agent,17 to value functions of level-1 I-POMDP. The advantage of more refined modeling and update in I-POMDPs is due to two factors. First is the ability to
keep track of the other agent’s state of beliefs to better predict its future actions. The second is the
ability to adjust the other agent’s time horizon as the number of steps to go during the interaction
decreases. Neither of these is possible within the classical POMDP formalism.
We continue with the simple example of I-POMDPi,1 of agent i. In Fig. 8 we display i’s
value function for the time horizon of 1, assuming that i’s initial belief as to the value j assigns
to TL, pj (T L), is as depicted in Fig. 5 (ii), i.e. i has no information about what j believes about
tiger’s location. This value function is identical to the value function obtained for an agent using
a traditional POMDP framework with noise, as well as single agent POMDP which we described
in Section 3.2. The value functions overlap since agents do not have to update their beliefs and
the advantage of more refined modeling of agent j in i’s I-POMDP does not become apparent. Put
another way, when agent i models j using an intentional model, it concludes that agent j will open
each door with probability 0.1 and listen with probability 0.8. This coincides with the noise factor
we described in Section 3.2.
16. The points in Fig. 7 again denote Dirac delta functions which integrate to the value equal to the points’ height.
17. The POMDP with noise is the same as level-0 I-POMDP.

66

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

t−1

t
bi

bi

L(L,GR)
L(L,GL)
0.5

0.5

L(L,GL)
0.4

0.4

L(L,GR)

0.2

0.5

0.3

0.2

0.498
0.1

0.1

L(L,GR)

0.496
0.494
0

0.2

0.4

0.6

0.8

0

L(OL/OR,*)

1

pjb_j(TL)

0

0.504
0.502

0.494
0

0.2

0.4

0.6

0.8

1

0.8

0

1

0

Pr(TL,b_j)

L(L,GR)

pjb_j(TL)

0.4

0.6

0.8

1

pjb_j
(TL)
0.02275
0.0227
0.02265

0.0226

0.0226

0.02255
0.0225

0.02245

0.02255
0.0225
0.02245

0.0224

0.0224

0.02235

0.02235

0.0223

L(OL/OR,*)

0.2

0.02265

Pr(TL, pj )

L(OL/OR,*)

0.496

0.6

0.0227

L(OL/OR,*)

0.498

0.4

0.02275

L(L,GL)

0.5

0.2

pjb_j
(TL)

L(L,GL)

0.506

Pr(TR,
pj )
Pr(TR,b_j)

0.3

Pr(TR,b_j)
Pr(TR,p
j)

Pr(TL,p
)
Pr(TL,b_j)
j

0.502

Pr(TR,b_j)
Pr(TR,p
j)

0.504

Pr(TL,b_j)
Pr(TL,
pj )

0.506

0.0223

0.02225

0.02225
0

0.2

0.4

0.6

0.8

1

0

pjb_j
(TL)

(a)

0.2

0.4

0.6

0.8

1

pjb_j
(TL)

(b)
<GL,S>

t

bi

0.3

1.8

1.6
0.25

Pr(TR,pj )

1.2

Pr(TR,b_j)

Pr(TL,p
)
Pr(TL,b_j)
j

1.4

1

0.8

0.2

0.15

0.1

0.6

0.4
0.05
0.2
0

0
0

0.2

0.4

0.6

pjb_j
(TL)

0.8

0

1

0.2

0.4

0.6

0.8

1

pj b_j
(TL)

(c)

Figure 7: A trace of the belief update of agent i. (a) depicts the prior according to which i is
uninformed about j’s beliefs. (b) is the result of the prediction step after i’s listening
action (L). The top half of (b) shows i’s belief after it has listened and given that j also
listened. The two observations j can make, GL and GR, each with probability dependent
on the tiger’s location, give rise to flat portions representing what i knows about j’s belief
in each case. The increased probability i assigns to j’s belief between 0.472 and 0.528 is
due to j’s updates after it hears GL and after it hears GR resulting in the same values in
this interval. The bottom half of (b) shows i’s belief after i has listened and j has opened
the left or right door (plots are identical for each action and only one of them is shown). i
knows that j has no information about the tiger’s location in this case. (c) is the result of
correction after i observes tiger’s growl on the left and no creaks hGL,Si. The plots in (c)
are obtained by performing a weighted summation of the plots in (b). The probability i
assigns to TL is now greater than TR, and information about j’s beliefs allows i to refine
its prediction of j’s action in the next time step.

67

G MYTRASIEWICZ & D OSHI

L
OR

OL

10

Value Function (U)

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
Level 1 I-POMDP

POMDP with noise

Figure 8: For time horizon of 1 the value functions obtained from solving a singly nested I-POMDP
and a POMDP with noise factor overlap.
L\();OL\(<GR,S>),L\(?)

L\();OR\(<GL,S>),L\(?)

L\();L\(<GL,*>),OL\(<GR,*>)

OL\();L\(*)

L\();L\(*)

L\();OR\(<GL,*>),L\(<GR,*>)

L\();L\(GL),OL\(GR)

L\();OR\(GL),L\(GR)

OR\();L\(*)

8

Value Function (U)

6

4

2

0

-2
0

0.2

0.4

0.6

0.8

1

pp_i(TL)
i (TL)
Level 1 I-POMDP

POMDP with noise

Figure 9: Comparison of value functions obtained from solving an I-POMDP and a POMDP with
noise for time horizon of 2. I-POMDP value function dominates due to agent i adjusting
the behavior of agent j to the remaining steps to go in the interaction.

68

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

8

Value Function (U)

7
6
5
4
3
2
1
0

0.2

0.4

0.6

0.8

1

p i (TL)
p_i(TL)
Level 1 I-POMDP

POMDP with noise

Figure 10: Comparison of value functions obtained from solving an I-POMDP and a POMDP with
noise for time horizon of 3. I-POMDP value function dominates due to agent i’s adjusting j’s remaining steps to go, and due to i’s modeling j’s belief update. Both factors
allow for better predictions of j’s actions during interaction. The descriptions of individual policies were omitted for clarity; they can be read off of Fig. 11.

In Fig. 9 we display i’s value functions for the time horizon of 2. The value function of
I-POMDPi,1 is higher than the value function of a POMDP with a noise factor. The reason is
not related to the advantages of modeling agent j’s beliefs – this effect becomes apparent at the time
horizon of 3 and longer. Rather, the I-POMDP solution dominates due to agent i modeling j’s time
horizon during interaction: i knows that at the last time step j will behave according to its optimal
policy for time horizon of 1, while with two steps to go j will optimize according to its 2 steps to go
policy. As we mentioned, this effect cannot be modeled using a POMDP with a static noise factor
included in the transition function.
Fig. 10 shows a comparison between the I-POMDP and the noisy POMDP value functions for
horizon 3. The advantage of more refined agent modeling within the I-POMDP framework has
increased.18 Both factors, i’s adjusting j’s steps to go and i’s modeling j’s belief update during
interaction are responsible for the superiority of values achieved using the I-POMDP. In particular,
recall that at the second time step i’s information as to j’s beliefs about the tiger’s location is as
depicted in Fig. 7 (c). This enables i to make a high quality prediction that, with two steps left to
go, j will perform its actions OL, L, and OR with probabilities 0.009076, 0.96591 and 0.02501,
respectively (recall that for POMDP with noise these probabilities remained unchanged at 0.1, 0,8,
and 0.1, respectively.)
Fig. 11 shows agent i’s policy graph for time horizon of 3. As usual, it prescribes the optimal
first action depending on the initial belief as to the tiger’s location. The subsequent actions depend
on the observations received. The observations include creaks that are indicative of the other agent’s
18. Note that I-POMDP solution is not as good as the solution of a POMDP for an agent operating alone in the environment shown in Fig. 3.

69

G MYTRASIEWICZ & D OSHI

[0 −− 0.029)
OL

[0.029 −− 0.089)

[0.089 −− 0.211)

L
*

L

<GR,S>

<GL,CL/CR>
<GR,*>

[0.211 −− 0.789)

[0.789 −− 0.911)

L

*

OL

OR
*

<GR,S>
<GL,CL\CR>

L

L
<GR,*>

[0.971 −− 1]

L

<GR,CL\CR>
<GR,CL\CR>
<GL,CL\CR>
<GL,S>
<GR,*> <GL,*>
<GL,*>
<GR,S>
<GL,S>

<GL,S>
<GR,CL\CR>

OL

[0.911 −− 0.971)

L

<GL,*>

L
*

<GR,*>

OR
<GL,*>

*

OR

L

Figure 11: The policy graph corresponding to the I-POMDP value function in Fig. 10.
having opened a door. The creaks contain valuable information and allow the agent to make more
refined choices, compared to ones in the noisy POMDP in Fig. 4. Consider the case when agent i
starts out with fairly strong belief as to the tiger’s location, decides to listen (according to the four
off-center top row “L” nodes in Fig. 11) and hears a door creak. The agent is then in the position to
open either the left or the right door, even if that is counter to its initial belief. The reason is that the
creak is an indication that the tiger’s position has likely been reset by agent j and that j will then
not open any of the doors during the following two time steps. Now, two growls coming from the
same door lead to enough confidence to open the other door. This is because the agent i’s hearing
of tiger’s growls are indicative of the tiger’s position in the state following the agents’ actions,
Note that the value functions and the policy above depict a special case of agent i having no
information as to what probability j assigns to tiger’s location (Fig. 5 (ii)). Accounting for and
visualizing all possible beliefs i can have about j’s beliefs is difficult due to the complexity of the
space of interactive beliefs. As our ongoing work indicates, a drastic reduction in complexity is
possible without loss of information, and consequently representation of solutions in a manageable
number of dimensions is indeed possible. We will report these results separately.

8. Conclusions
We proposed a framework for optimal sequential decision-making suitable for controlling autonomous
agents interacting with other agents within an uncertain environment. We used the normative
paradigm of decision-theoretic planning under uncertainty formalized as partially observable Markov
decision processes (POMDPs) as a point of departure. We extended POMDPs to cases of agents
interacting with other agents by allowing them to have beliefs not only about the physical environment, but also about the other agents. This could include beliefs about the others’ abilities, sensing
capabilities, beliefs, preferences, and intended actions. Our framework shares numerous properties
with POMDPs, has analogously defined solutions, and reduces to POMDPs when agents are alone
in the environment.
In contrast to some recent work on DEC-POMDPs (Bernstein et al., 2002; Nair et al., 2003),
and to work motivated by game-theoretic equilibria (Boutilier, 1999; Hu & Wellman, 1998; Koller
70

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

& Milch, 2001; Littman, 1994), our approach is subjective and amenable to agents independently
computing their optimal solutions.
The line of work presented here opens an area of future research on integrating frameworks for
sequential planning with elements of game theory and Bayesian learning in interactive settings. In
particular, one of the avenues of our future research centers on proving further formal properties of
I-POMDPs, and establishing clearer relations between solutions to I-POMDPs and various flavors
of equilibria. Another concentrates on developing efficient approximation techniques for solving
I-POMDPs. As for POMDPs, development of approximate approaches to I-POMDPs is crucial for
moving beyond toy problems. One promising approximation technique we are working on is particle
filtering. We are also devising methods for representing I-POMDP solutions without assumptions
about what’s believed about other agents’ beliefs. As we mentioned, in spite of the complexity of the
interactive state space, there seem to be intuitive representations of belief partitions corresponding
to optimal policies, analogous to those for POMDPs. Other research issues include the suitable
choice of priors over models,19 and the ways to fulfill the absolute continuity condition needed for
convergence of probabilities assigned to the alternative models during interactions (Kalai & Lehrer,
1993).

Acknowledgments
This research is supported by the National Science Foundation CAREER award IRI-9702132, and
NSF award IRI-0119270.

Appendix A. Proofs
Proof of Propositions 1 and 2. We start with Proposition 2, by applying the Bayes Theorem:

t−1
bti (ist ) = P r(ist |oti , at−1
i , bi ) =

)
,bt−1
P r(ist ,oti |at−1
i
i
t−1 t−1
t
P r(oi |ai ,bi )

P
t−1 )
= β ist−1 bit−1 (ist−1 )P r(ist , oti |at−1
i , is
P
P
t−1
t−1 )P r(at−1 |at−1 , ist−1 )
= β ist−1 bit−1 (ist−1 ) at−1 P r(ist , oti |at−1
i , aj , is
j
i
j
P
P
(13)
t−1
t−1 )P r(at−1 |ist−1 )
,
is
,
a
= β ist−1 bit−1 (ist−1 ) at−1 P r(ist , oti |at−1
j
j
i
j
P
P
t−1
i
t t−1 , ist−1 )P r(ist |at−1 , ist−1 )
= β ist−1 bit−1 (ist−1 ) at−1 P r(at−1
j |mj )P r(ot |is , a
j
P
P
t−1
i
t t−1 )P r(ist |at−1 , ist−1 )
= β ist−1 bit−1 (ist−1 ) at−1 P r(at−1
j |mj )P r(ot |is , a
j
P
P
t−1
t t−1 , ot )P r(ist |at−1 , ist−1 )
= β ist−1 bit−1 (ist−1 ) at−1 P r(at−1
i
j |mj )Oi (s , a
j

19. We are looking at Kolmogorov complexity (Li & Vitanyi, 1997) as a possible way to assign priors.

71

G MYTRASIEWICZ & D OSHI

To simplify the term P r(ist |at−1 , ist−1 ) let us substitute the interactive state ist with its components. When mj in the interactive states is intentional: ist = (st , θjt ) = (st , btj , θbjt ).

P r(ist |at−1 , ist−1 ) = P r(st , btj , θbjt |at−1 , ist−1 )
= P r(btj |st , θbjt , at−1 , ist−1 )P r(st , θbjt |at−1 , ist−1 )
= P r(btj |st , θbjt , at−1 , ist−1 )P r(θbjt |st , at−1 , ist−1 )P r(st |at−1 , ist−1 )
= P r(btj |st , θbjt , at−1 , ist−1 )I(θbjt−1 , θbjt )Ti (st−1 , at−1 , st )
(14)
b tj ).
When mj is subintentional: ist = (st , mtj ) = (st , htj , m

P r(ist |at−1 , ist−1 ) = P r(st , htj , m
b tj |at−1 , ist−1 )
b tj |at−1 , ist−1 )
b tj , at−1 , ist−1 )P r(st , m
= P r(htj |st , m
b tj , at−1 , ist−1 )P r(θbjt |st , at−1 , ist−1 )P r(st |at−1 , ist−1 )
= P r(htj |st , m
t
t
= P r(hj |s , m
b tj , at−1 , ist−1 )I(m
b tj )Ti (st−1 , at−1 , st )
b t−1
(14’)
j ,m

The joint action pair, at−1 , may change the physical state. The third term on the right-hand
side of Eqs. 14 and 140 above captures this transition. We utilized the MNM assumption to replace
the second terms of the equations with boolean identity functions, I( θbjt−1 , θbjt ) and I(m
b t−1
b tj )
j ,m
respectively, which equal 1 if the two frames are identical, and 0 otherwise. Let us turn our attention
to the first terms. If mj in ist and ist−1 is intentional:
P
P r(btj |st , θbjt , at−1 , ist−1 ) = ot P r(btj |st , θbjt , at−1 , ist−1 , otj )P r(otj |st , θbjt , at−1 , ist−1 )
Pj
= ot P r(btj |st , θbjt , at−1 , ist−1 , otj )P r(otj |st , θbjt , at−1 )
Pj
t−1 t t
t−1 , ot )
= ot τθjt (bt−1
j
j , aj , oj , bj )Oj (st , a

(15)

j

Else if it is subintentional:

P r(htj |st , m
b tj , at−1 , ist−1 ) =

=

=

P

t

Po j
t

Po j
otj

b tj , at−1 , ist−1 )
b tj , at−1 , ist−1 , otj )P r(otj |st , m
P r(htj |st , m

b tj , at−1 )
b tj , at−1 , ist−1 , otj )P r(otj |st , m
P r(htj |st , m

t
t
t−1 , ot )
δK (APPEND(ht−1
j
j , oj ), hj )Oj (st , a

(15’)

t−1 t
In Eq. 15, the first term on the right-hand side is 1 if agent j’s belief update, SE θj (bt−1
j , a j , oj )
generates a belief state equal to btj . Similarly, in Eq. 150 , the first term is 1 if appending the otj
to ht−1
results in htj . δK is the Kronecker delta function. In the second terms on the right-hand
j
side of the equations, the MNO assumption makes it possible to replace P r(o t |st , θbt , at−1 ) with
j

j

Oj (st , at−1 , otj ), and P r(otj |st , m
b tj , at−1 ) with Oj (st , at−1 , otj ) respectively.
Let us now substitute Eq. 15 into Eq. 14.
P
t−1 t t
t t−1 , ot )I(θ
bt−1 , θbt )Ti (st−1 , at−1 , st )
P r(ist |at−1 , ist−1 ) = ot τθjt (bt−1
j
j
j , aj , oj , bj )Oj (s , a
j
j
(16)
0
0
Substituting Eq. 15 into Eq. 14 we get,
P
t
t
t t−1 , ot )I(m
P r(ist |at−1 , ist−1 ) = ot δK (APPEND(ht−1
b t−1
b tj )
j
j , oj ), hj )Oj (s , a
j ,m
j

×Ti (st−1 , at−1 , st )

(16’)

72

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Replacing Eq. 16 into Eq. 13 we get:
P
P
t−1 t t
t−1
t−1 )
P r(at−1
|θjt−1 )Oi (st , at−1 , oti ) ot τθjt (bt−1
ist−1 bi (is
j , a j , o j , bj )
j
at−1
j
j
×Oj (st , at−1 , otj )I(θbjt−1 , θbjt )Ti (st−1 , at−1 , st )

bti (ist ) = β

P

(17)

Similarly, replacing Eq. 160 into Eq. 13 we get:

P
P
t−1
t t−1 , ot )
t−1 )
P r(at−1
bti (ist ) = β ist−1 bt−1
i
j |mj )Oi (s , a
i (is
at−1
j
P
t−1 t
t−1
t
t
t−1
t
bj ,m
b tj )Ti (st−1 , at−1 , st )
× ot δK (APPEND(hj , oj ), hj )Oj (s , a , oj )I(m
j

We arrive at the final expressions for the belief update by removing the terms
I(m
b t−1
b tj ) and changing the scope of the first summations.
j ,m
When mj in the interactive states is intentional:

I( θbjt−1 , θbjt )

P
P
t−1
t t−1 , ot )
bt−1 (ist−1 ) at−1 P r(at−1
bti (ist ) = β ist−1 :m
i
j |θj )Oi (s , a
b t−1
=θbjt i
j
j
P
t−1 t−1 t t
t−1
t−1
t
t
t−1
t
× ot τθjt (bj , aj , oj , bj )Oj (s , a , oj )Ti (s , a , s )

(170 )
and

(18)

j

Else, if it is subintentional:
P
P
t−1
t−1
t t−1 , ot )
(ist−1 ) at−1 P r(at−1
bti (ist ) = β ist−1 :m
t bi
i
j |mj )Oi (s , a
=
m
b
b t−1
j
j
j
P
t ), ht )O (st , at−1 , ot )T (st−1 , at−1 , st )
,
o
× ot δK (APPEND(ht−1
j
j
j
j i
j

(19)

j

Since proposition 2 expresses the belief bti (ist ) in terms of parameters of the previous time step
only, Proposition 1 holds as well.
Before we present the proof of Theorem 1 we note that the Equation 7, which defines value
iteration in I-POMDPs, can be rewritten in the following form, U n = HU n−1 . Here, H : B → B
is a backup operator, and is defined as,
HU n−1 (θi ) = max h(θi , ai , U n−1 )
ai ∈Ai

where h : Θi × Ai × B → R is,
h(θi , ai , U ) =

P
is

bi (is)ERi (is, ai ) + γ

P

o∈Ωi

P r(oi |ai , bi )U (hSEθi (bi , ai , oi ), θ̂i i)

and where B is the set of all bounded value functions U . Lemmas 1 and 2 establish important
properties of the backup operator. Proof of Lemma 1 is given below, and proof of Lemma 2 follows
thereafter.
Proof of Lemma 1. Select arbitrary value functions V and U such that V (θ i,l ) ≤ U (θi,l ) ∀θi,l ∈
Θi,l . Let θi,l be an arbitrary type of agent i.
73

G MYTRASIEWICZ & D OSHI





P

P

HV (θi,l ) = max
o∈Ωi P r(oi |ai , bi )V (hSEθi,l (bi , ai , oi ), θ̂i i)
is bi (is)ERi (is, ai ) + γ
ai ∈Ai
P
P
= is bi (is)ERi (is, a∗i ) + γ o∈Ωi P r(oi |a∗i , bi )V (hSEθi,l (bi , a∗i , oi ), θ̂i i)
P
P
∗
∗
∗
≤ is b
i (is)ERi (is, ai ) + γ
o∈Ωi P r(oi |ai , bi )U (hSEθi,l (bi , ai , oi ), θ̂i i)

P
P
≤ max
o∈Ωi P r(oi |ai , bi )U (hSEθi,l (bi , ai , oi ), θ̂i i)
is bi (is)ERi (is, ai ) + γ
ai ∈Ai

= HU (θi,l )

Since θi,l is arbitrary, HV ≤ HU .
Proof of Lemma 2. Assume two arbitrary well defined value functions V and U such that V ≤ U .
From Lemma 1 it follows that HV ≤ HU . Let θi,l be an arbitrary type of agent i. Also, let a∗i be
the action that optimizes HU (θi,l ).
0 ≤ HU (θi,l ) − HV (θi,l )



P

= max sumis bi (is)ERi (is, ai ) + γ o∈Ωi P r(oi |ai , bi )U (SEθi,l (bi , ai , oi ), hθ̂i i) −
ai ∈Ai

P
P
max
is bi (is)ERi (is, ai ) + γ
o∈Ωi P r(oi |ai , bi )V (SEθi,l (bi , ai , oi ), hθ̂i i)
ai ∈Ai
P
P
≤ is bi (is)ERi (is, a∗i ) + γ o∈Ωi P r(oi |a∗i , bi )U (SEθi,l (bi , a∗i , oi ), hθ̂i i) −
P
P
∗
∗
∗
o∈Ωi P r(oi |ai , bi )V (SEθi,l (bi , ai , oi ), hθ̂i i)
is bi (is)ERi (is, ai ) − γ
P
∗
∗
= γ o∈Ωi P r(oi |ai , bi )U (SEθi,l (bi , ai , oi ), hθ̂i i)−
P
∗
γ o∈Ωi P r(oi |a∗i , bi )V (SE

 θi,l (bi , ai , oi ), hθ̂i i)
P
∗
∗
∗
= γ o∈Ωi P r(oi |ai , bi ) U (SEθi,l (bi , ai , oi ), hθ̂i i) − V (SEθi,l (bi , ai , oi ), hθ̂i i)
P
≤ γ o∈Ωi P r(oi |a∗i , bi )||U − V ||
= γ||U − V ||

As the supremum norm is symmetrical, a similar result can be derived for HV (θ i,l ) − HU (θi,l ).
Since θi,l is arbitrary, the Contraction property follows, i.e. ||HV − HU || ≤ ||V − U ||.
Lemmas 1 and 2 provide the stepping stones for proving Theorem 1. Proof of Theorem 1 follows
from a straightforward application of the Contraction Mapping Theorem. We state the Contraction
Mapping Theorem (Stokey & Lucas, 1989) below:
Theorem 3 (Contraction Mapping Theorem). If (S, ρ) is a complete metric space and T : S → S
is a contraction mapping with modulus γ, then
1. T has exactly one fixed point U ∗ in S, and
2. The sequence {U n } converges to U ∗ .
Proof of Theorem 1 follows.
74

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Proof of Theorem 1. The normed space (B, || · ||) is complete w.r.t the metric induced by the supremum norm. Lemma 2 establishes the contraction property of the backup operator, H. Using Theorem 3, and substituting T with H, convergence of value iteration in I-POMDPs to a unique fixed
point is established.
We go on to the piecewise linearity and convexity (PWLC) property of the value function.
We follow the outlines of the analogous proof for POMDPs in (Hausktecht, 1997; Smallwood &
Sondik, 1973).
Let α : IS → R be a real-valued and bounded function. Let the space of such real-valued
bounded functions be B(IS). We will now define an inner product.
Definition 5 (Inner product). Define the inner product, h·, ·i : B(IS) × ∆(IS) → R, by
X
hα, bi i =
bi (is)α(is)
is

The next lemma establishes the bilinearity of the inner product defined above.
Lemma 3 (Bilinearity). For any s, t ∈ R, f, g ∈ B(IS), and b, λ ∈ ∆(IS) the following equalities
hold:
hsf + tg, bi = shf, bi + thg, bi
hf, sb + tλi = shf, bi + thf, λi
We are now ready to give the proof of Theorem 2. Theorem 4 restates Theorem 2 mathematically, and its proof follows thereafter.
Theorem 4 (PWLC). The value function, U n , in finitely nested I-POMDP is piece-wise linear and
convex (PWLC). Mathematically,
U n (θi,l ) = max
n
α

X

bi (is)αn (is)

n = 1, 2, ...

is

Proof of Theorem 4. Basis Step: n = 1
From Bellman’s Dynamic Programming equation,
U 1 (θi ) = max
ai

X

bi (is)ER(is, ai )

(20)

is

P
where ERi (is, ai ) = aj R(is, ai , aj )P r(aj |mj ). Here, ERi (·) represents the expectation of
R w.r.t. agent j’s actions. Eq. 20 represents an inner product and using Lemma 3, the inner product
is linear in bi . By selecting the maximum of a set of linear vectors (hyperplanes), we obtain a PWLC
horizon 1 value function.
Inductive Hypothesis: Suppose that U n−1 (θi,l ) is PWLC. Formally we have,
U n−1 (θi,l ) = max
αn−1

=

P

max

α̇n−1 ,

is bi (is)α

α̈n−1



P

n−1 (is)

is:mj ∈IMj bi

(is)α̇n−1 (is)
75

+

P

is:mj ∈SMj bi

(is)α̈n−1 (is)



(21)

G MYTRASIEWICZ & D OSHI

Inductive Proof: To show that U n (θi,l ) is PWLC.

U n (θi,l ) = max
at−1
i

(

X

t−1
bt−1
)ERi (ist−1 , at−1
i (is
i )+γ

X

t−1
n−1
P r(oti |at−1
(θi,l )
i , bi )U

oti

ist−1

From the inductive hypothesis:
(
P
t−1
t−1 )ER (ist−1 , at−1 )
U n (θi,l ) = max
i
ist−1 bi (is
i
at−1
i

+γ

P

oti

t−1
P r(oti |at−1
i , bi )

max

αn−1 ∈Γn−1

P

t
t n−1 (ist )
ist bi (is )α

)

)

t−1 t
t−1 t−1 t
t
Let l(bt−1
i , ai , oi ) be the index of the alpha vector that maximizes the value at b i = SE(bi , ai , oi ).
Then,
(
P
t−1
t−1 )ER (ist−1 , at−1 )
U n (θi,l ) = max
i
ist−1 bi (is
i
t−1
ai
)
P
P
t−1
t
t n−1
+γ ot P r(oti |at−1
ist bi (is )αl(bt−1 ,at−1 ,ot )
i , bi )
i

i

i

i

From the second equation in the inductive hypothesis:
(
P
P
t−1
t−1 )ER (ist−1 , at−1 ) + γ
n
t t−1 t−1
U (θi,l ) = max
i
ot P r(oi |ai , bi )
ist−1 bi (is
i
at−1
i

i

×



P

t
t n−1
ist :mtj ∈IMj bi (is )α̇l(bt−1 ,at−1 ,ot )
i
i
i

+

P

t
t n−1
ist :mtj ∈SMj bi (is )α̈l(bt−1 ,at−1 ,ot )
i
i
i

Substituting bti with the appropriate belief updates from Eqs. 17 and 17 0 we get:
(
P
P
t−1
t t−1 t−1
t−1 )ER (ist−1 , at−1 ) + γ
U n (θi,l ) = max
i
oti P r(oi |ai , bi )
ist−1 bi (is
i
t−1
ai
"


P
P
P
t−1
t−1 t−1
t−1
×β
)
P r(aj |θj ) Oi (st , at−1 , oti )
ist :mtj ∈IMj
ist−1 bi (is
at−1
j


P
t−1 t−1 t t
t−1
t
t
t−1
t
t
t−1
t−1
t
× ot Oj (s , a , oj ) τθjt (bj , aj , oj , bj )I(θbj , θbj )Ti (s , a , s )

)

j

n−1
t
×α̇l(b
t−1 t−1 t (is )
,ai ,oi )
i


P
P
P
t−1
t−1
t−1
t−1
+ ist :mt ∈SMj ist−1 bi (is )
P r(aj |mj ) Oi (st , at−1 , oti )
at−1
j
j


P
t−1 t
t−1
t
t
t−1
t
t
t
t−1
t−1
t
× ot Oj (s , a , oj ) δK (APPEND(hj , oj ) − hj )I(m
bj ,m
b j )Ti (s , a , s )
j
#)
n−1
t
×α̈l(b
t−1 t−1 t (is )
,a
,o )
i

i

i

76

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

and further
U n (θi,l ) = max
at−1
i

×
×

P

P

(

P

t−1
t−1 )ER (ist−1 , at−1 )
i
ist−1 bi (is
i

t−1
t−1 )
ist−1 bi (is



t t t−1 , ot )
j
otj Oj (s , a

P



at−1
j

+γ

P

oti

"

P

ist :mtj ∈IMj


t−1
P r(at−1
|θ
)
Oi (st , at−1 , oti )
j
j

t−1 t t
t−1 , at−1 , st )
bt−1 bt
τθjt (bt−1
j , aj , oj , bj )I(θj , θj )Ti (s



n−1
t
×α̇l(b
t−1 t−1 t (is )
,ai ,oi )
i


P
P
P
t−1
t−1
t−1
t−1
P r(aj |mj ) Oi (st , at−1 , oti )
+ ist :mt ∈SMj ist−1 bi (is )
at−1
j
j


P
t−1
t ) − ht )I(m
t )T (st−1 , at−1 , st )
× ot Ojt (st , at−1 , otj ) δK (APPEND(ht−1
,
o
b
,
m
b
j
j
j i
j
j
j
#)
n−1
t
×α̈l(b
t−1 t−1 t (is )
,a
,o )
i

i

i

Rearranging the terms of the equation:
U n (θ

(


P
P P
t−1
t−1 ) ER (ist−1 , at−1 ) + γ
)
=
max
b
(is
t−1
t−1 :m
i
i,l
oti
ist :mtj ∈IMj
i
i
is
∈IM
j
j
at−1
i


P
P
t−1 t−1
×
P r(aj |θj ) Oi (st , at−1 , oti ) ot Ojt (st , at−1 , otj )
at−1
j
j



n−1
t−1 t−1 t t
t−1
t
t−1
t−1
t
t
× τθjt (bj , aj , oj , bj )I(θbj , θbj )Ti (s , a , s )
α̇l(bt−1 ,at−1 ,ot ) (is )
i
i
i

P
P
P
P
+ ist−1 :mt−1 ∈SMj bit−1 (ist−1 ) ERi (ist−1 , at−1
oti
ist :mtj ∈SMj
oti
i )+γ
j


P
P
P r(ajt−1 |mt−1
) Oi (st , at−1 , oti ) ot Ojt (st , at−1 , otj )
×
j
at−1
j
j

)

n−1
t
t
t
α̈l(b
b t−1
b tj )Ti (st−1 , at−1 , st )
× δK (APPEND(ht−1
t−1 t−1 t (is )
j ,m
j , oj ) − hj )I(m
,ai ,oi )
i

P
t−1
t−1 )α̇n (ist−1 )
= max
ai
∈IMj bi (is
ist−1 :mt−1
j
at−1
i

P
t−1
t−1
t−1
n
+ ist−1 :mt−1 ∈SMj bi (is )α̈ai (is )
j

Therefore,
U n (θ

i,l )

= max
n
n
α̇ , α̈

+
=

P



P

t−1
t−1 )α̇n (ist−1 )
ist−1 :mt−1
∈IMj bi (is
j



t−1
t−1 )α̈n (ist−1 )
∈SMj bi (is
ist−1 :mt−1
j
P
t−1
t−1 )αn (ist−1 ) = maxhbt−1 , αn i
max
ist−1 bi (is
i
n
α
αn

77

(22)

G MYTRASIEWICZ & D OSHI

where, if mjt−1 in ist−1 is intentional then αn = α̇n :
α̇n (ist−1 )

ERi (ist−1 , at−1
i )



P P

P

t−1
P r(at−1
j |θj )



Oi (ist , at−1 , oti )
+ γ ot ist :mt ∈IMj
at−1
j
j
 i

P
t−1 t−1 t t
t−1
t
t
t−1
t
t
t−1
t−1
t
× ot Oj (isj , a , oj ) τθjt (bj , aj , oj , bj )I(θbj , θbj )Ti (s , a , s )

=

j

n−1
t
×αl(b
t−1 t−1 t (is )
,o )
,a
i

i

i

and, if mjt−1 is subintentional then αn = α̈n :
α̈n (ist−1 )

ERi (ist−1 , at−1
i )



P P

P

t−1
P r(at−1
j |θj )



Oi (ist , at−1 , oti )
+ γ ot ist :mt ∈SMj
at−1
i
j
j


P
t−1
t−1 t
t−1
t−1
t
t
t
t−1
t
t
t
bj ,m
b j )Ti (s , a , s )
× ot Oj (isj , a , oj ) δK (APPEND(hj , oj ) − hj )I(m

=

j

n−1
t
×αl(b
t−1 t−1 t (is )
,ai ,oi )
i

Eq. 22 is an inner product and using Lemma 3, the value function is linear in b t−1
i . Furthermore,
maximizing over a set of linear vectors (hyperplanes) produces a piecewise linear and convex value
function.

References
Ambruster, W., & Boge, W. (1979). Bayesian game theory. In Moeschlin, O., & Pallaschke, D. (Eds.), Game
Theory and Related Topics. North Holland.
Aumann, R. J. (1999). Interactive epistemology i: Knowledge. International Journal of Game Theory, pp.
263–300.
Aumann, R. J., & Heifetz, A. (2002). Incomplete information. In Aumann, R., & Hart, S. (Eds.), Handbook
of Game Theory with Economic Applications, Volume III, Chapter 43. Elsevier.
Battigalli, P., & Siniscalchi, M. (1999). Hierarchies of conditional beliefs and interactive epistemology in
dynamic games. Journal of Economic Theory, pp. 188–230.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of decentralized control
of markov decision processes. Mathematics of Operations Research, 27(4), 819–840.
Binmore, K. (1990). Essays on Foundations of Game Theory. Blackwell.
Boutilier, C. (1999). Sequential optimality and coordination in multiagent systems. In Proceedings of the
Sixteenth International Joint Conference on Artificial Intelligence, pp. 478–485.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions and computational leverage. Journal of Artificial intelligence Research, 11, 1–94.
Brandenburger, A. (2002). The power of paradox: Some recent developments in interactive epistemology.
Tech. rep., Stern School of Business, New York University, http://pages.stern.nyu.edu/ abranden/.
Brandenburger, A., & Dekel, E. (1993). Hierarchies of beliefs and common knowledge. Journal of Economic
Theory, 59, 189–198.
Dennett, D. (1986). Intentional systems. In Dennett, D. (Ed.), Brainstorms. MIT Press.
Fagin, R. R., Geanakoplos, J., Halpern, J. Y., & Vardi, M. Y. (1999). A hierarchical approach to modeling
knowledge and common knowledge. International Journal of Game Theory, pp. 331–365.
Fagin, R. R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning About Knowledge. MIT Press.
Fudenberg, D., & Levine, D. K. (1998). The Theory of Learning in Games. MIT Press.
78

A F RAMEWORK FOR S EQUENTIAL P LANNING IN M ULTI -AGENT S ETTINGS

Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gmytrasiewicz, P. J., & Durfee, E. H. (2000). Rational coordination in multi-agent environments. Autonomous Agents and Multiagent Systems Journal, 3(4), 319–350.
Harsanyi, J. C. (1967). Games with incomplete information played by ’Bayesian’ players. Management
Science, 14(3), 159–182.
Hauskrecht, M. (2000). Value-function approximations for partially observable markov decision processes.
Journal of Artificial Intelligence Research, pp. 33–94.
Hausktecht, M. (1997). Planning and control in stochastic domains with imperfect information. Ph.D. thesis,
MIT.
Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical framework and an algorithm. In Fifteenth International Conference on Machine Learning, pp. 242–250.
Kadane, J. B., & Larkey, P. D. (1982). Subjective probability and the theory of games. Management Science,
28(2), 113–120.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially observable
stochastic domains. Artificial Intelligence, 101(2), 99–134.
Kalai, E., & Lehrer, E. (1993). Rational learning leads to nash equilibrium. Econometrica, pp. 1231–1240.
Koller, D., & Milch, B. (2001). Multi-agent influence diagrams for representing and solving games. In Seventeenth International Joint Conference on Artificial Intelligence, pp. 1027–1034, Seattle, Washington.
Li, M., & Vitanyi, P. (1997). An Introduction to Kolmogorov Complexity and Its Applications. Springer.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning. In Proceedings
of the International Conference on Machine Learning.
Lovejoy, W. S. (1991). A survey of algorithmic methods for partially observed markov decision processes.
Annals of Operations Research, 28(1-4), 47–66.
Madani, O., Hanks, S., & Condon, A. (2003). On the undecidability of probabilistic planning and related
stochastic optimization problems. Artificial Intelligence, 147, 5–34.
Mertens, J.-F., & Zamir, S. (1985). Formulation of Bayesian analysis for games with incomplete information.
International Journal of Game Theory, 14, 1–29.
Monahan, G. E. (1982). A survey of partially observable markov decision processes: Theory, models, and
algorithms. Management Science, 1–16.
Myerson, R. B. (1991). Game Theory: Analysis of Conflict. Harvard University Press.
Nachbar, J. H., & Zame, W. R. (1996). Non-computable strategies and discounted repeated games. Economic
Theory, 8, 103–122.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003). Taming decentralized pomdps: Towards
efficient policy computation for multiagent settings. In Proceedings of the Eighteenth International
Joint Conference on Artificial Intelligence (IJCAI-03).
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control of a multiple access broadcast channel. In
Proceedings of the 35th Conference on Decision and Control.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). The complexity of markov decision processes. Mathematics
of Operations Research, 12(3), 441–450.
Russell, S., & Norvig, P. (2003). Artificial Intelligence: A Modern Approach (Second Edition). Prentice Hall.
Smallwood, R. D., & Sondik, E. J. (1973). The optimal control of partially observable markov decision
processes over a finite horizon. Operations Research, pp. 1071–1088.
Stokey, N. L., & Lucas, R. E. (1989). Recursive Methods in Economic Dynamics. Harvard Univ. Press.

79

