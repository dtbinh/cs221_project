Journal of Artificial Intelligence Research 46 (2013) 579-605

Submitted 4/12; published 4/13

Predicting Behavior in Unstructured Bargaining with a
Probability Distribution
David H. Wolpert

david.h.wolpert@gmail.com

Santa Fe Institute,
1399 Hyde Park Road, Santa Fe, NM 87501;
Information Sciences Group,
Los Alamos National Laboratory,
MS B256, Los Alamos, NM 87545

James W. Bono

jwbono@gmail.com

Abstract
In experimental tests of human behavior in unstructured bargaining games, typically
many joint utility outcomes are found to occur, not just one. This suggests we predict
the outcome of such a game as a probability distribution. This is in contrast to what
is conventionally done (e.g, in the Nash bargaining solution), which is predict a single
outcome. We show how to translate Nash’s bargaining axioms to provide a distribution over
outcomes rather than a single outcome. We then prove that a subset of those axioms forces
the distribution over utility outcomes to be a power-law distribution. Unlike Nash’s original
result, our result holds even if the feasible set is finite. When the feasible set is convex
and comprehensive, the mode of the power law distribution is the Harsanyi bargaining
solution, and if we require symmetry it is the Nash bargaining solution. However, in
general these modes of the joint utility distribution are not the experimentalist’s Bayesoptimal predictions for the joint utility. Nor are the bargains corresponding to the modes of
those joint utility distributions the modes of the distribution over bargains in general, since
more than one bargain may result in the same joint utility. After introducing distributional
bargaining solution concepts, we show how an external regulator can use them to optimally
design an unstructured bargaining scenario. Throughout we demonstrate our analysis in
computational experiments involving flight rerouting negotiations in the National Airspace
System. We emphasize that while our results are formulated for unstructured bargaining,
they can also be used to make predictions for noncooperative games where the modeler
knows the utility functions of the players over possible outcomes of the game, but does not
know the move spaces the players use to determine those outcomes.

1. Introduction
In game theory, bargaining refers to scenarios where two or more people must come to
a joint agreement on an outcome. In structured bargaining, the scenario is modeled as a
noncooperative game, with the players making explicitly delineated alternating moves, e.g.,
proposals and counter-proposals (Osborne & Rubinstein, 1994; Aumann & Hart, 1992). In
contrast, in unstructured bargaining, the scenario is modeled without any explicit delineation
of the alternating moves. Instead all that is known to the modeler is the feasible set of all
the joint-utilities that would arise for all the possible bargains the humans might reach.
Arguably, in most real-world bargaining scenarios, the interaction between the bargainers
c
2013
AI Access Foundation. All rights reserved.

Wolpert & Bono

is “free form”, and so requires an unstructured bargaining analysis. Therefore, to be able
to predict and potentially emulate the behavior of interacting humans, we need to have an
accurate model of human unstructured bargaining.
In unstructured bargaining there are two spaces; the space of bargains and the space of
joint expected utility vectors that the players assign to each such bargain. Note that it may
be that more than one bargain results in the same outcome. As shorthand, it is conventional
to leave the term “expected” implicit and just refer to “utility”. Similar shorthand is to
refer to a joint (expected) utility vector as an “outcome”.
Most game theoretic modeling to date on unstructured bargaining starts with specification of a “feasible set” of all of the possible outcomes. This is the only knowledge the
modeler has concerning the unstructured bargaining scenario. In particular, the modeler is
ignorant of the spaces of possible “moves” the players may make to reach a bargain — and
is even ignorant of the space of possible bargains.
Traditional unstructured bargaining is concerned with specifying a map that takes any
such feasible set S to a single outcome x ∈ S, i.e., a “point-valued solution concept” (Nash,
1950; Harsanyi & Selten, 1972; Kalai & Smorodinsky, 1975; Kalai, 1977). For example,
this was the case with Nash’s original work, where he showed how for any specified S, his
axioms force a unique prediction of an outcome x ∈ S. Some of this work regards the map
normatively, as providing fair or reasonable bargaining outcomes. Other work regards it
positively, as a prediction for what agreement will be reached by humans bargaining in an
unstructured manner (Nydegger & Owen, 1974; Roth & Malouf, 1979; Camerer, 2003). In
this paper we are concerned with this positive viewpoint. For an extensive discussion on the
interpretation of Nash’s solution, see the work of Rubinstein, Safra, and Thomson (1982).
In contradiction to this theoretical positive work, the experimental literature makes clear
that in the real world more than one bargain has non-zero probability of being the outcome
of any given unstructured bargaining problem (Camerer, 2003; Roth & Malouf, 1979).1 To
accommodate this, in this paper we consider maps that take any unstructured bargaining
problem’s feasible set S to a probability distribution over S, rather than to a single element
of S.2 Doing this, we derive a parameterized set of possible maps from bargaining problems
to distributions over those problems. (Intuitively, the parameters reflect the “bargaining
power” of the players.) We call such maps “distributional bargaining concepts”, and call
their images “utility distributions”.
There are many advantages to using distributional bargaining concepts, in addition to
their according with experimental data better than point-valued solution concepts. A major
one arises if there is an external regulator who can modify some aspects of the bargaining
game and has their own utility function over bargaining outcomes. By modifying the game,
the regulator changes the associated distribution, and therefore modifies the value of their
own expected utility. Accordingly, they can calculate the Bayes-optimal modification to the
game.
1. In fact, given the noisy nature of human behavior, it would be stunning if certain physically possible
outcomes actually occurred with exactly 0 probability, rather than some small, non-zero probability.
2. Our approach allows S to be either finite or infinite; for succinctness we will generically refer to a
“probability distribution” even if S is infinite and we properly should refer to a probability density
function.

580

Predictive Unstructured Bargaining

Our approach to deriving a distributional bargaining concept is to “translate” Nash’s
axioms of unstructured bargaining — conventionally applied to maps that produce a single
utility outcome rather than a utility distribution — to apply to distribution-valued maps.
To be precise, we use probabilistic versions of Nash’s axioms of Scale Invariance (SI) and
Independence of Irrelevant Alternatives (IIA). We also use a probabilistic version of the
axiom of Translation-Invariance of utilities (TI).
Adopting a Bayesian perspective, we view these axioms as formalizations of the ignorance of the modeler in many scenarios, rather than as assumptions about human behavior:
For us, SI means that the modeler does not know anything about how the relative probabilities of outcomes chosen by the players is likely to change if one simply scales the utilities
of all possible outcomes. Similarly, for us IIA means the modeler does not know anything
about how the relative probabilities of outcomes chosen by the players is likely to change
if a subset of of the possible outcomes is removed. And for us, TI means that the modeler
does not know anything about how the relative probabilities of outcomes chosen by the
players is likely to change if the utilities of all possible bargains are simply translated by
the same constant.
To these axioms we add the extra one that all outcomes in which all players do strictly
better than the default outcome have Non-Zero probability (NZ). (NZ is imposed because
it holds in all experiments, due to subject inattention if nothing else.)
There is other work, which, like ours, is also concerned with distributions over S (Peters
& Tijs, 1984). However, the work of Peters and Tijs (1984) differs from ours in two important
ways. First, while they also use probabilistic versions of SI and IIA, unlike us, they also
use the axiom of Pareto optimality (PAR). (As discussed in the conclusion, we need not
impose the assumption of binding contracts, and therefore need not assume the joint utility
is Pareto-optimal.) In addition, unlike us, they do not use NZ, nor do they (explicitly) use
TI. Second, Peters and Tijs translate IIA and SI into different probabilistic versions than
ours.
As a result of these difference, we arrive at a very different distributional solution concept
from that of Peters and Tijs. In particular, our axioms force the distribution to be a power
law over the set of joint utility outcomes Pareto superior to the default outcome. In contrast,
the axioms used by Peters and Tijs do not result in a tightly characterized solution, power
law or otherwise. Accordingly, the approach of Peters and Tijs has not been used to derive
the Bayes-optimal modification to a game that might be applied by an external regulator
of the game.
Perhaps most importantly, our version of IIA means our solution concept applies to
any bargaining game, even finite ones that are non-convex and not comprehensive. This
is quite important for a positive bargaining solution concept. In real-world unstructured
bargaining, it is common for the bargainers to only consider a finite number of possible
outcomes. (Typically, real human bargainers in the field do not consider the option of
tossing a weighted coin to choose among the possible bargains.) In such scenarios the
feasible set is finite.
More generally, since we dispense with PAR, there is nothing that restricts our analysis
to scenarios that are traditionally viewed as “bargaining”. As we discuss in the conclusion,
our results can also be used to predict the outcomes of noncooperative games whenever
the modeler only knows the feasible set of that game’s joint-utility outcomes, and cannot
581

Wolpert & Bono

tractably elaborate the move spaces of the players. In such situations, the best the modeler
can do is provide a distribution over the final joint utility outcome. Our distributional
bargaining concept provides a way to do this.
The current paper and the work of Peters and Tijs are not the only papers to extend
game theory by replacing solution concepts that are point-valued (or set-valued) with solution concepts that are probability distributions. In particular, the work of Wolpert and
Bono (2011) introduces a distribution-valued solution concept for noncooperative games
when the modeler does know the move spaces. That work introduces a map that has an
input the specification of an arbitrary non-cooperative game. The output of the map is
a distribution over all possible mixed strategy profiles (i.e., all player joint choices) in the
input noncooperative game. In contrast, the present paper introduces a map taking the
specification of an unstructured bargaining game as its input, and producing a distribution
over all player joint utilities as its output.
There are also recent papers in the artificial intelligence literature that, like ours, focus on
“non-equilibrium solution concepts” (Brafman & Tennenholtz, 2003; Rezek, Leslie, Reece,
Roberts, Rogers, Dash, & Jennings, 2008; Aydoğan & Yolum, 2012; Duan, Doğru, Özen, &
Beck, 2012). Rather than merely invoking equilibrium, Brafman and Tennenholtz (2003)
employ a reinforcement learning algorithm to efficiently achieve coordination in common
interest stochastic games. Rezek et al. (2008) look at game theoretic solution concepts
from a machine learning perspective, i.e. they assume players make inferences about their
opponents in a Bayesian framework and derive a novel fictitious play algorithm. Recent
work in AI that focuses on bargaining, such as that of Aydoğan and Yolum (2012) and
Duan et al. (2012), is particularly closely related to the current paper. The main difference
is that our paper focuses on an unstructured game, whereas the focus in previous work has
been on structured games.
1.1 Contribution of this Paper
Our first contribution in this paper is to derive our distributional bargaining concept, as
outlined above. After this we focus on two major advantages of a distributional bargaining
concept over point-valued or set-valued bargaining concepts: 1) the ability of a modeler to
apply decision theory to predict outcomes, and 2) the ability of an external regulator to
employ control theory to optimally regulate the system.
To elaborate on the first advantage, consider that the modeler of the outcome of the
bargaining will often have a loss function, measuring the quality to them of predicting the
joint expected utility outcome is x when the actual outcome is x0 .3 Given such a loss
function, and given a distribution over outcomes, there is a well-defined Bayes-optimal
prediction for a single joint utility outcome. This Bayes-optimal prediction will vary with
the loss function. Accordingly, whatever distributional bargaining concept is used, in general
the associated Bayes-optimal prediction is neither the most likely joint expected joint utility
outcome (the Harsanyi solution) nor the most likely bargain.
Let s be the vector-valued function taking bargains to the associated joint utility outcomes. In general, s need not be invertible. (E.g., if the feasible set is a set of K possible
3. As shorthand, we will often abbreviate “joint expected utility outcome” to “joint utility outcome”, or
even “outcome”.

582

Predictive Unstructured Bargaining

joint choices among the N players, where K ≥ N + 1.) In such cases we cannot go from a
prediction for the joint utility (whether made with a conventional bargaining concept or a
distributional one) to a prediction for a joint bargain. One would need a likelihood function
P (b | x) giving the relative probabilities of all bargains b given the joint utility x to invert
a prediction for joint utility into a prediction for the bargain reached.
On the other hand, if s is invertible, then the distribution over joint utilities does fix the
distribution over bargains. However, even in this case, if the Jacobian of s is non-uniform,
then the most likely joint utility is not the most likely joint bargain. Intuitively, s may
“concentrate” probability density on some regions of possible joint utility outcomes (those
corresponding to relatively many possible bargains), and diminish it in others.
Note though, that when the feasible set is countable and s is invertible, the issue of a nonuniform Jacobian of s disappears. This provides yet another benefit to using a distribution
bargaining concept — like ours — that is applicable to countable (and even finite) feasible
sets, not just convex and comprehensive ones.
As an illustration of the foregoing, we consider a scenario naturally modeled as unstructured bargaining. Often the path of an aircraft through the National Airspace System is
renegotiated inflight, e.g., due to unforeseen weather. Such negotiations do not follow any
particular protocol — they are unstructured. Accordingly, we use them to illustrate the
Nash distributional bargaining model. We emphasize the fact that even in this simple scenario, the map s from bargains to joint utility outcomes is non-invertible. As a result, while
it is straightforward to evaluate the probability density function over possible joint utility
outcomes, the same cannot be said for evaluating relative probabilities of various bargains.
To address the second major advantage of our distribution-valued solution concept,
i.e. the ability of an external regulator to employ control theory to optimally regulate the
system, we consider an external regulator who has a real-valued welfare function defined
over the bargains of the players, and who can modify some parameters of the bargaining
game. We show how a distributional solution concept allows such an external regulator to
perform Bayes-optimal configuration of the bargaining game, i.e., how the regulator can
set the parameters of the game under their control to optimize the expected welfare of the
resultant bargaining outcome. In the context of the Nash distributional bargaining concept,
we call this approach to setting parameters “Nash distributional bargaining management”.
As an example, suppose that the regulator can modify the set of allowed bargains
within some range. In this case, the Nash distributional bargaining concept can be used
to determine the regulator’s optimal modification. Similar external interventions would be
changing the default bargain, or even modifying the relative bargaining power of the players.
A more complicated type of external intervention is where the regulator presents a
non-binding suggested bargain to the players. We view such a suggested bargain as a
Schelling-like focal point for the negotiations, and introduce a model mapping any such
suggested bargain to an associated distortion over the distribution over bargaining outcomes.
This model provides the regulator with a well-defined algorithm for choosing the suggested
bargain that optimizes the expected welfare of the outcome of the negotiations. We use the
application of flight rerouting negotiations to demonstrate Nash distributional bargaining
management using such suggested bargains.
The type of optimal control of strategic agents captured by the Nash distributional
bargaining management concept is a recent theme in the artificial intelligence literature. For
583

Wolpert & Bono

example, there is a growing literature focused on optimally managing negotiations between
strategic, self-interested agents (Brafman & Tennenholtz, 1996; Chalamish & Kraus, 2012;
Lopez-Carmona, Marsa-Maestre, Klein, & Ito, 2012). The primary difference between our
paper and the current literature is that our bargaining game is unstructured, while the others
use a structured game. The management of unstructured flight rerouting negotiations can
also be considered a type of constrained automated mechanism design, like the framework
studied by Vorobeychik, Reeves, and Wellman (2012). That is, the mechanism in our
paper is the bargaining scenario as modified by the (automated) external regulator for each
unique flight rerouting game. Similarly, in the domain of air traffic management, the work
of Agogino and Tumer (2012) studies a multi-agent approach to managing air traffic flows
in which agents are reinforcement learners. Once again, the main difference between our
paper and these others is the difference between controlling structured and unstructured
games.
Despite the recent interest in optimal control of strategic agents, the foundation for these
ideas can be found in more conventional concepts such as correlated equilibrium. As Ashlagi,
Monderer, and Tennenholtz (2008) point out, correlated equilibrium is often conceptualized
as arising from some external party that provides recommended actions to game participants
but cannot enforce its recommendations. Ashlagi et al. look at the value of correlation,
i.e. the welfare improvement arising from an external parties recommendations, which is
directly analogous to the welfare improvement arising from Nash distributional bargaining
management.
1.2 Roadmap of this Paper
In section 1.3 we start with an overview of our notation. Then in section 2, we provide a
general definition of distributional bargaining concepts. After this we focus on the “Nash
distributional bargaining concept”, which is defined by NZ and probabilistic versions of TI,
SI and IIA. We next prove that Nash distributional bargaining concepts take the form of
a power law distribution. This result holds for any bargaining games, even non-convex,
non-comprehensive ones, and even ones with a finite set of possible bargains.
In section 3, we discuss the Nash distributional bargaining concept. We focus on its
mathematical structure, its physical meaning, and its normative implications for an external
regulator who can change the feasible set. We show that the mode of the Nash distributional
bargaining concept is the Harsanyi bargaining solution. We then point out that if we also
impose Nash’s symmetry axiom, we instead get the Nash bargaining solution as the mode
of the distribution over outcomes.
In section 4 we introduce our model of flight rerouting negotiations. We show that this
setting yields a feasible set that is not convex, comprehensive, or even connected. Here we
also demonstrate the NDB model and its use in prediction. Here we also explore the issues
of invertibility of the bargaining set mentioned above.
In section 5 we develop the concept of Nash distributional bargaining management,
where an external regulator can make a recommendation to the bargainers or modify some
aspect of the unstructured bargaining game. We demonstrate these concepts using the flight
rerouting scenario and show that the external regulator can make welfare improvements.
We conclude with a discussion of the results and directions for future research.
584

Predictive Unstructured Bargaining

1.3 Notation
We consider conventional N -dimensional unstructured bargaining problems. Any such problem is the pair of a bounded feasible set, S ⊂ RN , and a special disagreement point, d ∈ S.
We assume S contains at least two elements, and that there exists an x ∈ S such that
x  d (i.e, such that the generalized strict inequality {xi > di ∀i} holds). We refer to such
a problem as simple if d = 0 and no point x ∈ S (i.e., no set of N utility values for that
bargaining problem) has a component less than 0. Given a bargaining problem (S, d), we
use the term “outcome” to refer to any element of S. We will refer to (S, d) as standard
if S contains an open set and is convex and comprehensive4 .
We define A + B for two sets A, B ⊂ RN to mean the set of all x that can be written
as a + b for some a ∈ A, b ∈ B, and similarly for A − B. (So in particular, A − B is not the
same as A \ B.) We will also assume the usual topology over RN , etc. Given any A ⊂ RN
and k ∈ RN where k  0, we define kA to be the subset of of RN given by replacing each
x ∈ A by the Hadamard (i.e., component by component) product kx , (k1 x1 , k2 x2 , . . .).
N
We define AB for any two sets A, B ⊂ R
R similarly.
As shorthand, throughout we use “R ” with the measure implicit. In particular, when
feasible sets are finite, expressions like “ dx . . .” implicitly use the point mass measure, i.e.,
are equivalent to sums.

2. Nash Distributional Bargaining Concepts
From a Bayesian perspective, we are interested in the posterior probability density function,
P (x | S, d). Here we do not proceed by decomposing that distribution into a prior and a
likelihood. Instead, in analogy to Nash’s approach, we model P (x | S, d) directly, by using
Nash’s axioms to restrict its form.
In doing this it will be convenient to simplify notation, so that (again following Nash) we
talk in terms of maps from (S, d) to distributions over x rather than in terms of P (x | S, d):
Definition 1. An N -dimensional (distributional) bargaining concept is a map from
any N -dimensional bargaining problem (S, d) to a probability density function with support
restricted to S. (When S is countable, the image is a probability distribution rather than a
probability density function, and we will generically refer to a “distribution” with the implicit cardinality of S making clear whether we mean a probability distribution or probability
density function.)
We will generically indicate a distributional bargaining concept with the symbol µ, and
indicate its value for a problem (S, d) as µS,d . So µS,d (x) is the non-negative real number
produced by applying µ to (S, d) and then evaluating the resultant density function at x.
To use any unstructured bargaining approach to make a prediction about human bargaining behavior in a particular physical setting (laboratory or field), in addition to explicitly specifying the bargaining problem (S, d), there must also be a specification of the
physical setting. Often this specification is only implicit, but here we make ours explicitly.
We restrict attention to bargaining problems (S, d) and associated physical settings such
that every x ∈ S is physically possible. We then go further and also restrict attention to
4. Recall that S is comprehensive if ∀x ∈ S, x  y  d, y ∈ S.

585

Wolpert & Bono

physical settings such that all those physically possible x ∈ S where x  d occur with some
non-zero probability. We justify this by noting that in most laboratory and field settings,
even if the bargaining were structured with a Pareto efficient outcome that is a dominant
Nash equilibrium of the underlying game, there would still be a non-zero probability that
other outcomes arise, due to bounded rationality of the players. (A stronger version of this
reasoning — which we do not invoke here — would require that even outcomes where some
player does worse than at the disagreement point occur with non-zero probability.)
Given the foregoing, it is straightforward to translate Nash’s unstructured bargaining
axioms into the context of distributional bargaining concepts. In particular, SI, TI and IIA
get translated as follows:
Definition 2. An N -dimensional distributional bargaining concept µ is a Nash distributional bargaining (NDB) concept if for any S ⊂ RN ,
pSI ∀x, y, d ∈ S such that x, y  d, ∀k  1,
µS,d (x)
µS,d (y)

µ(kS,kd) (kx)
,
µ(kS,kd) (ky)

=

pTI ∀x, d ∈ S such that x  d, ∀a ∈ RN , µS,d (x) = µS−{a},d−a (x − a),
pIIA ∀T ⊆ S, ∀x, y, d ∈ T such that x, y  d,
µS,d (x)
µS,d (y)

=

µT,d (x)
µT,d (y)

NZ ∀x, d ∈ S such that x  d, µS,d (x) 6= 0
pSI means that if we rescale all coordinates of a problem to get a new problem, the
relative probability of any two points x and y in the original problem is the same as the
relative probability of the rescaled versions of those points in the new (rescaled) problem.
This is the “translation” of Nash’s scaling invariance axiom, SI, into the context of distributional bargaining concepts. For the results in this paper, we could have weakened pSI to
the following:
pSI∗ For any simple, standard, closed problem (S, 0), ∀x, y ∈ S such that x, y  0, ∀k  1,
µS,0 (x)
µS,0 (y)

=

µ(kS,0) (kx)
µ(kS,0) (ky)

(1)

For expository simplicity though, we use the stronger version given in Def. 2.
pTI implements the assumption of translation-invariance of utility functions. (Recall
that in our notation, the subtraction operator on sets is not the same as set subtraction.)
pIIA means that if we remove potential solutions other than x and y from a bargaining
problem, the relative probabilities of x and y don’t change. This is the “translation”
of Nash’s IIA axiom into the context of distributional bargaining concepts. Finally, as
586

Predictive Unstructured Bargaining

mentioned earlier, the NZ axiom says that all outcomes in which all players do strictly better
than they do under the default outcome receive some positive probability of occurring. This
axiom requires the solution concept to conform to the physical reality of human behavior.
We emphasize that pIIA and pSI are not assumptions about human behavior. Rather
they are reflections of ignorance by the modeler, just as prior probabilities in Bayesian
modeling are reflections of such ignorance. If the modeler does not know of a preferred
scale with which to predict player behavior, then they want to use a distribution that
is invariant under rescalings (Jaynes & Bretthorst, 2003). That is what pIIA formalizes.
Similarly, if the modeler does not know how removing a set of alternatives would affect the
relative probabilities of the remaining ones, then they want to use a distribution that does
not affect those relative probabilities. That is what pSI formalizes.5
In the remainder of this section we work through a derivation of our main mathematical
result, presented in Thm. 1. Loosely speaking, this result says that a bargaining concept µ
must be a product (over the players) of power law distributions, if it is to obey the axioms
presented above.
Given any two N -dimensional problems (S, d) and (T, d), define W ≡ S ∪ T . So S ⊆ W .
Therefore by pIIA, for any N -dimensional Nash bargaining concept µ, ∀x, y ∈ S such that
x, y  d, µS,d (x)/µS,d (y) = µW,d (x)/µW,d (y). Similarly, T ⊆ W , and therefore ∀x, y ∈ T
such that x, y  d, µT,d (x)/µT,d (y) = µW,d (x)/µW,d (y). This establishes the following:
Lemma 1. Fix any two N -dimensional problems (S, d) and (T, d). ∀x, y ∈ S ∩ T such that
x, y  d, for any N -dimensional NDB concept µ,
µS,d (x)
µS,d (y)

=

µT,d (x)
µT,d (y)

(2)

The kinds of geometric distortions underlying existence proofs of the Nash, KalaiSmorodinsky and egalitarian solutions cannot be applied when (S, d) is non-standard, and
especially when S contains a finite number of elements. This inability to handle finite S has
been one of the major obstacles of the Nash bargaining theory. In real-world unstructured
bargaining, it is quite common for people to bargain over a finite number of possible outcomes, without ever considering the possibility of using a randomization device to decide
the final bargain.6
In contrast, Lemma 1 means that if we can establish the form of µT,d for a particular
class of problems (T, d) with infinite T , then we have established its form for any pair
(S ⊂ T, d). This is true even if S is neither convex nor comprehensive. Indeed, S can
even be finite.7 The following result provides such a form of µT,d for a particular class of
problems (T, d).
5. Of course, if the modeler does have knowledge about preferred scales and / or about how the human
players would change their distribution if some alternatives were removed, then that knowledge should
replace pSI and /or pIIA, respectively.
6. Indeed, one could argue that real bargaining scenarios cannot involve an uncountably infinite number of
bargaining outcomes. After all, real human beings cannot specify all digits of any real number in a given
interval. When communicating with one another, real human beings are only able to specify
√ numbers
having a finite precision, together with a finite set of infinite-precision real numbers, like π, 2, etc.
7. Other authors have examined bargaining solutions on non-standard domains. For example, many papers
extend the Nash solution to non-convex S (Kaneko, 1980; Herrero, 1989; Conley & Wilkie, 1996; Zhou,

587

Wolpert & Bono

Theorem 1. Let µ be an N -dimensional NDB concept such that for any closed, simple,
standard problem (T, 0), µT,0 (x) is differentiable throughout the interior of T . Then there
are constants ai , i = 1, . . . n, independent
Q of S and d, such that for all problems (S, d) and
all x ∈ S such that x  d, µS,d (x) ∝ i (xi − di )ai .
Proof. Define S 00 ≡ S − {d} and x0 ≡ x − d. Then use pTI to write µS,d (x) = µS 00 ,0 (x0 ). So
it suffices to prove the theorem for all x0  0 for the bargaining problem (S 00 , 0).
Let S 0 ≡ {S 00 \ {x00 ∈ S 00 : ∃i where x00i ≤ 0} ∪ {0}}. So S 0 is S 00 without any x00 for
which at least one player is not doing better than the disagreement point, unioned with the
disagreement point. (S 0 , 0) is a simple bargaining problem. In addition, by pIIA, for all
x0 , y 0  0
µS 0 ,0 (x0 )
µS 0 ,0 (y 0 )

=

µS 00 ,0 (x0 )
µS 00 ,0 (y 0 )

Therefore it suffices to prove the theorem for problem (S 0 , 0).
Let T be the convex, comprehensive closure of S 0 . So (T, 0) is a simple, standard
bargaining problem whose interior contains (S 0 , 0). Therefore by pIIA, it suffices to prove
the proposition for (T, 0).
Let ΩT be the interior of T . Note that by hypothesis, µT,0 (.) is a differentiable function
over ΩT .
For any N -dimensional z  0, define the N -dimensional vector ln[z] , (ln(z1 ), ln(z2 ), . . .).
Make similar definitions for ez and for the sets ln[A] and eA where A is any subset of RN .
Note that with the exception of points with zero-valued components, any point in T can be
written as the vector ez for some z ∈ RN , since (T, 0) is a simple bargaining problem. So
in particular, every point in ΩT can be written ez for some z ∈ RN .
Say that z is a point such that µT,0 (ez ) is non-zero (so that ez is in T ). Define the
associated scalar P(z) , ln[µT,0 (ez )]. Note that µT,0 (.) is non-zero over ΩT by Def. 2(i). In
addition, it has a continuous derivative throughout ΩT by our hypothesis. Therefore P(.)
has continuous derivative throughout N ≡ ln[ΩT ].
Choose a vector k all of whose components equal 1 except for component i, which is
in the interval (0, 1]. Define the set of all such k as Ki . Note that since T is simple and
standard, Ki ΩT ⊆ ΩT . Therefore ln[k] + N ⊆ N .
Combining pSI and pIIA, ∀x0 , y ∈ ΩT
µT,0 (kx0 )
µT,0 (ky)

=

µT,0 (x0 )
µT,0 (y)

Taking logarithms we get
P(ln[k] + ln[x0 ]) − P(ln[k] + ln[y]) = P(ln[x0 ]) − P(ln[y])
∀x0 , y ∈ ΩT , k ∈ Ki .
1997; Mariotti, 1998a). They generally do so by replacing certain of Nash’s axioms. More recent
work furthers the analysis of non-standard problems to include finite domains (Mariotti, 1998b; Xu &
Yoshihara, 2006; Kıbrıs & Sertel, 2007; Peters & Vermeulen, 2010). However, a general conclusion of
this work is that its goal is out of reach; this work typically fails to find a convincing solution concept
that closely resemble Nash’s solution and is single-valued for all finite problems.

588

Predictive Unstructured Bargaining

Given that P must be differentiable across N , we can take the partial derivative of
both sides of this equation with respect to ln[ki ]. Evaluate those derivatives in the limit as
∂P(v)
ki → 1. Doing this for all allowed x0 , y establishes that ∂P(u)
∂ui − ∂vi = 0 ∀u, v ∈ N , i.e., it
establishes that

∂P(u)
∂ui

is constant across N . Therefore P is linear in coordinate i across N .
0

Since this is true for all coordinates i, P(x0 ) = ln[µS 0 ,0 (ex )] is a hyperplane across
Q 0Nai.
0
0
Accordingly, µS 0 ,o (x ) must be a product of monomials across ΩT , i.e., µT,0 (x ) ∝ i (xi )
across ΩT . (The coefficients of the hyperplane give the powers {ai } of the monomials.)
Converting from x0 back to x gives the claimed result.

We refer to a distributional bargaining concept that meets the conditions in Thm. 1 as a
power law distributional bargaining concept.
The axioms giving a power law distribution do not always hold in the real world. As
a simple example, the default bargain d may serve as a focal point, in which case one
might presume that µS,d (d) > 0. In such cases, either the differentiability assumption
of Theorem 1 must be relaxed, or one of the axioms defining NDB’s must be. More
generally, even when differentiability is assumed and we do not model d as a focal point,
it may be possible to motivate other distributional bargaining concepts besides the power
law bargaining concept. In particular, there are several axiomatic arguments that motivate
predicting the behavior of a single decision maker according to a logit distribution in their
utilities (Train, 2003). Logit distributions over utilities can also be motivated for modeling
behavior of multiple interacting players in a noncooperative game, e.g., with the arguments
originally used to motivate the logit Quantal Response Equilibrium (McKelvey & Palfrey,
1995), or with arguments based on maximum entropy inference (Wolpert, Harre, Olbrich,
Bertschinger, & Jost, 2012). These arguments suggest (but do not formally
derive) the idea
Q
of predicting behavior as a product of logit distributions, µS,d (x) ∝ i exp(xi ), rather than
with a product of monomials. (One advantage of such a distributional bargaining concept
is that it does not require specification of d, since replacing xi with xi − di in the exponents
does not change the value of the probability distribution.)
For the rest of this paper, unless specified otherwise, we will restrict attention to power
law distributional bargaining concepts. Note that the computational complexity of evaluating such a power law distributional concept is not an issue: one simply needs to evaluate
the product of monomials to get the relative probabilities. If the normalization constant is
also desired, at worst, it can be gleaned via standard Monte Carlo methods.
For (a power law bargaining concept) µS,d to be normalized when S is a standard
problem, each ai must exceed −1. More generally though, S could be countably infinite,
with the utilities of player i given by xi ∈ {1/k +di : k = 1, 2, . . .}. For µS,d to be normalized
in this case, ai must exceed 0. Therefore by Lemma 1, ai must always exceed 0.

3. Discussion of Power Law Distributional Bargaining Concepts
In this section we discuss assorted mathematical characteristics of power law distributional
bargaining concepts, together with some of their physical implications.
589

Wolpert & Bono

3.1 The Relation between Power Law Distributional Bargaining Concepts and
Point-Valued Bargaining Concepts
It should be emphasized that to derive the functional form of Thm. 1 there was no need for
analogs of the last two of Nash’s unstructured bargaining axioms, PAR and SYM. However
it is straightforward to incorporate them if desired. One natural way to translate PAR into
the context of distributional unstructured bargaining would be to require that improving
every player’s utility (i.e., changing any x  d in a way that shrinks no component xi )
cannot decrease probability density. Imposing this on an NDB concept would rule out
having any ai less than 0.8
Similarly SYM can be translated to mean that all the ai must have the same value.
We will call a power law bargaining concept that meets this additional requirement a fully
Nash distributional bargaining concept.
For any fully NDB concept µ and any (S, d), µS,D
Q
over its supportQis the distribution i (xi − di )α for some scalar α. Now for fixed d, the level
curves in x ofQ i (xi − di )α are independent of α (assuming α 6= 0). So in particular the
maximum of i (xi − di )α is independent of α. Accordingly, to find that maximum,
Q we can
take α = 1. Accordingly, the mode of µS,d (x) is the maximum over x ∈ S of i (xi − di ).
In other words, the Nash bargaining solution of a bargaining problem (S, d) is the mode of
any fully NDB concept applied to (S, d).
In addition, for any fixed δ > 0, as we send α → ∞, the probability of the event {x
lies more than δ away from the Nash bargaining solution} → 0. In this sense, all solutions
other than the Nash solution become impossible in that limit.
However, in general if the feasible set of possible bargains is uncountable, the function
taking joint bargains to joint utility outcomes will have a non-uniform Jacobian. In such
cases, the most likely bargain is not the bargain corresponding to the Nash solution.
Furthermore, for finite α, in general the Bayes-optimal guess for x is not the Nash
bargaining
R solution. For example, for quadratic loss functions, the Bayes-optimal guess
for x is dx µS,d (x)x. This differs from the Nash solution, argmaxx µS,d (x). Furthermore,
consider expanding the i’th coordinate of the border of S, while leaving its border along
axis j unchanged. Then for quadratic loss, this change to S will in general change the
Bayes-optimal guess of xj , even if it does not change the Nash bargaining solution point.
As a final comment, recall that is we do not impose the SYM requirement on our power
law bagaining concept, the αi of the players may differ, i.e., the players are allowed to be
heterogenous. In this case the mode of the solution concept is not the Nash bargaining
solution but the “weighted Nash solution” of (Harsanyi & Selten, 1972), evaluated for the
very constants αi that are the power law exponents of the NDB concept.
3.2 The Mathematical Structure of Power Law Bargaining Concepts
Nowhere does the definition of power law bargaining concept explicitly refer to differences
xi − di . So why do those differences arise in Thm. 1? Ultimately, the reason is that the
definition of a bargaining problem requires specifying both the feasible set and a special
8. A more extreme way to impose PAR would be to make predictions using µP AR(S),d , where P AR(S) is
the Pareto frontier of S. By Lemma 1, this is equivalent to “masking” µS,d to the Pareto frontier of S,
i.e., to replacing it with a new distribution µ0S,d whose support is restricted to the Pareto frontier of S,
where for all x, x0 on that frontier, µ0S,d (x)/µ0S,d (x0 ) = µS,d (x)/µS,d (x0 ).

590

Predictive Unstructured Bargaining

disagreement point d within the feasible set. Due to this, the translation invariance condition
translates the point d along with the point x where µ gets evaluated. This causes the
difference between those two points to be an invariant of µ, as reflected in Thm. 1.
Note also that µS,d is a product distribution over its support. In this sense, independence
of the players is automatic. Such independence is not an explicit part of the definition of a
power law bargaining concept. Ultimately, this independence arises from the fact that the
SI axiom of Def. 2 involves scaling each player’s utility independently.9
One must be careful in interpreting this player independence. It does not mean that
under a distribution µS,d , the utility values of the players are statistically independent.
Unless S is a box, in general the border of S will serve to couple the players. For example,
when d = 0, we have
Y
YZ
µS,d (xi ) =
[ dx−i µS,d (xi , x−i )]
i

i

S

YZ
Y
∝
[ dx−i (xj )aj ]
i

S

j

Y a
6∝
[xi i ]
i

= µS,d (x),

(3)

i.e., the product of the marginal distributions of the player utilities does not equal the joint
distribution of the player utilities.
As an example of the implications of this, say the border of S is changed by having
the range of possible utility values for player i grow for some particular range of utilities
of the other players, but stay the same elsewhere.10 Then the statistical coupling between
the players will generally change. Intuitively, after this modification to S, what I can infer
about the likely value of xj6=i given a particular value of xi will have changed. (This is
despite the independence of irrelevant alternatives axiom.)
3.3 Physical Meaning of Power Law Distributional Bargaining Concepts
Thm. 1 does not specify the values of µS,d (x) if x 6 d. However, Q
it is often the case that for
any x ∈ S where x 6 d, there is an x0 such that x0  x, d where i (x0i − di )ai is arbitrarily
close to zero.11 Moreover, given that x0  x, it would be quite peculiar if in experiments it
were the case that µS,d (x) > µS,d (x0 ). This strongly suggests that if µ meets the conditions
in Thm. 1, then we should stipulate that µS,d (x) = 0 for any x 6 d. (Formally though, we
do not need to make that requirement in the analysis of this paper.)
9. For example, if we modified that axiom by first rotating the space RN , then applying the scaling operators,
and then rotating back, we would no longer have a product distribution in the individual xi , but rather
in linear combinations of the xi .
10. Formally, we specify some box over the values x−i : M ≡ {xj ∈ [bj , tj ] : j 6= i, tj > bj ∀j 6= i}. Then
over M alone, we extend the associated border of S along coordinate i, i.e., ∀x−i ∈ M , we modify S by
expanding the set of xi such that (xi , x−i ) ∈ S.
11. As an example, let S be a sphere centered on d, and consider any x such that x1 < 0 while at the same
time x2 , x3 , . . . > 0. Then the distribution of the point (, x2 , x3 , . . .) can be arbitrarily close to zero by
taking  small enough.

591

Wolpert & Bono

How the components ai in Thm. 1 are physically interpreted is determined by how
the distribution µS,d is physically interpreted. Ultimately, that has to do with what we
physically mean by an “unstructured bargaining scenario”.
One possible physical meaning is that µS,d is a population average over all humans and
over all unstructured games that have feasible set S and disagreement point d. In this
interpretation, if one adopts a power law bargaining concept, it makes sense to have the
constants ai be identical. Another possibility is that the distribution is not interpreted as a
population average, but rather to refer to a set of N particular individuals involved in the
bargaining problem at hand. In this case, if one adopts a power law bargaining concept, the
constants ai will differ in general. This is for several reasons. One is that different people
have different bargaining styles, and different powers of persuasion. More generally, in some
bargaining scenarios certain players will only have weak power to affect the outcome, or at
least not be able to affect all aspects of the outcome.
In general these two interpretations are not mutually consistent. That’s because averaging (a population of) different distributions all proportional to a product of monomials will
not give a product of monomials, in general. Which interpretation one adopts ultimately
depends on which interpretation one feels best characterizes the bargaining scenario under
consideration.
There is also a third possible interpretation, in which one averages not only over the
bargainers, but also over a set of structured bargaining scenarios. In this interpretation,
the distribution of “unstructured bargaining” is interpreted as an average over distributions
of structured bargaining scenarios. Nash’s bargaining axioms would then be interpreted as
reflecting the ignorance of the external modeler concerning the structure of the game the
players are engaged in.
3.4 Knitting Together 2-Player Distributions to Get Multi-Player
Distributions
There are problematic aspects to requiring that IIA applies to the full joint distribution
of all N players’ utilities when n > 2. However it seems less objectionable to stipulate
that IIA applies to the distribution of any two players’ utilities, conditioned on the utilities
of the other player(s). For example, we can require that P (x1 , x2 | x3 , . . . xN , S, d) obeys
IIA.12 We can also impose the other conditions defining power law bargaining concepts to
any such conditional distribution. The result is that any such conditional distribution is a
product of monomials, i.e., for all i, j 6= i,
P (xi , xj | x−i,−j , S, d) ∝ (xi − di )ai (x−i,−j ) (xj − dj )aj (x−i,−j )

(4)

where for full generality the exponents can vary with the value of x−i,−j .
This is a set of n(n − 1) equations involving the full joint distribution µS,d = P (x |
S, d), parameterized by the matrices {ai (x−i,−j )}. There are an infinite number of joint
distributions P (x | S, d) that obey all of these equations simultaneously for some set of
matrices {ai (x−i,−j )}: by inspection, any distribution which is a power law bargaining
12. Note that this condition does not concern a of scenario where players 3 through N somehow fix their
utilities to the values x3 , . . . xN , and after that the players 1 and 2 bargain. Rather it concerns the full
N -player bargaining scenario where all N players bargain together.

592

Predictive Unstructured Bargaining

Q
concept (i.e., of the form µS,d (x) ∝ i (xi − di )ai ) obeys those equations for the degenerate
case where the matrices are all constants.
More generally, requiring that all Eq.’s (4) simultaneously hold for some given set of
matrices {ai (x−i,−j )} provides constraints on how the conditioned two-player bargaining
concepts P (xi , xj | x−i,−j , S, d) “knit together” to give a full N -player bargaining concept.
To be precise, given matrices {ai (x−i,−j )}, the full joint distribution µS,d (x) = P (x | S, d)
must obey the following equations:
Z
dx µS,d (x) = 1,
Z
ai (x−i,−j )
aj (x−i,−j )
dx0−i,−j µS,d (xi , xj , x0−i,−j )
∀i, j 6= i, µS,d (x) ∝ (xi − di )
(xj − dj )
(5)
Future work involves investigating the properties of such “knitting together” conditional
bargaining concepts.

4. NDB Flight Path Rerouting
In this section, we demonstrate NDB using the example of flight rerouting negotiations in
the National Airspace System. Flight rerouting negotiations take place between the humans
in an aircraft’s cockpit and those manning air traffic control (ATC) when severe weather
or air traffic result in the need for changes to the scheduled flight path while the aircraft
is already airborne. Such en route rerouting is often referred to as “tactical rerouting”, to
distinguish it from “strategic rerouting”, which takes place between the airlines operation
center and ATC when a flight must be rerouted before it is airborne.
Tactical rerouting negotiations can be initiated by either the cockpit or ATC. Though
they generally have a back-and-forth, offer/counter type of feel, these negotiations do not
follow any set protocol. Therefore, an unstructured bargaining approach is appropriate for
studying tactical rerouting negotiations. (Strategic rerouting negotiations might also be
well suited to an unstructured bargaining approach.) In addition, it is worth emphasizing
that although there is a natural tendency for a pilot to defer to ATC, in light of the greater
knowledge of the latter, legally all responsibility ultimately lies with the pilot.
Tactical rerouting negotiations generally result in a distance/heading pair that determines a way-point through which the pilot agrees to fly. For example, a negotiation might
look like this:
Cockpit: Denver Center, United 1492. Request 20 degrees right for weather.
Controller: United 1492, how long will you need that heading?
Cockpit: Looks like about 40 miles or so.
At this point the controller might grant permission, or he might make a counter proposal,
such as “can you accept 20 [degrees] left?”. In the latter case, negotiations often continue
in the manner above. The distance/heading pair can be summarized by (l, θ), where θ ∈
593

Wolpert & Bono

[−90, 90] is an angle in degrees, and l ∈ [0, N ] is a distance in miles.13 This constitutes the
set of bargains B, i.e., B = [0, E] × [−90, 90].
The cockpit and ATC have preferences over the bargains they can make, which are
summarized by utility functions over the set of available bargains, B. For example, the
cockpit doesn’t want to fly too far off course, but it also doesn’t want to fly too close to a
storm center. In many cases, ATC might want what is best for the cockpit. Yet in other
cases, ATC might be concerned about air traffic or other things (e.g., the impact of the
rerouting on other traffic) that the cockpit does not know or care about.
To evaluate the utilities of a bargain (l, θ) for the cockpit and ATC, we need to evaluate
certain features of the flight path that results from it. To determine the flight path that
results from (l, θ) we first translate that bargain into a way-point in a Cartesian coordinate
system. In this Cartesian coordinate system, we say that the flight’s current position is
w1 = (0, 0), and that the flight is pointing in the direction of the positive horizontal axis.
Let θr be the radians equivalent of θ. This means that the agreed way-point (l, θ), is located
at w2 = (l cos θr , l sin θr ) in Cartesian coordinates.
After meeting its way-point, w2 , the flight will return to a “fix” further along its original
flight path. In some rerouting negotiations, this fix is also part of the negotiated bargain.
However, such bargains are relatively uncommon. To simplify our model, we assume that,
whatever the bargain, the flight will return to the fix located along the horizontal axis at
point w3 = (E, 0) in Cartesian coordinates. Using linear interpolation to connect w1 , w2 and
w3 , we create a constant-speed, constant-altitude 3D flight path, Λl,θ (t) = (el,θ (t), nl,θ (t)).
In order to simplify notation, we will refer to the components of Λl,θ (t) as e(t) and n(t)
with the dependence on (l, θ) implicit.
We now use Λl,θ (t) = (e(t), n(t)) to calculate utility-relevant features of the bargain
(l, θ). These features include the total length of Λl,θ (t), given by
s
Z
∂e(t) 2 ∂n(t) 2
Ll,θ = dt
+
,
∂t
∂t
and whether or not Λl,θ (t) maintains a safe distance from weather with center C and radius
R,
(
1 if mint ||Λl,θ (t) − C|| > R
Dl,θ =
0 otherwise.
We assume the cockpit’s utility is a linear combination of total length and maintenance of
safe distance, given by:
sc (l, θ) = αL Ll,θ + αD Dl,θ
for real numbers αL ≤ 0 and αD ≥ 0.
We assume ATC’s utility is a linear combination of maintenance of safe distance and the
new flight path’s impact on existing air traffic. We express this through the traffic penalty
function H(e, n), defined over Cartesian coordinates. The total traffic penalty for flight
path Λl,θ (t) is
Z
Hl,θ =

dt H(e(t), n(t)).

13. We limit the angle to [−90, 90] because heading changes of more than 90 degrees are extremely uncommon.

594

Predictive Unstructured Bargaining

Feasible Set of Utility Outcomes
−800
d
−1000

Cockpit Utility

−1200

−1400

−1600

−1800

−2000

−2200
−600

−400

−200

0

200
400
ATC Utility

600

800

1000

1200

Figure 1: Feasible set of joint utility outcomes for the rerouting unstructured bargaining
problem. Not only is the feasible set non-convex and not comprehensive, it is not
even connected. Parameters are ac = aatc = 1, αL = −4, αD = 300, βH = −0.01
and βD = 300. The weather is a large circle with center C = (150, 20) and radius
R = 40. The traffic penalty is given by H(e, n) = n.
We express ATC’s utility as satc (l, θ) = βD D + βH Hl,θ for some real numbers βD ≥ 0 and
βH ≤ 0.
The only thing left to specify is the outcome when negotiations break down, i.e., the
default bargain, db . It might be that, in the event negotiations break down, the cockpit will
choose the path that maximizes its own objectives without getting clearance from ATC. In
this scenario, the punishment the cockpit receives as a result of changing its course without
ATC approval should also enter the cockpit’s objective. However, there are likely many
complicated and variable factors to consider when modeling the effect of punishment, such
as pilot attitude, idiosyncratic airline rules, etc. Hence, for simplicity, we simply take the
default bargain to be the original flight path, i.e., the straight line between (0, 0) and (E, 0).
Therefore, the bargaining game is given by
S = {(xc , xatc ) ∈ R2 : xc = sc (l, θ) and xatc = satc (l, θ) for some (l, θ) ∈ B}
and d = (sc (E, 0), satc (E, 0)).
Figure 1 shows the feasible set of utility outcomes associated with the flight rerouting
model. Note that this set is neither convex nor comprehensive. Furthermore, there are
breaks, so that the feasible set consists of disconnected subsets. Despite these irregularities
of the feasible set, the NDB distribution can be applied trivially, giving
µS,d (x) ∝ (xc − dc )ac (xatc − datc )aatc
595

Wolpert & Bono

Indifference Curves with Multiple Intersections
1.5

Heading (radians)

1

0.5

ATC
Cockpit

0

−0.5

−1

−1.5
0

50

100

150
Distance

200

250

300

Figure 2: Indifference curves of sc and satc plotted in the space of bargains for the flight
path rerouting problem. s(., .) is not invertible because the indifference curves
have multiple crossings. Parameters are ac = aatc = 1, αL = −4, αD = 300,
βH = −0.01 and βD = 300. The weather is a large circle with center C = (150, 20)
and radius R = 40. The traffic penalty is given by H(e, n) = n.
for all x ∈ S such that x  d, and µS,d (x) = 0 otherwise.
As discussed in the introduction, if we want to use the NDB distribution over joint
utilities to make predictions about the bargains in b ∈ B, but don’t have a likelihood
function P (b | x), then we must be able to invert the mapping s = (sc , satc ). Unfortunately,
s is not invertible, as can be seen by noting from figure 2 that the indifference curves of sc
and satc have multiple crossings.
Recall that if the map s were invertible, then to translate from the distribution over
joint utilities to the distribution over bargains we would have to use the Jacobian of s. To
be precise, if (l, θ) is a point in B, then the NDB distribution over bargains, µB
S,d , evaluated
at (l, θ) is given by
µB
S,d (l, θ) = µS,d (sc (l, θ), satc (l, θ)) |J(l, θ)|,
where J(l, θ) is the determinant of the Jacobian of
 ∂s
∂sc
c

∂θ
 ∂l
 ∂satc ∂satc
∂l

∂θ

s,


.


Note though that even if s were invertible, the Jacobian might not be well-defined. Indeed,
our flight-rerouting s contains discontinuities where flight paths cross from outside to inside
596

Predictive Unstructured Bargaining

NDB Concept Translated to the Space of Bargains
80
60
40

Heading

20
0
−20
−40
−60
−80
0

50

100

150
Distance

200

250

300

Figure 3: NDB concept mapped back into the space of bargains B 0 = [0, 1, ..., N ] ×
[−90, −89, ..., 89, 90], for ac = aatc = 1, αL = −4, αD = 300, βH = −0.01
and βD = 300. The weather is a large circle with center C = (150, 20) and radius
R = 40. The traffic penalty is given by H(e, n) = n.
the weather radius, and the variable Dl,θ jumps from 1 to zero. Partial derivatives of sc and
satc do not exist at such points in B.
Say that the set of bargains were countable, e.g., a grid imposed by the technological
constraints of aircraft or the cognitive constraints of pilots and air traffic controllers. In this
case there would be no need to worry about discontinuities or how to calculate the Jacobian.
s might still be non-invertible however, so that we cannot transform the distribution over
joint utilities to a distribution over bargains. In fact this happens with the flight-rerouting
s when the set of bargains is modified to be a grid of integer distances and degrees, i.e.,
B 0 = [0, 1, ..., N ] × [−90, −89, ..., 89, 90].

Regardless of the cardinality of B, one can translate a distribution over joint utilities to a
distribution over bargains even when s is non-invertible, so long as one knows the likelihood
of bargains given joint utility outcomes. As an example, say that B is countable, and all
bargains that give rise to any particular joint utility x are equally likely. In this case, if
there are |Bx | bargains that give rise to x, then the probability of any such bargain is simply
µS,d (x)/|Bx |. Using this assumption and the grid B 0 = [0, 1, ..., N ] × [−90, −89, ..., 89, 90],
figure 3 shows the NDB distribution for the flight-rerouting problem in the space of bargains.
597

Wolpert & Bono

5. NDB Management
Suppose a regulator, external to an unstructured bargaining game, has preferences over
the joint utility of the bargain reached, formalized as a real-valued social welfare function
U (x ∈ S). Suppose further that they can change some characteristics ρ of (S, d). For
example, they might be able to change the joint utility d of the default bargain in certain
ways, or replace S with one of several S 0 ⊂ S. In such cases, assuming they model the game
with an NDB µS,d , the regulator should choose
ρ = argmaxρ Eρ (U (x))
Z
= argmaxρ dx U (x)µSρ ,dρ (x)

(6)

More generally, ρ may affect s(.), dB , and/or B, not just their image (S, d). In this more
general setting the regulator should choose
ρ = argmaxρ Eρ (U )
Z
= argmaxρ dx U (x)µsρ (B ρ ),sρ (dρ ) (x)
B

(7)

More generally still, the social welfare function U might be replaced with a function W
defined over the space of bargains B. So long as sρ is invertible for all ρ, Eq. 7 still holds
for this variant. (Just define U (x ∈ sρ (Bρ )) ≡ W (s−1
ρ (x)).) However if sρ is non-invertible
for certain ρ, this equation must be modifed. In this situation, assuming they have the
associated likelihood function, the regulator should choose the action
ρ = argmaxρ Eρ (W (b))
Z
= argmaxρ dxdb P (b | x)W (b)µsρ (B ρ ),sρ (dρ ) (x)
B

(8)

In addition to these types of actions by the regulator, there are others that are not
considered in typical axiomatic analyses of unstructured bargaining — including the analysis
of this paper up to this point. An example is where the regulator’s intervention is simply to
suggest a bargain to the players before they start to bargain, i.e., to provide them a “focal
bargain” for their bargaining. To advise the regulator in such cases, we need to model the
effects of these types of action by the regulator, and use that model in Eq. 7 .
We refer to any of these types of optimal choice by the external regulator as “NDB
management”. We do not mean to claim that it is always possible to implement NDB
management. In some situations an external regulator may not be able to do anything that
will have a substantial effect on the distribution over possible bargains by the players. In the
rest of this section we present some preliminary analysis of NDB management, concentrating
on an example where the regulator’s possible action is to provide a focal bargain.
5.1 NDB Management in Rerouting Negotiations
Here we demonstrate a simple form of NDB management using the example of rerouting negotiations from Section 4, related to the focal point concept introduced by Schelling (1960).
Specifically, we assume that the regulator can make a suggestion for a flight rerouting to
598

Predictive Unstructured Bargaining

both the cockpit and ATC before they begin rerouting negotiations. The idea is that this
suggestion will serve as a focal bargain for negotiations, having the effect of raising the relative probability of bargains similar to the suggested one. This suggestion by an “external
regulator” could be implemented by some automated software that operates both in the
cockpit and in ATC.14
Use µB
S,d (l, θ) to indicate the distribution over (l, θ) ∈ B induced by the NBD for game
(S, d). (So if needed, µB
S,d (l, θ) implicitly depends on the Jacobian of s and a likelihood
function P (b | x) = P (l, θ | x).) Then we assume the regulator models the effects of a
suggested focal bargain (l0 , θ0 ) on the distribution over B as


ψl0 ,θ0 (l, θ) γ B
Pl0 ,θ0 (l, θ) ∝
µS,d (l, θ)1−γ
Z(B)
where ψ is a bivariate Gaussian distribution with mean (l0 , θ0 ) and covariance matrix Σ,
γ ∈ [0, 1] measures the strength of the impact of the manager’s suggestion on the players’
behavior, and Z(B) is the normalization constant for ψ. We call this the “NDB management
distribution”. It is clearly an ad hoc model of focal bargains; we use it here only as a simple
way to demonstrate NDB management.
We assume that to minimize her computational requirements, the regulator requires
that the space of bargains be a grid of integer distances and degrees, i.e., B 0 = [0, 1, ..., E] ×
[−90, −89, ..., 89, 90].15 As mentioned in Section 4, even in this finite set of possible bargains
there are instances where multiple bargains map to the same joint utility outcomes. As
usual, to address this we must specify P (b | x); here we make the simple assumption that
this likelihood is uniform over all b that map to x, and zero for all others. Accordingly,
µB
S,d (l, θ) =

µS,d (s(l, θ))
,
|Bx |

where |Bx | is the number of bargains in B that give rise to outcome x.
As usual, the regulator will choose her action — which here means suggest a heading and
distance — to maximize the resultant expected social welfare. Writing the social welfare
function as W (l, θ), this optimal action is
Z
argmax(l0 ,θ0 )∈B El0 ,θ0 ,S,d (W ) =
dl dθ W (l, θ)Pl0 ,θ0 (l, θ).
B

In general, W might depend on the impact of the rerouting on traffic flow over the entire
National Airspace System, in addition to depending on the utility of the cockpit and of ATC.
For simplicity though, here we take W (l, θ) = τ sc (l, θ) + (1 − τ )satc (l, θ) where τ ∈ [0, 1].
Figure 4 depicts the effect of a regulator’s recommendation on the predicted flight path.
The left panel is the NDB distribution over bargains, µB
S,d . It represents the prediction
before a regulator makes a suggestion. The NDB modal bargain produces the detoured
flight path denoted in the center panel by the sequence of two lines joined by an open
14. Experimental studies find empirical support for a theory of focal points and bargaining (Binmore,
Swierzbinski, Hsu, & Proulx, 1993).
15. In practice, this may not need to be an explicit requirement, since it is unlikely that non-integer values
would be considered by the players.

599

Wolpert & Bono

NDB Distribution over Bargains

Flight Paths

NDB Management Distribution

80

80
50

60

60

40
30

40

40

20

0

−20

20
10

Heading

North−South

Heading

20

0

0

−10
−20
−20

−40

−40

−30
−40

−60

−60

−50
−80

−80
0

50

100

150

Distance

200

250

300

0

50

100

150

West−East

200

250

300

0

50

100

150

200

250

300

Distance

Figure 4: Left Panel: NDB distribution over B 0 = [0, 1, ..., E] × [−90, −89, ..., 89, 90] with
ac = 1, aatc = 3, αL = −4, αD = 300, βH = 0.01 and βD = 300. Center Panel:
Weather is the large circle with center C = (150, 20) and radius R = 40, NDB
modal flight path (circle line), NDB management modal flight path (square line),
NDB management recommended waypoint (triangle line). Right Panel: NDB
management distribution with τ = .5, γ = .5, σθ = 360 and σl = 1, 200. The
traffic penalty is given by H(e, n) = n.

circle; the flight begins its detour at the point (0, 0) and returns to its original flight path
at (300, 0). The reason the modal flight path does not pass closer to the weather is that the
traffic penalty is given by H(e, n) = n, i.e., traffic is denser in the north. This means ATC is
rewarded for flight paths that pass further to the south. And since aatc = 3 > 1 = ac , ATC
is relatively successful at convincing the cockpit to accept bargains that result in longer,
more southerly flight paths.
The NDB management distribution is shown in the right panel. It represents the prediction after the regulator has made an optimal suggestion. In this case, the optimal suggestion
is (130, −10). The path associated with the optimal suggestion is denoted in the center panel
by the pair of lines joined by a triangle. The modal bargain of the NDB management distribution induced by that suggestion is denoted by the pair of lines joined by a square. Note
that the regulator’s suggestion has the effect of skewing the distribution toward bargains
with shorter distances l and smaller heading changes. This is because the regulator assigns
equal weight to ATC and cockpit utility, whereas the NDB distribution is skewed toward
outcomes that are better for ATC. As a result, the modal bargain shifts significantly to the
north.
600

Predictive Unstructured Bargaining

6. Conclusion and Future Work
In both laboratory and field unstructured bargaining experiments, it is typically found
that more than one bargain can arise. To accommodate this, in this paper we consider
maps that take any unstructured bargaining problem S to a probability distribution over
S, rather than to a single element of S. Our approach is to “translate” Nash’s axioms of
unstructured bargaining to apply to this distribution-valued map. Doing this, we derive
the “Nash distributional bargaining concept”, which maps any feasible set of joint utility
outcomes to a power law over that set. This power law nature is intriguing due to the
ubiquity of empirical power laws in the real economy, e.g., in wage distributions, city sizes,
etc.
Future work involves trying to translate variants of Nash’s axioms into a distributional
bargaining concept. For example, it may prove possible to translate the weak monotonicity
axiom of the Kalai-Smorodinsky solution concept (Kalai & Smorodinsky, 1975) into probabilistic terms. If so, then by combining it with our probabilistic versions of the remaining
Kalai-Smorodinsky axioms (which are shared with the axioms of the Nash bargaining concept), we may be able to produce a “Kalai-Smorodinsky distributional bargaining concept”.
There are many advantages to using distributional bargaining concepts, in addition to
their according with the experimental fact that multiple bargains can arise for any given
game. One advantage is that such concepts seamlessly accommodate feasible sets that are
not convex and comprehensive, and even finite feasible sets. Another advantage arises if
there is an external regulator who can modify some aspects of the bargaining game and has
their own utility function over bargaining outcomes. By modifying the game, the regulator
changes the associated distribution, and therefore modifies the value of their own expected
utility. Accordingly, they can calculate the Bayes-optimal modification to the game.
We emphasize that there is nothing in the formal definition of a bargaining game or in
our translation of Nash’s axioms that restricts our analysis to scenarios that are traditionally
viewed as “bargaining”. In particular, we do not assume binding contracts. Therefore there
is no reason to assume that the outcome of the game has to be Pareto optimal. (Without
a binding contract on a player’s behavior, that player will typically have both an incentive
and the ability to change her behavior in a way that helps her but hurts her opponent, and
thereby moves the final outcome off of the Pareto frontier.) For this reason, distributional
bargaining concepts can be applied to model any noncooperative game where the modeler
only knows the feasible set of the game’s joint utility outcomes, and does not know (or
cannot tractably elaborate) the full underlying extensive form of the game. In modeling
noncooperative game behavior with such limited information, the best the modeler can do
is provide a distribution over the final joint utility outcome. Intuitively speaking, such a
distribution amounts to an “average” over all extensive form games that have the associated
feasible set of possible joint utilities. The Nash distributional bargaining concept provides a
way to form such a distribution over outcomes, using axioms that seem applicable in many
scenarios not traditionally viewed as unstructured bargaining.16
16. The only subtlety with using the Nash distributional bargaining concept this way is that there needs to
be an outcome of the noncooperative game that can reasonably be expected to behave as the “default
point” d. However even this restriction can be dispensed with using some distributional bargaining
concepts, e.g., using the logit one mentioned at the end of Sec. 2.

601

Wolpert & Bono

As an illustration, in the work of Smith, Suchanek, and Williams (1988) an artificial
speculative market was run for T rounds. This created a bubble which burst, leaving some
test subjects poorer and some richer. Predicting the moves in the full extensive form game
the subjects engaged in over the T rounds is laborious, at best. As an alternative, one could
imagine predicting the experiment’s outcome based only on knowing its feasible set, i.e.,
knowing all ways the money could end up divided among the subjects at the end of the
experiment. Future work involves analyzing the Nash distributional bargaining concept for
such situations.
Such market experiments could prove to be a useful source of data for testing alternative
distributional bargaining theories, in addition to experiments explicitly viewed in terms of
unstructured bargaining. In particular, if it is possible to produce a Kalai-Smorodinsky
distributional bargaining concept, as discussed above, it may be possible to compare the
predictions of such an alternative theory against the NDB concept using data from market
experiments. Such a comparative study would be analogous to the types of meta-studies
conducted to compare non-cooperative solution concepts using experimental data (Wright
& Leyton-Brown, 2010).
As discussed above, the function s plays a prominent role in making predictions about
bargains rather than about joint utilities. Arguably, knowledge of s should even affect the
predictions we make over joint utility outcomes. As an example, say the set of possible
bargains is so large that the players cannot be expected to examine it completely, and that
s concentrates the vast majority of bargains onto a single joint utility x, e.g., one on the
Pareto frontier. In this case, it would seem reasonable to ascribe higher probability to that
x than we would if only a single bargain mapped to it. However traditional approaches to
unstructured bargaining ignore s when predicting joint utilities, relying on axioms solely
concerned with the space of joint utilities. For the most part, that is the approach we
adopted here. (The section on optimal recommendations diverged from this in modeling
the effect of the recommendation.) Future work involves incorporating knowledge of s
directly into the predictions of joint utility outcomes.

Acknowledgments
We would like to thank Sylvia Thoron for helpful discussion.

References
Agogino, A., & Tumer, K. (2012). A multiagent approach to managing air traffic flow.
Autonomous Agents and Multi-Agent Systems, 24, 1–25. 10.1007/s10458-010-9142-5.
Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). On the value of correlation. Journal
of Artificial Intelligence Research, 33, 575–613.
Aumann, R., & Hart, S. (1992). Handbook of Game Theory with Economic Applications.
North-Holland Press.
602

Predictive Unstructured Bargaining

Aydoğan, R., & Yolum, P. (2012). Learning opponent’s preferences for effective negotiation:
an approach based on concept learning. Autonomous Agents and Multi-Agent Systems,
24, 104–140. 10.1007/s10458-010-9147-0.
Binmore, K., Swierzbinski, J., Hsu, S., & Proulx, C. (1993). Focal points and bargaining.
International Journal of Game Theory, 22, 381–409. 10.1007/BF01240133.
Brafman, R. I., & Tennenholtz, M. (1996). On partially controlled multi-agent systems.
Journal of Artificial Intelligence Research, 4, 477–507.
Brafman, R. I., & Tennenholtz, M. (2003). Learning to coordinate efficiently: A modelbased approach. Journal of Artificial Intelligence Research, 19, 11–23.
Camerer, C. (2003). Behavioral Game Theory: Experiments in Strategic Interaction. Princeton University Press.
Chalamish, M., & Kraus, S. (2012). Automed: an automated mediator for multi-issue
bilateral negotiations. Autonomous Agents and Multi-Agent Systems, 24, 536–564.
10.1007/s10458-010-9165-y.
Conley, J. P., & Wilkie, S. (1996). An extension of the nash bargaining solution to nonconvex
problems. Games and Economic Behavior, 13, 26–38.
Duan, L., Doğru, M., Özen, U., & Beck, J. (2012). A negotiation framework for linked
combinatorial optimization problems. Autonomous Agents and Multi-Agent Systems,
25, 158–182. 10.1007/s10458-011-9172-7.
Harsanyi, J., & Selten, R. (1972). A generalized nash solution for two-person bargaining
games with incomplete information. Management Science, 18, 80–106.
Herrero, M. J. (1989). The nash program: Non-convex bargaining problems. Journal of
Economic Theory, 49 (2), 266 – 277.
Jaynes, E. T., & Bretthorst, G. L. (2003). Probability Theory : The Logic of Science.
Cambridge University Press.
Kalai, E. (1977). Proportional solutions to bargaining situations: Interpersonal utility
comparisons. Econometrica, 45 (7), 1623–1630.
Kalai, E., & Smorodinsky, M. (1975). Other solutions to nashs bargaining problem. Econometrica, 43 (3), 513518.
Kaneko, M. (1980). An extension of the nash bargaining problem and the nash social welfare
function. Theory and Decision, 12, 135–148. 10.1007/BF00154358.
Kıbrıs, O., & Sertel, M. (2007). Bargaining over a finite set of alternatives. Social Choice
and Welfare, 28, 421–437. 10.1007/s00355-006-0178-z.
Lopez-Carmona, M., Marsa-Maestre, I., Klein, M., & Ito, T. (2012). Addressing stability issues in mediated complex contract negotiations for constraint-based, nonmonotonic utility spaces. Autonomous Agents and Multi-Agent Systems, 24, 485–535.
10.1007/s10458-010-9159-9.
603

Wolpert & Bono

Mariotti, M. (1998a). Extending nash’s axioms to nonconvex problems. Games and Economic Behavior, 22 (2), 377 – 383.
Mariotti, M. (1998b). Nash bargaining theory when the number of alternatives can be finite.
Social Choice and Welfare, 15, 413–421. 10.1007/s003550050114.
McKelvey, R. D., & Palfrey, T. R. (1995). Quantal response equilibria for normal form
games. Games and Economic Behavior, 10, 6–38.
Nash, J. (1950). The bargaining problem. Econometrica, 18 (2), 155162.
Nydegger, R. V., & Owen, G. (1974). Two-person bargaining: An experimental
test of the nash axioms. International Journal of Game Theory, 3, 239–249.
10.1007/BF01766877.
Osborne, M., & Rubinstein, A. (1994). A Course in Game Theory. MIT Press, Cambridge,
MA.
Peters, H., & Tijs, S. (1984). Probabilistic bargaining solutions. In Operations Research
Proceedings. Springer-Verlag.
Peters, H., & Vermeulen, D. (2010). WPO, COR, and IIA bargaining solutions. accepted
by International Journal of Game Theory.
Rezek, I., Leslie, D. S., Reece, S., Roberts, S. J., Rogers, A., Dash, R. K., & Jennings,
N. R. (2008). On similarities between inference in game theory and machine learning.
Journal of Artificial Intelligence Research, 33, 259–283.
Roth, A. E., & Malouf, M. W. K. (1979). Game-theoretic models and the role of information
in bargaining. Psychological Review, 86 (6), 574–594.
Rubinstein, A., Safra, Z., & Thomson, W. (1992). On the interpretation of the nash bargaining solution and its extension to non-expected utility preferences. Econometrica,
60 (5), 1171–1186.
Schelling, T. (1960). The strategy of conflict. Harvard university press.
Smith, V. L., Suchanek, G. L., & Williams, A. W. (1988). Bubbles, crashes, and endogenous
expectations in experimental spot asset markets. Econometrica, 56 (5), pp. 1119–1151.
Train, K. E. (2003). Discrete Choice Methods with Simulation. Cambridge University Press.
Vorobeychik, Y., Reeves, D. M., & Wellman, M. P. (2012). Constrained automated mechanism design for infinite games of incomplete information. accepted by Journal of
Autonomous Agents and Multiagent Systems.
Wolpert, D. H., Harre, M., Olbrich, E., Bertschinger, N., & Jost, J. (2012). Hysteresis effects
of changing parameters in noncooperative games. Physical Review E, 85, 036102. DOI:
10.1103/PhysRevE.85.036102.
604

Predictive Unstructured Bargaining

Wolpert, D. H., & Bono, J. W. (2011). Distribution-valued solution concepts. working
paper.
Wright, J. R., & Leyton-Brown, K. (2010). Beyond equilibrium: Predicting human behavior
in normal form games. In Twenty-Fourth Conference on Artificial Intelligence (AAAI10). forthcoming.
Xu, Y., & Yoshihara, N. (2006). Alternative characterizations of three bargaining solutions
for nonconvex problems. Games and Economic Behavior, 57 (1), 86 – 92.
Zhou, L. (1997). The nash bargaining theory with non-convex problems. Econometrica,
65 (3), 681–685.

605

Journal of Artificial Intelligence Research 46 (2013) 687-716

Submitted 12/12; published 04/13

NuMVC: An Efficient Local Search Algorithm
for Minimum Vertex Cover
Shaowei Cai

SHAOWEICAI . CS @ GMAIL . COM

Key Laboratory of High Confidence Software Technologies
Peking University, Beijing, China

Kaile Su

K . SU @ GRIFFITH . EDU . AU

Institute for Integrated and Intelligent Systems
Griffith University, Brisbane, Australia

Chuan Luo

CHUANLUOSABER @ GMAIL . COM

Key Laboratory of High Confidence Software Technologies
Peking University, Beijing, China

Abdul Sattar

A . SATTAR @ GRIFFITH . EDU . AU

Institute for Integrated and Intelligent Systems
Griffith University, Brisbane, Australia

Abstract
The Minimum Vertex Cover (MVC) problem is a prominent NP-hard combinatorial
optimization problem of great importance in both theory and application. Local search has proved
successful for this problem. However, there are two main drawbacks in state-of-the-art MVC local
search algorithms. First, they select a pair of vertices to exchange simultaneously, which is timeconsuming. Secondly, although using edge weighting techniques to diversify the search, these
algorithms lack mechanisms for decreasing the weights. To address these issues, we propose two
new strategies: two-stage exchange and edge weighting with forgetting. The two-stage exchange
strategy selects two vertices to exchange separately and performs the exchange in two stages. The
strategy of edge weighting with forgetting not only increases weights of uncovered edges, but also
decreases some weights for each edge periodically. These two strategies are used in designing a
new MVC local search algorithm, which is referred to as NuMVC.
We conduct extensive experimental studies on the standard benchmarks, namely DIMACS
and BHOSLIB. The experiment comparing NuMVC with state-of-the-art heuristic algorithms
show that NuMVC is at least competitive with the nearest competitor namely PLS on the
DIMACS benchmark, and clearly dominates all competitors on the BHOSLIB benchmark. Also,
experimental results indicate that NuMVC finds an optimal solution much faster than the current
best exact algorithm for Maximum Clique on random instances as well as some structured ones.
Moreover, we study the effectiveness of the two strategies and the run-time behaviour through
experimental analysis.

1. Introduction
The Minimum Vertex Cover (MVC) problem consists of, given an undirected graph G = (V, E),
finding the minimum sized vertex cover, where a vertex cover is a subset S ⊆ V such that every
edge in G has at least one endpoint in S. MVC is an important combinatorial optimization problem
with many real-world applications, such as network security, scheduling, VLSI design and industrial
machine assignment. It is equivalent to two other well-known combinatorial optimization problems:
the Maximum Independent Set (MIS) problem and the Maximum Clique (MC) problem, which
c
2013
AI Access Foundation. All rights reserved.

C AI , S U , L UO & S ATTAR

have a wide range of applications in areas such as information retrieval, experimental design, signal
transmission, computer vision, and also bioinformatics problems such as aligning DNA and protein
sequences (Johnson & Trick, 1996). Indeed, these three problems can be seen as three different
forms of the same problem, from the viewpoint of practical algorithms. Algorithms for MVC
can be directly used to solve the MIS and MC problems. Due to their great importance in theory
and applications, these three problems have been widely investigated for the last several decades
(Carraghan & Pardalos, 1990; Evans, 1998; Pullan & Hoos, 2006; Richter, Helmert, & Gretton,
2007; Cai, Su, & Chen, 2010; Li & Quan, 2010b; Cai, Su, & Sattar, 2011).
Theoretical analyses indicate that these three problems MVC, MIS, and MC are computationally
hard. They are all NP-hard and the associated decision problems are NP-complete (Garey &
Johnson, 1979). Moreover, they are hard to solve approximately. It is NP-hard to approximate
MVC within any factor smaller than 1.3606 (Dinur & Safra, 2005), although one can achieve an
approximation ratio of 2 − o(1) (Halperin, 2002; Karakostas, 2005). Besides the inapproximability
of MVC, Håstad shows that both MIS and MC are not approximable within |V |1−ǫ for any ǫ > 0,
unless NP=ZPP1 (Håstad, 1999, 2001). Recently, this conclusion has been enhanced that MC is
not approximable within |V |1−ǫ for any ǫ > 0 unless NP=P (Zuckerman, 2006), derived from a
derandomization of Håstad’s result. Moreover, the currently best polynomial-time approximation
algorithm for MC is only guaranteed to find a clique within a factor of O(n(loglogn)2 /(logn)3 ) of
optimum (Feige, 2004).
The algorithms to solve MVC (MIS, MC) fall into two types: exact algorithms and heuristic
algorithms. Exact methods which mainly include branch-and-bound algorithms (Carraghan &
Pardalos, 1990; Fahle, 2002; Östergård, 2002; Régin, 2003; Tomita & Kameda, 2009; Li &
Quan, 2010b, 2010a), guarantee the optimality of the solutions they find, but may fail to give a
solution within reasonable time for large instances. Heuristic algorithms, which mainly include local
search algorithms, cannot guarantee the optimality of their solutions, but they can find optimal or
satisfactory near-optimal solutions for large and hard instances within reasonable time. Therefore,
it is appealing to use local search algorithms to solve large and hard MVC (MC, MIS) instances.
Early heuristic methods for Maximum Clique have been designed as initial responses to the
Second DIMACS Implementation Challenge (Johnson & Trick, 1996), where Maximum Clique is
one of the three challenge problems. After that, a huge amount of effort was devoted to designing
local search algorithms for MVC, MC and MIS problems (Aggarwal, Orlin, & Tai, 1997; Battiti &
Protasi, 2001; Busygin, Butenko, & Pardalos, 2002; Shyu, Yin, & Lin, 2004; Barbosa & Campos,
2004; Pullan, 2006; Richter et al., 2007; Andrade, Resende, & Werneck, 2008; Cai et al., 2010,
2011). A review of heuristic algorithms for these three problems can be found in a recent paper on
MVC local search (Cai et al., 2011).
This work is devoted to a more efficient local search algorithm for MVC. Typically, local search
algorithms for MVC solve the problem by iteratively solving the k-vertex cover problem. To solve
the k-vertex cover problem, they maintain a current candidate solution of size k, and exchange
two vertices iteratively until it becomes a vertex cover. However, we observe two drawbacks in
state-of-the-art MVC local search algorithms. First, they select a pair of vertices for exchanging
simultaneously according to some heuristic (Richter et al., 2007; Cai et al., 2010, 2011), which is
rather time-consuming, as will be explained in Section 3. The second drawback is about the edge
weighting techniques. The basic concept of edge weighting is to increase weights of uncovered
1. ZPP is the class of problems that can be solved in expected polynomial time by a probabilistic algorithm with zero
error probability.

688

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

edges to diversify the search. Previous MVC local search algorithms utilize different edge weighting
schemes. For example, COVER (Richter et al., 2007) increases weights of uncovered edges at each
step, while EWLS (Cai et al., 2010) and EWCC (Cai et al., 2011) increase weights of uncovered
edges only when reaching local optima. However, all these algorithms do not have a mechanism to
decrease the weights. We believe this is deficient because the weighting decisions made too long
ago may mislead the search.
To address these two issues in MVC local search algorithms, this paper proposes two new
strategies, namely two-stage exchange and edge weighting with forgetting. The two-stage exchange
strategy decomposes the exchanging procedure into two stages, i.e., the removing stage and the
adding stage, and performs them separately. It first selects a vertex and removes it from the current
candidate solution, and then selects a vertex in a random uncovered edge and adds it. The twostage exchange strategy yields an efficient two-pass move operator for MVC local search, in which
the first pass is a linear-time search for the vertex-to-remove, while the second pass is a lineartime search for the vertex-to-add. This is in contrast to the standard quadratic, all-at-once move
operator. Moreover, the two-stage exchange strategy renders the algorithm more flexible in that
we can employ different heuristics in different stages. Indeed, the NuMVC algorithm utilizes a
highly greedy heuristic for the removing stage, while for the adding stage, it makes good use of a
diversifying heuristic within a framework similar to focused random walk (Papadimitriou, 1991).
The second strategy we propose is edge weighting with forgetting. It increases weights of
uncovered edges by one at each step. Moreover, when the averaged edge weight achieves a
threshold, it reduces weights of all edges by multiplying a constant factor ρ (0 < ρ < 1) to forget
the earlier weighting decisions. To the best of our knowledge, this is the first time a forgetting
mechanism is introduced into local search algorithms for MVC.
The two strategies are combined to design a new local search algorithm called NuMVC. We
carry out a detailed experimental study to investigate the performance of NuMVC, and compare
it with PLS (Pullan, 2006), COVER (Richter et al., 2007) and EWCC (Cai et al., 2011), which
are leading heuristic algorithms for MVC (MC, MIS). Experimental results show that NuMVC
competes well with other solvers on the DIMACS benchmark, and shows a dramatic improvement
over existing results on the whole BHOSLIB benchmark. These parts of work have been published
in an early version of this paper (Cai, Su, & Sattar, 2012).
In this paper, we additionally carry out more experimental analyses and provides further
insights about the two strategies in NuMVC. We compare NuMVC with the exact algorithm
MaxCLQdyn+EFL+SCR (Li & Quan, 2010a), which is the best exact Maximum Clique algorithm
we found in the literature. Experimental results indicate that NuMVC finds an optimal solution
much faster than the exact algorithm on random instances as well as some structured ones. More
importantly, we conduct experimental investigations to study the run-time behaviour of NuMVC
and the effectiveness of the two new strategies in NuMVC.
The remainder of this paper is organized as follows. In the next section, we introduce some
definitions and notations used in this paper. We then present the two strategies: two-stage exchange
and edge weighting with forgetting. In Section 5, we describe the NuMVC algorithm. Section 6
presents the experimental study of NuMVC and comparative results to other algorithms, including
heuristic and exact algorithms. This is followed by more detailed investigations about the run-time
behaviour of NuMVC and the effectiveness of the two new strategies in Section 7. Finally, we
conclude the paper by summarizing the main contributions and some future directions.
689

C AI , S U , L UO & S ATTAR

2. Preliminaries
An undirected graph G = (V, E) consists of a vertex set V and an edge set E ⊆ V × V , where
each edge is a 2-element subset of V . For an edge e = {u, v}, we say that vertices u and v are the
endpoints of edge e. Two vertices are neighbors if and only if they both belong to some common
edge. We denote N (v) = {u ∈ V |{u, v} ∈ E}, the set of neighbors of a vertex v.
For an undirected graph G = (V, E), an independent set is a subset of V with pairwise nonadjacent elements and a clique is a subset of V with pairwise adjacent elements. The maximum
independent set and maximum clique problems are to find the maximum sized independent set
and clique in a graph, respectively.
We note that these three problems MVC, MIS and MC can be seen as three different forms of
the same problem, from the viewpoint of experimental algorithms. A vertex set S is an independent
set of G if and only if V \S is a vertex cover of G; a vertex set K is a clique of G if and only if V \K
is a vertex cover of the complementary graph G. To find the maximum independent set of a graph
G, one can find the minimum vertex cover Cmin for G and return V \Cmin . Similarly, to find the
′
maximum clique of a graph G, one can find the minimum vertex cover Cmin
for the complementary
′
graph G, and return V \Cmin
.
Given an undirected graph G = (V, E), a candidate solution for MVC is a subset of vertices.
An edge e ∈ E is covered by a candidate solution X if at least one endpoint of e belongs to
X. During the search procedure, NuMVC always maintains a current candidate solution. For
convenience, in the rest of this paper, we use C to denote the current candidate solution. The state
of a vertex v is denoted by sv ∈ {1, 0}, such that sv = 1 means v ∈ C, and sv = 0 means v ∈
/ C.
The step to a neighboring candidate solution consists of exchanging two vertices: a vertex u ∈ C is
removed from C, and a vertex v ∈
/ C is put into C. The age of a vertex is the number of steps since
its state was last changed.
As with most state-of-the-art MVC local search algorithms, NuMVC utilizes an edge weighting
scheme. For edge weighting local search, we follow the definitions and notations in EWCC (Cai
et al., 2011). An edge weighted undirected graph is an undirected graph G = (V, E) combined with
a weighting function w so that each edge e ∈ E is associated with a non-negative integer number
w(e) as its weight. We use w to denote the mean value of all edge weights.
Let w be a weighting function for G. For a candidate solution X, we set the cost of X as
X

cost(G, X) =

w(e)

e∈E and e is not covered by X

which indicates the total weight of edges uncovered by X. We take cost(G, X) as the evaluation
f unction, and NuMVC prefers candidate solutions with lower costs.
For a vertex v ∈ V ,
dscore(v) = cost(G, C) − cost(G, C ′ )
where C ′ = C\{v} if v ∈ C, and C ′ = C ∪ {v} otherwise, measuring the benefit of changing the
state of vertex v. Obviously, for a vertex v ∈ C, we have dscore(v) ≤ 0, and the greater dscore
indicates the less loss of covered edges by removing it out of C. For a vertex v ∈
/ C, we have
dscore(v) ≥ 0, and the higher dscore indicates the greater increment of covered edges by adding
it into C.
690

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

3. Two-Stage Exchange
In this section, we introduce the two-stage exchange strategy, which is adopted by the NuMVC
algorithm to exchange a pair of vertices.
As with most state-of-the-art MVC local search algorithms, NuMVC is an iterated k-vertex
cover algorithm. When finding a k-vertex cover, NuMVC removes one vertex from the current
candidate solution C and goes on to search for a (k − 1)-vertex cover. In this sense, the core of
NuMVC is a k-vertex cover algorithm — given a positive integer number k, searching for a k-sized
vertex cover. To find a k-vertex cover, NuMVC begins with a candidate solution C of size k, and
exchanges two vertices iteratively until C becomes a vertex cover.
Most local search algorithms for MVC select a pair of vertices to exchange simultaneously
according to a certain heuristic. For example, COVER selects a pair of vertices that maximize
gain(u, v) (Richter et al., 2007), while EWLS (Cai et al., 2010) and EWCC (Cai et al., 2011) select
a random pair of vertices with score(u, v) > 0. This strategy of selecting two vertices to exchange
simultaneously leads to a quadratic neighborhood for candidate solutions. Moreover, the evaluation
of a pair of vertices not only depends on the evaluations (such as dscore) of the two vertices, but also
involves the relationship between the two vertices, like “do they belong to a same edge”. Therefore,
it is rather time-consuming to evaluate all candidate pairs of vertices.
In contrast to earlier MVC local search algorithms, NuMVC selects the two vertices for
exchanging separately and exchanges the two selected vertices in two stages. In each iteration,
NuMVC first selects a vertex u ∈ C with the highest dscore and removes it. After that, NuMVC
selects a uniformly random uncovered edge e, and chooses one endpoint v of e with the higher
dscore under some restrictions and adds it into C. Note that this two-stage exchange strategy
resembles in some respect the min-conflicts hill-climbing heuristic for CSP (Minton, Johnston,
Philips, & Laird, 1992), which shows surprisingly good performance for the N-queens problem.
Selecting the two vertices for exchanging separately may in some cases miss some greedier
vertex pairs which consist of two neighboring vertices. However, as is usual in local search
algorithms, there is a trade-off between the accuracy of heuristics and the complexity per step.
Let R and A denote the set of candidate vertices for removing and adding separately. The time
complexity per step for selecting the exchanging vertex pair simultaneously is |R| · |A|; while the
complexity per step for selecting the two vertices separately, as in NuMVC, is only |R| + |A|. It
is worthy to note that, as heuristics in a local search algorithm are often based on intuition and
experience rather than on theoretically or empirically derived principles and insights, we cannot say
for certain that being less greedy is not a good thing (Hoos & Stützle, 2004). On the other hand, a
lower time complexity is always desirable.

4. Edge Weighting with Forgetting
In this section, we present a new edge weighting technique called edge weighting with forgetting,
which plays an important role in NuMVC.
The proposed strategy of edge weighting with forgetting works as follows. Each edge is
associated with a positive integer number as its weight, and each edge weight is initialized as one.
Then in each iteration, edge weights of the uncovered edges are increased by one. Moreover, when
the average weight achieves a threshold, all edge weights are reduced to forget the earlier weighting
decisions using the formula w(e) := ⌊ρ · w(e)⌋, where ρ is a constant factor between 0 and 1.
691

C AI , S U , L UO & S ATTAR

Note that edge weighting techniques in MVC local search, including the one in this work,
fall in the more general penalty idea for optimization problems, which dates back to Morris ’s
breakout method (Morris, 1993) and has been widely used in local search algorithms for constraint
optimization problems such as SAT (Yugami, Ohta, & Hara, 1994; Wu & Wah, 2000; Schuurmans,
Southey, & Holte, 2001; Hutter, Tompkins, & Hoos, 2002). Our results therefore provide further
evidence for the effectiveness and general applicability of this algorithmic technique.
Edge weighting techniques have been successfully used to improve MVC local search
algorithms. For example, COVER (Richter et al., 2007) updates edge weights at each step, while
EWLS (Cai et al., 2010) and EWCC (Cai et al., 2011) update edge weights only when reaching local
optima. However, all previous edge weighting techniques do not have a mechanism to decrease the
weights, which limits their effectiveness. The strategy of edge weighting with forgetting in this
work introduces a forgetting mechanism to reduce edge weights periodically, which contributes
considerably to the NuMVC algorithm.
The intuition behind the forgetting mechanism is that the weighting decisions made too long ago
are no longer helpful and may mislead the search, and hence should be considered less important
than the recent ones. For example, consider two edges e1 and e2 with w(e1 ) = 1000 and w(e2 ) =
100 at some step. We use ∆w(e) to denote the increase of w(e). According to the evaluation
function, in the next period of time, the algorithm is likely to cover e1 more frequently than e2 , and
we may assume that during this period ∆w(e1 ) = 50 and ∆w(e2 ) = 500, which makes w(e1 ) =
1000 + 50 = 1050 and w(e2 ) = 100 + 500 = 600. Without a forgetting mechanism, the algorithm
would still prefer e1 to e2 to be covered in the future search. This is not reasonable, as during this
period e2 is covered in much fewer steps than e1 is. Thus, e2 should take priority to be covered for
the sake of diversification. Now let us consider the case with the forgetting mechanism (assuming
ρ = 0.3 which is the setting in our experiments). Suppose w(e1 ) = 1000 and w(e2 ) = 100
when the algorithm performs the forgetting. The forgetting mechanism reduces the edge weights as
w(e1 ) = 1000×0.3 = 300 and w(e2 ) = 100×0.3 = 30. After a period of time, with ∆w(e1 ) = 50
and ∆w(e2 ) = 500, we have w(e1 ) = 300 + 50 = 350 and w(e2 ) = 30 + 500 = 530. In this case,
the algorithm prefers to cover e2 rather than cover e1 in the future search, as we expect.
Although being inspired by smoothing techniques in clause weighting local search algorithms
for SAT, the forgetting mechanism in NuMVC differs from those smoothing techniques in SAT
local search algorithms. According to the way that clause weights are smoothed, there are three
main smoothing techniques in clause weighting local search algorithms for SAT to the best of our
knowledge: the first is to pull all clause weights to their mean value using the formula wi :=
ρ · wi + (1 − ρ) · w, as in ESG (Schuurmans et al., 2001), SAPS (Hutter et al., 2002) and Swcca
(Cai & Su, 2012); the second is to subtract one from all clause weights which are greater than
one, as in DLM (Wu & Wah, 2000) and PAWS (Thornton, Pham, Bain, & Jr., 2004); and the last
is employed in DDWF (Ishtaiwi, Thornton, Sattar, & Pham, 2005), which transfers weights from
neighbouring satisfied clauses to unsatisfied ones. It is obvious that the forgetting mechanism in
NuMVC is different from all these smoothing techniques.
Recently, a forgetting mechanism was proposed for vertex weighting technique in the significant
MC local search algorithm DLS-MC (Pullan & Hoos, 2006), which is an important sub-algorithm in
PLS (Pullan, 2006) and CLS (Pullan, Mascia, & Brunato, 2011). The DLS-MC algorithm employs
a vertex weighting scheme which increases the weights of vertices (by one) not in the current clique
when reaching a local optimum, and periodically decreases weights (by one) for all vertices that
currently have a penalty. Specifically, it utilizes a parameter pd (penalty delay) to specify the
692

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

number of penalty increase iterations that must occur before the algorithm performs a forgetting
operation. However, Pullan and Hoos also observed that DLS-MC is very sensitive to the pd
parameter, and the optimal value of pd varies considerably among different instances. Indeed,
the performance of DLS-MC in is given by optimizing the pd parameter. In contrast, the forgetting
mechanism in NuMVC is much less sensitive to its parameters (as will be shown in Section 7.4),
and thus is more robust.
We also notice that the formula used in the forgetting mechanism in NuMVC has been adopted
in long-term frequency-based learning mechanisms for tabu search (Taillard, 1994). However, in
Taillar’s algorithm, the parameter ρ (using the term in this work) is always greater than one, and the
formula is used for penalizing a move rather than forgetting the penalties.

5. The NuMVC Algorithm
In this section, we present the NuMVC algorithm, which utilizes the strategies of two-stage
exchange and edge weighting with forgetting.
Algorithm 1: NuMVC
1

2
3
4
5
6
7
8
9
10
11
12

13
14
15

16
17
18
19
20

NuMVC (G,cutoff)
Input: graph G = (V, E), the cutoff time
Output: vertex cover of G
begin
initialize edge weights and dscores of vertices;
initialize the confChange array as an all-1 array;
construct C greedily until it is a vertex cover;
C ∗ := C;
while elapsed time < cutoff do
if there is no uncovered edge then
C ∗ := C;
remove a vertex with the highest dscore from C;
continue;
choose a vertex u ∈ C with the highest dscore, breaking ties in favor of the oldest
one;
C := C\{u}, confChange(u) := 0 and confChange(z) := 1 for each z ∈ N (u);
choose an uncovered edge e randomly;
choose a vertex v ∈ e such that confChange(v) = 1 with higher dscore, breaking ties
in favor of the older one;
C := C ∪ {v}, confChange(z) := 1 for each z ∈ N (v);
w(e) := w(e) + 1 for each uncovered edge e;
if w ≥ γ then w(e) := ⌊ρ · w(e)⌋ for each edge e;
return C ∗ ;
end

For better understanding the algorithm, we first describe a strategy called configuration checking
(CC), which is used in NuMVC. The CC strategy (Cai et al., 2011) was proposed for handling the
693

C AI , S U , L UO & S ATTAR

cycling problem in local search, i.e., revisiting a candidate solution that has been visited recently
(Michiels, Aarts, & Korst, 2007). This strategy has been successfully applied in local search
algorithms for MVC (Cai et al., 2011) as well as SAT (Cai & Su, 2011, 2012).
The CC strategy in NuMVC works as follows: For a vertex v ∈
/ C, if all its neighboring vertices
never change their states since the last time v was removed from C, then v should not be added
back to C. The CC strategy can be seen as a prohibition mechanism, which shares the same spirit
but differs from the well-known prohibition mechanism called tabu (Glover, 1989).
An implementation of the CC strategy is to maintain a Boolean array confChange for vertices.
During the search procedure, those vertices which have a confChange value of 0 are forbidden to
add into C. The confChange array is initialized as an all-1 array. After that, when a vertex v is
removed from C, confChange(v) is reset to 0, and when a vertex v changes its state, for each
z ∈ N (v), confChange(z) is set to 1.
We outline the NuMVC algorithm in Algorithm 1, as described below. In the beginning, all edge
weights are initialized as 1, and dscores of vertices are computed accordingly; confChange(v) is
initialized as 1 for each vertex v; then the current candidate solution C is constructed by iteratively
adding the vertex with the highest dscore (ties are broken randomly), until it becomes a vertex
cover. Finally, the best solution C ∗ is initialized as C.
After the initialization, the loop (lines 7-18) is executed until a given cutoff time is reached.
During the search procedure, once there is no uncovered edge, which means C is a vertex cover,
NuMVC updates the best solution C ∗ as C (line 9). Then it removes one vertex with the highest
dscore from C (line 10), breaking ties randomly, so that it can go on to search for a vertex cover
of size |C| = |C ∗ | − 1. We note that, in C, the vertex with the highest dscore has the minimum
absolute value of dscore since all these dscores are negative.
In each iteration of the loop, NuMVC swaps two vertices according to the strategy of two-stage
exchange (lines 12-16). Specifically, it first selects a vertex u ∈ C with the highest dscore to
remove, breaking ties in favor of the oldest one. After removing u, NuMVC chooses an uncovered
edge e uniformly at random, and selects one of e’s endpoints to add into C as follows: If there is
only one endpoint whose confChange is 1, then that vertex is selected; if the confChange values
of both endpoints are 1, then NuMVC selects the vertex with the higher dscore, breaking ties in
favor of the older one. The exchange is finished by adding the selected vertex into C. Along with
exchanging the two selected vertices, the confChange array is updated accordingly.
At the end of each iteration, NuMVC updates the edge weights (lines 17-18). First, weights of
all uncovered edges are increased by one. Moreover, NuMVC utilizes the forgetting mechanism to
decrease the weights periodically. In detail, if the averaged weight of all edges achieves a threshold
γ, then all edge weights are multiplied by a constant factor ρ (0 < ρ < 1) and rounded down to an
integer as edge weights are defined as integers in NuMVC. The forgetting mechanism forgets the
earlier weighting decision to some extent, as these past effects are generally no longer helpful and
may mislead the search.
We conclude this section by the following observation, which guarantees the executability of
line 15.
Proposition 1. For an uncovered edge e, there is at least one endpoint v of edge e such that
confChange(v) = 1.
Proof: Let us consider an arbitrary uncovered edge e = {v1 , v2 }. The proof includes two cases.
(a) There is at least one of v1 and v2 which never changes its state after initialization. Without
694

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

loss of generality, we assume v1 is such a vertex. In the initialization, confChange(v1 ) is set to
1. After that, only removing v1 from C (which corresponds to v’s state sv changing to 0 from
1) can make confChange(v1 ) be 0, but v1 never changes its state after initialization, so we have
confChange(v1 )= 1.
(b) Both v1 and v2 change their states after initialization. As e is uncovered, we have v1 ∈
/ C and
v2 ∈
/ C. Without loss of generality, we assume the last removing of v1 happens before the last
removing of v2 . The last time v1 is removed, v2 ∈ C holds. Afterwards, v2 is removed, which
means v2 changes its state, so confChange(v1 ) is set to 1 as v1 ∈ N (v2 ).

6. Empirical Results
In this section, we present a detailed experimental study to evaluate the performance of NuMVC
on standard benchmarks in the literature, i.e., the DIMACS and BHOSLIB benchmarks. We first
introduce the DIMACS and BHOSLIB benchmarks, and describe some preliminaries about the
experiments. Then, we divide the experiments into three parts. The purpose of the first part is to
demonstrate the performance of NuMVC in detail. The second is to compare NuMVC with state-ofthe-art heuristic algorithms. Finally, the last part is to compare NuMVC with state-of-the-art exact
algorithms.
6.1 The Benchmarks
Having a good set of benchmarks is fundamental to demonstrate the effectiveness of new solvers.
We use the two standard benchmarks in MVC (MIS, MC) research, the DIMACS benchmark and
the BHOSLIB benchmark. The DIMACS benchmark includes instances from industry and those
generated by various models, while the BHOSLIB instances are random ones of high difficulty.
6.1.1 DIMACS B ENCHMARK
The DIMACS benchmark is taken from the Second DIMACS Implementation Challenge for the
Maximum Clique problem (1992-1993)2 . Thirty seven graphs were selected by the organizers for
a summary to indicate the effectiveness of algorithms, comprising the Second DIMACS Challenge
Test Problems. These instances were generated from real world problems such as coding theory,
fault diagnosis, Keller’s conjecture and the Steiner Triple Problem, etc, and random graphs in
various models, such as the brock and p hat families. These instances range in size from less
than 50 vertices and 1,000 edges to greater than 4,000 vertices and 5,000,000 edges. Although being
proposed two decades ago, the DIMACS benchmark remains the most popular benchmark and has
been widely used for evaluating heuristic algorithms for MVC (Richter et al., 2007; Pullan, 2009;
Cai et al., 2011; Gajurel & Bielefeld, 2012), MIS (Andrade et al., 2008; Pullan, 2009) and MC
algorithms (Pullan, 2006; Katayama, Sadamatsu, & Narihisa, 2007; Grosso, Locatelli, & Pullan,
2008; Pullan et al., 2011; Wu, Hao, & Glover, 2012). In particular, the DIMACS benchmark has
been used for evaluating COVER and EWCC. It is convenient for us to use this benchmark also
to conduct experiments comparing NuMVC with COVER and EWCC. Note that as the DIMACS
graphs were originally designed for the Maximum Clique problem, MVC algorithms are tested on
their complementary graphs.
2. ftp://dimacs.rutgers.edu/pub/challenges

695

C AI , S U , L UO & S ATTAR

6.1.2 BHOSLIB B ENCHMARK
The BHOSLIB3 (Benchmarks with Hidden Optimum Solutions) instances were generated randomly
in the phase transition area according to the model RB (Xu, Boussemart, Hemery, & Lecoutre,
2005). Generally, those phase-transition instances generated by model RB have been proved to
be hard both theoretically (Xu & Li, 2006) and practically (Xu & Li, 2000; Xu, Boussemart,
Hemery, & Lecoutre, 2007). The SAT version of the BHOSLIB benchmark is extensively used
in the SAT competitions4 . Nevertheless, SAT solvers are much weaker than MVC solvers on these
problems, which remains justifiable when referring to the results of SAT Competition 2011 on this
benchmark. The BHOSLIB benchmark is famous for its hardness and influential enough to be
strongly recommended by the MVC (MC, MIS) community (Grosso et al., 2008; Cai et al., 2011).
It has been widely used in the recent literature as a reference point for new local search solvers to
MVC, MC and MIS5 . Besides these 40 instances, there is a large instance frb100-40 with 4,000
vertices and 572,774 edges, which is designed for challenging MVC (MC, MIS) algorithms.
The BHOSLIB benchmark was designed for MC, MVC and MIS, and all the graphs in this
benchmark are expressed in two formats, i.e., the clq format and the mis format. For a BHOSLIB
instance, the graph in clq format and the one in mis format are complementary to each other. MC
algorithms are tested on the graphs in clq format, while MVC and MIS algorithms are tested on
those in mis format.
6.2 Experiment Preliminaries
Before we discuss the experimental results, let us introduce some preliminary information about our
experiments.
NuMVC is implemented in C++. The codes of both NuMVC and EWCC are publicly available
on the first author’s homepage6 . The codes of COVER are downloaded online7 , and those of PLS
are kindly provided by its authors. All the four solvers are compiled by g++ with the ’-O2’ option.
All experiments are carried out on a machine with a 3 GHz Intel Core 2 Duo CPU E8400 and 4GB
RAM under Linux. To execute the DIMACS machine benchmarks8 , this machine requires 0.19
CPU seconds for r300.5, 1.12 CPU seconds for r400.5 and 4.24 CPU seconds for r500.5.
For NuMVC, we set γ = 0.5|V | and ρ = 0.3 for all runs, except for the challenging instance
frb100-40, where γ = 5000 and ρ = 0.3. Note that there are also parameters in other state-ofthe-art MVC (MC, MIS) algorithms, such as DLS-MC (Pullan & Hoos, 2006) and EWLS (Cai et al.,
2010). Moreover, the parameters in DLS-MC and EWLS vary considerably on different instances.
For each instance, each algorithm is performed 100 independent runs with different random seeds,
where each run is terminated upon reaching a given cutoff time. The cutoff time is set to 2000
seconds for all instances except for the challenging instance frb100-40, for which the cutoff
time is set to 4000 seconds due to its significant hardness.
For NuMVC, we report the following information for each instance:
• The optimal (or minimum known) vertex cover size (V C ∗ ).
3.
4.
5.
6.
7.
8.

http://www.nlsde.buaa.edu.cn/˜kexu/benchmarks/graph-benchmarks.htm
http://www.satcompetition.org
http://www.nlsde.buaa.edu.cn/˜kexu/benchmarks/list-graph-papers.htm
http://www.shaoweicai.net/research.html
http://www.informatik.uni-freiburg.de/˜srichter/
ftp://dimacs.rutgers.edu/pub/dsj/clique/

696

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

• The number of successful runs (“suc”). A run is said successful if a solution of size V C ∗ is
found.
• The “VC size” which shows the min (average, max) vertex cover size found by NuMVC in
100 runs.
• The averaged run time over all 100 runs (“time”). The run time of a successful run is the
time to find the V C ∗ solution, and that of a failed run is considered to be the cutoff time. For
instances where NuMVC does not achieve a 100% success rate, we also report the averaged
run time over only successful runs (“suc time”). The run time is measured in CPU seconds.
• The inter-quartile range (IQR) of the run time for 100 runs. The IQR is the difference between
the 75th percentile and the 25th percentile of a sample. IQR is one of the most famous robust
measures in data analysis (Hoaglin, Mosteller, & Tukey, 2000), and has been recommended
as a measurement of closeness of the sampling distribution by the community of experimental
algorithms (Bartz-Beielstein, Chiarandini, Paquete, & Preuss, 2010).
• The number of steps averaged over all 100 runs (“steps”). The steps of a successful run is
those needed to find the V C ∗ solution, while the steps of a failed run are those executed
before the running is cut off. For instances where NuMVC does not achieve a 100% success
rate, we also report the averaged steps over only successful runs (“suc steps”).
If there are no successful runs for an instance, the “time” and “steps” columns are marked with
“n/a”. When the success rate of a solver on an instance is less than 75%, the 75th percentile of the
run time sample is just the cutoff time and does not represent the real 75th percentile. In this case,
we do not report the IQR, and instead we mark with “n/a” on the corresponding column. Actually,
if the success rate of a solver on a certain instance is less than 75%, the solver should be considered
not robust on that instance given the cutoff time.
6.3 Performance of NuMVC
In this section, we report a detailed performance of NuMVC on the two benchmarks.
6.3.1 P ERFORMANCE OF N U MVC

ON

DIMACS B ENCHMARK

The performance results of NuMVC on the DIMACS benchmark are displayed in Table 1. NuMVC
finds optimal (or best known) solutions for 35 out of 37 DIMACS instances. Note that the 2 failed
instances are both brock graphs. Furthermore, among the 35 successful instances, NuMVC does
so consistently (i.e., in all 100 runs) for 32 instances, 24 of which are solved within 1 second.
Overall, the NuMVC algorithm exhibits excellent performance on the DIMACS benchmark except
for the brock graphs. Remark that the brock graphs are artificially designed to defeat greedy
heuristics by explicitly incorporating low-degree vertices into the optimal vertex cover. Indeed,
most algorithms preferring higher-degree vertices such as GRASP, RLS, k-opt, COVER and EWCC
also failed in these graphs.
6.3.2 P ERFORMANCE OF N U MVC

ON

BHOSLIB B ENCHMARK

In Table 2, we illustrate the performance of NuMVC on the BHOSLIB benchmark. NuMVC
successfully solves all BHOSLIB instances in terms of finding an optimal solution, and the size
697

C AI , S U , L UO & S ATTAR

Graph
Instance Vertices
brock200 2
brock200 4
brock400 2
brock400 4
brock800 2
brock800 4
C125.9
C250.9
C500.9
C1000.9
C2000.5
C2000.9
C4000.5
DSJC500.5
DSJC1000.5
gen200 p0.9 44
gen200 p0.9 55
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
hamming8-4
hamming10-4
keller4
keller5
keller6
MANN a27
MANN a45
MANN a81
p hat300-1
p hat300-2
p hat300-3
p hat700-1
p hat700-2
p hat700-3
p hat1500-1
p hat1500-2
p hat1500-3

200
200
400
400
800
800
125
250
500
1000
2000
2000
4000
500
1000
200
200
400
400
400
256
1024
171
776
3361
378
1035
3321
300
300
300
700
700
700
1500
1500
1500

V C∗

suc

VC size

188∗

100
100
96
100
0
0
100
100
100
100
100
1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
27
100
100
100
100
100
100
100
100
100

188
183
371(371.16,375)
367
779
779
91
206
443
932
1984
1920(1921.29,1922)
3982
487
985
156
145
345
335
325
240
984
160
749
3302
252
690
2221(2221.94,2223)
292
275
264
689
656
638
1488
1435
1406

183∗
371∗
367∗
776∗
774∗
91∗
206∗
443∗
932
1984
1920
3982
487∗
985∗
156∗
145∗
345∗
335∗
325∗
240∗
984∗
160∗
749∗
3302
252∗
690∗
2221
292∗
275∗
264∗
689∗
656∗
638∗
1488∗
1435∗
1406

NuMVC
time(suc time)
0.126
1.259
572.390(512.906)
4.981
n/a
n/a
< 0.001
< 0.001

0.128
2.020
2.935
1994.561(1393.303)
252.807
0.012
0.615
< 0.001
< 0.001

0.035
< 0.001
< 0.001
< 0.001

0.062
< 0.001

0.038
2.51
< 0.001

86.362
1657.880(732.897)
0.003
< 0.001

0.001
0.011
0.006
0.008
3.751
0.071
0.060

steps(suc steps)
137610
1705766
645631471(585032783)
6322882
n/a
n/a
136
3256
133595
1154155
231778
777848959(564895994)
7802785
3800
134796
1695
69
38398
1522
203
1
23853
42
15269
384026
6651
90642150
571607432(251509010)
100
98
1863
1248
1103
2868
445830
5280
10668

Table 1: NuMVC performance results, averaged over 100 independent runs, for the DIMACS
benchmark instances. The VC∗ column marked with an asterisk means that the minimum
known vertex cover size has been proved optimal.

698

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

of the worst solution it finds never exceeds V C ∗ + 1. NuMVC finds optimal solutions with 100%
success rate for 33 out of these 40 instances, and the averaged success rate over the remaining 7
instances is 82.57%. These results are dramatically better than existing results in the literature on
this benchmark. Also, NuMVC finds a sub-optimal solution of size V C ∗ + 1 for all BSHOSLIB
instances very quickly, always in less than 30 seconds. This indicates NuMVC can be used to
approximate the MVC problem efficiently even under very limited time.
Besides the 40 BHOSLIB instances in Table 2, there is a challenging instance frb100-40,
which has a hidden minimum vertex cover of size 3900. The designer of the BHOSLIB benchmark
conjectured that this instance will not be solved on a PC in less than a day within the next two
decades9 . The latest record for this challenging instance is a 3902-sized vertex cover found by
EWLS, and also EWCC.
We run NuMVC 100 independent trials within 4000 seconds on frb100-40, with γ = 5000
and ρ = 0.3 (this parameter setting yields the best performance among all combinations from
γ = 2000, 3000, ..., 6000 and ρ = 0.1, 0.2, ..., 0.5). Among these 100 runs, 4 runs find a 3902-sized
solution with the averaged time of 2955 seconds, and 93 runs find a 3903-sized solution (including
3902-sized) with the averaged time of 1473 seconds. Also, it is interesting to note that NuMVC
can locate a rather good approximate solution for this hard instance very quickly: the size of vertex
covers that NuMVC finds within 100 seconds is between 3903 and 3905.
Generally, finding a (k+1)-vertex cover is much easier than a k-vertex cover. Hence, for
NuMVC, as well as most other MVC local search algorithms which also solve the MVC problem
by solving the k-vertex cover problem iteratively, the majority of running time is used in finding
the best vertex cover C ∗ (of the run), and in trying, without success, to find a vertex cover of size
(|C ∗ | − 1).
6.4 Comparison with Other Heuristic Algorithms
In the recent literature there are five leading heuristic algorithms for MVC (MC, MIS), including
three MVC algorithms COVER (Richter et al., 2007), EWLS (Cai et al., 2010) and EWCC (Cai
et al., 2011), and two MC algorithms DLS-MC (Pullan & Hoos, 2006) and PLS (Pullan, 2006).
Note that EWCC and PLS are the improved versions of EWLS and DLS-MC respectively, and show
better performance over their original versions on DIMACS and BHOSLIB benchmarks. Therefore,
we compare NuMVC only with PLS, COVER and EWCC.
When comparing NuMVC with other heuristic algorithms, we report V C ∗ , “suc”, “time” as well
as IQR. The averaged run time over only successful runs (“suc time”) cannot indicate comparative
performance of algorithms correctly unless the evaluated algorithms have close success rates, and
f ∗(100−“suc”)
can be calculated by “time”∗100−cutof
, so we do not report these statistics. The results
“suc”
in bold indicate the best performance for an instance.
6.4.1 C OMPARATIVE R ESULTS

ON

DIMACS B ENCHMARK

The comparative results on the DIMACS benchmark are shown in Table 3. Most DIMACS instances
are so easy that they can be solved by all solvers with 100% success rate within 2 seconds, and thus
are not reported in the table. Actually, the fact that the DIMACS benchmark has been reduced to 11
useful instances really emphasizes the need to make a new benchmark.
9. http://www.nlsde.buaa.edu.cn/˜kexu/benchmarks/graph-benchmarks.htm

699

C AI , S U , L UO & S ATTAR

Graph
Instance Vertices
frb30-15-1
frb30-15-2
frb30-15-3
frb30-15-4
frb30-15-5
frb35-17-1
frb35-17-2
frb35-17-3
frb35-17-4
frb35-17-5
frb40-19-1
frb40-19-2
frb40-19-3
frb40-19-4
frb40-19-5
frb45-21-1
frb45-21-2
frb45-21-3
frb45-21-4
frb45-21-5
frb50-23-1
frb50-23-2
frb50-23-3
frb50-23-4
frb50-23-5
frb53-24-1
frb53-24-2
frb53-24-3
frb53-24-4
frb53-24-5
frb56-25-1
frb56-25-2
frb56-25-3
frb56-25-4
frb56-25-5
frb59-26-1
frb59-26-2
frb59-26-3
frb59-26-4
frb59-26-5

450
450
450
450
450
595
595
595
595
595
760
760
760
760
760
945
945
945
945
945
1150
1150
1150
1150
1150
1272
1272
1272
1272
1272
1400
1400
1400
1400
1400
1534
1534
1534
1534
1534

V C∗
420
420
420
420
420
560
560
560
560
560
720
720
720
720
720
900
900
900
900
900
1100
1100
1100
1100
1100
1219
1219
1219
1219
1219
1344
1344
1344
1344
1344
1475
1475
1475
1475
1475

suc

VC size

NuMVC
time (suc time)

steps (suc steps)

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
95
100
100
86
100
100
100
100
100
97
100
100
100
88
38
96
79
100

420
420
420
420
420
560
560
560
560
560
720
720
720
720
720
900
900
900
900
900
1100
1100
1100(1100.05,1101)
1100
1100
1219(1219.14,1220)
1219
1219
1219
1219
1344
1344(1344.03,1345)
1344
1344
1344
1475(1475.12,1476)
1475(1475.62,1476)
1475(1475.04,1476)
1475(1475.21,1476)
1475

0.045
0.053
0.191
0.049
0.118
0.515
0.447
0.178
0.563
0.298
0.242
4.083
1.076
2.757
10.141
2.708
4.727
13.777
3.973
10.661
38.143
176.589
606.165(532.805)
7.89
19.529
895.006(715.123)
205.352
51.227
266.871
39.893
470.682
658.961(617.485)
121.298
49.446
26.761
843.304(687.845)
1677.801(1160.020)
644.831(580.032)
1004.550(741.208)
61.907

37963
44632
173708
41189
105468
386287
334255
129279
422638
218800
208115
3679770
959874
2473081
9142719
2029588
3605881
10447444
3000680
8059236
24628019
113569606
386342329(343518242)
5092072
12690957
514619149(416394360)
117980833
29376406
152982736
22817023
259903023
350048132(326853745)
67043078
26030031
14109165
440874471(350993718)
875964146(592010913)
325417225(295226277)
517521634(375976753)
31682895

Table 2: NuMVC performance results, averaged over 100 independent runs, for the BHOSLIB
benchmark instances. All these BHOSLIB instances have a hidden optimal vertex cover,
whose size is shown in the VC∗ column.

As indicated in Table 3, NuMVC outperforms COVER and EWCC on all instances, and is
competitive with and complementary to PLS. For the eight hard instances on which at least one
solver fails to achieve a 100% success rate, PLS dominates on the brock graphs while NuMVC
dominates on the others, including the two putatively hardest instances C2000.9 and MANN a81
(Richter et al., 2007; Grosso et al., 2008; Cai et al., 2011), as well as keller6 and MANN a45.
700

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

Graph
Instance

V C∗

suc

PLS
time (IQR)

suc

brock400 2
brock400 4
brock800 2
brock800 4
C2000.9
C4000.5
gen400 p0.9 55
keller6
MANN a45
MANN a81
p hat1500-1

371∗
367∗
776∗
774∗
1920
3982
345∗
3302
690∗
2221
1488∗

100
100
100
100
0
100
100
92
1
0
100

0.15 (0.16)
0.03 (0.03)
3.89 (3.88)
1.31 (1.52)
n/a
67 (59)
15.17 (17)
559 (515)
1990 (n/a)
n/a
2.36 (3.07)

3
82
0
0
0
100
100
100
94
1
100

COVER
time (IQR)
1947 (n/a)
960 (988)
n/a
n/a
n/a
658 (290)
0.35 (0.1)
68 (6)
714 (774)
1995 (n/a)
18.10 (17.23)

suc

EWCC
time (IQR)

suc

NuMVC
time (IQR)

20
100
0
0
0
100
100
100
88
1
100

1778 (n/a)
25.38 (25.96)
n/a
n/a
n/a
739 (903)
0.05 (0.04)
3.76 (3.57)
763 (766)
1986 (n/a)
9.79 (9.77)

96
100
0
0
1
100
100
100
100
27
100

572 (646)
4.98 (6.14)
n/a
n/a
1994 (n/a)
252 (97)
0.03 (0.01)
2.51 (0.76)
86 (95)
1657 (n/a)
3.75 (3.19)

Table 3: Comparison of NuMVC with other state-of-the-art heuristic algorithms on the DIMACS
benchmark. The VC∗ column marked with an asterisk means that the minimum known
vertex cover size has been proved optimal.

For C2000.9, only NuMVC finds a 1920-sized solution, and it also finds a 1921-sized solution in
70 runs, while this number is 31, 6 and 32 for PLS, COVER, and EWCC respectively. Note that
PLS performs well on the brock family because it comprises three sub-algorithms, one of which
favors the lower degree vertices.
Table 3 indicates that C2000.9 and MANN a81 remain very difficult for modern algorithms,
as none of the algorithms can solve them with a good success rate in reasonable time. On the other
hand, other instances can be solved quickly (in less than 100 seconds) by at least one algorithm, PLS
or NuMVC, with a low IQR value (always less than 100), which indicates quite stable performance.
6.4.2 C OMPARATIVE R ESULTS

ON

BHOSLIB B ENCHMARK

In Table 4, we present comparative results on the BHOSLIB benchmark. For concentrating on the
considerable gaps in comparisons, we do not report the results on the two groups of small instances
(frb30 and frb35), which can be solved within several seconds by all solvers.
The results in Table 4 illustrate that NuMVC significantly outperforms the other algorithms
on all BHOSLIB instances, in terms of both success rate and averaged run time, which are also
demonstrated in Figure 1. We take a further look at the comparison between NuMVC and EWCC,
as EWCC performs obviously better than PLS and COVER on this benchmark. NuMVC solves 33
instances out of 40 with 100% success rate, 4 more instances than EWCC does. For those instances
solved by both algorithms with 100% success rate, the overall averaged run time is 25 seconds
for NuMVC and 74 seconds for EWCC. For other instances, the averaged success rate is 90% for
NuMVC, compared to 50% for EWCC.
The excellent performance of NuMVC is further underlined by the large gaps between NuMVC
and the other solvers on the hard instances. For example, on the instances where all solvers fail
to find an optimal solution with 100% success rate, NuMVC achieves an overall averaged success
rate of 82.57%, dramatically better than those of PLS, COVER and EWCC, which are 0.85%,
17.43% and 35.71% respectively. Obviously, the experimental results show that NuMVC delivers
701

C AI , S U , L UO & S ATTAR

Graph
Instance

V C∗

suc

PLS
time (IQR)

suc

frb40-19-1
frb40-19-2
frb40-19-3
frb40-19-4
frb40-19-5
frb45-21-1
frb45-21-2
frb45-21-3
frb45-21-4
frb45-21-5
frb50-23-1
frb50-23-2
frb50-23-3
frb50-23-4
frb50-23-5
frb53-24-1
frb53-24-2
frb53-24-3
frb53-24-4
frb53-24-5
frb56-25-1
frb56-25-2
frb56-25-3
frb56-25-4
frb56-25-5
frb59-26-1
frb59-26-2
frb59-26-3
frb59-26-4
frb59-26-5

720
720
720
720
720
900
900
900
900
900
1100
1100
1100
1100
1100
1219
1219
1219
1219
1219
1344
1344
1344
1344
1344
1475
1475
1475
1475
1475

100
100
100
100
95
100
100
21
100
100
30
3
2
100
79
1
6
20
21
10
1
0
0
11
27
0
0
3
0
30

10.42 (10.38)
85.25 (72.75)
9.06 (10.21)
77.39 (90.56)
496 (529.25)
52.31 (55.5)
170 (202.2)
1737 (n/a)
111 (130)
261 (300)
1658 (640)
1956 (n/a)
1989 (n/a)
93 (80)
967 (1305)
1982 (n/a)
1959 (n/a)
1771 (n/a)
1782 (n/a)
1955 (n/a)
1993 (n/a)
n/a
n/a
1915 (n/a)
1719 (n/a)
n/a
n/a
1978 (n/a)
n/a
1708 (420)

100
100
100
100
100
100
100
100
100
100
100
48
39
100
100
17
50
99
48
95
24
17
97
93
100
16
9
21
3
98

COVER
time (IQR)
1.58 (0.55)
17.18 (16.09)
5.06 (4)
11.79 (8.67)
124 (131)
14.34 (12.8)
38 (35.4)
110 (121)
21 (18)
105 (103 )
268 (305)
1325 (n/a)
1486 (n/a)
33 (25)
168 (246)
1796 (n/a)
1279 (n/a)
273 (223)
1428 (n/a)
423 (315)
1698 (n/a)
1598 (n/a)
537 (692)
476 (460)
168 (128)
1607 (n/a)
1881 (n/a)
1768 (n/a)
1980 (n/a)
431 (476)

suc

EWCC
time (IQR)

suc

100
100
100
100
100
100
100
100
100
100
100
82
56
100
100
30
81
100
81
100
56
52
100
100
100
21
7
64
20
100

0.55 (0.48)
11.30 (14.21)
2.97 (2.35)
13.79 (16.05)
41.71 (39.08)
9.07 (9.3)
15 (14.1)
56 (70.4)
15 (12.5)
42 (40.1)
124 (135)
905 (1379)
1348 (n/a)
24 (27)
85 (97)
1696 (n/a)
1006 (1270)
117 (136)
900 (1480)
125 (115)
1268 (n/a)
1387 (n/a)
285 (250)
183 (188)
80 (81)
1778 (n/a)
1930 (n/a)
1294 (n/a)
1745 (n/a)
174 (182)

100
100
100
100
100
100
100
100
100
100
100
100
95
100
100
86
100
100
100
100
100
97
100
100
100
88
37
96
79
100

NuMVC
time (IQR)
0.24 (0.18)
4.08 (3.77)
1.07 (1.03)
2.76 (2.83)
10.14 (10.54)
2.71 (2.6)
5 (5.1)
14 (11.9)
4 (4.3)
11 (10.9)
38 (46)
177 (149)
606 (788)
8 (7)
19 (19)
895 (1099)
205 (200)
51 (48)
266 (311)
40 (44)
470 (466)
659 (780)
121 (118)
50 (49)
27 (23)
843 (849)
1677 (n/a)
636 (788)
1004 (1391)
62 (70)

Table 4: Comparison of NuMVC with other state-of-the-art local search algorithms on the
BHOSLIB benchmark. All these BHOSLIB instances have a hidden optimal vertex cover,
whose size is shown in the VC∗ column.

702

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

100

2000

90

1800

80

1600

70

1400
average run time (s)

average success rate

the best performance for this hard random benchmark, vastly improving the existing performance
results. We also observe that, NuMVC always has the minimum IQR value for all instances, which
indicates that apart from its efficiency, the robustness of NuMVC is also better than other solvers.

60
50
40
30
20

1200
1000
800

PLS
COVER
EWCC
NuMVC

600
400

10
0
760(frb40)

PLS
COVER
EWCC
NuMVC

200
945(frb45)

0
760(frb40)

1150(frb50)1272(frb53) 1400(frb56) 1534(frb59)
number of vertices in graph

945(frb45)

1150(frb50)1272(frb53) 1400(frb56) 1534(frb59)
number of vertices in graph

Figure 1: Comparison of NuMVC and other local search algorithms on the BHOSLIB benchmark
in terms of success rate (left) and averaged run time (right)
We also compare NuMVC with COVER and EWCC on the challenging instance frb100-40.
Given the failure of PLS on large BHOSLIB instances, we do not run PLS on this instance.
The comparative results on frb100-40 are shown in Table 5, which indicates that NuMVC
significantly outperforms COVER and EWCC on this challenging instance.
Finally, we would like to remark that the performance of NuMVC on the BHOSLIB benchmark
is better than a four-core version of CLS (Pullan et al., 2011), even if we do not divide the run time
of NuMVC by 4 (the number of cores utilized by CLS). If we consider the machine speed ratio and
divide the run time of NuMVC by 4, then NuMVC would be dramatically better than CLS on the
BHOSLIB benchmark.
Size
of VC
3902
≤ 3903

suc
0
33

COVER
avg suc time
n/a
2768

suc
1
79

EWCC
avg suc time
2586
2025

suc
4
93

NuMVC
avg suc time
2955
1473

Table 5: Comparative results on the frb100-40 challenging instance. Each solver is executed
100 times on this instance with a timeout of 4000 seconds.

6.5 Comparison with Exact Algorithms
In this section, we compare NuMVC with a state-of-the-art exact Maximum Clique algorithm.
Generally, exact algorithms and heuristic algorithms are somewhat complementary in their
applications. Usually, exact algorithms find solutions for structured instances faster while heuristic
algorithms are faster on random ones.
703

C AI , S U , L UO & S ATTAR

Compared to MVC and MIS, many more exact algorithms are designed for the Maximum Clique
problem (Carraghan & Pardalos, 1990; Fahle, 2002; Östergård, 2002; Régin, 2003; Tomita &
Kameda, 2009; Li & Quan, 2010b, 2010a). The recent branch-and-bound MC algorithm MaxCLQ
(Li & Quan, 2010b) which utilizes MaxSAT inference technologies (Li, Manyà, & Planes, 2007)
to improve upper bounds shows considerable progress. Experimental results of MaxCLQ (Li &
Quan, 2010b) on some random graphs and DIMACS instances indicate that MaxCLQ significantly
outperforms previous exact MC algorithms. The MaxCLQ algorithm is further improved using two
strategies called Extended Failed Literal Detection and Soft Clause Relaxation, resulting in a better
algorithm denoted by MaxCLQdyn+EFL+SCR (Li & Quan, 2010a). Due to the great success of
MaxCLQdyn+EFL+SCR, we compare our algorithm only with MaxCLQdyn+EFL+SCR.
We compare NuMVC with MaxCLQdyn+EFL+SCR on the DIMACS benchmark instances.
The results of MaxCLQdyn+EFL+SCR are taken from the previous work (Li & Quan, 2010a).
MaxCLQdyn+EFL+SCR is not evaluated on the BHOSLIB benchmark which is much harder and
requires more effective technologies for exact algorithms (Li & Quan, 2010a).
The run time results of MaxCLQdyn+EFL+SCR are obtained on a 3.33 GHz Intel Core 2 Duo
CPU with linux and 4 Gb memory, which required 0.172 seconds for r300.5, 1.016 seconds for
r400.5 and 3.872 seconds for r500.5 to execute the DIMACS machine benchmarks (Li & Quan,
2010a). The corresponding run time for our machine is 0.19, 1.12 and 4.24 seconds. So, we multiply
the reported run time of MaxCLQdyn+EFL+SCR by 1.098 (=(4.24/3.872+1.12/1.016)/2=1.098, the
average of the two largest ratios). This normalization is based on the methodology established in
the Second DIMACS Implementation Challenge for Cliques, Coloring, and Satisfiability, and is
widely used for comparing different MaxClique algorithms (Pullan & Hoos, 2006; Pullan, 2006; Li
& Quan, 2010b, 2010a).
Graph
Instance
brock400 2
brock400 3
brock400 4
brock800 2
brock800 3
brock800 4
keller5
MANN a27
MANN a45

V C∗

371
369
367
776
775
774
749
252
690

NuMVC
suc
time
96
100
100
0
0
0
100
100
100

572.39
8.25
4.98
n/a
n/a
n/a
0.04
<0.001
86.86

MaxCLQdyn+EFL
+SCR time
125.06
251.44
119.24
5138.10
3298.39
2391.44
6884.46
0.17
21.169

Graph
Instance

V C∗

NuMVC
suc
time

p hat300-3
p hat700-2
p hat700-3
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
sanr200 0.9
sanr400 0.7

264
656
638
954
932
1488
1435
158
379

100
100
100
100
100
100
100
100
100

0.001
0.006
0.008
0.019
0.032
3.75
0.071
<0.001
0.008

MaxCLQdyn+EFL
+SCR time
1.31
3.27
1141.92
108.94
113860.40
3.10
866.51
5.20
97.72

Table 6: Comparison of NuMVC with the state-of-the-art exact MaxClique algorithm MaxCLQdyn+EFL+SCR for the DIMACS benchmark.

In Table 6, we present the performance of NuMVC and MaxCLQdyn+EFL+SCR on the
DIMACS instances. The results indicate that NuMVC finds an optimal solution much faster
than MaxCLQdyn+EFL+SCR on random instances such as the p hat and sanr instances. We
believe that similar results would hold for other hard random benchmarks like BHOSLIB ones, as
MaxCLQdyn+EFL+SCR is not evaluated on these instances due to their high hardness (Li & Quan,
2010a), while NuMVC performs very well on them.
For structured instances, we note that MaxCLQdyn+EFL+SCR is mainly evaluated on the
brock instances where NuMVC performs worst, but not on the open DIMACS instances such
704

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

as MANN a81, johnson32-2-4 and keller6, which remain very difficult to solve by exact
algorithms (Li & Quan, 2010a). Although MaxCLQdyn+EFL+SCR overall performs better,
NuMVC also finds an optimal solution significantly faster than MaxCLQdyn+EFL+SCR on some
structured instances, such as the two brock instances and keller5.
Finally, we would like to note that although heuristic solvers can find optimal solutions fast, they
are unable to prove the optimality of the solutions they find. On the other hand, the run time of an
exact algorithm is spent not only on finding an optimal solution but also on proving its optimality.
In this sense, heuristic and exact algorithms cannot be compared in a fair way. Nevertheless, our
experiments suggest that heuristic approaches are appealing for solving large instances in reasonable
short time.

7. Discussions
In this section, we first explore the run-time distribution of NuMVC on some representative
instances, and then investigate the effectiveness of the two-stage exchange strategy and the
forgetting mechanism in NuMVC. Finally, we analyze the performance of NuMVC with different
settings to its two parameters for the forgetting mechanism, which shows that NuMVC is not
sensitive to the parameters.
7.1 Run-time Distributions of NuMVC
In this subsection, we conduct an empirical study to gain deeper insights of the run-time behavior of
NuMVC. More specifically, we study the run-time distribution of NuMVC on several representative
instances. For the purpose of comparison, we also report the run-time distribution of EWCC, which
is the best competing MVC local search solver.
Consider a randomized algorithm solving a given optimization problem instance, and halting
as soon as an optimal solution is found. The run time of the algorithm can be viewed as a
random variable, which is fully described by its distribution, commonly referred to as the run-time
distribution (RTD) in the literature about algorithm performance modeling (Hoos & Stützle, 2004;
Bartz-Beielstein et al., 2010). The methodology of studying the run-time behavior of algorithms
based on RTDs has been widely used in empirical analysis of heuristic algorithms (Hoos & Stützle,
1999; Finkelstein, Markovitch, & Rivlin, 2003; Watson, Whitley, & Howe, 2005; Pullan & Hoos,
2006). We also follow the same methodology in our study here.
For studying typical run-time behaviour, we choose instances where NuMVC reaches an optimal
solution in all 100 runs, and are of appropriate difficulty. For the DIMACS benchmark, we select
brock400 4 and MANN a45, both of which are of reasonable size and hardness. Also, these two
instances represent two typical instance classes for NuMVC, as NuMVC has poor performance on
the brock instances, while it dominates other heuristic algorithms on the MANN instances. For
the BHOSLIB benchmark, frb56-25-5 and frb59-26-5 are selected. These are appropriate
instances for studying the run-time behavior of NuMVC, since they are neither too easy that can be
solved in a short time nor too difficult to reach a 100% success rate.
The empirical RTD graphs of NuMVC and EWCC are shown in Figure 2 (the RTD for each
instance is based on 100 independent runs that all reach a respective optimal solution). According to
the graphs, NuMVC shows a large variability in run time. Further investigation indicates that these
RTDs are quite well approximated by exponential distributions, labeled ed[m](x) = 1 − 2−x/m ,
where m is the median of the distribution. To test the goodness of the approximations, we use a
705

C AI , S U , L UO & S ATTAR

Empirical RTD of NuMVC and EWCC on MANN_a45

Empirical RTD of NuMVC and EWCC on brock400_4
1
0.9
0.8
0.7

1
0.9

RTD for NuMVC
ed[3.6]
RTD for EWCC
ed[12]

0.8
0.7
0.6
P(solve)

P(solve)

0.6
0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
−2
10

−1

0

10

1

2

10
10
run−time [CPU sec]

10

0
−1
10

3

10

Empirical RTD of NuMVC and EWCC on frb56−25−5

0.8
0.7

0.9

RTD for NuMVC
ed[19]
RTD for EWCC
ed[53]

0.8
0.7

2

3

10

4

10

RTD for NuMVC
ed[45]
RTD for EWCC
ed[116]

0.6
P(solve)

P(solve)

1

10
10
run−time [CPU sec]

1

0.6
0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
−1
10

0

10

Empirical RTD of NuMVC and EWCC on frb59−26−5

1
0.9

RTD for NuMVC
ed[59]
RTD for EWCC
ed[532]

0

10

1

10
run−time [CPU sec]

2

10

0
−1
10

3

10

0

10

1

10
run−time [CPU sec]

2

10

3

10

Figure 2: Run-time distributions (RTDs) of NuMVC and EWCC applied to two DIMACS instances
(top) and two BHOSLIB instances (bottom); these empirical RTDs are well approximated
by exponential distributions, labeled ed[m](x) = 1 − 2−x/m in the plots.

Kolmogorov-Smirnov test, which fails to reject the null hypothesis that the sampled run time
stems from the exponential distributions shown in the figures at a standard confidence level of
α = 0.05 with p-values between 0.19 and 0.88. For EWCC, the Kolmogorov-Smirnov test
shows its RTDs on MANN a45 and the two BHOSLIB instances are also exponential distributions,
while its RTD on brock400 4 is not from an exponential distribution.
The observation of exponential RTDs of NuMVC is consistent with similar results for other
high performance SLS algorithms, e.g., for MaxClique (Pullan & Hoos, 2006), for SAT (Hoos &
Stützle, 1999), for MAXSAT (Smyth, Hoos, & Stützle, 2003), and for scheduling problems (Watson
et al., 2005). By the arguments (Hoos & Stützle, 1999; Hoos & Stützle, 2004) made for stochastic
local search algorithms characterized by an exponential RTD, we conclude that, for NuMVC, the
probability of finding an optimal solution within a fixed amount of time (or steps) does not depend
on the run time in the past. Consequently, it is very robust w.r.t. the cutoff time and thus, the restart
706

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

time. Therefore, performing multiple independent runs of NuMVC in parallel will result in closeto-optimal parallelization speedup. Similar observations were made for most of the other DIMACS
instances and BHOSLIB instances.
Of practical interest is also the RTD analysis for NuMVC on difficult instances for which all
algorithms in our experiments fail to achieve a high success rate (i.e., 40%). The RTDs in these
cases would show where the algorithm stagnates and suggest an a-posteriori restart time for the
algorithm. For this purpose, we select MANN a81 and frb59-26-2 for analysis. The RTDs of
NuMVC on these two instances are illustrated in Figure 3. Interestingly, from these RTDs we do not
observe any obvious stagnation, which again confirms that NuMVC is robust w.r.t. the cutoff time
and thus the restart time. Therefore, by increasing the cutoff time, we can expect a higher success
rate of the algorithm on these difficult instances.
Empirical RTD of NuMVC on MANN_a81 and frb59−26−2
1
0.9
RTD of NuMVC on MANN_a81
0.8
RTD of NuMVC on frb59−26−2
0.7

P(solve)

0.6
0.5
0.4
0.3
0.2
0.1
0
1
10

2

3

10

10

4

10

run−time [CPU sec]

Figure 3: Run-time distributions (RTDs) of NuMVC on MANN a81 and frb59-26-2 instances,
for which NuMVC finds an optimal (or best known) solution in less than half runs.

7.2 Effectiveness of Two-Stage Exchange
To study the effectiveness of the two-stage exchange strategy, we compare NuMVC with its
alternative algorithm NuMVC0 which selects two vertices for exchanging simultaneously. In each
step, NuMVC0 first chooses an uncovered edge e uniformly at random, and then evaluates each pair
of vertices u and v where u is in the current candidate solution and v is one endpoint of e such
that conf Change(v) = 1. For evaluating the benefit (i.e., the decrement of the cost function)
of exchanging a vertex pair u and v, NuMVC0 first checks whether they are neighbors. If u
and v are neighbors, the benefit is dscore(u) + dscore(v) + w(e{u, v}); otherwise, the benefit
is dscore(u) + dscore(v). NuMVC0 selects the vertex pair with the greatest benefit to exchange.
In the NuMVC (and also NuMVC0 ) algorithm, there are only two candidate vertices to add to
the current candidate solution C (i.e., the endpoints of the selected uncovered edge). Hence, in
the worst case, NuMVC performs 2 + |C| evaluations, while NuMVC0 has to evaluate 2 × |C|
pairs of vertices. Moreover, NuMVC only needs to check the dscore of a vertex in each (vertex)
707

C AI , S U , L UO & S ATTAR

evaluation, while NuMVC0 performs a vertex-pair evaluation which involves a pair of vertices and
their relationship, and thus is more time-consuming. Based on the above analysis, we conjecture
that the complexity per step of NuMVC is at least 2 times lower than that of NuMVC0 . Also, as we
have mentioned in Section 3, the two-stage exchange strategy is less greedy than the one selecting
two vertices for exchanging simultaneously, as NuMVC0 does.
The investigation is carried out on 4 DIMACS instances from different families as well as
12 BHOSLIB instances. For the DIMACS benchmark, we select brock400 2, C4000.5,
MANN a45, and p hat 1500-1. These instances have different characteristics, as described
below (Pullan et al., 2011). Note that the following conclusions on DIMACS instances are for
the complementary DIMACS graphs.
• The DIMACS brock instances have minimum vertex covers that consist of medium to lower
degree vertices, and are designed to defeat greedy heuristics.
• The DIMACS C and p hat 1500-1 instances have minimum vertex covers that consist of
higher degree vertices and can be effectively solved by greedy heuristics.
• The DIMACS MANN instances have a large proportion of plateaus in the instance searchspace, and thus greedy heuristics are unsuitable to solve them.
• The BHOSLIB instances have minimum vertex covers consisting of vertices whose
distribution of vertex degree closely matches that for the complete graph. These are difficult
instances for both greedy and diversification heuristics.
Graph
Instance
brock400 2
C4000.5
MANN a45
p hat 1500-1
frb50-23-1
frb50-23-2
frb50-23-3
frb53-24-1
frb53-24-2
frb53-24-3
frb56-25-1
frb56-25-2
frb56-25-3
frb59-26-1
frb59-26-2
frb59-26-3

V C∗
371
3982
690
1488
1100
1100
1100
1219
1219
1219
1344
1344
1344
1475
1475
1475

suc
96
100
100
100
100
100
95
86
100
100
100
97
100
88
37
96

time
572
252
86
3.75
38
177
606
895
205
51
470
659
121
843
1677
636

NuMVC
steps
645631471
7802785
90642150
445830
24628019
113569606
386342329
514619149
117980833
29376406
259903023
350048132
67043078
440874471
875964146
325417225

#steps/sec (105 )
11.3
0.3
10.5
1.2
6.5
6.4
6.4
5.7
5.8
5.8
5.5
5.3
5.5
5.2
5.2
5.1

suc
19
100
100
100
100
100
63
45
100
100
72
52
100
45
21
69

time
1861
607
564
13.24
88
499
1312
1595
557
106
1088
1499
253
1572
1853
1545

NuMVC0
steps
837844749
6343304
186350533
381762
18125042
104841043
262559614
286396840
105863802
19685358
184323492
254973016
43062419
251520339
315425608
247273810

#steps/sec (105 )
4.5
0.1
3.3
0.3
2.1
2.1
2.0
1.8
1.9
1.9
1.7
1.7
1.7
1.6
1.7
1.6

Table 7: Comparative performance of NuMVC and NuMVC0 which selects two vertices for
exchanging simultaneously. The results are based on 100 independent runs for each solver
on each instance.
The comparative results of NuMVC and NuMVC0 are presented in Table 7. The results show
that NuMVC significantly outperforms NuMVC0 in terms of averaged run time, primarily due to
its much lower complexity per step. In each second, NuMVC performs 3-4 times more steps than
708

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

NuMVC0 , which supports our conjecture that the complexity per step of NuMVC is more than 2
times lower than that of NuMVC0 .
Now we turn our attention to comparing NuMVC and NuMVC0 in terms of step performance,
which is independent from the complexity per step. For brock and MANN graphs which are difficult
for greedy heuristics, NuMVC has a significantly better step performance than NuMVC0 . On the
other hand, for greedy-friendly graphs such as C4000.5 and p hat 1500-1, NuMVC needs
more steps to converge to an optimal solution than NuMVC0 does. These observations support our
argument that the two-stage exchange strategy is less greedy than the one that selects two vertices
for exchanging simultaneously, as NuMVC0 does.
We also observe that the step performance of NuMVC0 is better than that of NuMVC on
BHOSLIB instances. For instance, on those BHOSLIB instances where both algorithms have a
100% success rate, NuMVC needs about 1.2 times more steps than NuMVC0 to find an optimal
solution. This is what we do not expect and cannot yet explain. Nevertheless, as NuMVC makes
rather rapid modifications to a solution, a little degrade in step performance does not hurt.
Graph
Instance
C4000.5
MANN a45
p hat 1500-1
frb53-24-5
frb56-25-5
frb59-26-5

PLS
#steps/sec
85,318
1,546,625
170,511
841,346
801,282
706,436

COVER
#steps/sec
8,699
279,514
19,473
128,971
116,618
108,534

EWCC
#steps/sec
11,927
578,656
34,111
219,038
199,441
189,536

NuMVC
#steps/sec
30,963
1,053,978
118,888
570,425
522,561
511,014

Table 8: Complexity per step on selected instances
To further demonstrate the low complexity per step of NuMVC, we compare the number
of search steps per second between NuMVC and other state-of-the-art heuristic solvers on
representative instances. As indicated in Table 8, NuMVC executes many more steps in each second
than the other two MVC local search solvers COVER and EWCC do. For the instances in Table
8, each second NuMVC executes 4-6 times more steps than COVER, and 3-4 times more steps
than EWCC. This indicates that the two-stage exchange strategy can significantly accelerate MVC
local search algorithms. Although PLS performs more steps per second than NuMVC, it is an MC
local search algorithm whose search scheme is essentially different from those of MVC local search
algorithms.
7.3 Effectiveness of the Forgetting Mechanism
To study the effectiveness of the forgetting mechanism in NuMVC, we compare NuMVC with its
two alternative algorithms NuMVC1 and NuMVC2 , which are obtained from NuMVC by modifying
the edge weighting scheme as below.
• NuMVC1 works in the same way as NuMVC, except for not using the forgetting mechanism,
that is, deleting line 18 from Algorithm 1.
• NuMVC2 adopts the forgetting mechanism used in DLS-MC (Pullan & Hoos, 2006) for the
weighting scheme. More specifically, NuMVC2 increases all weights of uncovered edges by
709

C AI , S U , L UO & S ATTAR

one at the end of each step, and performs a forgetting operation every pd steps by decreasing
weights by one for all edges whose weights are greater than one. Note that pd is an instancedependent parameter.
The experiments were carried out with some representative instances from both benchmarks.
For the DIMACS benchmark, we select brock400 2, C4000.5, keller6, and MANN a45,
which are from different classes and of appropriate difficulty. For the BHOSLIB benchmark, we
select three instances for each of the three largest-sized instance groups respectively.
Graph
Instance Vertices
brock400 2
400
C4000.5
4000
keller6
3361
MANN a45
1035
frb53-24-1
1272
frb53-24-2
1272
frb53-24-3
1272
frb56-25-1
1400
frb56-25-2
1400
frb56-25-3
1400
frb59-26-1
1534
frb59-26-2
1534
frb59-26-3
1534

C∗

V
371
3982
3302
690
1219
1219
1219
1344
1344
1344
1475
1475
1475

NuMVC
suc time
96
572
100
252
100 2.51
100
86
86
895
100
205
100
51
100
470
97
659
100
121
88
843
37 1677
96
636

NuMVC1
suc time
22 1781
100
270
100 2.95
65 1187
60
925
100
243
100
49
85
914
63 1209
100
111
64 1229
21 1894
83
997

NuMVC2
pd (102 ) suc time
15 100
21
60 100
327
750 100 4.26
8 100
113
100
78
901
100 100
201
100 100
52
130
91
595
130
81
739
130 100
117
150
85
907
150
45 1439
150
97
652

Table 9: Comparative performance of NuMVC and its two alternatives NuMVC1 and NuMVC2 .
Each algorithm is performed 100 times on each instance.

An apparent observation from Table 9 is that the two algorithms with a forgetting mechanisms
(i.e., NuMVC and NuMVC2 ) outperform NuMVC1 on almost all instances. Particularly, due to
the missing of a forgetting mechanism, NuMVC1 performs significantly worse than the other two
algorithms on brock and MANN graphs. On the other hand, Table 9 demonstrates that NuMVC and
NuMVC2 exhibit competitive performance on the BHOSLIB benchmark, and dominate on different
types of DIMACS instances. More specifically, NuMVC outperforms NuMVC2 on C4000.5,
keller6 and MANN a45, but performs significantly worse than NuMVC2 on brock400 2. In
order to find out the genuine performance of NuMVC2 on brock instances, we test NuMVC2 on
the larger brock800 2 and brock800 4 instances. The results show that these two large brock
instances are substantially more difficult than the two brock400 instances, and NuMVC2 also fails
to solve neither of them.
Although NuMVC2 shows competitive performance with NuMVC, its performance is given by
optimizing the pd parameter for each instance. Moreover, as with DLS-MC (Pullan & Hoos, 2006),
NuMVC2 is considerably sensitive to the pd parameter. For example, our experiments show that on
the frb53-24 instances, NuMVC2 performs quite well with pd = 10000, but it fails to find an
optimal solution when pd is set to be a value less than 7000. Comparatively, NuMVC with the same
710

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

parameter setting performs quite well on all types of instances but the brock family. Actually, we
will show in the next section that NuMVC is not sensitive to its parameters.
It is also interesting to compare NuMVC with its alternatives which replace the forgetting
mechanism with the smoothing techniques similar to those in local search for SAT. Indeed, earlier
versions of NuMVC did use the smoothing techniques similar to those in SAT local search, and
they did not have good performance compared with NuMVC. It would be interesting to find out the
reasons for the success of the forgetting mechanism and the failure of those smoothing techniques
in MVC edge weighting local search algorithms such as NuMVC.
7.4 Parameters for the Forgetting Mechanism

(0.3|V |, 0.1)
(0.3|V |, 0.2)
(0.3|V |, 0.3)
(0.3|V |, 0.4)
(0.3|V |, 0.5)

brock400 2
100% (382)
100% (361)
100% (362)
95% (490)
90% (507)

MANN a45
100% (153)
100% (164)
100% (131)
100% (208)
100% (90)

C4000.5
100% (262)
100% (265)
100% (272)
100% (270)
100% (268)

frb53-24-1
80% (904)
85% (918)
70% (1058)
80% (995)
65% (1316)

frb53-24-2
100% (348)
100% (279)
100% (156)
100% (191)
100% (431)

frb56-25-1
100% (338)
90% (671)
95% (826)
100% (602)
100% (490)

frb56-25-2
80% (997)
70% (1197)
85% (819)
100% (885)
95% (922)

(0.4|V |, 0.1)
(0.4|V |, 0.2)
(0.4|V |, 0.3)
(0.4|V |, 0.4)
(0.4|V |, 0.5)

100% (261)
90% (736)
100% (402)
95% (375)
90% (612)

100% (133)
100% (207)
100% (176)
100% (169)
100% (190)

100% (250)
100% (245)
100% (258)
100% (253)
100% (264)

80% (899)
80% (860)
75% (1047)
70% (1009)
65% (1059)

100% (158)
100% (443)
100% (260)
100% (394)
100% (137)

90% (464)
85% (611)
90% (976)
90% (885)
95% (428)

70% (1601)
80% (851)
90% (1055)
85% (1019)
100% (851)

(0.5|V |, 0.1)
(0.5|V |, 0.2)
(0.5|V |, 0.3)
(0.5|V |, 0.4)
(0.5|V |, 0.5)

100% (523)
85% (950)
96% (572)
90% (499)
90% (968)

100% (107)
100% (69)
100% (86)
100% (169)
100% (148)

100% (262)
100% (259)
100% (252)
100% (251)
100% (249)

70% (1007)
75% (1061)
86% (850)
70% (931)
90% (805)

100% (416)
100% (482)
100% (205)
100% (219)
100% (361)

90% (714)
95% (706)
100% (470)
90% (632)
85% (933)

75% (1064)
70% (1228)
97% (625)
80% (1027)
85% (983)

(0.6|V |, 0.1)
(0.6|V |, 0.2)
(0.6|V |, 0.3)
(0.6|V |, 0.4)
(0.6|V |, 0.5)

100% (527)
80% (713)
75% (976)
100% (710)
85% (742)

100% (203)
100% (172)
100% (92)
100% (142)
100% (125)

100% (255)
100% (279)
100% (272)
100% (276)
100% (288)

70% (1109)
75% (944)
70% (1130)
75% (907)
80% (947)

100% (267)
100% (254)
100% (298)
100% (170)
100% (192)

100% (828)
90% (704)
90% (689)
100% (592)
100% (647)

90% (878)
70% (1306)
75% (862)
85% (1028)
80% (1109)

(0.7|V |, 0.1)
(0.7|V |, 0.2)
(0.7|V |, 0.3)
(0.7|V |, 0.4)
(0.7|V |, 0.5)

100% (410)
95% (781)
90% (826)
75% (1219)
90% (707)

100% (87)
100% (128)
100% (125)
100% (101)
100% (92)

100% (273)
100% (284)
100% (266)
100% (272)
100% (280)

65% (1186)
70% (1035)
75% (916)
85% (700)
70% (1085)

100% (358)
100% (220)
100% (206)
100% (338)
100% (352)

75% (1014)
80% (713)
80% (878)
100% (536)
90% (736)

75% (934)
90% (510)
80% (971)
85% (769)
70% (1044)

Table 10: Comparative performance of NuMVC with various parameter combinations (γ, ρ) for
the forgetting mechanism. For each instance, NuMVC is performed 20 times with each
parameter combination, except for the one adopted in this work (0.5|V |, 0.3), where the
results are based on 100 runs. For keller6, NuMVC performs almost the same with
various parameters, having the same success rate (100%) and tiny difference of averaged
run time (less than 1 second), and thus the results are not reported in the table.

711

C AI , S U , L UO & S ATTAR

The NuMVC algorithm has two parameters γ and ρ, which specify the forgetting mechanism.
Specifically, when the averaged weight of all edges achieves a threshold γ, all edge weights are
multiplied by a constant factor ρ (0 < ρ < 1). In this subsection, we investigate how NuMVC
performs with different settings to these two parameters. The investigation is carried out on both
DIMACS and BHOSLIB benchmarks. For the DIMACS benchmark, we select the four instances
used in the preceding subsection for the same reasons. For the BHOSLIB benchmark, we select
frb53-24-1, frb53-24-2, frb56-25-1 and frb56-25-2, which are of different sizes
and appropriate hardness.
Table 10 presents the performance of NuMVC with various parameter combinations of γ and
ρ on the representative instances. As we can see from Table 10, the parameter combination
(0.5|V |, 0.3) yields relatively good performance for all instances, and exhibits a better robustness
over the instances than other parameter combinations do.
On the other hand, we observe that NuMVC with various parameter combinations performs
comparably on these tested instances. For example, for all parameter settings, NuMVC achieves
a success rate of 100% for keller6, MANN a45, C4000.5 as well as frb53-24-1, and
the averaged run time difference on these instances is not so significant. For other instances, the
difference of success rate never exceeds 25% between any two parameter settings. This observation
indicates that NuMVC seems not sensitive to the two parameters. Actually, as we have mentioned
before, NuMVC exhibits very good performance for both DIMACS and BHOSLIB benchmarks
with a fixed parameter setting. This is an advantage compared to other forgetting mechanisms
such as the one used in DLS-MC (Pullan & Hoos, 2006), which is sensitive to its parameter. For
algorithms that are sensitive to their parameters, considerable parameter tuning is required in order
to get a good performance for a certain instance, which usually costs much more time than solving
the instance.

8. Conclusions and Future Work
In this paper, we presented two new local search strategies for the minimum vertex cover (MVC)
problem, namely two-stage exchange and edge weighting with forgetting. The two-stage exchange
strategy yields an efficient two-pass move operator for MVC local search algorithms, which
significantly reduces the time complexity per step. The forgetting mechanism enhances the
edge weighting scheme by decreasing weights when the averaged weight reaches a threshold,
to periodically forget earlier weighting decisions. Based on these two strategies, we designed
a slight, yet effective MVC local search algorithm called NuMVC. The NuMVC algorithm was
evaluated against the best known heuristic algorithms for MVC (MC, MIS) on standard benchmarks,
i.e., the DIMACS and BHOSLIB benchmarks. The experimental results show that NuMVC is
largely competitive on the DIMACS benchmark and dramatically outperforms other state-of-the-art
heuristic algorithms on all BHOSLIB instances.
Furthermore, we showed that NuMVC is characterized by exponential RTDs, which means it is
robust w.r.t. the cutoff parameters and the restart time, and hence has close-to-optimal parallelization
speedup. We also performed further investigations to provide further insights into the two new
strategies and their effectiveness. Finally, we conducted an experiment to study the performance of
NuMVC with different parameter settings, and the results indicate that NuMVC is not sensitive to
its parameters.
712

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

The two-stage exchange strategy not only has a lower time complexity per step, but also has
the flexibility to allow us to employ specific heuristics in different stages. An interesting research
direction is thus to apply this idea to other combinatorial problems whose essential tasks are also to
seek for an optimal subset with some fixed cardinality.

Acknowledgments
This work is supported by 973 Program 2010CB328103, ARC Future Fellowship FT0991785,
National Natural Science Foundation of China (61073033, 61003056 and 60903054), and
Fundamental Research Funds for the Central Universities of China (21612414). We would like
to thank the editor and anonymous reviewers for their valuable comments on earlier versions of this
paper. We would also like to thank Yanyan Xu for proofreading this paper.

References
Aggarwal, C., Orlin, J., & Tai, R. (1997). Optimized crossover for the independent set problem.
Operations Research, 45, 226–234.
Andrade, D. V., Resende, M. G. C., & Werneck, R. F. F. (2008). Fast local search for the maximum
independent set problem. In Workshop on Experimental Algorithms, pp. 220–234.
Barbosa, V. C., & Campos, L. C. D. (2004). A novel evolutionary formulation of the maximum
independent set problem. J. Comb. Optim., 8(4), 419–437.
Bartz-Beielstein, T., Chiarandini, M., Paquete, L., & Preuss, M. (Eds.). (2010). Experimental
Methods for the Analysis of Optimization Algorithms. Springer, Berlin, Heidelberg, New
York.
Battiti, R., & Protasi, M. (2001). Reactive local search for the maximum clique problem.
Algorithmica, 29(4), 610–637.
Busygin, S., Butenko, S., & Pardalos, P. M. (2002). A heuristic for the maximum independent set
problem based on optimization of a quadratic over a sphere. J. Comb. Optim., 6(3), 287–297.
Cai, S., & Su, K. (2011). Local search with configuration checking for SAT. In Proc. of ICTAI-11,
pp. 59–66.
Cai, S., & Su, K. (2012). Configuration checking with aspiration in local search for SAT. In Proc.
of AAAI-12, pp. 434–440.
Cai, S., Su, K., & Chen, Q. (2010). EWLS: A new local search for minimum vertex cover. In Proc.
of AAAI-10, pp. 45–50.
Cai, S., Su, K., & Sattar, A. (2011). Local search with edge weighting and configuration checking
heuristics for minimum vertex cover. Artif. Intell., 175(9-10), 1672–1696.
Cai, S., Su, K., & Sattar, A. (2012). Two new local search strategies for minimum vertex cover. In
Proc. of AAAI-12, pp. 441–447.
Carraghan, R., & Pardalos, P. (1990). An exact algorithm for the maximum clique problem.
Operations Research Letters, 9(6), 375–382.
713

C AI , S U , L UO & S ATTAR

Dinur, I., & Safra, S. (2005). On the hardness of approximating minimum vertex cover. Annals of
Mathematics, 162(2), 439–486.
Evans, I. (1998). An evolutionary heuristic for the minimum vertex cover problem. In Proceedings
of the Seventh International Conference on Evolutionary Programming(EP), pp. 377–386.
Fahle, T. (2002). Simple and fast: Improving a branch-and-bound algorithm for maximum clique.
In Proc. of European Symposium on Algorithms (ESA)-02, pp. 485–498.
Feige, U. (2004). Approximating maximum clique by removing subgraphs. SIAM J. Discrete Math.,
18(2), 219–225.
Finkelstein, L., Markovitch, S., & Rivlin, E. (2003). Optimal schedules for parallelizing anytime
algorithms: The case of shared resources. J. Artif. Intell. Res. (JAIR), 19, 73–138.
Gajurel, S., & Bielefeld, R. (2012). A fast near optimal vertex cover algorithm (novca).
International Journal of Experimental Algorithms (IJEA), 3, 9–18.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of NPcompleteness. Freeman, San Francisco, CA, USA.
Glover, F. (1989). Tabu search – part i. ORSA Journal on Computing, 1(3), 190–206.
Grosso, A., Locatelli, M., & Pullan, W. J. (2008). Simple ingredients leading to very efficient
heuristics for the maximum clique problem. J. Heuristics, 14(6), 587–612.
Halperin, E. (2002). Improved approximation algorithms for the vertex cover problem in graphs
and hypergraphs. SIAM Journal on Computing, 31(5), 1508–1623.
Håstad, J. (1999). Clique is hard to approximate within n1−ǫ . Acta Math, 182, 105–142.
Håstad, J. (2001). Some optimal inapproximability results. J. ACM, 48(4), 798–859.
Hoaglin, D. C., Mosteller, F., & Tukey, J. W. (Eds.). (2000). Understanding Robust and Exploratory
Data Analysis. Wiley Classics Library, Wiley, New York, NY.
Hoos, H., & Stützle, T. (2004). Stochastic Local Search: Foundations and Applications. Morgan
Kaufmann, San Francisco, CA, USA.
Hoos, H. H., & Stützle, T. (1999). Towards a characterisation of the behaviour of stochastic local
search algorithms for SAT. Artif. Intell., 112(1-2), 213–232.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling and probabilistic smoothing: Efficient
dynamic local search for SAT. In Proc. of CP-02, pp. 233–248.
Ishtaiwi, A., Thornton, J., Sattar, A., & Pham, D. N. (2005). Neighbourhood clause weight
redistribution in local search for SAT. In Proc. of CP-05, pp. 772–776.
Johnson, D. S., & Trick, M. (Eds.). (1996). Cliques, Coloring, and Satisfiability: Second DIMACS
Implementation Challenge, 1993, Vol. 26 of DIMACS Series in Discrete Mathematics and
Theoretical Computer Science. American Mathematical Society, Providence, RI, USA.
Karakostas, G. (2005). A better approximation ratio for the vertex cover problem. In Proc. of
ICALP-05, pp. 1043–1050.
Katayama, K., Sadamatsu, M., & Narihisa, H. (2007). Iterated k-opt local search for the maximum
clique problem. In Proc. of EvoCOP-07, pp. 84–95.
714

N U MVC: A N E FFICIENT L OCAL S EARCH A LGORITHM FOR M INIMUM V ERTEX C OVER

Li, C. M., Manyà, F., & Planes, J. (2007). New inference rules for max-sat. J. Artif. Intell. Res.
(JAIR), 30, 321–359.
Li, C. M., & Quan, Z. (2010a). Combining graph structure exploitation and propositional reasoning
for the maximum clique problem. In Proc. of ICTAI-10, pp. 344–351.
Li, C. M., & Quan, Z. (2010b). An efficient branch-and-bound algorithm based on maxsat for the
maximum clique problem. In Proc. of AAAI-10, pp. 128–133.
Michiels, W., Aarts, E. H. L., & Korst, J. H. M. (2007). Theoretical aspects of local search. Springer.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: A heuristic
repair method for constraint satisfaction and scheduling problems. Artif. Intell., 58(1-3), 161–
205.
Morris, P. (1993). The breakout method for escaping from local minima. In Proc. of AAAI-93, pp.
40–45.
Östergård, P. R. J. (2002). A fast algorithm for the maximum clique problem. Discrete Applied
Mathematics, 120(1-3), 197–207.
Papadimitriou, C. H. (1991). On selecting a satisfying truth assignment. In Proc. of FOCS-91, pp.
163–169.
Pullan, W. (2006). Phased local search for the maximum clique problem. J. Comb. Optim., 12(3),
303–323.
Pullan, W. (2009). Optimisation of unweighted/weighted maximum independent sets and minimum
vertex covers. Discrete Optimization, 6, 214–219.
Pullan, W., & Hoos, H. H. (2006). Dynamic local search for the maximum clique problem. J. Artif.
Intell. Res. (JAIR), 25, 159–185.
Pullan, W., Mascia, F., & Brunato, M. (2011). Cooperating local search for the maximum clique
problem. J. Heuristics, 17(2), 181–199.
Régin, J. C. (2003). Using constraint programming to solve the maximum clique problem. In Proc.
of CP-03, pp. 634–648.
Richter, S., Helmert, M., & Gretton, C. (2007). A stochastic local search approach to vertex cover.
In Proc. of KI-07, pp. 412–426.
Schuurmans, D., Southey, F., & Holte, R. C. (2001). The exponentiated subgradient algorithm for
heuristic boolean programming. In Proc. of IJCAI-01, pp. 334–341.
Shyu, S. J., Yin, P., & Lin, B. M. T. (2004). An ant colony optimization algorithm for the minimum
weight vertex cover problem. Annals of OR, 131(1-4), 283–304.
Smyth, K., Hoos, H. H., & Stützle, T. (2003). Iterated robust tabu search for max-sat. In Proc. of
Canadian Conference on AI-03, pp. 129–144.
Taillard, É. D. (1994). Parallel taboo search techniques for the job shop scheduling problem.
INFORMS Journal on Computing, 6(2), 108–117.
Thornton, J., Pham, D. N., Bain, S., & Jr., V. F. (2004). Additive versus multiplicative clause
weighting for SAT. In Proc. of AAAI-04, pp. 191–196.
715

C AI , S U , L UO & S ATTAR

Tomita, E., & Kameda, T. (2009). An efficient branch-and-bound algorithm for finding a maximum
clique with computational experiments. J. Global Optimization, 44(2), 311.
Watson, J.-P., Whitley, L. D., & Howe, A. E. (2005). Linking search space structure, run-time
dynamics, and problem difficulty: A step toward demystifying tabu search. J. Artif. Intell.
Res. (JAIR), 24, 221–261.
Wu, Q., Hao, J.-K., & Glover, F. (2012). Multi-neighborhood tabu search for the maximum weight
clique problem. Annals of OR, 196(1), 611–634.
Wu, Z., & Wah, B. W. (2000). An efficient global-search strategy in discrete lagrangian methods
for solving hard satisfiability problems. In Proc. of AAAI/IAAI-00, pp. 310–315.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2005). A simple model to generate hard
satisfiable instances. In Proc. of IJCAI-05, pp. 337–342.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2007). Random constraint satisfaction: Easy
generation of hard (satisfiable) instances. Artif. Intell., 171(8-9), 514–534.
Xu, K., & Li, W. (2000). Exact phase transitions in random constraint satisfaction problems. J.
Artif. Intell. Res. (JAIR), 12, 93–103.
Xu, K., & Li, W. (2006). Many hard examples in exact phase transitions. Theoretical Computer
Science, 355, 291–302.
Yugami, N., Ohta, Y., & Hara, H. (1994). Improving repair-based constraint satisfaction methods
by value propagation. In AAAI, pp. 344–349.
Zuckerman, D. (2006). Linear degree extractors and the inapproximability of max clique and
chromatic number. In Proc. of STOC-06, pp. 681–690.

716

Journal of Artificial Intelligence Research 46 (2013) 165-201

Submitted 7/12; published 2/13

Generating Extractive Summaries of Scientific Paradigms
Vahed Qazvinian

vahed@umich.edu

Department of EECS,
University of Michigan, Ann Arbor, MI, 48109

Dragomir R. Radev

radev@umich.edu

Department of EECS & School of Information,
University of Michigan, Ann Arbor, MI, 48109

Saif M. Mohammad

saif.mohammad@nrc-cnrc.gc.ca

National Research Council Canada,
Ottawa, Ontario, Canada, K1A 0R6

Bonnie Dorr
David Zajic
Michael Whidby
Taesun Moon

bonnie@umiacs.umd.edu
dmzajic@umiacs.umd.edu
mawhidby@umd.edu
tsmoon@umiacs.umd.edu

Institute for Advanced Computer Studies & Computer Science,
University of Maryland, College Park, MD, 20742

Abstract
Researchers and scientists increasingly find themselves in the position of having to
quickly understand large amounts of technical material. Our goal is to effectively serve
this need by using bibliometric text mining and summarization techniques to generate
summaries of scientific literature. We show how we can use citations to produce automatically generated, readily consumable, technical extractive summaries. We first propose
C-LexRank, a model for summarizing single scientific articles based on citations, which
employs community detection and extracts salient information-rich sentences. Next, we
further extend our experiments to summarize a set of papers, which cover the same scientific topic. We generate extractive summaries of a set of Question Answering (QA) and
Dependency Parsing (DP) papers, their abstracts, and their citation sentences and show
that citations have unique information amenable to creating a summary.

1. Introduction
In today’s rapidly expanding disciplines, scientists and scholars are constantly faced with
the daunting task of keeping up with knowledge in their field. In addition, the increasingly
interconnected nature of real-world tasks often requires experts in one discipline to rapidly
learn about other areas in a short amount of time. Cross-disciplinary research requires
scientists in areas such as linguistics, biology, and sociology to learn about computational
approaches and applications such as computational linguistics, biological modeling, and
social networks. Authors of journal articles and books must write accurate summaries of
previous work, ranging from short summaries of related research to in-depth historical notes.
Interdisciplinary review panels are often called upon to review proposals in a wide range of
c
2013
AI Access Foundation. All rights reserved.

Qazvinian et Al.

areas, some of which may be unfamiliar to panelists. Thus, they must learn about a new
discipline “on the fly” in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and (2)
summarization techniques that exploit the content of the material in both the citing and
cited papers.
It is generally agreed upon that manually written abstracts are good summaries of individual papers. More recently, Qazvinian and Radev (2008) argued that citation sentences
(i.e., set of sentences that appear in other papers and cite a given article) are useful in
creating a summary of important contributions of a research paper. Kaplan, Iida, and
Tokunaga (2009) introduced “citation-site” as a block of text that includes a citation and
discusses the cited text. This work used a machine learning method for extracting citations
from research papers and evaluates the result using an annotated corpus of 38 papers citing
4 articles. Moreover, Qazvinian and Radev (2010) showed the usefulness of using implicit
citations (i.e., context sentences, sentences that occur before or after a citation sentence
and do not explicitly cite the target paper, but discuss its contributions) in summary generation. Teufel (2005) argued that citations could contain subjective content, and that this
content can be exploited for summary generation. Additional work (Mohammad et al.,
2009) demonstrated the usefulness of citations for producing multi-document summaries of
scientific articles. Follow-up work indicated that further improvements to citation handling
enables the production of more fluent summaries (Whidby, 2012).
In our work, we develop summarization systems that exploit citations. Specifically,
• We compare and contrast the usefulness of abstracts and of citations in automatically
generating a technical summary on a given topic from multiple research papers. Our
findings suggest that abstracts and citations have some overlapping information but
they also have a significant amount of unique summary-amenable information. Particularly, we provide evidence that citation sentences contain crucial information that
is not available, or hard to extract, from abstracts and papers alone.
• We propose C-LexRank, a graph based summarization system. This method models a
set of citing sentences as a network in which vertices are sentences and edges represent
their lexical similarity. C-LexRank then identifies vertex communities (clusters) in
this network, and selects sentences from different communities to increase diversity in
the summary. Using 30 different sets of citation sentences extracted from 6 different
NLP topics in the ACL1 Anthology Network, we show that C-LexRank is effective
in producing a summary of a paper’s contributions. We compare C-LexRank with a
wide range of state-of-the-art summarization systems that leverage diversity (MMR,
DivRank, MASCS), employ graph structure (DivRank, LexRank), or employ sentence
compression (MASCS) to produce a summary.
• We extend our experiments from summarizing the contributions of a single article to
generating summaries of scientific topics. Our evaluation experiments for extractive
summary generation are applied to a set of 10 papers in the research area of Question
Answering (QA) and another set of 16 papers on Dependency Parsing (DP).
1. Association for Computational Linguistics

166

Generating Extractive Summaries of Scientific Paradigms

We provide some background for this work including the primary features of a technical
summary and also the types of input that are used in our study (full papers, abstracts, and
citation sentences).
1.1 Background
Automatically creating technical extractive summaries is significantly distinct from traditional multi-document summarization. Below we describe the primary characteristics of a
technical extractive summary and we present different types of input texts that we used for
the production of extractive summaries.
1.1.1 Technical Extractive Summaries
In the case of multi-document summarization, the goal is to produce a readable presentation of multiple documents, whereas in the case of technical summary creation, the goal
is to convey the key features and basic underpinnings of a particular field, early and late
developments, important contributions and findings, contradicting positions that may reverse trends or start new sub-fields, and basic definitions and examples that enable rapid
understanding of a field by non-experts.
A prototypical example of a technical summary is that of “chapter notes,” i.e., short
(50–500 word) descriptions of sub-areas found at the end of chapters of textbooks, such as
Jurafsky and Martin’s (2008). One might imagine producing such descriptions automatically, then hand-editing them and refining them for use in an actual textbook.
Previously Mohammad et al. (2009) conducted a human analysis of these chapter notes
and revealed a set of conventions, an outline of which is provided here (with example
sentences in italics):
1. Introductory/opening statement: The earliest computational use of X was in Y, considered by many to be the foundational work in this area.
2. Definitional follow up: X is defined as Y.
3. Elaboration of definition (e.g., with an example): Most early algorithms were based
on Z.
4. Deeper elaboration, e.g., pointing out issues with initial approaches: Unfortunately,
this model seems to be wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction of additional specific instances / historical background with citations:
Two classic approaches are described in Q.
7. References to other summaries: R provides a comprehensive guide to the details behind
X.
The notion of text level categories or zoning of technical papers—related to the summary
components enumerated above—has been investigated previously in the work of Teufel and
Moens (2002) and Nanba, Kando, and Okumura (2000). These earlier works focused on
167

Qazvinian et Al.

the analysis of scientific papers based on their rhetorical structure and on determining the
portions of papers that contain new results, comparisons to earlier work, etc. The work
described here focuses on the synthesis of technical summary based on knowledge gleaned
from rhetorical structure not unlike that of the work of these earlier researchers, but guided
by structural patterns along the lines of the conventions listed above.
Although our current approach to summary creation does not yet incorporate a fully
pattern-based component, our ultimate objective is to apply these patterns to guide the
creation and refinement of the final output. As a first step toward this goal, we use citation
sentences (closest in structure to the patterns identified by convention 7 above) to pick out
the most important content for summary creation.
1.1.2 Scholarly Texts
Published research on a particular topic can be summarized from two different kinds of
sources: (1) where an author describes her own work and (2) where others describe an
author’s work (usually in relation to their own work). The author’s description of her own
work can be found in her paper. How others perceive her work is spread across other papers
that cite her work.
Traditionally, technical summary generation has been tackled by summarizing a set of
research papers pertaining to the topic. However, individual research papers usually come
with manually-created “summaries”—their abstracts. The abstract of a paper may have
sentences that set the context, state the problem statement, mention how the problem is
approached, and the bottom-line results—all in 200 to 500 words. Thus, using only the
abstracts (instead of full papers) as input to a summarization system is worth exploring.
Whereas the abstract of a paper presents what the authors think to be the important
aspects of a paper, the citations to a paper capture what others in the field perceive as
the broader contributions of the paper. The two perspectives are expected to have some
overlap in their content, but the citations also contain additional information not found in
abstracts (Elkiss, Shen, Fader, Erkan, States, & Radev, 2008; Nakov & Hearst, 2012). For
example, authors may describe how a particular methodology from one paper was combined
with another from a different paper to overcome some of the drawbacks of each. Citations
are also indicators of what contributions described in a paper were influential over time.
Another feature that distinguishes citations from abstracts is that citations tend to have
a certain amount of redundant information. This is because multiple papers may describe
the same contributions of a target paper. This redundancy can be exploited by automatic
systems to determine the important contributions of the target paper.
Our goal is to test the hypothesis that an effective technical summary will reflect information on research not only from the perspective of its authors but also from the perspective
of others who use, commend, discredit, or add to it. Before describing our experiments with
technical papers, abstracts, and citations, we first summarize relevant prior work that used
these sources of information as input.
The rest of this paper is organized as follows. After reviewing the related work, we
present an analysis of citations and demonstrate that they contain summary-amenable
information. In the process, we develop C-LexRank, a citation-based summarization system.
In Section 5, we show that state-of-the-art automatic summarization systems create more
168

Generating Extractive Summaries of Scientific Paradigms

contentful summaries of citations of individual documents than those created simply by
random sampling. We also show that C-LexRank performs better than other state-of-theart summarization systems when producing both 100- and 200-word extracts. In Section 6,
we extend our experiments to summarize a set of papers representing the same scientific
topic using the source texts as well as citations to the topic papers. Additionally, we show
the usefulness of citation sentences in automatically generating a technical summary on a
given topic. We observe that, as expected, abstracts are useful in summary creation, but,
notably, we also conclude that citations contain crucial information not present in (or at
least, not easily extractable from) abstracts. We further discover that abstracts are authorbiased and thus complementary to the broader perspective inherent in citation sentences;
these differences enable the use of a range of different levels and types of information in the
summary.

2. Related Work
In this section, we review related prior work in two categories. First, we review previous
research on citation analysis, and then we discuss prior work on capturing diversity in
automatic text summarization.
2.1 Citation Analysis
Previous work has analyzed citation and collaboration networks (Teufel, Siddharthan, &
Tidhar, 2006; Newman, 2001) and scientific article summarization (Teufel & Moens, 2002).
Bradshaw (2002, 2003) benefited from citations to determine the content of articles and
introduce “Reference Directed Indexing” to improve the results of a search engine. Nanba,
Abekawa, Okumura, and Saito (2004) and Nanba et al. (2000) analyzed citation sentences
and automatically categorize citations into three groups using 160 pre-defined phrase-based
rules. This categorization was then used to build a tool to help researchers analyze citations
and write scientific summaries. Nanba and Okumura (1999) also discussed the same citation
categorization to support a system for writing a survey. Nanba and Okumura (1999) and
Nanba et al. (2000) reported that co-citation implies similarity by showing that the textual
similarity of co-cited papers is proportional to the proximity of their citations in the citing
article.
Previous work has shown the importance of the citation sentences in understanding
scientific contributions. Elkiss et al. (2008) performed a large-scale study on citations and
their importance. They conducted several experiments on a set of 2, 497 articles from the
free PubMed Central (PMC) repository2 and 66 from ACM digital library. Results from this
experiment confirmed that the average cosine between sentences in the set of citations to an
article is consistently higher than that of its abstract. They also reported that this number
is much greater than the average cosine between citation sentences and a randomly chosen
document, as well as between citation sentences and the abstract. Finally, they concluded
that the content of citing sentences has much greater uniformity than the content of the
corresponding abstract, implying that citations are more focused and contain additional
information that does not appear in abstracts.
2. http://www.pubmedcentral.gov

169

Qazvinian et Al.

Nakov and Hearst (2012) performed a detailed manual study of citations in the area
of molecular interactions and found that the set of citations to a given target paper cover
most information found in the abstract of that article, as well as 20% more concepts, mainly
related to experimental procedures.
Kupiec, Pedersen, and Chen (1995) used the abstracts of scientific articles as a target
summary. They used 188 Engineering Information summaries that are mostly indicative
in nature. Kan, Klavans, and McKeown (2002) used annotated bibliographies to cover
certain aspects of summarization and suggest guidelines that summaries should also include
metadata and critical document features as well as the prominent content-based features.
Siddharthan and Teufel (2007) described a new reference task and show high human
agreement as well as an improvement in the performance of argumentative zoning (Teufel,
2005). In argumentative zoning—a rhetorical classification task—seven classes (Own, Other,
Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to
their role in the author’s argument.
The problem of automatic related work summarization is addressed by Hoang and Kan
(2010). In their work, Hoang and Kan used a set of keywords representing a hierarchy of
paper topics and assigned a score to each input sentence to construct an extractive summary.
Athar (2011) addressed the problem of identifying positive and negative sentiment polarity in citations to scientific papers. Similarly, Athar and Teufel (2012) used context-enriched
citations to classify scientific sentiment towards a target paper.
2.2 Leveraging Diversity in Summarization
In summarization, a number of previous methods have focused on the diversity of perspectives. Mei, Guo, and Radev (2010) introduced DivRank, a diversity-focused ranking
methodology based on reinforced random walks in information networks. Their random
walk model, which incorporates the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices, showed promising results on the
Document Understanding Conference (DUC) 2004 dataset. DivRank is a state-of-the-art
graph-based method and it leverages the diversity of perspectives in summarization. Therefore, we chose this algorithm as an important baseline in our experiments and we will discuss
it in more detail in Section 4.
A similar ranking algorithm, described by Zhu, Goldberg, Van Gael, and Andrzejewski
(2007), is the Grasshopper ranking model, which leverages an absorbing random walk. This
model starts with a regular time-homogeneous random walk, and in each step the vertex with
the highest weight is set as an absorbing state. Paul, Zhai, and Girju (2010) addressed the
problem of summarizing opinionated text using Comparative LexRank, a random walk model
inspired by LexRank (Erkan & Radev, 2004). Comparative LexRank first assigns different
sentences to clusters based on their contrastiveness with each other. It then modifies the
graph based on cluster information and performs LexRank on the modified cosine similarity
graph.
Perhaps the most well-known summarization method to address diversity in summarization is Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998). This method
is based on a greedy algorithm that selects sentences in each step that are the least similar
170

Generating Extractive Summaries of Scientific Paradigms

to the summary so far. We compare our summarization output with that of MMR and
discuss this algorithm in more details in Section 4.
In prior work on evaluating independent contributions in content generation, Voorhees
(1998) studied IR systems and showed that relevance judgments differ significantly between
humans but relative rankings show high degrees of stability across annotators. In other
work, van Halteren and Teufel (2004) asked 40 Dutch students and 10 NLP researchers
to summarize a BBC news report, resulting in 50 different summaries. They also used 6
DUC-provided summaries, and annotations from 10 student participants and 4 additional
researchers, to create 20 summaries for another news article in the DUC datasets. They
calculated the Kappa statistic (Carletta, 1996; Krippendorff, 1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly
performed in naturally occurring text, without any copy-editing.
The diversity of perspectives and the growth of the factoid inventory (Qazvinian &
Radev, 2011b) also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing
a specific task (Spärck-Jones, 1999) or intrinsic where the quality of the summary itself is
evaluated, regardless of any applied task (van Halteren & Teufel, 2003; Nenkova & Passonneau, 2004). These evaluation methods assess the information content in the summaries
that are generated automatically.

3. Citation-Based Summarization
The ACL Anthology Network3 (AAN) is a manually curated resource built on top of the
ACL Anthology4 (Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, & Tan,
2008). AAN includes all the papers published by ACL and related organizations as well
as the Computational Linguistics journal over a period of four decades. AAN consists of
more than 18, 000 papers from more than 14, 000 authors, each distinguished with a unique
ACL ID, together with their full-texts, abstracts, and citation information. It also includes
other valuable metadata such as author affiliations, citation and collaboration networks, and
various centrality measures (Radev, Muthukrishnan, & Qazvinian, 2009; Joseph & Radev,
2007).
To study citations across different areas within Computational Linguistics, we first extracted six different sets of papers from AAN corresponding to 6 different NLP topics: Dependency Parsing (DP), Phrase-based Machine Translation (PBMT), Text Summarization
(Summ), Question Answering (QA), Textual Entailment (TE), and Conditional Random
Fields (CRF). To build each set, we matched the topic phrase against the title and the content of AAN papers, and extracted the 5 highest cited papers. Table 1 shows the number
of articles and the number of citation sentences in each topic5 . The number of citations in
each set shows that number of sentences that are used as an input to various summarization
systems in our experiments.
3. http://clair.si.umich.edu/anthology/
4. http://www.aclweb.org/anthology-new/
5. The number of incoming citations are from AAN’s 2008 release.

171

CRF

TE

QA

Summ

PBMT

DP

Qazvinian et Al.

ACL ID
C96-1058
P97-1003
P99-1065
P05-1013
P05-1012
N03-1017
W03-0301
J04-4002
N04-1033
P05-1033
A00-1043
A00-2024
C00-1072
W00-0403
W03-0510
A00-1023
W00-0603
P02-1006
D03-1017
P03-1001
D04-9907
H05-1047
H05-1079
W05-1203
P05-1014
N03-1023
N04-1042
W05-0622
P06-1009
W06-1655

Title
Three New Probabilistic Models For Dependency Parsing ...
Three Generative, Lexicalized Models For Statistical Parsing
A Statistical Parser For Czech
Pseudo-Projective Dependency Parsing
On-line Large-Margin Training Of Dependency Parsers
Statistical Phrase-Based Translation
An Evaluation Exercise For Word Alignment
The Alignment Template Approach To Statistical Machine Translation
Improvements In Phrase-Based Statistical Machine Translation
A Hierarchical Phrase-Based Model For Statistical Machine Translation
Sentence Reduction For Automatic Text Summarization
Cut And Paste Based Text Summarization
The Automated Acquisition Of Topic Signatures ...
Centroid-Based Summarization Of Multiple Documents ...
The Potential And Limitations Of Automatic Sentence Extraction ...
A Question Answering System Supported By Information Extraction
A Rule-Based Question Answering System For Reading ...
Learning Surface Text Patterns For A Question Answering System
Towards Answering Opinion Questions: Separating Facts From Opinions ...
Offline Strategies For Online Question Answering ...
Scaling Web-Based Acquisition Of Entailment Relations
A Semantic Approach To Recognizing Textual Entailment
Recognising Textual Entailment With Logical Inference
Measuring The Semantic Similarity Of Texts
The Distributional Inclusion Hypotheses And Lexical Entailment
Weekly Supervised Natural Language Learning ...
Accurate Information Extraction from Research Papers ...
Semantic Role Labelling with Tree CRFs
Discriminative Word Alignment with Conditional Random Fields
A Hybrid Markov/Semi-Markov CRF for Sentence Segmentation

Year
1996
1997
1999
2005
2005
2003
2003
2004
2004
2005
2000
2000
2000
2000
2003
2000
2002
2002
2003
2003
2004
2005
2005
2005
2005
2003
2004
2005
2006
2006

# citations
66
50
54
40
71
172
11
49
23
65
19
20
19
28
14
13
19
72
39
27
12
7
9
17
10
29
24
9
33
20

Table 1: Papers were extracted from 6 different NLP topics in AAN: Dependency Parsing
(DP), Phrase-based Machine Translation (PBMT), Text Summarization (Summ),
Question Answering (QA), Textual Entailment (TE), and Conditional Random
Fields (CRF). Each set consists of the 5 highest cited papers in AAN’s 2008 release
whose title and content matched the corresponding topic phrase.

172

Generating Extractive Summaries of Scientific Paradigms

Below we describe our approach to citation analysis, including our calculation of interjudge agreement. We then describe our C-LexRank method for extracting citation sentences.
3.1 Citation Analysis
To analyze citations, we designed an annotation task that requires explicit definitions that
distinguish between phrases that represent the same or different information units. Unfortunately, there is little consensus in the literature on such definitions. Therefore, following van
Halteren and Teufel (2003), Qazvinian and Radev (2011b) we made the following distinction. We define a nugget to be a phrasal information unit (i.e., any phrase that would
contain some information about the contributions of the cited paper). Different nuggets
may all represent the same atomic semantic unit, which we refer to as a factoid. In the
context of citations, a factoid refers to a unique contribution of a target paper mentioned
in a citation sentence. For example, the following set of citations to Eisner’s (1996) famous parsing paper illustrate the set of factoids about this paper and suggest that different
authors who cite a particular paper may discuss different contributions (factoids) of that
paper.
In the context of DPs, this edge based factorization method was proposed by Eisner (1996).
Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge
factorization of trees.
Eisner (1996) proposed an O(n3 ) parsing algorithm for PDG.
If the parse has to be projective, Eisner’s (1996) bottom-up-span algorithm can be used
for the search.
This example also suggests that different authors use different wordings (nuggets) to
represent the same factoids. For instance, cubic parsing and O(n3 ) parsing algorithm are
two nuggets that represent the same factoid about (Eisner, 1996). A similar example, which
we will use throughout the paper, is the paper by Cohn and Blunsom (2005) (identified with
the ACL ID W05-0622 in Table 1). This paper is cited in 9 different sentences within AAN.
All of these sentences are listed in Table 2. In each sentence, the nuggets extracted by the
annotators are underlined. As this table suggests, a citation sentence may not discuss any
of the contributions of the cited paper. For instance, the last sentence does not contain
any factoids about Cohn and Blunsom’s (2005) work. The nuggets that are identified using
the citation to the paper (Cohn & Blunsom, 2005) account for a total number of 3 factoids
(contributions) identified for this paper: f1 , tree structures; f2 , semantic role labeling; and
f3 , a pipelined approach.
Following these examples, we asked two annotators with background in Natural Language Processing to review each citing sentence and extract a list of phrases that represent
a contribution of the cited paper.6 Moreover, to ensure that the extracted nuggets are
explicitly mentioned in the citations, we asked the annotators to rely merely on the set
of citations to do the task and not on their background on this topic or the source of the
6. One of the annotators is an author of this paper.

173

Qazvinian et Al.

cited paper. Finally, we reviewed each list and collapsed phrases that represent the same
contribution (factoid).
Finding agreement between annotated well-defined nuggets is straightforward and can
be calculated in terms of Kappa. However, when nuggets themselves are to be extracted
by annotators, the task becomes less obvious. To calculate the agreement, we annotated
5 randomly selected citation sets twice (1 paper from each of the NLP areas in Table 1),
and designed a simple evaluation scheme based on Kappa. For each n-gram, w, in a given
citation sentence, we determine w is part of any nugget in either human annotations. If w
occurs in both or neither, then the two annotators agree on it, and otherwise they do not.
Based on this agreement setup, we can formalize the κ statistic as:
κ=

Pr(a) − Pr(e)
1 − Pr(e)

(1)

where P r(a) is the relative observed agreement among annotators, and P r(e) is the probability that annotators agree by chance if each annotator is randomly assigning categories.
Table 3 shows the unigram, bigram, and trigram-based κ between the two human annotators (Human1, Human2) in the five datasets that were annotated twice. These results
suggest that human annotators can reach substantial agreement when trigram nuggets are
examined, and have reasonable agreement for unigram and bigram nuggets.
3.2 C-LexRank
In this section we describe C-LexRank as a method to extract citing sentences that cover
a diverse set of factoids. Our method works by modeling the set of citations as a network
of sentences and identifying communities of sentences that cover similar factoids. Once a
good division of sentences is made, we extract salient sentences from different communities.
Figure 1 illustrates a representative example that depicts C-LexRank’s process.
3.2.1 Citation Summary Network
In the first step (as shown in Figure 1 (a)), we model the set of sentences that cite a specific
paper with a network in which vertices represent citing sentences and undirected weighted
edges show the degree of semantic relatedness between vertex pairs, normally quantified
by a similarity measure. We refer to this network as the Citation Summary Network of
an article. The similarity function should ideally assign high scores to sentence pairs that
have the same factoids, and should assign low scores to sentences that talk about different
contributions of the target paper.
Previously, Qazvinian and Radev (2008) examined 7 different similarity measures including TF-IDF with various IDF databases, longest common sub-sequence, generation
probability (Erkan, 2006), and the Levenstein distance on a training set of citations. They
showed that the cosine similarity measure that employs TF-IDF vectors assigns higher similarities to pairs that contain the same factoids. Following Qazvinian and Radev (2008), we
use the cosine similarity between TF-IDF vector models that employ a general IDF corpus7
to construct the citation summary network of each article.
7. We use the IDF corpus in the Mead summarization system (Radev et al., 2004), which is generated using
the English portion of the Hong Kong News parallel corpus (Ma, 2000).

174

Generating Extractive Summaries of Scientific Paradigms

1

factoid
f1

Citation Sentence
Our parsing model is based on a conditional random field model, however,
unlike previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et
al., 2006), we do not assume a particular tree structure, and instead find the
most likely structure and labeling.

2

f3

Some researchers (Xue & Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005, 2008) used a
pipelined approach to attack the task.

3

f2

They have been used for tree labelling, in XML tree labelling (Jousse et al.,
2006) and semantic role labelling tasks (Cohn & Blunsom, 2005).

4

f1

Finally, probabilistic models have also been applied to produce the structured output, for example, generative models (Thompson, Levy, & Manning,
2003), sequence tagging with classifiers (Màrquez et al., 2005; Pradhan et al.,
2005), and Conditional Random Fields on tree structures (Cohn & Blunsom
2005).

5

f3

As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument
classification, global inference, etc., and conquering them individually (Xue &
Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom, 2005; Punyakanok
et al., 2008; Toutanova et al., 2005, 2008).

6

f1 , f2

Although T-CRFs are relatively new models, they have already been applied to
several NLP tasks, such as semantic role labeling, semantic annotation, word
sense disambiguation, image modeling (Cohn & Blunsom, 2005; Tang et
al., 2006; Jun et al., 2009; Awasthi et al., 2007).

7

f2

The model can be used for tasks like syntactic parsing (Finkel et al., 2008)
and semantic role labeling (Cohn & Blunsom, 2005).

8

f1

Regarding novel learning paradigms not applied in previous shared tasks, we
find Relevant Vector Machine (RVM), which is a kernel-based linear discriminant inside the framework of Sparse Bayesian Learning (Johansson & Nugues,
2005) and Tree Conditional Random Fields (T-CRF) (Cohn & Blunsom,
2005), that extend the sequential CRF model to tree structures.

9

N/A

We use CRFs as our models for both tasks (Cohn & Blunsom, 2005).

Table 2: The AAN paper W05-0622 on CRF by Cohn & Blunsom (2005) is cited in nine
different AAN sentences. In each citation sentence, the nuggets extracted by the
annotators are underlined.

175

Qazvinian et Al.

Average κ
unigram bigram
Human1 vs. Human2
A00-1023
1.000
0.615
H05-1079
0.889
0.667
P05-1013
0.427
0.825
W03-0301
0.455
0.636
W05-0622
0.778
0.667
Average
0.710
0.682

trigram
0.923
0.556
0.975
0.818
0.778
0.810

Table 3: Agreement between different annotators in terms of Kappa in 5 citation sets.

(a) Citation summary network

(b) Community structure

(c) C-LexRank output

Figure 1: The C-LexRank method extracts citing sentences that cover a diverse set of factoids. The citation summary network in (a) models the set of sentences that cite
a specific paper, where vertices represent citing sentences and (weighted) edges
show the degree of semantic relatedness between vertex pairs. The community
structure in (b) corresponds to clustered sets of representative sentences extracted
from citation sentences. The C-LexRank output in (c) corresponds to the candidate sentences from different clusters that are used for building a summary.

3.2.2 Community Structure
In the second step (as shown in Figure 1 (b)), we extract vertex communities from the
citation summary network to generate summaries. We generate summaries by extracting
representative sentences from the citation summary network. Intuitively, a good summary
should include sentences that represent different contributions of a paper. Therefore, a good
sentence selection from the citation summary network will include vertices that are similar
to many other vertices and which are not very similar to each other. On the other hand,
a bad selection would include sentences that are only representing a small set of vertices
in the graph. This is very similar to the concept of maximizing social influence in social
networks (Kempe, Kleinberg, & Éva Tardos, 2003). Figure 2 shows an example in which the
selected two vertices in the citation summary networks represent a small subset of vertices
(left) and a larger subset of vertices (right). In our work we try to select vertices that
176

Generating Extractive Summaries of Scientific Paradigms

maximize the size of the set of vertices that they represent. We achieve this by detecting
different vertex communities in the citation summary network.

(a) Bad sentence selection

(b) Good sentence selection

Figure 2: Summaries are produced by using vertex coverage to select a set of representative
vertices corresponding to sentences. Selecting two similar vertices will cause the
summary to cover fewer contributions of the target paper in (a), while selecting
less similar vertices as the summary will increase the coverage of the summary
(b).

In order to find vertex communities and thus a good sentence selection, we exploit the
small-world property of citation summary networks. A network is called small-world, if
most of its vertices are not neighbors of each other, but can be reached from one another
by a small number of steps (Watts & Strogatz, 1998). Recent research has shown that a
wide range of natural graphs such as biological networks (Ravasz, Somera, Mongru, Oltvai,
& Barabási, 2002), food webs (Montoya & Solé, 2002), brain neurons (Bassett & Bullmore,
2006) and human languages (Ferrer i Cancho & Solé, 2001) exhibit the small-world property.
This common characteristic can be detected using two basic statistical properties: the
clustering coefficient C, and the average shortest path length `. The clustering coefficient
of a graph measures the number of closed triangles in the graph. It describes how likely
it is that two neighbors of a vertex are connected (Newman, 2003). Watts and Strogatz
(1998) define the clustering coefficient as the average of the local clustering values for each
vertex.
Pn
ci
C = i=1
(2)
n
The local clustering coefficient ci for the ith vertex is the number of triangles connected
to vertex i divided by the total possible number of triangles connected to vertex i. Watts
and Strogatz (1998) show that small-world networks are highly clustered and obtain relatively short paths (i.e., ` is small). Previous work (Qazvinian & Radev, 2011a) shows that
citation summary networks are highly clustered. These networks have small shortest paths
and obtain clustering coefficient values that are significantly larger than random networks.
Moreover, Qazvinian and Radev suggest that this is because of a community structure,
177

Qazvinian et Al.

{8} T-CRF
{8} T-CRF

{4} tree structures
{3} semantic role labeling

{4} tree structures

{1} TreeCRF
{9}

{7} semantic role labeling

{6} semantic role labeling; T-CRF

{2} pipelined approach
{6} semantic role labeling; T-CRF

{7} semantic role labeling
{3} semantic role labeling
{9}

{5} pipelined approach
{1} TreeCRF

{5} pipelined approach
{2} pipelined approach

Pajek

(a) Citation summary network

Pajek

(b) Latent community structure

{8} T-CRF

{8} T-CRF

{4} tree structures

{4} tree structures

{1} TreeCRF

{1} TreeCRF

{6} semantic role labeling; T-CRF

{6} semantic role labeling; T-CRF

{7} semantic role labeling
{3} semantic role labeling

{7} semantic role labeling
{3} semantic role labeling

{9}

{5} pipelined approach
{2} pipelined approach

{9}

{5} pipelined approach
{2} pipelined approach

Pajek

(c) Clustering output

Pajek

(d) C-LexRank ranking

Figure 3: The C-LexRank algorithm operates as follows on Cohn and Blunsom’s (2005)
citation summary network: In the network (a), the vertices are citation sentences
(annotated with their nuggets from Table 2), and each edge is the cosine similarity between the corresponding node pairs. (b) shows that the network has an
underlying structure which is captured by C-LexRank in (c). Finally, (d) shows
the C-LexRank output where vertex size is proportional to its LexRank value
within the cluster.

where each community is composed of a set of highly connected vertices with a small number
of edges that fall between communities.
Figure 3 (a) illustrates a real citation summary network built using the citation sentences
in Table 2 in which each vertex is labeled with its corresponding nugget. With some re178

Generating Extractive Summaries of Scientific Paradigms

arrangement of the vertices in Figure 3 (b), it becomes clear that the citation summary
network of this paper has an underlying community structure in which sentences that cover
similar factoids are closer to each other and form communities. For instance, in this network
there are at least 3 observable communities: one that is about “f1 : tree structure,” one
about “f2 : semantic role labeling” and the last one about the “f3 : pipelined approach” as
proposed by Cohn and Blunsom (2005).
In order to detect these communities automatically we use modularity. Modularity,
(Newman, 2004a), is a measure to evaluate the divisions that a community detection algorithm generates. For a division with g groups, they define matrix eg×g whose component
eij is the fraction of edges in the original network that connect vertices in components i, j.
Then the modularity Q can be defined as:
X
X
Q=
eii −
eij eki
(3)
i

ijk

Intuitively, Q is the fraction of all the edges that are embedded within communities minus
the expected value of the same quantity in a network with the same degrees but in which
edges are placed at random regardless of the community structure. Newman and Girvan
(2004) and Newman (2004b) showed across a wide range of simulated and real-world networks that larger Q values are correlated with better graph clusterings. It is also shown
by Newman (2004b) that if no edges exist that connect vertices across different clusters
then Q = 1, and conversely if the number of inter-cluster edges is no better than random
then Q = 0. Other work (Smyth & White, 2005) showed empirically that modularity works
well in practice in terms of both (a) finding good clusterings of vertices in networks where
community structure is evident, and (b) indicating what the appropriate number of clusters
k is for such a graph.
C-LexRank uses the clustering algorithm of Clauset, Newman, and Moore (2004), which
exploits modularity to detect vertex communities in a network. This network clustering
method, as discussed by Clauset et al. (2004) is a hierarchical agglomeration algorithm,
which works by greedily optimizing the modularity in a linear running time for sparse
graphs. More particularly, their method continuously merges vertex or cluster pairs with the
highest similarity and stops when modularity reaches the maximum value. This clustering
algorithm is efficient (O(n log2 n) in the number of nodes, n) and does not require a predetermined number of clusters. These two characteristics makes this community detection
algorithm particularly useful.
Figure 3 (c) shows how the clustering algorithm detects factoid communities in Cohn and
Blunsom’s (2005) citation summary network. In this figure, we have color-coded vertices
based on their community. The clustering algorithm assigns sentences 1, 4 and 8 (which
are all about the tree structures) to one cluster; sentences 3, 6 and 7 (which are all about
semantic role labeling) to another cluster; and finally assigns sentences 2, 5 and 9 (sentences
2 and 5 are both about pipelined approach) to the last cluster. This figure also shows that
sentence 6, which discusses two factoids (“semantic role labeling” and “T-CRF”) connects
the two vertex communities (corresponding to 2 factoids) as a bridge.
To evaluate how well the clustering method works in all of our datasets, we calculated
both the purity and the normalized mutual information (NMI) for the divisions in each
citation set, extracted using the community detection algorithm. Purity (Zhao & Karypis,
179

Qazvinian et Al.

2001) is a method in which each cluster is assigned to the class with the majority vote in
the cluster, and the accuracy of this assignment is then measured by dividing the number
of correctly assigned documents by N . More formally:
purity(Ω, C) =

1 X
max |ωk ∩ cj |
j
N

(4)

k

where Ω = {ω1 , ω2 , . . . , ωK } is the set of clusters and C = {c1 , c2 , . . . , cJ } is the set of
classes. ωk is interpreted as the set of documents in the cluster ωk and cj as the set of
documents in the class cj .
We also calculate the normalized mutual information (NMI). Manning, Raghavan, and
Schütze (2008) describe NMI as follows. Let us assume Ω = {ω1 , ω2 , . . . , ωK } is the set of
clusters and C = {c1 , c2 , . . . , cJ } is the set of classes. Then,
NMI(Ω, C) =

I(Ω; C)
[H(Ω) + H(C)]/2

(5)

where I(Ω; C) is the mutual information:
I(Ω, C) =

XX
k

=

P (ωk ∩ cj ) log

j

X X |ωk ∩ cj |
k

N

j

log

P (ωk ∩ cj )
P (ωk )P (cj )

N |ωk ∩ cj |
|ωk ||cj |

(6)
(7)

in which P (ωk ), P (cj ), and P (ωk ∩ cj ) are the probabilities of a document being in cluster
ωk , class cj , and in the intersection of ωk and cj , respectively; and H is entropy:
X
H(Ω) = −
P (ωk ) log P (ωk )
(8)
k

= −

X |ωk |
k

N

log

|ωk |
N

(9)

I(Ω; C) in Equation 6 measures the amount of information that we would lose about
the classes without the cluster assignments. The normalization factor ([H(Ω) + H(C)]/2)
in Equation 5 enables us to trade off the quality of the clustering against the number of
clusters, since entropy tends to increase with the number of clusters. For example, H(Ω)
reaches its maximum when each document is assigned to a separate cluster. Because NMI is
normalized, we can use it to compare cluster assignments with different numbers of clusters.
Moreover, [H(Ω) + H(C)]/2 is a tight upper bound for I(Ω; C), making NMI obtain values
between 0 and 1. Table 4 lists the average Purity and NMI across the papers in our collected
dataset, along with the analogous numbers for a division of the same size where vertices are
randomly assigned to clusters.
3.2.3 Ranking
The third step of the C-LexRank process (as shown in Figure 1 (c)) is applied after the
graph is clustered and the communities are formed. To produce the C-LexRank output,
180

Generating Extractive Summaries of Scientific Paradigms

purity(Ω, C)
purity(Ωrandom , C)
NMI(Ω, C)
NMI(Ωrandom , C)

average
0.461
0.389
0.312
0.182

95% Confidence Interval
[0.398, 0.524]
[0.334, 0.445]
[0.251, 0.373]
[0.143, 0.221]

Table 4: The average purity (in boldface) and normalized mutual information (NMI) values
are shown for the papers in our collected dataset, along with analogous values for
a division of the same size where vertices are randomly assigned to clusters.

we extract sentences from different clusters to build a summary. We start with the largest
cluster and extract sentences using LexRank (Erkan & Radev, 2004) within each cluster. In
other words, for each cluster Ωi we made a lexical network of the sentences in that cluster
(Ni ). Using LexRank we can find the most central sentences in Ni as salient sentences of Ωi
to include in the main summary. We choose, for each cluster Ωi , the most salient sentence
of Ωi , and if we have not reached the summary length limit, we do that for the second most
salient sentences of each cluster, and so on. The cluster selection is in order of decreasing
size. Figure 3 (d) shows Cohn and Blunsom’s (2005) citation summary network, in which
each vertex is plotted with a size proportional to its LexRank value within its cluster. This
figure shows how C-LexRank emphasizes on selecting a diverse set of sentences covering a
diverse set of factoids.
Previously, we mentioned that factoids with higher weights appear in a greater number
of sentences, and clustering aims to cluster such fact-sharing sentences in the same communities. Thus, starting with the largest community is important to ensure that the system
summary first covers the factoids that are more frequently mentioned in other citation
sentences and thus are more important.
The last sentence in the example in Table 2 is as follows. “We use CRFs as our models
for both tasks (Cohn & Blunsom, 2005).” This sentence shows that a citation may not
cover any contributions of the target paper. Such sentences are assigned by the community
detection algorithm in C-LexRank to clusters to which they are semantically most similar.
The intuition behind employing LexRank within each cluster is to try to avoid extracting
such sentences for the summary, since LexRank within a cluster enforces extracting the most
central sentence in that cluster. In order to verify this, we also try a variant of C-LexRank
in which we do not select sentences from clusters based on their salience in the cluster, but
rather in a round-robin fashion, in which all the sentences within a cluster are equally likely
to be selected. We call this variant C-RR.
Table 5 shows the 100-word summary constructed using C-LexRank for our exemplar
paper, in which different nuggets are illustrated in bold. This summary is a perfect summary
in terms of covering the different factoids about the paper. It includes citing sentences that
talk about “tree CRF,” “pipelined approach,” and “Semantic Role Labeling,” which are
indeed Cohn and Blunsom’s (2005) three main contributions.
181

Qazvinian et Al.

Our parsing model is based on a conditional random field model, however, unlike
previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et al., 2006), we do
not assume a particular tree structure, and instead find the most likely structure
and labeling.
Some researchers (Xue & Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom,
2005; Punyakanok et al., 2008; Toutanova et al., 2005, 2008) used a pipelined
approach to attack the task.
The model can be used for tasks like syntactic parsing (Finkel et al., 2008) and
Semantic Role Labeling (Cohn & Blunsom, 2005).
Table 5: This 100-word summary was constructed using C-LexRank for Cohn and Blunsom’s (2005) citation summary network. Factoids are shown in bold face.

4. Other Methods
In our experiments in Section 5 we compare C-LexRank to a number of other summarization
systems. We compare C-LexRank with random summarization to find a lower-bound on
the pyramid scores in our experiments. We use LexRank and C-RR as two variants of CLexRank to investigate the usefulness of community detection and salient vertex selection in
C-LexRank. We evaluate DivRank as a state of the art graph-based summarization system
that leverages diversity as well as MMR as a widely used diversity-based summarization
system. Finally, we use Multiple Alternate Sentence Compression Summarizer (MASCS) as
a system that does not rely merely on extraction, but rather produces a list of candidates
by applying pre-defined sentence-compression rules.
4.1 Random
This method simply chooses citations in random order without replacement. Since a citing
sentence may cover no information about the cited paper (as in the last sentence in Table 2),
randomization has the drawback of selecting citations that have no valuable information
in them. Moreover, the random selection procedure is more prone to produce redundant
summaries as citing sentences that discuss the same factoid may be selected.
4.2 LexRank
One of the systems that we compare C-LexRank with is LexRank (Erkan & Radev, 2004).
It works by first building a graph of all the documents (di ) in a cluster. The edges between
corresponding vertices (di ) represent the cosine similarity between them if the cosine value
is above a threshold (0.10 following Erkan & Radev, 2004). Once the network is built, the
system finds the most central sentences by performing a random walk on the graph.

p(dj ) = (1 − λ)

X
1
+λ
p(di )P (di → dj )
|D|
di

182

(10)

Generating Extractive Summaries of Scientific Paradigms

Equation 10 shows that the probability that the random walker would visit dj depends
on a random jump element as well as the probability that the random walk visits each of
its neighbors (di ) times the transition probability between di and dj , P (di → dj ).
Comparing C-LexRank summaries with the ones from LexRank gives insight into how
we can benefit from detecting communities in citation sets. Essentially, C-LexRank is the
same as LexRank if all the vertices are assigned to the same cluster. By construction,
C-LexRank should produce more diverse summaries covering different perspectives by capturing communities of sentences that discuss the same factoids.
4.3 MMR
Maximal Marginal Relevance (MMR) is proposed by Carbonell and Goldstein (1998) and
is a widely used algorithm in generating summaries that reflect the diversity of perspectives
in the source documents (Das & Martins, 2007). MMR uses the pairwise cosine similarity
matrix and greedily chooses sentences that are the least similar to those already in the
summary. In particular,
h
i
M M R = argminDi ∈D−A max Sim(Di , Dj )
Dj ∈A

(11)

where A is the set of documents in the summary, initialized to A = ∅. In Equation 11,
a sentence Di that is not in the summary A is chosen such that its highest similarity to
summary sentences maxDj ∈A Sim(Di , Dj ) is minimum among all unselected sentences.
4.4 DivRank
We also compare C-LexRank with a state-of-the-art graph-based summarization system
that leverages diversity, DivRank. DivRank is based on calculating stationary distribution
of vertices using a modified random walk model. Unlike other time-homogeneous random
walks (e.g., PageRank), DivRank does not assume that the transition probabilities remain
constant over time.
DivRank uses a vertex-reinforced random walk model to rank graph vertices based on
a diversity based centrality. The basic assumption in DivRank is that the transition probability from a vertex to other is reinforced by the number of previous visits to the target
vertex (Mei et al., 2010). Particularly, let us assume pT (u, v) is the transition probability
from any vertex u to vertex v at time T . Then,
pT (di , dj ) = (1 − λ).p∗ (dj ) + λ.

p0 (di , dj ).NT (dj )
DT (di )

(12)

where NT (dj ) is the number of times the walk has visited dj up to time T and,
DT (di ) =

X

p0 (di , dj )NT (dj )

(13)

dj ∈V

Here, p∗ (dj ) is the prior distribution that determines the preference of visiting vertex
dj , and p0 (u, v) is the transition probability from u to v prior to any reinforcement. Mei et
al. argue that the random walk could stay at the current state at each time, and therefore
183

Qazvinian et Al.

assumes a hidden link from each vertex to itself, thus defining p0 (u, v) as:
(
α. w(u,v)
if u 6= v
deg(u)
p0 (u, v) =
1−α
if u = v

(14)

Here, we try two variants of this algorithm: DivRank, in which p∗ (dj ) is uniform, and
DivRank with priors in which p∗ (dj ) ∝ l(Dj )−β , where l(Dj ) is the number of the words
in the document Dj and β is a parameter (0.1 in our experiments). This prior distribution
assigns larger probabilities to shorter sentences which will increase their likelihood of being
salient. This will cause more sentences to be included in the summary, and might increase
the factoid coverage. In our experiments, we follow Mei et al. (2010) and set λ = 0.90 and
α = 0.25.
4.5 MASCS
The last summarization system that we use as a baseline is Multiple Alternate Sentence
Compression Summarizer (MASCS) (Zajic, Dorr, Lin, & Schwartz, 2007). Similar to previous previous baseline systems, MASCS’s goal is to leverage diversity in summarization.
MASCS performs preprocessing on sentences that transforms them into new sentences,
thus expanding the pool of candidates available for inclusion in a summary beyond the
set of sentences that occur in source documents. This is what makes MASCS somewhat
non-extractive. In addition, the preprocessing used in MASCS for these experiments was
specifically adapted to the genre of citation sentences from scientific papers (Whidby, 2012).
More particularly, MASCS is a summarization system that utilizes Trimmer’s (Zajic
et al., 2007) sentence compression candidates to create summaries for a single or set of
documents. Summarization with MASCS is performed in three stages. In the first stage,
MASCS generates several compressed sentence candidates for every sentence in a document
from the cluster. The second stage involves calculating various ranking features for each of
the compressed sentence candidates. In the final stage, sentence candidates are chosen for
inclusion in the summary, and are chosen based on a linear combination of features.
Trimmer can leverage the output of any constituency parser that uses the Penn Treebank
conventions. At present, the Stanford Parser (Klein & Manning, 2003) is used. The set of
compressions is ranked according to a set of features that may include metadata about the
source sentences, details of the compression process that generated the compression, and
externally calculated features of the compression.
Summaries are constructed by iteratively adding compressed sentences from the candidate pool until a length threshold is met. Candidates are chosen by an implementation of
MMR that uses features directly calculated from a candidate, metadata about a candidate’s
source and its compression history, relevance of the candidate to the topic and redundancy
of the candidate to already selected candidates. The redundancy score in MASCS uses an
index of the words (w) in the document set:
X

log(λ.P (w|summary) + (1 − λ).P (w|corpus))

w

where λ is a weighting factor (set to 0.3 in our experiments).
184

(15)

Generating Extractive Summaries of Scientific Paradigms

5. Experiments
We used the 30 sets of citations listed in Table 1 and employ C-LexRank to produce two
extractive summaries with different summary lengths (100 and 200 words) for each set.
In addition to C-LexRank and C-RR, we also performed the same experiments with the
baseline methods described in Section 4, most of which are aimed at leveraging diversity in
summarization.
5.1 Evaluation
To evaluate our system, we use the pyramid evaluation method (Nenkova & Passonneau,
2004). Each factoid in the citations to a paper corresponds to a summary content unit
(SCU) in (Nenkova & Passonneau, 2004).
The score given by the pyramid method for a summary is the ratio of the sum of the
weights of its factoids to the sum of the weights of an optimal summary. This score ranges
from 0 to 1, and high scores show the summary content contains more heavily weighted
factoids. If a factoid appears in more citation sentences than another factoid, it is more
important, and thus should be assigned a higher weight. To weight the factoids we build a
pyramid, and each factoid falls in a tier. Each tier shows the number of sentences a factoid
appears in. Thus, the number of tiers in the pyramid is equal to the citation summary size.
If a factoid appears in more sentences, it falls in a higher tier. So, if the factoid fi appears
|fi | times in the citation summary it is assigned to the tier T|fi | .
The pyramid score formula that we use is computed as follows. Suppose the pyramid
has n tiers, Ti , where tier Tn on top and T1 on the bottom. The weight of the factoids in
tier Ti will be i (i.e. they appeared in i sentences). If |Ti | denotes the number of factoids in
tier Ti , and Di is the number of factoids in the summary that appear in Ti , then the total
factoid weight for the summary is as follows.
D=

n
X

i × Di

(16)

i=1

Additionally, the optimal pyramid score for a summary with X factoids, is
M ax =

n
X

i × |Ti | + j × (X −

i=j+1

n
X

|Ti |)

(17)

i=j+1

P
where j = maxi ( nt=i |Tt | ≥ X). Subsequently, the pyramid score for a summary is calculated as follows.
D
P =
(18)
M ax
5.2 Results and Discussion
Table 6 shows the average pyramid score of the summaries generated using different methods
with different lengths. Longer summaries result in higher pyramid scores since the amount
of information they cover is greater than shorter summaries. For the random sentence
extraction baseline, we repeat the experiment with 100 different randomly generated seed
values and report the average pyramid score of these summaries in Table 6. This table shows
185

Qazvinian et Al.

Method
Random
MMR
LexRank
DivRank
DivRank (with priors)
MASCS
C-RR
C-LexRank
C.I.=Confidence Interval

Length: 100 words
pyramid
95% C.I.
0.535
[0.526,0.645]
0.600
[0.501,0.699]
0.604
[0.511,0.696]
0.644
[0.580,0.709]
0.632
[0.545,0.719]
0.571
[0.477,0.665]
0.513
[0.436,0.591]
0.647
[0.565,0.730]

Length: 200 words
pyramid
95% C.I.
0.748
[0.740,0.755]
0.761
[0.685,0.838]
0.784
[0.725,0.844]
0.769
[0.704,0.834]
0.778
[0.716,0.841]
0.772
[0.705,0.840]
0.755
[0.678,0.832]
0.799
[0.732,0.866]

Table 6: Average pyramid scores are shown for two different summary lengths (100 words
and 200 words) for eight different methods, including a summary generator based
on random citation sentence selection. C-LexRank outperforms all other methods
that leverage diversity as well as random summaries and LexRank. Highest scores
for each input source are shown in bold.

that C-LexRank outperforms all other methods that leverage diversity as well as random
summaries and LexRank. The results in this table also suggest that employing LexRank
within each cluster is essential for the selection of salient citing sentences, as the average
pyramid scores from C-RR, where sentences are picked in a round-robin fashion, are lower.
5.2.1 Effect of Community Detection
The community detection that C-LexRank employs assigns highly similar citing sentences
to the same cluster. This enables C-LexRank to produce a diverse summary by selecting
sentences from different clusters. This selection is done by assigning a score to each vertex
using LexRank within the cluster. The modularity-based clustering method described in
Section 3.2.2, which works by maximizing modularity in a clustering, will always produce at
least 2 clusters. Intuitively, in a network in which all the vertices are assigned to the same
community, the fraction of edges that are embedded within that community is equal to the
expected value of the same quantity in a network in which edges are placed at random.
This will make Q obtain its lower-bound, Q = 0.
However, in the hypothetical case where all the vertices belong to the same cluster, CLexRank will be the same as LexRank (i.e., it will perform LexRank on the entire network).
Therefore, comparing C-LexRank and LexRank helps us understand the effect of clustering
on summary quality. Table 6 shows that C-LexRank produces summaries that obtain
higher pyramid scores at both 100 words and 200 words. Table 7 shows the 100-word
summary that was constructed using LexRank for Cohn and Blunsom’s (2005) citations.
This summary, unlike the one produced by C-LexRank (Table 5), does not cover all of
the factoids of the target paper (e.g., “pipelined approach”). Moreover, this summary has
redundant information (e.g., TreeCRF vs. T-CRF) and contains a citation sentence that
does not cover any of the contributions of Cohn and Blunsom.
186

Generating Extractive Summaries of Scientific Paradigms

The model can be used for tasks like syntactic parsing (Finkel et al., 2008) and
semantic role labeling (Cohn & Blunsom, 2005).
We use CRFs as our models for both tasks (Cohn & Blunsom, 2005).
Our parsing model is based on a conditional random field model, however, unlike
previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et al., 2006), we do
not assume a particular tree structure, and instead find the most likely structure
and labeling.
Although T-CRFs are relatively new models, they have already been applied to
several NLP tasks, such as semantic role labeling, semantic annotation, word
sense disambiguation, image modeling (Cohn & Blunsom, 2005; Tang et al., 2006;
Jun et al., 2009; Awasthi et al., 2007).
Table 7: The summary constructed using LexRank and Cohn and Blunsom’s (2005) citation
sentences. Compared to the C-LexRank summary (in Table 5), LexRank does not
produce a summary of all Cohn and Blunsom’s (2005) contributions (The summary
is not truncated for clarity).

5.2.2 Salient Vertex Extraction
Selecting representative sentences (vertices) from different clusters is done using LexRank
in the C-LexRank algorithm. More particularly, for each cluster, C-LexRank first extracts
a subgraph of the network representing vertices and edges in that cluster, and then employs
LexRank to assign a salience score to each vertex. An alternative idea could be selecting
sentences from clusters at random (C-RR). In C-RR, we traverse between clusters in a
round-robin fashion and randomly select a previously unselected sentence from the cluster
to include a summary.
Comparing C-LexRank with C-RR enables us to understand the effect of salience selection within communities. Selecting vertices that are not a good representative of the cluster
may result in picking sentences that do not cover contributions of the target paper (e.g.,
sentence 9 from Table 2 – vertex 9 in Figure 3 (d)). In fact, Table 6 shows that C-LexRank
produces summaries with relatively 20% and 5% higher pyramid scores than C-RR when
extracting 100 and 200 word summaries respectively. Moreover, C-RR performs better when
longer summaries are produced since it extracts a greater number of sentences from each
cluster increasing the likelihood of covering different factoids captured by different clusters.

6. Summaries of Scientific Topics
In previous sections, we described C-LexRank as a method to identify communities of
citations that discuss the same scientific contributions. We showed that C-LexRank is
effective in summarizing contributions of single scientific papers. However, the ultimate
goal of our work is to investigate whether citations have summary-amenable information
and also to build an end-to-end system that receives a query representing a scientific topic
(such as “dependency parsing”) and produces a citation-based automatic summary of the
given topic.
187

Qazvinian et Al.

In this section, we extend our experiments on using the tools explained in previous
sections for automatic summarization of scientific topics. Our evaluation experiments for
extractive summary generation are on a set of papers in the research area of Question
Answering (QA) and another set of papers on Dependency Parsing (DP). The two sets of
papers were compiled by selecting all the papers in AAN that had the words “Question
Answering” and “Dependency Parsing,” respectively, in the title and the content. There
were 10 papers in the QA set and 16 papers in the DP set. We also compiled the citation
sentences for the 10 QA papers and the citation sentences for the 16 DP papers.
6.1 Data Preparation
Our goal is to determine if citations do indeed have useful information that one will want
to put in a summary and if so, how much of this information is not available in the original papers and their abstracts. For this we evaluate each of the automatically generated
summaries using two separate approaches: nugget-based pyramid evaluation and ROUGE.
Recall-Oriented Understudy for Gisting Evaluation (ROUGE) is a metric that evaluates
automatic summaries by comparing them against a set of human written references (Lin,
2004).
Two sets of gold standard data were manually created from the QA and DP citation
sentences and abstracts, respectively:8 (1) We asked three annotators9 with background in
Natural Language Processing to identify important nuggets of information worth including
in a summary. (2) We asked four NLP researchers10 to write 250-word summaries of the
QA and DP datasets. Then we determined how well the different automatically generated
summaries perform against these gold standards. If the citations contain only redundant information with respect to the abstracts and original papers, then the summaries of citations
will not perform better than others.
6.1.1 Nugget Annotations
For our first evaluation approach we used a nugget-based evaluation methodology (Voorhees,
2003; Nenkova & Passonneau, 2004; Hildebrandt, Katz, & Lin, 2004; Lin & DemnerFushman, 2006). We asked three annotators with background in Natural Language Processing to review the citation sentences and/or abstract sets for each of the papers in the QA
and DP sets and manually extract prioritized lists of 2–8 “factoids,” or main contributions,
supplied by each paper. Each factoid was assigned a weight based on the frequency with
which it was listed by annotators as well as the priority it was assigned in each case. Our
automatically generated summaries were then scored based on the number and weight of
the nuggets that they covered.
More particularly, the annotators had two distinct tasks for the QA set, and one for the
DP set: (1) extract nuggets for each of the 10 QA papers, based only on the citations to
those papers; (2) extract nuggets for each of the 10 QA papers, based only on the abstracts
of those papers; and (3) extract nuggets for each of the 16 DP papers, based only on the
citations to those papers.
8. Creating gold standard data from complete papers is fairly arduous, and was not pursued.
9. Two of the annotators are authors of this paper.
10. All of the annotators are authors of this paper.

188

Generating Extractive Summaries of Scientific Paradigms

Human Performance: Pyramid score
Human1 Human2 Human3 Human4 Average
Input: QA citations
QA–CT nuggets
0.524
0.711
0.468
0.695
0.599
QA–AB nuggets
0.495
0.606
0.423
0.608
0.533
Input: QA abstracts
QA–CT nuggets
0.542
0.675
0.581
0.669
0.617
QA–AB nuggets
0.646
0.841
0.673
0.790
0.738
Input: DP citations
DP–CT nuggets
0.245
0.475
0.378
0.555
0.413
Table 8: Pyramid scores were computed for human-created summaries of QA and DP data.
The summaries were evaluated using nuggets drawn from QA citation sentences
(QA–CT), QA abstracts (QA–AB), and DP citation sentences (DP–CT).

One annotator completed the three tasks in full and the remaining two annotators
jointly completed tasks 1 and 3, providing us with two complete annotations of the QA
and DP citation sets and one annotation of the QA abstract set. For each task, annotators
constructed lists of 2–8 prioritized nuggets per paper. This gave us 81 distinct nuggets from
the QA citation set, 45 nuggets from the QA abstract set, and 144 nuggets from the DP
citation set. By collapsing similar nuggets, we were able to identify 34 factoids for the QA
citation set, 27 factoids for the QA abstract set, and 57 factoids for the DP citation set.
We obtained a weight for each factoid by reversing its priority out of 8 (e.g., a factoid listed
with priority 1 was assigned a weight of 8, a nugget listed with priority 2 was assigned a
weight of 7, etc.) and summing the weights over each listing of that factoid.11
6.1.2 Expert Summaries
In addition to nugget annotations, we asked four NLP researchers to write 250-word summaries of the QA citation set, QA abstract set and DP citation set.
Table 8 gives the pyramid scores of the 250-word summaries manually produced by
experts. The summaries were evaluated using the nuggets drawn from the QA citations, QA
abstracts, and DP citations. The average of their scores (listed in the rightmost column)
may be considered a good score to aim for by the automatic summarization methods.
Additionally, Table 9 presents ROUGE scores (Lin, 2004) of each of expert-written 250word summaries against each other (e.g., Human1 versus all others and so forth). The
average (last column) could be considered a ceiling in the performance of the automatic
summarization systems.

11. Results obtained with other weighting schemes that ignored priority ratings and multiple mentions of
a nugget by a single annotator showed the same trends as the ones shown by the selected weighting
scheme.

189

Qazvinian et Al.

Human Performance: ROUGE-2
human1 human2 human3 human4 average
Input: QA citations
QA–CT refs.
0.181
0.196
0.076
0.202
0.163
QA–AB refs.
0.112
0.140
0.071
0.158
0.120
Input: QA abstracts
QA–CT refs.
0.131
0.110
0.122
0.115
0.120
QA-AB refs.
0.265
0.198
0.180
0.254
0.224
Input: DP citations
DP–CT refs.
0.155
0.126
0.120
0.165
0.142
Table 9: ROUGE-2 scores were obtained for each of the manually created summaries by
using the other three as reference. ROUGE-1 and ROUGE-L followed similar
patterns.

6.2 Automatic Extractive Summaries
We used four summarization systems for our summary-creation approach: C-LexRank, CRR, LexRank and MASCS.
We automatically generated summaries for both QA and DP from three different types
of documents: (1) full papers from the QA and DP sets—QA and DP full papers (PA),
(2) only the abstracts of the QA and DP papers—QA and DP abstracts (AB), and (3) the
citation sentences corresponding to the QA and DP papers—QA and DP citations (CT).
We generated 24 (4 × 3 × 2) summaries, each of length 250 words, by applying MASCS,
LexRank, and C-LexRank on the three data types (citation sentences, abstracts, and full
papers) for both QA and DP. (Table 10 shows a fragment of one of the automatically
generated summaries from QA citation sentences.) We created six (3 × 2) additional 250word summaries by randomly choosing sentences from citations, abstracts, and full papers
of QA and DP. We will refer to them as random summaries.
Most of work in QA and paraphrasing focused on folding paraphrasing knowledge
into question analyzer or answer locater (Rinaldi et al., 2003; Tomuro, 2003).
In addition, number of researchers have built systems to take reading comprehension
examinations designed to evaluate children’s reading levels (Charniak et al., 2000;
Hirschman et al., 1999; Ng et al., 2000; Riloff & Thelen, 2000; Wang et al., 2000).
So-called “ definition ” or “ other ” questions at recent TREC evaluations (Voorhees,
2005) serve as good examples.
To better facilitate user information needs, recent trends in QA research have shifted
towards complex, context-based, and interactive question answering (Voorhees,
2001; Small et al., 2003; Harabagiu et al., 2005).
Table 10: A fragment of one of the MASCS-generated summaries is illustrated here using
the QA citation sentences as input.

190

Generating Extractive Summaries of Scientific Paradigms

System Performance: Pyramid score
Random C-LexRank C-RR LexRank MASCS
Input: QA citations
QA–CT nuggets
0.321
0.434
0.268
0.295
0.616
QA–AB nuggets
0.305
0.388
0.349
0.320
0.543
Input: QA abstracts
QA–CT nuggets
0.452
0.383
0.480
0.441
0.404
QA–AB nuggets
0.623
0.484
0.574
0.606
0.622
Input: QA full papers
QA–CT nuggets
0.239
0.446
0.299
0.190
0.199
QA–AB nuggets
0.294
0.520
0.387
0.301
0.290
Input: DP citations
DP–CT nuggets
0.219
0.231
0.170
0.372
0.136
Input: DP abstracts
DP–CT nuggets
0.321
0.301
0.263
0.311
0.312
Input: DP full papers
DP–CT nuggets
0.032
0.000
0.144
*
0.280
Table 11: Pyramid scores were computed for automatic summaries of QA and DP data. The
summaries were evaluated using nuggets drawn from QA citation sentences (QA–
CT), QA abstracts (QA–AB), and DP citation sentences (DP–CT). LexRank is
computationally intensive and so was not run on the DP-PA dataset, as indicated
by * (about 4000 sentences). Highest scores for each input source are shown in
bold.

Table 11 gives the pyramid score values of the summaries generated by the four automatic summarizers, evaluated using nuggets drawn from the QA citation sentences, QA
abstracts, and DP citation sentences. The table also includes results for the baseline random
summaries.
When we used the nuggets from the abstracts set for evaluation, the summaries created
from abstracts scored higher than the corresponding summaries created from citations and
papers. Further, the best summaries generated from citations outscored the best summaries
generated from papers. When we used the nuggets from citation sets for evaluation, the best
automatic summaries generated from citations outperform those generated from abstracts
and full papers. All these pyramid results demonstrate that citations can contain useful
information that is not available in the abstracts or the original papers, and that abstracts
can contain useful information that is not available in the citations or full papers.
Among the various automatic summarizers, MASCS performed best at this task, in
two cases exceeding the average human performance. Note also that the random summarizer outscored the automatic summarizers in cases where the nuggets were taken from a
source different from that used to generate the summary. However, one or two summarizers
still tended to do well. This indicates a difficulty in extracting the overlapping summaryamenable information across the two sources.
191

Qazvinian et Al.

System Performance: ROUGE-2
Random C-LexRank C-RR LexRank MASCS
Input: QA citations
QA–CT refs.
0.116
0.170
0.095
0.135
0.170
QA–AB refs.
0.083
0.117
0.076
0.070
0.103
Input: QA abstracts
QA–CT refs.
0.045
0.059
0.061
0.054
0.041
QA–AB refs.
0.121
0.136
0.122
0.203
0.134
Input: QA full papers
QA–CT refs.
0.030
0.036
0.036
0.282
0.040
QA–AB refs.
0.046
0.059
0.050
0.105
0.075
Input: DP citations
DP–CT refs.
0.107
0.132
0.087
0.049
0.101
Input: DP abstracts
DP–CT refs.
0.070
0.073
0.053
0.203
0.072
Input: DP full papers
DP–CT refs.
0.038
0.025
0.034
*
0.046
Table 12: ROUGE-2 scores of automatic summaries of QA and DP data. The summaries
were evaluated by using human references created from QA citation sentences
(QA–CT), QA abstracts (QA–AB), and DP citation sentences (DP–CT). These
results are obtained after Jack-knifing the human references so that the values can
be compared to those in Table 4. LexRank is computationally intensive and so
was not run on the DP full papers set, as indicated by ‘*’ (about 4,000 sentences).
Highest scores for each input source are shown in bold.

192

Generating Extractive Summaries of Scientific Paradigms

We then evaluated each of the random summaries and those generated by the four
summarization systems against the references. Table 12 lists ROUGE scores of summaries
when the manually created 250-word summary of the QA citation sentences, summary of the
QA abstracts, and the summary of the DP citation sentences, were used as gold standard.
When we use manually created citation summaries as reference, then the summaries
generated from citations obtained significantly better ROUGE scores than the summaries
generated from abstracts and full papers (p < 0.05) [Result 1]. This confirms that crucial information, amenable to creating a summary and present in citation sentences is not
available, or hard to extract, from abstracts and papers alone. Further, the summaries
generated from abstracts performed significantly better than those generated from the full
papers (p < 0.05) [Result 2]. This suggests that abstracts and citations are generally
denser in summary-amenable information than full papers.
When we use manually created abstract summaries as reference, then the summaries
generated from abstracts obtained significantly better ROUGE scores than the summaries
generated from citations and full papers (p < 0.05) [Result 3]. Further, and more importantly, the summaries generated from citations performed significantly better than those
generated from the full papers (p < 0.05) [Result 4]. Again, this suggests that abstracts
and citations are richer in summary-amenable information. These results also show that
abstracts of papers and citations have some overlapping information (Result 2 and Result 4), but they also have a significant amount of unique summary-amenable information
(Result 1 and Result 3).
Among the automatic summarizers, C-LexRank and LexRank perform best. This is
unlike the results found through the nugget-evaluation method, where MASCS performed
best. This suggests that MASCS is better at identifying more useful nuggets of information,
but C-LexRank and LexRank are better at producing unigrams and bigrams expected in a
summary. To some extent this may be due to MASCS’s compression preprocessing, which
breaks large, complex sentences into smaller, finer-grained units of content that correspond
better to the amount of content in a nugget.

7. Conclusion
In this paper, we investigated the usefulness of directly summarizing citation sentences
(set of sentences that cite a paper) in the automatic creation of technical summaries. We
proposed C-LexRank, a graph-based summarization model and generated summaries of
30 single scientific articles selected from 6 different topics in the ACL Anthology Network
(AAN). We also generated summaries of a set of Question Answering (QA) and Dependency
Parsing (DP) papers, their abstracts, and their citation sentences using four state-of-theart summarization systems (C-LexRank, C-RR, LexRank, and MASCS). We then used two
different approaches, nugget-based pyramid and ROUGE, to evaluate the summaries. The
results from both approaches and all four summarization systems show that both citation
sentences and abstracts have unique summary-amenable information. These results also
demonstrate that multidocument summarization—especially technical summary creation—
benefits considerably from citations.
We next plan to generate summaries using both citation sentences and abstracts together
as input. Given the overlapping content of abstracts and citation sentences, discovered in
193

Qazvinian et Al.

the current study, it is clear that redundancy detection will be an integral component of this
future work. Creating readily consumable technical summaries is a hard task, especially
when using only raw text and simple summarization techniques. Therefore, we intend to
combine these summarization and bibliometric techniques with suitable visualization methods towards the creation of iterative technical survey tools—systems that present summaries
and bibliometric links in a visually convenient manner and which incorporate user feedback
to produce even better summaries.
Current work on generating topic summaries is focused almost exclusively on extracting
diverse factoid-rich summaries. Meanwhile, the fluency of the produced summaries has been
mostly ignored. In future work, we plan to employ some post-processing techniques such
as reference scope extraction and sentence simplification, as described by Abu-Jbara and
Radev (2011), to generate more readable and cohesive summaries.

8. Acknowledgments
We would like to thank Ahmed Hassan, Rahul Jha, Pradeep Muthukrishan, and Arzucan
Özgür for annotations and Melissa Egan for preliminary developments. We are also grateful
to Ben Shneiderman, Judith Klavans, and Jimmy Lin for fruitful discussions and the anonymous reviewers for insightful readings and constructive guidance. The following authors,
Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic,
and Michael Whidby were supported, in part, by the National Science Foundation under
Grant No. IIS-0705832 (iOPENER: Information Organization for PENning Expositions on
Research) awarded to the University of Michigan and the University of Maryland. Any
opinions, findings, and conclusions or recommendations expressed in this material are those
of the authors and do not necessarily reflect the views of the National Science Foundation.
The following authors, Michael Whidby and Taesun Moon were supported, in part, by the
Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center (DoI/NBC) contract number D11PC20153. The U.S. Government is
authorized to reproduce and distribute reprints for Governmental purposes not withstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained
herein are those of the authors and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or
the U.S. Government.

References
Abu-Jbara, A., & Radev, D. (2011). Coherent citation-based summarization of scientific
papers. In Proceedings of the 49th Annual Conference of the Association for Computational Linguistics (ACL-11), pp. 500–509.
Athar, A. (2011). Sentiment analysis of citations using sentence structure-based features.
In Proceedings of the ACL 2011 Student Session, HLT-SS ’11, pp. 81–87.
Athar, A., & Teufel, S. (2012). Context-enhanced citation sentiment detection. In Proceedings of the 2012 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pp. 597–601, Montréal,
Canada. Association for Computational Linguistics.
194

Generating Extractive Summaries of Scientific Paradigms

Awasthi, P., Gagrani, A., & Ravindran, B. (2007). Image modeling using tree structured
conditional random fields. In Proceedings of the 20th international joint conference
on Artifical intelligence, IJCAI’07, pp. 2060–2065.
Bassett, D. S., & Bullmore, E. (2006). Small-world brain networks. The neuroscientist,
12 (6), 512–523.
Bird, S., Dale, R., Dorr, B. J., Gibson, B., Joseph, M., Kan, M.-Y., Lee, D., Powley, B.,
Radev, D. R., & Tan, Y. F. (2008). The acl anthology reference corpus: A reference
dataset for bibliographic research in computational linguistics. In Proceedings of the
International Conference on Language Resources and Evaluation, LREC 2008, 26
May - 1 June 2008, Marrakech, Morocco.
Bradshaw, S. (2002). Reference Directed Indexing: Indexing Scientific Literature in the
Context of Its Use. Ph.D. thesis, Northwestern University.
Bradshaw, S. (2003). Reference directed indexing: Redeeming relevance for subject search
in citation indexes. In Proceedings of the 7th European Conference on Research and
Advanced Technology for Digital Libraries.
Carbonell, J. G., & Goldstein, J. (1998). The use of MMR, diversity-based reranking for
reordering documents and producing summaries. In Proceedings of the 21st Annual
International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR-98), pp. 335–336.
Carletta, J. (1996). Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics, 22 (2), 249–254.
Charniak, E., Altun, Y., Braz, R. d. S., Garrett, B., Kosmala, M., Moscovich, T., Pang,
L., Pyo, C., Sun, Y., Wy, W., Yang, Z., Zeller, S., & Zorn, L. (2000). Reading
comprehension programs in a statistical-language-processing class. In Proceedings
of the 2000 ANLP/NAACL Workshop on Reading comprehension tests as evaluation for computer-based language understanding sytems - Volume 6, ANLP/NAACLReadingComp ’00, pp. 1–5.
Clauset, A., Newman, M. E. J., & Moore, C. (2004). Finding community structure in very
large networks. Phys. Rev. E, 70 (6), 066111.
Cohn, T., & Blunsom, P. (2005). Semantic role labelling with tree conditional random
fields. In Proceedings of the Ninth Conference on Computational Natural Language
Learning, pp. 169–172. Association for Computational Linguistics.
Das, D., & Martins, A. (2007). A survey on automatic text summarization. Literature
Survey for the Language and Statistics II course at CMU, 4, 192–195.
Eisner, J. (1996). Three new probabilistic models for dependency parsing: An exploration.
In Proceedings of the 34th Annual Conference of the Association for Computational
Linguistics (ACL-96), pp. 340–345. Association for Computational Linguistics.
Elkiss, A., Shen, S., Fader, A., Erkan, G., States, D., & Radev, D. R. (2008). Blind men
and elephants: What do citation summaries tell us about a research article?. Journal
of the American Society for Information Science and Technology, 59 (1), 51–62.
195

Qazvinian et Al.

Erkan, G. (2006). Language model-based document clustering using random walks. In
Proceedings of the HLT-NAACL conference, pp. 479–486, New York City, USA. Association for Computational Linguistics.
Erkan, G., & Radev, D. R. (2004). Lexrank: Graph-based centrality as salience in text
summarization. Journal of Artificial Intelligence Research (JAIR).
Ferrer i Cancho, R., & Solé, R. V. (2001). The small-world of human language. Proceedings
of the Royal Society of London B, 268 (1482), 2261–2265.
Finkel, J. R., Kleeman, A., & Manning, C. D. (2008). Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pp. 959–967, Columbus, Ohio.
Association for Computational Linguistics.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling of semantic roles. Comput. Linguist.,
28 (3), 245–288.
Harabagiu, S., Hickl, A., Lehmann, J., & Moldovan, D. (2005). Experiments with interactive
question-answering. In Proceedings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ’05, pp. 205–214.
Hildebrandt, W., Katz, B., & Lin, J. (2004). Overview of the TREC 2003 question-answering
track. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (HLT-NAACL ’04).
Hirschman, L., Light, M., Breck, E., & Burger, J. D. (1999). Deep read: a reading comprehension system. In Proceedings of the 37th annual meeting of the Association for
Computational Linguistics on Computational Linguistics, ACL ’99, pp. 325–332.
Hoang, C. D. V., & Kan, M.-Y. (2010). Towards automated related work summarization.
In Proceedings of the 23nd International Conference on Computational Linguistics
(COLING-10), pp. 427–435, Beijing, China. Coling 2010 Organizing Committee.
Johansson, R., & Nugues, P. (2005). Sparse bayesian classification of predicate arguments.
In Proceedings of the Ninth Conference on Computational Natural Language Learning
(CoNLL-2005), pp. 177–180, Ann Arbor, Michigan. Association for Computational
Linguistics.
Joseph, M. T., & Radev, D. R. (2007). Citation analysis, centrality, and the ACL Anthology. Tech. rep. CSE-TR-535-07, University of Michigan. Department of Electrical
Engineering and Computer Science.
Jousse, F., Gilleron, R., Tellier, I., & Tommasi, M. (2006). Conditional random fields for
xml trees. In In Workshop on Mining and Learning in Graphs.
Jun Hatori, Y. M., & Tsujii, J. On contribution of sense dependencies to word sense
disambiguation. Journal of Natural Language Processing.
Jurafsky, D., & Martin, J. H. (2008). Speech and Language Processing: An Introduction
to Natural Language Processing, Speech Recognition, and Computational Linguistics
(2nd edition). Prentice-Hall.
Kan, M.-Y., Klavans, J. L., & McKeown, K. R. (2002). Using the Annotated Bibliography
as a Resource for Indicative Summarization. In The International Conference on
Language Resources and Evaluation (LREC), Las Palmas, Spain.
196

Generating Extractive Summaries of Scientific Paradigms

Kaplan, D., Iida, R., & Tokunaga, T. (2009). Automatic extraction of citation contexts for
research paper summarization: A coreference-chain based approach. In Proceedings of
the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pp.
88–95, Suntec City, Singapore.
Kempe, D., Kleinberg, J., & Éva Tardos (2003). Maximizing the spread of influence through
a social network. In Proceedings of the 15th ACM SIGKDD international conference
on Knowledge Discovery and Data Mining (KDD-09), pp. 137–146. ACM.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of the
41st Annual Conference of the Association for Computational Linguistics (ACL-03),
pp. 423–430.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W.-t. (2005). Generalized inference with
multiple semantic role labeling systems. In Proceedings of the Ninth Conference on
Computational Natural Language Learning (CoNLL-2005), pp. 181–184, Ann Arbor,
Michigan. Association for Computational Linguistics.
Krippendorff, K. (1980). Content Analysis: An Introduction to its Methodology. Beverly
Hills: Sage Publications.
Kupiec, J., Pedersen, J., & Chen, F. (1995). A trainable document summarizer. In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR-95), pp. 68–73.
Lin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Proceedings of the ACL workshop on Text Summarization Branches Out.
Lin, J. J., & Demner-Fushman, D. (2006). Methods for automatically evaluating answers
to complex questions. Information Retrieval, 9 (5), 565–587.
Ma, X. (2000). Hong kong news parallel text. Linguistic Data Consortium, Philadelphia.
Manning, C. D., Raghavan, P., & Schütze, H. (2008). Introduction to Information Retrieval.
Cambridge University Press.
Màrquez, L., Comas, P., Giménez, J., & Català, N. (2005). Semantic role labeling as
sequential tagging. In Proceedings of the Ninth Conference on Computational Natural
Language Learning, CONLL ’05, pp. 193–196.
Mei, Q., Guo, J., & Radev, D. (2010). Divrank: the interplay of prestige and diversity
in information networks. In Proceedings of the 16th ACM SIGKDD international
conference on Knowledge Discovery and Data Mining (KDD-10), pp. 1009–1018.
Mohammad, S., Dorr, B., Egan, M., Hassan, A., Muthukrishan, P., Qazvinian, V., Radev,
D., & Zajic, D. (2009). Using citations to generate surveys of scientific paradigms. In
Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (HLT-NAACL ’09), pp. 584–592, Boulder,
Colorado.
Montoya, J. M., & Solé, R. V. (2002). Small world patterns in food webs. Journal of
theoretical biology, 214 (3), 405–412.
197

Qazvinian et Al.

Nakov, P., D. A., & Hearst, M. (2012). Do peers see more in a paper than its authors?.
Advances in Bioinformatics, special issue on Literature Mining Solutions for Life
Science Research.
Nanba, H., Abekawa, T., Okumura, M., & Saito, S. (2004). Bilingual PRESRI: Integration
of multiple research paper databases. In Proceedings of RIAO 2004, pp. 195–211,
Avignon, France.
Nanba, H., Kando, N., & Okumura, M. (2000). Classification of research papers using
citation links and citation types: Towards automatic review article generation. In
Proceedings of the 11th SIG Classification Research Workshop, pp. 117–134, Chicago,
USA.
Nanba, H., & Okumura, M. (1999). Towards multi-paper summarization using reference
information. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence (IJCAI-99), pp. 926–931.
Nenkova, A., & Passonneau, R. (2004). Evaluating content selection in summarization: The
pyramid method. In Proceedings of the North American Chapter of the Association
for Computational Linguistics - Human Language Technologies (HLT-NAACL ’04).
Newman, M. E. J. (2001). The structure of scientific collaboration networks. PNAS, 98 (2),
404–409.
Newman, M. E. J. (2003). The structure and function of complex networks. SIAM Review,
45 (2), 167–256.
Newman, M. E. J. (2004a). Analysis of weighted networks. Physical Review E, 70–056131.
Newman, M. E. J. (2004b). Fast algorithm for detecting community structure in networks.
Phys. Rev. E, 69, 066133.
Newman, M. E. J., & Girvan, M. (2004). Finding and evaluating community structure in
networks. Phys. Rev. E, 69, 026113.
Ng, H. T., Teo, L. H., & Kwan, J. L. P. (2000). A machine learning approach to answering questions for reading comprehension tests. In Proceedings of the 2000 Joint
SIGDAT conference on Empirical methods in natural language processing and very
large corpora: held in conjunction with the 38th Annual Meeting of the Association
for Computational Linguistics - Volume 13, EMNLP ’00, pp. 124–132.
Paul, M., Zhai, C., & Girju, R. (2010). Summarizing contrastive viewpoints in opinionated
text. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP-10), pp. 66–76.
Pradhan, S., Hacioglu, K., Ward, W., Martin, J. H., & Jurafsky, D. (2005). Semantic
role chunking combining complementary syntactic views. In Proceedings of the Ninth
Conference on Computational Natural Language Learning, CONLL ’05, pp. 217–220.
Punyakanok, V., Roth, D., & Yih, W. (2008). The importance of syntactic parsing and
inference in semantic role labeling. Computational Linguistics, 34 (2).
Qazvinian, V., & Radev, D. R. (2008). Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational
Linguistics (COLING-08), Manchester, UK.
198

Generating Extractive Summaries of Scientific Paradigms

Qazvinian, V., & Radev, D. R. (2010). Identifying non-explicit citing sentences for citationbased summarization.. In Proceedings of the 48th Annual Conference of the Association for Computational Linguistics (ACL-10), pp. 555–564, Uppsala, Sweden.
Qazvinian, V., & Radev, D. R. (2011a). Exploiting phase transition in latent networks
for clustering. In Proceedings of the Association for the Advancement of Artificial
Intelligence (AAAI-11).
Qazvinian, V., & Radev, D. R. (2011b). Learning from collective human behavior to introduce diversity in lexical choice. In Proceedings of the 49th Annual Conference of the
Association for Computational Linguistics (ACL-11), pp. 1098–1108.
Radev, D., Allison, T., Blair-Goldensohn, S., Blitzer, J., Çelebi, A., Dimitrov, S., Drabek,
E., Hakim, A., Lam, W., Liu, D., Otterbacher, J., Qi, H., Saggion, H., Teufel, S.,
Topper, M., Winkel, A., & Zhang, Z. (2004). MEAD - a platform for multidocument
multilingual text summarization. In LREC 2004, Lisbon, Portugal.
Radev, D. R., Muthukrishnan, P., & Qazvinian, V. (2009). The ACL anthology network
corpus. In ACL workshop on Natural Language Processing and Information Retrieval
for Digital Libraries, Singapore.
Ravasz, E., Somera, A., Mongru, D., Oltvai, Z., & Barabási, A. (2002). Hierarchical organization of modularity in metabolic networks. Science, 297 (5586), 1551.
Riloff, E., & Thelen, M. (2000). A rule-based question answering system for reading comprehension tests. In Proceedings of the 2000 ANLP/NAACL Workshop on Reading
comprehension tests as evaluation for computer-based language understanding sytems
- Volume 6, ANLP/NAACL-ReadingComp ’00, pp. 13–19.
Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & Mollá, D. (2003). Exploiting paraphrases in a question answering system. In Proceedings of the second international
workshop on Paraphrasing - Volume 16, PARAPHRASE ’03, pp. 25–32.
Siddharthan, A., & Teufel, S. (2007). Whose idea was this, and why does it matter?
attributing scientific work to citations. In Proceedings of the North American Chapter
of the Association for Computational Linguistics - Human Language Technologies
(HLT-NAACL ’07).
Small, S., Liu, T., Shimizu, N., & Strzalkowski, T. (2003). Hitiqa: An interactive question
answering system: A preliminary report. In Proceedings of the ACL 2003 Workshop
on Multilingual Summarization and Question Answering.
Smyth, S., & White, S. (2005). A spectral clustering approach to finding communities in
graphs. In Proceedings of the 5th SIAM International Conference on Data Mining,
pp. 76–84.
Spärck-Jones, K. (1999). Automatic summarizing: factors and directions. In Mani, I., &
Maybury, M. T. (Eds.), Advances in automatic text summarization, chap. 1, pp. 1 –
12. The MIT Press.
Tang, J., Hong, M., Li, J., & Liang, B. (2006). Tree-structured conditional random fields
for semantic annotation. In Proceedings of the 5th International Semantic Web Conference (ISWC’06), pp. 640–653.
199

Qazvinian et Al.

Teufel, S. (2005). Argumentative Zoning for Improved Citation Indexing. Computing Attitude and Affect in Text: Theory and Applications, 159–170.
Teufel, S., & Moens, M. (2002). Summarizing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics, 28 (4), 409–445.
Teufel, S., Siddharthan, A., & Tidhar, D. (2006). Automatic classification of citation function. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing (EMNLP-06), pp. 103–110, Sydney, Australia.
Thompson, C., Levy, R., & Manning, C. (2003). A generative model for FrameNet semantic
role labeling. In Proceedings of the Fourteenth European Conference on Machine
Learning ECML-2003, Croatia.
Tomuro, N. (2003). Interrogative reformulation patterns and acquisition of question paraphrases. In Proceedings of the second international workshop on Paraphrasing - Volume 16, PARAPHRASE ’03, pp. 33–40.
Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pp. 589–596, Ann Arbor, Michigan. Association for
Computational Linguistics.
Toutanova, K., Haghighi, A., & Manning, C. D. (2008). A global joint model for semantic
role labeling. Comput. Linguist., 34 (2), 161–191.
van Halteren, H., & Teufel, S. (2003). Examining the consensus between human summaries:
initial experiments with factoid analysis. In Proceedings of the HLT-NAACL 03 on
Text summarization workshop, pp. 57–64, Morristown, NJ, USA.
van Halteren, H., & Teufel, S. (2004). Evaluating information content by factoid analysis: human annotation and stability. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP-04), Barcelona.
Voorhees, E. M. (1998). Variations in relevance judgments and the measurement of retrieval
effectiveness. In Proceedings of the 21st Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR-98), pp. 315–323.
Voorhees, E. M. (2001). Overview of the TREC 2001 question answering track. In Text
REtrieval Conference.
Voorhees, E. M. (2003). Overview of the TREC 2003 question answering track. In Proceedings of the Twelfth Text Retrieval Conference (TREC-2003).
Voorhees, E. M. (2005). Using question series to evaluate question answering system effectiveness. In In HLT/EMNLP 2005.
Wang, W., J., A., Parasuraman, R., Zubarev, I., Brandyberry, D., & Harper, M. (2000).
A Question Answering System Developed as a Project in a Natural Language Processing Course. In In ANLP/NAACL Workshop on Reading Comprehension Tests as
Evaluation for ComputerBased Language Understanding Systems.
Watts, D. J., & Strogatz, S. (1998). Collective dynamics of small-world networks. Nature,
393, 440–442.
200

Generating Extractive Summaries of Scientific Paradigms

Whidby, M. A. (2012). Citation handling: Processing citation texts in scientific documents.
Master’s thesis, University of Maryland, Department of Computer Science, College
Park, MD.
Xue, N., & Palmer, M. (2004). Calibrating features for semantic role labeling. In Lin, D., &
Wu, D. (Eds.), Proceedings of EMNLP 2004, pp. 88–94, Barcelona, Spain. Association
for Computational Linguistics.
Zajic, D. M., Dorr, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks. Information Processing and
Management (Special Issue on Summarization).
Zhao, Y., & Karypis, G. (2001). Criterion functions for document clustering: Experiments
and analysis. Technical report TR #01–40, Department of Computer Science, University of Minnesota, Minneapolis, MN.
Zhu, X., Goldberg, A., Van Gael, J., & Andrzejewski, D. (2007). Improving diversity in
ranking using absorbing random walks. In Proceedings of the North American Chapter
of the Association for Computational Linguistics - Human Language Technologies
(HLT-NAACL ’07), pp. 97–104.

201

Journal of Artificial Intelligence Research 46 (2013) 413-447

Submitted 12/12; published 03/13

Qualitative Order of Magnitude Energy-Flow-Based Failure
Modes and Effects Analysis
Neal Snooke

nns@aber.ac.uk

Department of Computer Science, Aberystwyth University,
Penglais, Aberystwyth, Ceredigion, SY23 3DB, U.K.

Mark Lee

mhl@aber.ac.uk

Department of Computer Science, Aberystwyth University.

Abstract
This paper presents a structured power and energy-flow-based qualitative modelling approach that is applicable to a variety of system types including electrical and fluid flow. The
modelling is split into two parts. Power flow is a global phenomenon and is therefore naturally represented and analysed by a network comprised of the relevant structural elements
from the components of a system. The power flow analysis is a platform for higher-level
behaviour prediction of energy related aspects using local component behaviour models
to capture a state-based representation with a global time. The primary application is
Failure Modes and Effects Analysis (FMEA) and a form of exaggeration reasoning is used,
combined with an order of magnitude representation to derive the worst case failure modes.
The novel aspects of the work are an order of magnitude(OM) qualitative network
analyser to represent any power domain and topology, including multiple power sources,
a feature that was not required for earlier specialised electrical versions of the approach.
Secondly, the representation of generalised energy related behaviour as state-based local
models is presented as a modelling strategy that can be more vivid and intuitive for a range
of topologically complex applications than qualitative equation-based representations. The
two-level modelling strategy allows the broad system behaviour coverage of qualitative
simulation to be exploited for the FMEA task, while limiting the difficulties of qualitative
ambiguity explanation that can arise from abstracted numerical models. We have used the
method to support an automated FMEA system with examples of an aircraft fuel system
and domestic a heating system discussed in this paper.

1. Introduction
Qualitative representations (QR) and reasoning have a number of well documented advantages for Failure Modes and Effects Analysis. Three of the most important are that analysis
can be performed early in the design life cycle, that there is broad coverage of system behaviour and faults, and that the results are at a level of abstraction which readily maps to
system functional states.
QR has been widely applied to electrical systems (de Kleer, 1984; Mauss & Neumann,
1996; Price, Snooke, & Lewis, 2006; Flores & Farley, 1999) and used for a variety of design
analysis applications including design concept analysis (gaining an overview of system behaviour), diagnosis, FMEA safety analysis, incremental and multiple fault FMEA analysis
(Price & Taylor, 1997), fault tree analysis (FTA) (Price, Wilson, Timmis, & Cain, 1996),
and sneak circuit analysis (Price, Snooke, & Landry, 1996; Savakoor, Bowles, & Bonnell,
©2013 AI Access Foundation. All rights reserved.

Snooke & Lee

1993). The qualitative information is an enabling technique allowing significant system
nominal and failure states to be identified and distinguished easily and efficiently.
The qualitative results have limitations, the most significant being behavioural ambiguity where alternative behaviours are predicated by the simulation. While often seen as
a problem of qualitative techniques, such ambiguity can be turned into an advantage, provided they predict real physical behaviours since they indicate where key design parameters
exist. One of the benefits of the modelling structure developed in this paper is that many
such ambiguities are constrained and therefore enhance design knowledge rather than being
an analysis limitation.
The typical approaches to qualitative modelling and reasoning use qualitative versions
of numerical equations derived from component models. While this forms a sound platform,
it can be burdensome to extract the relevant from the unimportant with standard equations
(e.g fluid flow), and deal with landmarks, integrals or deviations etc. Given the appeal of
QR for abstracted explanation and its ability to provide broad analysis of system state, we
propose a more specialised view that can be applied to a wide range of engineering domains
based on the relationships between generalised physical variables shown in Figure 1.
power
network

ine
(en rtia
flux linkage
erg
y)
pressure momentum
momentum

resista

momentum

EMF, voltage (electrical)
pressure (hydraulic)
force (mechanical)

ce
tan
aci y)
cap nerg
(e

n ce

effort

flow

displacement
charge
volume
displacement

current
volumetric flow rate
velocity

Figure 1: Tetrahedron of state

This paper provides a consistent framework for several earlier implementations, and develops a supporting qualitative order of magnitude (OM) capability (Section 2.1) throughout
the models and simulation. The OM representation is derived from a many valued resistance
representation (Lee, 2000b) and although the many valued concept and its modelling and
reasoning benefits are completely applicable to the OM representation, the actual technique
for solving the circuits in that work was based on path labelling (Lee, 1999), and was not a
general solution applicable to any network topology. The present paper provides a general
solution and extends the technique from purely electrical systems into other domains. As
a necessary part of this effort an energy-flow-based formalisation is adopted, comprising a
global instantaneous power network (Section 2) and local component models that include
the notions of energy and time (Section 3).
414

Qualitative Energy-Flow-Based FMEA

Previous work by the authors (Snooke, 2007) on the modelling of fluid flow systems was
based on an electrical network analyser (Lee, 1999) but could only deal with series parallel
reducible circuits, and this becomes a limitation for topologically complex systems. The
incorporation of equivalent resistance reduction using star–delta (Y-∆) transforms (Mauss &
Neumann, 1996) into the OM representation (Section 2.2) allows effort and flow variables for
resistive networks of any topology to be solved by reduction and flow assignment expansion
(Section 2.3), and completes the work started by Lee. Complex network topologies are due
in part to the need to represent concepts such as the ‘atmosphere’ in fluid flow systems,
to allow failure modes such as leaks in closed systems, as well as vented elements in other
systems. This unique ‘zero’ node is common to several domains and is generalised in the
present work in Section 2.4.
Applying the techniques derived by Lee (1999) to non electrical systems reveals two
limitations. Firstly, the analysis supports only a single effort source in the network, and
secondly there is no concept of the substance flowing in the network. Section 2.5 extends the
analysis to include multiple power sources by applying the principle of superposition to allow
any system to be decomposed into multiple resistive networks. Section 2.6 considers the
representation of substances for networks where behaviour is dependent upon the substance
carrying the effort and flow. The final part of section 2 deals with some common special
cases that can simplify the analysis and maintain vividness of the representation by avoiding
unnecessary (Y-∆) transforms.
Section 3 of the paper is devoted to the local component concept that captures temporal
information and thus models the displacement and momentum aspects of Figure 1. The
OM modelling of time is the subject of section 3.1 and underlies a Finite State Machine
(FSM) based modelling approach. This representation is an improvement on a more restricted predecessor used for electrical component behaviour (Snooke, 1999), which was the
foundation for a variety of automated electrical design analysis techniques. Sections 3.2 and
3.3 provide concrete examples of component modelling and system modelling respectively.
Section 4 considers the use of the OM representation to perform exaggeration reasoning
for failure analysis and section 5 provides a brief overview of the strategy (Price, 1998; Lee,
Bell, & Coghill, 2001) used to process multiple simulation results into an FMEA output.
We provide two case study examples in sections 6 and 7. The first case study illustrates the
behaviour simulation for a fluid flow system with interesting characteristics, and the second
outlines the use of the simulation results to automatically produce a completed FMEA
report for an industrial system supplied by the sponsors of the work.
In this paper we utilise a two-level modelling strategy. The lower level (centre section
of Figure 1) provides a network-based global qualitative solver for linear resistive networks
using graph-based methods that determine instantaneous power (effort and flow) in a network. The higher level utilises the results of the lower level network analysis to decide
localized component behaviour from state-based models with a qualitative representation
of time as the only global parameter.
This two-level strategy gives significant benefits over other approaches to modelling the
kinds of electrical and hydraulic systems dealt with here. The two-level modelling allows
separation of the inherently global, instantaneous (single state) power variables, and the
local energy-based behaviour of components. This separation is particularly suitable for
qualitative representation because qualitative behaviour is fundamentally state-based; any
415

Snooke & Lee

qualitative simulation will produce a sequence of states of the power flows in a system’s
components. Power flow is a global phenomenon where cause and effect is undetermined
and can be very efficiently analysed as a network, avoiding problems of causal instability and
ambiguity arising from systems of equations generated from interacting components with
local behaviours (Skorstad, 1992a). Many QR approaches use a local propagation method
connecting the behaviours of components and have the appeal of generality, however, they
often require some way of including additional global information such as equivalent circuits (Sussman & Steele Jr, 1980), or mechanisms for propagating connectivity information
(Struss, Malik, & Sachenbacher, 1995). Our approach allows users to build reusable models
of components that can be placed in a library and used just by describing how components
are linked together.
Bond graphs share the same underlying energy-flow-based concept as the lower level
of our approach, and are a popular domain-independent graphical method for describing
a system first described by Paynter (1961) and developed by numerous others. They are
an excellent way of generating the systems of differential equations that describe system
behaviour. Qualitative versions of bond graphs can be produced (Ghiaus, 1999), however
these are qualitative versions of the standard numerical equations and therefore suffer the
same difficulties as any of the general equation-based constraint methods. A qualitative
version of bond graphs is successfully used to model energy flows associated with people in
a building (Tsai & Gero, 2010), where the movement of people is not determined by any
specific physical laws and thus there is no choice but to provide an ad-hoc model based on
high-level knowledge about the building use. It has been observed that qualitative models
often result in too many fault candidates, and a reasoning scheme based on past experience and ad-hoc system dynamics where it can be difficult to obtain the proper models
for quantitative methods (Samantaray & Ould, 2011). The most widely-used qualitative
equation-based constraint method has been QSIM (Kuipers, 1986). Kuipers has acknowledged the problems with QSIM for many applications - chattering, uncontrolled branching
of possibilities, asymptotic behaviours (Fouché & Kuipers, 1990). While some techniques
have been developed for minimising the problems (Clancy & Kuipers, 1997), the problems
with the approach remain for practical systems. As observed (Mosterman & Biswas, 2000),
reducing model complexity by eliminating higher-order derivatives and non-linear effects
leads to discontinuities in the system (i.e. state changes) and careful analysis of the underlying physical nature of the system is required when constructing models in order to ensure
that the simplified models correspond to real behaviour. That kind of careful analysis is not
possible when linking QSIM-style qualitative equation-based constraint methods to physical
components, and certainly not when wanting to be able to simulate faulty components.
Our two-level approach has achieved practical success even in less sophisticated versions than the present approach (Price & Struss, 2004) by avoiding direct translation of
differential equations to their qualitative versions, and rather representing phenomena explicitly as state-based representations at the local component behaviour level and providing
a constrained global representation. This does not limit the applicability because it is the
property of the model that is discrete or continuous rather than any inherent property of
the system (Struss, 2003), and the real question is what kind of model is appropriate for the
reasoning task at hand. The modelling in this paper requires minimal algebraic effort, by
representing component behaviour at an abstract level such that qualitative behaviour can
416

Qualitative Energy-Flow-Based FMEA

Effort, E
0
0
0
u
u
u

Resistance, R
0
r
∞
0
r
∞

Flow, F
?
0
0
t
f
0

Table 1: Qualitative electrical current assignment

be predicted for nominal and failure modes for topologically complex systems with many
component states.

2. The Qualitative Power Network
The power network (system circuit) is represented using resistances, R, as the structural
component. The simulation task is to derive the effort across each resistance and the flow
through each resistance given a power (effort or flow) source in the network. A minimal
useful qualitative quantity space uses [0, r, ∞] to represent resistance and [0, u] to represent
effort, E , at the supply terminals (Lee, 1999). A linear network uses the generalised version of Ohm’s Law for effort, flow, and resistance, E = F × R, and provides the current
assignment in Table 1. The first row is qualitatively ambiguous because any flow through
a zero resistance produces no effort loss. It is not physically possible to have an effort drop
across a zero resistance and hence the flow in the fourth row is shown as t to indicate an
impossible case.
A power network is considered as a graph G(T, A) containing nodes T and edges A that
connect exactly two nodes, and represent the circuit resistances. Each edge e ∈ A has a
resistance value R(e) and connects a pair of nodes e = ht1 ∈ T, t2 ∈ T i. Effort is measured
between two nodes E(t1 , t2 ) and flow is measured through an edge F (e). The degree of a
node is the number of connections to that node. Looped edges with both ends connected
to the same node therefore increase the degree of the node by two.
2.1 Order of Magnitude Representation
An order of magnitude representation for R allows more detailed modelling without introducing qualitative ambiguity by separating artefacts with significantly different characteristics (Raiman, 1991; Lee, 2000a). This enhancement improves the ability to represent
nominal behaviour such as distinguishing signal level power from actuator power in electrical circuits, and gravitational ‘head’ pressure from the system pump in selected fluid
transfer systems. The modelling of faults is also improved by allowing exaggerated faults to
be modelled, thus producing effects for faults where effects would otherwise be qualitatively
indistinguishable from nominal operation.
We define O(M ) qualitative resistance values R = [r1 = 0, r2 , . . . , ri−1 , rm = ∞] such
that ri+1  ri for any i. In addition ri /ri+1 = rj /rj+1 for any i, j ∈ N. Physically we
interpret this to mean that any number of rm valued resistors in series will be dominated
417

Snooke & Lee

a
0
0
0
r 3n
r 3n
∞

b
0
r 3n
∞
r 3m
∞
∞

a×b
0
0
?
r 3(n+m)
∞
∞

Table 2: OM Multiplication

by a single rm+1 valued resistor in the same series segment, and that in addition, the
magnitudes of the resistances are qualitatively of equal spacing. The OM representation
proposed has the benefit that it does not generate additional qualitative landmarks that
lead to potential ambiguity in the model, indeed its purpose is to allow qualitative distinctions for characteristics that are known to be of a significantly different magnitude. The
qualitative OM approach is somewhat analogous to the base 10 logarithmic scale used for
magnitude approximation in numerical reasoning, with two differences. For the qualitative
 assumption above to hold generally when mapping from numerical specifications to qualitative ones in practical systems, a coarser magnitude scale than 10 is required. Therefore,
models that distinguish values based on the numerical equivalent of three order of magnitude prefix ranges, such as A, mA, µA for electrical current, work well. Secondly,  may
be interpreted for a specific application type or domain. For example if we can assume in
some fluid system that the pressure in a system created by gravitational ‘head’ in a vertical
pipe is always dominated by the pump(s) working against it, regardless of the number of
pipe sections in the system, we can consider these at different qualitative magnitudes, even
though numerically this clearly is not the case. In this case a consistent set of qualitative
magnitudes, models, and interpretations is required for each application or domain that
separates phenomena considered to satisfy  for target systems.
The qualitative magnitude notation for q3n is introduced as a convenience for lower
magnitudes to indicate a quantity n orders of magnitude below q. Similarly, for convienience, q2n indicates a quantity n orders of magnitude above q and therefore q3n = q2(−n) .
The OM calculations follow the usual rules of sign algebra (Travé-Massuyès, Ironi, & Dague,
2004) but allows the domination of one magnitude by another as shown in Tables 2 and 3
for positive multiplication and addition respectively. Table 1 can now have u replaced by
u3n and r replaced by r 3m resulting in f 3(n−m) for the flow result in row 5.
2.2 Network Reduction
The OM representation is utilised for each of the resistance, effort, and flow variables of a
power network. The aim is to derive flow and effort values throughout the entire circuit, and
this is achieved by reducing the circuit to a single equivalent resistance, assigning the flow
value and expanding the network assigning effort and flow values at each level of expansion
according to the qualitative structures present. The network reduction is performed by
series and parallel (SP) circuit simplification with an OM version of the qualitative star–
delta (Y-∆) transformation (Mauss & Neumann, 1996) for non SP cases as follows.
418

Qualitative Energy-Flow-Based FMEA

+
0

0
0

+p3n
+p3n

+p3m

+p3m

+p3min(m,n)

−p

−p

3m

3m

∞
?

∞
?

+p3n if n < m
−p3m if m < n
? if n = m
∞
?

−p3n
−p3n
3n
−p
if n < m
+p3m if m < n
? if n = m

∞
∞

?
?

∞

?

∞
−p3min(m,n)
∞
?

?
∞
?

?
?

Table 3: OM addition
Within a network graph G, any nodes of degree two can be removed and have both
connected edges e1 , e2 replaced with an equivalent series edge of resistance:
R(e1 ¦ e2 ) = max(R(e1 ), R(e2 ))

(1)

Any edges e1 , e2 , . . . , en that share the same pair of nodes can be replaced by a single
equivalent parallel edge with the resistance:
R(e1 ||e2 || . . . ||en ) = min(R(e1 ), R(e2 ), . . . , R(ei ))

(2)

To facilitate vividness of the derived model we use the convention e1 ||e2 to label edges
representing a parallel combination of edges and e1 ¦ e2 to indicate a series combination.
Iterative application will produce a tree of equivalent resistances and will result either in
a single resistance R0 between the supply nodes t⊕ and t or a non-SP reducible circuit
fragment.
The majority of circuits are SP reducible, particularly if we consider that zero resistance
edges can be removed from the network to simplify the topology (Section 2.7). For the
remainder a Y-∆ transformation can be applied as shown in Figure 2. The introduction
of Y-∆ resistances unfortunately reduces the vividness of the representation since they are
not directly related to the original component structure; however, it provides a general
solution and can assign flow direction for any network, unlike earlier work (Lee, 2000a).
A qualitative signs version of this transformation was utilised before (Mauss & Neumann,
1996), however, an OM representation introduces the possibility of additional levels of
resistance for the transformed node and requires a more detailed analysis.
Edges e1 . . . en connected to a non SP reducible degree n star node are replaced by new
edges ejk that form an equivalent network where 1 ≤ j ≤ n, 1 ≤ k ≤ n, k > j. Using
RR (e) to notate the numerical resistance of edge e, a 3 node Y-∆ transformation is defined
(symmetrically for other edges) as:
RR (e12 ) =

RR (e1 )RR (e2 ) + RR (e1 )R(e3 ) + RR (e2 )RR (e3 )
RR (e3 )

(3)

Equation 3 is generalised as a star mesh transform defined for star nodes of any order as:
RR (ejk ) = RR (ej )RR (ek )

n
X
m=1

419

1/RR (em )

(4)

Snooke & Lee

Noting that for the OM qualitative values:


1 1
1
max
, , ... =
a b
min(a, b, ...)

(5)

the qualitative version of equation 4 for 1 ≤ m ≤ n is:
R(ejk ) =

R(ej )R(ek )
min(R(em ))

(6)

If the denominator is ∞, then all of the resistances must be ∞ and R(ejk ) = ∞. If any
of the star resistances, R(em ) = 0, then the result is undefined. Hence, for efficiency, zero
resistance edges are removed by combining the associated nodes into a super node which
will be discussed in the next subsection. If the numerator contains 0 or ∞, that value is the
result. For all other cases the result is determined by the magnitude indices of the values.
2c and R(e ) = r 2a and R(e ) = r 2b ,
If min(R(em )) = rm
j
k
j
j
2(a+b−c)
rjk

=

e1

2b
r2a
j rk

(7)

r2c
m

e12

e13
e2

e3
e23

Figure 2: ∆-Y transformation for 3, 4 and 5 nodes

2.3 Flow and Effort Assignment
The reduced network can have a flow (or effort for flow sources) assigned for its source
directly from the OM extended version of Table 1. The R = 0 case t is reported immediately
since it has a specific physical interpretation in most circumstances, such as an electrical
short circuit. When R = 0 contains another source (Section 2.5) not presently under
consideration, that edge is ignored. To determine the flow in a specific circuit component,
the edge hierarchy is expanded for the required edge. The sign of a flow determines its
420

Qualitative Energy-Flow-Based FMEA

direction relative to an arbitrary terminal order for each resistor, using a convention t1  t2
is f and t2  t1 is -f.
For edges e1 , e2 that are part of a series pair the flow through each is simply the flow
through the pair: F (e1 ) = F (e2 ) = F (e1 ¦ e2 ) and the effort across the edge is E(e1 ) =
F (e1 )R(e1 ) (noting that f 3n r 3m = u3(m+n) ). If R(e) = ∞, then F (e) = 0 and E(e)
is undetermined from the effort equation (see rows 3 and 6 in Table 1); however, unless
R(e1 ) = R(e2 ) = 0, E(e) = E(e1 ¦ e2 ).
For edges in parallel, E(e1 ) = E(e2 ) = E(e1||e2 ) and the flow is F (e) = E(e)/R(e).
When R = 0 it is physically impossible that E 6= 0, unless there is a short circuit at the
supply, which is treated as a special situation. If R = ∞ then F = 0 as in Table 1.
For Y-∆ edges we have a sum of flows,
Fek =

k−1
X

F (emk ) −

m=1

n
X

F (ekm )

(8)

m=k+1

The addition of mixed signs leads to the possibility of an ambiguous flow (Table 3), as would
be caused by a balanced bridge configuration. This indicates that the qualitative behaviour,
and possibly, future state of the system depends on the numerical values of the resistances
within one order of magnitude, signalling the analysis to try and obtain more detailed
information. At the level of resistances, an ambiguous flow value does not necessarily lead
to a reasoning impasse at the higher-level tasks such as FMEA if a higher-level behaviour
is not dependent on the value and the analysis tool reporting does not require it. Finally,
E(eek ) = F (eek )R(eek ).
Figure 3 exemplifies a number of the concepts in the preceding subsections. The notation
⊕ and  is used to identify positive and negative supply nodes, using a subscript to identify
the source when necessary for systems with multiple power sources. Working from left to
right in the Figure, a sequence of circuit reductions is performed to obtain a final single
equivalent resistance value. An overall flow value is then computed and distributed amongst
the circuit elements from right to left in the Figure as described above. Finally the flows
in the Y are calculated from the ∆ flows using equation 8, noting that the positive flow
directions are defined as away from the star centre node.
F (e1 ) = −F (e14 ) − F (e13 ) = −f 31 − f 34 = −f 31
F (e4 ) = −F (e14 ) − F (e43 ) = f 31 − f 33 = f 31
F (e3 ) = −F (e13 ) − F (e43 ) = f 34 + f 33 = f 33
2.4 Distinguished Node
Effort values are measured between two nodes and for convenience it is common practice
to identify one distinguished node in a network and make measurements relative to that
node. This then allows effort to be measured ‘at’ a node with the implicit assumption that
the second node is the distinguished node. This is commonly the case in electrical systems
where the distinguished node is called ground or earth and provides a reference node for
voltage measurement and is often defined to be the negative supply terminal in a single
source system.
421

Snooke & Lee

u

e0

e0
r

<1

r

<1

t1

>1
f
r

<1

e1

r

e13

<1
>4
f

t3

e3
r

<3

r

r

<1

e5
r

>1
f

r

<3

r

>2
f
r

r

<1

>1
f

t1

e0 : e14
|| ((e2 || e13)
: (e5 || e34))

r

e5
<2

r
>2
f

<2

<1
r
e14
|| ((e2 || e13)
: (e5 || e34))

e14

<1 e14

<3
r

<1

>2
f

e34

>2
f

r

t1

t3

t3

r

<1

e2 || e13

<1

<1 e14
>3
f

e4

r

<1

e0

t1
e2

>2
f

>3
f

t2

r

t1
e2

e0

e0

>2
f

<1

>1
f

r

<2

>2
f

r

<1

>1
f

>1
f

e2
|| e13
: e5 || e34

e5 || e34

0

non SP-reducible

star delta reduction

parallel reduction

series reduction

parallel reduction

Figure 3: Operation of the qualitative circuit solver showing reduction and flow assignment

For generality we define the symbol Z as the distinguished zero node for a network of
any type with specific instances, such as A to represent ‘atmosphere’ for fluid flow systems.
Notice that Z does not provide another landmark value in the qualitative effort space; it
provides a structural reference point in the resistive network.
The identification of Z allows the definition of ‘absolute’ effort relative to this point.
The qualitative effort levels [0, u] are supplemented – as for voltage (Lee, 1999) – with
two additional symbols that represent structural features of the circuit that emerge during
simulation. These additions are useful for interpretation of the simulation, but do not
change the quantity space itself. ∅ is used to indicate a floating effort present in circuit
fragments disconnected from any source and ∼ is used to indicate an effort that is between
the supply terminal’s voltage, i.e. E(⊕, ∼) = E(∼, ) within the same magnitude. ∼ is
associated to each source and is less useful for systems with multiple active effort sources.
It is always useful to distinguish ∅ from 0 and ? because ∅ is generally undefined and would
not provide a meaningful measurement, although it may read 0 if measured. The qualitative
value ? is different from ∅ because ? is qualitatively undetermined but has a value within
the scope of the modelled system, whereas ∅ has no value within the modelled system.
2.5 Multiple Effort Sources
The network solver calculates power consumed (P = E × F ) for a single source. For a linear
resistive network we can use a qualitative version of the principle of superposition where
more than one source is connected to a single network (noting a single system schematic
may, at any instant, actually comprise many isolated networks, possibly in different domains,
even though all components on the schematic appear connected). Each network is analysed
separately for each source by inhibiting all other effort sources and summing the results.
P
For sources s1 . . . sn , the qualitative flow through an edge e is given by ssn1 F (e) =
max(Fs1 (e) . . . Fsn (e)). Clearly where two opposing flows of the same magnitude exist, the
422

Qualitative Energy-Flow-Based FMEA

edge suffers from qualitative ambiguity. For failure configurations, the ambiguous result is
useful in highlighting to an engineer that a range of possible failure behaviour can occur.
Simple relational constraints may be used to resolve an ambiguity for the common special
case when there is a zero resistance between one of the supply nodes ⊕s1 , s1 and one of
⊕s2 , s2 . For example a statement of the relative power of two pumps will allow the direction
of flow in the reduced circuit to be derived, thereby allowing flows in the expanded network
resistances to be calculated.
Figure 4 illustrates a circuit schematic for an electrical system with two effort sources
and resistances of magnitude r31 and r. The individual flow contributions calculated for
source s1 are shown with a coarse broken line, and the flows for s2 with a fine broken line and
are annotated with the flow magnitudes. The flow sum is shown for each resistor. There is
partial flow ambiguity in the circuit, shown by the double-headed arrows next to resistors
in the centre section where opposing flow contributions occur at the same magnitude. If the
state-based or functional simulation requires the flow (direction), power or effort at these
resistors, then it is necessary to know the values of the resistors.
s1

-

+
f<0

u>0

f<1

f<0

0z

r>1

-

r>1

f<1

f<0

u>0

+
r

f<0

s2

f<0

f<0
f<0

f<1
f<0

s2 flow

f<1

r>1

s1 flow

Figure 4: Multiple Source Example

2.6 Representation of Substance Within the System
The discussion so far has not considered the nature of the flow. For some domains the flow,
such as electrical current, is implicit in the component models. In the thermal domain E is
a temperature difference and entropy flow is F . The product of these is ‘heat’ or thermal
power, P , and the resistance of each component is selected to represent the reciprocal of
thermal conductivity. Fluid transfer and hydraulic systems include the possibility that more
than one substance can be associated with a network, particularly when faults are present.
A concrete example is given by a fuel distribution system. If the supply tank becomes
empty, air will enter the system and the behaviour of pumps and other component may
change.
The substance-dependent behaviour is represented at the component level. The power
network provides an instantaneous view of effort and flow, so it cannot participate in propa423

Snooke & Lee

gation of flowing substance, it is, however, necessary for the components to be able to obtain
knowledge of the substance at the interface with other components. The network nodes are
considered as zero volume points of connection that instantly propagate substances from
component outflows to inflows. The local component behaviour provides substance information to the nodes in an appropriate qualitative timeframe according to the substance
present at inflows, capacitance and other behaviour. There can be several connections to a
node and flow direction may change during simulation. For these reasons a new concept is
included in the circuit solver; a list of substances is maintained at each node that contains
the output substances of each resistance connected to it at any instant of the simulation.
Figure 5 depicts a node connected to three resistances. The flow directions are likely to
cause e1 and e2 to modify the associated SUBSTANCE lists in t1. If e3 then requests the
substance from t1 using S(t1), then {S0, S1, S3} will be obtained.
assign
S(e1.t1) = {S0,S1}
S(e2.t1) = {S3,S0}
evaluate
S(t1)
result {S0, S1, S3}

t1 SUBSTANCE list
e1 {S0, S1}
e2 {S3, S0}

e1
t1

e2

e3

Figure 5: Node substance representation

The presence of more than one substance at a node will result in substance ‘mixing’
by virtue that all substances present are provided to any component that uses the node as
an input. We might consider using the flow magnitude information to indicate the ratio of
substance present, however, the reality will usually be that many other factors are involved,
leading to a process of diminishing returns for the modelling effort required.
The modelling of substance in this work is deliberately simple and provides qualitative
capabilities to match conventional bond graphs which address systems with free energy (mechanical, electrical, magnetic, incompressible fluid) in which all elements are conservative
except the resistance, where energy is dissipated. Sophisticated techniques have been added
to the Bond graph methodology (Brown, 2010), which allow numerical models of thermofluidic phenomena to be constructed; however, the complexity required of these models is such
that the benefits of the broad-based qualitative modelling for system engineering analysis
such as FMEA is lost. Some alternative approaches to qualitative reasoning have considered
changes of material state for example a ‘plug-based’ ontology (Skorstad, 1992b) which is
able to model the phase transitions in a steam boiler tube, however, the modelling required
is necessarily detailed and the 13 state envisionment produced illustrates the complexity
of the phenomena. Another approach to avoid complexity is used by Ghiaus who provides
a qualitative model for the Carnot refrigeration cycle (Ghiaus, 1999) based on equations
derived from a thermal bond graph and thus does not include substance properties as the
system is assumed to be in equilibrium, precluding analysis of many failure modes.
Within our proposed ontology, compressible fluids require representation of the state of
the substance and would require that the substance state is parameterised in the models.
424

Qualitative Energy-Flow-Based FMEA

Enthalpy flow could potentially be modelled by providing multiple forms of a substance (including phase changes) which are related to changes in containing volumes and pressures,
providing effort to a thermal circuit. In any modelling enterprise, a choice has to be made
as to the range of phenomena that are worth modelling, based on the required analysis.
Complex thermodynamic aspects are one area where we have not considered automated
analysis – at the overall system engineering level – to be worth the modelling effort because
it is not clear that there is sufficient range of faults supported by models that can provide qualitatively distinct useful behaviour predictions. This is possibly an area for future
research and may indeed provide fruitful results, but we make no claims at the current time.
2.7 Computational Enhancements
The network reduction and flow assignment detailed in the previous sections is sufficient
to solve the OM effort and flow parameters for any network topology. There are, however,
several additional concepts (Lee, 1999; Lee et al., 2001) that remain applicable using the
SPS technique, and can provide additional vividness in the representation as well as computational benefit. These enhancements deal with very common special cases to avoid the
need to use the Y-∆ transformation, which produces resistances that are not associated
with the original circuit components in a straightforward way.
Any nodes connected by 0 valued resistances can be aggregated as a single supernode,
with E(e) = 0 across any edges subsumed by the supernode. For an edge e = hti , tj i,
where ti , tj ∈ T , and R(e) = 0 a supernode tij is created with the label ti .tj , and the
edge is removed from the active graph. Once generated a supernode can participate in
further SP Star (SPS) reduction, however, upon expansion of the circuit it is not possible
to directly allocate flow to the edges represented by the subsumed edges as is done with
SPS expansions.
The flow within supernode edges can be deduced for unambiguous (non-ladder network)
cases using a qualitative version of the Kirchoff’s current law. With the exception of the
source nodes, for a node t and connected edges with flow F (e):
X

F (e) = 0

{e∈A:e=ht,xi ∨ e=hx,ti}

For supernode edge flow values, f1 3m1 ... fn 3mn , flows F 0 = {fx 3mx : mx = max (m1 ...m2 )}
dominate. For a node with |F | − 1 edges flowing into the node the unassigned flow edge
must be out of the node with a flow equal to the flow magnitude in F 0 towards the node.
The dual exists for flows in the other direction.
Two additional enhancements may be used to improve implementation performance of
the network analyser. Edges e = hti , ti i, ti ∈ T are a loop and can be removed from the
active graph since the loop is by-passed by a zero resistance path, hence is assigned zero
flow upon expansion. Loops typically represent shorted out parts of an electrical circuit for
example.
Degree one nodes, e ∈ {ht, ∅i, h∅, ti}, t ∈ T , are known as dangling edges and are also
removed from the graph together with the connected edge as and when they occur as a
result of other reductions.
425

Snooke & Lee

3. Local Component Models and OM Time
The resistive network calculates power consumption, P , but cannot model any component
that stores energy, En, since En = P × T for time T . Displacement and momentum are the
two domain independent characteristics resulting from the inclusion of time into the model
(Figure 1). Energy, displacement and momentum are fundamentally local characteristics of
components derived from the power variables and time. The representation of time follows
the OM approach described in Section 2.1. For a time t3n and flow f 3n , d3(n+m) = f 3n t3n
defines a displacement d such as quantity of substance.
A FSM representation of local component behaviour has proved sufficient for abstract
behaviours for the failure analysis task. Time is represented by state changes and is explicitly represented by state transitions that capture the qualitative integration of effort or flow
variables. The state of the power network is used to trigger transitions, as are input events
from outside the system (external interactions). The change of state of a component may
cause the structure or resistive parameters of the power network to change, thus triggering
a power network simulation and a sequence of events in other components. This results in
a system of interacting state machines, sharing time as a common variable that sequences
events.
We use a specialised subset of the UML language state chart notation (OMG, 2012)
to describe the FSM models. The model is comprised of a set of states S, events Σ, and
transitions δ = S × Σ → S. In addition s0 ∈ S defines the initial (default) state of the
component. Output actions A may be associated both with e ∈ Σ and also as entry actions
associated with s ∈ S. The UML provides for guard conditions on events and we refine
these conditions to produce e = (t, Tc , Dc , Fc , A). t represents a temporal condition for
the transition as a qualitative OM duration after which the transition can occur once Tc is
satisfied, provided Fc is true after this time. i.e. Tc triggers a transition and Fc and allows
it to complete (fire). Dc is a condition that must be satisfied during the transition, between
the satisfaction of Tc and Fc . All conditions may appeal to values from the network model,
and A may change the resistance values of the network model. States are used to represent
the qualitatively significant values of variables that are derived from the integration of
efforts and flows over time or higher-level states of components. For example capacitor
charged/discharged, or relay activated/deactivated. The following sections describe the
component level simulation and the component model syntax.
3.1 Component Level Simulation
The presence of multiple components in a system results in a set of independent interacting
FSMs. The only global variable at this level is time, and the OM representation provides for
sequencing of component behaviour at different timescales. The definition of OM applied
to time requires that a sequence of (non-cyclic) events in t3(n−1) occur before an event in
t3n .
The processing of events is carried out by maintaining a time-ordered priority list of
all component events with satisfied conditions. All events are ranked by their order of
magnitude time delay periods (referred to as time-slots subsequently) in the following way:
• All events e where Tc is satisfied are added to the end of Q for the priority specified
by t in the event or 0 if unspecified.
426

Qualitative Energy-Flow-Based FMEA

• Candidate events to fire are n ∈ N where N ⊆ Q and t(n) = t3x such that min(x) ∧
N 6= ∅, i.e all the events in the lowest order non-empty timeslot.
• All events n where Fc (n) is satisfied are fired, and events where ¬Fc (n) are removed
from the queue.
• Any events where Dc if not satisfied are removed from the queue.
When |n| = 0 the system has reached a steady state and there are no further changes of
state. When |n| > 1 there is non-determinism in the system. The important question for
FMEA is the longer term impact of the alternative behaviours on the potential worst case
faults and this usually depends on whether the alternative behaviours diverge to significantly
different functional (external) effects or are alternate paths through internal states that
converge on a common state. Very often it is the latter case. For example two relays wired
in parallel may not switch at exactly the same moment, resulting in two possible behaviour
paths and two intermediate states, neither of which is significant in most cases.
In some systems it is possible to make a concurrency assumption that specifies that the
final state reached at the end of time period t3x is independent of the ordering of the events
in the time period. The presence of race conditions or feedback loops between mutually
interacting components does not allow the concurrency assumption, allowing the system to
reach a qualitatively distinct state dependent on the detailed numerical timing of the events.
This is a case where the qualitative representation of time lacks enough detail, and this
qualitative ambiguity will not be detected if the concurrency assumption is falsely applied.
In most systems it is certainly reasonable to assume that all t = 0 events are concurrent,
provided only events associated with the power network have t = 0 because these rarely
have causal cycles, unless specific examples such as bistable logic gate configurations are
created. Usually such ‘memory’ features would be represented at a higher-level as the state
variables (e.g. electronic control unit), leaving the domain-based modelling with non cyclic
causality.
Generally, each possible ordering of the events in each time-slot must be considered
to determine if all branches (eventually) reach the same state and, if this is not the case,
generate a number of alternative behaviour paths. This is achieved by a breadth-first
search to determine converging or cyclic behaviours when the system reaches a state that
is identical to a previously encountered state. There is no assumption made about such
behaviour; it is necessary for the simulation to detect cyclic behaviour to allow termination
and appropriate reporting. Alternatively the simulation must seek additional information
or advice from the engineer, if, for example, no converging state is found within a reasonable
length behaviour path.
3.2 Component Modelling Examples
A graphical notation is used to describe component models, for example Figure 6. State
entry actions are placed inside the rectangle representing the state, and an event syntax
‘[if Tc [during]] event name [after t [Fc ]][/A]’ is used where [x] represents an optional element
x. The keyword during is used to specify that Dc = Tc , and also that Fc = Tc if Fc is
unspecified. If during is omitted then Dc = >. If Fc is omitted then Fc = >. The keyword
427

Snooke & Lee

after is used to specify t. If no after is specified, then the event is at t = 0, i.e. immediate
occurrence.
As an example assume we have the following OM time values F = {mS, Sec, hour, day}
and flow levels T = {low, normal, high}, where Sec and normal are given the OM index of zero.
A tank of a given volume may be defined by an event that changes from the ‘empty’ to ‘full’
state in a given time, e.g. ‘if F (tank inlet)==normal during filling after hour’ provides an implicit
volume of 21, or one order of magnitude bigger than a nominal volume, given the chosen
flow and time qualitative space. By explicitly including the volume in the event conditions
it can be made to represent a number of possible transitions after different durations, for
example, ‘if F (tank inlet)>0 during filling after tank volume/F (tank inlet)’, where tank volume is
defined by the component to have the qualitative volume value with magnitude 21. The
flow condition prevents an event that requires an infinite amount of time to fill the tank (0
is effectively normal2∞ )
Figure 6 shows the two-part model for a tank. The two levels of the model are shown
on the left, with graphic icons on the right that can be used to display simulation results
on any schematic containing an instance of this component. The structure is a single zero
resistance because the tank dissipates no energy during filling. The capacity of the tank is
defined by a local component variable volume, used in combination with the flow, F(tk), to
control the change of state, allowing several different capacity instances of the tank to be
created by parameterising the model.
Behaviour model - (energy)
OM parameters:
volume

empty
tk.level = 0
S(inlet.tk) =S(vent)

Component
schematic graphics
Tank

if F(tk) > 0 and
S(inlet)=="fluid"during

if -F(tk) > 0 and
emptying after volume/F(tk)

filling after volume/F(tk)

vent

Tank

full
tk.level = MAX
S(vent.tk) = S(inlet)

inlet

Structure model - (power)
inlet

tk
R=0

A

vent

Figure 6: Basic 3 tank, with no energy storage

The tank in Figure 6 does not store any energy because the effects of gravity are ignored
and hence there is no increase in potential energy as the tank fills. This aspect could be
included in the modelling by making the tank into a small magnitude effort source when its
level is non-zero. It will then include capacitance as well as displacement (zero stored energy
volume) provided by the duration the flow continues prior to change of state. Atmospheric
pressure is often a significant qualitative value in fluid flow systems and the behaviour of a
system may depend upon the pressure difference between atmospheric pressure and some
428

Qualitative Energy-Flow-Based FMEA

other point. A is defined as the fluid flow domain specialisation of Z and provides a global
node accessible from any component model that represents a connection to the atmosphere.
Examples include a vented tank or leaking pipe.
A different component is used to illustrate a component containing a dependent effort
source. Figure 7 shows a model for a (non-horizontal) pipe that contains either liquid or
a gas (air). A resistance of R(pipe) limits the flow in the pipe and represents a combined
measure of smoothness, length, Reynolds number etc. The pipe model is parameterised; an
ideal pipe is produced if length = 0. A pipe blockage fault model could set R(pipe) = ∞.
The filled pipe represents a small magnitude effort source. The pipe has an additional
resistance to represent the ‘pipe wall’ to facilitate fault modelling by providing a possible
connection to A. The event conditions cause the pipe to fill at a rate inversely proportional
to the flow through it. The rightmost event specifies that the pipe can prime itself, ie. if
fluid is present at the top inlet (only), it will fill without externally imposed flow.
Behaviour model - (energy)
OM parameters
length; width

empty
S(upper.pipe) = "air"
S(lower.pipe) = "air"
E(gravity) = 0
If F(pipe) > 0
and "fluid" in S(lower) during
lower_filling
after length*width/F(tk)

R(pipe) = length/width

If -F(pipe) > 0
and "fluid" in S(upper) during
upper_filling
after length*width/F(tk)

If F(pipe) == 0 and
"fluid" in S(upper) during
prime
after length*width/F(tk)

upper
leak
lower
leak

full
S(upper.pipe) = "fluid"
S(lower.pipe) = "fluid"
>1
E(gravity) = e

upper

Structure model - (power)
pipe
gravity

8

8

A

lower

Figure 7: Partial model for an vertical pipe
Figure 8 illustrates some general ways a component local behaviour may interact with
the structure for several common component types. Real component models will have
additional relationships and constraints associated with qualitative values and device states,
dependent on the details of the behaviour required.
3.3 System Modelling and Circuit Topology
Figure 9 is an artificial system of components similar to those described in the previous
section and will be used to illustrate topological aspects of the modelling and simulation.
In addition a valve with closed (R = ∞) and open (R = 0) states related to an external
position input is included, together with a simple pump that may be activated via ‘activate’
and ‘deactivate’ external events. If the electrical aspects of the system were included then
429

Snooke & Lee

Relay (power controller)

Transformer

Pump

(Gyrator)

output
structure

r

0/∞

coil

behaviour
relationship F(coil)

r

switch

primary

R(switch)

r

secondary

motor

E(coil)
E(secondary)
maintain P(coil) = P(secondary)

F(motor)

pump
input

E(pump)

Figure 8: Local/global relationships for several types of component

the pump model would include an electrical resistance and the events would be triggered
by the level of power (or current flow) in the electrical resistance, thus setting the effort
level of the pump appropriately.
Other qualitative aspects of the pump can easily be included in the model to create
different types of pump. The details of the design of the pump may allow flow when it
is inactive as is the case here, however, it is easy to include a state-controlled resistance
set to ∞ for a pump that does not allow flow when inactive. Similarly, with an additional
condition on the events, the pump can be made non-self priming if no fluid is present at the
input. A bi-directional pump type requires an additional state and selection of the correct
substance input.

Tank A
c=0

Tank C

Tank A

0

c=1
Tank C

>1

0

>0

Pipe D

Pipe A

Valve
Pipe D
Pipe B

>1

Valve

Pipe B

0/

8

Pipe A

A
Pump

P

Pump
Pipe C

Pipe C

>1

Tank B
Tank B
c=0

0

Figure 9: Pumped System

Consider the simulation of the system in Figure 9 from the state at the top of Table
4 with the pump off. The table summarises the changes of state of each component with
430

Qualitative Energy-Flow-Based FMEA

Tank A
full

Tank B
empty

Tank C
empty

Pipe A
empty
→prime, t32
full, ⊕ = u31

Pipe B
empty

Pipe C
empty

→start emptying
part filled
[→emptying, t31 ]
→prime, t32
full
→prime, t32
full, ⊕ = u31
→start filling
part filled
[→filling, t31 ]
Chronological non-determinism in time t31 : 1 Tank A →emptying; 2 Tank B →filling
Choose > 1
[→emptying, t31 ]
empty
→drain, t32
empty, ⊕ = 0
→drain, t32
→drain, t32
empty, ⊕ = 0
delete [→filling, t31 ]
¬(S(inlet)==“fluid”)
Choose > 2
[→filling, t31 ]
full
Fluid present at A
[→emptying, t31 ]
continues as for 1

Table 4: Simulation sequence for system in Figure 9

horizontal lines at the points where the network simulation takes place. The Pump Valve
Pipe D, do not change state maintaining the states off, closed and empty respectively. In
this example there are no resistance changes, only changes to effort sources. Initially the
vertical pipes prime and fill in sequence as fluid is propagated from inflow nodes to outflow
nodes, creating two pressure sources that cause a flow from the upper to lower tank and a
flow of air out of the vent of the lower tank and into the upper tank.
A qualitative ambiguity then results between two events of the same duration, raising
a question which could not be answered without knowing the numerical sizes of the tanks
(and the quantity in each if not full and empty at the start). In this case the simulation was
allowed to try the alternative behaviours. The first is that Tank A becomes empty before
Tank B fills and a steady state is reached with Tank A empty and Tank B is part filled. The
second possibility is that the lower tank becomes filled before the upper one empties. This
causes fluid to reach the A node, and this is built into a model of the atmosphere that
reports an abnormal condition because a substance other than ‘air’ is present. Finally a
steady state is reached where Tank A is empty and Tank B is full.
431

Snooke & Lee

If the pump is switched on and the valve opened, the pump effort source causes flow
up to both top tanks because the pump is a larger magnitude of effort than gravity in the
pipes, however Tank C is an order of size larger and the simulation is essentially the reverse
of previous example. Tank A overfilling is a spurious prediction because the simulation is
not able to reason that all of the fluid originally came from Tank A.
An illustration of flow ambiguity occurs if only Tank A is full and the valve is open
and pump off. The simulation starts as before until Pipe B is filled, then Pipe D becomes
filled from the bottom due to the effort from Pipe A. Pipe D then becomes a pressure source
and is ambiguous with the opposing effort from Pipe A. The ⊕PipeA and ⊕PipeB supply
nodes are both connected to a supernode allowing specification that E(Pipe D.gravity) <
E(Pipe A.gravity) to establish flow direction; Pipe D will fill and there will be a flow into Tank
C, as would occur if Tank C was lower than Tank A. It is large so never becomes full, and
subsequently drains through PipeD.
If E(Pipe D.gravity) > E(Pipe A.gravity), then Pipe D drains itself (producing air at the
valve) whereupon the cycle repeats. The simulation concludes that the pipe is oscillating
between the full and the empty state and its state is undetermined at this modelling resolution. There may be physical oscillation, however, usually this indicates that the system
is in a state that cannot be represented at the level of abstraction being used.
The final possibility is that E(Pipe D.gravity) = E(Pipe A.gravity), (it has filled to the
same height as Tank A) then there is no flow in Pipe D. In all cases there is flow to Tank C
which is either full or part filled at steady state.
These last examples were deliberately chosen to be at the limit of the representation,
if they occur during an FMEA during nominal operation, there is a clear indication that a
more detailed numerical model is required for the state or behaviour; if they occur during
a component fault then there is a signal that a failure mode has been encountered that
requires further detailed investigation.
An equation-based qualitative model of a system of tanks (Dressler, Böttcher, Montag,
& Brinkop, 1993) allows some comparisons to be drawn with our approach. Firstly, the
authors note an important feature also relevant to our approach; for diagnosis (and also
FMEA) a complete behaviour description is not necessary. In fact, a model containing too
much detail is likely to produce many qualitative ambiguities (or branching behaviour) on
unimportant behaviour aspects. These problems have been described in detail (Clancy,
Brajnik, & Kay, 1997) and strategies to avoid the problem such as model revision proposed,
with various tools used to support the revision of (QSIM) models. While this may eventually
result is a desired simulation result, such strategies prove problematic for FMEA due to
the range of behaviour likely to be encountered by the automatic insertion of component
failure models into the system; we need abstracted models that are nevertheless grounded
in the basic physics and are fully compositional.
To avoid such difficulties a simplified numerical model containing the relevant behavioural phenomena, can be used (Dressler et al., 1993) and subsequently refined this
into a qualitative one. During this refinement process, variables and values were mapped
onto qualitative values (0, +, ∞); however, some values were required to be treated differently with various landmarks being identified –namely the height of liquid in the tanks
and the atmospheric pressure. This illustrates the problem using the general equations,
since a numerical equation does not provide landmarks that represent states such as the
432

Qualitative Energy-Flow-Based FMEA

volume of a tank, and global features such as A. We suggest that explicit state identification, with temporal state changes, provides a more vivid qualitative model, within a
broadly applicable domain-independent generalised framework. Other components used by
Dressler, such as a valve, require mappings between an input command and the flow in the
valve, using an expression to set the valve flows (denoted i) at input and output as follows:
valve.status = :close → valve.i1 = valve.i2 = 0. We also note from the model that an open valve
is defined as valve.i1 = −valve.i2 and therefore propagates the flow locally when the valve
is open. This requires a predetermination of cause and effect in the power network, unlike
our global network model. In our approach a state description of the valve (relay in Figure
8 is analogous to an electric valve) that controls the qualitative resistance is more vivid
(and physically correct) than controlling the flow, since the flow depends on pressure, and
pressure cannot be determined locally. If an OM model is used, a partly blocked valve can
be represented by an OM higher resistance than normal. In this situation it is not feasible
to set the flow value based on valve control state, because it depends on the external system.
The A node is important to the circuit-based representation in the pumped system
example, however in general the Z node may also lead to power network ambiguity unless
the following condition is satisfied. The zero node must partition the graph into two disjoint
sets of edges sharing only Z and supply nodes. This topology naturally exists in many
practical fluid flow circuits, because the only connection between the negative (suction)
and positive (pressure) parts of the system are the pump itself and the atmosphere. For
others such as the contrived example in Figure 10, analysis is inherently limited since it
is impossible to determine qualitatively that a leak in Pipe E would cause liquid egress
or air ingress. The ambiguity is indicated because the direction of flow through the leak
(represented by the dashed resistances on the right of the figure) at either end of the pipe
is different with respect to the atmosphere, as shown by the arrows.
The example also illustrates two additional resistances used to represent a ‘leak’ failure
mode because of the qualitative difference in behaviour dependent upon leak position. The
resistance magnitude is used to indicate the severity of the leak where zero would produce
a complete fracture. The arrows on the leaks from PipeC and PipeD demonstrate an unambiguous qualitative behaviour, however PipeE shows conflicting leak flows with respect to A.
This qualitative result is exactly what we would expect in the absence of numerical information, and explicitly signals the limit of the behaviour predictions possible with limited
information about the system. Circuits that have resistances that bridge the Z node also
cannot have effort values assigned relative to Z.

4. Faults and Exaggeration Reasoning
Exaggeration reasoning (Weld, 1988b, 1990, 1988a) provides an alternative qualitative technique for explanation of the worst case effects without the need for differential qualitative
calculus. This form of reasoning is therefore suitable when we do not have detailed system equations, or, as for the global power network, reasoning about the causality is not
performed. A secondary advantage for our purpose is that exaggeration reasoning often
produces more concise explanations. The drawback, however, is that qualitative deviation
analysis is guaranteed sound, whereas exaggeration reasoning can lead to false predictions.
433

Snooke & Lee

med
P

0

Pipe A

Pipe A

Pipe B

Pipe B

P

l

l
Pipe E

Pipe E
0

Pipe C

A

Pipe C

?
Pipe D

m

m
l1 l2

l
m

m

0
Tank A

0

Tank A

Tank B

m

l

Pipe D

m

Tank B

A

Figure 10: A not forming disjoint graphs

For the purpose of FMEA there are two reasons why exaggerating faults is a reasonable
strategy: firstly, the worst case effects are required, which results in fault models at the
extremes of behaviour; secondly, the FMEA is intended as an aid to the engineer and is
therefore externally verified.
Using a simple electrical torch as an example, a corroded battery contact fault may raise
the contact resistance and the torch may dim, however, this resistance increase does not
change the qualitative behaviour. An exaggerated fault would be an OM increase in the
battery contact and the simulation predicts an OM reduction in light output. Of course
a OM reduction of light would likely not be visible, but for the FMEA the effect that
corrosion leads to a reduction in light is reasonable and provides a significant distinction
from a fracture effect where no light or circuit activity is present.
The OM representation provides for exaggerated forms of faults that model significant
differences in behaviour. For example a small leak in a fluid system pipe may allow air to be
sucked in, causing a mixture of fluid and air at the output; a fracture in the same position
may result in no output because the pump fails to operate with only air.
A common approach to reasoning about this qualitatively is to use a qualitative constraint-based deviation model and propagate the deviation through the system. Reasoning
about deviation works well for parameter value changes, but is less good where structure
or state changes occur, such as the air ingress example above where a new system state
occurs due to air in the pump. Each approach has its strengths, and so we might use
absolute/exaggeration reasoning to determine the impact of faults on major states and
operating modes (or regions of linear behaviour), followed by deviations to isolate finer
grained effects such as expected direction of value drift due to a fault.
434

Qualitative Energy-Flow-Based FMEA

5. Generating an FMEA
An automated FMEA is typically based on behaviour simulation for many component faults
and operational scenarios. Producing the FMEA has been previously described in detail
elsewhere (Price, 1998), however, since it is the end goal of the modelling and simulation
effort, we summarise the main steps as follows:
• The system is simulated with no failed components over the expected operating conditions.
• The system is simulated for each of the component failure modes contained in the component type models for every component instance, over the same operating conditions.
Multiple faults may also be considered for high failure rate component combinations.
• The output of the simulation is the qualitative value of all system variables for each
step (state) of the simulation. These are used by a completely separate functional
model that identifies specific (output) behaviour with the state of identified system
functions. Functions can be in one of four states (Achieved, Failed, Unexpected
Behaviour, Inoperative) based on the truth of Boolean trigger and effect expressions
that evaluate simulation output variables. The functional model is lightweight, but
can capture a hierarchy of system functions, including temporal aspects. Full details
of the functional model have been previously presented (Bell, Snooke, & Price, 2007).
• The nominal and failure functional states are compared, and used to indicate at a high
level of abstraction, the highest risk failed system functions and unexpected function
effects associated with each component fault.
• Further presentation, selection, ranking of the function states and risk factors computed from information associated with the function states and component reliability
allows an FMEA to be produced. The structure of a typical automatically generated
FMEA is shown in Figure 17 for our second example, discussed in Section 7.
It is the simulation step of this process that is of interest in this paper and the following
sections provide two example systems as illustrations.

6. Case Study Example: Domestic Heating System
Figure 12 shows a schematic for part of a simple domestic central heating system. The
complete model includes an electrical microcontroller that controls voltage to the pumps
and activators, in addition to a thermal system, we however focus on the fluid flow aspect.
The gas boiler and three way valve had been retrofitted some years after initial installation.
Either the gas boiler or wood burning stove can supply hot water. The radiators are
modelled as R = r, the pipes are ideal (length=0) and the boilers and hot water cylinder as
R = r31 because they are much larger diameter than the radiators. The components also
include a thermal element where flow is considered as the entropy flow rate and temperature
as effort. The connection between these components is a compound connection including the
fluid and thermal circuits. There is not space to detail the complete set of models, however,
Figure 13 shows the horizontal pipe model that includes both the fluid and thermal aspects.
435

Snooke & Lee

The fluid flow element is resistive and propagates the fluid based on flow direction. The
thermal element is represented by a further resistance. The length of the pipe determines
the thermal resistance of the pipe by conduction when there is no flow, and when there is
flow, there is no thermal resistance because the heated substance is transported. Thermal
power is provided by the boiler (by combustion) to create a temperature difference (effort)
between inlet and outlet. This thermal effort is then applied across the radiators which
provide power (heat) to the surrounding air.
A further aspect of the heating system is the interaction between the fluid flow and
temperature networks. For a flow rate such that the returning temperature of the fluid is
substantially higher than the ambient air, series connected radiators each consume a ratio
of the thermal power based on their dimensions. For low flow rates this assumption may
not hold. For example, to represent the situation that a low fluid flow will allow more
thermal power to dissipate, an additional resistance is included in the thermal radiator and
pipe models that allows direct specification based on the low flow rate, such that the outlet
temperature is at or close to the inlet temperature of the boiler (or atmosphere). In Figure
13 this is the low flow resistance, controlled by the flow. Figure 11 shows the effect of a low
fluid flow rate, on a series of radiators. The low flow elements have changed from ∞ to r31 .
The power decreases by OM from the source. The first radiator is hot (if the effort source
is higher because the flow is lower), the next one is an OM cooler, and so on.
valve
8

partial blockage directly limits
fluid flow (not thermal flow) of
pipe

pipe

/0
f>1

0

8

/0

E=u

Gas
Boiler
T

f>2

radiator

pipe
0

pipe
r

0

low_flow

f>2

radiator

radiator
pipe

r

0

low_flow

r>1

r
low_flow

r>1

r>1

pipe
0

Figure 11: Thermal flow circuit in a low fluid flow situation

Consider a small leak in vpipe0 directly below the pump + output. In Figure 12 during
operation of the gas boiler with the valve in the heating position, fluid flows through the
radiators and heat is transferred to the radiators. A small amount of fluid enters the atmosphere, causing a small flow of fluid from the header tank which is replaced by water from
the external water supply.
Now we try a holiday scenario with the simulation results summarised in Table 5. The
external water supply is isolated (in case of freezing). The flow model with the pump off,
derived from the component models and their connections, is in Figure 14. There is an
ambiguity concerning the gravity sources in the pipes p1, p7, p11(names abbreviated) which
oppose p0, p3, p6, p12, p13 at the same flow magnitude. The user can provide that the
436

Qualitative Energy-Flow-Based FMEA

Synchron
motorised
3 way valve

tank level

heating

+

water

-

Header tank

Out
vpipe16

hpipe2
vpipe1

Air Bleed
Valve

hpipe4

vpipe6

loft level

vpipe12

hpipe8

hpipe9

vpipe
13

-

boiler
upper

P

Manual
valve

+
vpipe0
boiler
lower

Vertical pipe
vpipe3

+

vpipe14

vpipe11

Out

Gas Boiler

vpipe7

P
vpipe15

Out

room level

In

In
Radiator

Radiator

Hot Water
Storage

Wood Burning
Stove

Figure 12: Domestic heating system

inactive
8

R(thermal) =
If F(pipe) > 0
and "fluid" in S(in)
flow_normal
actions
S(out) = S(out);

;

If F(pipe) < 0
and "fluid" in S(out)
flow_reverse
actions
S(in) = S(out);

If F(pipe) == 0
or not "fluid" in S(out)
or not "fluid" in S(in)
no_flow

active
R(thermal) = 0;
if F(pipe) < f<2 actions R(low_flow) = F(pipe);
pipe

in

R(pipe) = length/width

leak

T
low_flow
thermal

8

temperature_in

8

A

OM parameters
length; width

out

8

/0

temperature_out

Figure 13: Horizontal pipe with thermal aspect

437

Snooke & Lee

relationship between E(vpipe3.gravity) and E(vpipe11.gravity), is “=” and similarly for the
other two sets (p6=p7, p3=p11, p1=p0+p13+p12). The result is a flow from the header tank
to the leak via p16, p12 and p13. Air then enters p16 and it is no longer an effort source
(Table 5, row 3). In addition a secondary low flow exists via p1 and p0, these flows are in
opposition to each other, however the constraint above allows the p1 source to predominate.
Now there are four sources (p0, p1, p12, p13) and it turns out there are three flow patterns.
Figure 15 shows the flow system with all zero resistances and all dead branches removed,
using a different style arrow to show the contributing flow patterns. The radiator section
has higher resistance and therefore the main flow is due to effort from p12 and p13, drawing
air from A through the boiler, p13, and pump to the leak until air reaches p12 and its effort
becomes zero. The secondary flow pulls air toward the radiators until the cold side pipe
(p11) becomes empty, whereupon there is an ambiguity between effort from p3 and p0. By
specifying that the gravity sources p3>p1, the flow direction reverses and p0 and p1 fill with
air, p11 fills with fluid from p3. Since the pipes all have the same length and diameter there
are several possible behaviours; in the table we provide additional information rather than
generate the alternative behaviours – which would lead to uncertainty as to the fill state of
p0, p1, p2. Now p0, p1, p2, p12, p13 contain air. p3, p11, p6, p7 contain fluid and are opposing
equal sources and the system is stable.
Returning home and turning on the water causes the header tank to refill, however, the
pump does not produce flow because it does not self prime. p16 will prime and create a low
flow from the header tank but will take t31 to propagate to the pump. Hopefully it is not
winter.
In a subsequent step in the FMEA scenario the wood burning stove pump is started and
valve opened. A large flow of water is pulled from the header tank through the three way
valve the ‘wrong way’, air and water mix in the return pipe and are pumped past the air
bleed valve, where the air is removed. The gas boiler and pump can be restarted and the
heating system works correctly - other than a small flow from the header tank to the leak.
The resulting FMEA report will highlight the effect of the failure – no heat output at
any radiator after the holiday scenario step and that this is not as severe as other faults (a
burner fault for example) because it is not permanent.

7. Case Study Example: Aircraft Fuel System
This section describes a fuel system provided by the sponsor of the work. The modelling and
simulation described forms part of a wider objective to automate manually created FMEA
reports used to generate fault effect relationships as input to a Bayesian network based
diagnostic system. This effort is, in turn, part of a recent 32m UK government sponsored
programme seeking to research, develop and validate the necessary technologies for use in
unmanned aerial systems (ASTRAEA, 2009).
The system considered is a fuel system for a twin engine light aircraft shown in Figure 16
and was also available as a physical laboratory simulation on a configurable rig that allowed
validation of the results using fault insertion via additional components such as valves to
represent leaks. The requirements for this system did not include any thermal aspects
and due to the orientation changes of the system only pressure created by the pumps was
required, resulting in very little qualitative ambiguity, other than when valves are placed in
438

Qualitative Energy-Flow-Based FMEA

u

Pump

8

r>1
0

vpipe1_leak
vpipe0

A

u>1

u>1

vpipe1
0
boiler
3 way valve

hpipe4

0
0/
Air Bleed
water
Valve
r<1
0

8

8

0/

heating

8

0

hpipe2
0

vpipe14

A

0

u>1

vpipe6

0
vpipe3

u>1

Hot Water
r<1 Storage

u>1

Radiator
r
r

Radiator

u>1

vpipe15
0

u>1
u>1

vpipe7

vpipe11
0

0
hpipe8
0

hpipe10

0

vpipe12 0
0
r<1

hpipe9

Gas Boiler
u>1

u>1

vpipe16
0

0
vpipe13
u>1

0
u

Pump

Header tank

A

Figure 14: Flow model derived from Figure 12 (joints omitted)

very abnormal configurations. We created the qualitative simulation to model the physical
laboratory model, resulting in the aircraft engines being represented as tanks for example.
The system involves a number of tanks, valves and pumps that allow fuel to be stored and
transferred around the aircraft both to supply engines and to maintain aircraft trim during
flight.
The system is comprised of left and right fuel tanks situated in the aircraft wings (e.g.
OC WT LH) and left and right auxiliary tanks (e.g. TK AT LH). The engines were represented
in the physical test rig that was used for convenience, as tanks EH LH and EH RH. The wing
tanks are connected to the engines via pumps (e.g. CP FL LH) and pressure sensors (e.g.
439

Snooke & Lee

vpipe1_leak
>2
f

>2
f

r>1

u>1

>2
f

vpipe0

>2
f

vpipe13
>2
f

u>1

u>1

vpipe1

>2
f

Radiator
r

>2
f

Pump

Radiator
r
>2 f>2 >2
f
f

r

<1

Gas Boiler

>2
>3 f
f

u>1
>2
f

vpipe12

Header
tank

A

Figure 15: Simplified heating system with leak

PT FL LH) and flow metres (e.g. FT FL LH), which are also modelled as a tank to mimic the
hardware test rig that was used, with excess fuel being returned to the tank from which it
was drawn. Control of the fuel distribution is provided by four three-position valves (e.g.
TVL FL...), which are slaved in pairs for the left and right subsystems. The basic operation
of the system is to supply fuel from the wing tanks to the corresponding engine by setting
all the valves in the normal position. These valves also allow fuel to be supplied from
the left tank to the right engine allowing both engines to be fed from one tank (crossover
operation). It is possible to feed both engines from opposite tanks if desired, although this
is not a normal operating mode. In addition fuel can be transferred between the wing tanks
and from the auxiliary tanks to the wing tanks, although it is not possible to return fuel to
the auxiliary tanks. Failure modes were provided for most component categories including
pipe and tank leaks, pump failures and stuck or leaking valve failures for every component
instance.
A portion of the resulting FMEA output is shown in Figure 17. The textual descriptions are derived from the functional model (Bell et al., 2007; Price, 1998) and provide an
easily understood explanation of the fault effects and risk priority. Functions also interpret exaggerated behaviours into human-friendly phrases, for example if the return line to
a tank has OM lower flow than nominal and the outflow is nominal, the virtual ‘relative
level sensor’ has a value lower than expected. Of course other faults may provide an absolute
qualitative value to the tank level sensor if it for example becomes empty (0) when part
filled (l 30 ) was expected. The consistency of the fully automated FMEA analysis allows
other automated tasks to be performed such as a diagnosability analysis (Snooke, 2009;
Snooke & Price, 2012). The qualitative analysis allows the entire FMEA to be regenerated
following system modification in a matter of seconds and only the differences to the system
effects are presented to the engineer as an incremental FMEA. This allows any unforeseen
implications of design changes to be easily detected.
The first row of Figure 17 describes the effects of a blocked fuel return pipe near the RH
engine in different operating modes of the system called ‘steps’ in the output. The main
440

Qualitative Energy-Flow-Based FMEA

Boiler Radiator1
Pipe0
Pipe1
Pipe3
Pipe11
Pipe12 Pipe13 Pipe16 vpipe1 leak
Pump
on
full
full
full
full
full
full
full
F =f
F =f
F = −f
F = −f
F =f
F =f
F = f F = f 22 F = f 22
⊕=u
⊕ = u31 ⊕ = u31 ⊕ = u31 ⊕ = u31 ⊕ = u31 ⊕ = u31 ⊕ = u31 ⊕ = u31
Close supply to header tank, and switch off boiler and pump.
Flow non-determinism: F = f because E(Pipe3)↔E(Pipe11); F = f because E(Pipe6)↔ E(Pipe7);
Flow non-determinism: F = f in E(Pipe1) ↔ E(Pipe0), E(Pipe12), E(Pipe13);
Resolve> Pipe3=Pipe11; Pipe6=Pipe7; Pipe1=Pipe0+Pipe13+Pipe12
off
empty F = −f 23 F = f 23 F = −f 23 F = f 23 F = f 22 F = f 22 →empty
⊕=0
→empty
⊕=0
→empty
⊕=0
→empty
F =0
F =0
F = f 23
⊕=0
Flow non-determinism: F = −f 23 because E(Pipe3)↔ E(Pipe0);
Resolve> Pipe3>Pipe1;
F = f 23 F = −f 23 F = f 23 F = −f 23
F = −f 23
Event non-determinism: Pipe0→empty ↔ Pipe11→full
Resolve> Pipe0;
→empty
⊕=0
Event non-determinism: Pipe1→empty ↔ Pipe11→full
Resolve> Pipe11;
F = f 23
→full
⊕ = u31
full
empty
full
full
full
empty
empty
empty

Table 5: Extract of simulation for leak fault in heating system 15

functional effect is a RH engine supply malfunction (too much fuel), and the effect that
excess fuel is not returned to the tank. There is also an indication that the RH wing tank
level might be lower than expected; of course this is a theoretical qualitative worst case.
The second row deals with a fracture in the pipe just above the RH pump. The function
effect is again that the engine supply failes in normal operating mode, but additionally we
see that there is no flow at the flow transducer, and the RH wing tank level is higher than
expected, although this is probably not the primary consideration, but could be used to
indicate that fuel could be diverted to the remaining engine. This fault has a different effect
in cross feed mode (fuel taken from the opposite wing tank), as in this case it is the LH
tank level that is higher than expected due to the potential lack of returned fuel. Part of a
RH valve fault is shown and we see that fuel is returned to the wrong tank when the LH
engine is run.
The qualitative simulation has also been used to generate sets of symptoms that relate
qualitative measurements, symptoms and failures (Snooke, 2009) in order to allow a diagnosability analysis to be performed with the aim of assisting sensor selection. The structure
of the system has the greatest influence in these tasks and is exploited in a purely structural
441

Snooke & Lee

approach in related work (Rosich, 2012; Krysander & Frisk, 2008). For complex models
using high order differential equations, dealing with sensing of individual components such
as the valve example (Krysander & Frisk, 2008) the structure approach provides tractability. The system or product wide sensor placement analysis in early design investigation
benefits in addition from the (abnormal) behaviour response between multiple interacting
components in the presence of a fault, and is especially pertinent when only indirect sensing
is possible. The less detailed qualitative approach allows both aspects to be used, while
maintaining tractability. The comprehensive coverage of the system behaviour when linked
with the functional states of the system allows automated fault isolation activities and this
is the subject of another paper by one of the authors (Snooke & Price, 2012). The qualitative analysis is important for such tasks because it captures broad regions of similar system
behaviour at a meaningful level of abstraction.
Multiple faults can be used in the simulation noting that some combinations of fault
may produce qualitative ambiguity, representing critical tipping points of the system. For
example in the fuel system each pump normally operates segments of the system partitioned
by the valves. Multiple valve faults can result in the pumps working in opposition through
a complex pipe topology and it is almost certain that numerical information is required
to determine the actual flows because of the non-linearity abstracted into the qualitative
component states. For an FMEA the resulting answer predicting several possible behaviours
is reasonable, since if the highest risk effects are significant enough, they will be highlighted
to the engineer for detailed analysis.
Single fault FMEA is the norm because of the effort involved in multiple fault effect
determination. For the automated FMEA combinations of faults are feasible, although
some selection of fault combinations is still usually necessary to alleviate the computational
complexity O(N m ) associated with exploring m concurrent faults as has been previously
discussed (Price & Taylor, 1997).

8. Conclusions
Qualitative simulation is a powerful modelling concept that can support a wide range of
reasoning tasks. A range of electrical circuit design analysis tools have been based on this
approach and the authors’ electrical qualitative simulator known as mcirq is now in regular
industrial use.
The structural and behavioural models are compositional and do not encode system
functional information or make assumptions about their use. It is of course necessary to
decide on the range and phenomena to be included in the modelling, and so a library of
models is typically created for a specific application area (e.g. automotive electrical, aircraft
fuel system, general plumbing etc), and this will include the set of qualitative variables of
interest, their (suitably labelled) magnitudes and relevant component failure modes. The
models are then reusable components available for other systems within the application
area. The qualitative nature of the components makes them far less complex than numerical
equivalents and this also makes them reusable within an application area – and possibly
also to other application areas. The models should provide the major behaviours relevant
to the objective of high-level reasoning about potential effects, and not detailed analysis of
system performance, as has been demonstrated in the examples.
442

Qualitative Energy-Flow-Based FMEA

FL2-TR
FL-TR-LH

FL1-TR

FL6-TR

FL5-TR

TJ-TR-LH

TVL-TR-LH

FL7-TR
FL3-TR

CP-TR

TK-AT-LH

CP-AT-LH

TJ-TR-RH

TVL-TR-RH

FL4-TR

TP1-AT-LH

TK-AT-RH

TP2-AT-LH

TP2-AT-RH

DV-AT-LH

DV-AT-RH

TP1-AT-RH
CP-AT-RH

TP3-AT-LH

TP3-AT-RH
RP2-4-RL-LH

OC-WT-LH

RP1-RL-LH

TP1-WT-LH

TP2-WT-LH

DV-WT-LH

F-WT-LH

FL-TR-RH

CK-TR

TJ-AT-LH

RP2-4-RL-RH

FC-RL-RH

RP2-3-RL-LH

TP2-WT-RH

RP2-3-RL-RH

F-WT-RH
TJ-RL-LH
RL-FS-LH

RL-FS-RH

RL2-2-RL-LH

TP1-WT-RH
DV-WT-RH

TJ-RL-RH

RP2-2-RL-RH

TVL-RL-LH

FL1-1-FS-LH

TVL-RL-RH

FL1-1-FS-RH

FL1-2-FS-LH

FL1-2-FS-RH

FL1-3-FS-LH

FL1-3-FS-RH

TVL-FL-LH

TJ-FL-FS-RH

TVL-FL-RH

TP1-FL-LH

TP1-FL-RH

F-FL-LH

F-FL-RH

TP2-FL-LH

TP2-FL-RH

TP4-FL-LH
TP4-FL-RH
CP-FL-LH
CP-FL-RH

RP2-1-RL-LH

TP5-FL-LH
FT-FL-LH

TP5-FL-RH

RP2-1-RL-RH

FT-FL-RH

TP6-FL-LH

TP6-FL-RH

PT-FL-LH

PT-FL-RH

TP7-FL-LH

TP7-FL-RH

IV_FL_LH

TJ-FL-LH

TP3-FL-LH

TP8-FL-LH

IV_FL_RH

TP3-FL-RH

TJ-FL-FS-LH

OC-WT-RH

RP1-RL-RH

TJ-AT-RH

FC-RL-LH

TP8-FL-RH
TJ-FL-RH

Name

BravoFuelSyst em.vdx

Rev

4

Hist ory nst - original version 7/ 1/ 2008
nst - name changes 11/ 1/ 2008
nst - changed engines 25/ 1/ 2008
nst - changed engine ret urn. 8/ 4/ 2008

EH_RH

EH_LH

Bravo
ADTFschematic
Generic
fuel on
system

Figure 16: Example fuel system

443

Snooke & Lee

Figure 17: Portion of FMEA output for fuel system

This paper makes two notable contributions. Firstly, it provides an improved circuit
reasoning algorithm that gives a complete solution for all possible circuit topologies. This is
achieved by solving the problem of non series/parallel reducible circuits. Also the restriction
on single sources is removed and the resultant simulator is called (m2 cirq), for Multiple
source mcirq.
Secondly, the qualitative network modelling method is placed in the context of a modelling ontology based on separating global and local behaviour based on power flow. The
component models for fluid systems involve more aspects than electrical circuits and the
paper introduces several additional fundamental concepts necessary at the global level for
fluid flow modelling, including a distinguished zero node and propagation of substances
through a network. These techniques are illustrated by modelling a range of common fluid
flow components for simulation.
The ability of the QR to make predictions across multiple system states and operating
modes is complementary to other techniques. For example fault tree analysis for diagnosis of
a very similar fuel system to the example presented in Section 7 has been performed (Hurdle,
Bartletta, & Andrews, 2009) and uses pattern recognition to deal with multiple states. The
production of the fault trees and scenario (state) identification is a labour intensive manually
performed process and is described in exactly the same qualitative terms as produced by QR
behaviour prediction such as ‘high flow’. It is therefore likely that accuracy and coverage of
the FTA might be improved with a reduction in effort by using the QR behaviour predictions
instead of manual effort.
444

Qualitative Energy-Flow-Based FMEA

Many software tools have been developed to perform a variety of design analysis for
electrical systems, as mentioned in the introduction to this paper. By substituting the
enhanced simulator into these tools the same analyses can now be performed on other types
of complex topology systems and multiple domain systems where qualitative behaviours
can be used to answer failure mode questions.
8.1 Acknowledgments
This work was supported by Aberystwyth University, the Welsh Assembly Government,
BAE Systems and the DTI ASTRAEA Programme. We also thank the anonymous reviewers
for their helpful comments.

References
ASTRAEA (2009). http://www.projectastraea.co.uk/. ASTRAEA project homepage. Accessed 15 February 2013.
Bell, J., Snooke, N. A., & Price, C. J. (2007). A language for functional interpretation of
model based simulation. Advanced Engineering Informatics, 21 (4), 398–409.
Brown, F. T. (2010). Bond-graph based simulation of thermodynamic models. Journal of
Dynamic Systems, Measurement, and Control, 132 (6), 064501.
Clancy, D. J., Brajnik, G., & Kay, H. (1997). Model revision: Techniques and tools for
analyzing simulation results and revising qualitative models. In In 11th International
Workshop on Qualitative Reasoning, Cortona, Siena Italy.
Clancy, D. J., & Kuipers, B. (1997). Dynamic chatter abstraction: A scalable technique for
avoiding irrelevant distinctions during qualitative simulation. In 11th International
Workshop on Qualitative Reasoning about Physical Systems (QR 97), pp. 67–76.
de Kleer, J. (1984). How circuits work. Artificial Intelligence, 24, 205–280.
Dressler, O., Böttcher, C., Montag, M., & Brinkop, A. (1993). Qualitative and quantitative models in a model-based diagnosis system for ballast tank systems. In International Conference on Fault Diagnosis (TOOLDIAG), Toulouse. Extended report
version available at http://mqm.in.tum.de/ dressler/arm-1-93.ps.
Flores, J. J., & Farley, A. M. (1999). Reasoning about linear circuits: A model-based
approach. Artificial Intelligence Communications, 12 (1-2), 61–77.
Fouché, P., & Kuipers, B. (1990). An assessment of current qualitative simulation techniques. In 4th International Workshop on Qualitative Reasoning about Physical Systems (QR-90), pp. 195–209.
Ghiaus, C. (1999). Fault diagnosis of air conditioning systems based on qualitative bond
graph. Energy and Buildings, 30 (3), 221 – 232.
Hurdle, E., Bartletta, L., & Andrews, J. (2009). Fault diagnostics of dynamic system
operation using a fault tree based method. Reliability Engineering and System Safety,
94 (9), 1371–1380.
445

Snooke & Lee

Krysander, M., & Frisk, E. (2008). Sensor placement for fault diagnosis. Systems, Man
and Cybernetics, Part A: Systems and Humans, IEEE Transactions on, 38 (6), 1398
–1410.
Kuipers, B. J. (1986). Qualitative simulation. Artificial Intelligence, 29, 289–338.
Lee, M. H. (1999). Qualitative circuit models in failure analysis reasoning. Artificial Intellligence, 111, 239–276.
Lee, M. H. (2000a). Many-valued logic and qualitative modelling of electrical circuits. In
Proceedings 14th International Workshop on Qualitative Reasoning, (QR-2000).
Lee, M. H. (2000b). Qualitative modelling of linear networks in engineering applications.
In Proceedings 14th European Conference on Artificial Intelligence ECAI 2000, pp.
161–165, Berlin.
Lee, M. H., Bell, J., & Coghill, G. M. (2001). Ambiguities and deviations in qualitative circuit analysis. In Proceedings 15th International Workshop on Qualitative Reasoning,
QR ’01, pp. 51–58.
Mauss, J., & Neumann, B. (1996). Qualitative reasoning about electrical circuits using
series-parallel-star trees. In Proceedings 10th International Workshop on Qualitative
Reasoning, QR-96, pp. 147–153.
Mosterman, P. J., & Biswas, G. (2000). A comprehensive methodology for building hybrid
models of physical systems. Artificial Intelligence, 121, 171 – 209.
OMG (2012). Documents Associated With Unified Modeling Language (UML) Version 2.5.
Object Mangement Group, http://www.omg.org/spec/UML/2.5/Beta1/PDF.
Paynter, H. M. (1961). Analysis and Design of Engineering Systems. MIT Press.
Price, C. J. (1998). Function-directed electrical design analysis. Artificial Intelligence in
Engineering, 12 (4), 445–456.
Price, C. J., Snooke, N. A., & Landry, J. (1996). Automated sneak identification. Engineering Applications of Artificial Intelligence, 9 (4), 423–427.
Price, C. J., Snooke, N. A., & Lewis, S. D. (2006). A layered approach to automated
electrical safety analysis in automotive environments. Computers in Industry, 57 (5),
451–461.
Price, C. J., & Taylor, N. S. (1997). Multiple fault diagnosis from FMEA. In Proc. The
Ninth Conference on Innovative Applications of Artificial Intelligence (IAAI 97), pp.
1052–1057. AAAI.
Price, C. J., & Struss, P. (2004). Model-based systems in the automotive industry. AI
Magazine, 24 (4), 17–34.
Price, C. J., Wilson, M. S., Timmis, J., & Cain, C. (1996). Generating fault trees from
FMEA. In 7th International Workshop on Principles of Diagnosis, pp. 183–190, Val
Morin, Canada.
Raiman, O. (1991). Order of magnitude reasoning. Artificial Intelligence, 51, 11–38.
446

Qualitative Energy-Flow-Based FMEA

Rosich, A. (2012). Sensor placement for fault detection and isolation based on structural
models. In 8th IFAC Symposium on Fault Detection, Supervision and Safety of Technical Processes (SAFEPROCESS), pp. 391–396. IFAC.
Samantaray, & Ould (2011). Bond Graph Modelling of Engineering Systems, chap. Bond
Graph Model-Based Fault Diagnosis. Springer. ISBN 978-1-4419-9368-7.
Savakoor, D. S., Bowles, J. B., & Bonnell, R. D. (1993). Combining sneak circuit analysis and failure modes and effects analysis. In Proceedings Annual Reliability and
Maintainability Symposium, pp. 199–205.
Skorstad, G. (1992a). Finding stable causal interpretations of equations. In Faltings, &
Struss (Eds.), Recent advances in qualitative physics, pp. 399–413. MIT Press.
Skorstad, G. (1992b). Towards a qualitative lagrangian theory of fluid flow. In Proceedings of
the tenth national conference on Artificial intelligence, AAAI’92, pp. 691–696. AAAI
Press.
Snooke, N. A. (1999). Simulating electrical devices with complex behaviour. AI Communications, 12 (1,2), 45–58.
Snooke, N. A. (2007). M2 CIRQ: Qualitative fluid flow modelling for aerospace fmea applications. In Proceedings 21st international workshop on qualitative reasoning, pp.
161–169.
Snooke, N. A. (2009).
An automated failure modes and effects analysis based
visual matrix approach to sensor selection and diagnosability assessment.
In online proc. Prognostics and Health Management Conference (PHM09),
http://www.phmsociety.org/references/proceedings. PHM Society.
Snooke, N., & Price, C. (2012). Automated FMEA based diagnostic symptom generation.
Advanced Engineering Informatics, 26 (4), 870 – 888.
Struss, P. (2003). The discrete charm of diagnosis based on continuous models. In IFAC
Safeprocess ’03. International Federation of Automatic Control.
Struss, P., Malik, A., & Sachenbacher, M. (1995). Qualitative modelling is the key. In
Workshop Notes of 6th International Workshop on Principles of Diagnosis (DX-95),
pp. 99–106.
Sussman, G. J., & Steele Jr, G. L. (1980). CONSTRAINTS: a language for expressing
almost-hierarchical descriptions. Artificial Intelligence, 14, 1–39.
Travé-Massuyès, L., Ironi, L., & Dague, P. (2004). Mathematical foundations of qualitative
reasoning. AI Magazine., 24 (4), 91–106.
Tsai, J. J.-H., & Gero, J. S. (2010). A qualitative approach to an energy-based unified
representation for building design. Automation in Construction, 19 (1).
Weld, D. S. (1988a). Choices for comparative analysis: DQ analysis or exaggeration?. Artificial Intelligence in Engineering, 3 (3), 174 – 180.
Weld, D. S. (1988b). Comparative analysis. Artificial Intelligence, 36 (3), 333 – 373.
Weld, D. S. (1990). Exaggeration. Artificial Intelligence, 43 (3), 311 – 368.

447

Journal of Artificial Intelligence Research 46 (2013) 607-650

Submitted 9/12; published 4/13

Efficient Computation of the Shapley Value
for Game-Theoretic Network Centrality
Tomasz P. Michalak

Tomasz.Michalak@cs.ox.ac.uk

Department of Computer Science, University of Oxford
OX1 3QD, UK
Institute of Informatics, University of Warsaw
02-097 Warsaw, Poland

Karthik .V. Aadithya

Aadithya@eecs.berkeley.edu

Department of Electrical Engineering and Computer Sciences
University of California
Berkeley, CA 94720-4505, United States, USA

Piotr L. Szczepański

P.Szczepanski@stud.elka.pw.edu.pl

Institute of Informatics
Warsaw University of Technology
00-661 Warsaw, Poland

Balaraman Ravindran

Ravi@cse.iitm.ac.in

Computer Science and Engineering
Indian Institute of Technology Madras
Chennai, 600 036, India

Nicholas R. Jennings

NRJ@ecs.soton.ac.uk

School of Electronics and Computer Science
University of Southampton
SO17 1BJ Southampton, UK

Abstract
The Shapley value—probably the most important normative payoff division scheme in coalitional games—has recently been advocated as a useful measure of centrality in networks.
However, although this approach has a variety of real-world applications (including social
and organisational networks, biological networks and communication networks), its computational properties have not been widely studied. To date, the only practicable approach
to compute Shapley value-based centrality has been via Monte Carlo simulations which
are computationally expensive and not guaranteed to give an exact answer. Against this
background, this paper presents the first study of the computational aspects of the Shapley
value for network centralities. Specifically, we develop exact analytical formulae for Shapley value-based centrality in both weighted and unweighted networks and develop efficient
(polynomial time) and exact algorithms based on them. We empirically evaluate these algorithms on two real-life examples (an infrastructure network representing the topology of
the Western States Power Grid and a collaboration network from the field of astrophysics)
and demonstrate that they deliver significant speedups over the Monte Carlo approach. For
instance, in the case of unweighted networks our algorithms are able to return the exact
solution about 1600 times faster than the Monte Carlo approximation, even if we allow for
a generous 10% error margin for the latter method.
c
2013
AI Access Foundation. All rights reserved.

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

1. Introduction
In many network applications, it is important to determine which nodes and edges are more
critical than others. Classic examples include identifying the most important hubs in a road
network (Schultes & Sanders, 2007), the most critical functional entities in a protein network
(Jeong, Mason, Barabasi, & Oltvai, 2001), or the most influential people in a social network
(Kempe, Kleinberg, & Tardos, 2003). Consequently, the concept of centrality, which aims
to quantify the importance of individual nodes/edges, has been extensively studied in
the literature (Koschützki, Lehmann, Peeters, Richter, Tenfelde-Podehl, & Zlotowski, 2005;
Brandes & Erlebach, 2005).
v6

v4
v1
v5

v9

v7

v2
v10

v8
v12

v3

v11

v13

Figure 1: Sample network of 13 nodes
Generally speaking, centrality analysis aims to create a consistent ranking of nodes within a
network. To this end, centrality measures assign a score to each node that in some way corresponds to the importance of that node given a particular application. Since “importance”
depends on the context of the problem at hand, many different centrality measures have
been developed. Three of the most well-known and widely applied are: degree centrality,
closeness centrality and betweenness centrality.1 In this paper, we refer to these measures
as conventional/standard centrality. Degree centrality, in brief, quantifies the power of a
node by its degree, i.e., by the number of its adjacent edges. For instance, in the sample
network in Figure 1, nodes v1 and v2 have degree 5 and, if judged by degree centrality, these
are the most important nodes within the entire network. Conversely, closeness centrality
focuses on distances among nodes and gives high value to the nodes that are close to all
other nodes. With this measure, node v8 in Figure 1 is ranked top. The last measure—
betweenness centrality—considers shortest paths (i.e., paths that use the minimal number of
links) between any two nodes in the network. The more shortest paths the node belongs to,
the more important it is. With this measure, v2 in Figure 1 is more important than all the
other nodes (including v1 and v8 , which are chosen by other measures as the most important
node). Clearly, all these measures expose different characteristics of a node. Consider, for
instance, an epidemiology application, where the aim is to identify those people (i.e., nodes)
in the social network who have the biggest influence on the spread of the disease and should
become a focal point of any prevention or emergency measures. Here, degree centrality
1. Koschützki et al. (2005) and Brandes and Erlebach (2005) give a good overview of these and other
centrality measures.

608

Computation of the Shapley Value for Game-Theoretic Network Centrality

ranks top nodes with the biggest immediate sphere of influence—their infection would lead
to the highest number of adjacent nodes being exposed to the disease. On the other hand,
closeness centrality identifies those nodes whose infection would lead to the fastest spread
of the disease throughout the society. Finally, betweenness centrality reveals the nodes that
play a crucial role in passing the disease from one person in a network to another.2
The common feature of all the aforementioned standard measures is that they assess the
importance of a node by focusing only on the role that a node plays by itself. However,
in many applications such an approach is inadequate because of synergies that may occur
if the functioning of nodes is considered in groups. Referring again to Figure 1 and our
epidemiology example, a vaccination of individual node v6 (or v7 or v8 ) would not prevent
the spread of the disease from the left to the right part of the network (or vice versa).
However, the simultaneous vaccination of v6 , v7 and v8 would achieve this goal. Thus, in
this particular context, nodes v6 , v7 and v8 do not play any significant role individually,
but together they do. To quantify the importance of such groups of nodes, the notion of
group centrality was introduced by Everett and Borgatti (1999). Intuitively, group centrality
works broadly the same way as standard centrality, but now the focus is on the functioning
of a given group of nodes, rather than individual nodes. For instance, in Figure 1, the group
degree centrality of {v1 , v2 } is 7 as they both have 7 distinct adjacent nodes.
Although the concept of group centrality addresses the issue of synergy between the functions
is played by various nodes, it suffers from a fundamental deficiency. It focuses on particular,
a priori determined, groups of nodes and it is not clear how to construct a consistent ranking
of individual nodes using such group results. Specifically, should the nodes from the most
valuable group be ranked top? Or should the most important nodes be those which belong
to the group with the highest average value per node? Or should we focus on the nodes
which contribute most to every coalition they join? In fact, there are very many possibilities
to choose from.
A framework that does address this issue is the game theoretic network centrality measure. In
more detail, it allows the consistent ranking of individual nodes to be computed in a way that
accounts for various possible synergies occurring within possible groups of nodes (Grofman
& Owen, 1982; Gómez, González-Arangüena, Manuel, Owen, Del Pozo, & Tejada, 2003;
Suri & Narahari, 2008). Specifically, the concept builds upon cooperative game theory—a
part of game theory in which agents (or players) are allowed to form coalitions in order to
increase their payoffs in the game. Now, one of the fundamental questions in cooperative
game theory is how to distribute the surplus achieved by cooperation among the agents.
To this end, Shapley (1953) proposed to remunerate agents with payoffs that correspond to
their individual marginal contributions to the game. In more detail, for a given agent, such
an individual marginal contribution is measured as the weighted average marginal increase
in the payoff of any coalition that this agent could potentially join. Shapley famously proved
that his concept—known since then as the Shapley value—is the only division scheme that
meets certain desirable normative properties.3 Given this, the key idea of the game theoretic
network centrality is to define a cooperative game over a network in which agents are the
nodes, coalitions are the groups of nodes, and payoffs of coalitions are defined so as to meet
2. For the differences in interpretation of standard centralities see the work of Borgatti and Everett (2006).
3. See Section 3 for more details.

609

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

requirements of a given application. This means that the Shapley value of each agent in such
a game can then be interpreted as a centrality measure because it represents the average
marginal contribution made by each node to every coalition of the other nodes.4 In other
words, the Shapley value answers the question of how to construct a consistent ranking of
individual nodes once groups of nodes have been evaluated.
In more detail, the Shapley value-based approach to centrality is, on one hand, much more
sophisticated than the conventional measures, as it accounts for any group of nodes from
which the Shapley value derives a consistent ranking of individual nodes. On the other
hand, it confers a high degree of flexibility as the cooperative game over a network can be
defined in a variety of ways. This means that many different versions of Shapley value-based
centrality can be developed depending on the particular application under consideration, as
well as on the features of the network to be analyzed. As a prominent example, in which a
specific Shapley value-based centrality measure is developed that is crafted to a particular
application, consider the work of Suri and Narahari (2008) who study the problem of selecting
the top-k nodes in a social network. This problem is relevant in all those applications where
the key issue is to choose a group of nodes that together have the biggest influence on the
entire network. These include, for example, the analysis of co-authorship networks, the
diffusion of information, and viral marketing. As a new approach to this problem, Suri
and Narahari define a cooperative game in which the value of any group of nodes is equal
to the number of nodes within, and adjacent to, the group. In other words, it is assumed
that the agents’ sphere of influence reaches the immediate neighbors of the group. Whereas
the definition of the game is a natural extension of the (group) degree centrality discussed
above, the Shapley value of nodes in this game constitutes a new centrality metric that is,
arguably, qualitatively better than standard degree centrality as far as the node’s influence is
concerned. The intuition behind it is visible even in our small network in Figure 1. In terms
of influence, node v1 is more important than v2 , because it is the only node that is connected
to v4 and v5 . Without v1 it is impossible to influence v4 and v5 , while each neighbor of v2 is
accessible from some other node. Thus, unlike standard degree centrality, which evaluates
v1 and v2 equally, the centrality based on the Shapley value of the game defined by Suri and
Narahari recognizes this difference in influence and assigns a higher value to v1 than to v2 .
Unfortunately, despite the advantages of Shapley value-based centrality over conventional
approaches, efficient algorithms to compute it have not yet been developed. Indeed, given
a network G(V, E), where V is the set of nodes and E the set of edges, using the original
Shapley value formula involves computing the marginal contribution of every node to every
coalition which is O(2|V | ). Such an exponential computation is clearly prohibitive for bigger
networks (of, e.g, 100 or 1000 nodes). For such networks, the only feasible approach currently
outlined in the literature is Monte-Carlo sampling (e.g., Suri & Narahari, 2008; Castro,
Gomez, & Tejada, 2009). However, this method is not only inexact, but can be also very
time-consuming. For instance, as shown in our simulations, for a weighted network of about
16,000 nodes and about 120,000 edges, the Monte Carlo approach has to iterate 300, 000
4. We note that other division schemes or power indices from cooperative game theory, such as Banzhaf
power index (Banzhaf, 1965), could also be used as centrality measures (see, for instance, the discussion
in the work by Grofman & Owen, 1982). However, like most of the literature, we focus on the Shapley
value due to its desirable properties.

610

Computation of the Shapley Value for Game-Theoretic Network Centrality

b)

a)
v3

v5

v4

1
10

v1
v4

v2
v6

v1
v5

1

3

v6

2

v7

1
v2

v3

Figure 2: Sample unweighted and weighted networks of 6 and 7 nodes, respectively.
times through the entire network to produce the approximation of the Shapley value with
a 40% error margin.5 Moreover, exponentially more iterations are needed to further reduce
this error margin.
Against this background, we develop polynomial-time algorithms to compute Shapley valuebased centrality. Specifically, we focus on five underlying games defined over a network; those
games extend, in various directions, standard notions of degree and closeness centrality. As
our starting point, we consider the game defined by Suri and Narahari and propose an
exact, linear-time algorithm to compute the corresponding Shapley value-based centrality.
We denote this game by g1 . We then analyse the computational properties of four other
games defined over networks. We denote them g2 , g3 , g4 , and g5 , respectively. While each
of these games captures a different flavor of centrality, they all, similarly to the game of
Suri and Narahari, embrace one fundamental centrality idea: given a group of nodes C,
the function that defines the value of C in the game must somehow quantify the sphere of
influence of C over other nodes in the network. In particular:
g2 In this game the value of coalition C is a function of its own size and of the number
of nodes that are immediately reachable in at least k different ways from C. This
game is inspired by Bikhchandani, Hirshleifer, and Welch (1992) and is an instance of
the general threshold model introduced by Kempe, Kleinberg, and Tardos (2005). It
has a natural interpretation: an agent “becomes influenced” (with ideas, information,
marketing message, etc.) only if at least k of his neighbors have already become
influenced. For instance, given k = 2, the value of coalition {v1 , v2 } in Figure 2a is
4 as the coalition is of size 2 and there are two neighbors with no less than 2 edges
adjacent to this coalition.
g3 This game concerns weighted graphs (unlike g1 and g2 ). Here, the value of coalition C
depends on its size and on the set of all nodes within a cutoff distance of C, as measured
by the shortest path lengths on the weighted graph. For example, in Figure 2b, if the
cutoff is set to 8 then coalition {v2 } has value 4 as it is able to influence 3 nodes v3 ,
v6 , and v7 that are not further than 8 away from {v2 }. The cutoff distance should be
interpreted here as a “radius” of the sphere of influence of any coalition.
5. See Section 5 for the exact definition of the error margin.

611

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Game Graph
Value of a coalition C, i.e., ν(C)
Complexity
Accuracy
g1
U W ν(C) is the number of nodes in C and
O(|V | + |E|)
exact
those immediately reachable from C
g2
U W ν(C) is the number of nodes in C and
O(|V | + |E|)
exact
those immediately reachable from C,
but via at least k different edges
g3
W ν(C) is the number of nodes in C and O(|V ||E| + |V |2 log|V |) exact
those not further than dcutoff away
g4
W ν(C) is the sum of f (.)’s — the nonO(|V ||E| + |V |2 log|V |) exact
-increasing functions of the distance
between C and other nodes
g5
W ν(C) is the number of nodes in C and
O(|V ||E|)
approx.
those directly connected to C via edges
∼ 5-10%
which sum of weights exceeds Wcutoff
Table 1: Games considered in this paper and our results (U W denotes unweighted graphs
and W weighted).
g4 This game generalizes g3 by allowing the value of C to be specified by its size and an
arbitrary non-increasing function f (.) of the distance between C and the other nodes
in the network. For instance, the value of {v1 , v3 } in Figure 2b when our function is
1
1
f (d) = 1+d
is 2 × 1 + 3 × 21 + 1 × 31 + 1 × 14 = 4 12
. The intuition here is that the
coalition has more influence on closer nodes than on those further away—a property
that cannot be expressed with the standard closeness centrality. Thus, g4 can be seen
as an extension of closeness centrality.
g5 The last game is an extension of g2 to the case of weighted networks. Here, the value
of C depends on the adjacent nodes that are connected to the coalition with weighted
edges whose sum exceeds a given threshold wcutof f (recall that in g2 this threshold is
defined simply by the integer k). Whereas in g3 and g4 weights on edges are interpreted
as distance, in g5 they should be interpreted as a power of influence. For example, in
Figure 2b, when the threshold for each vertex is 5, the value of coalition {v1 , v3 } is 3
because this coalition of size two has only enough power to influence one additional
node v2 .
The computation of the Shapley value for each of the above five games (see Table 1 for an
overview) will be the main focus of the paper. These Shapley values are extensions of either
degree or closeness centrality metrics and their applications are all those settings in which
the influence of nodes on other nodes in the network has to be evaluated. Our results can
be summarized as follows:

• We demonstrate that it is possible to exactly and efficiently compute a number of
Shapley value-based network centrality measures. Our methods take advantage of both
612

Computation of the Shapley Value for Game-Theoretic Network Centrality

the network structure, as well as the specifics of the underlying game defined over a
network.
• For the first four games, we derive closed-form expressions for the Shapley values.
Based on these, we provide exact linear and polynomial-time algorithms that efficiently
compute the Shapley values, i.e., without the need to enumerate all possible coalitions.
Specifically, our algorithms run in O(|V | + |E|) for g1 and g2 and in O(|V ||E| +
|V |2 log|V |) for g3 and g4 . Furthermore, for the fifth measure of centrality, we develop a
closed-form polynomial time computable Shapley value approximation. This algorithm
has running time O(|V ||E|) and our experiments show that its approximation error
is about 5% for large networks. The summary of our algorithms’ performance can be
found in Table 1.
• We evaluate our algorithms on two real-life examples: an infrastructure network representing the topology of the Western States Power Grid and a collaboration network
from the field of astrophysics. The results show that our algorithms deliver significant
speedups over Monte Carlo simulations. For instance, given the unweighted network of
Western States Power Grid, our algorithms return the exact Shapley value for g1 and
g2 about 1600 times faster than the Monte Carlo method returns an approximation
with the 10% error margin.
The remainder of the paper is organized as follows. In Section 2 we discuss related work.
Notation and preliminary definitions are presented in Section 3. In Section 4 we analyse the
five types of centrality-related coalitional games and propose polynomial time Shapley value
algorithms for all of them. The results of numerical simulations are presented in Section 5
(with some details on the simulation setup presented in Appendix A). Conclusions and future
work follow. Finally, Appendix B provides a summary of the key notational conventions.

2. Related Literature
The issue of centrality is one of the fundamental research directions in the network analysis
literature. In particular, Freeman (1979) was the first to formalise the notion of centrality by
presenting the conventional centrality measures: degree, closeness and betweenness. Many
authors have subsequently worked on developing new centrality measures, or refining existing
ones (e.g., Bonacich, 1972; Noh & Rieger, 2004; Stephenson & Zelen, 1989), and developing
algorithms for efficient centrality computation (e.g., Brandes, 2001; Eppstein & Wang, 2001).
In this context, Grofman and Owen (1982) were the first to apply game theory to the topic
of centrality, where they focused on the Banzhaf power index (Banzhaf, 1965). In a followup work, Gómez et al. (2003) combined Myerson’s (1977) idea of graph-restricted games
(in which each feasible coalition is induced by a subgraph of a graph) with the concept of
centrality and proposed new Shapley value-based network centrality measures. In contrast
to Gómez et al., Suri and Narahari (2008, 2010) assumed all coalitions to be feasible, which
is the approach we also adopt in this paper.
The fundamental problem with the conventional models of coalitional games, i.e., their exponential complexity in the number of agents, that we tackle in this paper, has been also
613

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

1

2
v1

1
v2

4
3
1
v3
Figure 3: The Induced-subgraph representation of a sample coalitional game of 3 players. In
this game, values of coalitions {v1 }, {v2 }, {v3 }, {v1 , v2 }, {v1 , v3 }, {v2 , v3 }, and {v1 , v2 , v3}
are 1, 1, 1, 1+1+2, 1+1+3, 1+1+4, and 1+1+1+2+3+4, respectively.
studied in the literature on algorithmic aspects of coalitional games. Indeed, since the seminal work of Deng and Papadimitriou (1994), this issue has received considerable attention
from computer scientists. Specifically, as an alternative to the straightforward (but exponential) listing of all possible coalitions, a number of authors have proposed more efficient
representations for coalitional games. Such representations fall into two main categories
(Wooldridge & Dunne, 2006):
• Those that give the characteristic function a specific interpretation in terms of combinatorial structures such as graphs. This is the approach adopted by, for instance,
Deng and Papadimitriou (1994), Greco, Malizia, Palopoli, and Scarcello (2009), and
Wooldridge and Dunne (2006) and its advantage is that the ensuing representation is
always guaranteed to be succinct. However, the disadvantage is that it is not always
fully expressive, i.e., it cannot represent all coalitional games.
• Those that try to find a succinct, but still fully expressive, representation. This is, for
instance, the approach adopted by Conitzer and Sandholm (2004), Ieong and Shoham
(2005), and Elkind, Goldberg, Goldberg, and Wooldridge (2009). These representations are more general in that they completely capture all coalitional games of interest,
although they are not always guaranteed to be succinct.
Unfortunately, even for some succinctly representable games, computing the Shapley value
has been shown to be NP-Hard (or even worse, #P-Complete) for many domains, including weighted voting games (Deng & Papadimitriou, 1994), threshold network flow games
(Bachrach & Rosenschein, 2009) and minimum spanning tree games (Nagamochi, Zeng,
Kabutoya, & Ibaraki, 1997). Similarly, Aziz, Lachish, Paterson, and Savani (2009a) obtained negative results for a related problem of computing the Shapley-Shubik power index
for the spanning connectivity games that are based on undirected, unweighted multigraphs.
Also, Bachrach, Rosenschein, and Porat (2008b) showed that the computation of the Banzhaf
index for connectivity games, in which agents own vertices and control adjacent edges and
aim to become connected to the certain set of primary edges, is #P-Complete.
Fortunately, some positive results have also been discovered. Probably the most known
among these are due to Deng and Papadimitriou (1994) and Ieong and Shoham (2005). In
614

Computation of the Shapley Value for Game-Theoretic Network Centrality

more detail, Deng and Papadimitriou proposed a representation based on weighted graphs,
where a node is interpreted as an agent, and the weight of an edge is interpreted as the value
of cooperation between the two agents that are connected by this edge.6 The value of any
coalition is then defined as the sum of weights of all its internal edges, or, in other words,
the weights of edges belonging to a subgraph induced by members of the coalition. A threeplayer example of this formalism, called the induced-subgraph representation, can be found
in Figure 3. The downside of this representation is that it is not fully expressive. However,
the upside is that, for games that can be formalised as weighted graphs, this representation
is always concise. Furthermore, it allows the Shapley value to be computed in time linear
in the number of players. Specifically, in this case, the Shapley value is given by the the
following formula:

Shapley Value(vi ) = vi ’s self-loop weight +
vj

weight of edge from vi to vj
.
2
∈neighbours of v
X

(1)

i

Ieong and Shoham (2005) developed a representation consisting of a finite set of logical rules
of the following form: Boolean Expression → Real Number, with agents being the atomic
boolean variables. In this representation, the value of a coalition is equal to the sum of
the right sides of those rules whose left sides are satisfied by the coalition. This representation, called marginal contribution networks (or MC-Nets for short) is (i) fully expressive
(i.e., it can be used to model any game), (ii) exponentially more concise for some games,
and most importantly, (iii) allows the Shapley value to be computed in time linear in the
size of the representation, provided the boolean expressions in all rules are conjunctions
of (either positive or negative) atomic literals. In MC-Nets, the rules have an interesting
game-theoretic interpretation, as each rule directly specifies an incremental marginal contribution made by the agents featured in that rule. Now, using the additivity axiom met
by the Shapley value, it is possible to consider every rule as a separate “simple” game, then
using other axioms straightforwardly compute the Shapley value for this “simple” game,
and, finally, sum up the results for all “simple” games to obtained the Shapley value. Building on this, Elkind et al. (2009) developed extensions of MC-Nets to more sophisticated
(read-once) boolean expressions, while Michalak, Marciniak, Samotulski, Rahwan, McBurney, Wooldridge, and Jennings (2010a), Michalak, Rahwan, Marciniak, Szamotulski, and
Jennings (2010b) developed generalizations to coalitional games with externalities. Another
recently proposed representation formalism for coalitional games that allows for polynomial
calculations of the Shapley value are decision diagrams (Bolus, 2011; Aadithya, Michalak, &
Jennings, 2011; Sakurai, Ueda, Iwasaki, Minato, & Yokoo, 2011). Now, while MC-Nets offer
a fully-expressive representation that works for arbitrary coalitional games, it is possible to
speed up the Shapley value computation by focusing on specific (not necessarily fully expressive) classes of games. One particular class of games that has been investigated in detail is
weighted voting, for which both approximate (but strictly polynomial) (Fatima, Wooldridge,
& Jennings, 2007) and exact (but pseudo-polynomial) algorithms (Mann & Shapley, 1962;
Matsui & Matsui, 2000) have been proposed. Chalkiadakis, Elkind, and Wooldridge (2011)
provided a comprehensive discussion of this literature.
6. Also self-loops are allowed.

615

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Whereas the choice of representation has been the foremost consideration for efficient Shapley value computation in the context of conventional coalitional games, in this paper, we
face a rather different set of challenges:
• Unlike conventional coalitional games, conciseness is usually not an issue in the networks context. This is because the games that aim to capture network centrality
notions are completely specified by (a) the underlying network compactly represented
as a graph, and (b) a concise closed-form characteristic function expression for evaluating coalition values (see the next section for an example). Rather, the issue here
is that the exact specification for the characteristic function is dictated not by computational considerations, but by the real-world application of game theoretic network
centrality. In other words, the choice of representation for Shapley value computation
is already fixed by the centrality under consideration.
• Because the games in this paper are designed to reflect network centrality, the characteristic function definition often depends in a highly non-trivial way on the underlying
graph structure. Specifically, the value assigned by the characteristic function to each
subset of nodes depends not just on the subgraph induced by those nodes, but also on
the relationship between that subgraph and the rest of the network. For example, the
value assigned to a coalition of nodes may be based on the shortest path lengths to
nodes outside the coalition, or it may depend on the relationship between the coalition
and its neighbors.
Therefore, the specific challenge we tackle is to efficiently compute the Shapley value, given
a network and a game defined over it, where coalition values for this game are given by a
closed-form expression that depends non-trivially on the network. The key question here
is how to take advantage of (a) the network structure, and (b) the functional form for
the coalition values, so as to compute Shapley values efficiently, i.e., without the need to
enumerate all possible coalitions.
Finally, we conclude this section by mentioning that the Shapley value or other solution concepts from game theory have been applied to other network-related problems. For instance,
the application of the Shapley value (and the Nucleolus) to the problem of cost allocation
in the electric market transmission system was considered by Zolezzi and Rudnick (2002),
though the computational aspects were not discussed. The problem of maximizing the probability of hitting a strategically chosen hidden virtual network by placing a wiretap on a
single link of a communication network was analysed by Aziz, Lachish, Paterson, and Savani
(2009b). This problem can be viewed as a two-player win-lose (zero-sum) game, a wiretap
game. The authors not only provide polynomial-time computational results for this game,
but also show that one of the (key) strategies is the nucleolus of the simple cooperative
spanning connectivity game (Aziz et al., 2009a) mentioned above.

3. Preliminaries and Notation
In this section we formally introduce the basic concepts from graph theory and cooperative
game theory used throughout the paper. We then look more closely into a sample coalitional
616

Computation of the Shapley Value for Game-Theoretic Network Centrality

game defined over a network and into how the Shapley value of this game can be used as a
centrality measure.
A graph (or network ) G consists of vertices (or nodes) and edges, sets of which will be
denoted V (G) and E(G), respectively. Every edge from set E(G) connects two vertices in
set V (G).7 By (u, v) we will denote the edge connecting vertices u, v ∈ V (G). The number
of edges incident to a vertex is called a vertex degree. The neighboring vertices of v ∈ V
are all vertices connected to v in the graph. For a weighted network a weight (label) is
associated with every edge in E(G). A path is, informally, a sequence of connected edges.
The shortest path problem is to find a path between two given vertices in which the sum of
edge weights is minimized.
We now formalize the notions of coalitional games and the Shapley value. Specifically, let
us denote by A = {a1 , . . . , a|A| } the set of players that participate in a coalitional game. A
characteristic function ν :→ R assigns to every coalition C ⊆ A a real number representing
the quality of its performance, where it is assumed that ν(∅) = 0. A characteristic function
game is then a tuple (A, ν). Assuming that the grand coalition, i.e., the coalition of all
the agents in the game, has the highest value and is formed, one of the key questions in
coalitional game theory is how to distribute the gain from cooperation among the agents
so as to meet certain normative/positive criteria. To this end, Shapley (1953) proposed to
evaluate the role of each agent in the game by computing a weighted average of marginal
contributions of that agent to all possible coalitions he can belong to. The importance of
the Shapley value stems from the fact that it is the unique division scheme that meets four
desirable criteria:
(i) efficiency — all the wealth available to the agents in the grand coalition is distributed
among them;
(ii) symmetry — the payoffs to agents do not depend on their identity;
(iii) null player — agents with zero marginal contributions to all coalitions receive zero
payoff; and
(iv) additivity — values of two games sum up to the value computed for the sum of both
games.
In order to formalize this concept, let π ∈ Π(A) denote a permutation of agents in A, and
let Cπ (i) denote the coalition made of all predecessors of agent ai in π. More formally, if we
denote by π(j) the location of aj in π, then: Cπ (i) = {aj ∈ π : π(j) < π(i)}. The Shapley
value of ai , denoted SVi (ν), is then defined as the average marginal contribution of ai to
coalition Cπ (i) over all π ∈ Π (Shapley, 1953):
SVi (ν) =

1 X
[ν(Cπ (i) ∪ {ai }) − ν(Cπ (i))].
|A|!

(2)

π∈Π

7. Whereas our main focus in this paper is undirected graphs, we will also show how our results can be
readily extended to the case of directed graphs.

617

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Shapley provides the following intuition behind this formula: imagine that the players are
to arrive at a meeting point in a random order, and that every player ai who arrives receives
the marginal contribution that his arrival would bring to those already at the meeting point.
Now if we average these contributions over all the possible orders of arrival, we obtain SVi (ν),
ai ’s payoff in the game.
The formula in (2) can also be stated in the equivalent, but computationally less involved,
form:
X |C|!(|A| − |C| − 1)!
SVi (ν) =
[ν(C ∪ {ai }) − ν(C)].
(3)
|A|!
C⊆A\{ai }

In our network context, we will use G to define a coalitional game (V (G), ν) with set of
agents A = V (G) and characteristic function ν. Here the agents of the coalitional game are
the vertices of the graph G. Thus, a coalition of agents C is simply any subset of V (G).
Furthermore, the characteristic function ν : 2V (G) → R can be any function that depends
on the graph G as long as it satisfies the condition ν(∅) = 0. We use the phrase “value of
coalition C” to informally refer to ν(C).
We will first consider a sample characteristic function game defined over a network, as well
as its Shapley value that becomes a centrality measure. We will then discuss the advantages
of the game theoretic network centrality over conventional measures.
In more detail, consider the notion of “closeness centrality” of a node in a graph G(V, E),
which is traditionally defined as the reciprocal of the average distance of that node from
other (reachable) nodes in the graph (Koschützki et al., 2005). This definition captures the
intuitive idea that a node “in close proximity to many other nodes” is more valuable by
virtue of its central location, and hence should be assigned a higher centrality score.
The above measure, however, fails to recognize the importance of combinations of nodes. For
example, consider a typical application of closeness centrality: that of disseminating a piece
of information to all nodes in the network. At any time point t in the dissemination process,
define the random variable Ct to be the subset of nodes actively involved in propagating the
information. In this situation, a new node added to Ct would make maximum contribution
to the diffusion of information only if it is “in close proximity to nodes that are not currently
in close proximity to any node in Ct ”. Thus, while conventional closeness centrality only
takes into account average proximity to all other nodes, the actual importance of a node in
actual applications is based on a very different measure: proximity to nodes that are not in
close proximity to the random variable Ct .
We now show how coalitional game theory can be used to construct a centrality measure
that more faithfully models the above situation. Let C be an arbitrary subset of nodes from
the given network G(V, E). Then, for every such C, assign a value ν(C) given by
ν(C) =

X
v∈V (G)

1
,
1 + min{d(u, v)|u ∈ C}

where d(u, v) is the distance between nodes u and v (measured as the shortest path length
between u and v in graph G).
618

Computation of the Shapley Value for Game-Theoretic Network Centrality

The map ν defined above captures a fundamental centrality notion: that the intrinsic value
of a subset of nodes C in the context of such an application as information dissemination is
proportional to the overall proximity of the nodes in C to the other nodes in the network.
In effect, the map ν carries the original definition of closeness centrality to a global level,
where a measure of importance is assigned to every possible combination of nodes.
The map ν above is therefore a characteristic function for a coalitional game, where each
vertex of the network is viewed as an agent playing the game. It follows that if a node v has
a high Shapley value in this game, it is likely that v would “contribute more” to an arbitrary
randomly chosen coalition of nodes C in terms of increasing the proximity of C to other
nodes on the network. Thus, computing the Shapley values of this game yields a centrality
score for each vertex that is a much-improved characterization of closeness centrality.
The only difficulty in adopting such a game-theoretically inspired centrality measure is the
previously mentioned problem of exponential complexity in the number of agents. In the
next section, we show how to overcome this difficulty and compute the Shapley value for
many centrality applications (including the above formulation) in time polynomial in the
size of the network.

4. Algorithms for Shapley Value-Based Network Centrality
In this section, we present five characteristic function formulations ν(C), each designed to
convey a specific centrality notion. As already mentioned in the introduction, a common
element of all these formulations is that they aim to quantify, albeit in a different way, the
sphere of influence of the coalition C over the other nodes in the network. Specifically, in our
first game formulation, we start with the simplest possible idea that the sphere of influence
of a coalition of nodes C is the set of all nodes immediately reachable (within one hop) from
C. Subsequent games further generalize this notion. In particular, the second formulation
specifies a more sophisticated sphere of influence: one that includes only those nodes which
are immediately reachable in at least k different ways from C. The other three formulations
extend the notion of sphere of influence to weighted graphs. The third game defines sphere
of influence as the set of all nodes within a cutoff distance of C (as measured by shortest
path lengths on the weighted graph). The fourth formulation is an extreme generalization:
it allows the sphere of influence of C to be specified by an arbitrary function f (.) of the
distance between C and the other nodes. The final formulation is a straightforward extension
of the second game, to the case of weighted networks.
The relationships among all five games are graphically presented in Figure 4.
4.1 Game 1: ν1 (C) = #agents at most 1 degree away
Let G(V, E) be an unweighted, undirected network. We first define the “fringe” of a subset
C ⊆ V (G) as the set {v ∈ V (G) : v ∈ C (or) ∃u ∈ C such that (u, v) ∈ E(G)}, i.e., the
fringe of a coalition includes all nodes reachable from the coalition in at most one hop.
Based on the fringe, we define the coalitional game g1 (V (G), ν1 ) with respect to the network
G(V, E) by the characteristic function ν1 : 2V (G) → R given by
619

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Figure 4: Euler diagram showing the relationships among all five games considered in this
paper. Specifically, game g2 generalizes g1 ; g3 generalizes g1 and is further generalized by
g4 ; g5 generalizes g2 . Finally, we note that there are certain instances of games that can be
represented as g3 , g4 and g5 .

(
0
ν1 (C) =
size(fringe(C))

if C = ∅
otherwise.

.
The above game was applied by Suri and Narahari (2008) to find out influential nodes in
social networks and it was shown to deliver very promising results concerning the target
set selection problem (see Kempe, Kleinberg, & Tardos, 2003). It is therefore desired to
compute the Shapley values of all nodes for this game. We shall now present an exact
formula for this computation rather than obtaining it through Monte Carlo simulation as
was done by Suri and Narahari.
In more detail, to evaluate the Shapley value of node vi , consider all possible permutations
of the nodes in which vi would make a positive marginal contribution to the coalition of
nodes occurring before itself. Let the set of nodes occurring before node vi in a random
permutation of nodes be denoted Ci . Let the neighbours of node vi in the graph G(V, E)
be denoted NG (vi ) and the degree of node vi be denoted degG (vi ).
The key question to ask is: what is the necessary and sufficient condition for node vi to
“marginally contribute node vj ∈ NG (vi ) ∪ {vi } to fringe(Ci )”? Clearly, this happens if and
only if neither vj nor any of its neighbours are present in Ci . Formally, (NG (vj )∪{vj })∩Ci =
∅.
Now we are going to show that the above condition holds with probability

1
1+degG (vj ) .

Proposition 1. The probability that in a random permutation none of the vertices from
NG (vj ) ∪ {vj } occurs before vi , where vi and vj are neighbours, is 1+deg1G (vj ) .
620

Computation of the Shapley Value for Game-Theoretic Network Centrality

Algorithm 1: Computing the Shapley value for Game 1
Input: Unweighted graph G(V, E)
Output: Shapley values of all nodes in V (G) for game g1
foreach v ∈ V (G) do
SV[v] = 1+deg1 G (v) ;
foreach u ∈ NG (v) do
SV[v] += 1+deg1 G (u) ;
end
end
return SV;

Proof. We need to count the number of permutations that satisfy:
∀v∈(NG (vj )∪{vj }) π(vi ) < π(v).

(4)

To this end:
• Let us choose |(NG (vj ) ∪ {vj }| positions in the sequence of all elements from V . We

|V |
can do this in 1+deg
ways.
G (vj )
• Then, in the last degG (vj ) chosen positions, place all elements from (NG (vj ) ∪ {vj }) \
{vi }. Directly before these, place the element vi . The number of such line-ups is
(degG (vj ))!.
• The remaining elements can be arrange in (|V | − (1 + degG (vj ))! different ways.
All in all, the number of permutations satisfying condition (4) is:

|V |
(degG (vj ))!(|V
1+degG (vj )

| − (1 + degG (vj ))! =

|V |!
1+degG (vj ) ;

thus, the probability that one of such permutations is randomly chosen is

1
1+degG (vj ) .

Now, denote by Bvi ,vj the Bernoulli random variable that vi marginally contributes vj to
fringe(Ci ). From the above, we have:
E[Bvi ,vj ] = Pr[(NG (vj ) ∪ {vj }) ∩ Ci = ∅] =

1
.
1 + degG (vj )

Therefore, SVg1 (vi ), which is the expected marginal contribution of vi , is given by:
SVg1 (vi ) =

X

E[Bvi ,vj ] =

X
vj ∈{vi }∪NG (vi )

vj ∈{vi }∪NG (vi )

621

1
,
1 + degG (vj )

(5)

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

which is an exact closed-form expression for computing the Shapley value of each node on
the network.
It is possible to derive some intuition from the above formula. If a node has a high degree,
the number of terms in its Shapley value summation above is also high. But the terms
themselves will be inversely related to the degree of neighboring nodes. This gives the
intuition that a node will have high centrality not only when its degree is high, but also
whenever its degree tends to be higher in comparison to the degree of its neighboring nodes.
In other words, power comes from being connected to those who are powerless, a fact that
is well-recognized by the centrality literature (e.g., Bonacich, 1987). Following the same
reasoning, we can also easily predict how dynamic changes to the network, such as adding
or removing an edge, would influence the Shapley value.8 Adding an edge between a powerful
and a powerless node will add even more power to the former and will decrease the power
of the latter. Naturally, removing an edge would have the reverse effect.
Interestingly, although game g1 is quite different from the induced-subgraph representation
of Deng and Papadimitriou (1994), the formula for SVg1 (vi ) is, to some extent, similar to
formula (1). In particular, in both cases, the Shapley value of a node depends solely on the set
of its immediate neighbours. Moreover, in both cases, it is a linear combination of fractions
involving in the numerator the weight of edges between the node and its neighbours.9 The
difference is in the denominators, where in our case it depends on the degree of the involved
nodes. We will see that the next two games considered in this paper yield comparable to g1
closed-form expressions for the Shapley value.
Algorithm 1 directly implements expression (5) to compute the exact Shapley values of all
nodes in the network. It cycles through all nodes and their neighbours, so its running time
is O(|V | + |E|).
Finally, we note that Algorithm 1 can be adopted to directed graphs with a couple of simple
modifications. Specifically, in order to capture how many nodes we can access a given
node from, the degree of a node should be replaced with indegree. Furthermore, a set of
neighbours of a given node v should consist of those nodes to which an edge is directed from
v.
4.2 Game 2: ν2 (C) = #agents with at least k neighbors in C
We now consider a more general game formulation for an unweighted graph G(V, E), where
the value of a coalition includes the number of agents that are either in the coalition or
are adjacent to at least k agents who are in the coalition. Formally, we consider game g2
characterised by ν2 : 2V (G) → R, where
(
0
ν2 (C) =
|{v : v ∈ C (or) |NG (v) ∩ C| ≥ k}|

if C = ∅
otherwise.

8. Many real-life networks are in fact dynamic and the challenge of developing fast streaming algorithms
has recently attracted considerable attention in the literature (Lee, Lee, Park, Choi, & Chung, 2012).
9. Note that in case of g1 the weight of any edge is 1.

622

Computation of the Shapley Value for Game-Theoretic Network Centrality

The second game is an instance of the General Threshold Model that has been widely studied
in the literature (e.g., Kempe et al., 2005; Granovetter, 1978). Intuitively, in this model
each node can become active if a monotone activation function reaches some threshold. The
instance of this problem has been proposed by Goyal, Bonchi, and Lakshmanan (2010), where
the authors introduced a method of learning influence probabilities in social networks (from
users’ action logs). However, in many realistic situations much less information is available
about a network so it is not possible to assess specific probabilities with which individual
nodes become active. Consequently, much simpler models are studied. Bikhchandani et al.
(1992), for instance, “consider a teenager deciding whether or not to try drugs. A strong
motivation for trying out drugs is the fact that friends are doing so. Conversely, seeing friends
reject drugs could help persuade the teenager to stay clean”. This situation is modelled by
the second game; the threshold for each node is k and the activation function is f (S) =
|S|. Another example is viral marketing or innovation diffusion analysis. Again, in this
application, it is often assumed that an agent will “be influenced” only if at least k of his
neighbors have already been convinced (Valente, 1996). Note that this game reduces to
game g1 for k = 1.
Adopting notation from the previous subsection, we again ask: what is the necessary and
sufficient condition for node vi to marginally contribute node vj ∈ NG (vi ) ∪ {vi } to the value
of the coalition Ci ?
Clearly, if degG (vj ) < k, we have E[Bvi ,vj ] = 1 for vi = vj and 0 otherwise. For degG (nj ) ≥ k,
we split the argument into two cases. If vj 6= vi , the condition for marginal contribution
is that exactly (k − 1) neighbors of vj already belong to Ci and vj ∈
/ Ci . On the other
hand, if vj = vi , the marginal contribution occurs if and only if Ci originally consisted of at
most (k − 1) neighbors of vj . So for degG (vj ) ≥ k and vj 6= vi , we need to determine the
appropriate probability.
Proposition 2. The probability that in a random permutation exactly k − 1 neighbours of vj
1+degG (vj )−k
occur before vi , and vj occurs after vi , is: degG (vj )(1+deg
, where vj and vi are neighbors
G (vj ))
and degG (vj ) ≥ k.
Proof. We need to count the number of permutations that satisfy:

n

	
∃!K⊆NG (vj ) |K| = k − 1 ∧ ∀v∈K π(v) < π(vi ) ∧
o

	
∀v∈NG (vj )\K π(vi ) ≤ π(v) ∧ π(vi ) < π(vj ) .

(6)

To this end:
• Let us choose |(NG (vj ) ∪ {vj }| positions in the sequence of all elements from V . We

|V |
can do this in 1+deg
ways.
(v
)
G j
• Then, choose k − 1 elements from the set (NG (vj ) \ {vi }. The number of such choices
(vj )−1
is degGk−1
.
623

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Algorithm 2: Computing the Shapley value for Game 2
Input: Unweighted graph G(V, E), positive integer k
Output: Shapley value of all nodes in V (G) for game g2
foreach v ∈ V (G) do
SV[v] = min(1, 1+degk G (v) );
foreach u ∈ NG (v) do
G (u)−k+1
SV[v] += max(0, degGdeg
(u)(1+degG (u)) );
end
end
return SV;

• Then, in the first k − 1 chosen positions, place all elements chosen in previous step.
Directly after those, place the element vi , and then the remaining vertices chosen in
the first step. The number of such line-ups is (k − 1)!(1 + degG (vj ) − k)!.
• The remaining elements can be arrange in (|V | − degG (vj ) − 1)! different ways.
Taking all the above together, the number of permutations satisfying condition (6) is:
 degG −1
|V |
k−1 (k
1+degG (vj )

− 1)!(1 + degG (vj ) − k)!(|V | − degG (vj ) − 1)! =

|V |!(1+degG (vj )−k)
degG (vj )(1+degG (vj )) ;

thus, the probability that one of such permutations is randomly chosen is

1+degG (vj )−k
degG (vj )(1+degG (vj )) .

Using Proposition 2 we obtain:
E[Bvi ,vj ] =

1 + degG (vj ) − k
.
degG (vj )(1 + degG (vj ))

And for degG (vi ) ≥ k and vj = vi , we have:

E[Bvi ,vi ] =

k−1
X
n=0

1
k
=
.
1 + degG (vi )
1 + degG (vi )

As before, the Shapley values are given by substituting the above formulae into:
SVg2 (vi ) =

X

E[Bvi ,vj ].

vj ∈NG (vi )∪{vi }

Although this game is a generalization of game g1 , it can still be solved to obtain the Shapley
values of all nodes in O(|V | + |E|) time, as formalised by Algorithm 2.
624

Computation of the Shapley Value for Game-Theoretic Network Centrality

An even more general formulation of the game is possible by allowing k to be a function
of the agent, i.e., each node vi ∈ V (G) is assigned its own unique attribute k(vi ). This
translates to an application of the form: agent i is convinced if and only if at least ki of his
neighbors are convinced, which is a frequently used model in the literature (Valente, 1996).
The above argument does not use the fact that k is constant across all nodes. So this
generalized formulation can be solved by a simple modification to the original Shapley value
expression:
SV (vi ) =

k(vi )
+
1 + degG (vi )

X
vj ∈NG (vi )

1 + degG (vj ) − k(vj )
.
degG (vj )(1 + degG (vj ))

The above equation (which is also implementable in O(|V |+|E|) time) assumes that k(vi ) ≤
1+degG (vi ) for all nodes vi . This condition can be assumed without loss of generality because
all cases can still be modelled (we set k(vi ) = 1 + degG (vi ) for the extreme case where node
vi is never convinced no matter how many of its neighbors are already convinced).
Finally, we note that Algorithm 2 can be adapted to a case of directed graphs along the
same lines as Algorithm 1.
4.3 Game 3: ν3 (C) = #agents at most dcutoff away
Hitherto, our games have been confined to unweighted networks. But in many applications,
it is necessary to model real-world networks as weighted graphs. For example, in the coauthorship network mentioned in the introduction, each edge is often assigned a weight
proportional to the number of joint publications the corresponding authors have produced
(Newman, 2001).
This subsection extends game g1 to the case of weighted networks. Whereas game g1 equates
ν(C) to the number of nodes located within one hop of some node in C, our formulation
in this subsection equates ν(C) to the number of nodes located within a distance dcutoff of
some node in C. Here, distance between two nodes is measured as the length of the shortest
path between the nodes in the given weighted graph G(V, E, W ), where W : E → R+ is the
weight function.
Formally, we define game g3 , where for each coalition C ⊆ V (G),
(
0
ν3 (C) =
size({vi : ∃vj ∈ C | distance(vi , vj ) ≤ dcutoff })

if C = ∅
otherwise.

Clearly, g3 can be used in all the settings where g1 is applicable; for instance, in the diffusion
of information in social networks or to analyse research collaboration networks (e.g., Suri &
Narahari, 2010, 2008). Moreover, as a more general game, g3 provides additional modelling
opportunities. For instance, Suri and Narahari (2010) suggest that a “more intelligent” way
for sieving nodes in the neighbourhood would improve their algorithm for solving the target
selection problem (top-k problem). Now, g3 allows us to define a different cutoff distance
625

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Algorithm 3: Computing the Shapley value for Game 3
Input: Weighted graph G(V, E, W ), dcutoff > 0
Output: Shapley value of all nodes in G for game g3
foreach v ∈ V (G) do
DistanceVector D = Dijkstra(v,G);
extNeighbors(v) = ∅; extDegree(v) = 0;
foreach u ∈ V (G) such that u 6= v do
if D(u) ≤ dcutoff then
extNeighbors(v).push(u);
extDegree(v)++;
end
end
end
foreach v ∈ V (G) do
1
SV[v] = 1+extDegree(v)
;
foreach u ∈ extN eighbors(v) do
1
SV[v] += 1+extDegree(u)
;
end
end
return SV;

for each node in Suri and Narahari’s setting. Furthermore, g3 is a specific case of the more
general model g4 which will be discussed in next subsection.
We shall now show that even this highly general centrality game g3 is amenable to analysis
which yields an exact formula for the Shapley value. However, in this case the algorithm
for implementing the formula is not linear in the size of the network, but has O(|V ||E| +
|V |2 log|V |) complexity.
Before deriving the exact Shapley value formula, we introduce some extra notation. Define
the extended neighborhood NG (vj , dcutoff ) = {vk 6= vj : distance(vk , vj ) ≤ dcutoff }, i.e., the
set of all nodes whose distance from vj is at most dcutoff . Denote the size of NG (vj , dcutoff )
by degG (vj , dcutoff ). With this notation, the necessary and sufficient condition for node vi
to marginally contribute node vj to the value of coalition Ci is: distance(vi , vj ) ≤ dcutoff
and distance(vj , vk ) > dcutoff ∀vk ∈ Ci . That is, neither vj nor any node in its extended
neighborhood should be present in Ci . From the discussion in previous subsections and
Proposition 1, we know that the probability of this event is exactly 1+degG (v1j ,dcutoff ) . Therefore, the exact formula for the Shapley value of node vi in game g3 is:
SVg3 (vi ) =

X
vj ∈{vi }∪NG (vi ,dcutoff )

1
.
1 + degG (vj , dcutoff )

Algorithm 3 works as follows: for each node v in the network G(V, E), the extended neighborhood NG (v, dcutoff ) and its size degG (v, dcutoff ) are first computed using Dijkstra’s algorithm
626

Computation of the Shapley Value for Game-Theoretic Network Centrality

in O(|E| + |V |log|V |) time (Cormen, 2001). The results are then used to directly implement
the above equation, which takes maximum time O(|V |2 ). In practice this step runs much
faster because the worst case situation only occurs when every node is reachable from every
other node within dcutoff . Overall the complexity is O(|V ||E| + |V |2 log|V |).
Furthermore, to deal with directed graphs we need to redefine the notion of extDegree and
extN eighbors for a given node u in Algorithm 3. The former will be the number of vertices
from which the distance to u is smaller than, or equal to, dcutoff . The latter will be the set
of nodes whose distance from u is at most dcutoff .
Finally, we make the observation that the above proof does not depend on dcutoff being
constant across all nodes. Indeed, each node vi ∈ V (G) may be assigned its own unique
value dcutoff (vi ), where ν(C) would be the number of agents vi who are within a distance
dcutoff (vi ) from C. For this case, the above proof gives:
SV (vi ) =

X

vj :distance(vi ,vj )
≤dcutoff (vj )

4.4 Game 4: ν4 (C) =

1
.
1 + degG (vj , dcutoff (vj ))

P

vi ∈V (G) f (distance(vi , C))

This subsection further generalizes game g3 , again taking motivation from real-life network
problems. In game g3 , all agents at distances dagent ≤ dcutoff contributed equally to the
value of a coalition. However, this assumption may not always hold true because in some
applications we intuitively expect agents closer to a coalition to contribute more to its value.
For instance, we expect a Facebook user to exert more influence over his immediate circle
of friends than over “friends of friends”, even though both may satisfy the dcutoff criterion.
Similarly, we expect a virus-affected computer to infect a neighboring computer more quickly
than a computer two hops away.
In general, we expect that an agent at distance d from a coalition would contribute f (d) to
its value, where f (.) is a positive valued decreasing function of its argument. More formally,
we define game g4 , where the value of a coalition C is given by:
(
0
ν4 (C) = P

vi ∈V (G) f (d(vi , C))

if C = ∅
otherwise,

where d(vi , C) is the minimum distance: min{distance(vi , vj )|vj ∈ C}.
We note that it is possible to solve for the Shapley value in the above formulation by
constructing an MC-Nets representation (see Section 2 for more details on this formalism).
Indeed, the combinatorial structure of networks is to a certain extent similar to the structure
of MC-Nets. Consequently, the existence of a polynomial algorithm to compute the Shapley
value for MC-Nets strongly suggests that polynomial algorithms could be developed for
games defined over networks. Our results in this paper demonstrate that this is indeed
the case. However, it should be underlined that our approach to compute the Shapley
627

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

value is different from that applied in MC-Nets. This is because in our solutions we focus
on computing the expected contribution of every node in a random permutation of nodes
and not on disaggregating the game into a collection of “simple”, easily solvable, games
as it is done in MC-Nets. The difference in both approaches is clearly visible in the case
of g3 . Here, the MC-Nets would have O(|V |3 ) rules, whereas in the discussion below, we
propose a more efficient algorithm for g3 that runs in O(|V ||E| + |V |2 log|V |). This is a
considerable improvement because most real-world networks are sparse, i.e., E ∼ O(|V |)
(Reka & Barabási, 2002).
In the case of g3 , the key question to ask is: what is the expected value of the marginal
contribution of vi through node vj 6= vi to the value of coalition Ci ? Let this marginal
contribution be denoted M C(vi , vj ). Clearly:
(
0
M C(vi , vj ) =
f (distance(vi , vj )) − f (d(vj , Ci ))

if distance(vi , vj ) ≥ d(vj , Ci )
otherwise.

Let Dvj = {d1 , d2 ...d|V |−1 } be the distances of node vj from all other nodes in the network,
sorted in increasing order. Let the nodes corresponding to these distances be {w1 , w2 ...w|V |−1 },
respectively. Let kij + 1 be the number of nodes (out of these |V | − 1) whose distances to
vj are ≤ distance(vi , vj ). Let wkij +1 = vi (i.e., among all nodes that have the same distance
from vj as vi , vi is placed last in the increasing order).
We use literal wi to mean wi ∈ Ci and the literal wi to mean wi ∈
/ Ci . Define a sequence
of boolean variables pk = vj ∧ w1 ∧ w2 ∧ ... ∧ wk for each 0 ≤ k ≤ |V | − 1. Finally denote
expressions of the form M C(vi , vj |F ) to mean the marginal contribution of vi to Ci through
vj given that the coalition Ci satisfies the boolean expression F .

M C(vi , vj |pkij +1 ∧ wkij +2 ) = f (dkij +1 ) − f (dkij +2 ),
M C(vi , vj |pkij +2 ∧ wkij +3 ) = f (dkij +1 ) − f (dkij +3 ),
..
..
..
.
.
.
M C(vi , vj |p|V |−2 ∧ w|V |−1 ) = f (dkij +1 ) − f (d|V |−1 ),
M C(vi , vj |p|V |−1 ) = f (dkij +1 ).
With this notation, we obtain expressions for M C(vi , vj ) by splitting over the above mutually
exclusive and exhaustive (i.e., covering all possible non-zero marginal contributions) cases.
Now, we need to determine the probability of Pr(pk ∧ wk+1 ).
Proposition 3. The probability that in a random permutation none of the nodes from
1
.
{vj , w1 , . . . , wk } occur before vi and the node wk+1 occurs before vi is (k+1)(k+2)
Proof. Let us count the number of permutations that satisfy:
∀v∈{vj ,w1 ,...,wk } π(vi ) < π(v) ∧ π(vi < π(wk+1 ).
To this end:
628

(7)

Computation of the Shapley Value for Game-Theoretic Network Centrality

• Let us choose |{vj , w1 , . . . , wk }∪{vj }∪{wk+1 }| positions in the sequence of all elements
|V | 
from V . We can do this in k+3
ways.
• Then, in the last k + 1 chosen positions, we place all elements from {vj , w1 , . . . , wk }.
Directly before these, we place the element vi , and then vertex wk+1 . The number of
such line-ups is (k + 1)!.
• The remaining elements can be arrange in (|V | − (k + 3)! different ways.
Thus, the number of permutations satisfying (7) is:
|V | 
k+3 (k

+ 1)!(|V | − (k + 3))! =

|V |!
(k+1)(k+2) ,

and the probability that one of such permutations is randomly chosen is

1
(k+1)(k+2) .

With the above proposition we find that:
Pr(pk ∧ wk+1 )

1
∀ 1 + kij ≤ k ≤ |V | − 2.
(k + 1)(k + 2)

Using the M C(vi , vj ) equations and the probabilities Pr(pk ∧ wk+1 ):




X f (distance(vi , vj )) − f (dk+1 )
 + f (distance(vi , vj ))
E[M C(vi , vj )] = 
(k + 1)(k + 2)
|V |
|V |−2

k=1+kij

f (distance(vi , vj ))
−
=
kij + 2

|V |−2

X
k=kij +1

f (dk+1 )
.
(k + 1)(k + 2)

For vi = vj , a similar analysis produces:
|V |−2

E[M C(vi , vi )] = f (0) −

X
k=0

f (dk+1 )
.
(k + 1)(k + 2)

Finally the exact Shapley value is given by:
SVg4 (vi ) =

X

E[M C(vi , vj )].

vj ∈V (G)

Algorithm 4 implements the above formulae. For each vertex v, a vector of distances to
every other vertex is first computed using Dijkstra’s algorithm (Cormen, 2001). This yields
a vector Dv that is already sorted in increasing order. This vector is then traversed in
629

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Algorithm 4: Computing the Shapley value for Game 4
Input: Weighted graph G(V, E, W ), function f : R+ → R+
Output: Shapley value of all nodes in G for game g4
Initialise: ∀v ∈ V (G) set SV[v] = 0;
foreach v ∈ V (G) do
[Distances D, Nodes w] = Dijkstra(v,G);
sum = 0; index = |V|-1; prevDistance = -1, prevSV = -1;
while index > 0 do
if D(index) == prevDistance then
currSV = prevSV;
else
currSV = f (D(index))
1+index − sum;
end
SV[w(index)] += currSV;
sum +=

f (D(index))
index(1+index) ;

prevDistance = D(index), prevSV = currSV;
index--;
end
SV[v] += f(0) − sum;
end
return SV;

P f (dk+1 )
reverse, to compute the backwards cumulative sum
(k+1)(k+2) . At each step of the backward traversal, the Shapley value of the appropriate node w is updated according to the
E[M C(w, v)] equation. After the traversal, the Shapley value of v itself is updated according
to the E[M C(v, v)] equation. This process is repeated for all nodes v so that at the end of
the algorithm, the Shapley value is computed exactly in O(|V ||E| + |V |2 log|V |) time.
Our final observation is that Algorithm 4 works also for directed graphs as long as we use
the appropriate version of Dijkstra’s algorithm (see, e.g., Cormen, 2001).
4.5 Game 5: ν5 (C) = #agents with

P
(weights inside C) ≥ Wcutoff (agent)

In this subsection, we generalize game g2 for the case of weighted networks. Given a positive
weighted network G(V,
P E, W ) and a value Wcutoff (vi ) for every node vi ∈ V (G), we first
define W (vj , C) = vi ∈C W (vj , vi ) for every coalition C, where W (vi , vj ) is the weight of
the edge between nodes vi and vj (or 0 if there is no such edge). With this notation, we
define game g5 by the characteristic function:
(
0
ν5 (C) =
size({vi : vi ∈ C (or) W (vi , C) ≥ Wcutoff (vi )})
630

if C = ∅
otherwise.

Computation of the Shapley Value for Game-Theoretic Network Centrality

Algorithm 5: Computing the Shapley value for Game 5
Input: Weighted network G(V, E, W ), cutoffs Wcutoff (vi ) for each vi ∈ V (G)
Output: Shapley value of all nodes in G for game g5
foreach vi ∈ V (G) do
compute and store αi and βi ;
end
foreach vi ∈ V (G) do
SV[vi ] = 0;
foreach m in 0 to degG (vi ) do
ii ), σ = σ(X ii ), p = Pr{N (µ, σ 2 ) < W
compute µ = µ(Xm
cutoff (vi )};
m
p
SV[vi ] += 1+degG (vi ) ;
end
foreach vj ∈ NG (vi ) do
p = 0;
foreach m in 0 to degG (vj ) − 1 do
ij
ij
ij
compute µ = µ(Xm
), σ = σ(Xm
) and z = Zm
;
deg (v )−m

G j
p += z degG (vj )(deg
;
G (vj )+1)

end
SV[vi ] += p;
end
end
return SV;

The formulation above has applications in, for instance, the analysis of information diffusion, adoption of innovations, and viral marketing. Indeed, many cascade models of such
phenomena on weighted graphs have been proposed (e.g., Granovetter, 1978; Kempe et al.,
2003; Young, 2006) which work by assuming that an agent will change state from “inactive”
to “active” if and only if the sum of the weights to all active neighbors is at least equal to
an agent-specific cutoff.
Although we have not been able to come up with an exact formula for the Shapley value in
this game10 , our analysis yields an approximate formula which was found to be accurate in
practice.
In more detail, we observe that node vi marginally contributes node vj ∈ NG (vi ) to the value
of coalition Ci if and only if vj ∈
/ Ci and Wcutoff (vj ) − W (vi , vj ) ≤ W (vj , Ci ) < Wcutoff (vj ).
Let us denote by Bvi ,vj the Bernoulli random variable corresponding to this event. We will
need the following additional notation:
• let NG (vj ) = {vi , w1 , w2 ...wdegG (vj )−1 };
10. Computing the Shapley value for this game involves determining whether the sum of weights on specific
edges, adjacent to a random coalition, exceeds the threshold. This problem seems to be at least as hard
as computing the Shapley value in weighted voting games, which is #P-Complete (Elkind et al., 2009).

631

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

• let the weights of edges between vj and each of the nodes in NG (vj ) be Wj =
{W (vi , vj ), W1 , W2 ...WdegG (vj )−1 } in that order;
• let αj be the sum of all the weights in Wj and βj be the sum of the squares of all the
weights in Wj ;
• let kij be the number of nodes of NG (vj ) that occur before vi in Ci ;
• let Xtij be the sum of a t-subset of Wj \ {W (vi , vj )} drawn uniformly at random from
the set of all such possible t-subsets; and finally
• let Ymij be the event {kij = m ∧ vj ∈
/ Ci }.
Then:
degG (vj )−1

E[Bvi ,vj ] =

X

ij
Pr(Ymij ) Pr{Xm
∈ [Wcutoff (vj ) − W (vi , vj ), Wcutoff (vj ))},

m=0

where Pr(Ymij ) is obtained from Proposition 2:
Pr(Ymij )



degG (vj ) − m
degG (vj ) − 1 m! (degG (vj ) − m)!
=
.
=
(degG (vj ) + 1)!
degG (vj )(degG (vj ) + 1)
m

ij
Evaluating Pr{Xm
∈ [Wcutoff (vj ) − W (vi , vj ), Wcutoff (vj ))} is much more difficult because
ij
is a complicated function of the degG (vj ) − 1 numbers in Wj \
the distribution of Xm
ij
{W (vi , vj )}. However, we can obtain analytical expressions for the mean µ(Xm
) and variance
ij
σ 2 (Xm
). These are given by:

m
(αj − W (vi , vj ))
degG (vj ) − 1
(αj − W (vi , vj ))2
m(degG (vj ) − 1 − m)
ij
)=
(βj − W (vi , vj )2 −
).
σ 2 (Xm
(degG (vj ) − 1)(degG (vj ) − 2)
degG (vj ) − 1
ij
µ(Xm
)=

ij
Knowing only the mean and variance (not the exact distribution) of Xm
, we propose the
approximation:

ij
ij
ij
Xm
∼ N (µ(Xm
), σ 2 (Xm
)),

where N (µ, σ 2 ) denotes the Gaussian random variable with mean µ and variance σ 2 . This
approximation is similar to the randomised approach that has been proposed and tested by
Fatima et al. (2007).
With this approximation, we have:
632

Computation of the Shapley Value for Game-Theoretic Network Centrality

ij
ij
Zm
= Pr{Xm
∈ [Wcutoff (vj ) − W (vi , vj ), Wcutoff (vj ))}

given by

ij
Zm

"
1
erf
≈
2

ij
Wcutoff (vj ) − µ(Xm
)
√
ij
2σ(Xm )

!
− erf

ij
Wcutoff (vj ) − W (vi , vj ) − µ(Xm
)
√
ij
2σ(Xm )

!#
.

This allows us to write:
degG (vj )−1

E[Bvi ,vj ] =

X
m=0

degG (vj ) − m
Z ij .
degG (vj )(degG (vj ) + 1) m

The above equations are true only for vj 6= vi . For vj = vi we have:

1
E[Bvi ,vi ] ≈
1 + degG (vi )

degG (vi )

X

ii
ii
Pr{N (µ(Xm
), σ 2 (Xm
)) < Wcutoff (vi )},

m=0

where
ii
µ(Xm
)=

m
αi
degG (vi )

and

ii
σ 2 (Xm
)=

αi2
m (degG (vi ) − m)
(βi −
).
degG (vi ) (degG (vi ) − 1)
degG (vi )

P
Finally the Shapley value of node vi is given by vj ∈{vi }∪NG (vi ) E[Bvi ,vj ].
P
While inPeach graph
P it holds that vi ∈V (G) degG (vi ) ≤ 2|E|, Algorithm 5 implements an
O(|V | + vi ∈V (G) vj ∈NG (vi ) degG (vj )) ≤ O(|V | + |V ||E|) = O(|V ||E|) solution to compute
the Shapley value for all agents in game g5 using the above approximation.
Furthermore, we make the following observation: the approximation of the discrete random
ij
variable Xm
as a continuous Gaussian random variable is good only when degG (vj ) is large.
For small degG (vj ), one might as well use the brute force computation to determine E[Bvi ,vj ]
in O(2degG (vj )−1 ) time.
As far as directed graphs are concerned, in all calculations in Algorithm 5 we have to consider
the indegree of a node instead of degree. Furthermore, the set of neighbours of a node u
should be defined as the set of nodes vi connected with directed edge (u, vi ).
633

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

5. Simulations
In this section we evaluate the time performance of our exact algorithms for games g1 to
g4 and our approximation algorithm for game g5 . In more detail, we compare our exact
algorithms to the method of approximating the Shapley value via Monte Carlo sampling
which has been the only feasible approach to compute game-theoretic network centrality
available to date in the literature. First, we provide a detailed description of the simulation
setup; then, we present data sets and the simulation results.
5.1 Simulation Setup
There are a few approximation methods for the Shapley value that have been recently
proposed in literature. They can be divided into three groups—each referring to a specific
subclass of coalitional games under consideration:
1. First, let us consider the method proposed by Fatima et al. (2007) and elaborated
further by Fatima, Wooldridge, and Jennings (2008). This approach concerns weighted
voting games. In these games, each player has a certain number of votes (or in other
words, a weight). A coalition is “winning” if the number of votes in this coalition
exceeds some specific threshold, or “losing” otherwise. Fatima et al. propose the
following method to approximate the Shapley value in weighted voting games. Instead
of finding marginal contributions of players to all 2n coalitions, the authors consider
only n randomly-selected coalitions, one of each size (i.e., from 1 to n). Only for
these n coalitions are the player’s expected marginal contributions calculated and the
average of these contributions yields an approximation of the Shapley value. Whereas
Fatima et al. method is certainly attractive, it is only applicable to games in which the
value of a coalition depends on the sum of associated weights being in some bounds.
This is not the case for our games g1 to g4 .11
2. Another method was proposed by Bachrach, Markakis, Procaccia, Rosenschein, and
Saberi (2008a) in the context of simple coalitional games 12 in which the characteristic function is binary—i.e., each coalition has a value of either zero or one. For
these games, Bachrach et al. extend the approach suggested by Mann and Shapley
(1960) and provide more rigorous statistical analysis. In particular, Mann and Shapley
described the Monte Carlo simulations to estimate the Shapley value from a random
sample of coalitions. Bachrach at al. use this technique to compute the Banzhaf power
index and then they suggested using a random sample of permutations of all players
in order to compute the Shapley-Shubik index for simple coalitional games.13 The
computation of the confidence interval, which is crucial in such an approach, hinges
upon the binary form of the characteristic function for simple coalitional games. This
11. Recall that our approximation algorithm for g5 builds upon Fatima et al. method. This is because in
this game the marginal contribution of each node depends on the weights assigned to its incident edges.
12. Note that weighted voting games are simple coalitional games.
13. The Shapley-Shubik index is a very well-known application of the Shapley value that evaluates the power
of individuals in voting (Shapley & Shubik, 1954).

634

Computation of the Shapley Value for Game-Theoretic Network Centrality

Algorithm 6: Monte Carlo method to approximate the Shapley value
Input:
 Characteristic function v, maximum iteration maxIter
Output: Aproximation of Shapley value for game v
for vi ∈ V (G) do
SV[vi ] = 0 ;
end
for i = 1 to maxIter do
shuffle(V (G));
Marginal Contribution block
P=∅;
for vi ∈ V (G) do
SV[vi ] += v(P ∪ {vi }) - v(P) ;
P = P ∪ {vi } ;
end
end
for vi ∈ V (G) do
SV[vi ]
SV[vi ] = maxIter
;
end
return SV ;

method is more general than the one proposed by Fatima et al. (2007)—as weighted
voting games are a subset of simple coalitional games—but still it cannot be effectively
used for our games g1 to g4 , where the characteristic functions are not binary.
3. Unlike the first two methods, the last method described by Castro et al. (2009) can
be efficiently applied to all coalitional games in characteristic function game form,
assuming that the worth of every coalition can be computed in polynomial time. Here,
approximating the Shapley value involves generating permutations of all players and
computing the marginal contribution of each player to the set of players occurring
before it. The solution precision increases (statistically) with every new permutation
analysed. Furthermore, the authors show how to estimate the appropriate size of a
permutation sample in order to guarantee a low error. Given its broad applicability,
this method is used in our simulations as a comparison benchmark.
In more detail, in a preliminary step, we test what is the maximum number of Monte
Carlo iterations that can be performed in a reasonable time for any given game. This
maximum number of iterations, denoted maxIter, becomes an input to Algorithm 6 for
Monte Carlo sampling. In this algorithm, in each one of the maxIter iterations, a random
permutation of all nodes is generated. Then, using a characteristic function from the set
ν ∈ {ν1 , ν2 , ν3 , ν4 , ν5 }, it calculates the marginal contribution of each node to the set P
635

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

of nodes occurring before a given node in a random permutation.14 Finally, the algorithm
divides the aggregated sum of all contributions for each node by the number of iterations
performed. The time complexity of this algorithm is O(maxIter ∗ con), where con denotes
the number of operations necessary for computing the Marginal Contribution block. This
block is specifically tailored to the particular form of the characteristic function of each of
the games g1 to g5 . In particular, for game g1 (see Algorithm 6), it is constructed as follows.
Recall that, in this game, node vi makes a positive contribution to coalition P through itself
and through some adjacent node u under two conditions. Firstly, neither vi nor u are in P .
Secondly, there is no edge from P to vi or u. To check for these conditions in Algorithm 6
we store those nodes that have already contributed to the value of coalition P in an array
called: Counted. For each node vi , the algorithm iterates through the set of its neighbours
and for each adjacent node it checks whether this adjacent node is counted in the array
Counted. If not, the marginal contribution of the node vi is increased by one. In Appendix
A we describe the Marginal Contribution block for games g2 , . . . , g5 , respectively.15
Some details of how Algorithm 6 is applied to generate the Shapley value approximations for
games g1 to g4 , for which we propose exact polynomial solutions, differ from g5 , for which
we developed an approximate solution. Specifically, for games g1 to g4 :
1. We use the exact algorithm proposed in this paper to compute the Shapley value.
2. Then, we run Monte Carlo simulations 30 times.16 In every run:
• We perform maxIter Monte Carlo iterations.
• After every five iterations, we compare the approximation of the Shapley value
obtained via Monte Carlo simulation with the exact Shapley value obtained with
our algorithm.
• We record the algorithm’s runtime and the error, where the error is defined as the
maximum discrepancy between the actual Shapley value and the Monte Carlobased approximation of the Shapley value.
3. Finally, we compute the confidence interval using all iterations (0.95% confidence
level).17
In the case of game g5 we cannot determine the exact Shapley value for larger networks.
Therefore, we performed two levels of simulation: one level on small networks and one level
on large networks. Specifically:
1. For small networks, we generate 30 random instances of weighted complete graphs
with 6 nodes (denoted K6 ) and the same number of graphs with 12 nodes (denoted
14. Recall that the characteristic functions v1 , v2 , . . . , v5 correspond to games g1 , g2 , . . . , g5 , respectively.
15. The software package in C++ containing all our exact/approximation algorithms, as well as the Monte
Carlo approximation algorithms are available at www.tomaszmichalak.net.
16. For the purpose of comparison to our method, it suffices to use 30 iterations, as the standard errors
converge significantly to indicate the magnitude of the cost of using the Monte Carlo method.
17. Since for g4 each Monte Carlo iteration is relatively time consuming, we run it only once; thus, no
confidence interval is generated, i.e., the third step is omitted.

636

Computation of the Shapley Value for Game-Theoretic Network Centrality

K12 ) with weights drawn from a uniform distribution U (0, 1). Then, for each graph
and each of the two parameters Wcutoff (vi ) = 41 α(vi ) and Wcutoff (vi ) = 34 α(vi ):18
• We compute the exact Shapley value using formula (3).
• Then, we run our approximation algorithm and determine the error in our approximation.
• Finally, we run 2000 and 6000 Monte Carlo iterations for K6 and K12 , respectively.
2. For large networks, we again generate 30 random instances of weighted complete
graphs, but now with 1000 nodes (we denote them K1000 ). Then, for each graph
and each of the three parameters Wcutoff (vi ) = 14 α(vi ), Wcutoff (vi ) = 42 α(vi ), and
Wcutoff (vi ) = 43 α(vi )):
• We run our approximation algorithm for the Shapley value.
• Then, we run the fixed number (200000) of Monte Carlo iterations.
• Finally, we compute how the Monte Carlo solution converges to the results of our
approximation algorithm.
Having described the simulation setup, we will now discuss the data sets and, finally, the
simulation results.
5.2 Data Used in Simulations
We consider two networks that have already been well-studied in the literature. Specifically,
for games g1 − g3 we present simulations on an undirected, unweighted network representing
the topology of the Western States Power Grid (WSPG).19 This network (which has 4940
nodes and 6594 edges) has been studied in many contexts before (see, for instance, Watts &
Strogatz, 1998) and is freely available online (see, e.g., http://networkdata.ics.uci.edu/
data.php?id=107). For games g3 − g5 (played on weighted networks), we used the network
of astrophysics collaborations (abbreviated henceforth APhC) between Jan 1, 1995 and
December 31, 1999. This network (which has 16705 nodes and 121251 edges) is also freely
available online (see, e.g., http://networkdata.ics.uci.edu/ data.php?id=13) and has
been used in previous studies like Newman (2001).
5.3 Simulation Results
The results presented in this section show that our exact algorithms are, in general, much
faster then the Monte Carlo sampling, and this is the case even if we allow for generous
error tolerance. Furthermore, requiring smaller Monte Carlo errors makes the Monte Carlo
runtime exponentially slower than our exact solution.
18. Recall that αj is the sum of all the weights in Wj as defined in Section 4.5.
19. Note that with the distance threshold dcutoff replaced with a hop threshold kcutoff , game g3 can be played
on an unweighted network.

637

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

2000

1600

1600

1400

1400

1200

1200

1000
800

1000
800

600

600

400

400

200

200
0.43 ms

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

2000

100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 6: g2 , k = 2, WSPG (UW)
2000

Monte Carlo solution
Our exact solution

1800

0.55 ms

0

0

Figure 5: g1 , WSPG (UW)

Monte Carlo solution
Our exact solution

1800

1600

1600

1400

1400

1200

1200

Time (ms)

Time (ms)

Monte Carlo solution
Our exact solution

1800

Time (ms)

Time (ms)

2000

Monte Carlo solution
Our exact solution

1800

1000
800

1000
800

600

600

400

400

200

200
0.52 ms

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 7: g2 , ki =

degi
2 ,

10

0

0.53 ms

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 8: g2 , ki =

WSPG (UW)

3
4

10

0

degi , WSPG (UW)

In more detail, the simulation results for game g1 are shown in Figure 5. The dotted line
shows the performance of our exact algorithm which needs 0.43ms to compute the Shapley
value. In contrast, generating any reasonable Monte Carlo result takes a substantially longer
time (the solid line shows the average and the shaded area depicts the confidence interval
for Monte Carlo simulations). In particular, it takes on average more than 200ms to achieve
a 20% error and more than 2000ms are required to guarantee a 5% error (which is more
than 4600 times slower than our exact algorithm).
3
i
Figures 6 - 8 concern game g2 for different values of k (k = 2, ki = deg
2 , and ki = 4 degi ,
20
respectively, where degi is the degree of node vi ). The advantage of our exact algorithm
over Monte Carlo simulation is again exponential.

Replacing the distance threshold dcutoff with a hop threshold kcutoff enables game g3 to be
played on an unweighted network. Thus, similarly to games g1 and g2 , we test it on the
Western States Power Grid. The results are shown on Figures 9 and 10 for kcutoff being
equal to 2 and 3, respectively. The third game is clearly more computationally challenging
than g1 and g2 (note that the vertical axis is in seconds instead of milliseconds). Now,
20. Recall that in g2 the meaning of parameter k is as follows: the value of coalition C depends on the
number of nodes in the network with at least k neighbours in C.

638

28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13

Monte Carlo solution
Our exact solution

Time (s)

Time (s)

Computation of the Shapley Value for Game-Theoretic Network Centrality

12.88 s
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13

Monte Carlo solution
Our exact solution

13.03 s
100

Figure 9: g3 , kcutoff = 2, WSPG (UW)

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 10: g3 , kcutoff = 3, WSPG (UW)

our exact algorithm takes about 13s to complete. The much lower speedups of the exact
methods with respect to Monte Carlo approach stem from the fact that both algorithms
have to start with Dijkstra’s algorithm. Although this algorithm has to be run only once in
both cases it takes more than 12.5s for the considered network. This means that the exact
solution is slower by orders of magnitude (compared to games g1 and g2 ). The Monte Carlo
approach is also slower, but this slowdown is much less significant in relative terms.
Figures 11 and 12 show the performance of the algorithms for game g3 on the astrophysics
collaboration network that, unlike the Western States Power Grid, is a weighted network.
d
davg
We observe that increasing the value of dcutoff (here from dcutoff = avg
8 to dcutoff = 4 )
significantly worsens the performance of the Monte Carlo-based algorithm. This is because
the increasing number of nodes that have to be taken into account while computing marginal
contributions (see the inner loop in Algorithm 8) is not only more time consuming, but also
increases the Monte Carlo error.
1
For game g4 the performance of algorithms is shown in Figures 13 - 15 (for f (d) = 1+d
,
1
−d
f (d) = 1+d2 and f (d) = e , respectively). Whereas the Monte Carlo methods for the
first three games are able to achieve a reasonable error bound in seconds or minutes, for
the fourth game it takes more than 40 hours to approach 50% error. This is because the
inner loop of the Marginal Contribution block (see Algorithm 9) iterates over all nodes in
the network. Due to the time consuming performance we run the simulations only once.
Interestingly, we observe that the error of the Monte Carlo method sometimes increases
slightly when more iterations are performed. This confirms that the error of the Monte
Carlo method to approximate the Shapley value proposed in Castro et al. (2009) is only
statistically decreasing in time. Certain new randomly chosen permutations can actually
increase the error.

Figures 16, 17, 18 and 19 present comparisons of our approximation algorithm for game g5
against Monte Carlo sampling for small networks (for which the exact Shapley value can
be computed from the definition in formula (3)). In these figures, the horizontal dotted
line shows the running time of our solution, while the vertical dotted line shows its average
approximation error with the shaded area being the confidence interval. As previously, the
639

48
46
44
42
40
38
36
34
32
30
28
26
24
22
20
18
16
14
12
10
8
6
4

Monte Carlo solution
Our exact solution

160
140
120
100
80
60
40
20
3.47 min
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 11: g3 , dcutoff =

40

davg
8 ,

10

4.41 min

0

100

35

35

30

30

25

25

20

15

10

10

5

5
8.78 min
90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 13: g4 , f (d) =

1
1+d ,

10

40

APhC (W)

8.88 min

0
90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 14: g4 , f (d) =

APhC (W)

0

Monte Carlo solution
Our exact solution

100

0

davg
4 ,

10

20

15

100

80 70 60 50 40 30 20
Maximal allowable MC error (%)

40

Monte Carlo solution
Our exact solution

0

90

Figure 12: g3 , dcutoff =

APhC (W)

Time (h)

Time (h)

Monte Carlo solution
Our exact solution

180

Time (min)

Time (min)

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

1
,
1+d2

10

0

APhC (W)

Monte Carlo solution
Our exact solution

35
30

Time (h)

25
20
15
10
5
9.18 min

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 15: g4 , f (d) = e−d , APhC (W)
solid line shows the average, and the shaded area depicts the confidence interval for the
Monte Carlo simulations. We see in Figures 16, 17 and 18 that the approximation error
in our proposed algorithm is well-contained for small networks. Specifically, for K6 it is
about 10%; whereas for the bigger network K12 it is about 5%. However, we notice that, for
higher values of Wcutoff , the Monte Carlo method may slightly outperform our solution. See
640

7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

14
10.57
6. .74 %
91 %
%

Monte Carlo solution
Our approximation solution

Time (ms)

Time (ms)

14
10.43
6. .29 %
15 %
%

Computation of the Shapley Value for Game-Theoretic Network Centrality

0.38 ms
100 90

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

0

Monte Carlo solution
Our approximation solution

0.38 ms
100 90

4.
7
4. 1 %
9
5. 1 %
12
%

Monte Carlo solution
Our approximation solution

Time (ms)

Time (ms)

26
24
22
20
18
16
14
12
10
8
6
4
2
0

2.82 ms
100 90

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

0

Figure 17: g5 , Wcutoff = 34 αi , K6 (W),

5.
2
5. 6 %
5
5. 4 %
81
%

Figure 16: g5 , Wcutoff = 14 αi , K6 (W)

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

0

26
24
22
20
18
16
14
12
10
8
6
4
2
0

Monte Carlo solution
Our approximation solution

2.74 ms
100 90

Figure 18: g5 , Wcutoff = 14 αi , K12 (W)

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

0

Figure 19: g5 , Wcutoff = 43 αi , K12 (W)

in Figure 17 how the average approximation error of the Monte Carlo sampling achieved in
0.38ms is lower than the average error achieved by our method. Already for K12 this effect
does not occur (see Figure 19).
For large networks, where the exact Shapley value cannot be obtained, we are naturally
unable to compute exact approximation error. We believe that this error may be higher
than the values obtained for K6 and K12 . However, the mixed strategy, that we discussed
in Section 4 and that uses our approximation only for large degree vertices, should work
towards containing the error within practical tolerance bounds. As far as we believe that
Monte Carlo gives good results, from Figure 20, we can infer that our approximation solution
for large networks gives good results (within 5%) and is at least two times faster than the
Monte Carlo algorithm.
To summarise, our exact solutions outperform Monte Carlo simulations even if relatively
wide error margins are allowed. However, this is not always the case for our approximation
algorithm for game g5 . Furthermore, it should be underlined that if the centrality metrics
under consideration cannot be described with any of the games g1 to g4 for which exact
algorithms are now available, then Monte Carlo simulations are still a viable option.
641

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Our approximation solution
Monte Carlo solution Wcutoff=0.25α
Monte Carlo solution Wcutoff=0.50α
Monte Carlo solution Wcutoff=0.75α

22
20
18

Time (min)

16
14
12
10.09 min

10
8
6

0.75α

4

0.50α

2

0.25α

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 20: g5 , Wcutoff = 14 αi , Wcutoff = 24 αi , and Wcutoff = 43 αi , K1000 (W)

6. Conclusions and Future Work
The key finding of this paper is that the Shapley value for many centrality-related cooperative
games of interest played on networks can be solved analytically. The resulting algorithms
are not only error-free, but also run in polynomial time and, in practice, are much faster
than Monte Carlo methods. Approximate closed-form expressions and algorithms can also
be constructed for some classes of games played on weighted networks. Simulation results
show that these approximations are acceptable for a range of situations.
There are a number of directions for future work. On one hand, the Shapley value-based
extensions of other centrality notions, that suit particular applications, can be developed. As
a step in this direction, the first study of the Shapley value-based betweenness centrality has
been recently presented by Szczepański, Michalak, and Rahwan (2012). On the other hand,
it would be interesting to analyze what other coalitional games defined over a network would
better reflect centrality of nodes in certain real-life applications. In this spirit, recent works of
del Pozo, Manuel, González-Arangüena, and Owen (2011) and Amer, Giménez, and Magana
(2012) focus on generalized coalitional games in which the order of agents forming coalitions
matter. Nevertheless, there are still other classes of coalitional games, such as games with
either positive or negative externalities (Yi, 1997), that have been extensively studied in
game theory and that may yield interesting results when applied to network centrality.
Another interesting application for which a new class of coalitional games defined over a
network could be developed is the problem of influence maximization, already mentioned in
the introduction.
It is also interesting to analyse the properties of game-theoretic network centralities constructed on solution concepts from cooperative game theory other than the Shapley value.
In particular, if the game defined over a network belongs to the class of simple coalitional
games (i.e., with a binary characteristic function) then the Banzhaf power index (Banzhaf,
1965) could be also used as a centrality metric. Otherwise, more general solution concepts
such as the core (Osborne & Rubinstein, 1994) or the nucleolus (Schmeidler, 1969) could be
applied.
642

Computation of the Shapley Value for Game-Theoretic Network Centrality

Ultimately, it would be interesting to develop a more formal and general approach that
would allow us to construct coalitional games defined over networks that correspond to
other known centrality metrics or even entire families of them.21 Such an approach would
involve developing a group centrality first and then building a characteristic function of
a coalitional game upon it. Of course, while developing new centrality metrics based on
coalitional games, one should keep in mind computational the properties of the proposed
solutions. Although we were able to obtain satisfactory computational results for the games
considered in this paper, the computation of the game-theoretic network centrality may
become much more challenging for more complex definitions of the characteristic function.

Acknowledgments
We would like to thank three anonymous reviewers for their comments on the earlier version
of the paper that helped to improve it considerably. Also, we would like to thank dr Talal
Rahwan and dr Suri Rama Narayanam for proofreading, helpful comments and suggestions.
Tomasz Michalak was partially supported by the European Research Council under Advanced Grant 291528 (“RACE”). Nicholas R. Jennings (and partially Tomasz Michalak) was
supported by the ORCHID Project, funded by EPSRC (Engineering and Physical Research
Council) under the grant EP/I011587/1.

Appendix A. Marginal Contribution Blocks for Algorithm 6 for g2 -g5
Algorithm 7: Marginal Contribution block of Algorithm 6 for g2
Counted ← false ;
Edges ← 0 ;
foreach vi ∈ V (G) do
foreach u ∈ NG (vi ) ∪ {vi } do
Edges[u]++ ;
if !Counted[u] and ( Edges[u] ≥ k[u] or u = vi ) then
SV[vi ]++ ;
Counted[u] = true ;
end
end
end
For each of the games considered in our paper the Marginal Contribution block of Algorithm 6 takes a slightly different form. In the main text we explained the functioning of
this block for g1 . In this appendix, we discuss this block for the remaining four games. In
particular:
g2 : Here, node vi makes a positive contribution to a coalition P both through itself and
through some adjacent node u also under two conditions. Firstly, neither vi nor u are
21. We thank an anonymous reviewer for this suggestion.

643

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Algorithm 8: Marginal Contribution block of Algorithm 6 for g3
Counted ← false ;
foreach vi ∈ V (G) do
foreach u ∈ extN eighbors(vi ) ∪ {vi } do
if !Counted[u] then
SV[vi ]++ ;
Counted[u] = true ;
end
end
end
Algorithm 9: Marginal Contribution block of Algorithm 6 for g4
dist ← infinity ;
foreach vi ∈ V (G) do
foreach u ∈ V (G) do
if D[u] < dist[u] then
SV[vi ] += f(D[u]) - f(dist[u]) ;
dist[u] = D[u] ;
end
end
SV[vi ] += f(dist[u]) - f(0) ;
dist[vi ] = 0 ;
end

in P . Secondly, there is less than k edges from P to vi and there is exactly k − 1 edges
from P to u. In order to check the first condition in Algorithm 7 we use the array
Counted, and to check the second one, we use the array Edges. For each node vi ,
the algorithm iterates through the set of its neighbours and for each adjacent node it
checks whether this adjacent node meets these two conditions. If so, then the marginal
contribution of the node vi is increased by one.
g3/4 : In Marginal Contribution blocks for games g3 and g4 (Algorithms 8 and 9), all the
values that are dependent on the distance (extN eighbours and D) are calculated
using Dijkstra’s algorithm and stored in memory. These pre-computations allow us
to significantly speed up Monte Carlo methods. Now, in g3 node vi makes a positive
contribution to coalition P through itself and through some adjacent node u under
two conditions. Firstly, neither vi nor u are in P . Secondly, there is no edge length of
dcutoff from P to vi or u. To check for these conditions in Algorithm 8 we again use the
array Counted. For each node vi , the algorithm iterates through the set of its extended
neighbours and for each of them it checks whether this neighbour meets the conditions.
If so, the marginal contribution of the node vi is increased by one. In game g4 , node
vi makes a positive contribution to coalition P through each node (including itself)
that is closer to vi than to P . In Algorithm 9 we use array Dist to store distances
644

Computation of the Shapley Value for Game-Theoretic Network Centrality

Algorithm 10: Marginal Contribution block of Algorithm 6 for g5
Counted ← false ;
Weights ← 0 ;
foreach vi ∈ V (G) do
foreach u ∈ NG (vi ) ∪ {vi } do
weights[u]+= W (vi , u);
if !Counted[u] and ( weights[u] ≥ Wcutoff (u) or u = vi ) then
SV[vi ]++ ;
Counted[u] = true ;
end
end
end

from coalition P to all nodes in the graph and array D to store all distances from vi
to all other nodes. For each node vi , the algorithm iterates through all nodes in the
graph, and for each node u, if the distance from vi to u is smaller than from P to u,
the algorithm computes the marginal contribution as f (D[u]) − f (Dist[u]). The value
Dist[u] is then updated to D[u]—this is a new distance from P to u.
g5 : In game g5 , which is an extension of g2 to weighted graphs, node vi makes a positive
contribution to coalition P (both through itself and through some adjacent node u)
under two conditions. Firstly, neither vi nor u are in P . Secondly, the sum of weights on
edges from P to vi is less than Wcutoff (vi ) and the sum of weights on edges from P to u is
greater than, or equal to, Wcutoff (u)−W (vi , u) and smaller than Wcutoff (vi )+W (vi , u).
In order to check the first condition in Algorithm 10 we use the array Counted, and
to check the second one, we use the array W eights. For each node vi , the algorithm
iterates through the set of its neighbours and for each adjacent node it checks whether
this adjacent node meets these two conditions. If so, then the marginal contribution
of the node vi is increased by one.

Appendix B: Main Notation Used in the Paper
A

The set of players.

ai

A player in A.

C

A coalition.
A value of the coalition, where ν is characteristic function.

ν(C)
(A, ν)/gi
SVgj (vi )
G = (V, E)
G = (V, E, W )
W (v, u)

A coalitional game.
The Shapley value od the vertex vi in game gj .
Unweighted graph/network consisting of the set of vertices V and edges E.
Weighted graph/network.
Weight on the edge from v to u.
645

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

V (G)/V, E(G)/E
vi ∈ V

the set of vertices and edges in graph G.

deg(vi )

The vertex from the set V .
Degree of the vertex vi .

NG (vi )

Set of neighbours of vertex vi ∈ G.

distance(v, u)/d(v, u) The distance between vertices v and u.
Extended neighbourhood: NG (vj , dcutoff ) = {vk 6= vj : distance(vk , vj ) ≤
NG (vi , dcutof f )
dcutoff }.
Marginal contribution that vertex u makes through vertex v.
M C(u, v)
Π(A)
π ∈ Π(A)
π(i)
Cπ (i)

The set of all orders of players in A.
The single ordering of agents in A.
The position of i − th element in ordering π.
{aj ∈ π : π(j) < π(i)}.

E[·]

{v ∈ V (G) : v ∈ C (or) ∃u ∈ C such that (u, v) ∈ E(G)}.
The number assigned to vertex v used in Game 2. The minimum number of
adjacent nodes necessary to influence node vi .
The number assigned to vertex v used in Game 5. Minimum sum of weights
on adjacent edges necessary to influence node vi .
The expectation operator.

P[·]

The probability operator.

O(·)

The big O complexity notation.

f ringe(C)
k(vi )/ki
Wcutoff (vi )

B, X, Y
N (µ, σ 2 )
erf (·)

Random variables.
Normal distribution with mean µ and variance σ 2 .

αj

The error function.
The sum of all the weights of incident edges to vertex vj .

βj

The sum of the squares of all the weights of incident edges to vertex vj .

f (.)

A positive valued decreasing function.

Ki

The complete graph (clique) with i nodes.

References
Aadithya, K., Michalak, T., & Jennings, N. (2011). Representation of coalitional games with
algebraic decision diagrams. In AAMAS ’11: Proceedings of the 10th International
Joint Conference on Autonomous Agents and Multi-Agent Systems, pp. 1121–1122.
Amer, R., Giménez, J., & Magana, A. (2012). Accessibility measures to nodes of directed
graphs using solutions for generalized cooperative games. Mathematical Methods of
Operations Research, 75, 105–134.
Aziz, H., Lachish, O., Paterson, M., & Savani, R. (2009a). Power indices in spanning connectivity games. In AAIM ’09: Proceedings of the 5th International Conference on
Algorithmic Aspects in Information and Management, pp. 55–67.
646

Computation of the Shapley Value for Game-Theoretic Network Centrality

Aziz, H., Lachish, O., Paterson, M., & Savani, R. (2009b). Wiretapping a hidden network.
In WINE ’09: Proceedings of the the 5th Workshop on Internet & Network Economics,
pp. 438–446.
Bachrach, Y., Markakis, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A. (2008a).
Approximating power indices. In AAMAS ’08: Proceedings of the 7th International
Joint Conference on Autonomous Agents and Multi-Agent Systems, pp. 943–950.
Bachrach, Y., Rosenschein, J. S., & Porat, E. (2008b). Power and stability in connectivity
games. In AAMAS ’08: Proceedings of the 7th International Joint Conference on
Autonomous Agents and Multi-Agent Systems, pp. 999–1006.
Bachrach, Y., & Rosenschein, J. (2009). Power in threshold network flow games. Autonomous
Agents and Multi-Agent Systems, 18 (1), 106–132.
Banzhaf, J. F. (1965). Weighted Voting Doesn’t Work: A Mathematical Analysis. Rutgers
Law Rev., 19, 317–343.
Bikhchandani, S., Hirshleifer, D., & Welch, I. (1992). A theory of fads, fashion, custom,
and cultural change in informational cascades. Journal of Political Economy, 100 (5),
992–1026.
Bolus, S. (2011). Power indices of simple games and vector-weighted majority games by
means of binary decision diagrams. European Journal of Operational Research, 210 (2),
258–272.
Bonacich, P. (1972). Factoring and weighting approaches to status scores and clique identification. Journal of Mathematical Sociology, 2 (1), 113–120.
Bonacich, P. (1987). Power and centrality: A family of measures. American Journal of
Sociology, 92 (5), 1170–1182.
Borgatti, S. P., & Everett, M. (2006). A graph-theoretic framework for classifying centrality
measures. social networks. Social Networks, 28(4), 466–484.
Brandes, U. (2001). A faster algorithm for betweenness centrality. Journal of Mathematical
Sociology, 25 (2), 163–177.
Brandes, U., & Erlebach, T. (2005). Network Analysis: Methodological Foundations. Lecture
notes in computer science: Tutorial. Springer.
Castro, J., Gomez, D., & Tejada, J. (2009). Polynomial calculation of the shapley value
based on sampling. Computers & Operations Research, 36 (5), 1726–1730.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational Aspects of Cooperative Game Theory. Synthesis Lectures on Artificial Intelligence and Machine Learning.
Morgan & Claypool Publishers.
Conitzer, V., & Sandholm, T. (2004). Computing Shapley Values, manipulating value division schemes and checking core membership in multi-issue domains. In AAAI ’04:
Proceedings of the Nineteenth National Conference on Artificial Intelligence, pp. 219–
225.
Cormen, T. (2001). Introduction to algorithms. MIT Press.
647

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

del Pozo, M., Manuel, C., González-Arangüena, E., & Owen, G. (2011). Centrality in directed
social networks. a game theoretic approach. Social Networks, 33 (3), 191–200.
Deng, X., & Papadimitriou, C. (1994). On the complexity of cooperative solution concepts.
Mathematics of Operations Research, 19 (2), 257–266.
Elkind, E., Goldberg, L., Goldberg, P., & Wooldridge, M. (2009). A tractable and expressive
class of marginal contribution nets and its applications. Mathematical Logic Quarterly,
55 (4), 362–376.
Eppstein, D., & Wang, J. (2001). Fast approximation of centrality. In SODA ’01: Proceedings
of the Twelfth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 228–229.
Everett, M. G., & Borgatti, S. P. (1999). The centrality of groups and classes.. Journal of
Mathematical Sociology, 23 (3), 181–201.
Fatima, S. S., Wooldridge, M., & Jennings, N. (2007). A randomized method for the shapley
value for the voting game. In AAMAS ’07: Proceedings of the 11th International Joint
Conference on Autonomous Agents and Multi-Agent Systems, pp. 955–962.
Fatima, S. S., Wooldridge, M., & Jennings, N. (2008). A linear approximation method for
the shapley value. Artificial Intellilgence, 172 (14), 1673–1699.
Freeman, L. (1979). Centrality in social networks: Conceptual clarification. Social Networks,
1 (3), 215–239.
Gómez, D., González-Arangüena, E., Manuel, C., Owen, G., Del Pozo, M., & Tejada, J.
(2003). Centrality and power in social networks: A game theoretic approach. Mathematical Social Sciences, 46 (1), 27–54.
Goyal, A., Bonchi, F., & Lakshmanan, L. V. (2010). Learning influence probabilities in
social networks. In WSDM ’10: Proceedings of the 3rd ACM international conference
on Web search and data mining, pp. 241–250.
Granovetter, M. (1978). Threshold models of collective behavior. American Journal of
Sociology, 83 (6), 1420–1443.
Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2009). On the complexity of compact
coalitional games. In IJCAI ’09: Proceedings of the Twenty First International Joint
Conference on Artifical Intelligence, pp. 147–152.
Grofman, B., & Owen, G. (1982). A game-theoretic approach to measuring centrality in
social networks. Social Networks, 4, 213–224.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: a compact representation
scheme for coalitional games. In EC ’05: Proceedings of the Sixth ACM Conference on
Electronic Commerce, pp. 193–202.
Irwin, M., & Shapley, L. S. (1960). Values of Large Games, IV : Evaluating the Electoral
College by Montecarlo Techniques. CA : RAND Corporation, Santa Monica.
Jeong, H., Mason, S. P., Barabasi, A. L., & Oltvai, Z. N. (2001). Lethality and centrality in
protein networks. Nature, 411 (6833), 41–42.
Kempe, D., Kleinberg, J., & Tardos, É. (2003). Maximizing the spread of influence through
a social network. In KDD ’03: Proceedings of the Ninth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, pp. 137–146.
648

Computation of the Shapley Value for Game-Theoretic Network Centrality

Kempe, D., Kleinberg, J., & Tardos, É. (2005). Influential nodes in a diffusion model for
social networks. Automata, Languages and Programming, 3580, 99.
Koschützki, D., Lehmann, K., Peeters, L., Richter, S., Tenfelde-Podehl, D., & Zlotowski, O.
(2005). Centrality indices. Network analysis, Vol. 3418 of Lecture Notes in Computer
Science, pp. 16–61. Springer.
Lee, M.-J., Lee, J., Park, J. Y., Choi, R. H., & Chung, C.-W. (2012). Qube: a quick
algorithm for updating betweenness centrality. In Mille, A., Gandon, F. L., Misselis,
J., Rabinovich, M., & Staab, S. (Eds.), WWW, pp. 351–360. ACM.
Mann, I., & Shapley, L. (1962). Values of large games, VI: Evaluating the electoral college
exactly.. RAND Research Memorandum.
Matsui, T., & Matsui, Y. (2000). A survey of algorithms for calculating power indices of
weighted majority games. Journal of the Operations Research Society of Japan, 43,
71–86.
Michalak, T., Marciniak, D., Samotulski, M., Rahwan, T., McBurney, P., Wooldridge, M.,
& Jennings, N. (2010a). A logic-based representation for coalitional games with externalities. In AAMAS ’10: Proceedings of the 9th International Joint Conference on
Autonomous Agents and Multi-Agent Systems, pp. 125–132.
Michalak, T., Rahwan, T., Marciniak, D., Szamotulski, M., & Jennings, N. (2010b). Computational aspects of extending the Shapley Value to coalitional games with externalities.
In ECAI ’10: Proceedings of the Nineteenth European Conference on Artificial Intelligence.
Myerson, R. (1977). Graphs and cooperation in games. Mathematics of Operations Research,
2 (3), 225–229.
Nagamochi, H., Zeng, D., Kabutoya, N., & Ibaraki, T. (1997). Complexity of the minimum
base game on matroids. Mathematics of Operations Research, 22 (1), 146–164.
Newman, M. (2001). Scientific collaboration networks. II. Shortest paths, weighted networks,
and centrality. Physical Review E, 64 (1), 016132(1–7).
Noh, J., & Rieger, H. (2004). Random walks on complex networks. Physical Review Letters,
92 (11), 118701(1–4).
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory, Vol. 1 of MIT Press
Books. The MIT Press.
Reka, A., & Barabási, A. (2002). Statistical mechanics of complex networks. Reviews of
Modern Physics, 74, 47–97.
Sakurai, Y., Ueda, S., Iwasaki, A., Minato, S., & Yokoo, M. (2011). A compact representation
scheme of coalitional games based on multi-terminal zero-suppressed binary decision
diagrams. In PRIMA ’11: Public Risk Management Association, pp. 4–18.
Schmeidler, D. (1969). The nucleolus of a characteristic function game. SIAM Journal on
Applied Mathematics, 1163–1170.
Schultes, D., & Sanders, P. (2007). Dynamic highway-node routing. In SEA ’07: Proceedings
of 6th Workshop on Experimental and Efficient Algorithms. LNCS, pp. 66–79. Springer.
649

Michalak, Aadithya, Szczepański, Ravindran, & Jennings

Shapley, L. S. (1953). A value for n-person games. In Kuhn, H., & Tucker, A. (Eds.), In
Contributions to the Theory of Games, volume II, pp. 307–317. Princeton University
Press.
Shapley, L. S., & Shubik, M. (1954). A Method for Evaluating the Distribution of Power in
a Committee System. The American Political Science Review, 48 (3), 787–792.
Stephenson, K., & Zelen, M. (1989). Rethinking centrality: Methods and examples. Social
Networks, 11 (1), 1–37.
Suri, N., & Narahari, Y. (2008). Determining the top-k nodes in social networks using the
shapley value. In AAMAS ’08: Proceedings of the 7th International Joint Conference
on Autonomous Agents and Multi-Agent Systems, pp. 1509–1512.
Suri, N., & Narahari, Y. (2010). A Shapley Value based approach to discover influential
nodes in social networks. IEEE Transaction on Automation Science and Engineering,
99, 1–18.
Szczepański, P. L., Michalak, T., & Rahwan, T. (2012). A new approach to betweenness
centrality based on the shapley value. In AAMAS ’12: Proceedings of the 11th International Joint Conference on Autonomous Agents and Multi-Agent Systems, pp.
239–246.
Valente, T. (1996). Social network thresholds in the diffusion of innovations. Social Networks,
18 (1), 69–89.
Watts, D., & Strogatz, S. (1998). Collective dynamics of small-world networks. Nature,
393 (6684), 440–442.
Wooldridge, M., & Dunne, P. (2006). On the computational complexity of coalitional resource games. Artificial Intelligence, 170 (10), 835–871.
Yi, S.-S. (1997). Stable coalition structures with externalities. Games and Economic Behavior, 20 (2), 201–237.
Young, H. P. (2006). The Diffusion of Innovations in Social Networks. In Blume, B. L., &
Durlauf, S. N. (Eds.), Economy as an evolving complex system, Vol. 3 of Proceedings
volume in the Santa Fe Institute studies in the sciences of complexity Santa Fe Institute
Studies on the Sciences of Complexity, pp. 267–282. Oxford University Press US.
Zolezzi, J. M., & Rudnick, H. (2002). Transmission cost allocation by cooperative games
and coalition formation. IEEE Transactions on Power Systems, 17, 1008–1015.

650

Journal of Artificial Intelligence Research 46 (2013) 343-412

Submitted 08/12; published 03/13

A Hybrid LP–RPG Heuristic for Modelling Numeric
Resource Flows in Planning
Amanda Coles
Andrew Coles
Maria Fox
Derek Long

amanda.coles@kcl.ac.uk
andrew.coles@kcl.ac.uk
maria.fox@kcl.ac.uk
derek.long@kcl.ac.uk

Department of Informatics
King’s College London
Strand Building
London, WC2R 2LS, UK

Abstract
Although the use of metric fluents is fundamental to many practical planning problems,
the study of heuristics to support fully automated planners working with these fluents
remains relatively unexplored. The most widely used heuristic is the relaxation of metric
fluents into interval-valued variables — an idea first proposed a decade ago. Other heuristics
depend on domain encodings that supply additional information about fluents, such as
capacity constraints or other resource-related annotations.
A particular challenge to these approaches is in handling interactions between metric
fluents that represent exchange, such as the transformation of quantities of raw materials
into quantities of processed goods, or trading of money for materials. The usual relaxation
of metric fluents is often very poor in these situations, since it does not recognise that
resources, once spent, are no longer available to be spent again.
We present a heuristic for numeric planning problems building on the propositional
relaxed planning graph, but using a mathematical program for numeric reasoning. We
define a class of producer–consumer planning problems and demonstrate how the numeric
constraints in these can be modelled in a mixed integer program (MIP). This MIP is then
combined with a metric Relaxed Planning Graph (RPG) heuristic to produce an integrated
hybrid heuristic. The MIP tracks resource use more accurately than the usual relaxation,
but relaxes the ordering of actions, while the RPG captures the causal propositional aspects
of the problem. We discuss how these two components interact to produce a single unified
heuristic and go on to explore how further numeric features of planning problems can be
integrated into the MIP. We show that encoding a limited subset of the propositional problem to augment the MIP can yield more accurate guidance, partly by exploiting structure
such as propositional landmarks and propositional resources. Our results show that the
use of this heuristic enhances scalability on problems where numeric resource interaction
is key in finding a solution.

1. Introduction
Domain-independent planning research in the last decade has focussed, for the most part,
on propositional planning, leading to important discoveries and powerful new heuristics for
planning in propositional domains. Relatively little effort has been invested in planning with
metric fluents, despite their importance in representing many practical planning problems.
c
2013
AI Access Foundation. All rights reserved.

Coles, Coles, Fox & Long

Numbers are essential for efficiently encoding resources, such as money, fuel and materials.
The state-of-the-art remains the influential approach proposed by Hoffmann (2003), which
extends the ignore-delete-effects relaxation from propositional fluents to metric fluents by
tracking the accumulating upper bound on increasing fluents and ignoring decreasing (negative) effects. A symmetrical treatment of the fluents for the purposes of determining a
lower bound leads to a representation which is equivalent to an interval for each metric
fluent, with goals and preconditions being satisfied provided that some value in the interval
is sufficient to satisfy each condition. Although lpg (Gerevini, Saetti, & Serina, 2006) and
mips (Edelkamp, 2003) are both capable of handling metric fluents, both depend on the
same relaxation heuristic to offer search guidance. Other approaches have been explored
(Do & Kambhampati, 2001; Koehler, 1998), but have been comparatively less successful.
Planners using the MetricFF heuristic are generally not very effective at solving problems in which there are complex interactions between the values of numeric resources, such
as the exchange of quantities of one or more materials for the production of others. In
contrast, solving problems with numbers is at the heart of operational research and mathematical programming techniques. Many powerful solvers have been developed for solving
Linear Programming problems (LPs) and Mixed Integer Programming problems (MIPs),
in which problems are expressed as linear constraints over variables (which, in the case of
MIPs, can be integers). Although there have been efforts to exploit linear programming
in propositional planning, either to schedule actions (Long & Fox, 2003a) or directly, as a
heuristic (van den Briel, Benton, Kambhampati, & Vossen, 2007), relatively little work has
considered the exploitation of linear programming techniques to improve the behaviour of
numeric domain-independent planners (Kautz & Walser, 2000; Shin & Davis, 2005; Wolfman & Weld, 2000; Benton, van den Briel, & Kambhampati, 2007) (this work is further
considered in Section 1.1).
In this paper, we revisit the issue of planning with numeric resources, beginning with the
Metric Relaxed Planning Graph (RPG) heuristic (Hoffmann, 2003). We focus specifically
on domains that exclusively exhibit what we call producer-consumer behaviour (defined in
Section 2.3), in which actions increase or decrease numeric resources by fixed quantities.
Of course, this represents only a subset of possible numeric behaviours, but it is a common
and intuitive one. Furthermore, it is easy to recognise syntactically in a domain encoding,
so it is simple to resort to alternative strategies for domains that do not conform to this
constraint, which might include the use of producer-consumer relaxations or approximations
of domains with more complex numeric behaviour.
We explore the behaviour of the RPG heuristic, demonstrating how the very typical
patterns of interactions in producer-consumer numeric planning domains can lead to highly
uninformative heuristic guidance, particularly when domains offer opportunities for exchanges between metric variables. To address this, we introduce a novel heuristic based
on a mixed integer program (MIP), used alongside the RPG, to better capture numeric
constraints. Having described how the MIP is constructed, and how it can be used to complement the RPG, we discuss extensions of it to improve identified weakness, and also to
encode more information about the propositional behaviour of the problem. We evaluate
the lp-rpg heuristic by exploring the spectrum between, at one end, a strict separation of
numbers and propositions into the MIP and RPG components; and, on the other, discarding
344

A Hybrid LP-RPG Heuristic for Planning

the RPG entirely and encoding the preconditions and effects of actions entirely as a MIP.
In doing so, we will determine where the best trade-off between the two lies.
The work we report in this paper is an extension of our earlier work reporting development of lp-rpg (Coles, Fox, Long, & Smith, 2008). It extends that work both with
additional detail and with several variants of the core heuristic, exploring the impact of
tighter integration between the propositional and metric fluents in the heuristic.
1.1 Related Work
The integration of linear programming (LP) or MIP techniques into planning has been
considered in a number of contexts. The most relevant to the present work is the use of an
LP as the basis of a heuristic for propositional over-subscription planning problems (Benton,
Do, & Kambhampati, 2005). In this setting, the goal of planning is to find a plan with
maximum utility, defined in terms of the reward for the goals achieved, minus the costs
occurred in achieving them. Benton et al. use the LP as an optimisation tool to help to
decide which set of goals the planner should satisfy in order to achieve maximum reward.
Both the work of Benton et al. and the work described in this paper exploit a relaxation of
the action ordering rather than of their effects, and both employ an LP as well as an RPG
structure. The two key differences are that the focus in this work is on using the MIP to
capture interactions within numeric planning problems, and we will rely on a conventional
Relaxed Planning Graph (Hoffmann & Nebel, 2001) for propositional reasoning, rather than
also encoding this structure in the MIP. The work of van den Briel et al. (van den Briel et al.,
2007; van den Briel, Vossen, & Kambhampati, 2008) also explores the use of mathematical
programming to encode and solve planning problems.
The structure in the LP and MIP models proposed by Benton et al. (2007) and van den
Briel et al. (2008) makes them time consuming to solve, requiring actions to be selected
to satisfy preconditions and effects of other actions, and delete effects to be paired with
add effects. In contrast, the MIP and LP models we propose do not attempt to capture
most of the causal plan structure, making the construction of solutions to our programs at
each state much more feasible. A further difference is in the interaction between the two
components: the integration between the MIP and the RPG in lp-rpg is much tighter,
with the MIP being used in graph building to indicate variable bounds, and in relaxed plan
extraction to indicate the actions to use. By comparison, in the earlier approach, the MIP
is used solely to introduce a bias into RPG action selection, giving preference to actions
used in the solution of the MIP.
Linear programming has been exploited in planning in other work. Lpsat (Wolfman
& Weld, 2000) uses a planning-as-satisfiability approach, linked to the use on an LP solver
to ensure that literals representing (linear) constraints on metric fluents are maintained
during the plan construction. There is no heuristic guidance in the search, which is based
on a standard DPLL search for a satisfying assignment combined with confirmation that
the corresponding LP is satisfiable. Ip-sat (Kautz & Walser, 2000) uses a MIP encoding of
planning problems as the basis for solving them, as do Vossen et al. (1999), in a similar way
to the later work of van den Briel et al. (van den Briel et al., 2008). In these planners the MIP
is used directly at the heart of the solver, with planning problems being translated into MIPs
rather than being used to guide the search. Tm-lpsat (Shin & Davis, 2005) uses the lpsat
345

Coles, Coles, Fox & Long

system to solve planning problems with continuous processes. Kongming (Li & Williams,
2008) is another example of a planner that exploits compilation of planning problems into
mathematical programs, solved using CPLEX, to tackle hybrid mixed-continuous planning
problems. Our own colin system (Coles, Coles, Fox, & Long, 2012) also uses LP encodings
to manage reasoning about the effects of continuous processes.
A completely different use of linear and mixed integer programming in planning lies in
work by Ono and Williams (2008) and also Blackmore, Ono and Williams (2011) which uses
mixed integer programming as the foundation for solving the problem of risk allocation in
plan-level control systems.
Alternative approaches to handling numeric variables in planning include those implemented in MetricFF, discussed in detail in Section 3, Sapa (Do & Kambhampati, 2001)
and Resource-ipp (Koehler, 1998). In Sapa heuristic cost estimates generated using relaxed plan extraction are supplemented with additional costs representing the minimal set
of additional resource producing actions required to achieve the resource requirements of
the relaxed plans. This approach is straightforward to implement and is an interesting
modification of the pure relaxed plan heuristic, but it separates the problem of producing
resources from the solution of the rest of the problem, with the consequence that a relaxed
plan using a few steps with high resource demands will be constructed in preference to a
longer plan with lower demands. The heuristic value of the state will then be distorted by
the penalty attached to the relaxed plan to achieve its high resource requirement, potentially hugely overestimating the true distance of the state to the goal. Resource-ipp depends
on the identification of consumers and producers, as we do in this paper, and it then builds
a resource time map that tracks the production and consumption of the resources during
a Graphplan-based search for a plan. The approach leads to an extension of the mutex
relation that is used to constrain the search in Graphplan (Blum & Furst, 1995). However,
the iterative-deepening search used in Graphplan-based planners is not scalable to solve
large problems and forward state-space search has proved a dominant strategy in the past
decade.

2. Problem Definition
In this section we define the class of planning problems that we will consider in this paper.
They are a subset of the general class of pddl 2.1 non-temporal, numeric planning problems,
which represent linear producer–consumer problems. We include as an example the Settlers
domain, which will then be used throughout the paper to illustrate the ideas presented.
2.1 PDDL 2.1 Numeric Planning Problems
In this work, we are concerned with finding sequential plans to solve non-temporal, numeric
planning problems, as defined using (a subset of) pddl 2.1 (Fox & Long, 2003). Within
pddl, this class of problems can be defined as follows:1
1. pddl2.1 also supports the specification of an objective function to measure plan quality, defined over the
numeric variables in the planning problem, but in this work we focus only on minimising plan length.

346

A Hybrid LP-RPG Heuristic for Planning

• I is the initial state, where a state consists of a set of propositions, and/or an assignment of values to a set of numeric variables. For notational convenience, we refer to
the vector of numeric values in a given state as v and the propositional facts as F .
• A, a set of actions. Each a ∈ A is a tuple hpre, eff i:
– pre the preconditions of a: these conditions must hold in the state in which a is
to be executed.
– eff the effects of a: when a is applied, the state is updated according to these
effects. eff consists of:
∗ eff − , propositions to be deleted from the state;
∗ eff + , propositions to added to the state;
∗ eff n , effects acting upon numeric variables.
• G, a goal: a set of propositions F ? and a set of conditions over numeric variables,
N ? . Each of these sets may be empty. A state hF, vi is a goal state if F ? ⊆ F and v
satisfies each condition in N ? .
In the general case, pddl numeric conditions (as used in pre and N ? ) are expressed in
the form:
hf (v), op, ci
s.t.
op ∈ {≤, <, =, >, ≥}, c ∈ <
Numeric effects (as in eff n) are expressed as:
hv, op, f (v)i

s.t.

op ∈ {×=, +=, =, –=, ÷=}

In common with Hoffmann’s work on MetricFF (2003) we restrict our attention to
preconditions that can be expressed in Linear Normal Form (LNF). That is, the expression
f (v) within preconditions must be in the form of a weighted sum of the state variables
plus a constant, w.v + k. Likewise, we consider only numeric effects where f (v) is in LNF,
and op ∈ {+=, =, –=}. These restrictions guarantee termination in the construction of
the RPG when evaluating a state: introducing non-LNF preconditions, or scaling effects,
can lead to asymptotic numeric behaviour where certain conditions are only satisfied at an
infinite limit. For the lp-rpg heuristic we describe in this work, we further require that the
numeric behaviour of actions can be represented as producer–consumer behaviour. That is,
all effects cause constant increments or decrements to the variables they affect and, apart
from in specific circumstances, we do not permit assignment effects. We will precisely define
these notions and the circumstances in which we allow assignment effects later in the paper.
A solution to a planning problem is a (sequential) plan: a sequence of actions that transforms the initial state into a goal state, respecting all preconditions on action application.
In a state hF, vi, the application of an action with effects eff − ,eff + ,eff n yields a successor
state hF 0 , v0 i, where:
F 0 = (F \ eff − ) ∪ eff +
v0 [x] op (w.v + c) if ∃hv, op, w.v + ci ∈ eff n
v0 [x] = v[x]
otherwise
347

Coles, Coles, Fox & Long

2.2 An Example Problem: Settlers
The Settlers domain, introduced in the 2002 International Planning Competition (IPC) (Long
& Fox, 2003b) and used again in 2004 (Hoffmann & Edelkamp, 2005), is a good example
of a problem exhibiting interesting use of metric fluents. The aim in Settlers problems is
to build up transport and building infrastructure through the extraction, refinement and
transportation of materials. The numeric structure of the domain is perhaps the most sophisticated of the IPC domains to date. First, there are six numeric resources and several
actions that act upon each. The available resources, and the effects of actions upon them
(consumption of a resource is shown as a negative value and production is shown as a
positive value) are shown in Table 12 . Another interesting feature of this domain is that
not all resources can be directly produced: whilst the raw materials Timber, Stone and
Ore can be directly extracted, Wood, Coal and Iron must be refined from their respective
raw form. Finally, the domain contains transferable resources. In addition to the actions
shown in the table, by which resources can be refined or consumed to fuel transportation,
resources can be loaded and unloaded from vehicles. The effect of such load and unload
actions is to increase or decrease the amount of a resource on a vehicle, and decrease or
increase the amount stored at a given location. Apart from consuming or ‘producing’ (i.e.
releasing) the remaining cargo space of the vehicle, no resource is produced or consumed
during loading and unloading — it is only moved. However, expressing the model in pddl
requires the pair of effects described, decreasing one variable and increasing another, which
is indistinguishable from a combination of production and consumption.
2.3 Producer–Consumer Problems
We now define the constrained producer–consumer numeric behaviour considered in this
paper. We first define producer or consumer actions, with two categories of producer. Using
these we then define the notion of a producer–consumer variable. The identification of
consumers and producers is not a new idea — it is common to identify resource producers
and consumers in scheduling (for example, Laborie does so in his work on scheduling with
resource constraints, see Laborie, 2003).
2.3.1 Producer–Consumer Actions
A simple production action is defined as follows:
Definition 2.1 — Simple Producer
A ground action a is a simple producer of a given numeric variable v iff:
• it has an effect (increase (v) c) (where c is a positive constant) and
• it has no precondition that refers to v.
This definition has two important consequences:
2. The table represents a debugged version of the original domain available at http://sourceforge.net/
projects/tsgp/files/

348

A Hybrid LP-RPG Heuristic for Planning

Action
Move cart
Move train
Move ship
Fell timber
Quarry stone
Mine ore
Saw wood
Make coal
Smelt iron
Build cabin
Build quarry
Build mine
Build saw-mill
Build iron-works
Build coal-stack
Build dock
Build wharf
Build house
Build cart
Build train
Build ship
Build rail

Timber

Stone

Ore

Wood

Coal

Iron

-1
-2
+1
+1
+1
-1
-1

+1
+1
-2

-1

+1

-2
-2
-2

-2

-2
-2
-1

-2

-1
-2
-1

-1
-1

-2
-4
-1

Table 1: Production and consumption in the Settlers domain
1. A simple producer produces uniformly: if a state s satisfies its preconditions, then
the effect by a upon v is always to increase its value by the same constant amount, c,
irrespective of the precise details of s.
2. The potential maximum value of v that can be attained through the use of a is not
restricted by the value of v itself: there are no minimum or maximum bounds on v
that must hold to allow production.
We define a bounded producer as follows:
Definition 2.2 — Bounded Producer
A ground action a is a bounded producer of a numeric variable v iff:
• it has an effect (increase (v) c) (where c is a positive constant),
• it has a precondition (<= (v) (- ub c)) and
• it has no other preconditions depending on v.
A bounded producer, a, can only be applied if v ≤ (ub − c). Therefore, the maximum
amount of v that can be attained using a, denoted max prod (a, v), is ub, achieved by
applying a in a state where v = (ub − c). (In practice, such a state might not be reachable
and the actual upper bound on the value of v reachable using a might be lower than ub).
For an simple producer, a0 , we assume max prod (a0 , v) = ∞.
349

Coles, Coles, Fox & Long

We define a bounded consumer as follows:
Definition 2.3 — Bounded Consumer
A ground action a is a consumer with respect to a given numeric variable v iff:
• it has a precondition (>= (v) (+ lb c))),
• it has an effect (decrease (v) c), where c is a constant and
• it has no other preconditions depending on v.
This definition is analogous to the bounded producer, since it requires that v exceed a
minimum value before allowing consumption. As a consequence, lb is the minimum amount
of v that can be attained using a, denoted min cons(a, v) (by applying a in a state where
v = lb + c).
There are, of course, many other resource use behaviours that might be encoded in
planning domains. The producer-consumer behaviour we identify is a natural and intuitive
one (a producer produces a fixed quantity of a resource and a consumer consumes a fixed
quantity and depends on the availability of that quantity). There are variants that can
be compiled into this form (e.g. consumers that must leave a fixed sized store of resource
untouched simply translate the origin of the resource measurement) and we also consider,
below, other possible extensions of this basic behaviour. Nonetheless, we must emphasise
that the heuristic we develop in this paper is targeted at this producer-consumer behaviour
and its usefulness depends on how common such domains are in practice. The frequent
occurrence of this model in scheduling with resources suggests that it is a natural and
useful behaviour.
2.3.2 Producer–Consumer Variables
With these definitions of (bounded) producer and consumer actions, we define properties
of the variables that they manipulate:
Definition 2.4 — Producer–Consumer Variable
A variable v denotes a resource that is produced/consumed iff:
• the set prod (v) of actions that increase the value of v contains only bounded producers,
• the set cons(v) of actions that decrease the value of v contains only consumers,
• the upper bound on v is the same in all bounded producers for v and
• the lower bound on v is the same in all consumers for v.

350

A Hybrid LP-RPG Heuristic for Planning

2.3.3 Handling Integer Resources
Consumer actions (Definition 2.3) each require that the amount of a resource available
for consumption must be at least as much as the consumer actually consumes. In some
domain encodings, behaviour that is essentially consistent with producer–consumer patterns
is represented using a precondition on a consumer action that consumes c units of v in the
form v > k rather than v ≥ c (and k < c). In the general case, where v ∈ <, all we know is
that if v ≥ k +  (where epsilon is infinitesimal and positive), c units of v can be consumed,
suggesting a lower bound on v of (k − c + ). However, in the case that all consumers
consume integral quantities, we can rewrite the strict inequality since  must be 1 in this
case. Consider, for example, this fragment of a load action from Settlers:
:precondition (> (available timber l1) 0)
:effect
(decrease (available timber l1) 1)
Because the effects that change the quantities of available resources are integral, this
can be rewritten:
:precondition (>= (available timber l1) 1)
:effect
(decrease (available timber l1) 1)
A similar transformation can be used when all the constant effects on a variable are
rational, simply by finding the least common multiple, LCM , of the denominators of the
fractions involved and using  = 1/LCM .

3. MetricFF Revisited
In this section we briefly review the way in which MetricFF (Hoffmann, 2003) handles metric
fluents and highlight some of the weaknesses in this approach when faced with particular
kinds of numeric behaviours in planning domains.
3.1 The Metric Relaxed Planning Graph Heuristic
The Metric RPG heuristic is based on performing a relaxed reachability analysis forwards
from the state to be evaluated, where the reachability analysis is captured in a planning
graph (Blum & Furst, 1995) structure. Two elements of the domain are relaxed: delete
conditions of actions are ignored and optimistic upper and lower bounds are used to record
the interval of possible values that a metric fluent may reach. Positive effects on a metric
variable increase the upper bound on its reachable values and negative effects decrease its
lower bound. Satisfaction of preconditions is tested by checking some value in the interval of
each variable satisfies each metric condition in the precondition. It is interesting to note that
preconditions are tested individually, so it is possible, in principle, that no single value could
satisfy all the conditions simultaneously, even though each condition is separately satisfied
by some value. Conjunctions of convex preconditions, which includes linear conditions, will
be satisfiable by some value in the case that each condition is satisfiable in the interval
associated with a variable, except in the case that the conjunction is inconsistent, which is
likely to arise only in erroneous domain encodings.
351

Coles, Coles, Fox & Long

MetricFF allows preconditions to combine multiple variables, and effects to depend
on the values of other variables. In lp-rpg we allow linear combinations of variables in
preconditions, but effects must conform to the producer–consumer definitions above and
allow only constant increases or decreases.
Heuristic evaluation of a state using the Metric RPG heuristic is undertaken in two
phases: the graph expansion phase and the solution extraction phase. We now remind the
reader of these two processes for convenience of reference in the discussion that follows.
3.1.1 Metric RPG Expansion
Graph expansion can be concisely defined as follows:
Definition 3.1 — RPG Expansion
Let F (i) denote a fact layer, comprising:
• F P (i), a set of propositions;
• F V (i), an array of upper- and lower- bound pairs for each task numeric variable v
A(i) denotes an action layer, consisting of a list of ground actions. An RPG begins with a
fact layer, F (0), defined based on the state S to be evaluated:
• F P (0) contains the propositions that hold in S;
• Each entry hLBv , UBv i ∈ F V (0) is set to hS[v], S[v]i, i.e. the value of v in S.
The RPG is expanded by adding successive action layers, followed by new fact layers:
• Action layer A(i + 1) contains all actions a ∈ A, such that:
– the propositional preconditions of a are in F P (i);
– the numeric preconditions of a are satisfied for some values of the variables in
F V (i).
• Fact layer F P (i + 1) is then determined from A(i + 1):
– The propositions F P (i + 1) are each of F P (i), plus any new facts added by an
action in A(i + 1);
– The values of the numeric values F V (i + 1) are first set to F V (i), then updated
by extending the interval for each variable to include the values achieved by the
maximum and minimum possible assignment effects, for each action a ∈ A(i + 1)
in turn.
• Until the termination condition is met, the RPG is expanded with further actionlayer–fact-layer pairs.
The reachability analysis therefore consists of alternate steps: determining which actions
are applicable, and hence instantiating the next action layer, and then using these to extend
the next fact layer. This process is presented graphically in Figure 1, for a small problem
with facts f0 ...fn and numeric variables v0 , v1 . Considering first the propositions:
352

A Hybrid LP-RPG Heuristic for Planning

Fi

A i+1

F i+1

f0

A i+2

F i+2

f0

f0

f1

A

f1

A

f1

f2

B

f2

B

f2

f3

f3

f4

v0[0,0]

v0[0,2]
−2

v1[2,2]

+2

f4

>=

+2

2

C

>=

2

C

v0[0,4]
−2

v1[0,2]

v1[−2,2]

Figure 1: Portion of a relaxed planning graph, where C produces 2 units of v0, but consumes
2 units of v1
• Arrows from fact to action layers denote the precondition dependencies of actions —
for instance, action A can appear in layer A(i + 1) because f0 is present in F (i).
• Arrows from action to fact layers denote effects — for instance, f3 is in F (i + 1)
because it was added by A.
For the numeric variables v0 , v1 , the bounds are shown in square brackets. Action C
can be seen to have one precondition (v1 ≥ 2) and two effects (increase v0 by 2, decrease v1
by 2). C exhibits producer–consumer behaviour — it consumes 2 units of v1 , and produces
2 units of v0 . Its preconditions are satisfied in F (i) and therefore its effects are applied in
layer A(i + 1), where the upper bound on v0 has increased and the lower bound on v1 has
decreased. Moreover, the bounds change again in layer F (i + 2), through a further possible
application of C.
Because variable bounds can continue to diverge in this way, RPG expansion needs a
well-defined termination condition. In the positive case, we can terminate with success at
the first layer F (i) where all goal propositions are in F P (i) and all goal numeric expressions
are satisfied by F V (i) (in the relaxed sense). In the negative case, we terminate with failure
at F (i) if all three of the following hold:
1. no actions appear in A(i + 1) that were not present in A(i) (and hence no new propositions would be present in F P (i + 1)),
2. for all hitherto unsatisfied preconditions v ≥ c, of any action, U B(v) would not change
between F V (i) and F V (i + 1), and
3. for all hitherto unsatisfied preconditions v ≤ c, of any action, LB(v) would not change
between F V (i) and F V (i + 1).
353

Coles, Coles, Fox & Long

Algorithm 1: Adding an action to a relaxed plan
Data: R — a metric RPG, a — an action to include in the relaxed plan, q — the subgoal
queue
1 foreach propositional precondition pre of a do
2
l ← layer at which pre first appears;
3
if l > 0 then insert pre into q[l].prop;
4
5
6

foreach numeric precondition pre of a do
l ← layer at which pre first appears;
if l > 0 then insert pre into q[l].num;

The intuition behind these conditions is that the monotonic expansion of the RPG
implies that, if no new facts are appearing and no more numeric preconditions could become satisfied at a future layer, graph expansion has stagnated and the relaxed problem is
unsolvable.
3.1.2 Metric RPG Solution Extraction
Having expanded the planning graph and found that a relaxed solution exists (all the
goals have appeared), the next step is to extract a relaxed solution plan. This is done
by regressing through the planning graph, using a priority queue of intermediate sub-goals
(latest layer first). For each subgoal, an achieving action is added to the relaxed plan and
its preconditions are added to the queue as goals to be achieved at an earlier layer.
The relaxed plan extraction algorithm is shown in Algorithm 2. In lines 3– 6, the priority
queue is initialised with the top-level goals of the problem. Both propositional and numeric
goals are added to the priority queue to be achieved in the earliest fact layer in which they
appeared. Once the priority queue is seeded, solution extraction proceeds by regressing
layer-by-layer. For propositions, it suffices to find an action that adds the fact and then
to increment the heuristic value by one and add the preconditions of the achieving action
to the queue, using Algorithm 1. For numeric preconditions, the process is slightly more
involved:
• If the subgoal is to achieve v ≥ k or v ≤ k at layer F (l), and there is an action in A(l)
that assigns the value of k to v, then that action is chosen to satisfy the subgoal.
• Otherwise, if v ≥ k (or v ≤ k) must be achieved at layer F (l), then actions increasing
(decreasing) v are chosen from those in A(l) until the residual value of k (i.e. the
original value of k adjusted to take into account the effects of the selected actions) is
small enough (large enough) to be reachable in layer F (l − 1). The residual condition
v ≥ k (v ≤ k) is added to the queue as a subgoal to be achieved in F (l − 1), with the
modified value of k.
Note that at lines 13, 19, 25, 32 and 39, the actions chosen from action layer 1 are
recorded by adding them to the set ha. These are used as the basis of the helpful action set:
any action with an effect in common with the action set ha is considered helpful. Helpful
actions are an important element in the performance of MetricFF: the actions that achieve
effects that are exploited in the relaxed solution from a state are promoted in the search
from that state.
354

A Hybrid LP-RPG Heuristic for Planning

Algorithm 2: Relaxed plan extraction

5

Data: R - a metric RPG; F ? , N ? - problem goals
Result: ha - helpful actions, h - A heuristic value
ha ← ∅, h ← 0;
q ← deepest-first priority queue of goal layers;
foreach p ∈ F ? do
l ← layer at which p first appears;
insert p into q[l].prop;

6
7
8

foreach f ∈ N ? do
l ← layer at which f first holds;
insert f into q[l].num;

9

while q not empty do
(l, hprop, numi) ← pop(q);
foreach p ∈ prop do
h ← h + 1; a ← an achiever for p;
if a in action layer 1 then add a to ha;
prop ← prop \ add effects of a;
call Algorithm 1 with R, a, q;

1
2
3
4

10
11
12
13
14
15
16
17
18
19
20
21

foreach (v ≥ c) ∈ num do
if an action a ∈ A(l) assigned v = k, k ≥ c then
h ← h + 1;
if l = 1 then add a to ha;
call Algorithm 1 with R, a, q;
remove all (v ≥ c0 ), c0 ≤ k and (v ≤ c0 ), c0 ≥ k from num;

22
23
24
25
26

foreach (v ≤ c) ∈ num do
if an action a ∈ A(l) assigned v = k, k ≤ c then
h ← h + 1;
if l = 1 then add a to ha;
call Algorithm 1 with R, a, q;
remove all conditions (v ≤ c), c ≥ k from num;

27
28
29
30
31
32
33
34
35
36
37
38
39
40
41

foreach (v ≥ c) ∈ num do
while F V (l − 1)[v].upper < c do
h ← h + 1; a ← next increaser of v;
decrease c by δ(v, a);
if l is 1 then add a to ha;
call Algorithm 1 with R, a, q;
if l > 0 then insert (v ≥ c) into q[l − 1].num;
foreach (v ≤ c) ∈ num do
while F V (l − 1)[v].lower > c do
h ← h + 1; a ← next decreaser of v;
increase c by δ(v, a);
if l is 1 then add a to ha;
call Algorithm 1 with R, a, q;
if l > 0 then insert (v ≤ c) into q[l − 1].num;

355

Coles, Coles, Fox & Long

3.1.3 Use of the Relaxed Plan During Search
The relaxed plan, computed during heuristic calculation, is used in two ways during search.
MetricFF makes use of a two-stage search approach, and in both the number of actions
in the relaxed plan is used as a heuristic goal-distance estimate. The first search phase,
enforced hill-climbing (EHC), is a greedy hill climbing search approach. It can be thought
of as performing breadth first search forward from the initial state I, with state progression
through action application, until a state with new global-best heuristic value is found.
When such a state, S, is found, all other states are discarded and EHC search continues
in the same manner from S. This search strategy is incomplete due to its greedy nature,
discarding all states other than S could lead to the loss of a solution. As such, it is followed
by a complete WA* search, in order to guarantee completeness of the planner (subject to
sufficient time and memory).
Since the EHC phase is already incomplete, but designed to find solutions quickly,
MetricFF makes use of another completeness-sacrificing technique in order to attempt to
guide the planner to solutions more quickly. This technique is referred to as helpful action
pruning. Here, only actions that are in the helpful action set for each state are considered
for successor generation: the actions that are not helpful are discarded. Note this pruning
is not used in best-first search as it would compromise completeness. In practice helpful
action pruning improves the performance of MetricFF on many domains, however, it can
lead to difficulties if the actions needed to find a solution from a given state do not appear
in the helpful action set. To attempt to compensate for this, if EHC terminates after
considering only helpful actions, it returns to the state with the last global-best heuristic
value, and searches again considering all applicable actions, until either it terminates once
again (leading to WA*) or a state with a new global-best heuristic value is found, at which
point helpful-action pruning is re-enabled, and EHC continues.
3.2 Problems with the Metric RPG Heuristic
Although the Metric RPG is a powerful tool to support planning with metric fluents, there
are some common situations in planning problems in which the heuristic gives very flawed
guidance. These problems include resource persistence and cyclical resource transfer. We
discuss how each of these phenomena can result in misleading heuristic guidance through
the relaxed plan length poorly approximating the actual solution length and through helpful
action distortion.
3.2.1 Resource Persistence
Resource persistence is a consequence of using a relaxation that ignores negative effects.
When a resource is consumed it does not disappear in the relaxation of negative effects.
The opportunity to reuse the resource can suggest that there is a significantly shorter plan
available than is the case in reality. Although this problem occurs for both propositional
and metric fluents, the fact that metric fluents commonly encode resources that must be
carefully managed means that the problem is often more acute in domains with metric
fluents. For example, in a state in the Settlers domain in which 2 units of each resource
have been produced and no ship is required (either as a goal or as a means of travel to an
356

A Hybrid LP-RPG Heuristic for Planning

otherwise inaccessible location), no relaxed plan will require the production of any further
resources (see Table 1).
One approach for approximating the number of missing resource production actions
was introduced in the planner Sapa (Do & Kambhampati, 2001). If v = s in the state
being evaluated, and the relaxed plan consumes c units of v, but only produces p, then
if (c − p) > s, there is a shortfall on production of v, which would necessitate additional
actions being added to the relaxed plan. In this case, if the maximum amount of v that can
be produced by a single action is ∆v, the heuristic value is increased by:


c−p−s
.
∆v
This increase is a lower bound on the number of additional actions needed and, while it does
not indicate what the actions might be, serves to increase the heuristic value of states whose
relaxed plans have resource production shortfalls. It does, however, have two main limitations. First, relaxed plan extraction (such as the approach shown in Algorithm 2) chooses
actions without consideration for their undesirable resource consumption side-effects. A
good search choice might lead to a state with a worse heuristic value purely because of an
accident of the choice of achieving actions (consuming resource unnecessarily). Second, by
increasing the heuristic value without adding specific additional actions to the relaxed plan,
the helpful actions do not record the fact that appropriate resource production is helpful.
3.2.2 Cyclical Resource Transfer
The phenomenon of Cyclical Resource Transfer (CRT) is a consequence of the encoding
of actions that move resources around, combined with the relaxation of negative effects.
To encode movement of a resource, it is removed from one location and added to another.
The removal is encoded as a decrease and this is relaxed when building the Metric RPG.
As a result, moving resources appears to generate new resource at the destination, making
movement a spuriously attractive alternative to production. Consider a state in which 1
unit of timber and a cart are at a location, p1, and the goal is to have 2 units of timber
at p1. Clearly, the solution plan must involve the production of more timber. However, a
valid relaxed plan solution, found using the Metric RPG heuristic, is:
0:
1:

(load v1 p1 timber)
(unload v1 p1 timber)

3.2.3 Helpful Action Distortion
The problems of resource persistence and CRT have an important consequence for EHC
search. Not only do they result in relaxed plans with misleadingly short lengths, but the
relaxed plans typically contain too few production actions and useless transfer actions. In
this situation, production actions often do not appear in the helpful action set, and are
therefore not included in EHC search, even in states where they could conveniently be
applied. We refer to this problem as helpful action distortion. To illustrate how this arises,
again with reference to the Settlers domain, consider a state in which there is a unit of
timber at location a, and the goal is to have a unit of timber at location b. A relaxed
plan can use the timber to construct a cart and then load the same timber onto the cart to
transport it. The planner will therefore not consider producing more timber.
357

Coles, Coles, Fox & Long

4. Compiling Producer–Consumer Behaviour into a Mathematical
Program
In this section we describe how a mathematical program can be built to characterise the
interaction between numeric variables and action choices in the producer–consumer framework.
4.1 Constraints for Producer–Consumer Variables and Actions
The definition of producer–consumer variables (Definition 2.4) implies that actions have the
useful property that all preconditions on the variables can be derived from the effects of
actions, together with the global variable bounds. Specifically, for each action a:
• If a produces c units of v and has a precondition requiring v ≤ d, then d = ub(v) − c,
where ub(v) is the global upper bound on v after a has been applied. This condition
expresses both the effect of a on the upper bound and the precondition on the value
of v (since v ≤ ub(v)).
• If a consumes c units of v then v must satisfy v ≥ lb(v) + c before the action is
applied. Again, this expression leads to the effect and precondition being tied into
one constraint.
If the ordering of actions is relaxed (that is, the causal relations that force them to be
ordered are ignored) then the value of v after a series of actions has been applied, v 0 , is
given by:
X
v0 = v +
Ca .δ(v, a)
(1)
a∈A

where Ca is a non-negative (count) variable indicating how many times the action a is
applied, v 0 ∈ [lb(v), ub(v)] and δ(v, a) is defined as follows:
• If a produces c units of v then δ(v, a) = c;
• If a consumes c units of v then δ(v, a) = −c;
• Otherwise, δ(v, a) = 0.
Note that this equation is linear, since Definitions 2.2 and 2.3 require that δ(v, a) is constant
for any v and a.
These equations support the construction of a mathematical program consisting of one
variable for each action, a, and one variable and flow equation for each producer–consumer
variable, v. The program is, in fact, a mixed integer program (MIP), because the variables
(the action counts, a) represent applications of actions, which can only be integral. However,
a further relaxation can be exploited to allow the action count variables to take non-integral
values, yielding a linear program (LP). The significant potential benefit of doing so is that
LPs can be solved far more efficiently than MIPs.
358

A Hybrid LP-RPG Heuristic for Planning

4.2 Bounding Action Variables
Within the equation associated with each state variable (i.e. each Equation 1), each action
has a corresponding variable denoting how many times it has been applied. In general there
is no limit on the number of times an action can be applied. However, numeric decrease
effects and propositional delete effects may impose constraints in practice, due to limited
availability of resources. For example, if applying an action a increases v at the expense
of decreasing w, where w is a resource for which there is no producer, then the limit on
Ca will be an implicit consequence of the instance of Equation 1 governing the value of w.
Specifically, ai can never exceed w/δ(w, a): the value of w divided by the change a causes in
w. Moreover, because w is monotonically decreasing and δ(w, a) is constant for each action
a, the global upper bound on Ca can be set to w(I)/δ(w, a), where w(I) is the value of w
in the initial state, I.
If an action a increases v at the expense of irreversibly deleting some fact p, a fact it has
as a precondition, then clearly a can only be applied once — it is a one-shot action (Coles,
Coles, Fox, & Long, 2009). However, in contrast with the numeric delete effects discussed
above, this will not be captured by the producer–consumer constraints which are concerned
only with numeric change. However, the constraint on the use of a can be captured by
setting the upper bound on Ca to 1. Moreover, if a collection of actions a0 ...an−1 each
depend on a fact p and each irreversibly delete it, we can say that they form a one-shot
action set over p, and add the constraint:
Ca0 + Ca1 + ... + Can−1 ≤ 1

(2)

4.3 Assignment Constraints
In general, direct assignments of values to variables cannot be represented directly in constraints following the form of Equation 1. Assignments correspond, effectively, to statedependent increases or decreases. For instance, an assignment of the value 2 to a variable
v, in a state where v = 0, is equivalent to producing 2 units, but is equivalent to consuming
1 unit when applied in a state where v = 3. However, in the producer–consumer equations
there is no notion of state, only coefficients on action variables to denote their production or
consumption. Therefore, the state variables can only be subject to constant-valued change
and the MIP cannot be used to encode general assignment effects without extending it to
allow quadratic constraints (that is, constraints involving products of pairs of variables).
However, there are some specific conditions under which assignments can be safely modelled
within the mathematical program while retaining the linearity of constraints.
One class of assignment effects that can be encoded in the MIP is that in which the
actions with assignment effects to a variable, v = k, can only be applied in states in
which v = c for some known constant, c. In this case, the effect can be rewritten as an
increase effect on v of k − c, making the assignment actions follow the standard pattern for
producer/consumer actions. A particular case in which this rewriting is made possible is if
the following conditions hold of the set of actions A:
1. No action can depend upon or affect v before some condition is satisfied that is
achieved by each of the actions in A and only by actions in A.
2. Applying any action in A precludes any further assignments to v.
359

Coles, Coles, Fox & Long

These conditions ensure that the set of actions that can assign to v form a one-shot action
set and the value of v can be assumed to be 0 prior to application of one of the actions in
this set, with each action in the set being rewritten to increase v by its assignment value.
This situation is one that arises in encodings in which objects are created by certain actions
and those objects have associated metric variables that are initialised on object creation
(such as the capacity of a newly created vehicle in the Settlers domain).

5. The Linear Programming–Relaxed Planning Graph Heuristic
We have defined producer–consumer behaviour and shown how it supports construction of a
MIP in which action ordering is relaxed. A MIP is, in principle, hard to solve (constructing
solutions is NP-hard), so as the basis of a heuristic evaluation of states it seems sensible
to further relax the integrality constraints on action variables to reduce the problem to a
linear program. We will first consider how the LP can be used in the two stages of heuristic
evaluation of a state (reachability analysis and relaxed plan extraction) and then we will
reconsider the question of whether the relaxation to a linear program is necessary in practice
and what compromise there might be between the full MIP and the LP.
5.1 Overview
The context for the use of the LP is in a forward state-space search planner. The task
for which we intend to use it is the heuristic evaluation of states. Thus, we can assume
that we have a state (a complete assignment to the variables that define the problem, both
propositional and numeric) and we are interested in estimating the number of actions that
will be required to transition to a goal state. The approach we will use is based on the same
strategy as is used in MetricFF: first construct a reachability analysis using a layered graph
of alternating facts and actions and then extract a relaxed plan. To determine whether
an action can be applied in the reachability stage we check propositional preconditions in
the usual way and numeric preconditions are checked by determining whether some values
in the reachable ranges recorded for the metric variables will satisfy the condition. The
reachable ranges are calculated using the LP we have described, as we explain below.
Extraction of the relaxed plan involves determining which actions support required
conditions (both goals and preconditions of selected actions), and where the conditions
involve numeric variables we use the LP to decide which actions will be required and how
many of them will be used.
5.2 Using the LP during Graph Expansion
The graph expansion phase in calculation of the Metric RPG heuristic can be seen as
a layer-by-layer relaxed propositional reachability analysis, synchronised with a relaxed
numeric bounds analysis. Definition 3.1 shows that numeric variable bounds appear in the
graph expansion algorithm in two places. First, at F (i) they are used to determine which
actions can appear in A(i + 1), the next action layer (those whose preconditions are within
the reachable range). Then, the actions deemed applicable are used to update the variable
bounds for the subsequent fact layer, F (i + 1).
360

A Hybrid LP-RPG Heuristic for Planning

maximise: v00
v00 = 0 + 2.CC
v10 = 2 − 2.CC
v00 ≥ 0
v10 ≥ 0
C≥0

v00
1
1

v10

1

CC
-2
2

=
=
≥
≥
≥

1
1
1

(a) Equations

max
0
2
0
0
0

v00 ∈ [0, 2]
v10 ∈ [0, 2]
(c) Solutions for min/max

(b) Row–Column

Figure 2: LP to maximise the value of v0 in layer F (i + 2) of Figure 1 (treating layer Fi as
the initial state for this construction)

Due to the relaxed nature of the way numeric values are considered, the metric RPG
tends to produce highly optimistic bounds on the numeric values. Returning to Figure 1,
we can see that, in effect, the action C converts two units of v1 into two units of v0 and,
initially, just two units of v1 are present. Hence, at most, we could hope to produce two of
v1 , but in F V (i + 2), the upper bound on v1 is already 4. Ignoring the consumption effect
of C makes it possible to produce arbitrary amounts of v0 : C is applicable if the upper
bound on v1 is greater than or equal to two, which in reality is only true so long as C has
not yet been applied.
A more accurate estimate of the variable bounds in layer F V (i) can be found using the
LP encoding described in Section 4.1. The model is parameterised as follows:
• Only those action variables corresponding to actions in A(i) are used, which ensures
that only reachable actions are considered in computing resource bounds. Any relevant
one-shot action constraints are included. The absence of any restriction on the number
of action applications contrasts with the constraint used in Metric FF that each action
may only be applied once per action layer. In practice to prevent the LP variables
becoming unbounded, we set a finite, but large maximum value for action variables.
• The initial value of each variable v is set from the state, S, being evaluated.
• The post-value of a variable, v 0 , is F V (i)[v], the range of values it could reach by
F V (i). As always, v 0 ∈ [lb(v), ub(v)].
Substituting these parameters into the producer–consumer equation (Equation 1) yields:
X
F V (i)[v] = S[v] +
Ca .δ(v, a)
(3)
a∈A(i)

With this model, we can then use an LP solver to find upper and lower bounds on each
F V (i)[v], by setting the objective function accordingly.
Returning to our example, consider finding the upper bounds on the variables in F (i+2)
of Figure 1 starting from a state corresponding to the one given as Fi in that figure (thus, we
are considering the constraints in layer 2 following the state we are treating as our starting
point). The corresponding LP is shown in Figure 2, where the primed variables are the ones
361

Coles, Coles, Fox & Long

we use to represent the values of the numeric variables in the layer of interest (i.e. in this
case the layer 2 ahead of the state being evaluated). Maximising v00 , i.e. using the upper
bound of F V (i)[v0 ], yields the result 2: no greater value is possible, since setting C to a
value greater than 1 (and thus producing more v0 ) would lead to a violation of the constraint
v10 ≥ 0. The ranges of the variables computed, using the LP four times (minimising and
maximising each of the two variables), are shown in Figure 2c. As can be seen, these
improve over the bounds calculated in the same situation by MetricFF (Figure 1), which
are v00 ∈ [0, 4] and v10 ∈ [−2, 2] respectively.
5.2.1 Notes on LP Efficiency
As the LP has to be solved up to twice per variable per layer when expanding the planning
graph, it is important that steps are taken to minimise the computational cost. We reduce
the costs using a combination of techniques, some that avoid needing to solve the LP when
computing the bound on a given variable, and others which reduce the cost of solving the
LP itself.
1. First, as a consequence of the termination criteria for RPG expansion (Section 3.1.1),
there is no need to compute the upper (lower) bound on a given variable if its current
value is large enough (small enough) to satisfy all the preconditions and goals in which
it appears. In this case, we can avoid having to solve the LP to determine the variable
bound, and instead can re-use the bound computed at the previous layer, without
affecting the behaviour of the heuristic.
2. If a variable never appears in a numeric precondition or a goal, it can be entirely
excluded from the LP.
3. The bounds on variables change monotonically as additional layers are added to the
planning graph. Therefore, when computing the new upper (lower) bound on a variable v we can temporarily add to the LP the constraints corresponding to the bounds
computed at the preceding layer (each is added separately as the variable is first minimised and then maximised). By doing so, we refuse to admit a tighter variable bound
than in the previous layer.
4. Finally, If no actions with an increase (decrease) effect on a variable have yet been
added to the LP, we do not need to compute the upper (lower) bound on the variable,
as there is no effect by which the bound on the variable can be increased (decreased)
beyond the value in the state being evaluated.
5.3 Basic Use of the LP during Solution Extraction
We now consider how the LP can be used to give guidance in action selection during relaxed
plan extraction. First, we observe that the LP is not directly affected by propositions, and
hence cannot be used to find which actions are able to achieve a given fact. Thus, we
concern ourselves with how the LP can be used to identify which actions to use to attain
numeric subgoals — either the top-level numeric goals, or the numeric preconditions of
actions chosen during solution extraction.
362

A Hybrid LP-RPG Heuristic for Planning

Algorithm 3: Adding a weighted action to a relaxed plan

1
2
3
4
5
6
7
8
9
10
11
12

Data: R — a metric RPG, a — an action to include in the relaxed plan, q — the subgoal
queue, w — a weight
foreach propositional precondition pre of a do
l ← layer at which pre first appears;
if l > 0 then
if ∃(pre, k) ∈ q[l].prop then
if k < w then k ← w;
else insert (pre, w) into q[l].prop;
foreach numeric precondition pre of a do
l ← layer at which pre first appears;
if l > 0 then
if ∃({pre}, k) ∈ q[l].num then
if k < w then k ← w;
else insert ({pre}, w) into q[l].num;

The first key difference when using the LP during the relaxed plan extraction concerns
the choice of actions to achieve numeric preconditions. In the original metric RPG heuristic,
a given numeric precondition (e.g. x ≥ c) in fact layer i + 1 was regressed through all the
beneficial numeric effects at layer i, giving a residual numeric precondition (e.g. x ≥ c0 ) to
then be achieved in fact layer i. (This process is shown in lines 28 to 41 in Algorithm 2). In
the lp-rpg case, shown in Algorithm 4, a numeric precondition at layer l is (temporarily)
added to the LP generated for layer l (as a constraint (line 22). To find the actions to use
to achieve this, the LP is solved (line 23), with the objective being to minimise a weighted
sum across the action variables (one possible weighting scheme that will suffice for this
purpose is to minimise the sum of the action variables, though we return to the question of
appropriate weighting schemes later in the paper). Finally, the actions whose corresponding
variables are non-zero (line 24) are added to the relaxed plan (lines 25 to 34).
Second, we must accommodate the fact that the LP, being a relaxation of an underlying
MIP (in which the action variables are integers), may be solved by applying actions a nonintegral number of times. As a simple example, if there are several actions that increment a
given variable, and any of them alone would suffice to achieve a goal value for the variable,
then a valid optimal solution to the LP is any for which the sum of the variables corresponding to these actions is 1. If every action that had a non-zero action count variable in the
solution were considered ‘applied’ then the relaxed plan length could greatly over-estimate
the required number of actions. To mitigate this problem, each subgoal (e.g. x ≥ c) arising
during solution extraction is associated with a weight, and these weights, along with the
values given to action variables in the LP, are used to update the relaxed plan length. The
weights are manipulated throughout Algorithms 3 and 4, and their use can be summarised
as follows:

• Initially, each goal fact is added to the subgoal queue with associated weight 1, i.e.
each has to be achieved, entirely. Also note that, in contrast with Algorithm 1, the
363

Coles, Coles, Fox & Long

Algorithm 4: Relaxed plan extraction with LP

1
2
3
4
5

Data: R - a metric RPG; P G - propositional goals;
N G - numeric goals
Result: ha - helpful actions, h - A heuristic value
ha ← ∅, h ← 0;
q ← deepest-first priority queue of goal layers;
foreach p ∈ P G do
l ← layer at which p first appears;
insert (p, 1) into q[l].prop;

if |N G| > 1 then
l ← final layer of R;
8
insert (N G, 1) into q[l].num;

6
7

9
10
11
12

else
f ← the fact in N G;
l ← layer at which f first holds;
insert ({f }, 1) into q[l].num;

while q not empty do
(l, hprop, numi) ← pop(q);
foreach (p, w) ∈ prop do
h ← h + w;
a ← achiever for p;
if a in action layer 1 then add a to ha;
19
call Algorithm 3 with R, a, q, w;
20
prop ← prop \ add effects of a;

13
14
15
16
17
18

21
22
23
24
25
26
27
28
29
30
31
32
33
34

foreach (G, w) ∈ num do
LP’ ← LP(l) + the constraint(s) G;
solve LP’, minimising weighted action sum;
av ← {action variable (a = c) ∈ LP’ | c 6= 0};
foreach a ∈ av do
h ← h + w.c;
if a is in layer 1 then add a to ha;
foreach propositional precondition pre of a do
l ← layer at which pre first appears;
c ← min[c, 1];
if ∃(pre, k) ∈ q[l].prop then
if k < w.c then k ← w.c;
remove (pre, k) from q[l].prop;
else insert (pre, w.c) into q[l].prop;

subgoal queue here records the layer at which the goal is introduced as well as its
associated weight.
• An action a can be chosen to be applied Ca times to achieve a queued propositional/numeric sub-goal g, if either:
– it is chosen to be applied once (i.e. Ca = 1) as the achiever for some propositional
subgoal g, with associated weight w;
364

A Hybrid LP-RPG Heuristic for Planning

– the action was applied (given a non-zero value for Ca ) when solving the LP to
achieve some numeric subgoal g, with associated weight w.
• In both of these cases, the relaxed plan length is incremented by Ca .w.
• The weight given to the preconditions of a is then w0 = w.min[Ca , 1]. The weights
of these preconditions are used to update the weight attached to achieving the corresponding sub-goals at earlier layers:
– If a propositional precondition p of a is already a recorded subgoal at an earlier
layer, with some weight k, then its weight is updated to be max[k, w0 ]. Otherwise,
p is added as a subgoal to the RPG, to be satisfied in the first layer at which it
appears, with weight w0 .
– In the case of a being added to support a propositional sub-goal: if a numeric
precondition p of a is already a recorded subgoal at an earlier layer, with some
weight k, then its weight is updated to be max[k, w0 ]; otherwise, p is added as a
subgoal to the RPG, to be satisfied in the first layer at which it appears, with
weight w0 .
5.4 Consequences of the Use of the LP during Solution Extraction
Use of the LP to aid in the identification and selection of actions to support achievement
of numeric goals and subgoals in the extraction of a relaxed solution can lead to some
important consequences on the heuristic guidance offered by the relaxed solution. We have
already noted the problem that non-integral fragments of actions might be combined to
achieve numeric effects and indicated that this can be managed by handling fractional
preconditions and fractional action costs. However, there are other potential problems as
we now discuss.
5.4.1 Partially Applied Helpful Actions
Consider a situation in which there are precisely five possible ways to achieve a particular
numeric goal, and each of these uses three actions. A simple example of this is a small
problem in the Settlers domain, where there are five carts and a unit of timber available at
location A: the goal of having one unit of timber at location B can be achieved by loading
timber onto any of the carts at A, moving it from A to B, and then unloading it at B.
If the metric RPG heuristic were used to achieve this goal, three actions would be used.
Working backwards from the goal, the selected actions would be:
• in action layer three, an unload action (from cart c) to increase the amount of timber
at B;
• in action layer two, an action to move a cart c from A to B;
• in action layer one, an action to load a unit of timber at A onto the cart c.
When solving the LP to achieve the same goal (ignoring propositional preconditions),
with the objective of minimising the sum of the action variables, the solution returned will
365

Coles, Coles, Fox & Long

have an objective value of 2. If we denote the relevant load/unload action variable pair for
cart i as (li , ui ), the pool of solutions that could be returned is any satisfying:
∀

li = ui

i∈[1..5]

(

X

(li + ui )) = 2

i∈[1..5]

Then, for any non-zero variable ui = k the relevant action to move cart i from A to B will
also be added to the relaxed plan, with weight k (line 19, Algorithm 4).
For the purposes of providing a contribution to the relaxed plan length, it is unimportant
which of these solutions is returned: the sum of the action variables in each is 2, and the
sum of the move actions added is 1, giving a total relaxed plan length of 3. However, as
discussed in Section 3.1.2, the relaxed plan is also used to determine a set of helpful actions:
those with an effect in common with the actions in the relaxed plan that were chosen from
action layer one. In this example, action layer one consists of the action (or actions) to
load a unit of timber onto a cart at A. In the original metric RPG case, exactly one action
would be used. However, using the LP up to five actions could be (fractionally) used. The
consequence of this is that within the pool of LP solutions that could be returned, some
will lead to search having a much greater branching factor, in this case up to a factor of
five greater.
The source of this problem is the relaxation of integrality constraints on the action
variables and the extent to which it affects search depends on the precise solution returned
by the LP solver: different solvers may have a greater or lesser tendency to return solutions
in which the action variables are assigned non-integral values. An extreme response to this
problem would be to revert to the MIP and require all action variables to be integral rather
than real-valued. Alternatively, focussing on the issue identified here, one could require only
the action variables corresponding to actions from action layer one to be integers. In either
case, the result is a mixed integer programming problem, but since the cost of MIP-solving
is exponential in the number of integer variables, the difference between the variants can
be significant. This change only need be made at the point where we switch to using the
LP for solution extraction rather than graph expansion as, prior to this, the assignments
to the action variables are unimportant (only the value of the objective function is used).
Of course, the price we pay is potentially very significant, because MIP-solving is NP-hard,
while LP-solving is polynomial. However, in exchange for this shift in complexity, in the
example given above, we are left with only a single helpful action, as it is no longer possible
to fractionally load timber onto a cart: one cart must be chosen. The extent to which these
two possible integer-modifications affect search performance will be considered later in the
evaluation.
5.4.2 Preferring Earlier Actions
Within the Metric RPG heuristic there is an explicit preference for using actions that
appear earlier in the relaxed planning graph. As shown in Algorithm 1, when a fact is
needed, either as a goal or to satisfy a precondition of an action chosen for insertion into
the relaxed plan, it is queued as a sub-goal to be satisfied at the first fact layer in which
it appeared. Then, when an action is chosen to support that fact, it will be amongst the
earliest possible achievers. The intuition behind this preference for earlier actions is based
366

A Hybrid LP-RPG Heuristic for Planning

on the observation that, the later an action appears in the relaxed planning graph, the
greater the number of actions that need to be added to the relaxed plan to support its
preconditions. Therefore, preferring earlier actions usually leads to shorter relaxed plans
and hence to closer approximations of the optimal relaxed plan length.
Within the LP, if the objective is set to minimise the sum of the action variables (i.e. use
as few actions as possible), then there is no distinction between actions that appear earlier
in the RPG and those that appear later. Recalling that the LP disregards the propositional
preconditions of actions, failing to take into account when an action is first added to the
RPG can lead to LP-based relaxed plan extraction generating very poor quality solutions
and, consequently, very poor search guidance.
To address this, the pressure generated within the LP by the objective function has to
be tuned to prefer actions that need fewer supporting actions in the relaxed plan. This is
achieved by forcing the LP to favour actions that appear in the RPG earlier. We encode
a preference for earlier actions with the weighting scheme for the action variables in the
objective function. Actions appearing earlier are given smaller weights than those that
appear later. We propose (and, later, will evaluate) two ways of achieving this. The first,
and simpler, is to use a geometric series to dictate the coefficient given to an action variable
a based on the layer l in which it first appears. In this case, the objective coefficient on a
is:
kl
k ∈ < ∧ k > 1.
The value of k controls the extent to which earlier actions are preferred, and can be interpreted as treating k.n actions selected from layer l as exactly as good as selecting n actions
from layer l + 1 (so anything less than k.n actions from layer l will be preferable to selecting
n actions in layer l + 1). Throughout the remainder of the paper, we refer to this scheme
as layer-weighting with value k.
The second option is to record, as a cost for each action, an estimate of the number
of actions needed to support its propositional preconditions, and use this as its weight in
the objective function. This can be achieved by using the RPG cost propagation algorithm
from Sapa (Do & Kambhampati, 2003). To achieve this, as the planning graph is expanded,
costs for each fact and action are recorded and updated at each layer. Initially, for each fact
p in the state being evaluated, its cost at fact layer zero, cost(p, 0), is zero. (For each fact
p not true at time zero, cost(p, 0) = ∞.) These fact costs are then used to derive action
costs, using rules akin to those used by hadd /hmax (Bonet & Geffner, 2001). The cost of an
action a at layer t, cost(a, t) is defined according to one of either:
cost max (a, t) = max cost(p, t − 1)
p∈pre(a)

cost sum (a, t) =

X

cost(p, t − 1).

p∈pre(a)

These action costs, in turn, are used to update the costs of each proposition in the
subsequent fact layer, with the cost of each fact being reduced if there is now a cheaper
way to achieve it. For an action a in layer t, it can potentially reduce the cost of each of
the propositions p that it adds:
cost(p, t) = cost(a, t) + 1 iff (cost(a, t) + 1) < cost(p, t − 1)
= cost(p, t − 1) otherwise
367

Coles, Coles, Fox & Long

As the planning graph is expanded, this process of alternating action cost estimation,
and fact cost estimation, is used to propagate cost information through the RPG. When
setting the objective of the LP, we can use the cost of an action, cost(a, t), as the coefficient
of the action variable corresponding to a. Using the cost propagation as described, the
costs are derived solely on the basis of propositional preconditions and effects. Therefore,
cost(a, t) is an estimate of the number of actions needed to support the preconditions of
a. For our purposes, this is desirable: the LP will, itself, add actions to support numeric
preconditions, so an estimate of the number of actions needed to support the propositional
preconditions of an action is a measure of the cost impact upon the relaxed plan due to an
action being selected.

6. Adding Propositions to the LP
Benton et al. (2005) explore the idea of using an LP to guide search in propositional planning
problems in the context of over-subscription planning. In that work the LP is used to
determine which goal subset to achieve, to gain maximum utility. Whilst successful in
achieving that aim, the authors observe that the use of the LP as a heuristic to guide search
is very expensive, indeed too expensive to be feasible. With the propositions encoded in the
LP in the way they propose, the task of solving the LP becomes equivalent to solving the
entire planning problem with relaxed action ordering and non-integer action variables. In
this section, we reconsider the inclusion of propositions in the LP, considering the spectrum
of possibilities between including no propositions and a way to include all propositions.
Even though the focus of this work is on numeric problems, including some propositions
in the LP might still be of interest. For instance, supporting a propositional goal might
require the consumption of numeric resources. In the worst case, one could compile a
problem so that all the numeric goals become preconditions of an action that achieves a
dummy propositional fact goal, and modify the problem so that the only goal is goal. Since
there are no numeric goals in this modified problem, the LP will then only be used to solve
the individual preconditions of the dummy action when it is (inevitably) chosen, rather than
requiring the goals to be satisfied in conjunction, as described in Section 7.1. Since this
dummy-goal model is merely a reformulation of the original problem, the same information
should theoretically be accessible to be conveyed to the LP. More generally, we can hope to
identify intermediate landmark propositions, as well as final goals, that could usefully be
encoded in the LP.
6.1 Adding Propositional Goals to the LP
Although the LP we describe in Section 4 does not contain specific reference to propositions, and hence propositional goals, we can formulate constraints that act as a proxy for
them, by considering which actions achieve them. We do not need to introduce additional
variables. Instead, we add constraints to ensure that at least one achiever is chosen for each
propositional goal. For each goal fact g that is not true in the state being evaluated, then
for the list of actions [a0 ..an−1 ] that achieve g, we can add as a constraint to the LP for the
most recent layer in the planning graph:
a0 + ... + an−1 ≥ 1.
368

A Hybrid LP-RPG Heuristic for Planning

That is, at least one achieving action must be used or, more specifically, given actions
can be partially applied, a total of at least one achieving action must be used. An LP
containing these constraints can be used to augment the positive termination criteria for
graph expansion, detailed in Section 3.1.2. We then terminate at the first fact layer i where:
1. All goal propositions F ? are in F P (i) (as before);
2. All goal numeric expressions N ? are satisfied (individually) by F V (i) (as before);
3. The LP used to compute the numeric bounds for layer F (i) is still solvable when all
the constraints for propositional goals are added.
Use of the goal-checking LP has two key consequences. First, if the actions up to layer
F (i) cannot be used to satisfy the goals whilst respecting the other numeric constraints
in the LP, additional layers are added to the planning graph until the necessary actions
have appeared (or the termination criterion is reached). Thus, in reasoning about resource
persistence (Section 3.2.1), the heuristic is now better able to recognise cases where, although the propositional goals might appear to be individually reachable, either additional
production is needed to meet them collectively, alternative actions need to be used, or the
state is a dead-end. Second, the solution to the LP used to confirm point (3) above, is used
to indicate which actions to add to the relaxed plan to achieve the propositional goals. The
propositional preconditions of these actions will be satisfied in the usual way (line 28 of
Algorithm 4).
By requiring only that the sum of the action variables selected to achieve the goals is
at least one, and allowing such variables to be real-valued, the LP could, in theory, provide
weaker guidance than the RPG. This is a similar issue to that noted in Section 5.4.1 when
considering helpful actions, and could be ameliorated in a similar manner, namely by making
the goal-achieving action variables integral. We will return to this issue in the evaluation,
considering whether or not this benefits search.
6.2 Using Landmarks in the LP
A landmark fact (Hoffmann, Porteous, & Sebastia, 2004) is a propositional fact that must
be true at some point in every solution plan to a given planning problem. The first work
on landmarks (Porteous, Sebastia, & Hoffmann, 2001) proposed a method for extracting a
subset of the landmarks from a planning problem based on regressing from the goals using
the delete relaxation of FF. Since introduction of the idea in 2001 (Porteous et al., 2001),
landmarks have have come to play an increasingly important role in planning. Recent development of new techniques for extracting landmarks (Richter, Helmert, & Westphahl, 2008;
Zhu & Givan, 2003) and the development of heuristics based on different relaxations (Richter
& Westphal, 2010; Domshlak, Katz, & Lefler, 2010; Helmert & Domshlak, 2009; Karpas &
Domshlak, 2009) have allowed the planning community to exploit landmarks more successfully.
The relaxed plan extraction phase of the lp-rpg heuristic relaxes action ordering and
propositional preconditions and effects and might benefit substantially from delete-relaxation
landmarks. The use of landmark facts in the LP offers a further mechanism by which to
more tightly couple the LP and the RPG, allowing increased information sharing between
369

Coles, Coles, Fox & Long

the propositional and numeric components of the heuristic. If we know that a landmark
fact must occur in any solution plan, and it has not yet appeared on the path to a state
being evaluated, we can add constraints representing this to the LP, just as we did for
propositional goals. That is, that the sum of the action variables [a0 ..an−1 ] achieving a
given landmark must be greater than or equal to 1. As with propositional goals, this constraint introduces the need to provide numeric support for the action(s) chosen to support
the landmark. Goals are a special case of landmarks, but an important feature of goals is
that, even if they have been achieved on the path to the current state, if they are not true
in the current state then they must be reachieved. Constraints can be added to the LP to
ensure this. To reflect landmarks that have been achieved on the path to the current state,
the state is modified to record them and the record updated as new landmarks are seen.
This approach is similar to lama (Richter & Westphal, 2010).
A set of disjunctive landmarks is a set of propositional facts, any one of which must be
true in any solution to a planning problem. The extraction of disjunctive landmarks has
been considered (Gregory, Cresswell, Long, & Porteous, 2004) but it is even more difficult
than is the case with conjunctive landmarks to exploit them in planning systems. The
knowledge that a certain fact must be true can allow the planner to infer that certain actions
that must be present in solution plans, which can inform heuristics. However, disjunctive
landmarks are less informative. Disjunctive landmarks often arise from problem symmetry.
For example, we might know that in order to deliver a package from one place to another it
will have to be loaded in to a truck, but not which truck. A disjunctive landmark in which
the package is in some truck can be generated: even if we do not know which truck to use,
we know that one of the disjunctive landmarks must hold. Thus, in the context of numeric
resources, some truck must be fueled (assuming they all start empty), which might entail
other additional costs. It is therefore of interest to take account of disjunctive landmarks
in numeric reasoning.
We are able to make use of disjunctive landmarks in the LP to further constrain the
problem, ensuring that support is given to at least one fact within each (unreached) disjunctive landmark. When dealing with standard conjunctive landmarks, the constraint is
that a sum of at least one achiever must be added for each propositional landmark. For
disjunctive landmarks, however, the constraint is slightly different. A disjunctive landmark
set L is satisfied if any of its constituent landmarks are satisfied. That is, if we apply any
of the actions:
achieves(L) = {a | eff + (a) ∩ L 6= ∅}.
To encode the disjunctive landmarks in the LP (assuming it has not yet been met) there
are two possibilities. The first is to add a binary variable sf for each fact f ∈ L, with
constraints such that sf can only take the value 1 if there is at least a total of one action
adding f , and that at least one such variable sf has to take the value 1. That is, at least
one of the disjunctive landmarks has toP
be fully met. An alternative, potentially cheaper,
approach is to add a constraint that ( achieves(L)) ≥ 1. This allows the disjunctive
landmark to be considered satisfied if the sum across the action variables supporting any of
its constituent facts is at least 1. This is somewhat weaker than the constraint for individual,
non-disjunctive landmarks, as it does not guarantee that there is support of at least 1 for
any individual constituent fact. For instance, a two-fact disjunctive landmark is ‘satisfied’
if the support for each constituent fact is 0.5. We considered both of these approaches and
370

A Hybrid LP-RPG Heuristic for Planning

found that there is a negligible difference in performance (time taken and nodes expanded)
between the two encodings so no real saving is achieved by using the relaxed approach.
6.3 Managing Propositional Preconditions and Effects
So far, we have considered propositions that must be achieved in a planning problem due to
goals and landmarks. However, there is a second class of propositions: those that, given the
values assigned to the action variables in the LP, must also have supporting actions added
to the solution relaxed plan.
To extend the LP to capture propositional preconditions and effects, we first introduce
a binary variable f (an integer whose value is 0 or 1) for each fact f that is not true in
the state being evaluated. This is then involved in two constraints. First, for the actions
+
[a+
0 ...an−1 ] that add f :
+
a+
0 + ... + an−1 ≥ f.
In the case that the proposition corresponding to f is a goal, f = 1 and hence one or more
of the achieving actions must have a positive value (since the constraint is expressed using
continuous variables, the actions might only be partially applied in the relaxation). Then,
for the actions [ap0 ..apm−1 ] that have f as a precondition:
N.f ≥ ap0 + ... + apm−1
where we use N to denote a (sufficiently) large number. This constraint ensures that if at
least one of the actions depending on the proposition corresponding to f is used in a relaxed
plan, then f must be positive (that is, the corresponding proposition is required to be true
within the relaxation). The use of N is to ensure that f = 1 is sufficient to satisfy the
preconditions of many actions. This pair of constraints is effectively a conditional version
of the constraint to meet a propositional goal, described in Section 6.1. In cases where the
proposition is neither a goal nor a landmark, these constraints serve to enforce that at least
one action that adds f is chosen (has a positive value) in the LP if any action requiring f
is chosen even partially.3
6.4 Recognising Propositional Resources
Finally, we consider one other case where it is potentially useful to model propositions in
an equivalent numeric form. In pddl, finite domain integer resources can be modelled in
two ways: as numeric variables, or as a set of propositions. Consider the following two
formulations of the fell-timber action from the Settlers Domain (for simplicity the effect
on the metric tracking variable labour is omitted):
(:action fell-timber
:parameters (?p - place)
:precondition (has-cabin ?p)
3. It is tempting to consider replacing this constraint, with its slightly troublesome N , with constraints of
the form f ≥ api for each i. Unfortunately, this is not appropriate because an action variable, api , can
be greater than 1 (due to multiple applications of the action) and yet f = 1 is sufficient to satisfy the
precondition of all the action applications.

371

Coles, Coles, Fox & Long

:effect (increase (available timber ?p) 1)
)
(:action fell-timber
:parameters (?p - place ?n0 ?n1 - value)
:precondition (and (has-cabin ?p)
(timber ?p ?n0)
(less-than ?n0 ?n1))
:effect (and (not (timber ?p ?n0))
(timber ?p ?n1))
)
Each of these representations models the same situation, but each uses a different mechanism to do so. The first uses numeric variables, while the second uses propositions. When
using either the numeric or propositional formulations with the MetricFF heuristic, there
is little practical difference in the guidance given. In the numeric case, once the fell-timber
action has been applied, the upper bound on the amount of timber at place p is increased.
This means that any action consuming this amount of timber can be executed at subsequent
layers, regardless of how many other actions using the resource have also been applied. In
the propositional case, the delete effect on timber (i.e. that deleting the fact that there
previously was some) is also relaxed so, again, any number of actions requiring this amount
of timber can be applied.
Turning our attention to the lp-rpg heuristic, however, we can observe that although
the RPG part of the heuristic exhibits the same weakness as the propositional case, the
different relaxation used in the LP for numeric reasoning means that the consumption
of timber would not be disregarded. The LP relaxes action ordering, rather than delete
effects (or production/consumption effects), so if the resource is modelled numerically, this
interaction can be captured and accounted for. It is therefore in our interests when using
lp-rpg to convert resources modelled propositionally into a numeric formulation, so that
these can be reasoned with in the LP, rather than in the RPG.
Although the formulation of resources in the above example is an instance of a common
idiom used to capture numeric resources in a propositional encoding, there are situations in
which it is more natural to model resources propositionally from the outset. This is often
the case with binary resources: resources that are either present or not. Such resources are,
of course, a special case of the more general resource model described above. Consider the
propositional and numeric counterparts of an action to switch on a water pump:
(:action activate
:parameters (?p - pump)
:precondition (off ?p)
:effect (and (not (off ?p))
(on ?p))
)
(:action activate
:parameters (?p - pump)
372

A Hybrid LP-RPG Heuristic for Planning

:precondition (<= (pumping ?p) 0)
:effect (increase (pumping ?p) 1)
)
Corresponding actions can similarly be created to switch the pump off (the fact (on
?p) is deleted and (off ?p) added, or equivalently a unit of (pumping ?p) is consumed).
In many senses, the most natural formulation of this action is the first, using propositions.
This is the way most binary resources are encoded in benchmark domains. However, the
second formulation is equivalent (assuming the value of (pumping ?p) in the initial state is
1 or 0). If there is no interaction between a propositional resource and the other resources
identified in the planning problem, there is little motivation to add it to the LP, since no
numeric support is required. In the case where a binary resource has an impact on another
numeric variable it is, as we shall see, most efficient to model both as numeric resources.
Suppose we have some water pumps that can control the flow of water. Two ways to model
this in pddl are shown below:
(:action activate
:parameters (?p - pump)
:precondition (off ?p)
:effect
(and (increase (water-flow) 1)
(not (off ?p))
(on ?p))
)
(:action activate
:parameters (?p - pump)
:precondition (<= (pumping ?p) 0)
:effect (and (increase (pumping ?p) 1)
(increase (water-flow) 1))
)
The first of these two actions switches on a pump (a binary, propositional resource) and
produces a unit of (water-flow). If other actions in the domain have preconditions over the
water flow, such as an action to run a water wheel with a precondition (≥ (water-flow) 3)
then there is an interaction between the propositional and numeric variables of the problem.
If we use the first model of the action, the RPG will capture the propositional part of the
action (whether the pump is on or off) and the LP will encode only the numeric part of the
action. Since the RPG relaxes delete effects it will not represent the fact that (off ?p) is no
longer true and, hence, will not prevent the pump from being switched on many times. In
the LP built using the first formulation, the action activate does not consume any numeric
resources, so it can be used arbitrarily often to increase the water flow — the fact that
switching off is necessary to achieve this increase is ignored. Thus, mixing the propositions
and numeric resources in the action degrades the information available from the LP.
Using the second formulation, the state of the pump appears in the LP as a variable,
and if we use activate and deactivate to denote the actions for activating and deactivating
373

Coles, Coles, Fox & Long

a pump, the constraints on the pumping 0 variable are:
pumping 0 = init + activate − deactivate
pumping 0 ≥ 0
pumping 0 ≤ 1
It is now clear that the activate action can only be applied once: if it is applied again
then deactivate must be applied, with the corresponding effect on water-flow, in order to
satisfy the last of the above constraints. This provides useful guidance, as it indicates that
the water flow cannot reach 3 units using only these actions: actions to control more pumps,
or other means of increasing flow, need to be added to the LP, through further expansion
of the planning graph. If no means to attain sufficient flow can be found, then a dead-end
has been discovered that would otherwise have wasted search effort.
Static-analysis techniques capable of identifying propositional resources have been developed (such as the tim system described in Long & Fox, 2000). These can be used in a
preprocessing stage to recognise propositional resources in planning domains and translate
them into equivalent numeric resources. We use this translation approach for all recognised
resources, with the resulting numeric preconditions and effects being included in the LP in
the same way as other numeric variables. In doing so, the LP order-relaxation rather than
the RPG delete-relaxation is used to compute heuristic values, preventing impossible reuse
of the same resource in cases such as that described.

7. Extending the Scope of Numeric Reasoning in the LP
In Section 4 we discussed an LP encoding that captures the producer–consumer behaviour of
actions, as used in the first version of LPRPG (Coles et al., 2008). In this section we discuss
how this encoding can be enhanced, with the use of further numeric information representing
the structure of the planning problem, to improve the guidance that the resulting heuristic
can provide to the planner. We address two key issues here: ensuring that conjunctions
over numeric goals can be satisfied, and considering the issue of fractionally applied actions
in the LP.
7.1 Checking Numeric Goals alongside Propositional Goals
In Section 6.1 we noted that we can constrain the LP so that it finds actions to achieve
propositional goals. We can extend this further, to capture numeric goals, N ? , adding
each numeric goal directly to the LP as a constraint. As with propositional goals, this has
clear advantages in terms of resource persistence (Section 3.2.1), by insisting all goals are
simultaneously achievable, rather than just individually achievable. Additionally, though,
it raises the expressive power of the numeric goals we are able to handle to anything that
can be expressed in Linear Normal Form (LNF) — any LNF formula can be added to the
LP as a constraint.
7.2 Catalytic Resources
So far we have only considered numeric variables that conform to producer–consumer behaviour. However, there is another related class of variables that can also be expressed in
374

A Hybrid LP-RPG Heuristic for Planning

the LP in a similar way to producer–consumer variables. These are variables that represent resources that must be present in order for an action to be applied, but are not then
consumed. The same resources can be used to support many actions4 . An example of such
a resource is a catalyst in a chemical reaction. A catalyst must be either created through
reactions, or bought, but once present, it enables other reactions, or allows them to take
place more quickly. For these catalytic reactions, the resource must be present, but is not
consumed, though it could be that other non-catalytic reactions may consume the resource.
Another example one might consider is the building of a unit in a plant, to support some
process. The unit must be there in order for the process to occur, but once built it can
be used many times to enable other actions without necessitating its destruction. Often
in planning problems, the presence of such structures is represented by propositions, but
this need not be the case. If many indistinguishable processing units are present, or several
units of a catalyst are needed, it often makes more sense to represent these numerically5 .
To extend the lp-rpg heuristic to provide guidance in such problems, where some
actions require v ≥ c but do not affect the value of v, we encounter the difficulty that the
LP encodes no notion of time: ordering of actions is relaxed, so it is impossible to ascertain
the value of v at a specific time in order to determine whether the (catalyst) precondition is
satisfied or not. We therefore add additional constraints to the LP to determine the upper
and lower bounds on v obtained for the most optimistic or pessimistic possible ordering of
the actions whose variables are non-zero. To find an optimistic upper and lower bound on
v, v ↑ and v ↓ respectively, we add the constraints:
X
v ↑= v +
a. max(δ(v, a), 0)
(4)
a∈A
v ↓= v +

X

a. min(δ(v, a), 0)

(5)

a∈A
The upper bound is equivalent to ordering all production actions before all consumption
actions. For the lower bound, this is reversed, being equivalent to all consumption actions
being ordered before all production actions. The bounds are not the same as those computed
for the possible values of resource variables in the reachability graph, because here we are
considering only the actions that are actually selected for execution in the relaxed plan,
rather than those that could possibly be applied. As can be seen, neither requires an
explicit notion of time.
Using these bounds, if an action a with a precondition v ≥ c is to be applied, even
fractionally, then it must be the case that v ↑≥ c, otherwise the precondition on the action
could never be met with any ordering of the actions chosen. Of course, if v ↑≥ c, we cannot
guarantee that there is a legal ordering of the producers and consumers that achieves this
value. This need to have at least a feasible opportunity to satisfy the precondition can be
added to the LP through the introduction of a binary ([0, 1] integer) variable and a pair
of constraints. For the actions [a0 ..an−1 ] requiring a precondition v ≥ c, then using N to
4. We note that there is a complementary class of variables whose values must remain below a certain level
in order for actions to be applied, but are not then produced. These seem less useful in real problems,
but nonetheless can be handled in an analogous way.
5. In Section 6 we show how these could be captured in the LP even if they are expressed propositionally.

375

Coles, Coles, Fox & Long

denote a large number, and s to denote a new binary variable, we add the pair of constraints:
N.s ≥ a0 + ... + an−1
v ↑ ≥ lb(v) + (c − lb(v)).s

(6)

The first constraint forces s to take the value 1 if any of the actions requiring the
precondition v ≥ c are applied. The second constraint then determines the lower bound on
v ↑ based on the value of s: if s = 0, then the lower bound on v ↑ is lb(v), the global lower
bound on v. Otherwise, s = 1, since an action needing the precondition has been applied,
and thus v ↑≥ c. If there is no constraint that s = 1 then this implies that at least one
of the actions [a0 ...an−1 ] is applied. However, since a non-zero value of s only makes the
LP harder to solve, there is no pressure to set s = 1 for any reason other than that the
precondition must be satisfied. It is important to note that these constraints are only being
added to the LP in problems where v ≥ c preconditions are not matched by a v –= c effect.
The modified heuristic is able to support planning models that it previously could not. Its
behaviour on domains without these characteristics is entirely unaffected.

8. Results
In this section we present a thorough evaluation of the lp-rpg heuristic: comparing it to
state-of-the-art numeric planners, considering the use of different LP solvers and performing
ablation studies to determine the most effective of the many potential different configurations of lp-rpg discussed in this paper. These include the weighting of action variables in
the LP according to the RPG layer in which they appear, the inclusion of propositions and
numeric goals in the LP and the consideration of which variables in the LP should remain
integer and which should be relaxed to real numbers. All our tests were run on 3.4GHz
Pentium IV machines and were limited to 30 minutes and 1.5GB of memory. If a planner
or planner-configuration fails to report a solution within these limits it is deemed to have
failed to solve the problem.
8.1 Evaluation Domains
First we discuss our selection of evaluation domains. Out purpose in selecting these domains
is to select or construct examples that are informative in evaluating the behaviour of our
heuristic. Domains with numeric variables that do not conform to the producer-consumer
behaviour can be identified syntactically and a different planning strategy can then be
employed. Since the syntactic analysis is trivial, the overhead in making this decision is
negligible, so we can assume that the performance on domains to which our approach is not
applicable is consistent with whichever alternative strategy is selected for deployment.
We consider both existing competition benchmarks with producer–consumer behaviour,
and introduce some new domains that exhibit this behaviour. There are few current benchmarks that exhibit interesting producer–consumer behaviour, but in order to make a comparison that is as informative as possible we make use of those that do:
• the MPrime domain from IPC 1;
• the Rovers domain (‘Numeric’ variant) from IPC 3;
376

A Hybrid LP-RPG Heuristic for Planning

• the Settlers domain from IPC 3;
• an alternative encoding of the Settlers domain (described below);
• the Pathways domain from IPC 5. (We developed a metric domain derived from
the ‘Metric Time’ variant, by replacing the durative actions with comparable nontemporal actions.)
In addition to the standard IPC problem set for Settlers (problems 1–20), we introduce
some new problems that make full use of the scope of the domain. The domain allows
for building ships and transporting materials between disjoint islands; however, in the
benchmark set none of the problems require this. As building ships requires a large amount
of infrastructure, we therefore add some problems to further challenge the planners, where
materials must be imported from overseas in order to achieve goals. The first of our problems
(21) requires merely the building of a ship, 22 requires the import of timber from overseas,
and 23 requires building housing overseas. 24 adds some further goals to 23, requiring the
planner to achieve both goals on the mainland and build housing on the island. The final
problem, 25, considers 3 disjoint islands from which resources must be combined to achieve
a goal on each island. Each of these problems requires building much greater infrastructure
than required in the original IPC 3 settlers problems. We consider two variants of the
settlers domain: the standard IPC 3 domain, and an encoding based on the representation
of carts proposed by Gregory and Rendl (2008). Here, the number of carts at a given
location is represented as a numeric variable (carts-at ?location). There are then two
possible ‘move’ actions for carts: one that moves a cart and a unit of a resource from one
location to another; and one that moves the cart without moving any resources, i.e. the
cart moves whilst being empty. This encoding is possible because carts can only transport
a single unit of material, so it is not necessary to maintain specific named identities and
capacities for each cart.
In addition to the benchmark domains, we use two domains created during the development of lp-rpg:
• the Market Trader Domain (Coles et al., 2008);
• the Hydro Power Domain.
In the Market Trader Domain, a trader begins with a small amount of money, and
the goal is to have increased that to a certain level. This must be achieved by travelling
between markets, each of which sells some collection of goods at a certain price for each
good type, and buys at another (lower) price. Money can be made by buying items where
they are cheaper and transporting them (via camel) to other locations where they command
a higher price; moving has an associated cost as food is required for the camel. This is a
representation of a more general class of real-world trading problems where the aim is to
make money through buying, transporting and selling goods. The Hydro Power domain
is also concerned with financial gains, but the domain has a different structure: rather
than transportation, it is concerned with energy storage using hydroelectric reservoirs. By
buying electricity to pump water uphill during periods of low demand, when electricity is
cheaper, and storing this potential energy, the electricity can be sold at a higher price at
377

Coles, Coles, Fox & Long

Domain
Market Trader
Hydro Power
Pathways Metric
Settlers
Settlers Numeric Carts
MPrime
Rovers Numeric
Sugar
Total

lp-rpg
20
27
30
18
23
30
15
18
181

lp-rpg-FF
0
3
21
7
13
30
13
7
94

MetricFF
0
1
13
8
8
28
10
14
82

lpg–td
0
5
0
19
7
30
20
20
101

Table 2: Coverage achieved by different planners

times of greater demand. The domain encoding is augmented to take into account energy
loss: purchasing one unit of energy will not be sufficient to provide one unit of energy later
as there are losses in the storage process. In this problem, despite the temporal axis, we
are only interested in profit being made, so we do not force the planner to advance time
to a specific point: we only ask that sufficient profit has been made and the planner can
choose to advance time if necessary. In general this problem models continuous processes:
customer demand changes continuously. Here, we simplify the problem by discretising into
30 minute time intervals, using the demand schedule from the transformer domain (Bell,
Coles, Coles, Fox, & Long, 2009), based on the UK National Grid figures. Like the original
transformer model we represent temporal features of the problem with an advance time
action, rather than temporal pddl, since lp-rpg is not a temporal planner.
The final domain we consider is the Sugar domain (Radzi, 2011). Here, the objective
is to produce sugar through industrial processes, refining it from raw cane. This domain is
taken from a set of domains designed for optimisation planning: in each there are several
paths to the goal, originally included to allow the planner a choice of trajectories with the
challenge being to find better quality solutions. As this set of domains was designed to be
challenging as metric optimisation problems, most of the domains in the set are trivial for
standard MetricFF: if the plan metric is ignored then almost all of the problems are solved
in less than 1 second6 . Therefore, of these domains, we consider only the Sugar domain,
which remains challenging to MetricFF even when optimisation is not required: the number
of paths that appear to lead to the goal is so large that without good guidance it is difficult
to find any solution to the problem.

6. This is not to say that the other domains are uninteresting — they present an interesting challenge that
is explored by Radzi (2011), using a carefully modified variant of lp-rpg. However, the challenge is to
find good quality plans, where quality is determined by a more complex metric than simple plan length;
neither MetricFF nor lp-rpg in the form discussed here have difficulty finding poor quality plans for
these problems.

378

A Hybrid LP-RPG Heuristic for Planning

markettrader
100

hydropower
10000

LPRPG

1000

LPRPG
LPRPG-FF
FF
LPG

100
10

Time (s)

Time (s)

10

1

1
0.1
0.01
0.1

0.001
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
20
Problem Number

10000

LPRPG
LPRPG-FF
FF
Metric-LPG

100

30

sugar

pathwaysmetric
1000

25

1000

LPRPG
LPRPG-FF
FF
LPG

100
10

Time (s)

Time (s)

10

1

1

0.1
0.1
0.01

0.01

0.001

0.001
5

10

15
20
Problem Number

25

30

2

4

6

8

10
12
Problem Number

settlers
10000

1000

14

16

18

20

settlersnumeric
1000

LPRPG
LPRPG-FF
FF
LPG

LPRPG
LPRPG-FF
FF
LPG

100

Time (s)

Time (s)

100

10

10

1
1
0.1

0.1

0.01

0.01
5

10
15
Problem Number

20

25

5

10
15
Problem Number

mprime
1000

100

25

roversnumeric
1000

LPRPG
LPRPG-FF
FF
LPG

100

LPRPG
LPRPG-FF
FF
LPG

10
Time (s)

10
Time (s)

20

1

1

0.1

0.1

0.01

0.01

0.001

0.001
5

10

15
20
Problem Number

25

30

2

4

6

8

10
12
Problem Number

14

16

18

Figure 3: Comparison to MetricFF and lpg–td: time taken to solve problems

379

20

Coles, Coles, Fox & Long

Market Trader
200

Hydro Power
300

LPRPG
Best known

LPRPG
LPRPG-FF
FF
LPG
Best known

180
250
160
200
Solution Quality

Solution Quality

140
120
100

150

100
80
50
60
40

0
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
Problem Number

Pathways Metric
9000

7000

30

LPRPG
LPRPG-FF
FF
LPG
Best known

250

200
Solution Quality

6000
Solution Quality

25

Sugar
300

LPRPG
LPRPG-FF
FF
Metric-LPG
Best known

8000

20

5000
4000

150

3000

100

2000
50
1000
0

0
5

10

15
Problem Number

20

25

30

2

4

6

8

10
12
Problem Number

Settlers
900

700

1000

18

20

LPRPG
LPRPG-FF
FF
LPG
Best known

800
Solution Quality

600
Solution Quality

16

Settlers Num. Carts
1200

LPRPG
LPRPG-FF
FF
LPG
Best known

800

14

500
400
300

600

400

200
200
100
0

0
5

10

15
Problem Number

20

25

5

10

M-Prime
300

250

20

25

Rovers Numeric
300

LPRPG
LPRPG-FF
FF
LPG
Best known

250

LPRPG
LPRPG-FF
FF
LPG
Best known

200
Solution Quality

200
Solution Quality

15
Problem Number

150

150

100

100

50

50

0

0
5

10

15
Problem Number

20

25

30

2

4

6

8

10
12
Problem Number

14

Figure 4: Comparison to MetricFF and lpg–td: plan length

380

16

18

20

A Hybrid LP-RPG Heuristic for Planning

8.2 Comparison to Other Planners
We first compare the performance of lp-rpg to existing numeric planners. We use what
was found to be a strong (though not uniformly best) configuration of the planner, as we
will demonstrate in subsequent sections:
• Landmarks and Propositional Goals are added to the LP (as in Section 6.1);
• The weight of an action variable in the objective function used during solution extraction is 3l , where l is the layer at which it appeared during RPG expansion;
• Action variables corresponding to actions in action layer 1 are integral;
• IBM ILOG CPLEX version 12.1.0 is used as the LP solver.
We compare to the two historically most successful numeric planners: MetricFF (Hoffmann, 2003) and lpg–td (Gerevini et al., 2006). These remain state of the art, as many
modern planners (e.g. lama) do not handle numeric preconditions, only action costs. To
further clarify any differences in performance, we also compare to lp-rpg-FF: a reimplementation of MetricFF based on our lp-rpg code with the difference that, when computing
upper- and lower-bounds on numeric variables during RPG expansion, lp-rpg-FF allows
actions to be applied many times at the same action layer, rather than only once per action
layer as in MetricFF. Since the publication of our earlier comparison to lpg–td (Coles et al.,
2008), a new and improved version of lpg–td has been produced. This version of lpg–td
performs much better than the earlier version, and we use it in our results here. We also
consider a variant of lpg–td, Metric-lpg–td (Gerevini, Saetti, & Serina, 2008), designed to
be more responsive to plan metrics based on numeric variables. Our experiments showed
that Metric-lpg–td does not perform significantly differently to lpg–td in generating first
feasible plans for problems where plan length is the metric, apart from in the Pathways domain where lpg–td crashes on all problems. Therefore, we report performance for lpg–td
in all domains except Pathways where we report figures for Metric-lpg–td.
An interesting pattern emerges in the relative performance of the planners across our set
of evaluation domains, shown in Figure 3. The domains are organised with those at the top
being the most strongly numeric, relying on few propositions, and those towards the bottom
having more propositional structure and consequently less numeric structure. On the two
most heavily propositional domains (MPrime and Rovers) lpg–td is generally the most
successful planner, solving all of the problems in the evaluation sets, and often being the
fastest planner on these problems. The same pattern holds for the standard competition
problems (1-20) in the competition formulation of the Settlers domain. MetricFF also
performs quite well on the MPrime and Rovers domains, but struggles on the Settlers
domain due to the numeric structure present.
In the more strongly numeric domains, however, lpg–td performs poorly: indeed it fails
to solve a single problem in the Pathways and Market Trader domains. This is not due to
crashing, rather the planner just searches until it exhausts resource limits without finding
a solution. In Hydro Power, lpg–td solves the five easiest problems, but is not able to
scale beyond this. In our experiments we have observed that lpg–td struggles on domains
where there is limited propositional structure more generally, and the search guidance it gets
from the numeric problems is poor. Comparing the two Settlers variants also gives some
381

Coles, Coles, Fox & Long

interesting insights: when the carts are turned in to a numeric resource, lpg–td struggles
much more solving 7, instead of 19 problems, whereas the performance of lp-rpg is in fact
improved. Note that although lpg–td is successful on the IPC 3 problems, it cannot solve
the richer problems where ship building and overseas transport is required; whereas lp-rpg
is capable of solving such instances.
Turning our attention to the comparison with MetricFF we can observe that problems
solved by both planners are generally solved more quickly by MetricFF, particularly on
the domains with more propositional structure. This is because lp-rpg has the additional
overhead of solving an LP at each state (and partly, of course, due to the highly efficient
MetricFF code-base). Occasionally the general pattern is broken, with lp-rpg being faster;
this is because slight variations in the ordering of heuristically equivalent states can lead
to significant differences in performance. The results for lp-rpg-FF show similar over
all coverage to those for MetricFF, although sometimes solving different problems (again
branch orderings and a different code base can cause such differences, which are most marked
in the Pathways and Sugar domains), but serve to demonstrate that it is not our basic FF
implementation performing drastically differently to standard MetricFF that is causing the
gains we observe.
Looking at the numeric domains in particular, the lp-rpg heuristic is able to provide much better guidance, allowing lp-rpg to solve many more problems than MetricFF.
Notably, in the Market Trader domain neither MetricFF nor lp-rpg-FF can solve any
problems. This is due to the poor guidance the standard RPG heuristic gives in this domain, relaxing the transfer of numeric resources. The relaxed plan is to buy an item then
repeatedly sell the same item until sufficient profit has been made. Again, in Hydro Power,
a similar situation occurs: once one unit of energy has been pumped up, the same unit of
energy can be repeatedly sold at any future time of day, making sufficient profit without any
guidance. In Pathways, where chemical reactions must take place, the relaxation used by
MetricFF will allow units of the same substance to be used repeatedly, in several different
reactions, despite the fact that when they are used they are consumed. The lp-rpg heuristic does not permit this and therefore gives much better search guidance, allowing lp-rpg
to solve all problems in this domain. The numeric resource transfer present in the Settlers
domain leads to poor guidance from the MetricFF heuristic and MetricFF is able to solve
very few problems as a result. The use of the LP is very effective in this domain allowing
more problems to be solved. The different formulations seem to make little difference to
coverage for MetricFF, with neither making the problems easier for MetricFF to solve.
The quality of solutions (plan length) produced by the different planners is displayed in
Figure 4. We emphasize here that lp-rpg in its current form is not making any attempt
to minimise a general measure of plan quality, these results are merely intended to give an
indication as to whether there is a large degradation, or indeed a fortuitous increase, in
quality in moving from using a standard RPG heuristic to the hybrid lp-rpg approach.
The potential to improve plan quality using an lp-rpg-style approach has been explored
in other work for domains with preferences (Coles & Coles, 2011) and also for a range of
different metrics (Radzi, 2011). None of the problems we use have specified metric functions
to minimise, so instead we use the number of actions in the solution plan. This is the value
that the RPG heuristic will tend to minimise. None of the planners are run in optimisation
mode (where that is available) and all simply report the first plan found in search. On
382

A Hybrid LP-RPG Heuristic for Planning

most domains the quality of solutions produced by lp-rpg is comparable to that of those
produced by MetricFF and lpg–td. In the sugar domain the lp-rpg heuristic compares
favourably to lpg–td, although we could perhaps hope that running lpg–td in quality mode
would enable it to produce better solutions. In Pathways, lp-rpg produces particularly
long solutions, but there is a trade off, as it is also able to scale to solve far more problems.
We will return to the issue of solution length in this domain when considering the weighting
of action variables in the LP during solution extraction.
In summary, lpg–td seems to be generally successful on domains where there is sufficient propositional structure and MetricFF is generally very efficient on problems that it
is capable of solving. When the structure of the domain becomes heavily numeric both of
these planners perform poorly. lp-rpg, however, is able to solve many more problems than
any of the other planners, making use of search guidance in the LP that captures numeric
interactions well.
8.3 LP Solvers
In lp-rpg the construction and use of an LP is performed using functions commonly
available in a wide range of LP solvers: adding variables or constraints to a model, setting
variable bounds, marking whether variables are real or integer valued, changing the objective
function, and so on. The current implementation employs a minimal abstraction layer
between lp-rpg and the LP solver itself, so that almost any LP solver can be used. In this
section, we consider the use of three LP solvers:
• IBM ILOG CPLEX version 12.1.0, a commercial mixed integer-linear programming
solver.
• COIN-OR LP (CLP) version 1.12.0, an open-source LP solver. Where models feature
integer variables, CLP is used within COIN-OR Branch-and-Cut (CBC) version 2.5.0,
which is, again, open-source.
• LPSolve version 5.5.0.13, an open-source mixed integer-linear programming solver.
In our experiments with these LP solvers, we found that CPLEX is substantially more
robust than the other two, particularly when the LP is extended to include satisfying
propositional goals and landmarks. Thus, for the purposes of the comparison here, we will
use a configuration of lp-rpg (equivalent to that used in our earlier paper (Coles et al.,
2008)) that is not as efficient as those presented elsewhere in the paper, but is most robust
(caused CLP and LPSolve to crash less often) under testing:
• Propositional goals and landmarks are not added to the LP: it encodes numeric goals
only.
• The only integer variables are (potentially) helpful actions, or those with assignment
effects.
• The layer-weighting scheme with k = 1.1 is used.
We refer to this configuration as limited-lp-rpg.
383

Coles, Coles, Fox & Long

Market Trader
1000

Hydro Power
10

CPLEX
LPSolve
CLP

CPLEX
LPSolve
CLP

100
1
Time (s)

Time (s)

10

1
0.1
0.1

0.01

0.01
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
20
Problem Number

Pathways Metric
1000

100

25

30

Sugar
10000

CPLEX
LPSolve
CLP

CPLEX
LPSolve
CLP

1000

100

Time (s)

Time (s)

10

1

10
0.1
1

0.01

0.001

0.1
5

10

15

20

25

30

2

4

6

8

Problem Number

Settlers
10000

10

12

14

16

18

20

Problem Number

Settlers Numeric Carts
10000

CPLEX
LPSolve
CLP

1000

1000

CPLEX
LPSolve
CLP

100
Time (s)

Time (s)

100

10

10
1
1

0.1

0.1

0.01
5

10
15
Problem Number

20

25

5

10
15
Problem Number

M-Prime

25

Rovers Numeric
1000

CPLEX
LPSolve
CLP

100

100

10

10

Time (s)

Time (s)

1000

20

1

1

0.1

0.1

0.01

CPLEX
LPSolve
CLP

0.01
5

10

15
20
Problem Number

25

30

2

4

6

8

10
12
Problem Number

14

16

18

20

Figure 5: Time taken by limited-lp-rpg to solve problems using different LP solvers

384

A Hybrid LP-RPG Heuristic for Planning

Market Trader
300

Hydro Power
300

CPLEX
CLP
LPSolve

250

200
Plan Length

200
Plan Length

CPLEX
CLP
LPSolve

250

150

150

100

100

50

50

0

0
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
Problem Number

Pathways Metric
3000

2500

25

30

Sugar
55

CPLEX
CLP
LPSolve

CPLEX
CLP
LPSolve

50
45
40

2000

35

Plan Length

Plan Length

20

1500

30
25

1000

20
15

500

10
0

5
5

10

15
20
Problem Number

25

30

2

4

6

8

Settlers
500
450

10
12
Problem Number

14

16

18

20

Settlers Num. Carts
800

CPLEX
CLP
LPSolve

700

CPLEX
CLP
LPSolve

400
600
350
Plan Length

Plan Length

300
250
200

500
400
300

150
200
100
100

50
0

0
5

10
15
Problem Number

20

25

5

10
15
Problem Number

M-Prime
60

50

25

Rovers Numeric
120

CPLEX
CLP
LPSolve

100

CPLEX
CLP
LPSolve

80
Plan Length

40
Plan Length

20

30

60

20

40

10

20

0

0
5

10

15
Problem Number

20

25

30

2

4

6

8

10
12
Problem Number

14

16

18

20

Figure 6: Lengths of plans produced by limited-lp-rpg using different LP solvers

385

Coles, Coles, Fox & Long

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric Carts
MPrime
Rovers Numeric
Total

CPLEX
20
29
28
9
12
21
28
13
160

CLP
16
30
1
10
11
10
30
12
120

LPSolve
15
30
2
9
15
8
30
12
121

Table 3: Coverage of limited-lp-rpg with different LP solvers

The results of these tests are shown in Figure 5, and Table 3. Beginning with the Market
Trader domain, it is quite clear that CPLEX is faster than CLP in this domain. LPSolve,
in turn, substantially out-performs CPLEX on many problems, by two orders of magnitude.
This speed, though, comes at a cost in terms of robustness: CPLEX solves all 20 problems,
but LPSolve solves only 15. LPSolve also demonstrates strong performance in the Hydro
Power domain, while CLP falls between LPSolve and CPLEX.
The Pathways Metric domain illustrates the robustness of CPLEX to extension beyond
a basic producer–consumer model encoding. This domain contains actions with numeric
preconditions that must be true, but are not affected by the action. As described in Section 7.2, encoding this requires an integer variable for each such precondition. This domain
also contains goals referring to multiple numeric variables, e.g:
(>= (+ (available cycDp1) (available c-Myc-Max)) 3)))
As can be seen, in this domain, CPLEX is the only LP solver which allows lp-rpg
to solve anything but the smallest problems. Beyond problem 2, CLP is unable to solve
the LP to reach the goals from the initial state. Using LPSolve, solvable LPs are reported
unsolvable, so whilst the planner can make an attempt at search, erroneous state pruning
happening as a result of LPs being falsely declared unsolvable renders it unable to find
solutions.
In the Sugar domain, no solver leads to the planner performing particularly well, with no
more than 10 problems being solved. This contrasts with earlier results, shown in Table 2,
where a different configuration of lp-rpg, using CPLEX and richer LPs, was able to solve
18 problems. However, as noted at the start of this section, we have had to compromise the
performance of the planner when using CPLEX to allow a reasonable comparison with CLP
and LPSolve: using the richer LP models here, CPLEX solves 18 problems but LPSolve and
CLP both perform far worse (falsely claiming LPs are unsolvable, or returning suboptimal
solutions, to the detriment of the performance of the planner).
In the two different encodings of the Settlers domain, where carts are represented either explicitly or using the carts-at function, we can see how CPLEX is more robust
under alternative domain encodings. In the original IPC domain model, LPSolve performs
particularly well, while CLP is not markedly different to CPLEX. Using the numeric-carts
386

A Hybrid LP-RPG Heuristic for Planning

Domain

LPSolve
CLP
CPLEX
Build (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)
Market Trader
5.0
167.1
1.3
137.9
17.3
44.6
Hydro Power
0.7
1.4
0.3
5.9
9.4
6.6
Pathways Metric
0.0
1.3
4.3
8.8
4.4
3.1
Sugar
0.8
7.5
1.6
42.0
17.6
22.9
Settlers
2.6
31.9
2.2
170.6
165.5
87.6
Settlers Num. Carts
2.2
22.9
1.6
71.0
48.0
33.6
MPrime
3.1
5.4
5.3
11.5
76.2
6.3
Rovers Numeric
0.8
4.6
0.7
7.2
14.8
1.9
Average
1.9
30.3
2.2
56.8
44.1
25.8
Table 4: Time spent in LP building and solving using different LP solvers

encoding, however, CPLEX is considerably better than the other solvers, being robust to
the change in the LP structure arising from this alternative domain encoding.
In the MPrime and Rovers domain, CLP and LPSolve are consistently faster than
CPLEX.
Summarising, we can see that if limited-lp-rpg can solve a problem using CLP or
LPSolve, it is usually faster than using CPLEX, but CPLEX offers better coverage and its
greater robustness grants access to the richer encodings that allow better performance.
In order to investigate in more detail why the planner takes much longer to solve problems using CPLEX we devised some further tests. Table 4 shows the average time spent,
per state, building and solving the LP for each LP solver. (The building time is the time
required to integrate constraints inserted by lp-rpg into the internal model used by the
particular solver.) Note that these results are not necessarily directly comparable: the planners do not necessarily take the same paths through the state space, so might be evaluating
different states. The paths are, nonetheless, often similar and the times can be taken as
strongly indicative. To give the fairest possible comparison, we include in this table only
data for problems that were solved by all three configurations, so the data is presented for
each planner across exactly the same problem set. A startling observation is that CPLEX
typically spends an order of magnitude longer building LPs than the other two solvers (in
Settlers two orders of magnitude). So, while it often solves the LPs more quickly, the total time spent handling the LPs is generally greater. Indeed, for CPLEX, the time spent
solving the LPs is, in 5 out of 8 domains, dominated by time spent building them. This
indicates that, although CPLEX should be a good choice to use as the LP solver (it solves
the LPs efficiently), in practice other solvers are faster due to the substantial overheads of
building the large number of LPs necessary (at least one per state). These results suggest
that a robust LP solver with low LP building overheads could dramatically improve the
performance of lp-rpg.
When considering solution quality we note that the only way in which the LP Solvers can
direct search on to a different trajectory is if the solvers return different optimal solutions
to the same LP at some point during search. Recall that no planner configuration seeks
to directly minimise plan length: the objective function in the LPs uses a weighted sum
387

Coles, Coles, Fox & Long

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric Carts
MPrime
Rovers Numeric
Total

1l
18
30
30
11
13
22
29
10
163

1.1l
20
29
28
9
19
22
30
13
170

3l
20
27
30
18
18
23
30
15
181

5l
20
27
30
18
17
21
30
15
178

10l
20
26
30
20
16
21
30
15
178

hmax
20
28
30
16
20
23
30
14
181

hadd
20
28
30
16
22
23
29
15
183

Table 5: Coverage when varying LP objective function weighting schemes

of the number of actions. Figure 6 shows that a similar picture arises in plan length as in
time performance: LPSolve often leads the planner to shorter solutions than CPLEX (also
helping to explain why it is often faster, since it explores the search tree to a smaller depth).
In particular this happens in the Rovers Numeric and Sugar domains as well as the Settlers
variants. Across other domains there is little variation in the quality of solutions produced.
8.4 LP Objective Function Weighting Schemes
When using the LP during the solution extraction phase, one open issue is what weighting
scheme to use in the objective function. Since the LP ignores the propositional preconditions
of actions, using the simple objective of minimising the sum of action variables (layerweighting scheme with k = 1) gives the LP solver freedom to select equally between actions
appearing in any layers of the RPG, regardless of how many actions subsequently need to
be added to the relaxed plan to support them. As discussed earlier, using a layer-weighting
scheme with k > 1, or using a weighting scheme based on estimated costs of achieving
the preconditions of the actions, should encourage the LP solutions to favour actions that
are cheaper to apply. The hmax or hadd heuristics are both candidates on which to base
estimates for the cost of application of actions. Of course, there will be cases in which
the choice of an earlier action, or of one that has lower costs to achieve its preconditions,
will be a flawed choice, worse than the choice that would have been made using the k = 1
layer-weighting scheme: that is simply the nature of heuristics.
In this section, we evaluate a range of LP action-variable layer-weighting schemes and
also action-cost estimate schemes. We use k ∈ 1, 1.1, 3, 5, 10 for the layer-weighting schemes.
We consider setting action variable weights to 1 plus the cost of meeting their propositional
preconditions with hmax or hadd for the action-cost estimate schemes. The other parameters
of the planners are set to sensible defaults: action variables in the first action layer are
integral, and propositional goals and landmarks are included in the goal-checking LP.
Results showing coverage of each of these configurations are shown in Table 5. Our first
observation is that any other weighting scheme is better than using a layer-weighting with
k = 1 for action variables. This is particularly noticeable in the Settlers domain, where
there are several situations in which the earlier actions should be preferred. For example,
388

A Hybrid LP-RPG Heuristic for Planning

Weight 1 vs Weight 1.1: Time
Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight 1.1 (Nodes)

100

Weight 1.1 (Time (s))

Weight 1 vs Weight 1.1: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

Weight 1 vs Weight 3
Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight3 (Nodes)

Weight 3 (Time (s))

100

1000

Weight 1 vs Weight 3: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

100
Weight 1 (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

100
Weight 1 (Nodes)

Weight 1 vs Weight 5: Time
Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight5 (Nodes)

100

Weight 5 (Time (s))

Weight 1 vs Weight 5: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

Weight 1 vs Weight 10: Time

Weight 10 (Time (s))

100

1000

Weight 1 vs Weight 10: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight10 (Nodes)

1000

100
Weight 1 (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

100
Weight 1 (Nodes)

1000

Figure 7: Layer-weighting schemes in the LP (k = 1 versus k = x for different values of x)

389

Coles, Coles, Fox & Long

Weight 1 vs H Max: Nodes Expanded

Weight 1 vs H Max: Time

H Max (Time (s))

100

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

H Max (Nodes)

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Weight 1 (Time (s))

100

1

1000

10

100
Weight1 (Nodes)

Weight 1 vs H Add: Nodes Expanded

Weight 1 vs H Add: Time

H Add (Time (s))

100

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

H Add (Nodes)

1000

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Weight 1 (Time (s))

100

1

1000

10

100
Weight 1 (Nodes)

1000

Figure 8: Action weighting schemes in LP objective function (layer-weighting with k = 1
versus action-cost estimate based schemes using hadd or hmax )
supposing a cart is at location A, and a unit of a resource, initially available at both A
and B, needs to be moved to location C. There are two two-action solutions to the LP.
Annotating each action with the layer at which it appears, these are:
• load cart at A (0), unload cart at C (1);
• load cart at B (1), unload cart at C (1).
Using the layer-weighting scheme with k = 1 these two solutions are indistinguishable
(each has cost 2). However, if the latter is selected, two actions are needed as support in the
relaxed plan: moving the cart to B and moving the cart to C. Any of the alternative action
variable weighting schemes set the weight of loading the cart at B to be higher than that
of loading it at A, leading to a preference for the first solution, which is a better outcome
for search guidance. A similar phenomenon occurs in the Rovers domain, where favouring
earlier actions increases the preference for recharging the rovers at or close to their current
locations. In this domain this leads to better avoidance of dead-ends, since postponing
recharging actions risks the possibility that the rover will have too little charge left to get
back to a recharging location.
The data for the layer-weighting schemes show that the results peak at k = 3, a reasonable trade-off between minimising the number of actions chosen in the LP solution and
favouring earlier actions. The overall performance of hmax is the same, losing performance
390

A Hybrid LP-RPG Heuristic for Planning

Time Taken
Nodes Expanded
Plan Length
k=
1.1 3 5 10 hmax hadd 1.1 3 5 10 hmax hadd 1.1 3 5 10 hmax hadd
k=1 X X X X X
X X X X X X
X 7 X X X X
X
k = 1.1
X X X 7
7
X X X 7
X
X X X X
X
k=3
7 7 X
X
7 7 X
X
X X X
X
k=5
X X
X
7 X
7
7 X
X
k = 10
X
X
X
7
X
X
hmax
7
X
7

Table 6: Results of Two-Tailed Wilcoxon Signed Rank Tests comparing different LP weighting schemes. X indicates significance (0 = 0.05), colour indicates the better performer
(faster, fewer nodes expanded or shorter plans).

in Sugar and Rovers but gaining in Settlers and Hydro Power. Using hadd gives further
gains in Settlers, leading to very slightly better coverage than the layer-weighting scheme
with k = 3. The difference in performance between k = 3 and hadd weighting schemes is
domain-dependent. hadd only increases the weight of an action above 1 if it has propositional preconditions which are not true in the state being evaluated. In the Settlers domain,
where it gives particularly good performance, this is a sound approach, neatly capturing
the example case discussed earlier in this section: loading or unloading from a cart at its
current location is preferable to doing so at later locations. In domains where earlier actions
are preferable even if their propositional preconditions require little or no support, hadd fails
to give an adequate bias, and the layer-weighting scheme with higher k performs better.
For instance, in the Sugar domain, the best coverage is obtained by using k = 10, and the
k = 3 scheme also performs more strongly than hadd here.
Examining the performance of the configurations in more detail, comparing the time
taken to find solution plans and the number of nodes evaluated, scatterplots comparing
each of the configurations tested to layer-weighting with k = 1 are shown in Figures 7
and 8, for other layer-weighting schemes and action-cost estimate based schemes. As lprpg usually exhausts the time limit of 30 minutes before the memory limit, the time-taken
scatterplots are closely similar to the coverage table. Coverage is directly reflected in the
number of points on the far right of the graphs, indicating where a layer-weighting scheme
with k = 1 (the x-axis) was unable to find a solution within 30 minutes, but lp-rpg using
a different weighting scheme was able to solve the problem. There are more such points for
k = 3 and hadd than the other schemes, with the points for hadd appearing predominantly
in the Settlers domain and those for k = 3 spread across the domains.
Since it is not always clear from the scatterplots whether one configuration is better than
the other or whether the differences are significant, we have used the Two-Tailed Wilcoxon
Signed Rank Test to compare each pair of tested configurations in terms of time taken and
nodes evaluated, and also plan length.7 All tests are performed with p = 0.05. The results
of these tests are summarised in Table 6. A number of interesting observations can be made:
7. The Wilcoxon signed-rank test is a non-parametric statistical test used to compare a set of matched
samples (such as the pairs of results for two different planners on the same sequence of problems) to
assess whether their population mean ranks differ (i.e. it is a paired difference test). It is useful when
the absolute values are not necessarily comparable and when the samples are drawn from a completely

391

Coles, Coles, Fox & Long
1l vs 1.1l: Plan Length
10000

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000
3l (Plan Length)

1000
1.1l (Plan Length)

1l vs 3l: Plan Length
10000

100

100

10

10

1

1
1

10

100

1000

10000

1

10

100

1l (Plan Length)

1l vs 5l: Plan Length
10000

10l (Plan Length)

5l (Plan Length)

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

100

10

100

10

1

1
1

10

100

1000

10000

1

10

1l (Plan Length)

1000

10000

1000

10000

1l vs hadd: Plan Length
10000

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

hadd

1000

100
1l (Plan Length)

1.1l vs hmax
10000

hmax (Plan Length)

10000

1l vs 10l: Plan Length
10000

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

1000

1l (Plan Length)

100

10

100

10

1

1
1

10

100

1000

10000

l

1

10

100
l

1.1 (Plan Length)

1 (Plan Length)

Figure 9: Effect on quality of varying action weighting schemes in LP objective function

• While k = 3 gives better coverage than k = 5 or k = 10, it does not take a significantly
different amount of time nor explore a significantly different number of nodes.
• There is, however, a significant difference in time taken and nodes, in favour of k = 3,
when compared to all the other configurations tested.
• The action-cost based weighting schemes are slower than the layer-weighting schemes
for k ≥ 3. Furthermore, when compared to k = 3, they expand more nodes.
unknown distribution. The two-tailed version makes no assumption about which of the two contenders
has the better performance (i.e. the lower mean rank).

392

A Hybrid LP-RPG Heuristic for Planning

• The key strength of the action-cost based weighting schemes lies in the quality of the
solutions found. As a general trend, layer-weighting schemes gain coverage but lose
quality as k increases. The action-cost based schemes generate significantly shorter
plans than any other configuration tested, including layer-weighting with k = 1.
The increase in plan length for plans found using the layer-weighting scheme with increasing k can be seen in Figure 9. The reason for this increase is quite clear: the basic
objective function for the LP is to minimise number of actions. However, when weightings
are added to the actions according to the RPG layer in which they appear, longer plans
that make use of earlier actions can become more attractive. The increase in plan length is,
however, largely restricted to two domains. The first, and worst affected, is Pathways Metric, where the increase in k causes a preference for reactions that are less efficient (in terms
of the number of actions needed) but can be performed using actions from the earliest layers
of the planning graph. The second is Settlers with Numeric Carts, where the increase in k
leads again to solutions with a preference for resource production and refinement approaches
that are less efficient, but comprise actions that appear earlier in the planning graphs. As is
perhaps to be expected, when these two domains are excluded from the statistical analysis,
there is no significant difference in plan length between k = 3 and the action-cost based
schemes.
Of the weighting schemes tested the best results are obtained using k = 3 and hadd :
there is no single best option. The former is faster, expands fewer nodes, and gives a good
balance in coverage across the domains used. The latter is less prone to variations in plan
quality in some domains, and its strong performance in the Settlers domain leads to two
more problems being solved overall within the test domains.
8.5 The Use of Integer Constraints
In Section 5.4.1 we discussed the fact that the LP is a relaxation of a MIP, and proposed that
in certain situations it may be beneficial to not relax some action variables, making them
integral. In this section, we will explore this hypothesis, considering five configurations of
lp-rpg:
1. Minimal Integers: only actions with assignment effects (as in Section 4.3) are integers.
2. First-layer: as above, but variables for actions appearing in RPG layer 1 (the potentially helpful actions) are also integral.
3. Propositional-Goal Achievers: as above, but the variables for any actions that achieve
propositional goals or landmarks are also integral.
4. Numeric-Goal Achievers: as above, but additionally, the variables for actions affecting
numeric state variables that appear in numeric goals are also integral.
5. All: all (action) variables are integral.
The coverage of each of these configurations is shown in Tables 7 and 8, for layerweighting using k = 1.1 and k = 3, respectively. For the k = 3 results, the coverage is fairly
insensitive to the configuration used. There is a peak in coverage with the ‘Numeric Goal
393

Coles, Coles, Fox & Long

Domain

Minimal
First-Layer Prop. Goal Num. Goal
All
(Assignments) Actions
Achievers Achievers Variables
Market Trader
20
20
20
20
20
Hydro Power
23
29
29
29
29
Pathways Metric
30
28
28
30
29
Settlers
21
19
19
19
19
Settlers Num. Carts
22
22
22
22
21
MPrime
30
30
30
30
30
Rovers Numeric
14
13
13
13
15
Sugar
10
9
9
9
19
Total
170
170
170
172
182
Table 7: Coverage varying which action variables are integer in the MIP (layer-weighting
with k = 1.1)

Domain

Minimal
First-Layer Prop. Goal Num. Goal
All
(Assignments) Actions
Achievers Achievers Variables
Market Trader
20
20
20
20
20
Hydro Power
23
27
27
29
29
Pathways Metric
30
30
30
30
30
Settlers
20
18
18
18
15
Settlers Num. Carts
22
23
23
23
23
MPrime
30
30
30
30
30
Rovers Numeric
15
15
15
15
15
Sugar
20
18
18
18
20
Total
180
181
181
183
182
Table 8: Coverage varying which action variables are integer in the MIP (layer-weighting
with k = 3)

Achievers’ configuration, though the difference between that and the worst configuration
is only 3 problems. Using k = 1.1, there is a marked increase in coverage when all action
variables are integral. This is due to the Sugar domain: compared to the preceding configuration in the table, an additional 10 problems are solved. In the same domain, when
using k = 3, though, even better coverage is obtained when using only the minimal number
of integral action variables. Thus, it appears the need for integral variables in this domain
is reduced once the objective preference for earlier actions is sufficiently high. Disregarding
the Sugar domain, the results for k = 1.1 in other domains are very close, as in the k = 3
configuration.
It is interesting that, even with many integers in the MIP, the performance of lp-rpg
in terms of coverage is not very different from using no integers at all. Considering the
computational complexity of solving a MIP, rather than solving an LP, the time spent
calculating the heuristic should be considerably higher, rendering an all-integers approach
394

A Hybrid LP-RPG Heuristic for Planning

Minimal Ints vs First Layer Actions: Time

100

Minimal Ints vs First Layer Actions: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

First Layer Actions (Nodes)

First Layer Actions (Time (s))

1000

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

10

Minimal Ints vs (First Layer Actions +) Prop Goal Achievers: Time

100

1000

Minimal Ints vs (First Layer Actions +) Prop Goal Achievers: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Prop Goal Achievers (Nodes)

Prop Goal Achievers (Time (s))

1000

100
Minimal Ints (Nodes)

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

Minimal Ints vs (First Layer Actions + Prop Goal Achievers) Numeric Goal Achievers: Time

100

100
Minimal Ints (Nodes)

1000

Minimal Ints vs (First Layer Actions + Prop Goal Achievers) Numeric Goal Achievers: Nodes Exp. and

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Num. Goal Achievers (Nodes)

Num. Goal Achievers (Time (s))

1000

10

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

10

Minimal Ints vs All Ints: Time

All Ints (Time (s))

100

1000

Minimal Ints vs All Ints: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

All Ints (Nodes)

1000

100
Minimal Ints (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

10

100
Minimal Ints (Nodes)

Figure 10: Changing which variables are integer (k = 3)

395

1000

Coles, Coles, Fox & Long

Time Taken
First Prop. Num.
Layer Goal Goal
Acts Ach. Ach.
Minimal Ints
X
X
X
First Layer Acts
7
X
Prop. Goal Ach.
X
Num. Goal Ach.

Nodes Expanded
Plan Length
All First Prop. Num. All First Prop. Num. All
In- Layer Goal Goal In- Layer Goal Goal Ints Acts Ach. Ach. ts Acts Ach. Ach. ts
X
7
7
7
7
7
7
7
7
X
7
X X
7
7
7
X
X X
7
7
X
7
7

Table 9: Results of Two-Tailed Wilcoxon Signed Rank Tests comparing different sets of
variables as integers in the MIP (using a layer-weighting of k = 1.1). X indicates significance
(p = 0.05), colour indicates the better performer (faster, fewer nodes expanded or shorter
plans).

Time Taken
First Prop. Num.
Layer Goal Goal
Acts Ach. Ach.
Minimal Ints
X
X
X
First Layer Acts
X
X
Prop. Goal Ach.
X
Num. Goal Ach.

Nodes Expanded
Plan Length
All First Prop. Num. All First Prop. Num. All
In- Layer Goal Goal In- Layer Goal Goal Ints Acts Ach. Ach. ts Acts Ach. Ach. ts
X
7
7
7
7
7
7
7
7
X
7
X X
7
7
7
X
7
X
7
7
X
7
7

Table 10: Results of Two-Tailed Wilcoxon Signed Rank Tests comparing different sets of
variables as integers in the MIP (using a layer-weighting of k = 3). X indicates significance
(p = 0.05), colour indicates the better performer (faster, fewer nodes expanded or shorter
plans).

impractical. Investigating this further, scatterplots of the time taken and the number of
nodes expanded are shown in Figure 10. (The results shown are for k = 3, but the overall
picture is the same if k = 1.1 is used.) Each configuration is compared to the ‘Minimal
Integers’ configuration. The general trend, as one moves down the left column (increasing
the proportion of action variables that are integral), is for more points to drift above the line
y = x, i.e. the time taken to solve problems increases. At the same time, though, moving
down the right-hand column there is a decrease in the number of nodes evaluated. Thus,
increasing the proportion of integral action variables seems to improve search guidance,
though not sufficiently to allow a pay-off in terms of the time taken to solve problems.
To confirm significance of these observations, we applied Two-Tailed Wilcoxon Signed
Rank Tests, the results of which are in Tables 9 and 10, for k = 1.1 and k = 3, respectively.
In both cases, there is a consistent increase in the time taken to solve problems as the
proportion of integral action variables is increased: this difference is significant in every
case other than when using k = 1.1 and comparing ‘First Layer Actions’ to ‘Propositional
Goal Achievers’. There is no significant difference in plan length between any pair of
configurations. The results for nodes are somewhat less clear. It does not appear that
using ‘Minimal Integers’ leads to expansion of a significantly different number of nodes,
396

A Hybrid LP-RPG Heuristic for Planning

Domain

Minimal Ints
First Layer Actions
All Ints
Build (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)
Market Trader(20)
26.4
27.7
27.8
52.1
27.2
55.3
Hydro Power (21)
8.8
1.8
9.2
6.5
10.8
6.2
Pathways Metric (30)
58.5
271.2
64.9
834.0
65.4
3468.9
Sugar (16)
24.9
33.8
26.1
46.6
24.7
329.2
Settlers (9)
282.3
96.1
453.8
201.6
295.2
204.7
Settlers Num. Carts (16) 399.4
101.6
520.9
462.8
408.8
222.1
MPrime (28)
78.8
4.8
75.6
6.3
78.9
6.1
Rovers Numeric (11)
57.7
1.7
42.4
2.1
59.0
2.8
Average
117.1
67.3
152.6
201.5
121.3
536.9
Table 11: Time spent in building and solving LP varying which variables are integer in the
MIP; numbers shown with domains indicate how many problem instances were solved and
used in computing the reported average

when compared to any other configuration. In part this is due to a limitation of the
tests — only pairwise-solved problems can be included, so the difference in coverage is not
reflected in the data analysed in the test. Considering the other configurations, though,
‘All Integers’ evaluates fewer nodes than ‘First Layer Actions’ and ’Propositional Goal
Achievers’, regardless of which weight is used, but cannot be shown to expand fewer nodes
than ‘Numeric Goal Achievers’, the configuration that offered better coverage in Table 8.
Although we have seen that increasing the number of integer variables makes the planner
slower, it is somewhat surprising that the amount by which the planner is made slower is
not greater than it is. In theory we would expect that making all variables integer would
dramatically decrease the performance of the planner, however, it appears this is not so.
To investigate this further we consider how long was spent building and solving the LP in
each state for each configuration. Table 11 gives an indication of why we are seeing this
surprising result: in 5 out of 8 domains the solving time of the LP is less than the building
time (for the All Ints configuration), showing again that a key bottleneck in using the LP
solver is, in fact, in building the LP. Building time is, of course, not likely to vary between
configurations (the only variation is due to different states being expanded) and compared
to this the solving time is small. In two of the three remaining domains, Pathways and
Sugar, we see some increase in the cost of solving the LP when first-layer actions are made
integral, and a much larger increase when all actions are made integral, this is the pattern
that we expected. In the final domain, Market Trader, the main increase occurs when
first-layer action variables are integral: the structure of this domain means that integral
first layer actions often causes the variables in other layers to also become integral. Solving
times does, however, remain within an order of magnitude of building time, so the overhead
is not particularly large in this domain compared to the previous two.
397

Coles, Coles, Fox & Long

8.6 Including the Numeric Goal Conjunct
In Section 7.1 we discussed the possibility of including the entire numeric goal conjunct for
the problem in the LP. As well as theoretically increasing the ability to detect dead-ends —
by insisting all goals are attainable at the same time, rather than individually — this also
allows arbitrary LNF goals to be used, as found in domains such as Pathways. In this
section, we will investigate the impact of this extension. In particular, we explore whether
the use of a numeric-goal-checking LP including the numeric goal conjunct improves or
worsens performance. To ensure that there is no goal-checking LP in the case where the
numeric goal conjunction is not being used, for these tests, in both configurations, we
disable the inclusion of propositional goals and landmarks in the LP. To gain insights into
how the impact of a numeric-goal-checking LP is affected by the choice of action-variable
layer-weighting schemes, we consider two: k = 1.1 and k = 3.
Domain

Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Num. Carts
MPrime
Rovers Numeric
Total (Excl. Pathways)
Total

With
Num.Goal
(k = 3)
20
27
30
18
13
19
28
13
138
168

Without
Num. Goal
(k = 3)
20
27
0
14
14
18
29
13
135
135

With
Num. Goal
(k = 1.1)
20
29
28
9
12
21
28
13
132
160

Without
Num. Goal
(k = 1.1)
20
30
0
14
10
22
29
13
138
138

Table 12: Coverage varying whether the Numeric Goal conjunct is included in the LP (with)
or not (without)

The coverage results for running lp-rpg with and without the numeric goal conjunct
are shown in Table 12. Looking at the results excluding the Pathways domain (which
can only be solved if the numeric goal conjunct is included, since the goals are expressed
as an arbitrary LNF), one can make two immediate observations: the use of the numeric
goal conjunct improves performance with actions weighted as k = 3, solving 3 additional
problems, but its use worsens performance with actions weighted as k = 1.1, solving 6
fewer problems. This difference in impact is an interesting consequence of the relationship
between the RPG structure and the LP:
• In the no-goal-conjunct case, if a goal appears at fact layer l, the LP used to meet
this goal is LP (l) — the LP containing the actions up to action layer l (Algorithm 4,
line 22). This favours the earlier actions in the RPG, precluding any actions after
layer l from being used.
• In the numeric goal-conjunct case, the LP is extended until the layer l0 at which,
first, all goals appear, and second, an LP constrained to meet all the goals, using the
actions up to layer l0 , can be satisfied. For individual goals, this point may be later
398

A Hybrid LP-RPG Heuristic for Planning

With vs Without Numeric Goal Conjunct: Nodes Expanded

With vs Without Numeric Goal Conjunct: Time
Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

100

With Numeric Goal Conjunct (Nodes)

With Numeric Goal Conjunct (Time (s))

1000

10

1

0.1

0.01

0.001
0.001

0.01

0.1
1
10
100
Without Numeric Goal Conjunct (Time (s))

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

1000

100

10

1

1000

1

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

10

1

0.1

0.01

0.001
0.001

0.01

0.1
1
10
100
Without Numeric Goal Conjunct (Time (s))

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

1000
With Numeric Goal Conjunct (Nodes)

With Numeric Goal Conjunct (Time (s))

100

1000

With vs Without Numeric Goal Conjunct (Weight 3): Nodes Expanded

With vs Without Numeric Goal Conjunct (Weight 3): Time
1000

10
100
Without Numeric Goal Conjunct (Nodes)

100

10

1

1000

1

10
100
Without Numeric Goal Conjunct (Nodes)

1000

Figure 11: Including the numeric goals in the LP
than the layer l at which they first appeared, in which case actions that are in l0 but
not l can be chosen to meet these goals, whereas in the previous case they could not.
It is this difference in the actions that are available to support a given goal that makes
the action-variable weighting scheme important. The use of k = 3 rather than k = 1.1
means that while actions added between layer l and layer l0 (inclusive) are available to meet
a goal that first appeared at layer l, the objective function leads to a preference to use
earlier actions.
The coverage results in these experiments do not give an unequivocal picture of the
impact of this feature on performance. Figure 11 shows scatterplots of the time taken to
solve problems, and the number of nodes evaluated, when the numeric goal-conjunct is
used or not, for each of k = 1.1 and k = 3. There appears to be a general trend for the
inclusion of the numeric goal conjunct to reduce the number of nodes evaluated. In the case
of weighting by k = 3, a Two-Tailed Wilcoxon Signed Rank Test confirms that the use of
a numeric goal conjunct reduces the number of nodes evaluated (p = 0.05). Using k = 1.1
suggests a similar trend, but the results are not significant.
With both weighting schemes, the use of the numeric goal conjunct introduces a small
but statistically significant time overhead. This is due to the additional time taken to
evaluate each state: the RPG must be extended to the point at which all goals can be
satisfied together, rather than individually. Whether this pays off, i.e. whether the reduction
in nodes evaluated is sufficient to offset this, depends on the domain and appears to be
399

Coles, Coles, Fox & Long

correlated with the extent to which the numeric goals interact. At one extreme, in the
Rovers domain, all the goals are propositional and there is no difference in performance.
On the other hand, in the Sugar domain, and the Settlers domain when numeric carts are
used, it is beneficial. Both of these domains concern production and reprocessing of raw
materials, in one form or another, leading to interaction between goals. For instance, a unit
of a resource may be sufficient to satisfy goals individually, but additional production may
be required to support them both. In these cases use of the numeric goal conjunct improves
time performance. Inclusion of the numeric goal conjunct has no significant impact on the
length of plans produced (according to a Two-Tailed Wilcoxon Signed Rank Test).
To summarise the results in this section, the main benefit from the use of the numeric
goal conjunct is to be able to extend the expressivity of the planner to domains where
the goals are written using arbitrary LNF. The success of the approach in other domains
varies. In terms of coverage, whether or not it is better to use the numeric goal conjunct on
the evaluation domains depends on the weighting scheme. When using the layer-weighting
scheme, k = 3, the inclusion of the numeric goal conjunct is slightly beneficial and lp-rpg
is therefore set to use this configuration by default.
8.7 Including Propositions in the LP
In the previous section, we observed that the inclusion of the numeric goal conjunct in a
goal-checking LP has variable impact on performance, depending on the weighting scheme
used. Perhaps a more interesting use of a goal-checking LP is when using the LP to meet
propositional goals, and landmarks, as described in Section 6.1. To evaluate this technique
we consider four configurations:
1. No propositions: using a goal-checking LP containing only the numeric-goal conjunct.
2. Propositional goals: as in previous case, but also including the propositional goals in
the goal-checking LP.
3. Landmarks: as in previous case, but also including landmarks.
4. All propositions: as in previous case, but also constrained to ensure that if an action
variable is non-zero, actions are added to meet its propositional preconditions (as
described in Section 6.3).
These form a spectrum, from the case in which no information about propositions is
included in the LP at all, to the last in which the goal-checking LP must not only meet the
propositional goals, but also the preconditions of the actions chosen to do so. We consider
two layer-weighting schemes (k = 1.1 and k = 3), and action variables for actions in the
first action layer are integral.
The coverage results for k = 1.1 are shown in Table 13 and those for k = 3 in Table 14.
As can be seen, in both cases, a general pattern emerges: coverage improves up to and
including the configuration in which landmarks are included in the LP, but then declines
in the final ‘All Propositions’ configuration. Including all propositions appears, however, to
remain better than including no propositions at all.
Scatterplots illustrating the time taken and the number of nodes evaluated when solving
problems are shown in Figures 12 and 13 (for weights k = 1.1 and k = 3, respectively).
400

A Hybrid LP-RPG Heuristic for Planning

No Props vs Propositional Goals: Time

100

No Props vs Propositional Goals: Nodes Expanded
10000

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
settlers
Pathways Metric
Hydro Power
Market Trader

10

1

Sugar
Rovers Numeric
M-Prime
Settlers Numeric Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Propositional Goals (Nodes)

Propositional Goals (Time (s))

1000

100

10

0.1

0.01
0.01

1
0.1

1
10
No Props (Time (s))

100

1000

1

10

No Props vs Landmarks: Time

Landmarks (Time (s))

100

1000

10000

No Props vs Landmarks: Nodes Expanded
10000

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Numeric Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000
Landmarks (Nodes)

1000

100
No Props (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
No Props (Time (s))

100

1000

1

10

No Props vs All Props: Time

All Props (Time (s))

100

1000

10000

No Props vs All Props: Nodes Expanded
10000

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Numeric Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000
All Props (Nodes)

1000

100
No Props (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
No Props (Time (s))

100

1000

1

10

100
No Props (Nodes)

1000

Figure 12: Varying which propositions are included in the LP (k = 1.1)

401

10000

Coles, Coles, Fox & Long

No Props vs Propositional Goals: Time

Prop Goals (Time (s))

100

No Props vs Propositional Goals: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Propositional Goals (Nodes)

1000

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
No Props (Time (s))

100

1000

1

10

No Props vs Landmarks: Time

Landmarks (Time (s))

100

1000

No Props vs Landmarks: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

100

Landmarks

1000

100
No Props (Nodes)

10

1
10
0.1

0.01
0.01

1
0.1

1
10
No Props (Time (s))

100

1000

1

10

No Props vs All Props: Time

All Props (Time (s))

100

1000

No Props vs All Props: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

All Props (Nodes)

1000

100
No Props

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
No Props (Time (s))

100

1000

1

10

100
No Props (Nodes)

Figure 13: Varying which propositions are included in the LP (k = 3)

402

1000

A Hybrid LP-RPG Heuristic for Planning

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric
MPrime
Rovers Numeric
Total

No Props
20
29
28
9
12
21
28
13
160

Prop Goals
20
29
28
9
19
22
28
9
164

Landmarks
20
29
28
9
19
22
30
13
170

All Props
19
30
30
14
11
21
27
15
167

Table 13: Coverage when varying which propositions are included in the LP (using k = 1.1)

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric
MPrime
Rovers Numeric
Total

No Props
20
27
30
18
13
19
28
13
168

Prop Goals
20
27
30
18
18
23
28
13
177

Landmarks
20
27
30
18
18
23
30
15
181

All Props
20
28
30
19
11
21
30
14
173

Table 14: Coverage when varying which propositions are included in the LP (using k = 3)

Each scatterplot compares one of the configurations 2–4 in the enumerated list above to
configuration 1 (no propositions). Comparing the ‘Propositional Goals’ and ‘Landmarks’
configurations suggests that the inclusion of Landmarks leads to more problems being
solved more quickly compared to the ‘No propositions’ configuration. This is particularly
evident in the Settlers, Settlers Numeric Carts, MPrime and Rovers Numeric domains.
Whether using Landmarks offer gains over Propositional Goals alone depends on the structure of the problems. In the two Settlers variants, including propositional goals (such as
(connected-by-rail p1 p2) or (has-ironworks p3)) necessitates the use of specific actions ((build-rail p1 p2) or (build-ironworks p3), respectively). Since these actions
consume numeric resources, including propositional goals is sufficient to cause the LP to
introduce the production actions necessary to support the resource consumption. In Rovers,
the goals are to have communicated data generated from rock samples, soil analyses and
images. The actions adding these facts consume a number of units of energy, so there
is some benefit from including propositional goals in the LP. However, these actions also
have propositional preconditions: for instance, for a rover to communicate rock data, it
must have taken a sample of the rock. These propositional preconditions will be included
as landmarks in the LP, thereby forcing the production of not only the energy needed to
403

Coles, Coles, Fox & Long

communicate the requisite data, but also the energy to have acquired it. This leads to the
inclusion of additional recharge actions or, if no suitable actions are available, better deadend detection. A similar situation arises in MPrime: the actions that achieve the goal facts
do not consume any resources, but the actions that achieve the (landmark) preconditions
of these actions do, so including the landmarks improves informedness of the heuristic.
Two-Tailed Wilcoxon Signed Rank Tests — shown in Tables 15 and 16 — confirm
(p = 0.05) that with either k = 1.1 or k = 3 the use of Propositional Goals improves over
No Propositions, and the use of Landmarks improves over Propositional Goals. Indeed,
with p = 0.05 we have a total-ordering on time-performance for k = 3:
No Propositions ≺ Propositional Goals ≺ Landmarks
The results show similar behaviour for numbers of nodes evaluated (the right-hand
columns of Figures 12 and 13) although not all of the differences are statistically significant.
Wilcoxon tests also show that with neither k = 1.1 or k = 3 is the difference between
Propositional Goals and Landmarks statistically significant. Note, however, that using
Landmarks allows more problems to be solved. The comparisons for No Propositions,
Propositional Goals and Landmarks, with weight k = 1.1, are significant, the latter two both
improving upon the former. With weight k = 3 the same tests are inconclusive, although
using Landmarks allows 13 additional problems to be solved (recall that the comparisons
are restricted to the problems solved by both variants in the comparison).

No Props
Prop Goals
Landmarks

Time Taken
Prop LandAll
Goals marks Props
X
X
7
X
X
X

Nodes Expanded
Prop LandAll
Goals marks Props
X
X
X
7
X
X

Plan Length
Prop LandAll
Goals marks Props
7
7
X
7
X
X

Table 15: Results of Two-Tailed Wilcoxon Signed Rank Tests comparing inclusion of different propositions in the LP (using layer-weighting k = 1.1). X indicates significance
(p = 0.05), colour indicates the better performer (faster, fewer nodes expanded or shorter
plans).

No Props
Prop Goals
Landmarks

Time Taken
Prop LandAll
Goals marks Props
X
X
7
X
X
X

Nodes Expanded
Prop LandAll
Goals marks Props
X
7
X
7
X
X

Plan Length
Prop LandAll
Goals marks Props
7
7
X
7
X
X

Table 16: Results of Two-Tailed Wilcoxon Signed Rank Tests comparing inclusion of different propositions in the LP (using layer-weighting k = 3). X indicates significance (p = 0.05),
colour indicates the better performer (faster, fewer nodes expanded or shorter plans).

404

A Hybrid LP-RPG Heuristic for Planning

Domain

No Props
Landmarks
All Props
Build (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)
Market Trader (20)
27.8
52.1
28.0
52.3
29.7
74.5
Hydro Power (27)
9.3
6.6
9.8
6.6
11.7
5.5
Pathways Metric (30)
64.9
834.0
67.0
833.3
68.6
905.5
Sugar (18)
27.2
47.5
27.3
47.5
39.9
82.3
Settlers (8)
436.4
145.5
421.6
137.9
563.9
3153.0
Settlers Num. Carts (15) 204.5
434.8
252.5
133.0
244.2
2253.8
MPrime (28)
75.6
6.3
78.9
5.7
91.2
24.5
Rovers Numeric (11)
42.4
2.1
56.3
2.0
76.3
31.2
Average
111.0
191.1
117.7
152.3
140.7
816.3
Table 17: Time spent building and solving the LP, varying which propositions are included;
numbers shown with domains indicate how many problem instances were solved and used
in computing the reported average

The results shown at the bottom of Figures 12 and 13 indicate that the ‘All Propositions’
configuration has less consistent performance.
We can show, however, (p = 0.05) that with k = 1.1 or k = 3, the All Propositions
configuration takes longer to solve problems than both the Propositional Goals configuration
and the Landmarks configuration. It is perhaps surprising that it is feasible to consider
including propositions in the LP in this manner, given the results reported by van den Briel
et al. (2007). The key difference is that we are disregarding delete effects, so if a fact is
needed as a precondition it need only be added at most once. In the work reported by van
den Briel et al., however, in cases where a fact is required as a precondition but also deleted,
the delete effect must be balanced by an equivalent number of add effects (less one if the
fact was true initially).
Looking at the number of nodes evaluated when using the All Propositions configuration
we find an important result: as shown in the bottom right of Figures 12 and 13, whichever
weight is used, All Propositions tends to expand fewer nodes. Furthermore, All Propositions expands fewer nodes than all three of the other configurations and is the overall
best configuration in terms of nodes expanded (significant result, p = 0.05). Unfortunately,
the overhead associated with each node is higher and this results in poorer coverage and
time performance. This result indicates that numeric–propositional separation used in the
lp-rpg heuristic is a sensible trade-off, with the use of the RPG to meet propositional
preconditions sacrificing some performance in terms of nodes expanded in exchange for a
reduction in time taken to solve problems.
Table 17 shows the increase in costs associated with solving LPs including propositions.
These results are taken only from problems solved by all three configurations, although it
remains the case that the configurations will not expand exactly the same states in solving
the same problems. In three domains — Hydro Power, MPrime and Rovers Numeric —
the cost of building LPs dominates the cost of solving them, so there are no significant
decreases in performance in these domains. Of course, the cost of building LPs increases
as more propositions are included since more variables are required. In the Settlers domain
405

Coles, Coles, Fox & Long

there is an order of magnitude increase in the time spent solving LPs, indicating that
including all propositions in the LPs makes them more difficult to solve. Sometimes the
more informed search guidance gleaned from this information allows the planner to find
solutions expanding far fewer nodes.
Adding landmarks to the LP appears to make solving the LP slightly faster: this is
partly a side effect of the landmarks configuration needing to solve fewer LPs per state,
since it uses one LP to meet all the goals instead of one LP per goal. The All Propositions
configuration often only needs to solve one LP in the solution extraction phase. However,
the size and difficulty of this LP means that any benefits of solving fewer LPs (in terms of
time taken) are negated.
Tables 15 and 16 show the results of statistical tests comparing the lengths of the plans
found by the various configurations. The only significant results are that, regardless of
the weight used, All Propositions finds shorter plans than the other configurations. If all
propositions are included in the LP, then typically only a single LP call is made during
solution extraction, simultaneously achieving all goals and action preconditions. In the
other configurations, first, an LP call is made for the goals, and then, for each action A
added to support a propositional precondition of an action implied by the solution to this
LP, the numeric preconditions of A are met by another LP call. Thus, there can be several
LP calls rather than just one. By fragmenting the production of a relaxed plan in this
way, the efficacy of the relaxed plan is eroded. As an example, plan lengths are improved
in the two variants of Settlers. Here, the production of some resources requires building
infrastructure: a sawmill is required to refine timber into wood, and so on. If two units of
wood are to be made, then if the LP has no knowledge of propositional preconditions, there
is no difference in the LP between building two units of wood at location A, or one unit
at location B and one at location C — the need to build one or two sawmills, depending
on which option is chosen, will only be discovered when actions are then chosen to meet
the propositional preconditions of the actions required by the solution to the LP. If the LP
includes All Propositions, then in cases such as this, there is a difference between building
one sawmill and two, so the LP will prefer the single-sawmill solution, ultimately leading to
better search guidance. Thus, All Propositions produces plans that are significantly shorter,
in domains where the fact that propositions are outside the LP disguises the true costs of
the action choices in the solution to an LP.
8.8 Propositional Resource Analysis
At the end of Section 6.4, we identified conditions under which it is possible to turn propositions that model resources into an equivalent numeric representation. For our purposes,
with lp-rpg, this would allow the resource, within the heuristic, to be managed by the LP
rather than the RPG. To this end, we use three domains containing propositional stacks that
represent resources, and evaluate how this encoding affects the performance of lp-rpg, and
other numeric planners. The five planners evaluated are MetricFF, lpg–td, lp-rpg (using
the analysis that translates propositional resource stacks into an equivalent numeric representation as described in Long & Fox, 2000), lp-rpg with this analysis disabled, and
lp-rpg-FF. The three domains we use are:
406

A Hybrid LP-RPG Heuristic for Planning

Domain

MetricFF

lpg–td

lp-rpg-FF

Settlers Prop. Timber
Settlers Prop. Carts
MPrime Prop.
Total

7
4
29
40

4
4
30
38

8
10
29
68

lp-rpg
No Anal.
Anal.
5
16
13
22
29
30
47
47

Table 18: Coverage with and without propositional resource analysis

• A variant of the IPC3 Settlers domain, where the amount of timber at a given location
(or in a given vehicle) is represented by a stack of propositions, in the range n0 to
n10.
• A variant of the Settlers ‘Numeric Carts’ encoding, where the number of carts at a
given location is represented by a stack of propositions.
• The MPrime domain from IPC1, which in its encoding represents fuel level as a stack
of propositions.
In the first two of these, the propositional encoding enforces a limit of 10 on the amount
of timber (respectively, number of carts) at a given location. This is a necessary limitation forced by action grounding. In a numeric representation, decreasing one resource
and increasing another can be done by a single ground action, with appropriate numeric
effects and conditions. The ground action does not need to stipulate the precise levels of
the resource before and after the operation, so long as the limits on the resource levels are
respected. In the propositional case, however, one ground action is needed for each pair of
discrete levels of the two resources, with parameters to the action stipulating the pre- and
post-values of each resource. The level of 10 was chosen to avoid placing overly restrictive
bounds on the resource levels, while creating a manageable number of ground actions. This
issue of grounding also accounts for the limited choice of domains: a propositional stack
can only be used to represent resources that can take a modest range of discrete values. In
the Rovers domain, for example, the set of reachable energy levels for each rover is in the
range [0, 80]. In the Market Trader domain, the amount of money that could be held at
a given time is a real value with one decimal place (this is a consequence of the choice of
problem files), greater than or equal to zero.
Results for these domains are shown in Table 18. Comparing, first, lp-rpg without
analysis to lp-rpg with analysis, we can see that encoding the resources in the LP as
numbers grants a consistent improvement in performance. Figure 14 shows that the time
taken to find solutions is similarly improved, perhaps most strikingly in the second Settlers
variant (with propositional cart levels).
While MetricFF and lpg–td perform well in the MPrime domain, in the two Settlers
encodings, lp-rpg with the resource analysis performs markedly better, with the other
planners solving only a handful of problems. This contrasts with their earlier results, shown
in Table 2, where lpg–td in particular performed well on the Settlers domain. It is an
interesting contrast to see that here, the Propositional Timber variant — derived from the
IPC 3 model on which lpg–td performs well — leads to considerably worse performance.
407

Coles, Coles, Fox & Long

Settlers Propositional Stack Timber
10000

1000

Settlers Propositional Stack Carts
10000

LPRPG-Analysis
LPRPG-FF
FF
LPG
LPRPG-No-Analysis

1000

Time (s)

100

Time (s)

100

LPRPG-Analysis
LPRPG-FF
FF
LPG
LPRPG-No-Analysis

10

10

1

1

0.1

0.1
5

10

15
Problem Number

20

25

5

10

15
Problem Number

20

25

M-Prime Propositional Resource Stack
10000

1000

LPRPG-Analysis
LPRPG-FF
FF
LPG
LPRPG-No-Analysis

Time (s)

100

10

1

0.1

0.01
5

10

15
Problem Number

20

25

30

Figure 14: Using propositional resource analysis

This supports the role of resource analysis in allowing lp-rpg to be robust to which of
a number of almost-equivalent domain encodings is used, with mixed propositional and
numeric encodings of resource levels. We observed no significant change in the length of
plans produced by lp-rpg with or without propositional resource analysis.

9. Conclusions and Future Work
Most modern planning systems are ineffective at reasoning with numbers. However, managing complex numeric interactions is an important part of driving AI planning towards
future real-world application. In this paper we have shown that, by using a linear program
to model numeric resource flows, the ability of planners to reason with domains involving
such complex numeric interactions can be greatly improved.
The key contribution is the separation of the heuristic search control into a relaxed
planning graph, based on delete-relaxation, and a linear program that allows exact reasoning
about numeric constraints and relaxes action ordering.
We have explored how different configurations of the heuristic, in which we put more
or less information in to the LP, impact on the performance of the planner as a whole. An
exploration of different LP solvers reveals that they are more or less efficient at handling
various combinations of constraints. We found that, while LPSolve and CLP, in conjunction
with the limited version of lp-rpg published in 2008 (Coles et al., 2008), can solve simple
408

A Hybrid LP-RPG Heuristic for Planning

problems quickly, CPLEX coupled with the full power of the extended LPRPG is needed
to handle the most complex test instances.
Our work so far has focussed on developing search control methods that can perform
well on numeric planning problems with a particular character: the producer-consumer
behaviour we define in Sectionsect:prodcondefinition. Although we believe that this is a
common behaviour, in practice, numeric domains exhibit a range of other behaviours. There
are several possible ways to exploit the lp-rpg approach in these domains. One is to use the
approach on those actions that conform to the constraints of producer-consumer behaviour,
while pushing other numeric behaviour into a metric RPG in the same way that we currently
handle propositional goals and preconditions in a separate RPG. This would yield the
benefits of potentially better estimates for reachable ranges and action use costs for those
parts of the domain that we can express as producer-consumer behaviour. More challenging
is to consider how other behaviours can be relaxed into producer-consumer behaviour to
obtain useful heuristic information. For example, actions with production effects that vary
could be encoded as a family of producers of increasing capability, discretising the range of
production options and introducing them into the reachability analysis as their increased
production capabilities become available. In general, the relaxations must make reachability
at least as permissive as actual reachability (that is, an action must be applicable in the
relaxed reachability analysis at least as early as the action is actually reachable) and the
relaxed plan extraction should minimise the estimated cost to goal effectively (this is more
difficult because the relaxed plan is not optimal). Within these constraints, we believe that
the use of LP approximations can provide a tool for tackling a wider range of behaviours
than those we explore in this paper.
A further exciting challenge for our future work is to integrate lp-rpg with a method
for optimising plans according to a given objective function. The recent 2008 and 2011
planning competitions highlighted the importance of optimising planning with their emphasis on solution quality. This development is, however, non-trivial: the challenges lie in
the integration of cost optimisation between the LP and the RPG as well as in deciding
how to use a heuristic that trades off goal distance for quality during search. A first step in
this direction was accomplished by Radzi in her PhD thesis (Radzi, 2011), but the advances
in numeric planning described in this paper open up many possibilities for extending that
initial work.

References
Bell, K. R. W., Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). The role of AI planning
as a decision support tool in power substation management. Artificial Intelligence
Communications, 22 (1), 37–57.
Benton, J., Do, M. B., & Kambhampati, S. (2005). Over-subscription planning with numeric goals. In Proceedings of the 19th International Joint Conference on Artificial
Intelligence (IJCAI), pp. 1207–1213.
Benton, J., van den Briel, M., & Kambhampati, S. (2007). A hybrid linear programming
and relaxed plan heuristic for partial satisfaction planning problems. In Proceedings
of the 17th International Conference on Planning and Scheduling (ICAPS).
409

Coles, Coles, Fox & Long

Blackmore, L., Ono, M., & Williams, B. (2011). Chance-constrained optimal path planning
with obstacles. Robotics, IEEE Transactions on, 27 (6), 1080 –1094.
Blum, A., & Furst, M. (1995). Fast planning through planning graph analysis. In Proceedings
of the 14th International Joint Conference on Artificial Intelligence (IJCAI 95), pp.
1636–1642.
Bonet, B., & Geffner, H. (2001). Heuristic Search Planner 2.0. Artificial Intelligence Magazine, 22 (3), 77–80.
Coles, A., Coles, A., Fox, M., & Long, D. (2012). COLIN: Planning with continuous linear
numeric change. Journal of Artificial Intelligence Research, 44, 1–96.
Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008). A hybrid relaxed planning graph-lp
heuristic for numeric planning domains. In Proceedings of the Eighteenth International
Conference on Automated Planning and Scheduling (ICAPS 08).
Coles, A. J., & Coles, A. I. (2011). LPRPG-P: Relaxed plan heuristics for planning with
preferences. In Proceedings of the 21st International Conference on Automated Planning and Scheduling (ICAPS).
Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). Extending the use of inference
in temporal planning as forwards search. In Proceedings of the 19th International
Conference on Automated Planning and Scheduling (ICAPS 09).
Do, M. B., & Kambhampati, S. (2001). Sapa: a domain-independent heuristic metric temporal planner. In Proceedings of the European Conference on Planning (ECP’01).
Do, M. B., & Kambhampati, S. (2003). SAPA: A multi-objective metric temporal planner.
Journal of Artificial Intelligence Research, 20, 155–194.
Domshlak, C., Katz, M., & Lefler, S. (2010). When abstractions met landmarks. In Proceedings of the 20th International Conference on Planning and Scheduling (ICAPS).
Edelkamp, S. (2003). Taming numbers and durations in the model checking integrated
planning system. Journal of Artificial Intelligence Research, 20, 195–238.
Fox, M., & Long, D. (2003). PDDL2.1: An extension of PDDL for expressing temporal
planning domains. Journal of Artificial Intelligence Research, 20, 61–124.
Gerevini, A., Saetti, A., & Serina, I. (2006). An approach to temporal planning and scheduling in domains with predictable exogenous events. Journal of Artificial Intelligence
Research, 25, 187–231.
Gerevini, A., Saetti, A., & Serina, I. (2008). An approach to efficient planning with numerical
fluents and multi-criteria plan quality. Artificial Intelligence, 172 (8-9), 899–944.
Gregory, P., & Rendl, A. (2008). A constraint model for the settlers planning domain. In
Aylett, R. (Ed.), Proceedings of the UK Planning Special Interest Group (PlanSIG).
Herriot Watt University.
Gregory, P., Cresswell, S., Long, D., & Porteous, J. (2004). On the extraction of disjunctive landmarks from planning problems via symmetry reduction.. In Proceedings of
Conference on Symmetry in Search (SymCon 2004), pp. 34–41.
410

A Hybrid LP-RPG Heuristic for Planning

Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths and abstractions: What’s
the difference anyway?. In Proceedings of the 19th International Conference on Planning and Scheduling (ICAPS), pp. 162–169.
Hoffmann, J. (2003). The Metric-FF planning system: Translating “ignoring delete lists”
to numeric state variables. Journal of Artificial Intelligence Research, 20, 291–341.
Hoffmann, J., & Edelkamp, S. (2005). The deterministic part of IPC-4: An overview. Journal
of Artificial Intelligence Research, 24, 519–579.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks in planning. Journal
of Artificial Intelligence Research, 22, 215–278.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. Journal of Artificial Intelligence Research, 14, 253–302.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning with landmarks. In Proceedings
of the 21st International Joint Conference on Artificial Intelligence (IJCAI 09), pp.
1728–1733.
Kautz, H., & Walser, J. (2000). Integer optimization models of AI planning problems.
Knowledge Engineering Review, 15 (1), 101–117.
Koehler, J. (1998). Planning under resource constraints. In Proceedings of the European
Conference on Artificial Intelligence (ECAI’98), pp. 489–493.
Laborie, P. (2003). Algorithms for propagating resource constraints in AI planning and
scheduling: Existing approaches and new results. Artificial Intelligence, 143 (2), 151–
188.
Li, H. X., & Williams, B. C. (2008). Generative planning for hybrid systems based on flow
tubes. In Proceedings of the 18th International Conference on Automated Planning
and Scheduling, ICAPS, pp. 206–213.
Long, D., & Fox, M. (2000). Automatic synthesis and use of generic types in planning. In
Proceedings of Artificial Intelligence Planning and Scheduling (AIPS), pp. 196–205.
Long, D., & Fox, M. (2003a). Exploiting a Graphplan framework in temporal planning.
In Proceedings of the International Conference on Artificial Intelligence Planning and
Scheduling (ICAPS).
Long, D., & Fox, M. (2003b). The 3rd International Planning Competition: Results and
analysis. Journal of Artificial Intelligence Research, 20, 1–59.
Ono, M., & Williams, B. C. (2008). An efficient motion planning algorithm for stochastic
dynamic systems with constraints on probability of failure. In Proceedings of the 23rd
AAAI Conference on Artificial Intelligence, AAAI, pp. 1376–1382.
Porteous, J., Sebastia, L., & Hoffmann, J. (2001). On the extraction, ordering, and usage
of landmarks in planning. In Proceedings of the 6th European Conference on Planning
(ECP 01).
Radzi, N. H. M. (2011). Multi-Objective Planning using Linear Programming. Ph.D. thesis,
University of Strathclyde.
411

Coles, Coles, Fox & Long

Richter, S., Helmert, M., & Westphahl, M. (2008). Landmarks revisited. In Proceedings of
the 23rd AAAI Conference on Artificial Intelligence (AAAI 08), pp. 975–982.
Richter, S., & Westphal, M. (2010). The LAMA Planner: Guiding Cost-Based Anytime
Planning with Landmarks.. Journal of Artificial Intelligence Research, 39, 127–177.
Shin, J.-A., & Davis, E. (2005). Processes and continuous change in a SAT-based planner.
Journal of Artificial Intelligence Research, 166, 194–253.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). An LP-based heuristic
for optimal planning. In Principles and Practice of Constraint Programming (CP
2007), pp. 651–665.
van den Briel, M. H. L., Vossen, T., & Kambhampati, S. (2008). Loosely coupled formulations for automated planning: An integer programming perspective. Journal of
Artificial Intelligence Research, 31, 217–257.
Vossen, T., Ball, M. O., Lotem, A., & Nau, D. S. (1999). On the use of integer programming
models in AI planning. In Proceedings of the 16th International Joint Conference on
Artificial Intelligence (IJCAI 99), pp. 304–309.
Wolfman, S., & Weld, D. (2000). Combining linear programming and satisfiability solving
for resource planning. Knowledge Engineering Review, 15 (1).
Zhu, L., & Givan, R. (2003). Landmark extraction via planning graph propagation. In Proceedings of the Doctoral Consortium at the 13th International Conference on Planning
and Scheduling (ICAPS-DC 03), pp. 156–160.

412

Journal of Artificial Intelligence Research 46 (2013) 235-262

Submitted 11/12; published 02/13

Toward Supervised Anomaly Detection
Nico Görnitz
Marius Kloft

NICO . GOERNITZ @ TU - BERLIN . DE
KLOFT @ TU - BERLIN . DE

Machine Learning Laboratory, Technische Universität Berlin
Franklinstr. 28/29, Berlin, Germany
Computational Biology Center
Memorial Sloan-Kettering Cancer Center
New York City, USA

Konrad Rieck

KONRAD . RIECK @ UNI - GOETTINGEN . DE

University of Göttingen, Dep. of Computer Science
Goldschmidtstr. 7, 37077 Göttingen, Germany

Ulf Brefeld

BREFELD @ KMA . INFORMATIK . TU - DARMSTADT. DE

Technische Universität Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany
German Institute for International Educational Research
Schloßstr. 29, 60486 Frankfurt, Germany

Abstract
Anomaly detection is being regarded as an unsupervised learning task as anomalies stem from adversarial or unlikely events with unknown distributions. However, the predictive performance of
purely unsupervised anomaly detection often fails to match the required detection rates in many
tasks and there exists a need for labeled data to guide the model generation. Our first contribution
shows that classical semi-supervised approaches, originating from a supervised classifier, are inappropriate and hardly detect new and unknown anomalies. We argue that semi-supervised anomaly
detection needs to ground on the unsupervised learning paradigm and devise a novel algorithm
that meets this requirement. Although being intrinsically non-convex, we further show that the
optimization problem has a convex equivalent under relatively mild assumptions. Additionally, we
propose an active learning strategy to automatically filter candidates for labeling. In an empirical
study on network intrusion detection data, we observe that the proposed learning methodology requires much less labeled data than the state-of-the-art, while achieving higher detection accuracies.

1. Introduction
Anomaly detection deals with identifying unlikely and rare events. The classical approach to
anomaly detection is to compute a precise description of normal data. Every newly arriving instance is contrasted with the model of normality and an anomaly score is computed. The score
describes the deviations of the new instance compared to the average data instance and, if the deviation exceeds a predefined threshold, the instance is considered an anomaly or an outlier and
processed adequately (Markou & Singh, 2003a; Chandola, Banerjee, & Kumar, 2009; Markou &
Singh, 2003b).
Identifying data that exhibits irregular and suspicious traits is crucial in many applications such
as medical imaging and network security. In particular, the latter has become a vivid research area
as computer systems are increasingly exposed to security threats, such as computer worms, network
c
2013
AI Access Foundation. All rights reserved.

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Figure 1: Illustration of the two paradigms for semi-supervised learning.

attacks, and malicious code (Andrews & Pregibon, 1978). Network intrusion detection deals with
detecting previously unknown threats and attacks in network traffic. Conventional security techniques for intrusion detection are based on identifying known patterns of misuse, so called signatures (Roesch, 1999; Paxson, 1999) and thus—although being effective against known attacks—fail
to protect from novel threats. This brings anomaly detection into the focus of security research (e.g.,
Eskin, Arnold, Prerau, Portnoy, & Stolfo, 2002; Kruegel, Vigna, & Robertson, 2005; Stolfo, Apap,
Eskin, Heller, Hershkop, Honig, & Svore, 2005; Perdisci, Ariu, Fogla, Giacinto, & Lee, 2009).
Thus, anomaly detection is the most beneficial in learning scenarios where many regular data instances are given, which allows the machine to approximate the underlying distribution well and
leads to a concise model of normality. By contrast, outliers and anomalies are rare and can even
originate from changing distributions (e.g., novel classes of network attacks). Especially in adversarial settings, such as network intrusion detection, differences in training and test distributions are
eminent as novel threats and tactics are being continuously developed. As a consequence, anomaly
detection is generally considered an unsupervised task and prominent learning methods, including
one-class support vector machines (Schölkopf, Platt, Shawe-Taylor, Smola, & Williamson, 2001)
and support vector data descriptions (SVDD, Tax & Duin, 2004), implement this spirit. However,
the underlying assumptions on unsupervised methods are also their major drawback and in many
application areas, unsupervised methods fail to achieve the required detection rates. Especially in
adversarial application areas such as network intrusion detection, even a single undetected outlier
may already suffice to capture the system. Therefore, the goal of this article is to incorporate a
feedback-loop in terms of labeled data to make anomaly detection practical. By doing so, knowledge about historic threats and anomalies can be included in terms of labels and thus guide the
model generation toward better generalizations.
In this article, we cast anomaly detection into the paradigm of semi-supervised learning
(Chapelle, Schölkopf, & Zien, 2006). Usually, semi-supervised methods are deduced from existing
supervised techniques, augmented by an appropriate bias to take the unlabeled data into account.
For instance, a prominent bias assumes that the unlabeled data is structured in clusters so that closeness (with respect to some measure) is proportional to the probability of having the same class label
(Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005; Chapelle, Chi, & Zien, 2006; Sindhwani,
Niyogi, & Belkin, 2005). As a consequence, anomaly detection is often rephrased as a (multi-class)
classification problem (Almgren & Jonsson, 2004; Stokes & Platt, 2008; Pelleg & Moore, 2004;
Mao, Lee, Parikh, Chen, & Huang, 2009). Although assuming a cluster-structure of the data is
236

T OWARD S UPERVISED A NOMALY D ETECTION

80

100

60

AUC[0,0.01] [in %]

AUC[0,0.01] [in %]

80

60

40

40

20

unsupervised

unsupervised

supervised
semi−supervised

20

SSAD
0

1 5 10 15

50

75

supervised
semi−supervised

0

SSAD
1 5 10 15

100

50

75

100

% of labeled data

% of labeled data

Figure 2: Left: The standard supervised classification scenario with identical training and test distributions. Right: The anomaly detection setting with two novel anomaly clusters in the
test distribution.

often well justified in anomaly detection, recall that supervised learning techniques focus on discriminating concept classes while unsupervised techniques rather focus on data characterization.
In this article, we show that differences in training and test distributions as well as the occurrence
of previously unseen outlier classes render anomaly detection methods, derived from a supervised
technique, inappropriate as they are likely to miss out novel and previously unseen classes of anomalies as depicted. By contrast, we argue that successful anomaly detection methods inherently need
to ground on the unsupervised learning paradigm, see Figure 1. In sum, making anomaly detection practical requires the following key characteristics: (i) intrinsically following the unsupervised
learning paradigm to cope with unknown events and (ii) additionally exploiting label information to
obtain state-of-the-art results.
In Figure 2, we show the results of a controlled experiment that visualizes the different nature
of the semi-supervised methods derived from supervised and unsupervised paradigms, respectively.
On the left hand side, the achieved accuracies in the standard supervised classification scenario,
where training and test distributions are identical, is shown. The performance of the unsupervised
anomaly detection method is clearly outperformed by supervised and semi-supervised approaches.
However, we observe from the right hand side of the figure how fragile the latter methods can be
in an anomaly detection scenario. The experimental setup is identical to the former except that
we discard two anomaly clusters in the training set (see Figure 3). Note that this change is not an
arbitrary modification but an inherent characteristic of anomaly detection scenarios, where anomalies stem from novel and previously unseen distributions. Unsurprisingly, the (partially) supervised
methods fail to detect the novel outliers and are clearly outperformed by the unsupervised approach,
which robustly performs around unimpressive detection rates of about 40%. Finally, all methods
are clearly outperformed by our novel semi-supervised anomaly detection (SSAD) which is devised
from the unsupervised learning paradigm and allows for incorporating labeled data.
237

G ÖRNITZ , K LOFT, R IECK , & B REFELD

anomalies

normal
data
novel
anomalies

Figure 3: Left: training data stems from two clusters of normal data (gray) and one small anomaly
cluster (red). Right: two additional anomaly clusters (red) appear in the test data set.

The main contribution of this article is to provide a mathematical sound methodology for semisupervised anomaly detection. Carefully conducted experiments and their discussions show the
importance of distinguishing between the two semi-supervised settings as depicted in Figure 1. To
meet this requirement, we propose a novel semi-supervised anomaly detection technique that is
derived from the unsupervised learning paradigm, but allows for incorporating labeled data in the
training process. Our approach is based on the support vector data description (SVDD) and contains
the original formulation as a special case. Although the final optimization problem is not convex,
we show that an equivalent convex formulation can be obtained under relatively mild assumptions.
To guide the user in the labeling process, we additionally propose an active learning strategy to
improve the actual model and to quickly detect novel anomaly clusters. We empirically evaluate
our method on network intrusion detection tasks. Our contribution proves robust in scenarios where
the performance of baseline approaches deteriorate due to obfuscation techniques. In addition, the
active learning strategy is shown to be useful as a standalone method for threshold adaptation.
The remainder of this article is organized as follows. Section 2 reviews related work. The novel
semi-supervised anomaly detection methods are presented in Section 3 and Section 4 introduces
active learning strategies. Section 5 gives insights into the proposed learning paradigm and we
report on results for real-world network intrusion scenarios in Section 6. Section 7 concludes.

2. Related Work
Semi-supervised learning (Chapelle, Schölkopf et al., 2006) offers a mathematical sound framework
for learning with partially labeled data. For instance, transductive approaches to semi-supervised
learning assume a cluster structure in the data so that close points are likely to share the same label
while points that are far away are likely to be labeled differently. The transductive support vector
machine (TSVM, Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005) optimizes a max-margin
hyperplane in feature space that implements the cluster assumption. In its most basic formulation,
the TSVM is a non-convex integer programming problem on top of an SVM. This is computationally
very expensive so that Chapelle, Chi et al. (2006) propose a more efficient smooth relaxation of the
TSVM. Another related approach is low density separation (LDS) by Chapelle and Zien (2005),
where the cluster structure is modeled by a graph of distances.
238

T OWARD S UPERVISED A NOMALY D ETECTION

A broad overview of anomaly detection can be found in the work of Chandola et al. (2009).
Anomaly detection is being regarded as unsupervised learning task and therefore it is not surprising
that there exist a large number of applications employing unsupervised anomaly detection methods.
For instance, finding anomalies in network traffic (Eskin et al., 2002) or program behaviour (Heller,
Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007)
or annotating (Goh, Chang, & Li, 2005) and classifying images (Lai, Tax, Duin, Zbieta, Ekalska, &
Ik, 2004) and documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006)
Fully-supervised approaches for anomaly detection usually ignore unlabeled data during the
training-phase: for example, Almgren and Jonsson (2004) employ a max-margin classifier that separates the innocuous data from the attacks. Stokes and Platt (2008) present a technique which
combines approaches for effective discrimination (Almgren & Jonsson, 2004) and rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take a multi-view and co-training approach based
on Blum and Mitchell (1998) to learn from labeled and unlabeled data.
Support vector learning has also been extended to many non-standard settings such as one-class
learning (Schölkopf et al., 2001) and support vector data description (Tax & Duin, 2004). The
idea of the latter is to learn a hypersphere that encloses the bulk of the provided data so that all
instances that lie outside of the hypersphere are considered anomalous. By contrast, the one-class
SVM learns a hyperplane in some feature space that divides the data points from the origin with
maximum-margin. For translation-invariant kernel matrices, both approaches are equivalent.
There exist only a few semi-supervised methods that are based on unsupervised techniques.
Blanchard, Lee, and Scott (2010) propose a method which has the appealing option of specifying an upper threshold on the false-positives rate. However, this method needs to include test instances at training time and is not applicable in online and streaming scenarios such as anomaly
detection. The same holds true for an extension of the one-class SVM by Mũnoz Marí, Bovolo,
Gómez-Chova, Bruzzone, and Camp-Valls (2010) that incorporates labeled examples in a graphLaplacian regularization term. Tax (2001) proposes a straight-forward extension of the SVDD to
semi-supervised anomaly detection, where negatively labeled points are required to lie outside of the
hypersphere—otherwise a penalty is incurred. An advantage of this so-called SVDDneg approach
is that no further assumptions on the underlying data-generating probability distribution such as
manifold assumptions are imposed. Unfortunately, the primal SVDDneg problem is not a convex
optimization problem, which makes it very difficult to accurately optimize. Moreover, dual optimization as proposed in the work of Tax (2001) cannot be considered an sound alternative due to
possible duality gaps. However, in Appendix A we show that there is a convex reformulation of
the SVDDneg for translation-invariant kernels, such as RBF-kernels. The new formulation does not
suffer from duality gaps and can be easily solved by primal or dual descent methods. The same
problem occurs in related semi-supervised one-class methods as proposed by Liu and Zheng (2006)
and Wang, Neskovic, and Cooper (2005).
Another broad class of methods deals with learning from positive and unlabeled examples
(LPUE). Intrinsically, one aims at solving a two-class problem but only data from one class (the
positive class) is given together with unlabeled data points. LPUE can thus be applied to the problem setting at hand by identifying the outlier class with positively labeled data. Zhang and Lee
(2005) show that this class of methods can be viewed as a special case of semi-supervised learning
and emphasize that the SVDDneg (Tax, 2001) can be considered an instance of LPUE. Algorithmically, LPUE is often solved in an iterative manner by (i) identifying a reliable set of labeled
examples using a classifier and (ii) re-training the classifier given the new training set (Liu, Dai,
239

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Li, Lee, & Yu, 2003; Zhang & Lee, 2005; Blum & Mitchell, 1998). Though some work addresses
learning from non-i.i.d. data (e.g., Li & Liu, 2005), the underlying assumption usually implies that
training and test sets are drawn from the same distribution.
The present article builds upon a previous paper of the same authors (Görnitz, Kloft, & Brefeld,
2009). It extends the latter by a mathematical sound framework and intuitive philosophical insights.
In addition, we present a more general problem formulation employing arbitrary convex loss functions and the computation of the dual representation thereof, and a new empirical analysis with
comparisons to a larger variety of baseline approaches.

3. Semi-supervised Anomaly Detection
In anomaly detection tasks, we are given n observations x1 , . . . , xn ∈ X . The underlying assumption is that the bulk of the data stems from the same (unknown) distribution and we call this
part of the data normal. Some few observations, however, originate from different distributions
and are considered anomalies. These anomalies could for instance be caused by broken sensors or
network attacks and cannot be sampled by definition. The goal in anomaly detection is to detect
these anomalies by finding a concise description of the normal data, so that deviating observations
become outliers. We thus aim at finding a scoring function f : X → R which defines the model of
normality. Following the principle of empirical risk minimization, the optimization problem takes
the following form,
n
ηX
f ∗ = argmin Ω(f ) +
l(f (xi )),
n
f
i=1

where l : R → R is an appropriate loss function, Ω : Rd → R+ a regularizer on f , and η is a
trade-off parameter.
Our approach is based on the SVDD, which computes a hypersphere with radius R and center
c that encompasses the data. The hypersphere is our model of normality and the anomaly score for
an instance x is computed by its distance to the center c,
f (x) = ||φ(x) − c||2 − R2 .

(1)

Points lying outside of the ball (i.e., f (x) > 0) are considered anomalous, while points within
(f (x) < 0) are treated as normal data. The corresponding optimization problem is known as
support vector data description (SVDD, Tax, 2001) and has the following form
n
X
min R2 + ηu
ξi
R,c,ξ

s.t.

i=1

∀ni=1
∀ni=1

: kφ(xi ) − ck2 ≤ R2 + ξi
: ξi ≥ 0 ,

where the trade-off ηu balances the minimization of the radius and the sum of erroneously placed
points (that are, points lying outside of the normality radius). The parameter ηu also serves as
an estimate of the ratio between outliers and normal data in the n training examples. The resulting
Pproblem is convex and can be solved equivalently in dual space using the representation
c = ni=1 αi φ(xi ). As a consequence, the input data can be expressed equivalently by a kernel
function k(xi , xj ) = φ(xi )T φ(xj ) on X that corresponds to a feature map φ : X → F into a
reproducing kernel Hilbert space F (see, e.g., Müller, Mika, Rätsch, Tsuda, & Schölkopf, 2001).
240

T OWARD S UPERVISED A NOMALY D ETECTION

3.1 Semi-supervised Anomaly Detection
We now propose a novel approach to semi-supervised anomaly detection. The proposed method
generalizes the vanilla SVDD and processes unlabeled and labeled examples. While existing extensions of the SVDD employ dual optimization techniques and inherently suffer from duality gaps
due to their non-convexity, we propose a primal approach to semi-supervised anomaly detection. As
discussed earlier, for translation-invariant kernels, the one-class SVM is contained in our framework
as a special case. In addition to the n unlabeled examples x1 , . . . , xn ∈ X , we are now given m
∗ ) ∈ X × Y where Y denotes the set of class labels. For
labeled observations (x∗1 , y1∗ ), . . . , (x∗m , ym
simplicity, we will focus on Y = {+1, −1} where y ∗ = +1 encodes nominal data and y ∗ = −1
anomalies.
As argued in the introduction, the goal is to derive a method that grounds on the unsupervised
learning paradigm. We therefore stick to the hypersphere model of the SVDD and use the latter
as blueprint for dealing with unlabeled data. The inclusion of labeled examples follows a simple
pattern: If an example x∗ is labeled as nomial (y ∗ = +1), we require that it lies within the hypersphere. By contrast, if an example is an anomaly or member of an outlier class (y ∗ = −1), we want
to have it placed outside of the ball. A straight-forward extension of the SVDD using both labeled
and unlabeled examples is thus given by

min

R,γ,c,ξ

s.t.

2

R − κγ + ηu

n
X

ξi + η l

i=1

∀ni=1

:

∀n+m
j=n+1

n+m
X

2

ξj∗

j=n+1
2

kφ(xi ) − ck ≤ R + ξi

: yj∗ kφ(x∗j ) − ck2 − R2 ≤ −γ + ξj∗

∀ni=1 :

(2)

ξi ≥ 0,

∗
∀n+m
j=n+1 : ξj ≥ 0 ,

where γ is the margin of the labeled examples and κ, ηu , and ηl are trade-off parameters. Unfortunately, the inclusion of negatively labeled data renders the above optimization problem non-convex
and optimization in dual space is prohibitive. As a remedy, following the approach of Chapelle and
Zien (2005), we translate Equation (2) into an unconstrained problem. We thereby resolve the slack
terms from the above OP as follows
ξi = ` R2 − ||φ(xi ) − c||2
ξj∗ = ` yj∗



(3)



R2 − ||φ(x∗j ) − c||2 − γ .

For example, if we put `(t) = max{−t, 0} (i.e., the common hinge loss), we recover (2). Furthermore, by an application of the representer theorem, we obtain the support-vector expansion

c=

n
X
i=1

n+m
X

αi φ(xi ) +

j=n+1

241

αj yj∗ φ(x∗j )

(4)

G ÖRNITZ , K LOFT, R IECK , & B REFELD

1
hinge loss
Huber loss

linear

loss

0.75

0.5

quadratic

0.25

linear

0
0

0.5

1
yf(x)

1.5

2

Figure 4: Non-differentiable hinge loss (dashed) and differentiable Huber loss `∆=1,=0.5 (solid).
(see Appendix B for a detailed derivation). Combining (3) and (4), we can re-formulate optimization
problem (2) solely in terms of kernels and without any constraints as follows:
min

R,γ,α

2

R − κγ + ηu
+ ηl

n
X

` R2 − k(xi , xi ) + (2ei − α)0 Kα

i=1
n+m
X





` yj∗ R2 − k(x∗j , x∗j ) + (2e∗j − α)0 Kα − γ .

(5)

j=n+1

Hereby K = (kij )1≤i,j≤n denotes the kernel matrix given by kij = k(xi , xj ) = hφ(xi ), φ(xj )i
and e1 , . . . , en+m is the standard base of Rn+m . By rephrasing the problem as an unconstrained
optimization problem, its intrinsic complexity has not changed. Often, unconstrained optimization
is easier to implement than constrained optimization. While non-smooth optimization is possible
via e.g. non-convex bundle methods as described in the work of Do (2010), smooth optimization
methods such as conjugate gradient or Newton’s method are easier to apply. To obtain a smooth
optimization technique, we choose Huber’s robust loss (Huber, 1972). The Huber loss has two parameters controlling its quadratic approximation in terms of the center ∆ and its witdh , see Figure
4. The optimization function becomes differentiable and off-the-shelf gradient-based optimization
tools can be applied. The complete derivation of the gradients of optimization problem (5) using
Huber’s robust loss is shown in Appendix C.
3.2 Convex Semi-supervised Anomaly Detection
The optimization problem of the previous section is easy to implement but, unfortunately, nonconvex. Therefore, optimizers may find a good local optimum, but several restarts are necessary to
verify the quality of the solutions and in cases optimization might fail completely. We now show
that, under rather mild assumptions, namely that the data is processed to have unit norm in feature
space (as fulfilled by, e.g., RBF kernels), the above optimization problem can be converted into an
equivalent convex one. Our derivation is very general as it postulates nothing but the convexity of the
loss function. Our approach is based on a combination of Lagrangian duality and the notion of the
Fenchel-Legendre conjugate function. Fenchel duality for machine learning has been pioneered by
242

T OWARD S UPERVISED A NOMALY D ETECTION

Rifkin and Lippert (2007) under the assumption of full-rank kernels. Our approach is more general
and allows us to use any kernel K. As a byproduct of our derivation, we show that the classical
one-class SVM is a special case of a general class of density level set estimators that minimize a
convex risk functional and give a general dual criterion for this class. To this aim, we introduce the
Legendre-Fenchel conjugate for a given loss l(t) as
lc (z) = sup (zt − l(t))
t

and use a slightly different formulation of the SSAD problem, that is, we eliminate the hinge loss
slack variables ξ ∗ ,ξ and reformulate the problem with explicit loss functions:
n
n+m
X
X
1
2
||w|| − ρ − κγ + ηu
l(ti ) + ηl
l(tj )
2

min

ρ,γ,w,t

i=1

∀ni=1 :
∀n+m
j=n+1

s.t.

j=n+1

T

ti = (w φ(xi )) − ρ
: tj =

(yj∗ wT φ(x∗j ))

−

(P)
yj∗ ρ

−γ

and γ ≥ 0.
Note that, in the above problem, auxiliary variables ti are introduced to deal with non-differentiable
loss functions. Again, because of the convex nature of the stated optimization problem, we can
solve it in the dual space. To this aim, we use the Lagrange Theorem to incorporate the constraints
into the objective:
n
n+m
X
X
1
2
L = ||w|| − ρ − κγ + ηu
l(ti ) + ηl
l(tj )
2
i=1

−

−

n
X

j=n+1

αi ((wT φ(xi )) − ρ − ti )

i=1
n+m
X

(6)

αj ((yj∗ wT φ(x∗j )) − yj∗ ρ − γ − tj ) − δγ

j=n+1

An optimal solution can be found by solving the Lagrangian saddle point problem
max min EQ6.
α,δ ρ,γ,w,t

If we used a standard Lagrangian ansatz, we would now compute the derivate of the Lagrangian with
respect to the primal variables. However, a general loss function l(·) is not necessarily differentiable.
As a remedy, we only compute the derivatives wrt w, ρ and γ. Setting those to zero, yields the
optimality conditions
∀i :

0 ≤ αi ≤ η u

∀j :

0 ≤ αj ≤ η l
n
n+m
X
X
w =
αi φ(xi ) +
αj yj∗ φ(x∗j ).
i=1

j=n+1

243

(7)

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Inserting the above optimality conditions into the Lagrangian, the saddle point problem translates
into




n
n+m
X
X
αj ∗
1 T
αi
∗
max − α Kα + ηu
min l(ti ) + ti + ηl
min
l(tj ) + tj .
α
t
t∗
2
ηu
ηl
i=1

j=n+1

Converting the min into a max statement results in




n
n+m
X
X
αj ∗
1 T
αi
∗
max − α Kα − ηu
max − ti − l(ti ) − ηl
max
− tj − l(tj ) .
α
t
t∗
2
ηu
ηl
i=1

j=n+1

Now, making use of the Legendre-Fenchel conjugate lc (·) described above, we arrive at the following dual optimization problem
max
α

s.t.

n+m
n
X
X
αj
αi
1
lc (− )
lc (− ) − ηl
− αT Kα − ηu
2
ηu
ηl
j=n+1

i=1

1=

n
X
i=1

αi +

n+m
X

αj yj∗

j=n+1

(D)

and

κ≤

n+m
X

αj .

j=n+1

In contrast to existing semi-supervised approaches to anomaly detection (Tax, 2001; Liu & Zheng,
2006; Wang et al., 2005), strong duality holds as shown by the following proposition.
Proposition 3.1 For the optimization problems (P) and (D) strong duality holds.
Proof This follows from the convexity of (P) and Slater’s condition, which is trivially fulfilled for
all γ by adjusting tj : ∀γ > 0 ∃tj ∈ R : 0 = (yj∗ wT φ(x∗j )) − yj∗ ρ − γ − tj .

We observe that the above optimization problems (P) and (D) contain the non-convex variant as a
special case for translation-invariant kernels. Difficulties may arise in the presence of many equality
and inequality constraints, which can increase computational requirements. However, this is not
inherent; the left hand-side constraint can be removed by discarding the variable ρ in the initial
primal problem—this leaves the regularization path of the optimization problem invariant—and
the right hand side inequality can be equivalently incorporated into the objective function by a
Lagrangian argument (e.g., Proposition 12 in Kloft, Brefeld, Sonnenburg, & Zien, 2011). Note, that
the convex model has only an intuitive interpretation for normalized kernels. In order to deal with a
wider class of kernels, we need to resort to the more general non-convex formulation as presented
in Section 3.1.

4. Active Learning for Semi-supervised Anomaly Detection
In the previous section, we presented two optimization problems that incorporate labeled data into
an unsupervised anomaly detection technique. However, we have not yet addressed the question
of acquiring labeled examples. Many topical real-world applications involve millions of training
instances (Sonnenburg, 2008) so that domain experts can only label a small fraction of the unlabeled
data. Active learning deals with finding the instances that, once labeled and included in the training
244

T OWARD S UPERVISED A NOMALY D ETECTION

set, lead to the largest improvement of a re-trained model. In the following, we present an active
learning strategy that is well-suited for anomaly detection. The core idea is to query low-confidence
decisions to guide the user in the labeling process.
Our approach works as follows. First, we initialize our method by training it on the unlabeled
examples. The training set is then augmented by particular examples that have been selected by
the active learning rule. The candidates are labeled by a domain expert and added to the training
set. The model is retrained on the refined training set, which now consists of unlabeled and labeled
examples. Subsequently labeling- and retraining-steps are repeated until the required performance
is reached.
The active learning rule itself consists of two parts. We begin with a commonly used active
learning strategy which simply queries borderline points. The idea of the method is to choose the
point that is closest to the decision hypersphere (Almgren & Jonsson, 2004; Warmuth, Liao, Rätsch,
Mathieson, Putta, & Lemmen, 2003) to be presented to the expert:


kf (x)k


x0 = argmin
= argmin R2 − kφ(x) − ck2  .
(8)
x∈{x1 ,...,xn } maxk kf (xk )k
x∈{x1 ,...,xn }
For supervised support vector machines, this strategy is known as the margin strategy (Tong &
Koller, 2000). Figure 5 (a) shows an illustratation for semi-supervised anomaly detection.
When dealing with non-stationary outlier categories, it is beneficial to identify novel anomaly
classes as soon as possible. We translate this requirement into an active learning strategy as follows.
Let A = (aij )i,j=1,...,n+m be an adjacency matrix of the training instances, obtained by, for example,
a k-nearest-neighbor approach, where aij = 1 if xi is among the k-nearest neighbors of xj and 0
otherwise. We introduce an extended labeling ȳ1 . . . , ȳn+m for all examples by defining ȳi = 0 for
unlabeled instances and retaining the labels for labeled instances, i.e., ȳj = yj . Using these pseudo
labels, Equation (9) returns the unlabeled instance according to
x0

=

n+m
1 X
(ȳj + 1) aij .
xi ∈{x1 ,...,xn } 2k

argmin

(9)

j=1

The above strategy explores unknown clusters in feature space and thus labels orthogonal or complementary instances as illustrated in Figure 5 (b).
Nevertheless, using Equation (9) alone may result in querying points lying close to the center of
the actual hypersphere. These points will hardly contribute to an improvement of the hypersphere.
On the other hand, using the margin strategy alone does not allow for querying novel regions that
lie far away from the margin. In other words, only a combination of both strategies (8) and (9)
guarantees that points of interest are queried. Our final active learning strategy is therefore given by
x0 =

argmin
xi ∈{x1 ,...,xn }

= δ

n+m
kf (x)k 1 − δ X
+
(ȳj + 1) aij
c
2k

(10)

j=1

for δ ∈ [0, 1]. The combined strategy queries instances that are close to the boundary of the hypersphere and lie in potentially anomalous clusters with respect to the k-nearest neighbor graph, see
Figure 5 (c) for an illustration. Depending on the actual value of δ, the strategy jumps from cluster
to cluster and thus helps to identify interesting regions in feature space. For the special case of no
labeled points, our combined strategy reduces to the margin strategy.
245

G ÖRNITZ , K LOFT, R IECK , & B REFELD

(a) margin strategy

(b) cluster strategy

(c) combined strategy

Figure 5: Comparison of active learning strategies (queried points are marked in blue): (a) the
margin strategy queries data points that are closest to the decision boundary, (b) the cluster
strategy queries points in rarely labeled regions, and (c) the combined strategy queries
data points that are likely anomalies in clusters near the decision boundary.

Usually, an active learning step is followed by an optimization step of the semi-supervised
SVDD, to update the model with respect to recently labeled data. This procedure is of course timeconsuming and can be altered for practical settings, for instance by querying a couple of points
before performing a model update. Irrespectively of the actual implementation, alternating between
active learning and updating the model can be repeated until a desired predictive performance is
obtained.

5. Illustration of Proposed Learning Paradigm
In this section, we illustrate the weaknesses of existing learning paradigms in semi-supervised
anomaly detection settings by means of a controlled experiment on synthetic data. The results,
which have already been briefly sketched in the introduction (cf., Figure 2), are discussed in detail
here.
Table 1: Competitors for the toy data experiment.
two-class
supervised transductive
SVM
LDS

unsupervised
SVDD

one-class
LPUE
semi-supervised
neg
SVDD
SSAD

To this end, we generate the nominal training and validation data from two isotropic Gaussian
distributions in R2 and one anomaly cluster (shown in Figure 3 (left)). However, at testing time,
two novel anomaly clusters appear in test data (shown in Figure 3 (right)). This reflects the characteristic that anomalies can stem from novel, previously unseen distributions. We compare our
newly-developed method SSAD to the following baseline approaches: the unsupervised support
246

T OWARD S UPERVISED A NOMALY D ETECTION

80
70

AUC[0,0.01] [in %]

60
50
40
30
SVDD
SVM
10
0

(unsupervised)

SVDDneg (LPUE)

20

1 5 10 15

(supervised)

LDS

(transductive)

SSAD

(proposed method)

50

75

100

% of labeled data

Figure 6: Performance of various unsupervised, supervised and semi-supervised methods in the
anomaly detection setting.

vector domain description (SVDD, Tax & Duin, 2004), the corrected semi-supervised SVDDneg
(Tax, 2001) described in Appendix A, a supervised support vector machine (SVM, Boser, Guyon,
& Vapnik, 1992; Cortes & Vapnik, 1995), and the semi-supervised low-density separation (LDS,
Chapelle & Zien, 2005), see Table 1. As common in anomaly detection setups, we measure the area
under the ROC curve over the interval [0, 0.01] and report on AUCs averaged over 25 repetitions
with distinct training, validation, and test sets. In every repetition, parameters ηu , ηl are adjusted
on the respective validation set within the interval [10−2 , 102 ]. In all experiments, we used κ = 1.
Error bars correspond to standard errors.
The results are shown in Figure 6, where the horizontal axis shows different ratios of labeled
and randomly drawn unlabeled examples. Methods derived from the supervised learning paradigm
such as SVM and LDS cannot cope with novel outlier clusters and perform poorly for all ratios of
labeled and unlabeled examples; their performance remains below that of the unsupervised SVDD,
which does not utilize labeled data at all and is thus unaffected by incorporating labels in the training process. By contrast, the two semi-supervised methods derived from the unsupervised learning
paradigm clearly outperform all other baselines. However, the SVDDneg only benefits from anomalous labeled data and since these are sparse, needs a big fraction of labeled data to increase its
performance. Our semi-supervised method SSAD exploits every single labeled example and needs
only 15% of the labels to saturate around its optimum.
Figure 7 visualizes typical contour lines of hypotheses computed by SSAD for three different
scenarios. The figure shows a fully-supervised scenario where all instances are correctly labeled
(left), a semi-supervised solution where 25% of the data is labeled and 75% remains unlabeled
(center), and a completely unsupervised one using unlabeled data only (right). Unsurprisingly,
the fully supervised solution discriminates perfectly between normal data and outliers while the
unsupervised solution recognizes the latter as normal data by mistake. The intermediate semisupervised solution uses only little label information to also achieve a perfect separation of the
involved classes.
247

G ÖRNITZ , K LOFT, R IECK , & B REFELD

(a)

(b)

(c)

Figure 7: Different solutions for fully-supervised (left), semi-supervised (center), and unsupervised
anomaly detection using RBF kernels. Colors indicate unlabeled data (green), labeled
outliers (red), and labeled normal instances (violet).

0.3
SVDD

0.25

0.2

time

(unsupervised)

SVDDneg (LPUE)
LDS
SVM

(transductive)
(supervised)

SSAD

(proposed method)

0.15

0.1

0.05

0
50

100

150

200

examples

Figure 8: Execution times.

Figure 8 compares execution times of the different methods and shows the number of training
examples versus training time. For simplicity, we discarded 50% of the labels in the training data
at random. The results show that methods, such as SSAD, SVDDneg , and SVDD, that are derived
from the unsupervised learning principle, perform similarly. The SVM performs best but uses only
the labeled part of the training data and ignores the unlabeled examples. Low density separation
(LDS) performs worst due to its transductive nature.
Based on our observations, we draw the following conclusions. Anomaly detection scenarios
render methods derived from the supervised learning paradigm inappropriate. Even unsupervised
methods ignoring label information may perform better than their supervised peers. Intuitively,
discarding label information is sub-optimal. Our experiment shows that semi-supervised methods
from the unsupervised learning paradigm effectively incorporate label information and outperform
all other competitors. We will confirm these findings in Section 6.
248

T OWARD S UPERVISED A NOMALY D ETECTION

6. Real-World Network Intrusion Detection
The goal of network intrusion detection is to identify attacks in incoming network traffic. Classical
signature-based have proven insufficient for the identification of novel attacks, because signatures
need to be manually crafted in advance. Therefore machine learning approaches have been gaining
more and more attention by the intrusion detection research community.
The detection of unknown and novel attacks requires an adequate representation of network
contents. In the remainder, we apply a technique for embedding network payloads in vector spaces
derived from concepts of information retrieval (Salton, Wong, & Yang, 1975) and that has recently
been applied in the application domain of network intrusion detection (Rieck & Laskov, 2007). A
network payload x (the data contained in a network packet or connection) is mapped to a vector
space using a set of strings S and an embedding function φ. For each string s ∈ S the function
φs (x) returns 1 if s is contained in the payload x and 0 otherwise. By applying φs (x) for all
elements of S we obtain the following map
φ : X → R|S| ,

φ : x 7→ (φs (x))s∈S ,

(11)

where X is the domain of all network payloads. Defining a set S of relevant strings a priori is difficult as typical patterns of novel attacks are not available prior to their disclosure. As an alternative,
we define the set S implicitly and associate S with all possible strings of length n. This resulting
set of strings is often referred to as n-grams.
As a consequence of using n-grams, the network payloads are mapped to a vector space with
n
256 dimensions, which apparently contradicts efficient network intrusion detection. Fortunately,
a payload of length T comprises at most (T − n) different n-grams and, consequently, the map φ
is sparse, that is, the vast majority of dimensions is zero. This sparsity can be exploited to derive
linear-time algorithms for extraction and comparison of embedded vectors. Instead of operating
with full vectors, only non-zero dimensions are considered, where the extracted strings associated
with each dimension can be maintained in efficient data structures (Rieck & Laskov, 2008).
For our experiments, we consider HTTP traffic recorded within 10 days at Fraunhofer Institute
FIRST. The data set comprises 145,069 unmodified connections of average length of 489 bytes.
The incoming byte stream of each connection is mapped to a vector space using 3-grams as detailed
above. We refer to the FIRST data as the normal pool. The malicious pool contains 27 real attack
classes generated using the Metasploit framework (Maynor, Mookhey, Cervini, & Beaver, 2007). It
covers 15 buffer overflows, 8 code injections and 4 other attacks including HTTP tunnels and crosssite scripting. Every attack is recorded in 2–6 different variants using a virtual network environment
and a decoy HTTP server, where the attack payload is adapted to match characteristics of the normal
data pool. A detailed description of this data set is provided by Rieck (2009).
To study the robustness of our approach in a more realistic scenario, we also consider techniques
to obfuscate malicious content by adapting attack payloads to mimic benign traffic in feature space
(Fogla, Sharif, Perdisci, Kolesnikov, & Lee, 2006; Perdisci et al., 2009). As a consequence, the
extracted features deviate less from normality and the classifier is likely to be fooled by the attack.
For our purposes, it already suffices to study a simple cloaking technique by adding common HTTP
headers to the payload while the malicious body of the attack remains unaltered. We apply this
technique to the malicious pool and refer to the obfuscated set of attacks as cloaked pool.
249

G ÖRNITZ , K LOFT, R IECK , & B REFELD

6.1 Detection Performance
In this section, we evaluate the statistical performance of SSAD in intrusion detection, in comparison to the baseline methods SVDD and SVDDneg . In addition, our combined active learning
strategy is compared to random sampling.
We focus on two scenarios: normal vs. malicious and normal vs. cloaked data. For both settings,
the respective byte streams are translated into a bag-of-3-grams representation. For each experiment,
we randomly draw 966 training examples from the normal pool and 34 attacks, depending on the
scenario, either from the malicious or the cloaked pool. Holdout and test sets are also drawn at
random and consist of 795 normal connections and 27 attacks, each. We make sure that attacks of
the same attack class occur either in the training, or in the test set but not in both. We report on
10 repetitions with distinct training, holdout, and test sets and measure the performance by the area
under the ROC curve in the false-positive interval [0, 0.01] (AUC0.01 )
Figure 9(a) shows the results for normal vs. malicious data pools, where the x-axis depicts the
percentage of randomly drawn labeled instances. Irrespectively of the amount of labeled data, the
malicious traffic is detected by all methods equally well as the intrinsic nature of the attacks is well
captured by the bag-of-3-grams representation (cf., Wang, Parekh, & Stolfo, 2006; Rieck & Laskov,
2006). There is no significant difference between the classifiers.
Figure 9(b) shows the results for normal vs. cloaked data. First of all, the performance of the
unsupervised SVDD drops to just 70%. We obtain a similar result for the SVDDneg ; incorporating
cloaked attack information into the training process of the SVDD leads to an increase of about 5%
which is far from any practical value. Notice that the SVDDneg cannot make use of labeled data of
the normal class. Thus, its moderate ascent in terms of the number of labeled examples is credited
to the class ratio of 966/34 for the random labeling strategy. The bulk of additional information
cannot be exploited and has to be left out. By contrast, our semi-supervised method SSAD includes
all labeled data into the training process and clearly outperforms the two baselines. For only 5%
labeled data, SSAD easily beats the best baseline and for randomly labeling 15% of the available
data it separates normal and cloaked malicious traffic almost perfectly.
Nevertheless, labeling 15% of the data is not realistic for practical applications. We thus explore
the benefit of active learning for inquiring label information of borderline and low-confidence points.
Figure 9(c) shows the results for normal vs. cloaked data, where the labeled data for SVDDneg
and SSAD is chosen according to the active learning strategy in Equation (10). The unsupervised
SVDD does not make use of labeled information and is unaffected by this setup, remaining at an
AUC0.01 of 70%. Compared to the results when using a random labeling strategy (Figure 9(b)), the
performance of the SVDDneg increases significantly. The ascent of the SVDDneg is now steeper
and its performance yields 85% for just 15% labeled data. However, SSAD also improves for active
learning and dominates the baselines. Using active learning, we need to label only 3% of the data
for attaining an almost perfect separation, compared to 25% for a random labeling strategy. We
conclude that our active learning strategy effectively improves the performance and reduces the
manual labeling effort significantly.
In Figure 10 the impact of our active learning strategy given by Equation (10) is shown. We
compare the number of outliers detected by the combined strategy with the margin-based strategy
in Equation (8) (see also, Almgren & Jonsson, 2004) and by randomly drawing instances from the
unlabeled pool. As a sanity check, we also included the theoretical outcome for random sampling.
250

T OWARD S UPERVISED A NOMALY D ETECTION

1
1

AUC[0,0.01] [in %]

0.99
0.985
0.98
0.975
0.97
0.965

SVDD
SVDDneg
SSAD (proposed method)

0.8

0.7

SVDDneg
SSAD (proposed method)

0.955
0.95

0.9

SVDD

0.96

0

5

10

0

15

1

3

6

10

15

% of labeled data

% of labeled data

(a) Detection accuracies of regular attacks.

(b) Detection accuracies of cloaked attacks.

1

AUC[0,0.01] [in %]

AUC[0,0.01] [in %]

0.995

0.9

0.8

0.7

0

1

3

6

10

15

% of labeled data

(c) Detection accuracies of cloaked attacks using proposed active learning strategy for SSAD.

Figure 9: Results of the network intrusion detection experiment. While detection accuracies of
regular attacks are insignificantly different (see (a)), SSAD achieves up to 30% higher
accuracies than baseline approaches for cloaked data (see (b)). The proposed activity
learning strategy further increases the accuracy when labeled data is rare (see (c)).

The results show that the combined strategy effectively detects malicious traffic much faster than
the margin-based strategy.
6.2 Threshold Adaptation
The previous experiments have demonstrated the advantages of active learning for network intrusion
detection. So far, all results have been obtained using our method SSAD; however, the active
learning techniques devised in Section 4 are also applicable for calibrating other learning-based
methods. We herein focus on the vanilla SVDD with parameter value ν = 1, which corresponds
to classical centroid-based anomaly detection (e.g., Shawe-Taylor & Cristianini, 2004), such that
251

G ÖRNITZ , K LOFT, R IECK , & B REFELD

35
30

#outliers

25
20

optimal
active learning
margin
random (empirical)
random (theoretical)

15
10
5
0
0

1

2

3

4

5

7.5
10
labeled data in %

15

Figure 10: Number of novel attacks detected by the combined active learning strategy (blue line),
random sampling (red solid and dotted line), margin strategy (purple line) and upper
bound (light blue dotted line) for a single run.

results directly transfer to anomaly detectors as Anagram (Wang et al., 2006), McPad (Perdisci
et al., 2009) and TokDoc (Krueger, Gehl, Rieck, & Laskov, 2010).
We again draw a set of 3,750 network connections from the pool of normal data and split the
resulting set into a training set of 2,500 connections and a test partition of 1,250 events. Both sets
are mixed with cloaked attack instances. The SVDD is then trained on the normal training set
delivering a threshold R. For application of the learned hypersphere to the test set, we evaluate
different strategies for determining a radius R̂ using random sampling and active learning. In both
cases, the selected connections are labeled and a threshold is obtained by computing the mean of all
labeled instances:






R
maxi d(xi )
R̂ =
minj d(x
)

P
P j


 i d(xi )+ j d(xj )
#pos+#neg

: #pos = 0
: #pos > 0
: #pos = 0

∧
∧
∧

#neg = 0
#neg = 0
#neg > 0

:

∧

#neg > 0

#pos > 0

(12)
Where xi are the positive labeled examples, xj the negative examples and d(x) = ||φ(x) − c||
denotes the distance of the current sample x from the hyperspheres origin. Figure 11 shows the
ROC curve of the SVDD and the computed thresholds for various levels of labeled data. Results
have been averaged over 10 random draws of working sets. One can see that even for small amounts
of labeled data the active learning strategy finds a reasonable radius while the random strategy and
the vanilla SVDD completely fail with a false-positive rate of 0.5 and 1, respectively. This result
demonstrates that active learning strategies enable calibrating anomaly detectors with a significantly
252

T OWARD S UPERVISED A NOMALY D ETECTION

1

TPR

0.8

0.6

0.4
ROC curve
SVDD threshold
active learning 5%
random sampling 5%

0.2

0
0

0.2

0.4

0.6

0.8

1

FPR

Figure 11: Results of the threshold adaption experiment: ROC curve of the SVDD (grey) and
thresholds as determined by the SVDD (green), our proposed combined strategy (blue),
and random sampling (red) are shown (labeling 5% of the data).

reduced effort in comparison to random sampling and hence provide a valuable instrument when
deploying learning methods in practice.

7. Conclusion
In this article, we developed a framework for semi-supervised anomaly detection, which allows
for the inclusion of prior and expert knowledge. We discussed the conceptual difference of semisupervised models which are derived from unsupervised or supervised techniques and proposed a
generalization of the support vector data description to incorporate labeled data. The optimization
problem of semi-supervised anomaly detection (SSAD) is an unconstrained, continuous problem,
that allows for an efficient optimization by gradient-based methods and has a convex equivalent
under mild assumptions on the kernel function.
We approached semi-supervised anomaly detection from an unsupervised learning paradigm.
We proposed a novel active learning strategy that is specially tailored to anomaly detection. Our
strategy guides the user to in the labeling process by querying instances that are not only close to
the boundary of the hypersphere, but are also likely to contain instances of novel outlier categories.
Empirically, we applied semi-supervised anomaly detection to the application domain of network intrusion detection. We showed that rephrasing the unsupervised problem as a semi-supervised
task is beneficial in practice: SSAD proves robust in scenarios where the performance of baseline
approaches deteriorates due to obfuscation techniques. Moreover, we demonstrated the effectiveness of our active learning strategy on a couple of data sets and observed SSAD to significantly
improve the prediction accuracy by effectively exploiting the limited amount of labeled available.
We observed that only a handful labeled instances are necessary to boost the performance. This
characteristic is especially appealing in tasks where labeling data is costly such as network security
where the traffic has to be inspected for malicious patterns by an expert expert.
253

G ÖRNITZ , K LOFT, R IECK , & B REFELD

There are many possibilities to exploit and extend our learning approach as well as our active
learning strategy. For example, replacing the `2 -norm regularization by a sparsity-inducing `1 -norm
to incorporate automatic feature selection reduces the dimensionality of the solution. Learning
sparse feature representations are of great interest for other computer security applications such
as signature generation. A possible optimization strategy could be the linear programming (LP)
approach by Campbell and Bennett (2001) for data domain description. However, other choices
of regularizers are certainly possible including structured regularizers to incorporate hierarchies in
the learning process or non-isotropic norms to encode additional domain knowledge. Incorporating
multiple labels and rephrasing semi-supervised anomaly detection as a multi-task problem might
also improve accuracy in complex application domains.

Acknowledgments
The authors are very grateful to Klaus-Robert Müller for comments that helped improving the
manuscript. This work was supported in part by the German Bundesministerium für Bildung und
Forschung (BMBF) under the project PROSEC (FKZ 01BY1145), by the FP7-ICT Programme of
the European Community, under the PASCAL2 Network of Excellence, and by the German National Science Foundation (DFG) under GA 1615/1-1, MU 987/6-1, MU 987/11-1 and RA 1894/11. Furthermore, Marius Kloft acknowledges a PhD scholarship by the German Academic Exchange
Service (DAAD) and a postdoctoral fellowship by the German Research Foundation (DFG) as well
as funding by the Ministry of Education, Science, and Technology, through the National Research
Foundation of Korea under Grant R31-10008. A part of the work was done while Marius Kloft was
with Computer Science Division and Department of Statistics, University of California, Berkeley,
CA 94720-1758, USA.

Appendix A. Analysis of SVDDneg
In this appendix, we point out a limitation of previously published methods such as SVDDneg (Tax,
2001) as well as methods proposed by Hoi, Chan, Huang, Lyu, and King (2003), Liu and Zheng
(2006), Wang et al. (2005), and Yuan and Casasent (2004). These methods suffer from potential
duality gaps as they are optimized in dual space but—depending on the training data—run the risk
of originating from a non-convex optimization problem. For instance, this is the case if only a
single negatively labeled example is included in the training set. This issue is not addressed in the
aforementioned papers.
We exemplarily illustrate the problem for the SVDDneg . The SVDDneg incorporates labeled examples of the outlier class into the otherwise unsupervised learning process. As before, the majority
of the (unlabeled) data points shall lie inside the sphere while the labeled outliers are constrained
to lie outside of the normality ball. This results in a two-class problem where the positive class
consists of unlabeled data and the negative class is formed by the labeled outliers. After introducing
class labels y ∈ {+1, −1} (where unlabeled data points receive the class label yi = +1), the primal
optimization problem is given by
min

R,c,ξ

R2 + η

n
X

ξi

i=1

s.t. ∀ni=1 : yi kφ(xi ) − ck2 ≤ yi R2 + ξi
254

and ξi ≥ 0,

(13)

T OWARD S UPERVISED A NOMALY D ETECTION

4

5

x 10

objective

4

3

2

1

0
0

Primal
Dual

20

40

60

80

100

negatives

Figure 12: Exemplary duality gap for SVDDneg using a linear kernel and η = 100. The horizontal axis
shows the percentage of negative (anomalous) points in the training set and the vertical axis
shows the primal and dual objective values.

and the corresponding optimization problem in dual space is given by

max
α

s.t.

n
X
i=1
n
X

αi yi k(xi , xi ) −

n
X

αi αj yi yj k(xi , xj )

(14)

i,j=1

αi yi = 1 and 0 ≤ αi ≤ η

∀i = 1, . . . , n.

i=1

The existence of the duality gap is shown as follows: The second derivative of the primal constraints
g(xi ) = yi kφ(xi ) − ck2 − yi R2 − ξi ≤ 0 given by ∂ 2 g/∂c2 = 2yi is negative for outliers as their
label equals yi = −1. This turns the whole optimization problem non-convex. As a consequence,
the optimal solutions of the primal and dual problems may differ. Figure 12 shows an exemplary
plot of the duality gap for artificially generated data where one nominal Gaussian is surrounded by a
smaller ’anomalous’ Gaussian. During the labeling process more and more data points receive their
corresponding label and the more negative examples are present in the learning problem (horizontal
axis) the larger is the duality gap and the larger is the difference of the two objective values (vertical
axis). Note that the duality gap is not necessarily a monotonic function although the behavior is
likely the case. Furthermore, the maximization of the dual problem yields a lower bound on the
primal objective (blue line), whereas the latter is always greater or equal than the corresponding
dual (red line).
Nevertheless, the following Theorem shows for the class of translation-invariant kernel functions, there exists an equivalent convex re-formulation in form of the one-class SVM (Schölkopf
et al., 2001).
Theorem A.1 The solution α∗ found by optimizing the dual of the non-convex SVDDneg as stated
in Equations (14) is identical to the dual of the corresponding convex one-class SVM problem as
stated in Equation (15) if the kernel is translation-invariant, i.e., k(xi , xi ) = s ∀i, s ∈ R+ .
255

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Proof The dual of the one-class SVM is given by
max
α

−

n
1 X
αi αj yi yj k(xi , xj ),
2

n
X

s.t.

i,j=1

αi yi = 1 and

0 ≤ αi ≤ η

∀i.

(15)

i=1

The respective constraints are already equivalent. The dual SVDDneg objective with translationinvariant kernel reduces to
n
n
X
X
α∗ = argmax
αi yi k(xi , xi ) −
αi αj yi yj k(xi , xj )
α

i=1

= argmax

ns

α

i,j=1

n
X

αi yi −

i=1

ns −

= argmax
α

n
X

αi αj yi yj k(xi , xj )

i,j=1

n
X

αi αj yi yj k(xi , xj ),

(16)

i,j=1

where we substituted the equality constraint of Eq. (15) in the last step. Finally, Equation (16) is
precisely the one-class SVM dual objective, scaled by 12 and shifted by a constant ns. However, the
optimal solution α∗ is not affected by this transformation which completes the proof.


Appendix B. A Representer Theorem for SSAD
In this section, we show the applicability of the representer theorem for semi-supervised anomaly
detection.
Theorem B.1 (Representer Theorem in Schölkopf & Smola, 2002) Let H be a reproducing kernel Hilbert space with a kernel k : X × X → R, a symmetric positive semi-definite function on the
compact domain. For any function L : Rn → R, any nondecreasing function Ω : R → R. If

J ∗ := min J(f )f ∈H := min f ∈ H{Ω ||f ||2H + L (f (x1 ), . . . , f (xn ))}
is well-defined, then there exist α1 , . . . , αn ∈ R, such that
f (·) =

n
X

αi k(xi , ·)

(17)

i=1

J ∗.

achieves J(f ) =
Furthermore, if Ω is increasing, then each minimizer of J(f ) can be expressed
in the form of Eq. (17).
Proposition B.2 The representer theorem can be applied to the non-expanded version of Equation
(5).
Proof Recall the primal SSAD objective function which is given by
2

J(R, γ, c) =R − κγ + ηu

n
X

` R2 − ||φ(xi ) − c||2



i=1

+ ηl

n+m
X



` yj∗ R2 − ||φ(x∗j ) − c||2 − γ .

j=n+1

256

T OWARD S UPERVISED A NOMALY D ETECTION

Substituting T := R2 − ||c||2 leads to the new objective function
2

J(T, γ, c) =||c|| + T − κγ + ηu

n
X

` T − ||φ(xi )||2 + 2φ(xi )0 c



i=1

+ ηl

n+m
X



` yj∗ T − ||φ(x∗j )||2 + 2φ(x∗j )0 c − γ .

j=n+1

Expanding the center c in terms of labeled and unlabeled input examples is now covered by the
representer theorem. After the optimization, T can be easily re-substituted to obtain the primal
variables R, γ, and c. This completes the proof.


Appendix C. Computing the Gradients for Eq. (5)
In this section we compute the gradients of the SSAD formulation given by Eq. (5). This is a
neccessary step to implement the gradient-based solver for SSAD. To this end, we consider the
unconstrained optimization problem given by
min

R,γ,α

2

R − κγ + ηu

n
X

`∆, R2 − k(xi , xi ) + (2ei − α)0 Kα

i=1
n+m
X

+ ηl





`∆, yj∗ R2 − k(x∗j , x∗j ) + (2e∗j − α)0 Kα − γ ,

j=n+1

where `∆, is the Huber loss given by

 ∆−t
(∆+−t)2
`∆, (t) =
4

0

: t≤∆−
: ∆−≤t≤∆+
: otherwise.

For notational convenience, we focus on the Huber loss for `∆=0, (t) and move margin dependent
terms into the argument t and compute the gradients in several steps, as follows: first, we build the
gradient with respect to the primal variables R and c, which yields
∂ξi
= 2R`0 (R2 − ||φ(xi ) − c||2 )
∂R
∂ξi
= 2(φ(xi ) − c)`0 (R2 − ||φ(xi ) − c||2 ).
∂c

(18)

The derivatives of their counterparts ξj∗ for the labeled examples with respect to R, γ, and c are
given by
∂ξj∗


= 2yj∗ R`0 yj∗ R2 − ||φ(x∗j ) − c||2 − γ
∂R
∂ξj∗


= −`0 yj∗ R2 − ||φ(x∗j ) − c||2 − γ
∂γ
∂ξj∗


= 2yj∗ (φ(x∗j ) − c)`0 yj∗ R2 − ||φ(x∗j ) − c||2 − γ .
∂c
257

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Substituting the partial gradients, we resolve the gradient of Equation (5) with respect to the primal
variables as follows:
n
n+m
X
X ∂ξj∗
∂EQ5
∂ξi
= 2R + ηu
+ ηl
,
∂R
∂R
∂R

(19)

n+m
X ∂ξj∗
∂EQ5
= −κ + ηl
,
∂γ
∂γ

(20)

n
n+m
X
X ∂ξj∗
∂ξi
∂EQ5
= ηu
+ ηl
.
∂c
∂c
∂c

(21)

i=1

j=n+1

j=n+1

i=1

j=n+1

In the following, we extend our approach to allow for the use of kernel functions. An application of
the representer theorem shows that the center c can be expanded as
c=

n
X

αi φ(xi ) +

i=1

n+m
X

αj yj∗ φ(x∗j ).

(22)

j=n+1

According to the chain rule, the gradient of Equation (5) with respect to the αi/j is given by
∂EQ5
∂EQ5 ∂c
=
.
∂αi/j
∂c ∂αi/j
Using Equation (22), the partial derivatives

∂c
∂αi/j

resolve to

∂c
= φ(xi ) and
∂αi

∂c
= yj∗ φ(x∗j ),
∂αj

(23)

respectively. Applying the chain-rule to Equations (19),(20),(21), and (23) gives the gradients of
Equation (5) with respect to the αi/j .

References
Almgren, M., & Jonsson, E. (2004). Using active learning in intrusion detection. In Proc. of IEEE
Computer Security Foundation Workshop, pp. 88–89.
Andrews, D. F., & Pregibon, D. (1978). Finding the outliers that matter. Journal of the Royal
Statistical Society. Series B (Methodological), 40(1), 85–93. /
Blanchard, G., Lee, G., & Scott, C. (2010). Semi-Supervised Novelty Detection. Journal of Machine
Learning Research, ”, 2973–2973–3009–3009.
Blum, A., & Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In COLT’
98: Proc. of the eleventh annual conference on Computational learning theory, pp. 92–100,
New York, NY, USA. ACM.
Boser, B., Guyon, I., & Vapnik, V. (1992). A training algorithm for optimal margin classifiers. In
Haussler, D. (Ed.), Proceedings of the 5th Annual ACM Workshop on Computational Learning
Theory, pp. 144–152.
258

T OWARD S UPERVISED A NOMALY D ETECTION

Campbell, C., & Bennett, K. (2001). A linear programming approach to novelty detection. In Leen,
T., Dietterich, T., & Tresp, V. (Eds.), Advances in Neural Information Processing Systems,
Vol. 13, pp. 395–401. MIT Press.
Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: A survey. ACM Computing
Surveys, 41(3), 1–58.
Chapelle, O., Chi, M., & Zien, A. (2006). A continuation method for semi-supervised SVMs. In
ICML, pp. 185–192, New York, New York, USA. ACM.
Chapelle, O., & Zien, A. (2005). Semi-supervised classification by low density separation. In Proc.
of the International Workshop on AI and Statistics.
Chapelle, O., Schölkopf, B., & Zien, A. (2006). Semi-Supervised Learning (Adaptive Computation
and Machine Learning). MIT Press.
Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20, 273–297.
Do, T.-M.-T. (2010). Regularized bundle methods for large-scale learning problems with an application to large margin training of hidden Markov models. Ph.D. thesis, Pierre and Marie
Curie University Paris.
Eskin, E., Arnold, A., Prerau, M., Portnoy, L., & Stolfo, S. (2002). Applications of Data Mining
in Computer Security, chap. A geometric framework for unsupervised anomaly detection:
detecting intrusions in unlabeled data. Kluwer.
Fogla, P., Sharif, M., Perdisci, R., Kolesnikov, O., & Lee, W. (2006). Polymorphic blending attacks.
In Proc. of USENIX Security Symposium.
Goh, K.-S., Chang, E. Y., & Li, B. (2005). Using one-class and two-class svms for multiclass image
annotation. IEEE Transactions on Knowledge and Data Engineering, 17, 1333–1346.
Görnitz, N., Kloft, M., & Brefeld, U. (2009). Active and semi-supervised data domain description.
In ECML/PKDD (1), pp. 407–422.
Heller, K., Svore, K., Keromytis, A., & Stolfo, S. (2003). One class support vector machines for
detecting anomalous windows registry accesses. In Proc. of the workshop on Data Mining
for Computer Security.
Hoi, C.-H., Chan, C.-H., Huang, K., Lyu, M., & King, I. (2003). Support vector machines for class
representation and discrimination. In Proc. of the International Joint Conference on Neural
Networks.
Huber, P. (1972). Robust statistics: a review. Ann. Statist., 43, 1041.
Joachims, T. (1999). Transductive inference for text classification using support vector machines.
In International Conference on Machine Learning (ICML), pp. 200–209, Bled, Slowenien.
Kloft, M., Brefeld, U., Sonnenburg, S., & Zien, A. (2011). `p -norm multiple kernel learning. Journal
of Machine Learning Research, 12, 953–997.
Kruegel, C., Vigna, G., & Robertson, W. (2005). A multi-model approach to the detection of webbased attacks. Computer Networks, 48(5).
Krueger, T., Gehl, C., Rieck, K., & Laskov, P. (2010). TokDoc: A self-healing web application
firewall. In Proc. of 25th ACM Symposium on Applied Computing (SAC), pp. 1846–1853.
259

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Lai, C., Tax, D. M. J., Duin, R. P. W., Zbieta, E., Ekalska, P., & Ik, P. P. (2004). A study on
combining image representations for image classification and retrieval. International Journal
of Pattern Recognition and Artificial Intelligence, 18, 867–890.
Li, X.-l., & Liu, B. (2005). Learning from Positive and Unlabeled Examples with Different Data
Distributions. In ECML.
Liu, B., Dai, Y., Li, X., Lee, W. S., & Yu, P. S. (2003). Building Text Classifiers Using Positive and
Unlabeled Examples. In IEEE International Conference on Data Mining, pp. 179–186. IEEE
Comput. Soc.
Liu, Y., & Zheng, Y. F. (2006). Minimum enclosing and maximum excluding machine for pattern
description and discrimination. In ICPR ’06: Proc. of the 18th International Conference on
Pattern Recognition, pp. 129–132, Washington, DC, USA. IEEE Computer Society.
Mũnoz Marí, J., Bovolo, F., Gómez-Chova, L., Bruzzone, L., & Camp-Valls, G. (2010). SemiSupervised One-Class Support Vector Machines for Classification of Remote Sensing Data.
IEEE Transactions on Geoscience and Remote Sensing, 48(8), 3188–3197.
Manevitz, L. M., & Yousef, M. (2002). One-class svms for document classification. J. Mach. Learn.
Res., 2, 139–154.
Mao, C.-H., Lee, H.-M., Parikh, D., Chen, T., & Huang, S.-Y. (2009). Semi-supervised co-training
and active learning based approach for multi-view intrusion detection. In SAC ’09: Proc.
of the 2009 ACM symposium on Applied Computing, pp. 2042–2048, New York, NY, USA.
ACM.
Markou, M., & Singh, S. (2003a). Novelty detection: a review – part 1: statistical approaches. Signal
Processing, 83, 2481–2497.
Markou, M., & Singh, S. (2003b). Novelty detection: a review – part 2: neural network based
approaches. Signal Processing, 83, 2499–2521.
Maynor, K., Mookhey, K., Cervini, J. F. R., & Beaver, K. (2007). Metasploit Toolkit. Syngress.
Müller, K.-R., Mika, S., Rätsch, G., Tsuda, K., & Schölkopf, B. (2001). An introduction to kernelbased learning algorithms. IEEE Neural Networks, 12(2), 181–201.
Onoda, T., Murata, H., & Yamada, S. (2006). One class classification methods based non-relevance
feedback document retrieval. In WI-IATW ’06: Proc. of the 2006 IEEE/WIC/ACM international conference on Web Intelligence and Intelligent Agent Technology, pp. 393–396, Washington, DC, USA. IEEE Computer Society.
Park, J., Kang, D., Kim, J., Kwok, J. T., & Tsang, I. W. (2007). SVDD-based pattern denoising.
Neural Computation, 19, 1919–1938.
Paxson, V. (1999). Bro: A System for Detecting Network Intruders in Real-Time. Elsevier Computer
Networks, 31(23-24), 2435–2463.
Pelleg, D., & Moore, A. (2004). Active learning for anomaly and rare-category detection. In Proc.
Advances in Neural Information Processing Systems, pp. 1073–1080.
Perdisci, R., Ariu, D., Fogla, P., Giacinto, G., & Lee, W. (2009). McPAD: A multiple classifier
system for accurate payload-based anomaly detection. Computer Networks, 5(6), 864–881.
260

T OWARD S UPERVISED A NOMALY D ETECTION

Rieck, K. (2009). Machine Learning for Application-Layer Intrusion Detection. Ph.D. thesis, Berlin
Institute of Technology (TU Berlin).
Rieck, K., & Laskov, P. (2006). Detecting unknown network attacks using language models. In
Detection of Intrusions and Malware, and Vulnerability Assessment, Proc. of 3rd DIMVA
Conference, LNCS, pp. 74–90.
Rieck, K., & Laskov, P. (2007). Language models for detection of unknown attacks in network
traffic. Journal in Computer Virology, 2(4), 243–256.
Rieck, K., & Laskov, P. (2008). Linear-time computation of similarity measures for sequential data.
Journal of Machine Learning Research, 9(Jan), 23–48.
Rifkin, R. M., & Lippert, R. A. (2007). Value regularization and fenchel duality. Journal of Machine
Learning Research, 8, 441–479.
Roesch, M. (1999). Snort: Lightweight intrusion detection for networks. In Proc. of USENIX Large
Installation System Administration Conference LISA, pp. 229–238.
Salton, G., Wong, A., & Yang, C. (1975). A vector space model for automatic indexing. Communications of the ACM, 18(11), 613–620.
Schölkopf, B., & Smola, A. (2002). Learning with Kernels. MIT Press, Cambridge, MA.
Schölkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., & Williamson, R. C. (2001). Estimating
the support of a high-dimensional distribution. Neural Computation, 13(7), 1443–1471.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel methods for pattern analysis. Cambridge University Press.
Sindhwani, V., Niyogi, P., & Belkin, M. (2005). Beyond the point cloud: from transductive to
semi-supervised learning. In ICML, Vol. 1, pp. 824–831. ACM.
Sonnenburg, S. (2008). Machine Learning for Genomic Sequence Analysis. Ph.D. thesis, Fraunhofer
Institute FIRST. supervised by K.-R. Müller and G. Rätsch.
Stokes, J. W., & Platt, J. C. (2008). Aladin: Active learning of anomalies to detect intrusion. Tech.
rep., Microsoft Research.
Stolfo, S. J., Apap, F., Eskin, E., Heller, K., Hershkop, S., Honig, A., & Svore, K. (2005). A
comparative evaluation of two algorithms for windows registry anomaly detection. In Journal
of Computer Security, pp. 659–693.
Tax, D. M. J. (2001). One-class classification. Ph.D. thesis, Technical University Delft.
Tax, D. M. J., & Duin, R. P. W. (2004). Support vector data description. Machine Learning, 54,
45–66.
Tong, S., & Koller, D. (2000). Support vector machine active learning with applications to text
classification. In Proc. of the Seventeenth International Conference on Machine Learning,
San Francisco, CA. Morgan Kaufmann.
Vapnik, V. (1998). Statistical Learning Theory. Wiley, New York.
Wang, J., Neskovic, P., & Cooper, L. N. (2005). Pattern classification via single spheres. In Computer Science: Discovery Science (DS), Vol. 3735, pp. 241–252.
261

G ÖRNITZ , K LOFT, R IECK , & B REFELD

Wang, K., Parekh, J., & Stolfo, S. (2006). Anagram: A content anomaly detector resistant to mimicry
attack. In Recent Adances in Intrusion Detection (RAID), pp. 226–248.
Warmuth, M. K., Liao, J., Rätsch, G., Mathieson, M., Putta, S., & Lemmen, C. (2003). Active
learning with support vector machines in the drug discovery process. Journal of Chemical
Information and Computer Sciences, 43(2), 667–673.
Yuan, C., & Casasent, D. (2004). Pseudo relevance feedback with biased support vector machine.
In Proc. of the International Joint Conference on Neural Networks.
Zhang, D., & Lee, W. S. (2005). A simple probabilistic approach to learning from positive and
unlabeled examples. In Proceedings of the 5th Annual UK Workshop on . . . .

262

Journal of Artificial Intelligence Research 46 (2013) 511-577

Submitted 12/12; published 03/13

Probabilistic Planning for Continuous Dynamic Systems
under Bounded Risk
Masahiro Ono

ONO @ APPI . KEIO . AC . JP

Keio University
3-14-1 Hiyoshi, Kohoku-ku
Yokohama, Kanagawa, 223-8522 Japan

Brian C. Williams

WILLIAMS @ MIT. EDU

Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139 USA

Lars Blackmore

LARS . BLACKMORE @ SPACEX . COM

SpaceX
1 Rocket Road
Hawthorne, CA 90250 USA

Abstract
This paper presents a model-based planner called the Probabilistic Sulu Planner or the p-Sulu
Planner, which controls stochastic systems in a goal directed manner within user-specified risk
bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic
systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To
this end, we first develop a new plan representation called a chance-constrained qualitative state
plan (CCQSP), through which users can specify the desired evolution of the plant state as well as
the acceptable level of risk. An example of a CCQSP statement is “go to A through B within 30
minutes, with less than 0.001% probability of failure.” We then develop the p-Sulu Planner, which
can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop
the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2)
goal-directed planning in a continuous domain with temporal constraints. The first capability is to
ensures that the probability of failure is bounded. The second capability is essential for the planner
to solve problems with a continuous state space such as vehicle path planning. We demonstrate the
capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning
and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo
spacecraft.

1. Introduction
There is an increasing need for risk-sensitive optimal planning in uncertain environments, while
guaranteeing an acceptable probability of success. A motivating example for this article is the
Boeing concept of a future aerial personal transportation system (PTS), as shown in Figure 1. The
PTS consists of a fleet of small personal aerial vehicles (PAV) that enable the flexible point-to-point
transportation of individuals and families.
c
⃝2013
AI Access Foundation. All rights reserved.

O NO , W ILLIAMS , & B LACKMORE

In order to provide safety, PTS should be highly automated. In 2004, in the US, pilot error was
listed as the primary cause of 75.5% of fatal general aviation accidents, according to the 2005 Joseph
T. Nall Report (Aircraft Owners and Pilots Association Air Safety Foundation, 2005). Automated
path planning, scheduling, collision avoidance, and traffic management will significantly improve
the safety of PTS, as well as its efficiency. The challenges to operating such a system include
adapting to uncertainties in the environment, such as storms and turbulence, while satisfying the
complicated needs of users.
There is a substantial body of work on planning under uncertainty that is relevant. However,
our approach is distinctive in three key respects. First, our planner, the p-Sulu Planner, allows
users to explicitly limit the probability of constraint violation. This capability is particularly important for risk-sensitive missions where the impact of failure is significant. Second, the planner
is goal-directed, by which we mean that it achieves time-evolved goals within user-specified temporal constraints. Third, the planner works in a continuous state space. A continuous state space
representation fits naturally to many real-world applications, such as planning for aerial, space, and
underwater vehicles. It is also important for problems with resources.

Figure 1: Personal Transportation System (PTS). (Courtesy of the Boeing Company)
Figure 2 shows a sample PTS scenario. A passenger of a PAV starts in Provincetown, MA
and wants to go to Bedford within 30 minutes. The passenger also wants to go through a scenic
area and remain there between 5 and 10 minutes during the flight. There is a no-fly zone (NFZ)
and a storm that must be avoided. However, the storm’s future location is uncertain; the vehicle’s
location is uncertain as well, due to control error and exogenous disturbances. Thus there is a risk
of penetrating the NFZ or the storm. The passengers want to limit such risk to at most 0.001%.
In order to handle such a planning problem, we introduce a novel planner called the Probabilistic
Sulu Planner (p-Sulu Planner), building upon prior work on the model-based plan executive called
Sulu (Léauté & Williams, 2005). The p-Sulu Planner provides the following three capabilities, in
order to meet the needs described in the above scenario: 1) goal-directed planning in a continuous
domain, 2) near-optimal planning, and 3) risk-sensitive planning with risk bounds.
• Goal-directed planning in a continuous domain The p-Sulu Planner must plan actions with
continuous effects that achieve time evolved goals specified by users. In the case of the PTS
scenario in Figure 2, the PAV must sequentially achieve two temporally extended goals, called
512

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Figure 2: A sample plan for personal aerial vehicle (PAV)

episodes: going through the scenic area and then arriving at Bedford. There are additional
temporal constraints on the goals that are inherent to the scenario; some temporal constraints
come from physical limitations, such as fuel capacity, and others come from passenger requirements.
• Near-optimal stochastic planning Cost reduction and performance improvement are important issues for any system. In the PTS scenario, passengers may want to minimize the trip
time or fuel usage. The p-Sulu Planner finds a near-optimal control sequence according to the
user-defined objective function, while satisfying given constraints.
• Risk-sensitive planning with risk bounds Real-world systems are subject to various uncertainties, such as state estimation error, modeling uncertainty, and exogenous disturbance.
In the case of PAVs, the position and velocity of the vehicle estimated by the Kalman filter
typically involve Gaussian-distributed uncertainties; the system model used for planning and
control is not perfect; and the vehicles are subject to unpredictable disturbances such as turbulence. Under such uncertainty, the executed result of a plan inevitably deviates from the
original plan and hence involves risk of constraint violation. Deterministic plan execution is
particularly susceptible to risk when it is optimized in order to minimize a given cost function,
since the optimal plan typically pushes against one or more constraint boundaries, and hence
leaves no margin for error. For example, the shortest path in the PTS scenario shown in Figure
2 cuts in close to the NFZs and the storm, or more generally, constraint boundaries. Then,
a tiny perturbation to the planned path may result in a penetration into the obstacles. Such
risk can be reduced by setting a safety margin between the path and the obstacles, at a cost of
longer path length. However, it is often impossible to guarantee zero risk, since there is typically a non-zero probability of having a disturbance that is large enough to push the vehicle
out of the feasible region. Therefore, passengers of the vehicle must accept some risk, but at
the same time they need to limit it to a certain level. More generally, users of an autonomous
system under uncertainty should be able to specify their bounds on risk. The planner must
guarantee that the system is able to operate within these bounds. Such constraints are called
chance constraints.
513

O NO , W ILLIAMS , & B LACKMORE

1.1 Overview of the Planner
This section describes the inputs and outputs of the p-Sulu Planner informally. They are rigorously
defined in Section 2.
1.1.1 I NPUTS
Initial Condition The p-Sulu Planner plans a control sequence starting from the current state,
which is typically estimated from noisy sensor measurements. Therefore, the p-Sulu Planner takes
the probability distribution, instead of the point estimate, of the current state as the initial condition.
Stochastic Plant Model In the control community the planning problem is to generate a sequence
of control inputs that actuate a physical system, called the plant. The action model for a plant is
typically a system of real-valued equations over control, state and observable variables. The pSulu Planner takes as an input a linear stochastic plant model, which specifies probabilistic state
transitions in a continuous domain. This is a stochastic extension of the continuous plant model used
by Léauté and Williams (2005). In this paper we limit our focus to Gaussian-distributed uncertainty.
Chance-constrained qualitative state plan (CCQSP) In order to provide users with an intuitive way to command stochastic systems, we develop a new plan representation called a chanceconstrained qualitative state plan (CCQSP). It is an extension of qualitative state plan (QSP), developed and used by Léauté and Williams (2005), Hofmann and Williams (2006), and Blackmore,
Li, and Williams (2006). CCQSP specifies a desired evolution of the plant state over time, and is
defined by a set of discrete events, a set of episodes, which impose constraints on the plant state
evolution, a set of temporal constraints between events, and a set of chance constraints that specify
reliability constraints on the success of sets of episodes in the plan.
A CCQSP may be depicted as a directed acyclic graph, as shown in Figure 3. The circles
represent events and squares represent episodes. Flexible temporal constraints are represented as a
simple temporal network (STN) (Dechter, Meiri, & Pearl, 1991), which specifies upper and lower
bounds on the duration between two events (shown as the pairs of numbers in parentheses). The
plan in Figure 3 describes the PTS scenario depicted in Figure 2, which can be stated informally as:
“Start from Provincetown, reach the scenic region within 30 time units, and remain
there for between 5 and 10 time units. Then end the flight in Bedford. The probability
of failure of these episodes must be less than 1%. At all times, remain in the safe region
by avoiding the no-fly zones and the storm. Limit the probability of penetrating such
obstacles to 0.0001%. The entire flight must take at most 60 time units.”
A formal definition of CCQSP is given in Section 2.4.3.
Objective function The user of the p-Sulu Planner can specify an objective function (e.g., a cost
function). We assume that it is a convex function.
1.1.2 O UTPUT
Executable control sequence The p-Sulu Planner plans over a finite horizon. One of the two
outputs of the p-Sulu Planner is an executable control sequence over the horizon that satisfies all
constraints specified by the input CCQSP. In the case of the PTS scenario, the outputs are the vehicle’s actuation inputs, such as acceleration and ladder angle, that result in the nominal paths shown
514

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Figure 3: An example of a chance-constrained qualitative state plan (CCQSP), a new plan representation to specify the desired evolution of the plant state and the acceptable levels of
risk. In the PTS scenario in Figure 2, the passengers of a PAV would like to go from
Provincetown to Bedford, and fly over the scenic region on the way. The “safe region”
means the entire state space except the obstacles. Risk of the episodes must be within the
risk bounds specified by chance constraints.

in Figure 2. In order for the control sequence to be executable, it must be dynamically feasible. For
example, the curvature of the PAV’s path must not exceed the vehicles’ maneuverability.
Optimal schedule The other output of the p-Sulu Planner is the optimal schedule, a set of execution time steps for events in the input CCQSP that minimizes a given cost function. In the case of
the PTS scenario shown in Figure 3, a schedule specifies when to leave the scenic region and when
to arrive at Bedford, for example. The p-Sulu Planner finds a schedule that satisfies all the simple
temporal constraints specified by the CCQSP, and minimizes the cost function.
The two outputs – the control sequence and the schedule – must be consistent with each other:
the time-evolved goals are achieved on the optimal schedule by applying the control sequence to the
given initial conditions.
1.2 Approach
The p-Sulu Planner must solve a very difficult problem of generating an executable control sequence
for a CCQSP, which involves both combinatorial optimization of a discrete schedule and non-convex
optimization of a continuous control sequence. Our approach in this article is to develop the p-Sulu
Planner in three technical steps, which we call “spirals”.
In the first spiral, described in Section 4, we solve a special case of the CCQSP planning problem, where the feasible state space is convex (e.g., path planning problem without obstacles) and
the schedule is fixed, as shown in Figure 4-(a). This problem can be transformed into a convex optimization problem by the risk allocation approach, which is presented in our previous work (Ono
& Williams, 2008a). We obtain a feasible, near-optimal solution to the CCQSP planning problem
by optimally solving the convex optimization using an interior point method (Blackmore & Ono,
2009).
In the second spiral, which is presented in Section 5, we consider a CCQSP problem with a
non-convex state space in order to include obstacles, as in Figure 4-(b). We develop a branch and
bound-based algorithm, called non-convex iterative risk allocation (NIRA). Subproblems of the
branch-and-bound search of NIRA are convex chance-constrained optimal control problems, which
are solved in the first spiral. The NIRA algorithm cannot handle a problem with a flexible schedule.
515

O NO , W ILLIAMS , & B LACKMORE

In the third spiral, which is described in Section 6, we develop another branch and boundbased algorithm, namely the p-Sulu Planner, which can solve a general CCQSP planning problem
with a flexible schedule and obstacles. Subproblems of the branch-and-bound search of the pSulu Planner are non-convex chance-constrained optimal control problems, which are solved by the
NIRA algorithm.
dŚĞp-SuluWůĂŶŶĞƌ (Section 6)

NIRA (Section 5)

(Ono & Williams 2008b) (Section 4)

Fixed schedule

Fixed schedule

t=5

Simple temporal
constraints

[2 4]

t=5
t=1

C

t=1

Goal

[1 3]

Goal

C

Obstacle
Start

Start

(a) Convex, fixed schedule

Obstacle

Waypoint

Waypoint

Waypoint
Start

(b) Non-convex, fixed schedule

Goal

[0 5]

(c) Non-convex, flexible schedule

Figure 4: Three-spiral approach to the CCQSP planning problem

1.3 Related Work
Recall that the CCQSP planning problem is distinguished by its use of time-evolved goals, continuous states and actions, stochastic optimal solutions and chance constraints. While the planning
and control disciplines have explored aspects of this problem, its solution in total is novel, and our
approach to solving this problem efficiently through risk allocation is novel.
More specifically, there is an extensive literature on planning with discrete actions to achieve
temporally extended goals (TEGs), such as TLPlan (Bacchus & Kabanza, 1998) and TALPlan
(Kvarnstrom & Doherty, 2000), which treat TEGs as temporal domain control knowledge and prune
the search space by progressing the temporal formula. However, since these TEG planners assume
discrete state spaces, they cannot handle problems with continuous states and effects without discretization. Ignoring chance constraints, the representation of time evolved goals used by TLPlan
and the p-Sulu Planner is similar. TLPlan uses a version of metric interval temporal logic (MITL)
(Alur, Feder, & Henzinger, 1996) applied to discrete states, while the p-Sulu Planner uses qualitative state plans (QSPs) (Léauté & Williams, 2005; Hofmann & Williams, 2006; Li, 2010) over
continuous states. Li (2010) shows that, for a given state space, any QSP can be expressed in MITL.
The key difference that defines the p-Sulu Planner is the addition of chance constraints, together
with its use of continuous variables.
Several planners, particularly those that are employed as components of model-based executives, command actions in continuous state space. For example, Sulu (Léauté & Williams, 2005)
takes as input a deterministic linear model and QSP, which specifies a desired evolution of the plant
state as well as flexible temporal constraints, and outputs a continuous control sequence. Chekhov
(Hofmann & Williams, 2006) also takes as input a QSP and a nonlinear deterministic system model,
and outputs a continuous control sequence. In order to enable fast real-time plan execution, Chekhov
precomputes flow tubes, the sets of continuous state trajectories that end in the goal regions specified by the given plan. Kongming (Li, 2010) provides a generative planning capability for hybrid
516

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

systems, involving both continuous and discrete actions. It employs a compact representation of
hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for
discrete actions and flow tubes for continuous actions. These planners adapt to the effects of uncertainty, but do not explicitly reason about the effects of uncertainty during planning. For example,
Sulu employs a receding horizon approach, which continuously replans the control sequence using
the latest measurements. Chekhov’s flow tube representation of feasible policies allows the executive to generate new control sequences in response to disturbances on-line. The p-Sulu Planner is
distinct from these continuous planners in that it plans with a model of uncertainty in dynamics, instead of just reacting to it. Its plan guarantees the user-specified probability of success by explicitly
reasoning about the effects of uncertainty.
In AI planning literatures, a planning domain description language, PDDL+, supports mixed
discrete-continuous planning domains (Fox & Long, 2006). Probabilistic PDDL (Younes & Littman,
2004) and the Relational Dynamic influence Diagram Language (RDDL) (Sanner, 2011) can handle stochastic systems. Recently, Coles, Coles, Fox, and Long (2012) developed a forward-chaining
heuristic search planner named COLIN, which can deal with continuous linear change and durationdependent effects. However, these planners do not handle chance constraints. We note that the
outputs of the p-Sulu Planner is continuous in space but discrete in time. The time-dependent MDP
developed by Boyan and Littman (2000) can handle continuous time by encoding time in the state.
Extension of the p-Sulu Planner to continuous-time planning would be an interesting future direction.
Most work within the AI community on probabilistic planning has focused on planning in discrete domains and builds upon the Markov decision process (MDP) framework. A growing subcommunity has focused on extensions of MDPs to the continuous domain. However, tractability
is an issue, since they typically require partitioning or approximation of continuous state space.
A straightforward partitioning of continuous state and action spaces into discrete states and actions often leads to an exponential blow-up in running time. Furthermore, when the feasible state
space is unbounded, it is impossible to partition the space into a finite number of compact subspaces. An alternative approach is the function approximation (Boyan & Moore, 1995), but its
convergence is guaranteed only when the approximation error is bounded (Bertsekas & Tsitsiklis,
1996; Lagoudakis & Parr, 2003). Time-dependent MDPs (Boyan & Littman, 2000; Feng, Dearden,
Meuleau, & Washington, 2004) can do efficient partitioning of continuous state space, but make an
assumption that the set of available states and actions are finite (i.e., discrete). Hence, planning by
these MDPs in a continuous state space, such as Rn , requires to approximate the state space by a
finite number of discrete states. Our approach is essentially different from the MDP approaches in
that the continuous variables are directly optimized through convex optimization without discretization of continuous state space. Hence, the continuity of the state space does not harm the tractability
of the p-Sulu Planner.
A second point of comparison is the treatment of risk. Like the p-Sulu Planner, the MDP
framework offers an approach to marrying utility and risk. However, most MDP algorithms balance
the utility and risk by assigning a large negative utility to the event of constraint violation. Such an
approach cannot guarantee bounds on the probability of constraint violation. The constrained MDP
approach (Altman, 1999) can explicitly impose constraints. Dolgov and Durfee (2005) showed
that stationary deterministic policies for constrained MDPs can be obtained by solving a mixed
integer linear program (MILP). However, the constrained MDP framework can only impose bounds
on the expected value of costs, and again, cannot guarantee strict upper bounds on the probability
517

O NO , W ILLIAMS , & B LACKMORE

of constraint violation. In contrast, the p-Sulu Planner allows users to impose chance constraints,
which explicitly restrict the probability of constraint violation. As far as the authors know, the
risk-sensitive reinforcement learning approach proposed by Geibel and Wysotzki (2005) is the only
work that considers chance constraints in the MDP framework. They developed a reinforcement
learning algorithm for MDPs with a constraint on the probability of entering error states. Our work
is distinct from theirs in that the p-Sulu Planner is goal-directed, by which we mean that it achieves
time-evolved goals within user-specified temporal constraints. To summarize, no prior MDP work
supports continuous state and actions in combination with general continuous noise on transitions
while ensuring that the probability of failure is bounded.
Risk-sensitive control methods in a continuous domain have been extensively studied in the discipline of control theory. For example, the celebrated H∞ control method minimizes the effect of
disturbances on the output of a system while guaranteeing the stability of the system (Stoorvogel,
1992). Risk-sensitive control approaches allow users to choose the level of risk averseness through
the minimization of an expected exponentiated cost function (Jacobson, 1973; Fleming & McEneaney, 1995). However, these approaches do not address chance constraints and optimal scheduling. Several methods have been proposed for solving stochastic optimal control problems over
continuous variables with chance constraints. The method proposed by van Hessem (2004) turns
a stochastic problem into a deterministic problem using a very conservative ellipsoidal relaxation.
Blackmore (2006) proposes a sampling-based method called Particle Control, which evaluates joint
chance constraints by a Monte-Carlo simulation, instead of using a conservative bound. As a result,
the stochastic planning problem is reduced to a MILP problem. Although it has a theoretical guarantee that it can obtain the exactly optimal solution when an infinite number of samples are used,
computation time is an issue. Blackmore et al. (2006) and Nemirovski and Shapiro (2006) employed
Boole’s inequality to decompose a joint chance constraint into individual chance constraints. Although Boole’s inequality is less conservative than the ellipsoidal relaxation, their approach still
has non-negligible conservatism since it fixes each individual risk bound to a uniform value. Our
approach builds upon this approach, with modifications to allow flexible individual risk bounds.
To the best of the authors’ knowledge, the p-Sulu Planner is the first goal-directed planner that
is able to plan in a continuous state space with chance constraints.
1.4 Innovations
The p-Sulu Planner is enabled by six innovations presented in this article.
First, in order to allow users to command stochastic systems intuitively, we develop a new plan
representation, CCQSP (Section 2.4.3).
Second, in order to decompose a chance constraint over a disjunctive clause into a disjunction
of individual chance constraints, we introduce the risk selection approach (Section 5.1.2).
Third, in order to obtain lower bounds for the branch-and-bound search in NIRA, we develop
the fixed risk relaxation (FRR), a linear program relaxation of the subproblems (Section 5.4.2).
Fourth, we minimize the search space for the optimal schedule by introducing a new forward
checking method that efficiently prunes infeasible assignment of execution time steps (Section 6.2).
Fifth, in order to enhance the computation time of schedule optimization, we introduce a method
to obtain a lower bound for the branch-and-bound by solving fixed-schedule planning problems with
an partial assignment of a schedule. (Section 6.3)
518

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Sixth, in order to minimize the number of non-convex subproblems solved in the branch-andbound search, we introduce a variable ordering heuristic, namely the convex-episode-first (CEF)
heuristic, which explores the episodes with a convex feasible state region before the ones with a
non-convex state region (Section 6.2.2).
The rest of this article is organized as follows. Section 2 formally defines the CCQSP and
states the CCQSP planning problem. Section 3 derives the encoding of the problem as a chanceconstrained optimization problem, as well as the encodings of two limited versions of the CCQSP
planning problem: one with a fixed schedule and a convex state space, and another with a fixed
schedule and a non-convex state space. Section 4 reviews the solution to a fixed-schedule CCQSP
planning problem with a convex state space. Section 5 develops the NIRA algorithm, which solves
a fixed-schedule CCQSP planning problem with a non-convex state space, and Section 6 introduces
the p-Sulu Planner, which solves a CCQSP planning problem with a flexible schedule and a nonconvex state space. Finally, Section 7 shows simulation results on various scenarios, including the
personal transportation system (PTS).

2. Problem Statement
Recall that the p-Sulu Planner takes as input a linear stochastic plant model, which specifies the
effects of actions; an initial state description, describing a distribution over initial states; a CCQSP,
which specifies desired evolutions of the state variables, as well as acceptable levels of risk; and an
objective function. Its output is an executable control sequence and an optimal schedule. Planning
is performed over a finite horizon, since the p-Sulu Planner is incorporated with the finite-horizon
optimal control. We first define the variables used in the problem formulations. Then we define
elements of the inputs and outputs.
2.1 Definition of Time Step
We consider a series of discretized finite time steps t = 0, 1, 2, · · · N with a fixed time interval
∆T , where integer N is the size of the planning horizon. Since the time interval ∆T can take any
positive real value, it suffices to consider time steps with only integer indices to approximate the
system’s dynamics. We use the term “time step” to mean an integer index of the discretized time
steps, while using the term “time” to mean a real-valued time. We define sets T and T− as follows:
T := {0, 1, 2, · · · N }.
−

T := {0, 1, 2, · · · N − 1}.

(1)
(2)

We limit the scope of this article to a discrete-time stochastic system. This is because optimizing
a control sequence for a continuous-time stochastic system requires solving a stochastic differential
equation (SDE) repeatedly. Performing such a computation is not tractable except for very simple
problems.
2.2 Definitions of Events
An event denotes the start or end of an episode of behavior in our plan representation.
Definition 1. An event e ∈ E is a instance that is executed at a certain time step in T.
519

O NO , W ILLIAMS , & B LACKMORE

We define two special events, the start event e0 and the end event eE . Without loss of generality,
we assume that e0 is executed at t = 0. The end event eE represents the termination of the entire
plan.
2.3 Definitions of Variables
Variables used in our problem formulation involve a discrete schedule, a continuous state vector,
and a continuous control vector.
We formally define an event as well as a schedule as follows:
Definition 2. An execution time step s(e) ∈ T is an integer-valued scalar that represents the
time step at which an event e ∈ E is executed. A schedule s := [s(e0 ), s(e1 ), · · · s(eE )] is
a sequence of execution time steps of all the events e ∈ E. Finally, a partial schedule σ :=
[σ(e) ∈ s | e ∈ Eσ ⊆ E] is an ordered set of execution time steps of a subset of events Eσ .
By definition, the start event is executed at t = 0 i.e, s(e0 ) = 0. Following the notation of a
schedule, we denote by σ(e) the execution time of an event e ∈ Eσ . See also the definition of a
schedule (Definition 2).
We consider a continuous state space, where a state vector and a state sequence are defined as
follows:
Definition 3. A state vector xt ∈ Rnx is a real-valued vector that represents the state of the plant
at time step t. A state sequence x0:N := [x0 · · · xN ] is a vector of state variables from time step 0
to N .
Our actions are assignments to continuous decision variables, which are referred to as a control
vector:
Definition 4. A control vector ut ∈ Rnu is a real-valued vector that represents the control input to
the system at time step t. A control sequence u0:N −1 := [u0 · · · uN −1 ] is a vector of control inputs
from time 0 to N − 1.
2.4 Definitions of Inputs
This subsection defines the four inputs of the p-Sulu Planner: an initial condition, a stochastic plant
model, a CCQSP, and an objective function.
2.4.1 I NITIAL C ONDITION
The belief state at the beginning of the plan is represented by an initial state, which is assumed to
have a Gaussian distribution with a known mean x̄0 and a covariance matrix Σx0 :
x0 ∼ N (x̄0 , Σx0 ).

(3)

The parameters in (3) are specified by an initial condition, which is defined as follows:
Definition 5. An initial condition I is a pair I = ⟨x̄0 , Σx0 ⟩, where x̄0 is the mean initial state and
Σx0 is the covariance matrix of the initial state.
520

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

2.4.2 S TOCHASTIC P LANT M ODEL
The p-Sulu Planner controls dynamical systems in which actions correspond to the settings of continuous control variables, and whose effects are on continuous state variables. The p-Sulu Planner
specifies these actions and their effects through a plant model. A plant model is considered as a state
transition model in a continuous space. We employ a variant of a linear plant model with additive
Gaussian uncertainty that is commonly used in the context of chance-constrained stochastic optimal control (Charnes & Cooper, 1959; Nemirovski & Shapiro, 2006; Oldewurtel, Jones, & Morari,
2008; van Hessem, 2004), with a modification to consider controller saturation. Specifically, we
assume the following plant model:
xt+1 = At xt + B t µU (ut ) + wt

(4)

where wt ∈ Rnx is a state-independent disturbance at t-th time step that has a zero-mean Gaussian
distribution with a given covariance matrix denoted by Σwt :
wt ∼ N (0, Σwt ).

(5)

Although this model prohibits state-dependent disturbance, most types of noise involved in our
target applications are state independent. For example, in the PTS scenario introduced in Section
1, the primary source of uncertainty is a wind turbulence, which is typically not dependent on the
state of a vehicle. In the space rendezvous scenario discussed in Section 7.5, the main sources of
perturbations for a space craft are the tidal force and unmodeled gravitational effects of Sun, Moon,
and other planets (Wertz & Wiley J. Larson, 1999). Such noises can be modeled as a state-dependent
noise in practice when the scale of the planned actions is significantly smaller than that of the Solar
System.
not dependent on the state of the space craft. We note that our problem formulation can encode
time-varying noise by specifying different covariance matrices Σwt for each time step.
The set U ⊂ Rnu is a compact convex set that represents the continuous domain of the feasible
control inputs. If an infeasible control input ut ∈
/ U is given to the plant, its actuators saturate. The
function µU (·) : Rnu 7→ U in (4) represents the effect of actuator saturation as follows:
{
u
(if u ∈ U)
µU (u) :=
,
PU (u) (otherwise)
where PU (u) is a projection of u on U. For example, when u is one-dimensional and U = [l, u],
PU (u) = max(min(u, u), l). Note that µU introduces nonlinearity in the plant.
The parameters in (4) and (5) are specified by a stochastic plant model, which is defined as
follows:
Definition 6. A stochastic plant model M is a four-tuple M = ⟨A0:N −1 , B 0:N −1 , Σw0:N −1 , U⟩,
where A0:N −1 and B 0:N −1 are sets of N matrices A0:N −1 := {A0 , A1 , · · · AN −1 }, B 0:N −1 :=
{B 0 , B 1 , · · · B N −1 }, Σw0:N −1 is a set of N covariance matrices Σw0:N −1 = {Σw0 , Σw1 , · · · , ΣwN −1 },
and U ⊂ Rnu is a compact convex set that represents the domain of the feasible control inputs.
Note that xt , as well as wt , is a random variable, while ut is a deterministic variable. Figure
5 illustrates our plant model. In a typical plant model, the probability circles grow over time since
disturbance wt is added at every time step, as drawn in the figure. This effect represents a commonly
observed tendency that the distant future involves more uncertainty than the near future.
521

O NO , W ILLIAMS , & B LACKMORE

x2

x3

99.9%
99%
90%

x1
x0

99.9%
99%

x1

99.9%
99%
90%

Nominal
path

x2

90%

x3

Figure 5: Illustration of the stochastic plant model used by the p-Sulu Planner.
In order to mitigate the accumulation of uncertainty, we employ a close-loop control approach,
which generates the control input ut by incorporating a nominal control input ūt ∈ Rnu with an
error feedback, as follows:
ut = ūt + K t (xt − x̄t ),
(6)
where K t is a matrix representing a constant stabilizing feedback gain at time t and x̄t is the
nominal state vector. The nominal state x̄t is obtained by the following recursion:
x̄0 := x0
x̄t+1 = At x̄t + B t ūt .

(7)
(8)

A closed-loop control approach has been employed by Geibel and Wysotzki (2005) and Oldewurtel
et al. (2008) in the context of chance-constrained optimal control and shown that it significantly
improves performance.
In this closed-loop planning method, the nominal control input ūt is planned before the execution. The actual control input ut is computed in real time by using (6). The feedback term in
(6) linearly responds to the error xt − x̄t . By choosing the feedback gain K t appropriately, the
growth of the probability circles in Figure 5 can be slowed down. Neglecting the effect of controller
saturation (i.e., assuming U = Rnx ), it follows from (4) and (6) that xt has a Gaussian distribution
with a covariance matrix Σxt , which evolves as follows:
Σxt+1 = (At + B t K t )Σxt (At + B t K t )T + Σwt .

(9)

In a typical plant, some of the eigenvalues of A are one. Therefore, when there is no error feedback
(i.e., K t = 0), the “size” of Σxt grows by Σwt at each iteration. By choosing K t so that the
norm of the largest eigenvalue of (At + B t K t ) is less than one, the covariance Σxt does not grow
continuously. Such a feedback gain can be found by using standard control techniques, such as a
linear quadratic regulator (LQR) (Bertsekas, 2005). Since we consider a finite-horizon, discretetime planning problem, the optimal time-varying LQR gain K t is obtained by solving the finitehorizon, discrete-time Riccati equation. In practice, it often suffices to use the steady-state (i.e.,
time-invariant) LQR gain, which is obtained by solving the infinite-horizon, discrete-time Riccati
equation for simplicity. We note that the feedback gain K t can also be optimized in real time. This
approach is often used for robust and stochastic model predictive controls (Goulart, Kerrigan, &
Maciejowski, 2006; Oldewurtel et al., 2008; Ono, 2012). However, such an extension is beyond the
scope of this paper.
522

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

An issue is that, if the error xt − x̄t happens to be very large, the control input ut may exceed
its feasible domain U, resulting in actuator saturation. Therefore, (9) does not hold due to the
nonlinearity of the function µU (·). We address this issue through the risk allocation approach. More
specifically, we impose chance constraints on control saturation, and allocate risk to both state and
control constraints. This approach is discussed more in detail in Section 4.1.5.
2.4.3 C HANCE - CONSTRAINED Q UALITATIVE S TATE P LAN (CCQSP)
A qualitative state plan (QSP) (Léauté & Williams, 2005) is a temporally flexible plan that specifies
the desired evolution of the plant state. The activities of a QSP are called episodes and specify
constraints on the plant state. CCQSP is an extension of QSPs to stochastic plans that involve
chance constraints, defined as follows:
Definition 7. A chance-constrained qualitative state plan (CCQSP) is a four-tuple P = ⟨E, A, T , C⟩,
where E is a set of discrete events, A is a set of episodes, T is a set of simple temporal constraints,
and C is a set of chance constraints.
The four elements of a CCQSP are defined precisely in a moment. Like a QSP, a CCQSP can
be illustrated diagrammatically by a directed acyclic graph in which the discrete events in E are
represented by vertices, drawn as circles, and the episodes as arcs with ovals. A CCQSP has a start
event e0 and an end eE , which corresponds to the beginning and the end of the mission, respectively.
For example, Figure 3 shows the CCQSP of the PTS scenario. The state regions and obstacles in
the CCQSP are illustrated in Figure 2. It involves four events: E = {e0 , e1 , e2 , eE }. Their meanings
are described as follows.
1. The start event e0 corresponds to the take off of the PAV from Provincetown.
2. The second event e1 corresponds to the time step when PAV reaches the scenic region.
3. Event e2 is associated with the time instant when the PAV has just left the scenic region.
4. The end event eE corresponds to the arrival of the PAV in Bedford.
The CCQSP has four episodes A = {a1 , a2 , a3 , a4 } and two chance constraints C = {c1 , c2 }.
A natural language expression of the CCQSP is:
“ Start from Provincetown, reach the scenic region within 30 time units, and remain
there for between 5 and 10 time units. Then end the flight in Bedford. The probability
of failure of these activities must be less than 1%. At all times, remain in the safe region
by avoiding the no-fly zones and the storm. Limit the probability of penetrating such
obstacles to 0.0001%. The entire flight must take at most 60 time units.”
Below we formally define the three types of constraints - episodes, temporal constraints, and
chance constraint.
Episodes Each episode a ∈ A specifies the desired state of the system under control over a time
interval.
S
Definition 8. An episode a = ⟨eSa , eE
a , Πa (tS , tE ), Ra ⟩ has an associated start event ea and an end
N
event eE
a . Ra ∈ R is a region in a state space. Πa ⊆ T is a set of time steps at which the state xt
must be in the region Ra .

523

O NO , W ILLIAMS , & B LACKMORE

The feasible region Ra can be any subset of RN . We will approximate Ra with a set of linear
constraints later in Section 3.1.1.
Πa (tS , tE ) is a subset of T given as a function of the episode’s start time step tS = s(eSa ) and
its end time step tE = s(eE
a ). Different forms of Πa (tS , tE ) result in various types of episodes. The
following three types of episodes are particularly of interest to us:
1. Start-in episode: Πa (tS , tE ) = {tS }
2. End-in episode: Πa (tS , tE ) = {tE }
3. Remain-in episode: Πa (tS , tE ) = {tS , tS + 1, · · · , tE }
For a given episode a, the set of time steps at which the plant state must be in the region Ra is
obtained by substituting s(eSa ) and s(eE
a ), the execution time steps of the start event and the end
event of the episode, into tS and( tE . In other) words, an episode a requires that the plant state
is in Ra for all time steps in Πa s(eSa ), s(eE
a ) . For the rest of the article, we use the following
abbreviated notation:
(
)
Πa (s) := Πa s(eSa ), s(eE
a) .
Using this notation, an episode is equivalent to the following state constraint:
∧
xt ∈ Ra .

(10)

t∈Πa (s)

For example, in the CCQSP shown in Figure 3, there are four episodes: a1 (“Start in [Provincetown]”), a2 (“Remain in [Scenic region]”), a3 (“End in Bedford”), and a4 (“Remain in [safe region]”).
In Section 6, we solve a relaxed optimization problem with a partial schedule (Definition 2)
in order to obtain a lower bound on the optimal objective value. In such relaxed problems, only
a subset of the episodes that are relevant to the given partial schedule are imposed. We formally
define a partial episode set of a partial schedule σ as follows:
Definition 9. Given a partial schedule σ, A(σ) ⊆ A is its partial episode set, which is a subset of
A that involves the episodes whose start event and end event are assigned execution time steps.
}
{
A(σ) = a ∈ A | eSa ∈ Eσ ∧ eE
a ∈ Eσ ,
where the definition of Eσ is given in Definition 2.
Chance constraint Recall that a chance constraint is a probabilistic constraint that requires the
constraints defining each episode to be satisfied within a user-specified probability. A CCQSP can
have multiple chance constraints. A chance constraint is associated with at least one episode.
A chance constraint is formally defined as follows:
Definition 10. A chance constraint c = ⟨Ψc , ∆c ⟩ is a constraint requiring that:


∧ ∧
Pr 
xt ∈ Ra  ≥ 1 − ∆c ,

(11)

a∈Ψc t∈Πa (s)

where ∆c is a user-specified risk bound and Ψc ⊆ A is a set of episodes associated with the chance
constraint c.
524

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Note that every episode in a CCQSP must be associated with exactly one chance constraint.
Any episode in A must not be involved in more than one chance constraint or unassociated with any
chance constraint.
For example, the CCQSP shown in Figure 3 has two chance constraints, c1 and c2 . Their associated episodes are Ψc1 = {a1 , a2 , a3 } and Ψc2 = {a4 }. Therefore, c1 requires that the probability
of satisfying the three episodes a1 , a2 , and a3 (colored in green) is more than than 99%, while c2
requires that the probability of satisfying the episode a4 is more than 99.99999%.
We make the following assumption, which is necessary in order to guarantee the convexity of
constraints in Section 4.2.
Assumption 1.
∆c ≤ 0.5
This assumption requires that the risk bounds are less than 50%. We claim that this assumption
does not constrain practical applications, since typically the user of an autonomous system would
not accept more than 50% risk.
Temporal constraint A CCQSP includes simple temporal constraints (STCs) (Dechter et al.,
1991), which impose upper and lower bounds on the duration of episodes and on the temporal
distances between two events in E.
min max ⟩ is a constraint, specifyDefinition 11. A simple temporal constraint τ = ⟨eSτ , eE
τ , bτ , bτ
ing that the duration from a start event eSτ to an end event eE
τ be in the real-valued interval
max ] ⊆ [0, +∞].
[bmin
,
b
τ
τ

Temporal constraints are represented diagrammatically by arcs between nodes, labeled with the
max ], or by labels over episodes. For example, the CCQSP shown in Figure 3
time bounds [bmin
τ , bτ
has four simple temporal constraints. One requires the time between e0 and e1 to be at most 30
time units. One requires the time between e1 and e2 to be at least 5 units and at most 10 units. One
requires the time between e2 and eE to be at most 40 time units. One requires the time between e0
and eE to be at most 60 time units.
A schedule s is feasible if it satisfies all temporal constraints in the CCQSP. The number of
feasible schedules is finite, since T is discrete and finite. We denote by SF the domain of feasible
schedules, which is formally defined as follows:
SF = {s ∈ T|E| | ∀τ ∈T

S
max
bmin
≤ ∆T {s(eE
},
τ
τ ) − s(eτ )} ≤ bτ

(12)

where |E| is the number of events in the CCQSP. The temporal duration is multiplied by the time
are real-valued time, while s is a set of discrete time steps in T.
and bmin
interval ∆T because bmin
τ
τ
2.4.4 O BJECTIVE F UNCTION
In this section, we formally define the objective function.
Definition 12. An objective function J : UN × XN × SF 7→ R is a real-valued function over the
nominal control sequence ū0:N −1 , the nominal state sequence x̄1:N , and the schedule s. We assume
that J is a convex function over x̄1:N and ū0:N −1 .
525

O NO , W ILLIAMS , & B LACKMORE

A typical example of an objective function is the quadratic sum of control inputs, which requires
the total control efforts to be minimized:
J(ū0:N −1 , x̄1:N , s) =

N
−1
∑

||ūt ||2 .

t=0

Another example is:
J(ū0:N −1 , x̄1:N , s) = s(eE ),

(13)

which minimizes the total plan execution time, by requiring that the end event eE of the qualitative
state plan be scheduled as soon as possible.
There is often a need to minimize the expectation of a cost function. Note that, in our case, the
expectation of a function over x1:N and u0:N −1 can be reduced to a function over ū0:N −1 because it
follows from (4)-(6) that the probability distributions of x1:N and u0:N −1 are uniquely determined
by ū0:N −1 and K t . In practice, it is often more convenient to express the objective function as
a function of ū0:N −1 and x̄1:N , rather than as a function of ū0:N −1 . Since x̄1:N are specified by
ū0:N −1 using (8), the two expressions are equivalent. The conversion from the expectation of a cost
function to a function over nominal values can be conducted a priori.
If there is no controller saturation, such a conversion can often be obtained in a closed form.
The conversion is particularly straight forward when the cost function is polynomial, since the
expectation is equivalent to a combination of raw moments, which can be readily derived from the
cumulants. Note that the third and higher cumulants of the Gaussian distribution are zero. Below we
show examples of the conversion regarding three commonly-used cost functions: linear, quadratic,
and the Manhattan norm.
E[xt ] = x̄t
E[xTt Qxt ]

(14)
= x̄Tt Qx̄t +
√
nx
∑

E[||xt ||1 ] =

σxt ,i

i=1

tr(QΣxt )
(
)
x̄2t,i
2
1 1
,
1 F1 − , , −
π
2 2 2σx2t ,i

(15)
(16)

where Q is a positive definite matrix, σxt ,i is the ith diagonal element of Σxt , and 1 F1 (·) is a
confluent hypergeometric function. All functions above are convex. The expectation of a function
of ut can also be transformed to a function of ūt in the same manner. Note that the second term
on the right hand side of (15) is a constant. Hence, minimizing x̄Tt Qx̄t yields the same solution as
minimizing E[xTt Qxt ].
When there is controller saturation, it is difficult to obtain the conversion in a closed-form due
to the nonlinearity of µU (·) in (4). In practice, we use an approximation that assumes no saturation.
Since our closed-loop control approach explicitly limits the probability of controller saturation to a
small probability (see Section 4.1.5 for the detail), the approximation error is trivial. This claim is
empirically validated in Section 7.2.4.
2.5 Definitions of Outputs
The output of the p-Sulu Planner is an optimal solution, which consists of an optimal control sequence u⋆0:N −1 ∈ UN and an optimal schedule s⋆ ∈ SF .
526

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Definition 13. The optimal solution is a pair ⟨u⋆0:N −1 , s⋆ ⟩. The solution satisfies all constraints
in the given CCQSP (Definition 7), the initial condition I, and the stochastic plant model M . The
solution minimizes the given objective function J(u0:N −1 , x̄1:N , s) (Definition 12).
2.6 Problem Statement
We now formally define the CCQSP planning problem.
Problem 1: CCQSP Planning Problem
Given a stochastic plant model M = ⟨A0:N −1 , B 0:N −1 , Σw0:N −1 ⟩⟩, an initial condition I =
⟨x̄0 , Σx0 ⟩, a CCQSP P = ⟨E, A, T , C⟩, and an objective function J(u0:N −1 , x̄1:N , s), a CCQSP
planning problem is to find an optimal solution ⟨u⋆0:N −1 , s⋆ ⟩ for M, I, P , and J.
We note that the p-Sulu Planner gives a near-optimal solution to Problem 1. The p-Sulu Planner
employs two approximations, namely risk allocation (Section 4.1.1) and risk selection (Section
5.1.1), for the sake of computational tractability. As a result, its solution is not strictly optimal in
general. However, we empirically show in Section 7 that the suboptimality due to risk allocation
and risk selection is significantly smaller than existing approximation methods.

3. Problem Encoding
This section encodes the CCQSP planning problem stated in the previous section into a mathematical programming problem. Sections 4 - 6 then address how to solve this form of mathematical
problem. Recall that we build our CCQSP planner, the p-Sulu Planner, in three spirals. We first
present the problem encoding of a general CCQSP planning problem with a non-convex state space
and a flexible schedule (Figure 4-(c)) in Subsection 3.1. Then we present the encodings of the two
special cases of the CCQSP planning problem in Subsections 3.2 and 3.3: one with a non-convex
state space and a fixed schedule (Figure 4-(b)), and one with a convex state space and a fixed schedule (Figure 4-(a)).
3.1 Encoding of a CCQSP Planning Problem with a Non-convex State Space and Flexible
Schedule
3.1.1 E NCODING OF F EASIBLE R EGIONS
In order to encode Problem 1 into a mathematical programming problem, the geometric constraint
in (11), xt ∈ Ra , must be represented by algebraic constraints. For that purpose, we approximate
the feasible state regions Ra by a set of half-spaces, each of which is represented by a linear state
constraint.
Figure 6 shows two simple examples. The feasible region of (a) is outside of the obstacle, which
is approximated by a triangle. The feasible region of (b) is inside of the pickup region, which is
again approximated by a triangle. Each feasible region is approximated by a set of linear constraints
as follows:
(a)

3
∨

hTi x ≤ g i ,

(b)

i=1

3
∧

hTi x ≥ g i .

i=1

We approximate the feasible regions so that the set of linear constraints is a sufficient condition of
the original state constraint xt ∈ Ra .
527

O NO , W ILLIAMS , & B LACKMORE

Figure 6: Approximate representation of feasible regions by a set of linear constraints

We assume that the set of linear state constraints that approximates a feasible region has been
reduced to conjunctive normal form (CNF) as follows:
∧

∨

hTa,k,j xt − ga,k,j ≤ 0,

(17)

k∈Ka j∈Ja,k

where Ka = {1, 2, · · · |Ka |} and Jc,i = {1, 2, · · · |Jc,i |} are sets of indices. By replacing xt ∈ Ra
in (11) by (17), a chance constraint c is encoded as follows:

Pr 

∧

∧

∧

∨


hTc,a,k,j xt − gc,a,k,j ≤ 0 ≥ 1 − ∆c .

(18)

a∈Ψc t∈Πa (s) k∈Ka j∈Ja,k

In order to simplify the notation, we merge indices a ∈ Ψc , t ∈ Πa (s),
∑ and k ∈ Ka into a new
index i ∈ Ic (s), where Ic (s) = {1, 2, · · · |Ic (s)|} and |Ic (s)| = |Ka | · a∈Ψc |Πa (s)|. We let ai ,
ki , and ti the indices that correspond to to the combined index i, and let hc,i,j = hc,ai ,ki ,j . Using
these notations, the three conjunctions of (18) are combined into one, and we obtain the following
encoding of a chance constraint:

Pr 

∧

∨


hTc,i,j xti − gc,i,j ≤ 0 ≥ 1 − ∆c .

(19)

i∈Ic (s) j∈Jc,i

The specification of chance constraints given in (19) requires that all |Ic (s)| disjunctive clauses of
state constraints must be satisfied with a probability 1 − ∆c . The i’th disjunctive clause of the c’th
chance constraint is composed of |Jc,i | linear state constraints.
3.1.2 CCQSP P LANNING P ROBLEM E NCODING
Using (3), (4), (5), (6), and (19), a CCQSP planning problem (Problem 1), which is solved in the
third spiral, is encoded as follows:
528

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Problem 2: General CCQSP Planning Problem
min
ū0:N −1 ,s
s.t.

J(u0:N −1 , x̄1:N , s)

(20)

s ∈ SF

(21)

xt+1 = At xt + B t µU (ut ) + wt ,

∀t ∈ T

−

(22)

−

ut = ūt + K t (xt − x̄t ), ∀t ∈ T


∧
∧ ∨
Pr 
hTc,i,j xti − gc,i,j ≤ 0 ≥ 1 − ∆c .
c∈C

(23)
(24)

i∈Ic (s) j∈Jc,i

x0 ∼ N (x̄0 , Σx0 ),

wt ∼ N (0, Σwt ),

∀t ∈ T−

(25)

Recall that SF , formally defined in (12), is the set of schedules that satisfy all temporal constraints in the given CCQSP. This CCQSP execution problem is a hybrid optimization problem over
both discrete variables s (schedule) and continuous variables u0:N −1 (control sequence). Note that
the temporal constraints within Problem 2 are solved in Section 6. A similar problem encoding is
also employed in the chance-constraint MDP proposed by Geibel and Wysotzki (2005). However,
our encoding differs from Geibel and Wysotzki in two respects: 1) we optimize not only the continuous control sequence u0:N −1 but also the discrete schedule s with temporal constraints; 2) we
allow joint chance constraints, which require the satisfaction of multiple state constraints for a given
probability. Problem 2 is solved in Section 6.
3.2 Encoding of a CCQSP Planning Problem with a Non-convex State Space and Fixed
Schedule
A restricted version of a CCQSP planning problem with a fixed schedule, which is solved in the
second spiral, is obtained by fixing s in Problem 2 as follows:
Problem 3: CCQSP Planning Problem with a Fixed Schedule
J ⋆ (s) = min
ū0:N −1
s.t.

J ′ (u0:N −1 , x̄1:N )

(26)

xt+1 = At xt + B t µU (ut ) + wt ,

∀t ∈ T−
−

ut = ūt + K t (xt − x̄t ), ∀t ∈ T


∧
∧ ∨
Pr 
hTc,i,j xti − gc,i,j ≤ 0 ≥ 1 − ∆c ,
c∈C

(27)
(28)
(29)

i∈Ic (s) j∈Jc,i

x0 ∼ N (x̄0 , Σx0 ),

wt ∼ N (0, Σwt ),

∀t ∈ T−

(30)

where J ⋆ (s) is the optimal objective value of the CCQSP Planning problem with the schedule fixed
to s. Note that the schedule s, which is a decision variable in Problem 2, is treated as a constant in
Problem 3. Therefore, the objective function J ′ is now a function of only control sequence and mean
529

O NO , W ILLIAMS , & B LACKMORE

state, since we have fixed the schedule. Since we assumed that J is a convex function regarding to
u0:N −1 and x̄1:N , J ′ is also a convex function. Section 5 solves Problem 3.
3.3 Encoding of a CCQSP Planning Problem with a Convex State Space and Fixed Schedule
A more restrictive version of a CCQSP planning problem with a fixed schedule and a convex state
space, which is solved in the first spiral, is obtained by removing the disjunctions in the chance
constraints in Problem 3 as follows:
Problem 4: CCQSP Planning Problem with a Fixed Schedule and a Convex State Space

min
ū0:N −1

J ′ (u0:N −1 , x̄1:N )

(31)

xt+1 = At xt + B t µU (ut ) + wt ,

∀t ∈ T−
−

ut = ūt + K t (xt − x̄t ), ∀t ∈ T


∧
∧
Pr 
hTc,i xti − gc,i ≤ 0 ≥ 1 − ∆c .
c∈C

(32)
(33)
(34)

i∈Ic (s)

x0 ∼ N (x̄0 , Σx0 ),

wt ∼ N (0, Σwt ),

∀t ∈ T−

(35)

Section 4 solves Problem 4.

4. CCQSP Planning with a Convex State Space and a Fixed Schedule
This section presents the solution methods to Problem 4, which is the CCQSP planning problem
with a convex state space and a fixed schedule, as shown in Figure 4-(a). When there are no obstacles
in the environment and the execution time steps to achieve time-evolved goals are fixed, the CCQSP
planning problem is reduced to a convex chance-constrained finite-horizon optimal control problem.
In our past work we presented the risk allocation approach, which conservatively approximates
the chance-constrained finite-horizon optimal control problem by a tractable convex optimization
problem (Ono & Williams, 2008a, 2008b; Blackmore & Ono, 2009). Although an optimal solution
to the approximated convex optimization problem is not an exactly optimal solution to the original convex chance-constrained finite-horizon optimal control problem, its suboptimality is significantly smaller than previous approaches. This section gives a brief overview of the risk allocation
approach, as well as the solution to the convex chance-constrained finite-horizon optimal control
problem.
4.1 Deterministic Approximation of Problem 4
Evaluating whether a joint chance constraint (34) is satisfied requires computing an integral of a
multivariate probability distribution over an arbitrary region, since the probability in (34) involves
multiple constraints. Such an integral cannot be obtained in a closed form. We address this issue by
decomposing the intractable joint chance constraint (34) into a set of individual chance constraints,
each of which involves only a univariate probability distribution. The key feature of an individual
530

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Goal

Goal
Walls

Walls

Nominal path
Safety margin

Start

Start

(a) Uniform risk allocation

(b) Optimal risk allocation

Figure 7: Risk allocation strategies on the racing car example

chance constraint is that it can be transformed into an equivalent deterministic constraint that can
be evaluated analytically.
4.1.1 R ISK A LLOCATION A PPROACH
The decomposition can be considered as an allocation of risk. Through the decomposition, the risk
bound of the joint chance constraint is distributed to the individual chance constraints. There are
many feasible risk allocations. The problem is to find a risk allocation that results in the minimum
cost. We offer readers an intuitive understanding of the risk allocation approach using the example
below.

Racing Car Example Consider a racing car example, shown in Figure 7. The dynamics of the
vehicle have Gaussian-distributed uncertainty. The task is to plan a path that minimizes the time to
reach the goal, with the guarantee that the probability of crashing into a wall during the race is less
than 0.1% (chance constraint). Planning the control sequence is equivalent to planning the nominal
path, which is shown as the solid lines in Figure 7. To limit the probability of crashing into the wall,
a good driver would set a safety margin, which is colored in dark gray in Figure 7, and then plan the
nominal path outside of the safety margin.
The driver wants to set the safety margin as small as possible in order to make the nominal path
shorter. However, since the probability of crashing during the race is bounded, there is a certain
lower bound on the size of the safety margin. Given this constraint, there are different ways of
setting a safety margin; in Figure 7(a) the width of the margin is uniform; in Figure 7(b) the safety
margin is narrow around the corner, and wide at the other places.
An intelligent driver would take the strategy of (b), since he knows that going closer to the wall
at the corner makes the path shorter, while doing so at the straight line does not. A key observation
here is that taking a risk (i.e., setting a narrow safety margin) at the corner results in a greater reward
(i.e. time saving) than taking the same risk at the straight line. This gives rise to the notion of risk
allocation. The good risk allocation strategy is to save risk when the reward is small, while taking
it when the reward is great. As is illustrated in this example, the risk allocation must be optimized
in order to minimize the objective function of a joint chance-constrained stochastic optimization
problem.
531

O NO , W ILLIAMS , & B LACKMORE

4.1.2 D ECOMPOSITION OF C ONJUNCTIVE J OINT C HANCE C ONSTRAINTS THROUGH R ISK
A LLOCATION
We derive the mathematical representation of risk allocation by reformulating each chance constraint over a conjunction of constraints into a conjunction of chance constraints. The reformulation
was initially presented by Prékopa (1999) and introduced to chance-constrained optimal control by
Ono and Williams (2008b). The concept of risk allocation was originally developed by Ono and
Williams (2008a). Let Ci be a proposition that is either true or false. Then the following lemma
holds:
Lemma 1.
Pr

[N
∧

]
Ci ≥ 1 − ∆

⇐

N
∧

∃δi ≥ 0,

i=1

Pr [Ci ] ≥ 1 − δi ∧

i=1

N
∑

δi ≤ ∆

i=1

Proof.
Pr

[N
∧

]
Ci ≥ 1 − ∆ ⇔ Pr

i=1

⇐

∧

[N
∨
i=1
N
∑

]
Ci ≤ ∆

(36)

[ ]
Pr Ci ≤ ∆

(37)

c∈C i=1

⇔ ∃δi ≥ 0
⇔ ∃δi ≥ 0

N
∧
i=1
N
∧
i=1

N
∑
[ ]
Pr Ci ≤ δi ∧
δi ≤ ∆
i=1

Pr [Ci ] ≥ 1 − δi ∧

N
∑

δi ≤ ∆.

(38)

i=1

The overline C is the negation of a literal C. We use the following Boole’s inequality to obtain (37)
from (36):
]
[N
N
∑
∨
Pr[Cc,i ].
Pr
Cc,i ≤
i=1

i=1

The following result immediately follows from Lemma 1 by substituting a linear constraint
hTc,i xti − gc,i ≤ 0 for Ci for each chance constraint c.
Corollary 1. The following set of constraints is a sufficient condition of the joint chance constraint
(34) in Problem 4:



∧ ∧
∑
[ T
]
∃δc,i ≥ 0
Pr hc,i xti − gc,i ≤ 0 ≥ 1 − δc,i ∧
δc,i ≤ ∆c
(39)


c∈C

i∈Ic (s)

i∈Ic (s)

The newly introduced variables δc,i represent the upper bounds on the probability of violating
each linear state constraint. We refer to them as individual risk bounds. Each individual risk bound,
532

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

δc,i , can be viewed as the amount of risk allocated to the i’th clause. The fact that δc,i is a bound on
probability implies that 0 ≤ δc,i ≤ 1. The second term of (39) requires that the total amount of risk
is upper-bounded to the original risk bound ∆c . Here we find an analogue to the resource allocation
problem, where the allocation of a resource is optimized with an upper bound on the total amount
of available resource. Likewise, the allocation of risk δc,i must be optimized in order to minimize
the cost. Therefore, we call this decomposition method a risk allocation.
4.1.3 C ONSERVATISM OF R ISK A LLOCATION A PPROACH
As mentioned previously, the risk allocation approach gives a conservative approximation of the
original chance constraint. This subsection evaluates the level of conservatism of the risk allocation
approach.
Let Pf ail be the true probability of failure, defined as the probability that a solution violates the
constraints (i.e., the left hand side of (34)). Since (39) is a sufficient but not necessary condition
for (34), Pf ail is smaller than or equal to the risk bound ∆ in general: ∆ ≥ Pf ail . Hence, the
conservatism introduced by risk allocation is represented as
∆ − Pf ail .
The best-case scenario for the risk allocation approach is when the violations of all constraints
are mutually exclusive, meaning that a solution that violates one constraint always satisfies all the
other constraints. In that case, (39) becomes a necessary and sufficient condition for (34) and hence,
risk allocation does not involve any conservatism. Therefore,
∆ − Pf ail = 0.
On the other hand, the worst-case scenario is when all constraints are equivalent, meaning that
a solution that violates one constraint always violates all the other constraints. In such a case,
∆ − Pf ail =

N −1
∆,
N

where N is the number of constraints.
Most practical problems lie somewhere between the best-case scenario and the worst-case scenario, but typically closer to the best-case than to the worst-case scenario. For example, if there are
two separate obstacles in a path planning problem, collisions with the two obstacles are mutually
exclusive events. Collision with an obstacle at one time step does not usually imply collisions at
other time steps. A rough approximation of such a real-world situation is to assume that the satisfaction of constraints are probabilistically independent. With such an assumption, the true probability
of failure is:
∏
∏
Pf ail =
Pr [qc,i (u) ≤ 0] ≤ 1 −
(1 − δi ),
i∈Ic

i∈Ic

where Ic is the set of the index of all state constraints. Note that δi ≤ ∆. Therefore, the conservatism introduced by risk allocation is at the second order of ∆:
∆ − Pf ail ∼ O(∆2 ).
For example, if ∆ = 1%, the true probability of failure is approximately Pf ail ∼ 0.99%. In most
practical cases, the users prefer to set very small risk bounds, typically less than 1%. In such cases,
the conservatism introduced by risk allocation becomes very small.
533

O NO , W ILLIAMS , & B LACKMORE

4.1.4 C ONVERSION TO D ETERMINISTIC C ONSTRAINTS
Each individual chance constraint in (39) only involves a single linear constraint. Furthermore,
assuming that there is no actuator saturation, xti has a Gaussian distribution with the covariance
matrix given by (9). Hence, hTc,i xti has a univariate Gaussian distribution. The following lemma
transforms an individual chance constraint into an equivalent deterministic constraint that involves
the mean of state variables, instead of the random state variables:
Lemma 2. The following two conditions are equivalent.
[
]
Pr hTc,i xti − gc,i ≤ 0 ≥ 1 − δc,i ⇔ hTc,i x̄ti − gc,i ≤ −mc,i (δc,i )
where
mc,i (δc,i ) = −

√

2hTc,i Σx,ti hc,i erf −1 (2δc,i − 1).

(40)

Note that erf−1 is the inverse of the Gauss error function and Σx,ti is the covariance matrix
of xti . This lemma holds because −mc,i (·) is the inverse of cumulative distribution function of
univariate, zero-mean Gaussian distribution with variance hTc,i Σx,ti hc,i .
4.1.5 R ISK A LLOCATION A PPROACH FOR THE C LOSED - LOOP C ONTROL P OLICY
When a close-loop control policy is employed (i.e., K t ̸= 0 in (6)), there is a risk of actuator
saturation. Since the nonlinearity of the function µU (·) in (5) makes the probability distribution
of xti non-Gaussian, mc,i (·) cannot be obtained by (40). Although it is theoretically possible to
derive mc,i (·) for non-Gaussian distributions, it is very difficult in our case since the inverse of the
cumulative distribution function of xti cannot be obtained in a closed-form.
Our solution to this issue is summarized in Lemma 3 below, which allows us to assume that xti
is Gaussian-distributed and hence to use (40), even if there is a possibility of actuator saturation.
This approach is enabled by imposing additional chance constraints that bound the risk of actuator
saturation as follows:
Pr [ut ∈ U] ≥ 1 − ϵt , ∀t ∈ T− ,
(41)
where ϵt is the bound on the risk of actuator saturation at time step t. Using the method presented
in Section 3.1.2, we approximate U by a polytope as follows:
∧
ut ∈ U ⇐⇒
hU,i ut − gU,i ≤ 0
i∈IU

Assuming that xti is Gaussian-distributed, we use Lemma 2 to transform (41) into deterministic
constraints on nominal control inputs as follows:
∧
∑
hU,i ūt − gU,i ≤ −mU,t,i (ϵt,i ) ∧
ϵt,i ≤ ϵt , ∀t ∈ T− ,
(42)
i∈IU

i∈IU

where
mU,t,i (ϵc,i ) = −

√

2hTU,i Σx,t hU,i erf −1 (2ϵc,i − 1).

The following lemma holds:
534

(43)

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Lemma 3. The following set of constraints is a sufficient condition of the joint chance constraint
(34) in Problem 4:

∧ ∧
∃δc,i ≥ 0, ϵt ≥ 0
hTc,i x̄ti − gc,i ≤ −mc,i (δc,i )

c∈C i∈Ic (s)

Tcmax

∑
∑ ∑
∧
δc,i +
ϵt,i ≤ ∆c

t=0 i∈IU
i∈Ic (s)
∧ ∧
∧
hU,i ūt − gU,i ≤ −mU,t,i (ϵt,i ),
(44)
t∈T− i∈IU

(45)
where mc,i (·) and mU,t,i are given by (40) and (43). Tcmax is the last time step that the episodes
associated with the chance constraint c are executed, given the schedule s:
Tcmax = max s(eE
a ).
a∈Ψc

Intuitively, the constraint (44) requires that, with probability 1 − ∆c , the episode constraints are
satisfied and the actuators do not saturate until all episodes associated with c are executed.
Proof. We consider two plants: M = ⟨A0:N −1 , B 0:N −1 , Σw0:N −1 , U⟩ and
M ′ = ⟨A0:N −1 , B 0:N −1 , Σw0:N −1 , Rnu ⟩, where U ⊂ Rnu is a compact convex set (see Definition
6). The difference between the two plants is that M has a possibility of actuator saturation, while M ′
does not. As a result, while the probability distribution of the state variables of M is non-Gaussian,
that of M ′ is Gaussian. Note that M and M ′ result in different probability distributions of xti and
ut . In order to explicitly show which plant model is considered, we use notations such as xM
ti and
′
M
ut in this proof.
We first consider M ′ . It follows from Lemmas 1 and 2 that:

 
  max

Tc

∧
∧
∧
′
M′




Pr 
hTc,i xM
−
g
≤
0
∧
u
∈
U
≥
1
−
∆
.
(44) =⇒
c,i
c
ti
t


c∈C

t=0

i∈Ic (s)

Let w0:N −1 := [w0 · · · wN −1 ]. We define a feasible disturbance set, Wc (v 0:N −1 , s) ⊂ RN nx , as
follows:


  max

Tc



∧
∧
′
′

M



hTc,i xM
u
∈
U
.
−
g
≤
0
∧
Wc (v 0:N −1 , s) := w0:N −1 ∈ RN nx  
c,i
ti
t



i∈Ic (s)

t=0

(46)
Then, by definition,


  max
Tc
∧
∧
′
′
∧
∈ U = Pr [w0:N −1 ∈ Wc (v 0:N −1 , s)] .
Pr 
hTc,i xM
uM
ti − gc,i ≤ 0
t
i∈Ic (s)

t=0

535

O NO , W ILLIAMS , & B LACKMORE

Next we consider M . Note that M and M ′ are identical as long as there is no actuator saturations
M
(i.e., uM
t ∈ U). Therefore, for a given w 0:N −1 ∈ Wc (v 0:N −1 , s), it follows from (46) that xt =
′
′
M
M
xM
t and ut = ut . Hence,

  max

Tc
∧
∧
∧

w0:N −1 ∈ Wc (v 0:N −1 , s) =⇒ 
hTc,i xM
uM
ti − gc,i ≤ 0
t ∈U .
t=0

i∈Ic (s)

Accordingly, for a given c ∈ C,


∧

hTc,i xM
Pr 
ti − gc,i ≤ 0
i∈Ic (s)



∧

≥ Pr 





Tcmax

∧
hTc,i xM
ti − gc,i ≤ 0

∧



uM
t ∈U

t=0

i∈Ic (s)

≥ Pr [w0:N −1 ∈ Wc (v 0:N −1 , s)]
  max


Tc
∧
∧
′
′
∧
uM
∈ U
= Pr 
hTc,i xM
t
ti − gc,i ≤ 0
t=0

i∈Ic (s)

≥ 1 − ∆c .
This completes the proof of Lemma 3
We note that Lemma 3 is a probabilistic extension of the closed-loop robust model predictive
control (RMPC) methods proposed by Acikmese, Carson III, and Bayard (2011) and Richards and
How (2006). These methods avoid the risk of actuator saturation by imposing tightened control
constraints on ūt . Since we consider stochastic uncertainty, we replace the constraint tightening by
chance constraints.
4.2 Convex Programming Solution to Problem 4
Using Lemma 3, we replace the stochastic optimization problem, Problem 4, with the deterministic
convex optimization problem:
Problem 5: Deterministic Approximation of Problem 4
min
ū1:N ,δc,i ≥0,ϵt,i ≥0
s.t.

J ′ (u1:N , x̄1:N )
∀t ∈ T− ,
∧ ∧

(47)

x̄t+1
T
hc,i x̄ti

= At x̄t + B t ut

(48)

− gc,i ≤ −mc,i (δc,i )

(49)

c∈C i∈Ic (s)

∧ ∧

t∈T−

hU,i ūt − gU,i ≤ −mU,t,i (ϵt,i )

(50)

i∈IU

∧ ∑

Tcmax

δc,i +

c∈C i∈Ic (s)

536

∑ ∑
t=0 i∈IU

ϵt,i ≤ ∆c .

(51)

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

It follows immediately from Corollaries 1 and 2 that a feasible solution to Problem 5 is always
a feasible solution to Problem 4. Furthermore, Blackmore and Ono (2009) showed that an optimal
solution to Problem 5 is a near-optimal solution to Problem 4. The following lemma guarantees the
tractability of Problem 5.
Lemma 4. Problem 5 is a convex optimization problem.
Proof. The inverse error function erf−1 (x) is concave for x. Since we assume in Section 2.4.3 that
∆c ≤ 0.5, the feasible ranges of δ and ϵ are upperbounded by 0.5. Since the safety margin function
mc,i (δc,i ) and mU,t,i (ϵt,i ) are convex for 0 < δc,i ≤ 0.5 and 0 < ϵt,i ≤ 0.5, the constraints (49)
and (50) are convex within the feasible region. All other constraints are also convex since they are
linear. Finally, the objective function is convex by assumption (Section 2.4.4). Therefore, Problem
5 is a convex optimization problem.
Since Problem 5 is a convex optimization problem, it can be solved by an interior point method
optimally and efficiently. This completes our first spiral, planning for CCQSPs with a fixed schedule
and convex constraints. In the next section we present a solution method for a non-convex problem
through a branch-and-bound algorithm, whose subproblems are convex problems.

5. CCQSP Planning with a Non-convex State Space
Next, we consider the second spiral, comprised of Problem 3 in Section 3.2, a variant of the CCQSP
planning problem that involves a fixed schedule and non-convex constraints, such as obstacles, as
shown in Figure 4-(b). Once again, this is encoded as a chance-constrained optimization problem,
but the addition of the obstacle avoidance constraints requires disjunctive state constraints. Hence,
the problem results in a non-convex, chance-constrained optimization. This section introduces a
novel algorithm, called Non-convex Iterative Risk Allocation (NIRA), that optimally solves a deterministic approximation of Problem 3.
The solution to a CCQSP planning problem with a non-convex state space is two-fold. In the
first step, described in Section 5.1, we obtain a deterministic approximation of Problem 3. In order
to handle disjunctive chance constraints, we develop an additional decomposition approach called
risk selection, which reformulates each chance constraint over a disjunction of constraints into a disjunction of individual chance constraints. Once the chance constraints in (29) are decomposed into
a set of individual chance constraints through risk allocation and risk selection, the same technique
as in Section 4.1.4 is used to obtain equivalent deterministic constraints. As a result, we obtain a
disjunctive convex programming problem (Problem 6 in Section 5.1.3).
The deterministic disjunctive convex programming problem is solved in the second step, described in Sections 5.2-5.4. We introduce the NIRA algorithm (Algorithm 1) that significantly reduces the computation time without making any compromise in the optimality of the solution. The
reduction in computation time is enabled by our new bounding approach, Fixed Risk Relaxation
(FRR). FRR relaxes nonlinear constraints in the subproblems of the branch-and-bound algorithm
with linear constraints. In many cases, FRR of the nonlinear subproblems is formulated as a linear
programming (LP) or approximated by an LP. NIRA obtains a strictly optimal solution of Problem
6 by solving the subproblems exactly without FRR at unpruned leaf nodes of the search tree, while
other subproblems are solved approximately with FRR in order to reduce the computation time.
537

O NO , W ILLIAMS , & B LACKMORE

5.1 Deterministic Approximation
As in Section 4, we first obtain a deterministic approximation of Problem 3.
5.1.1 R ISK S ELECTION A PPROACH
The deterministic approximation is obtained by decomposing the non-convex joint chance constraint
(29) into a set of individual chance constraints, through risk allocation and risk selection. We revisit
the race car example to explain the concept of risk selection intuitively.

Figure 8: In the racing car example, the risk selection approach guarantees the 0.1% risk bound for
both paths, and lets the vehicle choose the better one.

Racing Car Example We consider the example shown in Figure 8, where a vehicle with uncertain
dynamics plans a path that minimizes the time to reach the goal. The vehicle is allowed to choose
one of the two routes shown in Figure 8. We impose a chance constraint that limits the probability
of crashing into a wall during the mission to 0.1%.
The satisfaction of the chance constraint can be guaranteed by the following process. First, for
each of the routes, we find a safety margin that limits the probability of crash throughout the route
to 0.1% from the start to the goal. Then, we let the vehicle plan a nominal path that operates within
the safety margins. Since both routes have a 0.1% safety margin, the chance constraint is satisfied
no matter which route the vehicle chooses. Therefore, the vehicle can optimize the path by choosing
the route that results in a smaller cost. The optimization process can be considered as a selection of
risk; the vehicle is given two options as in Figure 8, routes (a) and (b), both of which involve the
same level of risk; then the vehicle selects the one that results in less cost. Hence, we name this
decomposition approach as the risk selection.
5.1.2 D ECOMPOSITION OF C ONJUNCTIVE J OINT C HANCE C ONSTRAINT THROUGH R ISK
S ELECTION
In this subsection, we derive the mathematical representation of risk selection. Let Ci be a proposition that is either true or false. Then the following lemma holds:

538

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Lemma 5.
Pr

[N
∨

]
Ci ≥ 1 − ∆ ⇐

i=1

N
∨

Pr [Ci ] ≥ 1 − ∆

i=1

Proof. The following inequality always holds:
∀i

Pr

[N
∨

]
Ci ≥ Pr [Ci ] .

(52)

i=1

Hence,

Pr

[N
∨

]
Ci ≥ 1 − ∆ ⇐ ∃i Pr [Ci ] ≥ 1 − ∆ ⇔

i=1

N
∨

Pr [Ci ] ≥ 1 − ∆.

(53)

i=1

The following corollary follows immediately from Lemmas 3 and 5.
Corollary 2. The following set of constraints is a sufficient condition of the disjunctive joint chance
constraint (29) in Problem 3:

∃δc,i ≥ 0, ϵt ≥ 0


∧ ∧
c∈C



∧

∨

i∈Ic (s) j∈Jc,i

∑

Tcmax

δc,i +

∧ ∧

t∈T−

∑ ∑
t=0 i∈IU

i∈Ic (s)

∧

hTc,i,j xti − gc,i,j ≤ mc,i (δc,i )

ϵt,i ≤ ∆c





5

hU,i ūt − gU,i ≤ −mU,t,i (ϵt,i ).

(54)

i∈IU

Note that the resulting set of constraints (54) is a sufficient condition for the original chance
constraint (29). Therefore, any solution that satisfies (54) is guaranteed to satisfy (29). Furthermore,
although (54) is a conservative approximation of (29), the conservatism introduced by risk selection
is generally small in many practical applications. This claim is empirically validated in Section
7.2.3.
5.1.3 D ETERMINISTIC A PPROXIMATION OF P ROBLEM 3
Using Corollary 2, the non-convex fixed-schedule CCQSP planning problem (Problem 3) is approximated by the following deterministic convex optimization problem. For later convenience, we
label each part of the optimization problem as O (objective function), M (plant model), C (chance
constraints on states), D (chance constraints on control inputs), and R (risk allocation constraint).
539

O NO , W ILLIAMS , & B LACKMORE

Problem 6: Deterministic Approximation of Problem 3
min
ū1:N ,δc,i ≥0,ϵt,i ≥0
s.t.

(O :)

J ′ (u1:N , x̄1:N )

(55)

(M :) ∀t ∈ T− , x̄t+1 = At x̄t + B t ut
∧ ∧ ∨
(C :)
hTc,i,j x̄ti − gc,i,j ≤ −mc,i,j (δc,i )

(56)
(57)

c∈C i∈Ic (s) j∈Jc,i

(D :)

∧ ∧

hU,i ūt − gU,i ≤ −mU,t,i (ϵt,i )

(58)

t∈T− i∈IU

(R :)

∧ ∑

Tcmax

δc,i +

c∈C i∈Ic (s)

∑ ∑

ϵt,i ≤ ∆c .

(59)

t=0 i∈IU

It follows immediately from Corollary 2 that an optimal solution to Problem 6 is guaranteed
to be a feasible solution to the original problem with regard to satisfying the chance constraints
(Problem 3). Furthermore, we empirically demonstrate in Section 7.2.3 that it is a near-optimal
solution to Problem 3 in our applications.
5.2 NIRA: Branch and Bound-Based Solution to Problem 6
We next present the Non-convex Iterative Risk Allocation (NIRA) algorithm. Recall that NIRA
optimally solves Problem 6 by a branch-and-bound algorithm. The standard branch-and-bound
solution to problems involving disjunctive nonlinear constraints, such as those in Problem 6, is
to use a bounding approach in which the nonlinear convex relaxed subproblems are constructed
by removing all non-convex constraints below the corresponding disjunction. This approach was
used by Balas (1979) and Li and Williams (2005) for a different problem known as disjunctive linear
programming, whose subproblems are LPs instead of convex programmings. However, although the
standard branch-and-bound algorithm is guaranteed to find a globally optimal solution to Problem 6,
its computation time is slow because the algorithm needs to solve numerous nonlinear subproblems
in order to compute relaxed bounds.
Our new bounding approach, Fixed Risk Relaxation (FRR), addresses this issue by computing
lower bounds more efficiently. We observe that the relaxed subproblems are nonlinear convex optimization problems. FRR relaxes the nonlinear constraints to linear constraints. Particularly, when
the objective function is linear, an FRR of a subproblem (Problem 8) is an LP, which can be very
efficiently solved. The optimal objective value of an FRR of a subproblem is a lower bound of the
optimal objective value of the original subproblem.
NIRA solves the FRRs of the subproblems in order to efficiently obtain the lower bounds, while
solving the original subproblems exactly without relaxation at unpruned leaf nodes in order to obtain
an exact optimal solution. As a result, NIRA achieves significant reduction in computation time,
without any loss in optimality.
5.2.1 T HE NIRA A LGORITHM OVERVIEW
Algorithm 1 shows the pseudocode of the NIRA algorithm. Its input is the deterministic approximation of a non-convex chance-constrained optimal control problem (Problem 6), which is a five-tuple
540

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Algorithm 1 Non-convex Iterative Risk Allocation (NIRA) algorithm
function NIRA(problem) returns optimal solution to Problem 6
1: Set up queue as a FILO queue
2: Incumbent ← ∞
3: rootSubproblem ← obtainRootSubproblem(problem)
4: queue ← rootSubproblem
5: while queue is not empty do
6:
subproblem ← the last entry in queue
7:
Remove subproblem from queue
8:
lb ← obtainLowerBound(subproblem)
9:
if lb ≤ Incumbent then
10:
if c = |C| ∧ i = |Ic (s)| then
11:
(J, Ū ) ← Solve(subproblem)
12:
if J ⋆ < Incumbent then
⋆
13:
Incumbent ← J, Ū ← Ū //Update the optimal solution
14:
end if
15:
else
16:
i←i+1
17:
if i > |Ic (s)| then
18:
c ← c + 1, i ← 1
19:
end if
20:
for j ∈ Jc,i do
21:
newSubproblems ← Expand(subproblem,problem,c,i,j)
22:
Add newSubproblems to queue
23:
end for
24:
end if
25:
end if
26: end while
⋆
27: return Ū

⟨O, M, C, D, R⟩, as well as a fixed schedule s. Its output is an optimal nominal control sequence
⋆
Ū := [ū⋆0 · · · ū⋆N −1 ].
Each node of the branch-and-bound search tree corresponds to a subproblem that is a convex
chance-constrained optimization problem (Problem 5). We use a FILO queue to store subproblems
so that the search is conducted in a depth-first manner (Line 1). At each node, the corresponding
subproblem is solved to obtain a lower bound of the objective value of all subsequent subproblems
(Line 8). The details of the bounding approaches are explained in Subsection 5.4. If the lower bound
is larger than the incumbent, the algorithm prunes the branch. Otherwise, the branch is expanded
(Line 21). If a branch is expanded to the leaf without being pruned, subproblems are solved exactly
(Line 11). Subsection 5.3 explains our expansion procedure in detail. The NIRA algorithm always
⋆
results in a globally optimal solution to Problem 6, since the solution Ū is obtained by solving the
subproblems at leaf nodes exactly. The next two subsections introduces the branching and bounding
methods.
541

O NO , W ILLIAMS , & B LACKMORE

5.3 Branching
This subsection explains how NIRA constructs the root subproblem (Line 3 of Algorithm 1), as
well as how it expands the nodes (Line 21 of Algorithm 1). The root subproblem is a convex
optimal CCQSP planning problem without any chance constraints. When a node is expanded, the
subproblems of its children nodes are constructed by adding one constraint in a disjunction to the
subproblem of the parent node. In order to simplify notations, we let Cc,i,j represent each individual
chance constraint (57) in Problem 6:
{
T rue (if hTc,i,j − gc,i,j x̄ti ≤ −mc,i,j (δc,i ))
Cc,i,j :=
F alse (otherwise).
5.3.1 WALK - THROUGH E XAMPLE
We first present a walk-through example to intuitively explain the branching procedure. The example is an instance of Problem 6, which involves four individual chance constraints:
∧
∨
hT1,i,j x̄ti − g1,i,j ≤ −m1,i,j (δ1,i )
(60)
i∈{1,2} j∈{1,2}

Using this notation defined above, the set of individual chance constraints (57) is represented as
follows:
(C1,1,1 ∨ C1,1,2 ) ∧ (C1,2,1 ∨ C1,2,2 )

(61)

Figure 9-(a) shows a tree obtained by dividing the original problem into subproblems sequentially.
The subproblems corresponding to the tree’s four leaf nodes (Nodes 4-7 in Figure 9-(a)) exhaust all
conjunctive (i.e., convex) combinations among the chance constraints (61). On the other hand, the
subproblems corresponding to the three branch nodes (Nodes 1-3 in Figure 9-(a)) involve disjunctive
(i.e., nonconvex) clauses of chance constraints. We relax such non-convex subproblems to convex
subproblems by removing all clauses that contain disjunctions in order to obtain the search tree
shown in Figure 9-(b).

Figure 9: Branch-and-bound search tree for a sample disjunctive convex programming problem
(Problem 6) with constraints (60). (a) Tree of non-convex subproblems, (b) Tree of relaxed convex subproblems.

542

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

The non-convex problem (Problem 6) can be optimally solved by repeatedly solving the relaxed
convex subproblems using the algorithms presented in Section 4. The following subsections introduce the algorithms that construct a search tree with relaxed convex subproblems, such as the one
in Figure 9-(b).
5.3.2 R ELAXED C ONVEX S UBPROBLEM
The formulation of the relaxed convex subproblems is given in Problem 7. We represent the index
j as j(c, i) since the convex relaxation chooses only one disjunct for each disjunction specified by
⋆ the optimal objective value of the relaxed
(c, i). Let Ic be a set of indices for i. We denote by JSP
subproblem.
Problem 7: Convex Relaxed Subproblem of NIRA

⋆
JSP
=

min
ū1:N ,δc,i ≥0,ϵt,i ≥0
s.t.

(O :) J ′ (u1:N , x̄1:N )
(M :)
(C :)

∀t ∈ T− , x̄t+1 = At x̄t + B t ut
∧ ∧
hTc,i,j(c,i) x̄ti − gc,i,j(c,i) ≤ −mc,i,j(c,i) (δc,i ) (62)
c∈C i∈Ic

(D :)

∧ ∧

hU,i ūt − gU,i ≤ −mU,t,i (ϵt,i )

(63)

t∈T− i∈IU

(R :)

∧∑

Tcmax

δc,i +

c∈C i∈Ic

∑ ∑

ϵt,i ≤ ∆c .

(64)

t=0 i∈IU

Note that Problem 7 is identical to Problem 5. Hence, the algorithms introduced in Section 4
can be used to solve the relaxed subproblems.
5.3.3 C ONSTRUCTION OF ROOT S UBPROBLEM
The root subproblem is a special case of Problem 7 above with Ic being an empty set for all c ∈ C.
The function presented in Algorithm 2 is used in Line 3 of the NIRA algorithm (Algorithm 1)
to construct the root subproblem of the branch-and-bound tree. Note that, in Algorithm 2, we
use an object-oriented notation, such as subproblem.O, to represent the objective function O of
subproblem. The resulting root subproblem is as follows:
5.3.4 E XPANSION OF S UBPROBLEMS
In order to create a child subproblem of a subproblem, the function described in Algorithm 3 is
used in Line 21 of the NIRA algorithm (Algorithm 1). It adds the individual chance constraint
specified by the indices (c, i, j) as a conjunct. Note that the resulting child subproblem is still a
convex optimization, because the individual chance constraint is added conjunctively. The NIRA
algorithm (Algorithm 1) enumerates children nodes for all disjuncts in Jc,i (Lines 20-23).
543

O NO , W ILLIAMS , & B LACKMORE

Algorithm 2 Construction of the root subproblem of NIRA
function obtainRootSubproblem(problem) returns root subproblem
1: rootSubproblem.O ← problem.O
2: rootSubproblem.M ← problem.M
3: rootSubproblem.D ← problem.D
4: for c ∈ C do
5:
rootSubproblem.Ic ← ϕ ∑ max ∑
Tc
6:
rootSubproblem.Rc .lhs ← t=0
i∈IU ϵt,i
7:
rootSubproblem.Rc .rhs ← problem.Rc .rhs
8: end for
9: return rootSubproblem
Algorithm 3 Expansion of a subproblem of NIRA
function
Expand(subproblem, problem, c, i, j)
lem
1: subproblem.Ic ← subproblem.Ic ∪ i
2: subproblem.Rc .lhs ← subproblem.Rc .lhs + δc,i
3: return subproblem

returns

a

child

subprob-

5.4 Bounding
In this subsection, we present two implementations of the obtainLowerBound function in Line 8
of Algorithm 1. The first one uses the optimal solution of the convex subproblems (Problem 7) as
lower bounds. This approach typically results in extensive computation time. The second one solves
an LP relaxation of the convex subproblems, called fixed risk relaxation (FRR). FRR dramatically
reduces the computation time compared to the first implementation. The NIRA algorithm employs
the second implementation.
5.4.1 S IMPLE B OUNDING
Algorithm 4 shows the most straightforward way to obtain lower bounds. It simply solves the
convex relaxed subproblems (Problem 7) using the methods presented in Section 4.2. The optimal
objective value of a relaxed subproblem gives a lower bound of the optimal objective value of all the
subproblems below it. For example, the optimal solution of the relaxed subproblem at Node 2′ in
Figure 9-(b) gives a lower bound of the objective value of the subproblems at Nodes 4 and 5. This
is because the constraints of the relaxed subproblems are always a subset of the constraints of the
subproblems below. Note that optimization problems are formulated as minimizations.
However, despite the simplicity of this approach, its computation time is slow because the algorithm needs to solve a myriad of subproblems. For example, a simple path planning problem with
Algorithm 4 A simple implementation of the obtainLowerBound function in Line 8 of Algorithm 1
function obtainLowerBound-Naive(subproblem) returns a lower bound
1: Solve subproblem using algorithms presented in Section 4.2
2: return the optimal objective value

544

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

ten time steps and one rectangular obstacle requires the solution of 410 = 1, 048, 576 in the worst
case, although the branch-and-bound process often significantly reduces the number of subproblems
to be solved. Moreover, the subproblems (Problem 7) are nonlinear convex optimization problems
due to the nonlinearity of mc,i,j and mU,t,i in (62) and (63). A general nonlinear optimization problem requires significantly more solution time than more specific classes of optimization problems,
such as linear programmings (LPs) and quadratic programmings (QPs).
5.4.2 F IXED R ISK R ELAXATION
Our new relaxation approach, fixed risk relaxation (FRR), addresses this issue. FRR linearizes
the nonlinear constraints (62) and (63) in Problem 7 by fixing all the individual risk allocations,
δc,i and ϵt,i , to their upper bound ∆. When the objective function is linear, an FRR is an LP.
An FRR with a convex piecewise linear objective function can also be reformulated as an LP by
introducing slack variables (See Section 7.1.1 for an example.). A general convex objective function
can be approximated by a convex piecewise linear function. Hence, in many cases, the FRRs of
subproblems result in LPs, which can be solved very efficiently. The fixed risk relaxation of Problem
7 is as follows:
Problem 8: Fixed Risk Relaxation of Problem 7

JF⋆ RR = min
ū1:N
s.t.

J ′ (u1:N , x̄1:N )
∀t ∈ T− ,
∧ ∧

x̄t+1 = At x̄t + B t ut
hTc,i x̄ti − gc,i ≤ −mc,i,j(c,i) (∆c )

(65)

c∈C i∈Ic (s)

∧ ∧

hU,i ūt − gU,i ≤ −mU,t,i (∆c )

(66)

t∈T− i∈IU

Note that the nonlinear terms in (62) and (63), mc,i,j and mU,t,i , become constant by fixing δc,i
and ϵt,i to ∆c , which is a constant. The optimal objective value of the FRR provides a tightest lower
bound among the linear relaxations of constraints (62) and (63). The following lemmas hold:
Lemma 6. Problem 8 gives a lower bound to the optimal objective value of Problem 7:
⋆
JF⋆ RR ≤ JSP

Proof. mc,i,j (·) and mU,t,i (·) are monotonically decreasing functions. Since δc,i ≤ ∆c and ϵt,i ≤
∆c , all individual chance constraints (65) and (66) of the Fixed Risk Relaxation are less stricter
than the first conjunct of (62) and (63). Therefore, the cost of the optimal solution of the Fixed Risk
Relaxation is less than or equal to the original subproblem.
Lemma 7. FRR gives the tightest lower bound among the linear relaxations of constraints (62) and
(63).
Proof. The linear relaxation of (62) and (63) becomes tighter by fixing δc,i and ϵt,i to a lesser value.
However, setting δc,i and ϵt,i to values less than ∆c may exclude feasible solutions, such as the one
545

O NO , W ILLIAMS , & B LACKMORE

Algorithm 5 An FRR implementation of the obtainLowerBound function in Line 8 of Algorithm 1
function obtainLowreBound-FRR(subproblem) returns lower bound
1: for ∀(c, i, j) in subproblem.C do
2:
subproblem.Cc,i,j .rhs ← −mc,i,j (∆c ) //Apply fixed risk relaxation
3: end for
4: for ∀(t, i) do
5:
subproblem.Dt,i .rhs ← −mU,t,i //Apply fixed risk relaxation
6: end for
7: Remove subproblem.R
8: Solve subproblem using an LP solver
9: return the optimal objective value

that sets δc,i = ∆c for some (c, i). Hence, FRR is the tightest linear relaxation of (62) and (63),
resulting in the tightest lower bound.
Note that the optimal solution of Fixed Risk Relaxation (Problem 8) is typically an infeasible
solution to Problem 7, since setting δc,i = ϵt,i = ∆c violates the constraint (64).
Algorithm 5 implements the fixed risk relaxation. The LP relaxation is solved by an LP solver,
and its optimal objective value is returned.
This completes our second spiral, planning for CCQSPs with a fixed schedule and nonconvex
constraints. In the next section, we turn to our final spiral, which involves flexible temporal constraints.

6. CCQSP Planning with a Flexible Schedule
This section presents the complete p-Sulu Planner, which efficiently solves the general CCQSP
planning problem with a flexible schedule and a non-convex state space (Problem 2 in Section
3.1.2). The problem is to find a schedule of events s that satisfies simple temporal constraints, as
well as a nominal control sequence ū0:N −1 that satisfies the chance constraints and minimizes cost.
Our approach is to first generate a feasible schedule and then to extend it to a control sequence for
that schedule, while iteratively improving the candidate schedules using branch-and-bound.
We build the p-Sulu Planner upon the NIRA algorithm presented in the previous section. Recall
that NIRA optimizes the nominal control sequence ū0:N −1 given a fixed schedule s. The p-Sulu
Planner uses NIRA as a subroutine that takes a schedule s as an input, and outputs the optimal
objective value as well as an executable control sequence. We denote the optimal objective value
for a given schedule s as J ⋆ (s). Using this notation, the CCQSP planning problem with a flexible
schedule (Problem 2) can be rewritten as a schedule optimization problem as follows:
min J ⋆ (s).

s∈SF

(67)

Recall that the domain of feasible schedules SF (Definition 11) is a finite set, since we consider a
discretized, finite set of time steps T (see Section 2.1). Hence, the schedule optimization problem
(67) is a combinatorial constraint optimization problem, where the constraints are given in the form
of simple temporal constraints.
546

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Algorithm 6 The the p-Sulu Planner
function pSulu(ccqsp) returns optimal schedule and control sequence
1: Incumbent = ∞
2: Set up queue as a FILO queue
3: Eσ0 = {e0 }, σ0 (e0 ) = 0
//initialize the partial schedule
4: queue ← ⟨Eσ0 , σ0 ⟩
5: while queue is not empty do
6:
⟨Eσ , σ⟩ ← the last entry in queue
7:
Remove ⟨Eσ , σ⟩ from queue
8:
[J ⋆ , u0:N −1 ] ← obtainLowerBound(ccqsp, Eσ , σ)
9:
if J ⋆ < Incumbent then
10:
if Eσ = E then
11:
Incumbent ← J ⋆ , OptCtlSequence ← u0:N −1 , OptSchedule ← σ
12:
else
13:
expand(ccqsp, queue, e, Eσ , σ)
14:
end if
15:
end if
16: end while
17: return OptCtlSequence, OptSchedule

6.1 Algorithm Overview
Our solution approach is again to use a branch-and-bound algorithm. In the branch-and-bound
search, the p-Sulu Planner incrementally assigns an execution time step to each event in order to
find the schedule that minimizes J ⋆ (s) in (67). The objective function is evaluated by solving the
fixed schedule CCQSP planning problem using the NIRA algorithm. Although the combination of
the two branch-and-bound searches in the p-Sulu Planner and NIRA are equivalent to one unified
branch-and-bound search in practice, we treat them separately for ease of explanation.
As shown in Figure 12, the branch-and-bound algorithm searches for an optimal schedule by
incrementally assigning execution time steps to each event in a depth-first manner. Each node of the
search tree corresponds to a partial schedule (Definition 2), which assigns execution time steps to a
subset of the events included in the CCQSP. The partial schedule at the root node only involves an
assignment to the start node e0 . The tree is expanded by assigning an execution time step to one new
event at a time. For example, the node σ(e1 ) = 2 in Figure 12-(a) represents a partial schedule that
assigns the execution time step t = 0 to the event e0 and t = 2 to e1 , while leaving eE unassigned.
The the p-Sulu Planner obtains the lower bound of the objective function value J ⋆ (s) by solving
a CCQSP planning problem with a partial schedule that can be extended to s. The the p-Sulu
Planner minimizes the search space by dynamically pruning the domain through forward checking.
More specifically, after an execution time is assigned to an event at each iteration of the branch-andbound search, the the p-Sulu Planner runs a shortest-path algorithm to tighten the real-valued upper
and lower bounds of the execution time step of unassigned events according to the newly assigned
execution time step.
Algorithm 6 shows the pseudocode of the algorithm. At each node of the search tree, a fixedschedule CCQSP planning problem is solved with the given partial schedule. If the node is at the
547

O NO , W ILLIAMS , & B LACKMORE

leaf of the tree and the optimal objective value is less than the incumbent, the optimal solution is
updated (Line 11). If the node is not at the leaf, the optimal objective value of the corresponding
subproblem is a lower bound for the optimal objective value of subsequent nodes. If the lower
bound is less than the incumbent, the node is expanded by enumerating the feasible execution time
assignments to an unassigned event (Line 13). Otherwise, the node is not expanded, and hence
pruned. Details of this branch-and-bound process are described in later subsections.

Figure 10: (a) An example of CCQSP; (b) a plan that satisfies the CCQSP in (a)

Figure 11: (a) The directed distance graph representation of the CCQSP in Figure 10-(a); (b) the dgraph of (a), which shows the shortest distances between nodes; (c) the updated d-graph
after the execution time t = 2 is assigned to the event e1 .

(e0) = 0
(e1)

(e0) = 0

0
1

2

(e1) = 2

3

(eE)

(eE)
(a)

0
1

2
4

3
5

(b)

Figure 12: Branch-and-bound search over a schedule s. We assume that the time interval is ∆T =
1.0. (a)
The node σ(e0]) = 0 is expanded; De1 (σ) = {1, 2, 3} given σ(e0 ) = 0,
[ max
since de (σ), dmin
1) = 2
e (σ) = [0.8, 3.9] from Figure 11-(b); (b) the
] is
[ node σ(emin
(σ)
=
(σ),
d
expanded; DeE (σ) = 4, 5 given σ(e0 ) = 0 and σ(e1 ) = 2, since dmax
e
e
[3.6, 5.5] from Figure 11-(c).

548

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Walk-through example We present a walk-through example to give readers insight into the solution process. We consider a CCQSP shown in Figure 10-(a). The CCQSP specifies a mission to
go through a waypoint A and get to the goal region B while avoiding the obstacle C, as shown in
Figure 10-(b). We assume that the time interval is ∆T = 1.0.
Figures 11 and 12 illustrate the solution process. The the p-Sulu Planner algorithm is initialized
by assigning the execution time 0 to the start event e0 . Figure 11-(a) is the distance graph representation of the simple temporal constraints (Dechter, 2003) of the CCQSP. Note that a simple chance
constraint is equivalently represented as a pair of inequality constraints as follows:
s(e) − s(e′ ) ∈ [l, u] ⇐⇒ s(e) − s(e′ ) ≤ u ∧ s(e′ ) − s(e) ≤ −l.
The two inequality constraints are represented by two directional edges between each two nodes in
the distance graph. The the p-Sulu Planner runs an all-pair shortest-path algorithm on the distance
graph to obtain the d-graph shown in Figure 11-(b). A d-graph is a completed distance graph
where each edge is labeled by the shortest-path length. The d-graph represents the tightest temporal
constraints. Then the algorithm enumerates the feasible execution-time assignments for the event
e1 using the d-graph. According to the d-graph, the execution time for the event e1 must be between
0.8 and 3.9. Since we consider discrete time steps with the time interval ∆T = 1.0, the feasible
execution time steps for e1 are {1, 2, 3}. The idea behind enumerating all feasible execution time
steps is to assign an event, and thus to tighten the bounds of all unassigned events in order to ensure
feasibility.
At the node σ(e1 ) = 1, the the p-Sulu Planner solves the FRR of the fixed-schedule CCQSP
planning problem only with the “End in A” episode and the execution schedule σ(e1 ) = 1. In other
words, it tries to find the optimal path that goes through A at t = 1, but neglects the goal B and
obstacle C. If a solution exists, its optimal cost gives a lower bound on the objective value of all
feasible paths that go through A at t = 1. Assume here that such a solution does not exist. Then,
the the p-Sulu Planner prunes the node σ(e1 ) = 1, and goes to the next node σ(e1 ) = 2. It solves
the FRR of the corresponding fixed-schedule subproblem to find the best path that goes through A
at t = 2. Assume that the the p-Sulu Planner finds a solution. Then, the the p-Sulu Planner expands
the node in the following process. First, it fixes the execution time σ(e1 ) = 2 in the d-graph, and
runs a shortest-path algorithm in order to tighten the temporal constraints (11-(c)). Then the the pSulu Planner uses the updated d-graph to enumerate the feasible execution-time assignments for the
event eE , which are {4, 5}. It visits both nodes and solves the fixed-schedule subproblems exactly
with all episodes and a fully assigned schedule. For example, at the node σ(eE ) = 5, it computes
the best path that goes through A at t = 2 and reaches B at t = 5 while avoiding the obstacle C, as
shown in Figure 10-(b). Assume that the optimal objective values of the subproblems are 10.0 for
σ(eE ) = 4 and 8.0 for σ(eE ) = 5. The algorithm records the solution with σ(eE ) = 5 and its cost
8.0 as the incumbent.
The algorithm then backs up and visits the node σ(e1 ) = 3, where a relaxed subproblem with
only the “End in A” episode is solved to obtain the lower bound of the objective value of subsequent
nodes. The lower bound turns out to be 9.0, which exceeds the incumbent. Therefore, the branch is
pruned. Since there are no more nodes to expand, the algorithm is terminated, and the incumbent
solution is returned.
549

O NO , W ILLIAMS , & B LACKMORE

Algorithm 7 Implementation of expand function in Line 13 of Algorithm 6
function expand(ccqsp, queue, e, Eσ , σ)
1: Fix the distance between e0 and e to σ(e)∆T on the d-graph of ccqsp
2: Update the d-graph by running a shortest-path algorithm
3: Choose e′ from E\Eσ
//choose an unassigned event
4: Eσ ′ := Eσ ∪ e′
max
5: De′ (σ) := { t ∈ T | dmin
e′ (σ) ≤ t∆T ≤ de′ (σ)}
6: for t in De′ (σ) do
{
σ(e) (e ∈ Eσ )
′
7:
σ (e) :=
//update the partial schedule
t
(e = e′ )
8:
queue ← ⟨Eσ′ , σ ′ ⟩
9: end for
6.2 Branching
Algorithm 7 outlines the implementation of the expand() function in Algorithm 6. It takes a partial
schedule σ as an input, and adds to the queue a set of schedules that assign an execution time step to
an additional event e′ . In other words, the domain of the newly added schedules Eσ′ has one more
assigned event than the domain of the input partial schedule Eσ . The details of Algorithm 7 are
explained in the following parts of this subsection.
6.2.1 E NUMERATION OF F EASIBLE T IME S TEP A SSIGNMENTS USING D - GRAPH
When enumerating all feasible time steps, the simple temporal constraints must be respected. To
accomplish this, we use a d-graph to translate the bounds on the durations between two events into
the bounds on the execution time step of each event. It is shown by Dechter et al. (1991) that the
set of feasible execution times for an event e is bounded by the distance between e and e0 on the dgraph. A d-graph is a directed graph, where the weights of the edges represent the shortest distances
between nodes, as in Figure 11-(b). In order to obtain the d-graph representation, we first translate
the simple temporal constraints into a directed distance graph, as in Figure 11-(a). The weight of an
edge between two nodes (events) corresponds to the maximum duration of time from the origin node
to the destination node, as specified by the corresponding simple temporal constraint. The distance
takes a negative value to represent lower bounds. The d-graph (Figure 11-(b)) is obtained from the
distance graph (Figure 11-(a)) by running an all-pair shortest-path algorithm (Dechter et al., 1991).
Forward checking over a d-graph The the p-Sulu Planner algorithm incrementally assigns an
execution time step to each event, as explained in the walk-through example. The p-Sulu Planner
minimizes the search space through forward checking using the d-graph. As in forward checking
methods of Constraint Programming, our method prunes all values of unassigned variables (i.e.,
execution times of an unassigned event) that violate simple temporal constraints. What is different
here from normal forward checking is that no back tracking is performed, due to decomposability of
d-graph. The forward checking is conducted in the following process. Once an execution time step
t is assigned to an event e (i.e., σ(e) = t), the distance from e0 to e is fixed to t∆T , and the distance
from e to e0 is fixed to −t∆T on the distance graph (Line 1 of Algorithm 7). Recall that t is an
index of discretized time steps with a fixed interval ∆T , while the temporal bounds are given as
real-valued times (Section 2.1). We then run a shortest-path algorithm to update the d-graph (Line
550

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

2). Given a partial schedule σ, we denote the updated shortest distance from the start event e0 to e′
′
min
on the d-graph by dmax
e′ (σ), and the distance from e to e0 by de′ (σ).
For example, the execution time 2 is assigned to the event e1 in Figure 11-(c) (i.e., σ(e1 ) = 2),
so the distance between e0 and e1 is fixed to 2 and the distance in the opposite direction is fixed
to −2. Then we run a shortest-path algorithm again to update the d-graph. As a result, we obtain
min
updated distances dmax
eE (σ) = 5.5 and deE (σ) = −3.6.
max
Dechter et al. (1991) showed that de′ (σ) corresponds to the upper bound of the feasible execution time for an unassigned event e′ , while dmin
eE (σ) corresponds to the negative of the lower bound.
Hence, after a partial schedule σ is assigned to events e ∈ Eσ , the updated domain for an unassigned
max
event e′ ∈
/ Eσ is bounded by dmin
e′ (σ) and de′ (σ). Note that the domain of the execution time steps
max
e′ is included in, but not equal to [dmin
e′ (σ), de′ (σ)], because we only consider discrete execution
time steps in a finite set T. During the forward checking, the p-Sulu Planner only computes the
max
′
real-valued bounds [dmin
e′ (σ), de′ (σ)]. The feasible values of an unassigned variable e are not
′
enumerated until the search tree is expanded to e .
Enumerating the domain of execution time steps for an unassigned event We can readily
extract the feasible execution time steps for any unassigned event e′ ∈
/ Eσ from the updated d-graph
with a partial schedule σ. Let De′ (σ) be the domain of execution time steps for an unassigned event
e′ ∈
/ Eσ , given a partial schedule σ. The finite domain De′ (σ) is obtained as follows:
max
De′ (σ) := { t ∈ T | dmin
e′ (σ) ≤ t∆T ≤ de′ (σ)}.

Note that De (σ) may be empty when the temporal constraints are tight, even though they are feasible. The user of the p-Sulu Planner must make ∆T small enough so that De is not empty.
For example, Figure 11-(b) is the d-graph given the partial schedule {σ(e0 ) = 0}. According to
the d-graph, e1 must be executed between 0.8 and 3.9. Assuming that ∆T = 1, the set of feasible
execution time steps for e1 is De1 (σ) = {1, 2, 3}, as shown in Figure 12-(a). Likewise, Figure 11-(c)
is the d-graph given the partial schedule {σ(e0 ) = 0, σ(e1 ) = 2}; the feasible execution time of eE
is between 3.6 and 5.5. Hence, the set of feasible execution time steps for eE is DeE (σ) = {4, 5},
as shown in Figure 12-(b).
The enumeration is conducted in Line 6 in Algorithm 7. Then the algorithm creates extensions
of the input partial schedule by assigning each of the time steps to e′ (Line 7), and puts the extended
partial schedules in the queue (Line 8).
6.2.2 E FFICIENT VARIABLE O RDERING OF B RANCH - AND -B OUND S EARCH
When choosing the next event to assign a time step in Line 3 of Algorithm 7, two variable ordering
heuristics are found to be effective in order to reduce computation time.
The first heuristic is our new convex-episode-first (CEF) heuristic, which prioritizes events that
are not associated with non-convex constraints. The idea of the CEF heuristic is based on the
observation that subproblems of the branch-and-bound algorithm are particularly difficult to solve
when the episodes in A(Eσ ) involve non-convex state constraints. The “Remain in R2 \C” (2D
plane minus the obstacle C) episode in the walk-through example in Figures 10 is an example of
such non-convex episodes. Therefore, an effective approach to reduce the computation time of the
p-Sulu Planner is to minimize the number of non-convex subproblems solved in the branch-andbound process. This idea can be realized by sorting the events so that the episodes with a convex
feasible region are always examined in the branch-and-bound process before the episodes with a
551

O NO , W ILLIAMS , & B LACKMORE

non-convex feasible region. In the walk-through example, note that we visited the event e1 before
the event eE in this example. This is because the “End in A” episode only involves a convex state
constraint while “Remain in R2 \C” (2D plane minus the obstacle C) is non-convex.
The second one is the well-known most constrained variable heuristic. When the p-Sulu Planner
expands a node, it counts the number of feasible time steps of all unassigned events, and chooses
the one with the least number of feasible time steps. The second heuristic used to break ties in the
first heuristic.

6.3 Bounding
We next present the implementation of the obtainLowerBound() function in Line 8 of Algorithm 6.
The algorithm obtains the lower bound by solving a relaxed CCQSP planning problem with a fixed
partial schedule.
Algorithm 8 outlines the implementation of the obtainLowerBound() function. It takes a partial
schedule σ as an input, and outputs the lower bound of the objective function, as well as the optimal
control sequence, given the partial schedule σ. It constructs a relaxed optimization problem, which
only involves episodes whose start and end events are both assigned execution time steps (Line 1). If
the optimization problem involves non-convex constraints, the NIRA algorithm is used to obtain the
solution to the problem (Line 3). Otherwise we solve the FRR of the convex optimization problem
to obtain the lower bound efficiently (Line 5). If the input is a fully assigned schedule (Eσ = E),
the corresponding node is a leaf node. In such case we obtain an exact solution to the CCQSP
planning problem with the fixed schedule σ by running the NIRA algorithm (Line 3). The details of
Algorithm 8 are explained in the subsequent part of this subsection.

Algorithm 8 Implementation of obtainLowerBound function in Line 8 of Algorithm 6
function obtainLowerBound(ccqsp, Eσ , σ) returns optimal objective value and control sequence
1: subprblem ← Problem 9 with σ given ccqsp
2: if Eσ = E or A(σ) has episodes with non-convex state regions, then
3:
[J ⋆ , u0:N −1 ] ← NIRA(subprblem) //Algorithm 1
4: else
5:
J ⋆ ←obtainLowreBound-FRR(subprblem) //Algorithm 5
6:
u0:N −1 ← Φ
7: end if
8: return [J ⋆ , u0:N −1 ]

6.3.1 R ELAXED O PTIMIZATION P ROBLEM WITH PARTIAL S CHEDULE
We consider a relaxed optimization problem as follows:
552

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Problem 9: Relaxed Optimization Problem for a Partial Schedule σ
J ⋆ (σ) =

min
u0:N −1 ∈UN
s.t.

J(u0:N −1 , x̄1:N , σ)

(68)

∀t ∈ T− , xt+1 = At x̄t + B t ut
∧
∧
∧ ∧ ∨

(69)
hTc,a,k,j xt − gc,a,k,j ≤ −mc,a,k,j (δc,a,k )

c∈C a∈(Ψc ∩A(σ)) t∈Πa (σ) k∈Ka j∈Ja,k

(70)

∑

δc,a,k ≥ 1 − ∆c ,

(71)

k∈Ka ,a∈(Ψc ∩A(σ))

where J ⋆ (σ) is the optimal objective value of the relaxed subproblem with a partial schedule σ.
Recall that A(σ) is the partial episode set of σ, which only involves the episodes whose start and
end nodes are both assigned execution time steps by the partial schedule σ (Definition 9). For
notational simplicity, we merge the three conjunctions of (70) and obtain the following:
∧ ∧ ∨
hTc,i,j x̄ti − gc,i,j ≤ −mc,i,j (δc,i ).
c∈C i∈Ic (σ) j∈Jc,i

Note that this chance constraint is exactly the same as (57), except that a partial schedule σ is
specified instead of a fully assigned schedule s. Hence, Problem 9 is an instance of a non-convex
CCQSP planning problem with a fixed schedule (Problem 6), and can be optimally solved by the
NIRA algorithm. Also note that σ is a fully assigned schedule at the leaf node of the branch-andbound search tree.
The optimal objective value of Problem 9 gives a lower bound of the optimal objective value
of all the subsequent subproblems in the branch-and-bound tree. This property is formally stated
in Lemma 8 below. In order to prove this feature, we first define the concept of an extension of a
partial schedule as follows:
Definition 14. A schedule s : E 7→ T is an extension of a partial schedule σ : Eσ 7→ T if and only
if both assign the same time steps to all the events in the domain of σ:
σ(e) = s(e) ∀e ∈ Eσ .
For example, in Figure 12-(b), a fully assigned schedule {s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 4} and
{s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 5} is an extension of a partial schedule {σ(e0 ) = 0, σ(e1 ) = 2}.
The following lemma always holds:
Lemma 8. If a schedule s is an extension of a partial schedule σ, then the optimal objective value
of Problem 9 with σ is a lower bound of the optimal objective value with s:
J ⋆ (σ) ≤ J ⋆ (s).
Proof. Since σ is a partial schedule, Eσ ⊂ E, and hence A(σ) ⊆ A. Also, since σ(e) = s(e) for all
e ∈ Eσ , all the state constraints in the chance constraint (70) of Problem 9 with a partial schedule
σ are included in the problem with a full schedule s. This means that the feasible state space of the
553

O NO , W ILLIAMS , & B LACKMORE

problem with s is a subset of the one with σ. Hence, if the chance constraint (24) of the problem
with s is satisfied, the chance constraint (70) of the problem with σ is also satisfied. Therefore, the
problem with σ always results in a better (less) or equal cost than the problem with σ ′ , because the
former has looser constraints.

For example, in Figure 12-(b), e1 has been assigned an execution time step but eE has not.
Therefore, at node σ(e1 ) = 2, the chance-constrained optimization problem with only the “End in
A” episode is solved with the partial schedule {σ(e0 ) = 0, σ(e1 ) = 2} (see Figure 10-(a)). It gives
a lower bound of the cost of the problems with the fully assigned schedules {s(e0 ) = 0, s(e1 ) =
2, s(eE ) = 4} and {s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 5}.
Algorithm 8 obtains a lower bound by solving Problem 9 exactly using the NIRA algorithm, if
it involves episodes with non-convex state regions (Line 3). If the function is called on a leaf node,
Problem 9 is also solved exactly by NIRA. This is because the solutions of leaf subproblems are
candidate solutions of an optimal solution of the overall problem. Hence, by solving them exactly,
we can ensure the optimality of the branch-and-bound search.
6.3.2 F URTHER B OUNDING WITH FRR
If the relaxed subproblem (Problem 9) is convex, then the p-Sulu Planner solves the FRR of the subproblem, instead of solving it exactly with NIRA, in order to obtain a lower bound more efficiently
(Line 5 of Algorithm 8). Many practical CCQSP execution problems have only one episode that
has a non-convex feasible region. For example, in the CCQSP planning problem shown in Figures
2 and 3, only the “safe region” (R2 minus the obstacles) is non-convex, while “Provincetown” (start
region), “Scenic region,” and “Bedford” (goal region) are convex. In such a case subproblems are
solved exactly only at the leaf nodes, and their lower bounds are always evaluated by approximate
solutions of FRRs of the subproblems at the non-leaf nodes.

7. Results
In this section we empirically demonstrate that the p-Sulu Planner can efficiently operate various
systems within the given risk bound. We first present the simulation settings in Section 7.1. Section 7.2 presents the simulation results of the NIRA algorithm, and validates our claim that it can
efficiently compute a feasible and near-optimal solution. Section 7.3 demonstrates the p-Sulu Planner on two different benchmark problems. The simulation results highlight the p-Sulu Planner’s
capability to operate within the user-specified risk bound. Section 7.4 deploys the p-Sulu Planner
on the PTS scenarios, while Section 7.5 applies the p-Sulu Planner to the space rendezvous of an
autonomous cargo spacecraft to the International Space Station.
7.1 Simulation Settings
Recall that, as we stated in Section 2.4, the p-Sulu Planner takes four inputs: a stochastic plant model
M, an an initial condition I, a CCQSP P , and an objective function J. This section specifies M
and J, which are commonly used by all the problems in Sections 7.2-7.4. We specify P and I for
each problem in the corresponding section.
554

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

7.1.1 S TOCHASTIC P LANT M ODEL
This section explains the plant model used in Sections 7.2 - 7.4. Section 7.5 uses a different plant
model that is described in detail in Section 7.5.2. We consider a point-mass double-integrator plant,
as shown in (72)-(73). Parameters, such as umax , vmax , σ 2 , and ∆T are set individually for each
problem. This plant model is commonly assumed in literatures on unmanned aerial vehicle (UAV)
path planning (Kuwata & How, 2011; Léauté, 2005; Wang, Yadav, & Balakrishnan, 2007).
Our state vector xt consists of positions and velocities in x and y directions, while the control
vector consists of the accelerations:
xt := [x y vx vy ]T ,

ut := [ax ay ]T .

The plant model is specified by the following matrices:





1 0 ∆t 0
∆t2 /2
0
 0 1 0 ∆t 


0
∆t2 /2 
, B = 
 , Σw = 
A=
 0 0 1

 ∆t


0
0 0 0
1
0
∆t
∀t ∈ T, ||ut || ≤ umax , ||Cxt || ≤ vmax ,
(

where
C=

0 0 1 0
0 0 0 1

σ2 0
0 σ2
0 0
0 0

0
0
0
0


0
0 
 (72)
0 
0
(73)

)
.

The first constraint in (73) is imposed in order to limit the acceleration. This nonlinear constraint is
approximated by the following set of linear constraints:
∀t ∈ T, r n · ut ≤ umax (n = 1, 2, · · · , Nr )
]
[
2πn
2πn
, sin
r n = cos
Nr
Nr
We choose Nr = 16. The second constraint in (73) is imposed in order to limit the velocity. We use
the same linear approximation as above.
7.1.2 O BJECTIVE F UNCTION
In Sections 7.2.3, 7.3, and 7.4, the cost function is the Manhattan norm of the control input over the
planning horizon, as follows:
J(x̄ti , U , s) =

T
∑

(|ux,t | + |uy,t |) .

t=1

This cost function represents the total change in momentum, which is roughly proportional to the
fuel consumption of an aerial vehicle. Note that a minimization problem with the piece-wise linear
cost function above can be equivalently replaced by the following minimization problem with a
linear cost function and additional linear constraints by introducing slack variables µx,t and µy,t :
min

T
∑

(µx,t + µy,t )

t=1

s.t.

∀t ∈ T,

µx,t ≥ ux,t ∧ µx,t ≥ −ux,t ∧ µy,t ≥ uy,t ∧ µy,t ≥ −uy,t
555

O NO , W ILLIAMS , & B LACKMORE

In Section 7.2.4, we minimize expected quadratic cost as follows:
T
∑
[
]
J(x̄ti , U , s) =
E u2x,t + u2y,t .

(74)

t=1

7.1.3 C OMPUTING ENVIRONMENT
All simulations except for the ones in Section 7.2 are conducted on a machine with a dual-core
Intel Xeon CPU clocked at 2.40 GHz, and with 16 GB of RAM. The algorithms are implemented
in C/C++, and run on Debian 5.0.8 OS. The simulations in Section 7.2 are conducted on a machine
with a quad-core Intel Core i7 CPU clocked at 2.67 GHz, and with 8 GB of RAM. The algorithms
are implemented in Matlab, and run on Windows 7 OS. We used IBM ILOG CPLEX Optimization
Solver Academic Edition version 12.2 as the linear program solver, and SNOPT version 7.2-9 as
the convex optimization solver.
7.2 NIRA Simulation Results
We first statistically compare the performance of NIRA with the prior art. Recall that NIRA is
a solver for CCQSP planning problems with non-convex state constraints and a fixed schedule
(Problem 3), and used as a subroutine in the p-Sulu Planner.
7.2.1 C OMPARED A LGORITHMS
There are two existing algorithms that can solve the same problem:
1. Fixed risk allocation (Blackmore et al., 2006) - This approach fixes the risk allocation to a
uniform value. As a result, with an assumption that the cost function is linear, Problem 6
can be reformulated to a mixed-integer linear programming (MILP) problem, which can be
solved efficiently by a MILP solver, such as CPLEX.
2. Particle Control (Blackmore, 2006) - Particle Control is a sampling-based method, which
uses a finite number of samples to approximate the joint chance constraints. The control
sequence is optimized so that the number of samples that violate constraints is less than ∆c Np ,
where Np is the total number of samples. The optimization problem is again reformulated into
MILP, with an assumption that the cost function is linear.
We also compare NIRA with an MDP in Section 7.2.5. Although an MDP does not solve
exactly the same problem as NIRA, it can also avoid risk by considering a penalty cost of constraint
violations. The purpose of the comparison is to highlight the capabilities of chance-constrained
planning to provide a guarantee on the probability of failure.
7.2.2 P ROBLEM S ETTINGS
We compare closed-loop and open-loop NIRAs with the two algorithms on a 2-D path planning
problem with a randomized location of an obstacle, as shown in Figure 13. A vehicle starts from
[0, 0] and heads to the goal at [1.0, 1.0], while avoiding a rectangular obstacle. The obstacle with
edge length 0.6 is placed at a random location within the square region with its corners at [0, 0],
[1, 0], [1, 1], and [0, 1]. We consider ten time steps with the time interval ∆t = 1.0. We require that
556

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

the mean state at t = 10 is at [1.0, 1.0]. The risk bound is set to ∆ = 0.01. We set the standard
deviation of the disturbance as σ = 0.01. We use the expected quadratic cost function given in (74).
The steady-state LQR gain is used for the closed-loop NIRA with Q = I4 and R = 10000I2 , where
In is the n × n identity matrix and Q and R are cost matrices for the state and control variables,
respectively.
1

NIRA (closed-loop)
NIRA (open-loop)
Fixed Risk Allocation
Particle Control

0.8

NIRA (closed-loop)
NIRA (open-loop)

1
0.8

0.6

0.6

0.4

0.4

0.2

0.2
0

0
0

0.2

0.4

0.6

0.8

1

0

(a) Nominal trajectories

0.2

0.4

0.6

0.8

1

(b) Nominal trajectories and 3σ ellipses

Figure 13: (a) An instance of the 2-D path planning problem used in 7.2.3. The obstacle with a
fixed size is randomly placed within the unit square for each run. (b) The mean and the
standard deviation of the closed-loop and open-loop NIRAs.

7.2.3 P ERFORMANCE C OMPARISON
Recall that the solution of the NIRA algorithm, which is used by the p-Sulu Planner to solve subproblems, is not an exactly optimal solution to Problem 3, since risk allocation (Section 4.1.1) and
risk selection (Section 5.1.1) replace the chance constraint (29) with its sufficient condition (57)
∧ (59). Since the chance constraint (29) is very difficult to evaluate, all the previously proposed
methods solve optimization with its approximation. We provide empirical evidence that our risk
allocation/selection approach results in a solution that is significantly closer to the optimal solution
than the prior art, while satisfaction of the original constraint (29) is guaranteed.
We evaluate the suboptimality of the solutions by the difference between the risk bound, ∆ =
0.001, and the resulting probability of constraint violation, Pf ail , estimated by a Monte-Carlo simulation. 1 − Pf ail is equal to the left-hand-side value of (29) in Problem 3. Hence, the chance
constraint (29) is equivalent to:
Pf ail ≤ ∆.
The strictly optimal solution to this problem should achieve Pf ail = ∆, although such an exact
solution is unavailable, since there is no algorithm to solve Problem 3 exactly. A solution is suboptimal if Pf ail < ∆, and their ratio ∆/Pf ail represents the degree of suboptimality. A solution
violates the chance constraint if Pf ail > ∆.
557

O NO , W ILLIAMS , & B LACKMORE

Algorithm
NIRA (Closed-loop)
NIRA (Open-loop)
Fixed Risk Allocation
Particle Control
(100 particles)

Computation time
[sec]
54.8 ± 36.9
25.0 ± 13.1
0.42 ± 0.04

Probability of failure

Cost

0.0096 ± 0.0008
0.0095 ± 0.0008
(2.19 ± 0.40) × 10−4

0.666 ± 0.061
0.672 ± 0.068
0.726 ± 0.113

41.7 ± 12.8

0.124 ± 0.036

0.635 ± 0.048

Table 1: The averages and the standard deviations of the computation time, the probability of constraint violation, and the cost of the four algorithms. Each algorithms are run 100 times
with random location of an obstacle. The risk bound is set to ∆ = 0.01. Note that Particle
Control results in less cost than the other two methods because its solutions violate the
chance constraint.

Table 1 compares the performance of the four algorithms. The values in the table are the averages and the standard deviations of 100 runs with random locations for the obstacle. The probability
of constraint violation, Pf ail , is evaluated by Monte-Carlo simulations with 106 samples.
Comparison of closed-loop and open-loop NIRAs Before comparing NIRA with existing algorithms, we first compare the two variants of NIRA: the closed-loop and open loop NIRAs. Table
1 shows that the closed-loop NIRA results in less cost than the open-loop NIRA. Importantly, the
former outperforms the latter in all the 100 test cases. This reduction in cost by the closed-loop
approach is explained by Figure 13-(b), which shows the 3σ ellipses of the probability distribution
of the state. Since the closed-loop NIRA assumes a feedback control, the future position is less uncertain. As a result, the plan generated by the closed-loop NIRA is less conservative. In fact, Table
1 shows that Pf ail of the closed-loop NIRA is closer to the risk bound than that of the open-loop
NIRA. However, the closed-loop planning problem requires about twice as much solution time as
the open-loop one since it is more complicated due to additional chance constraints on control input.
Comparison with the fixed risk allocation approach Table 1 shows that closed and open NIRAs
result in the average probabilities of failure 0.0096 and 0.0095 respectively, which is within the userspecified risk bound ∆ = 0.01. On the other hand, the fixed risk allocation approach results in a
very conservative probability of failure, Pf ail = 0.000219, which is 98% smaller than ∆. This
result indicates that the solution by NIRA is significantly closer to the exactly optimal solution than
the fixed risk allocation approach. In fact, the NIRA algorithm results in less cost than the fixed risk
allocation approach in all the 100 runs. This is because it optimizes the risk allocation while the
fixed risk allocation approach uses the predetermined risk allocation.
Figure 14 shows the suboptimality measure ∆/Pf ail of the open-loop NIRA with different settings of the risk bound ∆. For all values of ∆, the suboptimality of NIRA is significantly smaller
than the fixed risk allocation approach. The graph shows a tendency that the suboptimality of NIRA
gets smaller for less ∆, while the suboptimality of the fixed risk allocation approach is approximately constant.
NIRA achieves the improvement in solution optimality with a cost of computation time; Table
1 shows that NIRA takes longer computation time than the risk allocation approach by the factor
558

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Pfail /

Figure 14: Suboptimality of NIRA and the fixed risk allocation approach. Strictly optimal solution
has ∆/Pf ail = 1. A smaller value of ∆/Pf ail indicates that the solution is suboptimal.

of two. Hence, NIRA and the fixed risk allocation approach provide users with a trade-off between
suboptimality and computation time.
Comparison with the Particle Control Table 1 shows that the average probability of failure of
the Particle Control approach is higher than the risk bound ∆ = 0.01, meaning that the approach
tends to generate infeasible solutions. On the other hand, NIRA guarantees the satisfaction of the
chance constraint since it employs a conservative approximation of the joint chance constraint.
Particle Control has a guarantee that its solution converges to an optimal solution when increasing the number of samples to infinity. However, using a large number of samples is impractical,
since computation time and memory usage grow exponentially as the number of samples increases.
For example, we used only 100 samples in the analysis in Table 1. When using 300 samples, it took
4596 seconds (about 1.5 hours) to solve the same problem with the obstacle’s centered at [0.5, 0.5].
Computation with 1000 samples could not be conducted, because of the shortage of memory. On
the other hand, the computation time of NIRA is significantly shorter than PC, while guaranteeing
the feasibility of the solution.
7.2.4 O PTIMAL P LANNING WITH E XPECTED C OST
Next we demonstrate the capability of the p-Sulu Planner to handle expected cost, instead of the cost
of the expected trajectory, for the same path planning problem presented above. Specifically, we
consider the expected quadratic cost function shown in (74). When conducting open-loop planning,
this cost function can be transformed to a function of nominal control inputs with a constant term
by using the equality (15). However, when performing closed-loop planning, this equality is not
exact, due to controller saturation. Nevertheless, we use (15) as an approximation of the expected
cost, as explained in Section 2.4.4. In this subsection we empirically evaluate the error of this
approximation.
559

O NO , W ILLIAMS , & B LACKMORE

Approximate expected cost
0.048434950 ± 0.010130589

Actual expected cost
0.048434956 ± 0.010130588

Table 2: Comparison of the approximate expected cost obtained by the closed-loop NIRA with the
actual expected cost. The table shows the mean and variance of 100 runs with random
location of the obstacle.

Table 2 compares the approximate expected cost function value obtained by the closed-loop
NIRA with the actual expected cost estimated by Monte-Carlo simulation with one million samples.
The path planning problem is solved 100 times with a randomized location of the obstacle. The risk
bound is set to ∆ = 0.01. As shown in the table, the approximate cost almost exactly agrees with
the actual cost. This is because our closed-loop planning approach explicitly bounds the risk of
controller saturation.
7.2.5 C OMPARISON WITH MDP
Next we compare NIRA with an MDP formulation. For the sake of tractability of the MDP, we
consider a single integrator dynamics with a two-dimensional state space and a two-dimensional
control input, which specifies the velocity of a vehicle. The rest of the problem setting is the same,
except that the state space is discretized into a 100-by-100 grid. We implement a finite-horizon
MDP-based path planner, which imposes a penalty c on an event of failure and minimizes the
expected cost based on explicit state dynamic programming. The MDP-based path planner imposes
a cost as follows:
]
[ T
∑(
)
u2x,t + u2y,t + cI(xt ) ,
E
t=1

where I(xt ) is an indicator function that is one if xt is in a obstacle and zero otherwise. The
resulting optimization problem is solved via dynamic programming.
We ran the MDP-based path planner with three values of penalty c: 1, 10, and 100. For each
choice of c, we conducted 100 simulations with a randomized obstacle position. Figure 14 shows
a typical output of the MDP-based path planner. Note that, with a small penalty (c = 1), the path
planner chooses to take a 100% risk of failure by ignoring the obstacle. This is simply because
the penalty of failure is smaller than the expected reduction of cost by going through an obstacle.
An issue of utilitarian approaches such as MDPs is that minimization of unconstrained cost can
sometimes lead to such impractical solution.
Table 3 shows the mean and the standard deviation of path lengths, as well as the maximum,
minimum, and the mean of the resulting probability of failure among the 100 runs. As expected,
by imposing a larger penalty, the MDP-path planner chooses a more risk-averse path, which has a
longer nominal path length. In this sense, an MDP can also conduct a trade-off between cost and
risk. MDP is particularly useful when the primary concern of the user is the cost of failure instead of
the probability of failure. On the other hand, when a user would like to impose a hard bound on the
probability of failure, our chance constrained planning approach has an advantage. Observe that,
even with the same penalty value, the MDP-based path planner results in a wide range of failure
probabilities depending on the location of the obstacle. Most notably, with c = 10, some of the
560

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

paths move directly across the obstacle, and in so doing, accept a 100% probability of failure, while
others go around the obstacle. Undesirable behaviors, such as crossing an obstacle, are likely to
be suppressed by imposing a greater penalty, but without a guarantee. Moreover, imposing a heavy
penalty on failure often results in an overly conservative, risk averse solution. On the other hand,
the behavior of NIRA with regarding to risk is predictable, in a sense that the path is guaranteed
to go around the obstacle, regardless of its location. This is because the chance constraint requires
that there exists a margin between the path and the boundary of the obstacle. The p-Sulu Planner
inherits this property from NIRA.

1

0.8

0.6

0.4

0.2

c=1
c=10
c=100

0
0

0.2

0.4

0.6

0.8

1

Figure 15: Optimal paths generated by an MDP-based planner with different penalty levels , c. The
red rectangle represents an obstacle. Note that the path with c = 1 cuts through the
obstacle.

Penalty c

path length

1
10
100

1.41 ± 0.00
1.54 ± 0.05
1.57 ± 0.06

Probability of failure
Max
Mean Min
1.000 1.000 1.000
1.000 0.375 0.096
0.1215 0.031 0.009

Table 3: 100 runs with a randomized obstacle location

7.3 The p-Sulu Planner Simulation Results
Next we present the simulation results of the p-Sulu Planner on two problems, in order to illustrate
its capability of planning with schedule constraints. We also empirically evaluate the scalability of
p-Sulu.
561

O NO , W ILLIAMS , & B LACKMORE

Figure 16: A sample CCQSP for a personal aerial vehicle’s path planning and scheduling problem.

Figure 17: Output of the p-Sulu Planner for the CCQSP in Figure 16 with three different settings
of the risk bound ∆obs , compared to the path planned by a deterministic planner, Sulu,
which does not consider chance constraints.

7.3.1 PATH P LANNING WITH O BSTACLES
In this simulation we test the p-Sulu Planner on a path planning problem in the environment shown
in Figure 17. The input CCQSP is shown in Figure 16. The CCQSP requires a vehicle to arrive at
the goal region within 15 minutes, by going through Waypoint 1 and Waypoint 2 with the temporal
constraints specified in Figure 16. It also imposes two chance constraints: one that requires the
vehicle to achieve the time-evolved goals with 90% certainty, and another that requires the vehicle
to limit the probability of violating the obstacles to ∆obs . We set ∆t = 1 and σ 2 = 0.0025.
Figure 17 shows the plans generated by the p-Sulu Planner with three different risk bounds:
∆obs = 10%, 0.1%, and 0.001%. The computation times were 79.9 seconds, 86.4 seconds, and
88.1 seconds, respectively. Figure 17 also shows the plan generated by Sulu, a deterministic planner
that does not explicitly consider uncertainty (Léauté & Williams, 2005). Observe that Sulu leaves no
margin between the path and obstacles. As a result, the Sulu path results in a 94.1% probability of
hitting obstacles, as estimated by a Monte-Carlo simulation with 107 samples. On the other hand, the
p-Sulu Planner leaves margins between the path and the obstacles in order to satisfy the risk bound,
562

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

specified in the chance constraint. The margins are larger for the plans with smaller risk bounds. The
probabilities of failure of the three plans generated by the p-Sulu Planner, estimated by Monte-Carlo
simulations with 107 samples, are 9.53%, 0.0964%, and 0.00095%, respectively. Hence the chance
constraints are satisfied. The schedule optimized by the p-Sulu Planner is {s(e0 ) = 0, s(e1 ) =
5, s(e2 ) = 10, s(eE ) = 15}, which satisfies all the temporal constraints in the CCQSP.
In Figure 16, it appears that the path cuts across the obstacle. This is due to the discretization
of time; the optimization problem only requires that the vehicle locations at each discrete time step
satisfy the constraints, and does not consider the state in between. This issue can be addressed by a
constraint-tightening method (Kuwata, 2003).
7.3.2 PATH P LANNING IN AN I NDOOR E NVIRONMENT

Figure 18: A sample CCQSP for a path planning problem in an indoor environment.

∆ = 10%
∆ = 1%
∆ = 0.1%

1.2
1
Goal
0.8
0.6
0.4
0.2
0

Start

−0.2
−0.2

0

0.2

0.4

0.6

0.8

1

1.2

Figure 19: Output of the p-Sulu Planner for the CCQSP in Figure 16 with three different settings
of the risk bound ∆obs .

We next give the p-Sulu Planner the CCQSP shown in Figure 18, which simulates a path planning problem in an indoor environment. A vehicle must get to the goal region at the other side of
the room in three to five seconds. The “Remain in safe region” episode requires the vehicle to stay
563

O NO , W ILLIAMS , & B LACKMORE

within the room and outside of the obstacle during the five-second planning horizon. The CCQSP
imposes two chance constraints shown in Figure 18. We set ∆t = 0.5 and σ 2 = 5.0 × 10−5 .
Given this CCQSP, the planner faces a choice: heading straight to the goal by going through
the narrow passage between the left wall and the obstacle minimizes the path length, but involves
higher risk of constraint violation; making a detour around the right side of the obstacle involves
less risk, but results in a longer path.
Figure 19 shows the p-Sulu Planner’s outputs with ∆obs = 10%, 1%, and 0.1%. The computation times were 35.1 seconds, 84.5 seconds, and 13.3 seconds, respectively. The result is consistent
with our intuition. When the p-Sulu Planner is allowed a 10% risk, the planner chooses to go straight
to the goal, resulting in the cost function value of 1.21; when the user gives a 1% or 0.1% risk bound,
it chooses the risk-averse path, resulting in the cost function values of 3.64 and 3.84, respectively.
This example demonstrates the p-Sulu Planner’s capability to make an intelligent choice in order to
minimize the cost, while limiting the risks to user-specified levels.
7.3.3 S CALABILITY A NALYSIS
In this subsection we conduct an empirical analysis of the scalability of the p-Sulu Planner, as the
environment becomes increasingly constrained.. As shown in Figure 20, we measured the computation time to solve a path planning problem with different numbers of obstacles and waypoints. In
all simulations, the path starts at [0, 12] and ends in a square region centered at [24, 12]. Figure 20
shows twenty simulation results, with zero to three obstacles and zero to four waypoints. Obstacles
and waypoints are represented by blue and red squares in the figure, respectively. The positions of
the center of the obstacles are [6, 12], [12, 12], and [18, 12], while the positions of the center of the
waypoints are [9, 9], [9, 15], [15, 15], and [15, 9]. The computation time is shown in the caption of
each subfigure in Figure 20.
By comparing the results in Figure 20 horizontally, we observe exponential growth in computation time with the number of obstacles. This result is expected since the number of disjunctive
clauses in the state constraint of the p-Sulu Planner increases exponentially with the number of
obstacles. Building a tractable extension of the p-Sulu Planner for a large number of obstacles is
future work. On the other hand, by comparing the results vertically, we find that the computation
time with the same number of obstacles and different number of waypoints stays in the same order
of magnitude. This is because adding an extra waypoint only increases the number of conjunctive
clauses in the state constraints.
In the remaining sections we describe the application of psulu to two real world problems, air
vehicle and space vehicle control. A third application, building energy management, using a variant
of the p-Sulu Planner, is reported by Ono, Graybill, and Williams (2012).
7.4 PTS Scenarios
Next, we deploy the p-Sulu Planner on PTS scenarios, the robotic air taxi system introduced in
Section 1.
7.4.1 S CENARIOS
We consider three scenarios, specified by the CCQSPs shown in Figure 21. Scenarios 1 and 2 are
similar to the scenic flight scenario introduced at the beginning of this paper (see Figure 1). In
564

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

0.003 sec

0.173 sec

51.47 sec

677.2 sec

0.016 sec

0.518 sec

48.25 sec

648.4 sec

0.034 sec

1.047 sec

118.3 sec

4327 sec

0.076 sec

2.613 sec

159.1 sec

5686 sec

0.036 sec

3.873 sec

165.9 sec

6845 sec

15
12
9

15
12
9

15
12
9

15
12
9

15
12
9
0

9

15

24

0

9

15

24

0

9

15

24

0

9

15

24

Figure 20: Computation time of the p-Sulu Planner for a path planning problem with different numbers of obstacles and waypoints.

Scenario 1, a personal aerial vehicle (PAV) takes off from Runway 71 of Provincetown Municipal
Airport (KPVC) in Provincetown, Massachusetts, fly over a scenic region, and lands on Runway 23
of Hanscom Field (KBED) in Bedford, Massachusetts. The vehicle is required to stay within the
scenic region at least for 2 minutes and at most for 10 minutes. The entire flight must take more than
13 minutes and less than 15 minutes. Scenario 2 is the same as Scenario 1, except for the runways
used for take-off and landing.
Scenario 3 simulates a leisure flight off the coast of Massachusetts. A PAV takes off Runway 7
of Provincetown Municipal Airport, and flies over two regions where whales are often seen. Then
the vehicle lands on Runway 11 of Hanscom Field.
We place three no-fly zones, as shown in Figure 22. The entire flight must take more than 13
minutes and less than 15 minutes. Each scenario has three chance constraints, {c1 , c2 , c3 }, as shown
in Figure 21. The first one, c1 , is concerned with the vehicle’s operation; it requires the vehicle to
take off from and land on the right runways at the right airports with less than 10 % probability
of failure. The second chance constraint, c2 , is concerned with the leisure activities; it requires
the vehicle to fly over the scenic regions with less than 10 % probability of failure. Finally, c3 is
concerned with the passenger’s safety; it requires the vehicle to limit the risk of penetrating the
no-fly zones to 0.01 %.
1. A runway of an airport is specified by a number, which represents the clockwise angle from the north. For example,
Runway 7 points 70 degrees away from the north.

565

O NO , W ILLIAMS , & B LACKMORE

7.4.2 P LANT PARAMETERS
We set umax = 250 m/s, which approximates the maximum cruise speed of private jet airplanes,
such as Gulfstream V. The maximum acceleration is determined from the maximum bank angle.
Assuming that an aircraft is flying at a constant speed, the lateral acceleration a is given as a function
of the bank angle ϕ as follows:
a = g · tan ϕ,
where g is the acceleration of gravity. Typically passenger aircraft limits the bank angle to 25
degrees for passenger comfort, even though the aircraft is capable of turning with a larger bank
angle. Hence, we use:
umax = 9.8 m/s2 · tan(25◦ ) = 4.6 m/s2 .
We set σ = 100 m and ∆T = 60 seconds.
7.4.3 S IMULATION R ESULTS
Figure 22 shows the paths planned by the p-Sulu Planner for the three scenarios. In all the scenarios,
all the episode requirements in the CCQSPs in Figure 21 are met within the specified temporal and
chance constraints.
Table 4 compares the performance of Sulu and the p-Sulu Planner. As expected, Sulu’s plans
result in excessive probabilities of failure in all scenarios. This is because Sulu does not consider
uncertainty in the planning process, although the PAV is subject to disturbance in reality. On the
other hand, the p-Sulu Planner successfully limits the probability of failure within the user-specified
risk bounds for all three scenarios. Furthermore, although the p-Sulu Planner significantly reduces
the risk of failure, its cost is higher than that of Sulu only by 9.5 - 12.8 %. Such a capability of
limiting the risk and maximizing the efficiency at the same time is a desirable feature for PTS,
which transports passengers.
Scenario number
Planner
Computation time [sec]
Pf ail,1
Pf ail,2
Pf ail,3
Cost function value J ⋆

1
Sulu
2.58
0.999
0.807
0.373
24.2

2

p-Sulu
60.2
9.12 × 10−2
8.46 × 10−2
2.74 × 10−5
27.5

Sulu
2.00
0.996
0.813
0.227
21.0

p-Sulu
390
9.14 × 10−2
8.59 × 10−2
2.62 × 10−5
23.7

3
Sulu
5.17
0.999
0.603
0.372
20.0

p-Sulu
198
9.23 × 10−2
7.65 × 10−2
2.81 × 10−5
22.3

Table 4: Performance Comparison of the prior art, Sulu, and the p-Sulu Planner. Pf ail,1 , Pf ail,2 ,
and Pf ail,3 represent the probabilities of failure regarding the chance constraints c1 , c2 ,
and c3 in Figure 21, respectively.

566

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Figure 21: The CCQSPs for the PTS scenarios.

Figure 22: The paths planned by the p-Sulu Planner.

567

O NO , W ILLIAMS , & B LACKMORE

As shown in Table 4, the p-Sulu Planner typically takes several minutes to compute the plan.
This length of computation time would be allowed for PTS applications, since we assume that the pSulu Planner is used for preplanning; before take-off, the passengers of a PAV specify requirements,
and the p-Sulu Planner creates a risk-sensitive flight plan. We assume that a real-time plan executive
executes the plan after take-off.
We note that it is more desirable to have a real-time risk-sensitive plan executive, since risk factors, such as the location of storms, change over time. Our future work is to reduce the computation
time of the p-Sulu Planner so that it can be used for real-time execution.
7.5 Space Rendezvous Scenario
The p-Sulu Planner is a general planner whose application is not limited to a specific plant model.
In order to show the generality of the planner, we deployed the p-Sulu Planner on a system whose
plant model is significantly different from PTS.
Specifically, we chose an autonomous space rendezvous scenario of the H-II Transfer Vehicle
(HTV), shown in Figure 23, as our subject. HTV is an unmanned cargo spacecraft developed by the
Japanese Aerospace Exploration Agency (JAXA), which is used to resupply the International Space
Station (ISS). Collision of the vehicle with the ISS may result in a fatal disaster, even if the collision
speed is low. For example, in August 1994, the Russian unmanned resupply vehicle Progress M34 collided with the Mir space station in a failed attempt to automatic rendezvous and docking.
As a result, one of the modules of Mir was permanently depressurized. In order to avoid such an
accident, HTV is required to follow a specified safety sequence during the automated rendezvous,
as described in the following subsection.

Figure 23: H-II Transfer Vehicle (HTV), a Japanese unmanned cargo vehicle, conducts autonomous
rendezvous with the International Space Station. Image courtesy of NASA.

7.5.1 HTV R ENDEZVOUS S EQUENCE
In HTV’s autonomous rendezvous mission, the final approach phase starts from the Approach Initiation (AI) point, which is located 5 km behind the ISS, as shown in Figure 24. First, HTV moves
to the R-bar Initiation (RI) point, which is located 500 m below the ISS, guided by the relative GPS
568

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

navigation. At the RI point, HTV switches the navigation mode to Rendezvous Sensor (RVS) Navigation. In RVS Navigation, HTV measures the distance to ISS precisely by beaming the laser to the
reflector placed on the nadir (earth-facing) side of the ISS. Then, HTV proceeds to the Hold Point
(HP), located 300 m below the ISS. It is required to hold at HP in order to perform a 180-degree
yaw-around maneuver. The new orientation of HTV allows the vehicle to abort the rendezvous
quickly in case of emergency. After the yaw-around maneuver, HTV resumes the approach, and
holds again at the Parking Point (PP), which is 30 m below the ISS. Finally, HTV approaches at
a distance of 10 meters from the ISS, and stops within the Capture Box (CB) of the ISS’s robotic
arm. The robotic arm then grabs HTV and docks it to the ISS. Please refer to the report by Japan
Aerospace Exploration Agency (2009) for the details of the rendezvous sequence.
RI

HP

PP CB

y
ISS

-300m

-30m -10m

ISS Orbit

-500 m

x

Earth
AI: Approach Initiation
RI: R-bar Initiation
HP: Hold Point
PP: Parking Point
CB: Capture Box

AI Point
-5000 m

Figure 24: HTV’s final approach sequence (Japan Aerospace Exploration Agency, 2009).
The rendezvous sequence described above is represented by the CCQSP shown in Figure 25. In
addition to the time-evolved goals specified in the actual rendezvous sequence, we specify temporal
constraints and chance constraints in the simulation, as shown in the figure. We require HTV to hold
at each intermediate goal for at least 240 seconds. The transition between the goals must take at
least 600 seconds, in order to make sure that the vehicle moves slowly enough. The entire mission
must be completed within 4800 seconds (1 hour 20 minutes). We require HTV to stay within the
Safe Zone, a conic area below the ISS, during the RVS navigation phase with 99.5% probability,
since otherwise the laser may not be reflected back to HTV properly. We assume that the goals are
square regions, with 10 m sides for RI and HP, 2 m sides for PP, and 1 m sides for CB. Finally, we
require that HTV achieves all the time-evolved goals with 99.5% success probability.
7.5.2 O RBITAL DYNAMICS
The rendezvous can be considered as a two-body problem, where a chaser spacecraft (e.g., HTV)
moves in relation to a target spacecraft (e.g., ISS), which is in a circular orbit. In such a problem,
it is convenient to describe the motion of the chaser spacecraft using a rotating frame that is fixed
to the target space craft, known as a Hill coordinate frame (Schaub & Junkins, 2003). As shown
in Figure 24, we set the x-axis pointing away from the center of the earth and the y-axis along the
569

O NO , W ILLIAMS , & B LACKMORE

Figure 25: A CCQSP representation of the HTV’s final approach sequence. We assume the same
time-evolved goals as the ones used for actual flight missions. The temporal constraints
and the chance constraints are added by the authors.

orbital velocity of the target spacecraft. Since HTV’s path is within the x-y plane, we don’t consider
the z-axis.
It is known that the relative motion of the chase spacecraft in the Hill coordinate frame is described by the following Clohessy-Wiltshire (CW) equation (Vallado, 2001):
ẍ = 2ω ẏ + 3ω 2 x + Fx
ÿ = 2ω ẋ + Fy
where ω is the angular speed of the target spacecraft’s orbit, and Fx and Fy are the force per unit
mass, or the acceleration in x and y directions. The first terms on the right-hand sides represent the
Coriolis force.
An object that follows the CW equation moves in an unintuitive manner. Its unforced motion is
not in a straight line due to the Coriolis effect; in general, an object cannot stay at the same position
without external force. For example, Figure 26 shows the fuel-optimal path to visit two waypoints,
A and B, and come back to the start. As can be seen in the figure, the optimal path is not typically a
straight line. The virtue of the p-Sulu Planner is that it can handle such irregular dynamic systems
in the same way as regular systems, just by setting the A and B matrices of our plant model (4)
appropriately.
The state vector consists of positions and velocity in the x − y plane:
x = [x y vx vy ]T
We obtain the discrete-time CW equation using the impulse-invariant discretization:
xk+1 = Axk + Buk ,
570

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

A

Start

B

Figure 26: A typical motion of spacecraft in the Hill coordinate frame. The solid line is the fuel
optimal path to visits A and B and returns to the Start in 30 minutes. Note that the
optimal path is not a straight line in the Hill coordinate frame.

where




A = 




B = 


4 − 3 cos(ω∆T )
−6{ω∆T − sin(ω∆T )}
3ω sin(ω∆T )
−6ω{1 − cos(ω∆T )}

0
1
0
0

sin(ω∆T )
ω
−2{1−cos(ω∆T )}
ω

2{1−cos(ω∆T )}
ω
4 sin(ω∆T )
− 3∆T
ω

cos(ω∆T )
−2 sin(ω∆T )


2 sin(ω∆T )
4 cos(ω∆T ) − 3

sin(ω∆T )
ω
−2{1−cos(ω∆T )}
ω

2{1−cos(ω∆T )}
ω
4 sin(ω∆T )
− 3∆T
ω

cos(ω∆T )
−2 sin(ω∆T )

2 sin(ω∆T )
4 cos(ω∆T ) − 3












We use the ISS’s orbital angular speed, ω = 0.001164 rad/sec, at which the station goes around the
Earth in 90 minutes. We choose the interval ∆T = 120 seconds. The number of time steps N is
set to 40. Hence, the entire plan is 4800 seconds (1 hour and 20 minutes). In the discretization, we
assumed impulse inputs as follows:
[

Fx
Fy

]
=

N
−1
∑

δ(t − ∆T · k)uk ,

k=0

where δ(·) is the Dirac delta function. Such an assumption is justified because the thrusters of the
Reaction Control System (RCS) of the spacecraft, which are used for the final approach maneuver,
operate for a very short duration (0.01 − 5.0 seconds) at each burn (Wertz & Wiley J. Larson, 1999).
We consider stochastic uncertainty w, added to the discrete-time dynamic equation:
xk+1 = Axk + Buk + w.
Such an assumption of additive uncertainty is commonly used in past research on autonomous
rendezvous and formation flight in space (Shields, Sirlin, & Wette, 2002; Smith & Hadaegh, 2007;
571

O NO , W ILLIAMS , & B LACKMORE

Campbell & Udrea, 2002). We assume that w has a
following covariance matrix:
 −6
10
0
 0
10−6
Σw = 
 0
0
0
0

zero-mean Gaussian distribution, with the

0
0
0
0


0
0 
.
0 
0

7.5.3 O BJECTIVE F UNCTION
We employ an objective function J that requires for the p-Sulu Planner to minimize the fuel consumption. It follows from the Tsiolkovsky rocket equation that the fuel consumption of spacecraft is
proportional to the total change in velocity, called Delta-V or ∆V (Wertz & Wiley J. Larson, 1999).
The total fuel consumption is the summation of the fuel consumption of reaction jets in x and y
directions for all time steps. Hence our objective function is described as follows:
J(u0:N ) = ∆Vx + ∆Vy
∫ (N −1)∆T
=
|Fx | + |Fy |dt
0

 ∫

k=N
  (N −1)∆T

∑−1 ∫ (N −1)∆T
 

=
δ(t − ∆T · k)ux,k dt + 
δ(t − ∆T · k)uy,k dt

 0
  0

=

k=0
k=N
∑−1

|ux,k | + |uy,k |.

k=0

7.5.4 S IMULATION R ESULT
Figure 27 shows the planning result of the p-Sulu Planner. We compare the result with Sulu, as
well as a nominal planning approach, in which we assume that HTV moves from AI to RI using a
two-impulse transition (called “CW guidance law”) (Matsumoto, Dubowsky, Jacobsen, & Ohkami,
2003; Vallado, 2001). From RI to CB, it follows a predetermined path that goes through the center
of the Safe Zone, as shown in Figure 27-(b), with a constant speed.
As shown in Figure 27, the optimal paths generated by the p-Sulu Planner and Sulu are not
straight. Such curved paths exploit the Coriolis effect to minimize fuel consumption.
Table 5 compares the performance of the three planning approaches. The two rows regarding
the probabilities of failure correspond to the two chance constraints specified in the CCQSP, shown
in Figure 25. The probabilities are evaluated by Monte Carlo simulation with one million samples.
As expected, the probabilities of failure of the path generated by the p-Sulu Planner are less
than the risk bounds specified by the CCQSP, shown in Figure 25. On the other hand, once again,
Sulu’s path results in almost 100% probability of failure. This is because Sulu minimizes the fuel
consumption without considering uncertainty. The resulting path pushes against the boundaries
of feasible regions, as is evident in Figure 27-(c). Also note that, although the p-Sulu Planner
significantly reduces the probability of constraint violation compared with Sulu, its cost (Delta V)
is higher than Sulu only by 0.2%. The p-Sulu Planner results in a significantly smaller cost (Delta
V) than the nominal planning approach. The 1.42 m/sec reduction in Delta V is equivalent to an
11.9 kg saving of fuel, assuming the 16, 500 kg mass of the vehicle and the 200 sec specific impulse
572

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Figure 27: Planning results of Sulu, the p-Sulu Planner, and a nominal planning approach. The
input CCQSP is shown in Figure 25.

(ISP ) of the thrusters. Although the p-Sulu Planner takes longer to compute the plan than the other
two approaches, the 11.4 second computation time is negligible compared with the 1 hour and 20
minute plan duration.

Computation time [sec]
Probability of failure Pf ail (Navigation)
Probability of failure Pf ail (Goals)
Cost function value (Delta V) J ⋆ [m/sec]

Sulu
3.9
0.92
1.0
7.30

the p-Sulu Planner
11.4
0.0024
0.0029
7.32

Nominal
0.09
< 10−6
< 10−6
8.73

Table 5: Performance comparison of Sulu, the p-Sulu Planner, and the nominal approach on the
HTV rendezvous scenario.

573

O NO , W ILLIAMS , & B LACKMORE

8. Conclusions
This article introduced a model-based planner, the p-Sulu Planner, which operates within userspecified risk bounds. The p-Sulu Planner optimizes a continuous control sequence and a discrete
schedule, given as input a continuous stochastic plant model, an objective function, and a newly
developed plan representation, a chance-constrained qualitative state plan (CCQSP). A CCQSP
involves time-evolved goals, simple temporal constraints, and chance constraints, which specify the
user’s acceptable levels of risk on subsets of the plan.
Our approach to developing the p-Sulu Planner was two-fold. In the first step, we developed
an efficient algorithm, called non-convex iterative risk allocation (NIRA), that can plan in a nonconvex state space but for a fixed schedule. We solved the problem based on the key concept of
risk allocation and risk selection, which achieves tractability by allocating the specified risk to individual constraints and by mapping the result into an equivalent disjunctive convex program. The
NIRA algorithm employs a branch-and-bound algorithm to solve the disjunctive convex program.
Its subproblems are fixed-schedule CCQSP problems with a convex state space, which can be solved
by our previously developed algorithms (Blackmore & Ono, 2009). We developed a novel relaxation method called fixed risk relaxation (FRR), which provides the tightest linear relaxation of the
nonlinear constraints in the convex subproblems.
In the second step, we developed the p-Sulu Planner, which can solve a CCQSP planning problem with a flexible schedule. The scheduling problem was formulated as a combinatorial constrained
optimization problem (COP), which is again solved by a branch-and-bound algorithm. Each subproblem of the branch-and-bound search is a CCQSP planning problem with a fixed schedule, which
is solved by NIRA. The domain of the feasible schedule is pruned by running a shortest-path algorithm on the d-graph representation of the given temporal constraints. The lower bounds of the optimal objective value of the subproblems are obtained by solving fixed-schedule CCQSP planning
problems where a subset of the state constraints are imposed. We proposed an efficient variable
ordering that prioritizes convex subproblems over non-convex ones. We demonstrated the p-Sulu
Planner on various examples, from a personal aerial transportation system to autonomous space
rendezvous, and showed that it can efficiently solve CCQSP planning problems with small suboptimality, compared to past algorithms.

Acknowledgments
This paper is based upon work supported in part by the Boeing Company under Grant No. MITBA-GTA-1 and by the National Science Foundation under Grant No. IIS-1017992. Any opinions,
findings, and conclusions or recommendations expressed in this publication are those of the authors
and do not necessarily reflect the view of the sponsoring agencies. We would like to thank Michael
Kerstetter, Scott Smith, Ronald Provine, and Hui Li at Boeing Company for their support. Thanks
also to Robert Irwin for advice on the draft.

References
Acikmese, B., Carson III, J. M., & Bayard, D. S. (2011). A robust model predictive control algorithm
for incrementally conic uncertain/nonlinear systems. International Journal of Robust and
Nonlinear Control, 21(5), 563–590.
574

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Aircraft Owners and Pilots Association Air Safety Foundation (2005). 2005 Joseph T. Nall Report
- accident trands and factors for 2004..
Altman, E. (1999). Constrained Markov decision processes. Stochastic modeling. Chapman &
Hall/CRC.
Alur, R., Feder, T., & Henzinger, T. A. (1996). The benefits of relaxing punctuality. Journal of the
ACM, 43.
Bacchus, F., & Kabanza, F. (1998). Planning for temporally extended goals. Annals of Mathematics
and Artificial Intelligence, pp. 5–27.
Balas, E. (1979). Disjunctive programming. Annals of Discrete Mathematics.
Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control Volume I (Third Edition).
Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming (1st edition). Athena
Scientific.
Blackmore, L. (2006). A probabilistic particle control approach to optimal, robust predictive control.
In Proceedings of the AIAA Guidance, Navigation and Control Conference.
Blackmore, L., Li, H., & Williams, B. C. (2006). A probabilistic approach to optimal robust path
planning with obstacles. In Proceedings of American Control Conference.
Blackmore, L., & Ono, M. (2009). Convex chance constrained predictive control without sampling.
In Proceedings of the AIAA Guidance, Navigation and Control Conference.
Boyan, J. A., & Littman, M. L. (2000). Exact solutions to time-dependent MDPs. In in Advances
in Neural Information Processing Systems, pp. 1026–1032. MIT Press.
Boyan, J. A., & Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximating the value function. Advances in Neural Information Processing Systems 7.
Campbell, M. E., & Udrea, B. (2002). Collision avoidance in satellite clusters. In Proceedings of
the American Control Conference.
Charnes, A., & Cooper, W. W. (1959). Chance-constrained programming. Management Science, 6,
73–79.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2012). Colin: Planning with continuous linear numeric
change. J. Artif. Intell. Res. (JAIR), 44, 1–96.
Dechter, R. (2003). Constraint Processing. Elsevier.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,
61–95.
Dolgov, D., & Durfee, E. (2005). Stationary deterministic policies for constrained MDPs with multiple rewards, costs, and discount factors. In In Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence (IJCAI-05, pp. 1326–1331.
Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming for structured
continuous markov decision problems. In Proceedings of the Proceedings of the Twentieth
Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI-04), pp. 154–
161, Arlington, Virginia. AUAI Press.
575

O NO , W ILLIAMS , & B LACKMORE

Fleming, W., & McEneaney, W. (1995). Risk-sensitive control on an infinite time horizon. SIAM
Journal on Control and Optimization, 33(6), 1881–1915.
Fox, M., & Long, D. (2006). Modelling mixed discrete-continuous domains for planning. Journal
of Artificial Intelligence Research, 27, 235–297.
Geibel, P., & Wysotzki, F. (2005). Risk-sensitive reinforcement learning applied to control under
constraints. Journal of Artificial Intelligence Research, 24, 81–108.
Goulart, P. J., Kerrigan, E. C., & Maciejowski, J. M. (2006). Optimization over state feedback
policies for robust control with constraints. Automatica, 42(4), 523 – 533.
Hofmann, A. G., & Williams, B. C. (2006). Robust execution of temporally flexible plans for bipedal
walking devices. In Proceedings of the International Conference on Automated Planning and
Scheduling (ICAPS-06).
Jacobson, D. (1973). Optimal stochastic linear systems with exponential performance criteria and
their relation to deterministic differential games. Automatic Control, IEEE Transactions on,
18(2), 124 – 131.
Japan Aerospace Exploration Agency (2009). HTV-1 mission press kit. Available on-line at http:
//www.jaxa.jp/countdown/h2bf1/pdf/presskit_htv_e.pdf.
Kuwata, Y., & How, J. P. (2011). Cooperative distributed robust trajectory optimization using receding horizon MILP. IEEE Transactions on Control Systems Technology, 19(2), 423–431.
Kuwata, Y. (2003). Real-time trajectory design for unmanned aerial vehicles using receding horizon
control. Master’s thesis, Massachusetts Institute of Technology.
Kvarnstrom, J., & Doherty, P. (2000). Talplanner: A temporal logic based forward chaining planner.
Annals of Mathematics and Artificial Intelligence.
Lagoudakis, M. G., & Parr, R. (2003). Least-squares policy iteration. Journal of Machine Learning
Research, 4, 2003.
Léauté, T. (2005). Coordinating agile systems through the model-based execution of temporal plans.
Master’s thesis, Massachusetts Institute of Technology.
Léauté, T., & Williams, B. C. (2005). Coordinating agile systems through the model-based execution of temporal plans. In Proceedings of the Twentieth National Conference on Artificial
Intelligence (AAAI).
Li, H., & Williams, B. C. (2005). Generalized conflict learning for hybrid discrete linear optimization. In Proc. 11th International Conf. on Principles and Practice of Constraint Programming.
Li, H. X. (2010). Kongming: A Generative Planner for Hybrid Systems with Temporally Extended
Goals. Ph.D. thesis, Massachusetts Institute of Technology.
Matsumoto, S., Dubowsky, S., Jacobsen, S., & Ohkami, Y. (2003). Fly-by approach and guidance
for uncontrolled rotating satellite capture. In Proceedings of AIAA Guidance, Navigation, and
Control Conference and Exhibit.
Nemirovski, A., & Shapiro, A. (2006). Convex approximations of chance constrained programs.
SIAM Journal on Optimization, 17, 969–996.
576

P ROBABILISTIC P LANNING FOR C ONTINUOUS DYNAMIC S YSTEMS UNDER B OUNDED R ISK

Oldewurtel, F., Jones, C. N., & Morari, M. (2008). A tractable approximation of chance constrained
stochastic MPC based on affine disturbance feedback. In Proceedings of Conference on Decision and Control.
Ono, M. (2012). Closed-loop chance-constrained MPC with probabilistic resolvability. In Proceedings of the IEEE Conference on Decision and Control.
Ono, M., Graybill, W., & Williams, B. C. (2012). Risk-sensitive plan execution for connected sustainable home:. In Proceedings of the 4th ACM Workshop On Embedded Systems (BuildSys).
Ono, M., & Williams, B. C. (2008a). An efficient motion planning algorithm for stochastic dynamic
systems with constraints on probability of failure. In Proceedings of the Twenty-Third AAAI
Conference on Artificial Intelligence (AAAI-08).
Ono, M., & Williams, B. C. (2008b). Iterative risk allocation: A new approach to robust model
predictive control with a joint chance constraint. In Proceedings of 47th IEEE Conference on
Decision and Control.
Prékopa, A. (1999). The use of discrete moment bounds in probabilistic constrained stochastic
programming models. Annals of Operations Research, 85, 21–38.
Richards, A., & How, J. (2006). Robust stable model predictive control with constraint tightening.
In American Control Conference, 2006, p. 6 pp.
Sanner, S. (2011). Relational dynamic influence diagram language (RDDL): Language description.
Available at http://users.cecs.anu.edu.au/˜ssanner/IPPC_2011/RDDL.
pdf.
Schaub, H., & Junkins, J. L. (2003). Analytical mechanics of space systems. American Institute of
Aeronautics and Astronautics, Inc.
Shields, J., Sirlin, S., & Wette, M. (2002). Metrology sensor characterization and pointing control
for the formation interferometer testbed (fit). In Proceedings of IEEE Aerospace Conference.
Smith, R., & Hadaegh, F. (2007). Distributed estimation, communication and control for deep space
formations. IET Control Theory and Applications.
Stoorvogel, A. (1992). The H∞ Control Problem: A State Space Approach. Prentice Hall.
Vallado, D. A. (2001). Fundamentals of Astrodynamics and Applications, Second Edition. Microcosm Press.
van Hessem, D. H. (2004). Stochastic inequality constrained closed-loop model predictive control
with application to chemical process operation. Ph.D. thesis, Delft University of Technology.
Wang, X., Yadav, V., & Balakrishnan, S. N. (2007). Cooperative uav formation flying with obstacle/collision avoidance. IEEE Transactions on Control Systems Technology, 15(4).
Wertz, J. R., & Wiley J. Larson, e. (1999). Space Mission Analysis and Design (Third Edition).
Microcosm/Springer.
Younes, H. L. S., & Littman, M. L. (2004). PPDDL1.0: An extension to pddl for expressing planning
domains with probabilistic effects. Tech. rep., Carnegie Mellon University.

577

Journal of Artificial Intelligence Research 46 (2013) 1 – 45

Submitted 07/12; published 01/13

Short and Long Supports for Constraint Propagation
Peter Nightingale
Ian P. Gent
Christopher Jefferson
Ian Miguel

pwn1@st-andrews.ac.uk
ian.gent@st-andrews.ac.uk
caj21@st-andrews.ac.uk
ijm@st-andrews.ac.uk

School of Computer Science, University of St Andrews,
St Andrews, Fife KY16 9SX, UK

Abstract
Special-purpose constraint propagation algorithms frequently make implicit use of short
supports — by examining a subset of the variables, they can infer support (a justification
that a variable-value pair may still form part of an assignment that satisfies the constraint)
for all other variables and values and save substantial work – but short supports have not
been studied in their own right. The two main contributions of this paper are the identification of short supports as important for constraint propagation, and the introduction of
HaggisGAC, an efficient and effective general purpose propagation algorithm for exploiting short supports. Given the complexity of HaggisGAC, we present it as an optimised
version of a simpler algorithm ShortGAC. Although experiments demonstrate the efficiency of ShortGAC compared with other general-purpose propagation algorithms where
a compact set of short supports is available, we show theoretically and experimentally
that HaggisGAC is even better. We also find that HaggisGAC performs better than
GAC-Schema on full-length supports. We also introduce a variant algorithm HaggisGACStable, which is adapted to avoid work on backtracking and in some cases can be faster
and have significant reductions in memory use. All the proposed algorithms are excellent
for propagating disjunctions of constraints. In all experiments with disjunctions we found
our algorithms to be faster than Constructive Or and GAC-Schema by at least an order of
magnitude, and up to three orders of magnitude.

1. Introduction
Constraint solvers typically employ a systematic backtracking search, interleaving the choice
of an assignment of a decision variable with the propagation of the constraints to determine
the consequences of the assignment made. Propagation algorithms can broadly be divided
into two types. The first are specialised to reason very efficiently about constraint patterns
that occur frequently in models. Examples include the global cardinality constraint (Régin,
1996) and the element constraint (Gent, Jefferson, & Miguel, 2006b). It is not feasible to
support every possible constraint expression with a specialised propagator in this way, in
which case general-purpose constraint propagators, such as GAC-Schema (Bessière & Régin,
1997), GAC2001/3.1 (Bessière, Régin, Yap, & Zhang, 2005), STR2 (Lecoutre, 2011) or
MDDC (Cheng & Yap, 2010) are used. These are typically more expensive than specialised
propagators but are an important tool when no specialised propagator is available.
A support in a constraint for a domain value of a variable is a justification that the value
may still form part of an assignment that satisfies the constraint. It is usually given in terms
of a set of literals: variable-value pairs corresponding to possible assignments to the other
c
2013
AI Access Foundation. All rights reserved.

Nightingale, Gent, Jefferson, & Miguel

variables in the constraint. One of the efficiencies typically found in specialised propagators
is the use of short supports: by examining a subset of the variables, they can infer support
for all other variables and values and save substantial work. This use is typically implicit,
i.e. achieved through a specialised algorithm which does not examine all variables in all
cases. One of our contributions is to highlight the general importance of short supports.
As an example, consider the element constraint xy = z, with x0 , x1 , x2 , y ∈ {0 . . . 2},
z ∈ {0 . . . 3}. This constraint is satisfied iff the element in position y of vector [x0 , x1 , x2 ]
equals z. Consider the set of literals S = {x0 7→ 1, y 7→ 0, z 7→ 1}. This set clearly satisfies
the definition of the constraint xy = z, but it does not contain a literal for each variable.
Any extension of S with valid literals for variables x1 and x2 is a support. S is an example
of a short support.
In our previous work we introduced ShortGAC (Nightingale, Gent, Jefferson, & Miguel,
2011), a general-purpose propagation algorithm that exploits short supports. Until the introduction of ShortGAC, general-purpose propagators relied upon supports involving all
variables. In this paper we develop the concept further and introduce a new algorithm
HaggisGAC,1 which is consistently more efficient than ShortGAC. Where available, the
use of compact sets of short supports allows HaggisGAC to outperform greatly existing general-purpose propagation algorithms. In some cases, HaggisGAC even approaches
the performance of special-purpose propagators. HaggisGAC is also very well suited to
propagating disjunctions of constraints, and outperforms the traditional Constructive Or
algorithm (Lagerkvist & Schulte, 2009; Würtz & Müller, 1996) by orders of magnitude.
HaggisGAC is also more efficient than GAC-Schema on full-length supports. We also
describe a variant, HaggisGAC-Stable, in which supports do not need to be deleted on
backtracking. Applied to full-length supports, this version has greatly reduced memory
usage.
ShortGAC, HaggisGAC and HaggisGAC-Stable are all instantiated with a function named findNewSupport (and are similar to GAC-Schema in this way). This function
can be specific to a constraint, and generate short supports procedurally. Alternatively, a
generic findNewSupport can retrieve short supports from a data structure.
Section 2 presents the necessary background, and Section 3 introduces the concept
of short support. Section 4 outlines the basic idea used to deal with implicit supports
throughout the paper. Section 5 gives full details of ShortGAC, including the complexity of
key operations and alternative implementations for when short supports are provided in list
form. Section 6 presents the new algorithm HaggisGAC as a development of ShortGAC.
Both ShortGAC and HaggisGAC are evaluated experimentally in Section 7. Section 8
describes HaggisGAC-Stable, with corresponding experiments in Section 9. Finally,
Sections 10 and 11 discuss related work and present our conclusions.

1. HaggisGAC is named for the legendary wild haggis of Scotland, which has both short legs and long
legs for walking around hills. Like its namesake, HaggisGAC copes with both full-length and shorter
supports and originates in Scotland. Details of the wild haggis can be found on Wikipedia, http:
//en.wikipedia.org/wiki/Wild_haggis, and in the Veterinary Record (King, Cromarty, Paterson, &
Boyd, 2007).

2

Short and Long Supports for Constraint Propagation

2. Supports, GAC, Triggers
A constraint satisfaction problem (CSP) is defined as a set of variables X, a function that
maps each variable to its domain, D : X → 2Z where each domain is a finite set, and a set
of constraints C. A constraint c ∈ C is a relation over a subset of the variables X. The
scope of a constraint c, named scope(c), is the set of variables that c constrains.
A solution to a CSP is a function s : X → Z that maps each variable x ∈ X to a value
from D(x), such that for every constraint c ∈ C, the values of scope(c) form a tuple that is
in c (i.e. the constraint is satisfied ).
During a systematic search for a solution to a CSP, values are progressively removed
from the domains D. Therefore, we distinguish between the initial domains and the current
domains. The function D refers to the current domains unless stated otherwise. A literal
is defined as a variable-value pair, and is written x 7→ v. A literal x 7→ v is valid if v is in
the current domain of x (i.e. v ∈ D(x)).
Definition 2.1. [Support] A support S for constraint c and domains D is defined as
a set of valid literals that contains exactly one valid literal for each variable in scope(c)
and satisfies c. Where necessary for disambiguation, we call such a support a full-length
support or simply long support, to contrast with short supports as defined later.
A property commonly established by constraint propagation algorithms is generalised
arc consistency (GAC) (Mackworth, 1977). A constraint c is GAC if and only if there exists
a full-length support for every valid literal of every variable in scope(c). GAC is established
by identifying all literals x 7→ v for which no full-length support exists and removing v from
the domain of x. We consider only algorithms for establishing GAC in this paper.
A GAC propagation algorithm is usually situated in a systematic search. Hence, it
must operate in three contexts: initialisation (at the root node), where support is established
from scratch; following the deletion of one or more domain values (as a result of a branching
decision and/or the propagation of other constraints), where support must be re-established
selectively; and upon backtracking, where data structures must be restored to the correct
state for this point in search. Our primary focus will be on the second context, operation
following value deletion, although we will discuss efficient backtracking in Section 8. A
GAC propagation algorithm would typically be called for each deleted domain value in
turn. Once the algorithm has been called for each such domain value, the constraint will
be GAC.
The propagation algorithms we present have the concept of active support, inspired by
GAC-Schema (Bessière & Régin, 1997). An active support is a support that is currently
in use to support a set of literals. Each literal has a set of active supports that support
it. When an active support is found to be invalid, it is removed. When the set for some
literal is empty, we say the literal has lost support. A new support is sought for the literal,
and if found the new support becomes active. If no new support is found, the literal has no
support and it is deleted.
In the propagation algorithms we present, for efficiency we make use of ‘watched literals’
as provided in Minion (Gent et al., 2006b), because propagators need not be called for every
deleted domain value to establish GAC. We say that propagators attach and remove triggers
on literals. When a domain value v for variable x is deleted, the propagator is called if and
3

Nightingale, Gent, Jefferson, & Miguel

only if it has a trigger attached to the literal x 7→ v. Doing so means that when a literal
is deleted which is not attached to a trigger, zero work is incurred. We should emphasise
that the use of watched literals is not fundamental to our work. If they are not available
in a given solver, our algorithms only need a minor adaptation. When called on any literal
removal, we may just return immediately if the literal is not in any active support, which
can be checked in time O(1). Thus our algorithms fit the traditional fine-grained scheme
(Bessière & Régin, 1997) except that in some cases they will not be invoked because they
use watched literals.

3. Short Supports
The concept of a short support is a generalisation of full-length support. It is defined below.
Definition 3.1. [Short support] A short support S for constraint c and domains D is
defined as a set of valid literals x 7→ v such that x ∈ scope(c), x occurs only once in S,
and every superset of S that contains one valid literal for each variable in scope(c) is a
full-length support. A strict short support is a short support that is not a full-length
support.
The definition of short support includes both extremes. The empty set is a short support when the constraint is entailed (i.e. every tuple on scope(c) within D satisfies the
constraint). Similarly, every full-length support S is necessarily a short support, because
the only superset of S is itself. In our case studies we will see examples of both empty short
supports and short supports that also happen to be full length.
Short supports can be used to maintain GAC. Just as with a full-length support, a
short support provides GAC support for each literal contained within it. We call this
explicit support for those literals. The new feature is that a short support also provides
support for all valid literals of all variables not contained in the short support. This is
because, by definition, every valid extension of the short support to cover all variables in
scope(c) is a full-length support. We say that a short support gives implicit GAC support
for all valid literals of variables not in the short support.
We also define the concept of a complete set of short supports for a constraint.
Definition 3.2. [Short support set] A short support set S(c, D) is a set of short supports
for constraint c under domains D, such that every full-length support S of c under D is a
(not necessarily strict) superset of at least one short support S 0 ∈ S(c, D).
A constraint may have many short support sets. This gives us some latitude to implement one that is efficient to compute.
It is natural to ask how we can identify correct short supports given a constraint c. A
simple but fundamental result is given in Lemma 3.3.
Lemma 3.3. Given a constraint c and domains D, the empty set {} is a short support for
c iff GAC propagation for the constraint not(c) leads to an empty domain.
Proof. {} is a short support if and only if every valid assignment to variables in scope(c)
satisfies c. Every assignment satisfies c iff every assignment violates not(c). If every assignment violates not(c), then GAC propagation for the constraint not(c) leads to an empty
4

Short and Long Supports for Constraint Propagation

domain. To complete this last equivalence, note that if any assignment does not violate
not(c), all literals in that assignment are supported, so GAC propagation cannot cause an
empty domain.
This lemma has two important consequences. First, we can check any short support for
correctness, not just the empty support. To check a short support S = {x1 7→ v1 , . . . , xk 7→
vk }, we can simply set D(x1 ) = {v1 }, . . . , D(xk ) = {vk }. All assignments now extend S, so
S is a short support iff {} is. Lemma 3.3 applies so we can check the correctness of S by
propagating not(c) and seeing if a domain is emptied.
The second consequence is negative, however. Determining whether GAC propagation
will empty a domain is polynomially equivalent to actually performing GAC propagation
(Bessière, Hebrard, Hnich, & Walsh, 2007). Since some constraints are NP hard to GAC
propagate, it follows that it is not easy even to check if the empty set is a short support.
Thus we cannot expect to find a method which is both fast and general for finding short
supports for a constraint.
Given the provable difficulty of finding short supports from a set of full-length supports,
we construct sets of short supports specifically for each of three experimental case studies
in Section 7. The focus of this paper is to show the value of strict short supports if they
are given to the system. The situation is analogous with that in an important area of
constraints, namely that of exploiting symmetries in constraint problems (Gent, Petrie,
& Puget, 2006). A large majority of research has assumed that sets of symmetries are
provided to the system, even though finding such sets is hard. This has not inhibited
research in exploiting symmetry, within which the automated detection of symmetry has
become an important subarea (Mears, 2009; Puget, 2005): however we leave the automated
construction of compact short support sets to future research. Analogously to patterns such
as matrix symmetries (Flener, Frisch, Hnich, Kiziltan, Miguel, Pearson, & Walsh, 2002),
we can at least identify a pattern which often lets us identify strict short supports, as we
now describe.
3.1 Short Supports and Disjunction
Strict short supports arise naturally from disjunctions. If a constraint can be expressed as
a disjunction of shorter constraints, then a set of strict short supports can be constructed
for it. Suppose we have the following constraint.
c(x1 , x2 , x3 , x4 ) ≡ c1 (x1 , x2 ) ∨ c2 (x2 , x3 ) ∨ c3 (c3 , x4 )
Suppose also that A = {x1 7→ 2, x2 7→ 1} is a valid assignment that satisfies c1 . If we satisfy
c1 , we satisfy c regardless of the values of x3 and x4 . Therefore A = {x1 7→ 2, x2 7→ 1} is a
strict short support for c.
Lemma 3.4. Given constraint c, a domain set D, and a set of constraints {c1 . . . ck } where
∀ci ∈ {c1 . . . ck } : scope(ci ) ⊆ scope(c) and c ≡ c1 ∨ · · · ∨ ck , the following is a short support
set (where we write fls(ci , D) to mean the full-length supports of ci w.r.t. domains D):
S(c, D) = {S | S ∈ fls(c1 , D) ∨ · · · ∨ S ∈ fls(ck , D)}
5

Nightingale, Gent, Jefferson, & Miguel

Proof. (a) Each element of S(c, D) is a short support according to Definition 3.1 by the
semantics of disjunction. (b) S(c, D) is a short support set by Definition 3.2. Every fulllength support of c must satisfy some disjunct ci , therefore the full-length support contains
a full-length support for ci that is included in S(c, D).
Lemma 3.4 allows a short support set to be created for any disjunction, given the initial
domains. We do this for two of our three case studies (for the third, the set is prohibitively
large).
Using a similar approach to Lemma 3.4 we can create a function that generates short
supports on demand. The function takes a valid literal x 7→ v and the current domains
D, and returns a short support that supports x 7→ v (explicitly or implicitly), or Null if
none exists. The function can be constructed as follows. We create new domains D0 where
D0 (x) = {v}, and otherwise D0 is identical to D. If no disjunct is satisfiable under D0 , then
the function returns Null. Otherwise, the function picks any disjunct ck that is satisfiable
under D0 , and returns a satisfying assignment of ck that is valid under D0 . For each of the
three case studies in Section 7, we created a function that follows this scheme with some
optimisations.
Propagating disjunctions is recognised to be an important topic. Many papers have
been published in this area (Würtz & Müller, 1996; Lhomme, 2003; Lagerkvist & Schulte,
2009; Jefferson, Moore, Nightingale, & Petrie, 2010). Exploiting strict short supports in the
algorithms ShortGAC, HaggisGAC and HaggisGAC-Stable allows us to outperform
the traditional Constructive Or algorithm (Würtz & Müller, 1996) by orders of magnitude.
3.2 Backtrack Stability of Short Supports
Within a search tree, propagation algorithms often spend significant time backtracking
data structures. Reducing or eliminating backtracking can improve efficiency. For example,
avoiding backtracking triggers can speed up a simple table propagator by more than 2 times
(Gent et al., 2006b), and MAC-6 and MAC-7 can be much more efficient (in both space
and time) if backtracking is avoided (Régin, 2005). There are two potential advantages
of reducing use of backtracking state: it saves time restoring data structures, and it saves
space by avoiding storing supports on the backtrack stack.
Definition 3.5. [Backtrack Stable] A short support of constraint c with current domains D
is backtrack stable iff it always remains a short support (according to Definition 3.1) after
backtracking up the search tree.
A short support s may support some variable x implicitly, and as we backtrack we may
add values back into the domain of x that are not consistent with s, meaning that s no
longer meets the definition of a short support. We give an example below.
Example 3.1. Consider the constraint b → M [x] = y, for a boolean variable b, array of
variables M and variables x and y. When b is assigned False, this constraint is entailed,
and so the empty short support can be used to support all literals in M, x and y. This
support is not backtrack stable, as on backtracking when True is restored to the domain of
b, the empty set is no longer a short support.
6

Short and Long Supports for Constraint Propagation

Any support that is full length is backtrack stable: whenever the support is valid it
supports all literals it contains. Backtrack stable supports always exist because we can use
full-length supports in all cases (as in GAC-Schema), although these may be much longer
than necessary.
In Section 8 we exploit backtrack stability to define a new algorithm.

4. ShortGAC: An Overview
This section summarises the key ideas of the ShortGAC propagation algorithm, along
with an illustrative example.2
ShortGAC maintains a set of short supports sufficient to support all valid literals of
the variables in the scope of the constraint it is propagating. We refer to these as the active
supports. The algorithm rests on exploiting the observation that, using short supports,
support can be established for a literal in two ways. First, as usual, a short support that
contains a literal supports that literal. Second, a literal x 7→ v is supported by a short
support that contains no literal of variable x. Hence, the only short supports that do not
support x 7→ v are those which contain a literal x 7→ w for some other value w 6= v.
The following data structures are central to the operation of the ShortGAC algorithm:
numSupports is the total number of active short supports.
supportsPerVar is an array (indexed by [x]) indicating the number of active short supports
containing each variable x.
supportListPerLit is an array (indexed by [x 7→ v]) of lists of active short supports containing each literal x 7→ v.
If the number of supports containing some variable x is less than the total number of
supports then there exists a support s that does not contain x. Therefore, s supports all
literals of x. The algorithm spends no time processing variables all of whose literals are
known to be supported in this way. Only for variables involved in all active supports do we
have to seek support for literals with no active supports.
To illustrate, we consider the element example from the introduction: xy = z, with
x0 , x1 , x2 , y ∈ {0 . . . 2}, and z ∈ {0 . . . 3}. This constraint is satisfied iff the element in
position y of vector [x0 , x1 , x2 ] equals z. Suppose in the current state ShortGAC is storing
just one support: A = {x0 7→ 1, y 7→ 0, z 7→ 1}. The data structures are as follows, where
× indicates that a literal is not valid.3
2. The details we present here are different from those we presented previously (Nightingale et al., 2011), as
we have optimised the data structures and algorithms compared with our previous work. The two most
significant changes are: we no longer keep a count of supports per literal, saving overhead in maintaining
this; and data is stored in a one dimensional vector by literal, instead of a two dimensional array by
variable/value, saving space if variables in a constraint have very different domain sizes. Experiments in
Appendix A demonstrate that the algorithms and data structures presented here perform better than
our previous implementation.
3. For clarity, we have presented the one-dimensional array supportListPerLit in a two-dimensional format.

7

Nightingale, Gent, Jefferson, & Miguel

Supports:
supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

x0 →
7 1, y 7→ 0, z
Variable
x1 x2
y
{} {} {A}
{} {}
{}
{} {}
{}
× ×
×
0
0
1
1

A:
x0
{}
{A}
{}
×
1

7→ 1
z
{}
{A}
{}
{}
1

All values of x1 and x2 have support, since their supportsPerVar counters are both
less than numSupports. Therefore the ShortGAC algorithm can ignore x1 and x2 and
only look for new supports of x0 , y and z. Consider finding a new support for literals
in z. ShortGAC can ignore the literals with at least one support – in this case z 7→ 1.
The algorithm looks for literals z 7→ a where supportListPerLit[z, a] = {}. Here, z 7→ 0
is such a literal, so ShortGAC seeks a new support for it. A possible new support is
B = {x1 7→ 0, y 7→ 1, z 7→ 0}. Following its discovery, we update the data structures:
Supports:
supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
x0
{}
{A}
{}
×
1

x0 →
7 1, y 7→ 0, z
x1 →
7 0, y 7→ 1, z
Variable
x1
x2
y
{B} {} {A}
{}
{} {B}
{}
{}
{}
×
×
×
1
0
2
2

7→ 1
7→ 0
z
{B}
{A}
{}
{}
2

Now variable x0 is also fully supported, since supportsPerVar[x0 ] < numSupports. There
remain three literals for which support has not been established: y 7→ 2, z 7→ 2 and z 7→ 3.
For the first two ShortGAC finds supports such as C = {x0 7→ 2, y 7→ 0, z 7→ 2} and
D = {x2 7→ 0, y 7→ 2, z 7→ 0}. No support exists for z 7→ 3, so 3 will be deleted, giving:
Supports:

supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
C:
D:
x0
{}
{A}
{C}
×
2

7→ 1, y 7→ 0, z
7→ 0, y 7→ 1, z
7→ 2, y 7→ 0, z
7→ 0, y 7→ 2, z
Variable
x2
y
{D} {A, C}
{}
{B}
{}
{D}
×
×
1
4
4

x0
x1
x0
x2
x1
{B}
{}
{}
×
1

7→ 1
7→ 0
7→ 2
7→ 0
z
{B, D}
{A}
{C}
×
4

All valid literals are now supported. Nothing further need be done until a change in
state, such as the removal of a value by a branching decision or propagation.
8

Short and Long Supports for Constraint Propagation

5. ShortGAC: Details
The key tasks in implementing ShortGAC are: data structure update; iteration over
variables where supportsPerVar equals numSupports; and iteration over the unsupported
values of a variable. This section describes the infrastructure that allows us to perform
each of these tasks efficiently.
5.1 ShortGAC Data Structures
An active short support S of arity k provides explicit support for each of the k literals it
contains. Therefore, a reference to S must appear in k of the lists of supportListPerLit. To
do this, we represent S with two types of object: ShortSupport and ShortSupportCell. The
ShortSupport object contains k ShortSupportCell objects, each of which contains a literal
x 7→ v,4 and a reference to the parent ShortSupport. The elements of the array supportListPerLit are doubly-linked lists of ShortSupportCells. Through the reference to the parent
ShortSupport, we can iterate through all active short supports for a given literal.
The algorithm iterates over all variables x where supportsPerVar[x] equals numSupports.
The following data structure represents a partition of the variables by the number of supports. It allows constant time size checking and linear-time iteration of each cell in the
partition, and allows any variable to be moved into an adjacent cell (i.e. if the number
of supports increases or decreases by 1) in constant time. It is inspired by the indexed
dependency array in Gecode (Schulte & Tack, 2010).
varsBySupport is an array containing a permutation of the variables. Variables are ordered
by non-decreasing number of active supports (supportsPerVar[x]).
supportNumLowIdx is an array of integers, indexed from 0 to the number of literals, that
being the maximum number of active supports possible. Either supportNumLowIdx[i] is
the smallest index in varsBySupport with i or more active supports, or (when there are
no such variables) supportNumLowIdx[i]= k where k is the total number of variables.
k acts as a sentinel value. The set of variables with i supports is:
varsBySupport[supportNumLowIdx[i] . . . supportNumLowIdx[i + 1] − 1]
Initially, all variables have 0 active supports, so supportNumLowIdx[0] = 0 and the rest
of the array is set to k.
The following table illustrates how the partition data structure works (on a different
example with 11 variables). Suppose supportsPerVar[x2 ] changed from 7 to 6. x2 and y1
(boxed) are swapped in varsBySupport and the cell boundary is moved so that x2 is in the
lower cell. Consequently, supportNumLowIdx[7] is incremented by 1.
varsBySupport[]
supportsPerVar
x2 updated

w1
6
w1

w2
6
w2

y1
7
x2

x1
7
x1

x2
7
y1

y2
7
y2

y3
7
y3

x3
7
x3

z1
8
z1

z2
8
z2

z3
8
z3

4. A literal x 7→ v is represented using a single integer i. There is a mapping between x 7→ v and i, which
allows O(1) access to x and v from i and vice-versa.

9

Nightingale, Gent, Jefferson, & Miguel

Require: sup: a ShortSupport
1: for all sc: ShortSupportCell ∈ sup do
2:
(x 7→ v) ← sc.literal
3:
if supportListPerLit[x 7→ v] = {} then
4:
attachTrigger(x 7→ v)
5:
Add sc to doubly linked list supportListPerLit[x 7→ v]
6:
supportsPerVar[x]++
7:
sx ← supportsPerVar[x]
8:
cellend ← supportNumLowIdx[sx ]−1
9:
swap(x, varsBySupport[cellend ])
10:
supportNumLowIdx[sx ]-11: numSupports++

Procedure 1: addSupport(sup)
For a variable x with supportsPerVar[x] = numSupports, ShortGAC iterates over the
values with zero explicit supports. To avoid iterating over all values, we use a set data
structure:
zeroLits is an array (indexed by [x]) of stacks containing the literals of variable x with zero
explicit support, in no particular order.
inZeroLits is an array (indexed by [x 7→ v]) of booleans indicating whether literal x 7→
v ∈ zeroLits[x].
When supportListPerLit[x 7→ v] is reduced to the empty list, if inZeroLits[x 7→ v] is
false then x 7→ v is pushed onto zeroLits[x] (and inZeroLits[x 7→ v] is set to true). As an
optimisation, values are not eagerly removed from the set; they are only removed lazily when
the set is iterated. Also, the set is not backtracked. During iteration, a non-zero value is
removed by swapping it to the top of the stack, and popping. This lazy maintenance never
costs work overall because, if the value would have been removed eagerly, then it will be
removed the next time the set is iterated, costing O(1). It can save work, because we may
never iterate over the list before the value would have been restored to the set again.
We use a free list to manage the set of ShortSupport objects to avoid the cost of unnecessary object construction/destruction. The ShortSupport object retrieved from the free list
may contain too few ShortSupportCell objects, so we use a resizable vector data structure.
The size is only ever increased.
5.2 Adding and Deleting Supports
When a support is added or deleted, all the data structures described above must be
updated. This is done by Procedures 1 (addSupport) and 2 (deleteSupport). Both these
procedures iterate through the given short support, and for each literal in it they update
supportListPerLit, supportsPerVar, varsBySupport and supportNumLowIdx. Procedure 2 also
inserts the literal into zeroLits if necessary. We briefly explain the maintenance of varsBySupport as it will become important in Section 6.2. Suppose we are adding support for
literal x 7→ v in Procedure 1. Because it has an additional support, x must be moved to the
next cell in varsBySupport. Line 8 finds the end of the cell that x is in, then we swap x to the
10

Short and Long Supports for Constraint Propagation

Require: sup: a ShortSupport
1: for all sc: ShortSupportCell ∈ sup do
2:
(x 7→ v) ← sc.literal
3:
Remove sc from doubly-linked list supportListPerLit[x 7→ v]
4:
supportsPerVar[x]-5:
if supportListPerLit[x 7→ v] = {} then
6:
removeTrigger(x 7→ v)
7:
if ¬inZeroLits[x 7→ v] then
8:
inZeroLits[x 7→ v] ← true
9:
zeroLits[x].push(x 7→ v)
10:
sx ← supportsPerVar[x]
11:
cellend ← supportNumLowIdx[sx +1]
12:
swap(x, varsBySupport[cellend ])
13:
supportNumLowIdx[sx +1]++
14: numSupports--

Procedure 2: deleteSupport(sup)

Require: x 67→ v (where v has been pruned from the domain of x)
1: while supportListPerLit[x 7→ v] 6= {} do
2:
deleteSupport(supportListPerLit[x 7→ v].pop())
3: repeat
4:
continueLoop ← false
5:
for all i ∈ {supportNumLowIdx[numSupports]. . . supportNumLowIdx[numSupports+1]-1} do
6:
y ← varsBySupport[i]
7:
if ShortGAC-variableUpdate(y) = true then
8:
continueLoop ← true
9:
break out of for loop Line 5
10: until continueLoop = false

Procedure 3: ShortGAC-Propagate: propagate(x 67→ v)

end of its cell using a subroutine swap(xi , xj ). This simple procedure (not given) locates
and swaps the two variables in varsBySupport, leaving other variables unaffected. To do so
it makes use of a second array, varsBySupInv, which is the inverse mapping of varsBySupport.
Having done this, the cell boundary is decremented so that (in its new position), x is now
in the higher cell. Another point to note is that addSupport will add a trigger for x 7→ v
if sup is the only active explicit support to contain that literal, while deleteSupport will
remove the trigger if the deleted support is the only support.
Finally, we note that we do not have special-purpose methods to undo these changes
on backtracking. On backtracking past the point where a support is added, we simply
call deleteSupport, and similarly we call addSupport when we backtrack past a support’s
deletion.
11

Nightingale, Gent, Jefferson, & Miguel

Require: variable x
1: for all (x 7→ v) ∈ zeroLits[x] do
2:
if supportListPerLit[x 7→ v] 6= {} then
3:
Remove (x 7→ v) from zeroLits[x]
4:
else
5:
if v ∈ D(x) then
6:
sup ← findNewSupport(x 7→ v)
7:
if sup = Null then
8:
prune(x 7→ v)
9:
else
10:
addSupport(sup)
11:
if supportListPerLit[x 7→ v] 6= {} then
12:
Remove (x 7→ v) from zeroLits[x]
13:
return true
14: return false

Procedure 4: ShortGAC-variableUpdate: (x). Here and in other pseudocode we abstract
the detailed maintenance of the zeroLits and inZeroLits data structures. It might seem that
the test on Line 11 must always succeed. However, although sup must support x 7→ v, it
does not have to contain x 7→ v as it might be an implicit support. The findNewSupport
function is discussed in Section 5.5.
5.3 The Propagation Algorithm
The ShortGAC propagator (Procedure 3) is only invoked when a literal contained in
one or more active short supports is pruned.5 It first deletes all supports involving the
pruned literal. Then it checks all variables y which are not implicitly supported, i.e. where
supportsPerVar[y]=numSupports (Line 5). Each such variable y is checked by Procedure 4
(ShortGAC-variableUpdate, described below). If this call results in a new support being
found, then the data structures will have changed (ShortGAC-variableUpdate(y) returns
true to indicate this) and we must break out of the for-all-loop (Line 9) and go round again.
Iteration therefore continues until either no new support is necessary or no new support
can be found.
ShortGAC-variableUpdate (Procedure 4) is used to check the status of every variable
lacking implicit support. It iterates over zeroLits, i.e. the literals for a variable which might
have zero explicit supports. Since zeroLits is maintained lazily, on each iteration we first
check that the literal does indeed have no explicit support, and correct zeroLits if necessary
(Lines 2–3). The important case is that the literal indeed has no support. Then, provided
that v is in the current domain of x, we must seek a new support by calling findNewSupport
for the constraint. If there is no support, value v must be pruned from the domain of x, or
if we have found a support we update data structures by calling addSupport.
To initialise data structures at the root of search, Lines 3–10 of Procedure 3 are invoked.
Notice that these lines do not refer to the parameter x 67→ v, and on first calling there are
no supports at all so the initial iteration at Line 5 is over all variables.
5. As we noted earlier, if watched literals are not available in a solver, a simple check can be made at the
start of the procedure, to return immediately if the removed literal is in no active support.

12

Short and Long Supports for Constraint Propagation

5.4 Complexity Analysis of ShortGAC
In this section we provide a complexity analysis of ShortGAC as it is used incrementally during search in a constraint solver. The analysis has as parameters the arity of the
constraint n, the maximum domain size d, and the cost f of calling findNewSupport. We
assume that both attaching and removing a trigger to a literal are O(1). This is the case
in Minion 0.12.
First we observe that the swap procedure executes in O(1) time: each operation in swap
is O(1) and it does not loop. Secondly we establish the time complexity of the procedures
addSupport and deleteSupport, which are key to the algorithm.
Lemma 5.1. Procedure 1 (addSupport) has time complexity O(n).
Proof. The outer loop on Line 1 iterates over the literals in the short support. In the worst
case, there are n literals. We now consider the steps within this loop. The list test on
Line 3 is O(1), as is the call to attachTrigger on Line 4. Adding the ShortSupportCell to
the doubly-linked list on Line 5 is O(1), as are the following five array dereferences. As
established above, the swap procedure is also O(1). Hence, addSupport is O(n).
Lemma 5.2. Procedure 2 (deleteSupport) has time complexity O(n).
Proof. Similarly to the add Support procedure, the outer loop on Line 1 has at most n
iterations. The removal from the doubly-linked list on Line 3 is O(1), as are the array
dereferences on Line 4 and subsequently. The list test on Line 5 and the call to removeTrigger on Line 6 are both O(1), as is the stack push operation on Line 9. Recalling once again
that the swap procedure is O(1), deleteSupport is O(n).
Theorem 5.3. Procedure 3 (ShortGAC-propagate) has time complexity in O(n2 d2 +ndf ).
The upper bound can be obtained, i.e. the worst case time complexity is in Ω(n2 d2 + ndf ).
Proof. Analysis for the first statement breaks down into three parts.
First, the loop on Line 1 is over the elements of supportListPerLit. The worst case occurs
when nd literals have an explicit support. Of these supports, a maximum of (n − 1)d + 1
can involve a particular literal, because this literal may be in the short support for every
literal of every other variable ((n − 1)d), and itself (1). The cost of the body of this loop is
O(n) from Lemma 5.2, so the total is O(n2 d). This will be dominated by the next part.
The second part is the loop from lines 3–10. The maximum number of iterations in
Line 5 is n when all supports are full length and so the iteration in Line 5 contains all n
variables. Successive calls to Procedure 4 at Line 7 can add at most O(d) new supports.
But each support addition triggers a restart of the loop beginning on Line 5 over all n
variables, for a total of at most O(n2 d) calls to Procedure 4. Each such call involves O(d)
iterations of the loop on Line 1 of Procedure 4. Therefore the innermost loop is run at most
O(n2 d2 ) times.
To complete the proof of the first statement, we consider the cost of the innermost loop
of Procedure 4. Within this loop, most operations are O(1), the exceptions being the call
to findNewSupport on Line 6 (cost f ) and the call to addSupport on Line 10 (cost n from
Lemma 5.1). But f is the dominating cost, since it must at least traverse the new support to
record it. However, of the n2 d2 iterations, there can be at most nd calls to findNewSupport,
13

Nightingale, Gent, Jefferson, & Miguel

after which time all valid literals will have an explicit support. So the cost is either O(n2 d2 )
or O(ndf ), whichever is greater. In any case the cost is O(n2 d2 + ndf ).
The upper bounds of ndf and n2 d2 can be attained in the worst case. If each literal
needs a new support, we have Ω(ndf ) calls to findNewSupport. We can have cost Ω(n2 d2 )
if there are nd literals with explicit support, each of size n, and each variable ends up
with (for example) d/2 values supported and d/2 values deleted. The worst case is thus
Ω(n2 d2 + ndf ).
Procedure 3 can be invoked at most n(d − 1) times in one branch of the search tree,
therefore the complexity for one branch is O(n3 d3 + n2 d2 f ).
5.4.1 A Second Complexity Analysis
The analysis above can be very conservative when the total number, and maximum size, of
short supports is small. Therefore, we give another complexity analysis with two additional
parameters: the maximum length l of short supports returned by findNewSupport, and
the total number s of distinct short supports that may be returned by findNewSupport.
This analysis also pertains to a branch of search rather than a single call to the propagate
algorithm.
The first part of this complexity analysis concerns the s short supports of length l.
Each short support may be added to the active set once, and may be deleted once down a
branch. Each short support must also be found by calling findNewSupport, with cost O(f ).
Lemma 5.1 shows that the addSupport procedure takes O(n) time. The same lemma can
be re-stated in terms of l, because the loop in addSupport will iterate O(l) times, giving a
total time of O(l). This also applies to deleteSupport. Since there are s short supports, the
cost of finding, adding and deleting (collectively processing) short supports is O(s(l + f ))
down a branch.
Secondly, the algorithm may make calls to findNewSupport that return Null. This can
happen at most n(d − 1) + 1 times, because this is the maximum number of domain values
that may be deleted. Therefore the cost is O(ndf ).
In addition, ShortGAC does some operations that have not been charged to either of
the above categories. To analyse these, we must do a top-down analysis of algorithm.
Procedure 3 is invoked O(s) times (each time a short support is invalidated). Lines 1–2
are already charged to processing short supports. The body of the loop on lines 3–10 may
be executed s times when a new support is found, and a further s times when no new
support is found, therefore O(s) times in total down a branch of search.
Now we come to the inner loop on lines 5–9. From Lemma 5.4 (below), unless a domain
is empty there is always one or more active short support. Therefore, at most l variables
will be contained in all active short supports, so at most l variables are in the relevant
partition of varsBySupport, and the loop body will be executed O(l) times.
Lemma 5.4. After initialisation, Procedure 3 always has at least one active short support
or a variable domain is empty.
Proof. Suppose the opposite. The algorithm is invoked each time a literal in an active short
support is pruned, therefore to delete all active short supports they must all contain one
literal x 7→ v. If all active short supports contain variable x, then all values in the domain
14

Short and Long Supports for Constraint Propagation

of x are not implicitly supported and must be explicitly supported. Therefore v must be
the last remaining value in D(x). Now to prune x 7→ v empties the domain and we have a
contradiction.
Down a branch, this causes O(sl) calls to ShortGAC-variableUpdate, on Line 7. Each
call to ShortGAC-variableUpdate takes O(d) time because there may be d − 1 invalid
literals or d explicitly supported literals in zeroLits. Other time spent in this procedure is
charged to processing short supports, or to pruning domains. Therefore in the top-down
analysis the cost is O(sld).
Overall, the time complexity is O(s(l + f ) + ndf + sld), a tighter bound in some cases
than the one given in the section above. For example, a SAT clause has s = n, f = n, l = 1
and d = 2, giving a time complexity of O(n2 ) for a branch of search.
5.5 Instantiation of findNewSupport
Similarly to GAC-Schema (Bessière & Régin, 1997), ShortGAC must be instantiated with
a findNewSupport function. The function takes a valid literal, and returns a support if one
exists, otherwise returns Null. One way to do this is to write a specialist findNewSupport
function for each constraint. We do this in each of the empirical case studies below. In
each case, the findNewSupport function is much simpler than a propagator for the same
constraint. We use Lemma 3.4 to build the findNewSupport functions, which reduces the
task to finding satisfying tuples of simple constraints like x < y and x = y.
The alternative is to write a generic version of findNewSupport for the case where all
short supports are given as a list. We now detail two generic instantiations of findNewSupport for lists, and in our case studies below we compare them with the specialist functions.
5.5.1 findNewSupport-List
We provide a generic instantiation named findNewSupport-List (Procedure 5) that takes a
list of short supports for each literal (supportList), including both the explicit and implicit
short supports for that literal. This is analogous to the Positive instantiation of GACSchema (Bessière & Régin, 1997). FindNewSupport-List has persistent state: listPos, an
array of integers indexed by variable and value, initially 0. This indicates the current
position in the supportList. The algorithm simply iterates through the list of supports,
seeking one where all literals are valid. ListPos is not backtracked, with the consequence
that when the end of the list is reached, we cannot fail immediately and must search again
from the start back to listPos. Down a branch of the search tree, any particular element
of the list may be looked at more than once. However, this algorithm is optimal in both
time and space across the search tree (Gent, 2012). This surprising result is achieved by
amortizing the cost across all branches. Practically, using listPos stops the algorithm always
starting from the first element of the list, and it seems to be a good tradeoff between avoiding
provably unnecessary work and doing too much data structure maintenance.
A constraint-specific findNewSupport can sometimes find shorter supports than findNewSupport-List. This is because a specific findNewSupport can take advantage of current
domains whereas the supportList may only contain supports given the initial domains. For
example, if the constraint becomes entailed, the specific findNewSupport can return the
15

Nightingale, Gent, Jefferson, & Miguel

Require: x, v, supportList
1: for all j ∈ {listPos[x, v]. . .(supportList[x, v].size-1)} do
2:
sup ← supportList[x, v, j]
3:
if all literals in sup are valid then
4:
listPos[x, v] ← j
5:
return sup
6: for all j ∈ {0 . . .listPos[x, v]−1} do
7:
sup ← supportList[x, v, j]
8:
if all literals in sup are valid then
9:
listPos[x, v] ← j
10:
return sup
11: return Null

Procedure 5: findNewSupport-List: findNewSupport(x, v). The first block searches from
the location of the previous support to the end of the support list. If it is unsuccessful the
search restarts from the start of the list in the second block. This circular approach removes
the need to backtrack listPos.
empty support whereas the list version we have presented cannot. We exploit this fact in
Case Study 3 below.
5.5.2 findNewSupport-NDList
The list instantiation has two major disadvantages. First, it can be inefficient because it
is unable to skip over sets of invalid tuples. The literature contains many solutions to
this problem in the context of full-length supports, for example binary search (Lecoutre &
Szymanek, 2006) or tries (Gent, Jefferson, Miguel, & Nightingale, 2007). Second, it can
require a large amount of memory. For each short support S, there are potentially nd
pointers to S, because there is a pointer to it for each literal that S implicitly supports.
In this section we give a second generic list instantiation based on NextDifference lists
(Gent et al., 2007). We have a single list (named supportList) containing all short supports (indexed by an integer), and a second list named NDList where for each support
s =supportList[j], for each literal in the support s[k], NDList[j][k] is the index of the next
support that does not contain literal s[k]. Thus, when searching the list, the algorithm
is able to jump over sets of short supports that all contain the same invalid literal. The
version of findNewSupport for NextDifference lists is given in Procedure 6.
This approach solves both of the problems with the list instantiation: it is able to jump
over sets of invalid short supports, and usually requires substantially less memory. In fact
it it is optimal in space (unlike the list instantiation): given t short supports of length at
most l, the NextDifference list is O(tl). However it uses only one list of supports, therefore
it can spend time searching through short supports that do not support the desired literal.
5.6 Literals of Assigned Variables
Suppose ShortGAC discovers a new support S that contains a literal x 7→ v, and x is assigned to v. Since x can take no value other than v, it is sound to remove x 7→ v from S and
save the overhead of adding it. We apply this minor optimisation in all cases when using
ShortGAC, and also in all cases when using HaggisGAC (described in Section 6). How16

Short and Long Supports for Constraint Propagation

Require: x, v, supportList, NDList
1: j ← listPos[x, v]
2: while j < supportList.size do
3:
sup ← supportList[j]
4:
nextDiff ← NDList[j]
5:
for k ∈ {0 . . . sup.size − 1} do
6:
(y 7→ b) ← sup[k]
7:
if b ∈
/ D(y) or (x = y and v 6= b) then
8:
j ← nextDiff [k] {Jump to next short support where y is assigned a different value.}
9:
continue while loop at Line 2
10:
listPos[x, v] ← j
11:
return sup
12: j ← 0
13: while j < listPos[x, v] do
14:
sup ← supportList[j]
15:
nextDiff ← NDList[j]
16:
for k ∈ {0 . . . sup.size − 1} do
17:
(y 7→ b) ← sup[k]
18:
if b ∈
/ D(y) or (x = y and v 6= b) then
19:
j ← nextDiff [k] {Jump to next short support where y is assigned a different value.}
20:
continue while loop at Line 13
21:
listPos[x, v] ← j
22:
return sup
23: return Null

Procedure 6: findNewSupport-NDlist: findNewSupport(x, v)

ever this optimisation cannot be used with HaggisGAC-Stable (described in Section 8)
because that algorithm retains active supports as it backtracks, and after backtracking the
literal x 7→ v may no longer be assigned.

6. HaggisGAC: Dealing with Both Full-Length and Strict Short Supports
We now introduce HaggisGAC. We show that it has better theoretical properties than
ShortGAC. Furthermore, experiments show it runs substantially faster in many cases on
strict short supports than ShortGAC (which is specialised for strict short supports), and
substantially faster on full-length supports than GAC-Schema.
6.1 Introduction and Motivating Example
ShortGAC is designed to exploit the concept of implicit support, but has some inefficiencies when dealing with explicit supports and especially full-length supports. Consider for
example the constraint AllDifferentExceptZero, in which the constraint is that all non-zero
values in the array must be different, but that zero may occur freely. This constraint might
be used, for example, in a timetabling problem where classes taking place in different rooms
must be different, but we use zero to represent a room being unused and this can occur
multiple times. Suppose we have AllDifferentExceptZero([w, x, y, z]), each variable with initial domain {0, 1, 2, 3}. Supports for the constraint are full-length supports in which every
17

Nightingale, Gent, Jefferson, & Miguel

non-zero value is different, or any three variables equalling zero where the last variable may
take any value. Suppose we execute ShortGAC and reach the following situation:
Supports:

supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
C:
D:
E:
w
{A, B, E}
{}
{}
{C}
4

w→
7 0, x 7→ 2, y 7→ 3, z 7→ 1
w→
7 0, x 7→ 3, y 7→ 2, z 7→ 1
w→
7 3, x 7→ 0, y 7→ 1, z 7→ 2
x 7→ 0, y 7→ 0, z 7→ 0
w 7→ 0, x 7→ 1, y 7→ 2, z 7→ 3
Variable
x
y
z
{C, D}
{D}
{D}
{E}
{C}
{A, B}
{A}
{B, E}
{C}
{B}
{A}
{E}
5
5
5
5

Notice that the lack of explicit supports for w 7→ 1 and w 7→ 2 is acceptable because we
have supportsPerVar[w] = 4 < numSupports = 5. Now suppose the literal y 7→ 0 is deleted
by some other constraint. This causes support D to be deleted, causing the following state:
Supports:

supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
C:
E:

7→ 0, x 7→ 2, y
7→ 0, x 7→ 3, y
7→ 3, x 7→ 0, y
7→ 0, x 7→ 1, y
Variable
x
y
{C}
×
{E}
{C}
{A} {B, E}
{B}
{A}
4
4
4

w
w
w
w

w
{A, B, E}
{}
{}
{C}
4

7→ 3, z
7→ 2, z
7→ 1, z
7→ 2, z

7→ 1
7→ 1
7→ 2
7→ 3

z
{}
{A, B}
{C}
{E}
4

At this point ShortGAC iterates through the zeroLits lists for all variables where
supportsPerVar = numSupports, in this case all four variables. It will discover that we must
find new supports for w 7→ 1, w 7→ 2 and z 7→ 0. However, this is inefficient for two reasons.
First, we should not need to check zeroLits[z] to discover z 7→ 0, because the support list for
z 7→ 0 became empty during the deletion of support D, so we could have discovered it then.
Second, we should only need to look at zeroLits[w] because the deletion of D has caused
w to lose its implicit support. We should not need to check zeroLits for x, y, z because
these variables were not implicitly supported prior to D’s deletion. Removing these two
reasons for inefficiency is the motivation behind our development of HaggisGAC. In this
example, it can focus directly on the literal z 7→ 0 and the set zeroLits[w] as the only literals
potentially needing new support.
The fundamental problem with ShortGAC is that it cannot efficiently detect when a
literal loses its last support. Every variable with no implicit support is checked every time
any support is deleted, so ShortGAC can take O(nd) time to find a single literal that needs
a new support or to discover that there is no such literal. To improve upon this, we wish
18

Short and Long Supports for Constraint Propagation

i
varsBySupport[i]
supportsPerVar
x2 updated
x3 updated
z2 updated
x1 updated
z3 updated
z1 updated
supportsPerVar

0
w1
6
w1
w1
w1
w1
w1
w1
6

1
w2
6
w2
w2
w2
w2
w2
w2
6

2
y1
7
x2
x2
x2
x2
x2
x2
6

3
x1
7
x1
x3
x3
x3
x3
x3
6

4
x2
7
y1
y1
y1
x1
x1
x1
6

5
y2
7
y2
y2
y2
y2
y2
y2
7

6
y3
7
y3
y3
y3
y3
y3
y3
7

7
x3
7
x3
x1
x1
y1
y1
y1
7

8
z1
8
z1
z1
z2
z2
z2
z2
7

9
z2
8
z2
z2
z1
z1
z3
z3
7

10
z3
8
z3
z3
z3
z3
z1
z1
7

Figure 1: Illustration of how deleteSupport concentrates all variables that have just lost
their last implicit support. See main text for the full description.

HaggisGAC to be able to detect the loss of a literal’s last explicit support in time O(1),
and the loss of a variable’s last implicit support in time O(1). Perhaps surprisingly, both
these goals are achievable by the use of data structures already existing in ShortGAC.
6.2 Finding Literals With No Support Efficiently
Of the two types of support, detecting when the last explicit support for a literal is lost
is the simpler task. When we delete a support, Procedure 2 iterates through the literals
in a short support. For each literal it removes a ShortSupportCell from the corresponding
supportListPerLit and updates data structures appropriately. If the list is empty – tested at
Line 5 of Procedure 2 – the literal has lost its last explicit support. We now add this literal
to a scratch list of literals which have lost their last explicit support: we describe below
how we process the scratch list. The additional cost is O(1) when we detect an empty list.
Because we are inside an existing test, there is zero additional cost when the literal has not
lost its last support. This contrasts with ShortGAC which tests (in Procedure 4) every
variable with no implicit support, for a worst case cost of O(n) even when no literal has
lost its last explicit support.
The more subtle task is to detect when a variable (and thus all literals involving it)
has lost its last implicit support. The reason this is more difficult is that we are seeking
variables that are not involved in the support being deleted, but in Procedure 2 we iterate
through the literals that are in the support being deleted. The variables we seek are
those x which have supportsPerVar[x] = numSupports after the support deletion, while they
had supportsPerVar[x] < numSupports before the support deletion. (Variables that have
supportsPerVar[x] = numSupports both before and after the deletion have no implicit support
now, but did not lose implicit support because of this deletion.) Fortunately, our existing
maintenance of data structures happens to compact exactly these variables into a particular
region of varsBySupport, so we can find them very easily and efficiently. The compaction
happens through the sequence of calls to the Procedure swap made by Procedure 2. We
first show a worked example and we then prove the general properties we need.
In Figure 1, we suppose there are 11 variables in a constraint, there are currently 8
supports, and we are deleting a support involving variables x1 , x2 , x3 , z1 , z2 and z3 , with
19

Nightingale, Gent, Jefferson, & Miguel

the literals deleted in an arbitrary order from top (start) to bottom (finish). Before we
start, the z variables already have supportsPerVar = numSupports = 8; variables x and y
have supportsPerVar = 7; and variables w have supportsPerVar = 6. As we process literals
in deleteSupport, pairs of variables are swapped (marked by boxes in each line) and the
boundaries move between cells (marked by vertical lines) of variables with equal supportsPerVar. At the end, w and x variables still have supportsPerVar = 6 < numSupports = 7.
The z variables have supportsPerVar=numSupports both before and after deletion. The only
variables that have lost their last implicit support are the y variables. The crucial point
is that at the end they lie precisely between the final boundary between 6 and 7 supports
(from i = 5), and the initial boundary between 7 and 8 supports (from i = 8). The following
simple results show that variables losing their last implicit support are always compacted
in a similar way.
Lemma 6.1. Suppose, before we delete a support S, that numSupports = p (and so numSupports = p − 1 afterwards). For a variable x to lose its last implicit support, it has p − 1
explicit supports both before and after the deletion of S.
Proof. If x initially has fewer than p−1 explicit supports, then x has more than one implicit
support and deleting S removes at most one of these. If x initially has p explicit supports,
then it is involved in S (since it is involved in all supports) and so has no implicit support
to lose. Hence, x must initially have p − 1 explicit supports and one implicit support and
S must be that one implicit support. Therefore after the deletion of S, x has p − 1 explicit
supports and no implicit supports.
Lemma 6.2. We set p as in Lemma 6.1, i as the value of supportNumLowIdx[p] when
deleteSupport is called, and j as the value of supportNumLowIdx[p − 1] when deleteSupport
exits. When deleteSupport finishes, the variables that lost their last implicit support during
the call to deleteSupport are exactly the set of variables at indices in the range [j, i) in
varsBySupport.
Proof. All variables with no implicit supports when deleteSupport exits lie at index j or
greater in varsBySupport. This establishes the lower bound on the index range.
Any variable z that has no implicit support at the start of the call must have p explicit
supports and so must be at index i or higher. z must be in the support being deleted,
because it is in all supports. When z is updated by deleteSupport, it is always swapped with
the variable at index supportNumLowIdx[p]. The index supportNumLowIdx[p] only increases
during deleteSupport, so z stays at index i or higher throughout. Thus the variables from
index i upwards at the finish are a permutation of those at the start, meaning that variables
which lost their last implicit support must be in the range [j, i). Finally, any variable in
the range [j, i) has no implicit support at the end of the call (as it is at index j or above)
but had an implicit support at the start (as it is before i). Therefore all and only variables
which lost their last implicit support lie at indices in the range [j, i).
From Lemma 6.2, after we run deleteSupport it is trivial to enumerate all variables
which have lost their last implicit support as a result. They are exactly the variables
varsBySupport[k] for k = j, j + 1, ...i − 1 with i and j as defined in the Lemma. Enumerating
this list is the only additional work over that already done by Procedure 2, so we have:
20

Short and Long Supports for Constraint Propagation

Corollary 6.3. Given a constraint on n variables, the additional work to identify variables
which have lost their last implicit support is O(1) for each such variable where there are
some, and O(1) if there are none.
Proof. We have already argued the case where there are variables which have lost implicit
support. If there are no such variables, there is still O(1) work to check that the range is
empty.
This low level of complexity contrasts very favourably with ShortGAC. When a support is deleted, Procedure 4 iterates over all variables with numSupports explicit supports.
In the worst case this is O(n) work even if no variable has lost its last implicit support, compared to the O(1) work that we now have. We now move on to the details of incorporating
these optimisations into a full suite of procedures for maintaining GAC.
6.3 HaggisGAC: Details
Two issues complicate the implementation of HaggisGAC compared with ShortGAC.
First, the Lemmas above depend on all literals in a support being deleted in a single pass.
Therefore, instead of acting immediately on finding a literal with no supports, we keep a list
of literals with lost supports for later treatment. Second, we now have two cases in which
we might detect lost support – when the lost support is explicit or implicit – compared to
the single case in ShortGAC, where all lost supports are detected in the same way.
We introduce two simple data structures for storing literals and variables that have lost
explicit or implicit support as we find them.
litsLostExplicitSupport is a set containing literals that have lost their final explicit support
and are not supported implicitly.
varsLostImplicitSupport is a set containing variables that have lost their final implicit
support.
We have to adapt the deleteSupport procedure from Procedure 2. The new version is
shown as Procedure 7. When we find a literal which has no explicit support, we immediately
check if it has an implicit support instead (Line 8). If it does not, then we add it to the
set litsLostExplicitSupport for later processing to find a new support or delete it. Variables
which have no implicit support are detected after all literals have been deleted. This is done
by lines 15-16, which are justified by Lemma 6.2.
The new propagate procedure is shown in Procedure 8. Like the earlier Procedure 3,
we first delete all supports involving the literal to be deleted, but the rest of the procedure
is very different. We first iterate through all literals which lost their last explicit support,
and then the variables which lost their last implicit support.
For the lost explicit supports, we call HaggisGAC-literalUpdate (Procedure 9). This
procedure has no analogue in ShortGAC, but is straightforward. The only point of interest
is that we still check whether a literal is supported, even though it was only added to
litsLostExplicitSupport if it was not. The reason is that some support found by an unrelated
call to findNewSupport might also support this literal. If so we are done, but if not then
Procedure 9 calls findNewSupport. If a new support is found it is added, but if not then
we have to prune the literal as being no longer supported.
21

Nightingale, Gent, Jefferson, & Miguel

Require: Short Support sup
1: oldIndex ← supportNumLowIdx[numSupports]
2: for all (x 7→ v) ∈ sup do
3:
Remove sup from supportListPerLit[x 7→ v]
4:
if supportListPerLit[x 7→ v] = {} then
5:
detachTrigger(x,v)
6:
if (x 7→ v) 6∈ zeroLits[x] then
7:
Add (x 7→ v) to zeroLits[x]
8:
if supportsPerVar[x] = numSupports then
9:
Add (x 7→ v) to litsLostExplicitSupport
10:
sPV ← supportsPerVar[x]
11:
swap(x, varsBySupport[sPV])
12:
supportNumLowIdx[sPV] ← supportNumLowIdx[sPV]+1
13:
supportsPerVar[x] ← sPV−1
14: numSupports-15: for all i ∈ {supportNumLowIdx[numSupports] . . . oldIndex − 1} do
16:
Add varsBySupport[i] to varsLostImplicitSupport

Procedure 7: HaggisGAC-DeleteSupport: (sup). One subtlety is that we must add (x 7→
v) to zeroLits (line 7) even if we also add it to litsLostExplicitSupport (line 9). The only
case where this matters is that we seek and find a new implicit support, i.e. not containing
x 7→ v, but this is later lost. At the later point Procedure 10 requires x 7→ v to be in zeroLits
because x 7→ v might still have no explicit support.
Require: x 67→ v (where v has been pruned from domain of x)
1: litsLostExplicitSupport ← {}
2: varsLostImplicitSupport ← {}
3: while supportListPerLit[x 7→ v] 6= {} do
4:
sup ← first element of supportListPerLit[x 7→ v]
5:
deleteSupport(sup)
6: for all (y 7→ b) ∈ litsLostExplicitSupport do
7:
HaggisGAC-literalUpdate(y 7→ b)
8: for all z ∈ varsLostImplicitSupport do
9:
HaggisGAC-variableUpdate(z)

Procedure 8: HaggisGAC-Propagate: propagate(x 67→ v)
For variables with lost implicit supports, we call HaggisGAC-variableUpdate (Procedure 10), which is similar to Procedure 4. The differences are that the return statements
from Procedure 4 are omitted; we check at every iteration whether a new implicit support
has been found for x and if so exit the loop; and we do not remove x 7→ v from zeroLits if a
new explicit support has been found, allowing this to be done lazily in a later call at Line 5.
We gain efficiency over ShortGAC for two reasons. First, variableUpdate is only
called for variables that have just lost implicit support. Second, there is no outer loop in
HaggisGAC-Propagate which must be restarted when a new support is found, as there
is in Procedure 3. If we write m for the number of variables which have lost their last
implicit support, we have reduced the worst case number of calls to variableUpdate from
HaggisGAC-Propagate from O(n2 d) where n is the arity of the constraint to m. Since
m ≤ n and m can often be much smaller than n or even zero, this is a significant gain.
22

Short and Long Supports for Constraint Propagation

Require: x 7→ v, where last explicit support of x 7→ v has been deleted
1: if v ∈ D(x) and supportsPerVar[x] = numSupports and
supportListPerLit[x 7→ v] = {} then
2:
sup ← findNewSupport(x, v)
3:
if sup = Null then
4:
prune(x 7→ v)
5:
else
6:
addSupport(sup)

Procedure 9: HaggisGAC-literalUpdate(x 7→ v)

Require: variable x
1: for all (x 7→ v) ∈ zeroLits[x] do
2:
if supportsPerVar[x] < numSupports then
3:
return
4:
if supportListPerLit[x 7→ v] 6= {} then
5:
Remove (x 7→ v) from zeroLits[x]
6:
else
7:
if v ∈ D(x) then
8:
sup ← findNewSupport(x 7→ v)
9:
if sup = Null then
10:
prune(x 7→ v)
11:
else
12:
addSupport(sup)

Procedure 10: HaggisGAC-variableUpdate(x)

6.4 Dealing Efficiently With Full-length Supports
When a full-length support is added, ShortGAC increments numSupports and supportsPerVar for every variable. Since we are only interested in the condition numSupports =
supportsPerVar[x], a full-length support cannot change this status for any variable. Therefore we can save overheads in the case where we add a full-length support. This is achieved
through a case split in HaggisGAC’s versions of addSupport and deleteSupport: if a
support is full length we do not update numSupports, supportsPerVar, and related data
structures. Note that the test we apply is not that the final support is of arity n, but
the initial one before the omission of any assigned literals as the optimisation is correct
even if assigned literals are omitted. We omit the pseudocode for this optimisation, as the
changes are straightforward. This optimisation often improves performance on instances
with all full-length supports by 20%, and has no important effect on our other instances
with runtimes all within ±2.5% with or without it. This optimisation is also applicable to
ShortGAC, but we did not implement it in that case because it does not address the key
inefficiency that algorithm has, i.e. the repeated checking of variables which cannot have
lost their last implicit support. This does not affect our experimental results dramatically:
in most cases we found that the improved performance of HaggisGAC was larger than
this optimisation provides.
23

Nightingale, Gent, Jefferson, & Miguel

7. Experimental Evaluation of ShortGAC and HaggisGAC
The Minion solver 0.12 (Gent, Jefferson, & Miguel, 2006a) was used for our experiments,
with the only changes being the additional propagators. In all experiments, all the compared
methods maintain GAC. Therefore, the solver explores the same search space in each case.
Since the number of nodes searched is invariant, we compare the rate of search exploration,
measured in search nodes per second.6
We used an 8-core machine with 2.27GHz Intel Xeon E5520 CPUs and 12GB memory,
running Ubuntu Linux. Where possible we ran 12 processes in parallel. For each combination of problem instance and propagator, we report the median of 11 runs.7 In some cases it
is not possible to run 12 processes in parallel because they exceed 1GB memory. For these,
we ran just one process at a time, and we report the median of 5 runs. These instances
are marked with a ‘‡’ in the tables of results. If one method exceeded 1GB, we sometimes
ran other comparable methods in series as well. This allows consistent comparison between
List and NDList, and different propagation algorithms. It also means that ‘‡’ in the tables
does not necessarily indicate that the method uses more than 1GB memory. We find the
median to be a very robust measure of performance, for reasons described in Appendix B.
In all cases, we imposed a time limit of one hour, and a limit of 1,000,000 search nodes
(whichever is first). To avoid short runs when the solver can find a solution easily, we
searched for all solutions. We report complete cpu times, i.e. we have not attempted to
measure the time attributable to the given propagator and we include any initialisation.
This has the advantage that we automatically take account of all factors affecting runtime,
including aspects (e.g. cache usage) that we may not realise affect runtime. It does however
mean that our results tend to understate the difference between methods being studied.
For each case study, we implemented a findNewSupport method for ShortGAC and
HaggisGAC specific to the constraint. We also used the generic list instantiation (Section 5.5.1) and the Next-Difference List instantiation (Section 5.5.2) for comparison where
possible. We compare ShortGAC and HaggisGAC with the special-purpose propagator
(when available).
We also compare with ShortGAC-Long (ShortGAC with full-length supports), with
HaggisGAC-Long, and with GAC-Schema (Bessière & Régin, 1997) as the closest equivalent algorithm without strict short supports. We discuss GAC-Schema further in Section 7.4.
GAC-Schema, ShortGAC-Long and HaggisGAC-Long use the same (constraint-specific)
findNewSupport as ShortGAC, and subsequently extend the short support to full length
using the minimum value for each extra variable.
In each case, the constraint can be compactly represented as a disjunction. Therefore
we compare ShortGAC and HaggisGAC with Constructive Or. The algorithm used is
based on Lagerkvist and Schulte’s (2009), without the rule for entailment detection. The

6. Source code for the solver with the three algorithms is available at http://www.cs.st-andrews.ac.
uk/~pn/haggisgac-source.tgz and problem instances and experimental results at http://www.cs.
st-andrews.ac.uk/~pn/haggisgac-data-instances.tgz.
7. In preliminary investigations, we found that running 12 processes in parallel gives consistent cpu time
results, and this consistency is improved by taking the median.

24

Short and Long Supports for Constraint Propagation

3

element
element long
element list
element ndlist
lex
lex long
squarepack
squarepack long
squarepack list
squarepack ndlist

2.5

2

1.5

1.25
1.1
1
0.9
1

10

100

1000

10000

100000

Figure 2: Summary comparison of ShortGAC and HaggisGAC. The x-axis is median
nodes per second for ShortGAC. The y-axis is speedup (or slowdown) of HaggisGAC, i.e. the ratio of ShortGAC nodes per second to that of HaggisGAC.
Hence 1 represents equal behaviour, while above 1 means that HaggisGAC was
faster.

implementation in Minion is fully incremental: each disjunct is propagated incrementally
down a branch of search and backtracked as the search backtracks.8
We do not compare with table constraints, as described by (for example) Gent et al.
(2007), because the constraints are too large. For example, the smallest element constraints
reported below have 638 allowed tuples, making it impossible even to generate and store the
list of allowed tuples.
To aid comparison between HaggisGAC and ShortGAC, in addition to the tables we
compare them graphically in Figure 2. This figure shows the relative speedup (or in some
cases slowdown) of using HaggisGAC compared with ShortGAC.
7.1 Case Study 1: Element
We use the quasigroup existence problem QG3 (Colton & Miguel, 2001) to evaluate ShortGAC and HaggisGAC on the element constraint. The problem class has one parameter
n, specifying the size of an n × n table (qg) of variables with domains {0 . . . n − 1}. Rows,
columns and one diagonal have GAC allDifferent constraints, following Colton and Miguel’s
model. The element constraints represent the QG3 property that (i ∗ j) ∗ (j ∗ i) = i (where
i and j are members of the quasigroup and ∗ is the quasigroup operator). This translates
as ∀i, j : element(qg, aux[i, j], i), and aux[i, j]= n × qg[i, j] + qg[j, i], where aux[i, j] has
domain {0 . . . n × n − 1}.
8. Personal communication with Pascal Van Hentenryck indicated that there is an unpublished optimisation
of Constructive Or whereby some disjuncts need not be propagated in some cases. We did not implement
this optimisation.

25

Nightingale, Gent, Jefferson, & Miguel

n
6
7
8
9
10

Watch
Elt.
27,825
22,259
15,635
15,898
15,088

Specific
6,956
4,866
2,773
2,374
‡
1,594

ShortGAC
List NDL
4,122 2,182
3,226 1,233
1,609
545
1,377
398
‡
‡
1,060
280

Long
25.9
‡
8.5
‡
3.6
‡
2.2
‡
1.6

Specific
11,131
9,035
5,652
5,419
‡
4,227

HaggisGAC
List NDL
5,300 2,473
4,833 1,415
2,367
622
2,116
451
‡
‡
1,911
317

Long
36.5
‡
15.2
‡
6.2
‡
3.7
‡
2.6

GAC
Sch.
22.5
7.1
‡
3.0
‡
3.0
mem

Con
Or
53.5
24.2
9.1
‡
6.2
‡
4.2

Table 1: Nodes searched per second for quasigroup existence problems. ‘mem’ indicates
running out of memory (>12 GB). Columns correspond to propagation algorithms.
Watch Elt is the special-purpose propagator. Both ShortGAC and HaggisGAC
have four instantiations: Specific (special-purpose findNewSupport function for the
constraint), List, NDL (Next-Difference List), and Long (as described in the text).
GAC-Sch is GAC-Schema, and Con Or is Constructive Or.

For the constraint element(X, y, z), the findNewSupport method for ShortGAC returns
tuples of the form hxi 7→ j, y 7→ i, z 7→ ji, where i is an index into the vector X and j is a
common value of z and xi . ShortGAC-list has all supports of this form. For Constructive
Or, we used (x0 = z ∧ y = 0) ∨ (x1 = z ∧ y = 1) ∨ · · · .
We compare ShortGAC and HaggisGAC with the special-purpose Watched Element
propagator (Gent et al., 2006b), GAC-Schema and Constructive Or. Table 1 presents our
results on QG3. Of the general purpose methods, using short supports (with Specific, List or
NDList instantiations) is dramatically better than any alternative. For example at n = 10,
even the HaggisGAC-List method (which is slower than HaggisGAC-Specific) is more
than 450 times faster than Constructive Or, the best of the other methods.
ShortGAC-Long runs about 10–20% faster than GAC-Schema for n = 6 to 8, slower at
n = 9 but better at n = 10 because GAC-Schema uses more memory. Recall that they both
use the same findNewSupport method, so this is a fair comparison of how efficiently they
exploit these supports. This is in contrast to our results reported previously (Nightingale
et al., 2011), where ShortGAC was about half the speed of GAC-Schema. Two substantial differences account for the improvement: the improved data structures described in
Section 5; and that we remove assigned literals from the full-length supports as described
in Section 5.6. HaggisGAC-Long is consistently faster than both ShortGAC-Long and
GAC-Schema.
While much faster than methods using full-length supports, list variants HaggisGACList and HaggisGAC-NDList are both slower than HaggisGAC-Element (and the same
is true for ShortGAC). This is to be expected as neither is specialised to the Element
constraint, and both have to deal with data structures containing the lists of tuples. Of the
two list variants, the NDList variant runs much more slowly. However, its memory usage is,
as we expected, much less than HaggisGAC-List. It used less than half as much memory
at n = 6, improving to almost 10 times less memory at n = 10.
HaggisGAC-Element is approximately twice as fast as ShortGAC-Element on these
instances. We believe this is because two variables are in all short supports – the index
and result variables – meaning that they are always supported explicitly. As can be seen
26

Short and Long Supports for Constraint Propagation

n

GACLex

3
4
5
6
7
8
9
10
12
14
16
18
20
22
24

104,955
103,950
95,420
80,841
72,307
66,445
64,267
57,208
48,146
36,751
30,057
22,432
16,625
12,450
9,526

ShortGAC
Specific Long
87,463 7,020
99,602 6,481
89,127 6,358
73,260 3,456
65,062 2,424
51,335 1,290
47,059
786
38,344
557
31,626
293
22,712
139
17,813
85.9
13,843
52.4
10,734
35.9
7,976
24.9
6,255
14.3

HaggisGAC
Specific Long
91,265 9,288
100,100 8,628
90,009 8,503
74,184 4,666
65,359 3,271
52,659 1,609
47,847
914
39,683
634
32,425
311
23,063
142
18,420
90.9
13,845
53.8
10,711
38.9
8,141
26.0
6,268
18.9

GACSchema
3,622
3,030
2,734
1,638
1,190
670
451
318
170
82.3
51.5
33.3
21.0
12.5
‡
7.3

Con
Or
5,735
4,997
4,104
2,109
1,188
456
263
184
105
‡
99.1
‡
62.6
‡
48.3
‡
36.7
‡
27.0
‡
21.8

Table 2: Nodes searched per second for BIBDs. GACLex is the special-purpose propagator,
and other columns are named as in Table 1.

in Figure 2, List, NDList and Long instantiations of HaggisGAC are also faster than
the same instantiations of ShortGAC but by a smaller margin. The special purpose
Watched Element propagator is the fastest method, being 3.6 times faster when n = 10.
Watched Element also appears to be scaling better as n increases. Constructive Or is
much slower than all the methods that exploit strict short supports, however it is faster
than HaggisGAC-Long. Overall it is clear that exploiting strict short supports is very
beneficial compared with other general purpose methods.
7.2 Case Study 2: Lex-ordering
We use the BIBD problem to evaluate ShortGAC and HaggisGAC on the lexicographic
ordering constraint. The lex constraint is placed on both the rows and columns, to perform
the ‘Double Lex’ symmetry breaking method (Flener et al., 2002). We use the BIBD model
given by Frisch, Hnich, Kiziltan, Miguel, and Walsh (2002), and the GACLex propagator given by Frisch, Hnich, Kiziltan, Miguel, and Walsh (2006). We use BIBDs with the
parameter values (4n + 3, 4n + 3, 2n + 1, 2n + 1, n).
For the constraint lexleq(X, Y ) on arrays X and Y , we define mxi = min(Dom(xi ))
and myi = max(Dom(yi )). The findNewSupport method for ShortGAC finds the lowest
index i ∈ {0 . . . n} such that mxi < myi , or i = n. The case i = n arises when X cannot
be lexicographically less than Y , so a support is sought for X = Y . If i < n, the support
contains xi 7→ mxi , yi 7→ myi . For each index j < i, if mxj = myj , then the short support
contains xj 7→ mxj , yj 7→ myj otherwise there is no valid support and Null is returned.
The lex constraint on two arrays of length n and domain size d has more than dn short
supports in any short support set, because all assignments where the two arrays are equal
satisfy the constraint and cannot be reduced. ShortGAC-List and ShortGAC-NDList
27

Nightingale, Gent, Jefferson, & Miguel

are not practical for any substantial constraint so we omit them from the comparison. For
Constructive Or we use the following representation with n + 1 disjuncts: (x0 < y0 ) ∨ (x0 =
y0 ∧ x1 < y1 ) ∨ · · · , including the final case where all pairs are equal.
Table 2 presents the results of our experiments on non-list based methods with values of
n from 3 to 24. It is clear that the best method is the special-purpose GACLex propagator,
with HaggisGAC coming second. On this problem, HaggisGAC and ShortGAC perform similarly. HaggisGAC and ShortGAC are by far the best general purpose methods.
For the largest instances they run about 1.5 times slower than the special purpose method,
while outperforming the next best method by almost 300 times. Again, HaggisGAC-Long
and ShortGAC-Long outperform GAC-Schema, and on these instances the difference is
even more marked.
HaggisGAC-Long can be substantially faster than ShortGAC-Long, as can be seen
in Figure 2: this is largely explained by the optimisation of Section 6.4.
To summarise, these experiments on the Lex constraint clearly show the benefit of
HaggisGAC and ShortGAC compared with other general-purpose propagation methods.
Their speed even approaches that of the special purpose GACLex propagator.
7.3 Case Study 3: Rectangle Packing
The rectangle packing problem (Simonis & O’Sullivan, 2008) (with parameters n, width and
height) consists of packing all squares from size 1 × 1 to n × n into the rectangle of size
width × height. This is modelled as follows: we have variables x1 . . . xn and y1 . . . yn , where
(xi , yi ) represents the Cartesian coordinates of the lower-left corner of the i × i square.
Domains of xi variables are {0 . . . width − i}, and for yi variables are {0 . . . height − i}.
Variables are branched on in decreasing order of i (to place the largest square first), with
xi before yi , smallest value first. The only type of constraint is non-overlap of squares i
and j: (xi + i ≤ xj ) ∨ (xj + j ≤ xi ) ∨ (yi + i ≤ yj ) ∨ (yj + j ≤ yi ). Minion does not
have the special-purpose non-overlap constraint (Simonis & O’Sullivan, 2008), so we only
report a comparison of general-purpose methods. For the experiment we used the optimum
rectangle sizes reported by Simonis and O’Sullivan.
The domains of xn and yn are reduced to break flip symmetries as described by Simonis
and O’Sullivan (2008). Our focus is performance of the non-overlap constraint, and so we
did not implement the commonly-used implied constraints.
The findNewSupport function for ShortGAC is as follows. If any of the four disjuncts
above are entailed given the current domains, return the empty support (indicating entailment). Otherwise, return a support with two literals to satisfy one of the four disjuncts.
The list used for ShortGAC-List and ShortGAC-NDList has all supports of size 2.
In Table 3, we compare HaggisGAC and ShortGAC with other general purpose
methods. We can see that HaggisGAC is the fastest method, with ShortGAC second.
HaggisGAC-List and HaggisGAC-NDList (as well as ShortGAC-List and ShortGACNDList) performed well compared to GAC-Schema and Constructive Or. However at
n = 20, HaggisGAC-List consumes 971MB memory and HaggisGAC-NDList 496MB,
and with n > 20 it was not possible to run these methods with 12 processes in parallel.
Interestingly, the performance of the two List variants of HaggisGAC is reversed from
Case Study 1: here, NDList is significantly faster than List in most cases. As expected,
28

Short and Long Supports for Constraint Propagation

n-w-h
18-31-69
19-47-53
20-34-85
21-38-88
22-39-98
23-64-68
24-56-88
25-43-129
26-70-89
27-47-148

Specific
14,923
38,329
13,949
8,568
8,059
31,486
12,317
5,310
25,860
2,943

ShortGAC
List
NDL
6,339
6,919
4,446
8,460
3,181
3,911
‡
2,668 ‡ 2,781
‡
1,865 ‡ 1,889
‡
1,226 ‡ 2,805
‡
1,717 ‡ 2,238
‡
‡
1,007
986
‡
‡
909
1,977
‡
‡
1,034
786

Long
1,093
1,282
914
641
599
718
492
377
455
252

Specific
19,524
39,185
21,000
12,262
11,966
30,628
16,075
10,228
23,132
4,677

HaggisGAC
List
NDL
7,999
7,988
5,295
9,330
4,261
4,734
‡
3,955 ‡ 3,981
‡
3,013 ‡ 2,896
‡
1,663 ‡ 3,863
‡
2,441 ‡ 3,152
‡
1,634 ‡ 1,506
‡
1,219 ‡ 2,577
‡
1,265 ‡ 1,187

Long
1,471
1,684
1,296
886
858
971
702
583
584
400

GAC
Sch.
1,033
1,181
775
592
518
590
474
348
376
272

Con
Or
441
478
276
245
185
349
167
96
245
74

Table 3: Nodes searched per second for Rectangle Packing instances. All columns are named
as in Table 1.

NDList used less memory, though less dramatically than before. It used from about 30%
to 50% of the memory of HaggisGAC-List.
Of the other methods, all are always at least 10 times slower than HaggisGAC.
HaggisGAC-Long is faster than GAC-Schema in all cases. Also ShortGAC-Long is
faster than GAC-Schema on all instances except 27-47-148 (this contradicts the result we
previously reported (Nightingale et al., 2011), and some explanation of this is given in the
first case study).
Table 3 shows that HaggisGAC (with the SquarePack instantiation) is substantially
faster than ShortGAC on most of the instances, with the exception of n = 23 and n = 26
where ShortGAC is slightly faster. When compared with ShortGAC for List, NDList,
and Long instantiations in Figure 2, we see that HaggisGAC is mostly between 10 and
50% faster. In summary, these results very clearly show the benefits of using strict short
supports.
7.4 Comparing HaggisGAC With GAC-Schema
Across all the above experiments, HaggisGAC-Long runs significantly faster than GACSchema – from a minimum of about 20% faster to more than three times faster – even
though our code contains overhead for dealing with strict short supports. We compared
memory usage across all experiments, and found very similar performance across all instances. We found that HaggisGAC-Long uses less than 5% more memory on all except
BIBD instances, and on BIBD it uses less than 17% more memory than GAC-Schema.
However, the comparison has been only on functional instantiations of full-length supports, and on constraints that admit strict short supports. In this section, we broaden the
comparison by using the list instantiations rather than functional ones, and using problem
instances that have been used previously for comparing table constraints.
We compared against GAC-Schema because it is very similar to HaggisGAC and
ShortGAC conceptually. All three algorithms maintain a list of supports for each literal,
which is updated and backtracked during search. GAC-Schema was carefully implemented
29

Nightingale, Gent, Jefferson, & Miguel

Sports
CarSeq
Graceful
PQueens
BIBD

20

10

5

2

1

0.5
100

1000

10000

100000

Figure 3: Comparison of GAC-Schema and HaggisGAC-List on full-length table constraints. x-axis is nodes per second for GAC-Schema, y-axis is speedup of
HaggisGAC-List.

following the pseudocode of the original paper (Bessière & Régin, 1997). While some code
is shared among all three algorithms, each was optimised independently. For example,
GAC-Schema has a different implementation of supportListPerLit, named SC in (Bessière &
Régin, 1997), which is specialised to full-length supports.
In contrast to GAC-Schema, other table constraint propagators such as STR2 (Lecoutre,
2011) and MDDC (Cheng & Yap, 2010) are entirely different to HaggisGAC, and it would
be difficult to create truly comparable implementations of them.
We report on the use of HaggisGAC-List only, because it searches for supports in the
same way as GAC-Schema (with one difference we discuss below.) We used the structured
instances from Gent et al. (2007), except the Semigroup class. In addition, we used Car
Sequencing instances from Nightingale (2011), specifically model B instances numbered
60-79. These instances contain a large number of ternary table constraints.
Figure 3 shows that HaggisGAC-List is almost always faster than GAC-Schema on
these problems. For BIBDs it is not clear which algorithm is better. HaggisGAC is
always at least marginally faster on the Sports Scheduling, Prime Queens and Graceful
Graphs instances, in most cases in the range 10-20% faster. HaggisGAC is substantially
faster on Car Sequencing. To seek new supports, HaggisGAC calls Procedure 5, and when
it finds a new support it stores the index of it in listPos. HaggisGAC does not backtrack
listPos as described in Section 5.5.1. GAC-Schema is similar, but it does backtrack listPos,
and it ensures optimality down a branch of search by iterating only from listPos to the
end of the list (Bessière & Régin, 1997). Profiling shows that GAC-Schema is hindered by
backtracking listPos (by block-copying memory) on Car Sequencing, where there are a very
large number of table constraints (2000 on instance 60) and large domains (some of size
30

Short and Long Supports for Constraint Propagation

over 1000). Alternative memory management techniques might speed up GAC-Schema, so
we do not claim that HaggisGAC is fundamentally 10 times faster than GAC-Schema.
7.5 Results Summary
To summarise the three case studies, HaggisGAC does indeed outperform ShortGAC
on many instances, sometimes by more than two times and commonly by more than 25%.
ShortGAC is only rarely faster, but only on one instance by as much as 10%. Overall, in
our experiments, HaggisGAC is clearly a better algorithm than ShortGAC. Furthermore,
HaggisGAC and ShortGAC perform very well compared to Constructive Or and GACSchema, a result which validates the idea of strict short supports.
Finally, we have shown experimentally that HaggisGAC can outperform GAC-Schema
on problems containing only full-length supports. We discuss why this should be in Appendix C as it is not a major focus of this paper.

8. Backtrack Stability and Short Supports
Within a search tree, HaggisGAC often spends significant time backtracking data structures. Reducing or eliminating backtracking can improve efficiency. For example MAC-6
and MAC-7 can be much more efficient (in both space and time) if backtracking is avoided
(Régin, 2005). In this section we present a new algorithm that saves time by not deleting
short supports on backtrack, and saves memory by bounding the total number of stored
short supports (including those on the backtrack stack).
The new algorithm requires that short supports have the backtrack stability property.
A short support is backtrack stable iff it remains a short support after backtracking (Section 3.2).
In our three case studies, we find that the short supports we construct for the element and
lex constraints are backtrack stable, but for rectangle packing they are not. For rectangle
packing, we generate the empty support when the constraint is entailed. The empty support
is not backtrack stable unless the constraint is entailed at the root node of search.
We introduce the algorithm HaggisGAC-Stable where we know all short supports
are backtrack stable. The key change is that we do not delete supports when we backtrack
past their point of introduction. Because they are stable, they are still correct at ancestors
of the node they were introduced at. This can save time over the previous algorithms, since
we sometimes need to do no work at all on backtracking. Also, as we show below, we obtain
very tight limits on space usage of stored supports.
To present HaggisGAC-Stable, we introduce the notion of a prime support of a
deleted literal. A prime support of a deleted literal is a support (either explicit or implicit)
which will be a valid support for that literal when the literal is restored on backtracking.
The invariant we maintain after deleting a literal is that either we have labelled a deleted
support on the backtrack stack as its prime support or the literal’s variable is currently
implicitly supported. With this invariant, we guarantee that when we backtrack to the
point where the literal is restored, it must be supported again: either by the prime support
which we can restore, or by the known implicit support.
The task of finding the prime support for a literal naturally splits into three cases. The
simplest case is that HaggisGAC-Stable itself deletes literals when not able to find a
31

Nightingale, Gent, Jefferson, & Miguel

necessary new support. The prime support is then just the implicit or explicit support
whose deletion caused the fruitless search for a new support.
The second case is where a literal is pruned by some other constraint or the search
procedure, and the pruned literal had an explicit support in this constraint. All its explicit
supports must be deleted as no longer valid, and we can label an arbitrary one to be the
literal’s prime support: we simply choose the last one to be deleted.
The third case is unfortunately complicated. It is that a literal is pruned outside the
current constraint, and the literal had an implicit support but no explicit support. This is
difficult precisely because the pruned literal does not have any link to its implicit support.
Providing and maintaining such a link throughout search would negate the efficiencies we
have gained. Our solution to this problem is to be lazy. The variable of the pruned
literal is implicitly supported. While we have any implicit support for the variable, we are
maintaining the invariant described above. So when the literal is pruned we need do nothing
in this case. We only need do any work when this variable loses its last implicit support,
if it ever does. When this happens, an invalid literal which had no explicit support must
by definition be in the relevant zeroLits list. Whereas previously we ignored invalid literals
when iterating through zeroLits, we now can label the deleted implicit support as a prime
support for the invalid literal.
We will show in Lemma 8.1 that HaggisGAC-Stable stores at any time at most
O(z) supports, where z is the total number of literals. This can save a lot of memory
because HaggisGAC and ShortGAC may store O(z 2 ) supports, because there can be
O(z) deletions of literals down a branch, and for each deletion a new set of O(z) supports
may be stored. Our experiments later will show that this difference in memory usage can
be significant in practice. At its most effective, memory usage was reduced by 20 times.
8.1 Details of HaggisGAC-Stable
In HaggisGAC-Stable, we have to control with great care the deletion and restoration of
supports, instead of (as in the rest of this paper) simply reversing the addition or deletion
of a support at a node by respectively deleting or adding it back when we backtrack past
that node. In short we never delete an active support on backtracking, and only add back
in a deleted support if it is a prime support for a literal with no current active support.
When deleting a support, we setup a counter numPrimeSupported. It is initially 0, and
is incremented each time we find the support is a prime support. When the propagation
algorithm finishes, for any support with numPrimeSupported = 0, the support can be destroyed and its space reclaimed. Otherwise, we place numPrimeSupported new pairs on the
backtrack stack. Each pair consists of the deleted support and the literal it is a prime
support for. On backtracking, when we pop a pair, we first check if any current support
already supports the literal. If so, we simply decrement numPrimeSupported, and if this
reduces to 0, again we reclaim the support’s space. If the literal is not supported, then we
restore the support via a call to addSupport. In this way all literals the support was prime
for are now guaranteed to be supported.
A relatively minor difference is that when we iterate through zeroLits we now delete invalid literals from zeroLits. We can do this because on backtracking we can restore them into
32

Short and Long Supports for Constraint Propagation

Require: x 7→ v, where last explicit support of x 7→ v has been deleted
1: if v ∈ D(x) then
2:
if supportsPerVar[x] = numSupports and supportListPerLit[x 7→ v] = {} then
3:
sup ← findNewSupport(x, v)
4:
if sup = Null then
5:
prune(x 7→ v)
6:
increment lastSupportPerLit[x 7→ v].numPrimeSupported
7:
push hx 7→ v, lastSupportPerLit[x 7→ v]i onto BacktrackStack
8:
else
9:
addSupport(sup)
10: else
11:
increment lastSupportPerLit[x 7→ v].numPrimeSupported
12:
push hx 7→ v, lastSupportPerLit[x 7→ v]i onto BacktrackStack

Procedure 11: HaggisGAC-Stable-literalUpdate: (x 7→ v). In comparison to Procedure 9, we update numPrimeSupported and BacktrackStack.

zeroLits because they are on the backtrack stack, and doing so enables the space complexity
result in Lemma 8.1.
HaggisGAC-Stable is similar to HaggisGAC. Where appropriate we simply describe
differences to save space. The Procedure HaggisGAC-Stable-Propagate is almost the
same as Procedure 8, calling the backtrack stable variants of deleteSupport, literalUpdate
(Procedure 11) and variableUpdate (Procedure 12). In addition, at the end of this algorithm
we destroy and reclaim the space for any deleted support for which numPrimeSupported = 0.
The Procedure HaggisGAC-Stable-DeleteSupport (called with support S) is also very
similar to its predecessor, Procedure 7, with some additions. First, it initialises numPrimeSupported for S to 0. Second, we have new data structures lastSupportPerLit for a deleted
literal x 7→ a and lastSupportPerVar for a variable x. In terms of Procedure 7, these are
both assigned to S at Line 9 and Line 16 (respectively). Note these assignments do not
make S a prime support: this will be checked later.
Procedure 11 is analogous to Procedure 9 but with enough differences that we show it
in detail here. It identifies prime supports, and when necessary increments numPrimeSupported and pushes invalid literal/support pairs onto the backtrack stack. We also present
Procedure 12 in detail, the analogue to Procedure 10. Again it identifies prime supports,
increments the counter and adds pairs to BacktrackStack. One difficult case arises, from
Line 17. Here, x 7→ a has been pruned, but externally to this constraint. If it had been
pruned by Procedure 11, it would not be in zeroLits. When x 7→ a is restored on backtracking we still need to make sure it has support. Since it has no explicit support (it is in
zeroLits), its last support must be this implicit support we are deleting. Therefore we store
the support on BacktrackStack. A minor change to note is that we remove literals from
zeroLits, at Lines 13 and 19.
Whenever a new search node (including the root) is entered, a Null is pushed onto
the BacktrackStack. This is used as a marker for the procedure HaggisGAC-StableBacktrack (Procedure 13), which processes literal/support pairs until it reaches the Null.
This restores prime supports for literals being put back into the domain on backtracking,
but only if no other support is currently known. If the numPrimeSupported counter for
33

Nightingale, Gent, Jefferson, & Miguel

Require: variable x
1: for all (x 7→ v) ∈ zeroLits[x] do
2:
if supportsPerVar[x] < numSupports then
3:
return
4:
if supportListPerLit[x 7→ v] 6= {} then
5:
Remove (x 7→ v) from zeroLits[x]
6:
else
7:
if v ∈ D(x) then
8:
sup ← findNewSupport(x, v)
9:
if sup = Null then
10:
prune(x 7→ v)
11:
increment lastSupportPerVar[x].numPrimeSupported
12:
push hx 7→ v, lastSupportPerVar[x]i onto BacktrackStack
13:
Remove (x 7→ v) from zeroLits[x]
14:
else
15:
addSupport(sup)
16:
else
17:
increment lastSupportPerVar[x].numPrimeSupported
18:
push hx 7→ v, lastSupportPerVar[x]i onto BacktrackStack
19:
Remove (x 7→ v) from zeroLits[x]
Procedure 12: HaggisGAC-Stable-variableUpdate: (x). This is similar to Procedure 10
with the addition of maintenance of numPrimeSupported and BacktrackStack.
1: while the top element of BacktrackStack is not Null do
2:
pop hx 7→ v, supi from BacktrackStack
3:
if sup has not yet been restored then
4:
if supportsPerVar(x) = numSupports and supportListPerLit[x 7→ v] = {} then
5:
HaggisGAC-Stable-AddSupport(sup)
6:
else
7:
{Another support exists for x 7→ v}
8:
decrement sup.numPrimeSupported
9:
if sup.numPrimeSupported = 0 then
10:
destroy sup and reclaim space
11:
if supportListPerLit[x 7→ v] = {} then
12:
Add (x 7→ v) to zeroLits[x]
13: pop Null from BacktrackStack

Procedure 13: HaggisGAC-Stable-Backtrack. Performs backtracking using BacktrackStack.

a support becomes zero, the support can be destroyed as it is no longer necessary. Note
that literals are put back into zeroLits if necessary at Line 12, reversing their deletion in
Procedure 12.
We cannot use the optimisation described in Section 5.6, of deleting literals in supports
for variables that are assigned, because this may break the backtrack stability property.
34

Short and Long Supports for Constraint Propagation

However, we retain the optimisation of Section 6.4 for full-length supports, but again omit
pseudocode showing this in the interest of focusing on the essential aspects of the algorithms.
8.2 Improved Space Complexity of HaggisGAC-Stable
Our approach improves the space complexity of HaggisGAC-Stable compared with HaggisGAC, as the following lemma shows.
Lemma 8.1. For a constraint involving z literals, at most 2z supports are stored, either as
active or as deleted supports on the backtrack stack.
Proof. We define a function from supports to literals. If the support is still active, it was
found from a call to findNewSupport for a specific literal, and we map the support to this
literal. Similarly, if the support is on the backtrack stack, then it is in a pair with at least
one literal it is a prime support for. Map the support to any one of these literals. Every
stored support falls into one of these two categories, because if a support is deleted and it
is not put onto the backtrack stack, its space is reclaimed. No three supports are mapped
to the same literal because:
• For valid literals, findNewSupport will not be called again if an existing active support
exists for that literal.
• For invalid literals, each literal appears in a pair on the backtrack stack at most
twice. The only case where a literal appears as often as twice is that a literal with a
prime support already on the stack is processed when its variable loses its last implicit
support. In this case, the literal must be in zeroLits, and the newly deleted implicit
support will be added to the backtrack stack for this literal. But this can only happen
once because we delete the literal from zeroLits the first time it happens.
Thus the number of supports is bounded above by 2z.
The bound 2z in Lemma 8.1 would improve to z if we maintained zeroLits eagerly
instead of lazily, but at the expense of higher overheads elsewhere.

9. Experimental Evaluation of HaggisGAC-Stable
We compare HaggisGAC-Stable to HaggisGAC using the same experimental setup as
in Section 7. As well as tables of results, we provide a graphical comparison of runtimes of
HaggisGAC-Stable and HaggisGAC in Figure 4, and of their memory usage in Figure 5.
Table 4 and Figure 4 shows results for the instances of Section 7.1. We present all
four instantiations of HaggisGAC-Stable, along with the fastest instantiation of HaggisGAC, the Watched Element special-purpose propagator, and Constructive Or (which
was faster than GAC-Schema in Table 1). For element, we observe about a 10% slowdown,
and again a slight slowdown for both List variants. For full-length supports, we see almost
identical performance.
Table 5 shows the results for instances of Section 7.2. HaggisGAC-Stable-Lex performs slightly worse than HaggisGAC-Lex, though is in fact never more than 10% worse
and very slightly faster on the largest instances. This might be because supports found
35

Nightingale, Gent, Jefferson, & Miguel

deep in search are likely to contain more literals than supports found earlier, meaning that
when we backtrack the longer supports are retained instead of replaced by the earlier and
more efficient short supports. If so, this advantage disappears for the Long variants. Indeed, HaggisGAC-Stable-Long performs much better than HaggisGAC-Long, and the
improvement increases with n, being about 4.5 times for n = 24.
The Rectangle Packing instantiation of ShortGAC described in Section 7 generates an
empty support when the constraint becomes entailed, causing all variables to be implicitly
supported from that point on. This empty support is not backtrack stable, so cannot
be used with HaggisGAC-Stable. We implemented a new backtrack stable variant of
findNewSupport, in which the empty support is not returned, but is otherwise the same
as before. The List and Long variants are not affected because they do not return the
empty support in this case. In Table 6, we use the instances from Section 7.3. Results show
significant slowdowns by using backtrack stability for rectangle packing, more than 2 times
for n = 24. This is probably because of the inability to return the empty support. On the
other hand, we see speedups of about 50% for the list variants, and in some cases a factor
of 2 speedup for full-length supports.
We see in Figure 5 that the memory usage goes down greatly when stability is used
on full-length supports, possibly contributing to speedups in these cases. The greatest
reductions are in the case of element, in two cases more than 20 times less memory. On the
other hand, there is no significant reduction in memory usage in any non-long variant.
We also tested HaggisGAC-Stable against GAC-Schema as in Section 7.4. This gave
very similar performance to HaggisGAC and was therefore better than GAC-Schema: we
omit detailed results. There was no significant memory advantage compared to HaggisGAC, with the Stable variant saving less than 25%. We therefore do not seem to gain the
advantages we saw earlier from backtrack stability on full-length supports.
We conclude that backtrack stability can speed up HaggisGAC significantly, and
greatly reduce memory usage when using full-length supports. However, care must be
used, because backtrack stability can be harmful if insisting on backtrack stability increases
the size of returned supports.

10. Related Work
Our use of counters to count supports is inspired by AC4 (Mohr & Henderson, 1986). There
has been some study of compressing the tuples of a constraint into a compact data structure
in order to make propagation more efficient. For example, Gent et al. (2007) used tries, and
Cheng and Yap (2010) applied MDDs. There has also been extensive study of searching
the list of tuples to find the first valid tuple. Approaches include binary search (Lecoutre &
Szymanek, 2006), trie search (Gent et al., 2007), and approaches similar to skip lists such as
NDLists (Gent et al., 2007) and hologram-tuples (Lhomme, 2004; Lhomme & Régin, 2005).
All these techniques are orthogonal to the main focus of this paper because they assist in
finding supports, not in maintaining the set of active supports. We have adapted NDLists
to contain short supports in Section 5.5.2; it may also be interesting to adapt some of the
other approaches.
STR2 maintains a sparse set of all valid satisfying tuples of the constraint (Lecoutre,
2011). Updated variable domains are computed from this set each time the algorithm is
36

Short and Long Supports for Constraint Propagation

n

WatchElt

6
7
8
9
10

27,825
22,259
15,635
15,898
15,088

HaggisGAC
Specific
11,131
9,035
5,652
5,419
‡
4,227

HaggisGAC-Stable
Specific
List NDList Long
10,305
4,881
2,358
30.3
8,302
4,225
1,349 ‡ 15.1
‡
4,986
1,950
550
7.0
‡
4,579
1,711
388
4.4
‡
‡
‡
4,008 ‡ 2,409
309
2.5

Con
Or
53.5
24.2
9.1
‡
6.2
‡
4.2

Table 4: Nodes searched per second for quasigroup existence problems. All columns are
named as in Table 1.
n

GACLex

3
4
5
6
7
8
9
10
12
14
16
18
20
22
24

104,955
103,950
95,420
80,841
72,307
66,445
64,267
57,208
48,146
36,751
30,057
22,432
16,625
12,450
9,526

HaggisGAC
Specific Long
91,265 9,288
100,100 8,628
90,009 8,503
74,184 4,666
65,359 3,271
52,659 1,609
47,847
914
39,683
634
32,425
311
23,063
142
18,420
90.9
13,845
53.8
10,711
38.9
8,141
26.0
6,268
18.9

HaggisGAC-Stable
Specific
Long
90,473
12,008
103,470
9,056
93,382
7,248
76,777
3,844
67,273
2,615
52,113
1,591
47,881
1,114
39,176
806
32,310
533
23,709
345
18,556
248
14,504
177
10,438
135
8,159
106
6,165
85

GACSchema
3,622
3,030
2,734
1,638
1,190
670
451
318
170
82.3
51.5
33.3
21.0
12.5
‡
7.3

Con
Or
5,735
4,997
4,104
2,109
1,188
456
263
184
105
‡
99.1
‡
62.6
‡
48.3
‡
36.7
‡
27.0
‡
21.8

Table 5: Nodes searched per second for BIBDs. GACLex is the special-purpose propagator
for Lex, and all other columns are named as in Table 1.
n-w-h
18-31-69
19-47-53
20-34-85
21-38-88
22-39-98
23-64-68
24-56-88
25-43-129
26-70-89
27-47-148

HaggisGAC
Specific
19,524
39,185
21,000
12,262
11,966
30,628
16,075
10,228
23,132
4,677

HaggisGAC-Stable
Specific
List NDList
16,950
9,544
8,383
22,580
4,663
8,264
12,865
4,950
4,840
‡
5,827
9,783 ‡ 6,492
‡
8,798 ‡ 4,744
4,319
‡
‡
28,987
2,377
4,511
‡
6,741 ‡ 3,894
3,998
‡
5,706 ‡ 2,405
2,199
‡
‡
27,507
1,689
4,024
‡
3,996 ‡ 1,591
1,735

Long
1,686
1,621
2,607
957
921
1,095
1,149
1,265
890
344

GACSchema
1,033
1,181
775
592
518
590
474
348
376
272

Table 6: Nodes searched per second for Rectangle Packing instances. All columns are named
as in Table 1.

37

Nightingale, Gent, Jefferson, & Miguel

element
element long
element list
element ndlist
lex
lex long
squarepack
squarepack long
squarepack list
squarepack ndlist

6
5
4
3
2
1.5
1.1
1
0.9
0.7
0.5
0.4
0.3
1

10

100

1000

10000

100000

1e+06

Figure 4: Summary comparison of HaggisGAC and HaggisGAC-Stable. The x-axis is
median nodes per second for HaggisGAC. The y-axis is speedup (or slowdown)
of HaggisGAC-Stable.

2

1

0.5

0.2

0.1

0.05

0.02
1000

element
element long
element list
element ndlist
lex
lex long
squarepack
squarepack long
squarepack list
squarepack ndlist
10000

100000

1e+06

1e+07

1e+08

Figure 5: Summary comparison of memory usage (KiB) of HaggisGAC and HaggisGACStable. The x-axis is median memory usage for HaggisGAC. The y-axis is
reduction (or increase) in usage of HaggisGAC-Stable, i.e. the ratio of HaggisGAC memory usage to that of HaggisGAC-Stable. Hence 1 represents
equal behaviour, while below 1 means that HaggisGAC-Stable used less memory.

38

Short and Long Supports for Constraint Propagation

invoked. There is no concept of maintaining support, or seeking new support for a literal.
It would be interesting to investigate adapting STR2 to handle short supports. This would
result in an entirely different algorithm to the ones presented in this paper, possibly with
complementary strengths.
The MDD propagator MDDC (Cheng & Yap, 2010) maintains an MDD incrementally
during search. The MDD is a compressed representation of the satisfying tuples of the
constraint. The time complexity of MDDC is linear in the initial size of the MDD, therefore
the degree of compression is vital to the efficiency of the algorithm. In some cases, if a
constraint is amenable to strict short supports, it will also compress well into an MDD
(given an appropriate variable ordering). For example, the lex constraint compresses well
partly because (given the variable order x1 , y1 , x2 , y2 , . . .) the constraint can be satisfied by
assigning a prefix of the variables. Lex is amenable to short supports for the same reason.
However, some constraints have a small set of short supports but cannot be compressed
effectively into an MDD. Suppose we have a disjunction of equality constraints for each pair
of n variables of domain size d. After n − 1 variables, the MDD must have d Cn−1 states.
Another property of MDD compression might indicate an interesting direction for future
work. Lex also compresses well into an MDD because multiple assignments to a prefix of the
variables lead to the same subsequent vertex (e.g. {x1 7→ 1, y1 7→ 1} and {x1 7→ 2, y1 7→ 2}).
This is something that our short support algorithms are not currently able to exploit.
Katsirelos and Walsh (2007) proposed a different generalisation of support, named ctuples. A c-tuple contains a set of values for each variable in the scope of the constraint. Any
valid tuple whose values are drawn from the c-tuple is a (full-length) support. Katsirelos and
Walsh give an outline of a modified version of GAC-Schema which directly stores c-tuples.
They also present experiments based on a different propagator, GAC3.1r, demonstrating
a modest speed improvement for c-tuples compared to conventional full-length supports.
When a c-tuple contains all values of some variable, it will nevertheless be recorded (in SC )
as support for each value individually (Katsirelos & Walsh, 2007). The algorithm has no
concept of implicit support.
In the context of Constructive Or, Lhomme (2003) observed that a support for one
disjunct A will support all values of any variable not contained in A. The concept is similar
to a short support albeit less general, because the length of the supports is fixed to the
length of the disjuncts. He presented a non-incremental Constructive Or algorithm for two
disjuncts.
Our algorithms have a similar flavour to GAC-Schema (Bessière & Régin, 1997), so it
was natural to compare them to GAC-Schema. However there are other GAC algorithms
such as GAC2001/3.1 (Bessière et al., 2005) and it would be interesting to compare these
to our algorithms.

11. Conclusions
We have introduced and detailed three general purpose propagation algorithms for short
supports. They each can either be given a specialised function to find new supports for each
constraint, or used with a function that accepts an explicit list of short supports. Where
strict short supports are available, all three algorithms perform very well, and provide much
39

Nightingale, Gent, Jefferson, & Miguel

better performance than the general purpose methods GAC-Schema and Constructive Or.
This shows the value of using strict short supports.
The first algorithm we studied was ShortGAC, for which we described improvements
compared to our earlier report on this algorithm (Nightingale et al., 2011). We identified a significant inefficiency with ShortGAC when dealing with explicit supports. We
introduced a new algorithm, HaggisGAC which corrects this flaw, has better theoretical
complexities, and performs much better than ShortGAC in our experiments. In three
case studies, HaggisGAC is far faster than the general purpose methods. In the best case
it even achieved speeds more than 90% of that of a special purpose propagator. Perhaps
remarkably, while able to deal with both strict short and full-length supports, HaggisGAC outperformed ShortGAC on strict short supports and GAC-Schema on full-length
supports, i.e. the cases which those algorithms are respectively specialised for.
Our third algorithm, HaggisGAC-Stable, can retain supports on backtracking. It can
be less effective than HaggisGAC if it invalidates the use of certain strict short supports,
but it can also be significantly faster on problems with only full-length supports, and reduce
memory usage greatly in those cases.
All the proposed algorithms are excellent for propagating disjunctions of constraints. In
all experiments with disjunctions we found our algorithms to be faster than Constructive Or
and GAC-Schema by at least an order of magnitude, and up to three orders of magnitude.
To summarise, we have shown the value of the explicit use of strict short supports in
general purpose propagation algorithms for generalised arc consistency. When strict short
supports are available, exploiting them yields orders of magnitude improvements for generic
propagation algorithms. In some cases, we even found that a generic algorithm can come
close to the performance of a specialised propagator. Previously, short supports do not
seem to have been recognised as important in their own right. Our overall contribution is
to correct this and focus on short supports as first class objects.

Acknowledgments
We would like to thank anonymous reviewers and Bilal Syed Hussain for their comments,
and EPSRC for funding this work through grants EP/H004092/1 and EP/E030394/1.

Appendix A. Comparison of ShortGAC and ShortGAC-IJCAI
In Section 4, we noted that we have optimised data structures and algorithms for ShortGAC, compared with our previous presentation (Nightingale et al., 2011). To demonstrate
that these are indeed improvements, we compared the two implementations of ShortGAC
on the three case studies used in this paper. We use the name ShortGAC-IJCAI for the
previous version. We are not quoting results from our previous work (Nightingale et al.,
2011), but have rerun all experiments using the environment described in Section 7. We
also updated the codebase to Minion 0.12 instead of Minion 0.10 in our earlier paper. For
each algorithm and instance, we report nodes searched per second and peak memory use.
Table 7 shows results for the instances of Section 7.1. It is clear from the results that
ShortGAC makes much better use of memory and is also faster than ShortGAC-IJCAI
40

Short and Long Supports for Constraint Propagation

0.16

low memory, Sections 7 and 9
high memory, Sections 7 and 9
List + NDList, Section 7.4
GAC-Schema + Constructive OR

0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0.1

1

10

100

1000

10000

100000

1e+06

Figure 6: Scatterplot of median nodes per second (x-axis) against the median absolute
deviation of this divided by the median (y-axis). We distinguish between the
main experiments of Sections 7 and 9, the cases where medians were of only 5
runs, the list variants used on table constraints in Section 7.4, and data in the
paper for GAC-Schema and Constructive Or.

on these instances. Table 8 shows the results for instances of Section 7.2. As with Element,
ShortGAC makes better use of memory and is faster than ShortGAC-IJCAI, although
improvements are not as great as before. In Table 9, we use the instances from Section 7.3.
As in the previous two case studies, ShortGAC is consistently better in both speed and
memory use. We conclude that the algorithms and data structures used in this paper are
indeed superior to those we used previously (Nightingale et al., 2011).

Appendix B. Median Absolute Deviation of our Experiments
In our experiments we report the median of either 11 or 5 runs. To assess how robust the
median was as a measure we looked, for each combination of instances and algorithm, at
the median absolute deviation (MAD), i.e. the median of the absolute difference of data
points from the median. Figure 6 shows the MAD for all algorithm/instance combinations
as a fraction of the median for that case. This shows 511 algorithm/instance combinations
we tested (including some combinations not reported in detail in this paper). For nodes per
second, the maximum MAD we found is always less than 15% of the median, with a worst
case of 14.5%. This was HaggisGAC-Long for n = 9 in Table 1. There were only four
more cases with MAD above 8% of the median. Figures for memory usage were even more
consistent, with only two cases (at 6.3% and 6.1%) showing MAD above 5% of the median
and and no others above 2%. Any major conclusions we draw do not regard a 10% change
of behaviour between one method and another as significant, and therefore we can say that
the median is a robust measure of performance.
41

Nightingale, Gent, Jefferson, & Miguel

n
6
7
8
9
10

ShortGAC
node rate
6,956
4,866
2,773
2,374
‡
1,594

ShortGAC-IJCAI
node rate
4,839
3,273
1,673
1,511
‡
1,294

ShortGAC
memory
5,684
6,624
8,996
12,560
‡
17,048

ShortGAC-IJCAI
memory
27,880
72,916
188,812
461,648
‡
991,768

Table 7: Nodes searched per second and memory use (KiB) for quasigroup existence problems. Comparison of ShortGAC with ShortGAC-IJCAI.

n
3
4
5
6
7
8
9
10
12
14
16
18
20
22
24

ShortGAC
node rate
87,463
99,602
89,127
73,260
65,062
51,335
47,059
38,344
31,626
22,712
17,813
13,843
10,734
7,976
6,255

ShortGAC-IJCAI
node rate
83,964
98,135
89,286
74,184
63,091
50,480
45,085
36,179
29,455
20,868
16,087
12,356
9,614
7,208
5,398

ShortGAC
memory
7,476
11,680
16,408
22,568
31,348
42,420
55,660
74,348
120,024
181,252
263,792
360,500
493,368
632,064
811,104

ShortGAC-IJCAI
memory
8,392
12,992
18,512
26,260
36,356
49,012
65,684
85,700
138,496
209,492
308,400
422,536
570,188
735,548
939,796

Table 8: Nodes searched per second and memory use for BIBD problems. Comparison of
ShortGAC with ShortGAC-IJCAI.

n-w-h
18-31-69
19-47-53
20-34-85
21-38-88
22-39-98
23-64-68
24-56-88
25-43-129
26-70-89
27-47-148

ShortGAC
node rate
14,923
38,329
13,949
8,568
8,059
31,486
12,317
5,310
25,860
2,943

ShortGAC-IJCAI
node rate
10,892
29,647
10,288
6,109
5,821
24,528
8,386
3,828
21,146
2,086

ShortGAC
memory
11,876
10,172
13,988
16,100
18,868
13,988
17,548
27,580
19,796
39,848

ShortGAC-IJCAI
memory
24,568
19,680
33,020
38,828
46,344
31,700
43,708
74,064
49,512
106,144

Table 9: Nodes searched per second and memory use for rectangle packing. Comparison of
ShortGAC with ShortGAC-IJCAI.

42

Short and Long Supports for Constraint Propagation

Appendix C. Comparison of GAC-Schema and HaggisGAC
We showed in Section 7.4 that HaggisGAC outperforms GAC-Schema when dealing with
full-length supports. This is despite the fact that HaggisGAC has small overheads for
dealing with strict short supports even when none exist. We now discuss briefly why this
may be so.
GAC-Schema has the concept of current supports – each literal has one current support,
which is one of the active supports that contain the literal. There is an additional data
structure S(τ ). For each active support τ , S(τ ) is a list of all literals that have τ as their
current support. Hence when τ is invalidated, GAC-Schema finds a new current support
for each literal in S(τ ) (or deletes the literal). In HaggisGAC we dispensed with this
entirely. The sign that a literal needs a new support is not that it lost its current support,
but that its support list (supportListPerLit) is empty. There is a small potential saving from
not maintaining S(τ ).
A second, possibly more important, difference is that GAC-Schema is more eager than
HaggisGAC. When a literal x 7→ v loses its current support, GAC-Schema will check if
other active supports containing x 7→ v are valid, an O(n) operation for each one. If they
are all invalid, GAC-Schema then calls findNewSupport. If this returns Null then x 7→ v
is deleted. HaggisGAC does none of this, avoiding completely the cost of checking validity. This is safe because if every support is invalid, the literal deletion from each support
will cause a call to deleteSupport and the last will result in the empty list, causing a call
to findNewSupport. Both approaches are correct, but GAC-Schema’s is wasteful because
it performs unnecessary validity checks. However, one cannot guarantee time saving, because GAC-Schema can perform deletions sooner, possibly affecting the way the propagator
interacts with the other propagators.

References
Bessière, C., Hebrard, E., Hnich, B., & Walsh, T. (2007). The complexity of reasoning with
global constraints. Constraints, 12 (2), 239–259.
Bessière, C., & Régin, J.-C. (1997). Arc consistency for general constraint networks: Preliminary results. In Proceedings IJCAI 1997, pp. 398–404.
Bessière, C., Régin, J.-C., Yap, R. H. C., & Zhang, Y. (2005). An optimal coarse-grained
arc consistency algorithm. Artificial Intelligence, 165 (2), 165–185.
Cheng, K. C. K., & Yap, R. H. C. (2010). An MDD-based generalized arc consistency
algorithm for positive and negative table constraints and some global constraints.
Constraints, 15 (2), 265–304.
Colton, S., & Miguel, I. (2001). Constraint generation via automated theory formation. In
Proceedings CP 2001, pp. 575–579.
Flener, P., Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., Pearson, J., & Walsh, T. (2002).
Breaking row and column symmetries in matrix models. In Proceedings CP 2002, pp.
462–476.
Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., & Walsh, T. (2002). Global constraints
for lexicographic orderings. In Proceedings CP 2002, pp. 93–108.
43

Nightingale, Gent, Jefferson, & Miguel

Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., & Walsh, T. (2006). Propagation algorithms
for lexicographic ordering constraints. Artificial Intelligence, 170 (10), 803–834.
Gent, I. P. (2012). An optimality result on maintaining list pointers during backtracking
search. Tech. rep. CIRCA preprint 2012/1, University of St Andrews.
Gent, I. P., Jefferson, C., & Miguel, I. (2006a). Minion: A fast scalable constraint solver.
In Proceedings ECAI 2006, pp. 98–102.
Gent, I. P., Jefferson, C., & Miguel, I. (2006b). Watched literals for constraint propagation
in Minion. In Proceedings CP 2006, pp. 182–197.
Gent, I. P., Jefferson, C., Miguel, I., & Nightingale, P. (2007). Data structures for generalised
arc consistency for extensional constraints. In Proceedings AAAI 2007, pp. 191–197.
Gent, I. P., Petrie, K., & Puget, J.-F. (2006). Handbook of Constraint Programming (Foundations of Artificial Intelligence), chap. Symmetry in Constraint Programming, pp.
329–376. Elsevier Science Inc., New York, NY, USA.
Jefferson, C., Moore, N. C. A., Nightingale, P., & Petrie, K. E. (2010). Implementing logical
connectives in constraint programming. Artificial Intelligence, 174 (16-17), 1407–1429.
Katsirelos, G., & Walsh, T. (2007). A compression algorithm for large arity extensional
constraints. In Proceedings CP 2007, pp. 379–393.
King, A., Cromarty, L., Paterson, C., & Boyd, J. (2007). Applications of ultrasonography
in the reproductive management of dux magnus gentis venteris saginati. Veterinary
record, 160 (3), 94.
Lagerkvist, M. Z., & Schulte, C. (2009). Propagator groups. In Proceedings CP 2009, pp.
524–538.
Lecoutre, C. (2011). STR2: optimized simple tabular reduction for table constraints. Constraints, 16 (4), 341–371.
Lecoutre, C., & Szymanek, R. (2006). Generalized arc consistency for positive table constraints. In Proceedings CP 2006, pp. 284–298.
Lhomme, O., & Régin, J.-C. (2005). A fast arc consistency algorithm for n-ary constraints.
In Proceedings AAAI 2005, pp. 405–410.
Lhomme, O. (2003). An efficient filtering algorithm for disjunction of constraints. In
Proceedings CP 2003, pp. 904–908.
Lhomme, O. (2004). Arc-consistency filtering algorithms for logical combinations of constraints. In Integration of AI and OR Techniques in Constraint Programming for
Combinatorial Optimization Problems (CP-AI-OR’04), pp. 209–224.
Mackworth, A. K. (1977). On reading sketch maps. In Reddy, R. (Ed.), IJCAI, pp. 598–606.
William Kaufmann.
Mears, C. D. (2009). Automatic Symmetry Detection and Dynamic Symmetry Breaking for
Constraint Programming. Ph.D. thesis, Clayton School of Information Technology,
Monash University.
Mohr, R., & Henderson, T. C. (1986). Arc and path consistency revisited. Artificial Intelligence, 28 (2), 225–233.
44

Short and Long Supports for Constraint Propagation

Nightingale, P. (2011). The extended global cardinality constraint: An empirical survey.
Artificial Intelligence, 175 (2), 586–614.
Nightingale, P., Gent, I. P., Jefferson, C., & Miguel, I. (2011). Exploiting short supports
for generalised arc consistency for arbitrary constraints. In Proceedings IJCAI 2011,
pp. 623–628.
Puget, J.-F. (2005). Automatic detection of variable and value symmetries. In Proceedings
CP 2005, pp. 475–489.
Régin, J.-C. (1996). Generalized arc consistency for global cardinality constraint. In Proceedings AAAI 1996, pp. 209–215.
Régin, J.-C. (2005). Maintaining arc consistency algorithms during the search without
additional space cost. In Proceedings CP 2005, pp. 520–533.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook of Constraint Programming.
Elsevier.
Schulte, C., & Tack, G. (2010). Implementing efficient propagation control. In Proceedings of
TRICS: Techniques foR Implementing Constraint programming Systems, a conference
workshop of CP 2010, St Andrews, UK.
Simonis, H., & O’Sullivan, B. (2008). Search strategies for rectangle packing. In Proceedings
CP 2008, pp. 52–66.
Würtz, J., & Müller, T. (1996). Constructive disjunction revisited. In Proceedings of
the 20th Annual German Conference on Artificial Intelligence: Advances in Artificial
Intelligence, KI ’96, pp. 377–386. Springer-Verlag.

45

Journal of Artificial Intelligence Research 46 (2013) 651–686

Submitted 10/12; published 04/13

Description Logic Knowledge and Action Bases
Babak Bagheri Hariri
Diego Calvanese
Marco Montali

BAGHERI @ INF. UNIBZ . IT
CALVANESE @ INF. UNIBZ . IT
MONTALI @ INF. UNIBZ . IT

KRDB Research Centre for Knowledge and Data
Free University of Bozen-Bolzano
Piazza Domenicani 3, 39100 Bolzano, Italy

Giuseppe De Giacomo
Riccardo De Masellis
Paolo Felli

DEGIACOMO @ DIS . UNIROMA 1. IT
DEMASELLIS @ DIS . UNIROMA 1. IT
FELLI @ DIS . UNIROMA 1. IT

Dipartimento di Ingegneria Informatica Automatica e Gestionale
Sapienza Università di Roma
Via Ariosto 25, 00185 Roma, Italy

Abstract
Description logic Knowledge and Action Bases (KAB) are a mechanism for providing both a
semantically rich representation of the information on the domain of interest in terms of a description logic knowledge base and actions to change such information over time, possibly introducing
new objects. We resort to a variant of DL-Lite where the unique name assumption is not enforced
and where equality between objects may be asserted and inferred. Actions are specified as sets
of conditional effects, where conditions are based on epistemic queries over the knowledge base
(TBox and ABox), and effects are expressed in terms of new ABoxes. In this setting, we address
verification of temporal properties expressed in a variant of first-order µ-calculus with quantification across states. Notably, we show decidability of verification, under a suitable restriction inspired
by the notion of weak acyclicity in data exchange.

1. Introduction
Recent work in business processes, services and databases is bringing forward the need of considering both data and processes as first-class citizens in process and service design (Nigam & Caswell,
2003; Bhattacharya, Gerede, Hull, Liu, & Su, 2007; Deutsch, Hull, Patrizi, & Vianu, 2009; Vianu,
2009; Meyer, Smirnov, & Weske, 2011). In particular, the so-called artifact-centric approaches,
which advocate a sort of middle ground between a conceptual formalization of dynamic systems
and their actual implementation, are promising to be effective in practice (Cohn & Hull, 2009). The
verification of temporal properties in the presence of data represents a significant research challenge (for a survey, see Calvanese, De Giacomo, & Montali, 2013), since taking into account how
data evolve over time results in systems that have an infinite number of states. Neither finite-state
model checking (Clarke, Grumberg, & Peled, 1999) nor most of the current techniques for infinitestate model checking, which mostly tackle recursion (Burkart, Caucal, Moller, & Steffen, 2001),
apply to this case. Recently, there have been some advancements on this issue (Cangialosi, De Giacomo, De Masellis, & Rosati, 2010; Damaggio, Deutsch, & Vianu, 2011; Bagheri Hariri, Calvanese,
De Giacomo, De Masellis, & Felli, 2011; Belardinelli, Lomuscio, & Patrizi, 2011), in the context
of suitably constrained relational database settings.
c
2013
AI Access Foundation. All rights reserved.

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

While most of this work is based on maintaining information in a relational database, for more
sophisticated applications it is foreseen to enrich data-intensive business processes with a semantic
level, where information can be maintained in a semantically rich knowledge base which allows for
operating with incomplete information (Calvanese, De Giacomo, Lembo, Montali, & Santoso, 2012;
Limonad, De Leenheer, Linehan, Hull, & Vaculin, 2012). This leads us to look into how to combine
first-order data, ontologies, and processes, while maintaining basic inference tasks (specifically
verification) decidable. In this setting, we capture the domain of interest in terms of semantically
rich formalisms as those provided by ontological languages based on Description Logics (DLs)
(Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). Such languages natively deal
with incomplete knowledge in the modeled domain. This additional flexibility comes with an added
cost, however: differently from relational databases, to evaluate queries we need to resort to logical
implication. Moreover, incomplete information combined with the ability of evolving the system
through actions results in a notoriously fragile setting w.r.t. decidability (Wolter & Zakharyaschev,
1999b, 1999a; Gabbay, Kurusz, Wolter, & Zakharyaschev, 2003). In particular, due to the nature
of DL assertions (which in general are not definitions but constraints on models), we get one of
the most difficult kinds of domain descriptions for reasoning about actions (Reiter, 2001), which
amounts to dealing with complex forms of state constraints (Lin & Reiter, 1994).
To overcome this difficulty, virtually all solutions that aim at robustness are based on a so-called
“functional view of knowledge bases” (Levesque, 1984): the KB provides the ability of querying
based on logical implication (“ask”), and the ability of progressing it to a “new” KB through forms
of updates (“tell”) (Baader, Ghilardi, & Lutz, 2012; Calvanese, De Giacomo, Lenzerini, & Rosati,
2011). Notice that this functional view is tightly related to an epistemic interpretation of the KB
(Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007a). Indeed our work is also related to
that on Epistemic Dynamic Logic (van Ditmarsch, van der Hoek, & Kooi, 2007), and, though out of
the scope of this paper, the decidability results presented here could find application in the context
of that research as well.
We follow this functional view of KBs. However, a key point of our work is that at each execution step external information is incorporated into the system in form of new individuals (denoted
by function terms), that is, our systems are not closed w.r.t. the available information. This makes
our framework particularly interesting and challenging. In particular, the presence of these individuals requires a specific treatment of equality, since as the system progresses and new information is
acquired, distinct function terms may be inferred to denote the same object.
Specifically, we introduce the so-called Knowledge and Action Bases (KABs). A KAB is
equipped with an ontology or, more precisely, a TBox expressed, in our case, in a variant of DLLiteA (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007b), which extends the core of the
Web Ontology Language OWL 2 QL (Motik, Cuenca Grau, Horrocks, Wu, Fokoue, & Lutz, 2012)
and is particularly well suited for data management. Such a TBox captures intensional information
on the domain of interest, similarly to UML class diagrams or other conceptual data models, though
as a software component to be used at run-time. The KAB includes also an ABox, which acts as a
storage or state. The ABox maintains the data of interest, which are accessed by relying on query answering based on logical implication (certain answers). Notably, our variant of DL-LiteA is without
the unique name assumption (UNA), and we allow for explicit equality assertions in the ABox. In
this way we can suitably treat function terms to represent individuals acquired during the execution.
Technically, the need of dealing with equality breaks the first-order rewritability of DL-LiteA query
answering, and requires that, in addition to the rewriting process, inference on equality is performed
652

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

(Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009). As a query language, we use unions of
conjunctive queries, possibly composing their certain answers through full FOL constructs. This
gives rise to an epistemic query language that asks about what is “known” by the current KB (Calvanese et al., 2007a). Apart from the KB, the KAB contains actions, whose execution changes
the state of the KB, i.e., its ABox. Such actions are specified as sets of conditional effects, where
conditions are (epistemic) queries over the KB and effects are expressed in terms of new ABoxes.
Actions have no static pre-conditions, whereas a process is used to specify which actions can be
executed at each step. For simplicity, we model such processes as condition/action rules, where the
condition is again expressed as a query over the KB.
In this setting, we address the verification of temporal/dynamic properties expressed in a firstorder variant of µ-calculus (Park, 1976; Stirling, 2001), where atomic formulae are queries over
the KB which can refer both to constants and to function terms, and where a controlled form of
quantification across states is allowed. Notice that all previous decidability results on actions over
DL KBs assumed that no information is coming from outside of the system, in the sense that no
new individual terms are added while executing actions (Calvanese et al., 2011; Baader et al., 2012;
Rosati & Franconi, 2012). In this paper, instead, we allow for arbitrary introduction of new terms.
Unsurprisingly, we show that even for very simple KABs and temporal properties, verification is
undecidable. However, we also show that for a rich class of KABs, verification is in fact decidable
and reducible to finite-state model checking. To obtain this result, following Cangialosi et al. (2010),
and Bagheri Hariri et al. (2011), we rely on recent results in data exchange on the finiteness of the
chase of tuple-generating dependencies (Fagin, Kolaitis, Miller, & Popa, 2005), though, in our case,
we need to extend the approach to deal with (i) incomplete information, (ii) inference on equality,
and (iii) quantification across states in the verification language.
The paper is organized as follows. In Section 2 we give preliminaries about DL-LiteA without
UNA , which is going to be our knowledge base formalism. Section 3 describes the KAB framework in detail, while Section 4 discusses its execution semantics. In Section 5 we introduce the
verification formalism for KABs. In Section 6, we show that verification of KABs is in general undecidable, even considering very simple temporal properties and KABs. In Section 7, we give our
main technical result: verification of weakly acyclic KABs is decidable in E XP T IME. In Section 8,
we extensively survey related work. Section 9 concludes the paper.

2. Knowledge Base Formalism
Description Logics (DLs) (Baader et al., 2003) are knowledge representation formalisms that are
tailored for representing the domain of interest in terms of concepts (or classes), denoting sets of
objects, and roles (or relations), denoting binary relations between objects. DL knowledge bases
(KBs) are based on an alphabet of concept and role names, and an alphabet of individuals. A
DL KB is formed by two distinct parts: a TBox, which represents the intensional level of the KB
and contains a description of the domain of interest in terms of universal assertions over concepts
and roles; and an ABox, which represents the instance level of the KB and contains extensional
information on the participation of individuals to concepts and roles.
For expressing KBs we use DL-LiteNU , a variant of the DL-LiteA language (Poggi, Lembo,
Calvanese, De Giacomo, Lenzerini, & Rosati, 2008; Calvanese, De Giacomo, Lembo, Lenzerini, &
Rosati, 2013) in which we drop the unique name assumption (UNA) in line with the standard Web
Ontology Language (OWL 2) (Bao et al., 2012). Essentially, DL-LiteNU extends the OWL 2 QL
653

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

profile of OWL 2, by including functionality assertions and the possibility to state equality between
individuals.
The syntax of concept and role expressions in DL-LiteNU is as follows:
R −→ P | P − ,
V −→ R | ¬R,

B −→ N | ∃R,
C −→ B | ¬B,

where N denotes a concept name, P a role name, and P − an inverse role.
Formally, in a DL-LiteNU KB (T, A), the TBox T is a finite set of TBox assertions of the form
B v C,

R v V,

(funct R),

called respectively concept inclusions, role inclusions, and functionality assertions. We follow
the usual assumption in DL-Lite, according to which a TBox may contain neither (funct P ) nor
(funct P − ) if it contains R v P or R v P − , for some role R (Poggi et al., 2008; Calvanese et al.,
2013). This condition expresses that roles in functionality assertions cannot be specialized.
DL-LiteNU TBoxes are able to capture the essential features of conceptual modeling formalisms,
such as UML Class Diagrams (or Entity-Relationship schemas), namely ISA between classes and
associations (relationships), disjointness between classes and between associations, typing of associations, and association multiplicities (in particular, mandatory participation and functionality).
The main missing feature is completeness of hierarchies, which would require the introduction of
disjunction and would compromise the good computational properties of DL-Lite.
The ABox A in a DL-LiteNU KB (T, A) is a finite set of ABox assertions of the form
N (t1 ),

P (t1 , t2 ),

t1 = t2 ,

called respectively, concept (membership) assertions, role (membership) assertions, and equality
assertions, where t1 , t2 are terms denoting individuals (see below). The presence of equality assertions in the ABox requires a specific treatment of equality that goes beyond the usual reasoning
techniques for DL-Lite based on first-order rewritability, although reasoning remains polynomial
(Artale et al., 2009). On the other hand, we do not allow for explicit disequality, though one can use
membership in disjoint concepts to assert that two individuals are different.
DL-LiteNU admits complex terms for denoting individuals. Such terms are inductively defined
by starting from a finite set of constants, and applying a finite set of (uninterpreted) functions of
various arity greater than 0. As a result, the set of individual terms is countably infinite. We
call function terms those terms involving functions. Also, the structure of terms has an impact on
inference over equality, which is a congruence relation on the structure of terms, i.e., if ti = t0i , for
i ∈ {1, . . . , n}, and f is a function symbol of arity n, then f (t1 , . . . , tn ) = f (t01 , . . . , t0n ). Apart
from this aspect related to equality, we can treat individuals denoted by terms simply as ordinary
individual constants in DLs.
We adopt the standard semantics of DLs based on FOL interpretations I = (∆I , ·I ), where ∆I
is the interpretation domain and ·I is the interpretation function such that tI ∈ ∆I , N I ⊆ ∆I , and
P I ⊆ ∆I ×∆I , for each term t,concept name N , and role name P . Coherently with the congruence
relation on terms, we have that (f (t1 , . . . , tn ))I = (f (t01 , . . . , t0n ))I , whenever tIi = t0i I , for i ∈
{1, . . . , n}.
Complex concepts and roles are interpreted as follows:
(∃R)I
(¬B)I

= {o | ∃o0 .(o, o0 ) ∈ RI },
= ∆I \ B I ,

(P − )I
(¬R)I
654

= {(o1 , o2 ) | (o2 , o1 ) ∈ P I },
= ∆ I × ∆I \ R I .

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

An interpretation I satisfies an assertion of the form:
• B v C, if B I ⊆ C I ;
• R v V , if RI ⊆ V I ;
• (funct R), if for all o, o1 , o2 we have that, if {(o, o1 ), (o, o2 )} ⊆ RI , then o1 = o2 ;
• N (t1 ), if tI1 ∈ N I ;
• P (t1 , t2 ), if (tI1 , tI2 ) ∈ P I ;
• t1 = t2 , if tI1 = tI2 .
I is a model of a KB (T, A) if it satisfies all assertions in T and A. KB (T, A) is satisfiable if it has
a model. We also say that an ABox A is consistent w.r.t. a TBox T if the KB (T, A) is satisfiable.
An assertion α is logically implied by a KB (T, A), denoted (T, A) |= α, if every model of (T, A)
satisfies α as well.
The following characterization of satisfiability and logical implication in DL-LiteNU is an easy
consequence of results by Artale et al. (2009).
Theorem 1 Checking satisfiability and logical implication in DL-LiteNU are PT IME-complete.
Proof. The PT IME lower bound is an immediate consequence of the same lower bound established
by Artale et al. (2009) for DL-LiteNU in which we do not allow the use of complex individual terms.
For the upper bound, Artale et al. (2009) provide a PT IME algorithm that is based on first using
functionality assertions to exhaustively propagate equality, and then resorting to a PT IME algorithm
(in combined complexity) for reasoning in DL-Lite in the absence of UNA. We can adapt that
algorithm by changing the first step, so as to propagate, again in PT IME, equality over terms in the
active domain not only due to functionalities, but also due to congruence.
Next we introduce queries. As usual (cf. OWL 2), answers to queries are formed by constants/terms denoting individuals explicitly mentioned in the ABox. The (active) domain of an
ABox A, denoted by ADOM(A), is the (finite) set of constants/terms appearing in concept, role, and
equality assertions in A. The (predicate) alphabet of a KB (T, A), denoted ALPH((T, A)) is the set
of concept and role names occurring in T ∪ A.
A union of conjunctive queries (UCQ) q over a KB (T, A) is a FOL formula of the form
∃~y1 .conj 1 (~x, ~y1 ) ∨ · · · ∨ ∃~yn .conj n (~x, ~yn ) with free variables ~x and existentially quantified variables ~y1 , . . . , ~yn . Each conj i (~x, y~i ) in q is a conjunction of atoms of the form N (z), P (z, z 0 ) where
N and P respectively denote a concept and a role name occurring in ALPH((T, A)), and z, z 0 are
constants in ADOM(A) or variables in ~x or y~i , for some i ∈ {1, . . . , n}. The certain answers to q
over (T, A) is the set ANS (q, T, A) of substitutions1 σ of the free variables of q with constants/terms
in ADOM(A) such that qσ evaluates to true in every model of (T, A), i.e., qσ is logically implied by
(T, A). Following the notation used for assertions, we denote this as (T, A) |= qσ. If q has no free
variables, then it is called boolean and its certain answers are either the empty substitution denoting
true or nothing denoting false.
Again, as an easy consequence of the results by Artale et al. (2009), we obtain the following
characterization of query answering in DL-LiteNU .
Theorem 2 Computing ANS (q, T, A) of an UCQ q over a DL-LiteNU KB (T, A) is PT IME-complete
in the size of T and A.
1. As customary, we can view each substitution simply as a tuple of constants, assuming some ordering of the free
variables of q.

655

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

Proof. As in the proof of Theorem 1, we can first propagate in PT IME equality over terms in the
active domain by using functionality and congruence closure, and then resort to query answering in
DL-Lite in the presence of UNA, which is in PT IME in the combined size of the TBox T and the
ABox resulting from the above equality propagation (actually, in AC0 in the size of this ABox).
We also consider an extension of UCQs, called ECQs, which are queries of the query language
EQL-Lite(UCQ) (Calvanese et al., 2007a), that is, the FOL query language whose atoms are UCQs
evaluated according to the certain answer semantics above. An ECQ over a KB (T, A) is a possibly
open formula of the form
Q −→ [q] | [x = y] | ¬Q | Q1 ∧ Q2 | ∃x.Q,
where [q] denotes the certain answers of a UCQ q over (T, A), [x = y] denotes the certain answers
of x = y over (T, A), that is, the set {hx, yi ∈ ADOM(A) | (T, A) |= (x = y)}, logical operators
have the usual meaning, and quantification ranges over elements of ADOM(A).
Formally we define the relation Q holds in (T, A) under substitution σ of all free variables in
Q, written T, A, σ |= Q, inductively as follows:
T, A, σ
T, A, σ
T, A, σ
T, A, σ
T, A, σ

|= [q]
|= [x = y]
|= ¬Q
|= Q1 ∧ Q2
|= ∃x.Q

if
if
if
if
if

(T, A) |= qσ,
(T, A) |= (x = y)σ,
T, A, σ 6|= Q,
T, A, σ |= Q1 and T, A, σ |= Q2 ,
exists t ∈ ADOM(A) such that T, A, σ[x/t] |= Q,

where σ[x/t] denotes the substitution obtained from σ by assigning to x the constant/term t (if x is
already present in σ its value is replaced by t, if not, the pair x/t is added to the substitution).
The certain answer to Q over (T, A), denoted ANS (Q, T, A), is the set of substitutions σ for the
free variables in Q such that Q holds in (T, A) under σ, i.e.,
ANS (Q, T, A)

= {σ | T, A, σ |= Q}.

Following the line of the proof by Calvanese et al. (2007a), but considering Theorem 2 for the
basic step of evaluating an UCQ, we get:
Theorem 3 Computing ANS (Q, T, A) of an ECQ Q over a DL-LiteNU KB (T, A) is PT IMEcomplete in the size of T and A.
We recall that DL-Lite enjoys a rewritability property, which states that for every UCQ q and
for every DL-Lite KB (T, A),
ANS (q, T, A)

= ANS (rew T (q), ∅, A),

where rew T (q) is a UCQ computed by the reformulation algorithm of Calvanese et al. (2007b).
Notice that, in this way, we have “compiled away” the TBox. This result can be extended to ECQs
as well, i.e., for every ECQ Q, ANS (Q, T, A) = ANS (rew T (Q), ∅, A) where the query rew T (Q) is
obtained from Q by substituting each atom [q] (where q is an UCQ) by [rew T (q)] (Calvanese et al.,
2007a). In our setting, we can again exploit rewritability, but only after having pre-processed the
ABox (in PT IME) by propagating equalities between individual terms in ADOM(A) according to
functionality assertions and congruence of terms.
656

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

We say that two ABoxes A1 and A2 are equivalent w.r.t. TBox T and predicate alphabet Λ,
denoted by
A1 ≡T,Λ A2 ,
if for every ABox assertion α2 ∈ A2 which is either a concept assertion N (t) with N ∈ Λ, role
assertion P (t1 , t2 ) with P ∈ Λ, or equivalence assertion t1 = t2 , we have (T, A1 ) |= α2 ; and viceversa, for every ABox assertion α1 ∈ A1 , which is either a concept assertion N (t) with N ∈ Λ,
role assertion P (t1 , t2 ) with P ∈ Λ, or equivalence assertion t1 = t2 , we have (T, A2 ) |= α1 .
Notice that if A1 ≡T,Λ A2 , then for every ECQ Q whose concept and role names belong to Λ we
have that ANS (Q, T, A1 ) = ANS (Q, T, A2 ). Notice also that, by applying Theorem 3 to the boolean
query [α] corresponding to the ABox assertion α, for each α in A1 and A2 , we obtain that ABox
equivalence can be checked in PT IME.

3. Knowledge and Action Bases
A Knowledge and Action Base (KAB) is a tuple K = (T, A0 , Γ, Π) where T and A0 form the
knowledge component (or knowledge base), and Γ and Π form the action component (or action
base). In practice, K stores the information of interest into a KB, formed by a fixed TBox T and
an initial ABox A0 , which evolves by executing actions Γ according to the sequencing established
by process Π. During the evolution new individuals can be acquired by the KB. Such individuals
are witnesses of new pieces of information inserted into the KAB from the environment the KAB
runs in (i.e., the external world). We represent these new objects as function terms. As the KAB
evolves, the identity of individuals should be intuitively preserved and this induces the necessity of
remembering equalities between terms denoting individuals discovered in the past. We describe in
detail the components of the KAB.
3.1 TBox
T is a DL-LiteNU TBox, used to capture the intensional knowledge about the domain of interest.
Such a TBox is fixed once and for all, and does not evolve during the execution of the KAB.
3.2 ABox
A0 is a DL-LiteNU ABox, which stores the extensional information of interest. Notice that A0 is
the ABox of the initial state of the KAB, and as the KAB evolves due to the effect of actions, the
ABox, which is indeed the state of the system, evolves accordingly to store up-to-date information.
Through actions we acquire new information from the external world by using calls to external
services represented through functions. Given that we have no information about these services,
except for their name and the parameters that are passed to them, the functions remain uninterpreted.
We only assume that the result of such service calls depends only on the passed parameters. Hence,
we represent the new individuals returned by service calls as function terms. The presence of
function terms has an impact on the treatment of equality, since in principle we need to close equality
w.r.t. congruence. While this closure generates an infinite number of logically implied equality
assertions, we are going to keep such assertions implicit, computing them only when needed.
657

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

3.3 Actions
Γ is a finite set of actions. An action γ ∈ Γ modifies the current ABox A by adding or deleting
assertions, thus generating a new ABox A0 . An action γ has the form
act(~x) : {e1 , . . . , en },
where act(~x) is the signature of γ and {e1 , . . . , en } is a (finite) set of effects forming the effect
specification of γ. The action signature is constituted by a name act and a list ~x of individual input
parameters, which need to be instantiated with actual individuals at execution time.2 An effect ei
has the form
[qi+ ] ∧ Q−
A0i ,
(1)
i
where
• qi+ is an UCQ, i.e., a positive query, which extracts the bulk data to process (obtained as the
certain answers of qi+ ); the free variables of qi+ include the action parameters;
+
• Q−
i is an arbitrary ECQ, whose free variables occur all among the free variables of qi , which
refines, by using negation and quantification, the result of qi+ . The query [qi+ ] ∧ Q−
i as a
whole extracts individual terms to be used to form the new state of the KAB (notice that the
UCQ-ECQ division is also a convenience to have readily available the positive part of the
condition, which we will exploit later);

• A0i is a set of (non-ground) ABox assertions, which include as terms: constants in A0 , free
variables of qi+ , and function terms f (~x) having as arguments ~x free variables of qi+ . These
terms, once grounded with the values extracted from [qi+ ] ∧ Q−
i , give rise to (ground) ABox
assertions, which contribute to form the next state of the KAB.
More precisely, given the current ABox A of K and a substitution θ for the input parameters of the
action γ, we denote by γθ the action instantiated with the actual parameters coming from θ. By
firing γθ on the state A, we get a new state A0 which is computed by simultaneously applying all
instantiated effects of γθ as follows:
• Each effect ei in γ of form (1) extracts from A the set ANS (([qi+ ] ∧ Q−
i )θ, T, A) of tuples of
terms in ADOM(A) and, for each such tuple σ, asserts a set A0i θσ of ABox assertions obtained
from A0i θ by applying the substitution σ for the free variables of qi+ . For each function term
f (~x)θ appearing in A0i θ, a new ground term is introduced having the form f (~x)θσ. These
terms represent new “constants” coming from the external environment the KAB is running
in.
We denote by ei θ(A) the overall set of ABox assertions, i.e.,
[

ei θ(A) =

A0i θσ.

σ∈ANS (([qi+ ]∧Q−
i )θ,T,A)

2. We disregard a specific treatment of output parameters, and assume instead that the user can freely pose queries over
the KB, extracting whatever information she/he is interested in.

658

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

• Moreover, let EQ(T, A) = {t1 = t2 | ht1 , t2 i ∈ ANS ([x1 = x2 ], T, A)}. Observe that, due
to the semantics of queries, the terms in EQ(T, A) must appear explicitly in ADOM(A), that
is, the possibly infinite number of equalities due to congruence do not appear in EQ(T, A),
though they are logically implied. Hence, the equalities in EQ(T, A) are the equality assertions involving terms in ADOM(A) that either appear explicitly in A, or are obtained by
closing these under functionality and congruence of terms.
The overall effect of the action γ with parameter substitution θ over A is the new ABox A0 =
DO (T, A, γθ) where
[
ei θ(A).
DO (T, A, γθ) = EQ(T, A) ∪
1≤i≤n

Notice that the presence of function terms in action effects makes the domain of the ABoxes obtained by executing actions continuously changing and in general unbounded in size. Notice also
that we do have a persistence assumption on equalities, i.e., we implicitly copy all equalities holding in the current state to the new one. This implies that, as the system evolves, we acquire new
information on equalities between terms, but never lose information on equalities already acquired.
Finally, we observe that in the above execution mechanism no persistence/frame assumption (except for equality) is made. In principle at every move we substitute the whole old state, i.e., ABox,
with a new one. On the other hand, it should be clear that we can easily write effect specifications
that copy big chunks of the old state into the new one. For example, [P (x, y)]
P (x, y) copies
the entire set of assertions involving the role P . In some sense, the execution mechanism adopted
in this paper is very basic and does not address any of the elaboration tolerance issues typical of
reasoning about actions, such as the frame problem, ramification problem or qualification problem
(Reiter, 2001)3 . This is not because we consider them irrelevant, on the contrary, they are relevant
and further research on such issues is desirable. We adopt this basic mechanism simply because
it is general enough to expose all difficulties we need to overcome in order to get decidability of
verification in this setting.
3.4 Process
The process component of a KAB is a possibly nondeterministic program that uses the KAB ABoxes
to store its (intermediate and final) computation results, and the actions in Γ as atomic instructions.
The ABoxes can be arbitrarily queried through the KAB TBox T , while they can be updated only
through actions in Γ. To specify such a process component we adopt a rule-based specification.
Specifically, a process is a finite set Π of condition/action rules. A condition/action rule π ∈ Π
is an expression of the form
Q 7→ γ,
where γ is an action in Γ and Q is an ECQ, whose free variables are exactly the parameters of
γ. The rule expresses that, for each tuple θ for which condition Q holds, the action γ with actual
parameters θ can be executed. Processes do not force the execution of actions but constrain them:
the user of the process will be able to choose any action that the rules forming the process allow.
Moreover, our processes inherit entirely their states from the KAB knowledge component (TBox
and ABox) (see, e.g., Cohn & Hull, 2009).
3. But see also the work by Kowalski and Sadri (2011).

659

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

Villain v Character
∃livesIn v Character
∃livesIn− v City
Character v ∃livesIn
(funct livesIn)
∃enemy v Villain
∃enemy− v Superhero
∃defeated v Villain
∃defeated− v Superhero
defeated v enemy
∃alterEgo v Superhero
∃alterEgo− v Character
(funct alterEgo)

0..1

alterEgo
enemy

Superhero

Character

1..1

livesIn

City

Villain

{subset}
defeated

Figure 1: KAB’s TBox for Example 1
We observe that we adopt a basic rule-based specification here because, in spite of its simplicity,
it is able to expose all the difficulties of our setting. Other choices are also possible, in particular,
the process could maintain its own state besides the one of the KAB. As long as such an additional
state is finite, or embeddable into the KAB itself, the results here would easily extend to such a case.
Example 1 Let us consider a KAB K = (T, A0 , Γ, Π) describing a super-heroes comics world,
where we have cities in which characters live. Figure 1 shows the TBox T and its rendering as a
UML Class Diagram. For the relationship between UML Class Diagrams and Description Logics
in general and DL-Lite in particular, we refer to the work by Berardi, Calvanese, and De Giacomo
(2005) and by Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, Rodrı́guez-Muro, and Rosati
(2009). As for the dynamics of the domain, characters can be superheroes or (super)villains, who
fight each other. As in the most classic plot, superheroes help the endeavors of law enforcement
fighting villains threatening the city they live in. When a villain reveals himself for perpetrating
his nefarious purposes against the city’s peace, he consequently becomes a declared enemy of all
superheroes living in that city. Each character lives in one city at the time. A common trait of
superheroes is a secret identity: a superhero is said to be the alter ego of some character, which is
his identity in common life. Hence, the ABox assertion alterEgo(s, p) means that the superhero s is
the alter ego of character p. Villains always try to unmask superheroes, i.e., find their secret identity,
in order to exploit such a knowledge to defeat them. Notice the subtle difference here: we use the
alterEgo(s, p) assertion to model the fact that s is the alter ego of p, whereas only by asserting s = p
we can capture the knowledge that s and p semantically denote the same individual. Γ may include
actions like the following ones:
BecomeSH(p, c) : { [Character(p) ∧ livesIn(p, c) ∧ ∃v.Villain(v) ∧ livesIn(v, c)]
{Superhero(sh(p)), alterEgo(sh(p), p)},
CopyAll }
states that if there exists at least one villain living in the city c, a new superhero sh(p) can be created,
with the purpose of protecting c. Such a superhero has p as alter ego. CopyAll is a shortcut for
explicitly copying all concept and role assertions to the new state (equality assertions are always
660

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

implicitly copied). Action
Unmask(s, p) : { [alterEgo(s, p)]
CopyAll }

{s = p},

states that superhero s, who is the alter ego of p, gets unmasked by asserting the equality between s
and p (it is now known that s = p). Action
Fight(v, s) : { ∃p.[Villain(v) ∧ Character(p) ∧ alterEgo(s, p)] ∧ [s = p]
CopyAll }

{defeated(v, s)},

states that when villain v fights superhero s, he defeats s if s has been unmasked, i.e., it is known
that s is equal to his alter ego. Action
Challenge(v, s) :
{ [Villain(v) ∧ Superhero(s) ∧ ∃p.alterEgo(s, p) ∧ livesIn(p, sc)] ∧ ¬[defeated(v, s)]
{livesIn(v, sc), enemy(v, s)},
CopyAll }
states that when villain v challenges superhero s and has not defeated him, next he lives in the same
city as s and is an enemy of s. Action
ThreatenCity(v, c) :
{ [Villain(v) ∧ Superhero(s) ∧ ∃p.alterEgo(s, p) ∧ livesIn(p, c)]
{enemy(v, s) ∧ livesIn(v, c)}
CopyAll }
states that when villain v threatens city c, then he becomes an enemy of all and only superheroes
that live in c.
A process Π might include the following rules:
[Character(p)] ∧ ¬[∃s.Superhero(s) ∧ livesIn(s, c)]
[Superhero(s) ∧ Character(c)]
[enemy(v, s)] ∧ ¬[∃v 0 .defeated(v 0 , s)]
[Villain(v) ∧ Superhero(s)]
[Villain(v) ∧ City(c)] ∧ ¬∃v 0 ([Villain(v 0 ) ∧ livesIn(v 0 , c)] ∧ ¬[v = v 0 ])

7→
7
→
7→
7→
7→

BecomeSH(p, c),
Unmask(s, c),
Fight(v, s),
Challenge(v, s),
ThreatenCity(v, c).

For instance, the first rule states that a character can become a superhero if the city does not already
have one, whereas the last one states that a villain can threaten a city, if the city does not have
another villain that is (known to be) distinct from him/her.
Notice that, during the execution, reasoning on the KB is performed. For instance, consider an
initial ABox
A0 = { Superhero(batman), Villain(joker), alterEgo(batman, bruce),
livesIn(bruce, gotham), livesIn(batman, gotham), livesIn(joker, city1) }.
In this state, bruce and batman live in the same city, and batman is the alter-ego of bruce, but it is not
known whether they denote the same individual. Executing Challenge(joker, batman) in A0 , which
is indeed allowed by the process Π, generates a new ABox with added assertions enemy(joker,
batman), livesIn(joker, gotham), and gotham = city1 is implied by the functionality on livesIn.

661

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

4. KAB Transition System
The semantics of KABs is given in terms of possibly infinite transition systems that represent the
possible evolutions of the KAB over time, as actions are executed according to the process. Notice
that such transition systems must be equipped with semantically rich states, since a full KB is associated to them. Formally we define the kind of transition system we need as follows: A transition
system Υ is a tuple of the form (U, T, Σ, s0 , abox , ⇒), where:
• U is a countably infinite set of terms denoting individuals, called universe;
• T is a TBox;
• Σ is a set of states;
• s0 ∈ Σ is the initial state;
• abox is a function that, given a state s ∈ Σ returns an ABox associated to s which has as
individuals terms of U, and which conforms to T ;
• ⇒ ⊆ Σ × Σ is a transition relation between pairs of states.
For convenience,
S we introduce the active domain of the whole transition system, defined as
ADOM (Υ) = s∈Σ ADOM (abox (s)). Also we introduce the (predicate) alphabet ALPH (Υ) of Υ as
the set of concepts and roles occurring in T or in the co-domain of abox .
The KAB generates a transition system of this form during its execution. Formally, given a
KAB K = (T, A0 , Γ, Π), we define its (generated) transition system ΥK = (U, T, Σ, s0 , abox , ⇒)
as follows:
• U is formed by all constants and all function terms inductively formed starting from
ADOM (A0 ) by applying the functions occurring in the actions in Γ;
• T is the TBox of the KAB;
• abox is the identity function (i.e., each state is simply an ABox);
• s0 = A0 is the initial state;
• Σ and ⇒ are defined by mutual induction as the smallest sets satisfying the following property: if s ∈ Σ, then for each rule Q 7→ γ, evaluate Q and, for each tuple θ returned, if
DO (T, abox (s), γθ) is consistent w.r.t. T , then s ⇒ s0 where s0 = DO (T, abox (s), γθ).
Notice that the alphabet ALPH(ΥK ) of ΥK is simply formed by the set ALPH(K) of concepts and
roles that occur in K.
The KAB transition system ΥK is an infinite tree with infinitely many different ABoxes in
its nodes, in general. In fact, to get a transition system that is infinite, it is enough to perform
indefinitely a simple action that adds new terms at each step, e.g., an action of the form
γ() : { [C(x)]

{C(f (x))}, CopyAll }.

Hence the classical results on model checking (Clarke et al., 1999), which are developed for finite
transition systems, cannot be applied directly for verifying KABs.
662

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

5. Verification Formalism
To specify dynamic properties over KABs, we use a first-order variant of µ-calculus (Stirling, 2001;
Park, 1976). µ-calculus is virtually the most powerful temporal logic used for model checking of
finite-state transition systems, and is able to express both linear time logics such as LTL and PSL,
and branching time logics such as CTL and CTL* (Clarke et al., 1999). The main characteristic of
µ-calculus is its ability of expressing directly least and greatest fixpoints of (predicate-transformer)
operators formed using formulae relating the current state to the next one. By using such fixpoint
constructs one can easily express sophisticated properties defined by induction or co-induction.
This is the reason why virtually all logics used in verification can be considered as fragments of
µ-calculus. Technically, µ-calculus separates local properties, asserted on the current state or on
states that are immediate successors of the current one, from properties talking about states that are
arbitrarily far away from the current one (Stirling, 2001). The latter are expressed through the use
of fixpoints.
In this work, we use a first-order variant of µ-calculus, where we allow local properties to be
expressed as ECQs, and at the same time we allow for arbitrary first-order quantification across
states. Given the nature of ECQs used for formulating local properties, first-order quantification
ranges over terms denoting individuals. Formally, we introduce the logic µLA defined as follows:
Φ −→ Q | ¬Φ | Φ1 ∧ Φ2 | ∃x.Φ | h−iΦ | Z | µZ.Φ,
where Q is a possibly open ECQ and Z is a second order predicate variable (of arity 0). We make use
of the following abbreviations: ∀x.Φ = ¬(∃x.¬Φ), Φ1 ∨ Φ2 = ¬(¬Φ1 ∧ ¬Φ2 ), [−]Φ = ¬h−i¬Φ,
and νZ.Φ = ¬µZ.¬Φ[Z/¬Z]. The formulae µZ.Φ and νZ.Φ respectively denote the least and
greatest fixpoint of the formula Φ (seen as the predicate transformer λZ.Φ). As usual in µ-calculus,
formulae of the form µZ.Φ (and νZ.Φ) must obey to the syntactic monotonicity of Φ w.r.t. Z, which
states that every occurrence of the variable Z in Φ must be within the scope of an even number of
negation symbols. This ensures that the least fixpoint µZ.Φ (as well as the greatest fixpoint νZ.Φ)
always exists.
The semantics of µLA formulae is defined over possibly infinite transition systems of the form
hU, T, Σ, s0 , abox , ⇒i seen above. Since µLA also contains formulae with both individual and
predicate free variables, given a transition system Υ, we introduce an individual variable valuation
v, i.e., a mapping from individual variables x to U, and a predicate variable valuation V , i.e., a
mapping from the predicate variables Z to subsets of Σ. With these three notions in place, we
assign meaning to formulae by associating to Υ, v, and V an extension function (·)Υ
v,V , which maps
Υ
formulae to subsets of Σ. Formally, the extension function (·)v,V is defined inductively as follows:
(Q)Υ
v,V
(¬Φ)Υ
v,V
(Φ1 ∧ Φ2 )Υ
v,V
(∃x.Φ)Υ
v,V
(h−iΦ)Υ
v,V
(Z)Υ
v,V
(µZ.Φ)Υ
v,V

=
=
=
=
=
=
=

{s ∈ Σ | ANS (Qv, T, abox (s)) = true},
Σ \ (Φ)Υ
v,V ,
Υ
(Φ1 )Υ
v,V ∩ (Φ2 )v,V ,
{s ∈ Σ | ∃t.t ∈ ADOM(abox (s)) and s ∈ (Φ)Υ
v[x/t],V },
Υ
0
0
0
{s ∈ Σ | ∃s .s ⇒ s and s ∈ (Φ)v,V },
V (Z),
T
{E ⊆ Σ | (Φ)Υ
v,V [Z/E] ⊆ E}.

Here Qv stands for the (boolean) ECQ obtained from Q by substituting its free variables according
to v. Intuitively, (·)Υ
v,V assigns to such constructs the following meaning:
663

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

• The boolean connectives have the expected meaning.
• The quantification of individuals is done over the terms of the “current” ABox. Notice that
such terms can be referred in later states where the associated ABox does not include such
terms anymore.
• The extension of h−iΦ consists of the states s such that, for some state s0 with transition
s ⇒ s0 , the formula Φ holds in s0 under valuation v .
• The extension of [−]Φ consists of the states s such that, for all states s0 with transition s ⇒ s0 ,
the formula Φ holds in s0 under valuation v.
• The extension of µZ.Φ is the smallest subset Eµ of Σ such that, when assigning to Z the
extension Eµ , the resulting extension of Φ (under valuation v) is contained in Eµ . That is, the
extension of µZ.Φ is the least fixpoint of the operator (Φ)Υ
v,V [Z/E] , where V [Z/E] denotes the
predicate valuation obtained from V by forcing the valuation of Z to be E.
• Similarly, the extension of νZ.Φ is the greatest subset Eν of Σ such that, when assigning to
Z the extension Eν , the resulting extension of Φ contains Eν . That is, the extension
of νZ.Φ
S
Υ
=
{E
⊆
Σ|E ⊆
.
Formally,
(νZ.Φ)
is the greatest fixpoint of the operator (Φ)Υ
v,V
v,V [Z/E]
(Φ)Υ
v,V [Z/E] }.
When Φ is a closed formula, (Φ)Υ
v,V does not depend on v or V , and we denote the extension of Φ
Υ
simply by (Φ) . A closed formula Φ holds in a state s ∈ Σ if s ∈ (Φ)Υ . In this case, we write
Υ, s |= Φ. A closed formula Φ holds in Υ, denoted by Υ |= Φ, if Υ, s0 |= Φ. We call model
checking the problem of verifying whether Υ |= Φ holds.
The next example shows some simple temporal properties that can be expressed in µLA .
Example 2 Considering the KAB of Example 1, we can easily express temporal properties as the
following ones.
• From now on all current superheroes that live in Gotham will live in Gotham forever (a form
of safety):
∀x.[Superhero(x) ∧ livesIn(x, gotham)] ⊃ νZ.([livesIn(x, gotham)] ∧ [−]Z).
• Eventually all current superheroes will be unmasked (a form of liveness):
∀x.[Superhero(x)] ⊃ µZ.([alterEgo(x, x)] ∨ [−]Z).
• There exists a possible future situation where all current superheroes will be unmasked (another form of liveness):
∀x.[Superhero(x)] ⊃ µZ.([alterEgo(x, x)] ∨ h−iZ).
• Along every future, it is always true, for every superhero, that there exists an evolution that
eventually leads to unmask him (a form of liveness that holds in every moment):
νY.(∀x.[Superhero(x)] ⊃ µZ.([alterEgo(x, x)] ∨ h−iZ)) ∧ [−]Y.
664

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

Consider two transition systems sharing the same universe and the same predicate alphabet.
We say that they are behaviorally equivalent if they satisfy exactly the same µLA formulas. To
formally capture such an equivalence, we make use of the notion of bisimulation (Milner, 1971),
suitably extended to deal with query answering over KBs.
Given two transition systems Υ1
=
hU, T, Σ1 , s01 , abox 1 , ⇒1 i and Υ2
=
hU, T, Σ2 , s02 , abox 2 , ⇒2 i sharing the same universe U, the same TBox T , and such that
ALPH (Υ1 ) = ALPH (Υ2 ) = Λ, a bisimulation between Υ1 and Υ2 is a relation B ⊆ Σ1 × Σ2 such
that (s1 , s2 ) ∈ B implies that:
1. abox (s1 ) ≡T,Λ abox (s2 );
2. if s1 ⇒1 s01 , then there exists s02 such that s2 ⇒2 s02 and (s01 , s02 ) ∈ B;
3. if s2 ⇒2 s02 , then there exists s01 such that s1 ⇒1 s01 and (s01 , s02 ) ∈ B.
We say that two states s1 and s2 are bisimilar if there exists a bisimulation B such that (s1 , s2 ) ∈
B. Two transition systems Υ1 with initial state s01 and Υ2 with initial state s02 are bisimilar if
(s01 , s02 ) ∈ B. The following theorem states that the formula evaluation in µLA is indeed invariant
w.r.t. bisimulation, so we can equivalently check any bisimilar transition systems.
Theorem 4 Let Υ1 and Υ2 be two transition systems that share the same universe, the same TBox,
and the same predicate alphabet, and that are bisimilar. Then, for two states s1 of Υ1 and s2 of Υ2
(including the initial ones) that are bisimilar, and for all closed µLA formulas Φ, we have that
s1 ∈ (Φ)Υ1

iff

s2 ∈ (Φ)Υ2 .

Proof. The proof is analogous to the standard proof of bisimulation invariance of µ-calculus (Stirling, 2001), though taking into account our bisimulation, which guarantees that ECQs are evaluated
identically over bisimilar states. Notice that the assumption that the two transition systems share the
same universe and the same predicate alphabet makes it easy to compare the answers to queries.
Making use of such a notion of bisimulation, we can, for example, redefine the transition system
generated by a KAB K = (T, A0 , Γ, Π) while maintaining bisimilarity, by modifying the definition
of ΥK = hU, T, Σ, s0 , abox , ⇒i given in Section 4 as follows.
(i) We modify DO() so that no function term t0 is introduced in the generated ABox A0 if in the
current ABox4 A there is already a term t such that (T, A) |= t = t0 .
(ii) If the ABox A0 = DO(T, abox (s), γθ) obtained from the current state s is logically equivalent
to the ABox abox (s00 ), for some already generate state s00 , we do not generate a new state, but
simply add s ⇒ s00 to ΥK .

6. Verification of KABs
It is immediate to see that verification of KABs is undecidable in general as it is easy to represent
Turing machines using a KAB. Actually we can do so using only a fragment of the capabilities of
KABs, as shown in the next lemma.
Lemma 5 Checking formulas of the form µZ.(N (a) ∨ h−iZ), where N is an atomic concept and
a is an individual occurring in A0 , is undecidable already for a KAB K = (T, A0 , Γ, Π) where:
4. Note that all terms that are present in the current ABox are preserved in the new ABox, together with equalities
between terms.

665

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

{First(c)}
{value(c, x)}
{value(c, av0 )}
{cell(cr , aq0 )}
{cell(n(c), aq0 ), next(c, n(c)), Last(n(c))}
{Last(c)}
{cell(c, #)}
{cell(cr , #)}
{Stop(0)}

[First(c)]
[cell(c, #) ∧ value(c, x)]
[cell(c, aq ) ∧ value(c, av )]
[cell(c, aq ) ∧ value(c, av ) ∧ next(c, cr )]
[cell(c, aq ) ∧ value(c, av ) ∧ Last(c)]
[cell(c, #) ∧ Last(c)]
[cell(c, #) ∧ First(c)]
[cell(c, #) ∧ next(c, cr )]
[cell(c, aqf )]

Figure 2: Effects of the action used to encode a transition δ(q, v, q 0 , v 0 , R) of a Turing Machine
• T is the empty TBox,
• the actions in Γ make no use of negation nor equality,
• Π is the trivial process that poses no restriction on executability of actions.
Proof. Given a Turing machine M = hQ, Σ, q0 , δ, qf , i, we show how to construct a corresponding
KAB KM = (∅, A0 , Γ, Π) that mimics the behavior of M. Specifically, we encode the halting
problem for M as a verification problem over KM . Roughly speaking, KM maintains the tape
and state information in the (current) ABox, and encodes the transitions of M as actions. Our
construction makes use of a tape that initially contains a unique cell, represented by the constant
0, and is extended on-the-fly as needed: cells to the right of 0 are represented by function terms of
the form n(n(· · · (0) · · · )), while cells to the left of 0 are represented by function terms of the form
p(p(· · · (0) · · · )). Then, we make use of one constant aq for each state q ∈ Q, of one constant av
for each tape symbol value v ∈ Σ, of a special constant #, and of the following concepts and roles:
• cell(c, h) models a cell of the tape, where c is a cell identifier, and h corresponds to the current
state of M, if the head of M currently points to c, or to # if the head does not currently point
to c;
• next(cl , cr ) models the relative position of cells, stating that cr is the cell immediately following cl ;
• value(c, v) models that cell c currently contains value v, with v ∈ Σ;
• First(c) and Last(c) respectively denote the current first cell and last cell of the portion of
tape explored so far.
• Stop(c) is used to detect when M halts.
The initial state of KM contains a unique cell and is defined as
A0 = { cell(0, aq0 ), value(0, a ), First(0), Last(0) }.
As for the action component, Γ contains an action with no parameters for each transition in δ, while
the process Π poses no restriction on executability of actions, i.e., it contains a rule true 7→ γ() for
each such action γ.
We now provide the specification of actions, detailing the case of a right shift transition
δ(q, v, q 0 , v 0 , R). The corresponding action specification consists of the set of effects shown in
Figure 2. The first effect maintains the first position of the tape unaltered. The second and third
666

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

effects deal with the cell values. They remain the same except for the current cell, that is updated
according to the transition. The next three effects deal with the right shift and the Turing Machine
state. If the current cell has a next cell and therefore is not the last one, then the head is moved
to the next cell and the state change of M is recorded there. In this case the last cell remains the
same. If instead the current cell is the last one, before moving the head the tape must be properly
extended. The function n/1 is used to create the identifier of this new successor cell, starting from
the identifier of the current one. Furthermore, since the transition corresponds to a right shift of one
cell, the first cell and all the cells immediately following a cell marked # will be marked # in the
next state. Finally, the last effect is used to identify the case in which M has reached a final state.
This is marked by inserting into the new state the special assertion Stop(0).
The construction for a left shift transition is done symmetrically, using the function p/1 to create
a new predecessor cell. By construction, KM satisfies the conditions of the theorem. Observe that,
in the transition system ΥKM generated by KM , every action corresponding to every transition of
M can be executed in each ABox/state s of ΥKM , and since T is empty, it will actually generate a
successor state of s. However, in each state, only the (unique) action that corresponds to the actually
executed transition of M will generate a successor state containing an ABox assertion of the form
cell(c, aq ), for some state q of M. Therefore, only those ABoxes/states properly corresponding to
configurations of M could eventually lead to an ABox/state in ΥKM where Stop(0) holds. And the
latter will happen if and only if M halts. More precisely, one can show by induction on the length
respectively of a halting computation of M and of the shortest path from the initial state of ΥKM
to a state where Stop(0) holds, that M halts if and only if KM |= µZ.([Stop(0)] ∨ h−iZ), which
concludes the proof.
From the previous lemma, which shows undecidability already in a special case, we immediately
obtain the following result.
Theorem 6 Verification of µLA formulae over KABs is undecidable.
We observe that Lemma 5 uses a KB that is constituted only by an ABox containing concept
and role assertions, and makes use only of conjunctive queries in defining actions effects. Moreover, the formula that we check makes no use of quantification at all, and can simply be seen as a
propositional CTL formula of the form EF p, expressing that proposition p eventually holds along
one path.

7. Verification of Weakly Acyclic KABs
In spite of Theorem 6, next we introduce a notable class of KABs for which verification of arbitrary
µLA properties is decidable. To do so, we rely on a syntactic restriction that resembles the notion of
weak acyclicity in data exchange (Fagin et al., 2005)5 , and that guarantees boundedness of ABoxes
generated by the execution of the KAB and, in turn, decidability of verification.
Now we are ready to introduce the notion of weak acyclicity in our context. We introduce the
edge-labeled directed dependency graph of a KAB K = (T, A0 , Γ, Π), defined as follows. Nodes,
called positions, are obtained from the TBox T: there is a node for every concept name N in T , and
two nodes for every role name P in T , corresponding to the domain and to the range of P . Edges
5. We use the original definition of weak acyclicity. However, our results can be applied also to other variants of weak
acyclicity (see discussion in Section 9).

667

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

Villain

livesIn,1

livesIn,2

City

enemy,1

Character

alterEgo,2

defeats,2

*
defeats,1

*

*

alterEgo,1

*
SuperHero

enemy,2

Figure 3: Weakly acyclic dependency graph for Example 1.
are drawn by considering every effect specification [q + ] ∧ Q−
A0 of each action contained in
Γ, tracing how values are copied or contribute to generate new values as the system progresses. In
particular, let p be a position corresponding to a concept/role component in the rewriting rew T (q + )
of q + with variable x. For every position p0 in A0 with the same variable x, we include a normal
edge p → p0 . For every position p00 in A0 with a function term f (~t) such that x ∈ ~t, we include a
∗
special edge p →
− p00 . We say that K is weakly-acyclic if its dependency graph has no cycle going
through a special edge.
Example 3 The KAB of Example 1 is weakly acyclic. Its dependency graph, shown in Figure 3,
does not contain any cycle going through special edges. For readability, self-loops are not shown
in the Figure (but are present for all nodes), and dashed edges are used to compactly represent the
contributions given by the rewriting of the queries. E.g., the dashed edge form Villain to Character
denotes that for every outgoing edge from Character, there exists an outgoing edge from Villain
with the same type and target. Hence, w.r.t. weak acyclicity dashed edges can be simply replaced
by normal edges.
We are now ready to state the main result of this work, which we are going to prove in the remainder
of this section.
Theorem 7 Verification of µLA properties for a weakly acyclic KAB is decidable in E XP T IME in
the size of the KAB.
We observe that the restriction imposed by weak acyclicity (or variants) is not too severe, and in
many real cases KABs are indeed weakly acyclic or can be transformed into weakly acyclic ones
at cost of redesign. Indeed, weakly acyclic KABs cannot indefinitely generate new values from the
old ones, which then depend on a chain of unboundedly many previous values. In other words,
current values depend only on a bounded number of old values. While unbounded systems exist in
theory, e.g., Turing machines, higher level processes, as those in business process management or
service-oriented modeling, typically require such a boundedness in practice. How to systematically
transform systems into weakly acyclic ones remains an open issue.
In the remainder of this section we present the proof of Theorem 7. We do so in several steps:
1. Normalized KAB. First we introduce a normalized form K̂ of the KAB K, which isolates the
contribution of equalities and of the TBox in actions effects of the KAB. An important point
is that normalizing the KAB preserves weak acyclicity.
668

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

2. Normalized DO(). Then, we introduce a normalized version DO NORM () of DO(), which avoids
to consider equalities in generating the bulk set of tuples to be used in the effects to generate
the next ABox. The transition system ΥK̂,NORM generated through this normalized version
DO NORM () of DO () by the normalized KAB K̂ is bisimilar to the transition system ΥK generated through DO() by K. Hence the two transition systems satisfy the same µLA formulae.

3. Positive dominant. The next step is to introduce what we call the positive dominant K++ of
the normalized KAB K̂. This is obtained from K̂ essentially by dropping equalities, negations,
and TBox. However K++ contains enough information in the positive part so that, when we
drop all of these features, the active domain of the transition system ΥK++ generated by
K++ “overestimates” the active domain of the transition system ΥK̂,NORM generated by the
normalized KAB K̂. Moreover, if the normalized (and hence the original) KAB is weakly
acyclic, so is its positive dominant. Finally if the positive dominant is weakly acyclic then the
size of the active domain of its transition system ΥK++ is polynomially bounded by the size
of its initial ABox, and hence so is the size of the active domain of ΥK̂,NORM . This implies
that the size of ΥK̂,NORM is finite and at most exponential in the size of its initial ABox.
4. Putting it all together. Tying these results together, we get the claim.
In the following, we detail each of these steps.
7.1 Normalized KAB
Given a KAB K = (T, A0 , Γ, Π), we build a KAB K̂ = (T, Â0 , Γ̂, Π), called the normalized form
of K, by applying a sequence of transformations that preserve the semantics of K while producing
a KAB of a format that is easier to study.
1. We view each ABox A as partitioned into a part collecting all concept and role assertions, and
a part collecting all equality assertions. We denote with A6E Q the former and with EQ(T, A)
the latter, after having closed it w.r.t. (the functionality assertions in) the TBox T . Notice that
such a closure can be computed in polynomial time in the size of A and T .
2. In K̂ all individuals appearing in equality assertions in an ABox also occur in special concept assertions of the form Dummy(t), where the concept Dummy is unrelated to the other
concepts and roles in the KAB. We do so by:
• adding concept assertions Dummy(t) for each t in an equality assertion in A0 that does
not appear elsewhere;
• adding to the right-hand part of each action effect ei a concept assertion Dummy(t) for
each t in an equality assertion in the right-hand part of ei ;
• adding to each action an effect specification of the form
[Dummy(x)]

{Dummy(x)}.

Notice that, as the result of this transformation, we get ABoxes containing the additional
concept Dummy, which however is never queried by actions effects and by the rules forming
the process. The impact of the transformation is simply that now the ADOM(A) of the ABoxes
669

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

A in the KAB transition system can be readily identified as the set of terms occurring in
concept and role assertion only (without looking at equality assertions).
Given an ABox A, we denote by Â the result of the two above transformations, which respectively add to A the closure of equalities and the extension of Dummy.
3. We manipulate each resulting effect specification
[q + ] ∧ Q−

Â0

as follows:
3.1. We replace [q + ] ∧ Q− by [rew T (q + )] ∧ rew T (Q− ) (Calvanese et al., 2007a), exploiting
the results by Calvanese et al. (2007b) and by Artale et al. (2009), which guarantee that,
for every ECQ Q and every ABox A where equalities are closed under functionality and
congruence, we have that
ANS (Q, T, A)

= ANS (rew T (Q), ∅, A).

3.2. We replace each effect specification [rew T (q + )] ∧ rew T (Q− )
Â0 , resulting from
+
−
Step 3.1, by a set of effect specifications [qi ] ∧ rew T (Q )
Â0 , one for each CQ qi in
+
the UCQ rew T (q ).
3.3. For each effect specification [qi+ ] ∧ rew T (Q− )
Â0 , we re-express qi+ so as to make
equalities used to join terms explicit and so as to remove constants from qi+ . Specifically,
we replace the effect specification by
[qi++ ] ∧ q = ∧ rew T (Q− )

Â0 ,

where:
• qi++ is the CQ without repeated variables obtained from qi+ by (i) replacing for each
variable x occurring in qi+ , the j-th occurrence of x except for the first one, by x[j] ;
and (ii) replacing each constant c with a new variable xc ;
V
V
• q = = [x = x[j] ] ∧ [xc = c] where (i) the first conjunction contains one equality
[x = x[j] ] for each variable x in qi+ and for each variable x[j] introduced in the step
above, and (ii) the second conjunction contains one equality for each constant c in
qi+ .
To clarify the latter consider the following example:
Example 4 Given a query
.
[qi+ ] = [N (x) ∧ P1 (x, y) ∧ P2 (c, x)],
Step 3.3 above replaces it by [qi++ ] ∧ q = , where
.
qi++ = N (x) ∧ P1 (x[2] , y) ∧ P2 (xc , x[3] ),
670

.
q = = [x = x[2] ] ∧ [x = x[3] ] ∧ [xc = c].

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

As for the correctness
of Step 3.3, it is immediate to notice that [qi+ ] is equivalent to [qi++ ∧
V
V
[j]
(x = x )∧ (xc = c)]. The equivalence between the latter and [qi++ ]∧q = is a consequence
of the construction by Artale et al. (2009), which shows that query entailment in the presence
of equalities can be reduced to query evaluation by saturating equalities w.r.t. transitivity,
reflexivity, symmetry, and functionality.
Given an action γ, we denote by γ̂ the action normalized as above.
Since all transformations preserve logical equivalence (as long as we do not query Dummy),
we have
Lemma 8 DO(T, A, γθ) ≡T,ALPH(K) DO(T, Â, γ̂θ).
Also the normalization of a KAB preserves weak acyclicity, which is a crucial consideration for
later results.
Lemma 9 If K is weakly acyclic, then also K̂ is weakly acyclic.
Proof. Consider each effect specification [q + ] ∧ Q−
A0 belonging to an action in K. The
contribution of this effect specification to the dependency graph G of K is limited to each CQ qi in
the UCQ rew T (q + ), and to the set of concept and role assertions of A0 . We observe that each such
qi corresponds to a query qi++ in K̂ in which each variable of qi occurs exactly once. For every free
variable x of qi that also appears in A0 , and for every occurrence of x in qi itself, an edge is included
in G. In the dependency graph Ĝ of K̂, only one of such edges appears, corresponding to the single
occurrence of the variable x in qi++ .
Notice that Dummy can be omitted from the dependency graph of Ĝ since, by definition of K̂,
Dummy does not occur in the left-hand side of effects except for the trivial effect [Dummy(x)]
{Dummy(x)}. This is not true for K, where Dummy is needed. Therefore, Ĝ is indeed a subgraph
of G, and hence weak acyclicity of G implies weak acyclicity of Ĝ.
7.2 Normalized DO()
Next we give a simplified version of DO(), which we call DO NORM (). We start by observing that
we can reformulate the definition of DO() given in Section 3. For that, we first need to define a
suitable notion of join of two queries. Let q1 and q2 be two ECQs, which may have free variables in
common, and let A1 and A2 be two ABoxes. Then we define ANS (q1 , ∅, A1 ) ./ ANS (q2 , ∅, A2 ) as
the set of substitutions σ over the free variables in q1 and q2 such that qi holds in ∅, Ai under σ, i.e.,
∅, Ai , σ |= qi , for i ∈ {1, 2}. Then, given an action γ̂ with parameters substitution θ and an ABox
Â, we have
[
DO (T, Â, γ̂θ) =
APPLY (T, Â, e, θ),
e in γ̂

where for an effect specification e : [q ++ ] ∧ q = ∧ Q−
APPLY (T, Â, e, θ)

[

=

Â0 , we have
Â0 θσ

σ∈ANS (q ++ θ,∅,Â)./ANS ((q = ∧Q− )θ,∅,Â)

671

∪ EQ(T, Â).

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

Instead, we define DO NORM () as
DO NORM (T, Â, γ̂θ)

=

[

APPLY NORM (T, Â, e, θ),

e in γ̂

where, for an effect specification e : [q ++ ] ∧ q = ∧ Q−
APPLY NORM (T, Â, e, θ)

Â0 , we have
[

=

Â0 θσ

∪ EQ(T, Â).

σ∈ANS (q ++ θ,∅,Â6E Q )./ANS ((q = ∧Q− )θ,∅,Â)

Notice that the only difference between DO() and DO NORM () is that in the latter we use only Â6E Q
instead of Â to compute the answers to the CQs q ++ θ.
The following lemma shows that the applications of DO() and of DO NORM () give rise to logically
equivalent ABoxes.
Lemma 10 DO(T, Â, γ̂θ) ≡T,ALPH(K) DO NORM (T, Â, γ̂θ).
Proof. In order to prove the claim, it is enough to show that for each concept/role assertion α2 ∈ DO NORM (T, Â, γ̂θ) whose concept/role name belongs to ALPH(K), we have that
(T, DO(T, Â, γ̂θ)) |= α2 , and for each concept/role assertion α1 ∈ DO(T, Â, γ̂θ) whose concept/role name belongs to ALPH(K), we have that (T, DO NORM (T, Â, γ̂θ)) |= α1 . We actually prove
a slightly stronger result:
(1) For each ABox assertion α2 ∈ APPLY NORM (T, Â, e, θ), we have that (T, APPLY(T, Â, e, θ)) |=
α2 .
(2) For each ABox assertion α1 ∈ APPLY(T, Â, e, θ), we have that (T, APPLY NORM (T, Â, e, θ)) |=
α1 .
For (1), by monotonicity of q ++ and the fact that Â6E Q ⊆ Â, we have that
[
[
Â0 θσ
is contained in
Â0 θσ,
σ∈(ANS (q ++ θ,∅,Â6E Q )./ANS ((q = ∧Q− )θ,∅,Â))

σ∈(ANS (q ++ θ,∅,Â)./ANS ((q = ∧Q− )θ,∅,Â))

hence the claim follows.
For (2), consider an ABox assertion α ∈ APPLY(T, Â, e, θ). By definition of APPLY(), we know
that there exists an effect e : [q ++ ] ∧ q = ∧ Q−
Â0 and an assignment σ to the free variables
++
=
of q
(which include also the free variables of q ∧ Q− ) such that σ ∈ (ANS (q ++ θ, ∅, Â) ./
=
ANS ((q ∧ Q− )θ, ∅, Â)) and α ∈ Â0 θσ. Let {x1 , . . . , xn } be all free variables in q ++ θ, and
σ = {x1 → t1 , . . . , xn → tn , }. For each variable xi , let N (xi ) be the (unique) concept atom
in q ++ θ in which xi occurs (similar considerations hold when xi occurs in a role atom). Then,
either N (ti ) ∈ Â6E Q , or for some t0i , N (t0i ) ∈ Â6E Q and (ti = t0i ) ∈ EQ(T, Â). In the former
case, let t00i denote ti , while in the latter case let t00i denote t0i . Then, consider the substitution
σ 0 = {x1 → t001 , . . . , xn → t00n , }. By construction, we have that σ 0 ∈ ANS (q ++ θ, ∅, Â6E Q ), and since
σ ∈ ANS ((q = ∧ Q− )θ, ∅, Â), and (t00i = ti ) ∈ EQ(T, Â) for each i ∈ {1, . . . , n}, we also have that
σ 0 ∈ ANS (q ++ θ, ∅, Â6E Q ) ./ ANS ((q = ∧ Q− )θ, ∅, Â). Since
• α ∈ Â0 θσ,
672

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

• σ and σ 0 are identical modulo EQ(T, Â) and
• EQ(T, Â) ⊆ APPLY NORM (T, Â, e, θ),
we can infer that (T, APPLY NORM (T, Â, e, θ)) |= α. Hence the claim holds.
By combining Lemma 8 and Lemma 10, we get that DO() on K and DO NORM () on K̂ behave
equivalently, when starting from equivalent ABoxes.
Lemma 11 If A1 ≡T,ALPH(K) A2 then DO(T, A1 , γθ) ≡T,ALPH(K) DO NORM (T, A2 , γ̂θ).
Proof. The claim is a direct consequence of Lemma 8, Lemma 10, the equivalence between A1 and
A2 , and the observation that logical equivalence is transitive.
Given a KAB K and its normalized version K̂, we call the transition system generated in the
same way as ΥK , but using DO NORM () on K̂ instead of DO() on K, the normalized transition system
generated by K̂, and denote it with ΥK̂,NORM .
Lemma 12 Given a KAB K, the transition systems ΥK and ΥK̂,NORM are bisimilar.
Proof. Let ΥK = (U, T, Σ, s0 , abox , ⇒) and ΥK̂,NORM = (U, T, ΣNORM , s0 , abox NORM , ⇒NORM ).
We define the relation B ⊆ Σ × ΣNORM as follows: (s1 , s2 ) ∈ B iff abox (s1 ) ≡T,ALPH(K)
abox NORM (s2 ) and show that B is a bisimulation. To do so, we prove that B is closed under the
definition of bisimulation itself. Indeed, if (s1 , s2 ) ∈ B, then:
• abox (s1 ) ≡T,ALPH(K) abox (s2 ) by definition.
• If s1 ⇒ s01 then there exists an action γ and a substitution θ such that s01 =
DO (T, abox (s1 ), γθ) (notice that abox (s1 ) = s1 ) and s01 is consistent w.r.t. T . Now let
us consider s02 = DO NORM (T, abox (s2 ), γ̂θ). Since abox (s1 ) ≡T,ALPH(K) abox (s2 ), then
by Lemma 11, we have s01 ≡T,ALPH(K) s02 . Therefore, s02 is consistent w.r.t. T , and hence
s2 ⇒NORM s02 , and (s01 , s02 ) ∈ B.
• Similarly, if s2 ⇒NORM s02 then there exists an action γ̂ and a substitution θ such that
s02 = DO NORM (T, abox (s2 ), γ̂θ) and s02 is consistent w.r.t. T . Now let us consider s01 =
DO (T, abox (s1 ), γθ). Since s2 ≡T,ALPH(K) s1 , then by by Lemma 11, we have s02 ≡T,ALPH(K)
s01 Therefore, s01 is consistent w.r.t. T , and hence s1 ⇒ s01 , and, considering that equivalence
enjoys symmetry, we have (s01 , s02 ) ∈ B.
This proves the claim.
The direct consequence of the above lemma is that, by considering the Bismulation Invariance
Theorem 4, we can faithfully check µLA formulas over ΥK̂,NORM instead of ΥK .
7.3 Positive Dominant
Our next step is to show that for a weakly acyclic KAB K, the normalized transition system ΥK̂,NORM
is finite. We do so by considering another transition system, which is behaviorally unrelated to
ΥK̂,NORM , and hence to ΥK , but whose active domain bounds the active domain of ΥK̂,NORM . We
obtain such a transition system essentially by ignoring all negative information and equalities. This
allows us to refer back to the literature on data exchange to show boundedness. We call such a
transition system positive dominant.
Given a normalized KAB K̂ = (T, Â0 , Γ̂, Π), we define the positive dominant of K as the KAB
6E Q

K+ = (∅, Â0 , {γ + }, {true 7→ γ + }).
673

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

The only action γ + is without parameters and its effect specification is constituted by CopyAll and
by one effect of the form
6E Q
[qi++ ]
A0i
for each effect [qi++ ] ∧ qi= ∧ Q−
A0i in every action of Γ̂. Observe that the parameters of the
i
actions in Γ̂ become simply free variables in γ + .
Notice that γ + is applicable at every step because the process trivially always allows it. The
resulting state is always consistent, since K+ has an empty TBox. Moreover, no equality assertion
is ever generated. The transition system ΥK+ is constituted by a single run, which incrementally
accumulates all the facts that can be derived by the iterated application of γ + over such increasing
ABox. This behavior closely resembles the chase of tuple-generating dependencies (TGDs) in data
exchange, where an application of γ + corresponds to a “parallel” chase step (Deutsch, Nash, &
Remmel, 2008).
From a technical point of view, notice that K+ is already in normalized form (i.e., K+ = Kˆ+ ),
and that DO() and DO NORM () are identical since neither equality nor negation are considered. Hence
ΥK+ = ΥKˆ+ ,NORM .
The next lemma shows that K+ preserves weak acyclicity of K̂.
Lemma 13 If K̂ is weakly acyclic then also its positive dominant K+ is weakly acyclic.
Proof. The claim follows from the fact that, by construction, the dependency graph G + of K+ is
equal to Ĝ. Indeed, both qi++ and its connection with Âi are preserved by K+ . Hence, we get the
claim.
Next we show that if K+ is weakly acyclic the active domain of the ABoxes in its transition
system ΥK+ are polynomially bounded by the active domain of the initial ABox.
Lemma 14 If K+ is weakly acyclic, then there exists a polynomial P(·) such that
6E Q

|ADOM(ΥK+ )| < P(|ADOM(Â0 )|).
Proof. We observe that there exists a strict connection between the execution of K+ and the chase
of a set of TGDs in data exchange. Therefore, the proof closely resembles the one by Fagin et al.
(2005, Thm. 3.9), where it is shown that for weakly acyclic TGDs, every chase sequence is bounded.
Let ΥK+ = (U, ∅, Σ, A0 6E Q , abox , ⇒), let G + = (V, E) be the dependency graph of K+ , and
let n = |ADOM(A0 6E Q )|. For every node p ∈ V , we consider an incoming path to be any (finite
or infinite) path ending in p. Let rank (p) be the maximum number of special edges on any such
incoming path. Since K+ is weakly acyclic by hypothesis, G + does not contain cycles going through
special edges, and therefore rank (p) is finite. Let r be the maximum among rank (pi ) over all
nodes. We observe that r ≤ |V |; indeed no path can lead to the same node twice using special
edges, otherwise G + would contain a cycle going through special edges, thus breaking the weak
acyclicity hypothesis. Next we observe that we can partition the nodes in V according to their rank,
obtaining a set of sets {V0 , V1 , . . . , Vr }, where Vi is the set of all nodes with rank i.
Let us now consider a state A obtained from A0 6E Q by applying the only action γ + contained
in K+ an arbitrary number of times. We now prove, by induction on i, the following claim: for
every i there exists a polynomial Pi such that the total number of distinct values c that occur in A at
positions in Vi is at most Pi (n).
674

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

(Base case) Consider p ∈ V0 . By definition, p has no incoming path containing special edges.
Therefore, no new values are stored in p along the run A0 6E Q ⇒ · · · ⇒ A. Indeed p can just store
values that are part of the initial ABox A0 6E Q . This holds for all nodes in V0 and hence we can fix
P0 (n) = n.
(Inductive step) Consider p ∈ Vi , with i ∈ {1, . . . , r}. The first kind of values that may be stored
inside p are those values that were stored inside p itself in A0 6E Q . The number of such values is at
most n. In addition, a value may be stored in p for two reasons: either it is copied from some other
position p0 ∈ Vj with i 6= j, or it is generated as a possibly new function term, built when applying
effects that contain a function in their head.
We first determine the number of fresh individuals that can be generated from function terms.
The possibility of generating and storing a new value in p as a result of an action is reflected by
the presence of special edges. By definition, any special edge entering p must start from a node
0
p0 ∈ V0 ∪ · · · ∪ Vi−1 . By
P induction hypothesis, the number of distinct values that can exist in p
is bounded by H(n) = j∈{0,...,i−1} Pj (n). Let ba be the maximum number of special edges that
enter a position, over all positions in the TBox; ba bounds the arity taken by each function term
contained in γ. Then for every choice of ba values in V0 ∪ · · · ∪ Vi−1 (one for each special edge that
can enter a position), the number of new values generated at position p is bounded by tf · H(n)ba ,
where tf is the total number of facts contained in all effects of γ + . Note that this number does not
depend on the data in A0 6E Q . By considering all positions in Vi , the total number of values that can
be generated is then bounded by F(n) = |Vi | · tf · H(n)ba . Clearly, F(·) is a polynomial, because
tf and ba are determined by γ + .
We count next the number of distinct values that can be copied to positions of Vi from positions
of Vj , with j 6= i. A copy is represented in the graph as a normal edge going from a node in Vj
to a node in Vi , with j 6= i. We observe first that such normal edges can start only from nodes in
V0 ∪· · ·∪Vi−1 , that is, they cannot start from nodes in Vj with j > i. We prove this by contradiction.
Assume that there exists p0 → p ∈ E, such that p ∈ Vi and p0 ∈ Vj with j > i. In this case, the
rank of p would be j > i, which contradicts the fact that p ∈ Vi . As a consequence, the number
of distinct values that can be copied to positions in Vi is bounded by the total number of values in
V0 ∪ · · · ∪ Vi−1 , which corresponds to H(n) from our previous consideration.
Putting it all together, we define Pi (n) = n + F(n) + H(n). Since Pi (·) is a polynomial, the
claim is proven.
Notice that, in the above claim, i is bounded by r, which is a constant. Hence, there exists a
fixed polynomial P(·) such that the number of distinct values that can exist in every state s ∈ Σ
is bounded by P(n). K+ is inflationary, because when γ + is applied it copies all concept and role
assertions from the current to the next state. Since ΥK+ contains only a single run, P(n) is a bound
for ADOM(ΥK+ ) as well.
The following lemma shows the key feature of the positive dominant.
Lemma 15 ADOM(ΥK̂ ) ⊆ ADOM(ΥK+ ).
6E Q

Proof. Let K̂ = (T, Â0 , Γ̂, Π) and K+ = (∅, Â0 , {γ + }, {true 7→ γ + }).
We first observe that, for every ABox A in ΥK̂ , ADOM(A) = ADOM(A6E Q ) by definition of K̂
(this is the role of the special concept Dummy).
675

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

We show by induction on the construction of ΥK̂ (U, T, Σ1 , Â0 , abox , ⇒1 ) and ΥK+ =
6E Q

(U, ∅, Σ2 , Â0 , abox , ⇒2 ), that for each state A1 ∈ Σ1 we have that there exists a state A2 ∈ Σ2
such that A1 6E Q ⊆ A2 .
6E Q
The base case holds for the initial states Â0 and Â0 of the two transition systems by definition.
For the inductive case, we have to show that, given A1 ∈ Σ1 and A2 ∈ Σ2 with A1 6E Q ⊆ A2 , for
each A01 ∈ Σ1 with A1 ⇒1 A01 , the unique state A02 ∈ Σ2 with A2 ⇒2 A02 is such that A01 ⊆ A02 . To
show this, note that A1 ⇒1 A01 if there exists an action γ of K̂ and a substitution θ for the parameters
of γ such that A01 = DO NORM (T, A1 , γθ). Similarly, taking into account that γ + has no parameters
and is always executable in ΥK+ , we have that A02 = DO(T, A2 , γ + ) = DO NORM (T, A2 , γ + ). By
construction of K+ , for each effect e1 ∈ γ of the form
e1 : [q ++ ] ∧ q = ∧ Q−

A0e1 ,

there is an effect e2 ∈ γ + of the form
e2 : [q ++ ]

6E Q

A0e1 ,

where A0e1 6E Q is obtained from A0e1 by removing all equality assertions. By induction hypothesis,
we have that A1 6E Q ⊆ A2 . By observing that ANS ([q ++ ]θ, ∅, A1 6E Q ) ./ ANS ((q = ∧ Q− )θ, ∅, A1 ) ⊆
6E Q
ANS ([q ++ ], ∅, A2 ), we then obtain that A0e1
⊆ A0e2 , where A0e1 = APPLY NORM (T, A1 , e1 , θ) and
A0e2 = APPLY(∅, A2 , e2 , ∅). Hence, we get the claim that A01 6E Q ⊆ A02 .
Now since for an ABox A of ΥK̂ the active domain ADOM(A) of A and ADOM(A6E Q ) are identical by construction, and since ADOM(ΥK̂ ) and ADOM(ΥK+ ) are simply the union of the active
domains of all generated ABoxes, we get the claim.
7.4 Putting it All Together
If a KAB K is weakly acyclic, then, by Lemma 9, its normalized form K̂ is weakly acyclic as well
and, by Lemma 13, so is its positive dominant K+ . Hence, by Lemma 14, the size of the active
domain ADOM(ΥK+ ) of the transition system ΥK+ of K+ is polynomially related to the size of its
initial ABox.
Now, by Lemma15, this implies that also the size of the active domain ADOM(ΥK̂,NORM ) of the
transition system ΥK̂ of K̂ is polynomially related to the size of its initial ABox. Hence, the number
of possible states of ΥK̂ is finite, and in fact at most exponential in the size of the initial ABox. It
follows that checking µLA formulae over ΥK̂ can be done in E XP T IME w.r.t. the size of K.
Finally, by Lemma 12, ΥK̂ and ΥK are bisimilar, and by the Bisimulation Invariance Theorem 4,
ΥK̂ and ΥK satisfy exactly the same µLA formulae. Hence, to check a µLA formula on ΥK it
is sufficient to check it over ΥK̂ , which can be done in E XP T IME. This concludes the proof of
Theorem 7.

8. Related Work
We provide now a detailed review of work that is related to the framework and the results presented
in the previous sections.
676

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

8.1 Combining Description Logics and Temporal Logics
Our work is deeply related to the research that studies combinations of description logics and temporal logics. Indeed, actions progress knowledge over time and, although temporal logics do not
mention actions, we can easily used them for describing progression mechanisms, including transition systems (see, e.g., Clarke et al., 1999; Calvanese, De Giacomo, & Vardi, 2002).
Such research has mostly explored the combination of standard description logics with standard
temporal logics at the level of models, which is certainly the most natural form of combination from
a logical point of view. Technically, this form of combination gives rise to a combined logic with
a two-dimensional semantics, where one dimension is for time and the other for the DL domain
(Schild, 1993; Wolter & Zakharyaschev, 1999b, 1999a; Gabbay et al., 2003). Unfortunately, from
a computational point of view, this form of combination suffers from a key undecidability result,
which makes it too fragile for many practical purposes: the possibility of specifying that roles
preserve their extension over time (the so called rigid roles) causes undecidability6 . Referring
to the domain of interest in Example 1, this would result, for example, in the undecidability of
theories that specify that each instance of Character livesIn the same City forever. Moreover, this
undecidability result already holds for concept satisfiability w.r.t. a fixed TBox (i.e., where the same
TBox axioms must hold at all time points), without ABoxes, and with only a single rigid role (Wolter
& Zakharyaschev, 1999b, 1999a; Gabbay et al., 2003). That is, it holds for a reasoning service that
is much simpler than conjunctive query answering (Calvanese, De Giacomo, & Lenzerini, 2008),
even with a fixed TBox and no data (no ABox assertions, hence no individual terms) and for one of
the simplest kinds of temporal formulae, namely “forever something is true” (safety) (Clarke et al.,
1999).
Decidability can be regained by: (i) dropping TBoxes altogether, but the decision problem is
still hard for non-elementary time (Gabbay et al., 2003); (ii) allowing temporal operators only on
concepts (Schild, 1993; Artale & Franconi, 1998, 2005; Gutiérrez-Basulto, Jung, & Lutz, 2012;
Jamroga, 2012), and in this case the complexity depends crucially on the description logic; (iii) allowing temporal operators only on TBox and ABox assertions (Lutz, Wolter, & Zakharyaschev,
2008; Baader et al., 2012). In fact cases (ii) and (iii) can be mixed (Baader & Laux, 1995; Wolter
& Zakharyaschev, 1998).
Allowing for temporal operators over assertions only (case (iii) above), is tightly related to the
functional approach adopted in this paper: the fact that we admit temporal operators only in front of
assertions allows us to consider temporal models whose time points are actually sets of models of
description logic assertions. Hence it keeps the temporal component distinct from the description
logic one, exactly as we do here. In particular, the results by Baader et al. (2012) can be directly
compared with ours. Apart from the obvious differences in the formalism used, one key point to get
decidability there is that the individual terms mentioned in the ABox assertions are fixed a priori. It
is possible that, by adapting the techniques presented here, those results could be extended to allow
functions for denoting terms, hence allowing for adding fresh individual terms during the temporal
evolution.

6. To lose decidability, it suffices to be able to specify/verify the persistence of binary predicates/roles, which allows
one to build an infinite grid and hence to encode any Turing-machine computation (Robinson, 1971; van Emde Boas,
1997).

677

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

8.2 Combining Description Logics and Actions
Somehow hampered by the undecidability results mentioned at the beginning of the section, also
combinations of description logics and action theories have been studied in the years. In particular,
Liu, Lutz, Milicic, and Wolter (2006b, 2006a) study combinations of description logics and action
theories at the level of models, but only w.r.t. the two classical problems in reasoning about actions,
namely projection and executability. Both of these problems require to explicitly give a sequence of
actions and then check a property of the resulting final state (projection), or check the executability
of the sequence of actions, each of which comes with a certain precondition (Reiter, 2001). More
sophisticated temporal properties (in particular, “forever something is true” mentioned above) would
lead to undecidability. By the way, notice that such undecidability result also deeply questions from
the computational point of view the possibility of adding (sound and complete) automated reasoning
capabilities to proposals such as OWL-S (Semantic Markup for Web Services) (Martin, Paolucci,
McIlraith, Burstein, McDermott, McGuinness, Parsia, Payne, Sabou, Solanki, Srinivasan, & Sycara,
2004).
Possibly the first proposal based implicitly on the functional view of the KB was the pioneering
work by De Giacomo, Iocchi, Nardi, and Rosati (1999), which adopts an epistemic description logic
(based on certain answers) combined with an action formalism to describe routines of a mobile
robot. Again, one important point there is that individual terms are bounded and fixed a priori.
The functional view approach was first spelled out by Calvanese, De Giacomo, Lenzerini, and
Rosati (2007), and by Calvanese et al. (2011). In that work, only projection and executability are
studied, however there is a distinction between the KB in the states and the actions (there specified
as updates), so that the framework gives rise to a single transition system whose states are labeled
by KBs (in fact the TBox is fixed while the ABox changes from state to state). However, again,
the individual terms considered are fixed a priori and hence the resulting transition system is finite.
So, although not studied in that work, sophisticated forms of temporal properties as those proposed
here are readily verifiable in that setting. Interestingly, apart from the KBs and action, in that work
also Golog-like programs are considered. These are programs whose atomic actions are defined
by the action formalism, and are combined using (usual and less usual) programming constructs,
such as sequence, while-loop, if-then-else, and nondeterministic pick of a value (Levesque, Reiter,
Lesperance, Lin, & Scherl, 1997; De Giacomo, Lespérance, & Levesque, 2000). An important
characteristic of these programs is that they have a finite number of control states (notice that the
memory storage of these programs is kept in the action theory, or the KB in our case). Although out
of the scope of this paper, this finiteness allows for easily extending our results to such program as
well.
An interesting alternative way to combine description logics and reasoning about actions is the
one reported by Gu and Soutchanski (2010). There, a description logics KB7 is used as a special
FOL theory describing the initial situation in a situation calculus basic action theory (Reiter, 2001).
Notice that as a result, TBox assertions do not act as state constraints (Lin & Reiter, 1994), which
would lead to undecidability as discussed above (Wolter & Zakharyaschev, 1999b, 1999a; Gabbay
et al., 2003), in fact they essentially do not persist in any way through actions.

7. They actually mainly focus on concepts only but in a description logic that includes the universal role, which allows
one to express TBox assertions as concepts (Baader et al., 2003).

678

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

8.3 Description Logics Update
Observe that effects of an action in our setting can be seen as a basic form of update of the previous
state (Katsuno & Mendelzon, 1991). Although our mechanism sidesteps the semantic and computational difficulties of description logic KB update (Liu et al., 2006b; De Giacomo, Lenzerini, Poggi,
& Rosati, 2009; Calvanese, Kharlamov, Nutt, & Zheleznyakov, 2010; Lenzerini & Savo, 2012) by
simply rejecting the execution of actions that would lead to an inconsistent state. Adopting proper
forms of update in our setting is an interesting issue for future research.
8.4 Artifacts and Data-Aware Processes
Our work is also closely related to research in verification of artifact-centric business processes
(Nigam & Caswell, 2003; Bhattacharya et al., 2007). Artifact-centric approaches model business
processes by giving equal importance to the control-flow perspective and the data of interest. An
artifact is typically represented as a tuple of a schema, which models the artifact type, together
with a set of actions/services that specify how the information maintained in the artifact can be
manipulated over time. Each action is usually represented in terms of pre- and post-conditions
that are respectively used to determine when the action is eligible for execution, and to relate the
current artifact state with the successor state obtained after the action execution. Pre- and postconditions are modeled as first-order formulae, and post-conditions employ existentially quantified
variables to account for external inputs from the environment. Differently from KABs, most of
the approaches targeting artifact-centric processes assume complete information about data, using
a relational database to maintain the artifacts’ information. As in this paper, the aim of such works
is to verify whether a relational artifact-centric process meets some temporal/dynamic property,
formalized using first-order variants of branching or linear temporal logics.
In the work by Deutsch et al. (2009), the infinite domain of the artifact’s database is equipped
with a dense linear order, which can be mentioned in pre-conditions, post-conditions, and properties.
Runs can receive unbounded external input from an infinite domain. Decidability of verification is
achieved by avoiding branching time properties, and by restricting the formulae used to specify
pre-, post-conditions and properties. In particular, the approach refers to read-only and read-write
database relations differently, querying the latter only by checking whether they contain a given tuple of constants. The authors show that this restriction is tight, and that integrity constraints cannot
be added to the framework, since even a single functional dependency leads to undecidability of
verification. Damaggio et al. (2011) extend this approach by disallowing read-write relations, but
this allows the extension of the decidability result to integrity constraints expressed as embedded
dependencies with terminating chase, and to any decidable arithmetic. This is a major difference
with our approach, where all concepts of the KAB are considered as read-write relations, and can
be arbitrarily queried to determine the progression of the system. Differently from these works,
Belardinelli et al. (2011) consider a first-order variant of CTL with no quantification across states
as verification formalism. The framework supports the incorporation of new values from the external environment as parameters of the actions; the corresponding execution semantics considers
all the possible actual values, thus leading to an infinite-state transition systems. As for decidability of verification, the authors show that, under the assumption that each state of the system
(constituted by the union of artifacts’ relational instances) has a bounded active domain, it is possible to construct a faithful abstract transition system which, differently from the original one, has
a finite number of states. Belardinelli, Lomuscio, and Patrizi (2012) improve the results by Belar679

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

dinelli et al. (2011) by introducing a semantic property of “uniformity” which, roughly speaking,
says that the transition system representing the execution of the process under study is not able to
distinguish among states that have the same constants and the same patterns of data. Under the assumptions of uniformity and state boundedness, decidability of verification is achieved for a richer
logic, namely CTL with quantification across states, interpreted under the active domain semantics.
The notion of state boundedness has also been adopted by the independently developed framework
of Bagheri Hariri, Calvanese, De Giacomo, Deutsch, and Montali (2012, 2013), where first-order
variants of µ-calculus, similar to the one considered here, are considered. There, beside differences
in the way data and external information are modeled, sufficient syntactic conditions that guarantee state boundedness are proposed. All these works are developed within the relational database
setting, and do not extend trivially to systems where actions change DL knowledge bases.
The connection between data-/artifact-centric business processes and data exchange that we exploit in this paper was first established by Cangialosi et al. (2010), and by De Giacomo, De Masellis,
and Rosati (2012). There the transition relation itself is described in terms of TGDs, which map the
current state, represented as a relational database instance, to the next one. Null values are used to
model the incorporation of new, unknown data into the system. The process evolution is essentially
a form of chase. Under suitable weak acyclicity conditions this chase terminates, guaranteeing, in
turn, that the system is finite-state. Decidability is then shown for a first-order µ-calculus without
first-order quantification across states. This approach was extended by Bagheri Hariri et al. (2011),
where TGDs were replaced by actions and a rule-based process that follow the same structure of
the KAB action component. In this revised framework, values imported from the external environment are represented by uninterpreted function terms, which play the same role as nulls in the
work by Cangialosi et al. (2010), and by De Giacomo et al. (2012). Since Bagheri Hariri et al.
(2011), Cangialosi et al. (2010), and De Giacomo et al. (2012) all rely on a purely relational setting, this choice leads to an ad-hoc interpretation of equality, where each null value/function term
is considered only equal to itself. Differently from these works, here we allow for sophisticated
schema constraints, i.e., the TBox itself, and provide at the same time a more fine-grained treatment
of equality, where individuals can be inferred to be equal due to the application of such schema
constraints and/or the execution of some action. This treatment of equality differentiates this work
also from the one of Bagheri Hariri, Calvanese, De Giacomo, and De Masellis (2011), which introduces a preliminary version of the framework here presented, where UNA is assumed and equality
is not considered. More specifically, Bagheri Hariri et al. (2011) propose semantic artifacts as a
means to represent artifacts and corresponding processes at a higher level of abstraction than relational artifacts, representing the artifact data with a semantically rich knowledge base operating
with incomplete information. KABs constitute a more general framework, which can be seamlessly
customized to account for semantic artifacts. A major difference with the work by Bagheri Hariri
et al. (2011) is also constituted by the verification formalism. In particular, both works focus on a
form of µ-calculus where ECQs are used to query the states of the system, but Bagheri Hariri et al.
(2011) do not support quantification across states, as done here.
Calvanese et al. (2012) investigate a framework for data-centric processes that mixes the approach proposed by Bagheri Hariri et al. (2013) for relational artifacts with the notion of knowledge
bases as used here. In particular, semantically-governed data-aware processes are introduced as a
mechanism to model a dynamic system working over a relational database, providing at the same
time a conceptual representation of the manipulated data in terms of a DL-Lite knowledge base. By
relying on ontology-based data access (Calvanese et al., 2009), declarative mappings are used to
680

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

connect the knowledge base with the underlying relational database. Differently from KABs, the
system evolves at the relational layer, and the knowledge base is used to understand and ultimately
govern such an execution at a higher level of abstraction.
We observe that the results presented here fully subsume those by Bagheri Hariri et al. (2011),
where the underlying description logic is OWL 2 QL. On the one hand, if we remove the possibility
of asserting functionality of roles in the knowledge component, and of equating individuals as a result of an action in the action component, we precisely obtain the setting presented by Bagheri Hariri
et al. (2011). On the other hand, for both frameworks the established complexity upper bounds are
the same.

9. Conclusions
In this paper we have studied verification of knowledge and action bases, which are dynamic systems
constituted by a knowledge base, expressed in description logics, and by an action specification that
changes the knowledge base over time. We have obtained an interesting decidability result by
relying on the notion of weak acyclicity, based on a connection with the theory of chase of TGDs in
relational databases.
In our work, we have used the original notion of weak acyclicity. However, it is easy to adopt
more advanced forms of acyclicity, since our results depend only on the ability of finding a finite
bound on the number of distinct function terms that are generated (when applying the chase). While
the majority of approaches that adopt forms of weak-acyclicity focus on databases (Marnette &
Geerts, 2010; Meier, Schmidt, Wei, & Lausen, 2010), Cuenca Grau, Horrocks, Krötzsch, Kupke,
Magka, Motik, and Wang (2012) investigate sophisticated forms of acyclicity in the context of
knowledge bases without UNA. Their results can thus be seamlessly applied to KABs. Interestingly,
to manage the impact of equalities in a setting without UNA, they resort to the singularization technique presented by Marnette (2009), which closely resembles the normalization of KABs introduced
in Section 7.
Weak acyclicity allows us to gain decidability by bounding the number of distinct function
terms that occur in the transition system. An alternative approach to gain decidability is to bound the
number of distinct terms occurring in the ABox assertions of a state. Variants of this notion of “state
boundedness” have been proposed recently in other contexts (Belardinelli et al., 2012; De Giacomo,
Lesperance, & Patrizi, 2012; Bagheri Hariri et al., 2013). It is of great interest to explore such an
approach in the setting presented here of actions acting on a description logic knowledge base.
We observe that our decidability result (as well as the ones commented here and in Section 8),
comes with an algorithm for verification that is exponential in the size of the initial ABox. This precludes a direct application of these techniques to large-scale systems, without a careful analysis of
how these can be modularized in small units to be verified (almost) separately. This is an important
direction for further investigation.
Acknowledgments
This research has been partially supported by the EU under the ICT Collaborative Project ACSI
(Artifact-Centric Service Interoperation), grant agreement n. FP7-257593, and under the large-scale
integrating project (IP) Optique (Scalable End-user Access to Big Data), grant agreement n. FP7318338.
681

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

References
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). The DL-Lite family and
relations. J. of Artificial Intelligence Research, 36, 1–69.
Artale, A., & Franconi, E. (1998). A temporal description logic for reasoning about actions and
plans. J. of Artificial Intelligence Research, 9, 463–506.
Artale, A., & Franconi, E. (2005). Temporal description logics. In Gabbay, D., Fisher, M., & Vila, L.
(Eds.), Handbook of Temporal Reasoning in Artificial Intelligence, Foundations of Artificial
Intelligence. Elsevier.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003). The
Description Logic Handbook: Theory, Implementation and Applications. Cambridge University Press.
Baader, F., Ghilardi, S., & Lutz, C. (2012). LTL over description logic axioms. ACM Trans. on
Computational Logic, 13(3), 21:1–21:32.
Baader, F., & Laux, A. (1995). Terminological logics with modal operators. In Proc. of the 14th
Int. Joint Conf. on Artificial Intelligence (IJCAI’95), pp. 808–814.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., & De Masellis, R. (2011). Verification of
conjunctive-query based semantic artifacts. In Proc. of the 24th Int. Workshop on Description Logic (DL 2011), Vol. 745 of CEUR Electronic Workshop Proceedings, http:
//ceur-ws.org/.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., De Masellis, R., & Felli, P. (2011). Foundations
of relational artifacts verification. In Proc. of the 9th Int. Conference on Business Process
Management (BPM 2011), Vol. 6896 of Lecture Notes in Computer Science, pp. 379–395.
Springer.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2012). Verification of relational data-centric dynamic systems with external services. Corr technical report arXiv:1203.0024, arXiv.org e-Print archive. Available at http://arxiv.org/abs/
1203.0024.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2013). Verification
of relational data-centric dynamic systems with external services. In Proc. of the 32nd ACM
SIGACT SIGMOD SIGART Symp. on Principles of Database Systems (PODS 2013).
Bao, J., et al. (2012). OWL 2 Web Ontology Language document overview (second edition). W3C
Recommendation, World Wide Web Consortium. Available at http://www.w3.org/
TR/owl2-overview/.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011). Verification of deployed artifact systems via data
abstraction. In Proc. of the 9th Int. Joint Conf. on Service Oriented Computing (ICSOC 2011),
Vol. 7084 of Lecture Notes in Computer Science, pp. 142–156. Springer.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012). An abstraction technique for the verification
of artifact-centric systems. In Proc. of the 13th Int. Conf. on the Principles of Knowledge
Representation and Reasoning (KR 2012), pp. 319–328.
Berardi, D., Calvanese, D., & De Giacomo, G. (2005). Reasoning on UML class diagrams. Artificial
Intelligence, 168(1–2), 70–118.
682

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

Bhattacharya, K., Gerede, C., Hull, R., Liu, R., & Su, J. (2007). Towards formal analysis of artifactcentric business process models. In Proc. of the 5th Int. Conference on Business Process
Management (BPM 2007), Vol. 4714 of Lecture Notes in Computer Science, pp. 288–234.
Springer.
Burkart, O., Caucal, D., Moller, F., & Steffen, B. (2001). Verification of infinite structures.. In
Handbook of Process Algebra. Elsevier Science.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodrı́guez-Muro, M., &
Rosati, R. (2009). Ontologies and databases: The DL-Lite approach. In Tessaris, S., & Franconi, E. (Eds.), Reasoning Web. Semantic Technologies for Informations Systems – 5th Int.
Summer School Tutorial Lectures (RW 2009), Vol. 5689 of Lecture Notes in Computer Science, pp. 255–356. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007a). EQL-Lite: Effective first-order query processing in description logics. In Proc. of the 20th Int. Joint Conf. on
Artificial Intelligence (IJCAI 2007), pp. 274–279.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007b). Tractable reasoning and efficient query answering in description logics: The DL-Lite family. J. of Automated
Reasoning, 39(3), 385–429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data complexity
of query answering in description logics. Artificial Intelligence, 195, 335–360.
Calvanese, D., De Giacomo, G., Lembo, D., Montali, M., & Santoso, A. (2012). Ontology-based
governance of data-aware processes. In Proc. of the 6th Int. Conf. on Web Reasoning and Rule
Systems (RR 2012), Vol. 7497 of Lecture Notes in Computer Science, pp. 25–41. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2008). Conjunctive query containment and answering under description logics constraints. ACM Trans. on Computational Logic, 9(3),
22.1–22.31.
Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2007). Actions and programs over description logic ontologies. In Proc. of the 20th Int. Workshop on Description Logic (DL 2007),
Vol. 250 of CEUR Electronic Workshop Proceedings, http://ceur-ws.org/, pp. 29–
40.
Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2011). Actions and programs over
description logic knowledge bases: A functional approach. In Lakemeyer, G., & McIlraith,
S. A. (Eds.), Knowing, Reasoning, and Acting: Essays in Honour of Hector Levesque. College
Publications.
Calvanese, D., De Giacomo, G., & Montali, M. (2013). Foundations of data aware process analysis:
A database theory perspective. In Proc. of the 32nd ACM SIGACT SIGMOD SIGART Symp.
on Principles of Database Systems (PODS 2013).
Calvanese, D., De Giacomo, G., & Vardi, M. Y. (2002). Reasoning about actions and planning in
LTL action theories. In Proc. of the 8th Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR 2002), pp. 593–602.
Calvanese, D., Kharlamov, E., Nutt, W., & Zheleznyakov, D. (2010). Updating ABoxes in DL-Lite.
In Proc. of the 4th Alberto Mendelzon Int. Workshop on Foundations of Data Management
683

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

(AMW 2010), Vol. 619 of CEUR Electronic Workshop Proceedings, http://ceur-ws.
org/, pp. 3.1–3.12.
Cangialosi, P., De Giacomo, G., De Masellis, R., & Rosati, R. (2010). Conjunctive artifact-centric
services. In Proc. of the 8th Int. Joint Conf. on Service Oriented Computing (ICSOC 2010),
Vol. 6470 of Lecture Notes in Computer Science, pp. 318–333. Springer.
Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model checking. The MIT Press, Cambridge,
MA, USA.
Cohn, D., & Hull, R. (2009). Business artifacts: A data-centric approach to modeling business
operations and processes. Bull. of the IEEE Computer Society Technical Committee on Data
Engineering, 32(3), 3–9.
Cuenca Grau, B., Horrocks, I., Krötzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z. (2012).
Acyclicity conditions and their application to query answering in description logics. In
Proc. of the 13th Int. Conf. on the Principles of Knowledge Representation and Reasoning
(KR 2012), pp. 243–253.
Damaggio, E., Deutsch, A., & Vianu, V. (2011). Artifact systems with data dependencies and
arithmetic. In Proc. of the 14th Int. Conf. on Database Theory (ICDT 2011), pp. 66–77.
De Giacomo, G., De Masellis, R., & Rosati, R. (2012). Verification of conjunctive artifact-centric
services. Int. J. of Cooperative Information Systems, 21(2), 111–139.
De Giacomo, G., Iocchi, L., Nardi, D., & Rosati, R. (1999). A theory and implementation of
cognitive mobile robots. J. of Logic and Computation, 9(5), 759–785.
De Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2009). On instance-level update and erasure in description logic ontologies. J. of Logic and Computation, Special Issue on Ontology
Dynamics, 19(5), 745–770.
De Giacomo, G., Lespérance, Y., & Levesque, H. J. (2000). ConGolog, a concurrent programming
language based on the situation calculus. Artificial Intelligence, 121(1–2), 109–169.
De Giacomo, G., Lesperance, Y., & Patrizi, F. (2012). Bounded situation calculus action theories
and decidable verification. In Proc. of the 13th Int. Conf. on the Principles of Knowledge
Representation and Reasoning (KR 2012), pp. 467–477.
Deutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic verification of data-centric business
processes. In Proc. of the 12th Int. Conf. on Database Theory (ICDT 2009), pp. 252–267.
Deutsch, A., Nash, A., & Remmel, J. B. (2008). The chase revisited. In Proc. of the 27th ACM
SIGACT SIGMOD SIGART Symp. on Principles of Database Systems (PODS 2008), pp. 149–
158.
Fagin, R., Kolaitis, P. G., Miller, R. J., & Popa, L. (2005). Data exchange: Semantics and query
answering. Theoretical Computer Science, 336(1), 89–124.
Gabbay, D., Kurusz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-dimensional Modal Logics:
Theory and Applications. Elsevier Science Publishers.
Gu, Y., & Soutchanski, M. (2010). A description logic based situation calculus. Ann. of Mathematics
and Artificial Intelligence, 58(1-2), 3–83.
684

D ESCRIPTION L OGIC K NOWLEDGE AND ACTION BASES

Gutiérrez-Basulto, V., Jung, J. C., & Lutz, C. (2012). Complexity of branching temporal description
logics. In Proc. of the 20th Eur. Conf. on Artificial Intelligence (ECAI 2012), pp. 390–395.
Jamroga, W. (2012). Concepts, agents, and coalitions in alternating time. In Proc. of the 20th Eur.
Conf. on Artificial Intelligence (ECAI 2012), pp. 438–443.
Katsuno, H., & Mendelzon, A. (1991). On the difference between updating a knowledge base and
revising it. In Proc. of the 2nd Int. Conf. on the Principles of Knowledge Representation and
Reasoning (KR’91), pp. 387–394.
Kowalski, R. A., & Sadri, F. (2011). Abductive logic programming agents with destructive
databases. Ann. of Mathematics and Artificial Intelligence, 62(1–2), 129–158.
Lenzerini, M., & Savo, D. F. (2012). Updating inconsistent description logic knowledge bases. In
Proc. of the 20th Eur. Conf. on Artificial Intelligence (ECAI 2012), pp. 516–521.
Levesque, H. J., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). GOLOG: A logic programming language for dynamic domains. J. of Logic Programming, 31, 59–84.
Levesque, H. J. (1984). Foundations of a functional approach to knowledge representation. Artificial
Intelligence, 23, 155–212.
Limonad, L., De Leenheer, P., Linehan, M., Hull, R., & Vaculin, R. (2012). Ontology of dynamic
entities. In Proc. of the 31st Int. Conf. on Conceptual Modeling (ER 2012).
Lin, F., & Reiter, R. (1994). State constraints revisited. J. of Logic Programming, 4(5), 655–678.
Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006a). Reasoning about actions using description
logics with general TBoxes. In Proc. of the 10th Eur. Conference on Logics in Artificial
Intelligence (JELIA 2006), Vol. 4160 of Lecture Notes in Computer Science. Springer.
Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006b). Updating description logic ABoxes. In Proc. of
the 10th Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR 2006),
pp. 46–56.
Lutz, C., Wolter, F., & Zakharyaschev, M. (2008). Temporal description logics: A survey. In Proc.
of the 15th Int. Symp. on Temporal Representation and Reasoning (TIME 2008), pp. 3–14.
Marnette, B. (2009). Generalized schema-mappings: from termination to tractability. In Proc.
of the 28th ACM SIGACT SIGMOD SIGART Symp. on Principles of Database Systems
(PODS 2009), pp. 13–22.
Marnette, B., & Geerts, F. (2010). Static analysis of schema-mappings ensuring oblivious termination. In Proc. of the 13th Int. Conf. on Database Theory (ICDT 2010), pp. 183–195.
Martin, D., Paolucci, M., McIlraith, S., Burstein, M., McDermott, D., McGuinness, D., Parsia, B.,
Payne, T., Sabou, M., Solanki, Srinivasan, N., & Sycara, K. (2004). Bringing semantics to
web services: The OWL-S approach. In Proc. of the 1st Int. Workshop on Semantic Web
Services and Web Process Composition (SWSWPC 2004).
Meier, M., Schmidt, M., Wei, F., & Lausen, G. (2010). Semantic query optimization in the presence
of types. In 111-122 (Ed.), Proc. of the 29th ACM SIGACT SIGMOD SIGART Symp. on
Principles of Database Systems (PODS 2010).
Meyer, A., Smirnov, S., & Weske, M. (2011). Data in business processes. EMISA Forum, 31(3),
5–31.
685

BAGHERI H ARIRI , C ALVANESE , D E G IACOMO , D E M ASELLIS , F ELLI , & M ONTALI

Milner, R. (1971). An algebraic definition of simulation between programs. In Proc. of the 2nd Int.
Joint Conf. on Artificial Intelligence (IJCAI’71), pp. 481–489.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2012). OWL 2 Web Ontology Language profiles (second edition). W3C Recommendation, World Wide Web Consortium. Available at http://www.w3.org/TR/owl2-profiles/.
Nigam, A., & Caswell, N. S. (2003). Business artifacts: An approach to operational specification.
IBM Systems Journal, 42(3), 428–445.
Park, D. M. R. (1976). Finiteness is Mu-ineffable. Theoretical Computer Science, 3(2), 173–181.
Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008). Linking
data to ontologies. J. on Data Semantics, X, 133–173.
Reiter, R. (2001). Knowledge in Action: Logical Foundations for Specifying and Implementing
Dynamical Systems. The MIT Press.
Robinson, R. (1971). Undecidability and nonperiodicity of tilings on the plane. Inventiones Math.,
12, 177–209.
Rosati, R., & Franconi, E. (2012). Generalized ontology-based production systems. In Proc. of the
13th Int. Conf. on the Principles of Knowledge Representation and Reasoning (KR 2012), pp.
435–445. AAAI Press.
Schild, K. (1993). Combining terminological logics with tense logic. In Proc. of the 6th Portuguese
Conf. on Artificial Intelligence (EPIA’93), Vol. 727 of Lecture Notes in Computer Science,
pp. 105–120. Springer.
Stirling, C. (2001). Modal and Temporal Properties of Processes. Springer.
van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2007). Dynamic epistemic logic. Springer.
van Emde Boas, P. (1997). The convenience of tilings. In Sorbi, A. (Ed.), Complexity, Logic, and
Recursion Theory, Vol. 187 of Lecture Notes in Pure and Applied Mathematics, pp. 331–363.
Marcel Dekker Inc.
Vianu, V. (2009). Automatic verification of database-driven systems: a new frontier. In Proc. of the
12th Int. Conf. on Database Theory (ICDT 2009), pp. 1–13.
Wolter, F., & Zakharyaschev, M. (1998). Satisfiability problem in description logics with modal
operators. In Proc. of the 6th Int. Conf. on the Principles of Knowledge Representation and
Reasoning (KR’98), pp. 512–523.
Wolter, F., & Zakharyaschev, M. (1999a). Modal description logics: Modalizing roles. Fundamenta
Informaticae, 39(4), 411–438.
Wolter, F., & Zakharyaschev, M. (1999b). Temporalizing description logic. In Gabbay, D., &
de Rijke, M. (Eds.), Frontiers of Combining Systems, pp. 379–402. Studies Press/Wiley.

686

Journal of Artificial Intelligence Research 46 (2013) 303–341

Submitted 09/12; published 03/13

Boolean Equi-propagation for Concise and Efficient SAT
Encodings of Combinatorial Problems
Amit Metodi
Michael Codish

amit.metodi@gmail.com
mcodish@cs.bgu.ac.il

Department of Computer Science
Ben-Gurion University of the Negev, Israel

Peter J. Stuckey

pjs@csse.unimelb.edu.au

Department of Computer Science and Software Engineering
and NICTA Victoria Laboratory
The University of Melbourne, Australia

Abstract
We present an approach to propagation-based SAT encoding of combinatorial problems, Boolean equi-propagation, where constraints are modeled as Boolean functions which
propagate information about equalities between Boolean literals. This information is then
applied to simplify the CNF encoding of the constraints. A key factor is that considering
only a small fragment of a constraint model at one time enables us to apply stronger, and
even complete, reasoning to detect equivalent literals in that fragment. Once detected,
equivalences apply to simplify the entire constraint model and facilitate further reasoning
on other fragments. Equi-propagation in combination with partial evaluation and constraint simplification provide the foundation for a powerful approach to SAT-based finite
domain constraint solving. We introduce a tool called BEE (Ben-Gurion Equi-propagation
Encoder) based on these ideas and demonstrate for a variety of benchmarks that our approach leads to a considerable reduction in the size of CNF encodings and subsequent
speed-ups in SAT solving times.

1. Introduction
In recent years, Boolean SAT solving techniques have improved dramatically. Today’s SAT
solvers are considerably faster and able to manage larger instances than yesterday’s. Moreover, encoding and modeling techniques are better understood and increasingly innovative.
SAT is currently applied to solve a wide variety of hard and practical combinatorial problems, often outperforming dedicated algorithms. The general idea is to encode a (typically,
NP) hard problem instance, µ, to a Boolean formula, ϕµ , such that the satisfying assignments of ϕµ correspond to the solutions of µ. Given such an encoding, a SAT solver can be
applied to solve µ.
Tailgating the success of SAT technology are a variety of tools which can be applied to
specify and then compile problem instances to corresponding SAT instances. The general
objective of such tools is to facilitate the process of providing high-level descriptions of how
the (constraint) problem at hand is to be solved. Typically, a constraint-based modeling
language is introduced and used to model instances. Drawing on the analogy to programming languages, given such a description, a compiler can then provide a low-level executable
c
2013
AI Access Foundation. All rights reserved.

Metodi, Codish, & Stuckey

for the underlying machine. Namely, in our context, a formula for the underlying SAT or
SMT solver.
For example, Cadoli and Schaerf (2005) introduce NP-SPEC, a logic-based specification
language which allows specifying combinatorial problems in a declarative way. At the core of
this system is a component which translates specifications to CNF formula. Similarly Sugar
(Tamura, Taga, Kitagawa, & Banbara, 2009) is a SAT-based constraint solver. To solve
a finite domain constraint satisfaction problem it is first modeled in a constraint language
(also called Sugar) and then encoded to a CNF formula and solved using the MiniSAT
solver (Eén & Sörensson, 2003). MiniZinc (Nethercote, Stuckey, Becket, Brand, Duck, &
Tack, 2007) is a constraint modeling language that is compiled by a variety of solvers to
the low-level target language FlatZinc for which there exist many solvers. In particular,
FlatZinc instances are solved by fzntini (Huang, 2008) by encoding them to CNF and
in fzn2smt by encoding to SMT-LIB (Barrett, Stump, & Tinelli, 2010).
Simplifying CNF formulae prior to the application of SAT solving is of the utmost
importance and there are a wide range of techniques that can be applied to achieve this
goal. See for example the work of Li (2003), Eén and Biere (2005), Heule, Järvisalo, and
Biere (2011), and Manthey (2012), and the references therein their work. All of these
techniques exhibit a clear trade-off between the amount of simplification obtained and the
time it requires. Moreover, the stronger techniques become prohibitive when the SAT model
involves hundreds of thousands of variables and millions of clauses. So in CNF simplification
tools, time limits on simplification techniques are imposed and/or approximations are used.
This paper takes a new approach to CNF simplification. Typically, a CNF is not a random collection of clauses, but rather has a structure derived from an application or specific
problem domain. When SAT solving is applied to encode and solve finite domain constraint
problems, the original constraint model is a manifest of this structure. Usually, the constraints are discarded once encoded to CNF. We advocate that maintaining the constraints
provides important structural information that can be applied to drive the process of CNF
simplification. To be specific, the constraints in a model induce a partitioning of their CNF
encoding to a conjunction of sub-formulae which we call “portions”.
The novelty in our approach to CNF simplification is that instead of considering the CNF
as a whole, we assume that it is partitioned into a conjunction of smaller portions. Then
simplification is repeatedly applied to individual portions. This facilitates a propagationbased process because the simplification of one portion propagates information to all of the
portions and this information may trigger further simplification in other portions.
Because portions are typically much smaller than the entire CNF we can effectively apply
stronger simplification algorithms. We introduce the notion of equi-propagation. Similar to
how unit propagation is about inferring unit clauses which can then be applied to simplify
CNF formulae, equi-propagation is about inferring equational consequences between literals
(and Boolean constants).
There is a wide body of research on CNF simplification that can be applied to implement
equi-propagation which is sometimes called equivalent literal substitution, for example by
Gelder (2005). Techniques typically involve binary clause based simplifications using, among
others, hyper binary resolution and binary implication graphs. See for example, the work
of Heule et al. (2011) and the references therein. The guiding principle in all of these works
304

Boolean Equi-propagation

is that techniques must be simple and efficient because of the prohibitive size of the CNF
to which they must apply.
Our approach is different and we focus on far richer forms of inference not even related
to the CNF structure of a formula. At one extreme we apply complete equi-propagation
which detects all equivalences implied by a formula. Clearly complete equi-propagation is
NP-hard. However, complete equi-propagators are feasible as we apply them only to small
portions of the formula. When complete equi-propagation is too slow we consider ad-hoc
techniques. All of these forms of equi-propagation have in common that they are not driven
by the CNF structure (e.g. binary clauses) but rather by the underlying constraint structure
from which a CNF was, or is being, generated.
The rest of this paper is structured as follows. Section 2 introduces a modeling language
for finite domain constraints which consists of just 5 constraint constructs and is sufficient
to illustrate the contribution of the paper. We argue that the constraints in a model induce
a natural partition of their CNF encoding to smaller portions and that this partition can be
used to drive the simplification of the CNF encoding. Section 3 presents equi-propagation
which is the first ingredient for our contribution. Equi-propagation is about learning information that will apply to simplify CNF encodings. Section 4 describes a practical basis for
implementing equi-propagation. Section 5 introduces the second ingredient: partial evaluation. Given the information derived using equi-propagation, partial evaluation applies
to simplify the constraints and in particular to remove Boolean variables from their CNF
encodings. Section 6 describes a tool, called BEE (Metodi & Codish, 2012) (Ben-Gurion
Equi-propagation Encoder) that is based on equi-propagation and partial evaluation. We
introduce here our full constraint language which is similar to Sugar and to the subset
of FlatZinc relevant for finite domain constraint problems. We also spell out the special
treatment of the all-different constraint in BEE. Section 7 demonstrates the application of
BEE. Section 8 presents an experimental evaluation. and Finally Section 9 presents our
conclusion.
This paper extends earlier work presented by Metodi, Codish, Lagoon, and Stuckey
(2011), which first introduced equi-propagation, and also the BEE tool paper (Metodi &
Codish, 2012). The BEE tool is available for download (Metodi, 2012).

2. Constraint Based Boolean Modeling
This section provides the basis for our contribution: a constraint-based modeling language,
together with a Boolean interpretation for each constraint in the language. This enables us
to view a constraint model as a conjunction of Boolean formulae and provides a structure
which drives the subsequent encoding to CNF.
We first introduce a simple and small fragment of a typical finite domain constraint-based
modeling language. This serves to illustrate our approach. Later, in Section 6, we show
the full language. We then discuss several options for Boolean representation of integers.
In this paper we adopt a particular unary representation, called the order encoding. Our
contribution is independent of this choice, although equi-propagation works well with it.
Finally we finish the section so that each of the constraints in the language fragment can
be viewed as a Boolean formula, and a constraint model as their conjunction.
305

Metodi, Codish, & Stuckey

(1)
(2)
(3)
(4)
(5)

new int(I, c1 , c2 )
int neq(I1 , I2 )
allDiff([I1 , . . . , In ])
int plus(I1 , I2 , I)
int array plus([I1 , . . . , In ], I)

0 ≤ c1 ≤ I ≤ c2
I1 6= I2
V
i<j Ii 6= Ij
I1 + I2 = I
I1 + · · · + In = I

Figure 1: A core constraint language
2.1 Constraint Language Fragment
We focus on a small fragment of a typical constraint modeling language detailed in Figure 1.
This serves to present the main ideas of the paper. Constraint (1) is about declaring finite
domain integer variables in the range [c1 ...c2 ]. For simplicity in the presentation we will
further assume that c1 ≥ 0. Constraints (2–3) are about difference of integer variables,
and constraints (4–5) are about sums of integer variables. As syntactic sugar we also allow
writing integer constants in constraints. For example, int neq(I, 5) which is short for
new int(I0 , 5, 5), int neq(I, I0 ).
2.2 Modeling Kakuro: an Example
A Kakuro puzzle is an n × m board of black and white cells. The black cells contain hints
and the white cells are to be filled by numbers between 1 and 9 (the bound 9 is often
generalized by a larger value r). The hints specify constraints on the sums of the values in
blocks of white cells to the right and/or below the hint. The numbers assigned to the white
cells in such a block are required to be “all different”. Figure 2 illustrates a 4 × 4 Kakuro
puzzle (left) and its solution (right).
To model a Kakuro puzzle we view it as a set of blocks (of white cells) where each block
B is a set of integer variables and is associated with a corresponding integer value, hint(B).
Each block B is associated with two constraints: the integers in B must sum to hint(B)
and must be all-different. Figure 3 illustrates the constraints corresponding to the Kakuro
instance in Figure 2.
2.3 Representing Integers
A fundamental design choice when encoding finite domain constraints concerns the representation of integer variables. Gavanelli (2007) surveys several of the possible choices (the

Figure 2: A 4 × 4 Kakuro puzzle (right) and its solution (left).
306

Boolean Equi-propagation

new
new
new
new
new
new
new

int(I1 , 1, 9)
int(I2 , 1, 9)
int(I3 , 1, 9)
int(I4 , 1, 9)
int(I5 , 1, 9)
int(I6 , 1, 9)
int(I7 , 1, 9)

int
int
int
int
int
int

array
array
array
array
array
array

plus([I1 , I2 ], 13)
plus([I1 , I3 ], 5)
plus([I3 , I4 , I5 ], 12)
plus([I2 , I4 , I6 ], 19)
plus([I6 , I7 ], 3)
plus([I5 , I7 ], 4)

allDiff([I1 , I2 ])
allDiff([I1 , I3 ])
allDiff([I3 , I4 , I5 ])
allDiff([I2 , I4 , I6 ])
allDiff([I6 , I7 ])
allDiff([I5 , I7 ])

Figure 3: Constraints for the Kakuro instance of Figure 2.

direct-, support- and log- encodings) and introduces the log-support encoding. Given a choice
of representation constraints are bit-blasted and interpreted as Boolean formulae. We focus
for now on the use a unary representation, the so-called, order-encoding (see, e.g. Crawford
& Baker, 1994; Bailleux & Boufkhad, 2003) which has many nice properties when applied
to small finite domains.
In the order-encoding, an integer variable X in the domain [0, . . . , n] is represented by
a bit vector X = [x1 , . . . , xn ]. Each bit xi is interpreted as X ≥ i so in particular the bit
sequence X constitutes a monotonic non-increasing Boolean sequence. For example, the
value 3 in the interval [0, 5] is represented in 5 bits as [1, 1, 1, 0, 0].
An important property of a Boolean representation for finite domain integers is the
ability to represent changes in the set of values a variable can take. It is well-known
that the order-encoding facilitates the propagation of bounds. Consider an integer variable
X = [x1 , . . . , xn ] with values in the interval [0, n]. To restrict X to take values in the range
[a, b] (for 1 ≤ a ≤ b ≤ n), it is sufficient to assign xa = 1 and xb+1 = 0 (if b < n). The
variables xa0 and xb0 for 0 ≥ a0 > a and b < b0 ≤ n are then determined true and false,
respectively, by unit propagation. For example, given X = [x1 , . . . , x9 ], assigning x3 = 1 and
x6 = 0 propagates to give X = [1, 1, 1, x4 , x5 , 0, 0, 0, 0], signifying that dom(X) = {3, 4, 5}.
We observe an additional property of the order-encoding for X = [x1 , . . . , xn ]: its ability
to specify that a variable cannot take a specific value 0 ≤ v ≤ n in its domain by equating
two variables: xv = xv+1 . This indicates that the order-encoding is well-suited not only to
propagate lower and upper bounds, but also to represent integer variables with an arbitrary,
finite set, domain. For example, given X = [x1 , . . . , x9 ], equating x2 = x3 imposes that
X 6= 2. Likewise x5 = x6 and x7 = x8 impose that X 6= 5 and X 6= 7. Applying
these equalities to X gives, X = [x1 , x2 , x2 , x4 , x5 , x5 , x7 , x7 , x9 ] (note the repeated literals),
signifying that dom(X) = {0, 1, 3, 4, 6, 8, 9}.
The order-encoding has many additional nice features that can be exploited to simplify
constraints and their encodings to CNF. To illustrate one, consider a constraint of the form
A + B = 5 where A and B are integer values in the range between 0 and 5 represented in
the order-encoding. At the bit level (in the order encoding) we have: A = [a1 , . . . , a5 ] and
B = [b1 , . . . , b5 ]. The constraint is satisfied precisely when B = [¬a5 , . . . , ¬a1 ]. Instead of
encoding the constraint to CNF, we substitute the bits b1 , . . . , b5 by the literals ¬a5 , . . . , ¬a1 ,
and remove the constraint. In section 3 we formalize this process of discovering equalities
between literals implied by a constraint and using them to simplify CNF encodings.
307

Metodi, Codish, & Stuckey

2.4 Bit Blasting
Given a constraint model and the decision on how to represent finite domain integer variables
at the bit level (we chose the order encoding), “bit-blasting” is the process of instantiating integer variables by corresponding bit vectors and interpreting constraints as Boolean
formulae.
Each integer variable, I, declared by a constraint of the form new int(I, c1 , c2 ) where
0 ≤ c1 ≤ c2 is represented as a bit-vector I = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ]. So, we may view a
constraint model as consisting only of Boolean variables and each constraint c corresponds
to a Boolean formula denoted as [[c]], the “bit-blasted” version of c. The specific definition
of [[·]] is not important. Just for illustration, note that one could define
^
[[new int(I, c1 , c2 )]] =
(xi+1 → xi )
c1 ≤i<c2

where I = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ] as well as
[[int neq(I1 , I2 )]] =

n
_

(xi xor yi )

i=1

where to simplify presentation we assume that I1 = [x1 , . . . , xn ] and I2 = [y1 , . . . , yn ] are
represented in the same number of bits. The mapping [[·]] extends in the natural way to apply
to conjunctions of constraints. So, given a constraint model such as the one in Figure 3,
integer variables are instantiated to unary (order encoding) bit vectors and each constraint
is viewed as a Boolean formula. The constraint model takes a Boolean representation as
the conjunction of these formulae.

3. Boolean Equi-propagation
In this section we present an approach to propagation-based SAT encoding, Boolean equipropagation, which propagates information about equalities between Boolean literals (and
constants). We prove that Boolean equi-propagation is stronger than unit propagation as
it determines at least as many fixed literals as unit propagation. We demonstrate, with an
example, the power of equi-propagation and show that it leads to a considerable reduction
in the size of the CNF encoding.
3.1 Boolean Equi-propagation
Let B be a set of Boolean variables. A literal is a Boolean variable b ∈ B or its negation
¬b. The negation of a literal `, denoted ¬`, is defined as ¬b if ` = b and as b if ` = ¬b.
The Boolean constants 1 and 0 represent true and false, respectively. The set of literals
is denoted L and L0,1 = L ∪ {0, 1}. The set of (free) Boolean variables that appear in a
Boolean formula ϕ is denoted vars(ϕ). We extend the vars function to sets of formulae in
the natural way.
An assignment, A, is a partial mapping
from Boolean
to 	constants, often

 
	  variables


viewed as the following set of literals: b A(b) = 1 ∪ ¬b A(b) = 0 . For a formula
ϕ and b ∈ B, we denote by ϕ[b] (likewise ϕ[¬b]) the formula obtained by substituting all
308

Boolean Equi-propagation

occurrences of b ∈ B in ϕ by true (false). This notation extends in the natural way for
sets of literals. We say that A satisfies ϕ if vars(ϕ) ⊆ vars(A) and ϕ[A] evaluates to true.
A Boolean Satisfiability (SAT) problem consists of a Boolean formula ϕ and determines if
there exists an assignment which satisfies ϕ.
A Boolean equality is a constraint ` = `0 where `, `0 ∈ L0,1 . An equi-formula E is a set
of Boolean equalities understood as a conjunction. The set of Boolean equalities is denoted
Leq
0,1 and the set of equi-formulae is denoted E.
Example 1. Suppose B = {x, y, z}. Then L0,1 = {0, 1, ¬x, x, ¬y, y, ¬z, x}. An example
assignment is A = {x, ¬z}, while B = {x, y, z, ¬y} is not an assignment (since it includes
{y, ¬y}). Given the formula ϕ = x ↔ (y ∨ ¬z) then ϕ[¬x] is the formula 0 ↔ (y ∨ ¬z) or
equivalently ¬y ∧ z. The formula ϕ[A] = 1 ↔ (y ∨ 1) which is equivalent to true, but A does
not satisfy ϕ since vars(ϕ) = {x, y, z} 6⊆ {x, z} = vars(A). An example equi-formula for B
is {x = 0, y = ¬z} or equivalently ¬x ∧ (y ↔ ¬z).
3.1.1 Equi-propagation
is a process of inferring equational consequences from a Boolean formula and given equational information. An equi-propagator for a formula ϕ is an extensive function µϕ : E → E
defined such that for all E ∈ E,
o
n


E ⊆ µϕ (E) ⊆ e ∈ Leq
0,1 ϕ ∧ E |= e
That is, a conjunction of equalities, at least as strong
true by ϕo∧ E. We say
n as E, made
eq 
that an equi-propagator µϕ is complete if µϕ (E) = e ∈ L0,1 ϕ ∧ E |= e . We denote
a complete equi-propagator for ϕ as µ̂ϕ . We assume that equi-propagators are monotonic:
E1 ⊆ E2 ⇒ µϕ (E1 ) ⊆ µϕ (E2 ). In particular, this follows, by definition, for complete
equi-propagators. In Section 3.3 we discuss several methods to implement complete and
incomplete equi-propagators.
Example 2. Consider the constraint
C = new int(X, 0, 4) ∧ new int(Y, 0, 4) ∧ int neq(X, Y)
and its corresponding Boolean representation ϕ = [[C]] on the bit representation where
X = [x1 , x2 , x3 , x4 ] and Y = [y1 , y2 , y3 , y4 ]
Assume the setting where
E=



y1 = 1, y2 = 1, y3 = 0, y4 = 0

	

signifying that Y = 2. Then, µ̂ϕ (E) = E ∪ {x2 = x3 } indicating that X 6= 2. This occurs
since ϕ ∧ E is equivalent to (x2 → x1 ) ∧ (x3 → x2 ) ∧ (x4 → x3 ) ∧ (¬x1 ∨ ¬x2 ∨ x3 ∨ x4 ) and
ϕ ∧ E |= x2 = x3 .
The following theorem states that complete equi-propagation is at least as powerful as
unit propagation.
309

Metodi, Codish, & Stuckey

Theorem 3. Let µ̂ϕ be a complete equi-propagator for a Boolean formula ϕ. Then, any
literal that is made true by unit propagation for any clausal representation of ϕ using the
equations in E is also determined true by µ̂ϕ (E).
Proof. Let ϕ be a Boolean formula, E an equi-formula, and let Cϕ and CE be any clausal
representations of ϕ and of E respectively. Clearly ϕ |= Cϕ and E |= CE . Let b be a
positive literal determined by unit propagation of Cϕ ∪ CE . Then by correctness of unit
propagation, Cϕ ∪ CE |= b. Hence, ϕ ∧ E |= b and thus µ̂ϕ (E) |= b = 1. The case for a
negative literal ¬b is the same, except that we infer b = 0.
The following example illustrates that equi-propagation can be more powerful than unit
propagation.
Example 4. Consider ϕ = (x1 ↔ x2 ) ∧ (x1 ∨ x2 ) ∧ (¬x1 ∨ ¬x2 ∨ ¬x3 ). The clausal
representation is (x1 ∨¬x2 )∧(¬x1 ∨x2 )∧(x1 ∨x2 )∧(¬x1 ∨¬x2 ∨¬x3 ) and no unit propagation
is possible, since there are no unit clauses. Equi-propagation (with no additional equational
information) gives: µ̂ϕ (∅) = {x1 = 1, x2 = 1, x3 = 0}.
3.1.2 Boolean Unifiers
It is sometimes convenient to view an equi-formula E in a generic “solved-form” as a Boolean
substitution, θE , which is a (most general) unifier for the equations in E. Boolean substitutions generalize assignments in that variables can be bound also to literals. A Boolean
	
substitution is an idempotent mapping θ : B → L0,1 where dom(θ) = b ∈ B  θ(b) 6= b
is finite. Note in particular that idempotence implies that θ(b) 6= ¬b for every b ∈ B.
Note also that θ is defined for all B and that its domain, dom(θ), includes those elements
for which
it is non-identity.
A Boolean substitution, θ, is viewed as the set θ =


	
b 7→ θ(b)  b ∈ dom(θ)
.
We
can
apply θ to another 	substitution θ0 , to obtain substi

tution (θ ◦ θ0 ) = b 7→ θ(θ0 (b))  b ∈ dom(θ) ∪ dom(θ0 ) . A unifier for equi-formula E
is a substitution θ such that |= θ(e), for each e ∈ E. A most-general unifier for E is a
substitution θ such that for any unifier θ0 of E, there exists substitution γ where θ0 = γ ◦ θ.
Example 5. Consider the equi-formula E ≡ {b1 = ¬b2 , ¬b3 = ¬b4 , b5 = b6 , b6 = b4 , b7 =
1, b8 = ¬b7 } then a unifier θ for E is {b2 7→ ¬b1 , b4 7→ b3 , b5 7→ b3 , b6 7→ b3 , b7 7→ 1, b8 7→ 0}.
Note that θ(E) is the trivially true equi-formula {b1 = ¬¬b1 , ¬b3 = ¬b3 , b3 = b3 , b3 = b3 , 1 =
1, 0 = ¬1}.
Consider the enumeration L0,1 = {0, 1, ¬b1 , b1 , ¬b2 , b2 , . . .} and let ≺ be the total (strict)
order on L0,1 such that 0 ≺ 1 ≺ ¬b1 ≺ b1 ≺ ¬b2 ≺ b2 · · · . We define a canonical most-general
unifier unifyE for any satisfiable equi-formula E where:


	
unifyE (b) = min ` ∈ L0,1  E |= b = `
That is, the substitution unifyE maps each b to the smallest literal equivalent to b given
E. We can compute unifyE in almost linear (amortized) time using a variation of the
union-find algorithm (Tarjan, 1975).
Example 6. For the equi-formula E and substitution θ from Example 5 we have that
unifyE = θ.
310

Boolean Equi-propagation

The following proposition provides the foundation for equi-propagation based Boolean
simplification. It allows us to apply equational information to simplify a given formula. In
particular, if E is an equi-formula about literals occurring in ϕ then unifyE (ϕ) is smaller
than ϕ in that it contains fewer variables.
Proposition 1. Let ϕ be a Boolean formula and E ∈ E be a satisfiable equi-formula. Then,
a. ϕ ∧ E ↔ unifyE (ϕ) ∧ E;
b. ϕ ∧ E is satisfiable if and only if unifyE (ϕ) is satisfiable; and
c. if σ is a satisfying assignment for unifyE (ϕ) then σ◦unifyE is a satisfying assignment
for ϕ ∧ E.
Proof. (a) Let θ = unifyE and assume that σ is a satisfying assignment of E, then we can
view σ as a substitution, and as a unifier of E. Hence, since θ is a most general unifier, there
exists a substitution γ such that σ = γ ◦θ. Clearly γ(b) = σ(b) for all variables b in the range
of θ. Hence, σ and γ agree on all variables in θ(ϕ) which implies that σ(θ(ϕ)) = γ(θ(ϕ))
meaning that σ(θ(ϕ)) = σ(ϕ). So, σ is a satisfying assignment of θ(ϕ) ∧ E if and only
if σ is a satisfying assignment of ϕ ∧ E. (b) The (→) direction follows from (a) and the
(←) direction from (c). (c) Assume σ is a satisfying assignment of unifyE (ϕ). Clearly
σ ◦ unifyE satisfies ϕ by construction. Also σ ◦ unifyE satisfies E since unifyE (E) is
trivial. Hence σ ◦ unifyE is a satisfying assignment of ϕ ∧ E.
3.1.3 The Equi-propagation Process
The equi-propagation process presented now is a central theme in this paper: Let Φ =
ϕ1 ∧ · · · ∧ ϕn be a partitioning of a Boolean formula to n portions, let µϕ1 , . . . , µϕn be corresponding equi-propagators, and take initial E = ∅. Satisfiability of Φ can be determined
as follows:
1. So long as possible, select ϕi such that µϕi (E) ) E and update E = µϕi (E).
2. Finally, when the equi-propagators apply no more, check if unifyE (Φ) is satisfiable.
3. If η is a satisfying assignment for unifyE (Φ) then unifyE ◦η is a satisfying assignment
for Φ.
We typically apply this equi-propagation theme to the Boolean representation Φ =
ϕ1 ∧ · · · ∧ ϕn of a constraint model C = C1 ∧ · · · ∧ Cn where ϕi = [[Ci ]]. Here we require that
each Ci is a “small” conjunction of constraints. Typically, the integer variables referred
to in each Ci are also declared in Ci (sometimes this requires duplicating the variable
declarations). For an individual constraint c we denote by c+ the conjunction of constraints
including c and the declarations for integer variables it refers to. The specifics of these
declarations will be clear from the context.
Example 7. Let C be the following constraint model:

new int(X, 1, 3) ∧ new int(Y, 1, 3) ∧ new int(Z, 1, 3) ∧
C=
int plus(X, Y, 3) ∧ int plus(Y, Z, 4) ∧ int neq(Y, Z)
We have
311



Metodi, Codish, & Stuckey

1. int plus+ (X, Y, 3) = int plus(X, Y, 3) ∧ new int(X, 1, 3) ∧ new int(Y, 1, 3),
2. int plus+ (Y, Z, 4) = int plus(Y, Z, 4) ∧ new int(Y, 1, 3) ∧ new int(Z, 1, 3),
3. int neq+ (Y, Z) = int neq(Y, Z) ∧ new int(Y, 1, 3) ∧ new int(Z, 1, 3).
As a basis for equi-propagation we take Φ = ϕ1 ∧ ϕ2 ∧ ϕ3 where ϕ1 = [[int plus+ (X, Y, 3)]],
ϕ2 = [[int plus+ (Y, Z, 4)]], and ϕ3 = [[int neq+ (Y, Z)]]. Denoting X = [1, x2 , x3 ], Y =
[1, y2 , y3 ], and Z = [1, z2 , z3 ] and applying corresponding complete equi-propagators and
starting with E0 = ∅ we have:
1. E1 = µ̂ϕ1 (E0 ) = E0 ∪ {x3 = 0, y3 = 0, x2 = ¬y2 };
2. E2 = µ̂ϕ2 (E1 ) = E1 ∪ {z2 = 1, y2 = ¬z3 };
3. E3 = µ̂ϕ3 (E2 ) = E2 ∪ {y2 = 0}.
At this point equi-propagation applies no more, and unifyE3 = {x2 7→ 1, x3 7→ 0, y2 7→
0, y3 7→ 0, z2 7→ 1, z3 7→ 1} . Now, unifyE3 (Φ) is a tautology (all of the Boolean variables
are determent by equi-propagation).
The following theorem clarifies that the order in which equi-propagators are applied in
the equi-propagation process does not influence the final result.
Theorem 8. The equi-propagation process is confluent.
Proof. Let Φ = ϕ1 ∧ · · · ∧ ϕn be a Boolean formula and µϕ1 , . . . , µϕn corresponding equipropagators. Let E1 = µϕir (µϕir−1 (. . . µϕi1 (∅) . . .)) and E2 = µϕjs (µϕjs−1 (. . . µϕj1 (∅) . . .))
be two different applications of the equi-propagation process. So by construction, for each
of the given equi-propagators, we have a property (?): µϕi (E1 ) = E1 and µϕi (E2 ) = E2 .
Now assume, in contradiction, that E1 6= E2 . Then w.l.o.g. there exists e ∈ E2 where e
not ∈ E1 (swap the roles of E1 and E2 if E2 ⊂ E1 ). E1 ( E2 . Let us focus on the first
step in the equi-propagation process leading to E2 that introduced the equation e ∈ E2 not
introduced to E1 : So, there exists an ` < s such that E = µϕj` (µϕj`−1 (. . . µϕj1 (∅) . . .)) ⊆ E1
and e ∈ µϕ`+1 (E) but e 6∈ E1 . But, if E ⊆ E1 , then by the monotonicity of µϕ`+1 , we have
that µϕ`+1 (E) ⊆ µϕ`+1 (E1 ) and hence e ∈ µϕ`+1 (E1 ) in contradiction to the construction
with property (?).
The following proposition provides an alternative, more efficient to implement, definition
for complete equi-propagation.
Proposition 2. Let ϕ be a Boolean formula and µ̂ϕ a complete equi-propagator for ϕ.
Define for E ∈ E,
n
o

 unify (ϕ) |= e
µ̄ϕ (E) = E ∪ e ∈ Leq
E
0,1
Then, µ̂ϕ (E) = µ̄ϕ (E). That is, µ̄ϕ implements a complete equi-propagator for ϕ.
312

Boolean Equi-propagation

Proof. Forthe first direction, (⇒):
	 By definition, we have that µ̂ϕ (E) → E. We also have
µ̂ϕ (E) → e  unifyE (ϕ) |= e
because by Proposition 1(a) ϕ ∧ E |= unifyE (ϕ). So,
µ̂ϕ (E) → µ̄ϕ (E). For the other direction, (⇐): Let e ∈ µ̄ϕ (E). If e ∈ E then the proof
is straightforward. Otherwise, let unifyE (ϕ) |= e and assume in contrary that e 6∈ µ̂ϕ (E),
or in other words that ϕ ∧ E 6|= e. This means that there exists an assignment σ that
satisfies ϕ ∧ E but does not satisfy e. By Lemma 1(a), σ also satisfies unifyE (ϕ) ∧ E and in
particular σ satisfies unifyE (ϕ). From our assumption that unifyE (ϕ) |= e we now have
that σ satisfies e. Contradiction.
Computing µ̄ϕ is considerably more efficient than µ̂ϕ since we can simply examine the
formula ϕ after the application of unifyE to determine new Boolean equality consequences.
Finally we comment: Our intention is that the equi-propagation process be applied not
only to make a SAT instance smaller but also to obtain an easier to solve representation.
However, decreasing the size of the CNF is not the main objective. In fact, often we explicitly
introduce redundancies to improve a SAT encoding. For example, consider an “if-thenelse” construct, x↔ITE(s,t,f), where propositional variable: s indicates the “selector”, t
indicates the “true branch”, f indicates the “false branch”, and x indicates the result. The
corresponding CNF is {{¬s, ¬t, x}, {¬s, t, ¬x}, {s, ¬f, x}, {s, f, ¬x}}. Eén and Sörensson
(2006) propose to add redundant clauses, {¬t, ¬f, x} and {t, f, ¬x}. They comment that
this improves the encoding and they observe that redundant clauses are often introduced
to achieve arc-consistency in the SAT encoding. We show that given a clausal encoding of
some formula Φ, application of equi-propagation can only strengthen unit propagation.
Theorem 9. Let C be a set of clauses, and suppose C |= E where E is an equi-formula.
Then unit propagation on unifyE (C) is at least as strong as unit propagation on C.
Proof. Unit propagation on C starting from assignment A0 repeatedly chooses a clause
c ∪ {l} ∈ C where {¬l0 | l0 ∈ c} ⊆ Ai and sets Ai+1 := Ai ∪ {l}. Unit propagation terminates
with Ak when no such clauses occur. Note that failure is detected when Ak contains both
a literal and its negation.
We show that using a order of unit propagation on unifyE (C) determined by that which
occurs on C starting from assignment B0 = unifyE (A0 ) we always obtain an assignment
Bi where Bi ⊇ unifyE (Ai ). The proof is by induction on the unit propagation steps in C.
The base case holds by construction.
Assume c ∪ {l} ∈ C where {¬l0 | l0 ∈ c} ⊆ Ai . Then by induction Bi ⊇ unifyE (Ai ) ⊇
{unifyE (¬l0 ) | l0 ∈ c}. Either unifyE (l) ∈ Bi in which case we set Bi+1 = Bi and the
induction holds. Or unifyE (l) 6∈ Bi . Now since c ∪ {l} ∈ C we have that {unifyE (l0 ) | l0 ∈
c}∪{unifyE (l)} ⊆ unifyE (C). Hence by unit propagation on unifyE (C) and Bi we obtain
Bi+1 := Bi ∪ {unifyE (l)}. Hence the induction holds.
Given that unit propagation reaches a unique fixpoint then any unit propagation order
on unifyE (A0 ) will end up with an assignment B where B ⊇ Bk ⊇ unify(Ak )
3.2 The Power of Equi-propagation
To illustrate the impact of equi-propagation we come back to the Kakuro example from
Section 2.2 (recall Figure 2). In fact solving such puzzles via SAT encodings is quite easy,
with and without equi-propagation. So the example should only be viewed as illustrating
313

Metodi, Codish, & Stuckey

a. Φ1

b. Φ2

c. Φ3

Figure 4: Applying complete equi-propagation to a Kakuro Instance using different models

the impact of equi-propagation on the size of the encoding. We compare 3 different models
of the problem, which each give different equi-propagation.
We consider, as a baseline for this discussion, the following Boolean representation
derived from a constraint model where the declarations which are not specified explicitly
are of the form new int(I, 1, h) where h is the smallest hint for a block that includes I or
the number 9 if that is smaller.
^
^
Φ1 =
[[int neq+ (Ii , Ij )]] ∧
[[int array sum+ (B, hint(B))]]
{I1 , . . . , Ik } ∈ Blocks
1≤i<j ≤k

B ∈ Blocks

Notice that there is one “int neq” conjunct for each pair of white cells in the same block,
and one “int array sum” conjunct for each block. Applying the equi-propagation process to
Φ1 with complete equi-propagators determines six integer values as depicted in Figure 4(a).
Figure 4(b) illustrates the impact of applying the equi-propagation process where the
equi-propagators are for allDiff constraints instead of for the individual int neq constraints. This determines seven integer variables and is formalized taking the following
Boolean representation of the constraint model (and introducing an equi-propagator for
each conjunct).
^
^
Φ2 =
[[allDiff+ (B)]] ∧
[[int array sum+ (B, hint(B))]]
B ∈ Blocks

B ∈ Blocks

Figure 4(c) illustrates the impact of applying the equi-propagation process where the equipropagators are for pairs, each consisting of an allDiff constraint together with its corresponding sum constraint. This form of equi-propagation is most powerful. It fixes integer
values for all of the white cells (in this example). We stress that equi-propagation reasons
only about equalities between Boolean literals and constants. Here we take the model as:
^

Φ3 =
[[allDiff+ (B)]] ∧ [[int array sum+ (B, hint(B))]]
B ∈ Blocks

To further demonstrate the impact of equi-propagation, Table 1 provides data for 15
additional instances,1 categorized as: “easy”, “medium” and “hard”. The first two columns
in the table indicate the instance category and ID. From the five columns headed “Integer
1. Instances available from http://4c.ucc.ie/~hcambaza/page1/page7/page7.html (generated by Helmut
Simonis).

314

hard

medium

easy

Boolean Equi-propagation

ID
168
169
170
171
172
188
189
190
191
192
183
184
185
186
187

Integer Variables
init Φ1 Φ2 Φ3 BEE
init
484 439 280
0 385 3872
467 456 440
0 440 3736
494 485 469
0 469 3952
490 406 393
0 422 3920
506 495 484
0 492 4048
476 461 455
0 461 3808
472 437 425
62 449 3776
492 481 480
0 480 3936
478 452 448 161 448 3824
499 481 478 136 478 3992
490 365 345
0 371 3920
506 489 484
23 486 4048
482 482 455 206 467 3856
472 466 454
0 466 3776
492 475 473
69 473 3936
Average compilation time in sec.

Boolean Variables
Φ1
Φ2
Φ3
1440
843
0
1823 1682
0
1961 1798
0
1280 1148
0
1676 1573
0
1939 1915
0
2017 1911
81
1998 1920
0
1864 1821
197
2455 2417
214
1151 1059
0
1613 1495
21
2181 2111
220
2115 2062
0
1991 1959
48
3.739 2.981 0.916

BEE
1170
1692
1805
1341
1634
1934
1976
1936
1828
2420
1168
1545
2144
2086
1960
0.477

Table 1: Applying SAT-based complete equi-propagation on Kakuro encoding
Variables”, the first four specify the number of unassigned white cells in the initial stage and
after each of the three complete equi-propagation processes described above. From the five
columns headed “Boolean variables”, the first four indicate the corresponding information
regarding the number of Boolean variables in the bit representations of the integers. So,
the smaller the number in the table, the more variables have been removed due to equipropagation. In particular, the Φ3 model completely solves 9 of the 15 instances. The two
columns titled BEE show the corresponding information obtained using a weaker form of
equi-propagation that is described in Section 4 below. The last row of the table indicates
the average time it takes to perform equi-propagation (in seconds) using each of the three
schemes, Φ1 , Φ2 , Φ3 , and the weaker scheme titled BEE. We will come back to discuss this
later after detailing how equi-propagation is performed. The results in the table indicate
the clear benefit in performing equi-propagation based on coarser portions of the model.
3.3 Implementing Equi-propagators
To implement complete equi-propagators we need to infer Boolean equalities implied by a
given Boolean formula, ϕ, and equi-formula, E. Based on Proposition 2, it is sufficient to
test for the condition
unifyE (ϕ) |= (`1 ↔ `2 )
(1)
We consider three techniques: using a SAT solver, using BDD’s, and using ad-hoc rules
applied to the Boolean representations of individual constraints.
It is straightforward to implement a complete equi-propagator using a SAT solver. To
test Condition (1) we consider the formula ψ = ϕ ∧ (`1 6↔ `2 ). If ψ is not satisfiable, then
Condition (1) holds. In this way, Condition (1) can be checked for all relevant equations
315

Metodi, Codish, & Stuckey

involving variables from unifyE (ϕ) (and constants 0,1). A major obstacle with this SATbased approach is that testing for a single equivalence, `1 ↔ `2 , is at least as hard as testing
for the satisfiability of ϕ. In fact testing for unsatisfiability is typically more expensive.
Hence the importance of our assumption that ϕ is only a small fragment of the CNF of
interest. In practice SAT-based equi-propagation is surprisingly fast. For illustration, in
the last row of Table 1 the average times for SAT-based complete equi-propagation for the
different models are indicated in the columns Φ1 , Φ2 , and Φ3 . It is interesting to observe
that the strongest technique, using Φ3 , is the fastest. This is because there are fewer (but
larger) conjuncts and hence fewer queries to the SAT solver.
We can implement a complete equi-propagator using binary decision diagrams (BDDs)
as follows. We construct a BDD for formula ϕ at the beginning of equi-propagation. When
new equational information E 0 is added to E we “simplify” the BDD for ϕ by conjoining
the BDD with a BDD for E 0 and then projecting out the variables that no longer appear in
unifyE (ϕ). Note that this “simplification” can increase the size of the BDD. In practice,
rather than these two steps, we can use the “Restrict” operation of Coudert and Madre
(1990) (“bdd simplify” in Somenzi, 2009) to create the new BDD more efficiently.
Given the BDD for unifyE (ϕ), we can explicitly test Condition (1) using a standard
BDD containment test (e.g., “bddLeq” in Somenzi, 2009). Just as in the SAT-based approach, this test is performed for all relevant equations involving variables from unifyE (ϕ)
(and constants 0,1). Alternately we can use the method of Bagnara and Schachte (1998)
(extended to extract literal equalities as opposed to just variable equalities) to extract all
the fixed literals and equivalent literal consequences of the BDD.
Example 10. Consider the BDD shown in Figure 5(a) which represents the formula:
ϕ ≡ new int(A, 0, 3) ∧ new int(B, 0, 3) ∧ int neq(A, B). Figure 5(b) depicts the The BDD
for unifyE (ϕ) where E = {B1 = 1, B2 = 1, B3 = 0 }. Here it is easy to see that equipropagation determines that A2 = A3 . Let E 0 = E ∪ {A2 = A3 }. Then Figure 5(c) shows
the simplified BDD for unifyE 0 (ϕ).
A major obstacle with this BDD-based approach concerns the size of the formula
unifyE (ϕ). For some constraints, the corresponding BDD is guaranteed to be polynomial (in the size of the constraint). The following result holds for an arbitrary constraint
ϕ, so it also holds for unifyE (ϕ).
Proposition 3. Let c be a constraint about k integer variables each represented with n bits
in the order encoding. Then, the number of nodes in the BDD representing [[c]] is bound by
O(nk ).
Proof. (Sketch) There are only n + 1 legitimate states for each n bit unary variable, and
the BDD cannot have more nodes than possible states.
Constraints like new int, int neq, and int plus involve at most 3 integer variables and
hence their BDD-based complete equi-propagators are polynomially bounded. However,
this is not the case for global constraints such as allDiff and int array plus where the
arity is not fixed. Moreover, it is well known that the allDiff constraint does not have a
polynomial sized BDD (Bessiere, Katsirelos, Narodytska, & Walsh, 2009).
316

Boolean Equi-propagation
7654
0123
A1:

:

:
7654
0123
7654
0123
B1
B1


 

0123
7654
0123 7654
7654
0123
r A 2 rrr A2: A2
r
r
:
r r
rr
rrr rrrr
 :
r
0123 7654
0123
0123
0123
7654
0123
B2 L 7654
B2
B2: 7654
B2: 7654
, LLB 2

::
L
:
L
:
,
 LLL
::
:
,  L
7654
0123
7654
0123
0123
,
A3: A3 l 7654
A3
,   : l -ll
,   l l : l
7654
0123
7654
0123
B3 S
S S B3;; - S S ;;
S S;-

7654
0123
A1
.
  .
.

.
 
.

0123
7654
0123
7654
A2
A2
.

.

.
. 
. 
0123
0123
7654
A3.
A3 > 7654
>>
>> .
>> .
>> .
>>
>.

7654
0123
A1
+
+

T





+



+

+
7654
0123
A2



T

T

(a) BDD for int nequ (A, B)

(b) Simpl’d wrt B=[1, 1, 0]

(c) Simpl’d wrt A2 =A3

Figure 5: BDDs for (a) ϕ ≡ new int3 (A, [0, 3]) ∧ new int3 (B, [0, 3]) ∧ int neq(A, B) (b)
unifyE (ϕ) where E = {B1 =1, B2 =1, B3 =0} and (c) unifyE 0 (ϕ) where E 0 =
E ∪ {A2 =A3 }. Full (dashed) lines correspond to true (false) edges. Edges to the
false node “F” are omitted for brevity.

Given the potential exponential run-time when performing SAT-based equi-propagation,
and the potential exponential size of BDD-based equi-propagators, we consider a third
approach where we implement equi-propagation by a collection of ad-hoc transition rules
for each type of constraint. While this approach is not complete — there are equations
implied by a constraint that are not detected — the implementation is fast, and works well
in practice. This is the topic of the next section.

4. Ad-hoc Equi-Propagation
We consider a rule-based approach to define equi-propagators. The definition is given as a
set of ad-hoc rules specified for each type of constraint. The novelty is that the approach
is not based on CNF, as in previous works, but rather driven by the bit blasted constraints
that are to be encoded to CNF. Our presentation focuses on the case where finite domain
integers are represented in the order encoding. For an integer X = [x1 , . . . , xn ], we often
write: X ≥ i to denote the equation xi = 1, X < i to denote the equation xi = 0, X 6= i to
denote the equation xi = xi+1 , and X = i to denote the pair of equations xi = 1, xi+1 = 0.
Moreover, to simplify notation when specifying the rules below, we view X = [x1 , . . . , xn ] as
a larger vector padded with sentinel cells such that all cells “to the left of” x1 take value 1
and all cells “to the right of” xn take the value 0. Basically this facilitates the specification
of the “end cases” in our formalism. We now consider each of the 5 constraints in the
language fragment presented in Section 2.
317

Metodi, Codish, & Stuckey

c = new int([x1 , . . . , xn ], 0, n)
if in E
then add in µc (E)
xi = 1 x1 = 1, . . . , xi−1 = 1
xi = 0 xi+1 = 0, . . . , xn = 0

(a)

c = int neq(X, Y ) where
X = [x1 , . . . , xn ] and Y = [y1 , . . . , yn ]

if in E
then add in µc (E)
X=i
Y 6= i
xi = yi+1 , yi = xi+1
X 6= i, Y 6= i
xi = ¬yi+1 , yi = ¬xi+1
X 6= i, Y 6= i
(b)

Figure 6: Ad-hoc rules for (a) new int and (b) int neq
c = int plus(X, Y, Z) where X = [x1 , . . . , xn ],
Y = [y1 , . . . , ym ], and Z = [z1 , . . . , zn+m ]

c = allDiff([Z1 , Z2 , Z3 , . . . , Zn ])
if in E
then add in µc (E)
Z1 , Z2 ∈ {i, j}

Z1 =
6 Z2 , Zk 6= i
Zk =
6 j (k > 2)

if in E
X ≥ i, Y ≥ j
X < i, Y < j
Z ≥ k, X < i
Z < k, X ≥ i
X=i
Z=k

(a)

then add in µc (E)
Z ≥i+j
Z <i+j−1
Y ≥k−i
Y <k−i
zi+1 = y1 , . . . , zi+m = ym
x1 = ¬yk , . . . , xk = ¬y1
(b)

Figure 7: Ad-hoc rules for (a) allDiff and (b) int plus

(1) The two rules in Figure 6(a) derive from the monotonicity in the order encoding
representation. These basically correspond to unit propagation, but at the constraint level.
(2) The first rule in Figure 6(b) considers cases when X is a constant (the symmetric
case can be handled by exchanging X and Y ). The other two rules capture templates
that commonly arise in the equi-propagation process. To illustrate the justification of the
third rule consider all possible truth values for the variables xi and xi+1 : (a) If xi = 0
and xi+1 = 1 then both integers in the relation take the form [. . . , 0, 1, . . .] violating their
specification as ordered, so this is not possible. (b) If xi = 1 and xi+1 = 0 then both
numbers take the form [1, . . . , 1, 0, . . . , 0] and are equal, violating the neq constraint. The
only possible bindings for xi and xi+1 are those where xi = xi+1 .
(3) In Figure 7(a) we illustrate a single rule for the allDiff constraint which considers
Hall sets of size 2. Here each Zi represents an integer in the order encoding and we focus
on the case when Z1 and Z2 are restricted by the equations in E to take only two possible
values, i or j. This can be expressed in E because [x1 , . . . , xn ] ∈ {i, j} (for i < j) means
that xk = 1 for k < i, xk = xk+1 for i ≤ k < j, and xk = 0 for j < k ≤ n. Z1 6= Z2 then
means adding the single equation xi = ¬yi (because Z1 and Z2 can take only two values).
In addition to this rule, we apply the rules for int neq(Zi , Zj ) for each pair of integers Zi
and Zj in the constraint.
318

Boolean Equi-propagation




z1 =1, . . . , z4 =1,
E0 =
z5 =0, . . . , z18 =0
X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],
Y = [y1 , . . . , y4 , y5 , y6 , . . . , y9 ],
Z = [1, 1, 1, 1, 0, . . . , 0]
E2 = E1 ∪ {y6 =0, . . . , y9 =0}
X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],
Y = [y1 , . . . , y4 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]
E4 = E3 ∪ {x6 =0, . . . , x9 =0}
X = [x1 , . . . , x4 , 0, . . . , 0],
Y = [y1 , . . . , y4 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

x2 =¬y3 ,
y2 =¬x3

−−−−−−−→
int neq

Z<5, X≥0

−−−−−−−→
int plus

Z<5, Y ≥0

−−−−−−−→
int plus

Z=4

−−−−−→
int plus

E1 = E0 ∪ {y5 =0}
X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],
Y = [y1 , . . . , y4 , 0, y6 , . . . , y9 ],
Z = [1, 1, 1, 1, 0, . . . , 0]

E3 = E2 ∪ {x5 =0}
X = [x1 , . . . , x4 , 0, x6 , . . . , x9 ],
Y = [y1 , . . . , y4 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

E5 = E4 ∪ {x1 =¬y4 , . . . , x4 =¬y1 }
X = [x1 , x2 , x3 , x4 , 0, . . . , 0],
Y = [¬x4 , ¬x3 , ¬x2 , ¬x1 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

y5 =0

−−−−→
new int

x =0

−−5−−→
new int

→

E6 = E5 ∪ {x2 =x3 }
X = [x1 , x2 , x2 , x4 , 0, . . . , 0],
Y = [¬x4 , ¬x2 , ¬x2 , ¬x1 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

Figure 8: Ad-hoc equi-propagation described in Example 11
(4) The first four rules of Figure 7(b) capture the standard propagation behavior for
interval arithmetics. The last two rules apply when one of the integers in the relation is a
constant. There are symmetric cases when replacing the role of X and Y .
(5) There are no special ad-hoc rules for equi-propagation of an int array plus constraint. These are simply viewed as a decomposition to a set of int plus constraints. Then
simplification is performed at that level using the rules for int plus. The decomposition
of int array plus is explained in Section 6.
Example 11 (ad-hoc equi-propagation). Consider the following (partial) constraint model,
from the context of the Kakuro example of Section 2.2, where we represent variables X, Y
and Z as X = [x1 , . . . , x9 ], Y = [y1 , . . . , y9 ] and Z = [z1 , . . . , z18 ] and assume some previous equi-propagation (on other constraints) has determined the current equi-formula E0 to
specify that integer variable Z = 4:

C=

new int(X, 0, 9) ∧ new int(Y, 0, 9) ∧ new int(Z, 0, 18) ∧
int plus(X, Y, Z) ∧ int neq(X, Y)



Figure 8 illustrates, step-by-step, the equi-propagation process on C using the ad-hoc rules
defined above. Each step corresponds to the application of one of the above defined ad-hoc
equi-propagation rules as indicated by the label on the transition. At each stage we illustrate
the derived equations (top part) and their application (as a unifier) to the state variables
X, Y and Z (lower part).
319

Metodi, Codish, & Stuckey

c = ordered([x1 , . . . , xn ]) (new int)
if
then replace with
n≤1
true
x1 = 1
ordered([1, x2 . . . , xn ])
xn = 0
ordered([x1 , . . . , xn−1 , 0])
, . . . , xn ])

xi = xi+1 ordered([x1 , . . . , xi , 
xi+1
Figure 9: Simplification rules for new int (crossed out elements have been removed).
To summarize, let us come back to Table 1. The numbers presented in the two columns
headed “BEE” specify the number of variables remaining after application of ad-hoc equipropagation. We also observe that our definition of ad-hoc equi-propagation is trivially
monotonic.

5. Constraint Model Partial Evaluation
Partial evaluation, together with equi-propagation, is the second important component in
our approach to compile constraint models to CNF. Partial evaluation is about simplifying a given constraint model in view of information that becomes available due to equipropagation. Typically, in the constraint simplification process, we apply alternating steps
of equi-propagation and partial evaluation. Examples of partial evaluation include constant
elimination and removing constraints which are tautologies. In this section we detail the
partial evaluation rules that apply for the five constraint types defined in the language
fragment presented in Section 2.
(1) A new int(I, c1 , c2 ) constraint specifies that an integer I = [x1 , . . . , xn ] is represented
in the order encoding and in particular that the corresponding bit sequence is sorted (not
increasing). We denote this as ordered([x1 , . . . , xn ]). Partial evaluation focuses on this
aspect of the constraint and ignores the bounds c1 , c2 specified in the constraint. The table
in Figure 9 specifies four simplification rules that apply. The first rule identifies tautologies,
the second and third rules remove leading ones and trailing zeros, and the fourth removes
(one of two) equated bits. In this figure, and in the subsequent, a crossed out element in a
sequence, indicates that it has been removed from the sequence.
(2) The simplification rules for a int neq constraint shown in Figure 10(a) are symmetric
when exchanging the role of X and Y . The first two rules identify tautologies. The third
rule is about X and Y which have an equal bit at position i. The corresponding bits can
be removed from the representation of X and Y , resulting in a shorter list of bits in their
representations. The last two rules are about removing leading ones and trailing zeroes and
are illustrated by the following example.
Example 12. Figure 10(b) shows two steps of partial evaluation, for a int neq constraint,
first removing leading ones, then removing trailing zeroes.

320

Boolean Equi-propagation

c = int neq(X, Y ) where
X = [x1 , . . . , xn ] and Y = [y1 , . . . , yn ]
if
then replace with
X = i, Y 6= i
true
xi = ¬yi
true
int neq(
x
[x1 , . . . , 
xi = yi
i , . . . , xn ],
yi , . . . , yn ])
[y1 , . . . , 
int neq([1, xi+1 , . . . , xn ],
X≥i≥2
[yi , yi+1 , . . . , yn ])
int neq([x1 , . . . , xi , 0],
X≤i
[y1 , . . . , yi , yi+1 ])




int neq(
 [x1 , . . . , x4 , 0, 0, 0],  −−P.E
−−→
int neq
[1, 1, 1, y4 , . . . , y7 ])


int neq(
 [x3 , x4 , 0, 0, 0],  −−P.E
−−→
int neq
[1, y4 , . . . , y7 ])


int neq(
 [x3 , x4 , 0], 
[1, y4 , y5 ])

(a)

(b)

Figure 10: (a) Simplification rules for int neq and (b) an example of their application.
c = allDiff([Z1 , . . . , Zn ]) where
Zi = [zi,1 , . . . , zi,m ] (1 ≤ i ≤ n)
if
then replace with
n≤1
true
^  dom(Z1 ) ∩ 
allDiff([Z2 , . . . , Zn ])
dom(Zk ) = ∅
k>1 [
|
dom(Zi )| = 2
allDiff([Z3 , . . . , Zn ])
i∈{1,2}

^
k

Zk 6= i

allDiff(
[z1,1 , . . . , 
z1,i+1
, . . . , z1,m ]
...
[zn,1 , . . . , 
zn,i+1
, . . . , zn,m ])

Figure 11: Simplification rules for allDiff

(3) Four rules for simplifying allDiff constraints are illustrated in Figure 11. The first,
is about detecting tautologies. The second, identifies cases when one of the integers in the
constraint (assume Z1 ) has a domain disjoint from all of the others. This rule also captures
the case when Z1 is a constant. The third rule removes a Hall set of size 2 (assume {Z1 , Z2 })
from the constraint. Note that the corresponding equi-propagation rule detects that the
values of Z3 , . . . , Zn are different from the values of {Z1 , Z2 } and then the next fourth rule
applies. The fourth rule is for the case when none of the integers in the constraint can take
a certain value i. This rule also captures the case when all of the numbers have leading
ones or trailing zeroes. The last two rules are illustrated in Example 14.
(4 & 5) The simplification rules shown in Figure 12 are symmetric when exchanging the
role of X and Y . The first two apply where (at least) one of X, Y and Z is a constant.
Because we have already applied equi-propagation to the constraint, it is a tautology. See
Example 13. The last two rules apply to remove leading ones and trailing zeroes. The
321

Metodi, Codish, & Stuckey

c = int plus(X, Y, Z) where X = [x1 , . . . , xn ],
Y = [y1 , . . . , ym ], and Z = [z1 , . . . , zn+m ]
if
then replace with
X=i
true
Z=k
true
int plus([xi+1 , . . . , xn ], Y,
X ≥ i, Z ≥ i
[zi+1 , . . . , zn+m ])
int plus([x1 , . . . , xi ], Y,
X ≤ i, Z ≤ i + m
[z1 , . . . , zi+m ])

Figure 12: Simplification rules for int plus.
(a) int plus(I1 , I2 , K)
(b) allDiff([I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 ])
(c) int array plus([I2 , I3 , I4 , I5 ], K)
Figure 13: Constraint Model for Examples 13–15
simplification rules of an int array plus constraint are straightforward generalizations of
the ones for int plus. See Example 15.
To summarise the rule based approach to apply equi-propagation and partial evaluation
we present the following sequence of three examples which focus on the simplification of the
three constraints given as Figure 13 where the integer variables I1 , . . . , I8 are defined in the
range between 1 and 8 and where K = 14.
Example 13. Consider equi-propagation of constraint (a) from Figure 13 where E0 specifies
that K = 14:




k1 =1, . . . , k14 =1
k15 = 0, k16 = 0
I1 = [1, i1,2 , . . . , i1,8 ],
I2 = [1, i2,2 , . . . , i2,8 ],
K = [1, 1, . . . , 1, 0, 0]
| {z }



14

 i1,2 =1, . . . , i1,6 =1,

i2,2 =1, . . . , i2,6 =1,


i1,7 =¬i2,8 , i1,8 =¬i2,7
I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],
I2 = [1, 1, 1, 1, 1, 1, ¬i1,8 , ¬i1,7 ],
K = [1, 1, . . . , 1, 0, 0]
| {z }
E1 = E0 ∪

E0 =

K=14

−−−−−→
int plus

14

Given E1 , the constraint is a tautology and removed by partial evaluation:



int plus(

P.E

 −−−
[1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],
−−→
int plus
[1, 1, 1, 1, 1, 1, ¬i1,8 , ¬i1,7 ], 14)



Example 14. Consider equi-propagation of constraint (b) from Figure 13 given E1 from
Example 13:
E1
I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],
I2 = [1, 1, 1, 1, 1, 1, ¬i1,8 , ¬i1,7 ]

i1,7 =¬i2,8 ,
i2,7 =¬i1,8

−−−−−−−−→
int neq

322

E2 = E1 ∪ {i1,7 =i1,8 }
I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],
I2 = [1, 1, 1, 1, 1, 1, ¬i1,7 , ¬i1,7 ]

Boolean Equi-propagation

Given E2 , the equi-propagation rule for allDiff detects that {I1 , I2 } is a Hall set (where
the two variables take values 6 and 8). and adds to E2 the set of equations, E 0 , that specify
that I3 , I4 , I5 , I6 , I7 , I8 6= 6, 8. The result is E3 = E2 ∪ E 0 and the result of this step gives
the following bindings (where the impact of E 0 is underlined):
I1
I2
I3
I4

= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]
= [1, 1, 1, 1, 1, 1, ¬i1,7 , ¬i1,7 ]
= [1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0]
= [1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0]

I5
I6
I7
I8

= [1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0]
= [1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0]
= [1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0]
= [1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]

Given E3 , partial evaluation of the constraint first removes the Hall set:
P.E

[allDiff([I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 ])] −−−−→ [allDiff([I3 , I4 , I5 , I6 , I7 , I8 ])]
allDiff

and then applies to remove three redundant bits in the underlying representation of each
remaining integer (which is not equal to 0, 6, 8):











allDiff([
[1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0],
[1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0],
[1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0],
[1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0],
[1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0],
[1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]])









 −−−−P.E
−−−−→
 allDifferent














allDiff([
[i3,2 , i3,3 , i3,4 , i3,5 , i3,7 ],
[i4,2 , i4,3 , i4,4 , i4,5 , i4,7 ],
[i5,2 , i5,3 , i5,4 , i5,5 , i5,7 ],
[i6,2 , i6,3 , i6,4 , i6,5 , i6,7 ],
[i7,2 , i7,3 , i7,4 , i7,5 , i7,7 ],
[i8,2 , i8,3 , i8,4 , i8,5 , i8,7 ]])












Example 15. Consider equi-propagation of constraint (c) from Figure 13 given E3 from
Example 14. The rules that apply derive from the decomposition of the int array plus
constraint to it int plus parts. These dictate that I3 , I4 , I5 ≤ 5:
E3
I2
I3
I4
I5

= [1, 1, 1, 1, 1, 1, ¬i1,7 , ¬i1,7 ],
= [1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0],
= [1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0],
= [1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0]

−−−−−−−−→
int array
plus

E4
I2
I3
I4
I5

= E3 ∪ {i3,7 =0, i4,7 =0, i5,7 =0}
= [1, 1, 1, 1, 1, 1, ¬i1,7 , ¬i1,7 ],
= [1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0],
= [1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0],
= [1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]

Applying partial evaluation simplifies the constraint as follows:







int array plus([
[1, 1, 1, 1, 1, 1, ¬i1,7 , ¬i1,7 ],
[1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0],
[1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0],
[1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]], 14 )







P.E
 −−−−−
−−−−→
 int array
plus








int array plus([
[¬i1,7 , ¬i1,7 ],
[i3,2 , i3,3 , i3,4 , i3,5 ],
[i4,2 , i4,3 , i4,4 , i4,5 ],
[i5,2 , i5,3 , i5,4 , i5,5 ]], 5 )








To summarize Examples 13–15 observe that in the initial constraint model 3 constraints
about 8 integers are represented in 56 bits. After constraint simplification 2 constraints
remain and the 8 integers are represented using 28 bits:
I1
I2
I3
I4

= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]
= [1, 1, 1, 1, 1, 1, ¬i1,7 , ¬i1,7 ]
= [1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0]
= [1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0]

I5
I6
I7
I8
323

= [1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]
= [1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0]
= [1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0]
= [1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]

Metodi, Codish, & Stuckey

6. Compiling Constraints with BEE
BEE (Ben-Gurion Equi-propagation Encoder) is a tool which applies to encode finite domain
constraint models to CNF. BEE was first introduced by Metodi and Codish (2012). During
the encoding process, BEE performs optimizations based on equi-propagation and partial
evaluation to improve the quality of the target CNF. BEE is implemented in (SWI) Prolog
and can be applied in conjunction with the CryptoMiniSAT solver (Soos, 2010) through a
Prolog interface (Codish, Lagoon, & Stuckey, 2008). CryptoMiniSAT offers direct support
for xor clauses, and BEE takes advantage of this feature. BEE can be downloaded (Metodi,
2012) where one can also find the examples from this paper and others.
The source language for the BEE compiler is also called BEE. It is a constraint modeling
language similar to FlatZinc (Nethercote et al., 2007), but with a focus on a subset of the
language relevant for finite domain constraint problems. Five of the constraint constructs
in the BEE language are those introduced in Section 2.1. The full language is presented in
Table 2.
In BEE Boolean constants “true” and “false” are viewed as (integer) values “1” and “0”.
Constraints are represented as (a list of) Prolog terms. Boolean and integer variables are
represented as Prolog variables, which may be instantiated when simplifying constraints.
in Table 2, X and Xs (possibly with subscripts) denote a literal (a Boolean variable or its
negation) and a vector of literals, I (possibly with subscript) denotes an integer variable,
and c (possibly with subscript) denotes an integer constant. On the right column of the
table are brief explanations regarding the constraints. The table introduces 26 constraint
templates.
Constraints (1-2) are about variable declarations: Booleans and integers. Constraint (3)
expresses a Boolean as an integer value. Constraints (4-8) are about Boolean (and reified
Boolean) statements. The special cases of Constraint (5) for bool array or([X1 , . . . , Xn ])
and bool array xor([X1 , . . . , Xn ]) facilitate the specification of clauses and of xor clauses
(supported directly in the CryptoMiniSAT solver by Soos, 2010). Constraint (8) specifies
that sorting a bit pair [X1 , X2 ] (decreasing order) results in the pair [X3 , X4 ]. This is a basic
building block for the construction of sorting networks (Batcher, 1968) used to encode cardinality (linear Boolean) constraints during compilation as described by Ası́n, Nieuwenhuis,
Oliveras, and Rodrı́guez-Carbonell (2011) and by Codish and Zazon-Ivry (2010). Constraints (9-14) are about integer relations and operations. Constraints (15-20) are about
linear (Boolean, Pseudo Boolean, and integer) operations. Constraints (21-26) are about
lexical orderings of Boolean and integer arrays.
A main design choice of BEE is that all integer variables are represented in the orderencoding. So, BEE is suitable for problems in which the integer variables take small or
medium sized values. The compilation of a constraint model to a CNF using BEE goes
through three phases.
1. Unary bit-blasting: integer variables (and constants) are represented as bit vectors in
the order-encoding.
2. Constraint simplification: three types of actions are applied: equi-propagation, partial
evaluation, and decomposition of constraints. Simplification is applied repeatedly until
no rule is applicable.
324

Boolean Equi-propagation

Declaring Variables
(1)
(2)
(3)

declare Boolean X
declare integer I, c1 ≤ I ≤ c2
(X ⇔ I = 1) ∧ (¬X ⇔ I = 0)

new bool(X)
new int(I, c1 , c2 )
bool2int(X, I)

op ∈ {or, and, xor, iff}

Boolean (reified) Statements
(4)
(5)
(6)
(7)
(8)

bool eq(X1 , X2 ) or bool eq(X1 , −X2 )
bool array op([X1 , . . . , Xn ])
bool array op reif([X1 , . . . , Xn ], X)
bool op reif(X1 , X2 , X)
comparator(X1 , X2 , X3 , X4 )

X1 = X2 or X1 = −X2
X1 op X2 · · · op Xn
X1 op X2 · · · op Xn ⇔ X
X1 op X2 ⇔ X
sort([X1 , X2 ]) = [X3 , X4 ]

Integer relations (reified)
rel ∈ {leq, geq, eq, lt, gt, neq}
and arithmetic
op ∈ {plus, times, div, mod, max, min}, op0 ∈ {plus, times, max, min}
(9)
(10)
(11)
(12)
(13)
(14)

int
int
int
int
int
int

rel(I1 , I2 )
rel reif(I1 , I2 , X)
array allDiff([I1 , . . . , In ])
abs(I1 , I)
op(I1 , I2 , I)
array op0 ([I1 , . . . , In ], I)

I1 rel I2
I
V1 rel I2 ⇔ X
i<j Ii 6= Ij
|I1 | = I
I1 op I2 = I
I1 op0 · · · op0 In = I

Linear Constraints
(15)
(16)
(17)
(18)
(19)
(20)

rel∈{leq, geq, eq, lt, gt}

bool array sum rel([X1 , . . . , Xn ], I)
bool array pb rel([c1 , . . . , cn ], [X1 , . . . , Xn ], I)
bool array sum modK([X1 , . . . , Xn ], c, I)
int array sum rel([I1 , . . . , In ], I)
int array lin rel([c1 , . . . , cn ], [I1 , . . . , In ], I)
int array sum modK([I1 , . . . , In ], c, I)

(Σ Xi ) rel I
(Σ ci ∗ Xi ) rel I
((Σ Xi ) mod c) = I
(Σ Ii ) rel I
(Σ ci ∗ Ii ) rel I
((Σ Ii ) mod c) = I

Lexical Order
(21)
(22)
(23)
(24)
(25)
(26)

bool arrays lex(Xs1 , Xs2 )
bool arrays lexLt(Xs1 , Xs2 )
bool arrays lex reif(Xs1 , Xs2 , X)
bool arrays lexLt reif(Xs1 , Xs2 , X)
int arrays lex(Is1 , Is2 )
int arrays lexLt(Is1 , Is2 )

Xs1 precedes (leq) Xs2 in the lex order
Xs1 precedes (lt) Xs2 in the lex order
X ⇔ Xs1 precedes (leq) Xs2 in the lex order
X ⇔ Xs1 precedes (lt) Xs2 in the lex order
Is1 precedes (leq) Is2 in the lex order
Is1 precedes (lt) Is2 in the lex order

Table 2: Syntax of BEE Constraints.
3. CNF encoding: the best suited encoding technique is applied to the simplified constraints.
Bit-blasting and equi-propagation in BEE follow the general descriptions from Sections 2.4 and 3.1. Bit-blasting is implemented through Prolog unification. Each declaration of the form new int(I, c1 , c2 ) triggers a unification I = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ] (to
ease presentation we assume that integer variables are represented in a positive interval
starting from 0 but there is no such limitation in practice as BEE also supports negatives
integers). BEE applies ad-hoc equi-propagators as described in Section 4. When an equality
of the form X = L (between a variable and a literal or a constant) is detected, then equipropagation is implemented by unifying X and L. This unification applies to all occurrences
of X and in this sense “propagates” to other constraints involving X.
Decomposition is about replacing complex constraints (for example about arrays) with
simpler constraints (for example about array elements). Consider, for instance, the constraint int array plus(As, Sum). It is decomposed to a list of int plus constraints applying
a straightforward divide and conquer recursive definition. At the base case, if As=[A] then
325

Metodi, Codish, & Stuckey

c = allDiff∗ ([Z1 , Z2 , Z3 , . . . , Zn ])
if in E
then add in µc (E)
i ∈ dom(Z1 )
i∈
/ dom(Zk ) (k > 1)

Z1 = i the authors

{i, j} ∩ dom(Zk ) = ∅
(k > 2)

dom(Z1 ) ⊆ {i, j}
dom(Z2 ) ⊆ {i, j}
Z1 6= Z2
Zk 6= i, Zk 6= j
(k > 2)

Figure 14: Simplification rules for allDiff∗ .
the constraint is replaced by a constraint of the form int eq(A,Sum) which equates the bits
of A and Sum, or if As = [A1 , A2 ] then it is replaced by int plus(A1 , A2 , Sum). In the general
case As is split into two halves, then constraints are generated to sum these halves, and
then an additional int plus constraint is introduced to sum the two sums.
As another example, consider the int plus(A1 , A2 , A) constraint. One approach, supported by BEE, decomposes the constraint as an odd-even merger (from the context of
odd-even sorting networks) (Batcher, 1968). Here, the sorted sequences of bits A1 and A2
are merged to obtain their sum A. This results in a model with O(n log n) comparator
constraints (and later in an encoding with O(n log n) clauses). Another approach, also
supported in BEE, does not decompose the constraint but encodes it directly to a CNF of
size O(n2 ), as in the context of so-called totalizers (Bailleux & Boufkhad, 2003). A hybrid
approach, leaves the choice to BEE, depending on the size of the domains of the variables
involved. Finally, we note that the user can configure BEE to fix the way it compiles this
constraint (and others).
CNF encoding is the last phase in the compilation of a constraint model. Each of the
remaining simplified (bit-blasted) constraints is encoded directly to a CNF. These encodings
are standard and similar to those applied in various tools. The BEE encodings are similar
to those applied in Sugar (Tamura et al., 2009).
6.1 The All-Different Constraint in BEE
The all-different constraint specifies that a set of integer variables take all different values
from their specified domains. This constraint has received much attention in the literature
(see for example the survey in van Hoeve, 2001). BEE provides special treatment for this
constraint.
In many applications, all-different constraints are applied to model the special case when
the constraint is about “permutation”. Namely, when [I1 , . . . , In ] are all different but may
take precisely n different values. BEE identifies this special case and applies two additional
ad-hoc equi-propagation rules for this case. The table of Figure 14 illustrates these rules.
We annotate the constraint with a “*” to emphasize that it has been detected that it is
about permutation. The first rule is about the case when only one integer (assume Z1 ) can
take the value i. The second rule is about the case where all variables except two, assume Z1 ,
Z2 , cannot take two values, assume i, j. Now, because the constraint is about permutation,
326

Boolean Equi-propagation

we can determine that Z1 and Z2 must take the two values i and j. To illustrate the second
rule consider the following example.
Example 16. Consider a constraint allDiff(I1 , . . . , I5 ) on 5 integer variables taking values in the interval [0, 4] (exactly 5 values) where E0 specifies that I3 , I4 and I5 cannot take
the values 0 and 1. Therefore we introduce equations which restrict I1 and I2 to take the
values 0 and 1, and the corresponding ad-hoc rule for permutation applies:
E0
I1
I2
I3
I4
I5



 x3,1 =1, x4,1 =1, 
x5,1 =1, x3,2 =1,
=


x4,2 =1, x5,2 =1
= [x1,1 , . . . , x1,4 ],
= [x2,1 , . . . , x2,4 ],
= [1, 1, x3,3 , x3,4 ],
= [1, 1, x4,3 , x4,4 ],
= [1, 1, x5,3 , x5,4 ]

dom(Ik ) ∩ {0, 1}=∅
k>2

−−−−−−−−−−−
−−−−→
∗
allDiff

E1
I1
I2
I3
I4
I5



 x1,2 =0, . . . , x1,4 =0, 
x2,2 =0, . . . , x2,4 =0
= E0 ∪


x1,1 =¬x2,1
= [x1,1 , 0, . . . , 0],
= [¬x1,1 , 0, . . . , 0],
= [1, 1, x3,3 , x3,4 ],
= [1, 1, x4,3 , x4,4 ],
= [1, 1, x5,3 , x5,4 ]

To facilitate the implementation of ad-hoc equi-propagation of all-different constraints,
BEE adopts a dual representation for integer variables occurring in these constraints combining the order encoding and the, so-called, direct encoding. This is essentially the
same as the encoding proposed by Gent and Nightingale (2004). When declaring an integer variable I, the bit-blast in the order encoding applies the corresponding unification
I = [x1 , . . . , xn ]. When encountering I in an allDiff constraint, an additional bit-blast
introduces I0 = [d0 , . . . , dn ] in the direct encoding, and a channeling formula channel(I, I0 )
is introduced.
The direct encoding is a unary representation I0 = [d0 , . . . , dn ] where each bit di is true
if and only if I0 = i. So, exactly one of the bits takes the value true. For example, the value
3 in the interval [0, 5] is represented in 6 bits as [0, 0, 0, 1, 0, 0]. In the dual representation
the following channeling formula captures the relation between the two representations of
an integer variable I = [x1 , . . . , xn ] and I0 = [d0 , . . . , dn ].

channel([x1 , . . . , xn ], [d0 , . . . , dn ]) =

d0 = ¬x1
∧ dn = xn


∧

n−1
^

(di ↔ xi ∧ ¬xi+1 )

i=1

Consider an allDiff constraint about m integer variables that can take different values between 0 and n. During constraint simplification, the allDiff([I1 , . . . , Im ]) constraint
is viewed through its direct encoding as a bit matrix where each row consists of the bits
[di0 , . . . , din ] for Ii in the direct encoding. The element dij is true iff Ii takes the value j.
The j th column specifies which of the Ii take the value j and hence, at most one variable
in a column may take the value true. This representation has one main advantage: in the
direct encoding we can decompose allDiff([I1 , . . . , Im ]), to a conjunction of n + 1 constraints, one for each column 0 ≤ j ≤ n, of the form bool array sum leq([d1j , . . . , dmj ], 1),
which is arc-consistent. As soon as di,j = 1 (Ii = j) we have di,j 0 = 0 (Ii 6= j 0 ) for all
2
j0 6= j. In contrast
 in the	 order encoding alone the decomposition to O(m ) constraints
int neq(Ii , Ij )  i < j
is not arc-consistent. We illustrate the advantage of the dual
encoding for the allDiff constraint in Section 8.1.
327

Metodi, Codish, & Stuckey

:- use module(bee compiler, [bCompile/2]).
:- use module(sat solver, [sat/1]).
solve(Instance, Solution) :encode(Instance, Map, Constraints),
bCompile(Constraints, CNF),
sat(CNF),
decode(Map, Solution).

Figure 15: A generic application of BEE.

7. Using BEE
A typical BEE application has the form depicted as Figure 15 where the predicate solve/2
takes a problem Instance and provides a Solution. The specifics of the application are
in the call to encode/3 which given the Instance generates the Constraints that solve
it together with a Map relating instance variables with constraint variables. The calls to
bCompile/2 and sat/1 compile the constraints to a CNF and solve it applying a SAT solver.
If the instance has a solution, the SAT solver binds the constraint variables accordingly.
Then, the call to decode/2, using the Map, provides a Solution in terms of the Instance
variables. The definitions of encode/3 and decode/2 are application dependent and provided by the user. The predicates bCompile/2 and sat/1 are part of the tool and provide
the interface to BEE and the underlying SAT solver.
7.1 Example BEE Application: Magic Graph Labeling
We illustrate the application of BEE using Prolog as a modeling language to solve a graph
labeling problem. Graph labeling is about finding an assignment of integers to the vertices
and edges of a graph subject to certain conditions. Graph labellings were introduced in
the 60’s and hundreds of papers on a wide variety of related problems have been published
since then. See for example the survey by Gallian (2011) with more than 1200 references.
Graph labellings have many applications. For instance in radars, X-ray crystallography,
coding theory, etc.
We focus here on the vertex-magic total labeling (VMTL) problem where one should
find for the graph G = (V, E) a labeling that is a one-to-one map V ∪ E → {1, 2, . . . , |V | +
|E|} with the property that the sum of the labels of a vertex and its incident edges is
a constant K independent of the choice of vertex. A problem instance takes the form
vmtl(G, K) specifying the graph G and a constant K. In the context of Figure 15, the
query solve(vmtl(G, K), Solution) poses the question: “Does there exist a vmtl labeling
for G with magic constant K?” It binds Solution to indicate such a labeling if one exists,
or to “unsat” otherwise. Figure 16 illustrates an example problem instance together with
its solution.
Figure 17 illustrates a Prolog program that implements the encode/3 predicate for the
VMTL problem. The call to predicate declareInts/4 introduces the constraints which
declare the integer variables for each vertex and edge in the graph, and generates the
map. The call to predicate sumToK/5 introduces the constraints that require the sum of
the labels for each vertex with its incident edges to equals K. The auxiliary predicate
328

Boolean Equi-propagation

An Instance
Instance = vmtl(G, K),
G = (V, E),
V = [1, 2, 3, 4],
E = [(1, 2), (1, 3),
(2, 3), (3, 4)],
K = 14

The Graph
4

2

36
 666



A Solution


1

V1
 V2

 V3
V4

= 4,
= 5,
= 1,
= 6,

E(1,2)
E(1,3)
E(2,3)
E(3,4)


= 7,
= 3, 

= 2, 
=8

Figure 16: A VMTL instance with a solution.
encode(vmtl((Vs,Es),K),Map,Constraints):append(Vs,Es,VEs), length(VEs,N),
declareInts(VEs,N,Map,Constraints-Cs2),
sumToK(Vs,Es,Map,K,Cs2-Cs3),
getVars(VEs,Map,Vars),
Cs3=[int array allDiff(Vars)].
declareInts([], , ,Cs-Cs).
declareInts([ID|IDs],N,[(ID,X)|Map],[new int(X,1,N)|CsH]-CsT):declareInts(IDs,N,Map,CsH-CsT).
sumToK([], , , ,Cs-Cs).
sumToK([VID|Vs],Es,Map,K,[int array plus(Vars,K)|CsH]-CsT):findall((X,Y),(member((X,Y),Es),(X=VID ; Y=VID)),EsIDs),
getVars([VID|EsIDs],Map,Vars),
sumToK(Vs,Es,Map,K,CsH-CsT).
getVars([], ,[]).
getVars([ID|IDs],Map,[Var|Vars]):member((ID,Var),Map),
getVars(IDs,Map,Vars).

Figure 17: encode/3 predicate for the VMTL application of BEE
The Map
((1, 2), E1 ),(1, V1 ),
((1, 3), E2 ),(2, V2 ),
((2, 3), E3 ),(3, V3 ),
((3, 4), E4 ),(4, V4 )

The Constraints
new int(V1 , 1, 8), new int(E1 , 1, 8), int array plus([V1 , E1 , E2 ], K),
new int(V2 , 1, 8), new int(E2 , 1, 8), int array plus([V2 , E1 , E3 ], K),
new int(V3 , 1, 8), new int(E3 , 1, 8), int array plus([V3 , E2 , E3 , E4 ], K),
new int(V4 , 1, 8), new int(E4 , 1, 8), int array plus([V4 , E4 ], K),
new int(K, 14, 14), allDiff([V1 , V2 , V3 , V4 , E1 , E2 , E3 , E4 ])

Figure 18: A VMTL instance with the constraints and map generated by encode/3.

getVars/3 receives a list of identifiers (vertices and edges) and extracts the corresponding
list of integer variables from the map.
Given the VMTL instance from Figure 16, the call to predicate encode/3 from Figure 17
generates the map and the constraints detailed in Figure 18.
329

Metodi, Codish, & Stuckey

Solving the constraints from Figure 18 binds the Map as follows, indicating a solution
(in unary order encoding):


(1,
 (2,
M =
(3,
(4,

[1, 1, 1, 1, 0, 0, 0, 0]),
[1, 1, 1, 1, 1, 0, 0, 0]),
[1, 0, 0, 0, 0, 0, 0, 0]),
[1, 1, 1, 1, 1, 1, 0, 0]),

((1, 2),
((1, 3),
((2, 3),
((3, 4),



[1, 1, 1, 1, 1, 1, 1, 0]),
[1, 1, 1, 0, 0, 0, 0, 0]), 
[1, 1, 0, 0, 0, 0, 0, 0]), 
[1, 1, 1, 1, 1, 1, 1, 1])

Using BEE to compile the constraints from Figure 18 generates a CNF which contains 301
clauses and 48 Boolean variables. Encoding the same set of constraints without applying
simplification rules generates a larger CNF which contains 642 clauses and 97 Boolean
variables.
In Section 8.3 we report that using BEE enables us to solve interesting instances of the
VMTL problem not previously solvable by other techniques.
7.2 BumbleBEE
The BEE distribution includes also a command line solver, which we call BumbleBEE.
BumbleBEE enables one to specify a BEE model in an input file where each line contains
a single constraint from the model and the last line specifies the type of goal. BumbleBEE
reads the input file, compiles the constraint model to CNF, solves the CNF using the
embedded CryptoMiniSAT solver (Soos, 2010) and outputs a set of bindings to the declared
variables in the model (or a message indicating that the constraints are not satisfiable).
Figure 19 contains on the left the BumbleBEE input file for the VMTL instance from
Figure 16 and on the right the BumbleBEE output, which is a solution for the constraint
model. In the example, the last line of the input file specifies the goal to the solver. The
options are:
1. solve satisfy: solve for a single satisfying assignment to the constraint model;
2. solve satisfy(c): solve for (at most) c satisfying assignments to the constraint model
where c is an integer value. When c ≤ 0 this option will solve for all solutions.
3. solve minimize(I): solve for a solution which minimizes the value of the integer
variable I. The solver outputs the intermediate solutions (with decreasing values of
I) encountered during the search for the minimum value of I.
4. solve maximize(I): similar to minimize, but maximizes.
Further details and more examples can be found in the BEE distribution (Metodi & Codish,
2012).

8. Experiments
We report on our experience in applying BEE. To appreciate the ease in its use the reader
is encouraged to view the example encodings available with the tool (Metodi & Codish,
2012). All experiments run on an Intel Core 2 Duo E8400 3.00GHz CPU with 4GB memory
under Linux (Ubuntu lucid, kernel 2.6.32-24-generic). BEE is written in Prolog and run
330

Boolean Equi-propagation

Content of BumbleBEE input file
new int(V1, 1, 8)
new int(V2, 1, 8)
new int(V3, 1, 8)
new int(V4, 1, 8)
new int(E1, 1, 8)
new int(E2, 1, 8)
new int(E3, 1, 8)
new int(E4, 1, 8)
int array plus([V1, E1, E2], 14)
int array plus([V2, E1, E3], 14)
int array plus([V3, E2, E3, E4], 14)
int array plus([V4, E4], 14)
int array allDiff([V1, V2, V3, V4, E1, E2, E3, E4])
solve satisfy

BumbleBEE output

V1 = 4
V2 = 5
V3 = 1
V4 = 6
E1 = 7
E2 = 3
E3 = 2
E4 = 8
− − − − − − − − −−
==========

Figure 19: Solving VMTL instance using BumbleBEE.

using SWI Prolog v6.0.2 64-bits. Comparisons with Sugar (v1.15.0) are based on the use of
identical constraint models, apply the same SAT solver (CryptoMiniSAT v2.5.1), and run
on the same machine. Times are reported in seconds.
8.1 Quasigroup Completion Problems
A Quasigroup Completion Problem (QCP) proposed by Gomes, Selman, and Crato (1997)
as a constraint satisfaction benchmark, is given as an n × n board of integer variables (in
the range [1, n]) in which some are assigned integer values. The task is to assign values to
all variables, so that no column or row contains the same value twice. The constraint model
is a conjunction of allDiff constraints. Ansótegui, del Val, Dotú, Fernández, and Manyà
(2004) argue the advantage of the direct encoding for QCP.
We consider 15 instances from the 2008 CSP competition.2 Table 3 considers three
settings: BEE with its dual encoding for allDiff constraints, BEE using only the order
encoding (equivalent to using int neq constraints instead of allDiff), and Sugar. The
table shows: the instance identifier (“sat” or “unsat”), compilation time (comp) in seconds,
clauses in the encoding (clauses), variables in the encoding (vars), and SAT solving time
(SAT) in seconds.
The results indicate that: (1) Application of BEE using the dual representation for
allDiff is 38 times faster and produces 20 times fewer clauses (in average) than when
using the order-encoding alone (despite the need to maintain two encodings); (2) Without
the dual representation, solving encodings generated by BEE is only slightly faster than
Sugar but BEE still generates CNF encodings 4 times smaller (on average) than those
generated by Sugar. Observe that 3 instances are found unsatisfiable by BEE (indicated
2. http://www.cril.univ-artois.fr/CPAI08/. The competition instances are specified using binary disequalities, but here we use the model with allDiff.

331

Metodi, Codish, & Stuckey

instance

25-264-0 sat
25-264-1 sat
25-264-2 sat
25-264-3 sat
25-264-4 sat
25-264-5 sat
25-264-6 sat
25-264-7 sat
25-264-8 sat
25-264-9 sat
25-264-10 unsat
25-264-11 unsat
25-264-12 unsat
25-264-13 unsat
25-264-14 unsat
Total

BEE (dual
comp clauses
(sec)
0.23 6509
0.20 7475
0.21 6531
0.21 6819
0.21 7082
0.21 7055
0.21 7712
0.21 7428
0.21 6603
0.21 6784
0.21 6491
0.12
1
0.16
1
0.12
1
0.23 5984

encoding)
vars
SAT
(sec)
1317
0.33
1508
3.29
1329
0.07
1374
0.83
1431
0.34
1431
3.12
1551
0.34
1496
0.13
1335
0.18
1350
0.19
1296
0.04
0
0.00
0
0.00
0
0.00
1210
0.07
8.93

BEE (order
comp clauses
(sec)
0.36 33224
0.30 34323
0.30 35238
0.29 32457
0.29 32825
0.30 33590
0.33 39015
0.30 36580
0.27 31561
0.27 35404
0.30 33321
0.28 37912
0.29 39135
0.29 35048
0.28 31093

encoding)
vars
SAT
(sec)
887
8.95
917
97.50
905
2.46
899
18.52
897
19.08
897
46.15
932
69.81
937
19.93
896
10.32
903
34.08
930
10.92
955
0.09
984
0.08
944
0.09
885
11.60
349.58

clauses

Sugar
vars

126733
127222
127062
127757
126777
126973
128354
127106
124153
128423
126999
125373
127539
127026
126628

10770
10798
10787
10827
10779
10784
10850
10794
10687
10853
10785
10744
10815
10786
10771

SAT
(sec)
34.20
13.93
8.06
44.03
85.92
41.04
12.67
7.01
9.69
38.80
57.75
0.47
0.57
0.56
15.93
370.63

Table 3: QCP results for 25 × 25 instances with 264 holes
by a CNF with a single clause and no variables). We comment that Sugar pre-processing
times are higher than those of BEE and not indicated in the table.
8.2 Word Design for DNA
This is Problem 033 of CSPLib which seeks the largest parameter n, such that there exists
a set S of n eight-letter words over the alphabet Σ = {A, C, G, T } with the following
properties: (1) Each word in S has exactly 4 symbols from {C, G}; (2) Each pair of
distinct words in S differ in at least 4 positions; and (3) For every x, y ∈ S: xR (the reverse
of x) and y C (the word obtained by replacing each A by T , each C by G, and vice versa)
differ in at least 4 positions.
Mancini, Micaletto, Patrizi, and Cadoli (2008) provide a comparison of several stateof-the-art solvers applied to the DNA word problem with a variety of encoding techniques.
Their best reported result is a solution with 87 DNA words, obtained in 554 seconds, using
an OPL (van Hentenryck, 1999) model with lexicographic order to break symmetry. Frutos,
Liu, Thiel, Sanner, Condon, Smith, and Corn (1997) present a strategy to solve this problem
where the four letters are modeled by bit-pairs [t, m]. Each eight-letter word can then be
viewed as the combination of a “t-part”, [t1 , . . . , t8 ], which is a bit-vector, and a “m-part”,
[m1 , . . . , m8 ], also a bit-vector. The authors report a solution composed from two pairs
of (t-part and m-part) sets3 [T1 , M1 ] and [T2 , M2 ] where |T1 | = 6, |M1 | = 16, |T2 | = 2,
|M2 | = 6. This forms a set S with (6 × 16) + (2 × 6) = 108 DNA words. Marc van Dongen
reports a larger solution with 112 words.4
Building on the approach described by Frutos et al. (1997), we pose conditions on sets
of “t-parts” and “m-parts”, T and M , so that their Cartesian product S = T × M will
satisfy the requirements of the original problem. From the three conditions below, T is
required to satisfy (10 ) and (20 ), and M is required to satisfy (20 ) and (30 ). For a set of
3. Their notions of t-part and m-part are slightly different than ours.
4. See http://www.cs.st-andrews.ac.uk/~ianm/CSPLib/.

332

Boolean Equi-propagation

bit-vectors V , the conditions are: (10 ) Each bit-vector in V sums to 4; (20 ) Each pair of
distinct bit-vectors in V differ in at least 4 positions; and (30 ) For each pair of bit-vectors
(not necessarily distinct) u, v ∈ V , uR (the reverse of u) and v C (the complement of v)
differ in at least 4 positions. This is equivalent to requiring that (uR )C differs from v in at
least 4 positions.
It is this strategy that we model in our BEE encoding. An instance takes the form
dna(n1 , n2 ) signifying the numbers of bit-vectors, n1 and n2 in the sets T and M . Without
loss of generality, we impose, to remove symmetries, that T and M are lexicographically
ordered. A solution is the Cartesian product S = T × M .
Using BEE, we find, in a fraction of a second, sets of t-parts of size 14 and m-parts of
size 8. This provides a solution of size 14 × 8 = 112 to the DNA word problem. Running
Comet (v2.0.1) we find a 112 word solution in about 10 seconds using a model by Håkan
Kjellerstrand.5 Using BEE, we also prove that there does not exist a set of 15 t-parts (0.15
seconds), nor a set of 9 m-parts (4.47 seconds). These facts were unknown prior to BEE.
Proving that there is no solution to the DNA word problem with more than 112 words,
without the restriction to the two part t-m strategy, is still an open problem.
8.3 Vertex Magic Total Labeling
MacDougall, Miller, Slamin, and Wallis (2002) conjecture that the n vertex complete graph,
Kn , for n ≥ 5 has a vertex magic total labeling with magic constants for a specific range of
values of k, determined by n. This conjecture is proved correct for all odd n and verified by
brute force for n = 6. We address the cases for n = 8 and n = 10 which involve 15 instances
(different values of k) for n = 8, and 23 (different values of k) for n = 10. Starting from
the simple constraint model (illustrated by the example in Figure 16), we add additional
constraints to exploit the fact that the graphs are symmetric: (1) We assume that the edge
with the smallest label is e1,2 ; (2) We assume that the labels of the edges incident to v1
are ordered and hence introduce constraints e1,2 < e1,3 < · · · < e1,n ; (3) We assume that
the label of edge e1,3 is smaller than the labels of the edges incident to v2 (except e1,2 ) and
introduce constraints accordingly. In this setting BEE can solve all except 2 instances with
a 4 hour timeout and Sugar can solve all except 4.
Table 4 gives results for the 10 hardest instances for K8 the 20 hardest instances for K10
with a 4 hour time-out. BEE compilation times are on the order of 0.5 sec/instance for K8
and 2.5 sec/instance for K10 . Sugar encoding times are slightly larger. The instances are
indicated by the magic constant, k; the columns for BEE and Sugar indicate SAT solving
times (in seconds). The bottom two lines indicate average encoding sizes (numbers of clauses
and variables).
The results indicate that the Sugar encodings are (in average) about 60% larger, while
the average SAT solving time for the BEE encodings is about 2 times faster (average excluding instances where Sugar times-out).
To address the two VMTL instances not solvable using the BEE models described above
(K10 with magic labels 259 and 258), we partition the problem fixing the values of e1,2
and e1,3 and maintaining all of the other constraints. Analysis of the symmetry breaking
constraints indicates that this results in 198 new instances for each of the two cases. The
5. See http://www.hakank.org/comet/word_design_dna1.co.

333

Metodi, Codish, & Stuckey

instance
BEE
K8
k
SAT (sec)
143
1.26
142
10.14
141
7.64
140
14.68
139
25.60
138
12.99
137
22.91
136
14.46
135
298.54
134
331.80
Average CNF size:
clauses
248000
vars
5688

Sugar
SAT (sec)
2.87
1.62
2.94
6.46
6.67
2.80
298.58
251.82
182.90
∞

instance
BEE
K10
k
SAT (sec)
277
5.31
276
7.11
275
13.57
274
4.93
273
45.94
272
22.74
271
7.35
270
6.03
269
5.20
268
94.44
267
88.51
266
229.80
265
1335.31
264
486.09
263
236.68
262
1843.70
261
2771.60
260
4873.99
259
∞
258
∞
Average CNF size:
clauses
1229000
vars
15529

402000
9370

Sugar
SAT (sec)
9.25
9.91
19.63
9.24
9.03
86.45
9.49
55.94
11.05
424.89
175.70
247.56
259.45
513.61
648.43
6429.25
7872.76
∞
∞
∞
1966000
25688

Table 4: VMTL results for K8 and K10 (4 hour timeout)

original VMTL instance is solved if any one of of these 198 instances is solved. So, we solve
them in parallel. Fixing e1,2 and e1,3 “fuels” the compiler so the encodings are considerably
smaller. The instance for k = 259 is solved in 1379.50 seconds where e1,2 = 1 and e1,3 = 6.
The compilation time is 2.09 seconds and the encoding consists in just over 1 million clauses
and 15 thousand variables.
To the best of our knowledge, the hard instances from this suite are beyond the reach of
all previous approaches to program the search for magic labels. The SAT based approach
presented by Jäger (2010) cannot handle these.6 The comparison with Sugar indicates the
impact of the compiler.
8.4 Balanced Incomplete Block Designs
This is Problem 028 of CSPlib (BIBD) where an instance is defined by a 5-tuple of positive
integers [v, b, r, k, λ] and requires to partition v distinct objects into b blocks such that each
block contains k different objects, exactly r objects occur in each block, and every two
distinct objects occur in exactly λ blocks.
6. Personal communication (Gerold Jäger), March 2012.

334

Boolean Equi-propagation

Figure 20: BIBD symmetry breaking.
The naive model for a BIBD instance [v, b, r, k, λ] introduces the following constraints
on a v by b Boolean incidence matrix: (1) exactly r ones in each row, (2) exactly k ones in
each column, and (3) exactly λ ones in each scalar product of two (different) rows.
This model does not contain a sufficient degree of information to trigger the equipropagation process. In order to take advantage of the BEE simplifications we added
symmetry breaking as described by Frisch, Jefferson, and Miguel (2004) and illustrated
in Figure 20: Each row is viewed as sequence of four parts A . . . D with sizes λ, (r − λ),
(r − λ), and (b − 2r + λ). The first row is fixed by assigning parts A and B with ones
(marked in black) and parts C and D with zeros (marked in white). The second row is
fixed by assign parts A and C with ones (marked in black) and parts B and D with zeros
(marked in white). For the third and all subsequent rows (marked in gray), the sum constraints are decomposed into summing each part (A . . . D) and then summing the results as
follows: A + B = λ, A + C = λ, C + D = r − λ, and B + D = r − λ. This ensures that the
row contains exactly r ones and that the scalar product with the first (and second) row is
λ. We denote this constraint model SymB (for symmetry breaking).
instance
[v, b, r, k, λ]
[7, 420, 180, 3, 60]
[7, 560, 240, 3, 80]
[12, 132, 33, 3, 6]
[15, 45, 24, 8, 12]
[15, 70, 14, 3, 2]
[16, 80, 15, 3, 2]
[19, 19, 9, 9, 4]
[19, 57, 9, 3, 1]
[21, 21, 5, 5, 1]
[25, 25, 9, 9, 3]
[25, 30, 6, 5, 1]
Total (sec)

comp
(sec)
1.65
3.73
0.95
0.51
0.56
0.81
0.23
0.34
0.02
0.64
0.10

BEE (SymB)
clauses
SAT
(sec)
698579
1.73
1211941 13.60
180238
0.73
116016
8.46
81563
0.39
109442
0.56
39931
0.09
113053
0.17
0
0.00
92059
1.33
24594
0.06
36.66

Sugar (SymB)
comp
clauses
SAT
(sec)
(sec)
12.01 2488136 13.24
11.74 2753113 36.43
83.37 1332241
7.09
4.24
466086
∞
23.58
540089
1.87
64.81
623773
2.26
2.27
125976
0.49
∞
—
—
31.91
3716
0.01
42.65
569007
8.52
16.02
93388
0.42
> 722.93

SatELite (SymB)
comp
clauses SAT
(sec)
(sec)
1.67
802576 2.18
2.73 1397188 5.18
1.18
184764 0.57
0.64
134146
∞
1.02
79542 0.20
1.14
105242 0.35
0.4
44714 0.09
10.45
111869 0.14
0.01
0 0.00
1.01
97623 8.93
1.2
23828 0.05
> 219.14

Table 5: BIBD results (180 sec. timeout)
Table 5 shows results comparing BEE (compilation time, clauses in encoding, and SAT
solving time) with Sugar using the SymB model. We also compare BEE with SatELite (Eén
335

Metodi, Codish, & Stuckey

& Biere, 2005), a CNF minimizer, where the input to SatELite is the CNF encoding for
the SymB model generated by BEE without applying any simplifications. Here compilation
time (comp) indicates the SatELite pre-processing time. The final row indicates the total
of compilation and SAT solving time over the entire suite for each approach. In all cases
time is measured in seconds.
This experiment indicates that BEE generates a significantly smaller CNF than Sugar
which affects the SAT solving time. Moreover, the Sugar compilation time is extremely
long. When comparing BEE with SatELite we can see that both output a CNF which
is similar in size but as SatELite is applied on the entire CNF, for some instances its
compilation time is significantly longer than its solving time.
instance
[v, b, r, k, λ]
[7, 420, 180, 3, 60]
[7, 560, 240, 3, 80]
[12, 132, 33, 3, 6]
[15, 45, 24, 8, 12]
[15, 70, 14, 3, 2]
[16, 80, 15, 3, 2]
[19, 19, 9, 9, 4]
[19, 57, 9, 3, 1]
[21, 21, 5, 5, 1]
[25, 25, 9, 9, 3]
[25, 30, 6, 5, 1]
Total

BEE (SymB)
comp
SAT
1.65
1.73
3.73 13.60
0.95
0.73
0.51
8.46
0.56
0.39
0.81
0.56
0.23
0.09
0.34
0.17
0.02
0.00
0.64
1.33
0.10
0.06
36.66

[M’06]
0.54
0.66
5.51
∞
12.22
107.43
53.23
∞
1.26
∞
∞
>900.00

Minion
SymB
1.36
1.77
∞
∞
1.42
13.40
38.30
1.71
0.67
∞
1.37
>600.00

SymB+
0.42
0.52
1.76
75.87
0.31
0.35
0.31
0.35
0.15
0.92
0.31
81.24

Table 6: BIBD results, comparison with Minion (times in seconds; 180 sec. timeout).
Table 6 shows results comparing BEE using the SymB model with the Minion constraint
solver (Gent, Jefferson, & Miguel, 2006). We consider three different models for Minion:
[M’06] indicates results using the BIBD model described by Gent et al. (2006), SymB uses
the same model we use for the SAT approach, SymB+ , is an enhanced symmetry breaking
model with all of the tricks applied also in the [M’06] model. For the columns with no
timeouts we show total times (for BEE this includes compile time and SAT solving). Note
that by using a clever modeling of the problem we have improved also the previous run-times
for Minion.
This experiment indicates that BEE is significantly faster than Minion on its BIBD
models ([M’06]). Only when tailoring our SymB model, does Minion becomes competitive
with ours.
8.5 Combining BEE with SatELite
We now demonstrate the impact of combining BEE and SatELite. We describe experiments involving two of the benchmarks where SatELite is applied to simplify the output of
BEE. The idea is to first apply the more powerful, but local, techniques, performed by BEE.
This reduces the size of the CNF and is fast. Then we apply SatELite which takes global
considerations on the CNF as a whole. We wish to determine if the smaller, simplified,
336

Boolean Equi-propagation

CNF is more amenable to further simplification using SatELite. The results indicate that
although CNF size is slightly decreased, solving times are most often increased, sometimes
drastically.
Tables 7 and 8 show our results. In both tables the four columns under the BEE
heading indicate: BEE compilation time, size of the encoding (clauses and variables), and the
subsequent SAT solving time. Similarly, the four columns under the ∆ SatELite heading
indicate the application of SatELite to the output of BEE: the SatELite processing time,
the size of the resulting CNF (clauses and variables), and the subsequent SAT solving time.
Table 7 illustrates the results for the BIBD benchmark of Section 8.4 and Table 8, the results
for the 10 hardest VMTL instances for K8 and for K10 described in Section 8.3. Observe
that applying SatELite to the output of BEE decreases the CNF size only slightly and does
not improve the SAT solving time. In fact, to the contrary, in most cases it renders a CNF
which takes more time to solve. In several cases, SAT solving time increases drastically to
introduce a timeout.
instance
[v, b, r, k, λ]
[7, 420, 180, 3, 60]
[7, 560, 240, 3, 80]
[12, 132, 33, 3, 6]
[15, 45, 24, 8, 12]
[15, 70, 14, 3, 2]
[16, 80, 15, 3, 2]
[19, 19, 9, 9, 4]
[19, 57, 9, 3, 1]
[21, 21, 5, 5, 1]
[25, 25, 9, 9, 3]
[25, 30, 6, 5, 1]

comp
(sec)
1.65
3.73
0.95
0.51
0.56
0.81
0.23
0.34
0.02
0.64
0.10

BEE
clauses
vars
698579
1211941
180238
116016
81563
109442
39931
113053
0
92059
24594

41399
58445
31947
19507
19693
26223
9273
6576
0
22098
2160

SAT
(sec)
1.73
13.60
0.73
8.46
0.39
0.56
0.09
0.17
0.00
1.33
0.06

comp
(sec)
1.88
3.14
1.20
0.66
0.98
1.13
0.38
12.49
0.00
0.97
1.14

∆ SatELite
clauses
vars
696914
1209788
179700
115938
78630
104760
39805
112314
0
91736
24028

38749
54043
28351
17642
15877
21116
7988
6230
0
18540
1926

SAT
(sec)
3.41
6.97
0.91
∞
0.35
0.50
0.16
0.37
0.00
10.34
0.09

Table 7: BIBD results, BEE combined with SatELite (180 sec. timeout)
Our results demonstrate that the application of SatELite to remove redundancies
from a CNF is often non-beneficial. Presumably the difference we see from our application
of SatELite to other CNF benchmarks results from the fact that BEE produces highly
optimized CNF output, while many CNF benchmarks have significant inefficiency in their
original encoding. If BEE removes a variable from the CNF, then it also instantiates that
variable, either to a constant or to an equivalent variable, and as such does not remove
potential propagations from the encoding, as captured by Theorem 9.

9. Conclusion
There is a considerable body of work on CNF simplification techniques with a clear trade-off
between amount of reduction achieved and invested time. Most of these approaches determine binary clauses implied by the CNF, which is certainly enough to determine Boolean
equalities. The problem is that determining all binary clauses implied by the CNF is
prohibitive when the SAT model may involve many (hundreds of) thousands of variables.
337

Metodi, Codish, & Stuckey

instance

K8

K10

143
142
141
140
139
138
137
136
135
134
267
266
265
264
263
262
261
260
259
258

comp
(sec)
0.51
0.27
0.20
0.19
0.18
0.18
0.18
0.18
0.18
0.18
0.65
0.65
0.65
0.65
0.65
0.65
0.65
0.65
0.65
0.65

BEE
clauses
vars
248558
248414
248254
248078
247886
247678
247454
247214
246958
246686
1228962
1228660
1228338
1227996
1227634
1227252
1226850
1226428
1225986
1225524

5724
5716
5708
5700
5692
5684
5676
5668
5660
5652
15529
15529
15529
15529
15529
15529
15529
15529
15529
15529

SAT
(sec)
1.26
10.14
7.64
14.68
25.6
12.99
22.91
14.46
298.54
331.8
88.51
229.8
1335.31
486.09
236.68
1843.7
2771.6
4873.99
∞
∞

comp
(sec)
2.60
2.59
2.59
2.60
2.59
2.60
2.59
2.59
2.58
2.59
3.02
3.01
3.02
3.02
3.01
3.02
3.04
3.02
3.03
3.01

∆ SatELite
clauses
vars
248250
248107
247947
247771
247579
247371
247147
246907
246651
246379
1228368
1228066
1227744
1227402
1227040
1226658
1226256
1225834
1225392
1224930

5452
5445
5437
5429
5421
5413
5405
5397
5389
5381
14990
14990
14990
14990
14990
14990
14990
14990
14990
14990

SAT
(sec)
0.98
3.22
32.81
3.50
6.18
12.18
77.16
97.69
705.48
∞
430.00
259.55
540.48
63.74
1008.06
1916.73
∞
∞
∞
∞

Table 8: VTML results, BEE combined with SatELite (4 hour timeout)

Typically only some of the implied binary clauses are determined, such as those visible by
unit propagation. The trade-off is regulated by the choice of the techniques applied to infer
binary clauses, considering the power and cost. See for example the work of Eén and Biere
(2005) and the references therein. There are also approaches (Li, 2003) that detect and use
Boolean equalities during run-time, which are complementary to our approach.
In our approach, the beast is tamed by introducing a notion of locality. We do not
consider the full CNF. Instead, by maintaining the original representation, a conjunction of
constraints, each viewed as a Boolean formula, we can apply powerful reasoning techniques
to separate parts of the model and maintain efficient pre-processing.
To this end, we introduce BEE, a compiler that follows this approach to encode finite
domain constraints to CNF. Applying optimizations based on ad-hoc equi-propagation and
partial evaluation rules on a high level view of the problem allows us to simplify the problem
more aggressively than is possible with a CNF representation. The resulting CNF models
can be significantly smaller than those resulting from straight translation.
It is well-understood that making a CNF smaller is not the ultimate goal: often smaller
CNF’s are harder to solve. Indeed, one often introduces redundancies to improve SAT
encodings: so removing them is counterproductive. Our experience is that BEE reduces
the size of an encoding in a way that is productive for the subsequent SAT solving. In
particular, by removing variables that can be determined “at compile time” to be definitely
equal (or definitely different) in any solution.
338

Boolean Equi-propagation

BEE uses ad-hoc equi-propagation and partial evaluation rules which keeps compilation
times typically small (measured in seconds) even for instances which result in several millions
of CNF clauses. And the reduction in SAT solving time can be larger by orders of magnitude.
Hence, we believe that Boolean equi-propagation makes an important contribution to the
encoding of CSPs to SAT.
BEE is currently tuned to represent integers in the order encoding. Ongoing work
aims to extend BEE for binary and additional number representations such as mixed radix
bases as considered by Eén and Sörensson (2006) and further by Codish, Fekete, Fuhs, and
Schneider-Kamp (2011).
Acknowledgments
We thank Vitaly Lagoon for the many insightful discussions concerning this research.
NICTA is funded by the Australian Government as represented by the Department of
Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT Centre of Excellence Program.

References
Ansótegui, C., del Val, A., Dotú, I., Fernández, C., & Manyà, F. (2004). Modeling choices in
quasigroup completion: SAT vs. CSP. In McGuinness, D. L., & Ferguson, G. (Eds.),
AAAI, pp. 137–142, San Jose, California, USA. AAAI Press / The MIT Press.
Ası́n, R., Nieuwenhuis, R., Oliveras, A., & Rodrı́guez-Carbonell, E. (2011). Cardinality
networks: a theoretical and empirical study. Constraints, 16 (2), 195–221.
Bagnara, R., & Schachte, P. (1998). Factorizing equivalent variable pairs in ROBDDbased implementations of Pos. In Haeberer, A. M. (Ed.), Algebraic Methodology and
Software Technology, 7th International Conference, AMAST ’98, Amazonia, Brasil,
January 4-8, 1999, Proceedings, Vol. 1548 of Lecture Notes in Computer Science, pp.
471–485.
Bailleux, O., & Boufkhad, Y. (2003). Efficient CNF encoding of Boolean cardinality constraints. In Rossi, F. (Ed.), CP, Vol. 2833 of LNCS, pp. 108–122, Kinsale, Ireland.
Springer.
Barrett, C., Stump, A., & Tinelli, C. (2010). The Satisfiability Modulo Theories Library
(SMT-LIB). www.SMT-LIB.org.
Batcher, K. E. (1968). Sorting networks and their applications. In AFIPS Spring Joint
Computing Conference, Vol. 32 of AFIPS Conference Proceedings, pp. 307–314, Atlantic City, NJ, USA. Thomson Book Company, Washington D.C.
Bessiere, C., Katsirelos, G., Narodytska, N., & Walsh, T. (2009). Circuit complexity and
decompositions of global constraints. In Proceedings of IJCAI 2009, pp. 412–418.
Cadoli, M., & Schaerf, A. (2005). Compiling problem specifications into SAT. Artificial
Intelligence, 162 (1-2), 89–120.
339

Metodi, Codish, & Stuckey

Codish, M., Fekete, Y., Fuhs, C., & Schneider-Kamp, P. (2011). Optimal base encodings
for pseudo-Boolean constraints. In Abdulla, P. A., & Leino, K. R. M. (Eds.), TACAS,
Vol. 6605 of Lecture Notes in Computer Science, pp. 189–204. Springer.
Codish, M., Lagoon, V., & Stuckey, P. J. (2008). Logic programming with satisfiability.
TPLP, 8 (1), 121–128.
Codish, M., & Zazon-Ivry, M. (2010). Pairwise cardinality networks. In Clarke, E. M., &
Voronkov, A. (Eds.), LPAR (Dakar), Vol. 6355 of Lecture Notes in Computer Science,
pp. 154–172. Springer.
Coudert, O., & Madre, J. C. (1990). A unified framework for the formal verification of
sequential circuits. In ICCAD, pp. 126–129.
Crawford, J. M., & Baker, A. B. (1994). Experimental results on the application of satisfiability algorithms to scheduling problems. In Hayes-Roth, B., & Korf, R. E. (Eds.),
AAAI, Vol. 2, pp. 1092–1097, Seattle, WA, USA. AAAI Press / The MIT Press.
Eén, N., & Biere, A. (2005). Effective preprocessing in SAT through variable and clause
elimination. In Bacchus, F., & Walsh, T. (Eds.), SAT, Vol. 3569 of Lecture Notes in
Computer Science, pp. 61–75. Springer.
Eén, N., & Sörensson, N. (2003). An extensible SAT-solver. In Giunchiglia, E., & Tacchella, A. (Eds.), SAT, Vol. 2919 of Lecture Notes in Computer Science, pp. 502–518.
Springer.
Eén, N., & Sörensson, N. (2006). Translating pseudo-Boolean constraints into SAT. JSAT,
2 (1-4), 1–26.
Frisch, A. M., Jefferson, C., & Miguel, I. (2004). Symmetry breaking as a prelude to implied
constraints: A constraint modeling pattern. In Proc. 16th Euro. Conf. on AI, 171175,
pp. 171–175. Press.
Frutos, A. G., Liu, Q., Thiel, A. J., Sanner, A. M. W., Condon, A. E., Smith, L. M., &
Corn, R. M. (1997). Demonstration of a word design strategy for DNA computing on
surfaces. Journal of Nucleic Acids Research, 25 (23), 4748–4757.
Gallian, J. A. (2011). A dynamic survey of graph labeling. The Electronic Journal of
Combinatorics, 18.
Gavanelli, M. (2007). The log-support encoding of CSP into SAT. In Bessiere, C. (Ed.),
CP, Vol. 4741 of LNCS, pp. 815–822, Providence, RI, USA. Springer.
Gelder, A. V. (2005). Toward leaner binary-clause reasoning in a satisfiability solver. Ann.
Math. Artif. Intell., 43 (1), 239–253.
Gent, I. P., Jefferson, C., & Miguel, I. (2006). Minion: A fast scalable constraint solver.
In Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), ECAI, Vol. 141 of
Frontiers in Artificial Intelligence and Applications, pp. 98–102. IOS Press.
Gent, I. P., & Nightingale, P. (2004). A new encoding of alldifferent into SAT. Proceedings
of the 3rd International Workshop on Modeling and Reformulating Constraint Satisfaction Problems, http://www-users.cs.york.ac.uk/frisch/Reformulation/04/
proceedings.pdf.
340

Boolean Equi-propagation

Gomes, C. P., Selman, B., & Crato, N. (1997). Heavy-tailed distributions in combinatorial
search. In Smolka, G. (Ed.), CP, Vol. 1330 of LNCS, pp. 121–135. Springer.
Heule, M., Järvisalo, M., & Biere, A. (2011). Efficient CNF simplification based on binary
implication graphs. In Sakallah, K. A., & Simon, L. (Eds.), SAT, Vol. 6695 of Lecture
Notes in Computer Science, pp. 201–215. Springer.
Huang, J. (2008). Universal Booleanization of constraint models. In CP2008, Vol. 5202 of
Lecture Notes in Computer Science, pp. 144–158.
Jäger, G. (2010). An effective SAT encoding for magic labeling. In Faigle, U., Schrader, R.,
& Herrmann, D. (Eds.), CTW, pp. 97–100.
Li, C. (2003). Equivalent literal propagation in the DLL procedure. Discrete Applied
Mathematics, 130 (2), 251–276.
MacDougall, J., Miller, M., Slamin, M., & Wallis, W. (2002). Vertex-magic total labelings
of graphs. Utilitas Mathematica, 61, 3–21.
Mancini, T., Micaletto, D., Patrizi, F., & Cadoli, M. (2008). Evaluating ASP and commercial
solvers on the CSPLib. Constraints, 13 (4), 407–436.
Manthey, N. (2012). Coprocessor 2.0 - a flexible CNF simplifier - (tool presentation). In
Cimatti, A., & Sebastiani, R. (Eds.), SAT, Vol. 7317 of Lecture Notes in Computer
Science, pp. 436–441. Springer.
Metodi, A. (2012). BEE. http://amit.metodi.me/research/bee/.
Metodi, A., & Codish, M. (2012). Compiling finite domain constraints to SAT with BEE.
TPLP, 12 (4-5), 465–483.
Metodi, A., Codish, M., Lagoon, V., & Stuckey, P. J. (2011). Boolean equi-propagation for
optimized SAT encoding. In Lee, J. H.-M. (Ed.), CP, Vol. 6876 of LNCS, pp. 621–636.
Springer.
Nethercote, N., Stuckey, P. J., Becket, R., Brand, S., Duck, G. J., & Tack, G. (2007).
Minizinc: Towards a standard CP modeling language. In Bessiere, C. (Ed.), CP2007,
Vol. 4741 of Lecture Notes in Computer Science, pp. 529–543, Providence, RI, USA.
Springer-Verlag.
Somenzi, F. (2009). CUDD: Colorado University Decision Diagram package. (Online,
accessed 13 April 2011). http://vlsi.colorado.edu/~fabio/CUDD/.
Soos, M. (2010). CryptoMiniSAT, v2.5.1. http://www.msoos.org/cryptominisat2.
Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling finite linear CSP
into SAT. Constraints, 14 (2), 254–272.
Tarjan, R. (1975). Efficiency of a good but not linear set union algorithm. JACM, 22 (2),
215–225.
van Hentenryck, P. (1999). The OPL Optimization Programming Language. MIT Press.
van Hoeve, W. J. (2001). The alldifferent constraint: A survey.. CoRR:http://arxiv.org/
abs/cs.PL/0105015.

341

Journal of Artificial Intelligence Research 46 (2013) 89-127

Submitted 03/12; published 01/13

Automatic Aggregation by Joint Modeling
of Aspects and Values
Christina Sauper
Regina Barzilay

csauper@csail.mit.edu
regina@csail.mit.edu

Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
32 Vassar St.
Cambridge, MA 02139 USA

Abstract
We present a model for aggregation of product review snippets by joint aspect identification and sentiment analysis. Our model simultaneously identifies an underlying set of
ratable aspects presented in the reviews of a product (e.g., sushi and miso for a Japanese
restaurant) and determines the corresponding sentiment of each aspect. This approach
directly enables discovery of highly-rated or inconsistent aspects of a product. Our generative model admits an efficient variational mean-field inference algorithm. It is also easily
extensible, and we describe several modifications and their effects on model structure and
inference. We test our model on two tasks, joint aspect identification and sentiment analysis on a set of Yelp reviews and aspect identification alone on a set of medical summaries.
We evaluate the performance of the model on aspect identification, sentiment analysis,
and per-word labeling accuracy. We demonstrate that our model outperforms applicable
baselines by a considerable margin, yielding up to 32% relative error reduction on aspect
identification and up to 20% relative error reduction on sentiment analysis.

1. Introduction
Online product reviews have become an increasingly valuable and influential source of information for consumers. The ability to explore a range of opinions allows consumers to both
form a general opinion of a product and gather information about its positive and negative
aspects (e.g., packaging or battery life). However, as more reviews are added over time, the
problem of information overload gets progressively worse. For example, out of hundreds of
reviews for a restaurant, most consumers will read only a handful before making a decision.
In this work, our goal is to summarize a large number of reviews by discovering the most
informational product aspects and their associated user sentiment.
To address this need, online retailers often use simple aggregation mechanisms to represent the spectrum of user sentiment. Many sites, such as Amazon, simply present a
distribution over user-assigned star ratings, but this approach lacks any reasoning about
why the products are given that rating. Some retailers use further breakdowns by specific
predefined domain-specific aspects, such as food, service, and atmosphere for a restaurant.
These breakdowns continue to assist in effective aggregation; however, because the aspects
are predefined, they are generic to the particular domain and there is no further explanation of why one aspect was rated well or poorly. Instead, for truly informative aggregation,
c
2013
AI Access Foundation. All rights reserved.

Sauper & Barzilay

each product needs to be assigned a set of fine-grained aspects specifically tailored to that
product.
The goal of our work is to provide a mechanism for effective unsupervised content
aggregation able to discover specific, fine-grained aspects and associated values. Specifically,
we represent each data set as a collection of entities; for instance, these can represent
products in the domain of online reviews. We are interested in discovering fine-grained
aspects of each entity (e.g., sandwiches or dessert for a restaurant). Additionally, we would
like to recover a value associated with the aspect (e.g., sentiment for product reviews). A
summary of the input and output can be found in Figure 1. Our input consists of short
text snippets from multiple reviews for each of several products. In the restaurant domain,
as in Figure 1, these are restaurants. We assume that each snippet is opinion-bearing
and discusses one of the aspects which are relevant for that particular product. Our output
consists of a set of dynamic (i.e., not pre-specified) aspects for each product, snippets labeled
with the aspect which they discuss, and sentiment values for each snippet individually and
each aspect as a whole. In Figure 1, the aspects identified for Tasca Spanish Tapas include
chicken, dessert, and drinks, and the snippets are labeled with the aspects they describe
and the correct polarity.
One way to approach this problem is to treat it as a multi-class classification problem.
Given a set of predefined domain-specific aspects, it would be fairly straightforward for
humans to identify which aspect a particular snippet describes. However, for our task
of discovering fine-grained entity-specific aspects, there is no way to know a priori which
aspects may be present across the entire data set or to provide training data for each; instead,
we must select the aspects dynamically. Intuitively, one potential solution is to cluster the
input snippets, grouping those which are lexically similar without prior knowledge of the
aspects they represent. However, without some knowledge of which words represent the
aspect for a given snippet, the clusters may not align to ones useful for cross-review analysis.
Consider, for example, the two clusters of restaurant review snippets shown in Figure 2.
While both clusters share many words among their members, only the first describes a
coherent aspect cluster, namely the drinks aspect. The snippets of the second cluster do
not discuss a single product aspect, but instead share expressions of sentiment.
To successfully navigate this challenge, we must distinguish between words which indicate aspect, words which indicate sentiment, and extraneous words which do neither. For
both aspect identification and sentiment analysis, it is crucial to know which words within
a snippet are relevant for the task. Distinguishing them is not straightforward, however.
Some work in sentiment analysis relies on a predefined lexicon or WordNet to provide some
hints, but there is no way to anticipate every possible expression of aspect or sentiment,
especially in user-generated data (e.g., use of slang such as “deeeeeee-lish” for “delicious”).
In lieu of an explicit lexicon, we can attempt to use other information as a proxy, such as
part of speech; for example, aspect words are likely to be nouns, while value words are more
likely to be adjectives. However, as we show later in this paper, this additional information
is again not sufficient for the tasks at hand.
Instead, we propose an approach to analyze a collection of product review snippets and
jointly induce a set of learned aspects, each with a respective value (e.g., sentiment). We
capture this idea using a generative Bayesian topic model where the set of aspects and
any corresponding values are represented as hidden variables. The model takes a collection
90

Automatic Aggregation by Joint Modeling of Aspects and Values

Input

Output

Tasca Spanish Tapas

Tasca Spanish Tapas

Review 1
The chicken was cooked perfectly
The dessert was good

Chicken
+ The chicken was cooked perfectly
− The chicken was tough and not tasty
+ Moist and delicious chicken

Review 2
The red wine’s not too cheap
An excellent creme brulee
Review 3
They used frozen small shrimp
The chicken was tough and not tasty
Pitcher sangria was pretty good
···
Douzo Sushi Bar
Review 1
The sushi is creative and pretty good
The ponzu was overpowering
Review 2
Real wasabi that’s so fresh!
My torched roll tasted rather bland
···

Dessert
+ The dessert was good
+ An excellent creme brulee

	

···

Drinks
− The red wine’s not too cheap
+ Pitcher sangria was pretty good
···
Douzo Sushi Bar
Sushi
+ The sushi is creative and pretty good
− My torched roll tasted rather bland
Condiments
− The ponzu was overpowering
+ Real wasabi that’s so fresh!
···
···

Figure 1: An example of the desired input and output of our system in the restaurant
domain. The input consists of a collection of review snippets for several restaurants. The
output is an aggregation of snippets by aspect (e.g., chicken and dessert) along with an
associated sentiment for each snippet. Note that the input data is completely unannotated;
the only information given is which snippets describe the same restaurant.

of snippets as input and explains how the observed text arises from the latent variables,
thereby connecting text fragments with the corresponding aspects and values.
Specifically, we begin by defining sets of sentiment word distributions and aspect word
distributions. Because we expect the types of sentiment words to be consistent across all
products (e.g., any product may be labeled as “great” or “terrible”), we allow the positive
and negative sentiment word distributions to be shared across all products. On the other
hand, in the case of restaurant reviews and similar domains, aspect words are expected to
be quite distinct between products. Therefore, we assign each product its own set of aspect
word distributions. In addition to these word distributions, our model takes into account
several other factors. First, we model the idea that each particular aspect of a product has
some underlying quality; that is, if there are already 19 snippets praising a particular aspect,
it’s likely that the 20th snippet will be positive as well. Second, we account for common
patterns in language using a transition distribution between types of words. For example,
it is very common to see the pattern “Value Aspect,” such as in phrases like “great pasta.”
Third, we model the distributions over parts of speech for each type of distribution. This
91

Sauper & Barzilay

Coherent aspect cluster
+

The martinis
were very good.
:::::::::
The drinks
both
wine and :::::::::
martinis - were tasty.
:::::::
:::::

-

The wine
list was pricey.
:::::::::
Their :::::
wine:::::::::::
selection is horrible.

Incoherent aspect cluster

+

The sushi is the :::::
best :::::
I’ve :::::
ever:::::
had.
Best
paella
I’d
ever
had.
:::::
::::::::::::::
The fillet was the best
steak we’d ever had.
:::::::::::::::::::::::::::
It’s the best
soup
I’ve
ever had.
::::::::::::::::::::::::::

Figure 2: Example clusters of restaurant review snippets generated by a lexical clustering
algorithm; words relevant to clustering are highlighted. The first cluster represents a coherent aspect of the underlying product, namely the drinks aspect. The latter cluster simply
shares a common sentiment expression and does not represent snippets discussing the same
product aspect. In this work, we aim to produce the first type of aspect cluster along with
the corresponding values.

covers the intuition that aspect words are frequently nouns, whereas value words are often
adjectives. We describe each of these factors and our model as a whole in detail in Section 4.
This formulation provides several advantages: First, the model does not require a set
of predefined aspects. Instead, it is capable of assigning latent variables to discover the
appropriate aspects based on the data. Second, the joint analysis of aspect and value
allows us to leverage several pieces of information to determine which words are relevant
for aspect identification and which should be used for sentiment analysis, including part
of speech and global or entity-specific distributions of words. Third, the Bayesian model
admits an efficient mean-field variational inference procedure which can be parallelized and
run quickly on even large numbers of entities and snippets.
We evaluate our approach on the domain of restaurant reviews. Specifically, we use a set
of snippets automatically extracted from restaurant reviews on Yelp. This collection consists
of an average of 42 snippets for each of 328 restaurants in the Boston area, representing
a wide spectrum of opinions about several aspects of each restaurant. We demonstrate
that our model can accurately identify clusters of review fragments that describe the same
aspect, yielding 32.5% relative error reduction (9.9 absolute F1 ) over a standalone clustering
baseline. We also show that the model can effectively identify snippet sentiment, with a
19.7% relative error reduction (4.3% absolute accuracy) over applicable baselines. Finally,
we test the model’s ability to correctly label aspect and sentiment words, discovering that
the aspect identification has high-precision, while the sentiment identification has highrecall.
Additionally, we apply a slimmed-down version of our model which focuses exclusively
on aspect identification to a set of lab- and exam-related snippets from medical summaries
provided by the Pediatric Environmental Health Clinic (PEHC) at Children’s Hospital
Boston. These summaries represent concise overviews of the patient information at a par92

Automatic Aggregation by Joint Modeling of Aspects and Values

ticular visit, as relayed from the PEHC doctor to the child’s referring physician. Our model
achieves 7.4% (0.7 absolute F1 ) over the standalone clustering baseline.
The remainder of this paper is structured as follows. Section 2 compares our work with
previous work on both aspect identification and sentiment analysis. Section 3 describes our
specific problem formulation and task setup more concretely. Section 4 presents the details
of our full model and various model extensions, and Section 5 describes the inference procedure and the necessary adjustments for each extension. The details of both data sets, the
experimental formulation, and results are presented in Section 6. We summarize our findings
and consider directions for future work in Section 7. The code and data used in this paper
are available online at http://groups.csail.mit.edu/rbg/code/review-aggregation.

2. Related Work
Our work falls into the area of multi-aspect sentiment analysis. In this section, we first
describe approaches toward document-level and sentence-level sentiment analysis (Section
2.1), which provide the foundation for future work, including our own. Then, we describe
three common directions of multi-aspect sentiment analysis; specifically, those which use
data-mining or fixed-aspect analysis (Section 2.2.1), those which incorporate sentiment
analysis with multi-document summarization (Section 2.2.2), and finally, those focused on
topic modeling with additional sentiment components (Section 2.2.3).
2.1 Single-Aspect Sentiment Analysis
Early sentiment analysis focused primarily on identification of coarse document-level sentiment (Pang, Lee, & Vaithyanathan, 2002; Turney, 2002; Pang & Lee, 2008). Specifically,
these approaches attempted to determine the overall polarity of documents. These approaches included both rule-based and machine learning approaches: Turney (2002) used
a rule-based method to extract potentially sentiment-bearing phrases and then compared
them to the sentiment of known-polarity words, while Pang et al. (2002) used discriminative
methods with features such as unigrams, bigrams, part-of-speech tags, and word position
information.
While document-level sentiment analysis can give us the overall view of an opinion, looking at individual sentences within the document yields a more fine-grained analysis. The
work in sentence-level sentiment analysis focuses on first identifying sentiment-bearing sentences and then determining their polarity (Yu & Hatzivassiloglou, 2003; Dave, Lawrence,
& Pennock, 2003; Kim & Hovy, 2005, 2006; Pang & Lee, 2008). Both identification of
sentiment-bearing sentences and polarity analysis can be performed through supervised
classifiers (Yu & Hatzivassiloglou, 2003; Dave et al., 2003) or similarity to known text (Yu
& Hatzivassiloglou, 2003; Kim & Hovy, 2005), through measures based on distributional
similarity or by using WordNet relationships.
By recognizing connections between parts of a document, sentiment analysis can be
further improved (Pang & Lee, 2004; McDonald, Hannan, Neylon, Wells, & Reynar, 2007;
Pang & Lee, 2008). Pang and Lee (2004) leverage the relationship between sentences to
improve document-level sentiment analysis. Specifically, they utilize both the subjectivity
of individual sentences and information about the strength of connection between sentences
in a min cut formulation to provide better sentiment-focused summaries of text. McDonald
93

Sauper & Barzilay

et al. (2007) examine a different connection, instead constructing a hierarchical model of
sentiment between sentences and documents. Their model uses complete labeling on a
subset of data to learn a generalized set of parameters which improve classification accuracy
at both document-level and sentence-level.
While none of the above approaches attempt to identify aspects or analyze sentiment in
an aspect-based fashion, the intuitions provide key insight into the approaches we take in
our work. For example, the importance of distinguishing opinion sentences follows our own
intuition about the necessity of identifying sentiment-bearing words within a snippet.
2.2 Aspect-Based Sentiment Analysis
Following the work in single-aspect document-level and sentence-level sentiment analysis
came the intuition of modeling aspect-based (also called “feature-based”) sentiment for review analysis. We can divide these approaches roughly into three types of systems based on
their techniques: systems which use fixed-aspect approaches or data-mining techniques for
aspect selection or sentiment analysis, systems which adapt techniques from multi-document
summarization, and systems which jointly model aspect and sentiment with probabilistic
topic models. Here, we examine each avenue of work with relevant examples and contrast
them with our own work.
2.2.1 Data-Mining and Fixed-Aspect Techniques for Sentiment Analysis
One set of approaches toward aspect-based sentiment analysis follow the traditional techniques of data mining (Hu & Liu, 2004; Liu, Hu, & Cheng, 2005; Popescu, Nguyen, &
Etzioni, 2005). These systems may operate on full documents or on snippets, and they generally require rule-based templates or additional resources such as WordNet both to identify
aspects and to determine sentiment polarity. Another approach is to fix a predetermined
relevant set of aspects, then focus on learning the optimal opinion assignment for these
aspects (Snyder & Barzilay, 2007). Below, we summarize each approach and compare and
contrast them to our work.
One set of work relies on a combination of association mining and rule-based extraction
of nouns and noun phrases for aspect identification. Hu and Liu (2004) and Liu et al. (2005)
developed a three-step system: First, initial aspects are selected by an association miner
and pruned by a series of rules. Second, related opinions for each aspect are identified in
a rule-based fashion using word positions, and their polarity is determined by WordNet
search based on a set of seed words. Third, additional aspects are identified in a similar
fashion based on position of the selected polarity words. In each of these steps, part-ofspeech information provides a key role in the extraction rules. In the later work, there is
an additional component to identify implicit aspects in a deterministic fashion; e.g., heavy
maps deterministically to <weight> (Liu et al., 2005). While their task is similar to ours
and we utilize part-of-speech information as an important feature as well, we additionally
leverage other distributional information to identify aspects and sentiment. Furthermore,
we avoid the reliance on WordNet and predefined rule mappings in order to preserve the
generality of the system. Instead, our joint modeling allows us to recover these relationships
without the need for additional information.
94

Automatic Aggregation by Joint Modeling of Aspects and Values

Other approaches also rely on WordNet relationships to identify not only sentiment
polarity, but also aspects, using the parts and properties of a particular product class.
Popescu et al. (2005) first use these relations to generate the set of aspects for a given
product class (e.g., camera). Following that, they apply relaxation labeling for sentiment
analysis. This procedure gradually expands sentiment from individual words to aspects to
sentences, similar to the Cascade pattern mentioned in the work of McDonald et al. (2007).
Like the system of Liu et al. (2005), their system requires a set of manual rules and several
outside resources. While our model does require a few seed words, it does not require any
manual rules or additional resources due to its joint formulation.
A separate direction of work relies on predefined aspects while focusing on improvement
of sentiment analysis prediction. Snyder and Barzilay (2007) define a set of aspects specific
to the restaurant domain. Specifically they define an individual rating model for each aspect,
plus an overall agreement model which attempts to determine whether the resulting ratings
should all agree or disagree. These models are jointly trained in a supervised fashion using
an extension of the PRanking algorithm (Crammer & Singer, 2001) to find the best overall
star rating for each aspect. Our problem formulation differs significantly from their work
in several dimensions: First, we desire a more refined analysis using fine-grained aspects
instead of coarse predefined features. Second, we would like to use as little supervised
training data as possible, rather than the supervised training required for the PRanking
algorithm.
In our work, we attempt to capture the intuitions of these approaches while reducing the
need for outside resources and rule-based components. For example, rather than supplying
rule-based patterns for extraction of aspect and sentiment, we instead leverage distributional patterns across the corpus to infer the relationships between words of different types.
Likewise, rather than relying on WordNet relationships such as synonymy, antonymy, hyponymy, or hypernymy (Hu & Liu, 2004; Liu et al., 2005; Popescu et al., 2005), we bootstrap
our model from a small set of seed words.
2.2.2 Multi-Document Summarization and its Application to Sentiment
Analysis
Multi-document summarization techniques generally look for repetition across documents
to signal important information (Radev & McKeown, 1998; Barzilay, McKeown, & Elhadad,
1999; Radev, Jing, & Budzikowska, 2000; Mani, 2001). For aspect-based sentiment analysis,
work has focused on augmenting these techniques with additional components for sentiment
analysis (Seki, Eguchi, Kanodo, & Aono, 2005, 2006; Carenini, Ng, & Pauls, 2006; Kim &
Zhai, 2009). In general, the end goal of these approaches is the task of forming coherent text
summaries using either text extraction or natural language generation. Unlike our work,
many of these approaches do not explicitly identify aspects; instead, they are extracted
through repeated information. Additionally, our model explicitly looks at the connection
between content and sentiment, rather than treating it as a secondary computation after
information has been selected.
One technique for incorporating sentiment analysis follows previous work on identification of opinion-bearing sentences. Seki et al. (2005, 2006) present DUC summarization
95

Sauper & Barzilay

systems designed to create opinion-focused summaries of task topics.1 In their system, they
employ a subjectivity component using a supervised SVM with lexical features, similar to
those in the work of Yu and Hatzivassiloglou (2003) and Dave et al. (2003). This component
is used to identify subjective sentences and, in the work of Seki et al. (2006), their polarity,
both in the task and in the sentences selected for the response summary. However, like
previous work and unlike our task, there is no aspect-based analysis in their summarization
task. It is also fully supervised, relying on a hand-annotated set of about 10,000 sentences
to train the SVM.
Another line of work focuses on augmenting the summarization system with aspect
selection similar to the data-mining approaches of Hu and Liu (2004), rather than using
single-aspect analysis. Carenini, Ng, and Zwart (2005) and Carenini et al. (2006) augment
the previous aspect selection with a user-defined hierarchical organization over aspects; e.g.,
digital zoom is part of the lens. Polarity of each aspect is assumed to be given by previous
work. These aspects are then incorporated into existing summarization systems – MEAD*
sentence extraction (Radev et al., 2000) or SEA natural language generation (Carenini &
Moore, 2006) – to form final summaries. Like the work of Seki et al. (2005, 2006), this work
does not create new techniques for aspect identification or sentiment analysis; instead, they
focus on the process of integrating these sources of information with summarization systems.
While the aspects produced are comparable across reviews for a particular product, the
highly-supervised nature means that this approach is not feasible for a large set of products
such as our corpus of reviews from many types of restaurants. Instead, we must be able to
dynamically identify relevant aspects.
A final line of related work relies on the traditional summarization technique of identifying contrastive or contradictory sentences. Kim and Zhai (2009) focus on generating
contrastive summaries by identifying pairs of sentences which express differing opinions on
a particular product feature. To do this, they define metrics of representativeness (coverage of opinions) and contrastiveness (alignment quality) using both semantic similarity
with WordNet matches and word overlap. In comparison to our work, this approach follows an orthogonal goal, as we try to find the most defining aspects instead of the most
contradictory ones. Additionally, while the selected pairs hint at disagreements in rating,
there is no identification of how many people agree with each side or the overall rating of
a particular aspect. In our work, we aim to produce both a concrete set of aspects and the
user sentiment for each, whether it is unanimous or shows disagreement.
Overall, while these methods are designed to produce output summaries which focus on
subjective information, they are not specifically targeted for aspect-based analysis. Instead,
aspects are identified in a supervised fashion (Carenini et al., 2005, 2006) or are not defined
at all (Seki et al., 2005, 2006; Kim & Zhai, 2009). In our work, it is crucial that we have
dynamically-selected aspects because it is not feasible to preselect aspects in a supervised
fashion.
2.2.3 Probabilistic Topic Modeling for Sentiment Analysis
The work closest to our own in the direction of aspect-based analysis focuses on the use of
probabilistic topic modeling techniques for identification of aspects. These may be aggre1. For task examples, see the work of Dang (2005, 2006).

96

Automatic Aggregation by Joint Modeling of Aspects and Values

gated without specific sentiment polarity (Lu & Zhai, 2008) or combined with additional
sentiment modeling either jointly (Mei, Ling, Wondra, Su, & Zhai, 2007; Blei & McAuliffe,
2008; Titov & McDonald, 2008a) or as a separate post-processing step (Titov & McDonald,
2008b). Like our work, these approaches share the intuition that aspects may be represented
as topics.
Several approaches focus on extraction of topics and sentiment from blog articles. In
one approach, they are used as expert articles for aspect extraction in combination with a
larger corpus of user reviews. Lu and Zhai (2008) introduce a model with semi-supervised
probabilistic latent semantic analysis (PLSA) which identifies sentiment-bearing aspects
through segmentation of an expert review. Then, the model extracts compatible supporting
and supplementary text for each aspect from the set of user reviews. Aspect selection is
constrained as in the rule-based approaches; specifically, aspect words are required to be
nouns. Our work differs from their work significantly. While we share a common goal of
identifying and aggregating opinion-bearing aspects, we additionally desire to identify the
polarity of opinions, a task not addressed in their work. In addition, obtaining aspects
from an expert review is unnecessarily constraining; in practice, while expert reviewers may
mention some key aspects, they will not mention every aspect. It is crucial to discover
aspects based on the entire set of articles.
There is work in the direction of aspect identification from blog posts. For example,
Mei et al. (2007) use a variation on latent Dirichlet allocation (LDA) similar to our own to
explicitly model both topics and sentiment, then use a hidden Markov model to discover
sentiment dynamics across topic life cycles. A general sentiment polarity distribution is
computed by combining distributions from several separate labeled data sets (e.g., movies,
cities, etc.). However, in their work, sentiment is measured at the document-level, rather
than topic-level. Additionally, the topics discovered by their model are very broad; for example, when processing the query “The Da Vinci Code”, returned topics may be labeled as
book, movie, and religion, rather than the fine-grained aspects we desire in our model, such
as those representing major characters or events. Our model expands on their work by discovering very fine-grained aspects and associating particular sentiment with each individual
aspect. In addition, by tying sentiment to aspects, we are able to identify sentiment-bearing
words and their associated polarities without the additional annotation required to train
an external sentiment model.
Sentiment may also be combined with LDA using additional latent variables for each
document in order to predict document-level sentiment. Blei and McAuliffe (2008) propose
a form of supervised LDA (sLDA) which incorporates an additional response variable, which
can be used to represent sentiment such as the star rating of a movie. They can then jointly
model the documents and responses in order to find the latent topics which best predict
the response variables for future unlabeled documents. This work is significantly different
from our work, as it is supervised and does not predict in a multi-aspect framework.
Building on these approaches comes work in fine-grained aspect identification with sentiment analysis. Titov and McDonald (2008a, 2008b) introduce a multi-grain unsupervised
topic model, specifically built as an extension to LDA. This technique yields a mixture of
global and local topics. Word distributions for all topics (both global and local) are drawn
at the global level, however; unlike our model. The consequence of this is that topics are
very easy to compare across all products in the corpus; however, the topics are more gen97

Sauper & Barzilay

eral and less dynamic than we hope to achieve because they must be shared among every
product. One consequence of defining global topics is difficulty in finding relevant topics for
every product when there is little overlap. For example, in the case of restaurant reviews,
Italian restaurants should have a completely different set of aspects than Indian restaurants.
Of course, if these factors were known, it would be possible to run the algorithm separately
on each subset of restaurants, but these distinctions are not immediately clear a priori. Increasing the number of topics could assist in recovering additional aspects; however because
the aspects are still global, it will still be difficult to identify restaurant-specific aspects.
For sentiment analysis, the PRanking algorithm of Snyder and Barzilay (2007) is incorporated in two ways: First, the PRanking algorithm is trained in a pipeline fashion after all
topics are generated (Titov & McDonald, 2008b); later, it is incorporated into the model
during inference in a joint formulation (Titov & McDonald, 2008a). However, in both cases,
as in the original algorithm, the set of aspects is fixed – each of the aspects corresponds to a
fixed set of of topics found by the model. Additionally, the learning problem is supervised.
Because of the fixed aspects, necessary additional supervision, and global topic distribution, this model formulation is not sufficient for our problem domain, which requires very
fine-grained aspects.
All of these approaches have structural similarity to the work we present, as they are
variations on LDA. None, however, has the same intent as our model. Mei et al. (2007)
model aspect and sentiment jointly; however their aspects are very vague, and they treat
sentiment at the document level rather than the aspect level. Likewise, Titov and McDonald
(2008b, 2008a) model “fine-grained” aspects, but they are still coarser than the aspects we
require, even if we were to increase the number of aspects, as their distributions are shared
globally. Finally, Lu and Zhai (2008), Blei and McAuliffe (2008), and Titov and McDonald
(2008b, 2008a) require supervised annotation or a supervised expert review that we do not
have. We attempt to solve each of these issues with our joint formulation in order to proceed
with minimal supervision and discover truly fine-grained aspects.

3. Problem Formulation
Before explaining the model details, we describe the random variables and abstractions of
our model, as well as some intuitions and assumptions.2 A visual explanation of model
components is shown in Figure 3. We present complete details and the generative story in
Section 4.
3.1 Model Components
Our model is composed of five component types: entities, snippets, aspects, values, and
word topics. Here, we describe each type and provide examples.

2. Here, we explain our complete model with value selection for sentiment in the restaurant domain. For
the simplified case in the medical domain where we would like to use only aspects, we may simply ignore
the value-related components of the model.

98

Automatic Aggregation by Joint Modeling of Aspects and Values

Tasca Spanish Tapas
Entity
Aspects

Chicken
+
−
+

The chicken was cooked perfectly
The chicken was tough and not tasty
Moist and delicious chicken
Snippets

Values

Dessert
+
+

The dessert was good
An excellent creme brulee
···

Douzo Sushi Bar
Sushi
+
−

The sushi is creative and pretty good
My torched roll tasted rather bland
···
···

Figure 3: Labeled model components from the example in Figure 1. Note that aspects
are never given explicit labels, and the ones shown here are presented purely for ease of
understanding; aspects exist simply as groups of snippets which share a common subject.
Also, word topics are not pictured here; a word topic (Aspect, Value, or Background) is
assigned to each word in each snippet. These model components are described at high level
in Section 3.1 and in depth in Section 4.

3.1.1 Entity
An entity represents a single object which is described in the review. In the restaurant
domain, these represent individual restaurants, such as Tasca Spanish Tapas, Douzo Sushi
Bar, and Outback Steakhouse.
3.1.2 Snippet
A snippet is a user-generated short sequence of words describing an entity. These snippets
can be provided by the user as is (for example, in a “quick reaction” box) or extracted
from complete reviews through a phrase extraction system such as the one from Sauper,
Haghighi, and Barzilay (2010). We assume that each snippet contains at most one single
aspect (e.g., pizza) and one single value type (e.g., positive). In the restaurant domain, this
corresponds to giving an opinion about one particular dish or category of dishes. Examples
from the restaurant domain include “Their pasta dishes are perfection itself ”, “they had
fantastic drinks”, and “the lasagna rustica was cooked perfectly”.
99

Sauper & Barzilay

3.1.3 Aspect
An aspect corresponds to one of several properties of an entity. In the restaurant domain
where entities represent restaurants, aspects may correspond to individual dishes or categories of dishes, such as pizza or alcoholic drinks. For this domain, each entity has its
own unique set of aspects. This allows us to model aspects at the appropriate granularity.
For example, an Italian restaurant may have a dessert aspect which pertains to information about a variety of cakes, pies, and gelato. However, most of a bakery’s menu would
fall under that same dessert aspect. Instead, to present a useful aspect-based summary,
it would require separate aspects for each of cakes, pies, and so on. Because aspects are
entity-specific rather than shared, there are no ties between restaurants which have aspects
in common (e.g., most sushi restaurants will have a sashimi aspect); we consider this a
point for potential future work. Note that it is still possible to compare aspects across
entities (e.g., to find the best restaurant for a burger ) by comparing their respective word
distributions.
3.1.4 Value
Values represent the information associated with an aspect. In the review domain, the two
value types represent positive and negative sentiment respectively. In general, it is possible
to use value to represent other distinctions; for example, in a domain where some aspects
are associated with a numeric value and others are associated with a text description, each
of these can be set as a value type. The intended distinctions may be encouraged by the
use of seed words (see Section 3.2), or they may be left unspecified for the model to assign
whatever it finds to best fit the data. The number of value types must be prespecified;
however, it is possible to use either very few or very many types.
3.1.5 Word Topic
While the words in each snippet are observed, each word is associated with an underlying
latent topic. The possible latent topics correspond to aspect, value, and a background
topic. For example, in the review domain, the latent topic of words great or terrible would
be Value, of words which represent entity aspects such as pizza would be Aspect, and of
stop words like is or of in-domain white noise like food would be Background.
3.2 Problem Setup
In this work, we assume that the snippet words are always observed, and the correlation
between snippets and entities is known (i.e., we know which entity a given snippet describes).
In addition, we assume part of speech tags for each word in each snippet. As a final source of
supervision, we may optionally include small sets of seed words for a lexical distribution, in
order to bias the distribution toward the intended meaning. For example, in the sentiment
case, we can add seed words in order to bias one value distribution toward positive and one
toward negative. Seed words are certainly not required; they are simply a tool to constrain
the model’s use of distributions to fit any prior expectations.
Note that in this formulation, the relevant aspects for each restaurant are not observed;
instead, they are represented by lexical distributions which are induced at inference time. In
100

Automatic Aggregation by Joint Modeling of Aspects and Values

the system output, aspects are represented as unlabeled clusters over snippets.3 Given this
formulation, the goal of this work is then to induce the latent aspect and value underlying
each snippet.

4. Model
Our model has a generative formulation over all snippets in the corpus. In this section,
we first describe in detail the general formulation and notation of the model, then discuss
novel changes and enhancements for particular corpora types. Inference for this model will
be discussed in Section 5. As mentioned previously, we will describe the complete model
including aspect values.
4.1 General Formulation
For this model, we assume a collection of all snippet words for all entities, s. We use si,j,w to
denote the wth word of the jth snippet of the ith entity. We also assume a fixed vocabulary
of words W .
We present a summary of notation in Table 1, a concise summary of the model in
Figure 4, and a model diagram in Figure 5. There are three levels in the model design:
global distributions common to all snippets for all entities in the collection, entity-level
distributions common to all snippets describing a single entity, and snippet- and word-level
random variables. Here, we describe each in turn.
4.1.1 Global Distributions
At the global level, we draw a set of distributions common to all entities in the corpus. These
include everything shared across a domain, such as the background stop-word distribution,
value types, and word topic transitions.
Background Distribution A global background word distribution θB is drawn to represent stop-words and in-domain white noise (e.g., “food” becomes white noise in a corpus
of restaurant reviews). This distribution is drawn from a symmetric Dirichlet with concentration parameter λB ; in our experiments, this is set to 0.2.
Value Distributions A value word distribution θVv is drawn for each value type v. For
example, in a review domain with positive and negative sentiment types, there will be a
distribution over words for the positive type and one for the negative type. Seed words
Wseedv are given additional probability mass on the value priors for type v; specifically,
a non-seed word receives  hyperparameter, while a seed word receives  + λV ; in our
experiments, this is set to 0.15.
Transition Distribution A transition distribution Λ is drawn to represent the transition
probabilities of underlying word topics. For example, it may be very likely to have a
Value Aspect transition in a review domain, which fits phrases like “great pizza.” In our
experiments, this distribution is given a slight prior bias toward more helpful transitions; for
3. If a label is desired, we can automatically extract one by selecting from the highest probability words
for a particular aspect. For simplicity and exactness, we provide manual cluster labels for the examples
in this paper.

101

Sauper & Barzilay

Data Set
s
si,j,w
ti,j,w ∗
W
Wseedv

Collection of all snippet words from all entities
wth word of jth snippet of ith entity
Part-of-speech tag corresponding to si,j,w
Fixed vocabulary
Seed words for value type v

Lexical Distributions
θB
i,a
a ∗)
θA
(θA
v
θV
θI ∗

Background word distribution
Aspect word distribution for aspect a of entity i
Value word distribution for type v
Ignored words distribution

Other Distributions
Λ
φi,a (φa ∗ )
ψ i (ψ ∗ )
η ∗

Transition distribution over word topics
Aspect-value multinomial for aspect a of entity i
Aspect multinomial for entity i
Part-of-speech tag distribution

Latent Variables
i,j
ZA
ZVi,j
i,j,w
ZW

Aspect selected for si,j
Value type selected for si,j
Word topic (A, V, B, I ∗ ) selected for si,j,w

Other Notation
K
A
V
B
I ∗

Number of aspects a
Indicator corresponding
Indicator corresponding
Indicator corresponding
Indicator corresponding

to
to
to
to

aspect word
value word
background word
ignored word

Table 1: Notation used in this paper. Items marked with
in Section 4.2.

∗

relate to extensions mentioned

example, encouraging sticky behavior by providing a small boost to self-transitions. This
bias is easily overridden by data; however, it provides a useful starting point.
4.1.2 Entity-Specific Distributions
There are naturally variations in the aspects which snippets describe and how many snippets
describe each aspect. For example, a mobile device popular for long battery life will likely
have more snippets describing the battery than a device which is known for its large screen.
Some domains may have enormous variation in aspect vocabulary; for example, in restaurant
reviews, two restaurants may not serve any of the same food items to compare. To account
102

Automatic Aggregation by Joint Modeling of Aspects and Values

Global Level:
Draw background word distribution θB ∼ Dirichlet(λB W )
For each value type v,
Draw value word distribution θVv ∼ Dirichlet(W + λV Wseedv )
Entity Level:
For each entity i,
i,a
Draw aspect word distributions θA
∼ Dirichlet(λA W ) for a = 1, . . . , K

Draw aspect value multinomial φi,a ∼ Dirichlet(λAV N ) for a = 1, . . . , K
Draw aspect multinomial ψ i ∼ Dirichlet(λM K)
Snippet Level:
For each snippet j describing the ith entity,
i,j
∼ ψi
Draw snippet aspect ZA
i,j

Draw snippet value ZVi,j ∼ φi,ZA

i,j,w−1
i,j,w
∼ Λ|ZW
Draw sequence of word topic indicators ZW
i,j
and value ZVi,j
Draw snippet word given aspect ZA

si,j,w ∼


i,Z i,j

θA A ,

Z i,j

θV V ,



θB ,

i,j,w
=A
when ZW
i,j,w
=V
when ZW
i,j,w
when ZW = B

Figure 4: A summary of our generative model presented in Section 4.1. We use Dirichlet(λW ) to denote a finite Dirichlet prior where the hyper-parameter counts are a scalar
times the unit vector of vocabulary items. For the global value word distribution, the prior
hyper-parameter counts are  for all vocabulary items and λV for Wseedv , the vector of
vocabulary items in the set of seed words for value v.

for these variations, we define a set of entity-specific distributions which generate both
aspect vocabulary and popularity, as well as a distribution over value types for each aspect.
i,a
Aspect Distributions An aspect word distribution θA
is drawn for each aspect a. Each
of these represents the distribution over unigrams for a particular aspect. For example, in
the domain of restaurant reviews, aspects may correspond to menu items such as pizza,
while in reviews for cell phones, they may correspond to details such as battery life. Each

103

Sauper & Barzilay

Value v
Background word
distribution

Transition
distribution

Value word
distributions

θB

Λ

θVv

Entity i

Aspect a
Aspect
multinomial

Aspect word
distributions

Aspect-value
multinomial

ψi

θAi,a

φi,a

Snippet j

Snippet aspect

Snippet value

ZAi,j

ZVi,j

HMM over snippet words

Λ

i,j,w−1
ZW

i,j,w
ZW

i,j,w+1
ZW

si,j,w−1

si,j,w

si,j,w+1

ZAi,j , θAi,a
ZVi,j , θVv
θB
Figure 5: A graphical description of the model presented in Section 4.1. A written description of the generative process is located in Figure 4. Curved arrows indicate additional links
which are present in the model but not drawn for readability.

104

Automatic Aggregation by Joint Modeling of Aspects and Values

aspect word distribution is drawn from a symmetric Dirichlet prior with hyperparameter
λA ; in our experiments, this is set to 0.075.
Aspect-Value Multinomials Aspect-value multinomials φi,a determine the likelihood
of each value type v for the corresponding aspect a. For example, if value types represent
positive and negative sentiment, this corresponds to agreement of sentiment across snippets.
Likewise, if value types represent formatting such as integers, decimals, and text, each aspect
generally prefers the same type of value. These multinomials are drawn from a symmetric
Dirichlet prior using hyperparameter λAV ; in our experiments, this is set to 1.0.
Aspect Multinomial The aspect multinomial ψ i controls the likelihood of each aspect
being discussed in a given snippet. This encodes the intuition that certain aspects are more
likely to be discussed than others for a given entity. For example, if a particular Italian
restaurant is famous for their pizza, it is likely that the pizza aspect will be frequently
discussed in reviews, while the drinks aspect may be mentioned only occasionally. The
aspect multinomial will encode this as a higher likelihood for choosing pizza as a snippet
aspect than drinks. This multinomial is drawn from a symmetric Dirichlet distribution with
hyperparameter λM ; in our experiments, this is set to 1.0.
4.1.3 Snippet- and Word-Specific Random Variables
Using the distributions described above, we can now draw random variables for each snippet
to determine the aspect and value type which will be described, as well as the sequence of
underlying word topics and words.
i,j
which this snippet will describe is drawn from the aspect
Aspect A single aspect ZA
i
multinomial ψ . All aspect words in the snippet (e.g., pizza in a corpus of restaurant
i,j
i,ZA

reviews) will be drawn from the corresponding aspect word distribution θA

.

Value Type A single value type ZVi,j is drawn conditioned on the selected aspect from the
i,j
corresponding aspect-value multinomial φi,ZA . All value words in the snippet (e.g., “great”
Z i,j

in the review domain) will be drawn from the corresponding value word distribution θV V .
i,j,1
i,j,m
is generWord Topic Indicators A sequence of word topic indicators ZW
, . . . , ZW
ated using a first-order Markov model parameterized by the transition matrix Λ. These
indicators determine which unigram distribution generates each word in the snippet. For
i,j,w
example, if ZW
= B, the wth word of this snippet is generated from the background word
distribution θB .

4.2 Model Extensions
There are a few optional components of the model which may improve performance for some
cases. We briefly list them here, then present the necessary modifications to the model in
detail for each case. Modifications to the inference procedure will be presented in Section 5.2.
First, for corpora which contain irrelevant snippets, we may introduce an additional word
distribution θI and word topic Ignore to allow the model to ignore certain snippets or
pieces of snippets altogether. Second, if it is possible to acquire part of speech tags for the
105

Sauper & Barzilay

snippets, using these as an extra piece of information is quite beneficial. Finally, for corpora
where every entity is expected to share the same aspects, the model can be altered to use
the same set of aspect distributions for all entities.
4.2.1 Ignoring Snippets
When snippet data is automatically extracted, it may be noisy, and some snippets may
violate our initial assumptions of having one aspect and one value. For example, we find
some snippets which were mistakenly extracted that have neither aspect nor value. These
extraneous snippets may be difficult to identify a priori. To compensate for this, we modify
the model to allow partial or entire snippets to be ignored through the addition of a global
unigram distribution, namely the Ignore distribution θI . This distribution is drawn from a
symmetric Dirichlet with concentration parameter λI .
The Ignore distribution differs from the Background distribution in that it includes both
common and uncommon words. It is intended to select whole snippets or large portions of
snippets, so some words may overlap with the Background distribution and other distributions. In order to successfully incorporate this distribution into our model, we must allow
i,j,w
to consider the Ignore topic I. Additionally, to ensure that
the word topic indicator ZW
it selects long segments of text, we give a large boost to the prior of the Ignore Ignore
sequence in the transition distribution Λ, similar to the boost for self-transitions.
4.2.2 Part-of-Speech Tags
Part-of-speech tags can provide valuable evidence in determining which snippet words are
drawn from each distribution. For example, aspect words are often nouns, as they represent
concrete properties or concepts in a domain. Likewise, in some domains, value words
describe aspects and therefore tend to be expressed as numbers or adjectives.
This intuition can be directly incorporated into the model in the form of additional
outputs. Specifically, we modify our HMM to produce both words and tags. Additionally,
a , η v , and η , similar to the corresponding unigram
we define distributions over tags ηA
V
B
distributions.
4.2.3 Shared Aspects
When domains are very regular, and every entity is expected to express aspects from a
consistent set, it is beneficial to share aspect information across entities. For example, in a
medical domain, the same general set of lab tests and physical exam categories are run on all
patients. Note that this is quite unlike the restaurant review case, where each restaurant’s
aspects are completely different (e.g., pizza, curry, scones, and so on).
Sharing aspects in this way can be accomplished by modifying the aspect distributions
i,a
a . Likewise, aspect-value multinomials φi,a become
θA
to become global distributions θA
shared across all entities as as φa . Treatment of the aspect multinomials depend on the
domain properties. If the distribution over aspects is expected to be the same across all
entities, it can also be made global; however, if each individual entity is expected to exhibit
variation in the number of snippets related to each aspect, they should be kept as entityspecific. For example, reviews for a set of cell phones may be expected to focus on varying
106

Automatic Aggregation by Joint Modeling of Aspects and Values

Value v
Background word
distribution

Transition
distribution

Value word
distributions

θB

Λ

θVv

Entity i

Aspect a
Aspect
multinomial

Aspect word
distributions

Aspect-value
multinomial

ψi

θAa

φa

Snippet j

Snippet aspect

Snippet value

ZAi,j

ZVi,j

HMM over snippet words

Λ

i,j,w−1
ZW

i,j,w
ZW

i,j,w+1
ZW

si,j,w−1

si,j,w

si,j,w+1

ZAi,j , θAi,a
ZVi,j , θVv
θB
Figure 6: A graphical description of the model with shared aspects presented in Section 4.2.
Note the similarities to Figure 5; however in this version, aspects are shared for the entire
corpus, rather than being entity-specific. It would also be possible to share the aspect
multinomial corpus-wide; in that case it would indicate that all entities share the same
general distribution over aspects, while in this version the individual entities are allowed to
have completely different distributions.

parts, depending on what is most unique or problematic about those phones. A graphical
description of these changes compared to the original model is shown in Figure 6.
107

Sauper & Barzilay

Mean-field Factorization
Q (θB , θV , Λ, θA , ψ, φ, Z)
= q (θB ) q (Λ)

N
Y

!
q (θVv )

v=1

Y


q ψ

i



K
 
Y

i,a
q θA
q φi,a

!

a=1

i

!
Y  i,j   i,j  Y  i,j,w 

q ZV q ZA
q ZW
w

j

Snippet Aspect Indicator
i,j
log q(ZA
= a) ∝ Eq(ψi ) log ψ i (a) +

X

i,j,w
i,a i,j,w
q(ZW
= A)Eq(θi,a ) log θA
(s
)+
A

w

N
X

q(ZVi,j = v)Eq(φi,a ) log φi,a (v)

v=1

Snippet Value Type Indicator
X
X
i,j
i,j,w
log q(ZVi,j = v) ∝
q(ZA
= a)Eq(φi,a ) log φi,a (v) +
q(ZW
= V )Eq(θVv ) log θVv (si,j,w )
a

w

Word Topic Indicator
 X





i,j,w+1 
i,j
i,j i,j,w 
i,j,w−1
i,j,w
+
= a Eq(θi,a ) log θA
, A Λ A, ZW
q ZA
s
log q ZW
= A ∝ log P ZW = A + Eq(Λ) log Λ ZW
A

a














i,j,w+1 

i,j,w−1
i,j,w
= V ∝ log P ZW = V + Eq(Λ) log Λ ZW
, V Λ V, ZW
log q ZW



+

X

q ZVi,j = v Eq(θVv ) log θVv si,j,w


v

log q

i,j,w
ZW

= B ∝ log P ZW = B + Eq(Λ) log Λ


i,j,w−1
,B Λ
ZW

i,j,w+1 
B, ZW



+ Eq(θB ) log θB si,j,w



Figure 7: The mean-field variational algorithm used during learning and inference to obtain posterior predictions over snippet properties and attributes, as described in Section 5.
Mean-field inference consists of updating each of the latent variable factors as well as a
straightforward update of latent parameters in round robin fashion.

5. Inference
The goal of inference in this model is to predict the aspect and value for each snippet i and
product j, given the text of all observed snippets, while marginalizing out the remaining
hidden parameters:
i,j
P (ZA
, ZVi,j |s)
We accomplish this task using variational inference (Blei, Ng, & Jordan, 2003). Specifically, the goal of variational inference is to find a tractable approximation Q(·) to the full
posterior of the model.
P (θB , θV , Λ, θA , ψ, φ, Z|s) ≈ Q(θB , θV , Λ, θA , ψ, φ, Z)
For our model, we assume a full mean-field factorization of the variational distribution,
shown in Figure 7. This variational approximation is defined as a product of factors q(·),
which are assumed to be independent. This approximation allows for tractable inference of
each factor individually. To obtain the closest possible approximation, we attempt to set
108



Automatic Aggregation by Joint Modeling of Aspects and Values

the q(·) factors to minimize the KL divergence to the true model posterior:
arg min KL(Q(θB , θV , Λ, θA , ψ, φ, Z)kP (θB , θV , Λ, θA , ψ, φ, Z|s))
Q(·)

5.1 Optimization
We optimize this objective using coordinate descent on the q(·) factors. Concretely, we
update each factor by optimizing the above criterion with all other factors fixed to their
current values:
q(·) ← EQ/q(·) log P (θB , θV , Λ, θA , ψ, φ, Z, s)
A summary of the variational update equations is given in Figure 7, and a graphical
representation of the involved variables for each step is presented in Figure 8. Here, we will
present the update for each factor.
5.1.1 Snippet Aspect Indicator
i,j
First, we consider the update for the snippet aspect indicator, ZA
(Figure 8a):
i,j
log q(ZA
= a) ∝ Eq(ψi ) log ψ i (a)
X
i,j,w
i,a i,j,w
+
q(ZW
= A)Eq(θi,a ) log θA
(s
)

+

(1b)

A

w
N
X

(1a)

q(ZVi,j = v)Eq(φi,a ) log φi,a (v)

(1c)

v=1

The optimal aspect for a particular snippet depends on three factors. First, we include the
likelihood of discussing each aspect a (Eqn. 1a). As mentioned earlier, this encodes the
prior probability that some aspects are discussed more frequently than others. Second, we
examine the likelihood of a particular aspect based on the words in the snippet (Eqn. 1b).
For each word which is identified as an aspect word, we add the probability that it discusses
this aspect. Third, we determine the compatibility of the chosen aspect type with the
current aspect (Eqn. 1c). For example, if we know the value type is most likely an integer,
the assigned aspect should accept integers.
5.1.2 Snippet Value Type Indicator
Next, we consider the update for the snippet value type indicator, ZVi,j (Figure 8b):
X
i,j
log q(ZVi,j = v) ∝
q(ZA
= a)Eq(φi,a ) log φi,a (v)

(2a)

a

+

X

i,j,w
q(ZW
= V )Eq(θVv ) log θVv (si,j,w )

(2b)

w

The best value type for a snippet depends on two factors. First, like the snippet aspect
indicator, we must take into consideration the compatibility between snippet aspect and
value type (Eqn. 2a). Second, for each word identified as a value word, we include the
likelihood that it comes from the given value type.
109

Sauper & Barzilay

v
θB

v
θVv

Λ

i

i

a
i,a
θA

ψi

j

Λ

ZV , θ V
ZA , θ A
θB

ψi

j

ZVi,j

w−1
ZW

w
ZW

w+1
ZW

sw−1

sw

sw+1

ZV , θV
ZA , θA
θB

i

θVv

ψ

j

i,j
ZA

w−1
ZW

θ

ZA
sw−1 θA

Z

φ

ψ

j

i

Λ

w
ZW

w+1
ZW

sw−1

sw

sw+1

θVv

i,a
θA

w+1
ZW

Λ

w−1
ZW

θ
sw

sw+1

Z

ZV
sw−1 θV

φ

Λ

θVv

i,a
θA

φi,a

θB
i

i,j
ZA

w
ZW

w
i. ZW
=A

w−1
ZW

v

a

i,a

ZVi,j

Λ

θB
i

i,a
θA

ZVi,j

v

a
i

φi,a

(b) Inference procedure for snippet value, ZVi,j

v
Λ

i,a
θA

i,j
ZA

Λ

i,j
(a) Inference procedure for snippet aspect, ZA

θB

θVv

a

φi,a

i,j
ZA

Λ

θB

a

i,a

ZVi,j

ψ

j

w
ZW

w+1
ZW

Λ

sw

sw+1

Z

i

i,j
ZA

w−1
ZW

ZVi,j
w
ZW

w+1
ZW

sw

sw+1

θ

w
ii. ZW
=V

sw−1 θB

w
iii. ZW
=B

i,j,w
(c) Inference procedure for word topic, ZW

Figure 8: Variational inference update steps for each latent variable. The latent variable
currently being updated is shown in a double circle, and the other variables relevant to the
update are highlighted in black. Those variables which have no impact on the update are
grayed out. Note that for snippet aspect (a) and snippet value type (b), the update takes
the same form for each possible aspect or value type. However, for word topic (c), the
update is not symmetric as the relevant variables are different for each possible word topic.

110

Automatic Aggregation by Joint Modeling of Aspects and Values

5.1.3 Word Topic Indicator
i,j,w
Finally, we consider the update for the word topic indicators, ZW
(Figure 8c). Unlike
the previous indicators, each possible topic has a slightly different equation, as we must
marginalize over all possible aspects and value types.






i,j,w
i,j,w−1
i,j,w+1 
log q ZW
= A ∝ log P ZW = A + Eq(Λ) log Λ ZW
, A Λ A, ZW
X

i,j
i,j i,j,w 
+
q ZA
= a Eq(θi,a ) log θA
s

(3a)

A

a






i,j,w
i,j,w−1
i,j,w+1 
log q ZW
= V ∝ log P ZW = V + Eq(Λ) log Λ ZW
, V Λ V, ZW
X


+
q ZVi,j = v Eq(θVv ) log θVv si,j,w

(3b)

v






i,j,w+1 
i,j,w−1
i,j,w
, B Λ B, ZW
= B ∝ log P ZW = B + Eq(Λ) log Λ ZW
log q ZW

+ Eq(θB ) log θB si,j,w

(3c)

The update for each topic is composed of the prior probability of having that topic, transition probabilities using this topic, and the probability of the word coming from the appropriate unigram distribution, marginalized over all possibilities for snippet aspect and value
indicators.
5.1.4 Parameter Factors
Updates for the parameter factors under variational inference are derived through simple
counts of the latent variables ZA , ZV , and ZW . Note that these do include partial counts;
i,j
= a1 ) = 0.35, it would contribute 0.35
if a particular snippet has aspect probability P (ZA
i
count to ψ (a1 ).
5.1.5 Algorithm Details
Given this set of update equations, the update procedure is straightforward. First, iterate
over the corpus computing the updated values for each random variable, then do a batch
update for all factors simultaneously. This update algorithm is run to convergence. In
practice, convergence is achieved by the 50th iteration, so the algorithm is quite efficient.
Note that the batch update means each update is computed using the values from the
previous iteration, unlike Gibbs sampling which uses updated values as it runs through the
corpus. This difference allows the variational update algorithm to be parallelized, yielding
a nice efficiency boost. Specifically, to parallelize the algorithm, we simply split the set
of entities evenly among processors. Updates for entity-specific factors and variables are
computed during the pass through the data, and updates for global factors are collected
and combined at the end of each pass.
111

Sauper & Barzilay

5.2 Inference for Model Extensions
As discussed in Section 4.2, we can add additional components to the model to improve
performance for data with certain attributes. Here, we briefly discuss the modifications to
the inference equations for each extension.
5.2.1 Ignoring Snippets
The main modifications to the model for this extension are the addition of the unigram
distribution θI and word topic I, which can be chosen by ZW . The update equation for ZW
is modified by the addition of the following:
i,j,w
log q(ZW
= I) ∝ log P (ZW = I) + Eq(θI ) log θI (si,j,w )

As in the other pieces of this equation (Eqn. 3), this is composed of the prior probability
for the word topic I and the likelihood that this word is generated by θI .
In addition, the transition distribution Λ must be updated to include transition probabilities for I∗ and ∗I. As mentioned earlier, the II transition receives high weight, while
all other transitions to and from I receive very low weight.
5.2.2 Part-of-Speech Tags
To add part of speech tags, the model is updated to include part-of-speech distributions
i,a
ηA , ηV , and ηB , one for each word topic. Note that unlike the unigram distributions θA
and θVv , the corresponding tag distributions are not dependent on snippet entity, aspect, or
value. These are included and referenced in the updates for ZW as follows:





i,j,w+1 
i,j,w
i,j,w−1
, A Λ A, ZW
log q ZW
= A ∝ log P ZW = A + Eq(Λ) log Λ ZW
 X

i,j i,j,w 
i,j
+ Eq(ηA ) log ηA ti,j,w +
= a Eq(θi,a ) log θA
s
q ZA
a

A






i,j,w+1 
i,j,w−1
i,j,w
, V Λ V, ZW
= V ∝ log P ZW = V + Eq(Λ) log Λ ZW
log q ZW
 X


q ZVi,j = v Eq(θVv ) log θVv si,j,w
+ Eq(ηV ) log ηV ti,j,w +
v






i,j,w
i,j,w−1
i,j,w+1 
log q ZW
= B ∝ log P ZW = B + Eq(Λ) log Λ ZW
, B Λ B, ZW


+ Eq(ηB ) log ηB ti,j,w + Eq(θB ) log θB si,j,w
Here, we define t as the set of all tags and ti,j,w as the tag corresponding to the word si,j,w .
5.2.3 Shared Aspects
A global set of shared aspects is a simplification of the model in that it reduces the total
a and aspect-value
number of parameters. This model redefines aspect distributions to be θA
multinomials to be φa . Depending on domain, it may also redefine the aspect multinomial
to be ψ. The resulting latent variable update equations are the same; only the parameter
112

Automatic Aggregation by Joint Modeling of Aspects and Values

factor updates are changed. Rather than collecting counts over snippets describing a single
entity, counts are collected across the corpus.

6. Experiments
We perform experiments on two tasks. First, we test our full model on joint prediction of
both aspect and sentiment on a corpus of review data. Second, we use a simplified version
of the model designed to identify aspects only on a corpus of medical summary data. These
domains are structured quite differently, and therefore present very different challenges to
our model.
6.1 Joint Identification of Aspect and Sentiment
Our first task is to test our full model by jointly predicting both aspect and sentiment on
a collection of restaurant review data. Specifically, we would like to dynamically select a
set of relevant aspects for each restaurant, identify the snippets which correspond to each
aspect, and recover the polarity of each snippet individually and each aspect as a whole.
We perform three experiments to evaluate our model’s effectiveness. First, we test the
quality of learned aspects by evaluating the predicted snippet clusters. Second, we assess
the quality of the polarity classification. Third, we examine per-word labeling accuracy.
6.1.1 Data Set
Our data set for this task consists of snippets selected from Yelp restaurant reviews by our
previous system (Sauper et al., 2010). The system is trained to extract snippets containing
short descriptions of user sentiment towards some aspect of a restaurant.4 For the purpose
of this experiment, we select only the snippets labeled by that system as referencing food.
In order to ensure that there is enough data for meaningful analysis, we ignore restaurants
that have fewer than 20 snippets across all reviews. While our model can easily operate on
restaurants with fewer snippets, we want to ensure that the cases we select for evaluation
are nontrivial; i.e., that there are a sufficient number of snippets in each cluster to make
a valid comparison. There are 13,879 snippets in total, taken from 328 restaurants in and
around the Boston/Cambridge area. The average snippet length is 7.8 words, and there
are an average of 42.1 snippets per restaurant. We use the MXPOST tagger (Ratnaparkhi,
1996) to gather POS tags for the data. Figure 9 shows some example snippets.
For this domain, the value distributions consist of one positive and one negative distribution. These are seeded using 42 and 33 seed words respectively. Seed words are hand-selected
based on the restaurant review domain; therefore, they include domain-specific words such
as delicious and gross. A complete list of seed words is included in Table 2.
6.1.2 Domain Challenges and Modeling Techniques
This domain presents two challenging characteristics for our model. First, there are a wide
variety of restaurants within our domain, including everything from high-end Asian fusion
cuisine to greasy burger fast food places. If we were to try to represent these using a single
4. For exact training procedures, please reference that paper.

113

Sauper & Barzilay

Positive
amazing
delightful
extraordinary
flavorful
generous
heaven
inexpensive
perfect
recommend
stimulating
wonderful

Negative
awesome
divine
fantastic
free
good
huge
love
phenomenal
rich
strong
yummy

best
enjoy
fav
fresh
great
incredible
nice
pleasant
sleek
tasty

delicious
excellent
favorite
fun
happy
interesting
outstanding
quality
stellar
tender

average
bland
disappointed
expensive
gross
lame
meh
poor
tacky
tiny
uninspiring

awful
boring
disgusting
fatty
horrible
less
mushy
pricey
tasteless
unappetizing
worse

bad
confused
dry
greasy
inedible
mediocre
overcooked
salty
terrible
underwhelming
worst

Table 2: Seed words used by the model for the restaurant corpus, 42 positive words and 33
negative words in total. These words are manually selected for this data set.
shared set of aspects, the number of aspects required would be immense, and it would
be extremely difficult for our model to make fine-grained distinctions between them. By
defining aspects separately for each restaurant as mentioned in Section 4, we can achieve
the proper granularity of aspects for each individual restaurant without an overwhelming
or overlapping selection of choices. For example, the model is able to distinguish that an
Italian restaurant may need only a single dessert aspect, while a bakery requires separate
pie, cake, and cookie aspects.
Second, while there are usually a fairly cohesive set of words which refer to any particular
aspect (e.g., the pizza aspect might be commonly be seen with the words slice, pepperoni,
and cheese), there are a near-unlimited set of potential sentiment words. This is especially
pronounced in the social media domain where there are many novel words used to express
sentiment (e.g., deeeeeeeelish as a substitute for delicious). As mentioned in Section 4, the
part-of-speech and transition components of the model helps to identify which unknown
words are likely to be sentiment words; however, we additionally need to identify the polarity of their sentiment. To do this, we can leverage the aspect-value multinomial, which
represents the likelihood of positive or negative sentiment for a particular aspect. If most
of the snippets about a given aspect are positive, it is likely that the word deeeeeeeelish
represents positive sentiment as well.
6.1.3 Cluster Prediction
i,j
The goal of this task is to evaluate the quality of aspect clusters; specifically the ZA
variable
in Section 4. In an ideal clustering, the predicted clusters will be cohesive (i.e., all snippets
predicted to discuss a given aspect are related to each other) and comprehensive (i.e., all
snippets which discuss an aspect are selected as such). For example, a snippet will be
assigned the aspect pizza if and only if that snippet mentions some aspect of pizza, such as
its crust, cheese, or toppings.

Annotation For this experiment, we use a set of gold clusters on the complete sets
of snippets from 20 restaurants, 1026 snippets in total (an average of 51.3 snippets per
restaurant). Cluster annotations were provided by graduate students fluent in English. Each
annotator was provided with a complete set of snippets for a particular restaurant, then
asked to cluster them naturally. There were 199 clusters in total, which yields an average
114

Automatic Aggregation by Joint Modeling of Aspects and Values

The noodles and the meat were actually +:::::::
pretty ::::::
good.
I +:::::::::::::
recommend the chicken noodle pho.
soggy.
The noodles were – ::::::
The chicken pho was also + good.
:::::
though.
The spring rolls and coffee were + good,
:::::
–
The spring roll wrappers were a little
dry tasting.
::::::::::::::::::
crispy
spring
rolls.
My + favorites
were
the
:::::::::
The Crispy Tuna Spring Rolls are + fantastic!
:::::::::
The
The
The
The

lobster roll my mother ordered was – ::::
dry and – scant.
:::::
portabella mushroom is my + go-to
sandwich.
:::::
bread on the sandwich was – stale.
:::::
rather ::::::::
measly.
slice of tomato was –:::::::

The shumai and california maki sushi were +::::::::
decent.
+
The spicy tuna roll and eel roll were perfect.
:::::::
not :::
so::::::
great.
The rolls with spicy mayo were –::::
+
love Thai rolls.
I :::::

Figure 9: Example snippets from our data set, grouped according to aspect. Aspect words
are underlined and colored blue, negative value words are labeled - and colored red, and
positive value words are labeled + and colored green. The grouping and labeling are not
given in the data set and must be learned by the model.

of 10.0 clusters per restaurant. These annotations are high-quality; the average annotator
agreement is 81.9 by the MUC evaluation metric (described in detail below). While we could
define a different number of clusters for each restaurant by varying the number of aspect
distributions, for simplicity we ask both baseline systems and our full model to produce
10 aspect clusters per restaurant, matching the average annotated number. Varying the
number of clusters will simply cause existing clusters to merge or split; there are no large
or surprising changes in clustering.
Baseline We use two baselines for this task, both using a clustering algorithm weighted
by TF*IDF as implemented by the publicly available CLUTO package (Karypis, 2002),5
using agglomerative clustering with the cosine similarity distance metric (Chen, Branavan,
Barzilay, & Karger, 2009; Chen, Benson, Naseem, & Barzilay, 2011).
The first baseline, Cluster-All, clusters over entire snippets in the data set. This
baseline will put a strong connection between things which are lexically similar. Because our
model only uses aspect words to tie together clusters, this baseline may capture correlations
between words which our model does not correctly identify as aspect words.
5. Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.

115

Sauper & Barzilay

Cluster-All
Cluster-Noun
Our model

Precision

Recall

57.3
68.6
74.3

60.1
70.5
85.3

F1
58.7
69.5
79.4

Table 3: Results using the MUC metric on cluster prediction for the joint aspect and value
identification task. While MUC has a deficiency in that putting everything into a single
cluster will artificially inflate the score, all models are set to use the same number of clusters.
Note that for this task, the Cluster-Noun significantly outperforms the Cluster-All
baseline, indicating that part of speech is a crucial piece of information for this task.

The second baseline, Cluster-Noun, works over only the nouns from the snippets.
Each snippet is POS-tagged using MXPOST (Ratnaparkhi, 1996),6 and any non-noun (i.e.,
not NN, NNS, NNP, or NNPS) words are removed. Because we expect that most aspects contain
at least one noun, this acts as a proxy for the aspect identification in our model.
Metric We use the MUC cluster evaluation metric for this task (Vilain, Burger, Aberdeen,
Connolly, & Hirschman, 1995). This metric measures the number of cluster merges and
splits required to recreate the gold clusters given the model’s output. Therefore, it can
concisely show how accurate our clusters are as a whole. While it would be possible to
artificially inflate the score by putting everything into a single cluster, the parameters on
our model and the likelihood objective are such that the model prefers to use all available
clusters, the same number as the baseline system.
Results Results for our cluster prediction task are in Table 3. Our model shows strong
performance over each baseline, for a total error reduction of 32% over the Cluster-Noun
baseline and 50% over the Cluster-All baseline. The most common cause of poor cluster
choices in the baseline systems is their inability to distinguish which words are relevant
aspect words. For example, in the Cluster-All baseline, if many snippets use the word
delicious, there may end up being a cluster based on that alone. The Cluster-Noun
baseline is able to avoid some of these pitfalls thanks to its built-in filter. It is able to
avoid common value words such as adjectives and also focus on what seems to be the most
concrete portion of the aspect (e.g., blackened chicken); however, it still cannot make the
correct distinctions where these assumptions are broken. Because our model is capable
of distinguishing which words are aspect words (i.e., words relevant to clustering), it can
choose clusters which make more sense overall.
6.1.4 Sentiment Analysis
We evaluate the system’s predictions of snippet sentiment using the predicted posterior
i,j
over the value distributions for the snippet (i.e., ZA
). For this task, we consider the binary
i,j
judgment to be simply the one with higher value in q(ZA
) (see Section 5). The goal of this
task is to evaluate whether our model correctly distinguishes the sentiment of value words.
6. Available at http://www.inf.ed.ac.uk/resources/nlp/local_doc/MXPOST.html.

116

Automatic Aggregation by Joint Modeling of Aspects and Values

Majority
Discriminative-Small
Seed
Discriminative-Large
Our model

Accuracy
60.7
74.1
78.2
80.4
82.5

Table 4: Sentiment prediction accuracy of our model compared to the Discriminative and
Seed baselines, as well as Majority representing the majority class (Positive) baseline.
One advantage of our system is its ability to distinguish aspect words from sentiment words
in order to restrict judgment to only the relevant terms; another is the leverage that it gains
from biasing unknown sentiment words to follow the polarity observed in other snippets
relating to the same aspect.
Annotation For this task, we use a set of 662 randomly selected snippets from the Yelp
reviews which express opinions. To get a clear result, this set specifically excludes neutral,
mixed, or potentially ambiguous snippets such as the fries were too salty but tasty or the
blackened chicken was very spicy, which make up about 10% of the overall data. This set is
split into a training set of 550 snippets and a test set of 112 snippets, then each snippet is
manually labeled positive or negative. For one baseline, we use the set of positive and
negative seed words which were manually chosen for our model, shown in Table 2. Note
that as before, our model has access to the full corpus of unlabeled data plus the seed words,
but no labeled examples.
Baseline We use two baselines for this task, one based on a standard discriminative
classifier and one based on the seed words from our model.
The Discriminative baseline for this task is a standard maximum entropy discriminative binary classifier7 over unigrams. Given enough snippets from enough unrelated aspects,
the classifier should be able to identify that words like great indicate positive sentiment and
those like bad indicate negative sentiment, while words like chicken are neutral and have
no effect. To illustrate the effect of training size, we include results for DiscriminativeSmall, which uses 100 training examples, and Discriminative-Large, which uses 550
training examples.
The Seed baseline simply counts the number of words from the same positive and
negative seed lists used by the model, Vseed+ and Vseed− , as listed in Table 2. If there are
more words from Vseed+ , the snippet is labeled positive, and if there are more words from
Vseed− , the snippet is labeled negative. If there is a tie or there are no seed words, we split
the prediction. Because the seed word lists are manually selected specifically for restaurant
reviews (i.e., they contain food-related sentiment words such as delicious), this baseline
should perform well.
Results The overall sentiment classification accuracy of each system are shown in Table 4). Our model outperforms both baselines. The obvious flaw in the Seed baseline is
7. Available at https://github.com/lzhang10/maxent.

117

Sauper & Barzilay

85

Accuracy

82.5
79.5

80

80.4

78.2 77.7
76.8

78.6

75
Discriminative
Seed
Our model

74.1
70

0

100 200 300 400 500 600
Number of snippets in training data

Figure 10: Discriminative baseline performance as the number of training examples increases. While performance generally increases, there are some inconsistencies. The main
issue with this baseline is that it needs to see examples of words in training data before it
can improve; this phenomenon can be seen at the plateau in this graph.

the inability to pre-specify every possible sentiment word. It does perform highly, due to
its tailoring for the restaurant domain and good coverage of the most frequent words (e.g.,
delicious, good, great), but the performance of our model indicates that it can generalize
beyond these seed words.
The Discriminative-Large outperforms the Seed baseline on this test set; however,
given the smaller training set of Discriminative-Small, it performs worse. The training
curve of the Discriminative baseline is shown in Figure 10. While the Discriminative
baseline system can correctly identify the polarity of statements containing information it
has seen in the past, it has two main weaknesses. First, every sentiment word must have
been present in training data. For example, in our test data, rancid appears in a negative
sentence; however, it does not appear in the training data, so the model labels the example
incorrectly. This is problematic, as there is no way to find training data for every possible
sentiment word, especially in social media data where novel words and typos are a frequent
occurrence. Our model’s ability to generalize about the polarity of snippets describing a
particular aspect allows it to predict sentiment values for words of unknown polarity. For
example, if there are already several positive snippets describing a particular aspect, the
system can guess that a snippet with unknown polarity will likely also be positive.
6.1.5 Per-Word Labeling Accuracy
The goal of this task is to evaluate whether each word is correctly identified as an aspect
word, value word, or background word. This distinction is crucial in order to achieve
correctness of both clustering and sentiment analysis, so errors here may help us identify
weaknesses of our model.
118

Automatic Aggregation by Joint Modeling of Aspects and Values

The rolls also were n’t
very well made .
:::::::: :::: ::::::
The pita was ::::::::
beyond dry
and :::::::
tasted like
cardboard !
:::
:::::::::::::::
falafel !
The Falafel King has the best
::::
The rolls with spicy mayo were not
so good .
::: :: :::::
!
Ordered the spicy tuna and california roll – they were amazing
:::::::::

Table 5: Correct annotation of a set of phrases containing elements which may be confusing,
on which annotators are tested before they are allowed to annotate the actual test data. Aspect words are colored blue and underlined; value words are colored orange and underlined
with a wavy line. Some common mistakes include: annotating n’t as background (because
it is attached to the background word was), annotating cardboard as an aspect because it
is a noun, annotating Falafel King as an aspect because it is in subject position.
Annotation Per-word annotation is acquired from Mechanical Turk. The per-word labeling task seems difficult for some Turk annotators, so we implement a filtering procedure
to ensure that only high-quality annotators are allowed to submit results. Specifically, we
ask annotators to produce labels for a set of “difficult” phrases with known labels (shown
in Table 5). Those annotators who successfully produced correct or mostly-correct annotations are allowed to access the annotation tasks containing new phrases. Each of these
unknown tasks is presented to 3 annotators, and the majority label is taken for each word.
In total, we test on 150 labeled phrases, for a total of 7,401 labeled words.
Baseline The baseline for this task relies again on the intuition that part-of-speech is a
useful proxy for aspect and value identification. We know that aspects usually represent
concrete entities, so they are often nouns, and value words are descriptive or counting, so
they are often adjectives or adverbs. Therefore, we again use the MXPOST tagger to find
POS for each word in the snippet. For the main baseline, Tags-Full, we assign each noun
(NN*) an aspect label, and each numeral, adjective, adverb, or verb participle (CD, RB*,
JJ*, VBG, VBN) a value label. For comparison, we also present results for a smaller tagset,
Small-Tags, labeling only nouns (NN*) as aspect and adjectives (JJ*) as values. Note that
each of the tags added in the Tags-Full baseline are beneficial to the baseline’s score.
Tree expansion Because our full model and the baselines are all designed to pick out
relevant individual words rather than phrases, they may not correspond well to the phrases
which humans have selected as relevant. Therefore, we also evaluate on a set of expanded
labels identified with parse trees from the Stanford Parser (Klein & Manning, 2003).8 Specifically, for each non-background word, we identify the largest containing noun phrase (for
both aspects and values) or adjective or adverb phrase (for values only) which does not
also contain oppositely-labeled words. For example, in the noun phrase blackened chicken,
if chicken was labeled as an aspect word and blackened was labeled as a background word,
both will now be labeled as aspect words. However, in the noun phrase tasty chicken where
“tasty” is already labeled as a value, the label will not be changed and no further expansion
will be attempted. As a final heuristic step, any punctuation, determiners, and conjunctions
8. Available at http://nlp.stanford.edu/software/lex-parser.shtml.

119

Sauper & Barzilay

Tree expansion procedure for aspect words:
1. Find noun phrases which contain the aspect word (pork).
The

+ innovative

::::::::::::

appetizers and the pork with apple glaze were the
NP1
NP2
NP3

+ highlights
:::::::::::

2. Select the largest noun phrase that does not contain value (sentiment) words.
• NP1 is valid; it does not contain value words. However, it is not the largest valid NP.
• NP2 is valid; it does not contain value words. It is the largest valid NP, so it is selected.
• NP3 contains a value word (+ innovative),
so it is invalid.
::::::::::::

3. Convert all background words within the selected noun phrase to aspect words
except punctuation, determiners, and conjunctions.
The

+ innovative

::::::::::::

appetizers and the pork with apple glaze were the

+ highlights
:::::::::::

Figure 11: The tree expansion procedure for value words, with an example snippet. The
procedure is similar for aspect words, except adjective phrases and adverb phrases are also
considered for expansion.
Aspect
Precision Recall

F1

Value
Precision Recall

F1

Tags-Small
Tree

79.9
74.0

79.5
83.0

79.7
78.2

78.5
79.2

45.0
57.4

57.2
66.5

Tags-Full
Tree

79.9
75.6

79.5
81.4

79.7
78.4

78.1
77.1

68.7
70.1

73.1
73.4

85.2
79.5

52.6
71.9

65.0
75.5

70.5
76.7

61.6
70.9

65.7
73.7

Our model
Tree

Table 6: Per-word labeling precision and recall of our model compared to the Tags-Small
and Tags-Full baselines, both with and without expansion by trees. Our model is most
precise on aspect and has better recall on value. Note that in general the process of expanding labels with the tree structure increases recall at the expense of precision.
which would be newly labeled as aspect or value words are ignored and kept as background
words. The steps of this procedure with an illustrative example are shown in Figure 11.
Results We evaluate all systems on precision and recall for aspect and value separately.
Results for all systems are shown in Table 6. Our model without the tree expansion is
highly precise at the expense of recall; however when the expansion is performed, its recall
improves tremendously, especially on value words.
While this result is initially disappointing, it is possible to adjust model parameters to
increase performance at this task; for example, for aspect words we could put additional
120

Automatic Aggregation by Joint Modeling of Aspects and Values

The moqueca was delicious
and :::::::
perfect winter food , ::::::
warm , filling and hearty but :::
not :::
too ::::::
heavy .
:::::::::
The bacon wrapped almond dates were ::::::::
amazing but the plantains with cheese were boring
.
::::::
the artichoke and homemade pasta appetizers were :::::
great

Table 7: High-precision, low-recall aspect word labeling by our full model. Note that a
human would likely identify complete phrases such as bacon wrapped almond dates and
homemade pasta appetizers; however, the additional noise degrades performance on the
clustering task.

start
A
V
B
I

A
0.06
0.19
0.02
0.22
0.00

V
0.00
0.03
0.32
0.26
0.00

B
0.94
0.77
0.47
0.43
0.01

I
0.00
0.01
0.01
0.17
0.99

end
0.00
0.00
0.18
0.06
0.00

Table 8: Learned transition distribution from our model. The pattern of high-precision of
aspect words is represented by a preference against continuing a string of several aspect
words, causing the model to prefer single, precise aspect words. Likewise, the better recall
of value words is indicated by a higher value of the V V transition, which can encourage
several words in a row to be marked as value words.
i,j,w
mass on the prior for ZW
= A or increase the Dirichlet hyperparameter λA . However,
while this increases performance on the word labeling task, it also decreases performance
correspondingly on the clustering task. By examination of the data, this correlation is
perfectly reasonable. In order to succeed at the clustering task, the model selects only the
most relevant portions of the snippet as aspect words. When the entire aspect and value
are identified, clustering becomes noisy. Table 7 shows some examples of the high-precision
labeling which achieves high clustering performance, and Table 8 shows an example of the
learned transition distribution which creates this labeling.

6.2 Aspect Identification with Shared Aspects
Our second task uses a simplified version of our model designed for aspect identification
only. For this task, we use a corpus of medical visit summaries. In this domain, each
summary is expected to contain similar relevant information; therefore, the set of aspects is
shared corpus-wide. To evaluate our model in this formulation, we examine the predicted
clusters of snippets, as in the full model.
6.2.1 Data Set
Our data set for this task consists of phrases selected from dictated patient summaries at the
Pediatric Environmental Health Clinic (PEHC) at Children’s Hospital Boston, specializing
in treatment of children with lead poisoning. Specifically, after a patient’s office visit and lab
results are completed, a PEHC doctor dictates a letter to the referring physician containing
121

Sauper & Barzilay

information about previous visits, current developmental and family status, in-office exam
results, lab results, current diagnosis, and plan for the future.
For this experiment, we select phrases from the in-office exam and lab results sections
of the summaries. Phrases are separated heuristically on commas and semicolons. In a domain which contains a significant amount of extraneous information, such as the restaurant
domain, we must extract phrases which we believe bear some relevance to the task at hand.
However, because the medical text is dense and nearly all relevant, a heuristic separation
is sufficient to extract relevant phrases. There are 6198 snippets in total, taken from 271
summaries. The average snippet length is 4.5 words, and there are an average of 23 snippets
per summary. As in the Yelp domain, we use the MXPOST tagger (Ratnaparkhi, 1996)
to gain POS tags. Figure 12 shows some example snippets. For this domain, there are
no values; we simply concentrate on the aspect-identification task. Unlike the restaurant
domain, we use no seed words.
6.2.2 Domain Challenges and Modeling Techniques
In contrast to the restaurant domain, the medical domain uses a single global set of aspects.
These represent either individual lab tests (e.g., lead level, white blood cell count) or particular body systems (e.g., lungs or cardiovascular ). Some aspects are far more common than
others, and it is very uncommon for a summary to include more than one or two snippets
about any given aspect. Therefore, as mentioned in Section 4.2, we model the aspect word
distributions and the aspect multinomial as shared between all entities in the corpus.
Also in contrast to the restaurant domain, aspects are defined by words taken from the
entire snippet. Rather than having aspects only associated with names of measurements
(e.g., ‘weight’), units and other descriptions of measurement (e.g., ‘kilograms’) are also
relevant for aspect definition. This property extends to both numeric and written measurements; for example, the aspect ‘lungs’ is commonly described as ‘clear to auscultation
bilaterally’. In order to achieve high performance, our model must leverage all of these
clues to provide proper aspect identification when the name of the measurement is missing
(e.g., “patient is 100 cm”). While part of speech will still be an important factor to model,
we predict that there will be greater importance on additional parts of speech other than
nouns.
Finally, our data set is noisy and contains some irrelevant snippets, such as section
headings (e.g., “Physical examination and review of systems”) or extraneous information.
As described in Section 4.2, we modify our model so that it can ignore partial or complete
snippets.
6.2.3 Cluster Prediction
As for joint aspect and sentiment prediction, the goal of this task is to evaluate the quality
of aspect identification. Because the aspects are shared across all documents, clusters are
generally much larger, and the set of annotated snippets represents only a fraction of each
cluster.
Annotation For this experiment, we use a set of gold clusters gathered over 1,200 snippets, annotated by a doctor who is an expert in the domain from the Pediatric Environmental Health Clinic at Children’s Hospital Boston. Note that as mentioned before, clusters
122

Automatic Aggregation by Joint Modeling of Aspects and Values

He was 113 cm in height
Patient’s height was 146.5 cm
Lungs: Clear bilaterally to auscultation
lungs were normal
Heart regular rate and rhythm; no murmurs
Heart normal S1 S2

Figure 12: Example snippets from the medical data set, grouped according to aspect.
Aspect words are underlined and colored blue. This grouping and labeling are not given in
the data set and must be learned by the model.

Cluster-All
Cluster-Noun
Our model

Precision

Recall

88.2
88.4
89.1

93.0
83.9
93.4

F1
90.5
86.1
91.2

Table 9: Results using the MUC metric on cluster prediction for the aspect identification
only task. Note that the Cluster-All baseline significantly outperforms Cluster-Noun,
the opposite of what we observe in the joint aspect and value prediction task. This is due
to the dependence of aspect identification on more than just the name of a lab test, such
as the units or other description of the test results, as mentioned in Section 6.2.2.

here are global to the domain (e.g., many patients have snippets representing blood lead
level, and these are all grouped into one cluster). The doctor was asked to cluster 100
snippets at a time (spanning several patients), as clustering the entire set would have been
infeasible for a human annotator. After all 12 sets of snippets were clustered, the resulting
clusters were manually combined to match up similar clusters from each set. For example, the blood lead level cluster from the first set of 100 snippets was combined with the
corresponding blood lead level clusters from each other set of snippets. Any cluster from
this final set with fewer than 5 members was removed. In total, this yields a gold set of
30 clusters. There are 1,053 snippets total, for an average of 35.1 snippets per cluster. To
match this, baseline systems and our full model are asked to produce 30 clusters across the
full data set.
Baselines & Metric To keep these results consistent with those on the previous task,
we use the same baselines and evaluation metric. Both baselines rely on a TF*IDFweighted clustering algorithm, specifically implemented with CLUTO package (Karypis,
2002) using agglomerative clustering with the cosine similarity distance metric. As before,
Cluster-All represents a baseline using unigrams of snippets from the entire data set,
while Cluster-Noun works over only the nouns from the snippets. We again use the
MUC cluster evaluation metric for this task. For more details on both baselines and the
evaluation metric, please see Section 6.1.3.
123

Sauper & Barzilay

Results For this experiment, our system demonstrates an improvement of 7% over the
Cluster-All baseline. Absolute performance is relatively high for all systems in the
medical domain, indicating that the lexical clustering task is less misleading than in the
restaurant domain. It is interesting to note that unlike in the restaurant domain, the
Cluster-All baseline outperforms the Cluster-Noun baseline. As mentioned in Section 6.2.2, the medical data is notable for the relevance of the entire snippet for clustering
(e.g., both ‘weight’ and ‘kilograms’ are useful to identify the weight aspect). Because of
this property, using only nouns to cluster in the Cluster-Noun baseline hurts performance
significantly.

7. Conclusions and Future Work
In this paper, we have presented an approach for fine-grained content aggregation using
probabilistic topic modeling techniques to discover the structure of individual text snippets.
Our model is able to successfully identify clusters of snippets in a data set which discuss
the same aspect of an entity as well as the associated values (e.g., sentiment). It requires
no annotation, other than a small list of seed vocabulary to bias the positive and negative
distributions in the proper direction.
Our results demonstrate that delving into the structure of the snippet can assist in
identifying key words which are important and unique to the domain at hand. When there
are values to be learned, the joint identification of aspect and value can help to improve the
quality of the results. The word labeling analysis reveals that the model learns a different
type of labeling for each task; specifically, a strict, high-precision labeling for the clustering
task and a high-recall labeling for sentiment. This follows the intuition that it is important
to identify specific main points for clustering, while in the sentiment analysis task, there
may often be several descriptions or conflicting opinions presented which all need to be
weighed together to determine the overall sentiment.
This model admits a fast, parallelized inference procedure. Specifically, the entire inference procedure takes roughly 15 minutes to run on the restaurant corpus and less than 5
minutes on the medical corpus. Additionally, the model is neatly extensible and adjustable
to fit the particular characteristics of a given domain.
There are a few limitations of this model which can be improved with future work:
First, our model makes no attempt to explicitly model negation or other word interactions,
increasing the difficulty of both aspect and sentiment analysis for our model. By performing error analysis, we find that negation is a common source of error for the sentiment
analysis task. Likewise, on the aspect side, the model can make errors when attempting to
differentiate aspects such as ice cream and cream cheese which share the common aspect
word cream, despite these phrases occurring as bigrams. By using these connections in a
stronger way, such as with an indicator variable for negation or a higher-order HMM, the
model could make more informed decisions.
Second, while defining aspects per-entity as in the restaurant domain has advantages in
that it is possible to get a very fine-grained set of applicable aspects, it also fails to leverage
some potential information in the data set. Specifically, we know that restaurants sharing
the same type (e.g., Italian, Indian, Bakery, etc.) should share some common aspects;
however, there are no ties between them in the current model. Likewise, even at a global
124

Automatic Aggregation by Joint Modeling of Aspects and Values

level, there may be some aspects which tie in across all restaurants. A hierarchical version
of this model would be able to tie these together and identify different types of aspects:
global (e.g., presentation), type-level (e.g., pasta for the Italian type), and restaurant-level
(e.g., the restaurant’s special dish).

Bibliographic Note
Portions of this paper have been published previously in a conference publication (Sauper,
Haghighi, & Barzilay, 2011); however this paper significantly extends that work. We describe several model generalizations and extensions (Section 4.2) and their effects on our
inference procedure (Section 5.2). We present new experimental results, including additional baseline comparisons and an additional experiment (Section 6.1). We also introduce
a new domain, medical summary text, which is quite different than the domain of restaurant
reviews and therefore requires several fundamental changes to the model (Section 6.2).

Acknowledgments
The authors acknowledge the support of the NSF (CAREER grant IIS-0448168), NIH (grant
5-R01-LM009723-02), Nokia, and the DARPA Machine Reading Program (AFRL prime
contract no. FA8750-09-C-0172). Thanks to Peter Szolovits and the MIT NLP group for
their helpful comments. Any opinions, findings, conclusions, or recommendations expressed
in this paper are those of the authors, and do not necessarily reflect the views of the funding
organizations.

References
Barzilay, R., McKeown, K. R., & Elhadad, M. (1999). Information fusion in the context of
multi-document summarization. In Proceedings of ACL, pp. 550–557.
Blei, D. M., & McAuliffe, J. (2008). Supervised topic models. In Advances in NIPS, pp.
121–128.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of
Machine Learning Research, 3, 993–1022.
Carenini, G., & Moore, J. D. (2006). Generating and evaluating evaluative arguments.
Artificial Intelligence, 170, 925–952.
Carenini, G., Ng, R., & Pauls, A. (2006). Multi-document summarization of evaluative text.
In Proceedings of EACL, pp. 305–312.
Carenini, G., Ng, R. T., & Zwart, E. (2005). Extracting knowledge from evaluative text.
In Proceedings of K-CAP, pp. 11–18.
Chen, H., Benson, E., Naseem, T., & Barzilay, R. (2011). In-domain relation discovery with
meta-constraints via posterior regularization. In Proceedings of ACL, pp. 530–540.
Chen, H., Branavan, S. R. K., Barzilay, R., & Karger, D. R. (2009). Global models of
document structure using latent permutations. In Proceedings of ACL/HLT, pp. 371–
379.
125

Sauper & Barzilay

Crammer, K., & Singer, Y. (2001). Pranking with ranking. In Advances in NIPS, pp.
641–647. MIT Press.
Dang, H. T. (2005). Overview of DUC 2005. In Proceedings of DUC at EMNLP/HLT.
Dang, H. T. (2006). Overview of DUC 2006. In Proceedings of DUC at NAACL/HLT.
Dave, K., Lawrence, S., & Pennock, D. M. (2003). Mining the peanut gallery: opinion
extraction and semantic classification of product reviews. In Proceedings of WWW,
pp. 519–528.
Hu, M., & Liu, B. (2004). Mining and summarizing customer reviews. In Proceedings of
SIGKDD, pp. 168–177.
Karypis, G. (2002). CLUTO a clustering toolkit. Tech. rep. 02-017, Dept. of Computer
Science, University of Minnesota. Available at http://www.cs.umn.edu˜cluto.
Kim, H. D., & Zhai, C. (2009). Generating comparative summaries of contradictory opinions
in text. In Proceedings of CIKM, pp. 385–394.
Kim, S., & Hovy, E. (2005). Automatic detection of opinion bearing words and sentences.
In Proceedings of IJCNLP, pp. 61–66.
Kim, S.-M., & Hovy, E. (2006). Automatic identification of pro and con reasons in online
reviews. In Proceedings of COLING ACL, pp. 483–490.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. In Proceedings of ACL,
pp. 423–430.
Liu, B., Hu, M., & Cheng, J. (2005). Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of WWW, pp. 342–351.
Lu, Y., & Zhai, C. (2008). Opinion integration through semi-supervised topic modeling. In
Proceedings of WWW, pp. 121–130.
Mani, I. (2001). Automatic summarization, Vol. 3. John Benjamins Pub Co.
McDonald, R., Hannan, K., Neylon, T., Wells, M., & Reynar, J. (2007). Structured models
for fine-to-coarse sentiment analysis. In Proceedings of ACL, pp. 432–439.
Mei, Q., Ling, X., Wondra, M., Su, H., & Zhai, C. (2007). Topic sentiment mixture: modeling
facets and opinions in weblogs. In Proceedings of WWW, pp. 171–180.
Pang, B., & Lee, L. (2004). A sentimental education: Sentiment analysis using subjectivity
summarization based on minimum cuts. In Proceedings of ACL, pp. 271–278.
Pang, B., & Lee, L. (2008). Opinion mining and sentiment analysis. Foundations and Trends
in Information Retrieval, 2, 1–135.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of EMNLP, pp. 79–86.
Popescu, A.-M., Nguyen, B., & Etzioni, O. (2005). OPINE: Extracting product features
and opinions from reviews. In Proceedings of EMNLP/HLT, pp. 339–346.
Radev, D., & McKeown, K. (1998). Generating natural language summaries from multiple
on-line sources. Computational Linguistics, 24 (3), 469–500.
126

Automatic Aggregation by Joint Modeling of Aspects and Values

Radev, D. R., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization of multiple documents: sentence extraction, utility-based evaluation, and user studies. In
Proceedings of the NAACL-ANLP Workshop on Automatic Summarization, pp. 21–
30.
Ratnaparkhi, A. (1996). A maximum entropy model for part-of-speech tagging. In Proceedings of EMNLP, pp. 133–142.
Sauper, C., Haghighi, A., & Barzilay, R. (2010). Incorporating content structure into text
analysis applications. In Proceedings of EMNLP, pp. 377–387.
Sauper, C., Haghighi, A., & Barzilay, R. (2011). Content models with attitude. In Proceedings of ACL, pp. 350–358.
Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2005). Multi-document summarization with
subjectivity analysis at DUC 2005. In Proceedings of DUC at EMNLP/HLT.
Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2006). Opinion-focused summarization and
its analysis at DUC 2006. In Proceedings of DUC at NAACL/HLT, pp. 122–130.
Snyder, B., & Barzilay, R. (2007). Multiple aspect ranking using the good grief algorithm.
In Proceedings of NAACL/HLT, pp. 300–307.
Titov, I., & McDonald, R. (2008a). A joint model of text and aspect ratings for sentiment
summarization. In Proceedings of ACL, pp. 308–316.
Titov, I., & McDonald, R. (2008b). Modeling online reviews with multi-grain topic models.
In Proceedings of WWW, pp. 111–120.
Turney, P. D. (2002). Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews. In Proceedings of ACL, pp. 417–424.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). A modeltheoretic coreference scoring scheme. In Proceedings of MUC, pp. 45–52.
Yu, H., & Hatzivassiloglou, V. (2003). Towards answering opinion questions: Separating
facts from opinions and identifying the polarity of opinion sentences. In Proceedings
of EMNLP, pp. 129–136. Association for Computational Linguistics.

127

Journal of Artificial Intelligence Research 46 (2013) 203-233

Submitted 06/12; published 02/13

Integrative Semantic Dependency Parsing via Eﬃcient
Large-scale Feature Selection
Hai Zhao
Xiaotian Zhang

zhaohai@cs.sjtu.edu.cn
xtian.zh@gmail.com

Shanghai Jiao Tong University,
800 Dongchuan Road, Shanghai,China

Chunyu Kit

ctckit@cityu.edu.hk

City University of Hong Kong,
Tat Chee Avenue, Kowloon, Hong Kong SAR, China

Abstract
Semantic parsing, i.e., the automatic derivation of meaning representation such as an
instantiated predicate-argument structure for a sentence, plays a critical role in deep processing of natural language. Unlike all other top systems of semantic dependency parsing
that have to rely on a pipeline framework to chain up a series of submodels each specialized for a speciﬁc subtask, the one presented in this article integrates everything into one
model, in hopes of achieving desirable integrity and practicality for real applications while
maintaining a competitive performance. This integrative approach tackles semantic parsing as a word pair classiﬁcation problem using a maximum entropy classiﬁer. We leverage
adaptive pruning of argument candidates and large-scale feature selection engineering to
allow the largest feature space ever in use so far in this ﬁeld, it achieves a state-of-the-art
performance on the evaluation data set for CoNLL-2008 shared task, on top of all but one
top pipeline system, conﬁrming its feasibility and eﬀectiveness.

1. Introduction
The purpose of semantic parsing is to derive the meaning representation for a sentence,
usually taking a syntactic parse as input. A popular formalism to represent this kind of
meaning is predicate-argument structure and, accordingly, the parsing is to instantiate the
predicate and argument(s) in such a structure properly with actual words or phrases from
a given sentence. In the context of dependency parsing, it becomes semantic dependency
parsing, which takes a syntactic dependency tree as input and outputs a ﬁlled predicateargument structure for a predicate, with each argument word properly labeled with its
semantic role in relation to the predicate.
Semantic role labeling (SRL) is one of the core tasks in semantic dependency parsing,
be it dependency or constituent based. Conventionally, it is tackled mainly through two
subtasks, namely, argument identiﬁcation and classiﬁcation. Conceptually, the former determines whether a word is a true argument of a predicate, and the latter what semantic
role it plays in relation to the predicate (or which argument it instantiates in a predicateargument structure). When no predicate is given, two other indispensable subtasks are
predicate identiﬁcation and disambiguation, one to identify which word is a predicate in
a sentence and the other to determine the predicate-argument structure for an identiﬁed
predicate in a particular context.
c
2013
AI Access Foundation. All rights reserved.

Zhao, Zhang & Kit

A pipeline framework was adopted in almost all previous researches to handle these subtasks one after another. The main reason for dividing the whole task of semantic dependency
parsing into multiple stages in this way is twofold: maintaining computational eﬃciency and
adopting diﬀerent favorable features for each subtask. In general, a joint learning system
of multiple components is slower than a pipeline system, especially in training. It is also
reported by Xue and Palmer (2004) that diﬀerent features do favor diﬀerent subtasks of
SRL, especially argument identiﬁcation and classiﬁcation. The results from the CoNLL
shared tasks in 2005 and 2008 (Carreras & Màrquez, 2005; Koomen, Punyakanok, Roth, &
Yih, 2005; Surdeanu, Johansson, Meyers, Màrquez, & Nivre, 2008; Johansson & Nugues,
2008) seem to suggest that the pipeline strategy has been the benchmark of technology for
the state-of-the-art performance on this speciﬁc NLP task.
When most SRL systems are pipeline, an integrated SRL system holds its unique merits, e.g., integrity of implementation, practicality for real applications, a single-stage feature
selection beneﬁting the whole system, an all-in-one model outputting all expected semantic role information, and so on. In particular, it takes into account the interactive eﬀect
of features favoring diﬀerent subtasks and hence holds a more comprehensive view of all
features working together as a whole. This article is intended to present our recent research to explore the feasibility of constructing an eﬀective integrated system for semantic
dependency parsing that melds all subtasks together into one, including predicate identiﬁcation/disambiguation and argument identiﬁcation/classiﬁcation, for both verbal and nominal
predicates, and uses the same feature set for all these subtasks. The core of our research is
to verify, through practical implementation and then empirical evaluation, the methodological soundness and eﬀectiveness of this approach. Its success, however, has to be rooted in
a solid technical foundation, i.e., a large-scale engineering procedure for eﬃcient mining of
eﬀective feature templates from a huge set of feature candidates, a feature space far richer
than others ever used before. It is this piece of engineering that brings the potentials of
this integrative approach into full play. Another focus of this article is hence to illustrate
its technical essentials.
Nevertheless, it is worth pointing out that the term integrative, when used in opposite to
pipeline, can be misleading to mean that all subtasks are carried out jointly in a single run.
Instead, it is used to highlight the integrity of our model and its implementation that uses
a single representation and feature set to accommodate all these subtasks. Although this
approach has its unique advantages in simplifying system engineering and feature selection,
the model we have implemented and will present below is not a joint one to accomplish
the whole semantic parsing through synchronous determination of both predicates and
arguments. These two types of indispensable objects in a semantic parse tree are recognized
in succession through decoding using the same trained model.
The rest of the article is organized as follows. Section 2 gives a brief overview of related
work, providing the background of our research. Section 4 presents our approach of adaptive
pruning of argument candidates to generate head-dependent word pairs for both training
and decoding, which underlies the whole process of semantic parsing. The other two key
procedures to optimize the parsing, namely, feature selection and decoding, are presented
in Section 5 and 6, respectively. The details of evaluation, including evaluation data, experimental results and a comprehensive comparative analysis of the results, are presented
204

Semantic Dependency Parsing

in Section 7. Finally, Section 8 concludes our research, highlighting its contributions and
the practicality and competitiveness of this approach.

2. Related Work
Note that SRL has almost become a surrogate for semantic dependency parsing in the literature of recent years. Most recent research eﬀorts in this ﬁeld, including the CoNLL shared
tasks in 2004 and 2005, have been focused on verbal predicates, thanks to the availability of
PropBank (Palmer, Gildea, & Kingsbury, 2005). As a complement to PropBank, NomBank
(Meyers, Reeves, Macleod, Szekely, Zielinska, Young, & Grishman, 2004) annotates nominal predicates and their correspondent semantic roles using a similar semantic framework.
Although oﬀering more challenges, SRL for nominal predicates has drawn relatively little
attention (Jiang & Ng, 2006). The issue of merging various treebanks, including PropBank,
NomBank and others, was once discussed in the work of Pustejovsky, Meyers, Palmer,
and Poesio (2005). The idea of merging these two treebanks was put into practice for the
CoNLL-2008 shared task (Surdeanu et al., 2008). The best system in CoNLL-2008 used two
diﬀerent subsystems to cope with verbal and nominal predicates, respectively (Johansson &
Nugues, 2008). Unfortunately, however, there has been no other integrative approach than
ours to illustrate a performance so close to that of this system.
In fact, there have been few research eﬀorts in this direction, except a recent one on
joint identiﬁcation of predicates, arguments and senses by Meza-Ruiz and Riedel (2009).
They formulate the problem into a Markov Logic Network, with weights learnt via 1-best
MIRA (Crammer & Singer, 2003) Online Learning method, and use Cutting Plane Inference
(Riedel, 2008) with Integer Linear Programming (ILP) as the base solver for eﬃcient joint
inference of the best choice of predicates, frame types, arguments and role labels with
maximal a posteriori probability. Using CoNLL-2008 data, their system achieves its best
semantic F1 80.16% on the WSJ test set. This is 0.75 percentage point lower than ours, to
be reported below, on the whole WSJ+Brown test set. Note that when trained on CoNLL2008 training corpus, a subset of WSJ corpus, an SRL system has a performance at least 10
percentage points higher on the WSJ than on the Brown test set (Surdeanu et al., 2008).
Both CoNLL-2008 and -2009 shared tasks1 are devoted to the joint learning of syntactic
and semantic dependencies, aimed at testing whether SRL can be well performed using
only dependency syntax input. The research reported in this article focuses on semantic
dependency parsing. To conduct a valid and reliable evaluation, we will use the data set and
evaluation settings of CoNLL-2008 and compare our integrated system, which is the best
SRL system in CoNLL-2009 (Zhao, Chen, Kit, & Zhou, 2009), against the top systems in
CoNLL shared tasks (Surdeanu et al., 2008; Hajič, Ciaramita, Johansson, Kawahara, Martı́,
Màrquez, Meyers, Nivre, Padó, Štěpánek, Straňák, Surdeanu, Xue, & Zhang, 2009).2 Note
that these systems achieved higher performance scores in CoNLL-2008 than in CoNLL-2009.
An integrative approach to dependency semantic parsing has its own pros and cons. To
deal with its main drawbacks, two key techniques need to be applied for the purpose of
1. Henceforth referred to as CoNLL-2008 and -2009, respectively.
2. CoNLL-2008 is an English-only task, while CoNLL-2009 is a multilingual one. Although both use the
same English corpus, except some more-sophisticated structures for the former (Surdeanu et al., 2008),
their main diﬀerence is that semantic predicate identiﬁcation is not required for the latter.

205

Zhao, Zhang & Kit

eﬃciency enhancement. One is to bring in auxiliary argument labels that enable further
improvement of argument candidate pruning. This signiﬁcantly facilitates the development
of a fast and lightweight SRL system. The other is to apply a greedy feature selection
algorithm to perform the task of feature selection from a given set of feature templates.
This helps ﬁnd as many features as possible that are of beneﬁt to the overall process of the
parsing. Many individual optimal feature template sets are reported in the literature to
have achieved an excellent performance on speciﬁc subtasks of SRL. This is the ﬁrst time
that an integrated SRL system is reported to produce a result so close to the state of the
art of SRL achieved by those pipelines with individual sub-systems each highly specialized
for a speciﬁc subtask or a speciﬁc type of predicate.

3. System Architecture
Dependencies between words in a sentence, be they syntactic or semantic, can be formulated
as individual edges in an abstract graph structure. In practice, a dependency edge has to be
built, and its type (usually referred to as its label) to be identiﬁed, through proper learning
and then decoding. Most conventional syntactic parsing makes use of a property of projectiveness stipulated by the well-formedness of a syntactic tree. In contrast, in dependency
parsing, new dependencies have to be built with regard to existing ones. However, this is
not the case for semantic parsing, for most semantic parsing results are not projective trees.
Instead, they are actually directed acyclic graphs, because the same word can serve as an
argument for multiple predicates. Inevitably, a learning model for semantic parsing has to
take all word pairs into account when exploring possible dependent relationships.
SRL as a speciﬁc task of semantic dependency parsing can be formulated as a word pair
classiﬁcation problem and tackled with various machine learning models, e.g., the Maximum
Entropy (ME) model as used by Zhao and Kit (2008). The ME model is also used in this
work but only for probability estimation to support the global decoding given below in
Section 6, which extends our model beyond a sequential model. Without any constraint,
a classiﬁer for this task has to deal with all word pairs in an input sequence and is thus
inevitably prone to poor computational eﬃciency and also unsatisfactory performance. A
straightforward strategy to alleviate these problems is to perform proper pruning on both
the training sample and test data.
A word pair consists of a word as semantic head and another as semantic dependent,
which are conventionally denoted as p (for predicate) and a (for argument), respectively. We
will follow this convention in the feature representation below. Since our approach uniﬁes
the two tasks of SRL, namely, predicate identiﬁcation/disambiguation and argument identiﬁcation/classiﬁcation, into one classiﬁcation framework, there is no need to diﬀerentiate
between verbal and non-verbal heads, because they are all handled in the same way. This
is one of the unique characteristics of our integrated system.
The overall architecture of our system is depicted in Figure 1. An input sentence from
a data set in use, be it a training, a development or a test set, is parsed into a word pair
sequence by a word pair generator using a pruning algorithm, e.g., the adaptive pruning
described below, to eliminate useless pairs. Word pairs so generated from each sentence of
the training set are used to train a word pair classiﬁer, which then supports the decoding
formulated in Section 6 to search for an optimal set of word pairs from a test sentence to
206

Semantic Dependency Parsing

Data set

Feature template set

Word pair generator

Feature selection procedure
Training

Word pair sequence

Selected feature set

Word pair classifier

Predicate decoding
Argument decoding
Yes

Any more predicate?

Next sentence

No

Figure 1: Illustration of system architecture and work ﬂow of training and testing
form a semantic parse tree. The decoding ﬁrst recognizes all predicates in a sentence and
then determines the arguments for each predicate by a beam search for their argument role
labels. The features used in the classiﬁer are selected from a predeﬁned feature space by a
greedy selection procedure using the training and the development set for repeated training
and testing to reﬁne a candidate feature set until no more performance gain is achievable
(see Section 5). Then the classiﬁer obtained this way with the selected features is tested on
the test set.

4. Adaptive Argument Pruning
Word pairs are derived from a sentence for the classiﬁer in the following ways. (1) For
predicate identiﬁcation/disambiguation, each word pair consists of the virtual root (VR) of
a semantic parse tree under construction (whose root is virtually preset), as head, and a
predicate candidate as its dependent. Theoretically, all words in the sentence in question
can be a predicate candidate. To reduce their number, we opt for a simple POS tag pruning
strategy that only verbs and nouns are allowed as predicate candidates. (2) For argument
identiﬁcation/classiﬁcation, each word pair consists of an identiﬁed predicate, as head, and
another word as its dependent (or its argument, in conventional term). Potentially, any
other word in the same sentence can be its argument candidate. Pruning oﬀ as many
argument candidates as possible is thus particularly signiﬁcant in improving the eﬃciency
and performance of the classiﬁer.
There are two ways to collect argument candidates for a given predicate, one from the
syntactic dependency tree and the other from the linear path of an input sentence. For
the former (referred to as synPth hereafter), we use a dependency version of the pruning
algorithm by Xue and Palmer (2004), which is given as follows with a necessary modiﬁcation
to allow a predicate itself also to be included in its own argument candidate list, because a
nominal predicate sometimes takes itself as its own argument.
207

Zhao, Zhang & Kit

ID
1
2
3
4
5
6
7
8
a.
b.
c.
d.
e.

FORMa
Investor
focus
shifted
quickly
,
traders
said
.

LEMMA
investor
focus
shift
quickly
,
trader
say
.

POS
NN
NN
VBD
RB
,
NNS
VBD
.

HEADb
2
3
7
3
7
7
0
7

DEPRELc
NMOD
SBJ
OBJ
MNR
P
SBJ
ROOT
P

PREDd

ARG Labele
A0

focus.01
shift.01

A1
A1
AM-MNR
A0

say.01

Word form, or token.
Syntactic head of the current token, identiﬁed by an ID.
Syntactic dependency relation of the current token to its HEAD.
Roleset of a semantic predicate.
Argument labels for semantic predicates in text order.

Table 1: An example of input sentence from CoNLL-2008 shared task data set
Initialization: Given a predicate as the current node in a syntactic dependency tree.
1. Collect all its syntactic children as argument candidates, by traversing the children
from left to right.
2. Reset the current node to its syntactic head and repeat Step 1 till the root of the tree.
3. Collect the root and stop.
This algorithm is eﬀective in collecting both words in the path from a given predicate
to the root and their children as argument candidates. However, a more eﬃcient one is still
needed to lend stronger support to our SRL system that is designed to tackle argument
identiﬁcation/classiﬁcation in a single stage. Following the observation that arguments
usually tend to surround their predicate in a close distance, the auxiliary label noMoreArg
is introduced to signify where the pruning stops collecting argument candidates. For training
sample generation, this label is assigned to the next word as soon as the arguments of the
current predicate have been saturated with previously collected words, in light of the original
training data as illustrated in Table 1. Accordingly, the pruning process stops collecting
any more candidates. For decoding, it signals the decoder to stop searching, along a similar
traverse as the pruning, for any more arguments for an identiﬁed predicate. This adaptive
technique improves the pruning eﬃciency signiﬁcantly, saving about 1/3 training time and
memory at the cost of missing very few more true arguments than the pruning without
this label, according to our experiments. The training sample generated this way from the
sentence in Table 1, by means of both POS pruning and the above pruning algorithm, is
illustrated in Table 2, with a few class labels in the third column.
To collect argument candidates along the linear path (referred to as linPth hereafter)
instead of the syntactic tree of a sentence, the classiﬁer will search through all words around
a given predicate. In a way similar to how the pruning along synPth is improved, two
auxiliary labels, namely, noMoreLeftArg and noMoreRightArg, are introduced to signify
where the adaptive pruning along linPth stops, skipping those words too far away from the
predicate. Given below is an example to illustrate how these two labels are used, where e in
208

Semantic Dependency Parsing

Head-dependent word pair
VR
Investor
VR
focus
VR
shifted
VR
traders
VR
said
focus
Investor
focus
focus
shifted focus
shifted quickly
shifted said
said
shifted
said
,
said
traders
said
.

Label
NONE PRED
01
01
NONE PRED
01
A0
noMoreArg
A1
AM-MNR
noMoreArg
A1
NONE ARG
A0
NONE ARG

Table 2: An example of training sample generated via pruning

the input sequence is a predicate with two arguments, labeled with A0 and A1, respectively.
The two labels are assigned to the next two words c and g, respectively, indicating no more
arguments farther than them from the predicate. Accordingly, the word sequence from c to
g are taken as training sample.
a b

c
d e f
g
h .
noMoreLeftArg A1
A0 noMoreRightArg

The total list of class labels in our model, including those from the CoNLL-2008 data set
and a few auxiliary ones newly introduced on purpose, is provided in Table 9 in Appendix A.
These labels are in three categories, namely, 22 PropBank sense labels as predicate classes,
54 argument classes, and 2–3 auxiliary labels as extra classes, for a total of 78-79. Pruning
along linPth needs one more label than that along synPth. Note that our work does not
assume whether the same sense label in the training and the test set means the same for
diﬀerent words. The tendency of a particular word form to associate with its senses in a
statistically signiﬁcant way throughout the data set allows our classiﬁer to predict sense
labels using word form features.
In principle, an auxiliary label is assigned to the last item in the sample that is generated
for a predicate via pruning along a traversal order, be it syntactic or linear. That is, it is
assigned to the ﬁrst item immediately after the last argument of the predicate has been
seen during the pruning. An auxiliary label is treated in exactly the same way as all other
argument labels during training and decoding, except its extra utility to signal where to
stop a search.

5. Feature Generation and Selection
Following many previous works (Gildea & Jurafsky, 2002; Carreras & Màrquez, 2005;
Koomen et al., 2005; Màrquez, Surdeanu, Comas, & Turmo, 2005; Dang & Palmer, 2005;
Pradhan, Ward, Hacioglu, Martin, & Jurafsky, 2005; Toutanova, Haghighi, & Manning,
209

Zhao, Zhang & Kit

2005; Jiang & Ng, 2006; Liu & Ng, 2007; Surdeanu, Marquez, Carreras, & Comas, 2007;
Johansson & Nugues, 2008; Che, Li, Hu, Li, Qin, Liu, & Li, 2008), we carefully examine
the factors involved in a wide range of features that have been or can be used to facilitate
the undertaking of the two SRL subtasks, for both verbal and nominal predicates. Our
endeavor is to further decompose these factors into some more fundamental elements, so
that the largest possible space of feature templates can be explored for more eﬀective and
novel combinations of them into features.
5.1 Feature Element
All features adopted for this work are intended to make full use of these elements, which are
mainly drawn from the word property and syntactic connection of a node in the syntactic
parse tree of an input sentence. The sequences or sets of tree nodes, whose basic elements
are drawn to form features via feature generation by means of many predeﬁned feature
templates, are identiﬁed through the path and family relations as stipulated below.
Word Property This type of elements include word form (denoted as form and its
split form as spForm),3 lemma (as lemma and spLemma), part-of-speech tag (as pos and
spPos), and syntactic and semantic dependency labels (as dprel and semdprel).4
Syntactic Connection This includes syntactic head (as h), left/right farthest/nearest
child (as slm, ln, rm and rn), and high/low support verb or noun. Note that along the
path from a given word to the root of a syntactic tree, the ﬁrst/last verb is called its
low/high support verb, respectively. This notion is widely adopted in the ﬁeld (Toutanova
et al., 2005; Xue, 2006; Jiang & Ng, 2006).5 In this work, we extend it to both nouns
and prepositions. Besides, we also introduce another syntactic head feature pphead for a
given word in question, to retain its left most sibling if headed by a preposition, or its
original head otherwise, aimed at drawing utility from the fact that a preposition usually
carries little semantic information. The positive eﬀect of this new feature is conﬁrmed by
our experiments.
Path There are two basic types of path from an argument candidate a to a given
predicate p, namely, the linear path linePath as the sequence of input words between
them (inclusive) and the other path dpPath between them (inclusive) as in their syntactic
dependency tree. Given the two paths from them to the root r of the tree that meet
at a node r , we have their common part dpPathShare from r to r, their diﬀerent parts
dpPathArgu and dpPathPred from a and p to r , respectively, and the path dpPath between
a and p. Similarly, we have a dpPath between any two nodes in a syntactic tree.
Family Two child sets are diﬀerentiated for a given predicate or argument candidate,
one (as children) including all syntactic children and the other (as noFarChildren) excluding only the leftmost and the rightmost one. The latter is introduced as a feature to
diﬀerentiate the modiﬁers (i.e., children) close to the head from those far away.
3. Note that in CoNLL-2008, many treebank tokens are split at the position of a hyphen (-) or a forward
slash (/), resulting in two types of form for each, namely, non-split and split.
4. The lemma and pos, for both training and test, are directly from the pre-analyzed columns of an input
ﬁle, automatically generated by the organizer of CoNLL shared tasks.
5. Note that the notion of the term support verb is slightly diﬀerent in these works. It is used here to refer
to a verb that introduces a long-distance argument to a nominal predicate from outside of the noun
phrase headed by the nominal predicate.

210

Semantic Dependency Parsing

Others There are also a number of other elements, besides those in the above categories,
that play a signiﬁcant role in feature generation. Many of them are derived from inter-word
relationships. Listed below are a number of representative ones.
dpTreeRelation It returns the relationship of a and p in an input syntactic tree. The
possible values for this feature include parent, sibling, etc.
isCurPred It checks whether a word in question is the current predicate, and returns the
predicate itself if yes, or a default value otherwise.
existCross It checks if a potential dependency relation between a given pair of words
may cross any existing relation in the semantic tree under construction.
distance It returns the distance between two words along a given path, be it dpPath or
linePath, in number of words.
existSemdprel It checks whether a given argument label under a predicate has been assigned to any other word.
voice It returns either Active or Passive for a verb and a default value for a noun.
baseline A small set of simple rules6 are used to generate SRL output as the baseline
for CoNLL evaluation (Carreras & Màrquez, 2005). This baseline output can be
selectively used as features, in two categories: baseline Ax tags the head of the ﬁrst
NP before and after a predicate as A0 and A1, respectively, and baseline Mod tags
the modal verb dependent of a predicate as AM-MOD.
A number of features such as existCross and existSemdprel have to depend on the
semantic dependencies or dependency labels in the existing part of a semantic parse tree
under (re)construction for a sentence, be it for training or decoding. Note that both training
and decoding ﬁrst take the candidate word pairs from a given sentence as input, as illustrated
in Table 2, and then undergo a process of selecting a subset of the candidates to (re)construct
a semantic parse tree, which consists of a root, some predicate(s) as its child(ren), and the
argument(s) of the predicate(s) as its grandchild(ren). The decoding infers an optimal
semantic tree for a sentence with the aid of a trained ME model (see Section 6). The
training reconstructs the gold standard semantic tree of an input sentence when scanning
through its word pairs in sequence and diﬀerentiating the true ones in the tree from the
others. The true ones rebuild the tree part by part. All features (including existCross
and existSemdprel) extracted from both the true ones, as in the partially (re)built parts
of the tree, and the others in the current context are fed to the ME model for training.
In other words, the feature generation is based on gold standard argument labels during
training and on predicted ones during decoding.
5.2 Feature Generation
Sequences of syntactic tree nodes are ﬁrst collected by means of the paths and/or the family
relations deﬁned above. Three strategies are then applied to combine elements of the same
type (e.g., form, spPos) from these nodes into a feature via string concatenation. The three
strategies of concatenation are: (1) sequencing (as seq), which concatenates given element
strings in their original order in the path, (2) unduplicating (as noDup), which further frees
6. Developed by Erik T K Sang, of the University of Antwerp, Belgium.

211

Zhao, Zhang & Kit

seq from adjacent duplicates, and (3) bagging (as bag), which concatenates unique element
strings in alphabetical order.
Given below are a number of typical feature templates to illustrate how individual
features are derived in the ways as described above, with the aid of the following operators:
x+y (the concatenation of x and y), x.y (the attribute y of x), x:y (the path from x to y),
and x:y|z (the collection of all instances of attribute z along the path from x to y).
a.lm.lemma The lemma of the leftmost child of the argument candidate a.
p.h.dprel The dependency label of the syntactic head of predicate candidate p.
p-1 .pos + p.pos The concatenation of the POS tags of two consecutive predicates.
a:p|dpPath.lemma.bag The bag of all lemmas along the dpPath from a to p.
a:p.highSupportNoun|linePath.dprel.seq The seq of all dependency labels along the
linePath from a to the high support noun of p.
In this way, a set of 781 feature templates,7 henceforth referred to as F T , is generated
to specify the allowable feature space for feature selection. Many of them are generated
by analogy to existing feature templates in the literature. For example, given a feature
template like a.lm.lemma which has been used in some previous works, its analogous ones
such as a.rm.lemma, a.rn.lemma and a.ln.lemma are included in the F T .
Predicate sense labels in the data set are also utilized as a type of element in various
feature templates in the F T . However, it is worth noting that the same sense label associated
with diﬀerent words, e.g., 02 in take.02 and in say.02, is not assumed to have anything
in common or anything to do with each other. For predicate disambiguation, however,
these features always combine a predicate sense with a word form, and hence naturally
diﬀerentiate between the same sense label for diﬀerent words. To predict a predicate sense
label is always to predict it in association with a word form. That is, a sense label is never
used in separation from a word form. In this way, our model gives a very high precision for
sense label prediction according to our empirical results.
5.3 Feature Template Selection
It is a complicated and hence computationally expensive task to extract an optimal subset
of feature templates from a large feature space. For the sake of eﬃciency, a greedy procedure
for feature selection has to be applied towards this goal, as illustrated in many previous
works, e.g., by Jiang and Ng (2006), and Ding and Chang (2008). The algorithm that
we implemented for this purpose is presented in Algorithm 1 below, which imposes fewer
assumptions than those in previous works, aiming at a higher eﬃciency. It repeats two
main steps until no further performance gain is achievable on the given development set:
1. Include any template from the rest of F T into the current set of candidate templates
if its inclusion would lead to a performance gain.
2. Exclude any template from the current set of candidate templates if its exclusion
would lead to no deterioration in performance. By repeatedly adding/removing the
7. Available at http://bcmi.sjtu.edu.cn/∼zhaohai/TSRLENAllT.txt, in a macro language as used in our
implementation, far not as readable as the notation of the illustrations given here.

212

Semantic Dependency Parsing

most/least useful template, the algorithm aims to return a better or smaller candidate
set for next round.
Given n candidate feature templates, the algorithm by Ding and Chang (2008) requires
O(n2 ) time to execute a training/test routine, whereas the one by Jiang and Ng (2006)
requires O(n) time, assuming that the initial set of feature templates is “good” enough
and the others can be handled in a strictly incremental way. The time complexity of our
algorithm can also be analyzed in terms of the execution time of the training-and-test routine
scr(M (.)), for all other subroutines such as sorting are negligible while compared against
its execution time. In Algorithm 1, recruitMore ﬁrst calls this routine |F T − S| ≤ n
times in the for loop, and then shakeOff calls it |Smax | ≤ n times to prepare for the
sorting, followed by at most another |Smax | times in the inner while loop. Assuming that
the ﬁrst while loop and the outer while in shakeOff iterate k1 and k2 times, respectively,
the algorithm is of O(k1 (|F T − S| + k2 (|Smax | + |Smax |))) = O(k1 k2 n) time.
Empirically, however, we have k1 , k2 << n, in that our experiments seldom show any
k1 > 5 or k2 > 10, especially when running with 1/10 F T randomly chosen as the initial S.
In particular, the ﬁrst while loop often iterates only 2-3 times, and after its ﬁrst iteration k2
drops rapidly. The observation that k1 k2 varies only in a very limited range suggests that
we may have O(k1 k2 n) = O(n) as an empirical estimation of the eﬃciency of the algorithm
in this particular context. A reasonable account for this is that as the ﬁrst while loop
comprises of only two functions, namely, recruitMore to recruit positive feature templates
and shakeOff to ﬁlter out negative ones, so as to improve the model in either case, it is
likely that the positive/negative ones remain positive/negative consistently throughout the
looping. As a result, only very few of them remain outside/inside the candidate set for
further recruiting/ﬁltering after a couple of iterations of the loop.
This eﬃciency allows a large-scale engineering of feature selection to be accomplished
at a reasonable cost of time. In our experiments with 1/10 F T randomly selected as the
initial S, the greedy selection procedure was performed along one of the two argument
candidate traverse schemes (i.e., the synPth and linPth) on NomBank, PropBank or their
s , Ss , Ss
l
l
l
combination, and output six feature template sets SN
P
N+P , SN , SP and SN+P , of
186, 87, 246, 120, 80 and 118 selected templates, respectively, for performance evaluation
and comparison. About 5500 machine learning routines ran for the synPth scheme and
nearly 7000 routines for the linPth. A contrastive analysis of these template sets, with a
focus on the top 100 or so most important templates from each of them, is presented in
Appendix A through Tables 9-17, where the rank columns present the rankings of feature
templates in terms of their importance in respective feature template sets. The importance
of a feature template in a template set is measured in terms of the performance change by
adding or removing that template, and the performance of a model using a template set is
measured by its labeled F1 score on a given test set, following the conventional practice of
SRL evaluation in CoNLL shared tasks.
It is interesting to note that the six template sets have a tiny intersection of only 5
templates, as listed in Table 10, each manifesting a notable variance of importance ranking
in diﬀerent sets. Excluding these ﬁve, the rest of the overlap of the top 100 of the synPth
s , S s and S s
sets SN
P
N +P is also very small, of only 11 templates, in contrast to that of the
l , S l and S l
linPth sets SN
P
N +P , which is about 4 times larger, of 46 templates; as listed in
213

Zhao, Zhang & Kit

Algorithm 1 Greedy Feature Selection
Input
A training data set: T
A development data set: D
The set of all feature templates: FT
Denotation
M (S) = M (S, T ), a model using feature template set S, trained on T ;
scr(M ) = scr(M, D), the evaluation score of model M on D;
Since T and D are ﬁxed, let scr(M (S)) = scr(M (S, T ), D) for brevity.
Algorithm
1: S = {f0 , f1 , ..., fk }, a random subset of F T ;
 F T : a globally accessible constant
2: while do
3:
Cr = recruitMore(S);
4:
if Cr == {} then return S;
5:
S  = shakeOff(S + Cr );
6:
if scr(M (S)) ≥ scr(M (S  )) then return S;
7:
S = S;
8: end while
1: function recruitMore(S)
 Retrieve more positive templates from F T − S
2:
Cr = {}, and p = scr(M (S));
3:
for each f ∈ F T − S do
4:
if p < scr(M (S + {f })) then Cr = Cr + {f };
5:
end for
6:
return Cr ;
7: end function
1: function shakeOff(Smax )
 Shake oﬀ useless templates from Smax
2:
while do
3:
S = S0 = Smax ;
4:
sort S in the descending ordera of scr(M (S − {f })) for each f ∈ S;
5:
while (S = S − {f0 }) = {} do
6:
Smax = argmaxx∈{Smax , S} scr(M (x));
 Drop f0 ∈ S if it is useless
7:
end while
8:
if S0 == Smax then return S0 ;
 If none dropped
9:
end while
10: end function
a. Namely in the ascending order of the importance of f in S, estimated by scr(M (S)) − scr(M (S − {f })).

214

Semantic Dependency Parsing

Tables 11 and 12, respectively. Besides these shared templates, these six sets hold 84, 71,
84, 69, 29 and 67 others in their top 100, as listed in Tables 13-18, respectively, where a
negative/positive subscript denotes a preceding/following word. For example, a.lm -1 .lemma
returns the lemma of the previous word of a’s left most child.
The rather small overlap of the six sets suggests that the greedy feature selection algorithm maintains a stable eﬃciency while working out these template sets of huge divergence,
lending evidence to support the empirical estimation above. Despite this divergence, each
of these template sets enables our SRL model to achieve a state-of-the-art performance on
the CoNLL-2008 data set,8 indicating the eﬀectiveness of this approach, for which more
details of evaluation will be provided in Section 7 below.

6. Decoding
Following exactly the same procedure of generating the training sample, our ME classiﬁer,
after training, outputs a series of labels for the sequence of word pairs generated from an
input sentence, inferring its predicates and their arguments one after another. Diﬀerent
from most existing SRL systems, it instantiates an integrative approach that conducts all
predication with the same trained model. However, following the common practice of incorporating task-speciﬁc constraints into a global inference (Roth & Yih, 2004; Punyakanok,
Roth, Yih, & Zimak, 2004), we opt for further developing a decoding algorithm to infer
the optimal argument structure for any predicate that is identiﬁed this way by the classiﬁer. The main diﬀerences of our work from Punyakanok et al. (2004) are that (1) they
use ILP for joint inference, which is exact, and we use beam search, which is greedy and
approximate, and (2) the constraints (e.g., no duplicate argument label is allowed) that
they impose on arguments through individual linear (in)equalities are realized through our
constraint fulﬁllment features (e.g., existCross and existSemdprel).
Speciﬁcally, the decoding is to identify the arguments among candidate words by inferring the best semantic role label for each candidate (cf. the training sample in Table
2 with one label per word). Let A = {a0 , a1 , ..., an−1 } be the candidates for a predicate,
where each ai embodies all available properties of a word, including a candidate label, and
let Ai = a0 a1 ... ai−1 be a partial argument structure (of our target under search) that has
been determined and ready for use as the context for inferring the next argument. Instead of
counting on best-ﬁrst search, which simply keeps picking the next best argument according
the conditional probability p(ai |Ai ), we resort to a beam search for a better approximation
of the global optimization for the maximal probability in
Ã = argmax
A ⊆A

n


p(ai |Ai ),

(1)

i=0

where Ai consists of the ﬁrst i elements of A . Ideally, the beam search returns the most
probable subset of A as arguments for the predicate in question. It rests on a conditional
maximum entropy sequential model incorporating global features into the decoding to infer
the arguments that are not necessarily in a sequential order. As in previous practice, our
8. Note that an early version of this model also illustrated a top-ranking performance on CoNLL-2009
multilingual data sets (Zhao, Chen, Kit, & Zhou, 2009).

215

Zhao, Zhang & Kit

ME model adopts a tunable Gaussian prior (Chen & Rosenfeld, 1999) to estimate p(ai |Ai )
and applies the L-BFGS algorithm (Nocedal, 1980; Nash & Nocedal, 1991) for parameter
optimization.

7. Evaluation
The evaluation of our SRL approach is conducted with various feature template sets on the
oﬃcial training/development/test corpora of CoNLL-2008 (Surdeanu et al., 2008). This
data set is derived by merging a dependency version of the Penn Treebank 3 (Marcus,
Santorini, & Marcinkiewicz, 1993) with PropBank and NomBank. Note that CoNLL-2008
is essentially a joint learning task on both syntactic and semantic dependencies. The research presented in this article is focused on semantic dependencies, for which the primary
evaluation measure is the semantic labeled F1 score (Sem-F1 ). Other scores, including the
macro labeled F1 score (Macro-F1 ), which was used to rank the participating systems in
CoNLL-2008, and Sem-F1 /LAS, the ratio between labeled F1 score for semantic dependencies and the labeled attachment score (LAS) for syntactic dependencies, are also provided
for reference.
7.1 Syntactic Input
Two types of syntactic input are used to examine the eﬀectiveness of our integrative SRL approach. One is the gold standard syntactic input available from the oﬃcial data set
and the other is the parsing results of the same data set by two state-of-the-art syntactic
parsers, namely, the MSTparser9 (McDonald, Pereira, Ribarov, & Hajič, 2005; McDonald
& Pereira, 2006) and the parser of Johansson and Nugues (2008). However, instead of using
the original MSTparser, we have it substantially enriched with additional features, following
Chen, Kawahara, Uchimoto, Zhang, and Isahara (2008), Koo, Carreras, and Collins (2008),
and Nivre and McDonald (2008). The latter one, henceforth referred to as J&N for short,
is a second-order graph-based dependency parser that takes advantage of pseudo-projective
techniques and resorts to syntactic-semantic reranking for further reﬁning its ﬁnal outputs.
However, only its 1-best outputs before the reranking are used for our evaluation, even
thought the reranking can slightly improve its parsing performance. Note that this reward of reranking through joint-learning for syntactic and semantic parsing is gained at a
huge computational cost. On the contrary, our approach is intended to show that highly
comparable results can be achieved at much lower cost.
7.2 Experimental Results
The eﬀectiveness of the proposed adaptive approach to pruning argument candidates is
examined with the above three syntactic inputs, and the results are presented in Table 3,10
where a coverage rate is the proportion of true arguments in pruning output. Note that
using auxiliary labels does not aﬀect this rate, which has to be accounted for by the choice
of traverse path and the quality of syntactic input, as suggested by its diﬀerence in the
synPth rows. The results show that the pruning reduces more than 50% candidates along
9. Available at http://mstparser.sourceforge.net.
10. Decimal ﬁgures in all tables herein are percentages unless otherwise speciﬁed.

216

Semantic Dependency Parsing

Syntactic Input (LAS)
MST (88.39%)
J&N (89.28%)
Gold (100.0%)

Path
linP th
synP th
linP th
synP th
linP th
synP th

Original
5.29M
2.15M
5.28M
2.15M
5.29M
2.13M

Pruning
1.57M
1.06M
1.57M
1.06M
1.57M
1.05M

Reduction
-70.32
-50.70
-70.27
-50.70
-70.32
-50.70

Coverage
100.0
95.0
100.0
95.4
100.0
98.4

Table 3: Reduction of argument candidates by the adaptive pruning

Path x
linPth
synPth
Reduction

x
SN
7,103
5,609
-21.03

SPx
7,214
5,470
-24.18

x
SN+P
7,146
5,572
-22.03

Table 4: Number of executions of the training-and-test routine in greedy feature selection
synPth, at the cost of losing 1.6-4.6% true ones, and more than 70% along linPth without
any loss. Nevertheless, the candidate set so resulted from synPth is 1/3 smaller in size than
that from linPth.
The number of times that the training-and-test routine is executed in the greedy selection
of all six feature sets are presented in Table 4, showing that synPth saves 21%-24% execution
times. Given the estimation of the time complexity of the selection algorithm as O(k1 k2 n)
for executing the routine, empirically we have 7 < k1 k2 < 10 on a feature space of size n = 781
for our experiments, verifying the very high eﬃciency of the algorithm.
As pointed out by Pradhan, Ward, Hacioglu, Martin, and Jurafsky (2004), argument
identiﬁcation (before classiﬁcation) is a bottleneck problem in the way of improving SRL
performance. Narrowing down the set of predicate candidates as much as possible in a reliable way has been shown to be a feasible means to alleviate this problem. The eﬀectiveness
of our adaptive pruning for this purpose can be examined through comparative experiments
in terms of time reduction and performance enhancement. The results from a series of such
experiments are presented in Table 5, showing that the adaptive pruning saves the training
and test time by about 30% and 60%, respectively, while enhancing the performance (in
Sem-F1 score) by 23.9%–24.8%, nearly a quarter. These results also conﬁrm a signiﬁcant
improvement upon its non-adaptive origin (Xue & Palmer, 2004) and the twofold beneﬁt
of pruning oﬀ arguments far away from their predicates, which follows from the assumption that true arguments tend to be close to their predicates. It is straightforward that
using the noMoreArg label reduces more training samples than not using (see Section 4)
and hence leads to a greater reduction of training time. Using this label also decreases
the test time remarkably. During decoding, a noMoreArg label, once assigned a probability
higher than all other possible role labels for the current word pair, hints the decoder to
stop working on the next word pair, resulting in a further test time reduction by 18.5–21.0
percentage points upon the non-adaptive pruning. The particularly low performance without pruning also reﬂects the soundness of the motivation for candidate pruning from both
217

Zhao, Zhang & Kit

Bank

Features

PropBank

87

NomBank
+
PropBank

246

Pruninga
−
−Adaptiveb
+Adaptive
−
−Adaptive
+Adaptive

Training
122,469s
109,094s
83,208s
432,544s
392,216s
305,325s

Redu.
-10.9
-32.1
-9.3
-29.4

Test
747s
372s
234s
2,795s
1,615s
1,029s

Redu.
-50.2
-68.7
-42.2
-63.2

Sem-F1
66.85
80.59
82.80
64.85
79.77
80.91

a. Syntactic input: MST; Traverse scheme: synPth; Machine conﬁguration: Four six-core
R
R
Xeon
X5690 3.46GHz processors, 48G memory.
Intel
b. The original pruning as in Xue and Palmer (2004), not using noMoreArg.

Table 5: Time reduction and performance enhancement by the adaptive pruning

Syn. Input
(LAS)
MST
(88.39%)

J&N
(89.28%)

Gold
(100%)

Feature
Set
Initial
Selected
Initial
Selected
Initial
Selected

Path x
linPth
synPth
linPth
synPth
linPth
synPth
linPth
synPth
linPth
synPth
linPth
synPth

NomiF1 xN
44.58
44.67
77.93
77.89
44.84
45.01
77.73
77.70
45.57
45.89
80.43
80.37

VerbF1 xP
58.83
63.24
82.72
82.80
58.84
63.26
83.21
83.90
61.79
67.63
89.44
90.37

NomiF1 xN+P
41.18
42.42
76.75
77.52
42.16
43.64
76.45
76.79
42.41
43.76
79.44
80.20

VerbF1 xN+P
56.34
61.28
82.30
83.24
56.40
61.36
82.70
83.71
59.09
65.51
89.07
90.27

SemF1 xN+P
51.14
54.79
80.05
80.91
51.36
55.12
80.15
80.88
53.12
57.77
84.99
86.02

SemF1 xN+P /LAS
57.86
61.99
90.56
91.54
57.53
61.74
89.77
90.59
53.12
57.77
84.99
86.02

Table 6: Performance of random initial and greedily selected feature sets
the machine learning and linguistic perspective. The pruning provides a more balanced
training dataset for classiﬁer training than without pruning. Note that without pruning,
most word pairs generated for the training are irrelevant and far away from the current
predicate, inevitably interfering with the informative features from the truly relevant ones
in the very small minority and, hence, leading to an unsatisfactory performance. Although
the pruning, especially its adaptive version, is rooted in a linguistic insight gained from empirical observations on real data, most previous works on semantic parsing simply took the
pruning as an indispensable step towards a good parsing performance, seldom paying much
attention to the poor performance without pruning nor comparing it with the performance
by diﬀerent pruning strategies.
Table 6 presents a comprehensive results of our semantic dependency parsing on the
three syntactic inputs aforementioned of diﬀerent quality. A number of observations can
be made from these results. (1) The greedy feature selection, as encoded in Algorithm 1
above, boosts the SRL performance drastically, raising the Sem-F1 scores in the synPth
rows from 54.79%–57.77% of the initial feature sets, the baseline, to 80.88%–86.02% of the
218

Semantic Dependency Parsing

Syn. Input
(LAS)

Feature set

Path x

MST
(88.39%)

l
SN
+P - Sense
s
SN
+P - Sense
l
SN
+ SPl
s
SN
+ SPs

linP th
synP th
linP th
synP th

NomiF1 xN+P
76.51
76.76
76.78
76.60

VerbF1 xN+P
82.09
82.75
82.20
82.76

SemF1 xN+P
79.82
80.30
79.99
80.24

SemF1 xN+P /LAS
90.30
90.85
90.50
90.78

Loss in
F1 xN+P
-0.29
-0.75
-0.07
-0.83

Table 7: Experimental results on feature ablation and feature set combination
selected feature sets, by an increment of 46.73%–48.90%. The rise in corresponding linPth
rows is even larger. Among the three inputs, the largest increment is on the gold standard,
suggesting that the feature selection has a greater eﬀect on an input of better quality.
(2) The traverse scheme synPth leads to a better model than linPth, as reﬂected in the
diﬀerence of Sem-F1 and Sem-F1 /LAS scores between them, indicating that this integrative
SRL approach is sensitive to the path along which argument candidates are traversed.
The diﬀerence of their Sem-F1 /LAS scores, for instance, is in the range of 7.14%–8.75%
and 0.91%–1.21% for the initial and the selected feature sets, respectively. The signiﬁcant
advantage of synPth is conﬁrmed consistently, even though an optimized feature set narrows
down the performance discrepancy between the two so radically. (3) The result that both
Nomi-F1 xN and Verb-F1 xP are higher than corresponding F1 xN +P consistently throughout
almost all experimental settings except one shows that the feature selection separately
on Nombank or PropBank (for verbal or nominal predicates, respectively) gives a better
performance than that on the combination Nombank+PropBank for both. This has to
be explained by the interference between the two data sets due to their heterogeneous
nature, namely, the interference between the nominal and verbal predicate samples. Hence,
optimizing a feature set speciﬁcally for a particular type of predicates is more eﬀective than
for both. (4) An overall comparison of our system’s SRL performance on the three syntactic
inputs of diﬀerent quality (as reﬂected in their LAS) shows that the performance as a whole
varies in accord with the quality of input. This is exhibited in the contrast of the Sem-F1
scores on these inputs, even though a small LAS diﬀerence may not necessarily lead to a
signiﬁcant performance diﬀerence (for instance, MST has a LAS of 0.89 percentage point
lower than J&N but gives a Sem-F1 score as high in one of the four experimental settings).
The table also shows that a LAS diﬀerence of 11.61 percentage points, from 88.39% to 100%,
corresponds to a Sem-F1 score diﬀerence of at most 5.14 percentage points, from 80.88% to
86.02%, in the best setting (i.e., using the selected feature set and taking synPth).
However, Sem-F1 scores cannot be trusted to faithfully reﬂect the competence of a
semantic parser, because the quality of syntactic input is also a decisive factor to decide
such scores. For this reason, we have the Sem-F1 /LAS ratio as an evaluation metric.
Interestingly, our parser’s scores of this ratio on the two syntactic inputs of a LAS 10.82–
11.61 percentage points below the gold standard are, contrarily, 4.57–5.52 percentage points
higher. This is certainly not to mean that the parser is able to rescue, in a sense, some true
semantic parses from an erroneous syntactic input. Instead, it can only be explained by the
parser’s high tolerance of imperfections in the syntactic input.
Table 7 further presents experimental results on feature ablation and feature set combination. The former is to examine the eﬀect of sense features and the latter that of feature
219

Zhao, Zhang & Kit

optimization. Along synPth, both the ablation of sense feature and the mix of two feature
sets respectively optimized (through the greedy selection) on the NomBank and PropBank
lead to a signiﬁcant performance loss of 0.75%–0.83%, in comparison with the performance
s
of the feature set SN
+P optimized on the combination of the two treebanks as given in Table
6. Along linPth, they lead to a much less signiﬁcant and an insigniﬁcant loss, respestively.
These results show that both sense features and the greedy selection of features are more
signiﬁcant in joining with the adaptive pruning along synPth to achieve a performance gain.
7.3 Comparison and Analysis
In order to evaluate the parser impartially in a comparative manner, its performance along
synPth is compared with that of the other state-of-the-art systems in CoNLL-2008. They are
chosen for this comparison because of being ranked among top four among all participants
in the shared task or using some sophisticated joint learning techniques. The one of Titov,
Henderson, Merlo, and Musillo (2009) that adopts a similar joint learning approach as
Henderson, Merlo, Musillo, and Titov (2008) is also included, because of their signiﬁcant
methodological diﬀerence from the others. In particular, the former has attained the best
performance to date in the direction of genuine joint learning. The reported performance of
all these systems on the CoNLL-2008 test set in terms of a series of F1 scores is presented in
Table 8 for comparison. Ours is signiﬁcantly better (t = 14.6, P < 0.025) than all the others
except the post-evaluation result of Johansson and Nugues (2008). Contrary to the best
three systems in CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, Dell’Orletta,
& Surdeanu, 2008; Che et al., 2008) that use SRL pipelines, our current work is intended
to integrate them into one. Another baseline, namely, our current model using the feature
set from the work of Zhao and Kit (2008), instead of a random set, is also included in the
table for comparison, showing a signiﬁcant performance enhancement on top of the previous
model and, then, a further enhancement by the greedy feature selection.
Although this work draws necessary support from the basic techniques (especially those
for traverse along synP th) underlying our previous systems for CoNLL-2008 and -2009
(Zhao & Kit, 2008; Zhao, Chen, Kit, & Zhou, 2009; Zhao, Chen, Kazama, Uchimoto, &
Torisawa, 2009), what marks its uniqueness is that all SRL sub-tasks are performed by one
integrative model with one selected feature set. Our previous systems dealt with predicate
disambiguation as a separate sub-task. This is our ﬁrst attempt at a fully integrated SRL
system.
The fact that our integrated system is yet to give a performance on a par with the postevaluation result of Johansson and Nugues (2008) seems attributable to a number of factors,
including the ad hoc features adopted in their work to handle linguistic constructions such
as raising/control and coordination. However, the most noticeable ones are the following
discrepancies between the two systems, in addition to pipeline vs. all-in-one integration.
(1) They have the n-best syntactic candidates as input, which without doubt provide more
useful information than the 1-best that we use. (2) Then, they exploit reranking as a joint
learning strategy to make fuller use of the n-best candidates and any intermediate semantic
result once available, resulting in a gain of 0.5% increment of Sem-F1 score. (3) They
use respective sub-systems to deal with verbal and nominal predicates in a more speciﬁc
manner, following the observation that adaptive optimization of feature sets for nominal
220

Semantic Dependency Parsing

Systemsa

LAS

Ours:Gold
Johansson:2008*d
Ours:MST
Ours:Johansson
Johansson:2008
Ours:Baselinee
Ciaramita:2008*
Che:2008
Zhao:2008*
Ciaramita:2008
Titov:2009
Zhao:2008
Henderson:2008*
Henderson:2008

100.0
89.32
88.39
89.28
89.32
88.39
87.37
86.75
87.68
86.60
87.50
86.66
87.64
86.91

SemF1
86.02
81.65
80.91
80.88
80.37
79.42
78.00
78.52
76.75
77.50
76.10
76.16
73.09
70.97

MacroF1
92.27
85.49
85.09
85.12
84.86
84.34
82.69
82.66
82.24
82.06
81.80
81.44
80.48
79.11

Sem-F1
/LAS
86.02
91.41
91.54
90.59
89.98
89.85
89.28
90.51
87.53
89.49
86.97
87.88
83.40
81.66

PredF1 b
89.25
87.22
87.15
86.47
85.40
86.60
83.46
85.31
78.52
83.46
–
78.26
81.42
79.60

ArguF1 c
84.54
79.04
78.01
78.29
78.02
76.08
75.35
75.27
75.93
74.56
–
75.18
69.10
66.83

VerbF1
90.27
84.78
83.23
83.71
84.45
81.71
80.93
80.46
78.81
80.15
–
77.67
75.84
73.80

NomiF1
80.20
77.12
77.52
76.79
74.32
76.07
73.80
75.18
73.59
73.17
–
73.28
68.90
66.26

a.
b.
c.
d.

Ranked according to Sem-F1 , and only ﬁrst authors are listed for the sake of space limitation.
Labeled F1 for predicate identiﬁcation and classiﬁcation.
Labeled F1 for argument identiﬁcation and classiﬁcation.
A superscript * indicates post-evaluation results, available from the oﬃcial website of CoNLL2008 shared task at http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.
e. Syntactic input and traverse scheme: as Ours:MST; Features: as Zhao:2008

Table 8: Performance comparison of the best existing SRL systems
or verbal predicates respectively is more likely to give a better performance than that for
a mix of both. This observation is also conﬁrmed by evidence in our experimental results:
F1 xN and F1 xP scores are consistently higher than respective F1 xN +P ones in Table 6 above.
Because of the integrative nature of our approach, however, our priority has to be given
to optimizing the whole feature set for both verbal and nominal predicates. It is nevertheless
understood that all these point to potential ways to further enhance our system, e.g., by
taking advantage of specialized feature sets for various kinds of words and/or utilizing some
joint learning techniques such as syntactic-semantic reranking, in a way that the integrity
of the system can be maintained properly.
The diﬀerence between the joint learning in the work of Johansson and Nugues (2008)
and that of Titov et al. (2009) is worth noting. The former is a kind of cascade-style joint
learning that ﬁrst has a syntactic submodel to provide the n-best syntactic trees and a
semantic submodel to infer correspondent semantic structures, and then a reranking model,
with the log probabilities of the syntactic trees and semantic structures as its features, to ﬁnd
the best joint syntactic-semantic analysis, resulting in an improvement on top of individual
submodels. In contrast to the former with a non-synchronous pipeline from syntactic to
semantic parsing, the latter adopts a stricter all-in-one strategy of joint learning, where
syntactic and semantic dependencies are learnt and decoded synchronously, based on an
augmented version of the transition-based shift-reduce parsing strategy (Henderson et al.,
2008). Regrettably, however, the performance of this approach is still far from the top of
the ranked list in Table 8, indicating the particular signiﬁcance of our current work.
221

Zhao, Zhang & Kit

Whether it is worth integrating some form of joint-learning into an integrative system
such as ours depends on the cost-eﬀectiveness of doing so. It has been illustrated that such
joint learning does lead to certain performance improvement, as in CoNLL shared task on
SRL and successive works, e.g., by Johansson and Nugues (2008). However, a great deal of
computational cost has to be paid in order to enable such a reranking procedure to handle
multiple syntactic inputs. This certainly makes it impractical for real applications, not to
mention that an integrative system is born with a particularly strong demand for integrity
to preclude itself from accommodating such a stand-alone submodel.

8. Conclusion
Semantic parsing, which aims to derive and instantiate the semantic structure of a sentence
via identifying semantic relations between words, plays a critical role in deep processing of
natural language. In this article, we have presented an integrative approach to semantic
dependency parsing in the form of semantic role labeling, its implementation as an all-inone word pair classiﬁer, and a comprehensive evaluation of it using three syntactic inputs
of diﬀerent quality. The evaluation results conﬁrm the eﬀectiveness and practicality of this
approach. The major contributions of this research are the following. It exhibits a signiﬁcant
success for the ﬁrst time that an integrative SRL system has achieved a performance next
only to that of the best pipeline system, indicating the potentials of the integrative approach
besides its practicality for real applications. The large-scale feature selection engineering
underlying the success of this work also demonstrates (1) how the largest feature space ever
in use in this ﬁeld is formed by allowing a wide range of ﬂexible (re)combinations of basic
elements extracted from the known features and properties of input words and (2) how a
speedy adaptive feature selection procedure is formulated and applied to select the most
eﬀective set of features from the allowable feature space.
The core techniques that have contributed to this success are developed based on the two
types of traverse path, along syntactic tree branches vs. linear input word sequence. Both
argument candidate pruning and feature selection are performed along an identical path.
The strategy of using auxiliary labels to facilitate argument candidate pruning, following the
observation that true arguments tend to be close to their predicates, works well with both
traverse schemes. Interestingly, although the feature selection procedure outputs two very
diﬀerent feature sets for each of NomBank, PropBank and their combination whilst working
along the two paths, both feature sets lead the SRL system to a very close performance on
the same test data, a competitive performance on top of all but one best pipeline system,
conﬁrming the robustness and eﬀectiveness of the feature selection procedure.
Evidence is also presented in our evaluation results to reconﬁrm the ﬁnding in the previous works of semantic parsing that feature sets optimized speciﬁcally for verbal or nominal
predicates outperform a collective one for both. However, the competitive performance of
the collective one that we have arrived at also suggests that a harmonious rival feature set
for both types of predicate as a whole is reachable and its slight performance diﬀerence
from the speciﬁc sets is fairly acceptable as the unavoidable small cost for exchange for
the higher integrity and practicality of an integrative SRL system. This competitiveness
is attributable at least to two main factors. One is the very large feature space in use,
which provides about a dozen times as many feature templates as those in the previous
222

Semantic Dependency Parsing

works (e.g., see Xue & Palmer, 2004; Xue, 2006). The other is the ME classiﬁer that can
accommodate so many features in one model. According to our experience in this piece of
work, the ME model is not vulnerable to the use of many overlapping features, from which
SVM and other margin-based learners usually suﬀer a lot.

Acknowledgments
The research reported in this article was partially supported by the Department of Chinese, Translation and Linguistics, City University of Hong Kong, through a post-doctorate
research fellowship to the ﬁrst author and a research grant (CTL UNFD-GRF-144611) to
the third and corresponding author, the National Natural Science Foundation of China
(Grants 60903119 and 61170114), the National Basic Research Program of China (Grant
2009CB320901), the National High-Tech Research Program of China (Grant 2008AA02Z315),
the Research Grants Council of HKSAR, China (Grant CityU 144410), and the City University of Hong Kong (Grant 7002796). Special thanks are owed to Richard Johansson for
kindly providing his syntactic output for the CoNLL-2008 shared task, to three anonymous
reviewers for their insightful comments and to John S. Y. Lee for his help.

Appendix A. Feature Templates and their Importance Rankings

Type
Predicate

Argument

A0˜5
AA, AM
C-A0˜4
R-A0˜4
R-AA
AM-PRD
AM-PRT
AM-REC
AM-TM
AM-TMP

PropBank
01˜21 (21)
AM-ADV C-AM-ADV
AM-CAU C-AM-CAU
AM-DIR C-AM-DIR
AM-DIS C-AM-DIS
AM-EXT C-AM-EXT
AM-LOC C-AM-LOC
AM-MNR C-AM-MNR
AM-MOD C-AM-NEG
AM-NEG C-AM-PNC
AM-PNC C-AM-TMP

R-AM-ADV
R-AM-CAU
R-AM-DIR
R-AM-EXT
R-AM-LOC
R-AM-MNR
R-AM-PNC
R-AM-TMP
C-R-AM-TMP
SU (54)

Extra/Auxiliary
NONE PRED
NONE ARG

Total
22

noMoreArg
(for synPth)

56

noMoreLeftArg
noMoreRighArg
(for linPth)

57

Table 9: The list of class labels for predicate and argument

Template
Rank in:
p.lm.dprel
a:p|dpPath.dprel
a.lemma + p.lemma
a.lemma + a.dprel + a.h.lemma
a.spLemma + p.spLemma

s
SN
+P
41
35
10
55
4

s
SN
39
31
44
40
97

SPs
6
52
4
49
15

l
SN
+P
82
2
5
112
13

Table 10: Overlap of the six resulted feature template sets

223

l
SN
113
62
36
69
68

SPl
60
2
6
44
26

Zhao, Zhang & Kit

Template
Rank in:
p−1 .pos + p.pos
p−1 .spLemma
p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos
a.isCurPred.lemma
a.isCurPred.spLemma
a:p|existCross
a:p|dpPath.dprel.bag
a:p|dpPathPred.spForm.bag
a:p|dpPath.spLemma.seq
a:p|linePath.spForm.bag
a.semdprel = A0 ?

s
SN
+P
2
27
7
83
36
48
47
97
67
85
50

s
SN
37
13
45
94
38
77
14
24
59
48
86

SPs
79
59
63
75
86
82
85
5
71
61
40

s
s
Table 11: Overlap of SN
, SPs and SN
+P besides Table 10

Template
Rank in:
p.spLemma + p.currentSense
p.currentSense + a.spLemma
p.voice + (a:p|direction)
p.children.dprel.noDup
p.rm.dprel
p.rm.form
p−1 .spLemma + p.spLemma
p.voice
p.form + p.children.dprel.noDup
p.lm.form + p.noFarChildren.spPos.bag + p.rm.form
p.lemma
p.lemma + p1 .lemma
p.spForm
p.spForm + p.children.dprel.bag
p.spForm + p.lm.spForm + p.noFarChildren.spPos.bag + p.rm.spForm
p.splemma
p.spLemma + p.h.spForm
p.spLemma + p1 .spLemma
p1 .pos
a−1 .isCurPred.lemma
a−1 .isCurPred.lemma + a.isCurPred.lemma
a−1 .isCurPred.spLemma + a.isCurPred.spLemma
a.isCurPred.Lemma + a1 .isCurPred.Lemma
a.isCurPred.spLemma + a1 .isCurPred.spLemma
a.spPos.baseline Ax + a.voice + (a:p|direction)
a.spPos.baseline Mod
a.h.children.dprel.bag
a.lm−1 .spPos
a.lm1 .lemma
a.children.spPos.seq + p.children.spPos.seq
a.rm.dprel + a.pos
a.rm.dprel + a.spPos
a.rm−1 .spPos
a.rm.lemma

224

l
SN
+P
18
33
65
11
60
113
38
26
96
88
4
7
39
91
104
9
100
72
76
67
42
14
29
50
24
86
97
47
49
19
21
30
6
36

l
SN
28
57
120
54
114
110
61
4
81
106
26
5
100
6
10
65
11
112
104
109
24
89
44
45
9
80
35
63
30
90
17
7
74
50

SPl
56
17
25
40
3
80
69
10
65
5
50
34
36
30
14
64
70
33
28
77
43
62
67
59
46
18
45
49
68
76
24
22
15
4

Semantic Dependency Parsing

28
27
3
75
12
53
32
94
16
79
43
110

a.rn.dprel + a.spPos
a−1 .lemma + a.lemma
a:p|dpPathArgu.dprel.seq
a:p|dpPathArgu.pos.seq
a:p|dpPathPred.dprel.seq
a.form
a.form + a.pos
a.form + a1 .form
a.spForm + a.spPos
a.spForm + a1 .spForm
a.spLemma + a.dprel
a.spLemma + a.h.spForm

33
46
96
79
64
94
93
31
73
38
118
2

72
37
1
9
35
78
32
38
48
52
7
51

l
l
Table 12: Overlap of SN
, SPl and SN
+P besides Table 10

Template
p.lemma + p.currentSense
p.currentSense + a.lemma
a.form + p.semdprel is ctype ?
a.form + p.semdprel is rtype ?
p.lm.form
p−1 .form + p.form
p−2 .form
p−2 .spForm + p−1 .spForm
p.form + p.dprel
p.lemma + p.h.form
p.spForm + p.dprel
p.spLemma + p.children.dprel.noDup
p.spLemma + p1 .spLemma
a.voice + (a:p|direction)
a is leaf in syntactic tree ?
a.lm.dprel + a.spPos
a.lm.pos + a.pos
a.pphead.spLemma
a.rm−1 .form
a.rm1 .spPos
a.highSupportVerb.form
a.lowSupportPorp.form
a.lowSupportPorp.spLemma
a−1 .pos
a−1 .spForm
a:p|dpPath.distance
a:p|dpPathArgu.spLemma.bag
a:p|dpPathPred.spPos.bag
a:p|linePath.dprel.bag
a.form + a.children.pos.seq
a.form + a.pos
a.spForm + a.children.spPos.seq
a.spForm + a.spPos
a.spLemma
a.spLemma + a1 .spLemma

Rank
82
57
3
5
47
71
78
15
74
10
46
43
49
23
16
67
50
19
81
79
56
51
69
70
85
9
96
93
88
53
1
76
87
11
60

225

Template
p.spLemma + p.currentSense
p.currentSense + a.spLemma
a.form + p.ctypeSemdprel
a.form + p.rtypeSemdprel
p.lm.spForm
p−1 .spLemma + p.spLemma
p−2 .spForm
p.form
p.lemma
p.pos
p.spForm + p.children.dprel.bag
p.spLemma + p.h.spForm
p1 .pos
a.children.adv.bag
a.lm.dprel + a.form
a.lm−1 .spLemma
a.lm.spPos
a.rm.dprel + a.spPos
a.rm−1 .spForm
a.rn.dprel + a.spForm
a.highSupportVerb.spForm
a.lowSupportPorp.lemma
a−1 .lemma + a1 .lemma
a−1 .pos + a.pos
a−1 .spPos + a1 .spPos
a:p|dpPath.spLemma.bag
a:p|dpPathPred.spLemma.bag
a:p|dpPathArgu.dprel.seq
a.semdprel = A2 ?
a.form + a.form
a.pos + a.children.spPos.seq
a.spForm + a.children.spPos.bag
a.spForm + a1 .spForm
a.spLemma + a.pphead.spForm
a.spPos + a.dprel + a.h.spPos

Rank
80
18
4
6
7
92
61
68
63
62
90
27
28
95
75
100
8
26
55
32
99
91
20
84
98
73
2
22
35
58
12
65
52
64
41

Zhao, Zhang & Kit

a1 .form
54
a1 .spForm
a1 .spPos
33
(a:p|dpTreeRelation) + p.form
(a:p|dpTreeRelation) + p.spPos
29
(a:p|dpTreeRelation) + a.spPos
(a:p|dpPath.dprel.seq) + p.spForm
36
a−1 .isCurPred.spLemma + a.isCurPred.spLemma
a.noFarChildren.spPos.bag + a.rm.spPos
a.children.spPos.seq + p.children.spPos.seq
a.highSupportNoun:p|dpPath.dprel.seq
(a.highSupportNoun:p|dpTreeRelation) + p.form
(a.highSupportVerb:p|dpTreeRelation) + a.spForm
(a.lowSupportVerb:p|dpTreeRelation) + a.spForm

83
25
30
17
21
34
89
66
72
42

s
Table 13: Feature templates of SN
besides Tables 10 and 11

Template
Rank
Template
p.rm.dprel
47
p.dprel
p.children.dprel.bag
66
p.lm.spPos
p.children.pos.seq
70
p.rm.dprel
p−2 .pos
23
p−2 .spForm + p−1 .spForm
p.dprel = OBJ ?
50
p.lemma + p.h.form
p.lemma+p1 .lemma
3
p.pos
p.spForm
76
p.spForm + p.children.dprel.noDup
p.splemma
9
p.spLemma+p1 .spLemma
p1 .spPos
21
a.lowSupportVerb:p|dpTreeRelation
a.children.adv.bag
20
a.dprel
a.children.dprel.bag
7
a.h.lemma
a.h.spLemma
72
a.lm.dprel + a.spPos
a.lm−1 .spLemma
54
a.pphead.lemma
a.pphead.spLemma
46
a.rm.dprel + a.spPos
a−1 .lemma + a1 .lemma
16
a−1 .pos
a−1 .spLemma + a.spLemma
29
a:p|linePath.distance
a:p|dpPath.distance
22
a:p|dpPathPred.dprel.bag
a:p|dpPath.spForm.seq
12
a:p|dpPathArgu.spForm.seq
a:p|dpPathArgu.spLemma.bag
84
a:p|dpPathPred.spLemma.bag
a:p|dpPathArgu.spLemma.seq
17
a:p|dpPath.spPos.bag
a:p|dpPathPred.spPos.bag
64
a:p|dpPathArgu.dprel.seq
a:p|linePath.spLemma.seq
42
a:p|linePath.spLemma.bag
a:p|dpPathPred.spPos
62
a.existSemdprel A0
a.existSemdprel A1
56
a.existSemdprel A2
a.semdprel = A2 ?
77
a.dprel = OBJ ?
a.form + a.children.pos.seq
10
a.pos + p.pos
a.spLemma + a.dprel
87
a.spLemma+a.dprel+a.h.spLemma
a1 .lemma
14
a1 .spPos
(a:p|dpTreeRelation) + a.spPos
81
(a:p|dpPath.dprel.seq) + p.spPos
a−1 .isCurPred.spLemma + a.isCurPred.spLemma
a−2 .isCurPred.lemma + a−1 .isCurPred.lemma
a.isCurPred.spLemma + a1 .isCurPred.spLemma
a.lowSupportVerb:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPathArgu.dprel.seq
a.lowSupportVerb:p|dpPathArgu.spPos.seq
a.lowSupportVerb:p|dpPathShared.dprel.seq

226

Rank
25
48
51
43
68
26
60
1
32
13
8
31
80
78
24
55
53
11
2
65
28
27
67
57
73
45
19
69
18
41
58
74
33
34
35
36

Semantic Dependency Parsing

a.lowSupportVerb:p|dpPathShared.spPos.seq
a.lowSupportVerb:p|dpPathPred.dprel.seq
a.lowSupportVerb:p|dpPathPred.spPos.seq
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
(a.highSupportVerb:p|dpTreeRelation) + a.spPos

37
38
39
83
30
44

Table 14: Feature templates of SPs besides Tables 10 and 11

Template
Rank
Template
p.lemma + p.currentSense
100
p.currentSense + a.lemma
a.form + p.semdprel is ctype ?
90
a.form + p.ctypeSemdprel
a.form + p.semdprel is rtype ?
92
a.form + p.rtypeSemdprel
p.dprel
8
p.children.pos.seq
p.rm.dprel
46
p.lowSupportProp:p|dpTreeRelation
p−1 .spForm + p.spForm
54
p.voice
p.lemma+p1 .lemma
18
p.pos + p.dprel
p.splemma
88
p.spLemma+p.h.spForm
p.spPos + p.children.dprel.bag
15
p.spPos + p1 .spPos
p1 .spForm
26
a−1 .isCurPred.lemma
a.isCurPred.pos
84
a.isCurPred.spPos
a1 .isCurPred.Lemma
37
a1 .isCurPred.spLemma
a:p|direction
57
(a:p|dpPath.dprel.seq) + a.spForm
a.form.baseline Mod
73
a.pos.baseline Mod
a.spForm.baseline Mod
75
a.baseline Mod
a.lm.Lemma
59
a.lm.spForm
a.lm.spPos
65
a.rm.lemma
a.highSupportNoun.pos
62
a.highSupportNoun.spPos
a.highSupportVerb.spPos
42
a.lowSupportNoun.pos
a.lowSupportPorp.spLemma
98
a.lowSupportVerb.pos
a−1 .spLemma+a.spLemma
38
a:p|dpPathPred.spLemma.seq
a:p|linePath.spForm.seq
80
a:p|linePath.spLemma.seq
a:p|linePath.spLemma.bag
86
a:p|linePath.spPos.seq
a:p|linePath.spPos.bag
66
a:p|dpPathPred.spPos
a.existSemdprel A0
49
a.existSemdprel A1
a.form
94
a.form = p.form ?
a.form + a.form
95
a.lemma
a.lemma + a.dprel
21
a.lemma + a.h.form
a.lemma + a.pphead.form
44
a.spForm = p.spForm ?
a.spLemma + a.pphead.spForm
24
a.spPos + a.spPos
(a:p|dpPath.dprel.seq) + p.form
45
(a:p|dpPath.dprel.seq) + p.spForm
(a:p|dpPath.dprel.seq) + a.form
13
p.lm.form + p.noFarChildren.spPos.bag + p.rm.form
a−2 .isCurPred.lemma + a−1 .isCurPred.lemma
a.isCurPred.pos + a1 .isCurPred.pos
a.isCurPred.spLemma + a1 .isCurPred.spLemma
a.form.baseline Ax + a.voice + (a:p|direction)
a.spForm.baseline Ax+ a.voice + (a:p|direction)
a.spPos.baseline Ax + a.voice + (a:p|direction)
a.highSupportNoun:p|dpPathShared.dprel.seq
a.highSupportVerb:p|dpPathShared.dprel.seq

227

Rank
61
91
93
6
12
9
5
3
14
28
96
22
11
74
76
60
81
20
87
56
63
53
51
39
1
40
43
29
89
82
25
52
64
99
23
77
78
79
30
68

Zhao, Zhang & Kit

16
31
32
33
34
17
69
70
71
72
58
19

a.lowSupportNoun:p|dpPath.dprel.seq
a.lowSupportNoun:p|dpPathArgu.dprel.seq
a.lowSupportNoun:p|dpPathArgu.spPos.seq
a.lowSupportNoun:p|dpPathShared.dprel.seq
a.lowSupportNoun:p|dpPathShared.spPos.seq
a.lowSupportNoun:p|dpPathPred.dprel.seq
a.lowSupportVerb:p|dpPathArgu.dprel.seq
a.lowSupportVerb:p|dpPathArgu.spPos.seq
a.lowSupportVerb:p|dpPathShared.dprel.seq
a.lowSupportVerb:p|dpPathShared.spPos.seq
(a.highSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportNoun:p|dpTreeRelation) + p.spPos
s
Table 15: Feature templates of SN
+P besides Tables 10 and 11

Template
Rank
Template
p−1 .spLemma
74
p−2 .form
p1 .spPos
19
a1 .isCurPred.Lemma
a1 .isCurPred.spLemma
53
a.children.dprel.bag
a.h.lemma
23
a.lm.dprel + a.pos
a.lm−1 .lemma
31
a.lm.Lemma
a.pphead.lemma
27
a.pphead.spLemma
a.lowSupportNoun.spPos
8
a.lowSupportPorp.form
a.lowSupportPorp.lemma
47
a.lowSupportPorp.spForm
a.lowSupportPorp.spLemma
57
a−1 .spPos
a−1 .spPos + a1 .spPos
54
a.semdprel = A2 ?
(a:p|dpTreeRelation) + p.pos
41
(a:p|dpTreeRelation) + p.spPos
a−2 .isCurPred.spLemma + a−1 .isCurPred.spLemma
a.lowSupportPorp:p|dpPathShared.dprel.seq
a.lowSupportPorp:p|dpPathShared.spPos.seq
a.lowSupportVerb:p|dpPath.dprel.seq
(a.highSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportNoun:p|dpTreeRelation) + p.pos
(a.lowSupportNoun:p|dpTreeRelation) + p.spPos

Rank
55
71
42
63
29
39
73
79
58
20
21
61
12
13
16
11
75
66

Table 16: Feature templates of SPl besides Tables 10 and 12

Template
p.rm.dprel
p.lowSupportNoun.spForm
p−1 .form + p.form
p−1 .pos+p.pos
p−1 .spLemma
p−2 .pos
p.dprel = OBJ ?
p.lemma + p.h.form
p.spPos + p1 .spPos
a.voice + (a:p|direction)
a.isCurPred.spLemma

Rank
88
16
103
32
13
18
59
42
34
75
29

228

Template
p.children.dprel.seq
p.lowSupportProp:p|dpTreeRelation
p−1 .lemma + p.lemma
p−1 .spForm + p.spForm
p−2 .form + p−1 .form
p−2 .spForm
p.form + p.dprel
p.pos + p.dprel
p1 .spForm
a.isCurPred.lemma
a.lm.dprel + a.dprel

Rank
27
72
91
40
99
39
95
1
86
43
98

Semantic Dependency Parsing

a.lm.dprel + a.pos
76
a.lm−1 .spLemma
a.lm.pos + a.pos
19
a.lm.spForm
a.lm.spPos
49
a.ln.dprel + a.pos
a.rm1 .spPos
21
a.highSupportNoun.lemma
a.highSupportNoun.pos
48
a.highSupportNoun.spPos
a.lowSupportVerb.pos
97
a.lowSupportVerb.spLemma
a.lowSupportVerb.spPos
12
a−1 .lemma
a−1 .spLemma+a.spLemma
77
a−2 .pos
a:p|linePath.distance
67
a:p|dpTreeRelation
a:p|dpPathPred.spPos
115
a.dprel = OBJ ?
a.form + p.form
83
a.pos + p.pos
a.spForm + p.spForm
87
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
119
a.spLemma+a.dprel+a.h.spLemma
a.spLemma + a.pphead.spForm
66
a.spLemma + a1 .spLemma
a1 .pos
52
a1 .spPos
(a:p|dpTreeRelation) + p.form
111
(a:p|dpTreeRelation) + p.spForm
(a:p|dpTreeRelation) + a.form
84
(a:p|dpTreeRelation) + a.spForm
(a:p|dpTreeRelation) + a.spPos
15
(a:p|dpPath.dprel.seq) + p.form
(a:p|dpPath.dprel.seq) + p.spForm
108
(a:p|dpPath.dprel.seq) + a.form
(a:p|dpPath.dprel.seq) + a.spForm
37
p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos
a−2 .isCurPred.lemma + a−1 .isCurPred.lemma
(a1 :p|direction) + (a2 :p|direction)
a.noFarChildren.spPos.bag + a.rm.spPos
a.highSupportVerb:p|dpTreeRelation
(a.highSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportNoun:p|dpTreeRelation) + p.form
(a.lowSupportNoun:p|dpTreeRelation) + p.spForm

3
107
25
14
51
78
101
102
20
116
92
53
60
55
23
8
41
56
70
117
58
105
22
85
47
82
71

l
Table 17: Feature templates of SN
besides Tables 10 and 12

Template
p.currentSense + a.spPos
p.lm.form
p.lowSupportNoun.spForm
p−1 .form + p.form
p−1 .spForm + p.spForm
p−2 .pos
p.form + p.dprel
p.spPos + p1 .spPos
p1 .spPos
a.isCurPred.lemma
a1 .isCurPred.Lemma
a.children.dprel.bag
a.lm−1 .lemma
a.lm.Lemma
a.lm.spForm
a.ln.dprel + a.pos
a.lowSupportNoun:p|dpTreeRelation
a−1 .lemma
a−1 .spPos

Rank
69
101
99
106
98
87
114
45
102
52
41
48
20
84
34
63
93
81
109

229

Template
p.rm.dprel
p.lm.spForm
p.lowSupportProp:p|dpTreeRelation
p−1 .pos+p.pos
p−2 .form + p−1 .form
p−2 .spForm
p.spForm + p.dprel
p1 .spForm
a.voice + (a:p|direction)
a.isCurPred.spLemma
a1 .isCurPred.spLemma
a.lm.dprel + a.dprel
a.lm−1 .spLemma
a.lm.pos + a.pos
a.lm.spPos
a.rm1 .spPos
a.lowSupportVerb.spLemma
a−1 .spLemma+a.spLemma
a−1 .spPos + a1 .spPos

Rank
117
51
74
1
40
54
115
37
10
66
64
70
17
8
59
111
15
31
92

Zhao, Zhang & Kit

a:p|linePath.distance
80
a:p|dpTreeRelation
a:p|dpPathPred.spPos
85
a.existSemdprel A2
a.semdprel = A2 ?
78
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
61
a.spLemma+a.dprel+a.h.spLemma
a.spLemma + a.pphead.spForm
62
a1 .lemma
a1 .spPos
44
(a:p|dpTreeRelation) + a.form
(a:p|dpTreeRelation) + a.spForm
73
(a:p|dpTreeRelation) + a.spPos
(a:p|dpPath.dprel.seq) + p.form
22
(a:p|dpPath.dprel.seq) + p.spForm
(a:p|dpPath.dprel.seq) + a.form
89
(a:p|dpPath.dprel.seq) + a.spForm
p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos
a−2 .isCurPred.lemma + a−1 .isCurPred.lemma
a−2 .isCurPred.spLemma + a−1 .isCurPred.spLemma
a.noFarChildren.spPos.bag + a.rm.spPos
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
(a.highSupportNoun:p|dpTreeRelation) + p.form
(a.highSupportNoun:p|dpTreeRelation) + p.spForm
(a.lowSupportNoun:p|dpTreeRelation) + p.spPos
(a.lowSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportVerb:p|dpTreeRelation) + a.spForm

57
77
71
90
68
25
58
83
103
108
23
46
95
55
35
116
118
56
105
107

l
Table 18: Feature templates of SN
+P besides Tables 10 and 12

References
Carreras, X., & Màrquez, L. (2005). Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of the Ninth Conference on Computational Natural
Language Learning, pp. 152–164, Ann Arbor, Michigan.
Che, W., Li, Z., Hu, Y., Li, Y., Qin, B., Liu, T., & Li, S. (2008). A cascaded syntactic
and semantic dependency parsing system. In Proceedings of the Twelfth Conference
on Computational Natural Language Learning, pp. 238–242, Manchester.
Chen, S. F., & Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum entropy
models. Technical report CMU-CS-99-108, School of Computer Science, Carnegie
Mellon University.
Chen, W., Kawahara, D., Uchimoto, K., Zhang, Y., & Isahara, H. (2008). Dependency
parsing with short dependency relations in unlabeled data. In Proceedings of the
Third International Joint Conference on Natural Language Processing, Vol. 1, pp.
88–94, Hyderabad, India.
Ciaramita, M., Attardi, G., Dell’Orletta, F., & Surdeanu, M. (2008). DeSRL: A lineartime semantic role labeling system. In Proceedings of the Twelfth Conference on
Computational Natural Language Learning, pp. 258–262, Manchester.
Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms for multiclass problems. The Journal of Machine Learning Research, 3 (Jan), 951–991.
230

Semantic Dependency Parsing

Dang, H. T., & Palmer, M. (2005). The role of semantic roles in disambiguating verb senses. In Proceedings of the 43rd Annual Meeting of the Association for Computational
Linguistics, pp. 42–49, Ann Arbor, Michigan.
Ding, W., & Chang, B. (2008). Improving Chinese semantic role classiﬁcation with hierarchical feature selection strategy. In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pp. 324–333, Honolulu, Hawaii.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational
Linguistics, 28 (3), 245–288.
Hajič, J., Ciaramita, M., Johansson, R., Kawahara, D., Martı́, M. A., Màrquez, L., Meyers,
A., Nivre, J., Padó, S., Štěpánek, J., Straňák, P., Surdeanu, M., Xue, N., & Zhang,
Y. (2009). The CoNLL-2009 shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the Thirteenth Conference on Computational
Natural Language Learning: Shared Task, pp. 1–18, Boulder, Colorado.
Henderson, J., Merlo, P., Musillo, G., & Titov, I. (2008). A latent variable model of
synchronous parsing for syntactic and semantic dependencies. In Proceedings of
the Twelfth Conference on Computational Natural Language Learning, pp. 178–182,
Manchester.
Jiang, Z. P., & Ng, H. T. (2006). Semantic role labeling of NomBank: A maximum entropy
approach. In Proceedings of the 2006 Conference on Empirical Methods in Natural
Language Processing, pp. 138–145, Sydney.
Johansson, R., & Nugues, P. (2008). Dependency-based syntactic–semantic analysis with
PropBank and NomBank. In Proceedings of the Twelfth Conference on Computational
Natural Language Learning, pp. 183–187, Manchester.
Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. In Proceedings of the 46th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pp. 595–603, Columbus, Ohio.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W.-T. (2005). Generalized inference with
multiple semantic role labeling systems. In Proceedings of the Ninth Conference on
Computational Natural Language Learning, pp. 181–184, Ann Arbor, Michigan.
Liu, C., & Ng, H. T. (2007). Learning predictive structures for semantic role labeling of
NomBank. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pp. 208–215, Prague.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, Special Issue on
Using Large Corpora: II, 19 (2), 313–330.
Màrquez, L., Surdeanu, M., Comas, P., & Turmo, J. (2005). A robust combination strategy
for semantic role labeling. In Proceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Language Processing, pp. 644–651,
Vancouver.
McDonald, R., & Pereira, F. (2006). Online learning of approximate dependency parsing
algorithms. In Proceedings of the Eleventh Conference of the European Chapter of the
Association for Computational Linguistics, pp. 81–88, Trento, Italy.
231

Zhao, Zhang & Kit

McDonald, R., Pereira, F., Ribarov, K., & Hajič, J. (2005). Non-projective dependency
parsing using spanning tree algorithms. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Natural Language Processing, pp.
523–530, Vancouver, British Columbia.
Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska, V., Young, B., & Grishman, R.
(2004). The NomBank project: An interim report. In Meyers, A. (Ed.), HLT-NAACL
2004 Workshop: Frontiers in Corpus Annotation, pp. 24–31, Boston.
Meza-Ruiz, I., & Riedel, S. (2009). Jointly identifying predicates, arguments and senses
using Markov logic. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational
Linguistics, pp. 155–163, Boulder, Colorado.
Nash, S. G., & Nocedal, J. (1991). A numerical study of the limited memory BFGS method
and truncated-Newton method for large scale optimization. SIAM Journal on Optimization, 1 (2), 358–372.
Nivre, J., & McDonald, R. (2008). Integrating graph-based and transition-based dependency parsers. In Proceedings of the 46th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pp. 950–958, Columbus,
Ohio.
Nocedal, J. (1980). Updating quasi-Newton matrices with limited storage. Mathematics of
Computation, 35 (151), 773–782.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). The Proposition Bank: An annotated
corpus of semantic roles. Computational Linguistics, 31 (1), 71–106.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., & Jurafsky, D. (2005). Semantic role
labeling using diﬀerent syntactic views. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pp. 581–588, Ann Arbor, Michigan.
Pradhan, S. S., Ward, W. H., Hacioglu, K., Martin, J. H., & Jurafsky, D. (2004). Shallow
semantic parsing using support vector machines. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for
Computational Linguistics, pp. 233–240, Boston.
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer
linear programming inference. In Proceedings of the 20th International Conference on
Computational Linguistics, pp. 1346–1352, Geneva.
Pustejovsky, J., Meyers, A., Palmer, M., & Poesio, M. (2005). Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and coreference. In Proceedings of the
Workshop on Frontiers in Corpus Annotations II: Pie in the Sky, pp. 5–12, Ann
Arbor, Michigan.
Riedel, S. (2008). Improving the accuracy and eﬃciency of map inference for markov logic.
In Proceedings of the Twenty-Fourth Conference Annual Conference on Uncertainty
in Artificial Intelligence, pp. 468–475, Corvallis, Oregon.
Roth, D., & Yih, W. (2004). A linear programming formulation for global inference in
natural language tasks. In Proceedings of the Eighth Conference on Computational
Natural Language Learning, pp. 1–8, Boston.
232

Semantic Dependency Parsing

Surdeanu, M., Johansson, R., Meyers, A., Màrquez, L., & Nivre, J. (2008). The CoNLL 2008
shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of
the Twelfth Conference on Computational Natural Language Learning, pp. 159–177,
Manchester.
Surdeanu, M., Marquez, L., Carreras, X., & Comas, P. R. (2007). Combination strategies
for semantic role labeling. Journal of Artificial Intelligence Research, 29, 105–151.
Titov, I., Henderson, J., Merlo, P., & Musillo, G. (2009). Online graph planarisation for
synchronous parsing of semantic and syntactic dependencies. In Proceedings of the
21st International Jont Conference on Artifical Intelligence, pp. 1562–1567, Pasadena,
California.
Toutanova, K., Haghighi, A., & Manning, C. D. (2005). Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pp. 589–596, Ann Arbor, Michigan.
Xue, N. (2006). Semantic role labeling of nominalized predicates in Chinese. In Proceedings
of Human Language Technology Conference of the North American Chapter of the
Association of Computational Linguistics, Main Conference, pp. 431–438, New York.
Xue, N., & Palmer, M. (2004). Calibrating features for semantic role labeling. In Proceedings
of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 88–
94, Barcelona.
Zhao, H., Chen, W., Kazama, J., Uchimoto, K., & Torisawa, K. (2009). Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies. In Proceedings of the Thirteenth Conference on Computational Natural Language
Learning: Shared Task, pp. 61–66, Boulder, Colorado.
Zhao, H., Chen, W., Kit, C., & Zhou, G. (2009). Multilingual dependency learning: A huge
feature engineering method to semantic dependency parsing. In Proceedings of the
Thirteenth Conference on Computational Natural Language Learning: Shared Task,
pp. 55–60, Boulder, Colorado.
Zhao, H., & Kit, C. (2008). Parsing syntactic and semantic dependencies with two singlestage maximum entropy models. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, pp. 203–207, Manchester.

233

Journal of Artificial Intelligence Research 46 (2013) 449–509

Submitted 9/12; published 03/13

Incremental Clustering and Expansion for Faster
Optimal Planning in Decentralized POMDPs
Frans A. Oliehoek

frans.oliehoek@maastrichtuniversity.nl

Maastricht University
Maastricht, The Netherlands

Matthijs T.J. Spaan

m.t.j.spaan@tudelft.nl

Delft University of Technology
Delft, The Netherlands

Christopher Amato

camato@csail.mit.edu

Massachusetts Institute of Technology
Cambridge, MA, USA

Shimon Whiteson

s.a.whiteson@uva.nl

University of Amsterdam
Amsterdam, The Netherlands

Abstract
This article presents the state-of-the-art in optimal solution methods for decentralized
partially observable Markov decision processes (Dec-POMDPs), which are general models for
collaborative multiagent planning under uncertainty. Building off the generalized multiagent A* ( GMAA*) algorithm, which reduces the problem to a tree of one-shot collaborative
Bayesian games (CBGs), we describe several advances that greatly expand the range of DecPOMDPs that can be solved optimally. First, we introduce lossless incremental clustering
of the CBGs solved by GMAA*, which achieves exponential speedups without sacrificing
optimality. Second, we introduce incremental expansion of nodes in the GMAA* search
tree, which avoids the need to expand all children, the number of which is in the worst case
doubly exponential in the node’s depth. This is particularly beneficial when little clustering
is possible. In addition, we introduce new hybrid heuristic representations that are more
compact and thereby enable the solution of larger Dec-POMDPs. We provide theoretical
guarantees that, when a suitable heuristic is used, both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally,
we present extensive empirical results demonstrating that GMAA*-ICE, an algorithm that
synthesizes these advances, can optimally solve Dec-POMDPs of unprecedented size.

1. Introduction
A key goal of artificial intelligence is the development of intelligent agents that interact with
their environment in order to solve problems, achieve goals, and maximize utility. While such
agents sometimes act alone, researchers are increasingly interested in collaborative multiagent
systems, in which teams of agents work together to perform all manner of tasks. Multiagent
systems are appealing, not only because they can tackle inherently distributed problems, but
because they facilitate the decomposition of problems too complex to be tackled by a single
c
2013
AI Access Foundation. All rights reserved.

Oliehoek, Spaan, Amato, & Whiteson

agent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Buşoniu, Babuška, &
De Schutter, 2008).
One of the primary challenges of multiagent systems is the presence of uncertainty. Even
in single-agent systems, the outcome of an action may be uncertain, e.g., the action may fail
with some probability. Furthermore, in many problems the state of the environment may be
uncertain due to limited or noisy sensors. However, in multiagent settings these problems
are often greatly exacerbated. Since agents have access only to their own sensors, typically a
small fraction of those of the complete system, their ability to predict how other agents will
act is limited, complicating cooperation. If such uncertainties are not properly addressed,
arbitrarily bad performance may result.
In principle, agents can use communication to synchronize their beliefs and coordinate
their actions. However, due to bandwidth constraints, it is typically infeasible for all agents
to broadcast the necessary information to all other agents. In addition, in many realistic
scenarios, communication may be unreliable, precluding the possibility of eliminating all uncertainty about other agents’ actions.
Especially in recent years, much research has focused on approaches to (collaborative)
multiagent systems that deal with uncertainty in a principled way, yielding a wide variety
of models and solution methods (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004;
Seuken & Zilberstein, 2008). This article focuses on the decentralized partially observable
Markov decision process (Dec-POMDP), a general model for collaborative multiagent planning under uncertainty. Unfortunately, solving a Dec-POMDP, i.e., computing an optimal
plan, is generally intractable (NEXP-complete) (Bernstein, Givan, Immerman, & Zilberstein,
2002) and in fact even computing solutions with absolutely bounded error (i.e., ǫ-approximate
solutions) is also NEXP-complete (Rabinovich, Goldman, & Rosenschein, 2003). In particular,
the number of joint policies grows exponentially with the number of agents and observations
and doubly exponentially with respect to the horizon of the problem.1 Though these complexity results preclude methods that are efficient on all problems, developing better optimal
solution methods for Dec-POMDPs is nonetheless an important goal, for several reasons.
First, since the complexity results describe only the worst case, there is still great potential
to improve the performance of optimal methods in practice. In fact, there is evidence that
many problems can be solved much faster than the worst-case complexity bound indicates
(Allen & Zilberstein, 2007). In this article, we present experiments that clearly demonstrate
this point: on many problems, the methods we propose scale vastly beyond what would be
expected for a doubly-exponential dependence on the horizon.
Second, as computer speed and memory capacity increase, a growing set of small and
medium-sized problems can be solved optimally. Some of these problems arise naturally while
others result from the decomposition of larger problems. For instance, it may be possible
to extrapolate optimal solutions of problems with shorter planning horizons, using them as
the starting point of policy search for longer-horizon problems as in the work of Eker and
Akın (2013), or to use such shorter-horizon, no-communication solutions inside problems with
communication (Nair, Roth, & Yohoo, 2004; Goldman & Zilberstein, 2008). More generally,
optimal policies of smaller problems can potentially be used to find good solutions for larger
problems. For instance, transfer planning (Oliehoek, 2010; Oliehoek, Whiteson, & Spaan,
1. Surprisingly, the number of states in a Dec-POMDP is less important, e.g., brute-force search depends on
the number of states only via its policy evaluation routine, which scales linearly in the number of states.

450

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

2013) employs optimal solutions to problems with few agents to better solve problems with
many agents. By performing (approximate) influence-based abstraction and influence search
(Witwicki, 2011; Oliehoek, Witwicki, & Kaelbling, 2012), optimal solutions of component
problems can potentially be used to find (near-)optimal solutions of larger problems.
Third, optimal methods offer important insights into the nature of specific Dec-POMDP
problems and their solutions. For instance, the methods introduced in this article enabled the
discovery of certain properties of the BroadcastChannel benchmark problem that make
it much easier to solve.
Fourth, optimal methods provide critical inspiration for principled approximation methods. In fact, almost all successful approximate Dec-POMDP methods are based on optimal
ones (see, e.g., Seuken & Zilberstein, 2007b, 2007a; Dibangoye, Mouaddib, & Chai-draa, 2009;
Amato, Dibangoye, & Zilberstein, 2009; Wu, Zilberstein, & Chen, 2010a; Oliehoek, 2010) or
locally optimal ones (Velagapudi, Varakantham, Scerri, & Sycara, 2011)2 , and the clustering technique presented in this article forms the basis of a recently introduced approximate
clustering technique (Wu, Zilberstein, & Chen, 2011).
Finally, optimal methods are essential for benchmarking approximate methods. In recent
years, there have been huge advances in the approximate solution of Dec-POMDPs, leading
to the development of solution methods that can deal with large horizons, hundreds of agents
and many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a;
Oliehoek, 2010; Velagapudi et al., 2011).
However, since computing even ǫ-approximate
solutions is NEXP-complete, any method whose complexity is not doubly exponential cannot
have any guarantees on the absolute error of the solution (assuming EXP6=NEXP). As such,
existing effective approximate methods have no quality guarantees.3
Consequently, it is difficult to meaningfully interpret their empirical performance without
the upper bounds optimal methods supply. While approximate methods can also be benchmarked against lower bounds (e.g., other approximate methods), such comparisons cannot
detect when a method fails to find good solutions. Doing so requires benchmarking against
upper bounds and, unfortunately, upper bounds that are easier to compute, such as QMDP
and QPOMDP, are too loose to be helpful (Oliehoek, Spaan, & Vlassis, 2008). As such,
benchmarking with respect to optimal solutions is an important part of the verification of any
approximate algorithm. Since existing optimal methods can only tackle very small problems,
scaling optimal solutions to larger problems is a critical goal.
1.1 Contributions
This article presents the state-of-the-art in optimal solution methods for Dec-POMDPs. In
particular, it describes several advances that greatly expand the horizon to which many DecPOMDPs can be solved optimally. In addition, it proposes and evaluates a complete algorithm
that synthesizes these advances. Our approach is based on the generalized multiagent A*
(GMAA*) algorithm (Oliehoek, Spaan, & Vlassis, 2008), which makes it possible to reduce
the problem to a tree of one-shot collaborative Bayesian games (CBGs). The appeal of this
2. The method by Velagapudi et al. (2011) repeatedly computes best responses in a way similar to DP-JESP
(Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). The best response computation, however, exploits
sparsity of interactions.
3. Note that we refer to methods without quality guarantees as approximate rather than heuristic to avoid
confusion with heuristic search, which is used throughout this article and is exact.

451

Oliehoek, Spaan, Amato, & Whiteson

approach is the abstraction layer it introduces, which has led to various insights into DecPOMDPs and, in turn, to the improved solution methods we describe.
The specific contributions of this article are:4
1. We introduce lossless clustering of CBGs, a technique to reduce the size of the CBGs
for which GMAA* enumerates all possible solutions, while preserving optimality. This
can exponentially reduce the number of child nodes in the GMAA* search tree, leading
to huge increases in efficiency. In addition, by applying incremental clustering (IC) to
GMAA*, our GMAA*-IC method can avoid clustering exponentially sized CBGs.
2. We introduce incremental expansion (IE) of nodes in the GMAA* search tree. Although
clustering may reduce the number of children of a search node, this number is in the
worst case still doubly exponential in the node’s depth. GMAA*-ICE, which applies
IE to GMAA*-IC, addresses this problem by creating a next child node only when it
is a candidate for further expansion.
3. We provide theoretical guarantees for both GMAA*-IC and GMAA*-ICE. In particular, we show that, when using a suitable heuristic, both algorithms are both complete
and search equivalent.
4. We introduce an improved heuristic representation. Tight heuristics like those based
on the underlying POMDP solution (QPOMDP ) or the value function resulting from
assuming 1-step-delayed communication (QBG ) are essential for heuristic search methods like GMAA* (Oliehoek, Spaan, & Vlassis, 2008). However, the space needed to
store these heuristics grows exponentially with the problem horizon. We introduce hybrid representations that are more compact and thereby enable the solution of larger
problems.
5. We present extensive empirical results that show substantial improvements over the
current state-of-the-art. Whereas Seuken and Zilberstein (2008) argued that GMAA*
can at best optimally solve Dec-POMDPs only one horizon further than brute-force
search, our results demonstrate that GMAA*-ICE can do much better. In addition, we
provide a comparative overview of the results of competitive optimal solution methods
from the literature.
The primary aim of the techniques introduced in this article is to improve scalability
with respect to the horizon. Our empirical results confirm that these techniques are highly
successful in this regard. As an added bonus, our experiments also demonstrate improvement
in scalability with respect to the number of agents. In particular, we present the first optimal
results on general (non-special case) Dec-POMDPs with more than three agents. Extensions
of our techniques to achieve further improvements with respect to the number of agents,
as well as promising ways to combine the ideas behind our methods with state-of-the-art
approximate approaches, are discussed under future work in Section 7.
4. This article synthesizes and extends research that was already reported in two conference papers (Oliehoek,
Whiteson, & Spaan, 2009; Spaan, Oliehoek, & Amato, 2011).

452

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

1.2 Organization
The article is organized as follows. Section 2 provides background on the Dec-POMDP model,
the GMAA* heuristic search solution method, as well as suitable heuristics. In Section 3, we
introduce lossless clustering of the CBGs and its integration into GMAA*. Section 4 introduces the incremental expansion of search nodes. The empirical evaluation of the proposed
techniques is reported in Section 5. We give a treatment of related work in Section 6. Future
work is discussed in Section 7 and conclusions are drawn in Section 8.

2. Background
In a Dec-POMDP, multiple agents must collaborate to maximize the sum of the common
rewards they receive over multiple timesteps. Their actions affect not only their immediate
rewards but also the state to which they transition. While the current state is not known to
the agents, at each timestep each agent receives a private observation correlated with that
state.



Definition 1. A Dec-POMDP is a tuple D, S, A, T, O, O, R, b0 , h , where
• D = {1, . . . ,n} is the finite set of agents.
	

• S = s1 , . . . ,s|S| is the finite set of states.

• A = ×i Ai is the set of joint actions a = ha1 , . . . , an i, where Ai is the finite set of actions
available to agent i.
• T is a transition function specifying the state transition probabilities Pr(s′ |s,a).
• O = ×i Oi is the finite set of joint observations. At every stage one joint observation
o = ho1 ,...,on i is received. Each agent i observes only its own component oi .
• O is the observation function, which specifies observation probabilities Pr(o|a,s′ ).
• R(s,a) is the immediate reward function mapping (s,a)-pairs to real numbers.
• b0 ∈ ∆(S) is the initial state distribution at time t = 0, where ∆(S) denotes the infinite
set of probability distributions over the finite set S.
• h is the horizon, i.e., the number of stages. We consider the case where h is finite.
At each stage t = 0 . . . h − 1, each agent takes an individual action and receives an individual
observation.
Example 1 (Recycling Robots). To illustrate the Dec-POMDP model, consider a team of robots tasked
with removing trash from an office building, depicted in Fig. 1. The robots have sensors to find marked
trash cans, motors to move around in order to look for cans, as well as gripper arms to grasp and carry
a can. Small trash cans are light and compact enough for a single robot to carry, but large trash cans
require multiple robots to carry them out together. Because more people use them, the larger trash
cans fill up more quickly. Each robot must also ensure that its battery remains charged by moving
to a charging station before it expires. The battery level for a robot degrades due to the distance the
robot travels and the weight of the item being carried. Each robot knows its own battery level but not
that of the other robots and only the location of other robots within sensor range. The goal of this
problem is to remove as much trash as possible in a given time period.
This problem can be represented as a Dec-POMDP in a natural way. The states, S, consist of
the different locations of each robot, their battery levels and the different amounts of trash in the
cans. The actions, Ai , for each robot consist of movements in different directions as well as decisions
453

Oliehoek, Spaan, Amato, & Whiteson

Figure 1: Illustration of the Recycling Robots example, in which two robots have to remove
trash in an office environment with three small (blue) trash cans and two large (yellow) ones.
In this situation, the left robot might observe that the large trash can next to it is full, and
the other robot that the small trash can is empty. However, none of them is sure of the trash
cans’ state due to limited sensing capabilities, nor do they see the state of trash cans further
away. In particular, one robot has no knowledge regarding the observations of the other robot.
to pick up a trash can or recharge the battery (when in range of a can or a charging station). The
observations, Oi , of each robot consist of its own battery level, its own location, the locations of other
robots in sensor range and the amount of trash in cans within range. The rewards, R, could consist of
a large positive value for a pair of robots emptying a large (full) trash can, a small positive value for
a single robot emptying a small trash can and negative values for a robot depleting its battery or a
trash can overflowing. An optimal solution is a joint policy that leads to the expected behavior (given
that the rewards are properly specified). That is, it ensures that the robots cooperate to empty the
large trash cans when appropriate and the small ones individually while considering battery usage.

For explanatory purposes, we also consider a much simpler problem, the so-called decentralized tiger problem (Nair et al., 2003).
Example 2 (Dec-Tiger). The Dec-Tiger problem concerns two agents that find themselves in a hallway with two doors. Behind one door, there is a treasure and behind the other is a tiger. The state
describes which door the tiger is behind—left (sl ) or right (sr )—each occurring with 0.5 probability
(i.e., the initial state distribution b0 is uniform). Each agent can perform three actions: open the left
door (aOL ), open the right door (aOR ) or listen (aLi ). Clearly, opening the door to the treasure will
yield a reward, but opening the door to the tiger will result in a severe penalty. A greater reward
is given for both agents opening the correct door at the same time. As such, a good strategy will
probably involve listening first. The listen actions, however, also have a minor cost (negative reward).
At every stage the agents get an observation. The agents can either hear the tiger behind the left
(oHL ) or right (oHR ) door, but each agent has a 15% chance of hearing it incorrectly (getting the wrong
observation). Moreover, the observation is informative only if both agents listen; if either agent opens
a door, both agents receive an uninformative (uniformly drawn) observation and the problem resets to
sl or sr with equal probability. At this point the problem just continues, such that the agents may be
able to open the door to the treasure multiple times. Also note that, since the only two observations
the agents can get are oHL , oHR , the agents have no way of detecting that the problem has been reset:
if one agent opens the door while the other listens, the other agent will not be able to tell that the
door was opened. For a complete specification, see the discussion by Nair et al. (2003).

Given a Dec-POMDP, the agents’ common goal is to maximize the expected cumulative
reward or return. The planning task entails finding a joint policy π = hπ1 , . . . ,πn i from the
space of joint policies Π, that specifies an individual policy πi for each agent i. Such an
454

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

individual policy in general specifies an individual action for each action-observation history
(AOH) θ~it = (a0i ,o1i , . . . ,ait−1 ,oti ), e.g., πi (θ~it ) = ati . However, it is possible to restrict our
attention to deterministic or pure policies, in which case πi maps each observation history
~ t to an action, e.g., πi (~o t ) = at . The number of such policies is
(OH) (o1i , . . . ,oti ) = ~oit ∈ O
i
i
i
h −1)/(|O |−1)
(|O
|
i
|Ai | i
and the number of joint policies is therefore

n|O∗ |h −1 
O |A∗ | |O∗ |−1 ,
(2.1)
where A∗ and O∗ denote the largest individual action and observation sets. The quality of a
particular joint policy is expressed by the expected cumulative reward it induces, also referred
to as its value.
Definition 2. The value V (π) of a joint policy π is
V (π) , E

h−1
hX
t=0


i

R(st ,at )π,b0 ,

(2.2)

where the expectation is over sequences of states, actions and observations.
The planning problem for a Dec-POMDP is to find an optimal joint policy π ∗ , i.e., a joint
policy that maximizes the value: π ∗ = arg maxπ V (π).
Because an individual policy πi depends only on the local information ~oi available to an
agent, the on-line execution phase is truly decentralized: no communication takes place other
than that modeled via actions and observations. The planning itself however, may take place
in an off-line phase and be centralized. This is the scenario that we consider in this article. For
a more detailed introduction to Dec-POMDPs see, e.g., the work of Seuken and Zilberstein
(2008) and Oliehoek (2012).
2.1 Heuristic Search Methods
In recent years, numerous Dec-POMDP solution methods have been proposed. Most of these
methods fall into one of two categories: dynamic programming and heuristic search methods.
Dynamic programming methods take a backwards or ‘bottom-up’ perspective by first considering policies for the last time step t = h − 1 and using them to construct policies for stage
t = h − 2, etc. In contrast, heuristic search methods take a forward or ‘top-down’ perspective
by first constructing plans for t = 0 and extending them to later stages.
In this article, we focus on the heuristic search approach that has shown state-of-the-art
results. As we make clear in this section, this method can be interpreted as searching over a
tree of collaborative Bayesian games (CBGs). These CBGs provide a convenient abstraction
layer that facilitates the explanation of the techniques introduced in this article.
This section provides some concise background on heuristic search methods. For a more
detailed description, see the work of Oliehoek, Spaan, and Vlassis (2008). For a further description of dynamic programming methods and their relationship to heuristic search methods,
see the work of Oliehoek (2012).
2.1.1 Multiagent A*
Szer, Charpillet, and Zilberstein (2005) introduced a heuristically guided policy search method
called multiagent A* (MAA*). It performs an A* search over partially specified joint policies,
455

Oliehoek, Spaan, Amato, & Whiteson

t=0

t=1

t=2

δi0

aLi
γiτ =2

ϕ2i

oHR

oHL
aOL

δi1

aOL

oHL

oHR

oHL

oHR

aLi

aLi

aOL

aLi

δi2

γiτ =1
Figure 2: An arbitrary policy for the Dec-Tiger problem. The figure illustrates the different
types of partial policies used in this paper. The shown past policy ϕ2i consists of two decision
rules δi0 , δi1 . Also shown are two sub-tree policies γiτ =1 , γiτ =2 (introduced in Section 3.1.2).
pruning joint policies that are guaranteed to be worse than the best (fully specified) joint policy
found so far. Oliehoek, Spaan, and Vlassis (2008) generalized the algorithm by making explicit
the expand and selection operators performed in the heuristic search. The resulting algorithm,
generalized MAA* (GMAA*) offers a unified perspective of MAA* and the forward sweep
policy computation method (Emery-Montemerlo, 2005), which differ in how they implement
GMAA*’s expand operator: forward sweep policy computation solves (i.e., finds the best
policy for) collaborative Bayesian games, while MAA* finds all policies for those collaborative
Bayesian games, as we describe in Section 2.1.2.
The GMAA* algorithm considers joint policies that are partially specified with respect
to time. These partially specified policies can be formalized as follows.
Definition 3. A decision rule δit for agent i’s decision for stage t is a mapping from action~ t → Ai .
observation histories for stage t to actions δit : Θ
i
In this article, we consider only deterministic policies. Since such policies need to condition
their actions only on observation histories, they are made up of decision rules that map length~ t → Ai . A joint decision rule δ t = hδ t , . . . ,δnt i specifies
t observation histories to actions: δit : O
1
i
a decision rule for each agent. Fig. 2 illustrates this concept, as well as that of a past policy,
which we introduce shortly. As discussed below, decision rules allow partial policies to be
defined and play a crucial role in GMAA* and the algorithms developed in this article.
Definition 4. A partial or past policy for stage t, ϕti , specifies the part of agent i’s policy
that relates to stages t′ < t. That is, it specifies the decision rules for the first t stages:
ϕti = (δi0 ,δi1 , . . . ,δit−1 ). A past policy for stage h is just a regular, or fully specified, policy
ϕhi = πi . A past joint policy ϕt = (δ 0 ,δ 1 , . . . ,δ t−1 ) specifies joint decision rules for the first t
stages.
GMAA* performs a heuristic search over such partial joint policies ϕt by constructing
a search tree as illustrated in Fig. 3a. Each node q = hϕt , v̂i in the search tree specifies a
past joint policy ϕt and heuristic value v̂. This heuristic value v̂ of the node represents an
optimistic estimate of the past joint policy Vb (ϕt ), which can be computed via
Vb (ϕt ) = V 0...t−1 (ϕt ) + H t...h−1 (ϕt ),
456

(2.3)

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

B(ϕ0 )

ϕ0
δ0

β0

δ 0′′

β 0′′
β 0′

δ 0′
ϕ1′

ϕ1
δ1

ϕ2

...

B(ϕ1 )

ϕ1′′

β1

δ 1′

...

B(ϕ1′ )

B(ϕ2 )

ϕ2′

(a) The MAA* perspective.

...

B(ϕ1′′ )

β 1′

...

B(ϕ2′ )

(b) The CBG perspective.

Figure 3: Generalized MAA*. Associated with every node is a heuristic value. The search
trees for the two perspectives shown are equivalent under certain assumptions on the heuristic,
as explained in Section 2.2.
where H t...h−1 is a heuristic value for the remaining h − t stages and V 0...t−1 (ϕt ) is the actual
expected reward ϕt achieves over the first t stages (for its definition, see Appendix A.3).
Clearly, when H t...h−1 is an admissible heuristic—a guaranteed overestimation—so is Vb (ϕt ).5
Algorithm 1 illustrates GMAA*. It starts by creating a node q 0 for a completely unspecified joint policy ϕ0 and placing it in an open list L. Then, it selects nodes (Algorithm 2) and
expands them (Algorithm 3), repeating this process until it is certain that it has found the
optimal joint policy.
The Select operator returns the highest ranked node, as defined by the following comparison operator.
Definition 5. The node comparison operator < is defined for two nodes q = hϕt ,v̂i, q ′ =
hϕt′ ,v̂ ′ i as follows:

′

, if v̂ 6= v̂ ′
v̂ < v̂
q < q ′ = depth(q) < depth(q ′ ) , otherwise if depth(q) 6= depth(q ′ )
(2.4)

 t
t′
ϕ <ϕ
, otherwise.
That is, the comparison operator first compares the heuristic values. If those are equal,
it compares the depth of the nodes. Finally, if nodes have equal value and equal depth, it
lexically compares the past joint policies. This ranking leads to A* behavior (i.e., selecting the
node from the open list with the highest heuristic value) of GMAA*, as well as guaranteeing
the same selection order in our incremental expansion technique (introduced in Section 4).
Ranking nodes with greater depth higher in case of equal heuristic value helps find tight
lower bounds early by first expanding deeper nodes (Szer et al., 2005) and is also useful in
incremental expansion.
5. More formally, H should not underestimate the value. Note that, unlike classical A* applications such as
path planning–in which an admissible heuristic should not overestimate–in our setting we maximize reward,
rather than minimize cost.

457

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 1 Generalized multiagent A*.
Input: a Dec-POMDP, an admissible heuristic H, an empty open list L
Output: an optimal joint policy π ∗
1: vGM AA ← −∞
2: q 0 ← hϕ0 = (), v̂ = +∞i
3: L.insert(q 0 )
4: repeat
5:
q ← Select(L)
6:
QExpand ← Expand(q, H)
7:
if depth(q) = h − 1 then
8:
{ QExpand contains fully specified joint policies, we only are interested in the best one }
9:
hπ, vi ← BestJointPolicyAndValue(QExpand )
10:
if v > vGM AA then
11:
π∗ ← π
{found a new best joint policy}
12:
vGM AA ← v
13:
L.Prune(vGM AA )
{(optionally) prune the open list}
14:
end if
15:
else
	

{add expanded children to open list}
16:
L.insert( q ′ ∈ QExpand | q ′ .v̂ > vGM AA )
17:
end if
18:
PostProcessNode(q, L)
19: until L is empty
20: return π ∗

Algorithm 2 Select(L): Return the highest ranked node from the open list.
Input: open list L, total order on nodes <
Output: the highest ranked node q ∗
1: q ∗ ← q ∈ L s.t. ∀q′ ∈L (q ′ 6= q =⇒ q ′ < q)
2: return q ∗

The Expand operator constructs QExpand , the set of all child nodes. That is, given a node
that contains partial joint policy ϕt = (δ 0 ,δ 1 , . . . ,δ t−1 ), it constructs Φt+1 , the set of all
ϕt+1 = (δ 0 ,δ 1 , . . . ,δ t−1 ,δ t ), by appending all possible joint decision rules δ t for the next time
step t. For all these ϕt+1 , a heuristic value is computed and a node is constructed.
After expansion, the algorithm checks (line 7) if the expansion resulted in fully specified
joint policies. If not, all children with sufficient heuristic value are placed in the open list
Algorithm 3 Expand(q, H). The expand operator of plain MAA*.
Input: q = hϕt , v̂i the search node to expand, H the admissible heuristic.
Output: QExpand the set containing all expanded child nodes.
1: QExpand ← {}
2: Φt+1 ← {ϕt+1 | ϕt+1 = (ϕt , δ t )}
3: for ϕt+1 ∈ Φt+1 do
4:
Vb (ϕt+1 ) ← V 0...t (ϕt+1 ) + H(ϕt+1 )
5:
q ′ ← hϕt+1 , Vb (ϕt+1 )i
6:
QExpand .Insert(q ′ )
7: end for
8: return QExpand

458

{create child node}

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Algorithm 4 PostProcessNode(q,L)
Input: q the expanded parent node, L the open list.
Output: the expanded node is removed.
1: L.Pop(q)

(line 16). If the children are fully specified, BestJointPolicyAndValue returns only the best
joint policy (and its value) from QExpand (see Algorithm 12 in Appendix A.1 for details of
BestJointPolicyAndValue). GMAA* also maintains a lower bound vGM AA which corresponds to the actual value of the best fully-specified joint policy found so far. If the newly
found joint policy has a higher value this lower bound is updated (lines 11 and 12). Also, any
nodes for partial joint policies ϕt+1 with an upper bound that is lower than the best solution
so far, Vb (ϕt+1 ) < vGM AA , can be pruned (line 13). This pruning takes additional time, but
can save memory. Finally, PostProcessNode simply removes the parent node from the open
list (this procedure is augmented for incremental expansion in Section 4). The search ends
when the list becomes empty, at which point an optimal joint policy has been found.
GMAA* is complete, i.e., it will search until it finds a solution. Therefore, in theory,
GMAA* is guaranteed to eventually produce an optimal joint policy (Szer et al., 2005).
However, in practice, this is often infeasible for larger problems. A major source of complexity
is the full expansion of a search node. The number of joint decision rules for stage t that can
form the children of a node at depth t in the search tree6 is


t
O |A∗ |n(|O∗ | ) ,
(2.5)
which is doubly exponential in t. Comparing (2.1) with (2.5), we see that the worst case
complexity of expanding a node for the deepest level in the tree t = h − 1 is comparable to
that of brute force search for the entire Dec-POMDP. Consequently, Seuken and Zilberstein
(2008) conclude that MAA* “can at best solve problems whose horizon is only 1 greater than
those that can already be solved by naı̈ve brute force search.”
2.1.2 The Bayesian Game Perspective
GMAA* makes it possible to interpret MAA* as the solution of a collection of collaborative
Bayesian games (CBGs). We employ this approach throughout this article, as it facilitates
the improvements to GMAA* that we introduce, each of which results in significant advances
in the state-of-the-art in Dec-POMDP solutions.
A Bayesian game (BG) models a one-shot interaction between a number of agents. It is
an extension of the well-known strategic game (also known as a normal form game) in which
each agent holds some private information (Osborne & Rubinstein, 1994). A CBG is a BG
in which the agents receive identical payoffs. In the Bayesian game perspective, each node q
in the GMAA* search tree, along with its corresponding partial joint policy ϕt , defines a
CBG (Oliehoek, Spaan, & Vlassis, 2008). That is, given state distribution b0 , for each ϕt ,
it is possible to construct a CBG B(b0 ,ϕt ) that represents the decision-making problem for
stage t given that ϕt was followed for the first t stages starting from b0 . When it is clear what
b0 is, we simply write B(ϕt ).
6. We follow the convention that the root has depth 0.

459

Oliehoek, Spaan, Amato, & Whiteson

Definition 6. A collaborative Bayesian game (CBG) B(b0 ,ϕt ) = hD, A, Θ, Pr(·), ui modeling
stage t of a Dec-POMDP, given initial state distribution b0 and past joint policy ϕt , consists
of:
• D, the set of agents {1 . . . n},
• A, the set of joint actions,
• Θ, the set of their joint types, each of which specifies a type for each agent θ =
hθ1 , . . . ,θn i,
• Pr(·), a probability distribution over joint types,
• u, a (heuristic) payoff function mapping joint type and action to a real number: u(θ,a).
In any Bayesian game, the type θi of an agent i represents the private information it holds.
For instance, in a Bayesian game modeling of a job recruitment scenario, the type of an agent
may indicate whether that agent is a hard worker. In a CBG for a Dec-POMDP, an agent’s
private information is its individual AOH. Therefore, the type θi of an agent i corresponds to
θ~it , its history of actions and observations: θi ↔ θ~it . Similarly, a joint type corresponds to a
joint AOH: θ ↔ ~θ t .
Consequently, u should provide a (heuristic) estimate for the long-term payoff of each
(~θ t ,a)-pair. In other words, the payoff function corresponds to a heuristic Q-value: u(θ,a) ↔
b ~θ t ,a). We discuss how to compute such heuristics in Section 2.2. Given ϕt , b0 , and the
Q(
correspondence of joint types and AOHs, the probability distribution over joint types is:
Pr(θ) , Pr(~θ t |b0 ,ϕt ),

(2.6)

where the latter probability is the marginal of Pr(s,~θ t |b0 ,ϕt ) as defined by (A.2) used in the
computation of the value of a partial joint policy V 0...t−1 (ϕt ) in Appendix A.3. Note that due
to the correspondence between types and AOHs, the size of a CBG B(b0 ,ϕt ) for a stage t is
exponential in t.
In a CBG, each agent uses a Bayesian game policy βi that maps individual types to actions:
βi (θi ) = ai . Because of the correspondence between types and AOHs, a (joint) policy for the
CBG β corresponds to a (joint) decision rule: β ↔ δ t . In the remainder of this article, we
assume deterministic past joint policies ϕt , which implies that only one ~θ t will have non-zero
probability given the observation history ~o t . Thus, β effectively maps observation histories
to actions. The number of such β for B(b0 ,ϕt ) is given by (2.5). The value of a joint CBG
policy β for a CBG B(b0 ,ϕt ) is:
X
b ~θ t ,β(~θ t )),
Pr(~θ t |b0 ,ϕt )Q(
(2.7)
Vb (β) =
~
θt

where β t (~θ t ) = hβi (θ~it )ii=1...n denotes the joint action that results from application of the
individual CBG-policies to the individual AOH θ~it specified by ~θ t .
Example 3. Consider a CBG for Dec-Tiger given the past joint policy ϕ2 that specifies to listen at the first two stages. At stage t = 2, each agent has four possible observation histories:
~ 2 = {(oHL ,oHL ), (oHL ,oHR ), (oHR ,oHL ), (oHR ,oHR )} that correspond directly to its possible types. The
O
i
probabilities of these joint types given ϕ2 are listed in Fig. 4a. Since the joint OHs together with
ϕ2 determine the joint AOHs, they also correspond to so-called joint beliefs: probability distributions
over states (introduced formally in Section 2.2). Fig. 4b shows these joint beliefs, which can serve as
the basis for the heuristic payoff function (as further discussed in Section 2.2).
460

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.261
0.047
0.047
0.016

(oHL ,oHR )
0.047
0.016
0.016
0.047

(oHR ,oHL )
0.047
0.016
0.016
0.047

(oHR ,oHR )
0.016
0.047
0.047
0.261

(a) The joint type probabilities.

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.999
0.970
0.970
0.5

(oHL ,oHR )
0.970
0.5
0.5
0.030

(oHR ,oHL )
0.970
0.5
0.5
0.030

(oHR ,oHR )
0.5
0.030
0.030
0.001

(b) The induced joint beliefs. Listed is the probability Pr(sl |~θ 2 ,b0 ) of
the tiger being behind the left door.

Figure 4: Illustration for the Dec-Tiger problem with a past joint policy ϕ2 that specifies
only listen actions for the first two stages.

Algorithm 5 Expand-CBG(q, H). The expand operator of GMAA* that makes use of CBGs.
Input: q = hϕt , v̂i the search node to expand.
b ~θ,a).
Input: H the admissible heuristic that is of the form Q(
Output: QExpand the set containing all expanded child nodes.
b
1: B(b0 ,ϕt ) ← ConstructBG(b0 ,ϕt , Q)
2: QExpand ← GenerateAllChildrenForCBG(B(b0 ,ϕt ))
3: return QExpand

{as explained in Section 2.1.2}

A solution to the CBG is a β that maximizes (2.7). A CBG is equivalent to a team
decision process and finding a solution is NP-complete (Tsitsiklis & Athans, 1985). However,
in the Bayesian game perspective of GMAA*, illustrated in Fig. 3b, the issue of solving a
CBG (i.e., finding the highest payoff β) is not so relevant because we need to expand all β.
That is, the Expand operator enumerates all β and appends them to ϕt to form the set of
extended joint policies

	
Φt+1 = (ϕt , β) | β is a joint CBG policy of B(b0 ,ϕt )
and uses this set to construct QExpand , the set of child nodes. The heuristic value of such a
child node q ∈ QExpand that specifies ϕt+1 = (ϕt , β) is given by
Vb (ϕt+1 ) = V 0...t−1 (ϕt ) + Vb (β).

(2.8)

The Expand operator that makes use of CBGs is summarized in Algorithm 5, which uses the
GenerateAllChildrenForCBG subroutine (Algorithm 13 in Appendix A.1). Fig. 3b illustrates
the Bayesian game perspective of GMAA*.
461

Oliehoek, Spaan, Amato, & Whiteson

2.2 Heuristics
To perform heuristic search, GMAA* defines the heuristic value Vb (ϕt ) using (2.3). In contrast, the Bayesian game perspective uses (2.8). These two formulations are equivalent when
b faithfully represents the expected immediate reward (Oliehoek, Spaan, & Vlasthe heuristic Q
sis, 2008). The consequence is that GMAA* via CBGs is complete (and thus finds optimal
solutions) as stated by the following theorem.
Theorem 1. When using a heuristic of the form
b ~θ t ,a) = Est [R(st ,a) | ~θ t ] + E~ t+1 [Vb (~θ t+1 ) | ~θ t , a],
Q(
θ

(2.9)

where Vb (~θ t+1 ) ≥ Qπ∗ (~θ t+1 , π ∗ (~θ t+1 )) is an overestimation of the value of an optimal joint
policy π ∗ , GMAA* via CBGs is complete.
Proof. See appendix.
In this theorem, Qπ (~θ t ,a) is the Q-value, i.e., the expected future cumulative reward of
performing a from ~θ t under joint policy π (Oliehoek, Spaan,P
& Vlassis, 2008). The expectation
t
~
of the immediate reward will also be written as R(θ ,a) = s∈S R(s,a) Pr(s|~θ t ,b0 ). It can be
computed using Pr(s|~θ t ,b0 ), a quantity we refer to as the joint belief resulting from ~θ t and
that we also denote as b. The joint belief itself can be computed via repeated application of
Bayes’ rule (Kaelbling, Littman, & Cassandra, 1998), or as the conditional of (A.2).
The rest of this subsection reviews several heuristics that have been used for GMAA*.
2.2.1 QMDP
b ~θ,a) is to solve the underlying MDP, i.e., to
One way to obtain an admissible heuristic Q(
assume the joint action is chosen by a single ‘puppeteer’ agent that can observe the true
state. This approach, known as QMDP (Littman, Cassandra, & Kaelbling, 1995), uses the
t
MDP value function Qt,∗
M (s ,a), which can be computed using standard dynamic programming
t
b ~t
techniques (Puterman, 1994). In order to transform the Qt,∗
M (s ,a)-values to QM (θ ,a)-values,
we compute:
X t,∗
b M (~θ t ,a) =
(2.10)
Q
Q (s,a) Pr(s|~θ t ,b0 ).
M

s∈S

Solving the underlying MDP has time complexity that is linear in h, which makes it,
especially compared to the Dec-POMDP, easy to compute. In addition, it is only necessary
to store a value for each (s,a)-pair, for each stage t. However, the bound it provides on the
optimal Dec-POMDP Q∗ -value function is loose (Oliehoek & Vlassis, 2007).
2.2.2 QPOMDP
Similar to the underlying MDP, one can define the underlying POMDP of a Dec-POMDP,
i.e., assuming the joint action is chosen by a single agent with access to the joint observation.7
7. Alternatively one can view this POMDP as a multiagent POMDP in which the agents can instantaneously
broadcast their private observations.

462

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Tree

Vector

t=0
t=1
t=2
t=3
Figure 5: Visual comparison of tree and vector-based Q representations.

The resulting solution can be used as a heuristic, called QPOMDP (Szer et al., 2005; Roth,
Simmons, & Veloso, 2005). The optimal QPOMDP value function satisfies:

Q∗P (bt , a) = R(bt ,a) +

X

P (ot+1 |bt ,a) max Q∗P (bt+1 ,at+1 ),
at+1

ot+1 ∈O

(2.11)

P
where bt is the joint belief, R(bt ,a) = s∈S R(s,a)bt (s) is the immediate reward, and bt+1 is
the joint belief resulting from bt by action a and joint observation ot+1 . To use QPOMDP , for
b P (~θ t ,a) , Qt (b~θt ,a).
each ~θ t , we can directly use the value for the induced joint belief: Q
P

There are two approaches to computing QPOMDP . One is to construct the ‘belief MDP
tree’ of all joint beliefs, illustrated in Fig. 5 (left). Starting with b0 (corresponding to the
empty joint AOH ~θ 0 ), for each a and o we compute the resulting ~θ 1 and corresponding
~1
belief bθ and continue recursively. Given this tree, it is possible to compute values for all the
nodes by standard dynamic programming.
Another possibility is to apply vector-based POMDP techniques (see Fig. 5 (right)). The
Q-value function for a stage QtP (b,a) can be represented using a set of vectors for each joint
t } (Kaelbling et al., 1998). Qt (b,a) is then defined as the maximum
action V t = {V1t , . . . ,V|A|
P
inner product:
QtP (b,a) , max b · vat .
t ∈V t
va
a

Given V h−1 , the vector representation of the last stage, we can compute V h−2 , etc. In order
to limit the growth of the number of vectors, dominated vectors can be pruned.
Since QMDP is an upper bound on the POMDP value function (Hauskrecht, 2000), QPOMDP
provides a tighter upper bound to Q∗ than QMDP . However, it is also more costly to compute
and store: both the tree-based and the vector-based approach may need to store a number of
values exponential in h.
463

Oliehoek, Spaan, Amato, & Whiteson

2.2.3 QBG
A third heuristic, called QBG , assumes that each agent in the team has access only to its
individual observation but it can communicate with a 1-step delay.8 We define QBG as
X
Q∗B (~θ t ,a) = R(~θ t ,a) + max
Pr(ot+1 |~θ t ,a)Q∗B (~θ t+1 ,β(ot+1 )),
(2.12)
β

ot+1 ∈O

t+1
where β = hβ1 (ot+1
1 ),...,βn (on )i is a tuple of individual policies βi : Oi → Ai for the CBG
t
constructed for ~θ ,a. Like QPOMDP , QBG can also be represented using vectors (Varaiya &
Walrand, 1978; Hsu & Marcus, 1982; Oliehoek, Spaan, & Vlassis, 2008) and the same two
manners of computation (tree and vector based) apply. It yields a tighter heuristic than
QPOMDP , but its computation has an additional exponential dependence on the maximum
number of individual observations (Oliehoek, Spaan, & Vlassis, 2008), which is particularly
troubling for the vector-based computation, since it precludes effective application of incremental pruning (A. Cassandra, Littman, & Zhang, 1997). To overcome this problem, Oliehoek
and Spaan (2012) introduce novel tree-based pruning methods.

3. Clustering
GMAA* solves Dec-POMDPs by repeatedly constructing CBGs and expanding all the joint
BG policies β for them. However, the number of such β is equal to the number of regular
MAA* child nodes given by (2.5) and thus grows doubly exponentially with the horizon h.
In this section, we propose a new approach for improving scalability with respect to h by
clustering individual AOHs. This reduces the number of β and therefore the number of
constructed child nodes in the GMAA* search tree.9
Previous research has also investigated such clustering: Emery-Montemerlo, Gordon,
Schneider, and Thrun (2005) propose clustering types based on the profiles of the payoff
functions of the CBGs. However, the resulting method is ad hoc. Even given bounds on the
error of clustering two types in a CBG, no guarantees can be made about the quality of the
Dec-POMDP solution, as the bound is with respect to a heuristic payoff function.
In contrast, we propose to cluster histories based on the probability these histories induce
over histories of the other agents and over states. The critical advantage of this criterion,
which we call probabilistic equivalence (PE), is that the resulting clustering is lossless: the
solution for the clustered CBG can be used to construct the solution for the original CBG and
the values of the two CBGs are identical. Thus, the criterion allows for clustering of AOHs
in CBGs that represent Dec-POMDPs while preserving optimality.10
In Section 3.1, we describe how histories in Dec-POMDPs can be clustered using the
notions of probabilistic and best-response equivalence. This allows histories to be clustered
8. The name QBG stems from the fact that such a 1-step delayed communication scenario can be modeled
as a CBG. Note, however, that the CBGs used to compute QBG are of a different form than the B(b0 ,ϕt )
discussed in Section 2.1.2: in the latter, types correspond to length-t (action-) observation histories; in the
former, types correspond to length-1 observation histories.
9. While CBGs are not essential for clustering, they provide a convenient level of abstraction that simplifies
exposition of our techniques. Moreover, this level of abstraction makes it possible to employ our results
concerning CBGs outside the context of Dec-POMDPs.
10. The probabilistic equivalence criterion and lossless clustering were introduced by Oliehoek et al. (2009).
This article presents a new, simpler proof of the optimality of clustering based on PE.

464

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

when it is rational to always choose the same action. In Section 3.2, we describe the application
of these results to GMAA*. Section 3.3 introduces improved heuristic representations that
allow for the computation over longer horizons.
3.1 Lossless Clustering in Dec-POMDPs
In this section, we discuss lossless clustering based on the notion of probabilistic equivalence.
We show that this clustering is lossless by demonstrating that probabilistic equivalence implies
best response equivalence, which describes the conditions that a rational agent will select the
same action for two of its types. To prove this implication, we show that the best response
depends only on the multiagent belief (i.e., the probability distribution over states and policies
of the other agents), which is the same for two probabilistically equivalent histories. Relations
to other equivalence notions are discussed in Section 6.
3.1.1 Probabilistic Equivalence Criterion
We first introduce the probabilistic equivalence criterion, which can be used to decide whether
two individual histories θ~ia ,θ~ib can be clustered without loss in value.
Criterion 1 (Probabilistic Equivalence). Two AOHs θ~ia ,θ~ib for agent i are probabilistically
equivalent (PE), written P E(θ~ia ,θ~ib ), when the following holds:
∀~θ6=i ∀s

Pr(s,~
θ 6=i |θ~ia ) = Pr(s,~
θ 6=i |θ~ib ).

(3.1)

These probabilities can be computed as the conditional of Pr(s,~θ t |b0 ,ϕt ), defined by (A.2).
In subsections 3.1.2–3.1.4, we formally prove that PE is a sufficient criterion to guarantee
that clustering is lossless. In the remainder of Section 3.1.1 we discuss some key properties of
the PE criterion in order to build intuition.
Note that the criterion can be decomposed into the following two criteria:
∀~θ6=i
∀~θ6=i ∀s

Pr(~
θ 6=i |θ~ia ) = Pr(~
θ 6=i |θ~ib ),

(3.2)

Pr(s|~
θ 6=i ,θ~ia ) = Pr(s|~
θ 6=i ,θ~ib ).

(3.3)

These criteria give a natural interpretation: the first says that the probability distribution
over the other agents’ AOHs must be identical for both θ~ia and θ~ib . The second demands that
the resulting joint beliefs are identical.
The above probabilities are not well defined without the initial state distribution b0 and
past joint policy ϕt . However, since we consider clustering of histories within a particular CBG
(for some stage t) constructed for a particular b0 ,ϕt , they are implicitly specified. Therefore
we drop these arguments, clarifying the notation.
Example 4. In Example 3, the types (oHL ,oHR ) and (oHR ,oHL ) of each agent are PE. To see this, note
that the rows (columns for the second agent) for these histories are identical in both Fig. 4a and
Fig. 4b. Thus, they specify the same distribution over histories of the other agents (cf. equation (3.2))
and the induced joint beliefs are the same (cf. equation (3.3)).

Probabilistic equivalence has a convenient property that our algorithms exploit: if it holds
for a particular pair of histories, then it will also hold for all identical extensions of those
histories, i.e., it propagates forwards regardless of the policies of the other agents.
465

Oliehoek, Spaan, Amato, & Whiteson

Definition 7 (Identical extensions). Given two AOHs θ~ia,t ,θ~ib,t , their respective extensions
θ~ a,t+1 = (θ~ a,t ,ai ,oi ) and θ~ b,t+1 = (θ~ b,t ,a′ ,o′ ) are called identical extensions if and only if
i

i

i

ai = a′i and oi = o′i .

i

i

i

Lemma 1 (Propagation of PE). Given θ~ia,t ,θ~ib,t that are PE, regardless of the decision rule
the other agents use (δ t6=i ), identical extensions are also PE:
∀ati ∀ot+1 ∀δt ∀st+1 ∀~θt+1
i

6=i

6=i

t+1
t
t+1 ~ t+1 ~ b,t t t+1 t
,θ 6=i |θi ,ai ,oi ,δ 6=i ) (3.4)
Pr(st+1 ,~
θ 6=i |θ~ia,t ,ati ,ot+1
i ,δ 6=i ) = Pr(s

Proof. The proof is listed in the appendix, but holds intuitively because if the probabilities
described above were the same before, they will also be the same after taking the same action
and seeing the same observation.
Note that, while the probabilities defined in (3.1) superficially resemble beliefs used in
POMDPs, they are substantially different. In a POMDP, the single agent can compute its
individual belief using only its AOH. It can then use this belief to determine the value of
any future policy, as it is a sufficient statistic of the history to predict the future rewards
(Kaelbling et al., 1998; Bertsekas, 2005). Thus, it is trivial to show equivalence of AOHs
that induce the same individual belief in a POMDP. Unfortunately, Dec-POMDPs are more
problematic. The next section elaborates on this issue by discussing the relation to multiagent
beliefs.
3.1.2 Sub-Tree Policies, Multiagent Beliefs and Expected Future Value
To describe the relationship between multiagent beliefs and probabilistic equivalence, we
must first discuss the policies an agent may follow and their resulting values. We begin
by introducing the concept of sub-tree policies. As illustrated in Fig. 2 (on page 456), a
(deterministic) policy πi can be represented as a tree with nodes labeled using actions and
edges labeled using observations: the root node corresponds to the first action taken, other
nodes specify the action for the observation history encoded by the path from the root node.
As such, it is possible to define sub-tree policies, γi , which correspond to sub-trees of agent
i’s policy πi (also illustrated in Fig. 2). In particular, we write
w
πi ~o t = γiτ =h−t
(3.5)
i

for the sub-tree policy of πi corresponding to w
observation history ~oit that specifies the actions
for the last τ = h − t stages. We refer to  as the policy consumption operator, since it
w
‘consumes’ the part of the policy corresponding to ~oit . Similarly we write γiτ =k ~o l = γiτ =k−l
i
(note that in (3.5), πi is just a τ = h-steps-to-go sub-tree policy) and use similar notation,
γ τ =k , for joint sub-tree policies. For a more extensive treatment of these different forms of
policy, we refer to the discussion by Oliehoek (2012).
Given these concepts, we can define the value of a τ = k-stages-to-go joint policy starting
from state s:
XX
w
Pr(s′ ,o|s,a)V (s′ , γ τ =k o ).
(3.6)
V (s,γ τ =k ) = R(s,a) +
s′

o

Here, a is the joint action specified by the roots of the individual sub-tree policies specified
by γ τ =k for stage t = h − k.
466

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

From this definition, it follows directly that the probability distribution over states s and
sub-tree policies over other agents γ 6=i is sufficient to predict the value of a sub-tree policy γi .
In fact, such a distribution is known as a multiagent belief bi (s,γ 6=i ) (Hansen, Bernstein, &
Zilberstein, 2004). Its value is given by
XX
V (bi ) = max
bi (s,γ 6=i )V (s,hγi , γ 6=i i),
(3.7)
γi

s

γ 6=i

and we refer to the maximizing γi as agent i’s best response for bi . This illustrates that a
multiagent belief is a sufficient statistic: it contains sufficient information to predict the value
of any sub-tree policy γi .
It is possible to connect action observation histories to multiagent beliefs by fixing the
policies of the other agents. Given that the other agents will act according to a profile of
policies π 6=i , agent i has a multiagent belief at the first stage of the Dec-POMDP: bi (s,π 6=i ) =
b0 (s). Moreover, agent i can maintain such a multiagent belief during execution. As such,
given π 6=i , each history θ~i induces a multiagent belief, which we will write as bi (s,γ 6=i |θ~i , π 6=i )
to make the dependence on θ~i , π 6=i explicit. The multiagent belief for a history is defined as
bi (s,γ 6=i |θ~i , π 6=i ) , Pr(s,γ 6=i |θ~i , b0 , π 6=i ),

(3.8)

and induces a best response via (3.7):
BR(θ~i |π 6=i ) , arg max
γi

XX
s

bi (s,γ 6=i |θ~i , π 6=i )V (s,γ 6=i ,γi ).

(3.9)

γ 6=i

From this we can conclude that two AOHs θ~ia ,θ~ib can be clustered together if they induce the
same multiagent belief.
However, this notion of multiagent belief is clearly quite different from the distributions
used in our notion of PE. In particular, to establish whether two AOHs induce the same
multiagent belief, we need a full specification of π 6=i . Nevertheless, we show that two AOHs
that are PE are also best response equivalent and that we can therefore cluster them. The
crux is that we can show that, if Criterion 1 is satisfied, the AOHs will always induce the
same multiagent beliefs for any π 6=i (consistent with the current past joint policy ϕ6=i ).
3.1.3 Best-Response Equivalence Allows Lossless Clustering of Histories
We can now relate probabilistic equivalence and the multiagent belief as follows.
Lemma 2 (PE implies multiagent belief equivalence). For any π 6=i , probabilistic equivalence
implies multiagent belief equivalence:


P E(θ~ia ,θ~ib ) ⇒ ∀s,γ6=i bi (s,γ 6=i |θ~ia , π 6=i ) = bi (s,γ 6=i |θ~ib , π 6=i )
(3.10)
Proof. See appendix.
This lemma shows that if two AOHs are PE, they produce the same multiagent belief.
Intuitively, this gives us a justification to cluster such AOHs together: since a multiagent
belief is a sufficient statistic we should act the same when we have the same multiagent belief,
but since Lemma 2 shows that θ~ia ,θ~ib induces the same multiagent beliefs for any π 6=i when
they are PE, we can conclude that we will always act the same in those histories. Formally,
we prove that θ~ia ,θ~ib are best-response equivalent if they are PE.
467

Oliehoek, Spaan, Amato, & Whiteson

Theorem 2 (PE implies best-response equivalence). Probabilistic equivalence implies bestresponse equivalence. That is


P E(θ~ia ,θ~ib ) ⇒ ∀π6=i BR(θ~ia |π 6=i ) = BR(θ~ib |π 6=i )
Proof. Assume any arbitrary π 6=i , then
BR(θ~ia |π 6=i ) = arg max

XX

bi (s,γ 6=i |θ~ia )V (s,γ 6=i ,γi )

= arg max

XX

bi (s,γ 6=i |θ~ib )V (s,γ 6=i ,γi ) = BR(θ~ib |π 6=i ),

γi

γi

s

s

γ 6=i

γ 6=i

where Lemma 2 is employed to assert the equality of bi (·|θ~ia ) and bi (·|θ~ib ).
This theorem is key because it demonstrates that when two AOHs θ~ia ,θ~ib of an agent are
PE, then that agent need not discriminate between them now or in the future. Thus, when
searching the space of joint policies, we can restrict our search to those that assign the same
sub-tree policy γi to θ~ia and θ~ib . As such, it directly provides intuition as to why lossless
clustering is possible. Formally, we define the clustered joint policy space as follows.
Definition 8 (Clustered joint policy space). Let ΠC ⊆ Π be the subset of joint policies that
is clustered: i.e., each πi that is part of a π ∈ ΠC assigns the same sub-tree policy to action
observation histories that are probabilistically equivalent.
Corollary 1 (Existence of an optimal clustered joint policy). There exists an optimal joint
policy in the clustered joint policy space:
max V (π) = max V (π)

π∈ΠC

π∈Π

(3.11)

Proof. It is clear that the left hand side of (3.11) is upper bounded by the right hand side,
since ΠC ⊆ Π. Now suppose that π ∗ = arg maxπ∈Π V (π) has strictly higher value than the
best clustered joint policy. For at least one agent i and one pair of PE histories θ~ia , θ~ib , π ∗ must
assign different sub-tree policies γia 6= γib (otherwise π ∗ would be clustered). Without loss of
generality we assume that there is only one such pair. It follows directly from Theorem 2 that
from this policy we can construct a clustered policy π C ∈ ΠC (by assigning either γia or γib
to both θ~ia , θ~ib ) that is guaranteed to have value no less than π ∗ , thereby contradicting the
assumption that π ∗ has strictly higher value than the best clustered joint policy.
This formally proves that we can restrict our search to ΠC , the space of clustered joint
policies, without sacrificing optimality.
3.1.4 Clustering with Commitment in CBGs
Though it is now clear that two AOHs that are PE can be clustered, making this result
operational requires an additional step. To this end, we use the abstraction layer provided
by Bayesian games. Recall that in the CBG for a stage, the AOHs correspond to types.
468

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Therefore, we want to cluster these types in the CBG. To accomplish the clustering of two
types θia ,θib , we introduce a new type θic to replace them, by defining:
∀θ6=i Pr(θic ,θ 6=i ) , Pr(θia ,θ 6=i ) + Pr(θib ,θ 6=i )
∀j ∀a

u(hθic ,θ 6=i i ,a)




Pr(θia ,θ 6=i )u(hθia ,θ 6=i i ,a) + Pr(θib ,θ 6=i )u( θib ,θ 6=i ,a)
.
,
Pr(θia ,θ 6=i ) + Pr(θib ,θ 6=i )

(3.12)
(3.13)

Theorem 3 (Reduction through commitment). Given that agent i in collaborative Bayesian
game B is committed to selecting a policy that assigns the same action for two of its types
θia ,θib , i.e., to selecting a policy βi such that βi (θia ) = βi (θib ), the CBG can be reduced without
loss in value for any agents. That is, the result is a new CBG B ′ in which agent i employs a
policy βi′ that reflects the clustering and whose expected payoff is the same as in the original
′
CBG: V B (βi′ ,β 6=i ) = V B (βi ,β 6=i ).
Proof. See appendix.
This theorem shows that, given that agent i is committed to taking the same action for
its types θia ,θib , we can reduce the collaborative Bayesian game B to a smaller one B ′ and
′
translate the joint CBG-policy β ′ found
 for B back to a joint CBG-policy β in B. This does
not necessarily mean that β = βi ,β 6=i is also a solution for B, because the best-response of
agent i against β 6=i may not select the same action for θia ,θib . Rather βi is the best-response
against β 6=i given that the same action needs to be taken for θia ,θib .11
Even though Theorem 3 only gives a conditional statement that depends on an agent being
committed to select the same action for two of its types, the previous subsection discussed
when a rational agent can make such a commitment. Combining these results gives the
following corollary.
Corollary 2 (Lossless Clustering with PE). Probabilistically equivalent histories θ~ia ,θ~ib can
be clustered without loss in heuristic value by merging them into a single type in a CBG.
Proof. Theorem 3 shows that, given that an agent i is committed to take the same action
for two of its types, those types can be clustered without loss in value. Since θ~ia ,θ~ib are PE,
they are best-response equivalent, which means that the agent is committed to use the same
sub-tree policy γi and hence the same action ai . Therefore we can directly apply clustering
without loss in expected payoff, which in a CBG for a stage of a Dec-POMDP means no loss
in expected heuristic value as given by (2.7).
Intuitively, the maximizing action is the same for θ~ia and θ~ib regardless of what (future)
joint policies π 6=i the other agents will use and hence we can cluster them without loss in
heuristic value. Note that this does not depend on which heuristic is used and hence also
holds for an optimal heuristic (i.e., when using an optimal Q-value function that gives the
true value). This directly relates probabilistic equivalence with equivalence in optimal value.12
11. Although we focus on CBGs, these results generalize to BGs with individual payoff functions. Thus, they
could potentially be exploited by algorithms for general-payoff BGs. Developing methods that do so is an
interesting avenue for future work.
12. The proof originally provided by Oliehoek et al. (2009) is based on showing that histories that are PE will
induce identical Q-values.

469

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 6 ClusterCBG(B)
Input: CBG B
Output: Losslessly clustered CBG B
1: for each agent i do
2:
for each individual type θi ∈ B.Θi do
3:
if Pr(θi ) = 0 then
4:
B.Θi ← B.Θi \θi
5:
continue
6:
end if
7:
for each individual type θi′ ∈ B.Θi do
8:
isProbabilisticallyEquivalent ← true
9:
for all hs,θ 6=i i do
10:
if Pr(s,θ 6=i |θi ) 6= Pr(s,θ 6=i |θi′ ) then
11:
isProbabilisticallyEquivalent ← false
12:
break
13:
end if
14:
end for
15:
if isProbabilisticallyEquivalent then
16:
B.Θi ← B.Θi \θi′
17:
for each a ∈ A do
18:
for all θ 6=i do
19:
u(θi ,θ 6=i ,a) ← min(u(θi ,θ 6=i ,a),u(θi′ ,θ 6=i ,a))
20:
Pr(θi ,θ 6=i ) ← Pr(θi ,θ 6=i ) + Pr(θi′ ,θ 6=i )
21:
Pr(θi′ ,θ 6=i ) ← 0
22:
end for
23:
end for
24:
end if
25:
end for
26:
end for
27: end for
28: return B

{Prune θi from B:}

{Prune θi′ from B:}
{ take the lowest upper bound }

Note that this result establishes a sufficient, but not necessary condition for lossless clustering.
In particular, given policies for the other agents, many types are best-response equivalent and
can be clustered. However, as far as we know, the criterion must hold in order to guarantee
that two histories have the same best-response against any policy of the other agents.
3.2 GMAA* with Incremental Clustering
Knowing which individual histories can be clustered together without loss of value has the
potential to speed up many Dec-POMDP methods. In this article, we focus on its application
within the GMAA* framework.
Emery-Montemerlo et al. (2005) showed how clustering can be incorporated at every stage
in their algorithm: when the CBG for a stage t is constructed, a clustering of the individual
histories (types) is performed first and only afterwards is the (reduced) CBG solved. The same
approach can be employed within GMAA* by modifying the Expand procedure (Algorithm 5)
to cluster the CBG before calling GenerateAllChildrenForCBG.
Algorithm 6 shows the clustering algorithm. It takes as input a CBG and returns the
clustered CBG. It performs clustering by performing pairwise comparison of all types of each
470

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

b
Algorithm 7 ConstructExtendedBG(B,β t−1 , Q)

Input: A CBG B for stage t − 1, and the joint BG policy followed β t−1 .
b ~θ,a).
Input: An admissible heuristic of the form Q(
′
Output: CBG B for stage t.
1: B ′ ← B
{make a copy of B that we subsequently alter}
2: for each agent i do
3:
B ′ .Θi = ConstructExtendedTypeSet(i)
{overwrite the individual type sets}
4: end for
5: B ′ .Θ ← ×i∈D Θi
{the new joint type set (does not have to be explicitly stored)}
6: for each joint type θ = (θ t−1 ,at−1 ,ot ) ∈ B ′ .Θ do
7:
for each state st ∈ S do
8:
Compute Pr(st |θ)
{from Pr(st−1 |θ t−1 ) via Bayes’ rule }
9:
end for
10:
Pr(θ) ← Pr(ot |θ t−1 ,at−1 ) Pr(θ t−1 )
11:
for each a ∈ A do
12:
q←∞
13:
for each history ~θ t represented by θ do
b ~θ t ,a))
b we can take the lowest upper bound }
14:
q ← min(q,Q(
{ if Q∗ ≤ Q
15:
end for
16:
B ′ .u(θ,a) ← q
17:
end for
18: end for
19: return B ′

agent to see if they satisfy the criterion, yielding O(|Θi |2 ) comparisons for each agent i. Each
comparison involves looping over all hs,θ 6=i i (line 9). If there are many states, some efficiency
could be gained by first checking (3.2) and then checking (3.3). Rather than taking the
average as in (3.13), on line 19 we take the lowest payoff, which can be done if we are using
upper bound heuristic values.
The following theorem demonstrates that, when incorporating clustering into GMAA*,
the resulting algorithm is still guaranteed to find an optimal solution.
Theorem 4. When using a heuristic of the form (2.9) and clustering the CBGs in GMAA*
using the PE criterion, the resulting search method is complete.
Proof. Applying clustering does not alter the computation of lower bound values. Also,
heuristic values computed for the expanded nodes are admissible and in fact unaltered as
guaranteed by Corollary 2. Therefore, the only difference with regular GMAA* is that the
class of considered joint policies is restricted to ΠC , the class of clustered joint policies: not
all possible child nodes are expanded, because clustering effectively prunes away policies that
would specify different actions for AOHs that are PE and thus clustered. However, Corollary 1
guarantees that there exists an optimal joint policy in this restricted class.
The modification of the Expand proposed above is rather naive. To construct B(b0 ,ϕt )
it must first construct all |Oi |t possible AOHs for agent i (given the past policy ϕti ). The
subsequent clustering involves pairwise comparison of all these exponentially many types.
Clearly, this is not tractable for later stages.
However, because PE of AOHs propagates forwards (i.e., identical extensions of PE histories are also PE), a more efficient approach is possible. Instead of clustering this exponentially
471

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 8 Expand-IC(q, H). The expand operator for GMAA*-IC.
Input: q = hϕt , v̂i the search node to expand.
b ~θ,a).
Input: H the admissible heuristic that is of the form Q(
Output: QExpand the set containing expanded child nodes.
1: B(ϕt−1 ) ← ϕt−1 .CBG
{retrieve previous CBG, note ϕt = (ϕt−1 , β t−1 )}
t−1 b
t
t−1
2: B(ϕ ) ← ConstructExtendedBG(B(ϕ
),β , Q)
3: B(ϕt ) ← ClusterBG(B(ϕt ))
4: ϕt .CBG ← B(ϕt )
{store pointer to this CBG}
5: QExpand ← GenerateAllChildrenForCBG(B(ϕt ))
6: return QExpand

growing set of types, we can simply extend the already clustered types of the previous stage’s
CBG, as shown in Algorithm 7. That is, given Θi , the set of types of agent i at the previous
stage t − 1, and βit−1 the policy agent i took at that stage, the set of types at stage t, Θ′i , can
be constructed as

	
Θ′i = θi′ = (θi ,βit−1 (θi ),oti ) | θi ∈ Θi ,oti ∈ Oi .
(3.14)
This means that the size of this newly constructed set is |Θ′i | = |Θi | · |Oi | . If the type set Θi at
the previous stage t − 1 was much smaller than the set of all histories |Θi | ≪ |Oi |t−1 , then the
new type set Θ′i is also much smaller: |Θ′i | ≪ |Oi |t . In this way, we bootstrap the clustering
at each stage and spend significantly less time clustering. We refer to the algorithm that
implements this type of clustering as GMAA* with Incremental Clustering (GMAA*-IC).
This approach is possible only because we perform an exact, value-preserving clustering for
which Lemma 1 guarantees that identical extensions will also be clustered without loss in
value. When performing the same procedure in a lossy clustering scheme (e.g., as in EmeryMontemerlo et al., 2005), errors might accumulate, and a better option might be to re-cluster
from scratch at every stage.
Expansion of a GMAA*-IC node takes exponential time with respect to both the number
of agents and types, as there are O(|A∗ |n|Θ∗ | ) joint CBG-policies and thus child nodes in the
GMAA*-IC search tree (A∗ is the largest action set and Θ∗ is the largest type set). Clustering
involves a pairwise comparison of all types of each agent and each of these comparisons needs
to check O(|Θ∗ |n−1 |S|) numbers for equality to verify (3.1). The total cost of clustering can
therefore be written as
O(n |Θ∗ |2 |Θ∗ |n−1 |S|),
which is only polynomial in the number of types. When clustering decreases the number of
types |Θ∗ |, it can therefore significantly reduce the number of child nodes and thereby the
overall time needed. However, when no clustering is possible, some overhead will be incurred.
3.3 Improved Heuristic Representation
Since clustering can reduce the number of types, GMAA*-IC has the potential to scale to
larger horizons. However, doing so has important consequences for the computation of the
heuristics. Previous research has shown that the upper bound provided by QMDP is often
too loose for effective heuristic search (Oliehoek, Spaan, & Vlassis, 2008). However, the
space needed to store tighter heuristics such as QPOMDP or QBG grows exponentially with the
horizon. Recall from Section 2.2.2 (see Fig. 5) that there are two approaches to computing
472

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

b with minimum size.
Algorithm 9 Compute Hybrid Q
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Qh−1 ← {R1 , . . . ,R|A| }
z ← |A| × |S|
for t = h − 2 to 0 do
~ t | × |A|
y ← |Θ
if z < y then
V ← VectorBackup(Qt+1 )
V ′ ← Prune(V)
Qt ← V ′
z ← |V ′ | × |S|
end if
if z ≥ y then
Qt ← TreeBackup(Qt+1 )
end if
end for

{vector representation of last stage}
{the size of the |A| vectors}
{size of AOH representation}

{From now on z ≥ y}

QPOMDP or QBG . The first constructs a tree of all joint AOHs and their heuristic values, which
is simple to implement but requires storing a value for each (~θ t , a)-pair, the number of which
grows exponentially with t. The second approach maintains a vector-based representation, as
is common for POMDPs. Though pruning can provide leverage, in the worst case, no pruning
is possible and the number of maintained vectors grows doubly exponentially with h − t, the
number of stages-to-go. Similarly, the initial belief and subsequently reachable beliefs can be
used to reduce the number of vectors retained at each stage, but as the number of reachable
beliefs is exponential in the horizon the exponential complexity remains.
Oliehoek, Spaan, and Vlassis (2008) used a tree-based representation for the QPOMDP and QBG heuristics. Since the
computational cost of solving the Dec-POMDP was the bottleneck, the inefficiencies in the representation could be overlooked. However, this approach is no longer feasible for the
longer horizons made possible by GMAA*-IC.

Hybrid

t=0

To mitigate this problem, we propose a hybrid represent=1
tation for the heuristics, as illustrated in Fig. 6. The main
insight is that the exponential growth of the two existing representations occurs in opposite directions. Therefore, we can
t=2
use the low space-complexity side of both representations: the
later stages, which have fewer vectors, use a vector-based representation, while the earlier stages, which have fewer histot=3
ries, use a history-based representation. This is similar to
the idea of utilizing reachable beliefs to reduce the size of the Figure 6: An illustration of
vector representation described above but, rather than stor- the hybrid representation.
ing vectors for the appropriate AOHs at each step, only the
values are needed when using the tree-based representation.
Algorithm 9 shows how, under mild assumptions, a minimally-sized representation can be
computed. Starting from the last stage, the algorithm performs vector backups, switching to
tree backups when they become the smaller option. For the last time step h − 1, we represent
473

Oliehoek, Spaan, Amato, & Whiteson

Qt by the set of immediate reward vectors13 , and variable z (initialized on line 2) keeps track
of the number of parameters needed to represent Qt as vectors for the time step at hand.
Note that z depends on how effective the vector pruning is, i.e., how large the parsimonious
representation of the piecewise linear and convex value function is. Since this is problem
dependent, z can be updated only after pruning has actually been performed (line 9). By
contrast y, the number of parameters in a tree representation, can be computed directly from
the Dec-POMDP (line 4). When z > y, the algorithm switches to tree backups.14

4. Incremental Expansion
The clustering technique presented in the previous section has the potential to significantly
speed up planning if much clustering is possible. However, if little clustering is possible, the
number of children in the GMAA* search tree will still grow super-exponentially. This section
presents incremental expansion, a complementary technique to deal with this problem.
Incremental expansion exploits recent improvements in effectively solving CBGs. First
note that during the expansion of the last stage t = h − 1 for a particular ϕh−1 , we are only
interested in the best child (ϕh−1 ,δ h−1,∗ ), which corresponds to the optimal solution of the
Bayesian game δ h−1,∗ ↔ β ∗ . As such, for this last stage, we can use new methods for solving
CBGs (Kumar & Zilberstein, 2010b; Oliehoek, Spaan, Dibangoye, & Amato, 2010) that can
provide speedups of multiple orders of magnitude over brute force search (enumeration).15
Unfortunately, the improvements to GMAA* afforded by this approach are limited: in order
to guarantee optimality, it still relies on expansion of all (child nodes corresponding to all)
joint CBG-policies β for the intermediate stages, thus necessitating a brute-force approach.
However, many of the expanded child nodes may have low heuristic values Vb and may therefore
never be selected for further expansion.
Incremental expansion overcomes this problem because it exploits the following key observation: if we can generate the children in decreasing heuristic order using an admissible
heuristic, we do not have to expand all the children. As before, an A* search is performed
over partially specified policies and each new CBG is constructed by extending the CBG for
the parent node. However, rather than fully expanding (i.e., enumerating all the CBG policies
of and thereby constructing all children for) each search node, we instantiate an incremental
CBG solver for the corresponding CBG. This incremental solver returns only one joint CBG
policy at a time, which is then used to construct a single child ϕt+1 = (ϕt , β). By revisiting
the nodes, only the promising child nodes are expanded incrementally.
Below, we describe GMAA*-ICE, an algorithm that combines GMAA*-IC with incremental expansion. We establish theoretical guarantees and describe the modifications to
BaGaBaB, the CBG solver that GMAA*-ICE employs, that are necessary to deliver the
child nodes in decreasing order.
13. Only in exceptional cases where a short horizon is combined with large state and action spaces will representing the last time step as vectors not be minimal. In such cases, the algorithm can be trivially adapted.
14. This assumes that the vector representation will not shrink again for earlier stages. Although unlikely in
practice, such cases would prevent the algorithm from computing a minimal representation.
15. Kumar and Zilberstein (2010b) tackle a slightly different problem; they introduce a weighted constraint satisfaction approach to solving the point-based backup in dynamic programming for Dec-POMDPs. However,
this point-based backup can be interpreted as a collection of CBGs (Oliehoek et al., 2010).

474

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

4.1 GMAA* with Incremental Clustering and Expansion
We begin by formalizing incremental expansion and incorporating it into GMAA*-IC, yielding GMAA* with incremental clustering and expansion (GMAA*-ICE). At the core of
incremental expansion lies the following lemma:
Lemma 3. Given two joint CBG policies β, β ′ for a CBG B(b0 ,ϕt ), if Vb (β) ≥ Vb (β ′ ), then
for the corresponding child nodes Vb (ϕt+1 ) ≥ Vb (ϕt+1′ ).
Proof. This holds directly by the definition of Vb (ϕt ) as given by (2.8):
Vb (ϕt+1 ) = V 0...(t−1) (ϕt ) + Vb (β)
≥ V 0...(t−1) (ϕt ) + Vb (β ′ ) = Vb (ϕt+1′ ).

It follows directly that, if for B(b0 ,ϕt ) we use a CBG solver that can generate a sequence
of policies β, β ′ , . . . such that
Vb (β) ≥ Vb (β ′ ) ≥ . . .

then, for the sequence of corresponding children

Vb (ϕt+1 ) ≥ Vb (ϕt+1′ ) ≥ . . . .

Exploiting this knowledge, we can expand only the first child ϕt+1 and compute its heuristic
value Vb (ϕt+1 ) using (2.8). Since all the unexpanded siblings will have heuristic values less
than or equal to that, we can modify GMAA*-IC to reinsert the node q into the open list L
to act as a placeholder for all its non-expanded children.
Definition 9. A placeholder is a node for which at least one child has been expanded. A
placeholder has a heuristic value equal to its last expanded child.
Thus, after expansion of a search node q’s child, we update q.v̂, the heuristic value of the
node, to Vb (ϕt+1 ), the value of the expanded child, i.e., we set q.v̂ ← Vb (ϕt+1 ). As such, we
can reinsert q into L as a placeholder. As mentioned above, this is correct because all the
unexpanded siblings (for which the parent node q now is a placeholder) have heuristic values
lower than or equal to Vb (ϕt+1 ). Therefore the next sibling q ′ represented by the placeholder
is always expanded in time: q ′ is always created before nodes with lower heuristic value are
selected for further expansion. We keep track of whether a node is a previously expanded
placeholder or not.
As before, GMAA*-ICE performs an A* search over partially specified policies. As in
GMAA*-IC, each new CBG is constructed by extending the CBG for the parent node and
then applying lossless clustering. However, rather than expanding all children, GMAA*-ICE
requests only the next solution β of an incremental CBG solver, from which a single child
ϕt+1 = (ϕt , β) is constructed. In principle GMAA*-ICE can use any CBG solver that is able
to incrementally deliver all β in descending order of Vb (β). We propose a modification of the
BaGaBaB algorithm (Oliehoek et al., 2010), briefly discussed in Section 4.3.
Fig. 7 illustrates the process of incremental expansion in GMAA*-ICE, with ϕt indexed
by letters. First, a CBG solver for the root node ha, 7i is created, and the optimal solution β ∗ is
computed, with value 6. This results in a child hb, 6i, and the root is replaced by a placeholder
node ha, 6i. As per Definition 5 (the node comparison operator), b appears before a in the
475

Oliehoek, Spaan, Amato, & Whiteson

Legend:
ϕt
v̂

t

a
7

a
6

Root node

a
6

β′

β∗
b
6

t+1

b
4
β∗

New B(a), Vb =6
c
4

t+2
hϕt , v̂i
in open list

ha, 7i

a
5.5

New B(b), Vb =4

ha, 6i
hc, 4i
hb, 4i

hb, 6i
ha, 6i

b
4

c
4

d
5.5
Next solution of
B(a), Vb =5.5

hd, 5.5i
ha, 5.5i
hc, 4i
hb, 4i

Figure 7: Illustration of incremental expansion, with the nodes in the open list at the bottom.
Past joint policies ϕt are indexed by letters. Placeholder nodes are indicated by dashes.
open list and hence is selected for expansion. Its best child hc, 4i is added and hb, 6i is replaced
by placeholder hb, 4i. Now the search returns to the root node, and the second best solution β ′
is obtained from the CBG solver, leading to child hd, 5.5i. Placeholder nodes are retained as
long as they have unexpanded children; only their values are updated.
When using GMAA*-ICE, we can derive lower and upper bounds for the CBG solution,
which can be exploited by the incremental CBG solver. The incremental CBG solver for
B(ϕt ) can be initialized with lower bound
vCBG = vGM AA − V 0...(t−1) (ϕt ),

(4.1)

where vGM AA is the value of the current best solution, and V 0...(t−1) (ϕt ) is the true expected
value of ϕt over the first t stages. Therefore, vCBG is the minimum value that a candidate
must generate over the remaining h − t stages in order to beat the current best solution. Note
that each time the incremental CBG solver is queried for a solution, vCBG is re-evaluated
(using (4.1)), because vGM AA may have changed.
When the used heuristic faitfully represents the immediate reward (i.e., is of the form
(2.9)), then, for the last stage t = h − 1, we can also specify an upper bound for the solution
of the CBG
v̄CBG = Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 ).
(4.2)
If this upper bound is attained, no further solutions will be required from the CBG solver.
The upper bound holds since by (2.8)
Vb (β) , Vb (ϕh ) − V 0...(h−2) (ϕh−1 )

= V (ϕh ) − V 0...(h−2) (ϕh−1 )
≤ Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 ).

In the first step, Vb (ϕh ) = V (ϕh ), because ϕh is a fully specified policy and the heuristic value
given by (2.8) equals the actual value when a heuristic that faithfully represents the expected
476

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Algorithm 10 Expand-ICE(q, H). The expand operator for GMAA*-ICE.
Input: q = hϕt , v̂i the search node to expand.
b ~θ,a).
Input: H the admissible heuristic that is of the form Q(
Output: QExpand the set containing 0 or 1 expanded child nodes.
1: if IsPlaceholder(q) then
2:
B(ϕt ) ← ϕt .CBG
{reuse stored CBG}
3: else
4:
B(ϕt−1 ) ← ϕt−1 .CBG
{retrieve previous CBG, note ϕt = (ϕt−1 , β t−1 )}
t−1
b
5:
B(ϕt ) ← ConstructExtendedBG(B(ϕt−1 ),β , Q)
t
t
6:
B(ϕ ) ← ClusterBG(B(ϕ ))
7:
B(ϕt ).Solver ← CreateSolver(B(ϕt ))
8:
ϕt .CBG ← B(ϕt )
{store pointer to this CBG}
9: end if
{set lower bound for CBG solution}
10: vCBG = vGM AA − V 0...(t−1) (ϕt )
11: if t = h − 1 then
12:
v̄CBG = Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 )
{upper bound only used for last stage CBG}
13: else
14:
v̄CBG = +∞
15: end if
b (β t )i ← B(ϕt ).Solver.NextSolution(vCBG ,v̄CBG )
{compute next CBG solution}
16: hβ t , V
t
17: if β then
18:
ϕt+1 ← (ϕt , β t )
{create partial joint policy}
t
t+1
0...t−1
t
b
b
19:
V (ϕ ) ← V
(ϕ ) + V (β )
{compute heuristic value}
20:
q ′ ← hϕt+1 , Vb (ϕt+1 )i
{create child node}
21:
QExpand ← {q ′ }
22: else
23:
QExpand ← ∅
{fully expanded: exists no solution s.t. V (β h−1 ) ≥ vCBG }
24: end if
25: return QExpand

Algorithm 11 PostProcessNode-ICE(q, L): Post processing of a node in GMAA*-ICE.
Input: q the last expanded node, L the open list.
Output: q is either removed or updated.
1: L.Pop(q)
2: if q is fully expanded or depth(q) = h − 1 then
3:
Cleanup q
{delete the node and the associated CBG and Solver}
4:
return
5: else
6:
c ← last expanded child of q
7:
q.v̂ ← c.v̂
{update heuristic value of parent node}
8:
IsPlaceholder(q) ← true
{remember that q is a placeholder}
9:
L.Insert(q)
{reinsert at appropriate position}
10: end if

477

Oliehoek, Spaan, Amato, & Whiteson

immediate reward is used. This implies that Vb (β) itself is a lower bound. In the second step
V (ϕh ) ≤ Vb (ϕh−1 ), because Vb (ϕh−1 ) is admissible. Therefore, we can stop expanding when
we find a β with (lower bound) heuristic value equal to the upper bound v̄CBG . This applies
only to the last stage because only then the first step is valid.
GMAA*-ICE can be implemented by replacing the Expand and the PostProcessNode
procedures of Algorithms 8 and 4 by Algorithms 10 and 11, respectively. Expand-ICE first
determines if a placeholder is being used and either reuses the previously constructed incremental CBG solver or constructs a new one. Then, new bounds are calculated and the next
CBG solution is obtained. Subsequently, only a single child node is generated (rather than
expanding all children as in Algorithm 13). PostProcessNode-ICE removes the last node
that was returned by Select only when all its children have been expanded. Otherwise, it
updates that node’s heuristic value and reinserts it in the open list. See Appendix A.2 for
GMAA*-ICE shown as a single algorithm.
4.2 Theoretical Guarantees
In this section, we prove that GMAA*-IC and GMAA*-ICE are search-equivalent. As a direct
result we establish that GMAA*-ICE is complete, which means that integrating incremental
expansion preserves the optimality guarantees of GMAA*-IC.
Definition 10. We call two GMAA* variants search-equivalent if they select exactly the
same sequence of non-placeholder nodes corresponding to past joint policies to expand in the
search tree using the Select operator.
For GMAA*-IC and GMAA*-ICE we show that the set of selected nodes are the same.
However, the set of expanded nodes can be different; in fact, it is precisely these differences
that incremental expansion exploits.
Theorem 5. GMAA*-ICE and GMAA*-IC are search-equivalent.
Proof. Proof is listed in Section A.4 of the appendix.
Note that Theorem 5 does not imply that the computational and space requirements
of GMAA*-ICE and GMAA*-IC are identical. On the contrary, for each expansion,
GMAA*-ICE generates only one child node to be stored on the open list. In contrast,
GMAA*-IC generates a number of child nodes that is, in the worst case, doubly exponential
in the depth of the selected node.16 However, GMAA*-ICE is not guaranteed to be more
efficient than GMAA*-IC. For example, in the case where all child nodes still have to be
generated, GMAA*-ICE will be slower due to the overhead it incurs.
Corollary 3. When using a heuristic of the form (2.9) GMAA*-ICE is complete.
Proof. Under the stated conditions, GMAA*-IC is complete (see Theorem 4).
GMAA*-ICE is search equivalent to GMAA*-IC, it is also complete.

Since

16. When a problem allows clustering, the number of child nodes grows less dramatically (see Section 3).

478

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

4.3 Incremental CBG Solvers
Implementing GMAA*-ICE requires a CBG solver that can incrementally deliver all β in
descending order of Vb (β). To this end, we propose to modify the Bayesian game Branch and
Bound (BaGaBaB) algorithm (Oliehoek et al., 2010). BaGaBaB performs an A*-search
over (partially specified) CBG policies. Thus, when applied within GMAA*-ICE, it performs
a second, nested A* search. To expand each node in the GMAA* search tree, a nested A*
search computes the next CBG solution.17 This section briefly summarizes the main ideas
behind BaGaBaB (for more information, see Oliehoek et al., 2010) and our modifications.
BaGaBaB works by creating a search tree in which the nodes correspond to partially
specified joint CBG policies. In particular, it represents a β as a joint action vector, a vector
hβ(θ 1 ), . . . ,β(θ |Θ| )i of the joint actions that β specifies for each joint type. Each node g in the
BaGaBaB search tree represents a partially specified vector and thus a partially specified
joint CBG policy. For example, a completely unspecified vector h·, · , . . . ,·i corresponds to
the root node, while an internal node

 g at depth d (root being at depth 0) specifies joint
actions for the first d joint types g = β(θ 1 ), . . . , β(θ d ), · , · , . . . ,· . The value of a node V (g)
is the value of the best joint CBG-policy consistent with it. Since this value is not known in
advance, BaGaBaB performs an A* search guided by an optimistic heuristic.
In particular, we can compute an upper bound on the value achievable for any such
partially specified vector by computing the maximum value of the complete information joint
policy that is consistent with it (i.e., a non-admissible joint policy that selects the maximizing
joint actions for the remaining joint types). Since this value is a guaranteed upper bound on
the maximum value achievable by a consistent joint CBG policy, it is an admissible heuristic.
We propose a modification to BaGaBaB to allow solutions to be incrementally delivered.
The main idea is to retain the search tree after a first call of BaGaBaB on a particular CBG
B(ϕt ) and update it during subsequent calls, thereby saving computational effort.
Standard A* search terminates when a single optimal solution has been found. This
behavior is the same when incremental BaGaBaB is called for the first time on a B(ϕt ).
However, during standard A*, nodes whose upper bound is lower than the best known lower
bound can be safely deleted, as they will never lead to an optimal solution. In contrast, in
an incremental setting such nodes cannot be pruned, as they could possibly result in the k-th
best solution and therefore might need to be expanded during subsequent calls to BaGaBaB.
Only nodes returned as solutions are pruned in order to avoid returning the same solution
twice. This modification requires more memory but does not affect the A* search process
otherwise.
When asked it for the k-th solution, BaGaBaB resets its internal lower bound to the value
of the next-best solution that was previously found but not returned (or to vCBG as defined in
(4.1) if no such solution was found). Then it starts an A* search initialized using the search
tree resulting from the (k − 1)-th solution. In essence, this method is similar to searching for
the best k solutions, where k can be incremented on demand. Recently it was shown that, for
fixed k, such a modification preserves all the theoretical guarantees (soundness, completeness,
17. While GMAA*-ICE could also use any other incremental CGB solver, there are few that avoid enumerating
all β before providing the first result and thus have the potential to work incrementally. An exception may
be the method of Kumar and Zilberstein (2010b), which employs AND/OR branch and bound search with
the EDAC heuristic (and is thus limited to the two-agent case). As a heuristic search method, it may be
amenable to an incremental implementation though to our knowledge this has not been attempted.

479

Oliehoek, Spaan, Amato, & Whiteson

optimal efficiency) of the A* algorithm (Dechter, Flerova, & Marinescu, 2012), but the results
trivially transfer to the setting where k is allowed to increase.

5. Experiments
In this section, we empirically test and validate all the proposed techniques: lossless clustering
of joint histories, incremental expansion of search nodes, and hybrid heuristic representations.
After introducing the experimental setup, we compare the performance of GMAA*-IC and
GMAA*-ICE to that of GMAA* on a suite of benchmark problems from the literature.
Next, we compare the performance of the proposed methods with state-of-the-art optimal
and approximate Dec-POMDP methods, followed by a case study of the scaling behavior
with respect to the number of agents. Finally, we compare memory requirements of the
hybrid heuristic representation to those of the tree and vector representations.
5.1 Experimental Setup
The most well-known Dec-POMDP benchmarks are the Dec-Tiger (Nair et al., 2003) and
BroadcastChannel (Hansen et al., 2004) problems. Dec-Tiger was discussed extensively
in Section 2. In BroadcastChannel, two agents have to transmit messages over a communication channel, but when both agents transmit at the same time a collision occurs that is
noisily observed by the agents. The FireFighting problem models a team of n firefighters
that have to extinguish fires in a row of nh houses (Oliehoek, Spaan, & Vlassis, 2008). Each
agent can choose to move to any of the houses to fight fires at that location; if two agents are
in the same house, they will completely extinguish any fire there. The (negative) reward of
the team of firefighters depends on the intensity of the fire at each house; when all fires have
been extinguished, reward of zero is received. In the Hotel 1 problem (Spaan & Melo, 2008),
travel agents need to assign customers to hotels with limited capacity. They can also send a
customer to a resort but this yields lower reward. In addition, we also use the following problems: Recycling Robots (Amato, Bernstein, & Zilberstein, 2007), a scaled-down version of
the problem described in Section 2; GridSmall with two observations (Amato, Bernstein, &
Zilberstein, 2006) and Cooperative Box Pushing (Seuken & Zilberstein, 2007a), a larger
two-robot benchmark. Table 1 summarizes these problems numerically, listing the number of
joint policies for different planning horizons.
Experiments were run on an Intel Core i5 CPU running Linux, and GMAA*, GMAA*-IC,
and GMAA*-ICE were implemented in the same code-base using the MADP Toolbox (C++)
(Spaan & Oliehoek, 2008). The vector-based QBG representation is computed using a variation of Incremental Pruning (adapted for computing Q-functions instead of regular value functions), corresponding to the NaiveIP method as described by Oliehoek and Spaan (2012).
To implement the pruning, we employ Cassandra’s POMDP-solve software (A. R. Cassandra,
1998).
For the results in Sections 5.2 and 5.3, we limited each process to 2Gb RAM and a
maximum CPU time of 3,600s. Reported CPU times are averaged over 10 independent runs
and have a resolution of 0.01s. Timings are given only for the MAA* search processes, since
480

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

problem primitives

Dec-Tiger

num. π for h

n

|S|

|Ai |

|Oi |

2

4

6

2

2

3

2

7.29e2

2.06e14

1.31e60

BroadcastChannel

2

4

2

2

6.40e1

1.07e9

8.51e37

GridSmall

2

16

5

2

1.563e4

9.313e20

1.175e88

Cooperative Box Pushing

2

100

4

5

1.68e7

6.96e187

1.96e4703

Recycling Robots

2

4

3

2

7.29e2

2.06e14

1.31e60

Hotel 1

2

16

3

4

5.90e4

1.29e81

3.48e1302

FireFighting

2

432

3

2

7.29e2

2.06e14

1.31e60

Table 1: Benchmark problem sizes and number of joint policies for different horizons.
computation of the heuristic is the same for both methods and can be amortized over multiple
runs.18 All problem definitions are available via http://masplan.org.
5.2 Comparing GMAA*, GMAA*-IC, and GMAA*-ICE
We compared GMAA*, GMAA*-IC, and GMAA*-ICE using the hybrid QBG representation. While all methods compute an optimal policy, we expect GMAA*-IC to be more efficient
than GMAA* when lossless clustering is possible. Furthermore, we expect GMAA*-ICE to
provide further improvements in terms of speedup and scaling to longer planning horizons.
The results are shown in Table 2. For all entries where we report results, the QBG heuristics
could be computed, thanks to the hybrid representation. Consequently, the performance of
GMAA*-IC is much better than all previously reported results, including those of Oliehoek
et al. (2009), who were often required to resort to QMDP for larger problems and/or horizons.
The entries marked by ‘§’ show the limits when using QMDP instead of QBG : in most of these
problems we can reach longer horizons with QBG . Only for FireFighting can GMAA*-ICE
with QMDP compute solutions for higher h than is possible with QBG (hence the missing “§”,
and showing that GMAA*-ICE is more efficient using a loose heuristic than GMAA*-IC).
Furthermore, the “†” entries indicate that the horizon to which we can solve a problem with
a tree-based QBG representation is often much shorter.
These results clearly illustrate that GMAA*-IC leads to a significant improvement in
performance. In all problems, GMAA*-IC was able to produce a solution more quickly and
to increase the largest solvable horizon over GMAA*. In some cases, GMAA*-IC is able to
drastically increase the solvable horizon.
Furthermore, the results clearly demonstrate that incremental expansion allows for significant additional improvements. In fact, the table demonstrates that GMAA*-ICE significantly outperforms GMAA*-IC, especially in problems where little clustering is possible.
The results in Table 2 also illustrate the efficacy of a hybrid representation. For problems
like GridSmall, Cooperative Box Pushing, FireFighting and Hotel 1 neither the
tree nor vector representation is able to provide a compact QBG heuristic for the longer hori18. The heuristics’ computation time ranges from less than a second to hours (for high h in some difficult
problems). Table 4 presents some heuristic computation time results.

481

Oliehoek, Spaan, Amato, & Whiteson

h
2
3
4
5
6
7

V ∗ TGMAA* (s)
Dec-Tiger
−4.000000
≤ 0.01
5.190812
§≤ 0.01
4.802755
563.09
7.026451
−
10.381625

TIC (s)

TICE (s)

h

≤ 0.01
≤ 0.01
§0.27
†21.03
−

≤ 0.01
≤ 0.01
≤ 0.01
§†0.02
46.43
∗

2
3
4
5
6
10
15
18
20
30
40
50
60
70
80

FireFighting hnh = 3,nf = 3i
2 −4.383496
0.09
≤ 0.01
≤ 0.01
3 −5.736969
§3.05
§0.11
0.10
4 −6.578834
1001.53 †950.51
1.00
5 −7.069874
−
−
†4.40
6 −7.175591
0.08
0.07
7
#
#
GridSmall
2
0.910000
≤ 0.01
≤ 0.01
≤ 0.01
3
1.550444
§0.90
§0.10
≤ 0.01
4
2.241577
*
†1.77 §†≤ 0.01
5
2.970496
−
0.02
6
3.717168
−
0.04
7
#
#
Hotel 1
2 10.000000
§≤ 0.0
≤ 0.01
≤ 0.01
3 16.875000
*
≤ 0.01
≤ 0.01
4 22.187500
§†≤ 0.01 §†≤ 0.01
5 27.187500
≤ 0.01
≤ 0.01
6 32.187500
≤ 0.01
≤ 0.01
7 37.187500
≤ 0.01
≤ 0.01
8 42.187500
≤ 0.01
≤ 0.01
9 47.187500
0.02
≤ 0.01
10
#
#
Cooperative Box Pushing
2
17.600000
§0.02
≤ 0.01
≤ 0.01
3
66.081000
∗
§†0.11 †≤ 0.01
4
98.593613
∗ §313.07
5
#
#

2
3
4
5
6
7
10
20
25
30
40
50
53
100
250
500
600
700
800
900
1000

V ∗ TGMAA* (s) TIC (s) TICE (s)
Recycling Robots
7.000000
≤ 0.01 ≤ 0.01
≤ 0.01
10.660125
§≤ 0.01 ≤ 0.01
≤ 0.01
13.380000
713.41 ≤ 0.01
≤ 0.01
16.486000
− †≤ 0.01 †≤ 0.01
19.554200
≤ 0.01
≤ 0.01
31.863889
≤ 0.01
≤ 0.01
47.248521
§≤ 0.01
≤ 0.01
56.479290
≤ 0.01 §≤ 0.01
62.633136
≤ 0.01
≤ 0.01
93.402367
0.08
0.05
124.171598
0.42
0.25
154.940828
2.02
1.27
185.710059
9.70
6.00
216.479290
−
28.66
−
−
BroadcastChannel
2.000000
≤ 0.01 ≤ 0.01
≤ 0.01
2.990000
≤ 0.01 ≤ 0.01
≤ 0.01
3.890000
§≤ 0.01 ≤ 0.01
≤ 0.01
4.790000
1.27 ≤ 0.01
≤ 0.01
5.690000
− ≤ 0.01
≤ 0.01
6.590000
†≤ 0.01 †≤ 0.01
9.290000
≤ 0.01
≤ 0.01
18.313228
≤ 0.01
≤ 0.01
22.881523
≤ 0.01
≤ 0.01
27.421850
≤ 0.01
≤ 0.01
36.459724
≤ 0.01
≤ 0.01
45.501604
≤ 0.01
≤ 0.01
48.226420
§≤ 0.01 §≤ 0.01
90.760423
≤ 0.01
≤ 0.01
226.500545
0.06
0.07
452.738119
0.81
0.94
543.228071
11.63
13.84
633.724279
0.52
0.63
−
−
814.709393
9.57
11.11
−
−

Table 2: Experimental results comparing regular GMAA*, GMAA*-IC, and GMAA*-ICE.
Listed are the computation times of GMAA* (TGMAA* ), GMAA*-IC (TIC ), and
GMAA*-ICE (TICE ), using the hybrid QBG representation. We use the following symbols:
‘−’ memory limit violations, ‘∗’ time limit overruns, ‘#’ heuristic computation exceeded memory or time limits, ‘§’ maximum planning horizon using QMDP , ‘†’ maximum planning horizon
using tree-based QBG . Bold entries indicate that only the methods proposed in this article
have computed these results.
zons. Apart from Dec-Tiger and FireFighting, computing and storing QBG (or another
tight heuristic) for longer horizons is the bottleneck to further scalability.
Together, these algorithmic improvements lead to the first optimal solutions for many
problem horizons. In fact, for the vast majority of problems tested, we provide results for
longer horizons than any previous work (the bold entries). These improvements are quite sub482

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

h
2
3
4
5
2
3
4
5
6
2
3
4
2
3
4
5
6
7
8
9
2
3

|BGh−1 |

|cBGt |
Dec-Tiger
4 1.0, 4.0
16 1.0, 4.0, 9.0
64 1.0, 4.0, 9.0, 23.14
256 1.0, 4.0, 9.0, 16.0, 40.43
FireFighting hnh = 3,nf = 3i
4 1.0, 4.0
16 1.0, 4.0, 16.0
64 1.0, 4.0, 16.0, 64.0
256 1.0, 4.0, 16.0, 64.0, 256.0
1024 1.0, 1.0, 2.0, 3.0, 6.0, 10.0
GridSmall
4 1.0, 4.0
16 1.0, 4.0, 10.50
64 1.0, 4.0, 10.50, 20.0
Hotel 1
16 1.0, 4.0
256 1.0, 4.0, 16.0
4096 1.0, 4.0, 8.0, 16.0
65536 1.0, 4.0, 4.0, 8.0, 16.0
1.05e6 1.0, 4.0, . . . , 4.0, 8.0, 16.0
1.68e7 1.0, 4.0, . . . , 4.0, 8.0, 16.0
2.68e8 1.0, 4.0, . . . , 4.0, 8.0, 16.0
4.29e9 1.0, 4.0, . . . , 4.0, 8.0, 16.0
Cooperative Box Pushing
25 1.0, 4.0
625 1.0, 4.0, 25.0

h
2
3
4
5
10
15
18
20
30
40
50
60
2
3
4
5
6
7
10
20
25
30
40
50
100
900

|BGh−1 | |cBGt |
Recycling Robots
4 1.0, remaining stages
16 1.0, remaining stages
64 1.0, remaining stages
256 1.0, remaining stages
262144 1.0, remaining stages
2.68e8 1.0, remaining stages
1.72e10 1.0, remaining stages
2.75e11 1.0, remaining stages
2.88e17 1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
BroadcastChannel
4 1.0 (for all t)
16 1.0 (for all t)
64 1.0 (for all t)
256 1.0 (for all t)
1024 1.0 (for all t)
4096 1.0 (for all t)
262144 1.0 (for all t)
2.75e11 1.0 (for all t)
2.81e14 1.0 (for all t)
2.88e17 1.0 (for all t)
1.0 (for all t)
1.0 (for all t)
1.0 (for all t)
1.0 (for all t)

≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0
≤ 4.0

Table 3: Experimental results detailing the effectiveness of clustering. Listed are the size of
the CBGs for t = h − 1 without clustering (|BGh−1 |), and the average CBG size for all stages
with clustering (|cBGt |).
stantial, especially given that lengthening the horizon by one increases the problem difficulty
exponentially (cf. Table 1).
5.2.1 Analysis of Clustering Histories
Table 3 provides additional details about the performance of GMAA*-IC, by listing the
number of joint types in the GMAA*-IC search, |cBGt |, for each stage t. These are averages
since the algorithm forms CBGs for different past policies, leading to clusterings of different
sizes.19 To see the impact of clustering, the table also lists |BGh−1 |, the number of joint types
in the CBGs constructed for the last stage without clustering, which is constant.
In Dec-Tiger, the time needed by GMAA*-IC is more than 3 orders of magnitude less
than that of GMAA* for horizon h = 4. For h = 5, this test problem has 3.82e29 joint
policies, and no other method has been able to optimally solve it. GMAA*-IC, however,
is able to do so in reasonable time. In Dec-Tiger, there are clear symmetries between the
19. Note that in some problem domains we report smaller clusterings than Oliehoek et al. (2009). Due to an
implementation mistake, their clustering was overly conservative, and did not in all cases treat two histories
as probabilistically equivalent, when in fact they were.

483

Oliehoek, Spaan, Amato, & Whiteson

observations that allow for clustering, as demonstrated by Fig. 4. Another key property is
that opening the door resets the problem, which may also facilitate clustering.
In FireFighting, for short planning horizons no lossless clustering is possible at any
stage, and as such, the clustering incurs some overhead. However, GMAA*-IC is still faster
than GMAA* because constructing the BGs using bootstrapping from the previous CBG
takes less time than constructing a CBG from scratch. Interesting counterintuitive results
occur for h = 6, which was solved within memory limits, in contrast to h = 5. In fact, using
QMDP we could compute optimal values V ∗ for h > 6, and it turns out that these are equal
to that for h = 6. The reason is that the optimal joint policy is guaranteed to extinguish all
fires in 6 stages. For subsequent stages, all the rewards will be 0. While this itself does not
influence clustering, the further analysis of Table 3 reveals that the CBG instances encountered
during the h = 6 search happen to cluster much better than those in h = 5, which is possible
because the heuristics vary with the horizon. In fact, π ∗ for h = 6 sends both agents to the
middle house at t = 0, while for h = 5, agents are dispatched to different houses. When both
agents fight fires at the same house, the fire is extinguished completely, and resulting joint
observations do not provide any new information. As a result, different joint types lead to the
same joint belief, which means they can be clustered. If agents visit different houses, their
observations do convey information, leading to different possible joint beliefs (which cannot
be clustered).
Hotel 1 allows for a large amount of clustering, and GMAA*-IC outperforms GMAA*
by a large margin, with the former reaching h = 9 and the latter h = 2. This problem
is transition and observation independent (Becker, Zilberstein, Lesser, & Goldman, 2003;
Nair, Varakantham, Tambe, & Yokoo, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo,
2007), which facilitates clustering, as we further discuss in Section 5.5. Unlike methods
specifically designed to exploit transition and observation independence, GMAA*-IC exploits
this structure without requiring a predefined explicit representation of it. Further scalability
is limited by the computation of the heuristic.
For BroadcastChannel, GMAA*-IC achieves an even more dramatic increase in performance, allowing the solution of up to horizon h = 900. Analysis reveals that the CBGs
constructed for all stages are fully clustered: they contain only one type for each agent. The
reason is as follows. When constructing a CBG for t = 1, there is only one joint type for the
previous CBG so, given β 0 , the solution for the previous CBG, there is no uncertainty with
respect to the previous joint action a0 . The crucial property of BroadcastChannel is that
the (joint) observation reveals nothing about the new state, but only about what joint action
was taken (e.g., ‘collision’ if both agents chose to ‘send’). As a result, the different individual
histories can be clustered. In a CBG constructed for stage t = 2, there is again only one joint
type in the previous game. Therefore, given the past policy, the actions of the other agents
can be perfectly predicted. Again the observation conveys no information so this process repeats. Thus, the problem has a special property which could be described as non-observable
given the past joint policy. GMAA*-IC automatically exploits this property. Consequently,
the time needed to solve each CBG does not grow with the horizon. The solution time, however, still increases super-linearly because of the increased amount of backtracking. As in
FireFighting, performance is not monotonic in the planning horizon. In this case however,
clustering is clearly not responsible for the difference. Rather, the only explanation is that
for certain horizons, there are many near-optimal joint policies, leading to more backtracking
and a higher search cost.
484

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

10

Nodes at depth t

10

Dec−Tiger, h=6 − Full Exp.
Dec−Tiger, h=6 − Inc. Exp.
GridSmall, h=6 − Full Exp.
GridSmall, h=6 − Inc. Exp.
FireFighting, h=5 − Full Exp.
FireFighting, h=5 − Inc. Exp.

5

10

0

10

0

1

2
t

3

4

Figure 8: Number of expanded partial joint policies ϕt for intermediate stages t = 0, . . . ,h − 2
(in log scale).

5.2.2 Analysis of Incremental Expansion
In Dec-Tiger for h = 5, GMAA*-ICE achieves a speedup of three orders of magnitude and
can compute a solution for h = 6, unlike GMAA*-IC. For GridSmall, it achieves a large
speedup for h = 4 and fast solutions for h = 5 and 6, where GMAA*-IC runs out of memory. Similar positive results are obtained for FireFighting, Cooperative Box Pushing
and Recycling Robots. In fact, when using QMDP , GMAA*-ICE is able to compute
solutions well beyond h = 1000 for the FireFighting problem, which stands in stark contrast to GMAA*-IC that only computes solutions to h = 3 with this heuristic. Note that
BroadcastChannel is the only problem for which GMAA*-IC is (slightly) faster than
GMAA*-ICE. Because this problem exhibits clustering to a single joint type, the overhead
of incremental expansion does not pay off.
To further analyze incremental expansion, we examined its impact on the number of nodes
expanded for intermediate stages t = 0, . . . ,h − 2. Fig. 8 shows the number of nodes expanded
in GMAA*-ICE and the number that would be expanded for GMAA*-IC (which can be
easily computed since they are search-tree equivalent). There is a clear relationship between
the results from Fig. 8 and Table 2, illustrating, e.g., why GMAA*-IC runs out of memory on
GridSmall h = 6. The plots confirm our hypothesis that, in practice, only a small number
of child nodes are queried.
5.2.3 Analysis of Hybrid Heuristic Representation
Fig. 9 illustrates the memory requirements in terms of number of parameters (i.e., real numbers) for the tree, vector, and hybrid representations for QBG , where the latter is computed
following Algorithm 9. Results for the vector representation are omitted when those representations grew beyond limits. The effectiveness of the vector pruning depends on the problem
and the complexity of the value function, which can increase suddenly, as for instance happens in Fig. 9c. These results show that, for several benchmark Dec-POMDPs, the hybrid
representation allows for significant savings in memory, allowing the computation of tight
heuristics for longer horizons.
485

Oliehoek, Spaan, Amato, & Whiteson

h

MILP

DP-LPC

DP-IPG

GMAA — QBG
IC

ICE

heur

BroadcastChannel, ICE solvable to h = 900
2
0.38
≤ 0.01
0.09
≤ 0.01
3
1.83
0.50
56.66
≤ 0.01
4
34.06
∗
*
≤ 0.01
5
48.94
≤ 0.01

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01

Dec-Tiger, ICE
2
0.69
3
23.99
4
∗
5

≤ 0.01
≤ 0.01
≤ 0.01
0.02

≤ 0.01
≤ 0.01
0.03
0.09

solvable to h = 6
0.05
0.32
60.73
55.46
−
2286.38
−

≤ 0.01
≤ 0.01
0.27
21.03

FireFighting (2 agents, 3 houses, 3 firelevels), ICE
2
4.45
8.13
10.34
≤ 0.01
3
−
−
569.27
0.11
4
−
950.51
GridSmall, ICE solvable to h = 6
2
6.64
11.58
0.18
3
∗
−
4.09
4
77.44

0.01
0.10
1.77

Recycling Robots, ICE solvable to h = 70
2
1.18
0.05
0.30
≤ 0.01
3
*
2.79
1.07
≤ 0.01
4
2136.16
42.02
≤ 0.01
5
−
1812.15
≤ 0.01
Hotel
2
3
4
5
9
10
15

1, ICE solvable to h = 9
1.92
6.14
0.22
315.16
2913.42
0.54
−
−
0.73
1.11
8.43
17.40
283.76

Cooperative Box Pushing
2
3.56
15.51
3
2534.08 −
4
−

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01
0.02
#

solvable to h ≫ 1000
≤ 0.01 ≤ 0.01
0.10
0.07
1.00
0.65
≤ 0.01
≤ 0.01
≤ 0.01

≤ 0.01
0.42
67.39

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01

≤ 0.01
≤ 0.01
0.02
0.02

≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01
≤ 0.01
#

0.03
1.51
3.74
4.54
20.26

(QPOMDP ), ICE solvable to h = 4
1.07
≤ 0.01 ≤ 0.01 ≤ 0.01
6.43
0.91
0.02
0.15
1138.61
*
328.97 0.63

Table 4: Comparison of runtimes with other methods. Total time of the GMAA* methods
is given by taking the time from the method column (‘IC’ or ‘ICE’) and adding the heuristic
computation time (‘heur’). We use the following symbols: ‘−’ memory limit violations, ‘*’
time limit overruns, ‘#’ heuristic computation exceeded memory or time limits.

486

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

10

Memory required

5

10

0

10

5

10

0

1

2

3
4
Horizon

5

10

6

(a) Dec-Tiger.

5

10

0

10

2

3
4
Horizon

5

10

6

(d) Recycling Robots.

10

0

1 2 3 4 5 6 7 8 9
Horizon

15

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

Tree
Vector
Hybrid

(c) Hotel 1.

15

Tree
Vector
Hybrid

Memory required

Memory required

10

10

1

10

20

(b) FireFighting.

15

10

10

Tree
Vector
Hybrid

10

10
Memory required

Memory required

10

Tree
Vector
Hybrid

Memory required

10

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

(e) BroadcastChannel.

10

1

2

3
4
Horizon

5

6

(f) GridSmall.

Figure 9: Hybrid heuristic representation. The y-axis shows number of real numbers stored
for different representations of QBG for several benchmark problems (in log scale).
5.3 Comparing to Other Methods
In this section, we compare GMAA*-IC and GMAA*-ICE to other methods from the literature. We begin by comparing the runtimes of our methods against the following state-ofthe-art optimal Dec-POMDP methods: MILP20 (Aras & Dutech, 2010) converts the DecPOMDP to a mixed integer linear program, for which numerous solvers are available. We
have used MOSEK version 6.0. DP-LPC21 (Boularias & Chaib-draa, 2008) performs dynamic programming with lossless policy compression, with CPLEX 12.4 as the LP solver.
DP-IPG (Amato et al., 2009) performs exact dynamic programing with incremental policy
20. The results reported here deviate from those reported by Aras and Dutech (2010). For a number of
problems, Aras et al. employed a solution method that solves the MILP as a series (a tree) of smaller
MILPs by branching on the continuous realization weight variables for earlier stages. That is, for each past
joint policy ϕt for some stage t, they solve a different MILP involving the subset of consistent sequences.
Additionally, for FireFighting and GridSmall, we use the benchmark versions standard to the literature
(Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras and Dutech (2010) use non-standard
versions. This explains the difference between our results and the ones reported in their article (personal
communication, Raghav Aras).
21. The goal of Boularias and Chaib-draa (2008) was to find non-dominated joint policies for all initial beliefs.
The previously reported results concerned run-time to compute the non-dominated joint policies, without
performing pruning on the full-length joint policies. In contrast, we report the time needed to compute the
actual optimal Dec-POMDP policy (given b0 ). This additionally requires the final round of pruning and
subsequently computing the value for each of the remaining joint policies for the initial belief. This additional overhead explains the differences in run time between what we report here and what was previously
reported (personal communication, Abdeslam Boularias).

487

Oliehoek, Spaan, Amato, & Whiteson

Problem
Dec-Tiger
Cooperative Box Pushing
GridSmall

h
6
3
5

m
7
3
3

VMBDP
9.91
53.04
2.32

V∗
10.38
66.08
2.97

Table 5: Comparison of optimal (V ∗ ) and approximate (VMBDP ) values.

generation that exploits known start state and knowledge about what states are reachable in
doing the DP backup.
Table 4, which shows the results of the comparison, demonstrates that, in almost all cases,
the total time of GMAA*-ICE (given by the sum of heuristic computation time and the time
for the GMAA*-phase) is significantly less than that of any other state-of-the-art methods.
Moreover, as demonstrated in Table 2, GMAA*-ICE can compute solutions for longer horizons for all these problems, except for Cooperative Box Pushing and Hotel 1.22 For
these problems, it is not possible to compute QBG for longer horizons. Overcoming this
problem could enable GMAA*-ICE to scale to further horizons as well.
The DP-LPC algorithm proposed by Boularias and Chaib-draa (2008) also improves the
efficiency of optimal solutions by a form of compression. The performance of their algorithm,
however, is weaker than that of GMAA*-IC. There are two main explanations for the performance difference. First, DP-LPC uses compression to more compactly represent the values
for sets of useful sub-tree policies, by using sequence form representation. The policies themselves, however, are not compressed: they still specify actions for every possible observation
history (for each policy it needs to select an exponential amount of sequences that make up
that policy). Hence, it cannot compute solutions for long horizons. Second, GMAA*-IC can
exploit knowledge of the initial state distribution b0 .
Overall, GMAA*-ICE substantially improves the state-of-the-art in optimally solving
Dec-POMDPs. Previous methods typically improved the feasible solution horizon by just
one (or only provided speed-ups for horizons that could already be solved). By contrast,
GMAA*-ICE dramatically extends the feasible solution horizon for many problems.
We also consider MBDP-based approaches, the leading family of approximate algorithms.
Table 5, which reports the VMBDP values produced by PBIP-IPG (Amato et al., 2009) (with
typical ‘maxTrees’ parameter setting m), demonstrates that the optimal solutions produced by
GMAA*-IC or GMAA*-ICE are of higher quality. PBIP-IPG was chosen because all other
MBDP algorithms with the same parameters achieve at most the same value. While not
exhaustive, this comparison illustrates that even the best approximate Dec-POMDP methods
in practice provide inferior joint policies on some problems. Conducting such analysis is
possible only if optimal solutions can be computed. Clearly, the more data that becomes
available, the more thorough the comparisons that can be made. Therefore, scalable optimal
solution methods such as GMAA*-ICE are critical for improving these analyses.
488

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

problem primitives

num. π for h

n

|S|

|A|

|O|

2

4

6

2

27

4

4

64

1.07e9

8.50e37

3

81

8

8

512

3.51e13

7.84e56

4

243

16

16

4.09e3

1.15e18

7.23e75

5

729

32

32

3.27e4

3.77e22

6.67e94

6

2187

64

64

2.62e5

9.80e55

6.15e113

Table 6: FireFightingGraph: the number of joint policies for different numbers of agents
and horizons, with 3 possible fire levels.
5.4 Scaling to More Agents
All of the benchmark problems in our results presented so far were limited to two agents. Here,
we present a case study on FireFightingGraph (Oliehoek, Spaan, Whiteson, & Vlassis,
2008), a variation of FireFighting allowing for more agents, in which each agent can only
fight fires at two houses, instead of at all of them. Table 6 highlights the size of these
problems, including the total number of joint policies for different horizons. We compared
GMAA*, GMAA*-IC, GMAA*-ICE (all using a QMDP heuristic), BruteForceSearch,
and DP-IPG, with a maximum run-time of 12 hours and running on an Intel Core i7 CPU,
averaged over 10 runs. BruteForceSearch is a simple optimal algorithm that enumerates
and evaluates all joint policies, and was implemented in the same codebase as the GMAA*
variations. DP-IPG results use the original implementation and were run on an Intel Xeon
computer. Hence, while the timing results are not directly comparable, the overall trends are
apparent. Also, since the DP-IPG implementation is limited to 2 agents, no results are shown
for more agents.
Fig. 10 shows the computation times for FireFightingGraph across different numbers of
of agents and planning horizons, while Table 7 lists the optimal values obtained. As expected,
the baseline BruteForceSearch performs very poorly, only scaling beyond h = 2 for 2
agents, while DP-IPG can only reach h = 4. On the other hand, regular GMAA* performs
relatively well, scaling to a maximum of 5 agents. However, GMAA*-IC and GMAA*-ICE
improve the efficiency of GMAA* by 1–2 orders of magnitude. As such, they substantially
outperform the other three methods, and scale up to 6 agents. The benefit of incremental expansion is clear for n = 3,4, where GMAA*-ICE can reach a higher horizon than GMAA*-IC.
Hence, although this article focuses on scalability in the horizon, these results show that the
methods we propose can also improve scalability in the number of agents.
5.5 Discussion
Overall, the empirical results demonstrate that incremental clustering and expansion offers
dramatic performance gains on a diverse set of problems. In addition, the results on Broad22. In Hotel 1, DP-IPG performs particularly well because the problem structure has limited reachability.
That is, each agent can fully observe its local state (but not that of the other agent) and in all local states
except one there is one action that dominates all others. As a result, DP-IPG can generate a small number
of possibly optimal policies.

489

Oliehoek, Spaan, Amato, & Whiteson

4

4

10

10

3

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

3

10

computation time (s)

computation time (s)

computation time (s)

4

10

3

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

4

h

#agents

3

2

2

3

5

4

7

6

8

9

10

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

h

#agents

(a) GMAA* results.

3

2

2

3

4

5

7

8

9

10

h

#agents

(b) GMAA*-IC results.

4

(c) GMAA*-ICE results.

4

10

10

3

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

computation time (s)

3

computation time (s)

4

6

10

2

10

1

10

0

10

−1

10

−2

10

−3

10

6
h

#agents

5

4

3

2

2

3

4

5

6

7

8

9

10

h

#agents

(d) BruteForceSearch results.

(e) DP-IPG results.

Figure 10: Comparison of GMAA*, GMAA*-IC, GMAA*-ICE, BruteForceSearch,
and DP-IPG on the FireFightingGraph problem. Shown are computation time (in log
scale) for various number of agents and horizons. Missing bars indicate that the method
exceeded time or memory limits. However, the DP-IPG implementation only supports 2
agents.

h
2
3
4
5
≥6

n=2
−4.394252
−5.806354
−6.626555
−7.093975
−7.196444

n=3
−5.213685
−6.654551
−7.472568

n=4
−6.027319
−7.391423
−8.000277

n=5
−6.846752

n=6
−7.666185

Table 7: Value V ∗ of optimal solutions to the FireFightingGraph problem, for different
horizons and numbers of agents.

castChannel illustrate a key advantage of our approach: when a problem possesses a property that makes a large amount of clustering possible, our clustering method exploits this
property automatically, without requiring a predefined explicit representation of it.
490

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Of course, not all problems admit great reductions via clustering. One domain property
that allows for clustering is when the past joint policy encountered during GMAA* makes the
observations superfluous, as with BroadcastChannel and FireFighting. In Dec-Tiger,
we see that certain symmetries can lead to clustering. However clustering can occur even
without these properties. In fact, for all problems and nearly all horizons that we tested, the
size of the CBGs can be reduced. Moreover, in accordance with the analysis of Section 3.2,
the improvements in planning efficiency are huge, even for modest reductions in CBG size.
One class of problems where we can say something a priori about the amount of clustering
that is possible is the class of Dec-POMDPs with transition and observation independence
(Becker et al., 2003). In such problems, the agents have local states and the transitions are
independent, which for two agents can be expressed as
Pr(s′1 , s′2 |s1 , s2 , a1 , a2 ) = Pr(s′1 |s1 , a1 ) Pr(s′2 |s2 , a2 ).

(5.1)

Similarly, the observations are assumed to be independent, which means that for each agent
the observation probability depends only on its own action and local state: Pr(oi |ai , s′i ). For
such problems, the probabilistic equivalence criterion (3.1) factors too. In particular, due to
transition and observation independence23 , (3.2) holds true for any θ~ia ,θ~ib . Moreover, (3.3)
factors as the product Pr(s1 , s2 |θ~1 , θ~2 ) = Pr(s1 |θ~1 ) Pr(s2 |θ~2 ) and thus holds if Pr(s1 |θ~1a ) =
Pr(s1 |θ~1b ). That is, two histories can be clustered if they induce the same ‘local belief’. As
such, the size of the CBGs directly corresponds to the product of the number of reachable local
beliefs. Since the transition and observation independent Hotel 1 problem is also locally
fully observable, and the local state spaces consist of four states, there are only four possible
local beliefs (which is consistent with the CBG size of 16 from Table 3). Moreover, we see
that this maximum size is typically only reached at the end of search. This is because good
policies defer sending customers to the hotel and thus do not visit local states where the hotel
is filled in the earlier stages.
In more general classes of problems, even other weakly coupled models (e.g., Becker,
Zilberstein, & Lesser, 2004; Witwicki & Durfee, 2010), the criterion (3.1) does not factor,
and hence there is no direct correspondence to the number of local beliefs. As such, only
by applying our clustering algorithm can we determine how well such a problem clusters.
This is analogous to, e.g., state aggregation in MDPs (e.g., discussed in Givan, Dean, &
Greig, 2003) where it is not known how to predict a priori how large a minimized model will
be. Fortunately, our empirical results demonstrate that, in domains that admit little or no
clustering, the overhead is small.
As expected, incremental expansion is most helpful for problems which do not allow for
much clustering. However, the results for, e.g., Dec-Tiger illustrate that there is a limit to
the amount of scaling that the method can currently provide. The bottleneck is the solution
of the large CBGs for the later stages: the CBG solver has to solve these large CBGs when
returning the first solution in order to guarantee optimality, but this takes takes a long time.
We expect that further improvements to CBG solvers can directly add to the efficacy of
incremental expansion.
Our experiments also clearly demonstrate that the Dec-POMDP complexity results, while
important, are only worst-case results. In fact, the scalability demonstrated in our experiments
clearly show that in many problems we successfully scale dramatically beyond what would be
23. This assumes no ‘external’ state variable s0 .

491

Oliehoek, Spaan, Amato, & Whiteson

expected for a doubly-exponential dependence on the horizon. Even for the smallest problems,
a doubly-exponential scaling in the horizon implies that it is impossible to compute solutions
beyond h = 4 at all, as indicated by the following simple calculation: let n = 2, |Ai | = 2
actions, |Oi | = 2| observations, then
5

|Ai |(n∗(|Oi |

))

4

/|Ai |(n∗(|Oi |

))

= 4.2950e9.

Thus, even in the simplest possible case, we see an increase of a factor 4.2950e09 from h = 4
to h = 5. Similarly, the next increment, from h = 5 to h = 6, increases the size of the search
space by a factor 1.8447e19. However, our experiments clearly indicate that in almost all
cases, things are not so dire. That is, even though matters look bleak in the light of the
complexity results, we are in many cases able to perform substantially better than this worst
case.

6. Related Work
In this section, we discuss a number of methods that are related to those proposed in this
article. Some of these methods have already been discussed in earlier sections. In Section 3, we
indicated that our clustering method is closely related to the approach of Emery-Montemerlo
et al. (2005) but is also fundamentally different because our method is lossless. In Section 5.3,
we discussed connections to the approach of Boularias and Chaib-draa (2008) which clusters
policy values. This contrasts with our approach which clusters the histories and thus the
policies themselves, leading to greater scalability.
In Section 3.1.2, we discussed the relationship between our notion of probabilistic equivalence (PE) and the multiagent belief. However, there is yet another notion of belief, employed
in the JESP solution method (Nair et al., 2003), that is superficially more similar to the PE
distribution. A ‘JESP belief’ for an AOH θ~i is a probability distribution Pr(s,~o6=i |θ~i , b0 , π 6=i )
over states and observation histories of other agents given a (deterministic) full policy of all
the other agents. It is a sufficient statistic, since it induces a multiagent belief, thus it also
allows for the clustering of histories. The crucial difference with, and the utility of, PE lies in
the fact that the PE criterion is specified over states and AOHs given only a past joint policy.
That is, (3.1) does not induce a multiagent belief.
Our clustering approach also resembles a number of methods that employ other equivalence
notions. First, several approaches exploit the notion of behavioral equivalence (Pynadath &
Marsella, 2007; Zeng et al., 2011; Zeng & Doshi, 2012). They consider, from the perspective
of a protagonist agent i, the possible models of another agent j. Since j affects i only through
its actions, i.e., its behavior, agent i can cluster together all the models of agent j that lead
to the same policy πj for that agent. That is, it can cluster all models of agent j that are
behaviorally equivalent. In contrast, we do not cluster models of other agents j, but histories
of this agent i if all the other agents, as well as the environment, are guaranteed to behave the
same in expectation, thus leading to the same best response of agent i. That is, our method
could be seen as clustering histories that are ‘expected environmental behavior equivalent’.
The notion of utility equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011) is closer to
PE because it also takes into account the (value of the) best-response of agent i (in particular,
it clusters two models mj and m′j if using BR(mj )—the best response against mj — achieves
the same value against m′j ). However, it remains a form of behavior equivalence in that it
clusters models of other agents, not histories of the protagonist agent.
492

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

There are also connections between PE and work on influence-based abstraction (Becker et
al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since the influence
(or point in parameter space, Becker et al., 2003) is a compact representation of the other
agents’ policies. Models of the other agents can be clustered if they lead to the same influence
on agent i. However, though more fine-grained, this is ultimately still a form of behavioral
equivalence.
A final relation to our equivalence notion is the work by Dekel, Fudenberg, and Morris
(2006), which constructs a distance measure and topology on the space of types with the
goal of approximating the infinite universal type space (the space of all possible beliefs about
beliefs about beliefs, etc.) for one-shot Bayesian games. Our setting, however, considers a
simple finite type space where the types directly correspond to the private histories (in the
form of AOHs) in a sequential problem. Thus, we do not need to approximate the universal
type space; instead we want to know which histories lead to the same future dynamics from
the perspective of an agent. Dekel et al.’s topology does not address this question.
Our incremental expansion technique is related to approaches extending A∗ to deal with
large branching factors in the context of multiple sequence alignment (Ikeda & Imai, 1999;
Yoshizumi, Miura, & Ishida, 2000). However, our approach is different because we do not
discard unpromising nodes but rather provide a mechanism to generate only the necessary
ones. Also, when proposing MAA*, Szer et al. (2005) developed a superficially similar approach that could be applied only to the last stage. In particular, they proposed generating
the child nodes one by one, each time checking if a child is found with value equal to its
parent’s heuristic value. Since the value of such a child specifies a full policy, its value is
a lower bound and therefore expansion of any remaining child nodes can be skipped. Unfortunately, a number of issues prevent this approach from providing substantial leverage in
practice. First, it cannot be applied to intermediate stages 0 ≤ t < h − 1 since no lower bound
values for the expanded children are available. Second, in many problems it is unlikely that
such a child node exists. Third, even if it does, Szer et al. did not specify an efficient way of
finding it. Incremental expansion overcomes all of these issues, yielding an approach that, as
our experiments demonstrate, significantly increases the size of the Dec-POMDPs that can
be solved optimally.
This article focuses on optimal solutions for Dec-POMDPs over a finite horizon. As part
of our evaluation, we compare against the MILP approach (Aras & Dutech, 2010), DPILP (Boularias & Chaib-draa, 2008) and DP-IPG (Amato et al., 2009), an extension of the
exact dynamic programming algorithm (Hansen et al., 2004). Research on finite-horizon DecPOMDPs has considered many other approaches such as bounded approximations (Amato,
Carlin, & Zilberstein, 2007), locally optimal solutions (Nair et al., 2003; Varakantham, Nair,
Tambe, & Yokoo, 2006) and approximate methods without guarantees (Seuken & Zilberstein,
2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Akın, 2010; Oliehoek, Kooi, & Vlassis, 2008;
Dibangoye et al., 2009; Kumar & Zilberstein, 2010b; Wu et al., 2010a; Wu, Zilberstein, &
Chen, 2010b).
In particular, much research has considered the optimal and/or approximate solution of
subclasses of Dec-POMDPs. One such subclass contains only Dec-POMDPs in which the
agents have local states that other agents cannot influence. The resulting models, such as the
TOI-Dec-MDP (Becker et al., 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) and NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe,
& Yokoo, 2008; Kumar & Zilberstein, 2009), can be interpreted as independent (PO)MDPs for
493

Oliehoek, Spaan, Amato, & Whiteson

each agent that are coupled through the reward function (and possibly an unaffectable state
feature). On the other hand, event-driven interaction models (Becker et al., 2004) consider
agents that have individual rewards but can influence each other’s transitions.
More recently, models that allow for limited transition and reward dependence have been
introduced. Examples are interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs with sparse interactions (Melo & Veloso, 2011), distributed POMDPs with coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions with
complex rewards (EDI-CR) (Mostafa & Lesser, 2011), and transition decoupled Dec-POMDPs
(Witwicki & Durfee, 2010; Witwicki, 2011). While the methods developed for these models often exhibit better scaling behavior than methods for standard Dec-(PO)MDPs, they typically
are not suitable when agents have extended interactions, e.g., to collaborate in transporting
an item. Also, there have been specialized models that consider the timing of actions whose
ordering is already determined (Marecki & Tambe, 2007; Beynier & Mouaddib, 2011).
Another body of work addresses infinite-horizon problems (Amato, Bernstein, & Zilberstein, 2010; Amato, Bonet, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein,
2009; Kumar & Zilberstein, 2010a; Pajarinen & Peltonen, 2011), in which it is not possible to
represent a policy as a tree. These approaches represent policies using finite-state controllers
that are then optimized in various ways. Also, since the infinite-horizon case is undecidable
(Bernstein et al., 2002), the approaches are approximate or optimal given a particular controller size. While there exists a boundedly optimal approach that can theoretically construct
a controller within any ǫ of optimal, it is only feasible for very small problems or a large ǫ
(Bernstein et al., 2009).
There has also been great interest in Dec-POMDPs that explicitly take into account communication. Some approaches try to optimize the meaning of communication actions without
semantics (Xuan, Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon,
& Vlassis, 2006; Goldman, Allen, & Zilberstein, 2007) while others use fixed semantics (e.g.,
broadcasting the local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et
al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007;
Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011). Since models used
in the first category (e.g., the Dec-POMDP-Com) can be converted to normal Dec-POMDPs
(Seuken & Zilberstein, 2008), the contributions of this article are applicable to those settings.
Finally, there are numerous models closely related to Dec-POMDPs, such as POSGs
(Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005),
and their graphical counterparts (Doshi, Zeng, & Chen, 2008). These models are more general in the sense that they consider self-interested settings where each agent has an individual
reward function. I-POMDPs are conjectured to also require doubly exponential time (Seuken
& Zilberstein, 2008). However, for the I-POMDP there have been a number of recent advances
(Doshi & Gmytrasiewicz, 2009). The current paper makes a clear link between best-response
equivalence of histories and the notion of best-response equivalence of beliefs in I-POMDPs.
In particular, this article demonstrates that two PE action-observation histories (AOHs) induce, given only a past joint policy, a distribution over states and AOHs of other agents, and
therefore will induce the same multiagent belief for any future policies of other agents. These
induced multiagent beliefs, in turn, can be interpreted as special cases of I-POMDP beliefs
where the model of the other agents are sub-intentional models in the form of a fixed policy
tree. Rabinovich and Rosenschein (2005) introduced a method that, rather than optimizing
494

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

the expected value of a joint policy, selects coordinated actions under uncertainty by tracking
the dynamics of an environment. This approach, however, requires a model of the ideal system
dynamics as input and in many problems, such as those considered in this article, identifying
such dynamics is difficult.

7. Future Work
Several avenues for future work are made possible by the research presented in this article.
Perhaps the most promising is the development of new approximate Dec-POMDP algorithms.
While this article focused on optimal methods, GMAA*-ICE can also be seen as a framework for approximate methods. Such methods could be derived by limiting the amount of
backtracking, employing approximate CBG solvers (Emery-Montemerlo, Gordon, Schneider,
& Thrun, 2004; Kumar & Zilberstein, 2010b; Wu et al., 2010a), integrating GMAA* methods for factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010;
Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011)
or using bounded approximations for the heuristics. In particular, it seems promising to combine approximate clustering with approximate factored GMAA* methods.
Lossy clustering could be achieved by generalizing the probabilistic equivalence criterion,
which is currently so strict that little or no clustering may be possible in many problems. An
obvious approach is to cluster histories for which the distributions over states and histories of
other agents are merely similar, as measured by, e.g., Kullback-Leibler divergence. Alternately,
histories could be clustered if they induce the same individual belief over states:
Pr(s|θ~i ) =

X

Pr(s,~
θ 6=i |θ~i ).

(7.1)

~
θ 6=i

While individual beliefs are not sufficient statistics for history, we hypothesize that they
constitute effective metrics for approximate clustering. Since the individual belief simply
marginalizes out the other agents’ histories from the probabilities used in the probabilistic
equivalence criterion, it is an intuitive heuristic metric for approximate clustering.
While this article focuses on increasing scalability with respect to the horizon, developing
techniques to deal with larger number of agents is an important direction of future work. We
plan to further explore performing GMAA* using factored representations (Oliehoek, Spaan,
Whiteson, & Vlassis, 2008). In that previous work, we could only exploit the factorization at
the last stage, since earlier stages required full expansions to guarantee optimality. However,
for such larger problems, the number of joint BG policies (i.e., number of child nodes) is
directly very large (earlier stages are more tightly coupled); therefore incremental expansion
is crucial to improving the scalability of optimal solution methods with respect to the number
of agents.
Another avenue for future work is to further generalize GMAA*-ICE. In particular, it
may be possible to flatten the two nested A∗ searches into a single A∗ search. Doing so
could lead to significant savings as it would obviate the need to solve an entire CBG before
expanding the next one. In our work, we employed the plain A∗ algorithm as a basis, but a
promising direction of future work is to investigate what A∗ enhancements from the literature
(Edelkamp & Schrödl, 2012) can benefit GMAA* most. In particular, as we described in
our experiments, different past joint policies can lead to CBGs of different sizes. One idea
495

Oliehoek, Spaan, Amato, & Whiteson

is to first expand parts of the search tree that lead to small CBGs, by biasing the selection
operator (but not the pruning operator, so as to maintain optimality).
Yet another important direction for future work is the development of tighter heuristics.
Though few researchers are addressing this topic, the results presented in this article underscore how important such heuristics are for solving larger problems. Currently, the heuristic
is the bottleneck in four out of the seven problems we considered. Moreover, two of the
problems where this is not the bottleneck can already be solved for long (h > 50) horizons.
Therefore, we believe that computing tight heuristics for longer horizons is the single most
important research direction for further improving the scalability of optimal Dec-POMDP
solution methods with respect to the horizon.
A different direction is to employ our theoretical results on clustering beyond the DecPOMDP setting to develop new solution methods for CBGs. For instance, a well-known
method for computing a local optimum is alternating maximization (AM): starting from an
arbitrary joint policy, compute a best response for some agent given that other agents keep
their policies fixed and then select another agent’s policy to improve, etc. One idea is to start
with a ‘completely clustered’ CBG, where all agents’ types are clustered together and thus
a random joint CBG policy has a simple form: each agent just selects a single action. Only
when improving the policy of an agent do we consider all its actual possible types to compute
its best response. Subsequently, we cluster together all types for which that agent selects the
same action and proceed to the next agent. In addition, since our clustering results are not
restricted to the collaborative setting, it may also be possible to employ them, using a similar
approach, to develop new solution methods for general-payoff BGs.
Finally, two of our other contributions can have a significant impact beyond the problem of
optimally solving Dec-POMDPs. First, the idea of incrementally expanding nodes introduced
in GMAA*-ICE can be applied in other A∗ search methods. Incremental expansion is most
useful when children can be generated in order of decreasing heuristic value without prohibitive
computational effort, and in problems with a large branching factor such as multiple sequence
alignment problems in computational biology (Carrillo & Lipman, 1988; Ikeda & Imai, 1999).
Second, representing PWLC value functions as a hybrid of a tree and a set of vectors can have
wider impact as well, e.g., in online search for POMDPs (Ross, Pineau, Paquet, & Chaib-draa,
2008).

8. Conclusions
This article presented a set of methods that advance the state-of-the-art in optimal solution
methods for Dec-POMDPs. In particular, we presented several advances that aim to extend
the horizon over which optimal solutions can be found. These advances build off the GMAA*
heuristic search approach and include lossless incremental clustering of the CBGs solved by
GMAA*, incremental expansion of nodes in the GMAA* search tree, and hybrid heuristic
representations. We provided theoretical guarantees that, when a suitable heuristic is used,
both incremental clustering and incremental expansion yield algorithms that are both complete and search equivalent. Finally, we presented extensive empirical results demonstrating
that GMAA*-ICE can optimally solve Dec-POMDPs of unprecedented size. We significanty
increase the planning horizons that can be tackled—in some cases by more than an order of
magnitude. Given that an increase of the horizon by one results in an exponentially larger
search space, this constitutes a very large improvement. Moreover, our techniques also im496

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

prove scalability with respect to the number of agents, leading to the first ever solutions of
general Dec-POMDPs with more than three agents. These results also demonstrated how
optimal techniques can yield new insights about particular Dec-POMDPs, as incremental
clustering revealed properties of BroadcastChannel that make it much easier to solve. In
addition to facilitating optimal solutions, we hope these advances will inspire new principled
approximation methods, as incremental clustering has already done (Wu et al., 2011), and
enable them to be meaningfully benchmarked.

Acknowledgments
We thank Raghav Aras and Abdeslam Boularias for making their code available to us. Research supported in part by AFOSR MURI project #FA9550-09-1-0538 and in part by NWO
CATCH project #640.005.003. M.S. is funded by the FP7 Marie Curie Actions Individual
Fellowship #275217 (FP7-PEOPLE-2010-IEF).

Appendix A. Appendix
A.1 Auxiliary algorithms
Algorithm 12 implements the BestJointPolicyAndValue function, which prunes all child
nodes that are not fully specified. Algorithm 13 generates all children of a particular CBG.
Algorithm 12 BestJointPolicyAndValue(QExpand ): Prune fully expanded nodes from a set
of nodes QExpand returning only the best one and its value.
Input: QExpand a set of nodes for fully specified joint policies.
Output: the best full joint policy in the input set and its value.
1: v ∗ = −∞
2: for q ∈ QExpand do
3:
QExpand .Remove(q)
4:
hπ, v̂i ← q
5:
if v > v ∗ then
6:
v∗ ← v
7:
π∗ ← π
8:
end if
9: end for
10: return hπ ∗ , v ∗ i

A.2 Detailed GMAA*-ICE algorithm
The complete GMAA*-ICE algorithm is shown in Algorithm 14.
A.3 Computation of V 0...t−1 (ϕt )
The quantity V 0...t−1 (ϕt ) is defined recursively via:
V 0...t−1 (ϕt ) = V 0...t−2 (ϕt−1 ) + Est−1 ,~θt−1 [R(st−1 ,δ t−1 (~θ t−1 )) | b0 , ϕt ].
497

(A.1)

Oliehoek, Spaan, Amato, & Whiteson

Algorithm 13 GenerateAllChildrenForCBG(B(ϕt )).
Input: CBG B(ϕt ).
Output: QExpand the set containing all expanded child nodes for this CBG.
1: QExpand ← {}
2: for all jointP
CBG policies β for B do
3:
Vb (β) ← θ Pr(θ)u(θ,β(θ))
4:
ϕt+1 ← (ϕt , β t )
{create partial joint policy}
t
t+1
0...t−1
t
b
b
5:
V (ϕ ) ← V
(ϕ ) + V (β )
{compute heuristic value}
6:
q ′ ← hϕt+1 , Vb (ϕt+1 )i
{create child node}
7:
QExpand .Insert(q ′ )
8: end for
9: return QExpand

The expectation is taken with respect to the joint probability distribution over states and
joint AOHs that is induced by ϕt :
X
Pr(st ,~θ t |b0 ,ϕt ) =
Pr(ot |at−1 ,st ) Pr(st |st−1 ,at−1 ) Pr(at−1 |ϕt ,~θ t−1 ) Pr(st−1 ,~θ t−1 |b0 ,ϕt ).
st−1 ∈S

(A.2)
Here, ~θ t = (~θ t−1 ,at−1 ,ot ) and Pr(at−1 |ϕt ,~θ t−1 ) is the probability that ϕt specifies at−1 for
AOH ~θ t−1 (which is 0 or 1 in case of deterministic past joint policy ϕt ).
A.4 Proofs
Proof of Theorem 1
Substituting (2.9) in (2.7) yields
X
b ~θ t ,δ t (~θ t ))
Vb (β) = Vb (δ t ) =
Pr(~θ t |b0 ,ϕt )Q(
~
θt

=

X
~
θt



Pr(~θ t |b0 ,ϕt ) Est [R(st ,δ t (~θ t )) | ~θ t ] + E~θt+1 [Vb (~θ t+1 ) | ~θ t , δ t (~θ t )]

= Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + E~θt+1 [Vb (~θ t+1 ) | b0 , ϕt , δ t ]

≥ Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + E~θt+1 [Qπ∗ (~θ t+1 , π ∗ (~θ t+1 )) | b0 , ϕt+1 = (ϕt , δ t )]
= Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + H ∗,t+1...h−1 (ϕt+1 ),

where H ∗ is an optimal admissible heuristic. Substituting this into (2.8) we obtain
Vb (ϕt+1 = (ϕt , δ t )) = V 0...t−1 (ϕt ) + Est ,~θt [R(st ,δ t (~θ t ) | b0 , ϕt ] + E~θt+1 [Vb (~θ t+1 ) | b0 , ϕt , δ t ]
≥ V 0...t−1 (ϕt ) + Est ,~θt [R(st ,δ t (~θ t )) | b0 , ϕt+1 ] + H ∗,t+1...h−1 (ϕt+1 )

{via (A.1)} = V 0...t (ϕt+1 ) + H ∗,t+1...h−1 (ϕt+1 ),
which demonstrates that the heuristic value Vb (ϕt ) used by GMAA* via CBGs using heuristic
of a form (2.9) is admissible, as it is lower bounded by the actual value for the first t plus
an admissible heuristic. Since it performs heuristic search with this admissible heuristic, this
algorithm is also complete.
498

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Algorithm 14 GMAA*-ICE
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:

vGM AA ← −∞
ϕ0 ← ()
v̂ ← +∞
q 0 ← hϕ0 , v̂i
LIE ← {q 0 }
repeat
q ← Select(LIE )
{q = hϕt , v̂i}
IE
L .pop(q)
if IsPlaceholder(q) then
B(ϕt ) ← ϕt .CBG
{reuse stored CBG}
else
{Construct extended BG and solver:}
B(ϕt−1 ) ← ϕt−1 .CBG
{note ϕt = (ϕt−1 , β t−1 )}
t−1
t
t−1
B(ϕ ) ← ConstructExtendedBG(B(ϕ ),β )
B(ϕt ) ← ClusterBG(B(ϕt ))
B(ϕt ).Solver ← CreateSolver(B(ϕt ))
ϕt .CBG ← B(ϕt )
end if
{Expand a single child:}
vCBG = vGM AA − V 0...(t−1) (ϕt )
v̄CBG = +∞
if last stage t = h − 1 then
v̄CBG = Vb (ϕh−1 ) − V 0...(h−2) (ϕh−1 )
end if
hβ t , Vb (β t )i ← B(ϕt ).Solver.NextSolution(vCBG ,v̄CBG )
if not β t then
{fully expanded: no solution s.t. V (β h−1 ) ≥ vCBG }
delete q (and its CBG + solver)
continue
{(i.e. goto line 8)}
end if
ϕt+1 ← (ϕt , β t )
Vb (ϕt+1 ) ← V 0...t−1 (ϕt ) + Vb (β t )
if last stage t = h − 1 then
{Note that π = ϕt+1 , V (π) = Vb (ϕt+1 ) }
if V (π) > vGM AA then
vGM AA ← V (π)
{found new lower bound}
π⋆ ← π
LIE .prune(vGM AA )
end if
delete q (and its CBG + solver)
else
q ′ ← hϕt+1 , Vb (ϕt+1 )i
LIE .insert(q ′ )
q ← hϕt , Vb (ϕt+1 )i
{ Update parent node q, which now is a placeholder }
LIE .insert(q)
end if
until LIE is empty

499

Oliehoek, Spaan, Amato, & Whiteson

Proof of Lemma 1
t
t+1
t
t+1 and ~
Proof. Assume an arbitrary ati ,ot+1
θ 6=i ,at6=i ,ot+1
θ 6=i = (~
i , δ 6=i ,s
6=i )). We have that
t+1

~ a,t t t
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
X
t
t
t+1 t t
t+1
θ 6=i |θ~ia,t )
Pr(ot+1
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
θ 6=i ,δ t6=i ) Pr(st ,~
=
i ,o6=i |ai ,a6=i ,s
st

=

X

t
t
t+1 t t
t+1
θ 6=i |θ~ib,t )
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
θ 6=i ,δ t6=i ) Pr(st ,~
Pr(ot+1
i ,o6=i |ai ,a6=i ,s

st

t+1
~ b,t t t
= Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
t+1

Because we assumed an arbitrary st+1 ,~
θ 6=i ,ot+1
i , we have that
∀st+1 ,~θt+1 ,ot+1
6=i

i

t+1
t+1 ~ t+1 t+1 ~ b,t t t
~ a,t t t
,θ 6=i ,oi |θi ,ai ,δ 6=i )
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i ) = Pr(s

(A.3)

In general we have that
t+1

Pr(s

t+1

t+1
t
,~
θ 6=i |θ~it ,ati ,ot+1
i ,δ 6=i ) =

~t t t
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
Pr(ot+1 |θ~ t ,at ,δ t )
i

=

i

i

6=i

t+1
~t t t
Pr(st+1 ,~
θ 6=i ,ot+1
i |θi ,ai ,δ 6=i )
P
t+1 t+1 t t t
t+1 ,~
~
θ
t+1 Pr(s
6=i ,oi |θi ,ai ,δ 6=i )
t+1
~
s
,θ
6=i

Now, because of (A.3), both the numerator and denominator are the same when substituting
θ~ia,t ,θ~ib,t in this equation. Consequently, we can conclude that
t+1
t
t+1 ~ t+1 ~ b,t t t+1 t
,θ 6=i |θi ,ai ,oi ,δ 6=i )
Pr(st+1 ,~
θ 6=i |θ~ia,t ,ati ,ot+1
i ,δ 6=i ) = Pr(s
t+1

t
t+1 , and ~
Finally, because ati , ot+1
θ 6=i were all arbitrarily chosen, we can conclude that
i , δ 6=i ,s
(3.4) holds.

Proof of Lemma 2
Proof. Assume an arbitrary π 6=i ,s and γ 6=i , then we have
bi (s,γ 6=i |θ~ia , π 6=i ) , Pr(s,γ 6=i |θ~ia , π 6=i ,b0 )
X
=
Pr(s,γ 6=i ,~
θ 6=i |θ~ia ,π 6=i ,b0 )
~
θ 6=i

{factoring the joint distribution}

=

X

Pr(s,~
θ 6=i |θ~ia ,π 6=i ,b0 ) Pr(γ 6=i |s,~
θ 6=i , θ~ia ,π 6=i ,b0 )

~
θ 6=i

500

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

{γ 6=i only depends on ~
θ 6=i ,π 6=i } =

X

Pr(s,~
θ 6=i |θ~ia ,π 6=i ,b0 ) Pr(γ 6=i |~
θ 6=i ,π 6=i )

{ s,~
θ 6=i only depend on ϕ6=i } =

X

Pr(s,~
θ 6=i |θ~ia ,ϕ6=i ,b0 ) Pr(γ 6=i |~
θ 6=i ,π 6=i )

{due to PE} =

X

Pr(s,~
θ 6=i |θ~ib ,ϕ6=i ,b0 ) Pr(γ 6=i |~
θ 6=i ,π 6=i )

~
θ 6=i

~
θ 6=i

~
θ 6=i

= [...] = Pr(s,γ 6=i |θ~ib ,π 6=i ,b0 ) = bi (s,γ 6=i |θ~ib , π 6=i )
We can conclude this holds for all π 6=i ,s and γ 6=i .
Proof of Theorem 5 (Search Equivalence)
To prove search equivalence, we explicitly write a node as a tuple q = hϕt , v̂, PHi, where ϕt
is the past joint policy, v̂ the node’s heuristic value, and PH a boolean indicating whether
it is a placeholder. We consider the equivalence of the maintained open lists. The open list
L maintained by GMAA*-IC contains only non-expanded nodes q. In contrast, the open
list LIE of GMAA*-ICE contains both non-expanded nodes q and placeholders (previously
expanded nodes), q̄. We denote the ordered subset of LIE containing non-expanded nodes
with Q and that containing placeholders with Q̄. We treat these open lists as ordered sets of
heuristic values and their associated nodes.
Definition 11. L and LIE are equivalent, L ≡ LIE if:
1. Q ⊆ L.
2. The q’s have the same ordering: L.remove(L \ Q) = Q.24
3. Nodes q in L but not Q have a placeholder q̄ that is the parent of and higher ranked
than q:
∀q=hϕt ,v̂q ,falsei∈(L\Q)

∃q̄=hϕt−1 ,v̂q̄ ,truei∈Q̄ s.t. (ϕt = (ϕt−1 , β) ∧ q < q̄).

4. There are no other placeholders.
Fig. 11 illustrates two equivalent lists in which the past joint policies are indexed with letters.
Note that the placeholders in LIE are ranked higher than the nodes in L that they represent.
Let us write IT-IC(L) and IT-ICE(LIE ) for one iteration (i.e., one loop of the main repeat
in Algorithm 1) of the respective algorithms. Let IT-ICE* denote the operation that repeats
IT-ICE as long as a placeholder was selected (so it ends when a q is expanded).
Lemma 4. If L ≡ LIE , then executing IT-IC(L) and IT-ICE*(LIE ) leads to new open lists
that are again equivalent: L ′ ≡ LIE′ .
Proof. When IT-ICE* selects a placeholder q̄, it generates child q ′ that was already present
in L (due to properties 3 and 4 of Definition 11) and inserts it. Insertion occurs at the
same relative location as IT-IC because both algorithms use the same comparison operator
(Definition 5). Together these facts guarantee that the insertion preserves properties 1 and 2.
24. A.remove(B) removes the elements of B from A without changing A’s ordering.

501

Oliehoek, Spaan, Amato, & Whiteson

LIE

L
Q
Vb

ϕt

7
5
4.5

c
d
e

3
3
2.5
1
0.5

f
g
h
i
j

Q̄

Vb

ϕt

5

d

3
3

f
g

Vb
8

4

ϕt
a

b

←

placeholder for {c,e,j}

←

same nodes: same position

←
o

placeholder for {h,i}
consistent ordering
for equal values

Figure 11: Illustration of equivalent lists. Past joint policies are indexed by letters. In this
example, a and b have been expanded earlier (but are not yet fully expanded in the ICE-case).
If there are remaining unexpanded children of q̄, IT-ICE* reinserts q̄ with an updated heuristic
value q̄.v̂ ← q ′ .v̂ that is guaranteed to be an upper bound on the value of unexpanded siblings
q ′′ since q ′ .v̂ = Vb (q ′ .ϕ) ≥ Vb (q ′′ .ϕ) = q ′′ .v̂ (preserving properties 3 and 4).
When IT-ICE* finally selects a non-placeholder q, it is guaranteed to be the same q as
selected by IT-IC (due to properties 1 and 2). Expansion in ICE generates one child q ′ (again
inserted at the same relative location as in IC) and inserts placeholder q̄ = hq.ϕ, q ′ .v̂, truei for
the other siblings q ′′ (again preserving properties 3 and 4).
Proof of Theorem 5. The fact that GMAA*-ICE and GMAA*-IC are search-equivalent follows directly from Lemma 4. Search equivalence means that both algorithms select the same
non-placeholders q to expand. Since both algorithms begin with identical (and therefore trivially equivalent) open lists, they maintain equivalent open lists throughout search. As such,
property 2 of Definition 11 ensures that every time IT-ICE* selects a non-placeholder, IT-IC
selects it too.

References
Allen, M., & Zilberstein, S. (2007). Agent influence as a predictor of difficulty for decentralized
problem-solving. In Proceedings of the Twenty-Second AAAI Conference on Artificial
Intelligence.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllers for
decentralized POMDPs. In Proc. of the AAMAS Workshop on Multi-Agent Sequential
Decision Making in Uncertain Domains.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007). Optimizing memory-bounded controllers
for decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllers for POMDPs and decentralized POMDPs. Autonomous Agents and Multi-Agent
Systems, 21 (3), 293–320.
502

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Amato, C., Bonet, B., & Zilberstein, S. (2010). Finite-state controllers based on Mealy
machines for centralized and decentralized POMDPs. In Proceedings of the TwentyFourth AAAI Conference on Artificial Intelligence.
Amato, C., Carlin, A., & Zilberstein, S. (2007). Bounded dynamic programming for decentralized POMDPs. In Proc. of the AAMAS Workshop on Multi-Agent Sequential
Decision Making in Uncertain Domains.
Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generation for
finite-horizon DEC-POMDPs. In Proc. of the International Conference on Automated
Planning and Scheduling.
Aras, R., & Dutech, A. (2010). An investigation into mathematical programming for finite
horizon decentralized POMDPs. Journal of Artificial Intelligence Research, 37 , 329–
396.
Becker, R., Carlin, A., Lesser, V., & Zilberstein, S. (2009). Analyzing myopic approaches for
multi-agent communication. Computational Intelligence, 25 (1), 31–50.
Becker, R., Zilberstein, S., & Lesser, V. (2004). Decentralized Markov decision processes with
event-driven interactions. In Proc. of the International Conference on Autonomous
Agents and Multi Agent Systems.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems.
Bernstein, D. S., Amato, C., Hansen, E. A., & Zilberstein, S. (2009). Policy iteration for
decentralized control of Markov decision processes. Journal of Artificial Intelligence
Research, 34 , 89–132.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity of
decentralized control of Markov decision processes. Mathematics of Operations Research,
27 (4), 819–840.
Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control (3rd ed., Vol. I). Athena
Scientific.
Beynier, A., & Mouaddib, A.-I. (2011). Solving efficiently decentralized MDPs with temporal
and resource constraints. Autonomous Agents and Multi-Agent Systems, 23 (3), 486–
539.
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming for decentralized
POMDPs with lossless policy compression. In Proc. of the International Conference on
Automated Planning and Scheduling.
Buşoniu, L., Babuška, R., & De Schutter, B. (2008). A comprehensive survey of multi-agent
reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C:
Applications and Reviews, 38 (2), 156–172.
Carlin, A., & Zilberstein, S. (2008). Value-based observation compression for DEC-POMDPs.
In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems.
503

Oliehoek, Spaan, Amato, & Whiteson

Carrillo, H., & Lipman, D. (1988). The multiple sequence alignment problem in biology.
SIAM Journal on Applied Mathematics, 48 (5), 1073–1082.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: A simple, fast,
exact method for partially observable Markov decision processes. In Proc. of Uncertainty
in Artificial Intelligence.
Cassandra, A. R. (1998). Exact and Approximate Algorithms for Partially Observable Markov
Decision Processes. Unpublished doctoral dissertation, Brown University.
Dechter, R., Flerova, N., & Marinescu, R. (2012). Search algorithms for m best solutions for
graphical models. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial
Intelligence.
Dekel, E., Fudenberg, D., & Morris, S. (2006). Topologies on types. Theoretical Economics,
1 (3), 275–309.
Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient errorbounded solutions for transition independent decentralized MDPs. In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems. (Submitted
for publication)
Dibangoye, J. S., Mouaddib, A.-I., & Chai-draa, B. (2009). Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs. In Proc. of the International
Conference on Autonomous Agents and Multi Agent Systems.
Doshi, P., & Gmytrasiewicz, P. (2009). Monte Carlo sampling methods for approximating
interactive POMDPs. Journal of Artificial Intelligence Research, 34 , 297–337.
Doshi, P., Zeng, Y., & Chen, Q. (2008). Graphical models for interactive POMDPs: representations and solutions. Autonomous Agents and Multi-Agent Systems, 18 (3), 376–416.
Edelkamp, S., & Schrödl, S. (2012). Heuristic search: theory and applications. Morgan
Kaufmann.
Eker, B., & Akın, H. L. (2010). Using evolution strategies to solve DEC-POMDP problems.
Soft Computing—A Fusion of Foundations, Methodologies and Applications, 14 (1), 35–
47.
Eker, B., & Akın, H. L. (2013). Solving decentralized POMDP problems using genetic
algorithms. Autonomous Agents and Multi-Agent Systems, 27 (1), 161–196.
Emery-Montemerlo, R. (2005). Game-Theoretic Control for Robot Teams. Unpublished
doctoral dissertation, Carnegie Mellon University.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions for partially observable stochastic games with common payoffs. In Proc. of the
International Conference on Autonomous Agents and Multi Agent Systems.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoretic
control for robot teams. In Proc. of the IEEE International Conference on Robotics and
Automation.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in
Markov decision processes. Artificial Intelligence, 14 (1–2), 163–223.
504

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Gmytrasiewicz, P. J., & Doshi, P. (2005). A framework for sequential planning in multi-agent
settings. Journal of Artificial Intelligence Research, 24 , 49–79.
Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning to communicate in a decentralized environment. Autonomous Agents and Multi-Agent Systems, 15 (1), 47–90.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange in cooperative
multi-agent systems. In Proc. of the International Conference on Autonomous Agents
and Multi Agent Systems.
Goldman, C. V., & Zilberstein, S. (2004). Decentralized control of cooperative systems:
Categorization and complexity analysis. Journal of Artificial Intelligence Research, 22 ,
143–174.
Goldman, C. V., & Zilberstein, S. (2008). Communication-based decomposition mechanisms
for decentralized MDPs. Journal of Artificial Intelligence Research, 32 , 169–202.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming for partially observable stochastic games. In Proc. of the National Conference on Artificial
Intelligence.
Hauskrecht, M. (2000). Value-function approximations for partially observable Markov decision processes. Journal of Artificial Intelligence Research, 13 , 33–94.
Hsu, K., & Marcus, S. (1982). Decentralized control of finite state Markov processes. IEEE
Transactions on Automatic Control , 27 (2), 426–431.
Huhns, M. N. (Ed.). (1987). Distributed Artificial Intelligence. Pitman Publishing Ltd.
Ikeda, T., & Imai, H. (1999). Enhanced A* algorithms for multiple alignments: optimal
alignments for several sequences and k-opt approximate alignments for large cases. Theoretical Computer Science, 210 (2), 341–374.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning and acting in partially
observable stochastic domains. Artificial Intelligence, 101 (1-2), 99–134.
Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming for decentralized
POMDPs with structured interactions. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems.
Kumar, A., & Zilberstein, S. (2010a). Anytime planning for decentralized POMDPs using
expectation maximization. In Proc. of Uncertainty in Artificial Intelligence.
Kumar, A., & Zilberstein, S. (2010b). Point-based backup for decentralized POMDPs: Complexity and new algorithms. In Proc. of the International Conference on Autonomous
Agents and Multi Agent Systems.
Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies for partially observable environments: Scaling up. In Proc. of the International Conference on Machine
Learning.
Marecki, J., Gupta, T., Varakantham, P., Tambe, M., & Yokoo, M. (2008). Not all agents are
equal: scaling up distributed POMDPs for agent networks. In Proc. of the International
Conference on Autonomous Agents and Multi Agent Systems.
505

Oliehoek, Spaan, Amato, & Whiteson

Marecki, J., & Tambe, M. (2007). On opportunistic techniques for solving decentralized
Markov decision processes with temporal constraints. In Proc. of the International
Conference on Autonomous Agents and Multi Agent Systems.
Melo, F. S., & Veloso, M. (2011). Decentralized MDPs with sparse interactions. Artificial
Intelligence, 175 (11), 1757–1789.
Mostafa, H., & Lesser, V. (2011). A compact mathematical formulation for problems with
structured agent interactions. In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains.
Nair, R., Roth, M., & Yohoo, M. (2004). Communication for improving policy computation in
distributed POMDPs. In Proc. of the International Conference on Autonomous Agents
and Multi Agent Systems.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralized POMDPs: Towards efficient policy computation for multiagent settings. In Proc.
of the International Joint Conference on Artificial Intelligence.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs:
A synthesis of distributed constraint optimization and POMDPs. In Proc. of the National Conference on Artificial Intelligence.
Oliehoek, F. A. (2010). Value-Based Planning for Teams of Agents in Stochastic Partially Observable Environments. Amsterdam University Press. (Doctoral dissertation, University
of Amsterdam)
Oliehoek, F. A. (2012). Decentralized POMDPs. In M. Wiering & M. van Otterlo (Eds.),
Reinforcement learning: State of the art (Vol. 12). Springer Berlin Heidelberg.
Oliehoek, F. A., Kooi, J. F., & Vlassis, N. (2008). The cross-entropy method for policy search
in decentralized POMDPs. Informatica, 32 , 341–357.
Oliehoek, F. A., & Spaan, M. T. J. (2012). Tree-based solution methods for multiagent
POMDPs with delayed communication. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence.
Oliehoek, F. A., Spaan, M. T. J., Dibangoye, J., & Amato, C. (2010). Heuristic search for identical payoff Bayesian games. In Proc. of the International Conference on Autonomous
Agents and Multi Agent Systems.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007). Dec-POMDPs with delayed communication. In Proc. of the AAMAS Workshop on Multi-Agent Sequential Decision Making
in Uncertain Domains.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2008). Optimal and approximate Q-value
functions for decentralized POMDPs. Journal of Artificial Intelligence Research, 32 ,
289–353.
Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting locality
of interaction in factored Dec-POMDPs. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems.
Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions for decentralized POMDPs. In Proc.
of the International Conference on Autonomous Agents and Multi Agent Systems.
506

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering of histories
in decentralized POMDPs. In Proc. of the International Conference on Autonomous
Agents and Multi Agent Systems.
Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2013). Approximate solutions for factored Dec-POMDPs with many agents. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems. (Submitted for publication)
Oliehoek, F. A., Witwicki, S., & Kaelbling, L. P. (2012). Influence-based abstraction for
multiagent systems. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial
Intelligence.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control of a multiple access broadcast
channel: Performance bounds. In Proc. of the 35th conference on decision and control.
Osborne, M. J., & Rubinstein, A. (1994). A course in game theory. The MIT Press.
Pajarinen, J., & Peltonen, J. (2011). Efficient planning for factored infinite-horizon DECPOMDPs. In Proc. of the International Joint Conference on Artificial Intelligence.
Panait, L., & Luke, S. (2005). Cooperative multi-agent learning: The state of the art.
Autonomous Agents and Multi-Agent Systems, 11 (3), 387–434.
Puterman, M. L. (1994). Markov Decision Processes—Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Pynadath, D. V., & Marsella, S. C. (2007). Minimal mental models. In Proceedings of the
Twenty-Second AAAI Conference on Artificial Intelligence.
Pynadath, D. V., & Tambe, M. (2002). The communicative multiagent team decision problem:
Analyzing teamwork theories and models. Journal of Artificial Intelligence Research,
16 , 389–423.
Rabinovich, Z., Goldman, C. V., & Rosenschein, J. S. (2003). The complexity of multiagent
systems: the price of silence. In Proc. of the International Conference on Autonomous
Agents and Multi Agent Systems.
Rabinovich, Z., & Rosenschein, J. S. (2005). Multiagent coordination by extended Markov
tracking. In Proc. of the International Conference on Autonomous Agents and Multi
Agent Systems.
Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms for
POMDPs. Journal of Artificial Intelligence Research, 32 , 664–704.
Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning about joint beliefs for executiontime communication decisions. In Proc. of the International Conference on Autonomous
Agents and Multi Agent Systems.
Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations for decentralized execution in multi-agent teams. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming for
decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
507

Oliehoek, Spaan, Amato, & Whiteson

Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming for DECPOMDPs. In Proc. of the International Joint Conference on Artificial Intelligence.
Seuken, S., & Zilberstein, S. (2008). Formal models and algorithms for decentralized decision
making under uncertainty. Autonomous Agents and Multi-Agent Systems, 17 (2), 190–
250.
Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning under uncertainty for teams of communicating agents. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems.
Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games for decentralized
multiagent planning under uncertainty. In Proc. of the International Conference on
Autonomous Agents and Multi Agent Systems.
Spaan, M. T. J., & Oliehoek, F. A. (2008). The MultiAgent Decision Process toolbox:
software for decision-theoretic planning in multiagent systems. In Proc. of the AAMAS
Workshop on Multi-Agent Sequential Decision Making in Uncertain Domains.
Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling up optimal heuristic search in
Dec-POMDPs via incremental expansion. In Proc. of the International Joint Conference
on Artificial Intelligence.
Spaan, M. T. J., Oliehoek, F. A., & Vlassis, N. (2008). Multiagent planning under uncertainty
with stochastic communication delays. In Proc. of the International Conference on
Automated Planning and Scheduling.
Sycara, K. P. (1998). Multiagent systems. AI Magazine, 19 (2), 79–92.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: A heuristic search algorithm for
solving decentralized POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
Tsitsiklis, J., & Athans, M. (1985). On the complexity of decentralized decision making and
detection problems. IEEE Transactions on Automatic Control , 30 (5), 440–446.
Varaiya, P., & Walrand, J. (1978). On delayed sharing patterns. IEEE Transactions on
Automatic Control , 23 (3), 443–445.
Varakantham, P., Kwak, J. young, Taylor, M. E., Marecki, J., Scerri, P., & Tambe, M. (2009).
Exploiting coordination locales in distributed POMDPs via social model shaping. In
Proc. of the International Conference on Automated Planning and Scheduling.
Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting loose a
SPIDER on a network of POMDPs: Generating quality guaranteed policies. In Proc.
of the International Conference on Autonomous Agents and Multi Agent Systems.
Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back the cup for distributed POMDPs: planning over continuous belief spaces. In Proc. of the International
Conference on Autonomous Agents and Multi Agent Systems.
Velagapudi, P., Varakantham, P., Scerri, P., & Sycara, K. (2011). Distributed model shaping
for scaling to decentralized POMDPs with hundreds of agents. In Proc. of the International Conference on Autonomous Agents and Multi Agent Systems.
Vlassis, N. (2007). A Concise Introduction to Multiagent Systems and Distributed Artificial
Intelligence. Morgan & Claypool Publishers.
508

Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs

Williamson, S. A., Gerding, E. H., & Jennings, N. R. (2009). Reward shaping for valuing communications during multi-agent coordination. In Proc. of the International Conference
on Autonomous Agents and Multi Agent Systems.
Witwicki, S. J. (2011). Abstracting Influences for Efficient Multiagent Coordination Under
Uncertainty. Unpublished doctoral dissertation, University of Michigan, Ann Arbor,
Michigan, USA.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction for weakly-coupled
Dec-POMDPs. In Proc. of the International Conference on Automated Planning and
Scheduling.
Wu, F., Zilberstein, S., & Chen, X. (2010a). Point-based policy generation for decentralized
POMDPs. In Proc. of the International Conference on Autonomous Agents and Multi
Agent Systems.
Wu, F., Zilberstein, S., & Chen, X. (2010b). Rollout sampling policy iteration for decentralized
POMDPs. In Proc. of Uncertainty in Artificial Intelligence.
Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning for multi-agent systems with
bounded communication. Artificial Intelligence, 175 (2), 487–511.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions in multi-agent cooperation: Model and experiments. In Proc. of the International Conference on Autonomous
Agents.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* with partial expansion for large branching
factor problems. In Proc. of the National Conference on Artificial Intelligence.
Zeng, Y., & Doshi, P. (2012). Exploiting model equivalences for solving interactive dynamic
influence diagrams. Journal of Artificial Intelligence Research, 43 , 211–255.
Zeng, Y., Doshi, P., Pan, Y., Mao, H., Chandrasekaran, M., & Luo, J. (2011). Utilizing
partial policies for identifying equivalence of behavioral models. In Proceedings of the
Twenty-Fifth AAAI Conference on Artificial Intelligence.

509

Journal of Artificial Intelligence Research 46 (2013) 263–302

Submitted 07/12; published 03/13

Parameterized Complexity Results for
Exact Bayesian Network Structure Learning
Sebastian Ordyniak

ordyniak@fi.muni.cz

Masaryk University Brno, Czech Republic

Stefan Szeider

stefan@szeider.net

Vienna University of Technology, Austria

Abstract
Bayesian network structure learning is the notoriously difficult problem of discovering a Bayesian network that optimally represents a given set of training data. In this
paper we study the computational worst-case complexity of exact Bayesian network structure learning under graph theoretic restrictions on the (directed) super-structure. The
super-structure is an undirected graph that contains as subgraphs the skeletons of solution
networks. We introduce the directed super-structure as a natural generalization of its undirected counterpart. Our results apply to several variants of score-based Bayesian network
structure learning where the score of a network decomposes into local scores of its nodes.
Results: We show that exact Bayesian network structure learning can be carried out in
non-uniform polynomial time if the super-structure has bounded treewidth, and in linear
time if in addition the super-structure has bounded maximum degree. Furthermore, we
show that if the directed super-structure is acyclic, then exact Bayesian network structure learning can be carried out in quadratic time. We complement these positive results
with a number of hardness results. We show that both restrictions (treewidth and degree)
are essential and cannot be dropped without loosing uniform polynomial time tractability
(subject to a complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard for “almost acyclic” directed super-structures. Furthermore,
we show that the restrictions remain essential if we do not search for a globally optimal
network but aim to improve a given network by means of at most k arc additions, arc
deletions, or arc reversals (k-neighborhood local search).

1. Introduction
Bayesian Network Structure Learning (BNSL) is the important task of discovering a Bayesian
network that represents a given set of training data. Unfortunately, solving the problem
optimally (Exact BNSL) is NP-complete (Chickering, 1996). A common and widely used
approach to overcome this complexity barrier is to exploit the structure of the problem.
This has also been a popular direction for BNSL and two main kinds of structural restrictions have been studied so far, i.e., (1) restrictions on the probability distribution
generating the input and (2) restrictions on the resulting Bayesian network. With the help
of these restrictions several tractable classes of BNSL have been identified. BNSL is solvable in non-uniform polynomial time if the distribution generating the input has bounded
treewidth (Narasimhan & Bilmes, 2004) or bounded degree (Pieter, Daphne, & Andrew,
2006) and it is solvable in (non-uniform) polynomial time if the resulting BN is a branching (Chow & Liu, 1968) or close to being a branching (Gaspers, Koivisto, Liedloff, Ordyniak,
c
2013
AI Access Foundation. All rights reserved.

Ordyniak & Szeider

& Szeider, 2012). These positive results are contrasted by a series of negative results for
the above mentioned restrictions, e.g., BNSL is known to be NP-hard if the resulting BN is
a polytree (Dasgupta, 1999) or a directed path (Meek, 2001). Recently, a novel approach
to restrict the structure of BNSL has been introduced (Tsamardinos, Brown, & Aliferis,
2006; Perrier, Imoto, & Miyano, 2008). Here a so-called super-structure, an undirected
graph on the same nodes as the resulting BN, is used to restrict the search space of BNSL
in advance. After the super-structure has been obtained, usually using an IT-based approach (Tsamardinos et al., 2006), one looks for solution networks whose skeletons are
contained in the super-structure. It hence becomes important that the super-structure is
sound, i.e., contains at least one optimal solution.
There are two main questions concerning the super-structure: First, how can a suitable and sound super-structure be obtained efficiently? And secondly, once such a superstructure is obtained, how can it be used to guide the search for an optimal solution? The
goal of this paper is to provide a theoretical analysis of the latter question, considering
super-structures that arise from a given local score function adapting the model of Parviainen and Koivisto (2010). We consider various combinations of restrictions in a systematic
way that allows us draw a broader picture of the complexity landscape of BNSL. We hope
that our analysis can help to understand the boundaries between tractable and intractable
cases of this important problem. Furthermore, we think that our results provide new insights that can help the search for more efficient and accurate heuristics. For our analysis,
we use the theoretical framework of parameterized complexity (Downey & Fellows, 1999)
which seems to be well suited for investigating the complexity of BNSL as it allows to take
structural properties (in terms of parameters) into account. To the best of our knowledge
parameterized complexity theory has not been employed in this context before.
1.1 Results
In this section we give a brief overview of our results.
1.1.1 Exact BNSL Using the Super-structure
In the first part of our paper we study the worst-case complexity of Exact BNSL under
graph-theoretic restrictions on the super-structure. One of the most prominent restrictions
on the super-structure that we consider is treewidth. Treewidth is an important and widely
used parameter that measures the similarity of a graph to a tree (Bodlaender, 1993, 1997,
2005; Greco & Scarcello, 2010). Similarly as for trees, many otherwise intractable problems
become tractable on graphs of bounded treewidth. More importantly, treewidth has already
been successfully applied in the context of Bayesian Reasoning (Darwiche, 2001; Dechter,
1999; Kwisthout, Bodlaender, & van der Gaag, 2010). It hence seems only natural to apply
treewidth to (Exact) BNSL.
Our results are as follows:
(1) Exact BNSL is solvable in non-uniform polynomial time if the treewidth of the superstructure is bounded by an arbitrary constant.
(2) Exact BNSL is solvable in linear time if both treewidth and maximum degree of the
super-structure are bounded by arbitrary constants.
264

Parameterized Complexity of Exact BNSL

By “non-uniform” we mean that the order of the polynomial depends on the treewidth.
We obtain results (1) and (2) by means of a dynamic programming algorithm along a
decomposition tree of the super-structure.
We show that—in a certain sense—both results are optimal:
(3) Exact BNSL for instances with super-structures of maximum degree 4 (but unbounded
treewidth) is not solvable in polynomial time unless P = NP. Thus, in (1) and (2) we
cannot drop the bound on the treewidth.
(4) Exact BNSL for instances with super-structures of bounded treewidth (but unbounded
maximum degree) is not solvable in uniform polynomial time unless FPT = W[1].
Thus, in (2) we cannot drop the bound on the degree.
FPT 6= W[1] is a widely accepted complexity theoretic assumption (Downey & Fellows,
1999) that is often considered as the parameterized analog to P 6= NP. We will provide
necessary background on parameterized complexity and fpt-reductions in Section 2.2.
1.1.2 Local Search BNSL Using the Super-structure
Since learning an optimal Bayesian network is computationally hard heuristic methods are
used in practice. A popular heuristic for BNSL is the so-called hill climbing procedure, or
local search. In particular, a highly competitive algorithm for learning large Bayesian networks (MMHC) uses local search to find an optimal solution inside a previously constructed
super-structure (Tsamardinos et al., 2006). We study the worst case complexity of a wellknown generalization of local search, the k-Neighborhood Local Search (or k-Local Search
for short). In this variant of local search one is allowed to modify not one but up to k arcs
in every step of the search. Hence by adjusting k, one is able to balance speed and accuracy. However, because the k-local search space is of order nO(k) , applying k-local search is
especially desirable for problems where the running time does only increase modestly with
respect to k. Similarly to result (2) we are able to show the following:
(5) k-Local Search BNSL is solvable in linear time if both the treewidth and the maximum
degree of the super-structure are bounded by arbitrary constants.
Clearly, this result is only of minor interest because we can already solve the Exact BNSL
problem under the same restrictions on the super-structure (see result (2)). However, in
contrast to the Exact BNSL problem one might be able to drop one of these restrictions
without losing uniform polynomial-time tractability. We show that this is again unlikely.
(6) k-Local Search BNSL for instances with super-structures that have either bounded
treewidth or bounded maximum degree is not solvable in uniform polynomial time
unless FPT = W[1].

1.1.3 Exact BNSL Using the Directed Super-structure
So far, one has only considered the super-structure as an undirected graph. We introduce the
directed super-structure as a more expressive way to restrict the search space of solutions,
i.e., once the directed super-structure has been fixed we restrict our search to solutions whose
265

Ordyniak & Szeider

Problem: Exact BNSL

Complexity

Result

Restrictions on Super-Structure
bounded treewidth
bounded treewidth

bounded max degree
–

linear time
in XP, W[1]-hard

–

bounded max degree

NP-hard

Corollary 1
Corollary 1,
Theorem 3
Theorem 2

poly-time
NP-hard

Corollary 2
Theorem 7

Complexity

Result

linear time
in XP, W[1]-hard
in XP, W[1]-hard

Proposition 6
Theorem 5
Theorem 6

in FPT
in FPT

Theorem 4
Theorem 4

Restrictions on Directed Super-Structure
acyclic
almost acyclic

–
bounded max degree

Problem: k-Local Search BNSL
Restrictions on Super-Structure
bounded treewidth
bounded treewidth
–

bounded max degree
–
bounded max degree

Restrictions on Operations
only arc deletion
only arc addition

Figure 1: Overview of complexity results. The complexity classes FPT and XP contain all
problems solvable in uniform polynomial-time and non-uniform polynomial-time,
respectively.

266

Parameterized Complexity of Exact BNSL

networks are contained in the directed super-structure. Again, we study the complexity of
Exact BNSL with respect to certain restrictions on the directed super-structure and obtain
the following result.
(7) Exact BNSL is solvable in quadratic time if the directed super-structure is acyclic.
The question arises whether we can extend this result by using a more general form of
acyclicity. We show, however, that this is not the case.
(8) Exact BNSL is NP-hard for directed super-structures that can be made acyclic by
deleting one node. Hardness even holds if the maximum in-degree and the maximum
out-degree of the directed super-structure are both bounded by 3.
A systematic overview of our results (1)–(8) is given in Figure 1.
1.2 Related Work
In this section we present relevant related work on BNSL. Further related work can be
found in the respective sections, i.e., we present related work on parameterized complexity
in Section 2.2, related work on treewidth and tree decompositions in Sections 2.3 and 3.1,
and in Section 7 we present related work on k-neighborhood local search.
1.2.1 Algorithms for Exact BNSL
To this date only a handful of exact algorithms for BNSL have been proposed. These can be
split into three groups: (A) exact algorithms that do not employ any restrictions (Parviainen
& Koivisto, 2010; Koivisto, 2006; Yuan, Malone, & Wu, 2011; Ott, Imoto, & Miyano, 2004;
Silander & Myllymäki, 2006; Yuan et al., 2011), (B) exact algorithms using restrictions
on the generating or target distribution (Pieter et al., 2006; Chechetka & Guestrin, 2007;
Friedman, Nachman, & Pe’er, 1999; Chow & Liu, 1968; Gaspers, Koivisto et al., 2012),
and (C) exact algorithms that use restrictions on the undirected super-structure (Friedman
et al., 1999; Kojima, Perrier, Imoto, & Miyano, 2010). Algorithms falling into group (A)
are only suited for small to medium sized Bayesian networks because they do not restrict
the search space in any way. On the other hand, the restrictions used by algorithms in
group (B) are more general than restrictions coming from the undirected super-structure
and up to now only non-uniform polynomial-time algorithms could be obtained using these
restrictions. To the best of our knowledge this paper is the first to employ an in-depth
theoretical analysis of the parameterized complexity of BNSL using restrictions on the
undirected super-structure. We obtain the first algorithm for exact BNSL with a uniform
polynomial running-time with respect to structural restriction on the undirected superstructure. A similar approach has been taken by Kojima et al. (2010), where the authors
propose an algorithm for exact BNSL that uses a “cluster-decomposition” of the undirected
super-structure. Even though their practical results are quite promising the authors provide
no theoretical analysis of the worst-case complexity of their algorithm beyond the trivial
bound that also applies to exact algorithms in group (A). Apart from exact algorithms there
also exists a variety of approximation algorithms using tree decomposition or degree-based
techniques (Pieter et al., 2006; Elidan & Gould, 2008; Karger & Srebro, 2001).
267

Ordyniak & Szeider

1.2.2 Hardness Results for Exact BNSL
There are also a number of hardness results for BNSL under restrictions of the resulting
Bayesian network. In particular, BNSL remains NP-hard if the in-degree of the resulting
Bayesian network is bounded by 2 (Chickering, 1996), and if the resulting Bayesian network
is a poly tree (Dasgupta, 1999), or a directed path (Meek, 2001). However, to the best of our
knowledge, no negative results for BNSL under restrictions of the (directed) super-structure
have been obtained.
1.3 Organization and Prior Work
This paper is organized as follows: In Section 2 we introduce the basic concepts and notions
that we use throughout the paper. We introduce the main object of our study (BNSL)
in Section 3. Section 4 shows how to use a dynamic programming algorithm on a tree
decomposition in order to show results (1) and (2). We provide a refined complexity analysis
of our algorithm in Section 5. In Section 6 we show the complexity boundaries for Exact
BNSL using a super-structure, i.e., we obtain results (3) and (4). We introduce k-Local
Search BNSL in Section 7 where we establish results (5) and (6). We introduce the directed
super-structure in Section 8 and show results (7) and (8). We conclude in Section 9. An
appendix contains the proofs of some technical claims.
A preliminary and shortened version of this paper appeared in the proceedings of UAI
2010. Apart from providing a higher level of detail and readability by giving more examples
and detailed proofs, the paper extends its previous version in four ways: by adding a
section on related work (Section 1.2), by providing a refined complexity analysis of our
main algorithmic result (Section 5), by providing a novel proof of Theorem 6 that allows
us to decrease the upper bound on the maximum degree of the super-structure from 5 to 3,
and by introducing the directed super-structure (Section 8).

2. Preliminaries
In this section we will introduce the basic concepts and notions that we will use throughout
the paper.
2.1 Basic Graph Theory
We will assume that the reader is familiar with basic graph theory (see, e.g., Diestel,
2000; Bang-Jensen & Gutin, 2009). We consider undirected graphs and directed graphs
(digraphs). A dag is a directed acyclic graph. We write V (G) = V and E(G) = E for the
sets of nodes and edges of a (directed or undirected) graph G = (V, E), respectively. We
denote an undirected edge between nodes u and v as {u, v} and a directed edge (or arc),
directed from u to v as (u, v). We write NG (v) for the set of neighbors of a node v ∈ V in G,
i.e., NG (v) = { u : (v, u) ∈ E or (u, v) ∈ E } if G is directed and NG (v) = { u : {u, v} ∈ E }
if G is undirected. For a subset V 0 ⊆ V we write G[V 0 ] to denote the induced subgraph G0 =
(V 0 , E 0 ) where E 0 = { e ⊆ V 0 : e ∈ E } if G is undirected and E 0 = { e ∈ V 0 × V 0 : e ∈ E } if
G is directed. If G is a digraph we define PG (v) = { u ∈ V (G) : (u, v) ∈ E(G) } as the set of
parents of v in G. Furthermore, for two directed graphs D1 and D2 we define D1 ∪D2 as the
268

Parameterized Complexity of Exact BNSL

union of D1 and D2 , i.e., V (D1 ∪ D2 ) = V (D1 ) ∪ V (D2 ) and E(D1 ∪ D2 ) = E(D1 ) ∪ E(D2 ).
Let G be a (directed or undirected) graph and e ∈ E(G) a directed or undirected edge in G.
We denote by G − e the (undirected or directed) graph, where G − e = (V (G), E(G) \ {e}).
Furthermore, for a subset X ⊆ V (G) we denote by G − X the graph induced by the nodes
in V \ X. If X contains only one node v we also write G − v instead of G − {v}. We call
an undirected graph G0 = (V 0 , E 0 ) the skeleton of a directed graph G if V 0 = V (G) and
E 0 = { {u, v} : (u, v) ∈ E(G) }.
2.2 Parameterized Complexity
Parameterized complexity provides a theoretical framework to distinguish between uniform
and non-uniform polynomial-time tractability with respect to a parameter. It has been
introduced and pioneered by Downey and Fellows (1999) and is receiving growing interest
as reflected by the recent publication of two further monographs (Flum & Grohe, 2006;
Niedermeier, 2006) and hundreds of research papers (see references in the above mentioned
monographs). In 2008 the Computer Journal has devoted two special issues on parameterized complexity in order to make the key methods and ideas known to a wide range of
computer scientists (Downey, Fellows, & Langston, 2008).
An instance of a parameterized problem is a pair (I, k) where I is the main part and k
is the parameter ; the latter is usually a non-negative integer. A parameterized problem
is fixed-parameter tractable if there exist a computable function f and a constant c such
that instances (I, k) of size n can be solved in time O(f (k)nc ). FPT is the class of all
fixed-parameter tractable decision problems. Fixed-parameter tractable problems are also
called uniform polynomial-time tractable because if k is considered constant, then instances
with parameter k can be solved in polynomial time where the order of the polynomial is
independent of k (in contrast to non-uniform polynomial-time running times such as nk ).
Parameterized complexity offers a completeness theory similar to the theory of NPcompleteness. One uses fpt-reductions which are many-one reductions where the parameter
for one problem maps into the parameter for the other. More specifically, problem L
reduces to problem L0 if there is a mapping R from instances of L to instances of L0
such that (i) (I, k) is a yes-instance of L if and only if (I 0 , k 0 ) = R(I, k) is a yes-instance
of L0 , (ii) k 0 ≤ g(k) for a computable function g, and (iii) R can be computed in time
O(f (k)nc ) where f is a computable function, c is a constant, and n denotes the size of
(I, k). The parameterized complexity class W[1] is considered as the parameterized analog
to NP. In particular, FPT = W[1] implies the (unlikely) existence of a 2o(n) algorithm for
n-variable 3SAT (Impagliazzo, Paturi, & Zane, 2001; Flum & Grohe, 2006). An example, for
a parameterized problem that is W[1]-complete under fpt-reductions is the parameterized
Maximum Clique problem (given a graph G and a parameter k ≥ 0, does G contain a
complete subgraph on k nodes?). Note that there exists a trivial non-uniform polynomialtime nk algorithm for the Maximum Clique problems that checks all sets of k nodes.
2.3 Tree Decompositions
Treewidth is an important graph parameter that indicates in a certain sense the “treelikeness” of an undirected graph (Bodlaender, 1993, 1997, 2005). On graphs of treewidth
bounded by a constant many otherwise intractable problems become tractable. Bucket
269

Ordyniak & Szeider

Elimination (Dechter, 1999) and Recursive Conditioning (Darwiche, 2001) are two important algorithmic concepts that apply to instances of bounded treewidth.
The treewidth of a graph G = (V, E) is defined via the following notion of decomposition:
a tree decomposition of G is a pair (T, χ) where T is a tree and χ is a labeling function with
χ(t) ⊆ V for every tree node t such that the following conditions hold:
1. Every node of G occurs in χ(t) for some tree node t.
2. For every edge {u, v} of G there is a tree node t such that u, v ∈ χ(t).
3. For every node v of G, let Tv be the subgraph of T induced by all nodes t such that
v ∈ χ(t). Then Tv is a (connected) subtree of T (“Connectedness Condition”).
The width of a tree decomposition (T, χ) is the size of a largest set χ(t) minus 1 among
all nodes t of T . A tree decomposition of smallest width is optimal. The treewidth of a
graph G, denoted tw(G), is the width of an optimal tree decomposition of G. It is known
that if (T, χ) is a tree decomposition of a graph G, then every clique of G is contained in
χ(t) for some tree node t ∈ V (T ) (Kloks, 1994, Lemma 2.2.2).
The following proposition will be useful to retrieve an upper bound on the treewidth of
a graph.
Proposition 1. Let G be an undirected graph and X ⊆ V (G). If the graph G − X contains
no edge, i.e., all the nodes in G − X are isolated, then tw(G) ≤ |X|.
Proof. Let V (G) \ X contain the nodes v1 , . . . , vn and let T be the tree with node set
{t, t1 , . . . , tn } and edge set { {t, ti } : 1 ≤ i ≤ n }. Then T together with the function χ such
that χ(t) = X and χ(ti ) = X ∪ {vi } for every 1 ≤ i ≤ n is a tree decomposition for G of
width |X|.
The main property of tree decompositions that allows for efficient bottom-up dynamic
programming algorithms for a wide spectrum of otherwise intractable problems is its wellknown separation property which is made precise in the following proposition.
Proposition 2. Let G be a graph, (T, χ) a tree decomposition for G, {t0 , t00 } be an edge in
T , and let T 0 and T 00 be the subtrees of T obtained from T after deleting the edge {t0 , t00 }
0
0
00
00
0
00
such
S that T contains tSand T contains t . Furthermore, define S = χ(t ) ∩ χ(t ), A =
t∈V (T 0 ) χ(t) and B =
t∈V (T 00 ) χ(t). Then the following two statements hold:
1. A ∩ B = S;
2. S separates the nodes in A from the nodes in B, i.e., there is no edge between a node
in A \ S and a node in B \ S.
Proof. The first statement follows immediately from Property (3) of a tree decomposition,
since the subtree containing a node v ∈ (A ∩ B) has to make use of the edge {t0 , t00 } in order
to be connected and hence v has to be contained in S.
To see the second statement, suppose for a contradiction that there is an edge between a
node a ∈ A \ S and a node b ∈ B \ S. It follows from the first statement of this proposition
that Ta and Tb are disjoint, i.e.,otherwise either a ∈ S or b ∈ S. But this contradicts
property (2) of a tree decomposition since {a, b} is an edge of G.
270

Parameterized Complexity of Exact BNSL

Given a graph G with n nodes and a constant w, it is possible to decide whether G
has treewidth at most w, and if so, to compute an optimal tree decomposition of G in
time O(n) (Bodlaender, 1996). Furthermore there exist powerful heuristics to compute tree
decompositions of small width in a practically feasible way (Koster, Bodlaender, & van
Hoesel, 2001; Gogate & Dechter, 2004; Dow & Korf, 2007). Recently, new randomized
heuristics have been studied in the context of Bayesian reasoning (Kask, Gelfand, Otten, &
Dechter, 2011; Gelfand, Kask, & Dechter, 2011).

3. Bayesian Network Structure Learning
In this section we define the theoretical framework for BNSL that we shall use for our
considerations. We closely follow the abstract framework used by Parviainen and Koivisto
(2010) which encloses a wide range of score-based approaches to structure learning. We
assume that the input data specifies a set N of nodes (representing random variables) and
a local score function f that assigns to each v ∈ N and each subset P ⊆ N \ {v} a nonnegative real number f (v, P ). Given the local score function f and a set N of nodes, the
problem is to find a dag D on N such that the score of D under f
X
f (D) :=
f (v, PD (v))
v∈N

is as large as possible (the dag D together with certain local probability distributions forms
a Bayesian network). This setting accommodates several popular scores like BDe, BIC and
AIC (Parviainen & Koivisto, 2010; Chickering, 1995).
We consider the following decision problem:
Exact Bayesian Network Structure Learning
Input:
A local score function f defined on a set N of nodes, a real
number s > 0.
Question: Is there a dag D on N such that f (D) ≥ s?
For the following complexity results we need to fix how the score function is represented in
the problem input. We cannot list the values f (v, P ) for all nodes v ∈ N and all subsets
P ⊆ N \ {v}, as this requires Ω(2|N | ) space. Therefore, we assume that f (v, P ) is only given
if it is different from 0; we call this the non-zero representation. This representation is
also used in other fields, for instance in constraint satisfaction, where only allowed tuples of
constraints are listed in the input (Tsang, 1993). An alternative representation of the score
function would be to list in the input all values f (v, P ) where |P | ≤ c for some constant
c. Let us call this the arity-c representation. This representation requires only polynomial
space if every node has at most c parents, for a small constant c, which is a reasonable
assumption. Given an arity-c representation of a score function, we can clearly compute in
linear time the corresponding non-zero representation. Hence the non-zero representation
is more general, and therefore we will base our complexity results on this encoding. All
tractability results also carry over to the arity-c representation.
The size of f (given in the non-zero representation) is the number of bits needed to
represent the tuples with f (v, P ) > 0 in a reasonable data structure, e.g., as a list of these
271

Ordyniak & Szeider

tuples where each tuple stores the node, the set of parents, and the value of the score
function. Clearly, the size of f exceeds the total number of all such tuples. We define
Pf (v) := { P ⊆ N : f (v, P ) > 0} ∪ {∅} to be the set of all potential parent sets of v. We
also define
δf := max |Pf (v)|;
v∈N

which will be an important measurement for our worst-case analysis of running times. In
particular, the above assumption on how f is represented implies the following:
(*) Let I = (N, f, s) be an instance of Exact Bayesian Network Structure Learning. Then δf is bounded by the size of f .
Let f be a local score function defined on a set N of nodes. The directed super-structure of
f is the directed graph Sf→ = (N, Ef ) where Ef contains an arc (u, v) if and only if u is a
potential parent of v, i.e., if u ∈ P for some P ∈ Pf (v). The (undirected) super-structure
of f , denoted Sf = (N, Ef ), is the skeleton of the directed super-structure.
v
P
f (v, P )
a
{d}
1
a {b, c, d}
0.5
b {a, f }
1
1
c
{e}
d
∅
1
e
{b}
1
f
{d}
1
g {c, d}
1
f

a

c

b

e

a

d

f
Sf→

g

c

b

d

e

f

g

Sf

Figure 2: A local score function f together with the directed super-structure Sf→ and the
undirected super-structure Sf .

Example 1. Figure 2 shows an example for a local-score function f defined on the set
N = {a, b, c, d, e, f, g} of nodes. The function f is given as a table containing all tuples
(v, P, f (v, P )) with v ∈ N and P ⊆ N \ {v} such that f (v, P ) > 0. Note that since f is
non-negative it holds that f (v, P ) = 0 for all the remaining pairs (v, P ), i.e., the pairs that
are not contained in the table. The figure also shows the directed super-structure Sf→ and
the unique super-structure Sf of f .
We say that a dag D on a set N of nodes is admissible for f if the skeleton of D is
a subgraph of the super-structure Sf . Furthermore, we say that a dag D on N is strictly
admissible for f if for every node v ∈ N we have PD (v) ∈ Pf (v). Note that every strictly
admissible dag is also admissible. Furthermore, there always exists a (strictly) admissible
dag with the highest score as shown by the following lemma.
272

Parameterized Complexity of Exact BNSL

Lemma 1. Let f be a local score function defined on a set N of nodes and let D be a dag
on N . Then there is a strictly admissible dag D0 on N with the same score as D.
Proof. If D is not (strictly) admissible, i.e., if there is a v ∈ N such that f (v, PD (v)) = 0,
we can delete all arcs (w, v) such that w ∈ PD (v). This does not decrease the score since
f (v, ∅) ≥ f (v, PD (v)) = 0 for every such a v.
a
b
e

a

c

d

f

g

b

Da

e

a

c

d

f

g

b
e

Db

Dc

a

c

d

f

g

b
e

c

d

f

g

Dd

Figure 3: Various dags in correspondence to the local-score function f of Example 1.
Example 2. Figure 3 shows four examples of dags on the the nodes a, b, c, d, e, f, g. Using
the local-score function f as defined in Example 1, we make the following observations:
a) Da is not admissible, because the super-structure Sf does not contain the edge {c, b}.
The score for Da is f (Da ) = 4.
b) Db is admissible but not strictly admissible, because the node a has parents c and d
but f (a, {c, d}) = 0. The score for Db is f (Db ) = 5.
c) Dc is the strictly admissible dag obtained from Db as described in the proof of Lemma 1.
We have f (Dc ) = f (Db ) = 5.
d) Dd is strictly admissible and is also an optimal dag for f . The score for Dd is f (Dd ) =
7.
3.1 Tree Decompositions
When presenting algorithms for graphs of bounded treewidth it is convenient to consider
tree decompositions in the following normal form (Kloks, 1994): A triple (T, χ, r) is a nice
tree decomposition of a graph G if (T, χ) is a tree decomposition of G, the tree T is rooted
at node r, and each node of T is of one of the following four types:
1. a leaf node: a node having no children;
2. a join node: a node t having exactly two children t1 , t2 , and χ(t) = χ(t1 ) = χ(t2 );
3. an introduce node: a node t having exactly one child t0 , and χ(t) = χ(t0 ) ∪ {v} for a
node v of G;
4. a forget node: a node t having exactly one child t0 , and χ(t) = χ(t0 ) \ {v} for a node
v of G.
273

Ordyniak & Szeider

For convenience we will also assume that χ(r) = ∅ for the root r of T . This can always be
achieved by adding forget nodes on top of the root (see Figure 4 for an example). For a nice
tree decomposition (T, χ, r) we define χ∗ (t) to be the union of all the sets χ(t0 ) where t0 is
contained in the subtree of T rooted at t. Furthermore, we denote by Ft the set of nodes
that have already been “forgotten” at node t, i.e.,
Ft = χ∗ (t) \ χ(t).
As stated in Section 2.3 one of the main properties of tree decompositions that allows for
efficient algorithms is the well-known separator property made precise in Proposition 2. The
following propositions provide different versions of this separator property of tree decompositions for each of the node types of a nice tree decomposition. Because these propositions
are well-known (Kloks, 1994) and are immediate consequences of the separator property
of tree decompositions we state them without proofs. The propositions summarize the
algorithmic properties of nice tree decompositions that we will use for the design of our
algorithm in Section 4.
Proposition 3. Let t be a join node with children t1 and t2 . Then Ft1 ∩ Ft2 = ∅ and there
is no edge between a node u ∈ Ft1 and a node v ∈ Ft2 in G.
Proposition 4. Let t be an introduce node with child t0 such that χ(t) = χ(t0 ) ∪ {v0 }. Then
there is no edge from v0 to a node v ∈ Ft in G. Furthermore, v0 ∈
/ Ft0 .
Proposition 5. Let t be a forget node with child t0 such that χ(t) = χ(t0 ) \ {v0 }. Then
there is no edge from v0 to a node v ∈ V (G) \ χ∗ (t) in G.
Given a tree decomposition of a graph G of width w, one can effectively obtain in
time O(|V (G)|) a nice tree decomposition of G with O(|V (G)|) nodes and of width at
most w (Kloks, 1994).
Example 3. Figure 4 shows a tree decomposition and a corresponding nice tree decomposition of the super-structure of Example 1.

4. A Dynamic Programming Algorithm for Exact Bayesian Network
Structure Learning
In this section we present the dynamic programming algorithm and establish our tractability
results. For the remainder of this section w denotes an arbitrary but fixed constant. Recall
from the previous section that δf is the maximum number of potential parent sets of a node.
Theorem 1. Given a set N of nodes and a local score function f on N whose superstructure Sf = (N, Ef ) has treewidth bounded by an arbitrary constant w. Then we can find
2(w+1)
in time O(δf
· |N |) a dag D on N with maximal score f (D).
Before we devise an algorithm to show Theorem 1 we state and prove a direct consequence of the theorem.
Corollary 1. Exact Bayesian Network Structure Learning can be decided in polynomial time for instances where the super-structure has bounded treewidth. The problem can
be decided in linear time if additionally the super-structure has bounded maximum degree.
274

Parameterized Complexity of Exact BNSL

∅
a
a, b
a, b, c
a, b, c, d

a, b, c, d
b, c, d

e, b, c

f, b, d

t

g, c, d

b, c, d
b, c, d

b, c, d

b, c, d

b, c

b, d

c, d

e, b, c

f, b, d

g, c, d

Figure 4: A tree decomposition (left) and a corresponding nice tree decomposition (right)
for the super-structure of Example 1.

Proof. The first statement follows immediately from the theorem since δf is bounded by
the total input size of the instance and w is a constant. Recall from Section 3 that the
local score function f is given as the list of all tuples for which f is non-zero and hence δf
is bounded by the total input size of the instance. The second statement follows since δf
is bounded whenever the maximum degree d of the super-structure is bounded as clearly
δf ≤ 2d .
In the following we will assume that we are given a set of nodes N and a local score
function f on N together with a nice tree decomposition (T, χ, r) for Sf of width at most w.
We are going to establish Theorem 1 by means of a dynamic programming algorithm along
a nice tree decomposition for Sf , computing local information at the nodes of the tree
decomposition that can then be put together to form an optimal dag. Our algorithm
closely follows the general approach used by algorithms on graphs (or structures) of bounded
treewidth (Bodlaender & Koster, 2008).
A partial solution for a tree node t ∈ V (T ) is a dag that can be obtained as the induced
subdigraph D[χ∗ (t)] of a strictly admissible dag D for f . For a tree node t let D(t) denote
the set of all partial solutions for t. For a partial solution D ∈ D(t) we set
ft (D) =

X

f (v, PD (v)),

v∈Ft

275

Ordyniak & Szeider

i.e., ft (D) is the sum of the scores of the nodes in Ft . Recall from the previous section that
Ft is the set of forgotten nodes at t.
The main idea underlying our algorithm is to reduce the space required to store a partial
solution with the help of a so-called record. This becomes possible because of the properties
of a tree decomposition manifested by Propositions 3, 4 and 5.
A record of a tree node t ∈ V (T ) is a triple R = (a, p, s) such that:
1. a is a mapping χ(t) → Pf (v), i.e., for every v ∈ χ(t) we have a(v) ∈ Pf (v);
2. p is a transitive binary relation on χ(t);
3. s is a non-negative real number.
Informally, for a tree node t ∈ V (T ) and a record R = (a, p, s), the mapping a fixes the
parent set of every node in χ(t), p is a compact representation of the reachability relation
between the nodes in χ(t) (using directed paths between nodes in χ∗ (t)), and s is the sum
of the scores of the nodes that have been forgotten for t, i.e., the nodes in Ft .
We say that a record represents a partial solution D ∈ D(t) if it satisfies the following
conditions:
1. a(v) ∩ V (D) = PD (v) for every v ∈ χ(t).
2. For every pair of nodes v1 , v2 ∈ χ(t) it holds that (v1 , v2 ) ∈ p if and only if D contains
a directed path from v1 to v2 .
We say that a record R = (a, p, s) of a tree node t ∈ V (T ) is valid if it represents some dag
D ∈ D(t) and s is the maximum score ft (D) over all dags in D(t) represented by R. We
say a partial solution D that is represented by R is maximal with respect to R if R is valid
and ft (D) = s. With each tree node t ∈ V (T ) we associate the set R(t) of all valid records
representing partial solutions in D(t).
In a certain sense, R(t) is a succinct representation of the optimal elements of D(t),
using space that only depends on w and δf , but not on |N |.
t

t
b

e

c

d

f

b

χ∗ (t)

g

c

d

e

Da

f

χ∗ (t)

g

Db

Figure 5: Two partial solutions for the node t of the nice tree decomposition from Example 3.

Example 4. Figure 5 shows two partial solutions Da and Db for the node t of the nice
tree decomposition Example 3. Da and Db are represented by all records R = (a, p, s) with
276

Parameterized Complexity of Exact BNSL

a(b) = {a, f }, a(c) = {e}, a(d) = ∅ and p = {(b, c), (d, b), (d, c)}. We have ft (Da ) = 3 >
ft (Db ) = 2 and it is easy to see that 3 is the maximum score over all partial solutions
represented by R. Hence the record R = (a, p, 3) is a valid record for t and Da is maximal
with respect to R, thus R ∈ R(t).
Our dynamic programming algorithm computes the set of all valid records in a bottom
up manner, i.e., starting from the leave nodes of the nice tree decomposition the algorithm
proceeds to the root node. The next three lemmas show how to compute the set of all valid
records for the introduce, forget and join nodes of the nice tree decomposition from the
valid records of its children. Informally, if t is an introduce node with child t0 such that
χ(t) = χ(t0 ) ∪ {v0 } then we compute the set R(t) of all valid records for t by checking for
each potential parent set P ∈ P(v0 ) for v0 and each valid record R ∈ R(t0 ) for t0 whether
the combination of P and R constitutes a valid record for t.
Lemma 2 (introduce node). Let t be an introduce node with child t0 . Then R(t) can be
computed from R(t0 ) in time O(δfw+1 ).
Proof. In the following we denote by v0 the node introduced by t, i.e., χ(t) = χ(t0 ) ∪ {v0 }.
We are going to establish the lemma with the help of the following claim whose proof can
be found in the appendix.
Claim 1. R(t) is the set of all records R = (a, p, s) such that there is a set P ∈ Pf (v0 ) and
a record R0 = (a0 , p0 , s0 ) ∈ R(t0 ) with:
1. a(v0 ) = P .
2. For every v ∈ χ(t0 ) it holds that a(v) = a0 (v).
3. s = s0 .
4. p is the transitive closure of the relation p0 ∪ { (u, v0 ) : u ∈ P } ∪ { (v0 , u) : u ∈
χ(t0 ) such that v0 ∈ a0 (u) }.
5. p is irreflexive.
It follows that R(t) can be computed by checking for every pair (P, R0 ), with P ∈ Pf (v0 )
and R0 ∈ R(t0 ), whether it satisfies the conditions (1)–(5). Since there are at most δf
possible sets P and at most O(δfw ) possible valid records for t0 (observe that |χ(t0 )| ≤ w)
the lemma follows from the fact that for every pair (P ,R0 ) the conditions can be checked in
time that only depends on w.
Informally, if t is a forget node with child t0 such that χ(t) = χ(t0 )\{v0 } then we compute
the set R(t) of all valid records for t by “projecting” the set R(t0 ) of valid records for t0 to
χ(t).
Lemma 3 (forget node). Let t be a forget node with child t0 . Then R(t) can be computed
from R(t0 ) in time O(δfw+1 ).
277

Ordyniak & Szeider

Proof. In the following we denote by v0 the forgotten node, i.e. χ(t) = χ(t0 ) \ {v0 }. We will
show that R(t) can be obtained as the “projection” of R(t0 ) to χ(t). Before doing so we
need some additional notation. Let R0 = (a0 , p0 , s0 ) ∈ R(t0 ). We define the projection of R0
to t, denoted R0 [t] as the record R = (a, p, s) such that:
1. a is the restriction of a0 to χ(t).
2. p = { (v, w) ∈ p0 : v, w ∈ χ(t) }.
3. s = s0 + f (v0 , a0 (v0 )).
Furthermore, we define the projection of R(t0 ) to t, denoted R(t0 )[t], as the set of all records
R0 [t] for R0 ∈ R(t0 ). We say that a record R = (a, p, s) ∈ R(t0 )[t] is maximal if there is no
s0 with s0 > s such that (a, p, s0 ) ∈ R(t0 )[t].
Again, we are going to establish the lemma with the help of the following claim whose
proof can be found in the appendix.
Claim 2. R(t) is the set of all maximal records in R(t0 )[t].
Since R(t0 ) contains at most O(δfw+1 ) records it is easy to see that R(t) can be computed
in time O(δfw+1 ).
Informally, if t is a join node with children t1 and t2 then we compute the set R(t) of
all valid records for t by checking for each record R1 ∈ R(t1 ) and each record R2 ∈ R(t2 )
whether the combination of R1 and R2 constitutes a valid record for t.
Lemma 4 (join node). Let t1 , t2 be the children of t in T . Then R(t) can be computed
2(w+1)
from R(t1 ) and R(t2 ) in time O(δf
).
Proof. We are going to establish the lemma with the help of the following claim (the rather
technical proof of this claim can be found in the appendix).
Claim 3. R(t) is the set of all records R = (a, p, s) such that there are records R1 =
(a1 , p1 , s1 ) ∈ R(t1 ) and R2 = (a2 , p2 , s2 ) ∈ R(t2 ) with:
1. a = a1 = a2 .
2. s = s1 + s2 .
3. p is the transitive closure of p1 ∪ p2 .
4. p is irreflexive, i.e., there is no v ∈ χ(t) such that (v, v) ∈ p.
It follows that R(t) can be computed by considering all pairs of records R1 ∈ R(t1 )
and R2 ∈ R(t2 ) and checking conditions (1)–(4). Since there are at most O(δfw+1 ) valid
records for every t ∈ V (T ) and for every such pair of records the time required to check the
conditions (1)–(4) does only depend on w, it follows that the running time of this procedure
2(w+1)
is at most O(δf
).
We are now ready to establish Theorem 1.
278

Parameterized Complexity of Exact BNSL

Proof of Theorem 1. Let N be a set of nodes, f a local score function on N where the
super-structure Sf has treewidth w (a constant) and |N | = n. We compute a nice tree
decomposition (T, χ, r) of Sf of width w and with O(n) nodes. This can be accomplished
in time O(n) (see the discussion in Section 2.3).
Next we compute the sets R(t) via a bottom-up traversal of T . For a leaf node t we can
compute R(t) just by considering all strictly admissible dags on the at most w + 1 nodes
in χ(t). This can clearly be done in time O(δfw+1 ) and we can then use Lemmas 4, 2 and 3
2(w+1)

to compute the sets R(t) for all other O(n) tree nodes in time O(δf
· n).
Since χ(r) = ∅, the partial solutions for the root r of T are exactly the strictly admissible
dags for f , and we have fr (D) = f (D) for each such dag D. After the computation of the
sets R(t) for all tree nodes t, the set R(r) contains exactly one record R = (∅, ∅, s). By the
above considerations, it follows that s is the largest score of all strictly admissible dags for
f , and, as noted in Section 3, this is also the largest score of any dag whose nodes belong
to N . It is now easy to compute a dag D with score f (D) = s via a top-down traversal of
T starting from r and using the information previously stored at each node in T . This can
2(w+1)
also be accomplished in time O(δf
· n).
We close this section with a remark concerning the relationship between the treewidth
of a Bayesian network and the treewidth of its super-structure. For Bayesian reasoning one
usually associates with the dag D of a Bayesian network its moral graph M (D) which is the
skeleton of D plus edges joining nodes that have a common child in D. The treewidth of a
Bayesian network is the treewidth of its moral graph (Darwiche, 2001; Dechter, 1999). We
observe that for every Bayesian network of bounded treewidth there is a super-structure of
bounded treewidth that contains the Bayesian network. Hence, such a Bayesian network can
be learned considering only super-structures of bounded treewidth. On the other hand if a
Bayesian network is contained in a super-structure of bounded treewidth then the Bayesian
network has bounded treewidth under the reasonable assumption that each of its nodes
has a bounded number of parents. Consequently, if a Bayesian network is learned from a
super-structure of bounded treewidth it is reasonable to assume that the Bayesian network
has bounded treewidth as well.

5. Refined Complexity Analysis
When presenting our algorithm in Section 4 we focused on a broad evaluation of its complexity. In this Section we provide a more fine-grained analysis of its running time.
In the following we assume that N is a set of nodes, f is a local score function on N ,
and (T, χ, r) is a nice tree decomposition of Sf of width at most w. We can improve on the
running time of our algorithm by using the following five ideas.
I1 By keeping the records for each node in the tree decomposition in some fixed order we
2(w+1)
)
can improve the time needed at a join node of the tree decomposition from O(δf
w+1
to O(δf ) without any additional cost for sorting. We achieve this by generating
new records in an ordered manner and keeping track of that order when the records
are stored.
279

Ordyniak & Szeider

I2 Parent sets that are supersets of parent sets with a higher score can be
disregarded (de Campos, Zeng, & Ji, 2009). This simple preprocessing rule usually
allows us to consider fewer than the potentially 2d potential parent sets. In the sequel
we will denote the preprocessed score function by f 0 . Clearly, |Pf (v)| ≤ |Pf 0 (v)| for
every v ∈ N .
I3 If the maximum in-degree of the resulting BN is bounded in advance one can use
this fact to further preprocess the score function in the natural way. In the following
we denote the resulting score function by f 00 . Clearly, |Pf 00 (v)| ≤ |Pf 0 (v)| for every
v ∈ N.
I4 Every node of the network might have a different number of potential parent sets.
Consequently, instead of using one upper bound for the number δf 00 of potential parent
sets it is more realistic to consider the actual number |Pf 00 (v)| of potential parent sets
for each node v ∈ N .
I5 Only valid records need to be stored by our algorithm, i.e., records that represent
acyclic networks.
Considering the ideas I1–I4 the worst-case complexity of our algorithm can be refined as
follows:
X
Y
O(w2 ·
|Pf 00 (v)|)
t∈V (T )

w2

v∈χ(t)

Q

Observe that
· v∈χ(t) |Pf 00 (v)| is the number of potential records for the tree node t.
Because of idea I5 in general not all of these records need to be stored by our algorithm.
This refined analysis suggests that the running time of our algorithm is dominated by the
maximum number of records that need to be stored for a tree node t ∈ V (T ).

6. Hardness Results for Exact Bayesian Network Structure Learning
The following result follows by a reduction due to Chickering (1996).
Theorem 2. Exact Bayesian Network Structure Learning is NP-hard for instances with super-structures of maximum degree 4.
Proof. Since we use a well-known reduction from Chickering, we will only sketch the argument. The reduction is from Feedback Arc Set (FAS). The problem asks whether a
digraph D = (V, E) can be made acyclic by deleting at most k arcs (the deleted arcs form
a feedback arc set of D). The problem is NP-hard for digraphs with skeletons of maximum
degree 4 (Karp, 1972). Given an instance (D, k) of FAS, where the skeleton of D has maximum degree 4, we construct a set V 0 = V (D)∪E(D) of nodes and a local score function f on
V 0 by setting f ((u, v), {u}) = 1 for all (u, v) ∈ E(D), f (v, { (u, v) : u ∈ PD (v) }) = |PD (v)|
for all v ∈ V (D), and f (v, P ) = 0 in all other cases. Clearly, the super-structure Sf (recall
the definition of Sf in Section 3) is isomorphic to the undirected graph obtained from the
skeleton of D after subdividing every edge once, hence the maximum degree of Sf is at
most the maximum degree of D which is 4. It is easy to see that D has a feedback arc set
of size ≤ k if and only if there exists a dag D0 whose skeleton is a spanning subgraph of Sf
with f (D0 ) ≥ 2 · |E| − k.
280

Parameterized Complexity of Exact BNSL

Theorem 3. Exact Bayesian Network Structure Learning parameterized by the
treewidth of the super-structure is W[1]-hard.
Proof. We devise an fpt-reduction from the following problem, which is well-known to be
W[1]-complete (Pietrzak, 2003).
Partitioned Clique
Input:
Parameter:
Question:

A k-partite graph G = (V, E) with partition V1 , . . . , Vk such
that |Vi | = |Vj | = n for 1 ≤ i < j ≤ k.
The integer k.
Are there nodes v1 , . . . , vk such that vi ∈ Vi for 1 ≤ i ≤ k
and {vi , vj } ∈ E for 1 ≤ i < j ≤ k? (The graph K =
({v1 , . . . , vk }, { {vi , vj } : 1 ≤ i < j ≤ k }) is a k-clique of G.)

Let G = (V, E) be an instance of this problem with partition V1 , . . . , Vk , |V1 | = · · · = |Vk | =
n and parameter k. Informally, we encode the given instance of Partitioned Clique
as an instance (N, f, s) of Exact Bayesian Structure Learning such that G has a
k-clique if and only if there is a Bayesian network D with f (D) ≥ s. To achieve this we
introduce a node nv for every node v of G and one node aij for every 1 ≤ i 6= j ≤ k. Then
a node that corresponds to a node in Vi achieves its maximum score for the parent set that
contains all nodes aij for 1 ≤ j ≤ k. A node aij achieves its maximum score if its parent set
corresponds to an edge of G between a node in Vi and a node in Vj . Hence, the edges of G
are encoded in the local score function of the nodes aij for 1 ≤ i 6= j ≤ k. By choosing the
proper scores for these nodes and the right threshold s we can assure that in any Bayesian
network whose score is higher than s every node aij has to attain its maximum score, and
all but at most k nodes that correspond to nodes of G do not achieve their maximum score.
It follows that the parent sets chosen for the nodes aij correspond to the edges of a k-clique
of G.
In order to make later calculations easier we will assume that k > 2 for the remainder
of this proof. Let α = k 2 − k − 1, ε = 2k and s = nkα + 1. We construct a set N of nodes
and a local score function f on N satisfying the following claims.
Claim 4. tw(Sf ) ≤ k(k − 1)/2
Claim 5. G has a k-clique if and only if there is a dag D such that f (D) ≥ s.
We will have shown the theorem after establishing the two claims.
We set A = { aij : 1 ≤ i < j ≤ k }, N = V (G) ∪ A, and Ai = { alk ∈ A : l = i or
k = i } for every 1 ≤ i ≤ k. We are now ready to define f . We set f (v, Ai ) = α for every
v ∈ Vi , and f (aij , {u, w}) = ε for every 1 ≤ i < j ≤ k, u ∈ Vi , w ∈ Vj , and {u, w} ∈ E(G).
Furthermore, we set f (v, P ) = 0 for all the remaining combinations of v and P . See Figure 6
for an illustration. Now, Claim 4 follows from Proposition 1 with X = A. Hence, it remains
to show Claim 5.
Before we go on to show Claim 5 we give some further notation and explanation. Let
0
0
E0 ⊆
S E(G). We denote by 0V (E ) the set of all nodes incident to edges in E , i.e., the
set e∈E 0 e. We say that E is representable if for every 1 ≤ i < j ≤ k it contains at
most one edge between a node in Vi and a node in Vj . We define eij (E 0 ) = {vi , vj } if E 0
contains the edge between vi ∈ Vi and vj ∈ Vj and eij = ∅ otherwise. We define D(E 0 ) to
281

Ordyniak & Szeider

v11

v12

v13

v11

v12

a13
v33

v21
v32

7→

a12

v33

v22
v31

v13

v21
v32

v23

v22

a23
v31

v23

Figure 6: An example for the graph G and the super-structure Sf according to the construction used in the proof of Theorem 3.
v11

v12

v13

v11

v12

a13
v33

v21
v32

7→

a12

v33

v22
v31

v13

v21
v32

v23

v22

a23
v31

v23
D(E 0 )

G

Figure 7: The example graph G from Figure 6 together with an edge set E 0 , given as the
bold edges in the illustration of G, and the resulting dag D(E 0 ).

be the directed graph with node set N and arc set { (v, aij ) : v ∈ eij (E 0 ) and 1 ≤ i < j ≤
k } ∪ { (aij , v) : v ∈
/ eij (E 0 ) and 1 ≤ i < j ≤ k }. Figure 7 shows D(E 0 ) for a representable
edge set of the example graph G from Figure 6.
The main idea to show Claim 5 is that f (D) ≥ s if and only if D has the form D(E 0 ) for
a representable edge set E 0 that corresponds to a k-clique in G. This is formally expressed
by the following claim whose proof can be found in the appendix.
Claim 6. The following statements are equivalent:
1. G has a k-clique.
2. There is a dag D with f (D) ≥ s.
3. There is a representable edge set E 0 ⊆ E(G) such that f (D(E 0 )) ≥ s.
282

Parameterized Complexity of Exact BNSL

Note that in contrast to Theorem 2, it is essential for Theorem 3 that the super-structure
has unbounded degree: if both degree and treewidth are bounded then the problem is
fixed-parameter tractable by Corollary 1 and so unlikely to be W[1]-hard.

7. k-Neighborhood Local Search
Important and widely used algorithms for BNSL are based on local search methods (Heckerman, Geiger, & Chickering, 1995). Usually the local search algorithm tries to improve
the score of a given dag by transforming it into a new dag by adding, deleting, or reversing
one arc at a time (in symbols: add, del, and rev, respectively). The main obstacle for
local search methods is the danger of getting stuck at a poor local optimum. A possibility
for decreasing this danger is to perform k > 1 elementary changes in one step, known as kNeighborhood Local Search or k-Local Search for short. For BNSL, when we try to improve
the score of a dag on n nodes, the k-local search space is of order nO(k) . Therefore, if carried
out by brute-force, k-local search is too costly even for small values of k. It is therefore not
surprising that most practical local search algorithms for BNSL consider 1-neighborhoods
only.
The study of the parameterized complexity of k-local search was initiated by Fellows
(2003). To date a collection of positive and negative results on the parameterized complexity
of k-local search for various combinatorial optimization problems are known. For instance,
k-local search has already been investigated for combinatorial problems on graphs (Khuller,
Bhatia, & Pless, 2003; Marx, 2008; Fellows, Rosamond, Fomin, Lokshtanov, Saurabh, & Villanger, 2009; Gaspers, Kim, Ordyniak, Saurabh, & Szeider, 2012), for the problem of finding
a minimum weight assignment for a Boolean constraint satisfaction instance (Krokhin &
Marx, 2012), for the stable marriage problem with ties (Marx & Schlotter, 2011), and for
the satisfiability problem (Szeider, 2011).
In this section we show that k-local search for BNSL can be solved in linear time if
the super-structure has bounded treewidth and bounded maximum degree. This result
is in good agreement with Theorem 1. However, in contrast to Exact BNSL it might
still be possible to drop one of these restrictions without losing uniform polynomial-time
tractability, but we show that this is not the case. We also investigate k-Local Search BNSL
for different combinations of allowed operations such as reversal, addition and deletion of
an arc. Our results are mostly negative. In fact, somewhat surprisingly, k-Local Search
BNSL remains hard even if edge reversal is the only allowed operation.
Before we state and show our results we define k-Local Search BNSL more formally.
Let k ≥ 0 and O ⊆ {add, del, rev}. Consider a dag D = (V, E). A directed graph
D0 = (V 0 , E 0 ) is a k-O-neighbor of D if
1. D0 is a dag,
2. V = V 0 ,
3. E 0 can be obtained from E by performing at most k operations from the set O.
For O ⊆ {add, del, rev} we consider the following parameterized decision problem.
283

Ordyniak & Szeider

k-O-Local Search Bayesian Network Structure Learning
Input:
Question:

A local score function f , a dag D that is admissible for f , and
an integer k.
Is there a k-O-neighbor D0 of D with a higher score than D?

Note that the problem does not change if we require D0 to be admissible, as we can always
avoid the addition of an inadmissible arc.
a

c

b

a

d

e

f

7→

g

c

b

d

e

f

g

D0

D

Figure 8: Two dags illustrating the notion of a k-O-neighbor in Example 5.
Example 5. Figure 8 shows two dags D and D0 such that D0 can be obtained from D by
either reversing or by deleting and adding the reverse of the bold arcs in D. It follows that
D0 is a 3-{rev}-neighbor and a 6-{add, del}-neighbor of D. The directed graph obtained
from D after only reversing the arc (a, d) contains a cycle (on the nodes {a, b, f, d}) but D0
does not. The score of D0 is larger than the score of D, since f (D) = 3 and f (D0 ) = 7
(using the score function f as depicted in Figure 2).
Proposition 6. k-Local Search Bayesian Network Structure Learning can be
decided in linear time for instances where the super-structure has bounded treewidth and
bounded maximum degree.
Proof. As the proof uses the same arguments as the proof of Theorem 1 we only sketch the
proof of this proposition.
In the following we will assume that we are given an instance I = (D, f, k) of k-OLocal Search Bayesian Network Structure Learning together with a nice tree
decomposition (T, χ, r) for Sf of width at most w and let d be the maximum degree of Sf .
As before we assume that w and d are constants. For a set S we denote by [S] the set of
all subsets of S.
The main difference to the proof of Theorem 1 is that we are now only interested in
solutions which are k-O-neighbors of D. To take this into account we need to slightly extend
the concept of a record for a tree node t ∈ V (T ). We include an integer c to reflect the “cost”
of a partial solution that is the smallest number of operations from O needed to obtain D.
For technical reasons we also include a mapping b that assigns the set of forgotten children
to every node contained in χ(t). This allows us to compute the value of c for a forget node.
A record of a tree node t ∈ V (T ) is a quintuple R = (a, b, p, c, s) such that:
284

Parameterized Complexity of Exact BNSL

1. a is a mapping χ(t) → Pf (v);
2. b is a mapping χ(t) → [Ft ];
3. p is a transitive binary relation on χ(t);
4. c is a non-negative integer;
5. s is a non-negative real number.
We say that a record represents a partial solution Dp ∈ D(t) if it satisfies the following
conditions:
1. a(v) ∩ V (Dp ) = PDp (v) for every v ∈ χ(t).
2. b(v) = { u ∈ Ft : (v, u) ∈ E(Dp ) } for every v ∈ χ(t).
3. For every pair of nodes v1 , v2 ∈ χ(t) it holds that (v1 , v2 ) ∈ p if and only if Dp contains
a directed path from v1 to v2 .
4. c ≤ k is the smallest integer such that Dp [Ft ] is a c-O-neighbor of D[Ft ].
We say that a record R = (a, b, p, c, s) of a tree node t ∈ V (T ) is valid if it represents some
dag Dp ∈ D(t) and s is the maximum score ft (D) over all dags in D(t) represented by R.
We say a partial solution D that is represented by R is maximal with respect to R if R is
valid and ft (D) = s. With each tree node t ∈ V (T ) we associate the set R(t) of all valid
records representing partial solutions in D(t).
It is now straightforward to adapt the dynamic programming algorithm of Section 4 to
the new setting. Observe that there are at most k + 1 possible values for c. Furthermore,
because every considered partial solution Dp is admissible, the number of possible values for
b(v) for every v ∈ χ(t) is bounded by 2d . It follows that the space requirement to store such
a record is at most (k + 1) · 2d(w+1) times the space requirement needed to store a record as
defined in Section 4. Using the same argumentation as in Section 4 this leads to an overall
running time of O((δfw+1 · (k + 1) · 2d(w+1) )2 · |V (D)|) ≤ O((dw+1 · (k + 1) · 2d(w+1) )2 · |V (D)|).
Since w and d are constants, this constitutes a linear running time.
Theorem 4. If O = {add} or O = {del}, then k-O-Local Search Bayesian Network
Structure Learning is solvable in polynomial time.
Proof. We only consider O = {add} as the proof for O = {del} is analogous. Let
I = (D, f, k) be the given instance of k-{add}-Local Search Bayesian Network
Structure Learning. Since we are only allowed to add arcs to D every step must leave
D acyclic. It follows that there is a k-{add}-neighbor D0 of D with f (D0 ) > f (D) if and
only if there is a node v ∈ V (D) such that the addition of at most k incoming arcs increases
the score of v and the resulting digraph remains acyclic. Now, for every P ⊆ V (D) \ {v}
one can easily check whether f (v, P ) > f (v, PD (v)) and whether P can be obtained from
PD (v) via the addition of at most k incoming arcs and whether the resulting digraph is
acyclic.
285

Ordyniak & Szeider

In view of Theorem 4 let us define a set O ⊆ {add, del, rev} to be non-trivial if O ∈
/ {∅,
{add}, {del}}.
Theorem 5. Let O ⊆ {add, del, rev} be non-trivial. Then k-O-Local Search Bayesian
Network Structure Learning is W[1]-hard for parameter tw(Sf ) + k.
Proof. We slightly modify the reduction given in the proof of Theorem 3. Let I = (G, k) be
the given instance of Partioned Clique and let N , f and s be defined in correspondence
to I and the proof of Theorem 3. We will distinguish two cases depending on whether it is
allowed to reverse an arc or not, i.e., depending on whether rev ∈ O.

For the case that rev ∈ O we claim that I 0 = (f, D, k 0 ) where D = D(∅) and k 0 = k2
is an instance of k 0 -O-Local Search Bayesian Network Structure Learning such
that G contains a k-clique if and only if there is a k 0 -O-neighbor D0 of D with f (D0 ) > f (D).
To see this let K be a k-clique in G. It follows from Claim 6 that there is a representable
edge set E 0 with f (D(E 0 )) ≥ s > f (D) = s − 1 and since E 0 is representable it is easy to
see that D(E 0 ) can be obtained from D by the reversal of at most k 0 arcs in D. Hence
D0 = D(E 0 ) is a k 0 -O-neighbor of D with f (D0 ) > f (D). To see the reverse let D0 be a
k 0 -O-neighbor of D with f (D0 ) > f (D) = s − 1. Hence D0 is a dag and since f (D0 ) is integer
it follows that f (D0 ) ≥ s. Again, using Claim 6, we have that there is a k-clique in G.
Now, for the only remaining case, i.e., the case that O = {add, del} we claim that
I 00 = (f, D, k 00 ) where k 00 = 2k 0 is an instance of k 0 -O-Local Search Bayesian Network
Structure Learning such that G contains a k-clique if and only if there is a k 00 -Oneighbor D0 of D with f (D0 ) > f (D). The proof uses the same arguments as in the case
that rev ∈ O only that we now need twice as many operations. That is, we have to replace
the reversal of an arc (u, v) with the deletion of the arc (u, v) followed by the addition of
the arc (v, u).

In the preliminary version of this paper (Ordyniak & Szeider, 2010) we showed the following theorem by a parametrized reduction from Red/Blue Non-Blocker, which had
been claimed to be W[1]-complete for graphs of bounded degree (Downey & Fellows, 1995).
However, recently we found out that this problem is in fact fixed-parameter tractable and
that the proof published in the work of Downey and Fellows (1995) contained a mistake (Fellows, 2012). We therefore use a reduction from Independent Set that does not require
the original instance to have bounded degree. This even allows us to strengthen our result
by decreasing the upper bound on the maximum degree of the super-structure from 5 to 3.
Theorem 6. Let O ⊆ {add, del, rev} be non-trivial. Then k-O-Local Search Bayesian
Network Structure Learning is W[1]-hard for parameter k. Hardness even holds if
the super-structure Sf has maximum degree 3.
Proof. We devise an fpt-reduction from the following problem which is known to be W[1]complete (Downey & Fellows, 1999).
286

Parameterized Complexity of Exact BNSL

Independent Set
Input:
An undirected graph G = (V, E) and an integer k.
Parameter: The integer k.
Question:
Does G have an independent set of size at least k, i.e., is there
a set S ⊆ V with |S| ≥ k such that {u, v} ∈
/ E for every pair
of nodes u, v ∈ S.
To simplify the initial construction we first prove the theorem for the case that the maximum
degree of the super-structure is at most 5. We later show how to refine the proof for superstructures with maximum degree at most 3.
Let (G = (V, E), k) be an instance of this problem and k 0 = 2k + 1 if rev ∈ O and
0
k = 2(2k + 1) otherwise. We construct a dag D and a local score function f such that G
has an independent set of size at least k if and only if D has a k 0 -O-neighbor D0 with a
higher score than D.
The dag D is obtained from G by applying the following steps (see Figure 9 for an
illustration):
1. We replace every node v ∈ V with the two nodes v 1 and v 2 and an arc (v 1 , v 2 ).
2. For every node v ∈ V we add a binary tree Tv1 with exactly |NG (v)| leaves. The root
of Tv1 is v 1 and all arcs of Tv1 are directed away from v 1 . Furthermore, we define lv1
to be a bijective mapping from NG (v) to the leaves of Tv1 .
3. For every node v ∈ V we add a binary tree Tv2 with exactly |NG (v)| leaves. The root
of Tv2 is v 2 and all arcs of Tv2 are directed towards v 2 . Furthermore, we define lv2 to
be a bijective mapping from NG (v) to the leaves of Tv2 .
4. For every edge {u, v} ∈ E, we add the arcs (lu1 (v), lv2 (u)) and (lv1 (u), lu2 (v)) to D.
5. We add a binary tree T1 with root r1 with exactly |V | leaves, whose edges are directed
away from r1 . We define lT1 to be a bijective mapping from V to the leaves of T1 .
6. We add a binary tree T2 with root r2 with exactly |V | leaves, whose edges are directed
towards r2 . We define lT2 to be a bijective mapping from V to the leaves of T2 .
7. For every v ∈ V (G), we add the arcs (v 1 , lT1 (v)) and (v 1 , lT2 (v)) to D.
8. We add the arc (r2 , r1 ) to D.
This completes the construction of D. Next we define the local score function f on V (D).
Let α = k − 1, β = |V (G)| and ε = 1.
1. For every n ∈ V (D) \ { v 1 : v ∈ V (G) } ∪ {r1 } we set f (n, PD (n)) = β.
2. For every v ∈ V (G) we set f (v 1 , {v 2 , lT1 (v)}) = ε, f (v 2 , PD (v 2 ) \ {v 1 }) = β, and
f (lT1 (v), PD (lT1 (v)) \ {v 1 }) = β.
3. We set f (r1 , PD (r1 )) = α and f (r2 , PD (r2 ) ∪ {r1 }) = β.
4. For all the remaining combinations of n ∈ V (D) and P ⊆ V (D) we set f (n, P ) = 0.
287

Ordyniak & Szeider

a

b

c

7→
r1

lT1 (a)

lT1 (b)
lT2 (a)

lT1 (c)
lT2 (b)

lT2 (c)

r2

a1

b1

la1 (b)

la1 (c)

la2 (b)

la2 (c)

a2

lb1 (a)

lb2 (a)

c1
lb1 (c)

lc1 (b)

lc1 (a)

lb2 (c)

lc2 (b)

lc2 (a)

b2

c2

Figure 9: Top: An example graph G. Bottom: The dag D resulting from G using the
construction in the proof of Theorem 6.

Evidently D is acyclic and both D and f can be constructed from G in polynomial time.
Observe that the super-structure Sf is exactly the skeleton of D. Hence, by construction,
the nodes v 1 for v ∈ V (D) have degree at most 5 while all other nodes of Sf have degree
at most 3 showing that the maximum degree of Sf is at most 5. Consequently, we can
establish the theorem with the help of the following claim whose proof can be found in the
appendix.

288

Parameterized Complexity of Exact BNSL

0

α
β

β

β

β
β

β

β
β

β

β
β

β

β
β

β
β

β
β

β

β

β

β

7→
0
β

β

β

β

β

β

β

β

ε
β

β

β

β

β

β

β 0

0

β
β

β 0

0
β

β

β

β

β

β

β
β

β

β

β

β

D0

D

Figure 10: The dag D from Figure 9 together with a k 0 -{rev}-neighbor D0 with f (D0 ) >
f (D). Here k = 1, k 0 = 2k + 1 = 5 and {a} is an independent set of size k in
the graph G from Figure 9. The score for every node is given by it’s label.

Claim 7. G has an independent set of size at least k if and only if D has a k 0 -O-neighbor
D0 with a higher score than D where k 0 = 2k + 1 if rev ∈ O and k 0 = 2(2k + 1) otherwise.
We now show how to alter the above construction to obtain the result for maximum
degree 3. The only nodes in Sf whose degree may exceed 3 are the nodes in { v 1 : v ∈ V (G) }.
The main idea to reduce the degree of these nodes is to further split their sets of neighbors
using binary trees. For our new construction we define a DAG D0 and a local score function
f 0 as follows. The DAG D0 is obtained from D by applying the following actions:
1. For every v ∈ V (G), we delete all nodes of Tv1 and all arcs incident with these nodes.
2. For every v ∈ V (G), we add the nodes v 1a and v 1b and the arcs (v 1a , v 1b ) and (v 1b , v 2 ).
3. For every v ∈ V (G) we add a binary tree Tv1a with exactly |NG (v)| + 1 leaves. The
root of Tv1a is v 1a and all arcs of Tv1a are directed away from v 1a . Furthermore, we
define lv1a to be a bijective mapping from NG (v) ∪ {lT2 (v)} to the leaves of Tv1a .
4. For every v ∈ V (G) we add the arcs (v 1b , lT1 (v)) and (lv1a (lT2 (v)), lT2 (v)).
This completes the construction of D0 . Next we define the local score function f 0 as follows:
1. For every n ∈ V (D0 ) \ { v 1a : v ∈ V (G) } ∪ {r1 } we set f 0 (n, PD0 (n)) = β.
2. For every v ∈ V (G) we set f 0 (v 1a , {v 1b }) = ε, f 0 (v 1b , {v 2 , lT1 (v)}) = β, f 0 (v 2 , PD0 (v 2 ) \
{v 1b }) = β, and f 0 (lT1 (v), PD0 (lT1 (v)) \ {v 1b }) = β.
289

Ordyniak & Szeider

3. We set f 0 (r1 , PD0 (r1 )) = α and f 0 (r2 , PD0 (r2 ) ∪ {r1 }) = β.
4. For all the remaining combinations of n ∈ V (D0 ) and P ⊆ V (D0 ) we set f 0 (n, P ) = 0.
It is easy to see that Sf 0 has maximum degree 3. Furthermore, using the same arguments
as in the proof of Claim 7 one can show that the graph G has an independent set of size k if
and only if the DAG D0 has a k 0 -O-neighbor D00 with a higher score than D0 (with respect
to f 0 ) where k 0 = 3k + 1 if rev ∈ O and k 0 = 2(3k + 1) otherwise.
Theorem 6 provides a surprising contrast to a similar study of k-local search for MAX-SAT
where the problem is fixed-parameter tractable for instances of bounded degree (Szeider,
2011). A possible explanation for the surprising hardness of k-O-Local Search Bayesian
Structure Learning could be that, in contrast to MAX-SAT, a global property of the
entire instance (acyclicity) must be checked.

8. The Directed Super-structure
In the previous sections we considered the problem of Exact BNSL and k-Local Search
BNSL under certain restrictions of the undirected super-structure. However, every strictly
admissible solution to Exact BNSL is actually contained in the more restrictive directed
super-structure. It is a natural question whether the additional information entailed in the
directed super-structure can be used to find new structural restrictions under which Exact
BNSL becomes tractable. It is well-known that Exact BNSL becomes significantly easier
if an ordering of the variables is given in advance. For instance, given an ordering of the
variables of a BN, Exact BNSL becomes solvable in polynomial time if the input is given in
the arity-c representation (Teyssier & Koller, 2005). Fixing an ordering of the variables in
a BN corresponds to restricting the search space to acyclic directed super-structures. Our
first observation of this section is that Exact BNSL is solvable in polynomial time if the
directed super-structure is a dag and the input to the problem is given in the more general
non-zero representation. It is important to note that there is no corresponding restriction
of the undirected super-structure, because restricting the directed super-structure to be
acyclic does not impose any restrictions on the undirected super-structure. Considering
this promising result it becomes natural to ask whether it is possible to gradually generalize
the class of acyclic directed super-structures. A natural approach would be to consider only
directed super-structures that can be made acyclic by deleting a small number k of nodes.
Such an approach looks promising as it is known that for every fixed k the directed superstructures that can be made acyclic by deleting at most k nodes can be recognized efficiently
(Chen, Liu, Lu, O’Sullivan, & Razgon, 2008). However, we show that this approach is
unlikely to work for Exact BNSL. Furthermore, in correspondence to the results in the
previous sections, NP-hardness even holds if we additionally bound the maximum in-degree
and out-degree of Sf→ .
Theorem 7. Let N be a set of nodes and f a local score function on N such that Sf→ is
acyclic. Then we can find in time O(|N |δf ) a dag D with maximal score f (D).
Proof. Because Sf→ is acyclic, it follows that every strictly admissible directed graph D is
also acyclic. Hence in order to compute a dag D with the highest score, it is sufficient to
290

Parameterized Complexity of Exact BNSL

compute for every n ∈ N a parent set with the highest score. This can clearly be done in
time O(|N |δf ) and so the result follows.
Corollary 2. Exact Bayesian Network Structure Learning is solvable in quadratic
time for acyclic directed super-structures.
Proof. This follows immediately from Theorem 7 because both N and δf are bounded by
the total input size of the problem. Recall from Section 3 that the local score function f is
given as the list of all tuples for which f is non-zero and hence δf is bounded by the total
input size of the instance.
Theorem 8. Exact Bayesian Network Structure Learning is NP-hard for instances where Sf→ can be made acyclic by deleting one node. Hardness even holds if we
additionally bound the maximum in-degree and the maximum out-degree of Sf→ by 3.
Proof. We devise a polynomial reduction from the restricted version of 3-SAT where every
literal is contained in at most two clauses. This version of 3-SAT is still NP-complete
(Garey & Johnson, 1979). Let φ be a 3-CNF formula with variables x1 , . . . , xn and clauses
C1 , . . . , Cm where Cj = lj,1 ∨ lj,2 ∨ lj,3 , for every 1 ≤ j ≤ m. We construct a set N of nodes,
a local score function f and a real number s > 0.
N contains the nodes d0 , . . . , dn and t0 , . . . , tn+m and additionally:
• For every 1 ≤ i ≤ n the nodes xi , xi , ai , bi .
• For every 1 ≤ j ≤ m the nodes lj,1 , lj,2 , lj,3 , Cj .
Let α = n + 1 and ε = 1. We define f as follows:
• We set f (d0 , {t0 }) = f (t0 , t1 ) = α.
• For every 1 ≤ i ≤ n we set:
– f (di , {di−1 }) = α.
– f (ai , {di }) = α and f (xi , {ai }) = f (xi , {ai }) = ε.
– f (bi , {xi }) = f (bi , {xi }) = α.
– f (ti , {ti+1 , bi }) = α.
• For every 1 ≤ j ≤ m we set:
– f (Cj , {lj,1 }) = f (Cj , {lj,2 }) = f (Cj , {lj,3 }) = α.
– f (tn+j , {tn+j+1 , Cj }) = α if j < m and f (tn+j , {Cj }) = α if j = m.
• For every 1 ≤ i ≤ n, 1 ≤ j ≤ m and 1 ≤ l ≤ 3 we set f (lj,l , {xi }) = α if lj,l = xi and
f (lj,l , {xi }) = α if lj,l = xi .
For all other combinations of v ∈ N and P ⊆ N we set f (v, P ) = 0. Furthermore, we set
s = (4n + 5m + 2)α + nε. An example of the directed super-structure constructed from a
3-CNF formula is shown in Figure 11. We establish the theorem by showing the following
claims.
291

Ordyniak & Szeider

Claim 8. Sf→ can be made acyclic by deleting at most one node.
Claim 9. Sf→ has maximum in-degree and maximum out-degree 3.
Claim 10. φ is satisfiable if and only if there is a dag D with score f (D) ≥ s.
d0

d1

d2

a1

a2

x1

x2

b1

l1,2

x3

l1,3

l2,1

b3

l2,2

l2,3

l3,1

C2
t1

x3

b2

C1
t0

a3

x2

x1

l1,1

d3

t2

t3

l3,2

l3,3

C3
t4

t5

t6

Figure 11: The directed super-structure for the formula φ = (x1 ∨ x2 ∨ x3 ) ∧ (x1 ∨ x2 ∨ x3 ) ∧
(x1 ∨ x2 ∨ x3 ) in the proof of Theorem 8.
It is easy to see that Sf→ − d0 is acyclic and hence Claim 8 holds. Since every literal
occurs in at most two clauses it is also easy to verify Claim 9. The proof of Claim 10 is
straightforward and can be found in the appendix.

9. Conclusion
We have studied the computational complexity of Bayesian Structure Learning (BNSL) under various restrictions on the (directed) super-structure, considering both Exact BNSL and
k-Local Search BNSL. We have obtained positive and negative results for the theoretical
worst-case complexities of the problems. Our main positive result states that Exact BNSL
is linear-time tractable if the super-structure has bounded treewidth and bounded maximum degree. We have contrasted out positive results with negative results, using techniques
and concepts from Parameterized Complexity. This theoretical framework is particularly
well-suited for such an investigation as it allows a fined-grained investigation that takes
structural aspects of problem instances into account. Our results point out which combinations of structural restrictions make the problems tractable and which restrictions cannot be
dropped without loosing tractability. Considering various combinations of restrictions in a
systematic way allows us draw a broader picture of the complexity landscape (see the table
in Section 1). We hope that our results provide a better understanding of the principles
292

Parameterized Complexity of Exact BNSL

of BNSL and contribute to its foundations. We hope that this understanding will also be
useful for the development of heuristic methods for practical BNSL systems.

Acknowledgments
A shorter and preliminary version of this paper appeared in UAI 2010. Research supported
by the European Research Council, grant reference 239962.

Appendix A
Proof of Claim 1 (Lemma 2). Let us first assume that R = (a, p, s) is a valid record of t and
let P = a(v0 ). Since R is valid it follows that it represents some partial solution D ∈ D(t)
such that D is maximal with respect to R. Now, D0 = D[χ∗ (t0 )] is a partial solution for t0
and it follows from Proposition 4 that D0 = D − v0 and furthermore ft0 (D0 ) = s. Hence,
D0 can be represented by some record R0 = (a0 , p0 , s0 ) such that R, R0 and P satisfy the
conditions of this claim. Furthermore, since R is valid and ft (D) = ft0 (D0 ) the maximality
of ft0 (D0 ) with respect to R0 follows from the maximality of ft (D) with respect to R and
hence R0 is a valid record of t0 .
To see the converse let P ∈ Pf (v0 ) and a valid record R0 = (a0 , p0 , s0 ) ∈ R(t0 ) be given.
Let R = (a, p, s) be the triple as defined via the conditions (1)-(4). Since R0 is a valid record
it represents some partial solution D0 such that D0 is maximal with respect to R0 . It is easy
to see that the digraph D with node set V (D0 ) ∪ {v0 } and arc set E(D0 ) ∪ { (u, v0 ) : u ∈
P } ∪ { (v0 , u) : u ∈ χ(t0 ) such that v0 ∈ a0 (u) } is acyclic if and only if p satisfies condition
(5), i.e., p is irreflexive. It follows that R represents D if and only if p satisfies condition (5)
and furthermore the maximality of D with respect to R follows from the maximality of D0
with respect to R0 . Hence, R is a valid record for t if and only if it satisfies the conditions
(1)–(5).
Proof of Claim 2 (Lemma 3). Let us first assume that R = (a, p, s) is a valid record of t.
Since R is a valid record it represents some solution D ∈ D(t) such that D is maximal with
respect to R. Now, let R0 = (a0 , p0 , s0 ) be such that a0 (v0 ) = PD (v0 ), a0 (v) = PD (v) for every
v ∈ χ(t), p0 is the union of p and all tuples (v0 , v) and (v, v0 ) where v ∈ χ(t) such that there
is a directed path from v0 to v respectively from v to v0 in D and s0 = s − f (v0 , PD (v0 )).
Note that because of Proposition 5 we can assume that PD (v0 ) ∈ Pf (v0 ) and hence D is
also represented by R0 . It follows from the maximality of D with respect to R that R0 is a
maximal element in R(t0 )[t].
To see the converse let R = (a, p, s) be a maximal element in R(t0 )[t]. Since R ∈ R(t0 )[t]
it follows that there is a record R0 = (a0 , p0 , s0 ) ∈ R(t0 ) such that R = R0 [t]. Hence, there is
a partial solution D represented by R0 such that ft0 (D) = s0 is maximal with respect to all
partial solutions represented by R0 . Clearly, D is also represented by R and the maximality
of D with respect to R follows from the fact that R is a maximal element in R(t0 )[t].
Proof of Claim 3 (Lemma 4). Let us first assume that R = (a, p, s) is a valid record for t.
Since R is valid it follows that it represents a partial solution D ∈ D(t) such that D is
maximal with respect to R. Let D1 = D[χ∗ (t1 )] and D2 = D[χ∗ (t2 )], i.e., D1 and D2
are the two subdigraphs of D induced by the nodes contained in the nodes of the subtrees
293

Ordyniak & Szeider

rooted at t1 and t2 , respectively. It follows from Proposition 3 that D = D1 ∪ D2 and
V (D1 ) ∩ V (D2 ) = χ(t). Hence, D1 and D2 are partial solutions for t1 and t2 , respectively.
For i ∈ {1, 2}, let Ri = (ai , pi , si ) be such that a = ai , (v, w) ∈ pi if and only if there is
a directed path from v to w in Di and si = fti (Di ). It follows directly from the definition
that R1 and R2 represent D1 and D2 , respectively, and since ft (D) = ft1 (D1 ) + ft2 (D2 ) the
maximality of D with respect to R implies the maximality of D1 and D2 with respect to
R1 and R2 , respectively. Hence, R1 and R2 are valid records for t1 and t2 , respectively, and
it is easy to see that R, R1 and R2 satisfy the conditions of the claim.
To see the converse let us assume that we are given R1 = (a1 , p1 , s1 ) ∈ R(t1 ) and
R2 = (a2 , p2 , s2 ) ∈ R(t2 ) and that the triple R = (a, p, s) as defined by the conditions (1)–
(3) satisfies condition (4). Since R1 and R2 are valid it follows that both represent some
partial solutions D1 and D2 such that D1 and D2 are maximal with respect to R1 and R2 ,
respectively. Furthermore, using Proposition 3 it follows that V (D1 ) ∩ V (D2 ) = χ(t) and
hence it follows from condition (4) that D = D1 ∪ D2 is a partial solution represented by R.
Now, ft (D) = ft1 (D1 ) + ft2 (D2 ) = s1 + s2 = s and the maximality of D with respect to
R follows from the maximality of D1 and D2 with respect to R1 and R2 , respectively. It
follows that R is a valid record for t.

Proof of Claim 6 (Theorem 3). (1)⇒(2) Suppose G has a k-clique K. Then f (D(E(K))) =
nkα − |V (K)|α + |E(K)|ε = nkα + k ≥ s and it remains to show that D(E(K)) is acyclic.
To see this note that every cycle in D(E(K)) has to use at least one node from V , because
D(E(K)) does not contain an arc between two nodes in A. But since K is a clique, every
node v ∈ V is either a sink, i.e., v has only incoming arcs, or a source, i.e., v has only
outgoing arcs and hence no cycle can use a node from V .
(2)⇒(3) Suppose D is a dag with f (D) ≥ s. Let A0 be the set of nodes in A with
f (a, PD (a)) = ε for every a ∈ A0 . It follows from the definition of f that for every aij ∈ A0
there is a unique edge e between a node in Vi and a node in Vj in G with PD (aij ) = e.
Let E 0 ⊆ E(G) be the set of all edges in G that correspond to a node in A0 . It follows
that E 0 is representable and we claim that every node v ∈ N has at least the local score
in D(E 0 ) as it has in D. By construction of D(E 0 ) the claim is trivially satisfied for every
node a ∈ A. Similarly, for every v ∈ V \ V (E 0 ) it holds that f (v, PD(E 0 ) (v)) = α and hence
f (v, PD(E 0 ) (v)) ≥ f (v, PD (v)). Furthermore, for every v ∈ V (E 0 ) it holds that f (v, PD (v)) =
0 and hence again f (v, PD(E 0 ) (v)) ≥ f (v, PD (v)). It follows that f (D(E 0 )) ≥ f (D) ≥ s.
(3)⇒(1) Suppose that E 0 ⊆ E(G) is a representable
edge set of G with f (D(E 0 )) ≥ s.

We will show that |V (E 0 )| = k and |E 0 | = k2 which implies that E 0 is the edge set of a
k-clique in G.
294

Parameterized Complexity of Exact BNSL

Because E 0 is an edge set, it holds that |E 0 | ≤

|V (E 0 )|
2



and hence:

f (D(E 0 )) − nkα = −|V (E 0 )|α + |E 0 |ε


|V (E 0 )|
0
≤ −|V (E )|α +
ε
2


|V (E 0 )|
0
2
= −|V (E )|(k − k − 1) +
2k
2
= −|V (E 0 )|(k 2 − k − 1) + (|V (E 0 )|2 − |V (E 0 )|)k
= −|V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )|k − |V (E 0 )|k + |V (E 0 )|
= −|V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )|
Because f (D(E 0 )) ≥ s, it follows that −|V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )| ≥ 1 and hence:
−|V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )| ≥ 1
−k 2 + |V (E 0 )|k + 1 ≥
|V (E 0 )| ≥

1
|V (E 0 )|
1
2
|V (E 0 )| + k − 1
k

1
1
|V (E 0 )| ≥ k − +
k |V (E 0 )|k
Since |V (E 0 )| is an integer and k > 2, we have that |V (E 0 )| ≥ k.

Furthermore, since E 0 is representable it can contain at most k2 edges and hence

f (D(E 0 ))−nkα ≤ −|V (E 0 )|α + k2 ε. Again, it follows from f (D(E 0 )) ≥ s that −|V (E 0 )|α +

k
2 ε ≥ 1 and:
 
k
0
−|V (E )|α +
ε ≥ 1
2
−|V (E 0 )|(k 2 − k − 1) + k 3 − k 2 ≥ 1
−|V (E 0 )|(k 2 − k − 1) ≥ −k 3 + k 2 + 1
−k 3 + k 2 + 1
−|V (E 0 )| ≥
k2 − k − 1
k+1
|V (E 0 )| ≤ k + 2
k −k−1
0
0
Since |V (E )| is an integer and k > 2, it follows that |V (E )| ≤ k and hence |V (E 0 )| = k.
Using this in f (D(E 0 )) − nkα ≥ 1 we get:
−kα + |E 0 |ε ≥ 1
−(k 3 − k 2 − k) + |E 0 |2k ≥ 1
k3 − k2 − k + 1
|E 0 | ≥
2k
2
k − k − 1 + k1
|E 0 | ≥
2
 
1 − k1
k
|E 0 | ≥
−
2
2
295

Ordyniak & Szeider

Because |E 0 | is an integer and k > 2, it follows that |E 0 | ≥
we have that f (D(E 0 )) ≥ s implies |V (E 0 )| = k and |E 0 | =
a k-clique in G.

k
. Putting everything together
2 
k
0
2 . Hence E is the edge set of



Proof of Claim 7 (Theorem 6). We first show the claim for the case that rev ∈ O and
k 0 = 2k + 1.
Let us first assume that G has an independent set S ⊆ V (G) of size at least k. We
obtain D0 from D by reversing the k 0 arcs in { (v 1 , v 2 ), (v 1 , lT1 (v)) : v ∈ S } ∪ {(r2 , r1 )}.
This decreases the score for r1 by α and increases the score for the nodes in { v 1 : v ∈ S }
by ε while the score of all the other nodes of D remains unchanged. Hence, f (D0 ) =
f (D) − α + |S|ε ≥ α + kε = f (D) + 1 > f (D) and it remains to show that D0 is acyclic.
To see this assume that D0 contains a cycle C. Because D is acyclic C must contain at
least one of the newly created arcs in D0 , i.e., C must contain either an arc (v 2 , v 1 ), an arc
(lT1 (v) , v 1 ) for some v ∈ S or the arc (r1 , r2 ). Because r2 is a sink in D0 , i.e., r2 has no
outgoing arcs, the cycle C cannot contain the arc (r1 , r2 ). Similarly, because D0 does not
contain a directed path from v 1 to lT1 (v) the cycle C cannot contain an arc (lT1 (v), v 1 ) for
any v ∈ S. Hence the cycle C must contain an arc (v 2 , v 1 ) for some v ∈ S. So suppose
that C contains the arc (v 2 , v 1 ) for some v ∈ S. Because D0 contains no directed paths
from a node of T2 to a node of T1 it follows that C cannot leave the node v 1 using the arc
(v 1 , lT2 (v)). Consequently, the cycle C must leave the node v 1 towards w2 for some neighbor
w of v in G. Because S is an independent set in G it follows that w ∈
/ S and hence the node
w2 is a sink in D0 contradicting the existence of the cycle C.
To see the reverse direction assume that D0 is a dag obtained from D by reversing at
most k 0 arcs. Note that the nodes in { v 1 : v ∈ V (G) } are the only nodes of D whose scores
are not yet maximum. Hence, in order for D0 to have a higher score than D the score for
at least one of these nodes has to be increased. Now, the score for such a node v 1 for some
v ∈ V (G) can only be increased by reversing the arcs (v 1 , v 2 ) and (v 1 , lT1 (v)). It is easy
to see that reversing the arc (v 1 , lT1 (v)) introduces a cycle C in D that uses only nodes in
V (T1 ) ∪ V (T2 ) ∪ {v 1 }. However, because every such cycle C contains the arc (r2 , r1 ) we
can destroy all of these cycles by additionally reversing the arc (r2 , r1 ). Because reversing
(r2 , r1 ) does only decrease the score of r1 by α this is also the cheapest way to destroy these
cycles. Now, α = (k − 1)ε and it follows that in order to increase the score of D the scores
of at least k nodes in { v 1 : v ∈ V (G) } have to be increased to ε. Let S be the set of nodes
in V (G) such that the score of the nodes in { v 1 : v ∈ S } has been increased in this manner.
As mentioned above |S| ≥ k and it remains to show that S is an independent set in G.
Suppose S is not an independent set and let u, v ∈ S be such that {u, v} ∈ E(G). Then
the arcs (v 2 , v 1 ) and (u2 , u1 ) together with the directed path from v 1 to u2 (using the arcs
in Tv1 and Tu2 ) and the directed path from u1 to v 2 (using the arcs in Tu1 and Tv2 ) form a
cycle in D0 contradicting the acyclicity of D0 . It follows that S is an independent set in G
of size at least k.
Hence, we have shown the theorem for the case rev ∈ O. It remains to show the
theorem for the only remaining non-trivial set with rev ∈
/ O, i.e., the set O = {add, del}.
Now k 0 = 2(2k + 1) and the idea is to replace every reversal of an arc (u, v) by a deletion
(of (u, v)) and an addition (of (v, u)).
296

Parameterized Complexity of Exact BNSL

Proof of Claim 10 (Theorem 8). We will prove Claim 10 with the help of the following
claim.
Claim 11. f (D) ≥ s and D is acyclic if and only if D satisfies the following conditions:
1 D contains at least the following arcs:
– The arc (t0 , d0 ).
– For every 1 ≤ i ≤ n, the arcs (di−1 , di ), (di , ai ), (bi , ti ) and (ti , ti−1 ).
– For every 1 ≤ i ≤ n, 1 ≤ j ≤ m and 1 ≤ l ≤ 3 the arc (xi , lj,l ) if lj,l = xi and
similarly the arc (xi , lj,l ) if lj,l = xi .
– For every 1 ≤ j ≤ m, the arcs (Cj , tn+j ) and (tn+j , tn+j−1 ) and exactly one of
the arcs (lj,1 , Cj ), (lj,2 , Cj ) and (lj,3 , Cj ).
2 For every 1 ≤ i ≤ n the digraph D contains the arcs (ai , xi ) and (xi , bi ) but not the
arcs (ai , xi ) and (xi , bi ) or D contains the arcs (ai , xi ) and (xi , bi ) but not the arcs
(ai , xi ) and (xi , bi ).
3 For 1 ≤ i ≤ n, 1 ≤ j ≤ m and 1 ≤ l ≤ 3, the following holds:
– If lj,l = xi and D contains the arc (lj,l , Cj ) then D does not contain the arc
(ai , xi ).
– If lj,l = xi and D contains the arc (lj,l , Cj ) then D does not contain the arc
(ai , xi ).
We will first show how the previous claim can be used to prove Claim 10. Suppose φ is
satisfiable and let β be a satisfying assignment for φ. Let D be the digraph that satisfies
condition 1 and additionally:
• For every 1 ≤ i ≤ n if β(xi ) = true then D contains the arcs (ai , xi ) and (xi , bi ),
otherwise D contains the arcs (ai , xi ) and (xi , bi ).
• For every 1 ≤ j ≤ m let lj,l any literal in the clause Cj that is satisfied by β; since
β is a satisfying assignment every clause Cj contains such a literal. Then D contains
the arc (lj,l , Cj ).
It follows that D satisfies the conditions 2 and 3 and hence (using Claim 11) f (D) ≥ s and
D is acyclic.
To see the reverse let D be a dag with f (D) ≥ s. It follows from Claim 11 that D
satisfies the conditions 1 –3. We claim that the assignment β with β(xi ) = true if and
only if D does not contain the arc (ai , xi ) is a satisfying assignment for φ. It follows from
condition 1 that for every 1 ≤ j ≤ m the digraph D contains an arc (lj,l , Cj ) for some
1 ≤ l ≤ 3. W.l.o.g., we can assume that lj,l = xi for some 1 ≤ i ≤ n (the case that lj,l = xi
is analog). Again using condition 1 it follows that D contains the arc (xi , lj,l ). Because of
condition 3 the digraph D does not contain the arc (ai , xi ) and hence β(lj,l ) = true.
Hence it only remains to show Claim 11. Let us first show that every dag D with f (D) ≥ s
satisfies conditions 1 –3. To see this observe that every node in V = {x1 , x1 , . . . , xn , xn }
297

Ordyniak & Szeider

has either score 0 or ε. Similarly, every node in V 0 = N \ V has either score 0 or α. It
follows that in every directed graph there are at most 4n + 5m + 2 nodes with score α
and at most 2n nodes with score ε. Hence, the maximum score for every directed graph
is (4n + 5m + 2)α + 2nε. Because α > nε and f (D) ≥ s it follows that in D every node
from V 0 must have score α and similarly at least n of the 2n nodes in V must have score ε.
Hence D satisfies condition 1.
To show condition 2 observe that because for every 1 ≤ i ≤ n the node bi must have
score α in D it holds that exactly one of the arcs (xi , bi ) and (xi , bi ) is in D. Now, if D
contains the arc (xi , bi ) for some 1 ≤ i ≤ n then D cannot contain the arc (ai , xi ) because
otherwise D would contain the cycle (d, ai , xi , bi , t, d). Similarly, if D contains the arc (xi , bi )
for some 1 ≤ i ≤ n then D cannot contain the arc (ai , xi ). It follows that for every 1 ≤ i ≤ n
at least one of the arcs (ai , xi ) and (ai , xi ) is missing in D. Since there are at most n nodes
in V with score 0 it follows that for every 1 ≤ i ≤ n exactly one of the arcs (ai , xi ) and
(ai , xi ) must be in D. It follows that D satisfies condition 2.
To see condition 3 suppose for some 1 ≤ i ≤ n, 1 ≤ j ≤ m and 1 ≤ l ≤ 3 with lj,l = xi
the digraph D contains both arcs (lj,l , Cj ) and (ai , xi ). It follows that D would contain the
cycle (d, ai , xi , lj,l , Cj , t, d), a contradiction. The case that lj,l = xi is analog and hence D
satisfies condition 3.
To see the reverse implication of the claim suppose that D is a digraph that satisfies
the conditions 1 –3. It is easy to see that f (D) = s and it hence only remains to show that
D is acyclic. Because Sf→ − (t0 , d0 ) is acyclic it follows that every cycle in D has to use the
arc (t0 , d0 ). Hence D contains a cycle if and only if there is a directed path P from d0 to t0
in D. It follows from condition 2 that there is no directed path from d0 to some bi in D,
for 1 ≤ i ≤ 3, and hence P cannot contain a node bi . Since the only other nodes in D with
arcs to {t0 , . . . , tn+m } are the nodes C1 , . . . , Cm it follows that P has to use a node Cj for
some 1 ≤ j ≤ m. Because of condition 1 the node Cj has exactly one incoming neighbor
(one of lj,1 , lj,2 , lj,3 ) say lj,l . Again using condition 1 the node lj,l has exactly one incoming
neighbor xi or xi if lj,l = xi or lj,l = xi , respectively. W.l.o.g. let us assume that lj,l = xi . It
follows from condition 3 that xi has no incoming neighbor and hence D contains no directed
path P from d0 to t0 .

References
Bang-Jensen, J., & Gutin, G. (2009). Digraphs (Second edition). Springer Monographs in
Mathematics. Springer-Verlag London Ltd., London.
Bodlaender, H. L. (1993). A tourist guide through treewidth. Acta Cybernetica, 11, 1–21.
Bodlaender, H. L. (2005). Discovering treewidth. In Proceedings of the 31st Conference
on Current Trends in Theory and Practice of Computer Science (SOFSEM’05), Vol.
3381 of Lecture Notes in Computer Science, pp. 1–16. Springer Verlag.
Bodlaender, H. L. (1996). A linear-time algorithm for finding tree-decompositions of small
treewidth. SIAM J. Comput., 25 (6), 1305–1317.
Bodlaender, H. L. (1997). Treewidth: algorithmic techniques and results. In Mathematical foundations of computer science 1997 (Bratislava), Vol. 1295 of Lecture Notes in
Computer Science, pp. 19–36. Springer, Berlin.
298

Parameterized Complexity of Exact BNSL

Bodlaender, H. L., & Koster, A. M. C. A. (2008). Combinatorial optimization on graphs of
bounded treewidth. Comput. J., 51 (3), 255–269.
Chechetka, A., & Guestrin, C. (2007). Efficient principled learning of thin junction trees. In
Platt, J. C., Koller, D., Singer, Y., & Roweis, S. T. (Eds.), Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference
on Neural Information Processing Systems, Vancouver, British Columbia, Canada,
December 3-6, 2007. MIT Press.
Chen, J., Liu, Y., Lu, S., O’Sullivan, B., & Razgon, I. (2008). A fixed-parameter algorithm
for the directed feedback vertex set problem. J. ACM, 55 (5), Art. 21, 19.
Chickering, D. M. (1995). A transformational characterization of equivalent Bayesian network structures. In Uncertainty in artificial intelligence (Montreal, PQ, 1995), pp.
87–98. Morgan Kaufmann, San Francisco, CA.
Chickering, D. M. (1996). Learning Bayesian networks is NP-complete. In Learning from
data (Fort Lauderdale, FL, 1995), Vol. 112 of Lecture Notes in Statist., pp. 121–130.
Springer Verlag.
Chow, C. I., & Liu, C. N. (1968). Approximating discrete probability distributions with
dependence trees. IEEE Transactions on Information Theory, 14, 462–467.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1-2), 5–41.
Dasgupta, S. (1999). Learning polytrees. In Laskey, K. B., & Prade, H. (Eds.), UAI
’99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence,
Stockholm, Sweden, July 30-August 1, 1999, pp. 134–141. Morgan Kaufmann.
de Campos, C. P., Zeng, Z., & Ji, Q. (2009). Structure learning of Bayesian networks using
constraints. In Danyluk, A. P., Bottou, L., & Littman, M. L. (Eds.), Proceedings of
the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, Vol. 382 of ACM International Conference
Proceeding Series, p. 15. ACM.
Dechter, R. (1999). Bucket elimination: a unifying framework for reasoning. Artificial
Intelligence, 113 (1-2), 41–85.
Diestel, R. (2000). Graph Theory (2nd edition)., Vol. 173 of Graduate Texts in Mathematics.
Springer Verlag, New York.
Dow, P. A., & Korf, R. E. (2007). Best-first search for treewidth. In Proceedings of the
Twenty-Second AAAI Conference on Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 1146–1151. AAAI Press.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Monographs in Computer Science. Springer Verlag, New York.
Downey, R. G., & Fellows, M. R. (1995). Fixed-parameter tractability and completeness.
II. On completeness for W [1]. Theoret. Comput. Sci., 141 (1-2), 109–131.
Downey, R. G., Fellows, M. R., & Langston, M. A. (2008). The computer journal special
issue on parameterized complexity: Foreword by the guest editors. The Computer
Journal, 51 (1), 1–6.
299

Ordyniak & Szeider

Elidan, G., & Gould, S. (2008). Learning bounded treewidth Bayesian networks. In Koller,
D., Schuurmans, D., Bengio, Y., & Bottou, L. (Eds.), Advances in Neural Information
Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December
8-11, 2008, pp. 417–424. MIT Press.
Fellows, M. R. (2003). Blow-ups, win/win’s, and crown rules: Some new directions in FPT.
In Bodlaender, H. L. (Ed.), Graph-Theoretic Concepts in Computer Science (WG
2003), Vol. 2880 of Lecture Notes in Computer Science, pp. 1–12. Springer Verlag.
Fellows, M. R., Rosamond, F. A., Fomin, F. V., Lokshtanov, D., Saurabh, S., & Villanger,
Y. (2009). Local search: Is brute-force avoidable?. In Boutilier, C. (Ed.), IJCAI
2009, Proceedings of the 21st International Joint Conference on Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009, pp. 486–491.
Fellows, M. R. (2012) Personal Communication.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory, Vol. XIV of Texts in
Theoretical Computer Science. An EATCS Series. Springer Verlag, Berlin.
Friedman, N., Nachman, I., & Pe’er, D. (1999). Learning Bayesian network structure from
massive datasets: The ”sparse candidate” algorithm. In Laskey, K. B., & Prade, H.
(Eds.), UAI ’99: Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, Stockholm, Sweden, July 30-August 1, 1999, pp. 206–215. Morgan
Kaufmann.
Garey, M. R., & Johnson, D. R. (1979). Computers and Intractability. W. H. Freeman and
Company, New York, San Francisco.
Gaspers, S., Kim, E. J., Ordyniak, S., Saurabh, S., & Szeider, S. (2012). Don’t be strict
in local search!. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial
Intelligence, AAAI 2012, Toronto, Ontaria, Canada, July 22-26, 2012. AAAI Press.
to appear.
Gaspers, S., Koivisto, M., Liedloff, M., Ordyniak, S., & Szeider, S. (2012). On finding
optimal polytrees. In to appear in AAAI 2012.
Gelfand, A., Kask, K., & Dechter, R. (2011). Stopping rules for randomized greedy triangulation schemes. In Burgard, W., & Roth, D. (Eds.), Proceedings of the Twenty-Fifth
AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California,
USA, August 7-11, 2011. AAAI Press.
Gogate, V., & Dechter, R. (2004). A complete anytime algorithm for treewidth. In Proceedings of the Proceedings of the Twentieth Conference Annual Conference on Uncertainty
in Artificial Intelligence (UAI-04), pp. 201–208, Arlington, Virginia. AUAI Press.
Greco, G., & Scarcello, F. (2010). On the power of structural decompositions of graph-based
representations of constraint problems. Artificial Intelligence, 174 (5–6), 382–409.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks: The
combination of knowledge and statistical data. Machine Learning, 20 (3), 197–243.
Impagliazzo, R., Paturi, R., & Zane, F. (2001). Which problems have strongly exponential
complexity?. J. of Computer and System Sciences, 63 (4), 512–530.
300

Parameterized Complexity of Exact BNSL

Karger, D. R., & Srebro, N. (2001). Learning markov networks: maximum bounded treewidth graphs. In SODA, pp. 392–401.
Karp, R. M. (1972). Reducibility among combinatorial problems. In Complexity of computer computations (Proc. Sympos., IBM Thomas J. Watson Res. Center, Yorktown
Heights, N.Y., 1972), pp. 85–103. Plenum, New York.
Kask, K., Gelfand, A., Otten, L., & Dechter, R. (2011). Pushing the power of stochastic
greedy ordering schemes for inference in graphical models. In Burgard, W., & Roth, D.
(Eds.), Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence,
AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press.
Khuller, S., Bhatia, R., & Pless, R. (2003). On local search and placement of meters in
networks. SIAM J. Comput., 32 (2), 470–487.
Kloks, T. (1994). Treewidth: Computations and Approximations. Springer Verlag, Berlin.
Koivisto, M. (2006). Advances in exact Bayesian structure discovery in Bayesian networks.
In UAI ’06, Proceedings of the 22nd Conference in Uncertainty in Artificial Intelligence, July 13-16 2006, Cambridge, MA, USA. AUAI Press.
Kojima, K., Perrier, E., Imoto, S., & Miyano, S. (2010). Optimal search on clustered
structural constraint for learning Bayesian network structure. J. Mach. Learn. Res.,
11, 285–310.
Koster, A. M. C. A., Bodlaender, H. L., & van Hoesel, S. P. M. (2001). Treewidth: Computational experiments. Electronic Notes in Discrete Mathematics, 8, 54–57.
Krokhin, A. A., & Marx, D. (2012). On the hardness of losing weight. ACM Transactions
on Algorithms, 8 (2), 19.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). The necessity of bounded
treewidth for efficient inference in Bayesian networks. In ECAI, pp. 237–242.
Marx, D. (2008). Searching the k-change neighborhood for TSP is W[1]-hard. Oper. Res.
Lett., 36 (1), 31–36.
Marx, D., & Schlotter, I. (2011). Stable assignment with couples: Parameterized complexity
and local search. Discrete Optimization, 8 (1), 25–40.
Meek, C. (2001). Finding a path is harder than finding a tree. J. Artif. Intell. Res., 15,
383–389.
Narasimhan, M., & Bilmes, J. A. (2004). PAC-learning bounded tree-width graphical models. In Chickering, D. M., & Halpern, J. Y. (Eds.), UAI ’04, Proceedings of the 20th
Conference in Uncertainty in Artificial Intelligence, July 7-11 2004, Banff, Canada,
pp. 410–417. AUAI Press.
Niedermeier, R. (2006). Invitation to Fixed-Parameter Algorithms. Oxford Lecture Series
in Mathematics and its Applications. Oxford University Press, Oxford.
Ordyniak, S., & Szeider, S. (2010). Algorithms and complexity results for exact Bayesian
structure learning. In Grünwald, P., & Spirtes, P. (Eds.), Proceedings of UAI 2010, The
26th Conference on Uncertainty in Artificial Intelligence, Catalina Island, California,
USA, July 8-11, 2010. AUAI Press, Corvallis, Oregon.
301

Ordyniak & Szeider

Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models for small gene networks.
In Altman, R. B., Dunker, A. K., Hunter, L., Jung, T. A., & Klein, T. E. (Eds.), Biocomputing 2004, Proceedings of the Pacific Symposium, Hawaii, USA, 6-10 January
2004, pp. 557–567. World Scientific.
Parviainen, P., & Koivisto, M. (2010). Bayesian structure discovery in Bayesian networks
with less space. J. Mach. Learn. Res., 9, 589–596.
Perrier, E., Imoto, S., & Miyano, S. (2008). Finding optimal Bayesian network given a
super-structure. J. Mach. Learn. Res., 9, 2251–2286.
Pieter, A., Daphne, K., & Andrew, Y. N. (2006). Learning factor graphs in polynomial time
and sample complexity. J. Mach. Learn. Res., 7, 1743–1788.
Pietrzak, K. (2003). On the parameterized complexity of the fixed alphabet shortest common supersequence and longest common subsequence problems. J. of Computer and
System Sciences, 67 (4), 757–771.
Silander, T., & Myllymäki, P. (2006). A simple approach for finding the globally optimal Bayesian network structure. In UAI ’06, Proceedings of the 22nd Conference in
Uncertainty in Artificial Intelligence, July 13-16 2006, Cambridge, MA, USA. AUAI
Press.
Szeider, S. (2011). The parameterized complexity of k-flip local search for SAT and MAX
SAT. Discrete Optimization, 8 (1), 139–145.
Teyssier, M., & Koller, D. (2005). Ordering-based search: A simple and effective algorithm
for learning bayesian networks. In UAI ’05, Proceedings of the 21st Conference in
Uncertainty in Artificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, pp. 548–
549. AUAI Press.
Tsamardinos, I., Brown, L., & Aliferis, C. (2006). The max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning, 65, 31–78.
Tsang, E. P. K. (1993). Foundations of Constraint Satisfaction. Academic Press.
Yuan, C., Malone, B., & Wu, X. (2011). Learning optimal Bayesian networks using A*
search. In Walsh, T. (Ed.), IJCAI 2011, Proceedings of the 22nd International Joint
Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011,
pp. 2186–2191. IJCAI/AAAI.

302

Journal of Artificial Intelligence Research 46 (2013) 129-163

Submitted 09/12; published 01/13

Undominated Groves Mechanisms
Mingyu Guo

Mingyu.Guo@liverpool.ac.uk

University of Liverpool, UK

Evangelos Markakis

markakis@gmail.com

Athens University of Economics and Business, Greece

Krzysztof R. Apt

apt@cwi.nl

CWI, the Netherlands

Vincent Conitzer

conitzer@cs.duke.edu

Duke University, USA

Abstract
The family of Groves mechanisms, which includes the well-known VCG mechanism (also
known as the Clarke mechanism), is a family of efficient and strategy-proof mechanisms.
Unfortunately, the Groves mechanisms are generally not budget balanced. That is, under
such mechanisms, payments may flow into or out of the system of the agents, resulting
in deficits or reduced utilities for the agents. We consider the following problem: within
the family of Groves mechanisms, we want to identify mechanisms that give the agents the
highest utilities, under the constraint that these mechanisms must never incur deficits.
We adopt a prior-free approach. We introduce two general measures for comparing
mechanisms in prior-free settings. We say that a non-deficit Groves mechanism M individually dominates another non-deficit Groves mechanism M ′ if for every type profile,
every agent’s utility under M is no less than that under M ′ , and this holds with strict
inequality for at least one type profile and one agent. We say that a non-deficit Groves
mechanism M collectively dominates another non-deficit Groves mechanism M ′ if for every
type profile, the agents’ total utility under M is no less than that under M ′ , and this holds
with strict inequality for at least one type profile. The above definitions induce two partial
orders on non-deficit Groves mechanisms. We study the maximal elements corresponding
to these two partial orders, which we call the individually undominated mechanisms and
the collectively undominated mechanisms, respectively.

1. Introduction
Mechanism design is often employed for coordinating group decision making among agents.
Often, such mechanisms impose payments that agents have to pay to a central authority.
Although maximizing revenue is a desirable objective in many settings (for example, if the
mechanism is an auction designed by the seller), it is not desirable in situations where no
entity is profiting from the payments. Some examples include public project problems as well
as certain resource allocation problems without a seller (e.g., the right to use a shared good in
a given time slot, or the assignment of take-off slots among airline companies). In such cases,
we would like to have mechanisms that minimize payments (or, even better, achieve budget
balance), while maintaining other desirable properties, such as being efficient, strategy-proof
and non-deficit (i.e., the mechanism does not need to be funded by an external source).
c
2013
AI Access Foundation. All rights reserved.

Guo, Markakis, Apt, & Conitzer

The family of Groves mechanisms, which includes the well-known VCG mechanism (also
known as the Clarke mechanism), is a family of efficient and strategy-proof mechanisms. In
many sufficiently general settings, including the settings that we will study in this paper,
the Groves mechanisms are the only efficient and strategy-proof mechanisms (Holmström,
1979). Unfortunately though, the Groves mechanisms are generally not budget balanced.
That is, under such mechanisms, payments may flow into or out of the system of the agents,
resulting in deficits or reduced utilities for the agents. Motivated by this we consider in
this paper the following problem: within the family of Groves mechanisms, we want to
identify mechanisms that give the agents the highest utilities, under the constraint that
these mechanisms never incur deficits.1
We adopt a prior-free approach, where each agent i knows only his own valuation vi ,
and there is no prior probability distribution over the other agents’ values. We introduce
two natural measures for comparing mechanisms in prior-free settings. Given a performance
indicator, we say that mechanism M individually dominates mechanism M ′ if for every type
profile of the agents, M performs no worse than M ′ from the perspective of each individual
agent, and this holds with strict inequality for at least one type profile and one agent.
We say that mechanism M collectively dominates mechanism M ′ if for every type profile,
M performs no worse than M ′ from the perspective of the set of agents as a whole, and
this holds with strict inequality for at least one type profile. In this paper, we focus on
maximizing the agents’ utilities. Given this specific performance indicator, individual and
collective dominance are determined by comparing either individual utilities or the sum of
the agents’ utilities, respectively.
The above definitions induce two partial orders on non-deficit Groves mechanisms. Our
goal in this work is to identify and study the maximal elements corresponding to these two
partial orders, which we call the individually undominated (non-deficit Groves) mechanisms
and the collectively undominated (non-deficit Groves) mechanisms, respectively. It should be
noted that the partial orders we focus on may be different from the partial orders induced by
other performance indicators, e.g., if the criterion is the revenue extracted from the agents.
1.1 Structure of the Paper
The presentation of our results is structured as follows: In Sections 2 and 3, we formally
define the notions of individual and collective dominance, as well as the family of Groves
mechanisms, and then provide some basic observations. We also establish some general
properties of anonymous Groves mechanisms which we use later on, and which may be
of independent interest. We then begin our study of individual dominance in Section 4,
where we give a characterization of individually undominated mechanisms. We also propose
two techniques for transforming any given non-deficit Groves mechanism into one that is
individually undominated.
In Sections 5 and 6 we study the question of finding collectively undominated mechanisms in two settings. The first (Section 5) is auctions of multiple identical units with
unit-demand bidders. In this setting, the VCG mechanism is collectively dominated by
1. The agents’ utilities may be further increased if we also consider mechanisms outside of the Groves
family (Guo & Conitzer, 2008a; de Clippel, Naroditskiy, & Greenwald, 2009; Faltings, 2005; Guo,
Naroditskiy, Conitzer, Greenwald, & Jennings, 2011), but in this paper we take efficiency as a hard
constraint.

130

Undominated Groves Mechanisms

other non-deficit Groves mechanisms, such as the Bailey-Cavallo mechanism (Bailey, 1997;
Cavallo, 2006). We obtain a complete characterization of collectively undominated mechanisms that are anonymous and linear (meaning that the redistribution is a linear function
of the ordered type profile; see Section 5 for the definition). In particular, we show that
the collectively undominated mechanisms that are anonymous and linear are exactly the
Optimal-in-Expectation Linear (OEL) redistribution mechanisms, which include the BaileyCavallo mechanism and were introduced by Guo and Conitzer (2010). The second setting
(Section 6) is public project problems, where agents must decide on whether and how to
finance a project. We show that in the case where the agents have identical participation costs, the VCG mechanism is collectively undominated. On the other hand, when the
participation costs can be different across agents, there exist mechanisms that collectively
dominate VCG. We finally show that when the participation costs are different across agents,
the VCG mechanism remains collectively undominated among all pay-only mechanisms.
1.2 Related Work
How to efficiently allocate resources among a group of competing agents is a well-studied
topic in economics literature. For example, the famous Myerson-Satterthwaite Theorem (Myerson & Satterthwaite, 1983) rules out the existence of efficient, Bayes-Nash incentive compatible, budget-balanced, and individually rational mechanisms. Cramton, Gibbons, and
Klemperer (1987) characterized the Bayes-Nash incentive compatible and individually rational mechanisms for dissolving a partnership, and gave the necessary and sufficient condition
for the possibility of dissolving partnership efficiently.
The main difference between these papers and ours is that we adopt a prior-free approach. That is, we do not assume that we know the prior distribution of the agents’
valuations. As a result of this, our notion of truthfulness is strategy-proofness, which is
stronger than Bayes-Nash incentive compatibility. In many sufficiently general settings,
including the settings that we will study in this paper, the Groves mechanisms are the only
efficient and strategy-proof mechanisms (Holmström, 1979). That is, the search of undominated Groves mechanisms is, in many settings, the search of efficient, strategy-proof, and
non-deficit mechanisms that are closest to budget-balance.
Recently, there has been a series of works on VCG redistribution mechanisms, which
are mechanisms that make social decisions according to the efficient and strategy-proof
VCG mechanism, and then redistribute some of the VCG payments back to the agents,
under certain constraints, such as that an agent’s redistribution should be independent
of his own type (therefore ensuring strategy-proofness), and that the total redistribution
should never exceed the total VCG payment (therefore ensuring non-deficit). Actually,
any non-deficit Groves mechanism can be interpreted as such a VCG-based redistribution
mechanism, and any (non-deficit) VCG redistribution mechanism corresponds to a nondeficit Groves mechanism (more details on this are provided in Section 2).
One example of a redistribution mechanism is the Bailey-Cavallo (BC) mechanism (Cavallo, 2006).2 Under the BC mechanism, every agent, besides participating in the VCG
2. In settings that are revenue monotonic, the Cavallo (2006) mechanism coincides with a mechanism
discovered earlier by Bailey (1997). The Bailey-Cavallo mechanism for a single-item auction was also
independently discovered by Porter, Shoham, and Tennenholtz (2004).

131

Guo, Markakis, Apt, & Conitzer

mechanism, also receives n1 times the minimal VCG revenue that could have been obtained
by changing this agent’s own bid. In some settings (e.g., a single-item auction), the BC
mechanism can successfully redistribute a large portion of the VCG payments back to the
agents. That is, in such settings, the BC mechanism both individually and collectively
dominates the VCG mechanism.
Guo and Conitzer (2009) proposed another VCG redistribution mechanism called the
worst-case optimal (WCO) redistribution mechanism, in the setting of multi-unit auctions
with nonincreasing marginal values. WCO is optimal in terms of the fraction of total VCG
payment redistributed in the worst case.3 Moulin (2009) independently derived WCO under
a slightly different worst-case optimality notion (in the more restrictive setting of multi-unit
auctions with unit demand only). Guo and Conitzer (2010) also proposed a family of VCG
redistribution mechanisms that aim to maximize the expected amount of VCG payment
redistributed, in the setting of multi-unit auctions with unit demand. The members of this
family are called the Optimal-in-Expectation Linear (OEL) redistribution mechanisms.
Finally, the paper that is the closest to what we study here is an early work by Moulin on
collectively undominated non-deficit Groves mechanisms (Moulin, 1986). It deals with the
problem of selecting an efficient public decision out of finitely many costless alternatives.4
Each agent submits to the central authority his utility for each alternative. Subsequently,
the central authority makes a decision that maximizes the social welfare. Moulin (1986,
Lemma 2) showed that the VCG mechanism is collectively undominated in the above setting.
This result generalizes an earlier result for the case of two public decisions by Laffont and
Maskin (1997).

2. Preliminaries
We first briefly review payment-based mechanisms (see, e.g., Mas-Colell, Whinston, &
Green, 1995).
2.1 Payment-Based Mechanisms
Assume that there is a set of possible outcomes or decisions D, a set {1, . . ., n} of agents
where n ≥ 2, and for each agent i, a set of types Θi and an (initial ) utility function
vi : D × Θi → R. Let Θ := Θ1 × · · · × Θn .
In a (direct revelation) mechanism, each agent reports a type θi ∈ Θi and based on this,
the mechanism selects an outcome and a payment to be made by every agent. Hence a
mechanism is given by a pair of functions (f, t), where f is the decision function and t is the
payment function that determines the agents’ payments, i.e., f : Θ → D, and t : Θ → Rn .
We put ti (θ) := (t(θ))i , i.e., the function ti computes the payment of agent i. For each
vector θ of announced types, if ti (θ) ≥ 0, agent i pays ti (θ), and if ti (θ) < 0, he receives
|ti (θ)|. When the true type of agent i is θi and his announced type is θi′ , his final utility
function is defined by
ui ((f, t)(θi′ , θ−i ), θi ) := vi (f (θi′ , θ−i ), θi ) − ti (θi′ , θ−i ),
3. This notion of worst-case optimality was also studied for more general settings (Gujar & Narahari, 2011;
Guo, 2011, 2012).
4. In our public project model, there is a cost associated with building the project.

132

Undominated Groves Mechanisms

where θ−i is the vector of types announced by the other agents.
2.2 Properties of Payment-Based Mechanisms
We say that a payment-based mechanism (f, t) is
P
P
• efficient if for all θ ∈ Θ and d ∈ D, ni=1 vi (f (θ), θi ) ≥ ni=1 vi (d, θi ),
P
• budget-balanced if ni=1 ti (θ) = 0 for all θ ∈ Θ,
P
• non-deficit if ni=1 ti (θ) ≥ 0 for all θ, i.e., the mechanism does not need to be funded
by an external source,
• pay-only if ti (θ) ≥ 0 for all θ and all i ∈ {1, . . ., n},
• strategy-proof if for all θ, i ∈ {1, . . ., n}, and θi′ ,
ui ((f, t)(θi , θ−i ), θi ) ≥ ui ((f, t)(θi′ , θ−i ), θi ),
i.e., for each agent i, reporting a false type, here θi′ , is not profitable.
2.3 Individual and Collective Dominance
We consider prior-free settings, where each agent i knows only his own function vi , and
there is no belief or prior probability distribution regarding the other agents’ initial utilities.
Payment-based mechanisms can naturally be compared in terms of either the effect on each
individual agent or the global effect on the whole set of agents. We therefore introduce
two measures for comparing such mechanisms. Given a performance indicator5 , we say
that mechanism (f ′ , t′ ) individually dominates mechanism (f, t) if for every type profile,
(f ′ , t′ ) performs no worse than (f, t) from the perspective of every agent, and this holds
with strict inequality for at least one type profile and one agent. We say that mechanism
(f ′ , t′ ) collectively dominates mechanism (f, t) if for every type profile, (f ′ , t′ ) performs no
worse than (f, t) from the perspective of the whole agent system, and this holds with strict
inequality for at least one type profile. In this paper, we focus on maximizing the agents’
utilities. Given this specific performance indicator, individual and collective dominance are
captured by the following definitions:
Definition 2.1 Given two payment-based mechanisms (f, t) and (f ′ , t′ ), we say that (f ′ , t′ )
individually dominates (f, t) if
• for all θ ∈ Θ and all i ∈ {1, . . ., n}, ui ((f, t)(θ), θi ) ≤ ui ((f ′ , t′ )(θ), θi ),
• for some θ ∈ Θ and some i ∈ {1, . . ., n}, ui ((f, t)(θ), θi ) < ui ((f ′ , t′ )(θ), θi ).
Definition 2.2 Given two payment-based mechanisms (f, t) and (f ′ , t′ ), we say that (f ′ , t′ )
collectively dominates (f, t) if
5. By a performance indicator we mean a function of the mechanism’s outcome that serves as a measure
for comparing mechanisms. E.g., it can be the final utility of an agent, or an arbitrary function of it,
or a function of the agent’s payment or any other function that depends on the decision rule and the
payment rule of the mechanism.

133

Guo, Markakis, Apt, & Conitzer

• for all θ ∈ Θ,

Pn

• for some θ ∈ Θ,

i=1 ui ((f, t)(θ), θi )

Pn

≤

i=1 ui ((f, t)(θ), θi )

Pn

i=1 ui ((f

<

Pn

′ , t′ )(θ), θ

i=1 ui ((f

i ),

′ , t′ )(θ), θ

i ).

For two payment-based mechanisms (f, t) and (f ′ , t′ ), clearly if (f ′ , t′ ) individually dominates (f, t), then it also collectively dominates (f, t). Theorem 3.4 shows that the reverse
implication however does not need to hold, even if we limit ourselves to special types of
mechanisms. That is, the fact that (f ′ , t′ ) collectively dominates (f, t) does not imply that
(f ′ , t′ ) individually dominates (f, t).
Given a set Z of payment-based mechanisms, individual and collective dominance induce
two partial orders on Z, and we are interested in studying the maximal elements with
respect to these partial orders. A maximal element with respect to the first partial order
will be called an individually undominated mechanism, i.e., it is a mechanism that is not
individually dominated by any other mechanism in Z. A maximal element for the second
partial order will be called a collectively undominated mechanism, i.e., it is a mechanism
that is not collectively dominated by any other mechanism in Z. The maximal elements
with respect to the two partial orders may differ and in particular, the notion of collectively
undominated mechanisms is generally a stronger notion. Clearly, if (f ′ , t′ ) ∈ Z is collectively
undominated, then it is also individually undominated. The reverse may not be true,
examples of which are provided in Section 4.2.
If we focus on the same decision function f , then individual and collective dominance are
strictly due to the difference of the payment functions. Hence, (f, t′ ) individually dominates
(f, t) (or simply t′ individually dominates t) if and only if
• for all θ ∈ Θ and all i ∈ {1, . . ., n}, ti (θ) ≥ t′i (θ), and
• for some θ ∈ Θ and some i ∈ {1, . . ., n}, ti (θ) > t′i (θ),
and t′ collectively dominates t if
P
P
• for all θ ∈ Θ, ni=1 ti (θ) ≥ ni=1 t′i (θ), and
P
P
• for some θ ∈ Θ, ni=1 ti (θ) > ni=1 t′i (θ).

We now define two transformations on payment-based mechanisms originating from the
same decision function. Both transformations build upon the surplus-guarantee concept
(Cavallo, 2006) for the specific case of the VCG mechanism6 .
Consider a payment-based mechanism
Pn (f, t). Given θ = (θ1 , . . ., θn ), let T (θ) be the
total amount of payments, i.e., T (θ) := i=1 ti (θ). For each i ∈ {1, . . ., n} let
SiBCGC (θ−i ) := ′inf T (θi′ , θ−i ).
θi ∈Θi

In other words, SiBCGC (θ−i ) is the surplus guarantee independent of the report of agent i.
We then define the payment-based mechanism (f, tBCGC ) by setting for i ∈ {1, . . ., n}
tBCGC
(θ) := ti (θ) −
i

SiBCGC (θ−i )
.
n

6. The first transformation was originally defined by Bailey (1997) and Cavallo (2006) for the specific case
of the VCG mechanism and by Guo and Conitzer (2008b) for non-deficit Groves mechanisms. We call
it the BCGC transformation after the authors of these papers (Bailey, Cavallo, Guo, Conitzer).

134

Undominated Groves Mechanisms

Also, for a fixed agent j, we define the payment-based mechanism (f, tBCGC(j) ) by
setting for i ∈ {1, . . ., n}

ti (θ) − SiBCGC (θ−i )
if i = j
BCGC(j)
(θ) :=
ti
ti (θ)
if i 6= j
After the first transformation (from (f, t) to (f, tBCGC )), every agent receives an additional7 amount of n1 times the surplus guarantee independent of his own type. During the
second transformation (from (f, t) to (f, tBCGC(j) ), agent j is chosen to be the only agent
who receives an additional amount. This additional amount equals the entirety of the surplus guarantee independent of j’s own type. For both transformations the agents’ additional
payments are independent of their own types, thus the strategy-proofness is maintained: if
(f, t) is strategy-proof, then so are (f, tBCGC ) and (f, tBCGC(j) ) for all j.
The following observations generalize some of the earlier results by Bailey (1997) and
Cavallo (2006).
Proposition 2.3
(i) Each payment-based mechanism of the form tBCGC is non-deficit.
(ii) If t is non-deficit, then either t and tBCGC coincide or tBCGC individually (and hence
also collectively) dominates t.
Proof. (i) For all θ and i ∈ {1, . . ., n} we have T (θ) ≥ SiBCGC (θ−i ), so
T

BCGC

(θ) =
=

n
X

i=1
n
X
i=1

tBCGC
(θ)
i

= T (θ) −

n
X
S BCGC (θ−i )
i

i=1

n

T (θ) − SiBCGC (θ−i )
≥ 0.
n

(ii) If t is non-deficit, then for all θ and all i ∈ {1, . . ., n} we have SiBCGC (θ−i ) ≥ 0, and
hence tBCGC
(θ) ≤ ti (θ).
2
i
The same claims hold for tBCGC(j) for j ∈ {1, . . ., n}, with equally simple proofs.

3. Groves Mechanisms
We first briefly review Groves mechanisms.
3.1 Preliminaries
Recall that a Groves (1973) Mechanism is a payment-based mechanism (f, t) such that
the following hold8 :
7. Receiving an additional positive amount means paying less and receiving an additional negative amount
means paying more.
P
8. Here and below j6=i is a shorthand for the summation over all j ∈ {1, . . ., n}, j 6= i.

135

Guo, Markakis, Apt, & Conitzer

P
• f (θ) ∈ arg maxd∈D ni=1 vi (d, θi ), i.e., the chosen outcome maximizes the allocation
welfare (the agents’ total valuation),
• ti : Θ → R is defined by ti (θ) := hi (θ−i ) − gi (θ), where
P
• gi (θ) := j6=i vj (f (θ), θj ),
• hi : Θ−i → R is an arbitrary function.

So gi (θ) represents the allocation welfare from the decision f (θ) with agent i ignored.
Recall now the following crucial result (see, e.g., Mas-Colell et al., 1995).
Groves Theorem (Groves, 1973) Every Groves mechanism is efficient and strategyproof.
For several decision problems, the only efficient and strategy-proof payment-based mechanisms are Groves mechanisms. This is implied by a general result by Holmström (1979),
which covers the two domains that we consider in Sections 5 and 6, and explains our focus
on Groves mechanisms. Hence from now on, we use the term “mechanism” to refer to a
Groves mechanism.
Focusing on the set of non-deficit Groves mechanisms, individually (respectively, collectively) undominated mechanisms are the mechanisms from this set that are not individually
(respectively, collectively) dominated by any other non-deficit Groves mechanism. As mentioned earlier, no matter which domain and which set of mechanisms we consider, collective
undominance always implies individual undominance. In Section 4.2 we show two examples of single-item auction scenarios, where collective undominance is strictly stronger than
individual undominance, for non-deficit Groves mechanisms. That is, there exists an individually undominated non-deficit Groves mechanism that is collectively dominated.
Recall that a special Groves mechanism, called the VCG or Clarke (1971) mechanism,
is obtained using9
X
vj (d, θj ).
hi (θ−i ) := max
d∈D

j6=i

In this case
ti (θ) := max
d∈D

X

vj (d, θj ) −

X

vj (f (θ), θj ),

j6=i

j6=i

which shows that the VCG mechanism is pay-only.
In what follows we introduce a slightly different notation to describe Groves mechanisms,
that makes the rest of our presentation more convenient. First, we denote the payment
function ti of the VCG mechanism by V CGi . Note now that each Groves mechanism (f, t)
can be defined in terms of the VCG mechanism by setting ti (θ) := V CGi (θ)−ri (θ−i ), where
ri : Θ−i → R is some function of θ−i . We refer then to r := (r1 , . . ., rn ) as a redistribution
function. Hence each Groves mechanism can be identified with a redistribution function r
and can be viewed as the VCG mechanism combined with a redistribution. That is, under
r the agents first participate in the VCG mechanism. Then, on top of that, agent i also
9. Here and below, whenever D is not a finite set, in order to ensure that the considered maximum exists,
we assume that f is continuous, and so is vi for each i, and also that the set D and all Θi are compact
subsets of some Rk .

136

Undominated Groves Mechanisms

receives a redistribution
amountPequal to ri (θ−i ). By definition, a Groves mechanism r is
P
non-deficit iff ni=1 V CGi (θ) ≥ ni=1 ri (θ−i ) for all θ ∈ Θ.
3.2 Dominance Relations
Using the new notation above, individual and collective dominance (among non-deficit
Groves mechanisms) can be described as follows:
Definition 3.1 A non-deficit Groves mechanism r′ individually dominates another nondeficit Groves mechanism r if
• for all i and all θ, ri′ (θ−i ) ≥ ri (θ−i ),
• for some i and some θ, ri′ (θ−i ) > ri (θ−i ).
Definition 3.2 A non-deficit Groves mechanism r′ collectively dominates another nondeficit Groves mechanism r if
P
P
• for all θ, i ri′ (θ−i ) ≥ i ri (θ−i ),
P
P
• for some θ, i ri′ (θ−i ) > i ri (θ−i ).
We now consider the mechanism that results from applying the BCGC transformation
to the VCG mechanism. We refer to this as the Bailey-Cavallo mechanism or simply the
BC mechanism (Bailey, 1997; Cavallo, 2006). The VCG mechanism is characterized by
the constant redistribution function rVCG = (0, 0, . . ., 0). After the BCGC transformation,
every agent i receives an additional amount of n1 times the surplus guarantee SiBCGC (θ−i ),
independent of his own type. That is, the BC mechanism is also a Groves mechanism, and
its redistribution function is
1
1
1
rBC = ( S1BCGC , S2BCGC , . . ., SnBCGC ).
n
n
n
Let θ′ := (θ1 , . . ., θi−1 , θi′ , θi+1 , . . ., θn ). Then starting from the VCG mechanism, we
have


SiBCGC (θ−i ) = ′inf

θi ∈Θi

n
X

k=1

max
d∈D

X
j6=k

vj (d, θj′ ) −

X
j6=k

vj (f (θ′ ), θj′ ) ,

that is,


SiBCGC (θ−i ) = ′inf 
θi ∈Θi

n
X

k=1

max
d∈D

X

vj (d, θj′ ) − (n − 1)

j6=k

n
X

k=1



vk (f (θ′ ), θk′ )

(1)

In many settings, we have that for all θ and for all i, SiBCGC (θ−i ) = 0, and consequently
the VCG and BC mechanisms coincide (e.g., see Proposition 6.1). Whenever they do not,
by Proposition 2.3(ii), BC individually and collectively dominates VCG. This is the case
for the single-item auction, as it can be seen that there SiBCGC (θ−i ) = [θ−i ]2 , where [θ−i ]2
is the second-highest bid among bids other than agent i’s own bid.
137

Guo, Markakis, Apt, & Conitzer

3.3 Anonymous Groves Mechanisms
Some of the proofs of our main results are obtained by arguing first about a special class of
Groves mechanisms, called anonymous Groves mechanisms. We provide here some results
about this class that we will utilize in later sections. We call a function f : An → B
permutation independent if for all permutations π of {1, . . ., n}, f = f ◦ π. Following
Moulin (1986), we call a Groves mechanism r = (r1 , . . ., rn ) anonymous 10 if
• all type sets Θi are equal,
• all functions ri coincide and each of them is permutation independent.
Hence, an anonymous Groves mechanism is uniquely determined by a single function r :
Θn−1 → R.
In general, the VCG mechanism is not anonymous. But it is anonymous when all the
type sets are equal and all the initial utility functions vi coincide. This is the case in the
two domains that we consider in later sections.
For any θ ∈ Θ and any permutation π of {1, . . ., n} we define θπ ∈ Θ by letting
θiπ := θπ−1 (i) .
Denote by Π(k) the set of all permutations of the set {1, . . ., k}. Given a Groves mechanism r := (r1 , . . ., rn ) for which the type set Θi is the same for every agent (and equal
to some set Θ0 ), we construct now a function r′ : Θ0n−1 → R, following Moulin (1986), by
setting
P
n
π
X
π∈Π(n−1) rj (x )
′
r (x) :=
,
n!
j=1

xπ

θπ .

where
is defined analogously to
Note that r′ is permutation independent, so r′ is an anonymous Groves mechanism. The
following lemma, which can be of independent interest, shows that some of the properties
of r transfer to r′ .
Lemma 3.3 Consider a Groves P
mechanism r and the corresponding anonymous Groves
n
′
mechanism r . Let V CG(θ) :=
i=1 V CGi (θ), and suppose that the V CG function is
permutation independent. Then:
(i) If r is non-deficit, so is r′ .
(ii) If an anonymous Groves mechanism r0 is collectively dominated by r, then it is collectively dominated by r′ .
Proof. For all θ ∈ Θ we have
n
X
i=1

ri′ (θ−i )

=

Pn P
i=1

π∈Π(n−1)

Pn

n!

j=1 rj ((θ−i )

π)

=

10. Our definition is slightly different than the one introduced by Moulin (1986) in that no conditions are
put on the utility functions and the permutation independence refers only to the redistribution function.

138

Undominated Groves Mechanisms

Pn P
i=1

π
π∈Π(n) ri (θ−i )

n!
where the last equality holds since in both terms we aggregate over all applications of all ri
functions to all permutations of n − 1 elements of θ.
Let t and t′ be the payment functions of the mechanisms r and r′ , respectively. We have
n
X

t′i (θ) = V CG(θ) −

n
X

ri′ (θ−i )

i=1

i=1

and for all π ∈ Π(n)

n
X

π

π

ti (θ ) = V CG(θ ) −

i=1

n
X

π
ri (θ−i
).

i=1

Hence by the assumption about V CG(θ) it follows that
P
Pn
n
π
X
π∈Π(n)
i=1 ti (θ )
′
ti (θ) =
n!

(2)

i=1

(i) is now an immediate consequence of (2).
To prove (ii) let t0 be the payment function of r0 . r collectively dominates r0 , so for all
θ ∈ Θ and all π ∈ Π(n)
n
n
X
X
π
t0i (θπ )
ti (θ ) ≤
i=1

i=1

with at least one inequality strict. Hence for all θ ∈ Θ
P
Pn
P
Pn 0 π
π
π∈Π(n)
i=1 ti (θ )
π∈Π(n)
i=1 ti (θ )
≤
n!
n!
with at least one inequality strict.
But the fact that r0 is anonymous and the assumption about V CG(θ) imply that for
all θ ∈ Θ and all permutations π of {1, . . ., n}
n
X

n
X

t0i (θπ ) =

t0i (θ),

i=1

i=1

so by (2) and the above inequality, we have that for all θ ∈ Θ
n
X
i=1

t′i (θ)

≤

n
X

t0i (θ),

i=1

with at least one inequality strict.

2

The assumption in Lemma 3.3 of permutation independence of V CG(θ) is satisfied in
both of the domains that we consider in Sections 5 and 6. So item (ii) states that if a Groves
mechanism considered in the sequel is not collectively undominated, then it is collectively
dominated by an anonymous Groves mechanism.
We now prove that for a large class of Groves mechanisms that includes the ones we
study in the sequel the introduced relations of dominance differ.
139

Guo, Markakis, Apt, & Conitzer

Theorem 3.4 Suppose n ≥ 3. Assume that the sets of types Θi are all equal to the set Θ0
which contains at least n−1 elements. Then two non-deficit anonymous Groves mechanisms
r and r′ exist such that r collectively dominates r′ but r does not individually dominate r′ .
Proof. Fix a non-deficit anonymous Groves mechanism determined by a permutation
independent function r : Θ0n−1 → R.
Let a1 , . . ., an−1 be arbitrary different elements of Θ0 . Define a permutation independent
function q : Θ0n−1 → R by putting

−1 if x is a permutation of (a1 , . . ., an−1 )
q(x) :=
2 otherwise
Then for each θ ∈ Θn0 at most two of its subsequences
θ−i may form a permutation
Pn
of (a1 , . . ., an−1 ). But n ≥ 3, so for all θ ∈ Θ,
q(θ
)
≥ 0. This implies that the
−i
i=1
anonymous Groves mechanism determined by the function r′ := r − q is non-deficit.
Trivially, the sum of payments under r is less than or equal to the sum of payments
′
under r′ , since r redistributes
Pn more money than r . Moreover for some θ ∈ Θ, for instance
θ = (a1 , . . ., a1 ), we have i=1 q(θ−i ) > 0. Finally, by definition, q(a1 , . . ., an−1 ) = −1.
These imply that r′ is collectively dominated by r but is not individually dominated by
r.
2

4. Individually Undominated Mechanisms: Characterization and
Algorithmic Techniques for General Domains
In this section, we focus on individually undominated non-deficit Groves mechanisms.
4.1 Non-deficit Groves Mechanisms
We start with a characterization of non-deficit Groves mechanisms.
Recall first that for a
P
type profile θ, we denote by V CG(θ) the total VCG payment, ni=1 V CGi (θ).
Proposition 4.1 A Groves mechanism r is non-deficit if and only if for all i and all θ,
X
′
rj (θ−j
)}
(3)
ri (θ−i ) ≤ ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

j6=i

′
are the reported types of the agents other than j when θi is replaced by θi′ .
Here, θ−j

Proof. We first prove
the “if” direction. For any i and θ, Equation 3 implies that
P ri (θ−i ) ≤
P
′
′
′
′
rj (θ−j ) for any θi ∈ Θi . If we let θi = θi , we obtain
V CG(θi , θ−i ) −
j rj (θ−j ) ≤
j6=i
P
V CG(θi , θ−i ) = i V CGi (θ). Thus, the non-deficit property holds.
We now prove the “only if” direction. ToP
ensure the non-deficit property, for any i,
′ ) ≤ V CG(θ ′ , θ ), or equivalently
′
rj (θ−j
any θ−i , and any θi , we must have ri (θ−i ) +
i −i
j6
=
i
P
′ ). Since θ ′ is arbitrary, Equation 3 follows.
rj (θ−j
2
ri (θ−i ) ≤ V CG(θi′ , θ−i ) −
i
j6=i

By replacing the “≤” in Equation 3 by “=”, we get a characterization of individually
undominated non-deficit Groves mechanisms.
140

Undominated Groves Mechanisms

Theorem 4.2 A Groves mechanism r is non-deficit and individually undominated if and
only if for all i and all θ,
X
′
ri (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
rj (θ−j
)}
(4)
θi ∈Θi

j6=i

′
Here, θ−j
are the reported types of the agents other than j when θi is replaced by θi′ .

Proof. We prove the “if” direction first. Any Groves mechanism r that satisfies Equation 4
is non-deficit by Proposition 4.1. Now suppose that r is individually dominated, that is,
there exists another non-deficit Groves mechanism r′ such that for all i and θ−i , we have
ri′ (θ−i ) ≥ ri (θ−i ), and for some i and θ−i , we have ri′ (θ−i ) > ri (θ−i ). For the i and θ−i that
make this inequality strict, we have
X
′
)}
rj (θ−j
ri′ (θ−i ) > ri (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

j6=i

≥ ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

X

′
rj′ (θ−j
)},

j6=i

which contradicts with the fact that r′ must satisfy Equation 3. It follows that r is individually undominated.
Now we prove the “only if” direction. Suppose Equation 4 is not P
satisfied. Then,
′ )}. Let
′
rj (θ−j
there exists some i and θ−i such that ri (θ−i ) < ′inf {V CG(θi , θ−i ) −
θi ∈Θi
j6
=
i
P
′ )} − r (θ ) (so that a > 0), and let r′ be the same as r,
rj (θ−j
a = ′inf {V CG(θi′ , θ−i ) −
i −i
θi ∈Θi

j6=i

except that for the aforementioned i and θ−i , ri′ (θ−i ) = ri (θ−i ) + a. To show that this does
not break the non-deficit constraint, consider any type vector (θi , θ−i ) where i and θ−i are
the same as before (that is, any type profile that is affected). Then,
X
′
rj (θ−j
)}
ri′ (θ−i ) = a + ri (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

= ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

j6=i

X

′
rj′ (θ−j
)}.

j6=i

r′

Thus, by Proposition 4.1, is non-deficit. This contradicts that r is individually undominated. Hence, Equation 4 must hold.
2
We now give an example of an individually undominated mechanism.
Example 4.3 Consider a single-item auction with n ≥ 3 agents. Agent i bids θi ∈ [0, ∞).
Let [θ]j be the jth highest type from the type profile θ. Let us consider the anonymous
Groves mechanism characterized by r(θ−i ) = n1 [θ−i ]2 . That is, under this mechanism,
besides paying the VCG payment, every agent receives n1 times the second highest other
bid. In fact, this mechanism is the BC mechanism for single-item auctions. To show that
r is individually undominated, it suffices to show Equation 4 is satisfied. We first observe
that for every agent, the second highest other bid is no more than the second highest bid,
141

Guo, Markakis, Apt, & Conitzer

which equals the total VCG payment. That is, r is non-deficit. Hence, Equation 3 holds
for all agents and all type profiles. Moreover, for every type profile θ, by setting θi′ = [θ−i ]2 ,
we can verify that Equation 4 holds. It follows that the BC mechanism is individually
undominated for single-item auctions.
In what follows, we first show two examples of single-item auction scenarios, where collective undominance is strictly stronger than individual undominance for non-deficit Groves
mechanisms. We then propose two techniques for generating individually undominated
mechanisms starting from known individually dominated mechanisms (if the initial mechanism is already individually undominated, then the techniques will return the same mechanism). One technique immediately produces an individually undominated mechanism.
However, it does not preserve anonymity. The second technique preserves anonymity, and
after repeated applications the result converges to an individually undominated mechanism.
We emphasize that we can start with any non-deficit Groves mechanism, including the BC
mechanism, the Worst-Case Optimal mechanism (Guo & Conitzer, 2009), the Optimal-inExpectation Linear mechanisms (Guo & Conitzer, 2010), and the VCG mechanism.
4.2 Collective Undominance is Strictly Stronger than Individual Undominance
We use two examples to show that collective undominance is, in general, strictly stronger
than individual undominance.
Example 4.4 Consider a single-item auction with 4 agents. We assume that for each
agent, the set of allowed types is the same, namely, integers from 0 to 3. Here, the VCG
mechanism is just the second-price auction.
Let us consider the following two anonymous non-deficit Groves mechanisms, which are
computer-generated for differentiating collective undominance and individual undominance.
Mechanism 1: r(θ−i ) = r([θ−i ]1 , [θ−i ]2 , [θ−i ]3 ), and the function r is given in Table 1.
([θ−i ]j is the jth highest type among types other than i’s own type.)
Mechanism 2: r′ (θ−i ) = r′ ([θ−i ]1 , [θ−i ]2 , [θ−i ]3 ), and the function r′ is given in Table 1.
With the above characterization, we have that mechanism
2 collectively dominates
P mechP
anism 1: for example, for the type profile (3, 2, 2, 2), i r(θ−i ) = 1/2 < 1 = i r′ (θ−i ).
On the other hand, mechanism 2 does not individually dominate mechanism 1: for example, r(3, 3, 2) = 1 > 5/6 = r′ (3, 3, 2). In fact, based on the characterization of individually
undominated non-deficit Groves mechanisms (Theorem 4.2), we are able to show that mechanism 1 is individually undominated.
Example 4.5 Consider a single-item auction with 5 agents. We assume that for each
agent, the set of allowed types is [0, ∞). Here, the VCG mechanism is just the second-price
auction.
Let us consider the following two anonymous non-deficit Groves mechanisms:
Mechanism 1:
142

Undominated Groves Mechanisms

r(0, 0, 0)
r(1, 0, 0)
r(1, 1, 0)
r(1, 1, 1)
r(2, 0, 0)
r(2, 1, 0)
r(2, 1, 1)
r(2, 2, 0)
r(2, 2, 1)
r(2, 2, 2)

0
0
1/4
1/4
0
1/12
0
1/2
0
1/2

r′ (0, 0, 0)
r′ (1, 0, 0)
r′ (1, 1, 0)
r′ (1, 1, 1)
r′ (2, 0, 0)
r′ (2, 1, 0)
r′ (2, 1, 1)
r′ (2, 2, 0)
r′ (2, 2, 1)
r′ (2, 2, 2)

0
0
1/4
1/4
0
7/24
1/6
1/2
1/4
1/2

r(3, 0, 0)
r(3, 1, 0)
r(3, 1, 1)
r(3, 2, 0)
r(3, 2, 1)
r(3, 2, 2)
r(3, 3, 0)
r(3, 3, 1)
r(3, 3, 2)
r(3, 3, 3)

0
1/4
0
2/3
1
0
2/3
0
1
0

r′ (3, 0, 0)
r′ (3, 1, 0)
r′ (3, 1, 1)
r′ (3, 2, 0)
r′ (3, 2, 1)
r′ (3, 2, 2)
r′ (3, 3, 0)
r′ (3, 3, 1)
r′ (3, 3, 2)
r′ (3, 3, 3)

0
1/4
1/4
2/3
19/24
1/6
5/6
7/12
5/6
1/2

Table 1: Computer-generated example mechanisms for differentiating collective undominance and individual undominance.

r(θ−i ) = 0 if all four types in θ−i are identical.
r(θ−i ) = [θ−i ]1 /4 if the highest three types in θ−i are identical, and they are strictly
higher than the lowest type in θ−i .
r(θ−i ) = [θ−i ]1 /6 if the highest two types in θ−i are identical, and they are strictly
higher than the third highest type in θ−i .
r(θ−i ) = 3[θ−i ]2 /16 if the highest type in θ−i is strictly higher than the second highest
type in θ−i , and the second highest type in θ−i is identical to the third highest type in θ−i .
r(θ−i ) = [θ−i ]2 /5 if the highest three types in θ−i are all different.
Mechanism 2 (BC):
r′ (θ−i ) = [θ−i ]2 /5.
With the above characterization, we have that mechanism
P 2 collectively dominates
mechanism 1: for example, for
the
type
profile
(3,
2,
2,
2,
2),
i r(θ−i ) = 4r(3, 2, 2, 2) +
P
r(2, 2, 2, 2) = 3/2 + 0 = 3/2 < i r′ (θ−i ) = 4r′ (3, 2, 2, 2) + r′ (2, 2, 2, 2) = 8/5 + 2/5 = 2. On
the other hand, mechanism 2 does not individually dominate mechanism 1: for example,
r(4, 4, 4, 1) = 1 > 4/5 = r′ (4, 4, 4, 1). In fact, based on the characterization of individually undominated non-deficit Groves mechanisms (Theorem 4.2), we are able to show that
mechanism 1 is individually undominated.
4.3 A Priority-Based Technique
Given a non-deficit Groves mechanism r and a priority order over agents π, we can improve
r into an individually undominated mechanism as follows:
1) Let π : {1, . . . , n} → {1, . . . , n} be a permutation representing the priority order.
That is, π(i) is agent i’s priority value (the lower the value, the higher the priority). π −1 (k)
is then the agent with the kth highest priority. The high-level idea of the priority-based
technique is that we go over the agents one by one. For the first agent (the agent with
the highest priority), we maximize his redistribution function subject to the constraint of
143

Guo, Markakis, Apt, & Conitzer

Proposition 4.1. For later agents, we do the same, but take into consideration that earlier
agents’ redistribution functions have been updated. A priority order can be arbitrary.
Generally, agents with high priorities benefit more from this technique, since for earlier
agents, there is more room for improvement.
2) Let i = π −1 (1), and update ri to
X

riπ (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

′
rj (θ−j
)}.

π(j)>1

That is, the update ensures that at this point rπ satisfies Equation 4 for i = π −1 (1).
It should be noted that during the above update, only the payment of agent i = π −1 (1)
is changed, and it is changed by
X
′
rj (θ−j
)} − ri (θ−i )
riπ (θ−i ) − ri (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

= ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

π(j)>1

X

′
rj (θ−j
)} = SiBCGC (θ−i ).

j

That is, essentially, the above update amounts to applying the BCGC(i) transformation
on r, where i = π −1 (1).
3) We will now consider the remaining agents in turn, according to the order π. In the
kth step, we update ri (i = π −1 (k)) to
X

riπ (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

′
)−
rj (θ−j

X

′
)}.
rjπ (θ−j

π(j)<k

π(j)>k

That is, the update ensures that at this point rπ satisfies Equation 4 when i = π −1 (k). To
avoid breaking the non-deficit property, when we make the update, we take the previous
k − 1 updates into account. For this update, what we are doing is essentially applying the
BCGC(π −1 (k)) transformation on the resulting mechanism from the previous update.
Overall, for every agent i,
riπ (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

X

′
)−
rj (θ−j

π(j)>π(i)

X

′
rjπ (θ−j
)}.

π(j)<π(i)

The new mechanism rπ satisfies the following properties:
Proposition 4.6 For all i and θ−i , riπ (θ−i ) ≥ ri (θ−i ).
Proof. First consider i = π −1 (1),
with the highest priority. For any θ−i , we have
Pthe agent
′
′
π
rj (θ−j )}. Since r is non-deficit, by Equation 3, we have
ri (θ−i ) = ′inf {V CG(θi , θ−i ) −
θi ∈Θi
j6=i
P
′ )}. Hence r π (θ ) ≥ r (θ ).
rj (θ−j
ri (θ−i ) ≤ ′inf {V CG(θi′ , θ−i ) −
−i
i −i
i
θi ∈Θi

j6=i

144

Undominated Groves Mechanisms

For any i 6= π −1 (1), riπ (θ−i ) equals
X

ri (θ−i ) + ′inf {V CG(θi′ , θ−i ) − ri (θ−i ) −
θi ∈Θi

X

′
rj (θ−j
)−

π(j)>π(i)

′
rjπ (θ−j
)}.

π(j)<π(i)

We must show that
inf {V CG(θi′ , θ−i ) − ri (θ−i ) −

θi′ ∈Θi

X

X

′
rj (θ−j
)−

π(j)>π(i)

′
rjπ (θ−j
)} ≥ 0.

(5)

π(j)<π(i)

Consider p = π −1 (π(i) − 1) (the agent immediately before i in terms of priority). For
any θi , θ−i , we have
X
X
V CG(θi , θ−i ) − ri (θ−i ) −
rj (θ−j ) −
rjπ (θ−j )
π(j)>π(i)

= V CG(θi , θ−i ) −

X

rj (θ−j ) −

X

′
rj (θ−j
)−

π(j)>π(p)

≥ ′inf {V CG(θp′ , θ−p ) −
θp ∈Θp

π(j)<π(i)

X

rjπ (θ−j ) − rpπ (θ−p )

X

′
rjπ (θ−j
)} − rpπ (θ−p ) = 0.

π(j)<π(p)

π(j)>π(p)

π(j)<π(p)

′ is the set of types reported by the agents other than j, when θ
In the above inequality, θ−j
p
′
is replaced by θp . Because θi is arbitrary, Equation 5 follows. Therefore, riπ (θ−i ) ≥ ri (θ−i )
for all i and θ−i .
2

Proposition 4.7 rπ is individually undominated.
Proof. Let i = π −1 (n). For all θ,
X
X
′
rjπ (θ−j
)} − riπ (θ−i ) = 0.
V CG(θ) −
rjπ (θ−j ) ≥ ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

j=1,...,n

j6=i

Hence rπ never incurs a deficit. So, rπ is non-deficit.
Using Proposition 4.6, we have for all i and all θ,
X
′
rj (θ−j
)−
riπ (θ−i ) = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

π(j)>π(i)

≥ ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

X

X

′
rjπ (θ−j
)}

π(j)<π(i)

′
rjπ (θ−j
)}.

j6=i

Because rπ is non-deficit, the opposite inequality must also be satisfied (Equation 3)—hence
we must have equality, that is, Equation 4 must hold. It follows that rπ is individually
undominated.
2
It should be noted that for the above technique, during the updates, we need to keep
track of the value of riπ (θ−i ) for all i and θ−i . That is, due to space complexity, the above
technique is more suitable for cases with few agents and few possible types. To reduce
145

Guo, Markakis, Apt, & Conitzer

space complexity, when we update, we could also recompute earlier updates in a recursive
fashion. By doing so, the later updates are much more difficult to compute compared to
the earlier updates. Fortunately, the earlier updates tend to be more important, because
there is generally more room for improvement during the earlier updates. Therefore, a
reasonable approximation would be to update only for a few high-priority agents and ignore
the remaining agents with low priorities.
4.4 An Iterative Technique that Preserves Anonymity
The previous technique will, in general, not produce an anonymous mechanism, even if the
input mechanism is anonymous. This is because agents higher in the priority order tend to
benefit more from the technique. Here, we will introduce another technique that preserves
anonymity.
Given an anonymous mechanism r, let r0 = r. For all i and all θ, let
rk+1 (θ−i ) =

X
n−1 k
1
′
′
rk (θ−j
)}.
r (θ−i ) +
inf
{V
CG(θ
,
θ
)
−
−i
i
n
n θi′ ∈Θi
j6=i

It is easily seen by induction that all the rk mechanisms are anonymous. If rk is anonymous, then for any π ∈ Π(n − 1), rk ((θ−i )π ) = rk (θ−i ) for all θ and all i. We also have
that V CG(θi′ , (θ−i )π ) = V CG(θi′ , θ−i ) for all θ, all θi′ , and all i. Finally, let ((θ−i )π , θi′ ) be
the type profile
are permuted according to π, and θi is replaced by
P where the types in θ−i P
′ ) for all θ, all i, and all θ ′ . The above
θi′ . We have j6=i rk (((θ−i )π , θi′ )−j ) = j6=i rk (θ−j
i
implies that rk+1 is also permutation independent, thus anonymous.
It should be noted that from rk to rk+1 , agent i’s payment is changed by

=
=
=

n−1 k
n r (θ−i )

+
1
n

1
n

rk+1 (θ−i ) − rk (θ−i )
P k ′
′, θ ) −
r (θ−j )} − rk (θ−i )
inf
{V
CG(θ
−i
i
′

θi ∈Θi

inf {V

θi′ ∈Θi

j6=i

CG(θi′ , θ−i )

−

P

j
1 BCGC
(θ−i ).
n Si

′ )}
rk (θ−j

That is, essentially, rk+1 is the resulting mechanism by applying the BCGC transform on
rk .
The next propositions immediately follow from Proposition 2.3:
Proposition 4.8 If r0 is non-deficit, then rk is non-deficit for all k.
Proposition 4.9 For all i and θ−i , rk (θ−i ) is nondecreasing in k.
Proposition 4.10 If rk+1 = rk , then rk is individually undominated.
Proposition 4.11 If rk is not individually undominated, then rk+1 individually dominates
rk .
Finally, the following proposition establishes convergence.
146

Undominated Groves Mechanisms

Proposition 4.12 As k → ∞, rk converges (pointwise) to an individually undominated
mechanism.
Proof. By Proposition 4.9, the rk (θ−i ) are nondecreasing in k, and since every rk is nondeficit by Proposition 4.8, they must be bounded; hence they must converge (pointwise).
For any i and θ−i , let
X
′
rk (θ−j
)} − rk (θ−i ).
dk = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

j6=i

Using Proposition 4.9, we derive the following inequality:
X
′
rk+1 (θ−j
)} − rk+1 (θ−i )
dk+1 = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

j6=i

≤ ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

X

′
rk (θ−j
)} − rk+1 (θ−i )

j6=i

= ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

=−

X

′
rk (θ−j
)}

j6=i

X
n−1 k
1
′
′
r (θ−i ) −
inf
{V
CG(θ
,
θ
)
−
rk (θ−j
)}
−i
i
n
n θi′ ∈Θi
j6=i

=

X
n−1 k
n−1
n−1
′
′
{V
CG(θ
,
θ
)
−
rk (θ−j
)} −
inf
r (θ−i ) =
dk .
−i
i
n θi′ ∈Θi
n
n
j6=i

As k → ∞, dk = ′inf {V CG(θi′ , θ−i ) −
θi ∈Θi

P

j6=i

′ )} − r k (θ ) → 0. So in the limit, Equark (θ−j
−i

tion 4 is satisfied. Thus, rk converges (pointwise, linearly) to an individually undominated
mechanism.
2
Similar to the priority-based technique, in the above iterative process, when computing
for rk , we need the value of rk−1 (θ−i ), for all θ−i . That is, due to space complexity, the
above technique is more suitable for cases with few agents and few possible types. To
reduce the space complexity we could also recompute rk−1 in a recursive fashion. By doing
so, rk becomes much more difficult to compute for large values of k. Fortunately, the earlier
iterative steps are more crucial, because there is generally more room for improvement
during the earlier steps. Therefore a reasonable approximation would be to only compute
a few iterative steps.

5. Multi-Unit Auctions with Unit Demand
In this section, we consider auctions where there are multiple identical units of a single
good and all agents have unit demand, i.e., each agent wants only one unit (if there is a
single unit of the good, we simply have the standard single-item auction). We focus on the
notion of collectively undominated mechanisms and how it relates to that of individually
undominated mechanisms. In particular, we first obtain an analytical characterization of
147

Guo, Markakis, Apt, & Conitzer

all collectively undominated Groves mechanisms that are non-deficit, anonymous, and have
linear payment functions, by proving that the Optimal-in-Expectation Linear redistribution
mechanisms (OEL) (Guo & Conitzer, 2010), which include the BC mechanism, are the only
collectively undominated Groves mechanisms that are anonymous and linear. We then show
that individual undominance and collective undominance are equivalent if we restrict our
consideration to Groves mechanisms that are anonymous and linear in the setting of multiunit auctions with unit demand. Note that even for single-item auctions, the examples
given in Section 4.2 show that this equivalence does not hold if we do not restrict ourselves
to linear and anonymous mechanisms.
If one mechanism collectively dominates another mechanism, then under the first mechanism, the agents’ expected total utility, if there was a prior distribution over the agents’
valuations, must be no less than that under the second mechanism, and strictly higher under minimal conditions on the prior distribution. Therefore, a good direction in which to
look for collectively undominated mechanisms is to start with those mechanisms that are
optimal-in-expectation.
The Optimal-in-Expectation Linear (OEL) redistribution mechanisms (Guo & Conitzer,
2010), described below, are special cases of non-deficit Groves mechanisms that are anonymous and linear. The OEL mechanisms are defined only for multi-unit auctions with unit
demand. In a unit demand multi-unit auction, there are m indistinguishable units for sale,
and each agent is interested in only one unit. For agent i, his type θi is his valuation for
winning one unit. We assume all bids (announced types) are bounded below by L and above
by U , i.e., Θi = [L, U ] (note that L can be 0).
A linear and anonymous Groves mechanism is characterized by a function r of the foln−1
P
cj [θ−i ]j (where [θ−i ]j is the jth highest bid among θ−i ). For
lowing form: r(θ−i ) = c0 +
j=1

OEL mechanisms, the cj ’s are chosen according to one of the following options (indexed by
integer parameter k, where k ranges from 0 to n, and k − m is odd):
k = 0:


 

n−i−1
m−1
ci = (−1)
/
for i = 1, . . . , m,
n−m−1
i−1

 

m
X
m−1
m−i n − i − 1
(−1)
c0 = U m/n − U
/
,
n−m−1
i−1
m−i

i=1

ci = 0 for other values of i.
k = 1, 2, . . . , m:


 

n−i−1
m−1
ci = (−1)
/
for i = k + 1, . . . , m,
n−m−1
i−1

 

m
X
m−1
m−i n − i − 1
(−1)
ck = m/n −
/
,
n−m−1
i−1
m−i

i=k+1

ci = 0 for other values of i.
148

Undominated Groves Mechanisms

k = m + 1, m + 2, . . . , n − 1:
ci = (−1)

m−i−1


 
n−m−1
i−1
for i = m + 1, . . . , k − 1,
/
n−i−1
m−1



ck = m/n −

k−1
X

(−1)

m−i−1

i=m+1



 

i−1
n−m−1
/
,
m−1
n−i−1

ci = 0 for other values of i.
k = n:
ci = (−1)m−i−1



 

i−1
n−m−1
/
for i = m + 1, . . . , n − 1,
m−1
n−i−1

c0 = Lm/n − L

n−1
X

(−1)

m−i−1

i=m+1


 
n−m−1
i−1
,
/
n−i−1
m−1



ci = 0 for other values of i.
For example, when k = m + 1, we have cm+1 = m/n and ci = 0 for all other i. For this
specific OEL mechanism, r(θ−i ) = m
n [θ−i ]m+1 . That is, besides participating in the VCG
mechanism, every agent also receives an amount that is equal to m/n times the (m + 1)th
highest bid from the other agents. Actually, this is exactly the BC mechanism for multi-unit
auctions with unit demand.
Besides being non-deficit, one property of the OEL mechanisms is that they are always
budget balanced in the following scenarios.
• [θ]1 = U and k = 0
• [θ]k+1 = [θ]k and k ∈ {1, . . ., n − 1}
• [θ]n = L and k = n
Using this property, we will prove that the OEL mechanisms are the only collectively
undominated non-deficit Groves mechanisms that are anonymous and linear.
We first show that the OEL mechanisms are collectively undominated.
Theorem 5.1 For multi-unit auctions with unit demand, there is no non-deficit Groves
mechanism that collectively dominates an OEL mechanism.
By using Lemma 3.3, we only need to prove this for the case of anonymous Groves
mechanisms.
Lemma 5.2 For multi-unit auctions with unit demand, there is no non-deficit anonymous
Groves mechanism that collectively dominates an OEL mechanism.
149

Guo, Markakis, Apt, & Conitzer

Proof. We first prove: no OEL mechanism with index k ∈ {1, . . ., n − 1} is collectively
dominated by a non-deficit anonymous Groves mechanism.
Suppose a non-deficit anonymous Groves mechanism r collectively dominates an OEL
mechanism with index k ∈ {1, . . ., n − 1}. We use rOEL to denote this OEL mechanism.
For any i and θ−i , we define the following function:
∆(θ−i ) = r(θ−i ) − rOEL (θ−i ).
P
Since r collectively dominates rOEL , we have that for any θ, ni=1 ∆(θ−i ) ≥ 0.
We also have that, whenever [θ]k+1 = [θ]k , the OEL mechanism is budget balanced.
OEL , the agents’ total payment is 0; in this case, since r is non-deficit, we
That is, under
Pn r
must have i=1 ∆(θ−i ) = 0.
Now we claim that ∆(θ−i ) = 0 for all θ−i . Let C(θ−i ) be the number of bids among
θ−i that equal [θ−i ]k . Hence, we must show that for all θ−i with C(θ−i ) ≥ 1, we have
∆(θ−i ) = 0.
We now prove it by induction on the value of C(θ−i ) (backwards, from n − 1 to 1).
Base case: C(θ−i ) = n − 1.
Suppose there is a θ−i with C(θ−i ) = n − 1. That is, all the bids in θ−i are identical.
When θi is also equal to the bids in θ−i , allPbids in θ are the same so that [θ]k+1 = [θ]k .
Hence, by our earlier observation, we have nj=1 ∆(θ−j ) = 0. But we know that for all j,
θ−j is the same set of bids. Hence ∆(θ−i ) = 0 for all θ−i when C(θ−i ) = n − 1.
Induction step.
Let us assume that for all θ−i , if C(θ−i ) ≥ p (where p ∈ {2, . . ., n − 1}), then ∆(θ−i ) = 0.
Now we consider any θ−i with
P C(θ−i ) = p − 1. When θi is equal to [θ−i ]k , we have [θ]k =
[θ]k+1 , which implies that nj=1 ∆(θ−j ) = 0. For all j with θj = [θ−i ]k , ∆(θ
Pn −j ) = ∆(θ−i ),
and for other j, C(θ−j ) = p. Therefore, by the induction assumption, j=1 ∆(θ−j ) is a
positive multiple of ∆(θ−i ), which implies that ∆(θ−i ) = 0.
By induction, we have shown that ∆(θ−i ) = 0 for all θ−i . This implies that r and
rOEL are identical. Hence, no other non-deficit anonymous Groves mechanism collectively
dominates an OEL mechanism with index k ∈ {1, . . ., n − 1}.
Now we prove: the OEL mechanism with index k = 0 is not collectively dominated
by a different non-deficit anonymous Groves mechanism.
Suppose a non-deficit anonymous Groves mechanism r collectively dominates an OEL
mechanism with index k = 0. We use rOEL to denote this OEL mechanism. For any i and
θ−i , we define the following function:
∆(θ−i ) = r(θ−i ) − rOEL (θ−i ).
P
Since r collectively dominates rOEL , we have that for any θ, ni=1 ∆(θ−i ) ≥ 0. We also
OEL , the agents’ total payment is 0; in this case,
have that, whenever [θ]1 = U , under rP
because r is non-deficit, we must have ni=1 ∆(θ−i ) = 0.
Now we claim that ∆(θ−i ) = 0 for all θ−i . Let C(θ−i ) be the number of bids among θ−i
that equal U . Hence, we must show that for all θ−i with C(θ−i ) ≥ 0, we have ∆(θ−i ) = 0.
We now prove it by induction on the value of C(θ−i ) (backwards, from n − 1 to 0).
150

Undominated Groves Mechanisms

Base case: C(θ−i = n − 1.
Suppose there is a θ−i with C(θ−i ) = n − 1. That is, all the bids in θ−i
Pnare equal to U .
When θi is also equal to the bids in U , by our earlier observation, we have j=1 ∆(θ−j ) = 0.
But we know that for all j, ∆(θ−j ) is the same value. Hence ∆(θ−i ) = 0 for all θ−i when
C(θ−i ) = n − 1.
Induction step.
Let us assume that for all θ−i , if C(θ−i ) ≥ p (where p ∈ {2, . . ., n − 1}), then ∆(θ−i ) = 0.
Now we consider any
Pθ−i with C(θ−i ) = p − 1. When θi is equal to U , we have [θ]1 = U ,
which implies that nj=1 ∆(θ−j ) = 0. For all j with θj = U , ∆(θ
Pn−j ) = ∆(θ−i ), and for
other j, C(θ−j ) = p. Therefore, by the induction assumption, j=1 ∆(θ−j ) is a positive
multiple of ∆(θ−i ), which implies that ∆(θ−i ) = 0.
By induction, we have shown that ∆(θ−i ) = 0 for all θ−i . This implies that r and
rOEL are identical. Hence, no other non-deficit anonymous Groves mechanism collectively
dominates the OEL mechanism with index k = 0.
It remains to prove: the OEL mechanism with index k = n is not collectively dominated by a different non-deficit anonymous Groves mechanism.
This case is similar to the case of k = 0 and we omit it here.
2
We now proceed to show that within the family of anonymous and linear non-deficit
Groves mechanisms, the OEL mechanisms are the only ones that are collectively undominated. Actually, they are also the only ones that are individually undominated, which is a
stronger claim since being individually undominated is a weaker property.
Theorem 5.3 For multi-unit auctions with unit demand, if an anonymous linear nondeficit Groves mechanism is individually undominated, then it must be an OEL mechanism.
Before proving this theorem, let us introduce the following lemma.
Lemma 5.4 Let I be the set of points (s1 , s2 , . . . , sk ) (U ≥ s1 ≥ s2 ≥ . . . ≥ sk ≥ L) that
satisfy Q0 + Q1 s1 + Q2 s2 + . . . + Qk sk = 0 (the Qi are constants). If the measure of I is
positive (Lebesgue measure on Rk ), then Qi = 0 for all i.
Proof. If Qi 6= 0 for some i, then for any U ≥ s1 ≥ s2 ≥ . . . ≥ si−1 ≥ si+1 ≥ . . . ≥ sk ≥ L,
to make Q0 + Q1 s1 + Q2 s2 + . . . + Qk sk = 0, si can take at most one value. As a result the
measure of I must be 0.
2
Now we are ready to prove Theorem 5.3.
Proof. Let r be a non-deficit anonymous linear Groves mechanism. We recall that a
Groves mechanism is anonymous and linear if r is a linear function defined as r(θ−i ) =
n−1
P
aj [θ−i ]j (where [θ−i ]j is the jth highest type among θ−i , and the aj ’s are constants).
a0 +
j=1

Under multi-unit auctions with unit demand, the total VCG payment equals m[θ]m+1
(m times the (m + 1)th bid). Under r, the agents’ total payment equals
m[θ]m+1 −

n
X

r(θ−i ) = m[θ]m+1 − na0 −

n n−1
X
X
i=1 j=1

i=1

151

aj [θ−i ]j .

Guo, Markakis, Apt, & Conitzer

The above total payment is a linear function in terms of the types among θ. For simplicity,
we rewrite the total payment as C0 + C1 [θ]1 + C2 [θ]2 + . . . + Cn [θ]n . The Ci are constants
determined by the ai . We have
C0 = −na0
C1 = −(n − 1)a1
C2 = −a1 − (n − 2)a2
C3 = −2a2 − (n − 3)a3
..
.
Cm = −(m − 1)am−1 − (n − m)am
Cm+1 = −mam − (n − m − 1)am+1 + m
Cm+2 = −(m + 1)am+1 − (n − m − 2)am+2
..
.
Cn−1 = −(n − 2)an−2 − an−1
Cn = −(n − 1)an−1
Given any θ−i , for any possible value of θi , we must have
That is, for any θ−i , we have inf

n
P

θi i=1

n
P

ti (θ) ≥ 0 (non-deficit).

i=1

ti (θ) ≥ 0. If for some θ−i , we have inf

n
P

θi i=1

ti (θ) > ǫ

(ǫ > 0), then we can reduce the payment of agent i by ǫ without violating the non-deficit
constraint, when the other agents’ types are θ−i . Therefore, if the mechanism is individually
n
P
ti (θ) = 0.
undominated, then for any θ−i , we have inf
θi i=1

We denote [θ−i ]j by sj (j = 1, . . . , n − 1). That is, s1 ≥ s2 ≥ . . . ≥ sn−1 .
n
P
The expression inf
ti (θ) then equals the minimum of the following expressions:
θi i=1

inf

L≤θi ≤sn−1

n
X

ti (θ)

i=1

inf

sn−1 ≤θi ≤sn−2

n
X

ti (θ)

i=1

..
.

inf

s2 ≤θi ≤s1

inf

s1 ≤θi ≤U

n
X

ti (θ)

n
X

ti (θ)

i=1

i=1

152

Undominated Groves Mechanisms

We take a closer look at

inf

n
P

L≤θi ≤sn−1 i=1

ti (θ). When L ≤ θi ≤ sn−1 , the jth highest type

[θ]j = sj for j = 1, . . . , n − 1, and the nth highest type [θ]n = θi (this case corresponds to
agent i being the agent with the lowest type). We have
inf

L≤θi ≤sn−1

n
X

ti (θ) =

i=1

inf

L≤θi ≤sn−1

(C0 + C1 s1 + C2 s2 + . . . + Cn−1 sn−1 + Cn θi )

= min{C0 + C1 s1 + . . . + Cn−1 sn−1 + Cn L, C0 + C1 s1 + . . . + Cn−1 sn−1 + Cn sn−1 }.
That is, because the expression is linear, the minimum is reached when θi is set to either
the lower bound L or the upper bound sn−1 .
Similarly, we have
inf

sn−1 ≤θi ≤sn−2

n
X

ti (θ) = min{C0 + C1 s1 + . . . + Cn−2 sn−2 + Cn−1 sn−1 + Cn sn−1 ,

i=1

C0 + C1 s1 + C2 s2 + . . . + Cn−2 sn−2 + Cn−1 sn−2 + Cn sn−1 },
..
.
inf

s2 ≤θi ≤s1

n
X

ti (θ) = min{C0 + C1 s1 + C2 s1 + C3 s2 + . . . + Cn sn−1 ,

i=1

C0 + C1 s1 + C2 s2 + C3 s2 + . . . + Cn sn−1 },
inf

s1 ≤θi ≤U

n
X

ti (θ) = min{C0 + C1 U + C2 s1 + . . . + Cn sn−1 ,

i=1

C0 + C1 s1 + C2 s1 + . . . + Cn sn−1 }.
Putting all the above together, we have that for any U ≥ s1 ≥ s2 ≥ . . . ≥ sn−1 ≥ L, the
minimum of the following expressions is 0.
• (n): C0 + C1 s1 + C2 s2 + . . . + Cn−1 sn−1 + Cn L
• (n − 1): C0 + C1 s1 + C2 s2 + . . . + Cn−1 sn−1 + Cn sn−1
• (n − 2): C0 + C1 s1 + C2 s2 + . . . + Cn−2 sn−2 + Cn−1 sn−2 + Cn sn−1
.
• ..
• (2): C0 + C1 s1 + C2 s2 + C3 s2 + . . . + Cn sn−1
• (1): C0 + C1 s1 + C2 s1 + C3 s2 + . . . + Cn sn−1
• (0): C0 + C1 U + C2 s1 + C3 s2 + . . . + Cn sn−1
153

Guo, Markakis, Apt, & Conitzer

The above expressions are numbered from 0 to n. Let I(i) be the set of points (s1 , . . . , sn−1 )
(U ≥ s1 ≥ s2 ≥ . . . ≥ sn−1 ≥ L) that make expression (i) equal to 0. There must exist at
least one i such that the measure of I(i) is positive. According to Lemma 5.4, expression
(i) must be the constant 0.
If expression (0) is constant 0, then the total payment under r is 0 whenever the highest
type is equal to the upper bound U . That is, for any θ, the total payment C0 + C1 [θ]1 +
C2 [θ]2 + . . . + Cn [θ]n must be a constant multiple of U − [θ]1 (the total payment is a linear
function). We have C0 = −U C1 and Cj = 0 for j ≥ 2. It turns out that the above equalities
of the Cj completely determine the values of the aj (the values of the aj can be solved for
based on the Cj by pure algebraic manipulations), and the corresponding mechanism is the
OEL mechanism with index k = 0. If expression (i) is constant for other values of i, then
the corresponding mechanism is the OEL mechanism with another index.
2
Hence, we have the following complete characterization in this context.
Corollary 5.5 For multi-unit auctions with unit demand, a non-deficit anonymous linear
Groves mechanism is individually / collectively undominated if and only if it is an OEL
mechanism.
Proof. This corollary can be proved by combining Theorem 5.1 and Theorem 5.3, as well
as the fact that a collectively undominated mechanism is also individually undominated. 2
The above corollary also shows that if we consider only Groves mechanisms that are
non-deficit, anonymous, and linear in the setting of multi-unit auctions with unit demand,
then individual undominance and collective undominance are equivalent. Thus, we have
characterized all individually/collectively undominated Groves mechanisms that are nondeficit, anonymous, and linear for multi-unit auctions with unit demand.

6. The Public Project Problem
We now study a well known class of decision problems, namely public project problems (see,
e.g., Mas-Colell et al., 1995; Moulin, 1988; Moore, 2006). In this setting a set of n agents
needs to decide on financing a project of cost c. An agent’s type is her private valuation for
the project if it takes place. We consider two versions of the problem.
6.1 Equal Participation Costs
In this case if the project takes place, each agent contributes the same share, c/n, so as
to cover the total cost. Hence the participation costs of all agents are the same. So the
problem is defined as follows.
Public project problem
Consider (D, Θ1 , . . ., Θn , v1 , . . ., vn ), where
• D = {0, 1} (reflecting whether a project is canceled or takes place),
• for all i ∈ {1, . . ., n}, Θi = [0, c], where c > 0,
154

Undominated Groves Mechanisms

• for all i ∈ {1, . . ., n}, vi (d, θi ) := d(θi − nc ),
When the agents employ a payment-based mechanism to decide on the project, then
in addition to c/n, each agent also has to pay or receive the payment, ti (θ), imposed
by the mechanism. By the result of Holmström (1979), the only efficient and strategyproof payment-based mechanisms in this domain are P
Groves mechanisms.
PnTo determine the
efficient outcome for a given type vector θ, note that ni=1 vi (d, θP
i ) = d( i=1 θi − c). Hence
efficiency here for a mechanism (f, t) means that f (θ) = 1 if ni=1 θi ≥ c and f (θ) = 0
otherwise, i.e., the project takes place if and only if the declared total value that the agents
have for the project exceeds its cost.
We first observe the following result.
Proposition 6.1 In the public project problem with equal participation costs, the BC mechanism coincides with VCG.
Proof. It suffices to check that in equation (1) it holds that SiBCGC (θ−i ) = 0 for all i
and all θ−i . Since VCG is a non-deficit mechanism, we have SiBCGC (θ−i ) ≥ 0, as the term
SiBCGC (θ−i ) is a sum of payments for some type vector. Hence all we need is to show that
there is a value
for θi′ that makes the expression in (1) equal to 0. Checking this is quite
P
′
′
simple. If j6=i θj < n−1
n c, then we take θi := 0 and otherwise θi := c. In the former case
the efficient outcome is to not implement the project whereas in the latter case, the opposite
occurs. It is easy to check that in both cases we have SiBCGC (θ−i ) = 0.
2
We now show that in fact VCG cannot be improved upon. Before stating our result, we
would
Plike to note that one ideally would like to have a mechanism that is budget-balanced,
i.e., i ti (θ) = 0 for all θ, so that in total the agents only pay the cost of the project and
no more. However this is not possible, since for the public project problem, no mechanism
exists that is efficient, strategy-proof, and budget balanced (Mas-Colell et al., 1995). Our
theorem below considerably strengthens this result, showing that VCG is optimal with
respect to minimizing the total payment of the agents.
Theorem 6.2 In the public project problem there exists no non-deficit Groves mechanism
that collectively dominates the VCG mechanism.
As with the case of unit-demand auctions, we first establish the desired conclusion for
anonymous Groves mechanisms and then extend it to arbitrary ones by Lemma 3.3. Notice
that VCG is anonymous in this setting and hence we can apply Lemma 3.3(ii).
Lemma 6.3 In the public project problem there exists no anonymous non-deficit Groves
mechanism that collectively dominates the VCG mechanism.
Proof. Suppose that an anonymous non-deficit Groves mechanism (r1 , ..., rn ) exists that
collectively dominates VCG. By anonymity, for all i ∈ {1, . . ., n} ri = r, for some function
r : [0, c]n−1 → R. Hence
n
X
∀ θ ∈ [0, c]n
r(θ−i ) ≥ 0
(6)
i=1

155

Guo, Markakis, Apt, & Conitzer

We will show that then for all x ∈ [0, c]n−1 , r(x) = 0 and thus r coincides with VCG.
We divide our proof into two cases.
n−1
X
n−1
xi ≥
Case 1: The vector x satisfies
c.
n
i=1
Given such an x, define C(x) = |{i : xi = c}|, i.e., given a vector x of n − 1 types, C(x)
is the number of agents who submitted c. Define the following predicate:
P (k) : ∀x ∈ [0, c]n−1 ((C(x) = k ∧

n−1
X
i=1

xi ≥

n−1
c) → r(x) = 0)
n

We now prove that P (k) holds for all k ∈ {0, . . ., n − 1}, using induction (going backwards from n − 1). Let ti (θ) = V CGi (θ) − r(θ−i ) be the payment function of agent i under
the mechanism r.
Base case.
Let x be such that C(x) = n − 1. Consider θ := (c, . . ., c) ∈ [0, c]n . Then for all
i ∈ {1, . . ., n}, θ−i = x. Clearly f (θ) = 1 and no agent is paying anything under the VCG
mechanism in this instance, i.e., V CGi (θ) = 0.
Since r is a non-deficit mechanism
n
n
n
n
X
X
X
X
r(θ−i ) = −nr(x),
r(θ−i ) = −
V CGi (θ) −
ti (θ) =
0≤
i=1

i=1

i=1

i=1

But then by (6) we have r(x) = 0.
Induction step.
Assume P (k) holds for some k ≥ 1. We will prove P (k − 1). Let x be such that
C(x) = k − 1 (note that x may have zero c’s). Since r is permutation independent, we can
assume without loss of generality that the elements of x are sorted in descending order (i.e.,
r(x) does not change by such a reordering). Consider the type vector θ = (c, x), that is the
concatenation of (c) and x. Hence θ starts with k c’s and the rest is like the rest of x. Note
that for i ∈ {1, . . ., k}, θ−i = x and C(θ−i ) = k − 1. For i ∈ {k
1, . . ., n}, C(θ−i ) = k,
P+
n
therefore by induction hypothesis, r(θ−i ) = 0. This means that i=1 r(θ−i ) = kr(x).
Furthermore, f (θ) = 1 since θ has at least one c, and no agent is paying payment under
the VCG mechanism. To see this, if k ≥ 2, then for every agent under θ, there is another
agent who submitted c hence the agent
pivotal. If k = 1, then no agent can alter the
P is not
decision outcome by the fact that
xi ≥ n−1
n · c, hence no agent is pivotal in this case as
well. Thus, for all i ∈ {1, . . ., n}, V CGi (θ) = 0, and because r is non-deficit
0≤

n
X

ti (θ) = −

n
X

r(θ−i ) = −kr(x)

i=1

i=1

But then by (6) we have that r(x) = 0. This concludes the induction step and consequently
r(x) = 0 for all vectors x that belong to Case 1.
n−1
X

n−1
c. The proof for this case uses a completely
n
i=0
symmetric argument to that of Case 1. We include it below for the sake of completeness.
Case 2: The vector x satisfies

xi <

156

Undominated Groves Mechanisms

Define C ′ (x) = |{i : xi = 0}|. In analogy to the predicate P (k) of Case 1, we define the
following predicate:
P ′ (k) : ∀x ∈ [0, c]n−1 ((C ′ (x) = k ∧

n−1
X

xi <

i=0

We now prove that
wards from n − 1).

P ′ (k)

n−1
c) → r(x) = 0)
n

holds for all k ∈ {0, ..., n − 1}, using induction (going back-

Base case.
Let x be such that C ′ (x) = n − 1, i.e., the zero vector. Consider θ := (0, ..., 0) ∈ [0, c]n .
Then for all i ∈ {1, . . ., n}, θ−i = x. Clearly f (θ) = 0 and no agent is payingP
anything
under
ti (θ) =
P the VCG mechanism. Hence if ti (θ) is the payment paid by agent i, then
− r(θ−i ) = −nr(x).
Since r is a non-deficit mechanism,
0≤

n
X

ti (θ) = −nr(x)

i=1

Then by (6) this implies that r(x) = 0.
Induction step.
Suppose P ′ (k) holds for some k ≥ 1. We will prove P ′ (k − 1). Let x be such that
C ′ (x) = k −1. Since r is permutation independent, we can assume without loss of generality
that the elements of x are sorted in increasing order so that all 0’s are on the left side of
x (note that it may also be that x does not have any 0’s, since k − 1 maybe equal to 0).
Consider the type vector θ = (0, x). So θ starts with k 0’s and the rest is like the rest of x.
Note that for i ∈ {1, . . ., k}, θ−i = x and C ′ (θ−i ) = kP
− 1. For i ∈ {k + 1, . . ., n}, C ′ (θ−i ) = k
and by induction hypothesis, r(θ−i ) = 0 and hence
r(θ−i ) = kr(x).
We note that f (θ) = 0 and that also no agent is paying payment under the VCG
mechanism. To see this, it is enough P
to verify that no agent is pivotal, which follows by
n−1
the fact that we are in the case that i=0
xi < n−1
n c. Since θ = (0, x), no agent can be
pivotal.P Therefore V CGi (θ) = 0 for every i ∈ {1, . . ., n}. Since r is non-deficit we have
0 ≤ − r(θ−i ) = −kr(x). By (6) we have r(x) = 0.
This completes the proof of the induction step and hence Case 2. Since Cases 1 and 2
cover all vectors x ∈ [0, c]n−1 , the proof of the Lemma is complete.
2
By using now Lemma 6.3 and Lemma 3.3(ii), the proof of Theorem 6.2 is complete.
An interesting open question is whether other mechanisms that share some of the properties of the VCG mechanism are also collectively undominated. In particular, we have
exhibited that VCG is a pay-only and anonymous mechanism. Are there other anonymous
or pay-only mechanisms that are collectively undominated for the public project problem
with equal participation costs?
We start with pay-only mechanisms. We provide a general observation that holds in
many domains other than public project problems, showing that the VCG mechanism dominates all other pay-only mechanisms.
157

Guo, Markakis, Apt, & Conitzer

Lemma 6.4 Let r be a Groves mechanism. Suppose that the following condition
for all i ∈ {1, . . ., n}:

11

holds

∀θ−i ∈ Θ−i ∃b∗i ∈ Θi such that V CGi (b∗i , θ−i ) − ri (θ−i ) = 0.
Then r individually dominates all other pay-only Groves mechanisms.
The condition essentially says that every agent is always able to make his payment equal
to 0 for any type vector θ−i of the other agents.
Proof. Suppose that there exists a pay-only mechanism r′ = (r1′ , . . ., rn′ ) different from
r = (r1 , . . ., rn ) and not dominated by r. Then, for some θ ∈ Θ and i ∈ {1, . . .n}, ri′ (θ−i ) >
ri (θ−i ). Let b∗i be the type of agent i that satisfies the condition of the theorem. Consider
θ′ = (b∗i , θ−i ). Then V CGi (θ′ ) = ri (θ−i ).
But then the payment of agent i under mechanism r′ for the profile θ′ is
t′i (θ′ ) = V CGi (θ′ ) − ri′ (θ−i ) < V CGi (θ′ ) − ri (θ−i ) = 0,
which is a contradiction, because r′ is a pay-only mechanism.

2

Theorem 6.5 Consider the public project problem with equal participation costs. Then for
a pay-only Groves mechanism r, the following are equivalent:
1. r is individually undominated,
2. r is the VCG mechanism,
3. r is collectively undominated.
Proof. 1 → 2. Consider a pay-only and individually undominated Groves mechanism r.
We claim that r is the VCG mechanism.
In the considered domain every agent i, given θ−i , can force his VCG payment to be 0
by declaring b∗i = c/n. Indeed, we then would have V CGi (c/n, θ−i ) = 0. Hence by Lemma
6.4 the VCG mechanism individually dominates all other pay-only mechanisms. This means
that there can be no other individually undominated mechanism than VCG.
2 → 3 holds by Theorem 6.2 and 3 → 1 holds by the definition.

2

The above theorem shows that for the public project problem with equal participation
costs, VCG is the only pay-only Groves mechanism that is individually/collectively undominated. In Appendix A, we show a similar result for anonymous Groves mechanisms,
but only for the case of two agents. That is, if there are exactly two agents, VCG is the
only anonymous Groves mechanism that is individually/collectively undominated. Further,
for n ≥ 3, Hervé Moulin (private communication) observed that for public project problems with equal participation costs, the VCG mechanism is not the only non-deficit Groves
mechanism that is collectively undominated.
11. This is a slight generalization of the Potential for Universal Relevance Nullification (PURN) condition
introduced by Cavallo (2006). An agent satisfies PURN if he can make his payment under the VCG
mechanism equal to 0 for any type vector θ−i of the other agents. Here, the only difference is that we
consider all Groves mechanisms instead of just VCG.

158

Undominated Groves Mechanisms

6.2 The General Case
The assumption that we have made so far in the public project problem that each agent’s
cost share is the same may not always be realistic. Indeed, it may be argued that ‘richer’
agents (such as larger enterprises) should contribute more. Does it matter if we modify the
formulation of the problem appropriately? The answer is ‘yes’. First, let us formalize this
version of the problem. We assume now that each initial utility function is of the form
vi (d, θi ) := d(θi − ci ),
P
where for all i ∈ {1, . . ., n}, ci > 0 and ni=1 ci = c.
In this setting, ci is the share of the project cost to be financed by agent i. We call the
resulting problem the general public project problem. It is taken from Moore (2006). For
this problem we have only two results, both concerning the individual dominance relation.
Theorem 6.6 In the general public project problem the VCG mechanism individually dominates all other pay-only Groves mechanisms.
Proof. Note that for any i and any θ−i , agent i can force his VCG payment to be 0 by
declaring ci , since ti (ci , θ−i ) = 0. By Lemma 6.4 the proof is complete.
2
The above theorem cannot be extended to non-deficit Groves mechanisms, as is illustrated by the following theorem. The theorem below also shows that if there is an
individually undominated mechanism in this setting, it cannot be a pay-only mechanism.
Theorem 6.7 For any n ≥ 3, an instance of the general public project problem with n
agents exists for which the BC mechanism individually dominates the VCG mechanism.
Proof. We will show this for n = 3. For n > 3, it is fairly simple to extend the proof.
We omit the details. The VCG mechanism is non-deficit, hence it suffices to show by
Proposition 2.3(ii) that the VCG and BC mechanisms do not coincide, for some choice of
c, c1 , c2 , c3 , with c1 + c2 + c3 = c.
To this end we need to find θ2 and θ3 so that S1BCGC (θ2 , θ3 ) > 0. Here
((R1 + R2 + R3 ) − L),
S1BCGC (θ2 , θ3 ) := min
′
θ1 ∈Θ1

where for θ′ := (θ1′ , θ2 , θ3 )
L := (n − 1)

n
X

vk (f (θ′ ), θk′ ),

k=1

R1 = max

X

vj (d, θj′ ) = max{0, θ2 + θ3 − (c2 + c3 )},

R2 = max

X

vj (d, θj′ ) = max{0, θ1′ + θ3 − (c1 + c3 )},

R3 = max

X

vj (d, θj′ ) = max{0, θ1′ + θ2 − (c1 + c2 )}.

d∈D

d∈D

d∈D

j6=1

j6=2

j6=3

159

Guo, Markakis, Apt, & Conitzer

Now, take c = 100, c1 = 10, c2 = 40, c3 = 50 and θ2 := 10, θ3 := 70. Then R1 +R2 +R3 =
θ1′ + 10 + max{0, θ1′ − 40}. Two cases arise.
Case 1 f (θ′ ) = 0.
Then L = 0, so (R1 + R2 + R3 ) − L ≥ 10.
Case 2 f (θ′ ) = 1.
Then L = 2(θ1′ + θ2 + θ3 − 100) = 2θ1′ − 40, so
(R1 + R2 + R3 ) − L = 50 − θ1′ + max{0, θ1′ − 40}
≥ (50 − θ1′ ) + (θ1′ − 40) ≥ 10.
This proves that S1BCGC (θ2 , θ3 ) ≥ 10. By taking any θ1′ ∈ [40, 100] we see that in fact
= 10.
2

S1BCGC (θ2 , θ3 )

By virtue of Theorem 6.6 the BC mechanism in the above proof is not pay-only.

7. Conclusions and Future Work
The family of Groves mechanisms, which includes the well-known VCG mechanism (also
known as the Clarke mechanism), is a family of efficient and strategy-proof mechanisms.
Unfortunately, the Groves mechanisms are generally not budget balanced. That is, under
such mechanisms, payments may flow into or out of the system of the agents, resulting
in deficits or reduced utilities for the agents. To identify non-deficit Groves mechanisms
that give the agents the highest utilities, we introduced two general measures for comparing
mechanisms in prior-free settings. Specifically, we say that a non-deficit Groves mechanism
M individually dominates another non-deficit Groves mechanism M ′ if for every type profile, every agent’s utility under M is no less than that under M ′ , and this holds with strict
inequality for at least one type profile and one agent. We say that a non-deficit Groves
mechanism M collectively dominates another non-deficit Groves mechanism M ′ if for every
type profile, the agents’ total utility (social welfare) under M is no less than that under
M ′ , and this holds with strict inequality for at least one type profile. The above definitions
induce two partial orders on non-deficit Groves mechanisms. This paper mainly focused on
studying the maximal elements corresponding to these two partial orders.
A number of interesting open problems remain. Specifically,
• We provided in Section 4.2 two examples showing that collective undominance is
strictly stronger than individual undominance. One example involves a discrete type
space, while the other example involves discontinuous redistribution functions. It
remains to be seen whether the two definitions of undominance coincide when the
type space is smoothly connected and the redistribution functions are continuous.
• We know from Guo and Conitzer (2010) that the OEL mechanisms are not the only
collectively undominated mechanisms in multi-unit auctions with unit demand, because there exist prior distributions under which other mechanisms achieve strictly
higher expected social welfare. That is, for multi-unit auctions with unit demand,
there exist other unknown collectively undominated mechanisms (based on nonlinear
redistribution functions). However, it remains to be seen whether there also exist
collectively undominated mechanisms (other than VCG) for public project problems.
160

Undominated Groves Mechanisms

• We proposed two techniques for generating individually undominated mechanisms.
Can we also derive techniques for generating collectively undominated mechanisms?
Acknowledgments
The authors would like to thank all three reviewers for their useful comments. We also
thank Hervé Moulin for valuable discussions. This work has been supported by the project
DIACODEM of the Dutch organization for scientific research (NWO), and by the project
AGT of the research funding program THALIS (co-financed by the European Social FundESF and Greek national funds). We also thank the National Science Foundation and the
Alfred P. Sloan Foundation for support under Awards IIS-0812113, IIS-0953756, and CCF1101659, and a Sloan Fellowship.

Appendix A. Uniqueness of VCG for the Case of Two Agents
Theorem A.1 Consider the public project problem with equal participation costs. When
the number of agents is n = 2, then for a non-deficit, and anonymous Groves mechanism
r, the following are equivalent:
1. r is individually undominated,
2. r is the VCG mechanism,
3. r is collectively undominated.
Proof. As in the proof of Theorem 6.5 it suffices to show that 1 → 2. So take a non-deficit,
anonymous, and individually undominated Groves mechanism, determined by the function
r.
For x ∈ [0, c], take θ := (x, x). If x ≥ c/2, then the efficient outcome is f (θ) = 1 and
no agent is pivotal, hence the total VCG payment is 0. If x < c/2, then the project is not
built and again no agent is pivotal. Hence in both cases the VCG payment is 0. If t is
the payment function corresponding to r, then we have that t1 (θ) + t2 (θ) = −2r(x). Since
r is non-deficit, we have that for every x ∈ [0, c], r(x) ≤ 0. But since r is individually
undominated, it cannot be the case that r(x) < 0 for some x, because then the VCG
mechanism would dominate r. Hence r coincides with the VCG mechanism.
2

References
Apt, K., Conitzer, V., Guo, M., & Markakis, E. (2008). Welfare undominated Groves mechanisms. In Proceedings of the Fourth Workshop on Internet and Network Economics
(WINE), pp. 426–437, Shanghai, China.
Bailey, M. J. (1997). The demand revealing process: to distribute the surplus. Public Choice,
91, 107–126.
Cavallo, R. (2006). Optimal decision-making with minimal waste: Strategyproof redistribution of VCG payments. In Proceedings of the International Conference on Autonomous
Agents and Multi-Agent Systems (AAMAS), pp. 882–889, Hakodate, Japan.
161

Guo, Markakis, Apt, & Conitzer

Clarke, E. H. (1971). Multipart pricing of public goods. Public Choice, 11, 17–33.
Cramton, P., Gibbons, R., & Klemperer, P. (1987). Dissolving a partnership efficiently.
Econometrica, 55 (3), 615–632.
de Clippel, G., Naroditskiy, V., & Greenwald, A. (2009). Destroy to save. In Proceedings
of the ACM Conference on Electronic Commerce (EC), pp. 207–214, Stanford, CA,
USA.
Faltings, B. (2005). A budget-balanced, incentive-compatible scheme for social choice. In
Agent-Mediated Electronic Commerce (AMEC), LNAI, 3435, pp. 30–43.
Groves, T. (1973). Incentives in teams. Econometrica, 41, 617–631.
Gujar, S., & Narahari, Y. (2011). Redistribution mechanisms for assignment of heterogeneous objects. J. Artif. Intell. Res. (JAIR), 41, 131–154.
Guo, M. (2011). VCG redistribution with gross substitutes. In Proceedings of the National
Conference on Artificial Intelligence (AAAI), San Francisco, CA, USA.
Guo, M. (2012). Worst-case optimal redistribution of VCG payments in heterogeneousitem auctions with unit demand. In Proceedings of the Eleventh International Joint
Conference on Autonomous Agents and Multi-Agent Systems (AAMAS), Valencia,
Spain.
Guo, M., & Conitzer, V. (2008a). Better redistribution with inefficient allocation in multiunit auctions with unit demand. In Proceedings of the ACM Conference on Electronic
Commerce (EC), pp. 210–219, Chicago, IL, USA.
Guo, M., & Conitzer, V. (2008b). Undominated VCG redistribution mechanisms. In Proceedings of the Seventh International Joint Conference on Autonomous Agents and
Multi-Agent Systems (AAMAS), pp. 1039–1046, Estoril, Portugal.
Guo, M., & Conitzer, V. (2009). Worst-case optimal redistribution of VCG payments in
multi-unit auctions. Games and Economic Behavior, 67 (1), 69–98.
Guo, M., & Conitzer, V. (2010). Optimal-in-expectation redistribution mechanisms. Artificial Intelligence, 174 (5-6), 363–381.
Guo, M., Naroditskiy, V., Conitzer, V., Greenwald, A., & Jennings, N. R. (2011). Budgetbalanced and nearly efficient randomized mechanisms: Public goods and beyond. In
Proceedings of the Seventh Workshop on Internet and Network Economics (WINE),
Singapore.
Holmström, B. (1979). Groves’ scheme on restricted domains. Econometrica, 47 (5), 1137–
1144.
Laffont, J., & Maskin, E. (1997). The theory of incentives: An overview, in: W. Hildenbrand,
ed., Advances in economics theory, Econometric Society Monograph in Quantitative
Economics. Cambridge University Press.
Mas-Colell, A., Whinston, M., & Green, J. R. (1995). Microeconomic Theory. Oxford
University Press.
Moore, J. (2006). General Equilibrium and Welfare Economics: An Introduction. Springer.
Moulin, H. (1988). Axioms of Cooperative Decision Making. Cambridge University Press.
162

Undominated Groves Mechanisms

Moulin, H. (1986). Characterizations of the pivotal mechanism. Journal of Public Economics, 31 (1), 53–78.
Moulin, H. (2009). Almost budget-balanced VCG mechanisms to assign multiple objects.
Journal of Economic Theory, 144 (1), 96–119.
Myerson, R., & Satterthwaite, M. (1983). Efficient mechanisms for bilateral trading. Journal
of Economic Theory, 28, 265–281.
Porter, R., Shoham, Y., & Tennenholtz, M. (2004). Fair imposition. Journal of Economic
Theory, 118, 209–228.

163

Journal of Artificial Intelligence Research 46 (2012) 47-87

Submitted 07/12; published 01/13

Optimal Rectangle Packing:
An Absolute Placement Approach
Eric Huang

ehuang@parc.com

Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304 USA

Richard E. Korf

korf@cs.ucla.edu

UCLA Computer Science Department
4532E Boelter Hall
University of California, Los Angeles
Los Angeles, CA 90095-1596 USA

Abstract
We consider the problem of finding all enclosing rectangles of minimum area that can
contain a given set of rectangles without overlap. Our rectangle packer chooses the xcoordinates of all the rectangles before any of the y-coordinates. We then transform the
problem into a perfect-packing problem with no empty space by adding additional rectangles. To determine the y-coordinates, we branch on the different rectangles that can be
placed in each empty position. Our packer allows us to extend the known solutions for a
consecutive-square benchmark from 27 to 32 squares. We also introduce three new benchmarks, avoiding properties that make a benchmark easy, such as rectangles with shared
dimensions. Our third benchmark consists of rectangles of increasingly high precision. To
pack them efficiently, we limit the rectangles’ coordinates and the bounding box dimensions
to the set of subset sums of the rectangles’ dimensions. Overall, our algorithms represent
the current state-of-the-art for this problem, outperforming other algorithms by orders of
magnitude, depending on the benchmark.

1. Introduction
Given a set of rectangles, our problem is to find all enclosing rectangles of minimum area
that will contain them without overlap. We refer to an enclosing rectangle as a bounding box,
to avoid confusion. The optimization problem is NP-hard, while the problem of deciding
whether a set of rectangles can be packed in a given bounding box is NP-complete, via a
reduction from bin-packing (Korf, 2003). The consecutive-square benchmark is a simple set
of increasingly difficult benchmarks for this problem, where the task is to find the bounding
boxes of minimum area that contain a set of squares of dimensions 1 × 1, 2 × 2, ..., up
to N × N (Korf, 2003). For example, Figure 1 is an optimal solution for N =32. We will
use this benchmark to explain many of the ideas in this paper, but our techniques are not
limited to packing squares, and apply to all rectangles.
Rectangle packing has many practical applications, including modeling some scheduling problems where tasks require resources that are allocated in contiguous chunks. For
example, consider the task of scheduling and allocating contiguous memory addresses to
programs. The width of a rectangle represents the length of time a program runs, and the
c
2012
AI Access Foundation. All rights reserved.

Huang & Korf

Figure 1: An optimal solution for N =32 of the consecutive-square benchmark, packing
squares of dimensions 1 × 1, 2 × 2, ..., 31 × 31, and 32 × 32 in a bounding box of minimum
area, which is 85 × 135.

48

Optimal Rectangle Packing: An Absolute Placement Approach

height represents the amount of contiguous memory it needs. A rectangle packing solution
tells us both when programs should be run, as well as which memory addresses they should
be assigned. Similar problems include scheduling when and where ships of different length
can be berthed along a single, long wharf (Li, Leong, & Quek, 2004), as well as the allocation and scheduling of radio frequency spectra usage (Mitola & Maguire, 1999). Rectangle
packing also appears when loading a set of rectangular objects on a pallet without stacking
them. Some cutting stock and layout problems also contain rectangle packing subproblems.
1.1 Overview
The remainder of this article is organized as follows. We first introduce various benchmarks in Section 2 that specifically define the rectangle packing instances we will solve.
In Section 3, we review the state-of-the-art rectangle packers and their techniques, which
provides a foundation upon which we present our new work. We follow in Section 4 with the
data collected and compare our work against the previous state-of-the-art using previous
benchmarks. We also compare the difficulty of previous benchmarks with our new ones.
In Section 5, we present a benchmark of rectangles of successively higher precision
dimensions, new solution techniques to handle this, and follow with experimental results.
Then we compare our methods to the competing search spaces used for packing highprecision rectangles, and show that our methods remain competitive.
Sections 6 and 7 explain various avenues for future work, concluding this article by
summarizing all of our contributions and results. We have previously published much of
this work in several conference papers (Huang & Korf, 2009, 2010, 2011).

2. Benchmarks
There are several reasons motivating our benchmarks. First, our benchmarks describe
instances with a single parameter N , allowing researchers to easily reproduce the instances.
Second, because the instances are unique, optimal solutions that are reported can be easily
validated by others. These are advantages over many real-world instance libraries and
randomly generated ones. Third, our benchmarks define an infinite set of instances where
each successive instance is harder than the previous. A solver is superior to another solver
if it can solve the same instance faster, or a larger instance in the same amount of time.
By contrast, comparison using a library of instances may require counting the number of
instances that are completed within a given time limit. Furthermore, with instance libraries,
often one solver performs well on one subset of instances while a competing solver performs
well on a different subset, making such comparisons inconclusive.
We believe our benchmarks capture some of the more difficult instances a rectangle
packer may face so we do not investigate the modeling and generation of random problems.
Although Clautiaux et al. (2007) and others have used random instances, the non-random
benchmarks used by Korf (2003) and Simonis and O’Sullivan (2008) have better facilitated
the comparison of state-of-the-art packers. However, for more comprehensive overviews,
we refer the reader to the numerous surveys available (Lodi, Martello, & Vigo, 2002; Lodi,
Martello, & Monaci, 2002; Dowsland & Dowsland, 1992; Sweeney & Paternoster, 1992).
49

Huang & Korf

2.1 Previous Benchmarks
Several of the previous benchmarks used in the literature can be shown to be easier than the
benchmarks that we propose. Part of this is due to the fact that benchmarks, like solvers,
may also be improved with further research, to ensure that they cover various properties of
rectangles, in addition to providing an easy way to compare performance among different
packers and measure progress.
The consecutive-square benchmark (Korf, 2003), is a simple set of increasingly difficult
instances, where the task is to find all bounding boxes of minimum area that contain a set
of squares of sizes 1 × 1, 2 × 2, ..., up to N × N . Prior to our work, many of the recent stateof-the-art packers used this popular benchmark to measure performance, including that of
Moffitt and Pollack (2006), Korf, Moffitt, and Pollack (2010), and Simonis and O’Sullivan
(2008). To date, the largest instance solved for this problem is N =32, shown in Figure 1,
using our packer (Huang & Korf, 2009). We do not consider the problem of packing squares
in a square because this benchmark gets much easier as the problem size increases, due to
large differences in the areas of consecutive square bounding boxes.
In the unoriented consecutive-rectangle benchmark (Korf et al., 2010), an instance is
a set of rectangles of sizes 1 × 2, 2 × 3, ..., up to N × (N + 1), and rectangles may be
rotated by 90-degrees. As we will subsequently explain, the fact that there are many pairs
of rectangles in this instance which share equal dimensions causes the optimal solutions to
leave no empty space, making this benchmark easy to solve. We include this benchmark for
completeness, but note that it is not an effective measure for comparing different packers.
Finding only the first optimal solution is another benchmark Simonis and O’Sullivan
(2011) have used in conjunction with problem instances from the unoriented consecutiverectangle benchmark. In contrast to our problem of finding all optimal solutions, they
measure the time it takes to find only the first optimal solution, which makes it much more
difficult to reliably compare against other solvers unless the focus of the research is on value
ordering and tie-breaking among bounding boxes of equal area.
For example, Simonis and O’Sullivan (2011) report that to find the first solution to
N =26 takes 3:28:20 (3 hours, 28 minutes, and 20 seconds). As shown in Table 8 on page
72, there are six solutions for N =26: 42 × 156, 52 × 126, 56 × 117, 63 × 104, 72 × 91, 78 × 84,
each requiring our solver CPU times of 0:32, 41:40, 53:19, 1:55:04, 1:33:22, and 8:53:01,
respectively. There are no smaller bounding boxes we needed to test because the optimal
solution has no empty space, so if we used Simonis and O’Sullivan’s termination criteria and
just returned the first optimal solution, we would only need 32 seconds. Therefore, finding
all minimum bounding boxes instead of just the first one is a benchmark which produces
harder problems for larger N , and better facilitates program comparisons.
2.2 Properties of Easy Benchmarks to Avoid
To motivate our new benchmarks, we will now explain why the previous benchmarks tended
to be much easier in comparison, and why we have constructed our new benchmarks to
describe instances consisting of rectangles with unique dimensions, without duplicates, and
without most of the area being occupied by only a few rectangles.
50

Optimal Rectangle Packing: An Absolute Placement Approach

(a) Solution in a 21 × 35 bounding
box for the unoriented instance 1 × 2,
2 × 3, ..., 11 × 12, 12 × 13.

(b) Solution in a 14 × 26 bounding
box for the unoriented instance 1 ×
12, 2 × 11, ..., 11 × 2, 12 × 1.

Figure 2: Examples of solutions for instances of rectangles with equal dimensions.

2.2.1 Rectangles With Equal Dimensions
In the unoriented consecutive-rectangle benchmark, all rectangles share a dimension with
another rectangle. For example, Figure 2a is an optimal solution for N =12. In optimal
solutions, rectangles of equal dimensions tend to line up next to each other, forming larger
rectangles and leaving little empty space. In Figure 2a, the 8 × 9 and 7 × 8 line up, as
do the 5 × 6 with the 4 × 5, and the 3 × 4 with the 2 × 3. In fact, the solutions to this
benchmark all have a much smaller percentage of empty space than similar-sized instances
from the consecutive-square benchmark, where all rectangles have unique dimensions. We
also notice that benchmarks with duplicate rectangles, such as that in Figure 2b, are solved
quickly.
2.2.2 Rectangles With Small Area and Small Dimensions
Figure 2b is also an example of a perfect packing, because there is no empty space in the
solution. Problems with perfect packings tend to be easy for two reasons. One is that if
we test bounding boxes in increasing order of area, we test fewer boxes, since we never test
a box with more than the minimum area required. The second is that for these problems,
rather than deciding for each rectangle where it should go in the bounding box, a more
efficient algorithm is to decide for each cell of empty space which rectangle should occupy
51

Huang & Korf

it. As soon as a small region of empty space is created that can’t accomodate any remaining
rectangles, the algorithm can backtrack.
In both the consecutive-square and the unoriented rectangle benchmarks, a few large
rectangles capture much of the total area in an instance. Thus, the packer does not search
too deeply before using up the allowable empty space. With little empty space, early
backtracking is very likely since we cannot find a place for the next rectangle. Therefore,
small rectangles in these benchmarks have an insignificant impact on the search effort.
In previous benchmarks, such as the consecutive-square benchmark, the retangles with
the largest area also have the largest dimensions, making it obvious which rectangles to
place first, because the largest rectangles are the most constrained, and impose the most
constraints on the remaining rectangles.
By contrast, in our new benchmarks there is a trade-off between rectangles with large
dimensions and those with large area. The widest rectangle in our oriented equal-perimeter benchmark, described below, has the smallest branching factor as we search for xcoordinates. However, it also has the least area, so during search it won’t constrain the
placement of the remaining rectangles much. This raises the non-trivial question of the best
variable ordering for non-square rectangles.
2.3 New Benchmarks
We propose several new benchmarks that are more difficult when comparing instances
with the same number of rectangles. Our experimental results make use of the following
benchmarks, in addition to the consecutive-square and unoriented consecutive-rectangle
benchmarks described above.
2.3.1 Equal-Perimeter Rectangles
First, we present the oriented equal-perimeter rectangle benchmark, where each instance is a
set of rectangles of sizes 1 × N , 2 × (N − 1), ..., (N − 1) × 2, N × 1, and rectangles may not be
rotated (see Figure 3). Given N , all rectangles are unique and have a perimeter of 2N +2. In
our experiments, this benchmark is much more difficult than either the consecutive-square
benchmark or the unoriented consecutive-rectangle benchmark (Korf et al., 2010) for the
same number of rectangles. We tested our state-of-the-art packer (Huang & Korf, 2010)
on both old and new benchmarks. N =22 from our oriented equal-perimeter benchmark
took over nine hours to solve, while N =22 from the consecutive-square and unoriented
consecutive-rectangle benchmarks took only one second and six seconds, respectively.
Second, we present the unoriented double-perimeter rectangle benchmark, where instances are described as a set of rectangles 1 × (2N − 1), 2 × (2N − 2), ..., (N − 1) × (N + 1),
N × N , and rectangles may be rotated by 90-degrees. All rectangles here are unique and
have a perimeter of 4N . Not only is this benchmark more difficult than the benchmarks
used previously in the literature, but this benchmark also is more difficult than the oriented
one we introduced in the previous paragraph. In our experiments using all of our techniques,
N =18 took over two days to solve.
So far, the benchmarks that we have discussed all have low-precision integer dimensions.
This property poses no problem for our packer, which enumerates the various integer coordinate locations where a rectangle may be placed. With high-precision values, however,
52

Optimal Rectangle Packing: An Absolute Placement Approach

Figure 3: An optimal solution for N =23 of the oriented equal-perimeter benchmark, packing
oriented rectangles of dimensions 1 × 23, 2 × 22, ..., 22 × 2, and 23 × 1 in a bounding box
of minimum area, which is 38 × 61.

53

Huang & Korf

the number of distinct positions increases dramatically. This motivates our study of packing rectangles with high-precision dimensions. In particular, we propose the unoriented
high-precision rectangle benchmark, where instances are described as a set of rectangles
1
1 1
1
1
1
1 × 2 , 2 × 3 , ..., up to N × N +1 . The methods used to solve this benchmark are quite
different from those used in the low-precision case.

3. Solution Techniques
In this section we describe previous solution strategies as well as the various new techniques
we use in our rectangle packer. We first describe our techniques as they apply to the consecutive-square benchmark, the oriented equal-perimeter benchmark, and the unoriented
double-perimeter benchmark. Our work on the unoriented high-precision rectangle benchmark is not included here because the methods are significantly different, and is deferred
to Section 5.
3.1 Previous Work
Some of the earlier work that focused on optimal methods for packing a set of rectangles in
a given bounding box were motivated by the problem of pallet loading. Dowsland (1987)
used depth-first search on an abstract graph representation of the search space to solve
the problem optimally on problem sets modeled after real-world pallet and box dimensions.
Although her problem instances contained an average of 30 rectangles and up to 50, her
benchmarks were far easier than those we consider here, as all of the rectangles were the
same size, and there was a significant amount of empty space in the solutions. Bhattacharya
et al. (1998) extended the work with additional lower bounds and pruning techniques based
on dominance conditions and demonstrated their work on the same benchmarks.
In examining rectangle packing instances where rectangles are of different dimensions,
Onodera et al. (1991) used depth-first search, in which each branching point in their search
space was a commitment to a particular non-overlap constraint between two rectangles.
Lower bound and graph reduction techniques were applied to prune the search space, allowing them to optimally solve problems with up to six rectangles.
Chan and Markov’s BloBB (2004) packer used branch-and-bound in order to find the
minimum area bounding box that can contain a set of rectangles. Their solver could handle
up to eleven rectangles, and they observed that instances with duplicate rectangles were
much easier, causing their packer to cluster such rectangles together in an optimal solution.
Lesh et al.’s solver (2004) used depth-first search, placing each rectangle first in the bottommost and left-most position in which it fit (the bottom-left heuristic, see Chazelle, 1983), to
determine whether or not a set of rectangles can be packed in a given enclosing rectangle.
They were able to handle about twenty-nine rectangles in ten minutes on average, but their
testbed consisted only of instances whose optimal solutions had no empty space.
Clautiaux et al. (2007) presented a branch-and-bound method in which all the x-coordinates for the rectangles were computed prior to any of the y-coordinates. While assigning
x-coordinates, their method uses a relaxation similar to the cumulative constraint (Aggoun
& Beldiceanu, 1993) which requires that the sum of the heights of all rectangles overlapping
a particular x-coordinate cannot exceed the height of the bounding box. The y-coordinates
are then determined using a search space derived from the bottom-left heuristic (Chazelle,
54

Optimal Rectangle Packing: An Absolute Placement Approach

1983), using optimized data structures from Martello and Vigo (1998). Beldiceanu and
Carlsson (2001) applied the plane sweep algorithm used in computational geometry to detect violations of the non-overlap constraints, and later adapted the technique to a geometric
constraint kernel (Beldiceanu, Carlsson, Poder, Sadek, & Truchet, 2007). Lipovetskii (2008)
proposed a branch-and-bound algorithm that placed rectangles in the lower-left hand positions.
The prior state-of-the-art, due to Korf (2003, 2004) and Simonis and O’Sullivan (2008),
both divide the rectangle packing problem into the containment problem and the minimal
bounding box problem. The former tries to pack a given set of rectangles in a given bounding
box, while the latter finds the bounding box of least area that can contain the given set of
rectangles. In both packers the algorithm for the minimal bounding box problem calls the
algorithm for the containment problem as a subroutine.
3.2 Our Overall Search Strategy
Like Korf et al.’s (2010) algorithm, we have a minimum bounding box solver which calls a
containment problem solver, and like Simonis and O’Sullivan (2008), we assign x-coordinates
prior to any of the y-coordinates.
Although we use some of Simonis and O’Sullivan’s (2008) ideas, we do not take a
constraint programming approach in which all constraints are specified to a general-purpose
solver like Prolog. Instead, we implemented our program from scratch in C++, allowing us
to more flexibly choose which constraints to use at what time and to naturally encode the
search space we use for the y-coordinates. We implemented a chronological backtracking
algorithm with dynamic variable ordering. Our algorithm works in five stages as it goes
from the root of the search tree down to the leaves:
1. The minimum bounding box algorithm generates an initial candidate set of bounding
boxes of various widths and heights.
2. The containment solver is called for each bounding box in order of increasing area,
and for each infeasible bounding box, we insert another back into the candidate set of
bounding boxes with a height one unit greater. If a packing was found, we continue
testing boxes of equal area to find all optimal solutions before terminating.
3. The containment solver first works on the x-coordinates in a model where variables
are rectangles and values are x-coordinate locations, using dynamic variable ordering
and a constraint that detects infeasible subtrees.
4. For each x-coordinate solution found, the problem is transformed into a perfect packing instance.
5. It then searches for a set of y-coordinates in a model where variables are empty corners
and values are rectangles.
We now describe in detail each of these steps.
55

Huang & Korf

3.3 Minimum Bounding Box Problem
One way to solve the minimum bounding box problem is to find the minimum and maximum
areas describing the set of candidate and potentially optimal bounding boxes. Boxes of all
sizes are generated with areas within this range, and then tested in non-decreasing order
of area until all solutions of smallest area are found. A lower bound on the area is the
sum of the areas of the given rectangles. An upper bound on the area is determined by
the bounding box of a greedy solution found by setting the bounding box height to that
of the tallest rectangle, and then placing the rectangles in the first available position when
scanning from left to right, and for each column scanning from bottom to top.
There are several techniques (Korf, 2003, 2004) that we use to prune the set of bounding
boxes, which we review here. We first generate a set of widths for our bounding boxes,
starting with the width of the widest rectangle up to the width of the greedy solution
described above. Then for each width, we generate a feasible height using lower bounds
which we will subsequently describe. The resulting bounding boxes are used to initialize
a min-heap sorted in non-decreasing order of area. The search proceeds by calling the
containment solver on the bounding box of minimal area in this heap. If the box is infeasible,
then we increase the height of the box by one, and insert the new box back into the min-heap.
For a given bounding box width, we initialize its height to the maximum of the following
lower bounds. First, the height must be at least the height of the tallest rectangle in the
instance. Second, the height must be large enough to accommodate the total area of the
rectangles in the instance. Third, for every pair of rectangles, if the sum of their widths
exceed the width of the bounding box, then the bounding box height must be at least
the sum of their heights, since they can’t appear side-by-side, but one must be on top of
the other. Fourth, the set of rectangles whose widths are greater than half the width of
the bounding box must all be stacked vertically, including the rectangle of smallest height
whose width is exactly half the width of the bounding box. Finally, if certain properties
exist for a given rectangle packing instance, we force the height to be greater than or equal
to the width to break symmetry. For example, one sufficient property is having an instance
consisting of just squares, since a solution in a W × H bounding box easily transforms into
another one in a H × W bounding box. Another sufficient property is when every rectangle
of dimensions w × h can correspond to another one of dimensions h × w.
For unoriented instances, given a bounding box width, certain rectangles may be forced
into one orientation, improving the lower bound on the bounding box height. Note that we
can also break the symmetry on the bounding box dimensions for every unoriented instance.
3.3.1 Anytime Algorithm
In a problem instance with many rectangles, or when an immediate solution is required,
Korf (2003) provides an anytime algorithm for the bounding box problem, replacing the one
described above, which also calls the containment problem solver. We first find a greedy
solution on a bounding box whose height is equal to the tallest rectangle, as described in the
previous section. We then repeatedly call the containment problem solver in the following
way. If the previous attempt for a given bounding box resulted in a packing or if its area
is greater than the area of the best solution seen so far, then we decrease the width by
one unit and attempt to solve the resulting bounding box problem. If instead the previous
56

Optimal Rectangle Packing: An Absolute Placement Approach

attempt were infeasible, then we increase the height of the bounding box by one unit. The
algorithm terminates when the width of the current bounding box is less than the width of
the widest rectangle.
3.4 Containment Problem
Korf’s (2003) absolute placement approach modeled rectangles as variables and positions in
the bounding box as values. Rectangles were placed in turn with a depth-first search, and
all possible locations were tested for each rectangle. By contrast, Simonis and O’Sullivan’s
(2008) packer assigned the x-coordinates of all the rectangles before any of the y-coordinates,
as suggested by Clautiaux et al. (2007), as well as using the cumulative constraint (Aggoun
& Beldiceanu, 1993), improving performance by orders of magnitude. The cumulative
constraint adds the height of all the rectangles that overlap a given x-coordinate location,
pruning if any of these values exceed the height of the bounding box. This constraint was
checked while exploring x-coordinates and also while exploring y-coordinates later on. We
improved on this by exploring the y-coordinates differently, modeling candidate locations
as variables, and rectangles as values (Huang & Korf, 2009), which made our packer over
an order of magnitude faster than that of Simonis and O’Sullivan’s.
Simonis and O’Sullivan (2008) furthermore applied the least-commitment principle (Yap,
2004) from constraint processing, by first committing the placement of rectangles to an
interval of x-coordinates instead of just a single x-coordinate value. These x-intervals are
explored in turn, and constrain the candidate individual x-coordinates explored later. This
works because committing to an x-interval can induce pruning via the cumulative constraint.
For example, picking an x-interval of [a, b] with a size that is smaller than the width of the
rectangle wr , implies that regardless of which x-coordinate the rectangle eventually takes,
it must contribute its height to each x-coordinate within the interval [b, a + wr ]. Finally,
the height of the bounding box constrains the cumulative heights of all rectangles for any
given x-coordinate, similar to the ideas of Beldiceanu et al. (2008). Larger intervals result in
weaker constraint propagation (less pruning) but a smaller branching factor, while smaller
intervals result in stronger constraint propagation but a larger branching factor. The size
of the intervals are experimentally determined.
For example, a 4 × 2 rectangle with x-coordinates restricted to the interval [0,2] contributes a height of 2 at x-coordinates 2 and 3 even prior to deciding its exact x-coordinate
value. This compulsory part (Lahrichi, 1982) constrains the cumulative height of the rectangles that may overlap x-coordinates 2 and 3 in the solution. If these interval assignments
were all infeasible, then searching for individual x-values is futile. However, if we do find
a set of interval assignments, then we still have to search for a set of single x-coordinate
values. Simonis and O’Sullivan (2008) assigned x-intervals, single x-coordinates, y-intervals,
and single y-coordinates, in that order.
3.5 Assigning X-Intervals and X-Coordinates
For the x-coordinates, we propose a pruning constraint adapted from Korf’s (2003) wastedspace pruning heuristic, a dynamic variable order to replace Beldiceanu’s (2008) fixed ordering, and a method to optimize the values assigned to our x-interval variables.
57

Huang & Korf

Figure 4: To test for violations of the cumulative constraint, the remaining space after
placing a 3 × 2 rectangle at x=2 is represented as the vector h3, 3, 1, 1, 1, 3i.

3.5.1 Pruning Infeasible Subtrees
We present a constraint-based formulation of Korf’s (2003) two-dimensional wasted space
pruning algorithm, adapted to the one-dimensional case. Given a partial solution, Korf’s
algorithm computed a lower bound on the amount of wasted space, which was then used
to prune against an upper bound. By contrast, we do not compute any numerical bounds
and instead detect infeasibility with a single constraint.
As rectangles are placed in the bounding box, the remaining empty space gets chopped
up into small irregular regions. Eventually the empty space is segmented into small enough
chunks such that they cannot accommodate any of the remaining unplaced rectangles, at
which point we backtrack. While assigning x-coordinates in a bounding box of height H,
we keep a histogram hv1 , v2 , . . . , vH i, where vi is the number of empty cells (units of empty
space) that are in empty columns of height i. For example, assume that in Figure 4 we
assigned only the x-coordinates of a 3 × 2 rectangle in a 6 × 3 bounding box. The resulting
histogram would be h3, 0, 9i, since there are 3 cells in empty columns of height 1, no empty
cells in columns of height 2, and 9 cells in empty columns of height 3.
Assume now that we only have left to place a 2 × 3 and a 2 × 2 rectangle. We can assign
the six cells of the 2 × 3 rectangle to the empty cells of v3 =9, leaving us with the remaining
empty cells h3, 0, 3i. At this point, we cannot assign the area of the 2 × 2, because we only
have 3 empty cells that can accommodate its height and we need 4, so we can prune.
In general, for a set of unplaced rectangles R and a bounding box of height H,

∀h, 

X

wr hr ≤

r∈R,hr ≥h

H
X


vi  ,

(1)

i=h

where a rectangle r ∈ R has dimensions wr × hr . That is, for every given height h, the
amount of space that can accommodate rectangles of height h or greater must be at least
the cumulative area of rectangles of height h or greater. We check this constraint after each
x-coordinate assignment.
58

Optimal Rectangle Packing: An Absolute Placement Approach

(a) x=2 is a dominated position for
the 4 × 4 square.

(b) x=0 is an undominated position
for the 4 × 4 square.

Figure 5: Example of dominance conditions.

3.5.2 Pruning With Dominance Conditions
Korf (2003) introduced a set of dominance conditions to prune positions where large rectangles are too close to the sides of the bounding box. For example, imagine that we must pack
the squares 4×4, 3×3, 2×2, and 1×1. In Figure 5a, the placement of the 4×4 square leaves
a 2 × 4 gap against the left side of the bounding box in which the 3 × 3 square cannot fit.
Only the 2 × 2 and 1 × 1 squares can fit within the gap, and in fact they both can be placed
entirely within the gap. Notice that in any solution with an arrangement as in Figure 5a,
we can always rearrange them as in Figure 5b without disturbing any other squares. Thus,
there is no need to try placing the 4 × 4 square at x=2 so long as we have tried placing it at
x=0. In general, a rectangle placement is dominated if it leaves a gap in which all rectangles
that can individually fit can also be packed together in the gap without protruding from
it. Although Korf hard-coded dominance rules for the consecutive-square benchmark, we
dynamically generate them for every instance with insignificant preprocessing overhead.
3.5.3 Variable Ordering
In the following subsections we consider two variable orders that work together in our packer.
We use a fixed ordering that governs which rectangle is assigned next. This ordering is used
for the x-intervals independently from its use on the single x-coordinate variables. At
any point in time, we also must choose whether to assign the next x-interval or the next
single x-coordinate variable. Since the ordering between x-intervals and single x-coordinate
variables is simpler, we present this technique first.
Ordering Between X-Intervals and X-Coordinates By Area Our variable order
is based on the observation that placing rectangles of larger area is more constraining
than placing those of smaller area. At all times we can either choose to assign a single
x-coordinate to a rectangle for which we previously had assigned an x-interval, or we can
assign an x-interval to a rectangle we have not yet made any assignments for. As shown in
Figure 4, either of these assignments will decrease the amount of empty space represented
in the cumulative constraint vector. We always pick next the variable that results in the
least remaining space.
59

Huang & Korf

Ordering Among Rectangles By Branching Factor There is a natural variable order
that arises from both the consecutive-square and unoriented consecutive-rectangle benchmarks when using the strategy of picking the most constrained variable next. For example,
in the consecutive-square benchmark, the largest rectangle is clearly the largest in height,
width, and area. However, in our new benchmarks the rectangle of largest width has the
smallest height, but not the largest area, making a good variable ordering non-obvious.
We propose a variable order over rectangles of various aspect ratios by picking the
variable with the fewest number of values first, to favor a smaller branching factor closer
to the root of the search tree. For the oriented equal-perimeter benchmark, recall that we
assign intervals to the x-coordinates before the individual x-coordinates, and like Simonis
and Sullivan (2008) we use a constant factor times the rectangle width to define the interval
size. The branching factor for the x-interval variables for a given rectangle is
 
Bw − rw
1
Bw 1
b=
− ,
(2)
=
Crw
C rw
C
where Bw is the bounding box width, rw is the rectangle width, and C is a constant chosen
experimentally. The numerator Bw − rw is the number of x-coordinate values that the
rectangle can have while still fitting in the bounding box, and the denominator Crw is the
size of the interval we will be assigning to the given rectangle. For example, if C=0.75 then
we would assign intervals of size three to a 4 × 2 rectangle.
We may drop the translational constant −1/C as well as the positive scalar Bw /C since
we are only interested in a relative ordering for the rectangles, leaving us with 1/rw which
means that for the oriented benchmark we should place the rectangles in order of decreasing
width. For the unoriented double-perimeter benchmark, our packer first tries all values for
a particular x-interval, and then rotates the rectangle 90-degrees before trying another set
of x-interval values. In this case the branching factor is


Bw − rw
Bw 1
Bw − rh
1
2
b=
=
+
+
− .
(3)
Crw
Crh
C rw
rh
C
As mentioned before, we can drop the scalar and translational constant, giving us
1
rw + rh
1
=
+
.
(4)
rw
rh
rw rh
Because all rectangles in a given instance have the same perimeter by definition, the
numerator of the result in Equation 4 is constant. Therefore for our unoriented benchmark,
we place the rectangles in order of decreasing area.
3.5.4 Determining Sizes of X-Intervals
On the consecutive-square benchmark, our packer used an interval size that is 0.35 times
the width of a given rectangle. We found that larger interval sizes improve the performance
of our packer on the new equal-perimeter benchmarks, and use a value of C=0.55 instead.
As we assign larger intervals to the short and wide rectangles, the x-interval variables
for these rectangles tend to have branching factors of three or less. We should balance the
sizes of these intervals so that the values assigned are equally constraining on their subtrees.
For example, consider C=0.55, a rectangle of width 20, and its set of possible x-coordinate
60

Optimal Rectangle Packing: An Absolute Placement Approach

values [0,23]. Without balancing the sizes of the intervals, our packer would explore interval
sizes of 20C = 11, such as x=[0,10], x=[11,21], and finally the remaining domain values
with a small interval of x=[22,23]. This results in small compulsory parts and therefore
large search subtrees in the first two branches, but a very large compulsory part and thus
a small search subtree in the third.
Since we must explore three branches anyway, we can balance the sizes of these interval
assignments by exploring x=[0,7], x=[8,15], and x=[16,23]. The eventual effect is a better
balance on the size of the search subtrees amongst branches. Our packer first computes the
branching factor induced by the global interval parameter C=0.55 for each rectangle, and
then it balances the number of values in each interval assignment.
Interactions Between Interval Assignment and Dominance Conditions On consecutive-square instances, for most of the squares there are several positions following x=0
that are dominated. Therefore, our packer first branches by assigning the degenerate interval x=[0,0] before exploring interval assignments for the undominated positions. Although
this technique increased the performance of our packer fivefold compared to leaving it out,
the same strategy slowed the performance fivefold on the oriented and unoriented doubleperimeter benchmark. The reason for this degradation of performance is as follows.
In our equal-perimeter benchmarks, the 1 × N rectangle can always partially fit in gaps
left by other rectangles, but it must always protrude out of those gaps, thereby eliminating
the dominance conditions we previously described. Without any dominated positions to
account for, simply applying the same strategy used for consecutive-squares on our new
benchmarks results in our packer committing to single x-coordinate values in situations
where it is more desirable to include those positions in a larger interval assignment. To
avoid this, our packer detects when there are no dominated positions and dynamically
chooses whether to assign the degenerate interval as the x-coordinate assignment, or to
immediately begin with interval assignments.
3.6 Perfect Packing Transformation
For every complete x-coordinate solution, we transform the problem instance into a perfect
packing problem instance before working on the y-coordinates. A perfect packing instance
is a rectangle packing problem with the property that the solution has no empty space.
The transformation is done by adding to the original set of rectangles a number of 1 × 1
rectangles necessary to increase the total area of the rectangles to that of the bounding box.
Although the new 1 × 1 rectangles increase the problem size, the hope is that the ease of
solving perfect packing instances will offset the difficulty of packing more rectangles. Next
we describe our search space for perfect packing. As we will show, our methods rely on the
perfect packing property of having no empty space.
3.7 Assigning Y-Coordinates
An alternative to asking “Where should this rectangle go?” is to ask “Which rectangle
should go here?” In the former model, rectangles are variables and empty locations are
values, whereas in the latter, empty locations are variables and rectangles are values. For
y-coordinates, we search the latter model. We use a 2D bitmap to draw in placed rectangles
61

Huang & Korf

to test for overlap, and we backtrack on positions that cannot accommodate any remaining
rectangles, or as required by Korf’s (2003) wasted space pruning rule.
3.7.1 Empty Corner Model
In all perfect packing solutions, every rectangle’s lower-left corner fits in some lower-left
empty corner formed by other rectangles, the sides of the bounding box, or a combination
of both. In this model, we have one variable per empty corner. In the final solution, since
each rectangle goes into exactly one empty corner, the number of empty corner variables is
equal to the number of rectangles in the perfect packing instance. The set of values is just
the set of unplaced rectangles.
This search space has the interesting property that variables are dynamically created
during search because the x- and y-coordinates of an empty corner are known only after the
rectangles that create it are placed. Furthermore, placing a rectangle in an empty corner
assigns both its x- and y-coordinates.
Note that the empty corner model can describe all perfect packing solutions. Given any
perfect packing solution, we can list a unique sequence of all the rectangles by scanning
left to right, bottom to top for the lower-left corners of the rectangles. This sequence
corresponds to a sequence of assignments from the root of this search space to a leaf in the
tree. This property also bounds the maximum size of the search space by N 0 ! where N 0 is
the number of rectangles after we have performed the perfect packing transformation.
3.7.2 Duplicate Rectangles
Due to the additional 1 × 1 rectangles from the perfect packing transformation, we have
introduced additional redundancy into the problem. A simple way to handle this is as
follows. For a particular empty corner, we never place a rectangle that is a duplicate of one
we have already tried at that position. This method of handling duplicates also applies to
duplicate rectangles in the original problem instance.

4. Experimental Results
We benchmarked our packers in Linux on a 2GHz AMD Opteron 246 with 2GB of RAM.
The packer we call KMP10 (Korf et al., 2010) was benchmarked on the same machine,
so we quote their published results. We do not include data for their relative placement
packer because it was not competitive. Results for Simonis and O’Sullivan’s packer (2008),
which we call SS08, are also quoted, obtained from SICStus Prolog 4.0.2 for Windows on a
3GHz Intel Xeon 5450 with 3.25GB of RAM. Since their machine is faster than ours, these
comparisons are a conservative estimate of our relative performance.
4.1 Previous Benchmarks
Because both the consecutive-square benchmark and the unoriented consecutive-rectangle
benchmarks (Korf et al., 2010) have been used in the literature to measure performance,
we include data collected using these two benchmarks.
62

Optimal Rectangle Packing: An Absolute Placement Approach

Size
N

KMP10
Time

SS08
Time

FixedOrder
Time

20
21
22
23
24
25
26
27
28
29
30
31
32

1:32
9:54
37:03
3:15:23
10:17:02
2:02:58:36
8:20:14:51
34:04:01:03

:02
:07
:51
3:58
5:56
40:38
3:41:43
11:30:02

:00
:03
:02
:14
:40
2:27
10:25
1:08:55
2:18:12:13

HK09
Time
:00
:03
:02
:12
:37
2:14
9:39
35:12
4:39:31
8:06:03
2:17:32:52
4:16:03:42
33:11:36:23

Table 1: CPU times required by various packers on the consecutive-square benchmark,
where the task is to pack squares from 1 × 1 up to N × N .

4.1.1 Consecutive Squares
Table 1 compares the CPU runtimes of four packers on the consecutive-square benchmark.
The first column specifies the instance size, which is both the number of squares and the
size of the largest one. The remaining columns specify the CPU times required by various
algorithms to find all the optimal solutions in the format of days, hours, minutes, and
seconds. When there are multiple boxes of minimum area, as for N =27 as listed in Table
8 of Appendix 4.4, we report the total time required to find all optimal bounding boxes.
We do this for two reasons. First, finding all minimum area bounding boxes removes the
question of which bounding box to test first if more than one have the same area. Second,
by providing all optimal solutions, other researchers working on rectangle packing can use
this information to verify the correctness of their programs.
HK09 includes our wasted space pruning rule for the x-coordinates, dynamic variable
ordering between x-intervals and x-coordinates, the perfect packing transformation, and
its related search space and inference rules. We have named this packer to be consistent
with our previous work (Huang & Korf, 2009). SS08 refers to the previous state-of-the-art
packer (Simonis & O’Sullivan, 2008). The largest problem previously solved was N =27
and took SS08 over 11 hours. We solved the same problem in 35 minutes and solved five
more open problems up to N =32. KMP10 refers to Korf et al.’s (2010) absolute placement
packer. FixedOrder assigns all x-intervals before any single x-coordinates, but includes all
of our other ideas. HK09’s dynamic variable ordering for the x-coordinates was an order
of magnitude faster than FixedOrder by N =28. The order of magnitude improvement
of FixedOrder over SS08 is likely due to our use of perfect packing for assigning the ycoordinates. We do not include the timing for a packer with perfect packing disabled
because it was not competitive (e.g., N =20 took over 2.5 hours).
63

Huang & Korf

Size
N
21
22
23
24
25
26
27
28
29
30
31
32

X-Coordinate
Solutions
665
283
391
870
193
1,026
244
2,715
11,129
10,244
73,614
37,742

Seconds
in X
0.35
0.95
6.54
19.41
73.38
313.81
1,181.53
8,987.36
15,677.20
124,399.74
214,575.08
1,916,312.67

Seconds
in Y
1.04
0.18
0.31
1.08
0.14
1.39
0.60
23.40
28.82
17.97
254.42
102.59

Ratio
X:Y
0.3
5.3
21.1
18.0
524.1
225.8
1,969.2
384.1
544.0
6,922.6
843.4
18,679.3

Table 2: CPU times spent searching for x- and y-coordinates for the consecutive-square
benchmark

In Table 2 the second column is the number of complete x-coordinate assignments our
packer found over the entire run of a particular problem instance. The third column is the
total time spent in searching for the x-coordinates. The fourth column is the total time
spent in performing the perfect packing transformation and searching for the y-coordinates.
Both columns represent the total CPU time over an entire run for a given problem instance.
The last column is the ratio of time in the third column to that of the fourth. Interestingly,
almost all of the time is spent on the x-coordinates as opposed to the y-coordinates, which
suggests that if we could efficiently enumerate the x-coordinate solutions, we could also
efficiently solve rectangle packing. This is confirmed by the relatively few x-coordinate
solutions that exist even for large instances. The data in Table 2 was obtained on a Linux
2.93GHz Intel Core 2 Duo E7500 machine in a separate experiment from that of Table 1,
which is why the total time spent on a given instance is different.
4.1.2 Unoriented Consecutive Rectangles
Table 3 compares the CPU times of our packer on the unoriented consecutive-rectangles
benchmark with that of Korf et al. (2010). Although the techniques due to Simonis and
O’Sullivan (2008) outperform those of Korf et al. on the consecutive-square benchmark,
there were no previously published results on this benchmark besides that of Korf et al.
Because this benchmark is easier than the consecutive-square benchmark, we do not break
down the contributions of each of our techniques, as these differences were delineated more
clearly in the previous section. The primary differentiating feature of this benchmark is
that rectangles are unoriented.
The first column gives the size of the problem instance. The second column gives the
performance of the previous state-of-the-art packer on this benchmark, using Korf et al.’s
code (2010). The third column gives the performance of our packer on this benchmark. All
64

Optimal Rectangle Packing: An Absolute Placement Approach

Size
N

KMP10
Time

HK10
Time

16
17
18
19
20
21
22
23
24
25
26
27
28
29

:01
:05
:17
:07
8:11
15:00
1:09:45
8:51:46
11:53:17
7:17:00:03

:00
:00
:00
:00
:05
:06
:17
:47
13:38
2:21:10
6:31:51
4:07:37:08
1:16:43:02
6:04:47:06

Table 3: CPU times required by two packers on the unoriented consecutive-rectangle benchmark, where the task is to pack unoriented rectangles of sizes 1×2, 2×3, ..., and N ×(N +1).

of the data in this table was collected on a Linux 2.93GHz Intel Core 2 Duo E7500 machine,
except for N =28 and N =29, which were collected on a Linux 2.53GHz Intel Xeon E5630
with 12GB of RAM, and which our experiments revealed to be 20% faster than the former
machine.
For this benchmark our techniques have allowed us to extend the known solutions from
N =25 to N =29 and allowed us to solve N =25 about 80 times faster than the previous
state-of-the-art on this benchmark.
4.2 Oriented Equal-Perimeter and Unoriented Double-Perimeter Rectangles
This section uses our new benchmarks to compare the techniques we have developed for
non-square instances. The techniques we discuss here, including the dynamic adjustment
of interval sizes and the generalized variable order based on branching factor, largely do
not affect the performance of our packer on the consecutive-square benchmark. In fact, we
tested this packer on that benchmark to see the effects of any extra overhead added by
our improvements. Our new packer resulted in only a five percent speedup compared to
our packer without these changes on the consecutive-square benchmark, likely due to minor
improvements in data structures, and balancing interval sizes. Therefore, we compare the
effects of these techniques only on our new benchmarks. Because the techniques we have
developed for our new benchmarks improve performance in both the oriented and unoriented
cases, we discuss them together.
Table 4 compares the performance of our packers on the oriented equal-perimeter benchmark while Table 5 compares the same packers using our unoriented double-perimeter
benchmark. The first column refers to the problem size of the instance, which is the number
65

Huang & Korf

Size
N

Boxes
Tested

HK09
Time

OptDom
Time

BrFactor
Time

C=0.55
Time

HK10
Time

13
14
15
16
17
18
19
20
21
22
23

7
7
10
9
8
12
12
11
9
15
16

:01
:02
:16
:57
5:56
1:06:32
6:35:48
1:18:51:34
3:21:31:46

:00
:01
:05
:16
1:21
14:47
1:26:16
7:36:09
13:33:16

:00
:00
:01
:02
:27
6:15
31:23
1:51:10
4:22:49

:00
:00
:00
:00
:03
:32
3:34
13:06
20:49
14:22:03

:00
:00
:00
:00
:02
:22
2:15
7:51
11:20
9:12:37
3:22:50:38

Table 4: CPU times required by various packers on the oriented equal-perimeter rectangle
benchmark, where the task is to pack oriented rectangles of sizes 1 × N , 2 × (N − 1), ...,
(N − 1) × 2, and N × 1.

Size
N

Boxes
Tested

HK09
Time

OptDom
Time

BrFactor
Time

C=0.55
Time

HK10
Time

11
12
13
14
15
16
17
18

12
17
13
17
21
35
27
35

:01
:20
1:45
28:48
1:43:01
1:16:46:44

:00
:04
:21
4:53
11:36
4:13:34
1:12:40:14

:00
:04
:21
4:53
11:36
4:13:34
1:12:40:14

:00
:01
:06
1:19
3:33
1:16:02
9:44:14

:00
:01
:06
1:15
2:34
1:01:54
7:53:50
2:02:10:38

Table 5: CPU times required by various packers on the unoriented double-perimeter rectangle benchmark, where the task is to pack unoriented rectangles of sizes 1 × (2N − 1),
2 × (2N − 2), ..., (N − 1) × (N + 1), and N × N .

66

Optimal Rectangle Packing: An Absolute Placement Approach

of rectangles. The second column gives the number of bounding boxes tested in order to
find all optimal solutions. The remaining columns represent the CPU times for different
versions of our packer in the format of days, hours, minutes, and seconds. We wrote our
packer in C++ and collected our data on a Linux 2.93GHz Intel Core 2 Duo E7500 machine.
From left to right, each successive packer improves on the previous one by including an
additional technique. The column called HK09 is data collected using only the techniques
developed specifically for consecutive-square packing, which include the perfect packing
transformation and its related inference rules, dynamic variable ordering between single
x-coordinates and x-intervals, and the wasted space pruning rule for the x-coordinates
(Huang & Korf, 2009). To compare against our new variable ordering over rectangles of
various aspect ratios, we used the order of decreasing area by default in HK09.
OptDom improves upon HK09 by dynamically detecting when dominance rules apply or
are inapplicable, and optimizes the x-interval assignments with this knowledge. BrFactor
improves upon OptDom in that it orders the oriented equal-perimeter benchmark by decreasing width and the unoriented double-perimeter benchmark by decreasing area. C=0.55
improves upon BrFactor in that we use an interval size of 0.55 instead of C=0.35 as we
did for the consecutive-square benchmark. Finally, HK10 improves upon C=0.55 by using
knowledge of the branching factor to rebalance the sizes of the interval assignments for the
x-coordinates.
Notice that OptDom, BrFactor, and C=0.55 introduce techniques that reduce the
branching factor, and so they have a greater effect on performance than HK10, whose
new technique seeks to make the intervals assigned equally constraining. Our experiments
reveal that these techniques interact with one another, and we note that without including
dominated positions in the intervals, the performance gained from the other techniques
appears muted. This interaction is also why we tune the global interval parameter C only
after including the other techniques that affect the branching factor.
Ordering by branching factor improved performance for our oriented equal-perimeter
benchmark but not for our unoriented benchmark. In the latter case, as seen in Table 5,
our technique of ordering by branching factor prescribes ordering by decreasing area, which
is what we gave the packer as a reasonable default. Therefore, there is no difference in the
algorithm nor in its performance between the OptDom and BrFactor columns of Table 5.
Note that the unoriented double-perimeter benchmark requires our packer to try over
twice as many bounding boxes for a given parameter N than that required for our oriented benchmark. This is due to having 2N -1 as the largest dimension in the unoriented
benchmark while having N as the largest dimension in the oriented benchmark. The larger
rectangles introduce a higher precision into the problem, and so we must try more bounding
boxes. The containment problem for an unoriented instance has a problem space that is
a factor of 2N larger than that of an oriented instance due to the two orientations of each
rectangle. Thus, an instance with N rectangles in this benchmark is incomparable to an
instance of N squares from the consecutive-square benchmark when evaluating benchmark
difficulty.
In summary, using all of our techniques together, we can solve N =21 of the oriented
equal-perimeter benchmark about 500 times faster and N =16 of the unoriented doubleperimeter benchmark about 40 times faster than the techniques we presented optimized
only for consecutive squares.
67

Huang & Korf

Size

Boxes Tested

CPU Time

N

Squares

Perimeter

Squares

Perimeter

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

10
5
14
12
14
20
17
19
19
17
21
22
30
27
21
30
36

9
8
12
12
11
9
15
16

:00
:00
:00
:00
:00
:01
:01
:07
:20
1:14
5:15
19:42
2:30:11
4:21:46
1:10:33:38
2:11:40:29
22:04:20:15

:00
:02
:22
2:15
7:51
11:20
9:12:37
3:22:50:38

Table 6: Number of bounding boxes tested and CPU time required to solve a given instance
in the consecutive-square and the oriented equal-perimeter benchmarks.

68

Optimal Rectangle Packing: An Absolute Placement Approach

4.3 Comparing Easy and Hard Benchmarks
The following tables compare the difficulty of various benchmarks using our packer (Huang
& Korf, 2010) with all optimizations enabled.
4.3.1 Consecutive Squares vs. Equal-Perimeter Rectangles
In Table 6, the first column indicates the number of rectangles in the instance. The second
and third columns labeled “Boxes Tested” give the number of bounding boxes that were
tested when finding all optimal solutions for the consecutive-square benchmark and the
oriented equal-perimeter benchmark, respectively. The fourth and fifth columns give the
performance of our rectangle packer on both benchmarks as well. Each data point in this
table was collected using a Linux 2.93GHz Intel Core 2 Duo E7500 using one process, one
thread, and one core.
Notice that for a given instance with the same number of rectangles, the oriented equalperimeter benchmark is significantly harder than the consecutive-square benchmark. This
is due to the fact that for a given problem size, the consecutive-square benchmark contains
many little squares that are typically easy to place – a property missing in the equalperimeter benchmark. In fact, by N =23 our packer requires over four orders of magnitude
more time to find the optimal solutions to our new benchmark compared to an instance
with the same number of items from the consecutive-square benchmark.
4.3.2 Unoriented Consecutive-Rectangles vs. Unoriented Double-Perimeter
Rectangles
Table 7 shows how removing certain properties results in successively more difficult benchmarks. We start with the unoriented consecutive-rectangle benchmark (Korf et al., 2010)
which contains many easy properties. In the “Doubly Scaled” column we pack 2 × 4, 4 × 6,
6×8, ..., (2N )×(2N +2) rectangles, which simply scales up the unoriented consecutive-rectangle benchmark by a factor of two. This benchmark is more difficult because integers of
higher magnitude lead to more x-coordinates to search, which in turn increases the branching factor of the problem. In the “Unique Dimensions” column we now pack rectangles of
sizes 1 × 2, 3 × 4, 5 × 6, ..., (2N − 1) × (2N ), which differs from the previous benchmark in
that all dimensions are unique. The last column distributes the area among the rectangles
more uniformly to avoid consolidating most of the area in the first few rectangles. This
column is also the culmination of all of the difficult properties which we have identified for
a rectangle packing benchmark, which we call our unoriented double-perimeter benchmark.
All data points in this table were collected using a Linux 2.93GHz Intel Core 2 Duo E7500
machine without any parallelization, except for N =28 and N =29, which were collected on
a Linux 2.53GHz Intel Xeon E5630 machine with 12GB of RAM, which we estimate to be
thirty percent faster.
4.4 Bounding Boxes of Minimum Area
In this section we list all of the optimal bounding boxes on various benchmarks found by
our program with all optimizations enabled. Notice that we do not duplicate the data for
69

Huang & Korf

Size
N
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

Unoriented
ConsecutiveRectangles
:00
:00
:00
:00
:00
:00
:00
:00
:05
:06
:17
:47
13:38
2:21:10
6:31:51
4:07:37:08
1:16:43:02
6:04:47:06

Doubly
Scaled
:00
:00
:00
:00
:00
:00
:01
:01
:09
:10
:29
1:13
27:37
6:41:20
1:02:12:06

Unique
Dimensions
:00
:00
:01
:00
:01
:01
:03
:11
:50
3:00
15:34
3:21:36
12:23:37

Unoriented
DoublePerimeter
:01
:06
1:15
2:34
1:01:54
7:53:50
2:02:10:38

Table 7: CPU time required for our optimized packer on various benchmarks of increasing
difficulty.

70

Optimal Rectangle Packing: An Absolute Placement Approach

the unoriented high-precision rectangle benchmark, and leave it in Table 10, Section 5.5.2,
since the discussion there refers to this data.
The first column in tables 8 and 9 refer to the size of the problem instance for their
respective benchmarks. The columns called Optimal Solution give the dimensions of the
optimal bounding boxes for a given instance. The next column called Empty Space gives
the percent of empty space in the optimal solution. The next column gives the number of
bounding boxes that were tested in order to find all optimal solutions for a given instance.

5. Absolute Placement on High-Precision Instances
Meir and Moser (1968) first proposed the problem of finding the smallest square that can
contain an infinite series of rectangles of sizes 11 × 12 , 12 × 31 , 13 × 14 , ..., etc. The rectangles
cannot overlap and are unoriented. The unit square has exactly enough area since the total
area of the rectangles in the infinite series is one. On the other hand, no space can be
wasted, suggesting that the task is infeasible. Inspired by this problem, we propose our last
benchmark and developed several new techniques.
We introduce the unoriented high-precision rectangle benchmark, where the task is to find
all bounding boxes of minimum area that can contain a finite set of unoriented rectangles
of sizes 11 × 12 , 12 × 13 , ..., up to N1 × N 1+1 . For example, for N =4 one must pack rectangles of
sizes 11 × 12 , 21 × 13 , 13 × 41 , and 14 × 51 . Alternatively, one may try to pack rectangles of sizes
60 × 30, 30 × 20, 20 × 15, and 15 × 12 into a 60 × 60 square, which is just the original instance
scaled up by a factor of 60, the least common multiple of the rectangle denominators. This
strategy is required for the broad class of recent rectangle-packers that explore the domain
of integer x- and y-coordinates for the rectangles and quickly break down at higher N .
For example, the optimal solution of N =15 has over 400 billion unique coordinate pairs
that rectangles can be assigned to. Our benchmark complements rather than replaces the
current low-precision benchmarks, which until now have neglected high-precision instances.
The remainder of this section is organized as follows. We first review some of the previous
work proposing solution techniques that may be unaffected by the precision of the rectangle
dimensions. Then we describe several adaptations of our low-precision techniques to the
high-precision case, along with some new techniques developed specifically for high-precision
rectangle instances, and finally follow with experimental results.
5.1 Previous Work
The relative placement approach of Moffitt and Pollack (2006) for rectangle packing, and
similar types of search spaces used in resource-constrained scheduling (Weglarz, 1999),
promises to be immune to the problem of high-precision rectangle instances. However, since
there are so many techniques that we have described in the previous sections that cannot
be extended to a packer working in the relative placement search space, we have decided
to stay within the absolute placement framework and attempt to mitigate the problems
introduced by high-precision numbers.
71

Huang & Korf

Consecutive Squares

Consecutive Rectangles

Size
N

Optimal
Solutions

Empty
Space

Boxes
Tested

Optimal
Solutions

Empty
Space

Boxes
Tested

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

1×1
2×3
3×5
5×7
5×12
9×11
11×14, 7×22
14×15
15×20
15×27
19×27
23×29
22×38
23×45
23×55
28×54, 27×56
39×46
31×69
47×53
34×85
38×88
39×98
64×68
56×88
43×129
70×89

0.00%
16.7%
6.67%
14.3%
8.33%
8.08%
9.09%
2.86%
5.00%
4.94%
1.36%
2.55%
2.03%
1.93%
1.98%
1.06%
0.50%
1.40%
0.84%
0.69%
0.99%
0.71%
0.64%
0.58%
0.40%
0.47%

1
1
1
1
1
1
3
2
4
5
3
6
5
8
13
10
5
14
12
14
20
17
19
19
17
21

0.00%
0.00%
0.00%
0.00%
0.00%
1.75%
0.00%
0.00%
1.79%
0.45%
0.00%
0.95%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
0.20%
0.00%
0.00%
0.00%
0.00%
0.00%

1
1
1
2
2
2
2
1
5
5
2
4
1
2
1
2
2
3
2
4
2
2
3
4
5
7

27
28
29
30
31
32

47×148, 74×94
63×123
81×106
51×186
91×110
85×135

0.37%
0.45%
0.36%
0.33%
0.33%
0.31%

22
30
27
21
30
36

1×2
2×4
4×5
5×8, 4×10
5×14
6×19
12×14
15×16
16×21, 14×24
17×26
22×26
21×35
26×35
32×35, 28×40
34×40
32×51
34×57
30×76
35×76, 38×70
35×88, 44×70, 55×56
39×91
44×92
40×115, 46×100
40×130, 52×100, 65×80
45×130, 65×90, 75×78
42×156, 52×126, 56×117,
63×104, 72×91, 78×84
63×116
56×145, 70×116
62×145

0.00%
0.00%
0.00%

3
3
2

Table 8: The optimal solutions for the consecutive-square benchmark, where the task is to
pack squares of sizes 1×1, 2×2, ..., and N ×N , and for the unoriented consecutive-rectangle
benchmark, where the task is to pack unoriented rectangles of sizes 1 × 2, 2 × 3, ..., and
N × (N + 1).

72

Optimal Rectangle Packing: An Absolute Placement Approach

Oriented Equal Perimeter

Unoriented Double Perimeter

Size
N

Optimal
Solutions

Empty
Space

Boxes
Tested

Optimal
Solutions

Empty
Space

Boxes
Tested

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

1×1
2×3
3×4
4×6
6×7
6×10
8×11
8×16
11×16
11×21
14×21
13×29
16×29
19×30, 15×38
24×29
23×36

0.00%
33.3%
16.7%
16.7%
16.7%
6.67%
4.55%
6.25%
6.25%
4.76%
2.72%
3.45%
1.94%
1.75%
2.30%
1.45%

1
1
1
1
4
2
2
5
6
8
6
7
7
7
10
9

0.00%
22.2%
8.33%
7.41%
6.86%
5.85%
3.08%
1.59%
1.50%
0.69%
1.15%
1.37%
0.71%
0.67%
0.67%
0.83%

1
1
2
2
8
9
11
8
13
8
12
17
13
17
21
35

17
18
19
20
21
22
23

24×41
24×48
32×42, 24×56
37×42
35×51
34×60
38×61

1.52%
1.04%
1.04%
0.90%
0.78%
0.78%
0.78%

8
12
12
11
9
15
16

1×1
3×3
3×8
6×9
6×17
9×19
13×20
18×21
13×41
24×30
29×33
21×59
38×41
38×51, 17×114
44×54
45×64, 30×96,
40×72, 48×60
39×88, 52×66
55×74

0.44%
0.57%

27
35

Table 9: The optimal solutions to the oriented equal-perimeter rectangle benchmark, where
the task is to pack oriented rectangles of sizes 1 × N , 2 × (N − 1), ..., (N − 1) × 2, and
N × 1, and to the unoriented double-perimeter rectangle benchmark, where the task is to
pack unoriented rectangles of sizes 1 × (2N − 1), 2 × (2N − 2), ..., (N − 1) × (N + 1), and
N × N.

73

Huang & Korf

(a)

(b)

Figure 6: Examples of mapping solutions to one where rectangles are in their left-most,
bottom-most positions.

5.2 Overall Strategy
Given an instance from our high-precision benchmark described in rational numbers, we
multiply all values by the least common multiple of the denominators to get an instance
with integer dimensions. We then apply the absolute placement solution techniques, with
improvements we will subsequently explain, in order to find the optimal solutions. Once
found, we divide all x- and y-coordinates describing the optimal solutions by the initial
scaling constant in order to obtain the optimal solutions for the original problem.
Note that we can map every solution to one where all rectangles are slid over to the
left and to the bottom as much as possible (Chazelle, 1983). For example, the solution in
Figure 6a can be transformed into that of Figure 6b. Since all rectangles are now propped
up from the left and below by other rectangles, each rectangle’s x-coordinate is the sum
of a subset of the widths of the other rectangles and each rectangle’s y-coordinate is the
sum of a subset of the heights of the other rectangles. Similarly, the width and height of
the bounding box must be the sum of a subset of the widths and heights of the rectangles,
respectively.
In the following subsections we first explain our techniques with respect to oriented
instances, and then follow with how to handle the unoriented case.
5.3 Minimum Bounding Box Problem
Since we build the initial set of bounding boxes from all pairwise combinations of widths
and heights within given ranges, the space is pruned by considering only bounding box
widths and heights equal to the subset sums of the rectangle widths, and the subset sums
of the rectangle heights, respectively. Recall from Section 4.4 that for every bounding box
width, we compute a lower bound on the height. We further modify this by rounding the
resulting bound up to the next subset sum of the rectangle heights.
74

Optimal Rectangle Packing: An Absolute Placement Approach

5.3.1 Precomputing Subset Sums
We compute the set of all subset sums prior to searching. For oriented rectangles which
cannot be rotated we compute two sets: one based only on the heights of the rectangles
representing the candidate y-coordinates, and one based just on their widths representing
the candidate x-coordinates. This distinction generates fewer subset sums compared to a
single set of subset sums generated from both widths and heights.
5.3.2 Pruning Combinations of Widths and Heights
We can reject some bounding boxes for which certain width and height combinations are
infeasible. This pruning technique relies on the observation that in certain cases, there may
be only one unique set of rectangles that generate a specific width (height) for the bounding
box.
For example, consider a bounding box width which can only be generated by a unique
set of rectangles. Now assume that the heights of the same set of rectangles also uniquely
determine the subset sum for a specific bounding box height. We say that this combination
of bounding box width and height is incompatible. The reason is that this set of rectangles
is the only way we can have a bounding box of the given width, and that implies this set
of rectangles must appear in the solution laid out horizontally to one another. Thus, the
same set of rectangles cannot appear stacked vertically in the solution. This contradicts the
implications of a bounding box of the given height. Note that in this particular example,
the only compatible height is the maximum height of the rectangles.
5.3.3 Learning From Infeasible Attempts
Recall that the algorithm for solving the minimal bounding box problem repeatedly calls
the algorithm to solve the containment problem. Bounding boxes are tested in order of
non-decreasing area until the first boxes with solutions are found. We can learn from the
infeasible attempts.
For example, consider having to pack N rectangles {r1 , r2 , ..., rN }. Note that we use a
pre-computed variable order for the rectangles. Let rd , d < N be the rectangle corresponding
to the deepest in the search tree our depth-first search was able to go, during the entire
search effort for a given bounding box. If the containmnet solver says this bounding box
is infeasible, then the next bounding box height that we should consider can be the next
greatest subset sum based on the smaller set {r1 , r2 , ..., rd+1 } instead of considering all N
rectangles. The intuition behind this is that since our containment solver failed to even
find an arrangement for the first d + 1 rectangles, it doesn’t make sense to involve any of
the remaining rectangles {rd+2 , ...rN } in the next largest subset sum for the bounding box
height.
This method resembles conflict-directed backtracking. In our implementation, we also
consider the effect of pruning using the wasted space heuristic as well.
5.4 Containment Problem
Similar to our low-precision methods, we first assign x-coordinates for the rectangles, then
conduct a perfect packing transformation, and finally work on the y-coordinates (Huang
75

Huang & Korf

& Korf, 2010). The main difference between our high-precision methods and our lowprecision methods are that instead of considering all possible integers as the domain of
x- and y-coordinates, we consider the smaller set of subset sums of the widths and heights
of the rectangles. The methods for using x-intervals remain unchanged and so we only
describe how we search individual x-coordinates here.
5.4.1 Assigning X-Coordinates
For oriented rectangles, we choose x-coordinates from the set of subset sums of rectangle
widths. Instead of precomputing the set as we did in the minimal bounding box problem,
here we generate it dynamically at every node during the search prior to branching on
various x-coordinate value assignments. The set is computed as follows:
1. Initialize the set with the value 0, as this represents placing a rectangle against the
left side of the bounding box.
2. For every rectangle r already assigned an x-coordinate at this point of the search,
insert into the set the sum of its x-coordinate and its width. This represents placing
a rectangle against the right side of r.
3. For every rectangle with its x-coordinate still unassigned, add its width to every
element in our set, and insert the new sums back into the set.
5.4.2 Perfect Packing Transformation
After assigning x-coordinates, we create a number of 1 × 1 rectangles to account for all
empty space in the original instance. The transformation results in a new instance, with no
empty space, and consists of the original rectangles plus the new 1 × 1 rectangles. Then for
a given empty corner in a partial solution, we ask which of the original unplaced rectangles
might fit there, or a 1 × 1 rectangle, essentially modeling empty corners as variables and
rectangles as values.
In our high-precision benchmark, solving N =15 requires creating over 1.5 billion 1 × 1
rectangles because we scaled the problem up by a very large number. Here our packer
simply requires too much memory and time. We avoid this problem by creating fewer and
much larger rectangles to account for the empty space.
Widening Existing Rectangles Assume in Figure 7a that the task is to pack three
rectangles. Here we have a 10 × 20, 20 × 10, and a 40 × 10 rectangle in a 60 × 50 bounding
box, and assume we have assigned x-coordinates but not y-coordinates. Given that the
x-coordinates are already assigned, in any resulting packing solution the space to the right
of the 40 × 10 rectangle must always be empty. Thus, we replace the 40 × 10 rectangle with
a 60 × 10 rectangle, effectively widening the original rectangle. Likewise, we replace the
20 × 10 rectangle with a 30 × 10 rectangle, and the 10 × 20 rectangle by a 30 × 20 rectangle,
as in Figure 7b. Our packer greedily attempts to widen the rectangles towards the right
before widening them towards the left. After solving the problem we can just return the
rectangles back to their original widths. This avoids creating many 1 × 1 rectangles during
the perfect packing transformation to represent empty space.
76

Optimal Rectangle Packing: An Absolute Placement Approach

(a) A partial solution where only xcoordinates are known.

(b) The result of widening the rectangles.

Figure 7: Widening existing rectangles.

(b) A solution without 60 × 1 rectangles for empty space.

(a) A partial solution where only xcoordinates are known.

Figure 8: Consolidating empty space into horizontal strips.

Turning Empty Space Into Large Rectangles In the partial solution of Figure 8a, we
have assigned only the x-coordinates of the rectangles in a 60 × 40 bounding box. Instead
of creating three hundred 1 × 1 rectangles to represent the empty space indicated by the
single hash marks, we can use ten 30 × 1 rectangles without losing any packing solutions.
Similarly, we represent the doubly-hashed empty space with twenty 30×1 rectangles instead
of six hundred 1 × 1 rectangles. Note that we cannot use 60 × 1 rectangles for the empty
space since we would inadvertently prune out the potential solution in Figure 8b.
5.4.3 Assigning Y-Coordinates
After the perfect packing transformation, we assign y-coordinates by asking which rectangle
can be placed in a given empty corner. As before, we enforce the constraint that the ycoordinate of each rectangle must be a subset sum of the rectangle heights. Note that the
rectangles we create via the perfect packing transformation are not included in the subset
sum calculations, since they represent empty space.
5.4.4 Handling Unoriented Instances
For unoriented instances, when computing the initial bounding box widths and heights,
we generate a single set of subset sums using both widths and heights from all rectangles
in the instance instead of keeping the widths separated from the heights. Likewise, when
generating the set of candidate x- and y-coordinates, we must add a fourth step to the
77

Huang & Korf

Size
N

Optimal
Solution

LCM

Bits of
Precision

HK10
Boxes

Subsets
Boxes

Mutex
Boxes

HK11
Boxes

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

1/2×1
1/2×4/3
1/2×19/12
5/6×1, 1/2×5/3
1/2×17/10
1/2×107/60
1/2×107/60
1/2×163/90
1/2×163/90
1/2×1817/990
1/2×7367/3960
1/2×67/36
1/2×185/99
1/2×169/90
1/2×79/42

2
6
12
60
60
420
840
2,520
2,520
27,720
27,720
360,360
360,360
360,360
720,720

2
6
8
12
12
18
20
23
23
30
30
37
37
37
39

1
1
2
30
20
1,979
4,033
39,357
13,571
2,682,948

1
1
2
5
7
59
151
693
1,083
7,489
31,196
66,425
289,217
549,135
1,171,765

1
1
2
4
7
44
107
465
755
4,901
22,822
38,827
162,507
382,059
651,041

1
1
2
4
7
29
46
124
192
585
1,641
2,366
5,027
9,548
15,334

Table 10: The minimum-area bounding boxes and number of bounding boxes tested when
packing unoriented rectangles 11 × 21 , 12 × 13 , 31 × 41 , ..., and N1 × N 1+1 .

bulleted list in subsection 5.4.1 where we add the height of every rectangle which has not
yet been placed, to every element in the set of subset sums, as this represents the possibility
of rotating the rectangle.
5.5 Experimental Results
We present two different data tables, one relating to improvements in the minimal bounding
box problem measured by the number of bounding boxes tested, and another one on the
overall CPU time for solving the entire rectangle-packing problem. We can separate our
experiments this way because our solution schema decouples the minimal bounding box
problem from the containment problem.
5.5.1 Minimum Bounding Box Problem
Table 10 shows the optimal solutions for our unoriented high-precision rectangle benchmark
along with various properties of the corresponding instances. The first two columns give the
problem size and the dimensions of the optimal solutions, respectively. The third gives the
least common multiple of the first N +1 integers. The fourth is the number of bits required
to represent the area of the minimal bounding box. Note that all but one of the optimal
solutions have a width of 12 , since the first rectangle is much larger than any of the others.
For N =12 and larger, the required precision exceeds that of a 32-bit integer.
The fifth through eighth columns compare the number of bounding boxes that various
packers test to find all optimal solutions on our unoriented high-precision rectangle bench78

Optimal Rectangle Packing: An Absolute Placement Approach

Size
N
6
7
8
9
10
11
12
13
14
15

HK10
Time

Empty Space
Time

Dynamic
Time

HK11
Time

:00
:02
1:11
1:51

:00
:00
:00
:03
1:57
41:40
7:30:26

:00
:00
:00
:00
:02
:57
6:38
2:20:12
1:05:56:14

:00
:00
:00
:00
:01
:18
:33
16:41
46:56
4:28:20

Table 11: CPU times of various packers to find all minimum-area bounding boxes containing
unoriented rectangles 11 × 12 , 21 × 13 , 13 × 41 , ..., and N1 × N 1+1 .
mark. For each column going from left to right, we add one new technique for the minimal
bounding box problem.
HK10 is the number of bounding boxes required when simply scaling up the problem
to an instance described completely in integers. The column called Subsets improves upon
the second by testing only those bounding boxes whose dimensions are constrained by our
subset sums technique. The column called Mutex improves upon the third by rejecting
bounding boxes if the subset sum corresponding to its width is mutually exclusive to the
subset sum corresponding to its height. HK11 improves upon the previous packer by using
information learned from an infeasible attempt to reject future bounding boxes.
Using all improvements, by N =10 we test 4,500 times fewer bounding boxes compared
to the previous state-of-the-art. On this instance HK10 ran out of memory on the last
bounding box because of the sheer number of 1 × 1 rectangles created during the perfect
packing transformation. The introduction of the prime number 11 as a denominator in the
problem instance is responsible for the increased difficulty between N =9 and N =10.
5.5.2 Containment Problem
Table 11 compares the performance of various packers using our techniques. Because we
have decoupled the minimal bounding box problem from the containment problem, in this
table we use all of our optimizations for the minimal bounding box problem, and only
compare the individual techniques applied to the containment problem. Therefore, the
performance data reported is what is required to solve the overall problem using various
containment problem packers.
The first column gives the size of the problem instance from our high-precision rectangle
benchmark. As in previous tables, each successive column from left to right improves upon
the previous column by an additional technique. The column called HK10 corresponds to
using the previous state-of-the-art with our improved minimal bounding box algorithm.
The column called Empty Space improves upon HK10 by precomputing all of the subset
79

Huang & Korf

sums prior to searching for the x-coordinates, and uses our techniques to consolidate empty
space in the y-coordinates. The column called Dynamic improves upon the previous one
by dynamically computing subset sums. Finally, the last column called HK11 adds the
ability to learn which unplaced rectangles to exclude from the subset sums computation
after exploring an infeasible subtree. This data was collected using a Linux eight core 3GHz
Intel Xeon X5460 without parallelization.
At N =10, the problem was scaled up 27,720 times in both dimensions, requiring HK10
to create 6,597,361 1 × 1 units of empty space during the perfect packing transformation
and causing it to run out of memory. Empty Space could not complete N =13 within a
day because of the sheer number of subset sums that must be explored for both x- and
y-coordinates, a problem avoided by Dynamic.
5.5.3 Comparison to Relative Placement
It is interesting to note that the number of bounding boxes appears to be increasing exponentially, mostly likely due to the exponential growth of the number of subset sums
introduced by each successive rectangle in our high-precision benchmark. The difficulty of
our unoriented high-precision rectangle benchmark is compounded by the fact that as the
precision increases, the branching factor for the single x- and y-coordinate values in the
containment problem also increases.
In contrast to our absolute placement technique, Moffitt and Pollack’s (2006) relative
placement techniques do not enumerate the different exact locations for the rectangles, and
therefore promise to be immune to the problem of high-precision rectangles. They used a
variable for every pair of rectangles to represent the relations above, below, left, and right.
Their search algorithm then required at least one of these non-overlapping constraints to
be true for every pair of rectangles. Their meta-CSP approach was modeled after work
by Dechter, Meiri, and Pearl (1991) on solving binary constraint satisfaction problems,
and included various pruning techniques such as model reduction, symmetry breaking, and
graph-based pruning heuristics (Korf et al., 2010). They solve the minimum bounding box
problem with a branch-and-bound algorithm, evaluating the size of the bounding box when
all non-overlapping relationships have been determined, and keeping track of the bounding
box of smallest area seen so far.
Note that by contrast, our solver tests bounding boxes in order of non-decreasing area.
Also, the size of their formulation uses N 2 variables while we use only N . Finally, their
packer only returns one optimal solution as opposed to ours, which does more work by
returning all of the optimal solutions.
We have been able to benchmark their code on our machine in order to provide some
kind of comparison between their methods and ours. This is a crude comparison, because
we cannot run their packer on our unoriented high-precision rectangle benchmark since they
have hard-coded into their packer the unoriented consecutive-rectangle benchmark, a much
easier benchmark as we have shown in Table 7.
The first column in Table 12 refers to the problem size. The second column called MP06
gives the CPU time required for Moffitt and Pollack’s code on problem instances from the
unoriented consecutive-rectangle benchmark, which uses low-precision rectangles. The third
column called HK11 gives the CPU time required by our packer on problem instances from
80

Optimal Rectangle Packing: An Absolute Placement Approach

Size
N

MP06
Time

HK11
Time

10
11
12
13
14
15

:03
:13
2:26
17:40
1:48:09
7:27:42

:01
:18
:33
16:41
46:56
4:28:20

Table 12: CPU times required by Moffitt and Pollack’s packer on the unoriented consecutiverectangle and our packer on the unoriented high-precision rectangle benchmarks.

the unoriented high-precision rectangle benchmark. Each data point in this table was
collected using an eight core 3GHz Intel Xeon X5460 in Linux without parallelization. Note
that our algorithm packs the same number of rectangles somewhat faster than that of Moffit
and Pollack’s.
5.6 Summary of High-Precision Rectangles
In this section we proposed a new benchmark consisting of instances with rectangles of
high-precision dimensions as well as techniques for using subset sums to limit the number of
positions that must be considered, rules to filter out these subset sums for both the minimal
bounding box and containment problems, methods to learn from infeasible subtrees, and
ways to reduce the number of rectangles created during the perfect packing transformation.
These techniques exploit no special properties of the benchmark, but are most useful for
rectangles with high-precision dimensions.
Using all of our methods, we solved six more problems up to N =15 on our new benchmark compared to using our low-precision packer on a scaled up instance. Our packer is
over two orders of magnitude faster at N =9 than the previous state-of-the-art, and tests
4,500 times fewer bounding boxes. A cursory comparison between the state-of-the-art using
the relative placement search space and our own shows that we perform slightly faster than
Moffitt and Pollack’s packer, on a benchmark which we have previously shown in Section
4.3.2 to be significantly more difficult than the unoriented consecutive-rectangle benchmark
that Moffitt and Pollack’s program was run on.

6. Future Work
Humans solve jigsaw puzzles both by asking where a particular piece should go, as well
as asking what piece should go in some empty region. Our packer makes use of both
models, the former for the x-coordinates and the latter for the y-coordinates. It would
be interesting to see how applicable this dual formulation is in other packing, layout, and
scheduling problems. Currently, we work on the x-coordinates by asking “Where does this
go?”, and we work on the y-coordinates by asking “What goes in this location?” Our
method has reduced the time spent in the y-coordinates so much that now the time spent
81

Huang & Korf

working on the x-coordinates is orders of magnitude greater than the time spent working
on the y-coordinates. This suggests that performance might be improved by considering
both models simultaneously.
As another direction for continued work, the data indicates that the number of bounding
boxes explored by our minimum bounding boxes solver is the main bottleneck to solving
larger instances of our unoriented high-precision rectangle benchmark. An observation we
can make is that across many of these bounding boxes, the same partial solutions are being
explored, resulting in much redundant computation. Consequently, a branch-and-bound
method that starts with a large bounding box, and gradually reduces its dimensions while
various packings are explored would be a promising avenue of further research.

7. Conclusions
We have presented several new improvements to the previous state-of-the-art in optimal
rectangle packing. Within the schema of assigning x-coordinates prior to y-coordinates, we
introduced a dynamic variable order for the x-coordinates, and a constraint that adapts
Korf’s (2003) wasted space pruning heuristic to the one-dimensional case. For the ycoordinates we work on the perfect packing transformation of the original problem, by
using a model that assigns rectangles to empty corners, and inference rules to reduce the
model’s variables.
Our improvements in the search for y-coordinates helped us solve N =27 of the consecutive-square benchmark over an order of magnitude faster than the previous state-ofthe-art, and our improvements in the search for x-coordinates also gave us another order
of magnitude speedup by N =28, compared to leaving those optimizations out. With all
our techniques, we are over 19 times faster than the previous state-of-the-art on the largest
problem solved to date, allowing us to extend the known solutions for the consecutive-square
benchmark from N =27 to N =32. Furthermore, the data show that very little time is spent
searching for y-coordinates, suggesting that rectangle packing may be largely reduced to
the problem of determining the x-coordinates.
All of the techniques presented to pick y-coordinates are tightly coupled with the dual
view of asking what must go in an empty location. Furthermore, while searching for xcoordinates, our pruning rule is based on the analysis of irregular regions of empty space,
and our dynamic variable order also rests on the observation that less empty space leads
to a more constrained problem. The success of these techniques in rectangle packing make
them worth exploring in many other packing, layout, and scheduling problems.
We have also introduced two new benchmarks, one oriented and one unoriented, that
include rectangles of various aspect ratios. These new benchmarks avoid various properties
of easy instances, which we have identified, and were shown to be much harder through a
side-by-side comparison between various benchmarks using the same state-of-the-art packer.
We have also proposed several search strategies to improve performance on our new benchmarks. We improved upon our strategies used to handle dominance conditions, proposed
a variable ordering heuristic based on increasing branching factor that generalizes previous
strategies, tuned a global interval parameter, and introduced a method to balance the sizes
of the intervals assigned to the x-coordinate variables.
82

Optimal Rectangle Packing: An Absolute Placement Approach

Our experiments revealed that it takes orders of magnitude more time to solve our new
benchmarks compared to instances from the consecutive-square benchmark with the same
number of rectangles. We therefore advocate the inclusion of these new, more difficult
benchmarks in the suite of benchmarks used for research in optimal rectangle packing. Finally, using all of our techniques together, we solved N =21 of the oriented equal-perimeter
benchmark about 500 times faster, and N =16 of the unoriented double-perimeter benchmark about 40 times faster than simply using methods tuned for consecutive-squares.
In order to test the limits of our rectangle packer, we presented a new high-precision benchmark specifically capturing the pathological case where each successive rectangle
quickly increases the precision required to represent coordinate locations. We presented
various techniques to adapt the absolute placement approach to handle these types of instances, including dynamically using subset sums to limit the number of coordinate values
that must be tested, mutex reasoning that allows us to reject certain combinations of subset
sums used for a bounding box’s width and height, a general method for rejecting future subset sums based on a previously infeasible search, and finally a memory-efficient adaptation
of our perfect packing transformation to high-precision rectangle instances.
We solved N =12 of the high-precision benchmark in half a minute, 800 times faster than
a basic version of our packer augmented with only the high-precision version of our perfect
packing inference rules so that it did not run out of memory. This was also the first instance
requiring precision exceeding the capacity of a 32-bit integer. Our techniques allowed us
to solve up to N =15 compared to N =9, the largest instance our low-precision techniques
alone could solve. Our methods also reduced the number of bounding boxes generated by
a factor of 4,500. At this point we are solving problems that require a minimum of 39 bits
of precision, which should meet the requirements of many real-world problems.
We then provided a comparison to the state-of-the-art relative placement packer showing
that our absolute-placement packer remains competitive even on rectangles of high-precision, and reported on promising avenues of research which may potentially give the absolute
placement approach a clear competitive edge over relative-placement methods.
Although we have mainly focused on obtaining optimal solutions in our benchmarks,
our work may be easily adapted to applications requiring quick suboptimal solutions by
simply replacing our algorithm for the minimum bounding box problem with alternatives
such as the anytime algorithm that we described in Section 3.3.1.
7.1 Comparison to Constraint Programming Methodologies
There are clearly tradeoffs between taking our ground-up programming approach in C++
and taking a constraint programming approach. While the latter provides quick prototyping
and reuse of constraint libraries that other researchers have already implemented, it also
forces the problem to be expressed in the abstract constraint language. Such an abstract
layer turns out to add unnecessary overhead for the algorithms and data structures that
one naturally uses to solve our problem of optimal rectangle packing.
For example, as we previously described, for the cumulative constraint, we simply add
a constant to a consecutive range in an integer array when we assign an x-coordinate to a
rectangle. When we backtrack, we scan the same array and just subtract the same constant.
Scanning and manipulating arrays, iteration, and fast pushing and popping of the program
83

Huang & Korf

stack in recursive algorithms are precisely the operations that modern computer hardware
has been optimized for. This is significant as we explore over two trillion search nodes
for N =32 in the square-packing benchmark, and in fact our solver spends about 75% of
its time on just these array manipulation operations alone. This is how we explain the
orders of magnitude speedup for processing just the x-coordinate solutions in a 1D array
instead of the 2D bitmap by Korf (2003). As we move from 1D arrays, to 2D bitmaps, to
abstract representations of variables and values in constraint programming, the patterns
of computation and data structures simply become too distant from what the underlying
hardware is optimized for.
For optimal rectangle packing, it happens that the algorithms and data structures that
naturally solve the problem map very nicely in form and function to the hardware of modern
computers. Note that one may always port this code into a constraint module that may be
called by a constraint solver, but there is still some computational indirection between this
module and the backtracking control logic of the constraint solver. The sacrifice we make
in our approach, however, is the fact that our solver is tailored specifically to the rectangle
packing problem as we have defined it, and it would require more implementation effort to
reconfigure our algorithms and heuristics for a slightly different rectangle packing problem.
We hope, however, that this latter problem is ameliorated by disciplined object-oriented,
modular software design.

8. Broader Lessons
Beyond the specific problem of rectangle packing, what broader lessons can we learn from
this work? We believe there are several.
One of the main applications of rectangle packing is to scheduling. As described in the
introduction, the rectangle packing problem is an abstraction of a scheduling problem where
different tasks take different amounts of time, and all require different amounts of a onedimensional resource that must be allocated contiguously, such as memory on a computer.
The width of the bounding box becomes the total time, the height the total amount of the
resource available, and each job becomes a rectangle with width equal to time duration,
and height equal to the amount of the resource required.
What we found, however, is that vast majority of the time used by our rectangle packer
is in assigning just the x-coordinates of the rectangles, subject to the cumulative constraint,
which is that for every x-coordinate in the bounding box, the sum of the heights of the rectangles that overlap that x-coordinate cannot exceed the height of the bounding box. This
important subpart of the rectangle-packing problem models a much more general problem
known as the resource-constrained scheduling problem. This is the same as the scheduling
problem described above, but without the constraint that the resource be allocated contiguously. For example, in scheduling tasks on a planetary rover with a limited power budget,
the sum of the power requirements of all the tasks that are active at any given time cannot
exceed the total power budget of the rover. Thus, this subpart of our rectangle packer can
be used to tackle this more general scheduling problem.
Another general lesson that can be learned from this work is that the absolute placement
approach to various packing problems in two, three, or more dimensions may be effective
even on problems with high precision dimensions. One might expect that absolute placement
84

Optimal Rectangle Packing: An Absolute Placement Approach

would not be competitive with relative placement approaches on these problems, but the
key to our success in this area is that instead of considering all possible placements, we
only consider placements that correspond to subset sums of the relevant dimensions. While
there is no guarantee that this approach will work in other high-precision packing problems,
we have shown that it is at least worth considering, and may be effective.
Perhaps the largest lesson to be learned here is both encouraging and discouraging. The
problem of rectangle packing is extremely simple, and can be understood by and played
as a game by children. Yet the research over the last decade described here shows that
the most efficient algorithms are quite complex. If the best algorithms for such a simple
problem are so complex, it is likely that the best algorithms for more complex problems
are even more complex, which is the discouraging part. The encouraging part is that the
history of this research has shown that each new idea can result in an order of magnitude
improvement over the previous state of the art on larger problems, suggesting that there is
still very significant progress to be made on this problem, and by extension others like it.

Acknowledgments
We wish to thank Reza Ahmadi, Adnan Darwiche, and Adam Meyerson for their advice
on this work. We also thank Michael Moffitt for making his packer available. This research was funded in part by the National Science Foundation under grant number IIS0713178. The source code of our optimal rectangle packer is open sourced and available at
http://code.google.com/p/rectpack.

References
Aggoun, A., & Beldiceanu, N. (1993). Extending chip in order to solve complex scheduling
and placement problems. Mathematical and Computer Modelling, 17 (7), 57–73.
Beldiceanu, N., & Carlsson, M. (2001). Sweep as a generic pruning technique applied to the
non-overlapping rectangles constraint. In CP ’01: Proceedings of the 7th International
Conference on Principles and Practice of Constraint Programming, pp. 377–391, London, UK. Springer-Verlag.
Beldiceanu, N., Carlsson, M., & Poder, E. (2008). New filtering for the cumulative constraint
in the context of non-overlapping rectangles. In Perron, L., & Trick, M. A. (Eds.),
CPAIOR, Vol. 5015 of Lecture Notes in Computer Science, pp. 21–35. Springer.
Beldiceanu, N., Carlsson, M., Poder, E., Sadek, R., & Truchet, C. (2007). A generic geometrical constraint kernel in space and time for handling polymorphic k-dimensional
objects. In Bessiere, C. (Ed.), CP, Vol. 4741 of Lecture Notes in Computer Science,
pp. 180–194. Springer.
Bhattacharya, S., Roy, R., & Bhattacharya, S. (1998). An exact depth-first algorithm for the
pallet loading problem. European Journal of Operational Research, 110 (3), 610–625.
Chan, H. H., & Markov, I. L. (2004). Practical slicing and non-slicing block-packing without
simulated annealing. In GLSVLSI ’04: Proceedings of the 14th ACM Great Lakes
symposium on VLSI, pp. 282–287, New York, NY, USA. ACM.
85

Huang & Korf

Chazelle, B. (1983). The bottomn-left bin-packing heuristic: An efficient implementation.
IEEE Transactions on Computers, C-32 (8), 697–707.
Clautiaux, F., Carlier, J., & Moukrim, A. (2007). A new exact method for the twodimensional orthogonal packing problem. European Journal of Operational Research,
183 (3), 1196–1211.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49 (1-3), 61–95.
Dowsland, K. A. (1987). An exact algorithm for the pallet loading problem. European
Journal of Operational Research, 31 (1), 78–84.
Dowsland, K. A., & Dowsland, W. B. (1992). Packing problems. European Journal of
Operational Research, 56 (1), 2–14.
Huang, E., & Korf, R. E. (2009). New improvements in optimal rectangle packing. In
Boutilier, C. (Ed.), IJCAI, pp. 511–516.
Huang, E., & Korf, R. E. (2010). Optimal rectangle packing on non-square benchmarks. In
AAAI’10: Proceedings of the 24th National Conference on Artificial intelligence, pp.
317–324. AAAI Press.
Huang, E., & Korf, R. E. (2011). Optimal packing of high-precision rectangles. In Burgard,
W., & Roth, D. (Eds.), AAAI. AAAI Press.
Korf, R. E. (2003). Optimal rectangle packing: Initial results. In Giunchiglia, E., Muscettola,
N., & Nau, D. S. (Eds.), ICAPS, pp. 287–295. AAAI.
Korf, R. E. (2004). Optimal rectangle packing: New results. In Zilberstein, S., Koehler, J.,
& Koenig, S. (Eds.), ICAPS, pp. 142–149. AAAI.
Korf, R. E., Moffitt, M. D., & Pollack, M. E. (2010). Optimal rectangle packing. Annals of
Operations Research, 179 (1), 261–295.
Lahrichi, A. (1982). Scheduling: the notions of hump, compulsory parts and their use in
cumulative problems. Comptes Rendus de Academie des Sciences, Paris, 294, 209–211.
Lesh, N., Marks, J., McMahon, A., & Mitzenmacher, M. (2004). Exhaustive approaches to
2d rectangular perfect packings. Information Processing Letters, 90 (1), 7–14.
Li, S. C., Leong, H. W., & Quek, S. K. (2004). New approximation algorithms for some dynamic storage allocation problems. In Chwa, K.-Y., & Munro, J. I. (Eds.), COCOON,
Vol. 3106 of Lecture Notes in Computer Science, pp. 339–348. Springer.
Lipovetskii, A. I. (2008). A geometrical approach to computation of the optimal solution
of the rectangle packing problem. American Mathematical Society Translations, 222,
93–110.
Lodi, A., Martello, S., & Monaci, M. (2002). Two-dimensional packing problems: A survey.
European Journal of Operational Research, 141 (2), 241–252.
Lodi, A., Martello, S., & Vigo, D. (2002). Recent advances on two-dimensional bin packing
problems. Discrete Applied Mathematics, 123 (1-3), 379–396.
Martello, S., & Vigo, D. (1998). Exact solution of the two-dimensional finite bin packing
problem. Management Science, 44 (3), 388–399.
86

Optimal Rectangle Packing: An Absolute Placement Approach

Meir, A., & Moser, L. (1968). On packing of squares and cubes. Journal of Combinatorial
Theory, 5 (2), 126–134.
Mitola, J., & Maguire, G. (1999). Cognitive radio: making software radios more personal.
IEEE Personal Communications Magazine, 6 (4), 13–18.
Moffitt, M. D., & Pollack, M. E. (2006). Optimal rectangle packing: A meta-csp approach.
In Long, D., Smith, S. F., Borrajo, D., & McCluskey, L. (Eds.), ICAPS, pp. 93–102.
AAAI.
Onodera, H., Taniguchi, Y., & Tamaru, K. (1991). Branch-and-bound placement for building
block layout. In DAC ’91: Proceedings of the 28th ACM/IEEE Design Automation
Conference, pp. 433–439, New York, NY, USA. ACM.
Simonis, H., & O’Sullivan, B. (2008). Search strategies for rectangle packing. In Stuckey,
P. J. (Ed.), CP, Vol. 5202 of Lecture Notes in Computer Science, pp. 52–66. Springer.
Simonis, H., & O’Sullivan, B. (2011). Almost square packing. In Achterberg, T., & Beck,
J. C. (Eds.), CPAIOR, Vol. 6697 of Lecture Notes in Computer Science, pp. 196–209.
Springer.
Sweeney, P. E., & Paternoster, E. R. (1992). Cutting and packing problems: A categorized,
application-orientated research bibliography. The Journal of the Operational Research
Society, 43 (7), 691–706.
Weglarz, J. (1999). Project scheduling: recent models, algorithms and applications. Springer,
Kluwer.
Yap, R. H. C. (2004). Constraint processing by rina dechter, morgan kaufmann publishers,
2003, hard cover: Isbn 1-55860-890-7, xx + 481 pages. Theory Pract. Log. Program.,
4 (5-6), 755–757.

87

